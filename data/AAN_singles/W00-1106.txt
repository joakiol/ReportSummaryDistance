Corpus -Based  Learn ing  of  Compound Noun Index ing  *Byung-Kwan Kwak,Jee-Hyub Kim,and Geunbae Lee tNLP Lab., Dept.
of CSEPohang University ofScience & Technology(POSTECH){nerguri,gblee} @postech.ac.krJ ung  Yun  SeoNLP Lab.,Dept.
of Computer  ScienceSogang Universityseojy@ccs.sogang.ac.krAbst ractIn this paper, we present a corpus-based learning method that canindex diverse types of compoundnouns using rules automatically ex-tracted from a large tagged corpus.We develop an efficient way of ex-tracting the compound noun index-ing rules automatically and performextensive experiments to evaluateour indexing rules.
The automaticlearning method shows about thesame performance compared withthe manual inguistic approach butis more portable and requires nohuman efforts.
We also evaluatethe seven different filtering meth-ods based on both the effectivenessand the efficiency, and present anew method to solve the problems ofcompound noun over-generation a ddata sparseness in statistical com-pound noun processing.1 In t roduct ionCompound nouns are more specific and ex-pressive than simple nouns, so they are morevaluable as index terms and can increasethe precision in search experiments.
Thereare many definitions for the compound nounwhich cause ambiguities as to whether a givencontinuous noun sequence is a compoundnoun or not.
We, therefore, need a clean" This research was supported by KOSEF specialpurpose basic research (1997.9 - 2000.8 #970-1020-301-3)t Corresponding authordefinition of compound nouns in terms of in-formation retrieval, so we define a compoundnoun as "any continuous noun sequence thatappears frequently in documents."
1In Korean documents, compound nouns arerepresented in various forms (shown in Table1), so there is a difficulty in indexing all typesof compound nouns.
Until now, there havebeen much works on compound noun index-ing, but they still have limitations of cover-ing all types of compound nouns and requiremuch linguistic knowledge to accomplish thisgoal.
In this paper, we propose a corpus-based learning method for compound nounindexing which can extract he rules automat-ically with little linguistic knowledge.Table 1: Various types of Korean compoundnoun with regard to "jeong-bo geom-saeg (in-formation retrieval)"jeong-bo-geom-saeg (information-retrieval)jeong-bo-eui geom-saeg (retrieval of information)jeong-bo geom-saeg (information retrieval)jeong-bo-leul geom-saeg-ha-neun(retrieving information)jeong-bo-geom-saeg si-seu-tem(information-retrieval system)As the number of the documents i growingretrieval, efficiency also becomes as importantas effectiveness.
To increase the efficiency, wefocus on reducing the number of indexed spu-rious compound nouns.
We perform experi-ments on several filtering methods to find thealgorithm that can reduce spurious compoundnouns most efficiently.1 The frequency threshold can be adjusted accord-ing to application systems.57The remainder of this paper ?
is organizedas follows.
Section 2 describes previous com-pound noun indexing methods for Korean andcompound noun filtering methods.
We showoverall compound noun indexing system ar-chitecture in Section 3, and expl~.~n each mod-ule of the system in Section 4 and 5 in de-tail.
We evaluate our method with standardKorean test collections in Section 6.
Finally,concluding remarks are given in Section 7.2 P rev ious  Research2.1 Compound Noun IndexingThere have been two different methodsfor compound noun indexing: statisticaland linguistic.
In one Statistical method,(Fagan, 1989) indexed phrases using sixdifferent parameters, including informationon co-occurrence of phrase elements, rela-tive location of phrase elements, etc., andachieved reasonable performance.
However,his method couldn't reveal consistent sub-stantial improvements on five experimentaldocument collections in effectiveness.
(Strza-lkowski et al, 1996; Evans and Zhai, 1996)indexed subcompounds from complex nounphrases using noun-phrase analysis.
Thesemethods need to find the head-modifier rela-tions from noun phrases and therefore requiredifficult syntactic parsing in Korean.For Korean, in one statistical method, (Leeand Ahn, 1996) indexed general Korean nounsusing n-grams without linguistic knowledgeand the experiment results showed that theproposed method might be Mmost as effec-tive as the linguistic noun indexing.
How-ever, this method can generate many spuri-ous n-grarn~ which decrease the precision insearch performance.
In linguistic methods,(Kim, 1994) used five manually chosen com-pound noun indexing rule patterns based onlinguistic knowledge.
However, this methodcannot index the diverse types of compoundnouns.
(Won et al, 2000) used a full parserand increased the precision in search experi-ments.
However, this linguistic method can-not be applied to unrestricted texts robustly.In summary, the previous methods,whether they are statistical or linguistic,have their own shortcomings.
Statisticalmethods require signiAcant amounts ofco-occurrence information for reasonableperformance and can not index the diversetypes of compound nouns.
Linguistic meth-ods need compound noun indexing rulesdescribed by human and sometimes resultin meaningless compound nouns, whichdecreases the performance of informationretrieval systems.
They cannot also cover thevarious types of compound nouns because ofthe limitation of human linguistic knowledge.In this paper, we present a hybrid methodthat uses linguistic rules but these rules areautomatically acquired from a large corpusthrough statistical learning.
Our method gen-erates more diverse compound noun index-ing rule patterns than the previous tandardmethods (Kim, 1994; Lee et ah, 1997), be-cause previous methods use only most gen-eral rule patterns (shown in Table 2) and arebased solely on human linguistic knowledge.Table 2: Typical hand-written compoundnoun indexing rule patterns for KoreanNoun without case makers / NounNoun with a genitive case maker / NounNoun with a nominal case maker oran accusative case maker \[Verbal common oun or adjectival common nounNoun with an adnominal ending \] NounNoun within predicate particle phrase / Noun(The two nouns before and after a slashin the pattern can form a single compound noun.
)2.2 Compound Noun F i l ter ingCompound noun indexing methods, whetherthey are statistical or linguistic, tend to gen-erate spurious compound nouns when theyare actually applied.
Since an information re-trieval system can be evaluated by its effec-tiveness and also by its efficiency (van Rijs-bergen, 1979), the spurious compound nounsshould be efficiently filtered.
(Kando et al,1998) insisted that, for Japanese, the smallerthe number of index terms is, the better theperformance of the information retrieval sys-tem should be.58For Korean, (Won et al, 2000) showedthat segmentation f compound nouns is moreefficient than compound noun synthesis insearch performance.
There have been manyworks on compound noun filtering methods;(Kim, 1994) used mutual information only,and (Yun et al, 1997) used mutual informa-tion and relative frequency of POS (Part-Of-Speech) pairs together.
(Lee et ai., 1997) usedstop word dictionaries which were constructedmanually.
Most of the previous methods forcompound noun filtering utilized only oneconsistent method for generated compoundnouns irrespective of the different origin ofcompound noun indexing rules, and the meth-ods cause many problems due to data sparse-hess in dictionary and training data.
Ourapproach solves the data sparseness problemby using co-occurrence information on auto-matically extracted compound noun elementstogether with a statistical precision measurewhich fits best to each rule.3 Overa l l  Sys tem Arch i tec tureThe compound noun indexing system pro-posed in this paper Consists of two majormodules: one for automatically extractingcompound noun indexing rules (in Figure 1)and the other for indexing documents, fil-tering the automatically generated compoundnouns, and weighting the indexed compoundnouns (in Figure 2).Compound ~ Tagged Corpus 1Compound ~ RNoun StatisticalInformation~ Roles withPrecisionExtracted Rules IFiltered RulesFigure 1: Compound noun indexing-rule ex-traction module (control flow =~, data flowCompound Noun~-----~-~ Indexing I ~_Indexing~'~ /Rules wig f--.......?-.~Compound//Compound Noun\[ s~i~ \[ Infonnadon \[Find \[ Compound I Nouns , IWeighted.
CompoundNounsFigure 2: Compound noun indexing, filtering,and weighting module (control flow =~, dataflow ~)4 Automat ic  Ext rac t ion  ofCompound Noun Index ing  Ru lesThere are three major steps in automaticallyextracting compound noun indexing rules.The first step is to collect compound nounstatistical information, and the second step isto extract the rules from a large tagged cor-pus using the collected statistical information.The final step is to learn each rule's precision..4.1 Col lect ing Compound NounStatist icsWe collect initial compound noun seeds whichwere gathered from various types of well-balanced ocuments uch as ETRI Kemongencyclopaedia 2 nd many dictionaries on theInternet, and we collected 10,368 seeds, asshown in Table 3.
The small number of seedsare bootstrapped to extract the Compoundnoun indexing rules for various corpora.Table 3: Collected compound noun seedsNo.
of 2 3 Totalcomponent elementsETRI Kemong encyclomedia 5,100 2,088 7,188Internet dictionaries 2,071 1,109 3,180To collect more practical statistics on thecompound nouns, we made a 1,000,000 eo-jeol(Korean spacing unit which corresponds2 Courteously provided by ETRI, Korea.59to an English word or phrase) tagged cor-pus for a compound noun indexing experi-ment from a large document set (Korean In-formation Base).
We collected complete com-pound nouns (a continuous noun sequencecomposed of at least two nouns on the condi-tion that both the preceding and the followingPOS of the sequence axe not nouns (Yoon etal., 1998)) composed of 1 - 3 no, ms from thetagged training corpus (Table 4).Table 4: Statistics for complete compoundnounsNo.
of 1 2 3component elementsVocabulary 264,359 200,455 63,7904.2  Ext rac t ing  Index ing  Ru lesWe define a template (in Table 5) to extractthe compound noun indexing rules from aPOS tagged corpus.The template means that if a front-condition-tag, a rear-condition-tag, and sub-string-tags are coincident with input sentencetags, the lexical item in the synthesis positionof the sentence can be indexed as a compoundnoun as "x /  y (for 3-noun compounds, x /y / z)".
The tags used in the template arePOS (Part-Of-Speech) tags and we use thePOSTAG set (Table 17).The following is an algorithm to extractcompound noun indexing rules from a largetagged corpus using the two-noun compoundseeds and the template defined above.
Therule extraction scope is limited to the endof a sentence or, if there is a conjunctiveending (eCC) in the sentence, only to theconjunctive nding of the sentence.
A ruleextraction example is shown in Figure 3.Algorithm 1: Extracting compound nounindexing rules (for 2-noun compounds)Read TemplateRead Seed(Consist of Constituent 1 / Constituent 2)TokeD/ze Seed into ConstituentsPut Constituent 1 into Key1 and Constituent 2?
into Key2While (Not(End of Documents)){Read Initial Tag of SentenceWhile (Not(End of Sentence or eCC)){Read NeIt Tag of SentenceIf (Read Tag =ffi Key1){While (Not(End of Sentence or eCC))(Read Next Tag of SentenceIf (Current Tag == Key2)Write Rule accordingto the Template}}}The next step is to  refine the extractedrules to select he proper ones.
We used a rulefiltering algorithm (Algorithm 2) using thefrequency together with the heuristics thatthe rules with negative lexical items (shown inTable 6) will make spurious compound nouns.Algorithm 2: Filtering extracted rules us-ing frequency and heuristicsI.
For each compound noun seed, selectthe rules whose frequency is greater than 2.2.
Among rules selected by step 1, selectonly rules that are extractedat least by 2 seeds.3.
Discard rules which containnegative lexical items.Table 5: The template to extract the com-pound noun indexing rules,ofront-condition-tag Isub-string-tags (tag 1 tag 2 ... tag n-1 tag n) \[rear-condition-tag Isynthesis locations (x y)lexicon x / lexicon y(for 3-noun compounds,synthesis locations (x, y, z)lexicon x / lexicon y / lexicon z)Table 6: Negativenegative items (tags)je-oe(MC) (exclude)eobs(E) (not exist)mos-ha(D) (can not)lexical item examplesexample phrasesno-jo-leul je-oe-han hoe-eui(meeting excluding union)sa-gwa-ga eobs~neun na-mu(tree without apple)dog-lib-eul mos-han gug-ga(country thatcannot be liberated)We automatically extracted and filtered out60Tagged ~t~B,~baI-Ib,MC< .kong-bo >.iC<tmb.MC< geom-sacg >fron~com~illm1_~g I sub_s~ring_mgs (~ I lag2 ... tag n-I ~n)~rcar_cond~iol~.
I~ syn~lcsls location (x y) ~> lexicon x I Icxlco~ y(i.formafioa~?uicvallIndcxlag Rule:B I MC.jC<leul> MC I y I l 3Figure 3: Rule Extraction Process Example2,036 rules from the large tagged corpus (Ko-rean Information Base, 1,000,000 eojeol) us-ing the above Algorithm 2.
Among the ill-tered rules, there are 19 rules with negativelexical items and we finally selected 2,017rules.
Table 7 shows a distribution of the finalrules according to the number of elements intheir sub-string-tags.Table 7: Distribution of extracted rules bynumber of elements in sub-string-tagsNo.
Distribution Example2 tags 79.6 % MC MC3 tags 12.6 % MC jO(eui) MC4 tags 4.7 % MC y eCNMG MC5 tags 1.5 % MC MC jO(e)DI<sog-ha-neun) MCover 6 tags 1.6 %The automatically extracted rules havemore rule patterns and lexical items thanhuman-made rules so they can cover morediverse types of compound nouns (Table 8).When checking the overlap between the tworule collections, we found that the manual in-guistic rules are a subset of our automaticallygenerated statistical rules.
Table 9 showssome of the example rules newly generatedfrom our extraction algorithm, which wereoriginally missing in the manual rule patterns.4.3 Learning the Precision o fExt racted  RulesIn the proposed method, we use the precisionof rules to solve the compound noun over-generation and the data sparseness problems.The precision of a rule can be defined byTable 8: Comparison between the automati-cally extracted rules and the manual rulesMethodManuallinguisticmethodOur methodNo.
of No.
ofgeneral lexical termsrule patterns used in rule patterns1623 78Table 9: Examples of newly added rule pat-ternsRuleNoun + bound noun / NounNoun + suffix / NounNoun + suffix + assignment verb +adnominal ending / Nouncounting how many indexed compound nouncandidates generated by the rule are actualcompound nouns:YactuatPrec(rule) = Ncandidatewhere Prec(rule) is the precision of a rule,Ndctual is the number of actual compoundnouns, and Ncandidat e is the number of com-pound noun candidates generated by the au-tomatic indexing rules.To  calculate the precision, we need a defin-ing measurement for compound noun identi-fication.
(Su et al, 1994) showed that theaverage mutual information of a compoundnoun tends to be higher than that of a non-compound noun, so we try to use the mutualinformation as the measure for identifying thecompound nouns.
If the mutual informationof the compound noun candidate is higherthan the average mutual information of thecompound noun seeds, we decide that it isa compound noun.
For mutual information(MI), we use two different equations: one fortwo-element compound nouns (Church andHanks, 1990) and the other for three-elementcompound nouns (Suet  al., 1994).
The equa-tion for two-element compound nouns is asfollow:P(x,y)I(x;y) = log 2 P(x) x P(y)61where x and y are two words in the corpus,and I(x; y) is the mutual information of thesetwo words (in this order).
Table 10 showsthe average MI value of the two and threeelements.Table 10: Average value of the mutual infor-mation (MI) of compound noun seeds.Number of elements \[ 2 I 3Average MI 3.56 3.62The MI was calculated from the statistics ofthe complete compound nouns collected fromthe tagged training corpus (see Section 4.1).However, complete compound nouns arecontinuous noun sequences and cause thedata sparseness problem.
Therefore, we needto expand the statistics.
Figure 4 showsthe architecture of the precision learningmodule by expanding the statistics of thecomplete compound nouns along with analgorithmic explanation (Algorithm 3) of theprocess.
Table 11 shows the improvement inthe average precision during the repetitiveexecution of this learning process.Norm Statistical )Compound Norm of Rules ~ '~ Rule incision (step 5) (s~ 2.7) l~v v\[ (step s)Figure 4: Learning the precision of the com-pound noun indexing rules (The steps areshown in Algorithm 3)Algorithm 3:i.
Calculate all rules' initial precisionusing initial complete compound nounstatistical information.2.
Calculate the average precisionof the rules.3.
Multiply a rule's precision bythe frequency of the compound noun madeby the  ru le .We ca l l  th i s  va lue  the  mod i f ied  f requency(MF).4.
Collect the same compound nouns, andsum all the modified frequenciesfor each compound noun.5.
If the sunm~ed modified frequency is greaterthan a threshold, add this compound nounto the complete compound nouns ta t i s t i ca l  information.6.
Calculate all rules' precision againusing the changed complete compound nouns ta t i s t i ca l  in fo rmat ion .7.
Calculate the average precision of the rules.8.
If the average precision of the rules isequal to the previous average precision,stop.
Othervise, go to step 2.Table 11: Improvement in the average preci-sion of rulesLearning 1 2 3 4 5 6cyclesAvg.
prec.
0.19 0.23 0.39 0.44 0.45 0.45of rules5 Compound Noun Index ing ,F i l te r ing ,  and  Weight ingIn this section, we explai n how to use the au-tomatically extracted rules to actually indexthe compound nouns, and describe how to fil-ter and weight the indexed compound nouns.5.1 Compound Noun I ndex ingTo index compound nouns from documents,we use a natural anguage processing engine,SKOPE (Standard KOrean Processing En-gine) (Cha et al, 1998), which processes doc-uments by analysing words into morphemesand tagging part-of-speeches.
The taggingresults are compared with the automaticallylearned compound noun indexing rules and, ifthey are coincident with each other, we indexthem as compound nouns.
Figure 5 shows aprocess of the compound noun indexing withan example.5.2 Compound Noun F i l ter ingAmong the indexed compound nouns above,still there can be meaningless compoundnouns, which increases the number of indexterms and the search time.
To solve com-pound noun over-generation problem, we ex-periment with seven different filtering meth-ods (shown in Table 12) by analyzing their62?. "
Tagging Result:bbal-li jeong-bo-leul B<bbal-li>geom-saeg-ha-netm ~ Auaty~ ~.
~ MC<jeong-bo >(~evmg I \~'~?_"2:?
?
"/ ~ jc<,e.,>information I \ tagging / /1..~ ?
:-t.~..~ \] ~ --  - /  / I.MU< geom-saeg >/ I eCNMG<neun>"~d~"~g"l~ ~es-- \ ] 'X~mpoun"~ Indexedi 1,2 .Complete l /  ~ geom-saeg_ C?mp~No~ ~ - -  (mf , ,~o .
/Statistical Information \] retrieval)Figure 5: Compound noun indexing processrelative effectiveness and efficiency, as shownin Table 16.
These methods can be dividedinto three categories: first one using MI, sec-ond one using the frequency of the compoundnouns (FC), and the last one using the fre-quency of the compound noun elements (FE).MI (Mutual Information) is a measure of wordassociation, and used under the assumptionthat a highly associated word n-gram is morelikely to be a compound noun.
FC is usedunder the assumption that a frequently en-countered word n-gram is more likely to be acompound than a rarely encountered n-gram.FE is ;used under the assumption that a wordn-gram with a frequently encountered specificelement is more likely to be a compound.
Inthe method of C, D, E, and F, each thresholdwas decided by calculating the average num-ber of compound nouns of each method.Table 12: Seven different filtering methods(MI) A.
Mutual information of compoundnoun elements (0)(MI) B.
Mutual information of compoundnoun elements(average of MI of compound noun seeds)(FC) C. Frequency of compound nounsin the training corpus (4)(FC) D. Frequency of compound nounsin the test corpus (2)(FE) E. Frequency of compound noun headsin the training corpus (5)(FE) F. Frequency of compound noun modifiersin the training corpus (5)G. No filtering(The value in parantheses is a threshold.
)Among these methods, method B gener-ated the smallest number of compound nounsbest efficiency and showed the reasonable f-fectiveness (Table 16).
On the basis of thisfiltering method, we develop a smoothingmethod by combining the precision of ruleswith the mutual information of the compoundnoun elements, and propose our final filteringmethod (H) as follows:P(x, y) + ~ ?
Precision T(x, y) = log 2 P(x) x P(y)where a is a weighting coefficient and Preci-sion is the applied rules learned in Section 4.3.For the three-element compound nouns, theMI part is replaced with the three-element MIequation 3 (Su et al, 1994).6 Exper iment  Resu l tsTo calculate the similarity between a docu-ment and a query, we use the p-norm retrievalmodel (Fox, 1983) and use 2.0 as the p-value.We also use fhe  component nouns in a com-pound as the indexing terms.
We follow thestandard TREC evaluation schemes (Saltonand Buckley, 1991).
For single index terms,we use the weighting method atn.ntc (Lee,1995).6.1 Compound Noun Index ingExper imentsThis experiment shows how well the proposedmethod can index diverse types of compoundnouns than the previous popular methodswhich use human-generated compound nounindexing rules (Kim, 1994; Lee et al, 1997).For simplicity, we filtered the generated com-pound nouns using the mutual information ofthe compound noun elements with a thresh-old of zero (method A in Table 12).Table 13 shows that the terms indexed byprevious linguistic approach are a subset ofthe ones made by our statistical approach.This means that the proposed method cancover more diverse compound nouns than the3PD (x, ~, z)I(x;y;z) = log 2 Px(x,y,z)63Table 13: Compound noun indexing coverageexperiment (With a 200,000 eojeol Korean In-formation Base)Manuallinguisticrule patternsOurautomaticrule patternsNo.
ofgenerated actual 22,276 30,168compound nouns.
(+35.4 %)No.
ofgenerated actual 7,892compound nounswithout overlapmanual inguistic rule method.
We perform aretrieval experiment to evaluate the automat-ically extracted rules.
Table 144 and table 155show that our method has slightly better re-call and l l -point average precision than themanual inguistic rule method.Table 14: Compound noun indexing effective-ness experiment IAvg.
recallManual linguisticrule patterns82.66Our automaticrule patterns83.62(+1.16 %)ll-pt.
42.24 42.33avg.
precision (+0.21%)No.
of 504,040 515,801index terms (+2.33 %)Table 15: Compound noun indexing effective-ness experiment IIAvg.
recallll-pt, avg.precisionNo.
ofindex termsManual linguisticrule patterns86.3234.331,242,458Our automaticrule patterns87.50(+1.35 %)34.54(+0.61%)1,282,818(+3.15 %)4 With KTSET2.0 test collections (Courteouslyprovided by KT, Korea.
(4,410 documents and 50queries))s With KRIST2.0 test collection (Courteously pro-vided by KORDIC, Korea.
(13,514 documents and 30queries))6.2 Retrieval Experiments UsingVarious Filtering MethodsIn this experiment, we compare the seven fil-tering methods to find out which one is thebest in terms of effectiveness and efficiency.For this experiment, we used our automaticrules for the compound noun indexing, andthe test collection KTSET2.0.
To check theeffectiveness, we used recall and l l -point  av-erage precision.
To check the efficiency, weused the number of index terms.
Table 16shows the results of the various filtering ex-periments.From Table 16, the methods using mu-tual information reduce the number of in-dex terms, whereas they have lower precision.The reason of this lower precision is that MIhas a bias, i.e., scoring in favor of rare termsover common terms, so MI seems to have aproblem in its sensitivity to probabil ity es-t imation error (Yang and Pedersen, 1997).In this experiment 6, we see that method Bgenerates the smallest number of compoundnouns (best efficiency) and our final propos-ing method H has the best recall and precision?
(effectiveness) with the  reasonable number ?
ofcompound nouns (efficiency).
We can con-clude that the filtering method H is the best,considering the effectiveness and the efficiencyat the same time.7 Conc lus ionIn this paper, we presented a method to ex-tract the compound noun indexing rules au-tomatically from a large tagged corpus, andshowed that this method can index compoundnouns appearing in diverse types of docu-ments.In the view of effectiveness, this method isslightly better than the previous linguistic ap-proaches but requires no human effort.The proposed method also uses no parserand no rules described by humans, there-fore, it can be applied to unrestricted textsvery robustly and has high domain porta-6 Our Korean NLQ (Natural Lan-guage Querying) demo system (located in'http:/ /nlp.postech.ac.kz /Resarch/POSNLQ/')can be tested.64Table 16: Retrieval experimentA B CAverage 83.62 83.62 83.62recall (+0.00) (+0.00)ll-pt, avg.
42.45 42.42 42.49precision (-0.07) (+0.09)Precision 52.11 52.44 52.07at 10 Docs.No.
of 515,80 508,20 514,54 5~index terms (-1.47) (-0.24) (-+resultsD83.62(+0.00)42.55(+0.24)52.8047,27+6.10)of various filteringE F83.62(+0.00)42.72(+0.64)52.26572,36(+10.97)83.62(+0.00)42.48(+0.07)51.89574,04(+11.29); methodsG84.32(+0.84)42.48(+0.07)52.81705,98(+36.87)H84.32(.+0.84)42.75(+o.71)52.98509,90(-1.14)bility.
We also presented a filtering methodto solve the compound noun over-generationproblem.
Our proposed filtering method (H)shows good retrieval performance both in theview of the effectiveness and the efficiency.In the future, we need to perform someexperiments on much larger commercialdatabases to test the practicality of ourmethod..
Finally, our method doesn't  require lan-guage dependent knowledge, so it needs to beverified whether it can be easily applied toother languages.ReferencesJeongwon Cha, Geunbae Lee, and Jong-HyeokLee.
1998.
Generalized unknown morphemeguessing for hybrid pos tagging of korean.In Proceedings of SIXTH WORKSHOP ONVERY LARGE CORPORA in Coling-ACL 98.K.
W. Church and P. Hanks.
1990.
Word associ-ation norms, mutual information, and lexicog-raphy.
Computational Linguistics, 16(1):22-29.David A. Evans and Chengxiang Zhai.
1996.Noun-phrase analysis in unrestricted text forinformation retrieval.
In Proceedingof the 3~thAnnual Meetinof the Association for Computa-tional Linguistics, Santa Cruz, CA, pages 17-24.Joel L. Fagan.
1989.
The effectiveness of a non-syntactic approach to automatic phrase index-ing for document retrieval.
JASIS, 40(2):115-132.E.
A.
Fox.
1983.
Extending the Boolean and Vec-tor Space Models of Information Retrieval withP-norm Queries and Multiple Concept Types.Ph.D.
thesis, Cornell Univ.Noriko Kando, Kyo Kageura, Masaharu Yoshoka,and Keizo Oyama.
1998.
Phrase processingmethods for japanase text retrieval.
SIGIR fo-rum, 32(2):23-28.Pan Koo Kim.
1994.
The automatic indexingof compound words from korean text based onmutual information.
Journal of KISS (in Ko-rean), 21(7):1333-1340.Joon Ho Lee and Jeong Soo Ahn.
1996.
Usingn-grams for korean text retrieval.
In SIGIR'96,pages 216-224.Hyun-A Lee, Jong-Hyeok Lee, and Geunbae Lee.1997.
Noun phrase indexing using clausalsegmentation.
Journal of KISS (in Korean),24(3):302-311.Joon Ho Lee.
1995.
Combining multiple videncefrom different properties of weighting schemes.In SIGIR'95, pages 180-188.Gerard Salton and Chris Buckley.
1991.Text retrieval conferences evaluation pro-gram.
In .ftp://)2p.cs.corneU.edu/pub/smart/,trec_eval.7.0beta.tar.gz.Tomek Strzalkowski, Louise Guthrie, Jussi Karl-gren, Jura Leistensnider, Fang Lin, Jose Perez-Carballo, Troy Straszheim, Jin Wang, and JonWilding.
1996.
Natural anguage informationretrieval: Trec-5 report.
In The Fifth TextREtrieval conference (TREC-5), NIST Specialpublication, pages 500-238.Keh-Yih Su, Mind-Wen Wu, and Jing-ShinChang.
1994.
A corpus-based approach to au-tomatic ompound extraction.
In Proceedingsof ACL 94, pages 242-247.C.
J. van Rijsbergen.
1979.
Information Re-trieval.
University of Computing Science,Lodon.Hyungsuk Won, Mihwa Park, and Geunbae Lee.2000.
Integrated multi-level indexing methodfor compound noun processing.
In Journal o.fKISS, 27(1) (in Korean), pages 84-95.65TagMCTBDIIjseGSeCNMMeCC+sos.sfTable 17: The POS (Part-Of-Speech) set of POSTAGcommon nounpronounadverbirregular verbassignment verbauxiliary particleprefmal endingnominal endingconjunctive endingprefixother symbolsentence closerforeign wordMPGKHI~EjoeCNDIeCNMGYS cs -shDescriptionproper nounadnouninterjectionregular adjectiveexistential predicateother particleattx conj endingadnomina l  end ingpredicative particlesuffixleft parenthesissentence connectionChinese characterTagMDSDRHIjceGEeCNDCeCNBbsus 's ,Descriptionbound nounnumeralregular verbirregular adjectivecase particlefinal endingquote conj endingadverbial endingauxiliary verbunit symbolright parenthesissentence commaYiming Yang and Jan O. Pedersen.
1997.
A com-parative study on feature selection in text cat-egorization.
In Douglas H. Fisher, editor, Pro-ceedings of ICML-97, l~th International Con-ference on Machine Learning, pages 412--420,Nashville, US.
Morgan Kaufmann Publishers,San Francisco, US.Jun-Tae Yoon, Eui-Seok Jong, and Mansuk Song.1998.
Analysis of korean compound noun in-dexing using lexical information between ouns.Journal of KISS (in Korean), 25(11):1716-1725.Bo-Hyun Yun, Yong-Jae Kwak, and Hae-ChangRim.
1997.
A korean information retrievalmodel alleviating syntactic term mismatches.In Proceedings ofthe Natural Language Process-ing Pacific Rim Symposium, pages 107-112.66
