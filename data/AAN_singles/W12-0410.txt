Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 63?71,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsArgument Formation in the Reasoning Process: Toward a GenericModel of Deception DetectionDeqing Li and Eugene Santos, Jr.Thayer School of EngineeringDartmouth CollegeHanover, N.H., U.S.A{ Deqing.Li, Eugene.Santos.Jr }@Dartmouth.eduAbstractResearch on deception detection has beenmainly focused on two kinds ofapproaches.
In one, people considerdeception types and taxonomies, and usedifferent counter strategies to detect andreverse deception.
In the other, peoplesearch for verbal and non-verbal cues inthe content of deceptive communication.However, general theories that studyfundamental properties of deceptionwhich can be applied in computationalmodels are still very rare.
In this work,we propose a general model of deceptiondetection guided by a fundamentalprinciple in the formation ofcommunicative deception.
Experimentalresults using our model demonstrate thatdeception is distinguishable fromunintentional misinformation.IntroductionConventional research on deception detectionfocuses on deception taxonomies and deceptioncues.
Unfortunately, both of them neglect the factthat deception is rooted in the formation ofarguments mainly because such formation is notdirectly observable.
However, since theformation of arguments is where theimplementation of deception starts, it isnecessary to study it in depth.The act of deceiving involves two processes:the formation of deceptive arguments (thereasoning) and the communication of deception.The communication part is intuitive tounderstand and has been the focus of recentresearch efforts in deception detection.
Thereasoning part is a necessary component ofdeception because deceiving has been found torequire a heavier cognitive load than telling thetruth (Greene et.
Al, 1985).
The reasoning processinvolves generating and selecting argumentswhile the communication process involveswording and phrasing of the arguments.Deception detection in the process ofcommunication is not ideal because firstly, it iseasy to hide deceptive cues using carefulwording and phrasing, and secondly, wordingand phrasing of communication are mediated bythe framing of the other party?s response (e.g.
theanswer to the question ?Did you go to classtoday??
always starts with ?Yes, I?
or ?No, I?
).On the other hand, it is hard to hide the intent ofdeception by distorting arguments formed in thereasoning process because it requires higher-order deception that takes the other party?s intentand even the other party?s belief about thespeaker?s intent into consideration.
Higher-orderdeception demands much more cognitive loadthan first-order deception in order to retrieve thememory about the other party?s intent andleverage the original reasoning process behind it.Thus, the reasoning process provides moreeffective and reliable observations than thecommunication process.
Moreover, it also guidesand explains some observations in thecommunication process such as compellingnessand level of detail of a story.We will illustrate the formation of deceptivearguments in the next section, according towhich, we propose three hypotheses of thefundamental differences between deception andnon-deception.
In Section 3, we describe ourmodel of detection and the data simulationprocess.
Experiment setting and results are63discusses in Section 4, followed by conclusionsand future work in Section 5.1 Formation of Deceptive ArgumentThe reasoning process can be regarded asinference based on the conditional relationshipbetween arguments by assuming that humanreasoning is akin to informal logic.
Sincedeceivers intentionally reach the conclusion thatthey target at, we propose that the act ofdeceiving is to reason by supposing the truth ofdeceivers?
targeted arguments, but the truth ofthe targeted arguments is not actually believed bythe deceivers.
For example, if a person is askedto lie about his attitude on abortion, he mightraise arguments such as ?fetuses are human?,?god will punish anyone who aborts children?and ?children have the right to live?.
He did notraise these arguments because he believed inthem but because they support the falseconclusion that he is against abortion.
It is thusnatural to imagine that the conclusion comes intodeceivers?
minds before the arguments.According to Levi (1996), ?The addition of thesupposition to the agent?s state of full belief doesnot require jettisoning any convictions alreadyfully believed.
The result of this modification ofthe state of full belief by supposition is a newpotential state of full belief containing theconsequences of the supposition added and theinitial state of full belief?, which means that thereasoning with a supposition is a regularreasoning with the addition of a piece ofknowledge that has been assumed before thereasoning starts.
It also follows that the reasoningwith a supposition can be exactly the same as aregular reasoning in which the supposition in theformer case is a true belief.
That is to say, thereasoning in deception formation can be regardedto follow the same scheme as that in truthargumentation.
However, even if deceiver andtruth teller share the same reasoning scheme,their beliefs and processes of reasoning aredifferent.
In particular, if an opinion-based storyis required from the speaker, truth tellerspropagate beliefs from evidence, while deceiversadapt beliefs to suppositions.
If an event-basedstory is required, truth tellers retrieve relevantmemory which is based on past behavior andpast behavior is based on past belief, which waspropagated from past evidence, while deceiverssuppose a part of the event and adapt his fantasyto the supposition.
This fundamental differencein the reasoning of deceiver and truth teller isunavoidable due to the intentionality ofdeceivers.
It provides reasoning a stable groundon which schemes of deception detection can bebuilt.As we have discussed, the product ofreasoning from truth teller and deceiver may beexactly the same.
However it is hardly true in thereal world because they do not share the samebelief system that supports their reasoning.
If inany case they do share the same belief system,they would reach the same conclusion withoutany deception and there would be no need todeceive.
In order to mimic truth teller?s story,deceiver may manipulate his conclusion anddistort other arguments to support themanipulated conclusion, but the supportingarguments are biased by his honest but untruthfulbelief system.
Therefore, discrepancies inarguments that deceivers are reluctant tobelieve but truth tellers embrace can beexpected.
On the other hand, deception has beendefined as ?a relationship between twostatements?
(Shibles, 1988), according to which,deception is a contradiction between belief andexpression.
A deceiver may lie about the polarityof belief as well as the strength or extent of beliefas long as his belief expression deviates from hishonest reasoning.
The more manipulation he didto mimic the truth, the farther he deviates fromhimself.
Therefore, discrepancies inarguments that are manipulated by deceiverscan be expected.
The above two discrepancies indeception have been popularly embraced byexisting researchers (Mehrabian, 1972; Wiener &Mehrabian, 1968; Johnson & Raye, 1981,Markus, 1977).
Our focus is to explain andmeasure them in terms of human reasoning, andargue that these two discrepancies follow ourproposal that deceptive reasoning is reasoningwith presupposition, due to which thediscrepancies are the fundamental differencebetween deception and truth that produces otherobservable patterns.2 Hypotheses and JustificationWe have argued that the basic discrepancy indeceptive reasoning exists in inconsistency anduntruthfulness.
Inconsistency means that thearguments in the story contradict with what thespeaker would believe.
Untruthfulness meansthat the arguments in the story contradict withwhat an honest person would believe in order toreach the conclusion.
On the other hand,inconsistency indicates that an honest person64should behave as he always does, which requiressome familiarity with the speaker, whereasuntruthfulness indicates that an honest personshould behave as a reasonable and convincingperson, which requires some knowledge of thetopic domain.
Opinion change violates theformer one but not the latter one as it changes theprior knowledge but still maintains truthfulness,and innovation violates the latter one but not theformer one as innovation is convincing but notexpectable.
They do not violate both so they arenot deceptions.
However, these two elements arenot the unique characteristics of deceptionbecause random manipulations without anypurpose to deceive such as misinformation alsoshow inconsistency and untruthfulness.Fortunately, deceivers can be distinguished bythe manner they manipulate arguments.
Wepropose the following hypotheses that can beexpected in deceptive stories but not others.Firstly, explicit manipulations in deceptioncontinuously propagate to other arguments whichbecome implicit manipulations.
The purpose, ofcourse, is to spread the manipulation to theconclusion.
The propagation spreads tosurrounding arguments and the influence ofmanipulation decreases as the propagationspreads farther away, which randommanipulations do not exhibit.
If one overlooksthe abnormality of the explicit manipulations, thestory would seem to flow smoothly from thearguments to the conclusion because theconnection between the arguments is not broken.Inconsistency is particularly important whenindividual difference should be considered.Secondly, there is a correspondence betweeninconsistency and untruthfulness.
Someinconsistencies were manipulated significantlybecause the deceiver wants to convince thelistener of the argument and these argumentsseem more reasonable to support the conclusionafter manipulation.
Therefore, the significantmanipulations are often convincing, but there arealso exceptions in which deceivers overlymanipulate arguments that are usually ignored bytruth tellers.
We call these Type I incredibility:incredibility due to over-manipulation.
Thearguments that are not convincing usually can befound in the inconsistencies that were slightlymanipulated or ignored by the deceiver becausedeceivers do not know that they are importantsupports to the conclusion but truth tellers neverneglect these details.
This is called Type IIincredibility: incredibility due to ignorance.
TypeI and Type II incredibility are two examples ofunconvincing arguments (According to DePauloet.
al (2003), liars tell less compelling tales thantruth tellers), which can be quantitativelymeasured in the reasoning process.
On the otherhand, random manipulations do not show thiscorrespondence between inconsistency anduntruthfulness.
Measuring untruthfulness isparticularly effective in detecting deception fromgeneral population whom the detector is notfamiliar with.Thirdly, deceptions are intentional, whichmeans the deceiver assumes the conclusionbefore inferring the whole story.
Or in otherwords, deceivers fit the world to their mind,which is a necessary component of intentionalityaccording to Humberstone (1992).
They areconvincers who reach arguments fromconclusions, while others reach conclusions fromarguments.
According to the satisfaction ofintention (Mele, 1992), an intention is "satisfied"only if behavior in which it issues is guided bythe intention-embedded plan.
Thus, deceiverschoose the best behavior (argument in this case)that is guided (inferred in this case) by his desire(conclusion in this case), but not any behaviorthat can fulfill his desire.
In particular, deceiverswill choose the state of the argument in the storythat is most effective compared with other statesof the argument in reaching the conclusion of thestory (e.g.
the best state of whether ?an unbornbaby is a life?
towards the conclusion ofsupporting abortion is no).
In deception, theinconsistent arguments are usually effective tothe conclusion, while in random manipulation theinconsistent arguments are not.Inconsistency, untruthfulness, propagatedmanipulation and intentionality are the guidingconcepts of our deception detection method,which is a general model independent of thedomain knowledge.3 MethodologyIn this work, we will not only test the hypothesesproposed above, but also provide acomputational model to identify the discrepancyin arguments that are manipulated by deceiversand the discrepancy in arguments that are not asconvincing as truth tellers?.3.1 Computational Model of DeceptionDetectionWe propose a generic model to detect deceptionthrough the reasoning process without assuminghuman?s reasoning scheme.
As shown in Figure651, the model is composed of two networks:Correlation Network and Consensus Network.Correlation Network connects each agent withagents who correlate with him in a specificargument.
Neighbors in the Correlation Networkrepresent acquaintances who can anticipate eachother?s arguments.
Consensus Network connectsagents with similar conclusions.
Neighbors in theConsensus Network represent people who agreewith each other.
We have pointed out thatdeception is deviation from one?s own subjectivebeliefs, but not deviation from the objectivereality or from the public.
Thus CorrelationNetwork is essential in predicting an agent?sbelief according to neighbors who can expecteach other.
This idea of measuring individualinconsistency has been discussed in our formerwork (Santos et.
Al, 2010), which also providesdetails on the computation.
The ConsensusNetwork provides a sampled population of truthtellers who reach the same conclusion as thedeceiver.
If the deceiver told the truth, he shouldbehave in no difference with the population.
Theuntruthfulness of the deceiver can be evaluatedby comparing the deceiver with the truth tellers.Functionality of the arguments can be revealedfrom the history data of the deceiver.
Bystudying the history data, we can evaluate whicharguments are effective to which from theperspective of the deceiver.Figure 1: Architecture of the model of deceptiondetection3.2 Date Collection and SimulationTo test the hypotheses we proposed, we simulatethe reasoning process of a deceiver according toour assumption that deceivers pre-supposeconclusions before reasoning.
The deceiver wesimulate is a plaintiff in a lawsuit of a rape caseshown in a popular Hong Kong TV episode.
Thecase is described as following.
A femalecelebrity coded as A claims that she was raped byan Indian young man coded as B.
A claims thatshe keeps away from B because both her and hermother do not like the physical odor of Indians.A claims that B once joined her birthday partywithout any invitation and fed A drugs.
B thenconveyed A home and raped A.
After A?sboyfriend arrived, A called police.
However, thetruth is that B is a fan of A and joined A?s party atA?s invitation.
A lied about her aversion toIndians because she used to prostitute to Indians.Besides, B is new to the party club, so it isunlikely for him to obtain drugs there.
A useddrugs and enticed B to have sex with her.
Thisartificial scenario is a simplification of a possiblelegal case, which provides realistic explanationscompared with simulation data that simulatedeception arbitrarily without considering theintent of deceiver.
We did not use real cases orlab surveys because they either do not have theground truth of the speaker?s truthfulness or lacksufficient information about the reasoning of thedeceiver.
Data that do have both ground truth andsufficient information such as military combatscenarios are mostly focused on behavioraldeception instead of communicative deception.In addition, real cases may contain noisy data inwhich the communication content is mediated byfactors other than reasoning.
For the purpose ofevaluating hypotheses about deceptive reasoningit is ideal to use clean data that only contains thesemantic meaning of arguments.
The evaluationof the hypotheses guides the development of ourdetection model, which we will apply to real dataeventually.A?s belief system is represented by a BayesianNetwork (BN) (Pearl, 1988).
BNs have beenused to simulate human reasoning processes forvarious purposes and have been shown to beconsistent with the behavior of human (Tenenbauet.
Al, 2006).
A BN is a graphical structure inwhich a node represents a propositionalargument and the conditional probabilitybetween nodes represent the conditionalrelationship between arguments.
For example,the reasoning that B drives A home because Bknows A?s address can be encoded in theconditional probabilityP(B_drive_A_home|B_know_A_s_adr)=0.9.
Inorder to eliminate the variation due to wording,the semantics of the arguments instead of thephrases are encoded in the nodes.
We designed aBN representing A?s belief system and also a BN66representing the belief system of a true victim ofthe rape case according to the description of thescenario and some common sense.
Morespecifically, we connect two arguments if theircausal relationship is explicitly described by thedeceiver or by the jury when they are analyzingthe intent of the deceiver.
The conditionalprobabilities between states of arguments are setas 0.7 to 0.99 according to the certainty of thespeaker if they are explicitly described.
As to thestates that are not mentioned in the case, they areusually implied in or can be inferred from thescenario if their mutual exclusive states aredescribed in the scenario, such as the probabilityof A_hate_Indian given that B?s relation with A?smother is good and that A used to prostitute toIndians.
Otherwise the mutual exclusive statesare given the same or similar probabilitiesindicating that they are uncertain.
To make surethat the discrepancies in deception are resultedfrom the manner of reasoning instead of from theinherent difference between the deceiver?s beliefsystem and the true victim?s belief system, weminimize the difference between their beliefsystems.
Specifically, we keep all theirconditional probabilities the same by assumingthat both are rational people with the samecommon sense.
Only their prior probabilities ofA?s experience as prostitute and whether B isnew to the party or not are adjusted differently,because they are the essential truth in a truevictim?s perspective.
That is to say, those who donot like Indians could not prostitute to them, andto obtain drugs from the party club, B has to be aregular guest.
However, as a result of sharing asimilar belief system with the true victim, thedeceiver?s story may become highly convincing.Although we expect it to be hard to detect theuntruthfulness of the deceiver, the deceiver?ssimulation is not unrealistic because somedeceivers are consistently found to be morecredible than others based on the research byBond and Depaulo (2008).
It is highly likely thata randomized BN with a perturbed copy can alsoserve our purposes, but again, building beliefsystems based on the intent of deception willprovide more realistic data, more convincingresults and more intuitive explanations.
The BNof the deceiver is depicted in Figure 2.
Itsconditional probability tables are shown inAppendix A.The process of reasoning is represented by theprocess of inferencing, and the product ofreasoning is represented by the inferredprobabilities of the nodes.
Computing posteriorprobabilities, P(A|E), is not feasible here since itdoes not consider the consistency over allvariables.
Consider the following example.Suppose 10 people join a lottery of which exactlyone will win.
By computing posteriorprobabilities, we obtain the result that no one willwin because each of them wins with probability0.1.
To retain the validity of the probability ofeach variable as well as the consistency over allvariables, we propose the following inference.We first perform a belief revision and obtain themost probable world, which is the completeinference with the highest joint probability.
Thenfor each variable, we compute its posteriorprobability given that all other variables are setas evidence with the same assignment as in themost probable world.
By inferring the lotteryexample in this way, in each of its inferred worlda different person wins with equal probability.Specifically, the probability of a person winninggiven all others not winning is 1, and theprobability of a person winning given all but onewinning is 0.
As we proposed earlier, thereasoning process of the deceiver presupposesher target arguments, that is, she was raped, byadding the argument as an extra piece ofevidence.
The inference results of A in bothdeceptive and honest cases and those of a truevictim are shown in Table 1.
The argumentsB_relation_with_A_s_mother=bad,B_drive_A_home=true, A_is_celebrity=true andA_s_boyfriend_catch_on_the_scene=true are setas evidence as suggested by the scenario.Figure 2: BN of the deceiver in the rape casePeople express attitudes as binary beliefs incommunication if not as beliefs with fuzzyconfidence, but not as degree of belief67formulated by real-valued probabilities.
To mapdegree of belief to binary beliefs, we need toknow how much confidence is sufficient for aperson to believe in an attitude.
Or in otherwords, what is the probability threshold ofsomething being true.
Research has suggestedthat truth threshold varies by proposition and byindividual, which means it is a subjectivecriterion (Ferreira, 2004).
Since we use simulateddata, we arbitrarily choose 0.66 as the thresholdsince it equally spaces the interval of anargument being true, unknown and false.
Thenthe binary beliefs in the deceptive story andhonest story of the deceiver and those in the truevictim?s story would be the same as Table 2.
Toverify the inferred beliefs, we compare Table 2with the scenario.
An argument is validated if itis in the same state as described in the scenarioor in the unknown state given that it is ignored inthe scenario.
We verified that 13 out of the 16arguments in the deceptive story correspondswith what the deceiver claims, all of thearguments in the honest story corresponds withwhat is the truth.
Although it is hard to verify thetrue victim?s story because we do not have itsground truth, we observe that all the argumentsare reasonable and most are contrary to thedeceiver?s honest story except the evidence.Arguments Decept.HonestTrueB_relation_with_As_mother=good 0 0 0A_have_exp_of_prostitution=T 0.66 0.88 0.11A_hate_Indian=T 0.74 0.07 0.89A_is_nice_to_B=T 0.18 0.88 0.18B_relation_with_A=rape 0.98 0.16 0.96B_in_A_s_party_by=self 0.9 0.4 0.90B_knows_A_s_adr=T 0.95 0.95 0.95B_drive_A_home=T 1 1 1B_is_new_to_party=T 0.76 0.82 0.16A_have_drug_from=B 0.76 0.07 0.92sex_by=rape 0.93 0.08 0.98As_boyfriend_catch_on_the_scene=T 1 1 1A_is_celebrity=T 1 1 1B_refuse_to_pay=T 0.8 0.85 0.50A_claim_being_raped=T 0.6 0.7 0.60cry_for_help=T 0.8 0.2 0.80Table 1: Inferred results of the deceiver?s deceptivestory, her honest story and a true victim?s storyThe computation of the discrepancies assumesacquaintance of the deceiver, which requiressufficient number of history data and neighborsof the deceiver.
To achieve it, we simulate 19agents by perturbing the deceiver?s BN andanother 10 agents by perturbing the true victim?sBN.
In total, we have 29 truth telling agents and1 deceiving agent.
We simulate 100 runs oftraining data by inferring the network of eachagent 100 times with different evidence at eachrun, and convert them to binary beliefs.
Trainingdata is assumed to contain no deception.
Thisapproach of inconsistency detection is borrowedfrom our past work (Santos et.
Al, 2010).Arguments Decept.HonestTrueB_relation_with_As_mother bad bad badA_have_exp_of_prostitution unknn T FA_hate_Indian T F TA_is_nice_to_B F T FB_relation_with_A rape fan rapeB_in_A_s_party_by self unknn selfB_knows_A_s_adr T T TB_drive_A_home T T TB_is_new_to_party T T FA_have_drug_from B self Bsex_by rape entice rapeAs_boyfriend_catch_on_the_scene T T TA_is_celebrity T T TB_refuse_to_pay T T unknnA_claim_being_raped unknn T unknncry_for_help T F TTable 2: Binary beliefs of the deceiver?s deceptivestory, honest story and a true victim?s story4 Experiment and resultsTo test the hypotheses, we compare the result ofdeceptive story with the result of misinformativestory.
A misinformative story is simulated byadding random error to the inferred results of thearguments.?
Propagation of manipulationTo calculate inconsistency we predict binarybeliefs in the deceptive story using GroupLens(Resnick et.
Al, 1994) based on stories ofneighboring agents in the Correlation Network.We then compare the binary beliefs in thedeceptive story with predicted binary beliefs tomeasure deviation of each argument due toinconsistency.
We measure how many standard(std.)
deviations the prediction error in deceptivestory deviates from the prediction error intraining data, and plot them according to theirlocations in the BN, as shown in Figure 3.
The68width of the links represents the sensitivity ofeach variable to its neighbors.We observe that the variables at theboundaries of the graph and not sensitive toneighbors (e.g.
B_is_new_to_party) are ignoredby the deceiver, while the variables in the centeror sensitive to others (e.g.
A_hate_Indian) aremanipulated significantly.
It demonstrates thatmanipulations propagate to closely relatedarguments.
Unrelated arguments are probablyconsidered as irrelevant or simply be ignored bythe deceiver.
On the other hand, if we comparedeceptive story with honest story in Table 2, weobtain 9 arguments manipulated by the deceiver.Out of these 9 arguments, 8 are successfullyidentified as inconsistent by Figure 3 if weassume the suspicion threshold is 3 std.deviations.Figure 3: Inconsistency deviation of each variable?
Correspondence between inconsistencyand untruthfulnessTo compute untruthfulness, we calculate thedeviation of the binary beliefs in the deceptivestory from the population of truth teller?s storieswho agrees with the deceiver in the ConsensusNetwork.
We then compare the deviation due toinconsistency with respect to the deceiver herselfand that due to untruthfulness with respect totruth tellers.
The result is shown in Table 3.The correlation between the deviation due toinconsistency and that due to untruthfulness is -0.5186, which means that untruthfulness has alarge negative correlation with inconsistency.
Itcredits our hypothesis that significantmanipulations are often convincing andunconvincing arguments usually can be found inslightly manipulated or ignored arguments.
Theonly exception in the result is the argumentB_knows_As_address, which is not manipulatedbut convincing.
It is probably because theevidence B_drive_A_home enforced it to remainhonest.
Type I incredibility does not occur in thiscase, but type II incredibility appears in theargument B_is_new_to_party andB_refuse_to_pay.
The deceiver ignored thesearguments, which results in the incredibility ofthe story.
The correlation between inconsistencyand untruthfulness in misinformative storiesranges between 0.3128 and 0.9823, whichdemonstrates that the negative correction cannotbe found in misinformative stories.
If wecompare the deceptive story and the true story inTable 2, we find out that 3 arguments in thedeceptive story are unconvincing.
By observingthe untruthfulness in Table 3, we find out that 2of the 3 arguments are out of at least 1.44 std.deviations of the sample of true stories and all ofthem are out of at least 0.95 std.
deviations.
Thesmall deviations indicate a high credibility of thedeceiver, which is caused by the similaritybetween the belief systems of the deceiver andthe true victim.Belief Incon.
Untru.B_relation_with_As_mother=good N/A N/AA_have_exp_of_prostitution=T 3.48 0.95A_hate_Indian=T 3.48 0.28A_is_nice_to_B=T 3.31 0.28B_relation_with_A=rape 3.25 0B_in_A_s_party_by=self 3.39 0.28B_knows_A_s_adr=T 0.04 0B_drive_A_home=T N/A N/AB_is_new_to_party=T 0 1.59A_have_drug_from=B 2.93 0sex_by=rape 3.95 0As_boyfriend_catch_on_the_scene=T N/A N/AA_is_celebrity=T N/A N/AB_refuse_to_pay=T 0.48 1.44A_claim_being_raped=T 4.63 0.41cry_for_help=T 3.37 0.41Table 3: Comparison of inconsistency anduntruthfulness of the deceiver?
FunctionalityFunctionality means that the manipulatedarguments are effective in reaching the goal andat the same time satisfies the evidence.
In otherwords, we can expect the manipulated argumentsfrom the goal and the evidence.
The calculation69of functionality is as following.
For eachinconsistent argument, we measure its correlationwith other arguments in the past using trainingdata.
We then predict each argument?s binarybelief based on the value of the conclusion andthe evidence.
If the predicted belief correspondswith the belief in the deceptive story, the variableis functional.
We compare the results ofdeceptive story with those of misinformativestory.
In Table 4, all but one manipulatedarguments in the deceptive story complies withthe value expected by the conclusion andevidence, but none of the inconsistent argumentsin misinformative stories does.
Although theresult shown in Table 5 comes from a randomsample of misinformative story, we observed thatmost of the samples show the same functionalityrate.
Therefore, the functionality rate ofdeceptive story is 6/7, while the functionality rateof misinformative story is around 0/3.Arguments Pred.
Decept.A_have_exp_of_prostitution=T 0.24 0.5A_hate_Indian=T 0.85 1A_is_nice_to_B=T 0.07 0B_relation_with_A=rape 0.99 1B_in_A_s_party_by=self 1 1A_claim_being_raped=T 0.58 0.5cry_for_help=T 0.86 1Table 4: Functionality of the deceiver?s storyArguments Pred.
Misinfo.B_in_A_s_party_by=self 0.45 0B_knows_A_s_adr=T 0.90  0.5A_claim_being_raped=T 0.94 0.5Table 5: Functionality of a mininformative story5 Conclusion and future workWe proposed in this work two fundamentaldiscrepancies in deceptive communications:discrepancies in arguments that deceivers arereluctant to believe but truth tellers embrace anddiscrepancies in arguments that are manipulatedby deceivers.
The proposal follows the followingthree assumptions: The act of deceiving iscomposed of deceptive argument formation andargument communication; Deception is formedin the reasoning process rather than thecommunication process; Reasoning is interactionbetween arguments, and deceptive reasoning isreasoning with presupposition.
Then weproposed three hypotheses in order to distinguishdeception from unintentional misinformation:manipulations propagate smoothly throughclosely related arguments, inconsistency anduntruthfulness are negatively correlated, anddeceptive arguments are usually functional todeceiver?s goal and evidence.
To evaluate and tomeasure these hypotheses from communicationcontent, we designed a generic model ofdeception detection.
In the model, agents arecorrelated with others to expect each other?sconsistency in beliefs and consenting agents arecompared with each other to evaluate thetruthfulness of beliefs.
Our experimental resultscredit the hypotheses.
The main contribution ofthis work is not to follow or reject the path thatlinguistic cues have laid out, but to suggest a newdirection in which deeper information about theintent of deceivers is carefully mined andanalyzed based on their cognitive process.In the future, we will further develop themodel by designing and implementing detectionmethods based on the hypotheses.
Currently weuse simulated data based on an artificial story,which is closer to a real legal case that providesconcrete information about the reasoning ofdeceivers with minimum noise.
In the future, wewill apply the model to survey data that iscommonly used in the area.
Various naturallanguage processing techniques can be utilized inthe retrieval of the reasoning process.Specifically, Latent dirichlet alocation (Blei et.Al, 2002) can be used to categorize the sentencesinto topics (or arguments), sentiment analysis(Liu.
2010) can be used to extract the polarity ofeach argument, and various BN constructors suchas PC algorithm (Spirtes et.
Al, 1993) can be usedto construct the belief systems.
On the otherhand, linguistic cues have been observed in pastresearch (DePaulo et.
al, 2003), but has not beendefined or explained quantitatively.
The study ofthe pattern of deceptive reasoning can ultimatelyprovide guidance and explanations to existingobservations in deception cueing.AcknowledgmentsThis work was supported in part by grants fromAFOSR, ONR, and DHS.ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal ofMachine Learning Research, Lafferty, John.Ed., 3 (4?5): 993?1022.Bella M. DePaulo, James J. Lindsay, Brian E.Malone, Laura Muhlenbruck, Kelly Charlton, and70Harris Cooper.
2003.
Cues to deception.Psychological Bulletin, 129(1): 74-118.Ulisses Ferreira.
2004.
On the Foundations ofComputing Science.
Lecture Notes in ComputerScience, 3002:46-65.John O. Greene, H. Dan O'hair, Micheal J. Cody, andCatherine Yen.
1985.
Planning and Control ofBehavior during Deception.
HumanCommunication Research, 11:335-64.I.
L. Humberstone.
1992.
Direction of Fit.
Mind,101(401): 59-84.Marcia K. Johnson and Carol L. Raye.
1981.
RealityMonitoring.
Psychological Bulletin, 88:67?85.Isaac Levi.
1996.
For the Sake of the Argument.Cambridge University Press.
New York, NY, USA.Bing Liu.
2010.
Sentiment Analysis and Subjectivity.Handbook of Natural Language Processing Issue,1st ed., Taylor and Francis Group, Eds.
CRC Press,1-38.Hazel Markus.
1977.
Self-schemata and ProcessingInformation about the Self.
Journal of Personalityand Social Psychology, 35:63?78.Albert Mehrabian.
1972.
Nonverbal Communication.Aldine Atherton, Chicago, USA.Alfred R. Mele.
1992.
Springs of Action:Understanding Intentional Behavior.
OxfordUniversity Press.
New York, NY, USA.Judea Pearl.
1988.
Probabilistic Reasoning inIntelligent Systems.
Morgan Kaufmann Publishers,San Francisco, CA, USA.Paul Resnick, Neophytos Iacovou, Mitesh Suchak,Peter Bergstrom, and John Riedl.
1994.GroupLens: An Open Architecture forCollaborative Filtering of Netnews.
Proc.
of theConference on Computer Supported CooperativeWork, 175-186.
ACM Press, Chapel Hill, NC,USA.Eugene Santos, Jr. and Deqing Li.
2010.
DeceptionDetection in Multi-Agent Systems.
IEEETransactions on Systems, Man, and Cybernetics:Part A, 40(2):224-235.Warren Shibles.
1988.
A Revision of the Definition ofLying as an Untruth Told with Intent to Deceive.Argumentation, 2:99-115.Peter Spirtes, Clark N. Glymour, and RichardScheines, 1993.
Causation, Prediction, and Search.Springer-Verlag, New York, NY, USA.Morton Wiener and Albert Mehrabian.
1968.Language within Language: Immediacy, a Channelin Verbal Communication.
Meredith Corporation,New York, NY, USA.71
