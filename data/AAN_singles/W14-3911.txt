Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 94?101,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsAIDA: Identifying Code Switching in Informal Arabic TextHeba ElfardyDepartment of Computer ScienceColumbia UniversityNew York, NYheba@cs.columbia.eduMohamed Al-Badrashiny, Mona DiabDepartment of Computer ScienceThe George Washington UniversityWashington, DC{badrashiny, mtdiab}@gwu.eduAbstractIn this paper, we present the latest versionof our system for identifying linguisticcode switching in Arabic text.
The systemrelies on Language Models and a tool formorphological analysis and disambigua-tion for Arabic to identify the class of eachword in a given sentence.
We evaluatethe performance of our system on the testdatasets of the shared task at the EMNLPworkshop on Computational Approachesto Code Switching (Solorio et al., 2014).The system yields an average token-levelF?=1score of 93.6%, 77.7% and 80.1%,on the first, second, and surprise-genretest-sets, respectively, and a tweet-levelF?=1score of 4.4%, 36% and 27.7%, onthe same test-sets.1 IntroductionMost languages exist in some standard form whilealso being associated with informal regional vari-eties.
Some languages exist in a state of diglos-sia (Ferguson, 1959).
Arabic is one of thoselanguages comprising a standard form known asModern Standard Arabic (MSA), that is used ineducation, formal settings, and official scripts; anddialectal variants (DA) corresponding to the na-tive tongue of Arabic speakers.
While these vari-ants have no standard orthography, they are com-monly used and have become pervasive acrossweb-forums, blogs, social networks, TV shows,and normal daily conversations.
Arabic dialectsmay be divided into five main groups: Egyptian(including Libyan and Sudanese), Levantine (in-cluding Lebanese, Syrian, Palestinian and Jorda-nian), Gulf, Iraqi and Moroccan.
Sub-dialectalvariants also exist within each dialect (Habash,2010).
Speakers of a specific Arabic Dialecttypically code switch between their dialect andMSA, and less frequently between different di-alects, both inter and intra-sententially.
The iden-tification and classification of these dialects indiglossic text can enhance semantic predictability.In this paper we modify an existing systemAIDA (Elfardy and Diab, 2012b), (Elfardy et al.,2013) that identifies code switching between MSAand Egyptian DA (EDA).
We apply the modifiedsystem to the datasets used for evaluating systemsparticipating at the EMNLP Workshop on Com-putational Approaches to Linguistic Code Switch-ing.12 Related WorkDialect Identification in Arabic is crucial for al-most all NLP tasks, and has recently gained in-terest among Arabic NLP researchers.
One ofthe early works is that of (Biadsy et al., 2009)where the authors present a system that identifiesdialectal words in speech through acoustic signals.Zaidan and Callison-Burch (2011) crawled a largedataset of MSA-DA news commentaries and an-notated part of the dataset for sentence-level di-alectalness employing Amazon Mechanical Turk.Cotterell and Callison-Burch (2014) extended theprevious work by handling more dialects.
In (Cot-terell et al., 2014), the same authors collect andannotate on Amazon Mechanical Turk a large setof tweets and user commentaries pertaining to fiveArabic dialects.
Bouamor et al.
(2014) select a setof 2,000 Egyptian Arabic sentences and have themtranslated into four other Arabic dialects to presentthe first multidialectal Arabic parallel corpus.Eskander et al.
(2014) present a system for han-dling Arabic written in Roman script ?Arabizi?.Using decision trees; the system identifies whethereach word in the given text is a foreign word ornot and further divides non foreign words into four1Another group in our lab was responsible for the organi-zation of the task, hence we did not officially participate inthe task.94classes: Arabic, Named Entity, punctuation, andsound.In the context of machine-translation, Salloumand Habash (2011) tackle the problem of DA toEnglish Machine Translation (MT) by pivotingthrough MSA.
The authors present a system thatuses DA to MSA transfer rules before applyingstate of the art MSA to English MT system toproduce an English translation.
In (Elfardy andDiab, 2012a), we present a set of guidelines fortoken-level identification of DA while in (Elfardyand Diab, 2012b), (Elfardy et al., 2013) we tacklethe problem of token-level dialect-identificationby casting it as a code-switching problem.
El-fardy and Diab (2013) presents our solution for thesentence-level dialect identification problem.3 Shared Task DescriptionThe shared task for ?Language Identificationin Code-Switched Data?
(Solorio et al., 2014)aims at allowing participants to perform word-level language identification in code-switchedSpanish-English, MSA-DA, Chinese-English andNepalese-English data.
In this work, we only fo-cus on MSA-DA data.
The dataset has six tags:1. lang1: corresponds to an MSA word, ex.??@Q?
@, AlrAhn2meaning ?the current?;2.
lang2: corresponds to a DA word, ex.
?KP@,ezyk meaning ?how are you?;3.
mixed: corresponds to a word with mixedmorphology, ex.????
?A ??
@ , Alm>lw$wnmeaning ?the ones that were excluded or re-jected?;4.
other: corresponds to punctuation, numbersand words having punctuation or numbers at-tached to them;5. ambig: corresponds to a word where theclass cannot be determined given the currentcontext, could either be lang1 or lang2; ex.the phrase ?A????
?, klh tmAm meaning ?all iswell?
is ambiguous if enough context is notpresent since it can be used in both MSA andEDA.6.
NE: corresponds to a named-entity, ex.
Q??
?,mSr meaning ?Egypt?.2We use Buckwalter transliteration schemehttp://www.qamus.org/transliteration.htm4 ApproachWe use a variant of the system that was pre-sented in (Elfardy et al., 2013) to identify thetag of each word in a given Arabic sentence.The original approach relies on language mod-els and a morphological analyzer to assign tagsto words in an input sentence.
In this new vari-ant, we use MADAMIRA (Pasha et al., 2014);a tool for morphological analysis and disam-biguation for Arabic.
The advantage of usingMADAMIRA over using a morphological ana-lyzer is that MADAMIRA performs contextualdisambiguation of the analyses produced by themorphological analyzer, hence reducing the pos-sible options for analyses per word.
Figures 1 il-lustrates the pipeline of the proposed system.4.1 PreprocessingWe experiment with two preprocessing tech-niques:1.
Basic: In this scheme, we only perform a ba-sic clean-up of the text by separating punc-tuation and numbers from words, normal-izing word-lengthening effects, and replac-ing all punctuation, URLs, numbers and non-Arabic words with PUNC, URL, NUM, andLAT keywords, respectively2.
Tokenized: In this scheme, in addition tobasic preprocessing, we use MADAMIRAtoolkit to tokenize clitics and affixes by ap-plying the D3-tokenization scheme (Habashand Sadat, 2006).
For example, the word Ym.'.,bjdwhich means ?with seriousness?
becomes?Yg.+H.
?, ?b+ jd?
after tokenization.4.2 Language ModelThe ?Language Model?
(LM) module uses the pre-processed training data to build a 5-gram LM.
Alltokens in a given sentence in the training data aretagged with either lang1 or lang2 as described inSection 5.
The prior probabilities of each lang1and lang2 words are calculated based on their fre-quency in the training corpus.
SRILM toolkit(Stolcke, 2002) and the tagged corpora are thenused to build the LM.3If tokenized preprocess-ing scheme is used, then the built LM is tokenizedwhere all tokens corresponding to a certain wordare assigned the same tag corresponding to the tag3A full description of the approach is presented in (El-fardy and Diab, 2012b).95sentence Preprocessing NE List Combiner Tagged SentenceLM +Viterbi searchMADAMIRA Basic preprocessingsentence PreprocessingNE ListCombinerTaggedSentenceLM +Viterbi searchMADAMIRATokenized preprocessing b aFigure 1: AIDA pipeline.
a) The pipeline with the basic preprocessing scheme.
b) The pipelinewith the tokenized preprocessing scheme.of the original word.
For example, if Ym.'., bjd istagged as lang2, both ?+H.
?, b+ and ?Yg.
?, jd gettagged as lang2.For any new untagged sentence, the ?LanguageModel?
module uses the already built LM andthe prior probabilities via Viterbi search to findthe best sequence of tags for the given sentence.If there is an out-of-vocabulary word in the in-put sentence, the ?Language Model?
leaves it un-tagged.4.3 MADAMIRAUsing MADAMIRA, each word in a given un-tagged sentence is tokenized, lemmatized, andPOS-tagged.
Moreover, the MSA and Englishglosses for each morpheme of the given wordare provided.
Since MADAMIRA uses two possi-ble underlying morphological analyzers CALIMA(Habash et al., 2012) and SAMA (Maamouri et al.,2010), as part of the output, MADAMIRA indicateswhich of them is used to retrieve the glosses.4.4 Named Entities ListWe use the ANERGazet (Benajiba et al., 2007) toidentify named-entities.
ANERGazet consists ofthe following Gazetteers:?
Locations: 1,545 entries corresponding tonames of continents, countries, cities, etc.(ex.
H.Q?
??
@ , Almgrb ) which means ?Mo-rocco?;?
People: 2,100 entries corresponding tonames of people.
(ex.
Y?
?, fhd);?
Organizations: 318 entries corresponding tonames of Organizations such as companiesand football teams.
(ex.
???
??, t$lsy mean-ing ?Chelsea?4.5 CombinerEach word in the input sentence can get differ-ent tags from each module.
Thus, the ?Combiner?module uses all of these decisions and the follow-ing set of rules to assign the final tag to each wordin the input sentence.1.
If the word contains any numbers or punctu-ation, it is assigned other tag;2.
Else if the word is present in any of thegazetteers or if MADAMIRA assigns itnoun prop POS tag, the word is tagged asNE;3.
Else if the word is (or all of its morphemesin the tokenized scheme are) identified by theLM as either lang1 or lang2, the word is as-signed the corresponding tag;4.
Else if the word?s morphemes are assigneddifferent tags, the word is assigned the mixedtag;5.
Else if the LM does not tag the word (i.e.
theword is considered an out of vocabulary wordby the LM) and:?
If MADAMIRA retrieved the glossesfrom SAMA, the word is assigned alang1 tag;?
Else if MADAMIRA outputs that theglosses were retrieved from CALIMA,then the word is assigned a lang2 tag?
Else if the word is still untagged (i.e.non-analyzable), the word is assignedlang2 tag.5 Experiments and Results5.1 Training PhaseThe training data that is used to build our LM con-sists of two main sources:1.
Shared-task?s training data (STT): 119,326words collected from Twitter.
They are man-ually annotated on the token-level.
We splitthis corpus into:(a) Training-set; (STT-Tr); 107,398 tweetsrepresenting 90% of STT and used fortraining the system96(b) Development-set; (STT-Dev): 11,928words representing 10% of STT andused for tuning the system.2.
Web-log training data (WLT): 8 millionwords.
Half of which comes from lang1 cor-pora while the other half is from lang2 cor-pora.
The data is weakly labeled where alltokens in the sentence/comment are assignedthe same tag according to the dialect of theforum (MSA or EDA) it was crawled from.During the development phase, we use STT-Tr andWLT to train our system.
We run several experi-ments to test the different setups and evaluate theperformance of each of these setups on STT-Dev.Once we find the optimal configuration, we thenuse it to retrain the system using all of STT-Tr,STT-Dev, and WLT.Since the size of STT is very small comparedto WLT ( 0.1% of WLT size), the existence of sixdifferent tags in this corpus can add noise to thealready weakly labeled WLT data.
Thus, to makeSTT consistent with WLT, we changed the labelsof STT as follows:?
If the number of lang1 tokens in the tweet ex-ceeds the number of lang2 tokens; we assignall tokens in the tweet lang1 tag.?
Otherwise, all tokens in the tweet are as-signed lang2 tag.All tokens in STT tagged as NE have been used toenrich our named entity list.5.2 Development PhaseTwo different setups are tested using WLT andSTT-Tr:?
Surface form setup; uses the basic prepro-cessing pipeline described earlier on both theinput data and on the training data used tobuild the LM?
Tokenized form setup: uses the tokenizedpreprocessing pipeline described earlier onboth the input data and the training data usedto build the LM.As mentioned earlier, since the size of STT-Tr ismuch smaller than that of WLT, this causes bothdatasets to be statistically incomparable.
We triedincreasing the weights assigned by the LM to STT-Tr by duplicating STT-Tr.
We experimented withone, four, and eight copies of STT-Tr for each ofthe basic and tokenized experimental setups.The shared task evaluation script has been usedto evaluate each setup.
The evaluation scriptproduces two main sets of metrics.
The firstmetric yields the accuracy, precision, recall, andF?=1score for code switching classification on thetweet-level, while the second set of metrics usesevaluates performance of each tag on the token-level.
In this paper, we add an extra metric corre-sponding to the weighted average of the tag on thetoken level F?=1score in order to rank our overallperformance against other participating groups inthe task.Tables 1 and 2 summarize our results for bothSurface Form and Tokenized Form setups on STT-Dev.
In all experiments, the Tokenized Form setupoutperforms the Surface Form setup.As shown in Table 2, the system that yieldsthe best weighted-average token-level F?=1score(77.6%) on the development-set is Tokenized-2.Throughout the rest of the paper, we will use thesystem?s name ?AIDA?
; to refer to this best con-figuration (Tokenized-2).Accuracy Precision Recall F?=1Tokenized-1 51.5% 43.7% 97.4% 60.3%Tokenized-2 52.5% 44.2% 97.4% 60.8%Tokenized-8 54.2% 45.1% 96.9% 61.6%Surface-1 45.4% 40.9% 99.5% 57.9%Surface-2 45.8% 41.1% 99.5% 58.1%Surface-8 46.5% 41.4% 99.5% 58.5%Table 1: Results on STT-Dev using the tweet-levelevaluation.
(-1, -2, and -8) correspond to the num-ber of copies of STT-Tr that were added to WLT5.3 Testing PhaseThree blind test sets have been used for the evalu-ation:?
Test1: 54,732 words of 2,363 tweets col-lected from some unseen users in the trainingset;?
Test2: Another 32,641 words of 1,777 tweetscollected from other unseen users in the train-ing set;?
Surprise: 12,017 words of 1,222 sentencesfrom collected from Arabic commentaries.Table 3 shows the distribution of each test set overthe different tags97ambig lang1 lang2 mixed NE other Avg-F?=1Tokenized-1 0.0% 79.5% 71.5% 0.0% 83.6% 98.9% 77.5%Tokenized-2 0.0% 79.6% 71.6% 0.0% 83.6% 98.9% 77.6%Tokenized-8 0.0% 79.5% 71.4% 0.0% 83.6% 98.9% 77.5%Surface-1 0.0% 76.0% 65.4% 0.0% 83.6% 98.9% 73.5%Surface-2 0.0% 76.1% 65.6% 0.0% 83.6% 98.9% 73.7%Surface-8 0.0% 76.2% 65.5% 0.0% 83.6% 98.9% 73.7%Table 2: Results on STT-Dev using the token-level evaluation.
(-1, -2, and -8) correspondto the number of copies of STT-Tr that were added to WLTambig lang1 lang2 mixed NE otherTest1 0.0% 81.5% 0.3% 0.0% 10.9% 7.3%Test2 0.4% 32.0% 45.3% 0.0% 13.2% 9.0%Surprise 0.9% 22.4% 57.7% 0.0% 9.1% 9.9%Table 3: Test sets tag distributionsTables 4, 5, and 6 show the tweet-level evalua-tion on the three test sets.
While tables 7, 8, and9 show the token-level evaluation on the same testsets.
The tables compare the results of our bestsetup against the other systems that participated inthe task4.To make the comparison easier, we have calcu-lated the overall weighted F?=1score for all sys-tems using the three test sets together.Table 10 shows the F?=1score of each systemaveraged over all three test-sets.
Our system out-performs all other systems in the token-level eval-uation and comes in the second place after CMUin the tweet-level classification.Accuracy Precision Recall F?=1AIDA 45.2% 2.3% 93.8% 4.4%CMU 86.1% 5.2% 53.1% 9.5%A3-107 60.5% 2.5% 71.9% 4.8%IUCL 97.4% 11.1% 12.5% 11.8%MSR-IN 94.7% 9.7% 34.4% 15.2%Table 4: Tweet-level evaluation on Test1 set.Accuracy Precision Recall F?=1AIDA 44.0% 22.2% 95.6% 36.0%CMU 66.2% 29.2% 73.4% 41.7%A3-107 46.9% 21.3% 82.3% 33.8%IUCL 76.6% 27.1% 24.9% 26.0%MSR-IN 71.4% 18.3% 21.2% 19.6%Table 5: Tweet-level evaluation on Test2 set.4The results of the other groups have been obtained fromthe workshop website.
We use ??MSR-IN?
to refer to ?MSR-India?Accuracy Precision Recall F?=1AIDA 55.6% 16.3% 91.2% 27.7%CMU 79.8% 20.7% 41.2% 27.6%A3-107 45.7% 12.8% 83.3% 22.2%IUCL 87.7% 25.0% 15.8% 19.4%MSR-IN 84.8% 17.3% 16.7% 17.0%Table 6: Tweet-level evaluation on Surprise set.ambig lang1 lang2 mixed NE other Avg-F?=1AIDA 0.0% 94.5% 5.6% 0.0% 85.0% 99.4% 93.6%CMU 0.0% 94.4% 9.0% 0.0% 74.0% 98.1% 92.2%A3-107 0.0% 93.8% 5.7% 0.0% 73.4% 87.4% 90.9%IUCL 0.0% 88.2% 14.2% 0.0% 0.6% 0.6% 72.0%MSR-IN 0.0% 94.2% 15.8% 0.0% 57.7% 91.1% 89.8%Table 7: Token-level evaluation on Test1 set.6 Error AnalysisTables 11, 12, and 13 show the confusion matri-ces of our best setup for all six tags over the threetest sets.
The rows represent the gold-labels whilethe columns represent the classes generated byour system.
For example, row 4-column 2 corre-sponds to the percentage of words that have lang1(i.e.
MSA) gold-label and were incorrectly clas-sified as ambig.
The diagonal of each matrix cor-responds to the correctly classified instances.
Allcells of each matrix add-up to 100%.
In all threetables, it?s clear that the highest confusability isbetween lang1 and lang2 classes.
In Test-set1,since the majority of words (81.5%) have a lang1gold-label and a very tiny percentage (0.3%) hasambig lang1 lang2 mixed NE other Avg-F?=1AIDA 0.0% 73.4% 73.2% 1.0% 91.8% 98.1% 77.7%CMU 0.0% 76.3% 81.3% 0.0% 73.4% 98.4% 79.9%A3-107 0.0% 62.0% 49.4% 0.0% 67.5% 75.0% 58.0%IUCL 0.0% 59.0% 59.3% 0.0% 13.1% 1.7% 47.7%MSR-IN 1.5% 58.7% 50.5% 0.0% 42.4% 43.8% 51.3%Table 8: Token-level evaluation on Test2 set.98ambig lang1 lang2 mixed NE other Avg-F?=1AIDA 0.0% 66.6% 81.9% 0.0% 87.9% 99.9% 80.1%CMU 0.0% 68.0% 82.1% 0.0% 61.2% 97.5% 77.8%A3-107 0.0% 53.8% 61.3% 0.0% 62.3% 96.1% 62.6%IUCL 0.0% 48.8% 60.9% 0.0% 5.5% 2.0% 46.7%MSR-IN 0.0% 56.3% 69.8% 0.0% 33.2% 96.6% 65.4%Table 9: Token-level evaluation on Surprise set.Tweet Avg-F?=1Token Avg-F?=1AIDA 20.2% 86.8%CMU 24.3% 86.4%A3-107 18.4% 76.6%IUCL 18.2% 61.0%MSR-IN 17.1% 74.2%Table 10: Overall tweet-level and token-levelF?=1scores.
(Averaged over the three test-sets)a lang2 gold-label, the percentage of words thathave a gold label of lang1 and get classified aslang2 is much larger than in the other two test-setsand much larger than the opposite-case where theones having a gold-label of lang2 get classified aslang1.Table 14 shows examples of the words that weremisclassified by AIDA.
All of the shown exam-ples are quite challenging.
In example 1, the mis-classified named-entity refers to the name of a TVshow but the word also means ?clearly?
which is a?lang1?
word.
Similarly in example 2, the named-entity can mean ?stable?
which is again a ?lang1?word.
Another misclassification is that in exam-ple 3, where a mixed-morphology ?mixed?
wordmeaning ?those who were excluded/rejected?
ismisclassified as being a ?lang2?
word.
Whenwe looked at why this happened, we found thatthe word wasn?t tokenized by MADAMIRA.
Ourapproach only assigns ?mixed?
tag if after tok-enization, different morphemes of the word getdifferent tags.
Since in this example the wordwasn?t tokenized, it could not get the ?mixed?
tag.However, ?lang2?
tag (assigned by AIDA) is thesecond most appropriate tag since the main mor-pheme of the word is dialectal/lang2.
An exampleof a ?mixed?
word that was correctly classified byAIDA is ?XZ?Jk, Ht&dy meaning ?will lead to?where the main morpheme ?XZ?K, t&dy ?lead to?is ?lang1?
and the clitic h, H ?will?
is ?lang2?.Examples 4 and 5 show instances of the confus-ability between ?lang1?
and ?lang2?
classes.
Bothwords in these two examples can belong to eitherone of ?lang1?
and ?lang2?
classes depending onthe context.One interesting observation is that AIDA, out-performs all other systems tagging named-entities.This suggests the robustness of the NER approachused by AIDA.The performance on the other tags varies acrossthe three test-sets.AIDA (Predicted)ambig lang1 lang2 mixed NE otherambig 0.0% 0.0% 0.0% 0.0% 0.0% 0.0%lang1 0.0% 74.4% 5.7% 0.0% 1.3% 0.0%lang2 0.0% 0.1% 0.2% 0.0% 0.0% 0.0%mixed 0.0% 0.0% 0.0% 0.0% 0.0% 0.0%NE 0.0% 1.5% 0.3% 0.0% 9.1% 0.1%other 0.0% 0.0% 0.0% 0.0% 0.0% 7.3%Table 11: The token-level confusion matrix for thebest performing setup on Test1 set.AIDA (Predicted)ambig lang1 lang2 mixed NE otherambig 0.0% 0.3% 0.1% 0.0% 0.0% 0.0%lang1 0.0% 28.8% 2.8% 0.1% 0.2% 0.1%lang2 0.0% 16.4% 28.3% 0.5% 0.2% 0.1%mixed 0.0% 0.0% 0.0% 0.0% 0.0% 0.0%NE 0.0% 1.0% 0.6% 0.0% 11.5% 0.2%other 0.0% 0.0% 0.0% 0.0% 0.0% 8.9%Table 12: The token-level confusion matrix for thebest performing setup on Test2 set.AIDA (Predicted)ambig lang1 lang2 mixed NE otherambig 0.0% 0.6% 0.3% 0.0% 0.0% 0.0%lang1 0.0% 19.0% 2.9% 0.0% 0.5% 0.0%lang2 0.0% 14.5% 42.7% 0.0% 0.5% 0.0%mixed 0.0% 0.0% 0.0% 0.0% 0.0% 0.0%NE 0.0% 0.5% 0.6% 0.0% 8.0% 0.0%other 0.0% 0.0% 0.0% 0.0% 0.0% 9.9%Table 13: The token-level confusion matrix for thebest performing setup on Surprise set.99Sentence Word Gold-Label AIDA-LabelEx.
1.
Allylp AlEA$rp w AlnSf msA?
s>kwn DyfAlAstA?
Emrw Allyvy fy brnAmjh bwDwHElY qnAp AlHyAp?J????A?
ZA????J?
@ ??Q??A??
@ ??J??
@???
h???K.?
m.?AKQK.??
??J??
@ ?Q??XAJ?B@?AJm?
'@?AJ?bwDwH, h??
?K.NE lang1Ex.
2. wlsh mqhwr yA EynY mn vAbt bA$AAlbTl wSAlH bA$A slym AllY AvbtwA AnnZrthm fykm SH??J.?
@ A?AK.IK.AK??
??J?
AKP????
??????J?
??EQ??
@ @?J.K @???
@ ????
A?AK.l?'A?
?l?vAbt,IK.AK NE lang1Ex.
3.
Anh tAnY yqwm hykwn mE Alm>lw$yn????A??
@ ?????J?
?
??K?GAK ?K @Alm>lw$yn,????A??
@ mixed lang2Ex.
4. kfAyh $bEnA mnk AgAnyky Alqdymh jmylhlkn AlAn lAnTyq Swtk wlA Swrtk hwynAbqh???
??J?g.???Y??
@ ?
?JK A?
@ ?J?
AJ?J.?
?KA????K.AJK??
?KP??
B?
?K?
??J?B?B@lAnTyq,?J?B lang1 lang2Ex.
5.
AlrAbT Ally byqwl >ny Swrt Hlqp mErAmz jlAl gyr SHyH .
dh fyrws ElY Alfysbwk .
rjA?
AlH?rQ?
@P ?????gHP??
?G@ ???JK.???
@ ?.@Q?
@??
?
@???
??Q?
?X .
iJm?Q?
?Cg.PYm?
'@ ZAg.P .
??K.Hlqp,??
?g lang2 lang1Table 14: Examples of the words that were misclassified by AIDA7 Conclusion and Future WorkIn this work, we adapt a previously proposed sys-tem for automatic detection of code switching ininformal Arabic text to handle twitter data.
Weexperiment with several setups and report the re-sults on two twitter datasets and a surprise-genretest-set, all of which were generated for the sharedtask at EMNLP workshop for Computational Ap-proaches to Code Switching.
In the future we planon handling other Arabic dialects such as Levan-tine, Iraqi and Moroccan Arabic as well as adapt-ing the system to other genres.8 AcknowledgmentThis work has been funded by the NSF CRI CodeSwitching Project.We would like to thank Mahmoud Ghoneim forhis thorough review of the paper and the data.
Wealso thank the anonymous reviewer for the usefulcomments.ReferencesYassine Benajiba, Paolo Rosso, and Jos Miguel Bene-druiz.
2007.
Anersys: An arabic named entityrecognition system based on maximum entropy.
InIn Proceedings of CICLing-2007.Fadi Biadsy, Julia Hirschberg, and Nizar Habash.2009.
Spoken arabic dialect identification usingphonotactic modeling.
In Proceedings of the Work-shop on Computational Approaches to Semitic Lan-guages at the meeting of the European Associa-tion for Computational Linguistics (EACL), Athens,Greece.Houda Bouamor, Nizar Habash, and Kemal Oflazer.2014.
A multidialectal parallel corpus of arabic.
InProceedings of LREC.Ryan Cotterell and Chris Callison-Burch.
2014.
Amulti-dialect, multi-genre corpus of informal written100arabic.
In Proceedings of the Language Resourcesand Evaluation Conference (LREC).Ryan Cotterell, Adithya Renduchintala, Naomi Saphra,and Chris Callison-Burch.
2014.
An algerianarabic-french code-switched corpus.
In LRECWorkshop on Free/Open-Source Arabic Corporaand Corpora Processing Tools.Heba Elfardy and Mona Diab.
2012a.
Simplifiedguidelines for the creation of large scale dialectalarabic annotations.
In Proceedings of LREC.Heba Elfardy and Mona Diab.
2012b.
Token levelidentification of linguistic code switching.
In Pro-ceedings of COLING, Mumbai, India.Heba Elfardy and Mona Diab.
2013.
Sentence-LevelDialect Identification in Arabic.
In Proceedings ofACL2013, Sofia, Bulgaria, August.Heba Elfardy, Mohamed Al-Badrashiny, and MonaDiab.
2013.
Code Switch Point Detection in Arabic.In Proceedings of the 18th International Conferenceon Application of Natural Language to InformationSystems (NLDB2013), MediaCity, UK, June.Ramy Eskander, Mohamed Al-Badrashiny, NizarHabash, and Owen Rambow.
2014.
Foreign wordsand the automatic processing of arabic social me-dia text written in roman script.
In Proceedings ofthe First Workshop on Computational Approaches toCode-Switching.
EMNLP 2014, Conference on Em-pirical Methods in Natural Language Processing,October, 2014, Doha, Qatar.Ferguson.
1959.
Diglossia.
Word 15.
325340.Nizar Habash and Fatiha Sadat.
2006.
Arabic prepro-cessing schemes for statistical machine translation.Nizar Habash, Ramy Eskander, and AbdelAtiHawwari.
2012.
A Morphological Analyzer forEgyptian Arabic.
In NAACL-HLT 2012 Workshopon Computational Morphology and Phonology(SIGMORPHON2012), pages 1?9.Nizar Habash.
2010.
Introduction to arabic naturallanguage processing.
Advances in neural informa-tion processing systems.Mohamed Maamouri, Dave Graff, Basma Bouziri,Sondos Krouna, Ann Bies, and Seth Kulick.
2010.Ldc standard arabic morphological analyzer (sama)version 3.1.Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,Ahmed El Kholy, Ramy Eskander, Nizar Habash,Manoj Pooleery, Owen Rambow, and Ryan M. Roth.2014.
MADAMIRA: A Fast, Comprehensive Toolfor Morphological Analysis and Disambiguation ofArabic.
In Proceedings of LREC, Reykjavik, Ice-land.Wael Salloum and Nizar Habash.
2011.
Dialectalto standard arabic paraphrasing to improve arabic-english statistical machine translation.
In Proceed-ings of the First Workshop on Algorithms and Re-sources for Modelling of Dialects and Language Va-rieties.
Association for Computational Linguistics.Thamar Solorio, Elizabeth Blair, Suraj Maharjan, SteveBethard, Mona Diab, Mahmoud Gonheim, AbdelatiHawwari, Fahad AlGhamdi, Julia Hirshberg, Ali-son Chang, , and Pascale Fung.
2014.
Overviewfor the first shared task on language identification incode-switched data.
In In Proceedings of the FirstWorkshop on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on EmpiricalMethods in Natural Language Processing, October,2014, Doha, Qatar.Andreas Stolcke.
2002.
Srilm an extensible languagemodeling toolkit.
In Proceedings of ICSLP.Omar F Zaidan and Chris Callison-Burch.
2011.
Thearabic online commentary dataset: an annotateddataset of informal arabic with high dialectal con-tent.
In ACL.101
