Proceedings of the NAACL HLT 2013 Demonstration Session, pages 32?35,Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational LinguisticsKELVIN: a tool for automated knowledge base constructionPaul McNamee, James MayfieldJohns Hopkins UniversityHuman Language Technology Center of ExcellenceTim Finin, Tim OatesUniversity of MarylandBaltimore CountyDawn LawrieLoyola University MarylandTan Xu, Douglas W. OardUniversity of MarylandCollege ParkAbstractWe present KELVIN, an automated system forprocessing a large text corpus and distilling aknowledge base about persons, organizations,and locations.
We have tested the KELVINsystem on several corpora, including: (a) theTAC KBP 2012 Cold Start corpus which con-sists of public Web pages from the Universityof Pennsylvania, and (b) a subset of 26k newsarticles taken from English Gigaword 5th edi-tion.Our NAACL HLT 2013 demonstration per-mits a user to interact with a set of search-able HTML pages, which are automaticallygenerated from the knowledge base.
Eachpage contains information analogous to thesemi-structured details about an entity that arepresent in Wikipedia Infoboxes, along withhyperlink citations to supporting text.1 IntroductionThe Text Analysis Conference (TAC) KnowledgeBase Population (KBP) Cold Start task1 requiressystems to take set of documents and produce acomprehensive set of <Subject, Predicate, Object>triples that encode relationships between and at-tributes of the named-entities that are mentioned inthe corpus.
Systems are evaluated based on the fi-delity of the constructed knowledge base.
For the2012 evaluation, a fixed schema of 42 relations (orslots), and their logical inverses was provided, forexample:?
X:Organization employs Y:Person1See details at http://www.nist.gov/tac/2012/KBP/task_guidelines/index.html?
X:Person has-job-title title?
X:Organization headquartered-in Y:LocationMultiple layers of NLP software are required forthis undertaking, including at the least: detection ofnamed-entities, intra-document co-reference resolu-tion, relation extraction, and entity disambiguation.To help prevent a bias towards learning aboutprominent entities at the expense of generality,KELVIN refrains from mining facts from sourcessuch as documents obtained through Web search,Wikipedia2, or DBpedia.3 Only facts that are as-serted in and gleaned from the source documents areposited.Other systems that create large-scale knowledgebases from general text include the Never-EndingLanguage Learning (NELL) system at CarnegieMellon University (Carlson et al 2010), and theTextRunner system developed at the University ofWashington (Etzioni et al 2008).2 Washington Post KBNo gold-standard KBs were available to us to assistduring the development of KELVIN, so we relied onqualitative assessment to gauge the effectiveness ofour extracted relations ?
by manually examining tenrandom samples for each relations, we ascertainedthat most relations were between 30-80% accurate.Although the TAC KBP 2012 Cold Start task was apilot evaluation of a new task using a novel evalua-tion methodology, the KELVIN system did attain thehighest reported F1 scores.42http://en.wikipedia.org/3http://www.dbpedia.org/40.497 0-hop & 0.363 all-hops, as reported in the prelimi-nary TAC 2012 Evaluation Results.32During our initial development we worked witha 26,143 document collection of 2010 WashingtonPost articles and the system discovered 194,059 re-lations about 57,847 named entities.
KELVIN learnssome interesting, but rather dubious relations fromthe Washington Post articles5?
Sen. Harry Reid is an employee of the ?Repub-lican Party.?
Sen. Reid is also an employee ofthe ?Democratic Party.??
Big Foot is an employee of Starbucks.?
MacBook Air is a subsidiary of Apple Inc.?
Jill Biden is married to Jill Biden.However, KELVIN also learns quite a number ofcorrect facts, including:?
Warren Buffett owns shares of Berkshire Hath-away, Burlington Northern Santa Fe, the Wash-ington Post Co., and four other stocks.?
Jared Fogle is an employee of Subway.?
Freeman Hrabowski works for UMBC,founded the Meyerhoff Scholars Program, andgraduated from Hampton University and theUniversity of Illinois.?
Supreme Court Justice Elena Kagan attendedOxford, Harvard, and Princeton.?
Southwest Airlines is headquartered in Texas.?
Ian Soboroff is a computer scientist6 employedby NIST.73 Pipeline Components3.1 SERIFBBN?s SERIF tool8 (Boschee et al 2005) providesa considerable suite of document annotations thatare an excellent basis for building a knowledge base.The functions SERIF can provide are based largely5All 2010 Washington Post articles from English Gigaword5th ed.
(LDC2011T07).6Ian is the sole computer scientist discovered in processinga year of news.
In contrast, KELVIN found 52 lobbyists.7From Washington Post article (WPB ENG 20100506.0012in LDC2011T07).8Statistical Entity & Relation Information Finding.Slotname Countper:employee of 60,690org:employees 44,663gpe:employees 16,027per:member of 14,613org:membership 14,613org:city of headquarters 12,598gpe:headquarters in city 12,598org:parents 6,526org:country of headquarters 4,503gpe:headquarters in country 4,503Table 1: Most prevalent slots extracted by SERIF fromthe Washington Post texts.Slotname Countper:title 44,896per:employee of 39,101per:member of 20,735per:countries of residence 8,192per:origin 4,187per:statesorprovinces of residence 3,376per:cities of residence 3,376per:country of birth 1,577per:age 1,233per:spouse 1,057Table 2: Most prevalent slots extracted by FACETS fromthe Washington Post texts.on the NIST ACE specification,9 and include: (a)identifying named-entities and classifying them bytype and subtype; (b) performing intra-documentco-reference analysis, including named mentions,as well as co-referential nominal and pronominalmentions; (c) parsing sentences and extracting intra-sentential relations between entities; and, (d) detect-ing certain types of events.In Table 1 we list the most common slots SERIFextracts from the Washington Post articles.3.2 FACETSFACETS, another BBN tool, is an add-on pack-age that takes SERIF output and produces role andargument annotations about person noun phrases.FACETS is implemented using a conditional-9The principal types of ACE named-entities are per-sons, organizations, and geo-political entities (GPEs).GPEs are inhabited locations with a government.
Seehttp://www.itl.nist.gov/iad/mig/tests/ace/2008/doc/ace08-evalplan.v1.2d.pdf.33Figure 1: Simple rendering of KB page about former Florida congressman Joe Scarborough.
Many facts are correct?
he lived in and was employed by the State of Florida; he has a brother George; he was a member of the RepublicanHouse of Representatives; and, he is employed by MSNBC.exponential learner trained on broadcast news.
Theattributes FACETS can recognize include general at-tributes like religion and age (which anyone mighthave), as well as role-specific attributes, such asmedical specialty for physicians, or academic insti-tution for someone associated with an university.In Table 2 we report the most prevalent slotsFACETS extracts from the Washington Post.103.3 CUNY toolkitTo increase our coverage of relations we also in-tegrated the KBP Slot Filling Toolkit (Chen et al2011) developed at the CUNY BLENDER Lab.Given that the KBP toolkit was designed for the tra-ditional slot filling task at TAC, this primarily in-volved creating the queries that the tool expected asinput and parallelizing the toolkit to handle the vastnumber of queries issued in the cold start scenarios.To informally gauge the accuracy of slotsextracted from the CUNY tool, some coarse as-sessment was done over a small collection of 807New York Times articles that include the string?University of Kansas.?
From this collection, 4264slots were identified.
Nine different types of slotswere filled in order of frequency: per:title (37%),per:employee of (23%), per:cities of residence(17%), per:stateorprovinces of residence (6%),10Note FACETS can independently extract some slots thatSERIF is capable of discovering (e.g., employment relations).org:top members/employees (6%), org:member of(6%), per:countries of residence (2%), per:spouse(2%), and per:member of (1%).
We randomly sam-pled 10 slot-fills of each type, and found accuracyto vary from 20-70%.3.4 CoreferenceWe used two methods for entity coreference.
Un-der the theory that name ambiguity may not be ahuge problem, we adopted a baseline approach ofmerging entities across different documents if theircanonical mentions were an exact string match af-ter some basic normalizations, such as removingpunctuation and conversion to lower-case charac-ters.
However we also used the JHU HLTCOECALE system (Stoyanov et al 2012), which mapsnamed-entity mentions to the TAC-KBP referenceKB, which was derived from a 2008 snapshot of En-glish Wikipedia.
For entities that are not found in theKB, we reverted to exact string match.
CALE entitylinking proved to be the more effective approach forthe Cold Start task.3.5 Timex2 NormalizationSERIF recognizes, but does not normalize, temporalexpressions, so we used the Stanford SUTime pack-age, to normalize date values.34Figure 2: Supporting text for some assertions about Mr. Scarborough.
Source documents are also viewable byfollowing hyperlinks.3.6 Lightweight InferenceWe performed a small amount of light inference tofill some slots.
For example, if we identified thata person P worked for organization O, and we alsoextracted a job title T for P, and if T matched a setof titles such as president or minister we assertedthat the tuple <O, org:top members employees, P>relation also held.4 Ongoing WorkThere are a number of improvements that we are un-dertaking, including: scaling to much larger corpora,detecting contradictions, expanding the use of infer-ence, exploiting the confidence of extracted infor-mation, and applying KELVIN to various genres oftext.5 Script OutlineThe KB generated by KELVIN is best explored us-ing a Wikipedia metaphor.
Thus our demonstrationconsists of a web browser that starts with a list ofmoderately prominent named-entities that the usercan choose to examine (e.g., investor Warren Buf-fett, Supreme Court Justice Elena Kagan, SouthwestAirlines Co., the state of Florida).
Selecting anyentity takes one to a page displaying its known at-tributes and relations, with links to documents thatserve as provenance for each assertion.
On everypage, each entity is hyperlinked to its own canon-ical page; therefore the user is able to browse theKB much as one browses Wikipedia by simply fol-lowing links.
A sample generated page is shown inFigure 1 and text that supports some of the learnedassertions in the figure is shown in Figure 2.
Wealso provide a search interface to support jump-ing to a desired entity and can demonstrate access-ing the data encoded in the semantic web languageRDF (World Wide Web Consortium, 2013), whichsupports ontology browsing and executing complexSPARQL queries (Prud?Hommeaux and Seaborne,2008) such as ?List the employers of people livingin Nebraska or Kansas who are older than 40.?ReferencesE.
Boschee, R. Weischedel, and A. Zamanian.
2005.
Au-tomatic information extraction.
In Proceedings of the2005 International Conference on Intelligence Analy-sis, McLean, VA, pages 2?4.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., and Tom M.Mitchell.
2010.
Toward an architecture for never-ending language learning.
In Proceedings of theTwenty-Fourth Conference on Artificial Intelligence(AAAI 2010).Z.
Chen, S. Tamang, A. Lee, X. Li, and H. Ji.
2011.Knowledge Base Population (KBP) Toolkit @ CUNYBLENDER LAB Manual.Oren Etzioni, Michele Banko, Stephen Soderland, andDaniel S. Weld.
2008.
Open information extractionfrom the web.
Commun.
ACM, 51(12):68?74, Decem-ber.E Prud?Hommeaux and A. Seaborne.
2008.
SPARQLquery language for RDF.
Technical report, WorldWide Web Consortium, January.Veselin Stoyanov, James Mayfield, Tan Xu, Douglas W.Oard, Dawn Lawrie, Tim Oates, and Tim Finin.
2012.A context-aware approach to entity linking.
In Pro-ceedings of the Joint Workshop on Automatic Knowl-edge Base Construction and Web-scale Knowledge Ex-traction, AKBC-WEKEX ?12, pages 62?67, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.World Wide Web Consortium.
2013.
Resource Descrip-tion Framework Specification.
?http://http://www.w3.org/RDF/.
?
[Online; accessed 8 April,2013]?.35
