A New Feature Selection Score for Multinomial NaiveBayes Text Classification Based on KL-DivergenceKarl-Michael SchneiderDepartment of General LinguisticsUniversity of Passau94032 Passau, Germanyschneide@phil.uni-passau.deAbstractWe define a new feature selection score for textclassification based on the KL-divergence betweenthe distribution of words in training documents andtheir classes.
The score favors words that have asimilar distribution in documents of the same classbut different distributions in documents of differentclasses.
Experiments on two standard data sets in-dicate that the new method outperforms mutual in-formation, especially for smaller categories.1 IntroductionText classification is the assignment of predefinedcategories to text documents.
Text classificationhas many applications in natural language process-ing tasks such as E-mail filtering, prediction of userpreferences and organization of web content.The Naive Bayes classifier is a popular machinelearning technique for text classification because itperforms well in many domains, despite its simplic-ity (Domingos and Pazzani, 1997).
Naive Bayes as-sumes a stochastic model of document generation.Using Bayes?
rule, the model is inverted in order topredict the most likely class for a new document.We assume that documents are generated accord-ing to a multinomial event model (McCallum andNigam, 1998).
Thus a document is represented asa vector di = (xi1 .
.
.
xi|V |) of word counts whereV is the vocabulary and each xit ?
{0, 1, 2, .
.
. }
in-dicates how often wt occurs in di.
Given model pa-rameters p(wt|cj) and class prior probabilities p(cj)and assuming independence of the words, the mostlikely class for a document di is computed asc?
(di) = argmaxjp(cj)p(d|cj)= argmaxjp(cj)|V |?t=1p(wt|cj)n(wt,di)(1)where n(wt, di) is the number of occurrences of wtin di.
p(wt|cj) and p(cj) are estimated from train-ing documents with known classes, using maximumlikelihood estimation with a Laplacean prior:p(wt|cj) =1 + ?di?cj n(wt, di)|V | + ?|V |t=1?di?cj n(wt, di)(2)p(cj) =|cj |?|C|j?=1 |cj?
|(3)It is common practice to use only a subset ofthe words in the training documents for classifi-cation to avoid overfitting and make classificationmore efficient.
This is usually done by assigningeach word a score f(wt) that measures its useful-ness for classification and selecting the N highestscored words.
One of the best performing scoringfunctions for feature selection in text classificationis mutual information (Yang and Pedersen, 1997).The mutual information between two random vari-ables, MI(X;Y ), measures the amount of informa-tion that the value of one variable gives about thevalue of the other (Cover and Thomas, 1991).Note that in the multinomial model, the wordvariable W takes on values from the vocabulary V .In order to use mutual information with a multi-nomial model, one defines new random variablesWt ?
{0, 1} with p(Wt = 1) = p(W = wt) (Mc-Callum and Nigam, 1998; Rennie, 2001).
Then themutual information between a word wt and the classvariable C isMI(Wt;C) =|C|?j=1?x=0,1p(x, cj) logp(x, cj)p(x)p(cj)(4)where p(x, cj) and p(x) are short for p(Wt = x, cj)and p(Wt = x).
p(x, cj), p(x) and p(cj) are esti-mated from the training documents by counting howoften wt occurs in each class.2 Naive Bayes and KL-DivergenceThere is a strong connection between Naive Bayesand KL-divergence (Kullback-Leibler divergence,relative entropy).
KL-divergence measures howmuch one probability distribution is different fromanother (Cover and Thomas, 1991).
It is defined (fordiscrete distributions) byKL(p, q) =?xp(x) log p(x)q(x) .
(5)By viewing a document as a probability distribu-tion over words, Naive Bayes can be interpreted inan information-theoretic framework (Dhillon et al,2002).
Let p(wt|d) = n(wt, d)/|d|.
Taking loga-rithms and dividing by the length of d, (1) can berewritten asc?
(d)=argmaxjlog p(cj) +|V |?t=1n(wt, d) log p(wt|cj)=argmaxj1|d| log p(cj) +|V |?t=1p(wt|d) log p(wt|cj)(6)Adding the entropy of p(W |d), we getc?
(d)= argmaxj1|d| log p(cj) ?|V |?t=1p(wt|d) logp(wt|d)p(wt|cj)= argminjKL(p(W |d), p(W |cj)) ?1|d| log p(cj)(7)This means that Naive Bayes assigns to a documentd the class which is ?most similar?
to d in termsof the distribution of words.
Note also that theprior probabilities are usually dominated by docu-ment probabilities except for very short documents.3 Feature Selection using KL-DivergenceWe define a new scoring function for feature selec-tion based on the following considerations.
In theprevious section we have seen that Naive Bayes as-signs a document d the class c?
such that the ?dis-tance?
between d and c?
is minimized.
A classifi-cation error occurs when a test document is closerto some other class than to its true class, in terms ofKL-divergence.We seek to define a scoring function such thatwords whose distribution in the individual trainingdocuments of a class is much different from the dis-tribution in the class (according to (2)) receive alower score, while words with a similar distributionin all training documents of the same class receivea higher score.
By removing words with a lowerscore from the vocabulary, the training documentsof each class become more similar to each other,and therefore, also to the class, in terms of word dis-tribution.
This leads to more homogeneous classes.Assuming that the test documents and training doc-uments come from the same distribution, the simi-larity between the test documents and their respec-tive classes will be increased as well, thus resultingin higher classification accuracy.We now make this more precise.
Let S ={d1, .
.
.
, d|S|} be the set of training documents, anddenote the class of di with c(di).
The average KL-divergence for a word wt between the training doc-uments and their classes is given byKLt(S) =1|S|?di?SKL(p(wt|di), p(wt|c(di))).
(8)One problem with (8) is that in addition to the con-ditional probabilities p(wt|cj) for each word andeach class, the computation considers each individ-ual document, thus resulting in a time requirementof O(|S|).1 In order to avoid this additional com-plexity, instead of KLt(S) we use an approxima-tion K?Lt(S), which is based on the following twoassumptions: (i) the number of occurrences of wtis the same in all documents that contain wt, (ii)all documents in the same class cj have the samelength.
Let Njt be the number of documents in cjthat contain wt, and letp?d(wt|cj) = p(wt|cj)|cj |Njt(9)be the average probability of wt in those documentsin cj that contain wt (if wt does not occur in cj , setp?d(wt|cj) = 0).
Then KLt(S) reduces toK?Lt(S) =1|S||C|?j=1Njtp?d(wt|cj) logp?d(wt|cj)p(wt|cj).
(10)Plugging in (9) and (3) and defining q(wt|cj) =Njt/|cj |, we getK?Lt(S) = ?|C|?j=1p(cj)p(wt|cj) log q(wt|cj).
(11)Note that computing K?Lt(S) only requires a statis-tics of the number of words and documents for each1Note that KLt(S) cannot be computed simultaneously withp(wt|cj) in one pass over the documents in (2): KLt(S) re-quires p(wt|cj) when each document is considered, but com-puting the latter needs iterating over all documents itself.class, not per document.
Thus K?Lt(S) can be com-puted in O(|C|).
Typically, |C| is much smallerthan |S|.Another important thing to note is the following.By removing words with an uneven distribution inthe documents of the same class, not only the doc-uments in the class, but also the classes themselvesmay become more similar, which reduces the abilityto distinguish between different classes.
Let p(wt)be the number of occurrences of wt in all trainingdocuments, divided by the total number of words,q(wt) =?|C|j=1 Njt/|S| and defineK?t(S) = ?p(wt) log q(wt).
(12)K?t(S) can be interpreted as an approximation of theaverage divergence of the distribution of wt in theindividual training documents from the global dis-tribution (averaged over all training documents inall classes).
If wt is independent of the class, thenK?t(S) = K?Lt(S).
The difference between the twois a measure of the increase in homogeneity of thetraining documents, in terms of the distribution ofwt, when the documents are clustered in their trueclasses.
It is large if the distribution of wt is similarin the training documents of the same class but dis-similar in documents of different classes.
In analogyto mutual information, we define our new scoringfunction as the differenceKL(wt) = K?t(S) ?
K?Lt(S).
(13)We also use a variant of KL, denoted dKL, wherep(wt) is estimated according to (14):p?
(wt) =|C|?j=1p(cj)p(wt|cj) (14)and p(wt|cj) is estimated as in (2).4 ExperimentsWe compare KL and dKL to mutual information,using two standard data sets: 20 Newsgroups2 andReuters 21578.3 In tokenizing the data, only wordsconsisting of alphabetic characters are used afterconversion to lower case.
In addition, all numbersare mapped to a special token NUM.
For 20 News-groups we remove the newsgroup headers and use astoplist consisting of the 100 most frequent words of2http://www.ai.mit.edu/?jrennie/20Newsgroups/3http://www.daviddlewis.com/resources/testcollections/reuters21578/dKLKLMIVocabulary SizeClassificationAccuracy1000001000010001001010.80.60.40.20Figure 1: Classification accuracy for 20 News-groups.
The curves have small error bars.the British National Corpus.4 We use the ModAptesplit of Reuters 21578 (Apte?
et al, 1994) and useonly the 10 largest classes.
The vocabulary size is111868 words for 20 Newsgroups and 22430 wordsfor Reuters.Experiments with 20 Newsgroups are performedwith 5-fold cross-validation, using 80% of the datafor training and 20% for testing.
We build a sin-gle classifier for the 20 classes and vary the num-ber of selected words from 20 to 20000.
Figure 1compares classification accuracy for the three scor-ing functions.
dKL slightly outperforms mutual in-formation, especially for smaller vocabulary sizes.The difference is statistically significant for 20 to200 words at the 99% confidence level, and for 20to 2000 words at the 95% confidence level, using aone-tailed paired t-test.For the Reuters dataset we build a binary classi-fier for each of the ten topics and set the number ofpositively classified documents such that precisionequals recall.
Precision is the percentage of posi-tive documents among all positively classified doc-uments.
Recall is the percentage of positive docu-ments that are classified as positive.In Figures 2 and 3 we report microaveraged andmacroaveraged recall for each number of selectedwords.
Microaveraged recall is the percentage of allpositive documents (in all topics) that are classifiedas positive.
Macroaveraged recall is the average ofthe recall values of the individual topics.
Microav-eraged recall gives equal weight to the documentsand thus emphasizes larger topics, while macroav-eraged recall gives equal weight to the topics andthus emphasizes smaller topics more than microav-4http://www.itri.brighton.ac.uk/?Adam.Kilgarriff/bnc-readme.htmldKLKLMIVocabulary SizePrecision/RecallBreakevenPoint1000001000010001001010.950.90.850.80.750.7Figure 2: Microaveraged recall on Reuters at break-even point.dKLKLMIVocabulary SizePrecision/RecallBreakevenPoint1000001000010001001010.950.90.850.80.750.7Figure 3: Macroaveraged recall on Reuters at break-even point.eraged recall.Both KL and dKL achieve slightly higher valuesfor microaveraged recall than mutual information,for most vocabulary sizes (Fig.
2).
KL performs bestat 20000 words with 90.1% microaveraged recall,compared to 89.3% for mutual information.
Thelargest improvement is found for dKL at 100 wordswith 88.0%, compared to 86.5% for mutual infor-mation.For smaller categories, the difference betweenthe KL-divergence based scores and mutual infor-mation is larger, as indicated by the curves formacroaveraged recall (Fig.
3).
KL yields the high-est recall at 20000 words with 82.2%, an increase of3.9% compared to mutual information with 78.3%,whereas dKL has its largest value at 100 words with78.8%, compared to 76.1% for mutual information.We find the largest improvement at 5000 words with5.6% for KL and 2.9% for dKL, compared to mutualinformation.5 ConclusionBy interpreting Naive Bayes in an information the-oretic framework, we derive a new scoring methodfor feature selection in text classification, based onthe KL-divergence between training documents andtheir classes.
Our experiments show that it out-performs mutual information, which was one ofthe best performing methods in previous studies(Yang and Pedersen, 1997).
The KL-divergencebased scores are especially effective for smaller cat-egories, but additional experiments are certainly re-quired.In order to keep the computational cost low,we use an approximation instead of the exact KL-divergence.
Assessing the error introduced by thisapproximation is a topic for future work.ReferencesChidanand Apte?, Fred Damerau, and Sholom M.Weiss.
1994.
Towards language independent au-tomated learning of text categorization models.In Proc.
17th ACM SIGIR Conference on Re-search and Development in Information Retrieval(SIGIR ?94), pages 23?30.Thomas M. Cover and Joy A. Thomas.
1991.
El-ements of Information Theory.
John Wiley, NewYork.Inderjit S. Dhillon, Subramanyam Mallela, and Ra-hul Kumar.
2002.
Enhanced word clustering forhierarchical text classification.
In Proc.
8th ACMSIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 191?200.Pedro Domingos and Michael Pazzani.
1997.
Onthe optimality of the simple bayesian classifierunder zero-one loss.
Machine Learning, 29:103?130.Andrew McCallum and Kamal Nigam.
1998.
Acomparison of event models for Naive Bayes textclassification.
In Learning for Text Categoriza-tion: Papers from the AAAI Workshop, pages 41?48.
AAAI Press.
Technical Report WS-98-05.Jason D. M. Rennie.
2001.
Improving multi-classtext classification with Naive Bayes.
Master?sthesis, Massachusetts Institute of Technology.Yiming Yang and Jan O. Pedersen.
1997.
A com-parative study on feature selection in text catego-rization.
In Proc.
14th International Conferenceon Machine Learning (ICML-97), pages 412?420.
