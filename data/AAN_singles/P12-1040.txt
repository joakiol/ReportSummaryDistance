Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379?388,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsA Discriminative Hierarchical Model for Fast Coreference at Large ScaleMichael WickUniversity of Massachsetts140 Governor?s DriveAmherst, MAmwick@cs.umass.eduSameer SinghUniversity of Massachusetts140 Governor?s DriveAmherst, MAsameer@cs.umass.eduAndrew McCallumUniversity of Massachusetts140 Governor?s DriveAmherst, MAmccallum@cs.umass.eduAbstractMethods that measure compatibility betweenmention pairs are currently the dominant ap-proach to coreference.
However, they sufferfrom a number of drawbacks including diffi-culties scaling to large numbers of mentionsand limited representational power.
As thesedrawbacks become increasingly restrictive,the need to replace the pairwise approacheswith a more expressive, highly scalable al-ternative is becoming urgent.
In this paperwe propose a novel discriminative hierarchicalmodel that recursively partitions entities intotrees of latent sub-entities.
These trees suc-cinctly summarize the mentions providing ahighly compact, information-rich structure forreasoning about entities and coreference un-certainty at massive scales.
We demonstratethat the hierarchical model is several ordersof magnitude faster than pairwise, allowing usto perform coreference on six million authormentions in under four hours on a single CPU.1 IntroductionCoreference resolution, the task of clustering men-tions into partitions representing their underlyingreal-world entities, is fundamental for high-level in-formation extraction and data integration, includingsemantic search, question answering, and knowl-edge base construction.
For example, coreferenceis vital for determining author publication lists inbibliographic knowledge bases such as CiteSeer andGoogle Scholar, where the repository must knowif the ?R.
Hamming?
who authored ?Error detect-ing and error correcting codes?
is the same?
?R.Hamming?
who authored ?The unreasonable effec-tiveness of mathematics.?
Features of the mentions(e.g., bags-of-words in titles, contextual snippetsand co-author lists) provide evidence for resolvingsuch entities.Over the years, various machine learning tech-niques have been applied to different variations ofthe coreference problem.
A commonality in manyof these approaches is that they model the prob-lem of entity coreference as a collection of deci-sions between mention pairs (Bagga and Baldwin,1999; Soon et al, 2001; McCallum and Wellner,2004; Singla and Domingos, 2005; Bengston andRoth, 2008).
That is, coreference is solved by an-swering a quadratic number of questions of the form?does mention A refer to the same entity as mentionB??
with a compatibility function that indicates howlikely A and B are coreferent.
While these modelshave been successful in some domains, they also ex-hibit several undesirable characteristics.
The first isthat pairwise models lack the expressivity requiredto represent aggregate properties of the entities.
Re-cent work has shown that these entity-level prop-erties allow systems to correct coreference errorsmade from myopic pairwise decisions (Ng, 2005;Culotta et al, 2007; Yang et al, 2008; Rahman andNg, 2009; Wick et al, 2009), and can even providea strong signal for unsupervised coreference (Bhat-tacharya and Getoor, 2006; Haghighi and Klein,2007; Haghighi and Klein, 2010).A second problem, that has received significantlyless attention in the literature, is that the pair-wise coreference models scale poorly to large col-lections of mentions especially when the expected379Name:,Jamie,Callan,Ins(tu(ons:-CMU,LTI.,Topics:{WWW,,IR,,SIGIR},Name:Jamie,Callan,Ins(tu(ons:,Topics:-IR,Name:,J.,Callan,Ins(tu(ons:-CMU,LTI,Topics:-WWW,Name:,J.,Callan,Ins(tu(ons:-LTI,Topics:-WWW,Name:,James,Callan,Ins(tu(ons:-CMU,Topics:{WWW,,IR,,largeIscale},Coref?-Jamie,Callan,Topics:-IR,J.,Callan,Inst:-LTI, J.,Callan,Topic:-WWW,J.,Callan,Inst:-CMU,Jamie,Callan,Topics:-IR, J.,Callan,Inst:-CMU, James,Callan,Topics:-WWW,Inst:CMU,J.,Callan,Topics:-IR,Inst:-CMU,J.,Callan,Topics:-LIS,Figure 1: Discriminative hierarchical factor graph for coreference: Latent entity nodes (white boxes)summarize subtrees.
Pairwise factors (black squares) measure compatibilities between child and parentnodes, avoiding quadratic blow-up.
Corresponding decision variables (open circles) indicate whether onenode is the child of another.
Mentions (gray boxes) are leaves.
Deciding whether to merge these two entitiesrequires evaluating just a single factor (red square), corresponding to the new child-parent relationship.number of mentions in each entity cluster is alsolarge.
Current systems cope with this by eitherdividing the data into blocks to reduce the searchspace (Herna?ndez and Stolfo, 1995; McCallum etal., 2000; Bilenko et al, 2006), using fixed heuris-tics to greedily compress the mentions (Ravin andKazi, 1999; Rao et al, 2010), employing special-ized Markov chain Monte Carlo procedures (Milchet al, 2006; Richardson and Domingos, 2006; Singhet al, 2010), or introducing shallow hierarchies ofsub-entities for MCMC block moves and super-entities for adaptive distributed inference (Singh etal., 2011).
However, while these methods help man-age the search space for medium-scale data, eval-uating each coreference decision in many of thesesystems still scales linearly with the number of men-tions in an entity, resulting in prohibitive computa-tional costs associated with large datasets.
This scal-ing with the number of mentions per entity seemsparticularly wasteful because although it is commonfor an entity to be referenced by a large numberof mentions, many of these coreferent mentions arehighly similar to each other.
For example, in authorcoreference the two most common strings that referto Richard Hamming might have the form ?R.
Ham-ming?
and ?Richard Hamming.?
In newswire coref-erence, a prominent entity like Barack Obama mayhave millions of ?Obama?
mentions (many occur-ring in similar semantic contexts).
Deciding whethera mention belongs to this entity need not involvecomparisons to all contextually similar ?Obama?mentions; rather we prefer a more compact repre-sentation in order to efficiently reason about them.In this paper we propose a novel hierarchical dis-criminative factor graph for coreference resolutionthat recursively structures each entity as a tree of la-tent sub-entities with mentions at the leaves.
Ourhierarchical model avoids the aforementioned prob-lems of the pairwise approach: not only can it jointlyreason about attributes of entire entities (using thepower of discriminative conditional random fields),but it is also able to scale to datasets with enor-mous numbers of mentions because scoring enti-ties does not require computing a quadratic numberof compatibility functions.
The key insight is thateach node in the tree functions as a highly compactinformation-rich summary of its children.
Thus, asmall handful of upper-level nodes may summarizemillions of mentions (for example, a single nodemay summarize all contextually similar ?R.
Ham-ming?
mentions).
Although inferring the structureof the entities requires reasoning over a larger state-space, the latent trees are actually beneficial to in-ference (as shown for shallow trees in Singh etal.
(2011)), resulting in rapid progress toward highprobability regions, and mirroring known benefitsof auxiliary variable methods in statistical physics(such as Swendsen and Wang (1987)).
Moreover,380each step of inference is computationally efficientbecause evaluating the cost of attaching (or detach-ing) sub-trees requires computing just a single com-patibility function (as seen in Figure 1).
Further,our hierarchical approach provides a number of ad-ditional advantages.
First, the recursive nature of thetree (arbitrary depth and width) allows the model toadapt to different types of data and effectively com-press entities of different scales (e.g., entities withmore mentions may require a deeper hierarchy tocompress).
Second, the model contains compatibil-ity functions at all levels of the tree enabling it to si-multaneously reason at multiple granularities of en-tity compression.
Third, the trees can provide splitpoints for finer-grained entities by placing contex-tually similar mentions under the same subtree.
Fi-nally, if memory is limited, redundant mentions canbe pruned by replacing subtrees with their roots.Empirically, we demonstrate that our model isseveral orders of magnitude faster than a pairwisemodel, allowing us to perform efficient coreferenceon nearly six million author mentions in under fourhours using a single CPU.2 Background: Pairwise CoreferenceCoreference is the problem of clustering mentionssuch that mentions in the same set refer to the samereal-world entity; it is also known as entity disam-biguation, record linkage, and de-duplication.
Forexample, in author coreference, each mention mightbe represented as a record extracted from the authorfield of a textual citation or BibTeX record.
Themention record may contain attributes for the first,middle, and last name of the author, as well as con-textual information occurring in the citation string,co-authors, titles, topics, and institutions.
The goalis to cluster these mention records into sets, eachcontaining all the mentions of the author to whichthey refer; we use this task as a running pedagogicalexample.Let M be the space of observed mention records;then the traditional pairwise coreference approachscores candidate coreference solutions with a com-patibility function ?
: M ?
M ?
< that mea-sures how likely it is that the two mentions re-fer to the same entity.1 In discriminative log-1We can also include an incompatibility function for whenlinear models, the function ?
takes the form ofweights ?
on features ?
(mi,mj), i.e., ?
(mi,mj) =exp (?
?
?(mi,mj)).
For example, in author coref-erence, the feature functions ?
might test whetherthe name fields for two author mentions are stringidentical, or compute cosine similarity between thetwo mentions?
bags-of-words, each representing amention?s context.
The corresponding real-valuedweights ?
determine the impact of these features onthe overall pairwise score.Coreference can be solved by introducing a set ofbinary coreference decision variables for each men-tion pair and predicting a setting to their values thatmaximizes the sum of pairwise compatibility func-tions.
While it is possible to independently makepairwise decisions and enforce transitivity post hoc,this can lead to poor accuracy because the decisionsare tightly coupled.
For higher accuracy, a graphi-cal model such as a conditional random field (CRF)is constructed from the compatibility functions tojointly reason about the pairwise decisions (McCal-lum and Wellner, 2004).
We now describe the pair-wise CRF for coreference as a factor graph.2.1 Pairwise Conditional Random FieldEach mention mi ?
M is an observed variable, andfor each mention pair (mi,mj) we have a binarycoreference decision variable yij whose value de-termines whether mi and mj refer to the same en-tity (i.e., 1 means they are coreferent and 0 meansthey are not coreferent).
The pairwise compatibilityfunctions become the factors in the graphical model.Each factor examines the properties of its mentionpair as well as the setting to the coreference decisionvariable and outputs a score indicating how likelythe setting of that coreference variable is.
The jointprobability distribution over all possible settings tothe coreference decision variables (y) is given as aproduct of all the pairwise compatibility factors:Pr(y|m) ?n?i=1n?j=1?
(mi,mj , yij) (1)Given the pairwise CRF, the problem of coreferenceis then solved by searching for the setting of thecoreference decision variables that has the highestprobability according to Equation 1 subject to thethe mentions are not coreferent, e.g., ?
:M?M?
{0, 1} ?
<381Jamie,Callan, Jamie,Callan,J.,Callan,J.,Callan, J.,Callan,J.,Callan, Jamie,Callan, Jamie,Callan,v,Jamie,Callan,J.,Callan,v,v,v,J.,Callan,J.,Callan, J.,Callan,J.,Callan,Jamie,Callan,Figure 2: Pairwise model on six mentions: Opencircles are the binary coreference decision variables,shaded circles are the observed mentions, and theblack boxes are the factors of the graphical modelthat encode the pairwise compatibility functions.constraint that the setting to the coreference vari-ables obey transitivity;2 this is the maximum proba-bility estimate (MPE) setting.
However, the solutionto this problem is intractable, and even approximateinference methods such as loopy belief propagationcan be difficult due to the cubic number of determin-istic transitivity constraints.2.2 Approximate InferenceAn approximate inference framework that has suc-cessfully been used for coreference models isMetropolis-Hastings (MH) (Milch et al (2006), Cu-lotta and McCallum (2006), Poon and Domingos(2007), amongst others), a Markov chain MonteCarlo algorithm traditionally used for marginal in-ference, but which can also be tuned for MPE in-ference.
MH is a flexible framework for specify-ing customized local-search transition functions andprovides a principled way of deciding which localsearch moves to accept.
A proposal function q takesthe current coreference hypothesis and proposes anew hypothesis by modifying a subset of the de-cision variables.
The proposed change is acceptedwith probability ?:?
= min(1,P r(y?)Pr(y)q(y|y?
)q(y?|y))(2)2We say that a full assignment to the coreference variablesy obeys transitivity if ?
ijk yij = 1 ?
yjk = 1 =?
yik = 1When using MH for MPE inference, the second termq(y|y?
)/q(y?|y) is optional, and usually omitted.Moves that reduce model score m y be accepted andan optional temperature can be used for annealing.The primary advantages of MH for coreference are(1) only the compatibility functions of the changeddecision variables need to be evaluated to ccept amove, and (2) the proposal function can enforce thetransitivity constraint by exploring only variable set-tings that result in valid coreference partitionings.A commonly used propos l distribution for coref-erence is the following: (1) randomly select twomentions (mi,mj), (2) if the mentions (mi,mj) arein the same entity cluster according to y then moveone mention into a singleton cluster (by setting thenecessary decision variables to 0), otherwise, movemention mi so it is in the same cluster as mj (bysetting the necessary decision variables).
Typically,MH is employed by first initializing to a singletonconfiguration (all entities have one mention), andthen executing the MH for a certain number of steps(or until the predicted coreference hypothesis stopschanging).This proposal distribution always moves a sin-gle mention m from some entity ei to another en-tity ej and thus the configuration y and y?
only dif-fer by the setting of decision variables governing towhich entity m refers.
In order to guarantee transi-tivity and a valid coreference equivalence relation,we must properly remove m from ei by untetheringm from each mention in ei (this requires computing|ei| ?
1 pairwise factors).
Similarly?again, for thesake of transitivity?in order to complete the moveinto ej we must coref m to each mention in ej (thisrequires computing |ej | pairwise factors).
Clearly,all the other coreference decision variables are in-dependent and so their corresponding factors can-cel because they yield the same scores under y andy?.
Thus, evaluating each proposal for the pairwisemodel scales linearly with the number of mentionsassigned to the entities, requiring the evaluation of2(|ei|+ |ej | ?
1) compatibility functions (factors).3 Hierarchical CoreferenceInstead of only capturing a single coreference clus-tering between mention pairs, we can imagine mul-tiple levels of coreference decisions over different382granularities.
For example, mentions of an authormay be further partitioned into semantically similarsets, such that mentions from each set have topicallysimilar papers.
This partitioning can be recursive,i.e., each of these sets can be further partitioned, cap-turing candidate splits for an entity that can facilitateinference.
In this section, we describe a model thatcaptures arbitrarily deep hierarchies over such lay-ers of coreference decisions, enabling efficient in-ference and rich entity representations.3.1 Discriminative Hierarchical ModelIn contrast to the pairwise model, where each en-tity is a flat cluster of mentions, our proposed modelstructures each entity recursively as a tree.
Theleaves of the tree are the observed mentions witha set of attribute values.
Each internal node of thetree is latent and contains a set of unobserved at-tributes; recursively, these node records summarizethe attributes of their child nodes (see Figure 1), forexample, they may aggregate the bags of contextwords of the children.
The root of each tree repre-sents the entire entity, with the leaves containing itsmentions.
Formally, the coreference decision vari-ables in the hierarchical model no longer representpairwise decisions directly.
Instead, a decision vari-able yri,rj = 1 indicates that node-record rj is theparent of node-record ri.
We say a node-record ex-ists if either it is a mention, has a parent, or has atleast one child.
Let R be the set of all existing noderecords, let rp denote the parent for node r, that isyr,rp = 1, and ?r?
6= rp, yr,r?
= 0.
As we describein more detail later, the structure of the tree and thevalues of the unobserved attributes are determinedduring inference.In order to represent our recursive model of coref-erence, we include two types of factors: pairwisefactors ?pw that measure compatibility between achild node-record and its parent, and unit-wise fac-tors ?rw that measure compatibilities of the node-records themselves.
For efficiency we enforce thatparent-child factors only produce a non-zero scorewhen the corresponding decision variable is 1.
Theunit-wise factors can examine compatibility of set-tings to the attribute variables for a particular node(for example, the set of topics may be too diverseto represent just a single entity), as well as enforcepriors over the tree?s breadth and depth.
Our recur-sive hierarchical model defines the probability of aconfiguration as:Pr(y, R|m) ?
?r?R?rw(r)?pw(r, rp) (3)3.2 MCMC Inference for Hierarchical modelsThe state space of our hierarchical model is substan-tially larger (theoretically infinite) than the pairwisemodel due to the arbitrarily deep (and wide) latentstructure of the cluster trees.
Inference must simul-taneously determine the structure of the tree, the la-tent node-record values, as well as the coreferencedecisions themselves.While this may seem daunting, the structures be-ing inferred are actually beneficial to inference.
In-deed, despite the enlarged state space, inferencein the hierarchical model is substantially fasterthan a pairwise model with a smaller state space.One explanatory intuition comes from the statisti-cal physics community: we can view the latent treeas auxiliary variables in a data-augmentation sam-pling scheme that guide MCMC through the statespace more efficiently.
There is a large body of lit-erature in the statistics community describing howthese auxiliary variables can lead to faster conver-gence despite the enlarged state space (classic exam-ples include Swendsen and Wang (1987) and slicesamplers (Neal, 2000)).Further, evaluating each proposal during infer-ence in the hierarchical model is substantially fasterthan in the pairwise model.
Indeed, we can replacethe linear number of factor evaluations (as in thepairwise model) with a constant number of factorevaluations for most proposals (for example, addinga subtree requires re-evaluating only a single parent-child factor between the subtree and the attachmentpoint, and a single node-wise factor).Since inference must determine the structure ofthe entity trees in addition to coreference, it is ad-vantageous to consider multiple MH proposals persample.
Therefore, we employ a modified variantof MH that is similar to multi-try Metropolis (Liuet al, 2000).
Our modified MH algorithm makes kproposals and samples one according to its modelratio score (the first term in Equation 2) normalizedacross all k. More specificaly, for each MH step, wefirst randomly select two subtrees headed by node-383records ri and rj from the current coreference hy-pothesis.
If ri and rj are in different clusters, wepropose several alternate merge operations: (also inFigure 3):?
Merge Left - merges the entire subtree of rj intonode ri by making rj a child of ri?Merge Entity Left - merges rj with ri?s root?Merge Left and Collapse - merges rj into ri thenperforms a collapse on rj (see below).?
Merge Up - merges node ri with node rj by cre-ating a new parent node-record variable rp with riand rj as the children.
The attribute fields of rp areselected from ri and rj .Otherwise ri and rj are subtrees in the same entitytree, then the following proposals are used instead:?
Split Right - Make the subtree rj the root of a newentity by detaching it from its parent?
Collapse - If ri has a parent, then move ri?s chil-dren to ri?s parent and then delete ri.?
Sample attribute - Pick a new value for an at-tribute of ri from its children.Computing the model ratio for all of coreferenceproposals requires only a constant number of com-patibility functions.
On the other hand, evaluatingproposals in the pairwise model requires evaluat-ing a number of compatibility functions equal to thenumber of mentions in the clusters being modified.Note that changes to the attribute values of thenode-record and collapsing still require evaluatinga linear number of factors, but this is only linear inthe number of child nodes, not linear in the numberof mentions referring to the entity.
Further, attributevalues rarely change once the entities stabilize.
Fi-nally, we incrementally update bags during corefer-ence to reflect the aggregates of their children.4 Experiments: Author CoreferenceAuthor coreference is a tremendously importanttask, enabling improved search and mining of sci-entific papers by researchers, funding agencies, andgovernments.
The problem is extremely difficult dueto the wide variations of names, limited contextualevidence, misspellings, people with common names,lack of standard citation formats, and large numbersof mentions.For this task we use a publicly available collec-tion of 4,394 BibTeX files containing 817,193 en-tries.3 We extract 1,322,985 author mentions, eachcontaining first, middle, last names, bags-of-wordsof paper titles, topics in paper titles (by running la-tent Dirichlet alocation (Blei et al, 2003)), and lastnames of co-authors.
In addition we include 2,833mentions from the REXA dataset4 labeled for coref-erence, in order to assess accuracy.
We also include?5 million mentions from DBLP.4.1 Models and InferenceDue to the paucity of labeled training data, we didnot estimate parameters from data, but rather setthe compatibility functions manually by specifyingtheir log scores.
The pairwise compatibility func-tions punish a string difference in first, middle, andlast name, (?8); reward a match (+2); and rewardmatching initials (+1).
Additionally, we use the co-sine similarity (shifted and scaled between ?4 and4) between the bags-of-words containing title to-kens, topics, and co-author last names.
These com-patibility functions define the scores of the factorsin the pairwise model and the parent-child factorsin the hierarchical model.
Additionally, we includepriors over the model structure.
We encourage eachnode to have eight children using a per node factorhaving score 1/(|number of children?8|+1), managetree depth by placing a cost on the creation of inter-mediate tree nodes ?8 and encourage clustering byplacing a cost on the creation of root-level entities?7.
These weights were determined by just a fewhours of tuning on a development set.We initialize the MCMC procedures to the single-ton configuration (each entity consists of one men-tion) for each model, and run the MH algorithm de-scribed in Section 2.2 for the pairwise model andmulti-try MH (described in Section 3.2) for the hi-erarchical model.
We augment these samplers us-ing canopies constructed by concatenating the firstinitial and last name: that is, mentions are onlyselected from within the same canopy (or block)to reduce the search space (Bilenko et al, 2006).During the course of MCMC inference, we recordthe pairwise F1 scores of the labeled subset.
Thesource code for our model is available as part of theFACTORIE package (McCallum et al, 2009, http:3http://www.iesl.cs.umass.edu/data/bibtex4http://www2.selu.edu/Academics/Faculty/aculotta/data/rexa.html384!"#!$#!%#!
"#!$# !
"#!$# !$#&"#!"#!$#&"#&"#!
"#$%&'()%)*' +*,-*'.
*/' +*,-*'0"$)1'.
*/' +*,-*'23' +*,-*'.*/'%"4'56&&%3(*'!"#!"#$%&'7)%)*'&"#&$#!'"#&"#&$#!
'"#73&#)8#-9)'&$# !
'"#56&&%3(*'Figure 3: Example coreference proposals for the case where ri and rj are initially in different clusters.//factorie.cs.umass.edu/).4.2 Comparison to Pairwise ModelIn Figure 4a we plot the number of samples com-pleted over time for a 145k subset of the data.
Re-call that we initialized to the singleton configurationand that as the size of the entities grows, the cost ofevaluating the entities in MCMC becomes more ex-pensive.
The pairwise model struggles with the largecluster sizes while the hierarchical model is hardlyaffected.
Even though the hierarchical model is eval-uating up to four proposals for each sample, it is stillable to sample much faster than the pairwise model;this is expected because the cost of evaluating a pro-posal requires evaluating fewer factors.
Next, weplot coreference F1 accuracy over time and show inFigure 5a that the prolific sampling rate of the hierar-chical model results in faster coreference.
Using theplot, we can compare running times for any desiredlevel of accuracy.
For example, on the 145k men-tion dataset, at a 60% accuracy level the hierarchicalmodel is 19 times faster and at 90% accuracy it is31 times faster.
These performance improvementsare even more profound on larger datasets: the hi-erarchical model achieves a 60% level of accuracy72 times faster than the pairwise model on the 1.3million mention dataset, reaching 90% in just 2,350seconds.
Note, however, that the hierarchical modelrequires more samples to reach a similar level of ac-curacy due to the larger state space (Figure 4b).4.3 Large Scale ExperimentsIn order to demonstrate the scalability of the hierar-chical model, we run it on nearly 5 million authormentions from DBLP.
In under two hours (6,700seconds), we achieve an accuracy of 80%, and inunder three hours (10,600 seconds), we achieve anaccuracy of over 90%.
Finally, we combine DBLPwith BibTeX data to produce a dataset with almost 6million mentions (5,803,811).
Our performance onthis dataset is similar to DBLP, taking just 13,500seconds to reach a 90% accuracy.5 Related WorkSingh et al (2011) introduce a hierarchical modelfor coreference that treats entities as a two-tieredstructure, by introducing the concept of sub-entitiesand super-entities.
Super-entities reduce the searchspace in order to propose fruitful jumps.
Sub-entities provide a tighter granularity of coreferenceand can be used to perform larger block moves dur-ing MCMC.
However, the hierarchy is fixed andshallow.
In contrast, our model can be arbitrarilydeep and wide.
Even more importantly, their modelhas pairwise factors and suffers from the quadraticcurse, which they address by distributing inference.The work of Rao et al (2010) uses streamingclustering for large-scale coreference.
However, thegreedy nature of the approach does not allow errorsto be revisited.
Further, they compress entities byaveraging their mentions?
features.
We are able toprovide richer entity compression, the ability to re-visit errors, and scale to larger data.Our hierarchical model provides the advantagesof recently proposed entity-based coreference sys-tems that are known to provide higher accuracy(Haghighi and Klein, 2007; Culotta et al, 2007;Yang et al, 2008; Wick et al, 2009; Haghighi andKlein, 2010).
However, these systems reason over asingle layer of entities and do not scale well.Techniques such as lifted inference (Singla andDomingos, 2008) for graphical models exploit re-dundancy in the data, but typically do not achieveany significant compression on coreference data be-385Samples versus Time0 500 1,000 1,500 2,000Running time (s)050,000100,000150,000200,000250,000300,000350,000400,000Number of SamplesHierar Pairwise(a) Sampling PerformanceAccuracy versus Samples0 50,000 100,000 150,000 200,000Number of Samples0.00.10.20.30.40.50.60.70.80.91.0F1AccuracyHierar Pairwise(b) Accuracy vs. samples (convergence accuracy as dashes)Figure 4: Sampling Performance Plots for 145k mentionsAccuracy versus Time0 250 500 750 1,000 1,250 1,500 1,750 2,000Running time (s)0.00.10.20.30.40.50.60.70.80.91.0F1AccuracyHierar Pairwise(a) Accuracy vs. time (145k mentions)Accuracy versus Time0 10,000 20,000 30,000 40,000 50,000 60,000Running time (s)0.00.10.20.30.40.50.60.70.80.9F1AccuracyHierar Pairwise(b) Accuracy vs. time (1.3 million mentions)Figure 5: Runtime performance on two datasetscause the observations usually violate any symmetryassumptions.
On the other hand, our model is ableto compress similar (but potentially different) obser-vations together in order to make inference fast evenin the presence of asymmetric observed data.6 ConclusionIn this paper we present a new hierarchical modelfor large scale coreference and demonstrate it onthe problem of author disambiguation.
Our modelrecursively defines an entity as a summary of itschildren nodes, allowing succinct representations ofmillions of mentions.
Indeed, inference in the hier-archy is orders of magnitude faster than a pairwiseCRF, allowing us to infer accurate coreference onsix million mentions on one CPU in just 4 hours.7 AcknowledgmentsWe would like to thank Veselin Stoyanov for his feed-back.
This work was supported in part by the CIIR, inpart by ARFL under prime contract #FA8650-10-C-7059,in part by DARPA under AFRL prime contract #FA8750-09-C-0181, and in part by IARPA via DoI/NBC contract#D11PC20152.
The U.S. Government is authorized toreproduce and distribute reprints for Governmental pur-poses notwithstanding any copyright annotation thereon.Any opinions, findings and conclusions or recommenda-tions expressed in this material are those of the authorsand do not necessarily reflect those of the sponsor.386ReferencesAmit Bagga and Breck Baldwin.
1999.
Cross-documentevent coreference: annotations, experiments, and ob-servations.
In Proceedings of the Workshop on Coref-erence and its Applications, CorefApp ?99, pages 1?8,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Eric Bengston and Dan Roth.
2008.
Understandingthe value of features for coreference resolution.
InEmpirical Methods in Natural Language Processing(EMNLP).Indrajit Bhattacharya and Lise Getoor.
2006.
A latentDirichlet model for unsupervised entity resolution.
InSDM.Mikhail Bilenko, Beena Kamath, and Raymond J.Mooney.
2006.
Adaptive blocking: Learning to scaleup record linkage.
In Proceedings of the Sixth Interna-tional Conference on Data Mining, ICDM ?06, pages87?96, Washington, DC, USA.
IEEE Computer Soci-ety.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal on MachineLearning Research, 3:993?1022.Aron Culotta and Andrew McCallum.
2006.
Prac-tical Markov logic containing first-order quantifierswith application to identity uncertainty.
In HumanLanguage Technology Workshop on ComputationallyHard Problems and Joint Inference in Speech and Lan-guage Processing (HLT/NAACL), June.Aron Culotta, Michael Wick, and Andrew McCallum.2007.
First-order probabilistic models for coreferenceresolution.
In North American Chapter of the Associa-tion for Computational Linguistics - Human LanguageTechnologies (NAACL HLT).Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric bayesianmodel.
In Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 848?855.Aria Haghighi and Dan Klein.
2010.
Coreference reso-lution in a modular, entity-centered model.
In NorthAmerican Chapter of the Association for Computa-tional Linguistics - Human Language Technologies(NAACL HLT), pages 385?393.Mauricio A. Herna?ndez and Salvatore J. Stolfo.
1995.The merge/purge problem for large databases.
In Pro-ceedings of the 1995 ACM SIGMOD internationalconference on Management of data, SIGMOD ?95,pages 127?138, New York, NY, USA.
ACM.Jun S. Liu, Faming Liang, and Wing Hung Wong.
2000.The multiple-try method and local optimization inmetropolis sampling.
Journal of the American Statis-tical Association, 96(449):121?134.Andrew McCallum and Ben Wellner.
2004.
Conditionalmodels of identity uncertainty with application to nouncoreference.
In Neural Information Processing Sys-tems (NIPS).Andrew McCallum, Kamal Nigam, and Lyle Ungar.2000.
Efficient clustering of high-dimensional datasets with application to reference matching.
In In-ternational Conference on Knowledge Discovery andData Mining (KDD), pages 169?178.Andrew McCallum, Karl Schultz, and Sameer Singh.2009.
FACTORIE: Probabilistic programming via im-peratively defined factor graphs.
In Neural Informa-tion Processing Systems (NIPS).Brian Milch, Bhaskara Marthi, and Stuart Russell.
2006.BLOG: Relational Modeling with Unknown Objects.Ph.D.
thesis, University of California, Berkeley.Radford Neal.
2000.
Slice sampling.
Annals of Statis-tics, 31:705?767.Vincent Ng.
2005.
Machine learning for coreference res-olution: From local classification to global ranking.
InAnnual Meeting of the Association for ComputationalLinguistics (ACL).Hoifung Poon and Pedro Domingos.
2007.
Joint infer-ence in information extraction.
In AAAI Conferenceon Artificial Intelligence, pages 913?918.Altaf Rahman and Vincent Ng.
2009.
Supervised mod-els for coreference resolution.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing: Volume 2 - Volume 2, EMNLP?09, pages 968?977, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Delip Rao, Paul McNamee, and Mark Dredze.
2010.Streaming cross document entity coreference reso-lution.
In International Conference on Computa-tional Linguistics (COLING), pages 1050?1058, Bei-jing, China, August.
Coling 2010 Organizing Commit-tee.Yael Ravin and Zunaid Kazi.
1999.
Is Hillary RodhamClinton the president?
disambiguating names acrossdocuments.
In Annual Meeting of the Association forComputational Linguistics (ACL), pages 9?16.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine Learning, 62(1-2):107?136.Sameer Singh, Michael L. Wick, and Andrew McCallum.2010.
Distantly labeling data for large scale cross-document coreference.
Computing Research Reposi-tory (CoRR), abs/1005.4298.Sameer Singh, Amarnag Subramanya, Fernando Pereira,and Andrew McCallum.
2011.
Large-scale cross-document coreference using distributed inference andhierarchical models.
In Association for Computa-tional Linguistics: Human Language Technologies(ACL HLT).387Parag Singla and Pedro Domingos.
2005.
Discrimina-tive training of Markov logic networks.
In AAAI, Pitts-burgh, PA.Parag Singla and Pedro Domingos.
2008.
Lifted first-order belief propagation.
In Proceedings of the 23rdnational conference on Artificial intelligence - Volume2, AAAI?08, pages 1094?1099.
AAAI Press.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to coref-erence resolution of noun phrases.
Comput.
Linguist.,27(4):521?544.R.H.
Swendsen and J.S.
Wang.
1987.
Nonuniversal crit-ical dynamics in MC simulations.
Phys.
Rev.
Lett.,58(2):68?88.Michael Wick, Aron Culotta, Khashayar Rohanimanesh,and Andrew McCallum.
2009.
An entity-based modelfor coreference resolution.
In SIAM InternationalConference on Data Mining (SDM).Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, TingLiu, and Sheng Li.
2008.
An entity-mention model forcoreference resolution with inductive logic program-ming.
In Association for Computational Linguistics,pages 843?851.388
