Linear Encodings of Linguistic AnalysesSamuel  S. Epste inBel l  Communicat ions  Research445 South Street, 2Q-350Morr is town,  NJ  07960-1910USAepstein@ f lash.bel lcore.com1.
IntroductionNatural languages contain families of expressions suchthat the number of readings of an expression is anexponential function of the length of the expression.Two well-known cases involve prepositional phraseattachment and coordination.
Other cases discussedbelow involve anaphora and relative operator scope.For example, in(l) John said that Bill said that ... that Harry saidthat he thinks that he thinks that ... that he thinksthat it is raining.each he can refer to any of John, Bill ..... Harly?
Thus if(1) contains n names and m occurrences of he, thissentence has n m readings (assuming that all anaphoricrelationships are intrasentential).
We discuss belowfamilies of expressions whose ambiguities grow asvarious exponential functions (factorial, Fibonacci,Catalan) of expression length.It appears, then, that exhaustive linear-timeprocessing of natural language is impossible.
Anexponentially ong answer cannot be produced in lineartime.
2 On the other hand, human processing of naturallanguage seems to be at least linearly fast.
The readyexplanation for this is that people do not recover allreadings of ambiguous expressions.
This is clearlycorrect, as far as it goes.This paper shows how to encode in linear space theexponentially arge numbers of readings associated withvarious families of expressions.
The availability ofthese encodings removes an apparent obstacle toexhaustive analyses of these expressions in linear time.The encodings may thus be useful for practicalcomputational purposes.
They may also provide a better1.
(1) is of course highly unnatural in a sense.
However, it effectivelyisolates for study a phenomenon that is intrinsic to naturallanguage.
Similar observations apply to the examples below.2.
It is of course also the case that an exponentially long answercaunot be produced in polynomial time.
If the problem cannot bereformulated so that answers are not exponentially long, thequestion of tractability does not arise.
See \[Garey and Johnson 79\]and \[Barton, Berwick, and Ristad 87\] for related iscussions.basis than exponential-space encodings for explanationsof how humans process language.For each of the linguistic constructions discussed inthis paper, there is a simple program that generatesanalyses of the construction.
If there are no constraintson what counts as a linguistic analysis, then aspecification of a program, which requires constantspace, together with a specification of an inputexpression, which requires linear space, could count as alinear encoding of an analysis of the input.
Intuitively,there is a vast qualitative divide between a(program,input) pair on one hand, and, for instance, aforest of constituent structure trees on the other hand.More generally, a question arises of how to distinguishanalyses from procedures that yield analyses.
Thispaper will not attempt to answer this questiondefinitively.
The analyses presented in Sections 2 - 4 allsatisfy a notion of "legal" analysis that excludes(program,input) pairs.
Sections 2 and 3 discusspolynomial space analyses.
Section 4 adds arepresentational device to the repertory of Sections 2and 3, so that linear space analyses are possible.
Section5 infol~mally discusses a variety of issues, including thedistinction between analysis and procedure.2.
Analyses in Conjunctive NormalFormAssume that example (1) involves no ambiguities exceptfor antecedents of pronouns.
Assume further that thelength of the analysis of (1), aside from the specificationof antecedents of pronouns, grows linearly) Let theproposition q comprise all aspects of the analysis of (1),aside from specifications of antecedents of pronouns.Let the proposition p.. comprise the specification thatthe j-th name in (1I 'J is the antecedent of the i-thoccurrence of he.
(For example, Pl,2 comprises the3.
These assumptions, and similar assumptions for other examplesbelow, permit a briefer discussion than would otherwise bepossible.
Reservations about these assumptions do not affect thesubstance of the discussion.
Our concern with (1) focuses onexponentially growing possibilities for assigning antecedents opronouns.108 1specification that Bill is the antecedent of the mostshallowly embedded he.)
Let n be the number of namesin (1) and let m be the number of occun'ences of he.Then an exhaustive analysis of (1) can take thefollowing form:(l-a) (q & Pl,~ & P2,1 & "'" & Pro,1 ) v(q & Pl,1 & P2,I & "'" & Pro-l,1 & Pra,2 ) v(q & Pl,n & P2,n & "'" & Pm,n )(l-a), which contains n m disjuncts, is in DisjunctiveNormal Form (DNF).
Each disjunct fully specifies apossible interpretation of (1).
It is an implicitassumption in much of the literature that the proper formfor linguistic analyses is DNF.
An analysis in DNFamounts to a listing of possible global interpretations.
(l-a) is logically equivalent to the following:aatement in Conjunctive Normal Form (CNF):(l-b) q & (Pl,1 v Pl,2 v ... v Pl.n ) &(P2,1 v P2,2 v ... v P2,n )(3) the block in the box on the table ... in thekitchenAs \[Church and Patil 82\] discuss, examples like (3) aresimilar to other structures with systematic attachmentambiguities, such as coordination structures.
While thenumber of readings of (3)4is thus exponential in thelength of (3), (3) has an O(n ) length analysis in CNF asfollows:(3-a) q & (Pl,0) &(3~a-2)(P2,0 v P2,1 ) &(3-a-3)(P3,0 v P3,1 v P3,2 ) &(P3,1 zo P2,1 ) &(3-a-4)(P4,0 v P4,1 v P4,2 v P4,3 ) &(P4,1 D P2,l ) &(P4,1 D (P3,I v P3,2)) &(P4,2 D P3,2 ) &(Pro,1 v Pro.2 v ... v pm,n )(l-b) contains m+l conjuncts.
The length of anexhaustive analysis of (1) is exponential in the numberof pronouns in (1) when the analysis is given in DNF,but linear in the number of pronouns when the analysisis given in CNF.
However, (l--b) is not linear in thelength of (1), because each of m conjuncts contains nclisjuncts, so that a total of mxn literals is required tospecify anaphoric possibilities.The following example has an analysis in DNF thatgrows as the factorial of the length of the input:(2) John told Bill that Tom told him that Fred toldhim tha!
... that Jim told him that Harry told himthat it is raining.The first occurrence of him can have John or Bill asantecedent.
The second occurrence of him can haveJohn or Bill or Tom as antecedent, and so on.
(2) has anobvious analysis in CNF whose length is a quadraticfunction of the length of the input, namely(2:a) q & (PL1 v Pl 2 ) &;(P2,1 vP2 ,2v ,  -~2,3 )&(Pro,1 v Pro,2 v ... v Pm,m+l )where the notation follows the same conventions as in(l-a,b).The number of readings for the following nounphrase grows as the Catalan of the number ofprepositional phrases:(3-a-k)(Pk,0 v Pk,1 V ... V Pk,k_l ) &;(3-a-k,1)(Pk,l D P2,1 ) &(Pk,l D (P3,1 v P3,2)) &(Pk,1 D (Pk-l,l v Pk-l,2 v ... v Pk_l,k.2)) &(3-a-k,m)(Pk,m ~ Pm+l,m ) &(Pk,,n D (Pm+2,m v Pm+2,m+l)) &(Pk,m D (Pk-l,m V Pk-l,m+l V ... V Pk_l,k_2)) &In (3-a), Pi' comprises the specification that constituent i, J  .
.
.
.
attaches to constmmnt j, where the block ~s constituent 0,in the box is constituent 1, on the table is constituent 2,and so on.
Constituent k must attach to some constituentthat lies to its left.
If constituent k attaches to2 109constituent m, then the constituents between constituentm and constituent k cannot attach to constituents to theleft of constituent m. 4For each pair (k,m), the number of atoms in (3-a-k,m) is fl(k,m) = ,~ ' i .
fl(k,m) is quadratic in k. Foreach k, then, the number of atoms in (3-a-k) is f2(k) =k+ l (k , i ) ,  a cubic function in k. The number ofatoms in (3-a) (excluding atoms hidden in q) is thus=f2( i ) ,  a quartic function in n. (3-a) is certainly notthe most compressed CNF analysis of (3).
It is,however, easy to describe.Given an exhaustive analysis in DNF, choosing aglobal interpretation requires exactly one operation ofselecting a disjunct.
Foi" (l-b) and (2-a), choosing aglobal interpretation requires a number of selections thatis linear in the length of the input.
I am aware of noother reason for preferring DNF to CNF for analyses ofexamples like (1) and (2).
In favor of preferring CNFthere is the practical advantage of polynomial-spaceoutput, with its implications for speed of processing.There is also the possibility of more accuratepsycholinguistic modeling.
It seems likely that peoplemake decisions on antecedents for pronouns in exampleslike (1) and (2) locally, on a pronoun-by-pronoun basis,and that they do not choose among global analyses.
5 Incontrast, the conjuncts of (3-a) clearly do not correspondone-to-one with processing decisions.
Section 4discusses an analysis of (3) whose components maycorrespond to local decisions on attachment sites.3.
Encodings with non-atomicpropositional constantsIt is possible to get a cubic length analysis of (3) byintroducing constants tbr non-atomic propositions.
Form<k, let r. be v Pk_l.k_2 ).
K,m (Pk-I m+l V Pk-I m+2 v ...Then (3-a-k,m) is equ3)alent to: '(3-b-k,m) (Pk m D Pm+l m ) &(Pk,~n D (Pm+2,m v Pm+2,m+t )) &(Pkm D (Pk 2 m V rk.
1 m) )(Pklm D (Pk~l',m v ,'k,n~))Of course, the space required to define the r km mustfigure in the space required to encode an analys\[s of (3)along the lines of (3-b-k,m).
rk,m_ l -= (Pk-l,m v rk,m) , so4.
(Pl ~ (P2 v ... v pj)) is equivalent to(-'Pl v P2 v ... v pj), SO that(3-a) is in CNF.5.
This is not to suggest that people produce an exhaustive analysis inCNF prior to choosing a reading.
The hypothesis rather thatfragments of a CNF representation are produced (in some sense)during processing.it requires quadratic space to define all the rk.
m. Arevised version of (3) with (3-b-k,m) in place of (3-a-k,m) throughout requires cubic space.
6Tree representations of single readings for exampleslike (3) may be viewed as follows: edges correspond toatomic propositions that comprise specifications like"constituent i attaches to constituent j" or "constituent iprojects to constituent j.,,7 A non-terminal node Acorresponds to a constituent, but also corresponds to theconjunction of the atomic propositions that correspondto edges that A dominates.
Thus the root node of thetree corresponds to a proposition that comprises a fullspecification of constituent structure.The situation is essentially the same \['or sharedforests.
(\[Tomita 87\] discusses shared forests andpacked shared forests.)
Edges ill shared forestscorrespond to atomic propositions, and non-terminalnodes correspond to non-atomic propositions.
Toextend this perspective, shared forests compress theinformation in non-shared forests by exploiting theintroduction of constants for non-atomic propositions.In a shared forest, the subtree that a node dominates iswritten only once.
In effect, then, a constant isintroduced that represents the conjunction thatcorresponds to the node.
This constant is a constituentof the fornmlas that correspond to superior nodes.While shared forests are more compressed thanunshared forests, the number of nodes in the sharedforest representation of (3) is still exponential in thelength of (3).In a packed shared forest, a packed node that doesnot dominate any packed nodes corresponds to adisjunction of conjunctions of atomic propositions.Packed nodes that dominate other packed nodescorrespond to disjunctions of conjunctions of atomic andnon-atomic propositions.
In effect, for each node(packed or non-packed), a constant is introduced thatabbreviates the formula that corresponds to the node.Exploitation of constants for non-atomic propositionspemfits more significant compression for packed sharedforests than for shared forests.
The packed root node ofa packed shared forest for (3) cotxesponds to adisjunction of conjunctions whose size in atoms isexponential in the length of (3).
However, the numberof nodes of a packed shared forest for (3) goes up as thesquare of the length of (3).
The number of edges of thepacked shared forest (a more authentic measure of thesize of the forest) goes up as the cube of the length.6.
Further compression is possible if we allow quantification oversubscript indices.
However, quantification over artifacts ofrepresentation may uncontroversially involve crossing the dividebetween analysis and procedure.7.
Details of constituent s ructure are not relevant to the discussionhere.
For example, we will not distinguish "X attaches to V" from"X attaches toVP.
"2110 34.
Encodings that introducestructural constantsA linear length encoding of an analysis of (1) is possibleif we use the constant A = {John, Bill .
.
.
.
.
Harry} in theencoding as follows:(l-c) q & (antecedent(pronoun l) e A) &(antecedent(pronoun 2) e A) &(antecedent(pronounm) e A)Note that "x ~ Y" is short-hand for the disjunction of thestatements "x = y," where y ranges over Y, so that (l-c)is not very different from (l-b).
Examples belowinvolve tYeer use of constants that correspond to sets oflinguistic entities, I will call such constants "structural.
"A linear analysis of (2) is possible if we introduceconstants A 1 .
.
.
.
.
A , where A.
= {John, Bill}, A;  = A 1u {Tom}, A 3 = A 2 ~ {Fred}, ..l., Am = Am-I U {Jim}:(2-b) q &(matecedent(pronoun t) E A1) &(antecedent(pronoun2) ~ A 2) &quantifier Qi takes scope over Qi-I to its immediate left,then the quantifier Qi+l to the immediate right of Qicannot take scope over Qi" (See \[Epstein 88\] for adiscussion of relative operator scope.)
It follows that thenumber of relative operator scope readings for (4) growsas the Fibonacci of the length of (4).
8 However, a linearencoding of an exhaustive analysis of (4) is as follows:(4-a) q &\[((Q1 > Q2 ) & (L\] = Q2)) v((Q2 > Q1 ) & (L1 = T))\[ &\[Q1 > Q3 \] &\[((L 1 > Q3 ) & (L 2 = Q3)) v((Q3 > L1) & (L2 = T))\] &\[Qk-2 > Qk+l \] &\[Qk-t > Qk+l \] &\[((Lk_ 1 > Qk+l ) & (L k = Qk+l)) v((Qk+l > Lk-1) & (Lk = T))\] &(antecedent(pronounm) E Am)Because A. can be defined in terms of Ai_l, only linear1space is required to define these constants.
It isconvenient o mix definitions of constants with otheraspects of the encoding of (2), as follows:(2-c) q & A a = {John, Bill} &(antecedent(pronounl) ~ A i &A 2 = (A 1 u {Tom})) &(antecedent(pronoun2) E A 2 &A 3 = (A 2 u {Fred})) &Here q represents aspects of the analysis of (4) asidefrom the specification of relative operator scope, and Qirepresents the i-th quantifier in (4), reading from the left.The L. are introduced constants corresponding to1quantifiers that can have lower scope than some moredeeply embedded quantifier.
"Q > Q."
means that Q. ihas higher scope than Qj.
For all Q, ' '~ < T" is true and"Q > T" is false.
9 Note that if we delete from (4-a)propositions that assign values to introduced constants,such as "(L l = Q2)," the resulting statement is in CNF.Section 3 discussed cubic length analyses of (3) withpropositional constants.
(3) has a linear analysis withstructural constants as follows:(antecedent(pronoun .)
~ A .
&I 11 -2  1111-1A m = (Am.
1 u {Jim})) &(antecedent(pronoun m) ~ A m )For (2), the introduction of structural constants permits alinear encoding.
For the following example, theintroduction of structural constants likewise permits alinear encoding:(4) Many teachers expect several students to expectmany teachers to expect several students to ... toexpect many teachers to expect several students toread some book.Each quantifier in (4) can take scope over the quantifierto its immediate left (if any), and can take scope over thequantifier to its immediate right (if any).
However, if a8.
The most deeply embedded clause in (4) has 2 possible relativescope readings.
The second most deeply embedded clansc in (4)has 3 possible relative scope readings (many>several>some,many>some>several several>many>some).
Let S. be the k-th.
.
.
.
k most deeply embedded clause m (4).
(S k ts immediately embeddedin S..;) '  Given that' S_ has a total ofn (relative operator) scoperead~l~s, and that S ~tas a total of m scope readings then the?
k - !  "
" ' subject of S. can take scope over all the quantifiers in S?
k+\[ , .
k '  accounting f6r n global readings over S..,.
Alternatively, thesubject of S can take scope over the subj\[~ of S Then both.
k k+.
l "  these subjects take scope over all the qu'mtifiers m S .
Thek- l  .
second alternative thus accounts for m additional global readingsover  Sk+ 1.9.
(4-a) does not explicitly state, for example that Q > Q. but thist .
3 '  fact can be derived from (4-a) through apphcatmn of thetransitivity of relative operator scope?
Generally speaking,linguistic representations don't explicitly include all theirconsequences.4 111(3-c) q &\[(ap(PP1) = NP) & (AP 1 = NP) &(RE 1 = {NP, PP1})I &\[(ap(PP2) e REI) & (AP 2 = ap(PP2)) &(RE 2 = (RE 1 q" AP2) u {PP2})\] &(5-a) q &\[(ap(PPt) e {VP, NP}) & (AP t = ap(PP1)) &(RE~ = {VP, NP} T AP~) &(OG 1 = {VP}- {API))\] &\[(ap(PP2) E REI) & (AP 2 = ap(PP2)) &(RE 2 = (RE l $ AP2) u {PP2} ) &(OG 2 = OG t - {AP2})\] &\[(ap(PP k) ~ REk.
1) & (AP k = ap(PPk) &(RE k = (REk_ 1 1" APk) u {PPk})\] &Here q represents aspects of the analysis of (3) asidefrom the specification of attachment points for theprepositional phrases.
The desired solutions consist ofspecifications of attachment possibilities, stated in theform "ap(PPk) e X" ("attachment point of the k-th PP isone of the elements of X' ) in (3~c).
The AP k and RE Kare introduced constants.
AP k is the attachment point ot10 o PPk" RE k represents the right edge of a constituentstructure tree for the string consisting of the block andthe first k PP's.
(3-c) is in a sort of relaxed CNF, asdiscussed above in connection with (4-a), and in Section5 below.
"T" in (3-c) is defined so that RE TAP = {AP}u {X ~ RE I X precedes AP}.
(When PPk to the rightof PP.
attaches above PP., PP.
is not in the right edge of1 1 l .the resulting structure, and is unavadable for attachmentby material to the right of PPk.
)As for (3), the number of readings of the followingexmnple (from \[Church and Patil 82\]) grows as theCatalan of the number of prepositional phrases:(5) Put the block in the box on the table ... in thekitchen.However, there is an important difference between (3)and (5).
In (3), any number of PP's can attach to theblock, any number of PP's can attach to the box, and soon, No NP in (3) requires complements.
(Dr the boxmust attach to the block, but only because the block isthe only NP that lies to the left of in the box.)
In (5), onthe other hand, put requires one NP argument and onePP argument, and cannot accept any othercomplements.
11 An analysis of (5) along the lines of (3-c) would incorrectly include readings where more thanone PP attaches to put, and readings where no PPattaches to put.
A linear analysis of (5) is as follows:10.
"PP attaches toPP " really means that PP attaches tothe object ofi .
.
k .
i .
.
the preposmon head of PP.
Thts usage permits a brtefer?
k .
discussion than would otherwise be possible.1 l. This characterization of put is not strictly speaking correct, but thenecessary qualifications are irrelevant to the discussion here.\[(ap(PP k) E REk_ l) & (AP k = ap(PPk) &(RE k = (REk.
1 T APk) L.) {PPk} ) &(OG k = OGk.
1 - {APk})\] &\[(ap(PPn) E OGn_ l \[\] REn.1)\](5-a) is similar to (3-c), but includes the additionalconstants OG OG is the open (theta-)gnd for thek"  ksubstructure corresponding to put the block followed bythe first k PP's.
OG k is either {VP}, if none of the first kPP's is attached to V, or is empty Non-empty OG" kindicates that for each constituent X in OG., some PP.,.
.
K 1k<i~n, must attach to X.
\[\] in (5-a) is defined so thatA \[\] B is equal to A if A is non-empty, and is otherwiseequal to B.
The final conjunct in (5-a) captures therequirement that if none of the first n-1 PP's attaches toput, then the final PP must attach to this verb.5.
IssuesThe example constructions presented above illustrate avariety of abstract cases.
In (1), local ambiguities areindependent of each other.
The assignment of anantecedent o a pronoun in (1) does not affect thepossibilities of antecedent assignment for otherpronouns in (1).
An analysis of (1) in CNF need notinclude more than one appearance of any literal.
(2) issimilar to (1) in this respect.
In (4), local ambiguitiesare interdependent, but local ambiguity possibilitiesdepend only on ambiguity possibilities in neighboringclauses.
There is thus a bound on how manyambiguities can interact.
In (3), on the other hand, thereis no such bound.
Choosing an attachment site for PP.. .
.
.
J affects the attachment posslbdlttes for PP ..... no matterhow large k is.
(5) is similar to (3), but ats*~ involves aglobal filter associated with the verb put.
(3-c) and (5-a)employ a richer repertory of operators on structuralconstants (-, , q') than is found in (l-c), (2-b), (2-c), and(4-a).
( l-b) may qualify relatively easily as an exhaustiveanalysis of (1), according to a common conception ofwhat constitutes an analysis.
(3-c), on the other hand,appears to have some of the ealxnarks of a procedure.The similarity of introduced constants to local variablesis obvious.
In particular, the constants AP; and RE; of112 c 5(3-c) conld be replaced with two local variables AP andRE that receive destructive assignment.
(3--c) alsoen'@oys the operators "$" and "~", which might beregarded as corresponding toprocedures.
Whether (3--c)is an analysis or a procedure for computing analyses isultimately a matter of selecting a definition for"linguistic analysis.
"Criteria Io ta  successful definition of "linguisticanalysis" might appeal to psychological reality.
Onepossible requirement is that components of analysescorrespond to partial analyses built during humanprocessing.
When definitions of constants (assignmentsto local variables) are blended into what is otherwise aCNF formula, as in (2-c), (3-c), (4-a), and (5-a), theresult might be called "relaxed CNF."
Somewhat moreprecisely, suppose that a formula in "relaxed CNF" is aconjunction of "relaxed isjunctions," where a "relaxeddisjunction" is the conjunction of a "generalizeddisjunction" with an "assignment formula."
A"generalized disjunction" rnay be either a disjunction ofatoms, or a statement of the form "x c A."
An"assignment formula" is a conjunction of statements hatassign values to constants.
Given such a relaxed CNFformula as an exhaustive analysis, obtaining an analysisof a single reading requires for' each generalizeddisjunction the choice of a disjunct or the selection of anelement.
Such single--reading analyses may be producedby deterministic variants of non-detemfinistic processingmodels that produce exhaustive analyses in relaxedCNF.
Relaxed CNF is compatible with a variety ofprocessing models.
For example, a component of theform "x e A" might be produced before componentsthat specify the corrtents of A.Recent work on Kolmogorov complexity mightprovide alternative criteria for the definition of"linguistic analysis."
(\[Li and Vitffnyi 89\] is a recentsurvey ol' work on Kolmogorov complexity.)
Inparticular, notions of time-bounded algorithmiccomplexity, such as the "logical depth" of \[Bennett 88\],may be relevant.
Following a third alternative, asatisfactory definition of "analysis" may involve acorrespondence principle, along lhe lollowing lines:N 13" i t  every component of a l%al analysis specificallymentions one or more components of the input.
For thisto work, "component" and "mention" themselves requireappropriate definitions.
Arbitrary fragments of analysescannot count as "components."
Mention" should betransitive.Analyses in relaxed CNF may be more compatiblewith principle-based grammars than are tree-basedanalyses.
(\[Chomsky 81\] is the seminal work onprinciple-based grammars.)
Constituent structure doesnot occupy as central a place in the principleobasedparadigm as in other' grammatical paradigms.
Eachgeneralized isjunction (or its deterministic counterpart)supplies a piece of information about tire analyzedexpression.
Assignment formulas capture logicaldependencies among thcse pieces.
Each of the examplesin this oar~er illustrates a sinele Nlenomenon.
RelaxedCNF can also capture interactions among ptmn~mmr~a.Analyses like (3-c) are probably less easily readablethan packed shared forests.
Full analyses that specifyconstituent structure information together with relativeoperator scope information, information on anaphora,and so on, will be even less readable.
It may be possibleto devise a more graphically oriented notation for linearencodings of linguistic analyses.Wlmtever cozrception of "linguistic analysis" mayultimately prove most useful, it seems clear that workingwith relaxed Conjunctive Norrnal Form has advantagesover working with Disjunctive Normal Form lorcomputational pplications.
Relaxed CNF also appearsto have advantages over DNF for psycholinguisticmodeling.
Introduced constants (local variables) haveobvious utility in implementations.
They may also playa role in human processing of language.
In particular, ashuman processing proceeds, explicit details ofpreviously encountered structure may recede into thebackground yet remain accessible.Acknowledgments1 am indebted for comments and discussions to StevenAbney, Yves Caseau, and Andrew Ogielski.Responsibility tbr errors is entirely mine.ReferencesE.
Barton, R. Berwick, and E. Ristad, ComputationalComplexity and Natural Ixmguage, M_IT Press,Cmnbridge, Massachusetts, 1987.C.
Bennett, "Logical Depth and Physical Complexity,"in R. Herken (ed.
), The Universal J~ring Machine; AHalf-Century Survey, pp.
227-258, Oxford UniversityPress, Oxford, 1988.N.
Chomsky, Lectures on Government and Binding,Foris Publications, Dordrecht, 1981.K.
Church and R. Patil, "Coping with SyntacticAmbiguity or How to Put the Block in the Box on theTable," American Journal of ComputationalLinguistics, 8:3-4, pp.
139-:149, 1982.S.
Epstein, "Principle-Based Interpretation of NaturalLanguage Quantifiers," Proceedings of the SeventhNational Conference on Artificial Intelligence (AAAI-88), pp.
718~723, 1988.M.
Garey and D. Johnson, Computers and Intractability,W.
H. Freeman, San Francisco, 1979.M.
Li and P. Vitgnyi, Kolmogorov Complexity and ItsApplications (Revised Version), Report CS-R8901,Centre tor Mathematics and Computer Science,Amsterdam, 1989.M.
Tomita, "An Efficient Augmented-Context-FreeParsing Algorithm," Computational Linguistics, 13:1-2,pp.
31-46, 1987.6 113
