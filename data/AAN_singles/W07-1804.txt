Proceedings of SPEECHGRAM 2007, pages 25?32,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsGrammar-based context-specific statistical language modellingRebecca JonsonDepartment of Linguistics, Go?teborg University & GSLTBox 200, 40530 Go?teborg, Swedenrj@ling.gu.seAbstractThis paper shows how we can combinethe art of grammar writing with the powerof statistics by bootstrapping statistical lan-guage models (SLMs) for Dialogue Systemsfrom grammars written using the Grammati-cal Framework (GF) (Ranta, 2004).
Further-more, to take into account that the probabil-ity of a user?s dialogue moves is not staticduring a dialogue we show how the samemethodology can be used to generate dia-logue move specific SLMs where certain di-alogue moves are more probable than others.These models can be used at different pointsof a dialogue depending on contextual con-straints.
By using grammar generated SLMswe can improve both recognition and un-derstanding performance considerably overusing the original grammar.
With dialoguemove specific SLMs we would be able toget a further improvement if we had an op-timal way of predicting the correct languagemodel.1 IntroductionSpeech recognition (ASR) for dialogue systems isoften caught in the trap of the sparse data problemwhich excludes the possibility of using statisticallanguage models (SLMs).
A common approach isto write a grammar for the domain either as a speechrecognition grammar (SRG) or as an interpreta-tion grammar which can be compiled into a speechrecognition grammar (SRG) using some grammardevelopment platform such as Gemini, Regulus orGF (Rayner et al, 2000; Rayner et al, 2006; Ranta,2004).
The last option will assure that the linguis-tic coverage of the ASR and interpretation are keptin sync.
ASR for commercial dialogue systems hasmainly focused on grammar-based approaches de-spite the fact that SLMs seem to have a better over-all performance (Knight et al, 2001; Bangalore andJohnston, 2003).
This probably depends on the time-consuming work of collecting corpora for trainingSLMs compared with the more rapid and straight-forward development of SRGs.
However, SLMs aremore robust for out-of-coverage input, perform bet-ter in difficult conditions and seem to work betterfor naive users as shown in (Knight et al, 2001).SRGs on the other hand are limited in their coveragedepending on how well grammar writers succeed inpredicting what users may say.An approach taken in both dialogue systems anddictation applications is to write a grammar for theparticular domain and generate an artificial corpusfrom the grammar to be used as training corpus forSLMs (Galescu et al, 1998; Bangalore and John-ston, 2003; Jonson, 2006).
These grammar-basedmodels are not as accurate as the ones built fromreal data as the estimates are artificial, lacking a re-alistic distribution.
However, as has been shown in(Bangalore and Johnston, 2003; Jonson, 2006) thesegrammar-based statistical models seem to have amuch more robust behaviour than their correspond-ing grammars which leaves us with a much betterstarting point in the first development stage in a di-alogue system.
It is a way of compromising be-tween the ease of grammar writing and the robust-25ness of SLMs.
With this methodology we can usethe knowledge and intuition we have about the do-main and include it in our first SLM and get a muchmore robust behaviour than with a grammar.
Fromthis starting point we can then collect more datawith our first prototype of the system to improve ourSLM.
In this paper the advantage of this method isshown further by evaluating a different domain ingreater detail.Context-specific models have shown importantrecognition performance gain (Baggia et al, 1997;Riccardi et al, 1998; Xu and Rudnicky, 2000;Lemon and Gruenstein, 2004) and have usually beenof two types: created as state-specific grammars orbuilt from collected data partitioned according to di-alogue states.
Both methods have their disadvan-tages.
In the first case, we constrain the user heavilywhich makes them unsuitable for use in a more flex-ible system such as an information-state based sys-tem.
This can be solved by having a back-off methodbut leaves us with extra processing (Lemon and Gru-enstein, 2004).
In the latter case, we have an evenmore severe sparse data problem than when creat-ing a general SLM as we need enough data to get agood distribution of data over dialogue states.
In aninformation-state based system where the user is notrestricted to only a few dialogue states this problemgets even worse.
In addition, why we chose to workwith grammar-based SLMs in the first place was be-cause data is seldom available in the first stage of di-alogue system development.
This leaves us with therequirement of an SLM that although being context-specific does not constrain the user and which as-sures a minimal coverage of expressions for a cer-tain context.
In (Gruenstein et al, 2005) this is ac-complished by dynamically populating a class-basedSLMs with context-sensitive content words and ut-terances.
In this paper, we will show how we canuse the same methodology as in (Jonson, 2006) tocreate context-specific SLMs from grammars basedon dialogue moves that match these criteria.This study is organized as follows.
First, we in-troduce our methodology for developing SLMs fromgrammars.
Section 3 describes the data collection oftest utterances and how we have partitioned the datainto different test sets depending on grammar cov-erage, types of users and types of dialogue moves.In section 4, we show and discuss the results of thedifferent models for different test sets and finally wedraw some conclusions from the experiments.2 Grammar-based SLMsIn (Jonson, 2006) we described how we could gen-erate an SLM from an interpretation grammar writ-ten in GF for an MP3 player application and geta much more robust behaviour than by using theoriginal grammar for ASR.
In this study, we ap-proach a different domain using a GF grammar writ-ten for a dialogue system application called Agen-daTalk (Ericsson et al, 2006).
It is one of severalapplications that has been developed in the TALKproject (www.talk-project.org) and has been builtwith the TrindiKit toolkit and the GoDiS dialoguesystem (Larsson, 2002) as a GoDiS application.
Itworks as a voice interface to a graphical calendar.Apart from evaluating a different domain in a moreextensive way to see if the tendency we found in(Jonson, 2006) is consisting over domains, we havedriven the methodology a bit further to be able togenerate context-specific SLMs that favour certainparts of the grammar, in our case certain dialoguemoves.
We call these SLMs ?dialogue move spe-cific SLMs?
(DMSLMs).
Both types of models areobtained by generating all possible utterances froma GF grammar, building trigram SLMs from thegrammar-based corpus using the SRI language mod-elling toolkit (Stolcke, 2002) and compiling theminto recognition packages.
For comparison we havealso compiled the GF grammar directly into a Nu-ance speech recognition grammar using the GF com-piler.2.1 Building a general SLM fromgrammar-based corporaThe GF grammar written for the calendar domainconsists of 500 GF functions (rules) where 220 aredomain-specific and 280 inherited from a domain-independent grammar.
It exists in two equivalentlanguage versions that share the same GF functions:English and Swedish.
We have used GF?s facili-ties to generate a corpus from the Swedish versionconsisting of all possible meaningful utterances gen-erated by the grammar to a certain depth of theanalysis trees in GF?s abstract syntax.
The gram-mar is written on the phrase level accepting spoken26language utterances such as e.g.
?add a bookingplease?.
The resulting corpus consists of 1.7 millionutterances and 19 million words with a vocabularyof only 183 words.
All utterances in the corpus oc-cur exactly once.
However, all grammar rules arenot expanded which leaves us with a class-taggedcorpus without e.g.
all variants of date expressionsbut with the class date.
What we get in the end istherefore a class-based SLM that we compile into arecognition package together with a rule-based de-scription of these classes.
The SLM has 3 differ-ent classes: time, date and event and the do-main vocabulary when including all distinct wordsin these classes make up almost 500 words.Adding real speech corporaIn (Jonson, 2006) we saw that the use of real cor-pora in interpolation with our artificial corpus wasonly valuable as long as the real corpora approxi-mated the language of use.
The big news corpus wehad available did not give any significant improve-ment but the transcribed Swedish speech corpus weused was much more helpful.
In this study we havetherefore once again used the GLSC corpus to im-prove our word occurrence estimates by interpolat-ing it with our grammar-based SLM.
The Gothen-burg Spoken Language (GSLC) corpus consists oftranscribed Swedish spoken language from differentsocial activities such as auctions, phone calls, meet-ings, lectures and task-oriented dialogue (Allwood,1999).
The corpus is composed of about 1,300,000words and is turn-based which gives it long utter-ances including e.g.
transcribed disfluencies.
Fromthis corpus we have built an SLM which we haveinterpolated with our grammar-based SLM keepingour domain vocabulary.
This means we are just con-sidering those n-grams in the GSLC corpus whichmatch the domain vocabulary to hopefully get amore realistic probability distribution for these.
Wewill call this model our Extended SLM.2.2 Dialogue move specific SLMsSLMs capture the lexical context statistics in a spe-cific language use.
However, the statistical distribu-tion in a dialogue is not static but varies by boost-ing and lowering probabilities for different wordsand expressions depending on contextual appropri-ateness.
It is not only words and expressions thatvary their distribution but on a semantic level differ-ent conceptual messages will be more or less prob-able as a user utterance at different points of thedialogue.
This means that certain dialogue moveswill have a higher degree of expectancy at a specificpoint of the dialogue.
To capture this phenomenon,we want to build models that raise the probability ofcertain dialogue moves in certain contexts by givinga higher probability for utterances expressing thesedialogue moves.
These are models where utterancescorresponding to a certain dialogue move are moresalient (e.g.
a model where all ways of answeringyes or no are more plausible than other utterances).Such a model will account for the fact that the expec-tation of dialogue moves a user will perform variesin a dialogue and thereby their statistics.
We canobtain this by using a version of the grammar-basedcorpus where the dialogue moves for each utteranceare generated which allows us to partition the corpusin different ways based on dialogue moves.
We canthen take out part of the corpus e.g.
all utterancescorresponding to a certain dialogue move, create anSLM and interpolate it with the general grammar-based SLM.
In this way, we get SLMs where certaindialogue moves are more probable than others andwhere minimally all possible expressions for these,which the grammar describes, are covered.
By in-terpolating with the general SLM we put no hardconstraints on the expected dialogue move so theuser can in fact say anything at any point in the di-alogue despite the raised expectancy for certain dia-logue moves.
We just boost the expected probabilityof certain dialogue moves and their possible expres-sions.
By using contextual constraints in the infor-mation state we could then predict which model touse and switch SLMs on the fly so that we obtain arecognizer that takes account of expected user input.2.2.1 Partitioning the training data by dialoguemovesIn GoDiS, dialogue moves are activity related andexist in seven different types: request moves,answer moves, ask moves (i.e.
questions), yesand no ( yn) answers, greet moves, quit movesand feedback and sequencing moves which arecalled ICM:s (Larsson, 2002).
We have chosen tofocus on the first four of these dialogue move typesto build up our DMSLMs.
We have used GF to gen-27erate a corpus with all possible dialogue moves andtheir combinations with their corresponding expres-sions.
From this corpus we have extracted all utter-ances that can be interpreted as an answer moveor a sequence of answer moves, all expressions forspecification of a request (in GoDiS what type ofaction to perform e.g.
deleting a booking), all waysof expressing questions in our grammars (i.e.
askmoves) and all possible yn answers.
This leaves uswith four new sets of training data.The decision to partition the data in this way wasbased on the distribution of dialogue moves in ourdata where the moves we focus on are the most com-mon ones and the most critical for achievement ofthe dialogue tasks.
As these dialogue moves are ab-stract and domain-independent it would be possibleto use a domain-independent prediction of these di-alogue moves and thereby the language models al-though the structure of the SLMs would be differentin different domains.2.2.2 Building dialogue move specific SLMsFor each set of dialogue move specific trainingdata we created an SLM that only captures ways ofexpressing a specific dialogue move.
However, weare looking for less constrained models which justalter the probability of certain dialogue moves.
Byinterpolating the SLMs built on dialogue move spe-cific corpora with the general grammar-based SLMwe achieve models with contextual probabilities butthat generalize to avoid constraining the user input.The interpolation of these models was carried outwith the SRILM toolkit based on equation 1.
Theoptimal lambda weight was estimated to 0.85 for allmodels with the SRILM toolkit using held-out data.Pdmslm(W ) = ?Pmovespec(W ) + (1?
?
)Pgeneral(W ) (1)We ended up with four new SLMs, so called DM-SLMs, in which either the probability of answer,ask, request or yn moves were boosted.3 Test DataThe collection of test data was carried out by hav-ing people interacting with the AgendaTalk systemusing the grammar-based SLM.
The test group in-cluded both naive users with no experience of thesystem whatsoever and users that had previous ex-perience with the system to varying extents.
Wehave classified the latter group as expert users al-though the expertise varies considerably.
All userswere given a printed copy of a calendar month withscheduled bookings and some question marks andwere assigned the task of altering the voice-basedcalendar so that the graphical calendar would lookthe same as the printed copy except for the questionmarks which they were to find values for by query-ing the system.
This would mean that they wouldhave to add, delete and alter bookings as well as findout information about their schedule e.g.
the time ofan event.
The tasks could be carried out in any orderand there were many different ways to complete theschedule.The data collection gave us a recording test setof 1000 recorded utterances from 15 persons (all na-tive, 8 female, 7 male).
This unrestricted test set wasused to compare recognition performance betweenthe different models under consideration.
We alsopartitioned the test set in various ways to exploredifferent features.
The test set was parsed to get alin-coverage utterances that the original GF grammarcovers to create an in-coverage test set from these.In addition, we partitioned the data by users with atest set with the naive user utterances and anothertest set from the expert users.
In this way we couldexplore how our models performed under differentconditions.
Different dialogue system applicationswill have a different distribution of users.
Somesystems will always have a large number of naiveor less experienced users who will use more out-of-coverage utterances and more out-of-vocabulary(OOV) words whereas users of other applicationswill have the opportunity to obtain considerable ex-perience which will allow them to adapt to the sys-tem, in particular to its grammar and vocabulary.The recordings for the unrestricted test set have anOOV rate of 6% when using our domain vocabulary.The naive test set makes up 529 of these recordingswith an OOV rate of 8% whereas the expert test setof 471 recordings has a lower OOV rate of 4%.
Thein-coverage test set consists of 626 utterances leav-ing us with an in-coverage rate of 62.6% for the un-restricted test set.
This shows the need for a more ro-bust way of recognition and interpretation if we ex-pect to expose the system to less experienced users.For the evaluation of the DMSLMs we have par-titioned the test data by dialogue moves.
The utter-28ances corresponding with the four dialogue moveschosen for our DMSLMs were divided into four testsets.
The utterances left were used to create a fifthtest set where none of our four DMSLMs wouldapply but where we would need to use the gen-eral model.
If we look at the distribution of thetest data considering dialogue moves we find that75.4% of the test data falls into our four dialoguemove categories and that only 24.6% of the datawould require the general model.
This part of thetest data includes dialogue moves such as greetings,quit moves and dialogue move sequences with com-binations of different moves.
The most common di-alogue move in our data is an answer move or asequence of answer moves resulting in commonutterances such as: ?a meeting on friday?
as answerto system questions such as ?what booking do youwant to add?
?.4 Experimental ResultsTo evaluate the recognition performance of our dif-ferent types of models we ran several experimentson the different test sets.
We report results on worderror rate (WER), sentence error rate (SER) and alsoon a semantic level by reporting what we call dia-logue move error rate (DMER).
The dialogue moveerror rate was obtained by parsing the recognized ut-terances and comparing these to a parsed version ofthe transcriptions, calculating the rate of correctlyparsed dialogue moves.
The calculation was donein the same way as calculation of concept error rate(CER) proposed by (Boros et al, 1996) where thedegree of correctly recognized concepts is consid-ered.
In our case this means the degree of correctlyrecognized dialogue moves.
For parsing we haveused a phrase-spotting grammar written in Prologthat pattern matches phrases to dialogue moves.
Us-ing the original GF interpretation grammar for pars-ing would have restricted us to the coverage of thegrammar which is not an optimal choice togetherwith SLMs.
Ideally, we would like to use a robustversion of GF to be able to use the original GF gram-mar both for parsing and SLM generation and bythat assure the same linguistic coverage.
Attemptsto do this have been carried out in the TALK projectfor theMP3 domain by training a dialogue move tag-ger on the same type of corpus that was used forthe DMSLMs where dialogue moves occur togetherwith their corresponding utterances.
Other meth-ods of relaxing the constraints of the GF parser arealso under consideration.
Meanwhile, we are using asimple robust phrase spotting parser.
We have inves-tigated both how our grammar-based SLMs performin comparison to our grammar under different condi-tions to see how recognition and understanding per-formance varies as well as how our DMSLMs per-form in comparison to the general grammar-basedSLM.
The results are reported in the following sec-tions.
All models have the same domain vocabularyand the OOV figures presented earlier thereby applyfor all of them.4.1 Grammar-based SLMs vs. grammarsTable 1 shows the results for our different languagemodels on our unrestricted test set of 1000 utter-ances as well as for the part of this test set which isin-coverage.
As expected they all performmuch bet-ter on the in-coverage test set with the lowest WERobtained with our grammar.
On the unrestricted testset we can see an important reduction of both WER(26% and 38% relative improvement) and DMER(24% and 40% relative improvement) for the SLMsin comparison to the grammar which indicates therobustness of these to new user input.In table 2 we can see how the performance of allour models are better for the expert users with a rel-ative word error rate reduction from 25% to 32% incomparison to the results for the naive test set.
Thesame pattern is seen on the semantic level with im-portant reduction in DMER.
The result is expectedas the expert users have greater knowledge of thelanguage of the system.
This is consistent with theresults reported in (Knight et al, 2001).
It is alsoreflected in the OOV figures discussed earlier wherethe naive users seem to have used many more un-known words than the expert users.This shows that the models perform very dif-ferently depending on the types of users and howmuch they hold to the coverage of the grammar.Our grammar-based SLM gives us a much more ro-bust behaviour which is good when we expect lessexperienced users.
However, we can see that weget a degradation in in-coverage performance whichwould be critical if we are to use the model in a sys-tem where we expect that the users will achieve cer-29Table 1: Results on unrestricted vs in-coverage test setModel Unrestricted In-coverageWER SER DMER WER SER DMERGrammar 39.0% 47.6% 43.2% 10.7% 16.3% 10.3%Grammar-based SLM 29.0% 39.7% 33.0% 14.8% 18.3% 13.7%Extended SLM 24.0% 35.2% 25.8% 11.5% 15.8% 10.4%Table 2: Results on naive vs expert usersModel Naive users Expert usersWER SER DMER WER SER DMERGrammar 46.6% 50.3% 54.7% 31.7% 44.4% 33.2%Grammar-based SLM 34.4% 42.9% 41.3% 23.8% 35.9% 25.8%Extended SLM 27.6% 38.2% 29.5% 20.7% 31.8% 22.7%tain proficiency.
The Extended SLM seem to per-form well in all situations and if we look at DMERthere is no significant difference in performance be-tween this model and our grammar when it comesto in-coverage input.
In most systems we will prob-ably have a range of users with different amountsof experience and even experienced users will failto follow the grammar in spontaneous speech.
Thispoints towards the advisability of using an SLM asit is more robust and if it does not degrade too muchon in-coverage user input like the Extended SLMit would be an optimal choice.From the results it seems that we have found a cor-relation between the DMER and WER in our systemwhich indicates that if we manage to lower WER wewill also achieve better understanding performancewith our simple robust parser.
This is good news as itmeans that we will not only capture more words withour SLMs but also more of the message the user istrying to convey in the sense of capturing more dia-logue moves.
This will definitely result into a betterdialogue system performance overall.
Interestingly,we have been able to obtain this just by convertingour grammar into an SLM.4.2 Dialogue move specific SLMs vs GeneralSLMsWe have evaluated our DMSLMs on test sets foreach model which include only utterances that corre-spond to the dialogue moves in the model.
It shouldbe mentioned that the test sets may include utter-ances not covered by the original GF grammar e.g.
adifferent wording for the same move.
The results foreach DMSLM on its specific test set and the perfor-mance of the grammar-based SLM and the ExtendedSLM are reported in tables 3, 4, 5 and 6.Table 3: Ask Move SLMModel WER SER DMERGrammar-based SLM 39.2% 68.4% 51.8%Ask DMSLM 31.8% 68.9% 48.7%Extended SLM 30.1% 58.0% 44.6%Table 4: Answer Move SLMModel WER SER DMERGrammar-based SLM 17.3% 22.0% 16.3%Answer DMSLM 15.7% 20.1% 14.1%Extended SLM 18.2% 22.0% 16.7%Table 5: Request Move SLMModel WER SER DMERGrammar-based SLM 29.1% 44.3% 27.0%Request DMSLM 17.0% 36.1% 14.7%Extended SLM 26.3% 42.6% 22.1%Apart from these four dialogue moves our testdata includes a lot of different dialogue moves anddialogue move combinations that we have not con-sidered.
As we have no specific model for these wewould need to use a general model in these cases.This means that apart from predicting the four di-alogue moves we have considered we would alsoneed to predict when none of these are expected anduse the general model for these situations.
In table7 we can see how our general models perform onthe rest of the test set.
This shows that they seem tohandle this part of the test data quite well.30Table 6: YN Move SLMModel WER SER DMERGrammar-based SLM 37.3% 27.3% 22.7%YN DMSLM 21.5% 16.5% 11.9%Extended SLM 25.0% 18.2% 12.5%Table 7: General SLM on rest of test dataModel WER SER DMERGrammar-based SLM 22.2% 42.7% 31.7%Extended SLM 19.6% 39.8% 26.0%We can see that the gain we get in recognitionperformance varies for the different models and thatrelative improvement in WER goes from 9% for theanswer model to 42% for our DMSLMs on appro-priate test sets.
We can see that our models havemost problems with ask moves and yn answers.In the case of ask moves this seems to be becauseour GF grammar is missing a lot of syntactic con-structions of question expressions.
This would thenexplain why the Extended SLM gets a much betterfigure here.
The GSLC corpus does capture moreof this expressive variation of questions.
In otherwords we seem to have failed to capture and predictthe linguistic usage with our hand-tailored grammar.In the case of yn answers the result reveals that ourgrammar-based SLM does not have a realistic distri-bution of these expressions at all.
This seems to besomething the GSLC corpus contribute, consideringthe good results for the Extended SLM.
However,we can see that we can achieve the same effect byboosting the probability of yes and no answers inour DMSLM.If we look at the overall achievement in recog-nition performance, using our DMSLMs when ap-propriate and in other cases the general SLM, theaverage WER of 22% (27% DMER) is consider-ably lower than when using the general model forthe same test data (29% WER, 33% DMER).
If wehad an optimal method for predicting what languagemodel to use we would be able to decrease WER by24% relative.
If we chose to use the ExtendedSLM in the cases our DMSLMs do not cover wecould get an even greater reduction.We have also tested how well our DMSLMs per-form on the general test set (i.e.
all 1000 utter-ances) to see how bad the performance would be ifwe chose the wrong model.
In table 8 we can seethat this approach yields an average WER of 30%which is a minimal degradation in comparison tothe general grammar-based SLM.
On the contrary,some of our models actually perform better than ourgeneral grammar-based SLM or very similarly.
Thisimplies that there is no substantial risk on recogni-tion performance if our prediction model would fail.This means that we could obtain very good resultswith important recognition improvement even withan imperfect prediction accuracy.
We have a relativeimprovement of 24% to gain with only a minimalloss.Table 8: DMSLMs on general test setModel WER SERAnswer DMSLM 34.7% 55.6%Ask DMSLM 28.2% 46.2%Request DMSLM 26.5% 43.2%YN DMSLM 29.8% 44.0%5 Concluding remarksOur experimental results show that grammar-basedSLMs give an important reduction in both WER andDMER in accordance with the results in (Jonson,2006).
We reach a relative improvement of 26%and a further 17% if we interpolate our grammar-based SLM with real speech data.
The correlationof the DMER and the WER in our results indicatesthat the improved recognition performance will alsopropagate to the understanding performance of oursystem.Context-specific language models (statistical andrule-based) have shown important recognition per-formance gain in earlier work (Baggia et al, 1997;Xu and Rudnicky, 2000; Lemon and Gruenstein,2004; Gruenstein et al, 2005) and this study reaf-firms that taking into account statistical languagevariation during a dialogue will give us more accu-rate recognition.
The method we use here has the ad-vantage that we can build statistical context-specificmodels even when no data is available, assuring aminimal coverage and by interpolation with a gen-eral model do not constrain the user input unduly.The language model switch will be triggered bychanging a variable in our information state: the pre-dicted dialogue move.
However, to be able to choose31which language model suits the current informationstate best we need a way to predict dialogue moves.The prediction model could either be rule-based ordata based.
Our first experimental tests with ma-chine learning for dialogue move prediction seemspromising and we hope to report on these soon.
Op-timally, we want a prediction model that we canuse in different GoDiS domains to be able to gen-erate new DMSLMs from our domain-specific GFgrammar for the dialogue moves we have consideredhere.Our experiments show that we could achievean overall reduction in WER of 46% and 40% inDMER if we were able to choose our best suitedSLM instead of our compiled GF grammar.
Natu-rally, we would have to take into account dialoguemove prediction accuracy to get a more realistic fig-ure.
However, our experiments also show that theeffect on performance if we failed to use the correctmodel would not be too harmful.
This means wehave much more to gain than to lose even if the di-alogue move prediction is not perfect.
This makesthis approach a very interesting option in dialoguesystem development.AcknowledgmentI would like to thank Nuance Communications Inc.for making available the speech recognition soft-ware used in this work.ReferencesJens Allwood.
1999.
The Swedish spoken languagecorpus at Go?teborg University.
In Proceedings ofFonetik?99: The Swedish Phonetics Conference.Paolo Baggia, Morena Danieli, Elisabetta Gerbino,Loreta Moisa, and Cosmin Popovici.
1997.
Con-textual information and specific language models forspoken language understanding.
In Proceedings ofSPECOM.Srinivas Bangalore andMichael Johnston.
2003.
Balanc-ing data-driven and rule-based approaches in the con-text of a multimodal conversational system.
In Pro-ceedings of the ASRU Conference.M.
Boros, W. Eckert, F. Gallwitz, G. Go?rz, G. Hanrieder,and H. Niemann.
1996.
Towards understanding spon-taneous speech: Word accuracy vs. concept accuracy.In Proceedings of ICSLP, Philadelphia, PA.S.
Ericsson, G. Amores, B. Bringert, H. Burden,A.
Forslund, D. Hjelm, R. Jonson, S. Larsson,P.
Ljunglo?f, P. Manchon, D. Milward, G. Perez, andM.
Sandin.
2006.
Software illustrating a unified ap-proach to multimodality and multilinguality in the in-home domain.
Deliverable D1.6, TALK Project.Lucian Galescu, Eric Ringger, and James Allen.
1998.Rapid language model development for new task do-mains.
In In Proceedings of the ELRA First Interna-tional Conference on Language Resources and Evalu-ation (LREC).Alexander Gruenstein, Chao Wang, and Stephanie Sen-eff.
2005.
Context-sensitive statistical language mod-eling.
In Proceedings of Interspeech.Rebecca Jonson.
2006.
Generating statistical languagemodels from interpretation grammars in dialogue sys-tems.
In Proceedings of EACL, Trento, Italy.S.
Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-ing, and I. Lewin.
2001.
Comparing grammar-basedand robust approaches to speech understanding: Acase study.
In Proceedings of Eurospeech.Staffan Larsson.
2002.
Issue-based Dialogue Manage-ment.
Ph.D. thesis, Go?teborg University.Oliver Lemon and Alexander Gruenstein.
2004.
Multi-threaded context for robust conversational interfaces:Context-sensitive speech recognition and interpreta-tion of corrective fragments.
ACM Trans.
Comput.-Hum.
Interact., 11(3):241?267.Aarne Ranta.
2004.
Grammatical framework.
a type-theoretical grammar formalism.
The Journal of Func-tional Programming, 14(2):145?189.Manny Rayner, Beth Ann Hockey, Frankie James, Eliz-abeth Owen Bratt, Sharon Goldwater, and Jean MarkGawron.
2000.
Compiling language models from alinguistically motivated unification grammar.
In Pro-ceedings of the COLING.Manny Rayner, Beth Ann Hockey, and Pierrette Bouil-lon.
2006.
Putting Linguistics into Speech Recogni-tion: The Regulus Grammar Compiler.
CSLI Publica-tions.Guiseppe Riccardi, Alexandros Potamianos, andShrikanth Narayanan.
1998.
Language model adapta-tion for spoken language systems.
In Proceedings ofthe ICSLP, Australia.Andreas Stolcke.
2002.
SRILM - An extensible lan-guage modeling toolkit.
In Proceedings of ICSLP,Denver, Colorado.Wei Xu and Alex Rudnicky.
2000.
Language model-ing for dialog system.
In Proceedings of ICSLP 2000,Beijing, China.32
