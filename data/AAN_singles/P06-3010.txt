Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 55?60,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Hybrid Relational Approach for WSD ?
First ResultsLucia SpeciaN?cleo Interinstitucional de Ling?
?stica Computational ?
ICMC ?
University of S?o PauloCaixa Postal 668, 13560-970, S?o Carlos, SP, Brazillspecia@icmc.usp.brAbstractWe present a novel hybrid approach forWord Sense Disambiguation (WSD)which makes use of a relational formalismto represent instances and backgroundknowledge.
It is built using InductiveLogic Programming techniques to com-bine evidence coming from both sourcesduring the learning process, producing arule-based WSD model.
We experimentedwith this approach to disambiguate 7highly ambiguous verbs in English-Portuguese translation.
Results showedthat the approach is promising, achievingan average accuracy of 75%, which out-performs the other machine learning tech-niques investigated (66%).1 IntroductionWord Sense Disambiguation (WSD) is concernedwith the identification of the correct sense of anambiguous word given its context.
Although it canbe thought of as an independent task, its importanceis more easily realized when it is applied to particu-lar tasks, such as Information Retrieval or MachineTranslation (MT).
In MT, the application we arefocusing on, a WSD (or translation disambigua-tion) module should identify the correct translationfor a source word when options with differentmeanings are available.As shown by Vickrey et al (2005), we believethat a WSD module can significantly improve theperformance of MT systems, provided that suchmodule is developed following specific require-ments of MT, e.g., employing multilingual senserepositories.
Differences between monolingual andmultilingual WSD are very significant for MT,since it is concerned only with the ambiguities thatappear in the translation (Hutchins and Sommers,1992).In this paper we present a novel approach forWSD, designed focusing on MT.
It follows a hy-brid strategy, i.e., knowledge and corpus-based,and employs a highly expressive relational for-malism to represent both the examples and back-ground knowledge.
This approach allows theexploitation of several knowledge sources, to-gether with evidences provided by examples ofdisambiguation, both automatically extractedfrom lexical resources and sense tagged corpora.This is achieved using Inductive Logic Pro-gramming (Muggleton, 1991), which has notbeen exploited for WSD so far.
In this paper weinvestigate the disambiguation of 7 highly am-biguous verbs in English-Portuguese MT, usingknowledge from 7 syntactic, semantic and prag-matic sources.In what follows, we first present some relatedapproaches on WSD for MT, focusing oh theirlimitations (Section 2).
We then give some basicconcepts on Inductive Logic Programming and de-scribe our approach (Section 3).
Finally, we presentour initial experiments and the results achieved(Section 4).2 Related workMany approaches have been proposed for WSD,but only a few are designed for specific applica-tions, such as MT.
Existing multilingual approachescan be classified as (a) knowledge-based ap-proaches, which make use of linguistic knowledgemanually codified or extracted from lexical re-sources (Pedersen, 1997; Dorr and Katsova, 1998);(b) corpus-based approaches, which make use ofknowledge automatically acquired from text usingmachine learning algorithms (Lee, 2002; Vickrey etal., 2005); and (c) hybrid approaches, which em-ploy techniques from the two other approaches (Zi-novjeva, 2000).55Hybrid approaches potentially explore the ad-vantages of both other strategies, yielding accurateand comprehensive systems.
However, they arequite rare, even in monolingual contexts (Stevensonand Wilks, 2001, e.g.
), and they are not able to in-tegrate and use knowledge coming from corpus andother resources during the learning process.In fact, current hybrid approaches usually em-ploy knowledge sources in pre-processing steps,and then use machine learning algorithms to com-bine disambiguation evidence from those sources.This strategy is necessary due to the limitations ofthe formalism used to represent examples in themachine learning process: the propositional formal-ism, which structures data in attribute-value vectors.Even though it is known that great part of theknowledge regarding to languages is relational(e.g., syntactic or semantic relations among wordsin a sentence) (Mooney, 1997), the propositionalformalism traditionally employed makes unfeasiblethe representation of substantial relational knowl-edge and the use of this knowledge during thelearning process.According to the attribute-value representation,one attribute has to be created for every feature, andthe same structure has to be used to characterize allthe examples.
In order to represent the syntacticrelations between every pair of words in a sentence,e.g., it will be necessary to create at least one attrib-ute for each possible relation (Figure 1).
This wouldresult in an enormous number of attributes, sincethe possibilities can be many in distinct sentences.Also, there could be more than one pair with thesame relation.Sentence: John gave to Mary a big cake.verb1-subj1 verb1-obj1 mod1-obj1 ?give-john give-cake big-cake ?Figure 1.
Attribute-value vector for syntactic relationsGiven that some types of information are not avail-able for certain instances, many attributes will havenull values.
Consequently, the representation of thesample data set tends to become highly sparse.
It iswell-known that sparseness on data ensue seriousproblems to the machine learning process in general(Brown and Kros, 2003).
Certainly, data will be-come sparser as more knowledge about the exam-ples is considered, and the problem will be evenmore critical if relational knowledge is used.Therefore, at least three relevant problems arisefrom the use of a propositional representation incorpus-based and hybrid approaches: (a) the limita-tion on its expressiveness power, making it difficultto represent relational and other more complexknowledge; (b) the sparseness in data; and (c) thelack of integration of the evidences provided byexamples and linguistic knowledge.3 A hybrid relational approach for WSDWe propose a novel hybrid approach for WSDbased on a relational representation of both exam-ples and linguistic knowledge.
This representationis considerably more expressive, avoids sparsenessin data, and allows the use of these two types ofevidence during the learning process.3.1 Sample dataWe address the disambiguation of 7 verbs selectedaccording to the results of a corpus study (Specia,2005).
To build our sample corpus, we collected200 English sentences containing each of the verbsfrom a corpus comprising fiction books.
In a previ-ous step, each sentence was automatically taggedwith the translation of the verb, part-of-speech andlemmas of all words, and subject-object syntacticrelations with respect to the verb (Specia et al,2005).
The set of verbs, their possible translations,and the accuracy of the most frequent translationare shown in Table 1.Verb # Translations Most frequenttranslation - %come 11 50.3get 17 21give 5 88.8go 11 68.5look 7 50.3make 11 70take 13 28.5Table 1.
Verbs and their possible senses in our corpus3.2 Inductive Logic ProgrammingWe utilize Inductive Logic Programming (ILP)(Muggleton, 1991) to explore relational machinelearning.
ILP employs techniques of both MachineLearning and Logic Programming to build first-order logic theories from examples and backgroundknowledge, which are also represented by means offirst-order logic clauses.
It allows the efficient rep-resentation of substantial knowledge about theproblem, and allows this knowledge to be used dur-ing the learning process.
The general idea underly-ing ILP is:Given:-  a set of positive and negative examples E =E+ ?
E-- a predicate p specifying the target relation tobe learned56- knowledge ?
of a certain domain, describedaccording to a language Lk, which specifies whichother predicates qi can be part of the definition of p.The goal is: to induce a hypothesis (or theory) hfor p, with relation to E and ?, which covers mostof the E+, without covering the E-, that is, K ?
hE+ and K ?
h  E-.To implement our approach we chose Aleph(Srinivasan, 2000), an ILP system which provides acomplete relational learning inference engine andvarious customization options.
We used the follow-ing options, which correspond to the Progol mode(Muggleton, 1995): bottom-up search, non-incremental and non-interactive learning, and learn-ing based only on positive examples.
Fundamen-tally, the default inference engine induces a theoryiteratively by means of the following steps:1.
One instance is randomly selected to be gen-eralized.2.
A more specific clause (bottom clause) ex-plaining the selected example is built.
It consists ofthe representation of all knowledge about that ex-ample.3.
A clause that is more generic than the bottomclause is searched, by means of search and gener-alization strategies (best first search, e.g.).4.
The best clause found is added to the theoryand the examples covered by such clause are re-moved from the sample set.
If there are more in-stances in the sample set, return to step 1.3.3 Knowledge sourcesThe choice, acquisition, and representation of syn-tactic, semantic, and pragmatic knowledge sources(KSs) were our main concerns at this stage.
Thegeneral architecture of the system, showing our 7groups of KSs, is illustrated in Figure 2.Several of our KSs have been traditionally em-ployed in monolingual WSD (e.g., Agirre and Ste-venson, 2006), while other are specific for MT.Some of them were extracted from our sample cor-pus (Section 3.1), while others were automaticallyextracted from lexical resources1.
In what follows,we briefly describe, give the generic definition andexamples of each KS, taking sentence (1), for the?to come?, as example.
(1) ?If there is such a thing as reincarnation, Iwould not mind coming back as a squirrel?.KS1: Bag-of-words ?
a list of ?5 words (lem-mas) surrounding the verb for every sentence(sent_id).1Michaelis?
and Password?
English-Portuguese Dictionar-ies, LDOCE (Procter, 1978), and WordNet (Miller, 1990).KS2: Part-of-speech (POS) tags of contentwords in a ?5 word window surrounding the verb.KS3: Subject and object syntactic relations withrespect to the verb under consideration.KS4: Context words represented by 11 colloca-tions with respect to the verb: 1st preposition to theright, 1st and 2nd words to the left and right, 1stnoun, 1st adjective, and 1st verb to the left andright.KS5: Selectional restrictions of verbs and se-mantic features of their arguments, given byLDOCE.
Verb restrictions are expressed by lists ofsemantic features required for their subject and ob-ject, while these arguments are represented withtheir features.The hierarchy for LDOCE feature types definedby Bruce and Guthrie (1992) is used to account forrestrictions established by the verb for features thatare more generic than the features describing thewords in the subject / object roles in the sentence.Ontological relations extracted from WordNet(Miller, 1990) are also used: if the restrictions im-posed by the verb are not part of the description ofits arguments, synonyms or hypernyms of thosearguments that meet the restrictions are considered.KS6: Idioms and phrasal verbs, indicating thatthe verb occurring in a given context could have aspecific translation.bag(sent_id, list_of_words).bag(sent1,[mind, not, will, i, reincarnation, back, as, a,squirrel])has_pos(sent_id, word_position, pos).has_pos(sent1, first_content_word_left, nn).has_pos(sent1, second_content_word_left, vbp)....has_rel(sent_id, subject_word, object_word).has_rel(sent1, i, nil).rest(verb, subj_restrition, obj_ restriction ,translation)rest(come, [], nil, voltar).rest(come, [animal,human], nil, vir).
...feature(noun, sense_id, features).feature(reincarnation, 0_1, [abstract]).feature(squirrel, 0_0, [animal]).has_collocation(sent_id, collocation_type, collocation)has_collocation(sent1, word_right_1, back).has_collocation(sent1, word_left_1, mind).
?relation(word1, sense_id1, word2 ,sense_id2).hyper(reincarnation, 1, avatar, 1).synon(rebirth, 2, reincarnation, -1).57Figure 2.
System architectureKS7: A count of the overlapping words in dic-tionary definitions for the possible translations ofthe verb and the words surrounding it in the sen-tence, relative to the total number of words.The representation of all KSs for each exampleis independent of the other examples.
Therefore, thenumber of features can be different for differentsentences, without resulting in sparseness in data.In order to use the KSs, we created a set of rulesfor each KS.
These rules are not dependent on par-ticular words or instances.
They can be very simple,as in the example shown below for bag-of-words,or more complex, e.g., for selectional restrictions.Therefore, KSs are represented by means of rulesand facts (rules without conditions), which can beintensional, i.e., it can contain variables, making therepresentation more expressive.Besides the KSs, the other main input to the sys-tem is the set of examples.
Since all knowledgeabout them is expressed by the KSs, the representa-tion of examples is very simple, containing only theexample identifier (of the sentence, in our case,such as, ?sent1?
), and the class of that example (inKS4KS7KS6KS1ILP InferenceEngineRules to use Bag-of-words (10)Rules to use Collo-cationsKS2POS of the NarrowContext (10)Rules to use POSKS3Subject-object syn-tactic relationsRules to use syntac-tic relationsRules to use contextwith phrasal verbsand idiomsKS5Verbs selectionalrestrictionsRules to use selec-tional restrictionsSubject-object syn-tactic relationsNouns semanticfeaturesRules to use defini-tions overlappingOverlapping count-ingRule-basedmodelInstancesBag-of-words (10)POStaggerLDOCE WordnetHierarchical rela-tionsFeature typeshierarchyBilingual MRDsDefinitions over-lappingBag-of-words (200)Bag-of-words (10)Mode + type +general definitionsPhrasal verbs andidiomsBag-of-words (10)11 CollocationsParserVerb definitionsand examplesLDOCE + Pass-wordexp(verbal_expression, translation)exp('come about', acontecer).exp('come about', chegar).
?highest_overlap(sent_id, translation, overlapping).highest_overlap(sent1, voltar, 0.222222).highest_overlap(sent2, chegar, 0.0857143).has_bag(Sent,Word) :-bag(Sent,List), member(Word,List).58our case, the translation of the verb in that sen-tence).In Aleph?s default induction mode, the order ofthe training examples plays an important role.
Oneexample is taken at a time, according to its order inthe training set, and a rule can be produced basedon that example.
Since examples covered by a cer-tain rule are removed from the training set, certainexamples will not be used to produce rules.
Induc-tion methods employing different strategies inwhich the order is irrelevant will be exploited infuture work.In order to produce a theory, Aleph also requires?mode definitions?, i.e., the specification of thepredicates p and q (Section 3.2).
For example, thefirst mode definition below states that the predicatep to be learned will consist of a clausesense(sent_id, translation), which can be instanti-ated only once (1).
The other two definitions statethe predicates q, has_colloc(sent_id, colloc_id, col-loc), with at most 11 instantiations, andhas_bag(sent_id, word), with at most 10 instantia-tions.
That is, the predicates in the conditional pieceof the rules in the theory can consist of up to 11collocations and a bag of up to 10 words.
One modedefinition must be created for each KS.Based on the examples and background knowl-edge, the inference engine will produce a set ofsymbolic rules.
Some of the rules induced for theverb ?to come?, e.g., are illustrated in the box be-low.The first rule checks if the first preposition tothe right of the verb is ?out?, assigning the transla-tion ?sair?
if so.
The second rule verifies if the sub-ject-object arguments satisfy the verb restrictions,i.e, if the subject has the features ?animal?
or ?hu-man?, and the object has the feature ?concrete?.Alternatively, it verifies if the sentence contains thephrasal verb ?come at?.
Rule 3 also tests the verbselectional restrictions and the first word to the rightof the verb.4 Experiments and resultsIn order to assess the accuracy of our approach, weran a set of initial experiments with our sample cor-pus.
For each verb, we ran Aleph in the defaultmode, except for the following parameters:The accuracy was calculated by applying therules to classify the new examples in the test setaccording to the order these rules appeared in thetheory, eliminating the examples (correctly orincorrectly) covered by a certain rule from thetest set.
In order to cover 100% of the examples,we relied on the existence of a rule without con-ditions, which generally is induced by Aleph andpoints out to the most frequent translation in thetraining data.
When this rule was not generated byAleph, we add it to the end of theory.
For all theverbs, however, this rule only classified a few ex-amples (form 1 to 6).In Table 2 we show the accuracy of the theorylearned for each verb, as well as accuracyachieved by two propositional machine learningalgorithms on the same data: Decision Trees(C4.5) and Support Vector Machine (SVM), allaccording to a 10-fold cross-validation strategy.Since it is rather impractical to represent certainKSs using attribute-value vectors, in the experi-ments with SVM and C4.5 only low level fea-tures were considered, corresponding to KS1, KS2,KS3, and KS4.
On average, Our approach outper-forms the two other algorithms.
Moreover, its accu-racy is by far better than the accuracy of the mostfrequent sense baseline (Table 1).For all verbs, theories with a small number ofrules were produced (from 19 to 33 rules).
Bylooking at these rules, it becomes clear that all KSsare being explored by the ILP system and thus arepotentially useful for the disambiguation of verbs.5 Conclusion and future workWe presented a hybrid relational approach forWSD designed for MT.
One important character-istic of our approach is that all the KSs weresense(sent_id,translation).sense(sent1,voltar).sense(sent2,ir).
:- modeh(1,sense(sent,translation)).
:- modeb(11,has_colloc(sent,colloc_id,colloc)).
:- modeb(10,has_bag(sent,word)).
?1.
sense(A, sair) :-has_collocation(A, preposition_right, out).2. sense(A, chegar) :-satisfy_restrictions(A, [animal,human],[concrete]);has_expression(A, 'come at').3. sense(A, vir) :-satisfy_restriction(A, [human],[abstract]),has_collocation(A, word_right_1, from).set(evalfn, posonly): learns from positive examples.set(search, heuristic): turns the search strategy heuristic.set(minpos, 2): establishes as 2 the minimum number ofpositive examples covered by each rule in the theory.set(gsamplesize, 1000): defines the number of randomlygenerated negative examples to prune the search space.59Verb AlephAccuracyC4.5AccuracySVMAccuracycome 0.82 0.55 0.6Get 0.51 0.36 0.45Give 0.96 0.88 0.88Go 0.73 0.73 0.72look 0.83 0.66 0.84make 0.74 0.76 0.76Take 0.66 0.35 0.41Average 0.75 0.61 0.67Table 2.
Results of the experiments with Alephautomatically extracted, either from the corpus ormachine-readable lexical resources.
Therefore, thework could be easily extended to other words andlanguages.In future work we intend to carry out experi-ments with different settings: (a) combinations ofcertain KSs; (b) other sample corpora, of differentsizes, genres / domains; and (c) different parametersin Aleph regarding search strategies, evaluationfunctions, etc.
We also intend to compare our ap-proach with other machine learning algorithms us-ing all the KSs employed in Aleph, by pre-processing the KSs in order to extract binary fea-tures that can be represented by means of attribute-value vectors.
After that, we intend to adapt ourapproach to evaluate it with standard WSD datasets, such as the ones used in Senseval2.ReferencesE.
Agirre and M. Stevenson.
2006 (to appear).
Knowl-edge Sources for Word Sense Disambiguation.
InWord Sense Disambiguation: Algorithms, Applica-tions and Trends, Agirre, E. and Edmonds, P.
(Eds.),Kluwer.M.L.
Brown, J.F.
Kros.
2003.
Data Mining and the Im-pact of Missing Data.
Industrial Management andData Systems, 103(8):611-621.R.
Bruce and L. Guthrie.
1992.
Genus disambiguation: Astudy in weighted performance.
In Proceedings of the14th COLING, Nantes, pp.
1187-1191.B.J.
Dorr and M. Katsova.
1998.
Lexical Selection forCross-Language Applications: Combining LCS withWordNet.
In Proceedings of AMTA?1998, Langhorne,pp.
438-447.W.J.
Hutchins and H.L.
Somers.
1992.
An Introductionto Machine Translation.
Academic Press, Great Brit-ain.H.
Lee.
2002.
Classification Approach to Word Selectionin Machine Translation.
In Proceedings ofAMTA?2002, Berlin, pp.
114-123.2http://www.senseval.org/G.A.
Miller, R.T. Beckwith, C.D.
Fellbaum, D. Gross, K.Miller.
1990.
WordNet: An On-line Lexical Database.International Journal of Lexicography, 3(4):235-244.R.J.
Mooney.
1997.
Inductive Logic Programming forNatural Language Processing.
In Proceedings of the6th International ILP Workshop, Berlin, pp.
3-24.S.
Muggleton.
1991.
Inductive Logic Programming.
NewGeneration Computing, 8 (4):295-318.S.
Muggleton.
1995.
Inverse Entailment and Progol.New Generation Computing Journal, 13: 245-286.B.S.
Pedersen.
1997.
Lexical Ambiguity in MachineTranslation: Expressing Regularities in the Polysemyof Danish Motion Verbs.
PhD Thesis, Center forSprogteknologi, Copenhagen.P.
Procter (editor).
1978.
Longman Dictionary of Con-temporary English.
Longman Group, Essex, England.L.
Specia.
2005.
A Hybrid Model for Word Sense Dis-ambiguation in English-Portuguese MT.
In Proceed-ings of the 8th CLUK, Manchester, pp.
71-78.L.
Specia, M.G.V Nunes, M. Stevenson.
2005.
Exploit-ing Parallel Texts to Produce a Multilingual Sense-tagged Corpus for Word Sense Disambiguation.
InProceedings of RANLP-05, Borovets, pp.
525-531.A.
Srinivasan.
2000.
The Aleph Manual.
Technical Re-port.
Computing Laboratory, Oxford University.URL:http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/aleph_toc.html.M.
Stevenson and Y. Wilks.
2001 The Interaction ofKnowledge Sources for Word Sense Disambiguation.Computational Linguistics, 27(3):321-349.D.
Vickrey, L. Biewald, M. Teyssier, and D. Koller.2005.
Word-Sense Disambiguation for MachineTranslation.
In Proceedings of HLT/EMNLP-05, Van-couver.N.
Zinovjeva.
2000.
Learning Sense DisambiguationRules for Machine Translation.
Master?s Thesis, De-partment of Linguistics, Uppsala University.60
