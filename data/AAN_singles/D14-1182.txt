Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1752?1757,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsSometimes Average is Best: The Importance of Averaging for Predictionusing MCMC Inference in Topic ModelingViet-An NguyenComputer ScienceUniversity of MarylandCollege Park, MDvietan@cs.umd.eduJordan Boyd-GraberComputer ScienceUniversity of ColoradoBoulder, COjbg@boydgraber.orgPhilip ResnikLinguistics and UMIACSUniversity of MarylandCollege Park, MDresnik@umd.eduAbstractMarkov chain Monte Carlo (MCMC) approxi-mates the posterior distribution of latent vari-able models by generating many samples andaveraging over them.
In practice, however, itis often more convenient to cut corners, usingonly a single sample or following a suboptimalaveraging strategy.
We systematically study dif-ferent strategies for averaging MCMC samplesand show empirically that averaging properlyleads to significant improvements in prediction.1 IntroductionProbabilistic topic models are powerful methods to un-cover hidden thematic structures in text by projectingeach document into a low dimensional space spannedby a set of topics, each of which is a distribution overwords.
Topic models such as latent Dirichlet alloca-tion (Blei et al., 2003, LDA) and its extensions discoverthese topics from text, which allows for effective ex-ploration, analysis, and summarization of the otherwiseunstructured corpora (Blei, 2012; Blei, 2014).In addition to exploratory data analysis, a typical goalof topic models is prediction.
Given a set of unanno-tated training data, unsupervised topic models try tolearn good topics that can generalize to unseen text.Supervised topic models jointly capture both the textand associated metadata such as a continuous responsevariable (Blei and McAuliffe, 2007; Zhu et al., 2009;Nguyen et al., 2013), single label (Rosen-Zvi et al.,2004; Lacoste-Julien et al., 2008; Wang et al., 2009)or multiple labels (Ramage et al., 2009; Ramage et al.,2011) to predict metadata from text.Probabilistic topic modeling requires estimating theposterior distribution.
Exact computation of the poste-rior is often intractable, which motivates approximateinference techniques (Asuncion et al., 2009).
One popu-lar approach is Markov chain Monte Carlo (MCMC), aclass of inference algorithms to approximate the targetposterior distribution.
To make prediction, MCMC al-gorithms generate samples on training data to estimatecorpus-level latent variables, and use them to generatesamples to estimate document-level latent variables fortest data.
The underlying theory requires averaging onboth training and test samples, but in practice it is oftenconvenient to cut corners: either skip averaging entirelyby using just the values of the last sample or use a singletraining sample and average over test samples.We systematically study non-averaging and averagingstrategies when performing predictions using MCMC intopic modeling (Section 2).
Using popular unsupervised(LDA in Section 3) and supervised (SLDA in Section 4)topic models via thorough experimentation, we showempirically that cutting corners on averaging leads toconsistently poorer prediction.2 Learning and Predicting with MCMCWhile reviewing all of MCMC is beyond the scope ofthis paper, we need to briefly review key concepts.1Toestimate a target density p(x) in a high-dimensionalspace X , MCMC generates samples {xt}Tt=1while ex-ploring X using the Markov assumption.
Under thisassumption, sample xt+1depends on sample xtonly,forming a Markov chain, which allows the sampler tospend more time in the most important regions of thedensity.
Two concepts control sample collection:Burn-in B: Depending on the initial value of theMarkov chain, MCMC algorithms take time to reachthe target distribution.
Thus, in practice, samples beforea burn-in period B are often discarded.Sample-lag L: Averaging over samples to estimatethe target distribution requires i.i.d.
samples.
However,future samples depend on the current samples (i.e., theMarkov assumption).
To avoid autocorrelation, we dis-card all but every L samples.2.1 MCMC in Topic ModelingAs generative probabilistic models, topic models definea joint distribution over latent variables and observableevidence.
In our setting, the latent variables consist ofcorpus-level global variables g and document-level lo-cal variables l; while the evidence consists of words wand additional metadata y?the latter omitted in unsu-pervised models.During training, MCMC estimates the posteriorp(g, lTR|wTR,yTR) by generating a training Markovchain of TTRsamples.2Each training sample i pro-vides a set of fully realized global latent variables?g(i),which can generate test data.
During test time, given a1For more details please refer to Neal (1993), Andrieu etal.
(2003), Resnik and Hardisty (2010).2We omit hyperparameters for clarity.
We split data intotraining (TR) and testing (TE) folds, and denote the trainingiteration i and the testing iteration j within the correspondingMarkov chains.1752Training burn-in Btr Training lag LTR Training lag LtrTraining period TtrTestburn-inBteTestperiodTteTestlagLte1 23 424243 43 4444 41234Samples used in Single Final (SF)Samples used in Single Average (SA)Samples used in Multiple Final (MF)Samples used in Multiple Average (MA)Training chainSingle test chainssample i intraining chain(learned model)test chain isample j intest chain i(prediction S(i,j))Discarded samples during trainingDiscarded samples during testSelected samples during trainingSelected samples during testFigure 1: Illustration of training and test chains in MCMC, showing samples used in four prediction strategies studiedin this paper: Single Final (SF), Single Average (SA), Multiple Final (MF), and Multiple Average (MA).learned model from training sample i, we generate a testMarkov chain of TTEsamples to estimate the local latentvariables p(lTE|wTE,?g(i)) of test data.
Each samplej of test chain i provides a fully estimated local latentvariables?lTE(i, j) to make a prediction.Figure 1 shows an overview.
To reduce the ef-fects of unconverged and autocorrelated samples, dur-ing training we use a burn-in period of BTRand asample-lag of LTRiterations.
We use TTR= {i | i ?
(BTR, TTR] ?
(i ?
BTR) mod LTR= 0} to denote theset of indices of the selected models.
Similarly, BTEand LTEare the test burn-in and sample-lag.
Theset of indices of selected samples in test chains isTTE= {j | j ?
(BTE, TTE] ?
(j ?BTE) mod LTE= 0}.2.2 Averaging StrategiesWe use S(i, j) to denote the prediction obtained fromsample j of the test chain i.
We now discuss differentstrategies to obtain the final prediction:?
Single Final (SF) uses the last sample of last testchain to obtain the predicted value,SSF= S(TTR, TTE).
(1)?
Single Average (SA) averages over multiple sam-ples in the last test chainSSA=1|TTE|?j?TTES(TTR, j).
(2)This is a common averaging strategy in which weobtain a point estimate of the global latent variablesat the end of the training chain.
Then, a single testchain is generated on the test data and multiple sam-ples of this test chain are averaged to obtain the finalprediction (Chang, 2012; Singh et al., 2012; Jiang etal., 2012; Zhu et al., 2014).?
Multiple Final (MF) averages over the last sam-ples of multiple test chains from multiple modelsSMF=1|TTR|?i?TTRS(i, TTE).
(3)?
Multiple Average (MA) averages over all samplesof multiple test chains for distinct models,SMA=1|TTR|1|TTE|?i?TTR?j?TTES(i, j), (4)3 Unsupervised Topic ModelsWe evaluate the predictive performance of the unsu-pervised topic model LDA using different averagingstrategies in Section 2.LDA: Proposed by Blei et al.
in 2003, LDA posits thateach document d is a multinomial distribution ?doverK topics, each of which is a multinomial distribution?kover the vocabulary.
LDA?s generative process is:1.
For each topic k ?
[1,K](a) Draw word distribution ?k?
Dir(?)2.
For each document d ?
[1, D](a) Draw topic distribution ?d?
Dir(?
)(b) For each word n ?
[1, Nd]i.
Draw topic zd,n?
Mult(?d)ii.
Draw word wd,n?
Mult(?zd,n)In LDA, the global latent variables are topics {?k}Kk=1and the local latent variables for each document d aretopic proportions ?d.Train: During training, we use collapsed Gibbs sam-pling to assign each token in the training data with atopic (Steyvers and Griffiths, 2006).
The probability of1753assigning token n of training document d to topic k isp(zTRd,n= k | zTR?d,n,wTR?d,n, wTRd,n= v) ?N?d,nTR,d,k+ ?N?d,nTR,d,?+K?
?N?d,nTR,k,v+ ?N?d,nTR,k,?+ V ?, (5)where NTR,d,kis the number of tokens in the trainingdocument d assigned to topic k, and NTR,k,vis the num-ber of times word type v assigned to topic k. Marginalcounts are denoted by ?, and?d,ndenotes the countexcluding the assignment of token n in document d.At each training iteration i, we estimate the distribu-tion over words?
?k(i) of topic k as?
?k,v(i) =NTR,k,v(i) + ?NTR,k,?
(i) + V ?
(6)where the counts NTR,k,v(i) and NTR,k,?
(i) are taken attraining iteration i.Test: Because we lack explicit topic annotations forthese data (c.f.
Nguyen et al.
(2012)), we use perplexity?a widely-used metric to measure the predictive powerof topic models on held-old documents.
To computeperplexity, we follow the estimating ?
method (Wal-lach et al., 2009, Section 5.1) and evenly split each testdocument d into wTE1dand wTE2d.
We first run Gibbssampling on wTE1dto estimate the topic proportion?
?TEdof test document d. The probability of assigning topic kto token n inwTE1dis p(zTE1d,n= k | zTE1?d,n,wTE1,??
(i)) ?N?d,nTE1,d,k+ ?N?d,nTE1,d,?+K???
?k,wTE1d,n(i)(7)whereNTE1,d,kis the number of tokens inwTE1dassignedto topic k. At each iteration j in test chain i, we canestimate the topic proportion vector?
?TEd(i, j) for testdocument d as?
?TEd,k(i, j) =NTE1,d,k(i, j) + ?NTE1,d,?
(i, j) +K?
(8)where both the counts NTE1,d,k(i, j) and NTE1,d,?
(i, j)are taken using sample j of test chain i.Prediction: Given?
?TEd(i, j) and??
(i) at sample jof test chain i, we compute the predicted likeli-hood for each unseen token wTE2d,nas S(i, j) ?p(wTE2d,n|?
?TEd(i, j),??
(i)) =?Kk=1?
?TEd,k(i, j) ??
?k,wTE2d,n(i).Using different strategies described in Section 2,we obtain the final predicted likelihood for each un-seen token p(wTE2d,n|??TEd,??)
and compute the perplex-ity as exp(?(?d?nlog(p(wTE2d,n|??TEd,??
)))/NTE2)where NTE2is the number of tokens in wTE2.Setup: We use three Internet review datasets in ourexperiment.
For all datasets, we preprocess by tokeniz-ing, removing stopwords, stemming, adding bigrams tolll l l l l l l llll l l l l l l llll l l l l l l lRestaurant ReviewsMovie ReviewsHotel Reviews11601200124019502000205021002150750775800600 700 800 900 1000600 700 800 900 1000600 700 800 900 1000Number of training iterationsPerplexitylMultiple?Average Multiple?Final Single?Average Single?FinalFigure 2: Perplexity of LDA using different averagingstrategies with different number of training iterationsTTR.
Perplexity generally decreases with additionaltraining iterations, but the drop is more pronouncedwith multiple test chains.the vocabulary, and we filter using TF-IDF to obtain avocabulary of 10,000 words.3The three datasets are:?
HOTEL: 240,060 reviews of hotels from TripAdvi-sor (Wang et al., 2010).?
RESTAURANT: 25,459 reviews of restaurants fromYelp (Jo and Oh, 2011).?
MOVIE: 5,006 reviews of movies from RottenTomatoes (Pang and Lee, 2005)We report cross-validated average performance overfive folds, and use K = 50 topics for all datasets.
Toupdate the hyperparameters, we use slice sampling (Wal-lach, 2008, p. 62).4Results: Figure 2 shows the perplexity of the fouraveraging methods, computed with different numberof training iterations TTR.
SA outperforms SF, showingthe benefits of averaging over multiple test samplesfrom a single test chain.
However, both multiple chainmethods (MF and MA) significantly outperform thesetwo methods.This result is consistent with Asuncion et al.
(2009),who run multiple training chains but a single test chainfor each training chain and average over them.
Thisis more costly since training chains are usually signif-icantly longer than test chains.
In addition, multipletraining chains are sensitive to their initialization.3To find bigrams, we begin with bigram candidates thatoccur at least 10 times in the corpus and use a ?2test to filterout those having a ?2value less than 5.
We then treat selectedbigrams as single word types and add them to the vocabulary.4MCMC setup: TTR= 1, 000, BTR= 500, LTR= 50,TTE= 100, BTE= 50 and LTE= 5.1754MSEpR.squared0.600.650.700.750.250.300.350.401000 2000 3000 4000 50001000 2000 3000 4000 5000Number of iterations(a) Restaurant reviews MSEpR.squared90001000011000120001300030000310003200033000340001000 2000 3000 4000 50001000 2000 3000 4000 5000Number of iterations(b) Movie reviews MSEpR.squared0.4000.4250.4500.4750.5000.5000.5250.5500.5750.600600 700 800 900 1000600 700 800 900 1000Number of iterations(c) Hotel reviewsMultiple AverageMultiple FinalSingle AverageSingle FinalFigure 3: Performance of SLDA using different averaging strategies computed at each training iteration.4 Supervised Topic ModelsWe evaluate the performance of different predictionmethods using supervised latent Dirichlet allocation(SLDA) (Blei and McAuliffe, 2007) for sentiment anal-ysis: predicting review ratings given review text.
Eachreview text is the document wdand the metadata ydisthe associated rating.SLDA: Going beyond LDA, SLDA captures the rela-tionship between latent topics and metadata by mod-eling each document?s continuous response variableusing a normal linear model, whose covariates arethe document?s empirical distribution of topics: yd?N (?T?zd, ?)
where ?
is the regression parameter vec-tor and?zdis the empirical distribution over topics ofdocument d. The generative process of SLDA is:1.
For each topic k ?
[1,K](a) Draw word distribution ?k?
Dir(?
)(b) Draw parameter ?k?
N (?, ?)2.
For each document d ?
[1, D](a) Draw topic distribution ?d?
Dir(?
)(b) For each word n ?
[1, Nd]i.
Draw topic zd,n?
Mult(?d)ii.
Draw word wd,n?
Mult(?zd,n)(c) Draw response yd?
N (?T?zd, ?)
wherez?d,k=1Nd?Ndn=1I [zd,n= k]where I [x] = 1 if x is true, and 0 otherwise.In SLDA, in addition to the K multinomials {?k}Kk=1,the global latent variables also contain the regressionparameter ?kfor each topic k. The local latent variablesof SLDA resembles LDA?s: the topic proportion vector?dfor each document d.Train: For posterior inference during training, follow-ing Boyd-Graber and Resnik (2010), we use stochasticEM, which alternates between (1) a Gibbs samplingstep to assign a topic to each token, and (2) optimizingthe regression parameters.
The probability of assigningtopic k to token n in the training document d isp(zTRd,n= k | zTR?d,n,wTR?d,n, wTRd,n= v) ?N (yd;?d,n, ?)
?N?d,nTR,d,k+ ?N?d,nTR,d,?+K?
?N?d,nTR,k,v+ ?N?d,nTR,k,?+ V ?
(9)where ?d,n= (?Kk?=1?k?N?d,nTR,d,k?+ ?k)/NTR,dis themean of the Gaussian generating ydif zTRd,n= k. Here,NTR,d,kis the number of times topic k is assigned totokens in the training document d;NTR,k,vis the numberof times word type v is assigned to topic k; ?
representsmarginal counts and?d,nindicates counts excluding theassignment of token n in document d.We optimize the regression parameters ?
using L-BFGS (Liu and Nocedal, 1989) via the likelihoodL(?)
= ?12?D?d=1(yTRd?
?T ?zTRd)2?12?K?k=1(?k??
)2(10)At each iteration i in the training chain, the estimatedglobal latent variables include the a multinomial?
?k(i)and a regression parameter ?
?k(i) for each topic k.Test: Like LDA, at test time we sample the topic as-signments for all tokens in the test datap(zTEd,n= k | zTE?d,n,wTE) ?N?d,nTE,d,k+ ?N?d,nTE,d,?+K???
?k,wTEd,n(11)Prediction: The predicted value S(i, j) in this case isthe estimated value of the metadata review ratingS(i, j) ?
y?TEd(i, j) =??
(i)Tz?TEd(i, j), (12)where the empirical topic distribution of test document dis z?TEd,k(i, j) ?1NTE,d?NTE,dn=1I[zTEd,n(i, j) = k].1755MSEpR?squared0.600.650.700.300.350.4050 100 150 20050 100 150 200Number of Topics(a) Restaurant reviews MSEpR?squared0.600.700.800.900.000.100.200.300.4040 60 8040 60 80Number of Topics(a) Restaurant reviews MSEpR?squared0.400.420.440.460.480.520.540.560.580.6050 100 150 20050 100 150 200Number of Topics(a) Restaurant reviewsMLRSLDA?MASLDA?MFSLDA?SASLDA?SFSVRFigure 4: Performance of SLDA using different averaging strategies computed at the final training iteration TTR,compared with two baselines MLR and SVR.
Methods using multiple test chains (MF and MA) perform as well as orbetter than the two baselines, whereas methods using a single test chain (SF and SA) perform significantly worse.Experimental setup: We use the same data as in Sec-tion 3.
For all datasets, the metadata are the reviewrating, ranging from 1 to 5 stars, which is standard-ized using z-normalization.
We use two evaluationmetrics: mean squared error (MSE) and predictive R-squared (Blei and McAuliffe, 2007).For comparison, we consider two baselines: (1) multi-ple linear regression (MLR), which models the metadataas a linear function of the features, and (2) support vec-tor regression (Joachims, 1999, SVR).
Both baselinesuse the normalized frequencies of unigrams and bigramsas features.
As in the unsupervised case, we report av-erage performance over five cross-validated folds.
Forall models, we use a development set to tune their pa-rameter(s) and use the set of parameters that gives bestresults on the development data at test.5Results: Figure 3 shows SLDA prediction results withdifferent averaging strategies, computed at differenttraining iterations.6Consistent with the unsupervisedresults in Section 3, SA outperforms SF, but both areoutperformed significantly by the two methods usingmultiple test chains (MF and MA).We also compare the performance of the four pre-diction methods obtained at the final iteration TTRofthe training chain with the two baselines.
The results inFigure 4 show that the two baselines (MLR and SVR) out-perform significantly the SLDA using only a single test5For MLR we use a Gaussian prior N (0, 1/?)
with ?
=a ?
10bwhere a ?
[1, 9] and b ?
[1, 4]; for SVR, we useSVMlight(Joachims, 1999) and vary C ?
[1, 50], whichtrades off between training error and margin; for SLDA, we fix?
= 10 and vary ?
?
{0.1, 0.5, 1.0, 1.5, 2.0}, which tradesoff between the likelihood of words and response variable.6MCMC setup: TTR= 5, 000 for RESTAURANT andMOVIE and 1, 000 for HOTEL; for all datasets BTR= 500,LTR= 50, TTE= 100, BTE= 20 and LTE= 5.chains (SF and SA).
Methods using multiple test chains(MF and MA), on the other hand, match the baseline7(HOTEL) or do better (RESTAURANT and MOVIE).5 Discussion and ConclusionMCMC relies on averaging multiple samples to approxi-mate target densities.
When used for prediction, MCMCneeds to generate and average over both training sam-ples to learn from training data and test samples to makeprediction.
We have shown that simple averaging?notmore aggressive, ad hoc approximations like taking thefinal sample (either training or test)?is not just a ques-tion of theoretical aesthetics, but an important factor inobtaining good prediction performance.Compared with SVR and MLR baselines, SLDA usingmultiple test chains (MF and MA) performs as well asor better, while SLDA using a single test chain (SF andSA) falters.
This simple experimental setup choice candetermine whether a model improves over reasonablebaselines.
In addition, better prediction with shortertraining is possible with multiple test chains.
Thus, weconclude that averaging using multiple chains producesabove-average results.AcknowledgmentsWe thank Jonathan Chang, Ke Zhai and Mohit Iyyer forhelpful discussions, and thank the anonymous reviewersfor insightful comments.
This research was supportedin part by NSF under grant #1211153 (Resnik) and#1018625 (Boyd-Graber and Resnik).
Any opinions,findings, conclusions, or recommendations expressedhere are those of the authors and do not necessarilyreflect the view of the sponsor.7This gap is because SLDA has not converged after 1,000training iterations (Figure 3).1756ReferencesChristophe Andrieu, Nando de Freitas, Arnaud Doucet, andMichael I. Jordan.
2003.
An introduction to MCMC formachine learning.
Machine Learning, 50(1-2):5?43.Arthur Asuncion, Max Welling, Padhraic Smyth, andYee Whye Teh.
2009.
On smoothing and inference fortopic models.
In UAI.David M. Blei and Jon D. McAuliffe.
2007.
Supervised topicmodels.
In NIPS.David M. Blei, Andrew Ng, and Michael Jordan.
2003.
LatentDirichlet allocation.
JMLR, 3.David M. Blei.
2012.
Probabilistic topic models.
Commun.ACM, 55(4):77?84, April.David M. Blei.
2014.
Build, compute, critique, repeat: Dataanalysis with latent variable models.
Annual Review ofStatistics and Its Application, 1(1):203?232.Jordan Boyd-Graber and Philip Resnik.
2010.
Holistic sen-timent analysis across languages: Multilingual supervisedlatent Dirichlet allocation.
In EMNLP.Jonathan Chang.
2012. lda: Collapsed Gibbs sampling meth-ods for topic models.
http://cran.r-project.org/web/packages/lda/index.html.
[Online;accessed 02-June-2014].Qixia Jiang, Jun Zhu, Maosong Sun, and Eric P. Xing.
2012.Monte Carlo methods for maximum margin supervisedtopic models.
In NIPS.Yohan Jo and Alice H. Oh.
2011.
Aspect and sentimentunification model for online review analysis.
In WSDM.Thorsten Joachims.
1999.
Making large-scale SVM learningpractical.
In Advances in Kernel Methods - Support VectorLearning, chapter 11.
Cambridge, MA.Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.
2008.DiscLDA: Discriminative learning for dimensionality re-duction and classification.
In NIPS.D.
Liu and J. Nocedal.
1989.
On the limited memory BFGSmethod for large scale optimization.
Math.
Prog.Radford M. Neal.
1993.
Probabilistic inference using Markovchain Monte Carlo methods.
Technical Report CRG-TR-93-1, University of Toronto.Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.2012.
SITS: A hierarchical nonparametric model usingspeaker identity for topic segmentation in multiparty con-versations.
In ACL.Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.2013.
Lexical and hierarchical topic regression.
In NeuralInformation Processing Systems.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploiting classrelationships for sentiment categorization with respect torating scales.
In ACL.Daniel Ramage, David Hall, Ramesh Nallapati, and Christo-pher Manning.
2009.
Labeled LDA: A supervised topicmodel for credit attribution in multi-labeled corpora.
InEMNLP.Daniel Ramage, Christopher D. Manning, and Susan Dumais.2011.
Partially labeled topic models for interpretable textmining.
In KDD, pages 457?465.Philip Resnik and Eric Hardisty.
2010.
Gibbssampling for the uninitiated.
Technical ReportUMIACS-TR-2010-04, University of Maryland.http://drum.lib.umd.edu//handle/1903/10058.Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers, andPadhraic Smyth.
2004.
The author-topic model for authorsand documents.
In UAI.Sameer Singh, Michael Wick, and Andrew McCallum.
2012.Monte Carlo MCMC: Efficient inference by approximatesampling.
In EMNLP, pages 1104?1113.Mark Steyvers and Tom Griffiths.
2006.
Probabilistic topicmodels.
In T. Landauer, D. Mcnamara, S. Dennis, andW.
Kintsch, editors, Latent Semantic Analysis: A Road toMeaning.
Laurence Erlbaum.Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, andDavid Mimno.
2009.
Evaluation methods for topic models.In Leon Bottou and Michael Littman, editors, ICML.Hanna M Wallach.
2008.
Structured Topic Models for Lan-guage.
Ph.D. thesis, University of Cambridge.Chong Wang, David Blei, and Li Fei-Fei.
2009.
Simultaneousimage classification and annotation.
In CVPR.Hongning Wang, Yue Lu, and Chengxiang Zhai.
2010.
La-tent aspect rating analysis on review text data: A ratingregression approach.
In SIGKDD, pages 783?792.Jun Zhu, Amr Ahmed, and Eric P. Xing.
2009.
MedLDA:maximum margin supervised topic models for regressionand classification.
In ICML.Jun Zhu, Ning Chen, Hugh Perkins, and Bo Zhang.
2014.Gibbs max-margin topic models with data augmentation.Journal of Machine Learning Research, 15:1073?1110.1757
