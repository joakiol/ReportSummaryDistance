Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 45?52, New York City, June 2006. c?2006 Association for Computational LinguisticsInvestigating Lexical Substitution Scoring for Subtitle GenerationOren Glickman and Ido DaganComputer Science DepartmentBar Ilan UniversityRamat Gan, Israel{glikmao,dagan}@cs.biu.ac.ilMikaela Keller and Samy BengioIDIAP Research InstituteMartigny,Switzerland{mkeller,bengio}@idiap.chWalter DaelemansCNTSAntwerp, Belgiumwalter.daelemans@ua.ac.beAbstractThis paper investigates an isolated settingof the lexical substitution task of replac-ing words with their synonyms.
In par-ticular, we examine this problem in thesetting of subtitle generation and evaluatestate of the art scoring methods that pre-dict the validity of a given substitution.The paper evaluates two context indepen-dent models and two contextual models.The major findings suggest that distribu-tional similarity provides a useful comple-mentary estimate for the likelihood thattwo Wordnet synonyms are indeed substi-tutable, while proper modeling of contex-tual constraints is still a challenging taskfor future research.1 IntroductionLexical substitution - the task of replacing a wordwith another one that conveys the same meaning -is a prominent task in many Natural Language Pro-cessing (NLP) applications.
For example, in queryexpansion for information retrieval a query is aug-mented with synonyms of the original query words,aiming to retrieve documents that contain these syn-onyms (Voorhees, 1994).
Similarly, lexical substi-tutions are applied in question answering to identifyanswer passages that express the sought answer indifferent terms than the original question.
In natu-ral language generation it is common to seek lex-ical alternatives for the same meaning in order toreduce lexical repetitions.
In general, lexical sub-stitution aims to preserve a desired meaning whilecoping with the lexical variability of expressing thatmeaning.
Lexical substitution can thus be viewedwithin the general framework of recognizing entail-ment between text segments (Dagan et al, 2005), asmodeling entailment relations at the lexical level.In this paper we examine the lexical substitu-tion problem within a specific setting of text com-pression for subtitle generation (Daelemans et al,2004).
Subtitle generation is the task of generat-ing target language TV subtitles for video recordingsof a source language speech.
The subtitles shouldbe of restricted length, which is often shorter thanthe full translation of the original speech, yet theyshould maintain as much as possible the meaningof the original content.
In a typical (automated)subtitling process the original speech is first trans-lated fully into the target language and then the tar-get translation is compressed to optimize the lengthrequirements.
One of the techniques employed inthe text compression phase is to replace a target lan-guage word in the original translation with a shortersynonym of it, thus reducing the character length ofthe subtitle.
This is a typical lexical substitutiontask, which resembles similar operations in othertext compression and generation tasks (e.g.
(Knightand Marcu, 2002)).This paper investigates the task of assigning like-lihood scores for the correctness of such lexical sub-stitutions, in which words in the original translationare replaced with shorter synonyms.
In our experi-ments we use WordNet as a source of candidate syn-onyms for substitution.
The goal is to score the like-lihood that the substitution is admissible, i.e.
yield-ing a valid sentence that preserves the original mean-ing.
The focus of this paper is thus to utilize thesubtitling setting in order to investigate lexical sub-45stitution models in isolation, unlike most previousliterature in which this sub-task has been embeddedin larger systems and was not evaluated directly.We examine four statistical scoring models, oftwo types.
Context independent models score thegeneral likelihood that the original word is ?replace-able?
with the candidate synonym, in an arbitrarycontext.
That is, trying to filter relatively bizarresynonyms, often of rare senses, which are abundantin WordNet but are unlikely to yield valid substitu-tions.
Contextual models score the ?fitness?
of thereplacing word within the context of the sentence, inorder to filter out synonyms of senses of the originalword that are not the right sense in the given context.We set up an experiment using actual subti-tling data and human judgements and evaluate thedifferent scoring methods.
Our findings suggestthe dominance, in this setting, of generic context-independent scoring.
In particular, considering dis-tributional similarity amongst WordNet synonymsseems effective for identifying candidate substitu-tions that are indeed likely to be applicable in actualtexts.
Thus, while distributional similarity alone isknown to be too noisy as a sole basis for meaning-preserving substitutions, its combination withWord-Net alows reducing the noise caused by the manyWordNet synonyms that are unlikely to correspondto valid substitutions.2 Background and Setting2.1 SubtitlingAutomatic generation of subtitles is a summariza-tion task at the level of individual sentences or occa-sionally of a few contiguous sentences.
Limitationson reading speed of viewers and on the size of thescreen that can be filled with text without the imagebecoming too cluttered, are the constraints that dy-namically determine the amount of compression incharacters that should be achieved in transformingthe transcript into subtitles.
Subtitling is not a trivialtask, and is expensive and time-consuming when ex-perts have to carry it out manually.
As for other NLPtasks, both statistical (machine learning) and linguis-tic knowledge-based techniques have been consid-ered for this problem.
Examples of the former are(Knight and Marcu, 2002; Hori et al, 2002), and ofthe latter are (Grefenstette, 1998; Jing and McKe-own, 1999).
A comparison of both approaches inthe context of a Dutch subtitling system is providedin (Daelemans et al, 2004).
The required sentencesimplification is achieved either by deleting mate-rial, or by paraphrasing parts of the sentence intoshorter expressions with the same meaning.
As aspecial case of the latter, lexical substitution is oftenused to achieve a compression target by substitutinga word by a shorter synonym.
It is on this subtaskthat we focus in this paper.
Table 1 provides a fewexamples.
E.g.
by substituting ?happen?
by ?occur?
(example 3), one character is saved without affectingthe sentence meaning .2.2 Experimental SettingThe data used in our experiments was collected inthe context of the MUSA (Multilingual Subtitling ofMultimedia Content) project (Piperidis et al, 2004)1and was kindly provided for the current study.
Thedata was provided by the BBC in the form of Hori-zon documentary transcripts with the correspondingaudio and video.
The data for two documentarieswas used to create a dataset consisting of sentencesfrom the transcripts and the corresponding substitu-tion examples in which selected words are substi-tuted by a shorter Wordnet synonym.
More con-cretely, a substitution example thus consists of anoriginal sentence s = w1 .
.
.
wi .
.
.
wn, a specificsource word wi in the sentence and a target (shorter)WordNet synonym w?
to substitute the source.
SeeTable 1 for examples.
The dataset consists of 918substitution examples originating from 231 differentsentences.An annotation environment was developed to al-low efficient annotation of the substitution exampleswith the classes true (admissible substitution, in thegiven context) or false (inadmissible substitution).About 40% of the examples were judged as true.Part of the data was annotated by an additional an-notator to compute annotator agreement.
The Kappascore turned out to be 0.65, corresponding to ?Sub-stantial Agreement?
(Landis and Koch, 1997).
Sincesome of the methods we are comparing need tuningwe held out a random subset of 31 original sentences(with 121 corresponding examples) for developmentand kept for testing the resulting 797 substitution ex-1http://sinfos.ilsp.gr/musa/46id sentence source target judgment1 The answer may be found in the behaviour of animals.
answer reply false2 .
.
.
and the answer to that was - Yes answer reply true3We then wanted to know what would happen ifwe delay the movement of the subject?s left handhappen occur true4 subject topic false5 subject theme false6 people weren?t laughing they were going stone sober.
stone rock false7 if we can identify a place where the seizures are coming from then we can go inand remove just that small area.identify place false8 my approach has been the first to look at the actual structure of the laugh sound.
approach attack false9 He quickly ran into an unexpected problem.
problem job false10 today American children consume 5 times more Ritalin than the rest of the worldcombinedconsume devour falseTable 1: Substitution examples from the dataset alng with their annotationsamples from the remaining 200 sentences.3 Compared Scoring ModelsWe compare methods for scoring lexical substitu-tions.
These methods assign a score which is ex-pected to correspond to the likelihood that the syn-onym substitution results in a valid subtitle whichpreserves the main meaning of the original sentence.We examine four statistical scoring models, oftwo types.
The context independent models scorethe general likelihood that the source word can bereplaced with the target synonym regardless of thecontext in which the word appears.
Contextual mod-els, on the other hand, score the fitness of the targetword within the given context.3.1 Context Independent ModelsEven though synonyms are substitutable in theory,in practice there are many rare synonyms for whichthe likelihood of substitution is very low and will besubstitutable only in obscure contexts.
For exam-ple, although there are contexts in which the wordjob is a synonym of the word problem2, this is nottypically the case and overall job is not a good tar-get substitution for the source problem (see example9 in Table 1).
For this reason synonym thesaurusessuch as WordNet tend to be rather noisy for practi-cal purposes, raising the need to score such synonymsubstitutions and accordingly prioritize substitutionsthat are more likely to be valid in an arbitrary con-text.2WordNet lists job as a possible member of the synset for astate of difficulty that needs to be resolved, as might be used insentences like ?it is always a job to contact him?As representative approaches for addressing thisproblem, we chose two methods that rely on statisti-cal information of two types: supervised sense dis-tributions from SemCor and unsupervised distribu-tional similarity.3.1.1 WordNet based Sense Frequencies(semcor)The obvious reason that a target synonym cannotsubstitute a source in some context is if the sourceappears in a different sense than the one in whichit is synonymous with the target.
This means that apriori, synonyms of frequent senses of a source wordare more likely to provide correct substitutions thansynonyms of the word?s infrequent senses.To estimate such likelihood, our first measure isbased on sense frequencies from SemCor (Miller etal., 1993), a corpus annotated with Wordnet senses.For a given source word u and target synonym v thescore is calculated as the percentage of occurrencesof u in SemCor for which the annotated synset con-tains v (i.e.
u?s occurrences in which its sense issynonymous with v).
This corresponds to the priorprobability estimate that an occurrence of u (in anarbitrary context) is actually a synonym of v. There-fore it is suitable as a prior score for lexical substi-tution.33.1.2 Distributional Similarity (sim)The SemCor based method relies on a supervisedapproach and requires a sense annotated corpus.
Our3Note that WordNet semantic distance measures such asthose compared in (Budanitsky and Hirst, 2001) are not appli-cable here since they measure similarity between synsets ratherthan between synonymous words within a single synset.47second method uses an unsupervised distributionalsimilarity measure to score synonym substitutions.Such measures are based on the general idea ofHarris?
Distributional Hypothesis, suggesting thatwords that occur within similar contexts are seman-tically similar (Harris, 1968).As a representative of this approach we use Lin?sdependency-based distributional similarity database.Lin?s database was created using the particular dis-tributional similarity measure in (Lin, 1998), appliedto a large corpus of news data (64 million words) 4.Two words obtain a high similarity score if they oc-cur often in the same contexts, as captured by syn-tactic dependency relations.
For example, two verbswill be considered similar if they have large commonsets of modifying subjects, objects, adverbs etc.Distributional similarity does not capture directlymeaning equivalence and entailment but rather alooser notion of meaning similarity (Geffet and Da-gan, 2005).
It is typical that non substitutable wordssuch as antonyms or co-hyponyms obtain high sim-ilarity scores.
However, in our setting we applythe similarity score only for WordNet synonyms inwhich it is known a priori that they are substitutableis some contexts.
Distributional similarity may thuscapture the statistical degree to which the two wordsare substitutable in practice.
In fact, it has beenshown that prominence in similarity score corre-sponds to sense frequency, which was suggested asthe basis for an unsupervised method for identifyingthe most frequent sense of a word (McCarthy et al,2004).3.2 Contextual ModelsContextual models score lexical substitutions basedon the context of the sentence.
Such modelstry to estimate the likelihood that the target wordcould potentially occur in the given context of thesource word and thus may replace it.
More con-cretely, for a given substitution example consist-ing of an original sentence s = w1 .
.
.
wi .
.
.
wn,and a designated source word wi, the contextualmodels we consider assign a score to the substi-tution based solely on the target synonym v andthe context of the source word in the original sen-4available at http://www.cs.ualberta.ca/?lindek/downloads.htmtence, {w1, .
.
.
, wi?1, wi+1, .
.
.
, wn}, which is rep-resented in a bag-of-words format.Apparently, this setting was not investigated muchin the context of lexical substitution in the NLP lit-erature.
We chose to evaluate two recently proposedmodels that address exactly the task at hand: the firstmodel was proposed in the context of lexical model-ing of textual entailment, using a generative Na?
?veBayes approach; the second model was proposedin the context of machine learning for informationretrieval, using a discriminative neural network ap-proach.
The two models were trained on the (un-annotated) sentences of the BNC 100 million wordcorpus (Burnard, 1995) in bag-of-words format.
Thecorpus was broken into sentences, tokenized, lem-matized and stop words and tokens appearing onlyonce were removed.
While training of these modelsis done in an unsupervised manner, using unlabeleddata, some parameter tuning was performed usingthe small development set described in Section 2.3.2.1 Bayesian Model (bayes)The first contextual model we examine is the oneproposed in (Glickman et al, 2005) to model tex-tual entailment at the lexical level.
For a given tar-get word this unsupervised model takes a binary textcategorization approach.
Each vocabulary word isconsidered a class, and contexts are classified as towhether the given target word is likely to occur inthem.
Taking a probabilistic Na?
?ve-Bayes approachthe model estimates the conditional probability ofthe target word given the context based on corpus co-occurrence statistics.
We adapted and implementedthis algorithm and trained the model on the sen-tences of the BNC corpus.For a bag-of-words context C ={w1, .
.
.
, wi?1, wi+1, .
.
.
, wn} and target wordv the Na?
?ve Bayes probability estimation for theconditional probability of a word v may occur in agiven a context C is as follows:P(v|C) =P(C|v) P(v)P(C|v) P(v)+P(C|?v) P(?v)?P(v)?w?C P(w|v)P(v)?w?C P(w|v)+P(?v)?w?C P(w|?v)(1)where P(w|v) is the probability that a word w ap-pears in the context of a sentence containing v andcorrespondingly P(w|?v) is the probability that w48appears in a sentence not containing v. The prob-ability estimates were obtained from the processedBNC corpus as follows:P(w|v) =|w appears in sentences containing v||words in sentences containing v|P(w|?v) =|w occurs in sentences not containing v||words in sentences not containing v|To avoid 0 probabilities these estimates weresmoothed by adding a small constant to all countsand normalizing accordingly.
The constant valuewas tuned using the development set to maximizeaverage precision (see Section 4.1).
The estimatedprobability, P(v|C), was used as the confidencescore for each substitution example.3.2.2 Neural Network Model (nntr)As a second contextual model we evaluated theNeural Network for Text Representation (NNTR)proposed in (Keller and Bengio, 2005).
NNTR isa discriminative approach which aims at modelinghow likely a given word v is in the context of a pieceof text C, while learning a more compact represen-tation of reduced dimensionality for both v and C.NNTR is composed of 3 Multilayer Perceptrons,noted mlpA(), mlpB() and mlpC(), connected asfollow:NNTR(v, C) = mlpC [mlpA(v),mlpB(C)].mlpA(v) and mlpB(C) project respectively thevector space representation of the word and textinto a more compact space of lower dimensionality.mlpC() takes as input the new representations of vand C and outputs a score for the contextual rele-vance of v to C.As training data, couples (v,C) from the BNC cor-pus are provided to the learning scheme.
The targettraining value for the output of the system is 1 if v isindeed in C and -1 otherwise.
The hope is that theneural network will be able to generalize to wordswhich are not in the piece of text but are likely to berelated to it.In essence, this model is trained by minimizingthe weighted sum of the hinge loss function overnegative and positive couples, using stochastic Gra-dient Descent (see (Keller and Bengio, 2005) for fur-ther details).
The small held out development set ofthe substitution dataset was used to tune the hyper-parameters of the model, maximizing average preci-sion (see Section 4.1).
For simplicity mlpA() andmlpB() were reduced to Perceptrons.
The outputsize of mlpA() was set to 20, mlpB() to 100 and thenumber of hidden units of mlpC() was set to 500.There are a couple of important conceptual differ-ences of the discriminative NNTR model comparedto the generative Bayesian model described above.First, the relevancy of v to C in NNTR is inferredin a more compact representation space of reduceddimensionality, which may enable a higher degreeof generalization.
Second, in NNTR we are able tocontrol the capacity of the model in terms of num-ber of parameters, enabling better control to achievean optimal generalization level with respect to thetraining data (avoiding over or under fitting).4 Empirical Results4.1 Evaluation MeasuresWe compare the lexical substitution scoring methodsusing two evaluation measures, offering two differ-ent perspectives of evaluation.4.1.1 AccuracyThe first evaluation measure is motivated by simu-lating a decision step of a subtitling system, in whichthe best scoring lexical substitution is selected foreach given sentence.
Such decision may correspondto a situation in which each single substitution maysuffice to obtain the desired compression rate, ormight be part of a more complex decision mecha-nism of the complete subtitling system.
We thusmeasure the resulting accuracy of subtitles createdby applying the best scoring substitution examplefor every original sentence.
This provides a macroevaluation style since we obtain a single judgmentfor each group of substitution examples that corre-spond to one original sentence.In our dataset 25.5% of the original sentenceshave no correct substitution examples and for 15.5%of the sentences all substitution examples were an-notated as correct.
Accordingly, the (macro aver-aged) accuracy has a lower bound of 0.155 and up-per bound of 0.745.494.1.2 Average PrecisionAs a second evaluation measure we compare theaverage precision of each method over all the exam-ples from all original sentences pooled together (amicro averaging approach).
This measures the po-tential of a scoring method to ensure high precisionfor the high scoring examples and to filter out low-scoring incorrect substitutions.Average precision is a single figure measure com-monly used to evaluate a system?s ranking ability(Voorhees and Harman, 1999).
It is equivalent to thearea under the uninterpolated recall-precision curve,defined as follows:average precision =?Ni=1 P(i)T (i)?Ni=1T (i)P(i) =?ik=1T (k)i(2)where N is the number of examples in the testset (797 in our case), T (i) is the gold annotation(true=1, false=0) and i ranges over the examplesranked by decreasing score.
An average precisionof 1.0 means that the system assigned a higher scoreto all true examples than to any false one (perfectranking).
A lower bound of 0.26 on our test set cor-responds to a system that ranks all false examplesabove the true ones.4.2 ResultsFigure 1 shows the accuracy and average precisionresults of the various models on our test set.
The ran-dom baseline and corresponding significance levelswere achieved by averaging multiple runs of a sys-tem that assigned random scores.
As can be seen inthe figures, the models?
behavior seems to be con-sistent in both evaluation measures.Overall, the distributional similarity basedmethod (sim) performs much better than theother methods.
In particular, Lin?s similarityalso performs better than semcor, the othercontext-independent model.
Generally, the contextindependent models perform better than the contex-tual ones.
Between the two contextual models, nntris superior to Bayes.
In fact the Bayes model is notsignificantly better than random scoring.4.3 Analysis and DiscussionWhen analyzing the data we identified several rea-sons why some of the WordNet substitutions werejudged as false.
In some cases the source word asappearing in the original sentence is not in a sensefor which it is a synonym of the target word.
For ex-ample, in many situations the word answer is in thesense of a statement that is made in reply to a ques-tion or request.
In such cases, such as in example 2from Table 1, answer can be successfully replacedwith reply yielding a substitution which conveys theoriginal meaning.
However, in situations such as inexample 1 the word answer is in the sense of a gen-eral solution and cannot be replaced with reply.
Thisis also the case in examples 4 and 5 in which subjectdoes not appear in the sense of topic or theme.Having an inappropriate sense, however, is not theonly reason for incorrect substitutions.
In example 8approach appears in a sense which is synonymouswith attack and in example 9 problem appears in asense which is synonymous with a quite uncommonuse of the word job.
Nevertheless, these substitu-tions were judged as unacceptable since the desiredsense of the target word after the substitution is notvery clear from the context.
In many other cases,such as in example 7, though semantically correct,the substitution was judged as incorrect due to stylis-tic considerations.Finally, there are cases, such as in example 6in which the source word is part of a collocationand cannot be replaced with semantically equivalentwords.When analyzing the mistakes of the distributionalsimilarity method it seems as if many were not nec-essarily due to the method itself but rather to imple-mentation issues.
The online source we used con-tains only the top most similar words for any word.In many cases substitutions were assigned a score ofzero since they were not listed among the top scoringsimilar words in the database.
Furthermore, the cor-pus that was used for training the similarity scoreswas news articles in American English spelling anddoes not always supply good scores to words ofBritish spelling in our BBC dataset (e.g.
analyse,behavioural, etc.
).The similarity based method seems to performbetter than the SemCor based method since, as notedabove, even when the source word is in the appro-priate sense it not necessarily substitutable with thetarget.
For this reason we hypothesize that apply-ing Word Sense Disambiguation (WSD) methods to50Figure 1: Accuracy and Average Precision Resultsclassify the specificWordNet sense of the source andtarget words may have only a limited impact on per-formance.Overall, context independent models seem to per-form relatively well since many candidate synonymsare a priori not substitutable.
This demonstrates thatsuch models are able to filter out many quirky Word-Net synonyms, such as problem and job.Fitness to the sentence context seems to be a lessfrequent factor and not that trivial to model.
Localcontext (adjacent words) seems to play more of arole than the broader sentence context.
However,these two types of contexts were not distinguished inthe bag-of-words representations of the two contex-tual methods that we examined.
It will be interestingto investigate in future research using different fea-ture types for local and global context, as commonlydone for Word Sense Disambiguation (WSD).
Yet,it would still remain a challenging task to correctlydistinguish, for example, the contexts for which an-swer is substitutable by reply (as in example 2) fromcontexts in which it is not (as in example 1).So far we have investigated separately the perfor-mance of context independent and contextual mod-els.
In fact, the accuracy performance of the (con-text independent) sim method is not that far fromthe upper bound, and the analysis above indicated arather small potential for improvement by incorpo-rating information from a contextual method.
Yet,there is still a substantial room for improvement inthe ranking quality of this model, as measured by av-erage precision, and it is possible that a smart com-bination with a high-quality contextual model wouldyield better performance.
In particular, we wouldexpect that a good contextual model will identify thecases in which for potentially good synonyms pair,the source word appears in a sense that is not substi-tutable with the target, such as in examples 1, 4 and5 in Table 1.
Investigating better contextual modelsand their optimal combination with context indepen-dent models remains a topic for future research.5 ConclusionThis paper investigated an isolated setting of the lex-ical substitution task, which has typically been em-bedded in larger systems and not evaluated directly.The setting allowed us to analyze different types ofstate of the art models and their behavior with re-spect to characteristic sub-cases of the problem.The major conclusion that seems to arise fromour experiments is the effectiveness of combining aknowledge based thesaurus such as WordNet withdistributional statistical information such as (Lin,1998), overcoming the known deficiencies of eachmethod alone.
Furthermore, modeling the a pri-ori substitution likelihood captures the majority ofcases in the evaluated setting, mostly because Word-Net provides a rather noisy set of substitution candi-dates.
On the other hand, successfully incorporatinglocal and global contextual information, as similarto WSD methods, remains a challenging task for fu-ture research.
Overall, scoring lexical substitutions51is an important component in many applications andwe expect that our findings are likely to be broadlyapplicable.References[Budanitsky and Hirst2001] Alexander Budanitsky andGraeme Hirst.
2001.
Semantic distance in word-net: An experimental, application-oriented evalua-tion of five measures.
In Workshop on WordNet andOther Lexical Resources: Second Meeting of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 29?34.
[Burnard1995] Lou Burnard.
1995.
Users ReferenceGuide for the British National Corpus.
Oxford Uni-versity Computing Services, Oxford.
[Daelemans et al2004] Walter Daelemans, Anja Ho?thker,and Erik Tjong Kim Sang.
2004.
Automatic sen-tence simplification for subtitling in dutch and english.In Proceedings of the 4th International Conferenceon Language Resources and Evaluation, pages 1045?1048.
[Dagan et al2005] Ido Dagan, Oren Glickman, andBernardo Magnini.
2005.
The pascal recognising tex-tual entailment challenge.
Proceedings of the PAS-CAL Challenges Workshop on Recognising TextualEntailment.
[Geffet and Dagan2005] Maayan Geffet and Ido Dagan.2005.
The distributional inclusion hypotheses and lex-ical entailment.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics (ACL?05), pages 107?114, Ann Arbor, Michigan,June.
Association for Computational Linguistics.
[Glickman et al2005] Oren Glickman, Ido Dagan, andMoshe Koppel.
2005.
A probabilistic classifica-tion approach for lexical textual entailment.
In AAAI,pages 1050?1055.
[Grefenstette1998] Gregory Grefenstette.
1998.
Produc-ing Intelligent Telegraphic Text Reduction to Providean Audio Scanning Service for the Blind.
pages 111?117, Stanford, CA, March.
[Harris1968] Zelig Harris.
1968.
Mathematical Struc-tures of Language.
New York: Wiley.
[Hori et al2002] Chiori Hori, Sadaoki Furui, RobMalkin,Hua Yu, and Alex Waibel.
2002.
Automaticspeech summarization applied to english broadcastnews speech.
volume 1, pages 9?12.
[Jing and McKeown1999] Hongyan Jing and Kathleen R.McKeown.
1999.
The decomposition of human-written summary sentences.
In SIGIR ?99: Proceed-ings of the 22nd annual international ACM SIGIR con-ference on Research and development in informationretrieval, pages 129?136, New York, NY, USA.
ACMPress.
[Keller and Bengio2005] Mikaela Keller and Samy Ben-gio.
2005.
A neural network for text representation.In Wodzisaw Duch, Janusz Kacprzyk, and Erkki Oja,editors, Artificial Neural Networks: Biological Inspi-rations ICANN 2005: 15th International Conference,Warsaw, Poland, September 11-15, 2005.
Proceedings,Part II, volume 3697 / 2005 of Lecture Notes in Com-puter Science, page p. 667.
Springer-Verlag GmbH.
[Knight and Marcu2002] Kevin Knight and DanielMarcu.
2002.
Summarization beyond sentenceextraction: a probabilistic approach to sentencecompression.
Artif.
Intell., 139(1):91?107.
[Landis and Koch1997] J. R. Landis and G. G. Koch.1997.
The measurements of observer agreement forcategorical data.
Biometrics, 33:159?174.
[Lin1998] Dekang Lin.
1998.
Automatic retrieval andclustering of similar words.
In Proceedings of the17th international conference on Computational lin-guistics, pages 768?774, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.
[McCarthy et al2004] Diana McCarthy, Rob Koeling,JulieWeeds, and John Carroll.
2004.
Finding predom-inant senses in untagged text.
In ACL, pages 280?288,Morristown, NJ, USA.
Association for ComputationalLinguistics.
[Miller et al1993] George A. Miller, Claudia Leacock,Randee Tengi, and Ross T. Bunker.
1993.
A semanticconcordance.
In HLT ?93: Proceedings of the work-shop on Human Language Technology, pages 303?308, Morristown, NJ, USA.
Association for Compu-tational Linguistics.
[Piperidis et al2004] Stelios Piperidis, Iason Demiros,Prokopis Prokopidis, Peter Vanroose, Anja Ho?thker,Walter Daelemans, Elsa Sklavounou, Manos Kon-stantinou, and Yannis Karavidas.
2004.
Multimodalmultilingual resources in the subtitling process.
InProceedings of the 4th International Language Re-sources and Evaluation Conference (LREC 2004), Lis-bon.
[Voorhees and Harman1999] Ellen M. Voorhees andDonna Harman.
1999.
Overview of the seventh textretrieval conference.
In Proceedings of the SeventhText REtrieval Conference (TREC-7).
NIST SpecialPublication.
[Voorhees1994] Ellen M. Voorhees.
1994.
Query expan-sion using lexical-semantic relations.
In SIGIR ?94:Proceedings of the 17th annual international ACM SI-GIR conference on Research and development in infor-mation retrieval, pages 61?69, New York, NY, USA.Springer-Verlag New York, Inc.52
