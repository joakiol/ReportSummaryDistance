Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 206?211,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsCombination of Recurrent Neural Networks and Factored LanguageModels for Code-Switching Language ModelingHeike Adelheike.adel@student.kit.eduNgoc Thang VuInstitute for Anthropomatics, Karlsruhe Institute of Technology (KIT)thang.vu@kit.eduTanja Schultztanja.schultz@kit.eduAbstractIn this paper, we investigate the appli-cation of recurrent neural network lan-guage models (RNNLM) and factoredlanguage models (FLM) to the task oflanguage modeling for Code-Switchingspeech.
We present a way to integrate part-of-speech tags (POS) and language in-formation (LID) into these models whichleads to significant improvements in termsof perplexity.
Furthermore, a comparisonbetween RNNLMs and FLMs and a de-tailed analysis of perplexities on the dif-ferent backoff levels are performed.
Fi-nally, we show that recurrent neural net-works and factored language models canbe combined using linear interpolation toachieve the best performance.
The finalcombined language model provides 37.8%relative improvement in terms of perplex-ity on the SEAME development set anda relative improvement of 32.7% on theevaluation set compared to the traditionaln-gram language model.Index Terms: multilingual speech processing,code switching, language modeling, recurrentneural networks, factored language models1 IntroductionCode-Switching (CS) speech is defined as speechthat contains more than one language (?code?).
Itis a common phenomenon in multilingual com-munities (Auer, 1999a).
For the automated pro-cessing of spoken communication in these sce-narios, a speech recognition system must be ableto handle code switches.
However, the compo-nents of speech recognition systems are usuallytrained on monolingual data.
Furthermore, thereis a lack of bilingual training data.
While therehave been promising research results in the areaof acoustic modeling, only few approaches so faraddress Code-Switching in the language model.Recently, it has been shown that recurrent neu-ral network language models (RNNLMs) can im-prove perplexity and error rates in speech recogni-tion systems in comparison to traditional n-gramapproaches (Mikolov et al, 2010; Mikolov et al,2011).
One reason for that is their ability to han-dle longer contexts.
Furthermore, the integrationof additional features as input is rather straight-forward due to their structure.
On the other hand,factored language models (FLMs) have been usedsuccessfully for languages with rich morphologydue to their ability to process syntactical features,such as word stems or part-of-speech tags (Bilmesand Kirchhoff, 2003; El-Desoky et al, 2010).The main contribution of this paper is the appli-cation of RNNLMs and FLMs to the challengingtask of Code-Switching.
Furthermore, the two dif-ferent models are combined using linear interpo-lation.
In addition, a comparison between them isprovided including a detailed analysis to explaintheir results.2 Related WorkFor this work, three different topics are investi-gated and combined: linguistic investigation ofCode-Switching, recurrent neural network lan-guage modeling and factored language models.In (Muysken, 2000; Poplack, 1978; Bokamba,1989), it is observed that code switches occur atpositions in an utterance where they do not violatethe syntactical rules of the involved languages.
Onthe one hand, Code-Switching can be regarded asa speaker dependent phenomenon (Auer, 1999b;Vu, Adel et al, 2013).
On the other hand, par-ticular Code-Switching patterns are shared acrossspeakers (Poplack, 1980).
It can be observed thatpart-of-speech tags may predict Code-Switchingpoints more reliable than words themselves.
The206authors of (Solorio et al, 2008a) predict Code-Switching points using several linguistic features,such as word form, language ID, part-of-speechtags or the position of the word relative to thephrase (BIO).
The best result is obtained by com-bining those features.
In (Chan et.al., 2006), fourdifferent kinds of n-gram language models arecompared to predict Code-Switching.
It is dis-covered that clustering all foreign words into theirpart-of-speech classes leads to the best perfor-mance.In the last years, neural networks have been usedfor a variety of tasks, including language model-ing (Mikolov et al, 2010).
Recurrent neural net-works are able to handle long-term contexts sincethe input vector does not only contain the cur-rent word but also the previous hidden layer.
Itis shown that these networks outperform tradi-tional language models, such as n-grams whichonly contain very limited histories.
In (Mikolovet al, 2011), the network is extended by factoriz-ing the output layer into classes to accelerate thetraining and testing processes.
The input layercan be augmented to model features, such as part-of-speech tags (Shi et al, 2011; Adel, Vu et al,2013).
In (Adel, Vu et al, 2013), recurrent neuralnetworks are applied to Code-Switching speech.
Itis shown that the integration of POS tags into theneural network, which predicts the next languageas well as the next word, leads to significant per-plexity reductions.A factored language model refers to a word as avector of features, such as the word itself, morpho-logical classes, POS tags or word stems.
Hence, itprovides another possibility to integrate syntacti-cal features into the language modeling process.In (Bilmes and Kirchhoff, 2003), it is shown thatfactored language models are able to outperformstandard n-gram techniques in terms of perplexity.In the same paper, generalized parallel backoff isintroduced.
This technique can be used to general-ize traditional backoff methods and to improve theperformance of factored language models.
Due tothe integration of various features, it is possible tohandle rich morphology in languages like Arabicor Turkish (Duh and Kirchhoff, 2004; El-Desokyet al, 2010).3 Code-Switching Language Modeling3.1 MotivationSince there is a lack of Code-Switching data, lan-guage modeling is a challenging task.
Traditionaln-gram approaches may not provide reliable esti-mates.
Hence, more general features than wordsshould be integrated into the language models.Therefore, we apply recurrent neural networks andfactored language models.
As features, we usepart-of-speech tags and language identifiers.3.2 Using Recurrent Neural Networks AsLanguage ModelThis section describes the structure of the recur-rent neural network (RNNLM) that we use asCode-Switching language model.
It has been pro-posed in (Adel, Vu et al, 2013) and is illustratedin figure 1.w(t)f(t)s(t)y(t)c(t)U1  VWU2Figure 1: RNNLM for Code-Switching(based upon a figure in (Mikolov et al, 2011))Vectorw(t), which represents the current word us-ing 1-of-N coding, forms the input of the recur-rent neural network.
Thus, its dimension equalsthe size of the vocabulary.
Vector s(t) con-tains the state of the network and is called ?hid-den layer?.
The network is trained using back-propagation through time (BPTT), an extension ofthe back-propagation algorithm for recurrent neu-ral networks.
With BPTT, the error is propagatedthrough recurrent connections back in time for aspecific number of time steps t. Hence, the net-work is able to remember information for severaltime steps.
The matrices U1, U2, V , and W con-tain the weights for the connections between thelayers.
These weights are learned during the train-ing phase.
Moreover, the output layer is factorized207into classes which provide language information.In this work, four classes are used: English, Man-darin, other languages and particles.
Vector c(t)contains the probabilities for each class and vectory(t) provides the probabilities for each word givenits class.
Hence, the probability P (wi|history) iscomputed as shown in equation 1.P (wi|history) = P (ci|s(t))P (wi|ci, s(t)) (1)It is intended to not only predict the next word butalso the next language.
Hence according to equa-tion 1, the probability of the next language is com-puted first and then the probability of each wordgiven the language.
Furthermore, a vector f(t)is added to the input layer.
It provides features(in this work part-of-speech tags) correspondingto the current word.
Thus, not only the currentword is activated but also its features.
Since thePOS tags are integrated into the input layer, theyare also propagated into the hidden layer and back-propagated into its history s(t).
Hence, not onlythe previous feature is stored in the history but alsofeatures from several time steps in the past.3.3 Using Factored Language ModelsFactored language models (FLM) are another ap-proach to integrate syntactical features, such aspart-of-speech tags or language identifiers into thelanguage modeling process.
Each word is re-garded as a sequence of features which are usedfor the computation of the n-gram probabilities.If a particular sequence of features has not beendetected in the training data, backoff techniqueswill be applied.
For our task of Code-Switching,we develop two different models: One model withonly part-of-speech tags as features and one modelincluding also language information tags.
Un-fortunately, the number of possible parameters israther high: Different feature combinations fromdifferent time steps can be used to predict thenext word (conditioning factors), different back-off paths and different smoothing methods maybe applied.
To detect useful parameters, the ge-netic algorithm described in (Duh and Kirchhoff,2004) is used.
It is an evolution-inspired techniquethat encodes the parameters of an FLM as binarystrings (genes).
First, an initializing set of genes isgenerated.
Then, a loop follows that evaluates thefitness of the genes and mutates them until theiraverage fitness is not improved any more.
As fit-ness value, the inverse perplexity of the FLM cor-responding to the gene on the development set isWt-1        Pt-1     Pt-2Wt-1        Pt-2 Wt-1        Pt-1Pt-2 Wt-1 Pt-1unigramFigure 2: Backoff graph of the FLMused.
Hence, parameter solutions with lower per-plexities are preferred in the selection of the genesfor the following iteration.
In (Duh and Kirch-hoff, 2004), it is shown that this genetic methodoutperforms both knowledge-based and random-ized choices.
For the case of part-of-speech tagsas features, the method results in three condition-ing factors: the previous word Wt?1 and the twoprevious POS tags Pt?1 and Pt?2.
The backoffgraph obtained by the algorithm is illustrated infigure 2.
According to the result of the genetic al-gorithm, different smoothing methods are used atdifferent backoff levels: For the backoff from threefactors to two factors, Kneser-Ney discounting isapplied.
If the probabilities for the factor combi-nation Wt?1Pt?2 could not be estimated reliably,absolute discounting is used.
In all other cases,Witten-Bell discounting is applied.
An overviewof the different smoothing methods can be foundin (Rosenfeld, 2000).4 Experiments and Results4.1 Data CorpusSEAME (South East Asia Mandarin-English) is aconversational Mandarin-English Code-Switchingspeech corpus recorded from Singaporean andMalaysian speakers (D.C. Lyu et al, 2011).
Itwas used for the research project ?Code-Switch?jointly performed by Nanyang Technological Uni-versity (NTU) and Karlsruhe Institute of Technol-ogy (KIT).
The recordings consist of spontanouslyspoken interviews and conversations of about 63hours of audio data.
For this task, we deleted allhesitations and divided the transcribed words intofour categories: English words, Mandarin words,particles (Singaporean and Malaysian discourseparticles) and others (other languages).
These cat-egories are used as language information in thelanguage models.
The average number of Code-Switching points between Mandarin and English208is 2.6 per utterance and the duration of monolin-gual segments is quite short: The average dura-tion of English and Mandarin segments is only0.67 seconds and 0.81 seconds respectively.
In to-tal, the corpus contains 9,210 unique English and7,471 unique Mandarin vocabulary words.
We di-vided the corpus into three disjoint sets (training,development and test set) and assigned the databased on several criteria (gender, speaking style,ratio of Singaporean and Malaysian speakers, ra-tio of the four categories, and the duration in eachset).
Table 1 lists the statistics of the corpus inthese sets.Train set Dev set Eval set# Speakers 139 8 8Duration(hrs) 59.2 2.1 1.5# Utterances 48,040 1,943 1,018# Token 525,168 23,776 11,294Table 1: Statistics of the SEAME corpus4.2 POS Tagger for Code-Switching SpeechTo be able to assign part-of-speech tags to ourbilingual text corpus, we apply the POS taggerdescribed in (Schultz et al, 2010) and (Adel, Vuet al, 2013).
It consists of two different mono-lingual (Stanford log-linear) taggers (Toutanovaet al, 2003; Toutanova et al, 2000) and a com-bination of their results.
While (Solorio et al,2008b) passes the whole Code-Switching text toboth monolingual taggers and combines their re-sults using different heuristics, in this work, thetext is splitted into different languages first.
Thetagging process is illustrated in figure 3.Mandarin is determined as matrix language (themain language of an utterance) and English as em-bedded language.
If three or more words of theembedded language are detected, they are passedto the English tagger.
The rest of the text is passedto the Mandarin tagger, even if it contains foreignwords.
The idea is to provide the tagger as muchcontext as possible.
Since most English words inthe Mandarin segments are falsely tagged as nounsby the Mandarin tagger, a postprocessing step isapplied.
It passes all foreign words of the Man-darin segments to the English tagger in order toreplace the wrong tags with the correct ones.Wt-1 P2un-igr-gamu?ut-i?
PiW???a?n-igr-gamu?u?gnP???1a21?igr-gauP?-i???u?a?a?a?????
??PiPig1a21?
?1-gga u?
ut-i?- Pi?
?1-gga u?
u?gnP?
?r1?1 ?1?1?gnP??g?i1?
Pi a?PiPig1a21???
?a?Pig?Figure 3: Tagging of Code-Switching speech4.3 EvaluationFor evaluation, we compute the perplexity of eachlanguage model on the SEAME development andevaluation set und perform an analysis of the dif-ferent back-off levels to understand in detail thebehavior of each language model.
A traditional 3-gram LM trained with the SEAME transcriptionsserves as baseline.4.3.1 LM PerformanceThe language models are evaluated in terms of per-plexity.
Table 2 presents the results on the devel-opment and test set.Model dev set test setBaseline 3-gram 285.87 285.25FLM (pos) 263.57 271.57FLM (pos + lid) 263.84 276.99RNNLM (pos) 233.50 268.05RNNLM (pos + lid) 219.85 239.21Table 2: Perplexity resultsIt can be noticed that both the RNNLM and theFLM model outperform the traditional 3-grammodel.
Hence, adding syntactical features im-proves the word prediction.
For the FLM, it leadsto no improvement to add the language identifieras feature.
In contrast, clustering the words intotheir languages on the output layer of the RNNLMleads to lower perplexities.2094.3.2 Backoff Level AnalysisTo understand the different results of the RNNLMand the FLM, an analysis similar to the one de-scribed in (Oparin et al, 2012) is performed.
Foreach word, the backoff-level of the n-gram modelis observed.
Then, a level-dependent perplexity iscomputed for each model as shown in equation 2.PPLk = 10?
1Nk?wklog10P (wk|hk) (2)In the equation, k denotes the backoff-level, Nkthe number of words on this level, wk the currentword and hk its history.
Table 3 shows how ofteneach backoff-level is used and presents the level-dependent perplexities of each model on the de-velopment set.1-gram 2-gram 3-gram# occurences 6894 11628 6226Baseline 3-gram 5,786.24 165.82 28.28FLM (pos) 4,950.31 147.70 30.99RNNLM 3,231.02 151.67 21.24Table 3: Backoff-level-dependent PPLsIn case of backoff to the 2-gram, the FLM pro-vides the best perplexity, while for the 3-gram andbackoff to the 1-gram, the RNNLM performs best.This may be correlated with the better over-all per-plexity of the RNNLM in comparison to the FLM.Nevertheless, the backoff to the 2-gram is usedabout twice as often as the backoff to the 1-gramor the 3-gram.4.4 LM InterpolationThe different results of RNNLM and FLM showthat they provide different estimates of the nextword.
Thus, a combination of them may reducethe perplexities of table 2.
Hence, we apply lin-ear interpolation to the probabilities of each twomodels as shown in equation 3.P (w|h) = ??PM1(w|h)+(1??
)?PM2(w|h) (3)The equation shows the computation of the pob-ability for word w given its history h. PM1 de-notes the probability provided by the first modeland PM2 the probability from the second model.Table 4 shows the results of this experiment.
Theweights are optimized on the development set.The interpolation of RNNLM and FLM leads tothe best results.
This may be caused by the supe-rior backoff-level-dependent PPLs in comparisonPPL PPLModel weight on dev on evalFLM + 3-gram 0.7, 0.3 211.13 227.57RNNLM + 3-gram 0.8, 0.2 206.49 227.08RNNLM + FLM 0.6, 0.4 177.79 192.08Table 4: Perplexities after interpolationto the 3-gram model.
While the RNNLM performsbetter for the 3-gram and for the backoff to the 1-gram, the FLM performs the best in case of back-off to the 2-gram which is used more often thanthe other levels (table 3).5 ConclusionsIn this paper, we presented two different methodsfor language modeling of Code-Switching speech:Recurrent neural networks and factored languagemodels.
We integrated part-of-speech tags andlanguage information to improve the performanceof the language models.
In addition, we ana-lyzed their behavior on the different backoff lev-els.
While the FLM performed better in case ofbackoff to the 2-gram, the RNNLM led to a bet-ter over-all performance.
Finally, the models werecombined using linear interpolation.
The com-bined language model provided 37.8% relative im-provement in terms of perplexity on the SEAMEdevelopment set and a relative improvement of32.7% on the evaluation set compared to the tra-ditional n-gram LM.ReferencesH.
Adel, N.T.
Vu, F. Kraus, T. Schlippe, and T. Schultz.2013 Recurrent Neural Network Language Model-ing for Code Switching Conversational Speech In:Proceedings of ICASSP 2013.P.
Auer 1999 Code-Switching in Conversation , Rout-ledge.P.
Auer 1999 From codeswitching via language mixingto fused lects toward a dynamic typology of bilin-gual speech In: International Journal of Bilingual-ism, vol.
3, no.
4, pp.
309-332.J.A.
Bilmes and K. Kirchhoff.
2003 Factored Lan-guage Models and Generalized Parallel Backoff In:Proceedings of NAACL, 2003.E.G.
Bokamba 1989 Are there syntactic constraints oncode-mixing?
In: World Englishes, vol.
8, no.
3, pp.277-292.J.Y.C.
Chan, PC Ching, T. Lee, and H. Cao 2006Automatic speech recognition of Cantonese-English210code-mixing utterances In: Proceeding of Inter-speech 2006.K.
Duh and K. Kirchhoff.
2004.
Automatic Learningof Language Model Structure, pg 148.
In: Proceed-ings of the 20th international conference on Compu-tational Linguistics.A.
El-Desoky, R. Schlu?ter, H.Ney 2010 AHybrid Mor-phologically Decomposed Factored Language Mod-els for Arabic LVCSR In: NAACL 2010.D.C.
Lyu, T.P.
Tan, E.S.
Cheng, H. Li 2011 An Anal-ysis of Mandarin-English Code-Switching SpeechCorpus: SEAME In: Proceedings of Interspeech2011.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993 Building a large annotated corpus of english:The penn treebank In: Computational Linguistics,vol.
19, no.
2, pp.
313330.T.
Mikolov, M. Karafiat, L. Burget, J. Jernocky and S.Khudanpur.
2010 Recurrent Neural Network basedLanguage Model In: Proceedings of Interspeech2010.T.
Mikolov, S. Kombrink, L. Burget, J. Jernocky andS.
Khudanpur.
2011 Extensions of Recurrent Neu-ral Network Language Model In: Proceedings ofICASSP 2011.P.
Muysken 2000 Bilingual speech: A typology ofcode-mixing In: Cambridge University Press, vol.11.I.
Oparin, M. Sundermeyer, H. Ney, J.-L. Gauvain2012 Performance analysis of Neural Networksin combination with n-gram language models In:ICASSP, 2012.S.
Poplack 1978 Syntactic structure and social func-tion of code-switching , Centro de Estudios Puertor-riquenos, City University of New York.S.
Poplack 1980 Sometimes ill start a sentence inspanish y termino en espanol: toward a typology ofcode-switching In: Linguistics, vol.
18, no.
7-8, pp.581-618.R.
Rosenfeld 2000 Two decades of statistical languagemodeling: Where do we go from here?
In: Proceed-ings of the IEEE 88.8 (2000): 1270-1278.T.
Schultz, P. Fung, and C. Burgmer, 2010 Detectingcode-switch events based on textual features.Y.
Shi, P. Wiggers, M. Jonker 2011 Towards RecurrentNeural Network Language Model with Linguisticsand Contextual Features In: Proceedings of Inter-speech 2011.T.
Solorio, Y. Liu 2008 Part-of-speech tagging forEnglish-Spanish code-switched text In: Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing.
Association for Computa-tional Linguistics, 2008.T.
Solorio, Y. Liu 2008 Learning to predict code-switching points In: Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcessing.
Association for Computational Linguis-tics, 2008.K.
Toutanova, C.D.
Manning 2000 Enriching theknowledge sources used in a maximum entropy part-of-speech tagger In: Proceedings of the 2000 JointSIGDAT conference on Empirical methods in natu-ral language processing and very large corpora: heldin conjunction with the 38th Annual Meeting of theAssociation for Computational Linguistics, vol.
13.K.
Toutanova, D. Klein, C.D.
Manning, and Y. Singer2003 Feature-rich part-of-speech tagging with acyclic dependency network In: Proceedings ofNAACL 2003.N.T.
Vu, D.C. Lyu, J. Weiner, D. Telaar, T. Schlippe, F.Blaicher, E.S.
Chng, T. Schultz, H. Li 2012 A FirstSpeech Recognition System For Mandarin-EnglishCode-Switch Conversational Speech In: Proceed-ings of Interspeech 2012.N.T.
Vu, H. Adel, T. Schultz 2013 An Investigation ofCode-Switching Attitude Dependent Language Mod-eling In: In Statistical Language and Speech Pro-cessing, First International Conference, 2013.N.
Xue, F. Xia, F.D.
Chiou, and M. Palmer 2005 Thepenn chinese treebank: Phrase structure annotationof a large corpusk In: Natural Language Engineer-ing, vol.
11, no.
2, pp.
207.211
