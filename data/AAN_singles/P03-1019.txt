A Comparative Study on Reordering Constraints in Statistical MachineTranslationRichard Zens and Hermann NeyChair of Computer Science VIRWTH Aachen - University of Technology{zens,ney}@cs.rwth-aachen.deAbstractIn statistical machine translation, the gen-eration of a translation hypothesis is com-putationally expensive.
If arbitrary word-reorderings are permitted, the search prob-lem is NP-hard.
On the other hand, ifwe restrict the possible word-reorderingsin an appropriate way, we obtain apolynomial-time search algorithm.In this paper, we compare two different re-ordering constraints, namely the ITG con-straints and the IBM constraints.
Thiscomparison includes a theoretical dis-cussion on the permitted number of re-orderings for each of these constraints.We show a connection between the ITGconstraints and the since 1870 knownSchro?der numbers.We evaluate these constraints on twotasks: the Verbmobil task and the Cana-dian Hansards task.
The evaluation con-sists of two parts: First, we check howmany of the Viterbi alignments of thetraining corpus satisfy each of these con-straints.
Second, we restrict the search toeach of these constraints and compare theresulting translation hypotheses.The experiments will show that the base-line ITG constraints are not sufficienton the Canadian Hansards task.
There-fore, we present an extension to the ITGconstraints.
These extended ITG con-straints increase the alignment coveragefrom about 87% to 96%.1 IntroductionIn statistical machine translation, we are givena source language (?French?)
sentence fJ1 =f1 .
.
.
fj .
.
.
fJ , which is to be translated into a targetlanguage (?English?)
sentence eI1 = e1 .
.
.
ei .
.
.
eI .Among all possible target language sentences, wewill choose the sentence with the highest probabil-ity:e?I1 = argmaxeI1{Pr(eI1|fJ1 )} (1)= argmaxeI1{Pr(eI1) ?
Pr(fJ1 |eI1)} (2)The decomposition into two knowledge sourcesin Eq.
2 is the so-called source-channel approachto statistical machine translation (Brown et al,1990).
It allows an independent modeling of tar-get language model Pr(eI1) and translation modelPr(fJ1 |eI1).
The target language model describesthe well-formedness of the target language sentence.The translation model links the source language sen-tence to the target language sentence.
It can be fur-ther decomposed into alignment and lexicon model.The argmax operation denotes the search problem,i.e.
the generation of the output sentence in the tar-get language.
We have to maximize over all possibletarget language sentences.In this paper, we will focus on the alignmentproblem, i.e.
the mapping between source sen-tence positions and target sentence positions.
Asthe word order in source and target language maydiffer, the search algorithm has to allow certainword-reorderings.
If arbitrary word-reorderings areallowed, the search problem is NP-hard (Knight,1999).
Therefore, we have to restrict the possiblereorderings in some way to make the search prob-lem feasible.
Here, we will discuss two such con-straints in detail.
The first constraints are based oninversion transduction grammars (ITG) (Wu, 1995;Wu, 1997).
In the following, we will call these theITG constraints.
The second constraints are the IBMconstraints (Berger et al, 1996).
In the next section,we will describe these constraints from a theoreticalpoint of view.
Then, we will describe the resultingsearch algorithm and its extension for word graphgeneration.
Afterwards, we will analyze the Viterbialignments produced during the training of the align-ment models.
Then, we will compare the translationresults when restricting the search to either of theseconstraints.2 Theoretical DiscussionIn this section, we will discuss the reordering con-straints from a theoretical point of view.
We willanswer the question of how many word-reorderingsare permitted for the ITG constraints as well as forthe IBM constraints.
Since we are only interestedin the number of possible reorderings, the specificword identities are of no importance here.
Further-more, we assume a one-to-one correspondence be-tween source and target words.
Thus, we are inter-ested in the number of word-reorderings, i.e.
permu-tations, that satisfy the chosen constraints.
First, wewill consider the ITG constraints.
Afterwards, wewill describe the IBM constraints.2.1 ITG ConstraintsLet us now consider the ITG constraints.
Here, weinterpret the input sentence as a sequence of blocks.In the beginning, each position is a block of its own.Then, the permutation process can be seen as fol-lows: we select two consecutive blocks and mergethem to a single block by choosing between two op-tions: either keep them in monotone order or invertthe order.
This idea is illustrated in Fig.
1.
The whiteboxes represent the two blocks to be merged.Now, we investigate, how many permutations areobtainable with this method.
A permutation derivedby the above method can be represented as a binarytree where the inner nodes are colored either black orwhite.
At black nodes the resulting sequences of thechildren are inverted.
At white nodes they are kept inmonotone order.
This representation is equivalent tosource positionstargetpositionswithout inversion with inversionFigure 1: Illustration of monotone and inverted con-catenation of two consecutive blocks.the parse trees of the simple grammar in (Wu, 1997).We observe that a given permutation may be con-structed in several ways by the above method.
Forinstance, let us consider the identity permutation of1, 2, ..., n. Any binary tree with n nodes and all in-ner nodes colored white (monotone order) is a pos-sible representation of this permutation.
To obtaina unique representation, we pose an additional con-straint on the binary trees: if the right son of a nodeis an inner node, it has to be colored with the oppo-site color.
With this constraint, each of these binarytrees is unique and equivalent to a parse tree of the?canonical-form?
grammar in (Wu, 1997).In (Shapiro and Stephens, 1991), it is shown thatthe number of such binary trees with n nodes isthe (n ?
1)th large Schro?der number Sn?1.
The(small) Schro?der numbers have been first describedin (Schro?der, 1870) as the number of bracketings ofa given sequence (Schro?der?s second problem).
Thelarge Schro?der numbers are just twice the Schro?dernumbers.
Schro?der remarked that the ratio betweentwo consecutive Schro?der numbers approaches 3 +2?2 = 5.8284... .
A second-order recurrence forthe large Schro?der numbers is:(n+ 1)Sn = 3(2n?
1)Sn?1 ?
(n?
2)Sn?2with n ?
2 and S0 = 1, S1 = 2.The Schro?der numbers have many combinatori-cal interpretations.
Here, we will mention only twoof them.
The first one is another way of view-ing at the ITG constraints.
The number of permu-tations of the sequence 1, 2, ..., n, which avoid thesubsequences (3, 1, 4, 2) and (2, 4, 1, 3), is the largeSchro?der number Sn?1.
More details on forbiddensubsequences can be found in (West, 1995).
Theinteresting point is that a search with the ITG con-straints cannot generate a word-reordering that con-tains one of these two subsequences.
In (Wu, 1997),these forbidden subsequences are called ?inside-out?transpositions.Another interpretation of the Schro?der numbers isgiven in (Knuth, 1973): The number of permutationsthat can be sorted with an output-restricted double-ended queue (deque) is exactly the large Schro?dernumber.
Additionally, Knuth presents an approxi-mation for the large Schro?der numbers:Sn ?
c ?
(3 +?8)n ?
n?
32 (3)where c is set to 12?(3?2?
4)/pi.
This approxi-mation function confirms the result of Schro?der, andwe obtain Sn ?
?
((3 +?8)n), i.e.
the Schro?dernumbers grow like (3 +?8)n ?
5.83n.2.2 IBM ConstraintsIn this section, we will describe the IBM constraints(Berger et al, 1996).
Here, we mark each position inthe source sentence either as covered or uncovered.In the beginning, all source positions are uncovered.Now, the target sentence is produced from bottom totop.
A target position must be aligned to one of thefirst k uncovered source positions.
The IBM con-straints are illustrated in Fig.
2.Juncovered positioncovered positionuncovered position for extension1 jFigure 2: Illustration of the IBM constraints.For most of the target positions there are k per-mitted source positions.
Only towards the end of thesentence this is reduced to the number of remaininguncovered source positions.
Let n denote the lengthof the input sequence and let rn denote the permittednumber of permutations with the IBM constraints.Then, we obtain:rn ={ kn?k ?
k!
n > kn!
n ?
k (4)Typically, k is set to 4.
In this case, we obtain anasymptotic upper and lower bound of 4n, i.e.
rn ??
(4n).In Tab.
1, the ratio of the number of permitted re-orderings for the discussed constraints is listed asa function of the sentence length.
We see that forlonger sentences the ITG constraints allow for morereorderings than the IBM constraints.
For sentencesof length 10 words, there are about twice as manyreorderings for the ITG constraints than for the IBMconstraints.
This ratio steadily increases.
For longersentences, the ITG constraints allow for much moreflexibility than the IBM constraints.3 SearchNow, let us get back to more practical aspects.
Re-ordering constraints are more or less useless, if theydo not allow the maximization of Eq.
2 to be per-formed in an efficient way.
Therefore, in this sec-tion, we will describe different aspects of the searchalgorithm for the ITG constraints.
First, we willpresent the dynamic programming equations and theresulting complexity.
Then, we will describe prun-ing techniques to accelerate the search.
Finally, wewill extend the basic algorithm for the generation ofword graphs.3.1 AlgorithmThe ITG constraints allow for a polynomial-timesearch algorithm.
It is based on the following dy-namic programming recursion equations.
Duringthe search a table Qjl,jr,eb,et is constructed.
Here,Qjl,jr,eb,et denotes the probability of the best hy-pothesis translating the source words from positionjl (left) to position jr (right) which begins with thetarget language word eb (bottom) and ends with theword et (top).
This is illustrated in Fig.
3.Here, we initialize this table with monotone trans-lations of IBM Model 4.
Therefore, Q0jl,jr,eb,et de-notes the probability of the best monotone hypothe-sis of IBM Model 4.
Alternatively, we could use anyother single-word based lexicon as well as phrase-based models for this initialization.
Our choice isthe IBM Model4 to make the results as comparableTable 1: Ratio of the number of permitted reorderings with the ITG constraints Sn?1 and the IBM constraintsrn for different sentence lengths n.n 1 ... 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20Sn?1/rn ?
1.0 1.2 1.4 1.7 2.1 2.6 3.4 4.3 5.6 7.4 9.8 13.0 17.4 23.3 31.4jl jre betFigure 3: Illustration of the Q-table.as possible to the search with the IBM constraints.We introduce a new parameter pm (m=?
monotone),which denotes the probability of a monotone combi-nation of two partial hypotheses.Qjl,jr,eb,et = (5)maxjl?k<jr,e?,e??
{Q0jl,jr,eb,et ,Qjl,k,eb,e?
?Qk+1,jr,e?
?,et ?
p(e??|e?)
?
pm,Qk+1,jr,eb,e?
?Qjl,k,e?
?,et ?
p(e??|e?)
?
(1?
pm)}We formulated this equation for a bigram lan-guage model, but of course, the same method canalso be applied for a trigram language model.
Theresulting algorithm is similar to the CYK-parsing al-gorithm.
It has a worst-case complexity of O(J3 ?E4).
Here, J is the length of the source sentenceand E is the vocabulary size of the target language.3.2 PruningAlthough the described search algorithm has apolynomial-time complexity, even with a bigramlanguage model the search space is very large.
A fullsearch is possible but time consuming.
The situationgets even worse when a trigram language model isused.
Therefore, pruning techniques are obligatoryto reduce the translation time.Pruning is applied to hypotheses that translate thesame subsequence f jrjl of the source sentence.
Weuse pruning in the following two ways.
The firstpruning technique is histogram pruning: we restrictthe number of translation hypotheses per sequencef jrjl .
For each sequence fjrjl , we keep only a fixednumber of translation hypotheses.
The second prun-ing technique is threshold pruning: the idea is to re-move all hypotheses that have a low probability rela-tive to the best hypothesis.
Therefore, we introducea threshold pruning parameter q, with 0 ?
q ?
1.Let Q?jl,jr denote the maximum probability of alltranslation hypotheses for f jrjl .
Then, we prune ahypothesis iff:Qjl,jr,eb,et < q ?Q?jl,jrApplying these pruning techniques the computa-tional costs can be reduced significantly with almostno loss in translation quality.3.3 Generation of Word GraphsThe generation of word graphs for a bottom-topsearch with the IBM constraints is described in(Ueffing et al, 2002).
These methods cannot beapplied to the CYK-style search for the ITG con-straints.
Here, the idea for the generation of wordgraphs is the following: assuming we already haveword graphs for the source sequences fkjl and fjrk+1,then we can construct a word graph for the sequencef jrjl by concatenating the partial word graphs eitherin monotone or inverted order.Now, we describe this idea in a more formal way.A word graph is a directed acyclic graph (dag) withone start and one end node.
The edges are annotatedwith target language words or phrases.
We also al-low ?-transitions.
These are edges annotated withthe empty word.
Additionally, edges may be anno-tated with probabilities of the language or translationmodel.
Each path from start node to end node rep-resents one translation hypothesis.
The probabilityof this hypothesis is calculated by multiplying theprobabilities along the path.During the search, we have to combine two wordgraphs in either monotone or inverted order.
Thisis done in the following way: we are given twoword graphs w1 and w2 with start and end nodes(s1, g1) and (s2, g2), respectively.
First, we addan ?-transition (g1, s2) from the end node of thefirst graph w1 to the start node of the second graphw2 and annotate this edge with the probability of amonotone concatenation pm.
Second, we create acopy of each of the original word graphs w1 and w2.Then, we add an ?-transition (g2, s1) from the endnode of the copied second graph to the start node ofthe copied first graph.
This edge is annotated withthe probability of a inverted concatenation 1 ?
pm.Now, we have obtained two word graphs: one for amonotone and one for a inverted concatenation.
Thefinal word graphs is constructed by merging the twostart nodes and the two end nodes, respectively.Let W (jl, jr) denote the word graph for thesource sequence f jrjl .
This graph is constructedfrom the word graphs of all subsequences of (jl, jr).Therefore, we assume, these word graphs have al-ready been produced.
For all source positions k withjl ?
k < jr, we combine the word graphs W (jl, k)and W (k + 1, jr) as described above.
Finally, wemerge all start nodes of these graphs as well as allend nodes.
Now, we have obtained the word graphW (jl, jr) for the source sequence f jrjl .
As initializa-tion, we use the word graphs of the monotone IBM4search.3.4 Extended ITG constraintsIn this section, we will extend the ITG constraintsdescribed in Sec.
2.1.
This extension will go beyondbasic reordering constraints.We already mentioned that the use of consecutivephrases within the ITG approach is straightforward.The only thing we have to change is the initializa-tion of the Q-table.
Now, we will extend this idea tophrases that are non-consecutive in the source lan-guage.
For this purpose, we adopt the view of theITG constraints as a bilingual grammar as, e.g., in(Wu, 1997).
For the baseline ITG constraints, theresulting grammar is:A ?
[AA] | ?AA?
| f/e | f/?
| ?/eHere, [AA] denotes a monotone concatenation and?AA?
denotes an inverted concatenation.Let us now consider the case of a source phraseconsisting of two parts f1 and f2.
Let e denote thecorresponding target phrase.
We add the productionsA ?
[e/f1 A ?/f2] | ?e/f1 A ?/f2?to the grammar.
The probabilities of these pro-ductions are, dependent on the translation direction,p(e|f1, f2) or p(f1, f2|e), respectively.
Obviously,these productions are not in the normal form of anITG, but with the method described in (Wu, 1997),they can be normalized.4 Corpus StatisticsIn the following sections we will present results ontwo tasks.
Therefore, in this section we will showthe corpus statistics for each of these tasks.4.1 VerbmobilThe first task we will present results on is the Verb-mobil task (Wahlster, 2000).
The domain of thiscorpus is appointment scheduling, travel planning,and hotel reservation.
It consists of transcriptionsof spontaneous speech.
Table 2 shows the corpusstatistics of this corpus.
The training corpus (Train)was used to train the IBM model parameters.
Theremaining free parameters, i.e.
pm and the modelscaling factors (Och and Ney, 2002), were adjustedon the development corpus (Dev).
The resulting sys-tem was evaluated on the test corpus (Test).Table 2: Statistics of training and test corpus forthe Verbmobil task (PP=perplexity, SL=sentencelength).German EnglishTrain Sentences 58 073Words 519 523 549 921Vocabulary 7 939 4 672Singletons 3 453 1 698average SL 8.9 9.5Dev Sentences 276Words 3 159 3 438Trigram PP - 28.1average SL 11.5 12.5Test Sentences 251Words 2 628 2 871Trigram PP - 30.5average SL 10.5 11.4Table 3: Statistics of training and test corpusfor the Canadian Hansards task (PP=perplexity,SL=sentence length).French EnglishTrain Sentences 1.5MWords 24M 22MVocabulary 100 269 78 332Singletons 40 199 31 319average SL 16.6 15.1Test Sentences 5432Words 97 646 88 773Trigram PP ?
179.8average SL 18.0 16.34.2 Canadian HansardsAdditionally, we carried out experiments on theCanadian Hansards task.
This task contains the pro-ceedings of the Canadian parliament, which are keptby law in both French and English.
About 3 millionparallel sentences of this bilingual data have beenmade available by the Linguistic Data Consortium(LDC).
Here, we use a subset of the data containingonly sentences with a maximum length of 30 words.Table 3 shows the training and test corpus statistics.5 Evaluation in TrainingIn this section, we will investigate for each of theconstraints the coverage of the training corpus align-ment.
For this purpose, we compute the Viterbialignment of IBM Model 5 with GIZA++ (Och andNey, 2000).
This alignment is produced without anyrestrictions on word-reorderings.
Then, we checkfor every sentence if the alignment satisfies each ofthe constraints.
The ratio of the number of satisfiedalignments and the total number of sentences is re-ferred to as coverage.
Tab.
4 shows the results forthe Verbmobil task and for the Canadian Hansardstask.
It contains the results for both translation direc-tions German-English (S?T) and English-German(T?S) for the Verbmobil task and French-English(S?T) and English-French (T?S) for the CanadianHansards task, respectively.For the Verbmobil task, the baseline ITG con-straints and the IBM constraints result in a similarcoverage.
It is about 91% for the German-Englishtranslation direction and about 88% for the English-German translation direction.
A significantly higherTable 4: Coverage on the training corpus for align-ment constraints for the Verbmobil task (VM) andfor the Canadian Hansards task (CH).coverage [%]task constraint S?T T?SVM IBM 91.0 88.1ITG baseline 91.6 87.0extended 96.5 96.9CH IBM 87.1 86.7ITG baseline 81.3 73.6extended 96.1 95.6coverage of about 96% is obtained with the extendedITG constraints.
Thus with the extended ITG con-straints, the coverage increases by about 8% abso-lute.For the Canadian Hansards task, the baseline ITGconstraints yield a worse coverage than the IBMconstraints.
Especially for the English-French trans-lation direction, the ITG coverage of 73.6% is verylow.
Again, the extended ITG constraints obtainedthe best results.
Here, the coverage increases fromabout 87% for the IBM constraints to about 96% forthe extended ITG constraints.6 Translation Experiments6.1 Evaluation CriteriaIn our experiments, we use the following error crite-ria:?
WER (word error rate):The WER is computed as the minimum num-ber of substitution, insertion and deletion oper-ations that have to be performed to convert thegenerated sentence into the target sentence.?
PER (position-independent word error rate):A shortcoming of the WER is the fact that itrequires a perfect word order.
The PER com-pares the words in the two sentences ignoringthe word order.?
mWER (multi-reference word error rate):For each test sentence, not only a single refer-ence translation is used, as for the WER, but awhole set of reference translations.
For eachtranslation hypothesis, the WER to the mostsimilar sentence is calculated (Nie?en et al,2000).?
BLEU score:This score measures the precision of unigrams,bigrams, trigrams and fourgrams with respectto a whole set of reference translations with apenalty for too short sentences (Papineni et al,2001).
BLEU measures accuracy, i.e.
largeBLEU scores are better.?
SSER (subjective sentence error rate):For a more detailed analysis, subjective judg-ments by test persons are necessary.
Eachtranslated sentence was judged by a human ex-aminer according to an error scale from 0.0 to1.0 (Nie?en et al, 2000).6.2 Translation ResultsIn this section, we will present the translation resultsfor both the IBM constraints and the baseline ITGconstraints.
We used a single-word based searchwith IBM Model 4.
The initialization for the ITGconstraints was done with monotone IBM Model 4translations.
So, the only difference between the twosystems are the reordering constraints.In Tab.
5 the results for the Verbmobil task areshown.
We see that the results on this task are sim-ilar.
The search with the ITG constraints yieldsslightly lower error rates.Some translation examples of the Verbmobil taskare shown in Tab.
6.
We have to keep in mind,that the Verbmobil task consists of transcriptionsof spontaneous speech.
Therefore, the source sen-tences as well as the reference translations may havean unorthodox grammatical structure.
In the firstexample, the German verb-group (?wu?rde vorschla-gen?)
is split into two parts.
The search with theITG constraints is able to produce a correct transla-tion.
With the IBM constraints, it is not possible totranslate this verb-group correctly, because the dis-tance between the two parts is too large (more thanfour words).
As we see in the second example, inGerman the verb of a subordinate clause is placed atthe end (?u?bernachten?).
The IBM search is not ableto perform the necessary long-range reordering, as itis done with the ITG search.7 Related WorkThe ITG constraints were introduced in (Wu, 1995).The applications were, for instance, the segmenta-tion of Chinese character sequences into Chinese?words?
and the bracketing of the source sentenceinto sub-sentential chunks.
In (Wu, 1996) the base-line ITG constraints were used for statistical ma-chine translation.
The resulting algorithm is simi-lar to the one presented in Sect.
3.1, but here, weuse monotone translation hypotheses of the full IBMModel 4 as initialization, whereas in (Wu, 1996) asingle-word based lexicon model is used.
In (Vilar,1998) a model similar to Wu?s method was consid-ered.8 ConclusionsWe have described the ITG constraints in detail andcompared them to the IBM constraints.
We draw thefollowing conclusions: especially for long sentencesthe ITG constraints allow for higher flexibility inword-reordering than the IBM constraints.
Regard-ing the Viterbi alignment in training, the baselineITG constraints yield a similar coverage as the IBMconstraints on the Verbmobil task.
On the CanadianHansards task the baseline ITG constraints were notsufficient.
With the extended ITG constraints thecoverage improves significantly on both tasks.
Onthe Canadian Hansards task the coverage increasesfrom about 87% to about 96%.We have presented a polynomial-time search al-gorithm for statistical machine translation based onthe ITG constraints and its extension for the gen-eration of word graphs.
We have shown the trans-lation results for the Verbmobil task.
On this task,the translation quality of the search with the base-line ITG constraints is already competitive with theresults for the IBM constraints.
Therefore, we ex-pect the search with the extended ITG constraints tooutperform the search with the IBM constraints.Future work will include the automatic extractionof the bilingual grammar as well as the use of thisgrammar for the translation process.ReferencesA.
L. Berger, P. F. Brown, S. A. D. Pietra, V. J. D. Pietra,J.
R. Gillett, A. S. Kehler, and R. L. Mercer.
1996.Language translation apparatus and method of usingcontext-based translation models, United States patent,patent number 5510981, April.P.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. DellaPietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, andP.
S. Roossin.
1990.
A statistical approach to machineTable 5: Translation results on the Verbmobil task.type automatic humanSystem WER [%] PER [%] mWER [%] BLEU [%] SSER [%]IBM 46.2 33.3 40.0 42.5 40.8ITG 45.6 33.9 40.0 37.1 42.0Table 6: Verbmobil: translation examples.source ja, ich wu?rde den Flug um viertel nach sieben vorschlagen.reference yes, I would suggest the flight at a quarter past seven.ITG yes, I would suggest the flight at seven fifteen.IBM yes, I would be the flight at quarter to seven suggestion.source ich schlage vor, dass wir in Hannover im Hotel Gru?nschnabel u?bernachten.reference I suggest to stay at the hotel Gru?nschnabel in Hanover.ITG I suggest that we stay in Hanover at hotel Gru?nschnabel.IBM I suggest that we are in Hanover at hotel Gru?nschnabel stay.translation.
Computational Linguistics, 16(2):79?85,June.K.
Knight.
1999.
Decoding complexity in word-replacement translation models.
Computational Lin-guistics, 25(4):607?615, December.D.
E. Knuth.
1973.
The Art of Computer Program-ming, volume 1 - Fundamental Algorithms.
Addison-Wesley, Reading, MA, 2nd edition.S.
Nie?en, F. J. Och, G. Leusch, and H. Ney.
2000.An evaluation tool for machine translation: Fast eval-uation for MT research.
In Proc.
of the Second Int.Conf.
on Language Resources and Evaluation (LREC),pages 39?45, Athens, Greece, May.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
In Proc.
of the 38th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 440?447, Hong Kong, October.F.
J. Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In Proc.
of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 295?302, July.K.
A. Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
Technical Report RC22176 (W0109-022),IBM Research Division, Thomas J. Watson ResearchCenter, September.E.
Schro?der.
1870.
Vier combinatorische Probleme.Zeitschrift fu?r Mathematik und Physik, 15:361?376.L.
Shapiro and A.
B. Stephens.
1991.
Boostrap percola-tion, the Schro?der numbers, and the n-kings problem.SIAM Journal on Discrete Mathematics, 4(2):275?280, May.N.
Ueffing, F. J. Och, and H. Ney.
2002.
Generationof word graphs in statistical machine translation.
InProc.
Conf.
on Empirical Methods for Natural Lan-guage Processing, pages 156?163, Philadelphia, PA,July.J.
M. Vilar.
1998.
Aprendizaje de Transductores Subse-cuenciales para su empleo en tareas de Dominio Re-stringido.
Ph.D. thesis, Universidad Politecnica de Va-lencia.W.
Wahlster, editor.
2000.
Verbmobil: Foundationsof speech-to-speech translations.
Springer Verlag,Berlin, Germany, July.J.
West.
1995.
Generating trees and the Catalan andSchro?der numbers.
Discrete Mathematics, 146:247?262, November.D.
Wu.
1995.
Stochastic inversion transduction gram-mars, with application to segmentation, bracketing,and alignment of parallel corpora.
In Proc.
of the 14thInternational Joint Conf.
on Artificial Intelligence (IJ-CAI), pages 1328?1334, Montreal, August.D.
Wu.
1996.
A polynomial-time algorithm for statis-tical machine translation.
In Proc.
of the 34th AnnualConf.
of the Association for Computational Linguistics(ACL ?96), pages 152?158, Santa Cruz, CA, June.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Com-putational Linguistics, 23(3):377?403, September.
