Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 611?618,Sydney, July 2006. c?2006 Association for Computational LinguisticsExamining the Role of Linguistic Knowledge Sources in the AutomaticIdentification and Classification of ReviewsVincent Ng and Sajib Dasgupta and S. M. Niaz ArifinHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{vince,sajib,arif}@hlt.utdallas.eduAbstractThis paper examines two problems indocument-level sentiment analysis: (1) de-termining whether a given document is areview or not, and (2) classifying the po-larity of a review as positive or negative.We first demonstrate that review identifi-cation can be performed with high accu-racy using only unigrams as features.
Wethen examine the role of four types of sim-ple linguistic knowledge sources in a po-larity classification system.1 IntroductionSentiment analysis involves the identification ofpositive and negative opinions from a text seg-ment.
The task has recently received a lot ofattention, with applications ranging from multi-perspective question-answering (e.g., Cardie et al(2004)) to opinion-oriented information extraction(e.g., Riloff et al (2005)) and summarization (e.g.,Hu and Liu (2004)).
Research in sentiment analy-sis has generally proceeded at three levels, aim-ing to identify and classify opinions from doc-uments, sentences, and phrases.
This paper ex-amines two problems in document-level sentimentanalysis, focusing on analyzing a particular typeof opinionated documents: reviews.The first problem, polarity classification, hasthe goal of determining a review?s polarity ?
pos-itive (?thumbs up?)
or negative (?thumbs down?
).Recent work has expanded the polarity classifi-cation task to additionally handle documents ex-pressing a neutral sentiment.
Although studiedfairly extensively, polarity classification remains achallenge to natural language processing systems.We will focus on an important linguistic aspectof polarity classification: examining the role of avariety of simple, yet under-investigated, linguis-tic knowledge sources in a learning-based polarityclassification system.
Specifically, we will showhow to build a high-performing polarity classifierby exploiting information provided by (1) high or-der n-grams, (2) a lexicon composed of adjectivesmanually annotated with their polarity information(e.g., happy is annotated as positive and terrible asnegative), (3) dependency relations derived fromdependency parses, and (4) objective terms andphrases extracted from neutral documents.As mentioned above, the majority of work ondocument-level sentiment analysis to date has fo-cused on polarity classification, assuming as in-put a set of reviews to be classified.
A relevantquestion is: what if we don?t know that an inputdocument is a review in the first place?
The sec-ond task we will examine in this paper ?
reviewidentification ?
attempts to address this question.Specifically, review identification seeks to deter-mine whether a given document is a review or not.We view both review identification and polar-ity classification as a classification task.
For re-view identification, we train a classifier to dis-tinguish movie reviews and movie-related non-reviews (e.g., movie ads, plot summaries) usingonly unigrams as features, obtaining an accuracyof over 99% via 10-fold cross-validation.
Simi-lar experiments using documents from the bookdomain also yield an accuracy as high as 97%.An analysis of the results reveals that the high ac-curacy can be attributed to the difference in thevocabulary employed in reviews and non-reviews:while reviews can be composed of a mixture ofsubjective and objective language, our non-reviewdocuments rarely contain subjective expressions.Next, we learn our polarity classifier using pos-itive and negative reviews taken from two movie611review datasets, one assembled by Pang and Lee(2004) and the other by ourselves.
The result-ing classifier, when trained on a feature set de-rived from the four types of linguistic knowl-edge sources mentioned above, achieves a 10-foldcross-validation accuracy of 90.5% and 86.1% onPang et al?s dataset and ours, respectively.
To ourknowledge, our result on Pang et al?s dataset isone of the best reported to date.
Perhaps more im-portantly, an analysis of these results show that thevarious types of features interact in an interestingmanner, allowing us to draw conclusions that pro-vide new insights into polarity classification.2 Related Work2.1 Review IdentificationAs noted in the introduction, while a review cancontain both subjective and objective phrases, ournon-reviews are essentially factual documents inwhich subjective expressions can rarely be found.Hence, review identification can be viewed as aninstance of the broader task of classifying whethera document is mostly factual/objective or mostlyopinionated/subjective.
There have been attemptson tackling this so-called document-level subjec-tivity classification task, with very encouragingresults (see Yu and Hatzivassiloglou (2003) andWiebe et al (2004) for details).2.2 Polarity ClassificationThere is a large body of work on classifying thepolarity of a document (e.g., Pang et al (2002),Turney (2002)), a sentence (e.g., Liu et al (2003),Yu and Hatzivassiloglou (2003), Kim and Hovy(2004), Gamon et al (2005)), a phrase (e.g., Wil-son et al (2005)), and a specific object (such as aproduct) mentioned in a document (e.g., Morinagaet al (2002), Yi et al (2003), Popescu and Etzioni(2005)).
Below we will center our discussion ofrelated work around the four types of features wewill explore for polarity classification.Higher-order n-grams.
While n-grams offer asimple way of capturing context, previous workhas rarely explored the use of n-grams as fea-tures in a polarity classification system beyond un-igrams.
Two notable exceptions are the work ofDave et al (2003) and Pang et al (2002).
Interest-ingly, while Dave et al report good performanceon classifying reviews using bigrams or trigramsalone, Pang et al show that bigrams are not use-ful features for the task, whether they are used inisolation or in conjunction with unigrams.
Thismotivates us to take a closer look at the utility ofhigher-order n-grams in polarity classification.Manually-tagged term polarity.
Much work hasbeen performed on learning to identify and clas-sify polarity terms (i.e., terms expressing a pos-itive sentiment (e.g., happy) or a negative senti-ment (e.g., terrible)) and exploiting them to dopolarity classification (e.g., Hatzivassiloglou andMcKeown (1997), Turney (2002), Kim and Hovy(2004), Whitelaw et al (2005), Esuli and Se-bastiani (2005)).
Though reasonably successful,these (semi-)automatic techniques often yield lex-icons that have either high coverage/low precisionor low coverage/high precision.
While manuallyconstructed positive and negative word lists exist(e.g., General Inquirer1), they too suffer from theproblem of having low coverage.
This prompts usto manually construct our own polarity word lists2and study their use in polarity classification.Dependency relations.
There have been severalattempts at extracting features for polarity classi-fication from dependency parses, but most focuson extracting specific types of information such asadjective-noun relations (e.g., Dave et al (2003),Yi et al (2003)) or nouns that enjoy a dependencyrelation with a polarity term (e.g., Popescu and Et-zioni (2005)).
Wilson et al (2005) extract a largervariety of features from dependency parses, butunlike us, their goal is to determine the polarity ofa phrase, not a document.
In comparison to previ-ous work, we investigate the use of a larger set ofdependency relations for classifying reviews.Objective information.
The objective portionsof a review do not contain the author?s opinion;hence features extracted from objective sentencesand phrases are irrelevant with respect to the po-larity classification task and their presence maycomplicate the learning task.
Indeed, recent workhas shown that benefits can be made by first sepa-rating facts from opinions in a document (e.g, Yuand Hatzivassiloglou (2003)) and classifying thepolarity based solely on the subjective portions ofthe document (e.g., Pang and Lee (2004)).
Moti-vated by the work of Koppel and Schler (2005), weidentify and extract objective material from non-reviews and show how to exploit such informationin polarity classification.1http://www.wjh.harvard.edu/?inquirer/spreadsheet guid.htm2Wilson et al (2005) have also manually tagged a list ofterms with their polarity, but this list is not publicly available.612Finally, previous work has also investigated fea-tures that do not fall into any of the above cate-gories.
For instance, instead of representing thepolarity of a term using a binary value, Mullenand Collier (2004) use Turney?s (2002) method toassign a real value to represent term polarity andintroduce a variety of numerical features that areaggregate measures of the polarity values of termsselected from the document under consideration.3 Review IdentificationRecall that the goal of review identification isto determine whether a given document is a re-view or not.
Given this definition, two immediatequestions come to mind.
First, should this prob-lem be addressed in a domain-specific or domain-independent manner?
In other words, should a re-view identification system take as input documentscoming from the same domain or not?Apparently this is a design question with nodefinite answer, but our decision is to performdomain-specific review identification.
The reasonis that the primary motivation of review identifi-cation is the need to identify reviews for furtheranalysis by a polarity classification system.
Sincepolarity classification has almost exclusively beenaddressed in a domain-specific fashion, it seemsnatural that its immediate upstream component ?review identification ?
should also assume do-main specificity.
Note, however, that assumingdomain specificity is not a self-imposed limita-tion.
In fact, we envision that the review identifica-tion system will have as its upstream component atext classification system, which will classify doc-uments by topic and pass to the review identifieronly those documents that fall within its domain.Given our choice of domain specificity, the nextquestion is: which documents are non-reviews?Here, we adopt a simple and natural definition:a non-review is any document that belongs to thegiven domain but is not a review.Dataset.
Now, recall from the introduction thatwe cast review identification as a classificationtask.
To train and test our review identifier, weuse 2000 reviews and 2000 non-reviews from themovie domain.
The 2000 reviews are taken fromPang et al?s polarity dataset (version 2.0)3, whichconsists of an equal number of positive and neg-ative reviews.
We collect the non-reviews for the3Available from http://www.cs.cornell.edu/people/pabo/movie-review-data.movie domain from the Internet Movie Databasewebsite4, randomly selecting any documents fromthis site that are on the movie topic but are not re-views themselves.
With this criterion in mind, the2000 non-review documents we end up with areeither movie ads or plot summaries.Training and testing the review identifier.
Weperform 10-fold cross-validation (CV) experi-ments on the above dataset, using Joachims?
(1999) SVMlight package5 to train an SVM clas-sifier for distinguishing reviews and non-reviews.All learning parameters are set to their defaultvalues.6 Each document is first tokenized anddowncased, and then represented as a vector ofunigrams with length normalization.7 FollowingPang et al (2002), we use frequency as presence.In other words, the ith element of the documentvector is 1 if the corresponding unigram is presentin the document and 0 otherwise.
The resultingclassifier achieves an accuracy of 99.8%.Classifying neutral reviews and non-reviews.Admittedly, the high accuracy achieved using sucha simple set of features is somewhat surpris-ing, although it is consistent with previous re-sults on document-level subjectivity classificationin which accuracies of 94-97% were obtained (Yuand Hatzivassiloglou, 2003; Wiebe et al, 2004).Before concluding that review classification is aneasy task, we conduct an additional experiment:we train a review identifier on a new dataset wherewe keep the same 2000 non-reviews but replacethe positive/negative reviews with 2000 neutral re-views (i.e., reviews with a mediocre rating).
In-tuitively, a neutral review contains fewer termswith strong polarity than a positive/negative re-view.
Hence, this additional experiment would al-low us to investigate whether the lack of strongpolarized terms in neutral reviews would increasethe difficulty of the learning task.Our neutral reviews are randomly chosen fromPang et al?s pool of 27886 unprocessed movie re-views8 that have either a rating of 2 (on a 4-pointscale) or 2.5 (on a 5-point scale).
Each review thenundergoes a semi-automatic preprocessing stage4See http://www.imdb.com.5Available from svmlight.joachims.org.6We tried polynomial and RBF kernels, but none yieldsbetter performance than the default linear kernel.7We observed that not performing length normalizationhurts performance slightly.8Also available from Pang?s website.
See Footnote 3.613where (1) HTML tags and any header and trailerinformation (such as date and author identity) areremoved; (2) the document is tokenized and down-cased; (3) the rating information extracted by reg-ular expressions is removed; and (4) the documentis manually checked to ensure that the rating infor-mation is successfully removed.
When trained onthis new dataset, the review identifier also achievesan accuracy of 99.8%, suggesting that this learningtask isn?t any harder in comparison to the previousone.Discussion.
We hypothesized that the high accu-racies are attributable to the different vocabularyused in reviews and non-reviews.
As part of ourverification of this hypothesis, we plot the learn-ing curve for each of the above experiments.9 Weobserve that a 99% accuracy was achieved in allcases even when only 200 training instances areused to acquire the review identifier.
The abil-ity to separate the two classes with such a smallamount of training data seems to imply that fea-tures strongly indicative of one or both classes arepresent.
To test this hypothesis, we examine the?informative?
features for both classes.
To getthese informative features, we rank the features bytheir weighted log-likelihood ratio (WLLR)10:P (wt|cj) logP (wt|cj)P (wt|?cj),where wt and cj denote the tth word in the vocab-ulary and the jth class, respectively.
Informally,a feature (in our case a unigram) w will have ahigh rank with respect to a class c if it appears fre-quently in c and infrequently in other classes.
Thiscorrelates reasonably well with what we think aninformative feature should be.
A closer examina-tion of the feature lists sorted by WLLR confirmsour hypothesis that each of the two classes has itsown set of distinguishing features.Experiments with the book domain.
To under-stand whether these good review identification re-sults only hold true for the movie domain, weconduct similar experiments with book reviewsand non-reviews.
Specifically, we collect 1000book reviews (consisting of a mixture of positive,negative, and neutral reviews) from the Barnes9The curves are not shown due to space limitations.10Nigam et al (2000) show that this metric is effec-tive at selecting good features for text classification.
Othercommonly-used feature selection metrics are discussed inYang and Pedersen (1997).and Noble website11, and 1000 non-reviews thatare on the book topic (mostly book summaries)from Amazon.12 We then perform 10-fold CV ex-periments using these 2000 documents as before,achieving a high accuracy of 96.8%.
These resultsseem to suggest that automatic review identifica-tion can be achieved with high accuracy.4 Polarity ClassificationCompared to review identification, polarity classi-fication appears to be a much harder task.
Thissection examines the role of various linguisticknowledge sources in our learning-based polarityclassification system.4.1 Experimental SetupLike several previous work (e.g., Mullen and Col-lier (2004), Pang and Lee (2004), Whitelaw et al(2005)), we view polarity classification as a super-vised learning task.
As in review identification,we use SVMlight with default parameter settingsto train polarity classifiers13 , reporting all resultsas 10-fold CV accuracy.We evaluate our polarity classifiers on twomovie review datasets, each of which consists of1000 positive reviews and 1000 negative reviews.The first one, which we will refer to as Dataset A,is the Pang et al polarity dataset (version 2.0).
Thesecond one (Dataset B) was created by us, with thesole purpose of providing additional experimentalresults.
Reviews in Dataset B were randomly cho-sen from Pang et al?s pool of 27886 unprocessedmovie reviews (see Section 3) that have either apositive or a negative rating.
We followed exactlyPang et al?s guideline when determining whethera review is positive or negative.14 Also, we tookcare to ensure that reviews included in Dataset Bdo not appear in Dataset A.
We applied to these re-views the same four pre-processing steps that wedid to the neutral reviews in the previous section.4.2 ResultsThe baseline classifier.
We can now train ourbaseline polarity classifier on each of the two11www.barnesandnoble.com12www.amazon.com13We also experimented with polynomial and RBF kernelswhen training polarity classifiers, but neither yields better re-sults than linear kernels.14The guidelines come with their polarity dataset.
Briefly,a positive review has a rating of ?
3.5 (out of 5) or ?
3 (outof 4), whereas a negative review has a rating of ?
2 (out of 5)or ?
1.5 (out of 4).614System Variation Dataset A Dataset BBaseline 87.1 82.7Adding bigrams 89.2 84.7and trigramsAdding dependency 89.0 84.5relationsAdding polarity 90.4 86.2info of adjectivesDiscarding objective 90.5 86.1materialsTable 1: Polarity classification accuracies.datasets.
Our baseline classifier employs as fea-tures the k highest-ranking unigrams according toWLLR, with k/2 features selected from each class.Results with k = 10000 are shown in row 1 of Ta-ble 1.15 As we can see, the baseline achieves anaccuracy of 87.1% and 82.7% on Datasets A andB, respectively.
Note that our result on DatasetA is as strong as that obtained by Pang and Lee(2004) via their subjectivity summarization algo-rithm, which retains only the subjective portionsof a document.As a sanity check, we duplicated Pang et al?s(2002) baseline in which all unigrams that appearfour or more times in the training documents areused as features.
The resulting classifier achievesan accuracy of 87.2% and 82.7% for Datasets Aand B, respectively.
Neither of these results aresignificantly different from our baseline results.16Adding higher-order n-grams.
The negativeresults that Pang et al (2002) obtained when us-ing bigrams as features for their polarity classi-fier seem to suggest that high-order n-grams arenot useful for polarity classification.
However, re-cent research in the related (but arguably simpler)task of text classification shows that a bigram-based text classifier outperforms its unigram-based counterpart (Peng et al, 2003).
Thisprompts us to re-examine the utility of high-ordern-grams in polarity classification.In our experiments we consider adding bigramsand trigrams to our baseline feature set.
However,since these higher-order n-grams significantly out-number the unigrams, adding all of them to thefeature set will dramatically increase the dimen-15We experimented with several values of k and obtainedthe best result with k = 10000.16We use two-tailed paired t-tests when performing signif-icance testing, with p set to 0.05 unless otherwise stated.sionality of the feature space and may underminethe impact of the unigrams in the resulting clas-sifier.
To avoid this potential problem, we keepthe number of unigrams and higher-order n-gramsequal.
Specifically, we augment the baseline fea-ture set (consisting of 10000 unigrams) with 5000bigrams and 5000 trigrams.
The bigrams and tri-grams are selected based on their WLLR com-puted over the positive reviews and negative re-views in the training set for each CV run.Results using this augmented feature set areshown in row 2 of Table 1.
We see that accu-racy rises significantly from 87.1% to 89.2% forDataset A and from 82.7% to 84.7% for Dataset B.This provides evidence that polarity classificationcan indeed benefit from higher-order n-grams.Adding dependency relations.
While bigramsand trigrams are good at capturing local dependen-cies, dependency relations can be used to capturenon-local dependencies among the constituents ofa sentence.
Hence, we hypothesized that our n-gram-based polarity classifier would benefit fromthe addition of dependency-based features.Unlike most previous work on polarity classi-fication, which has largely focused on exploitingadjective-noun (AN) relations (e.g., Dave et al(2003), Popescu and Etzioni (2005)), we hypothe-sized that subject-verb (SV) and verb-object (VO)relations would also be useful for the task.
Thefollowing (one-sentence) review illustrates why.While I really like the actors, the plot israther uninteresting.A unigram-based polarity classifier could be con-fused by the simultaneous presence of the posi-tive term like and the negative term uninterestingwhen classifying this review.
However, incorpo-rating the VO relation (like, actors) as a featuremay allow the learner to learn that the author likesthe actors and not necessarily the movie.In our experiments, the SV, VO and AN re-lations are extracted from each document by theMINIPAR dependency parser (Lin, 1998).
Aswith n-grams, instead of using all the SV, VO andAN relations as features, we select among themthe best 5000 according to their WLLR and re-train the polarity classifier with our n-gram-basedfeature set augmented by these 5000 dependency-based features.
Results in row 3 of Table 1 aresomewhat surprising: the addition of dependency-based features does not offer any improvementsover the simple n-gram-based classifier.615Incorporating manually tagged term polarity.Next, we consider incorporating a set of featuresthat are computed based on the polarity of adjec-tives.
As noted before, we desire a high-precision,high-coverage lexicon.
So, instead of exploiting alearned lexicon, we manually develop one.To construct the lexicon, we take Pang et al?spool of unprocessed documents (see Section 3),remove those that appear in either Dataset A orDataset B17, and compile a list of adjectives fromthe remaining documents.
Then, based on heuris-tics proposed in psycholinguistics18 , we hand-annotate each adjective with its prior polarity (i.e.,polarity in the absence of context).
Out of the45592 adjectives we collected, 3599 were labeledas positive, 3204 as negative, and 38789 as neu-tral.
A closer look at these adjectives reveals thatthey are by no means domain-dependent despitethe fact that they were taken from movie reviews.Now let us consider a simple procedure P forderiving a feature set that incorporates informationfrom our lexicon: (1) collect all the bigrams fromthe training set; (2) for each bigram that contains atleast one adjective labeled as positive or negativeaccording to our lexicon, create a new feature thatis identical to the bigram except that each adjec-tive is replaced with its polarity label19; (3) mergethe list of newly generated features with the listof bigrams20 and select the top 5000 features fromthe merged list according to their WLLR.We then repeat procedure P for the trigramsand also the dependency features, resulting in atotal of 15000 features.
Our new feature set com-prises these 15000 features as well as the 10000unigrams we used in the previous experiments.Results of the polarity classifier that incorpo-rates term polarity information are encouraging(see row 4 of Table 1).
In comparison to the classi-fier that uses only n-grams and dependency-basedfeatures (row 3), accuracy increases significantly(p = .1) from 89.2% to 90.4% for Dataset A, andfrom 84.7% to 86.2% for Dataset B.
These resultssuggest that the classifier has benefited from the17We treat the test documents as unseen data that shouldnot be accessed for any purpose during system development.18http://www.sci.sdsu.edu/CAL/wordlist19Neutral adjectives are not replaced.20A newly generated feature could be misleading for thelearner if the contextual polarity (i.e., polarity in the presenceof context) of the adjective involved differs from its prior po-larity (see Wilson et al (2005)).
The motivation behind merg-ing with the bigrams is to create a feature set that is morerobust in the face of potentially misleading generalizations.use of features that are less sparse than n-grams.Using objective information.
Some of the25000 features we generated above correspond ton-grams or dependency relations that do not con-tain subjective information.
We hypothesized thatnot employing these ?objective?
features in thefeature set would improve system performance.More specifically, our goal is to use procedure Pagain to generate 25000 ?subjective?
features byensuring that the objective ones are not selectedfor incorporation into our feature set.To achieve this goal, we first use the followingrote-learning procedure to identify objective ma-terial: (1) extract all unigrams that appear in ob-jective documents, which in our case are the 2000non-reviews used in review identification [see Sec-tion 3]; (2) from these ?objective?
unigrams, wetake the best 20000 according to their WLLR com-puted over the non-reviews and the reviews in thetraining set for each CV run; (3) repeat steps 1 and2 separately for bigrams, trigrams and dependencyrelations; (4) merge these four lists to create our80000-element list of objective material.Now, we can employ procedure P to get a list of25000 ?subjective?
features by ensuring that thosethat appear in our 80000-element list are not se-lected for incorporation into our feature set.Results of our classifier trained using these sub-jective features are shown in row 5 of Table 1.Somewhat surprisingly, in comparison to row 4,we see that our method for filtering objective fea-tures does not help improve performance on thetwo datasets.
We will examine the reasons in thefollowing subsection.4.3 Discussion and Further AnalysisUsing the four types of knowledge sources pre-viously described, our polarity classifier signifi-cantly outperforms a unigram-based baseline clas-sifier.
In this subsection, we analyze some of theseresults and conduct additional experiments in anattempt to gain further insight into the polarityclassification task.
Due to space limitations, wewill simply present results on Dataset A below,and show results on Dataset B only in cases wherea different trend is observed.The role of feature selection.
In all of our ex-periments we used the best k features obtained viaWLLR.
An interesting question is: how will theseresults change if we do not perform feature selec-tion?
To investigate this question, we conduct two616experiments.
First, we train a polarity classifier us-ing all unigrams from the training set.
Second, wetrain another polarity classifier using all unigrams,bigrams, and trigrams.
We obtain an accuracy of87.2% and 79.5% for the first and second experi-ments, respectively.In comparison to our baseline classifier, whichachieves an accuracy of 87.1%, we can see thatusing all unigrams does not hurt performance, butperformance drops abruptly with the addition ofall bigrams and trigrams.
These results suggestthat feature selection is critical when bigrams andtrigrams are used in conjunction with unigrams fortraining a polarity classifier.The role of bigrams and trigrams.
So far wehave seen that training a polarity classifier usingonly unigrams gives us reasonably good, thoughnot outstanding, results.
Our question, then, is:would bigrams alone do a better job at capturingthe sentiment of a document than unigrams?
Toanswer this question, we train a classifier using allbigrams (without feature selection) and obtain anaccuracy of 83.6%, which is significantly worsethan that of a unigram-only classifier.
Similar re-sults were also obtained by Pang et al (2002).It is possible that the worse result is due to thepresence of a large number of irrelevant bigrams.To test this hypothesis, we repeat the above exper-iment except that we only use the best 10000 bi-grams selected according to WLLR.
Interestingly,the resulting classifier gives us a lower accuracyof 82.3%, suggesting that the poor accuracy is notdue to the presence of irrelevant bigrams.To understand why using bigrams alone doesnot yield a good classification model, we examinea number of test documents and find that the fea-ture vectors corresponding to some of these docu-ments (particularly the short ones) have all zeroesin them.
In other words, none of the bigrams fromthe training set appears in these reviews.
This sug-gests that the main problem with the bigram modelis likely to be data sparseness.
Additional experi-ments show that the trigram-only classifier yieldseven worse results than the bigram-only classifier,probably because of the same reason.Nevertheless, these higher-order n-grams play anon-trivial role in polarity classification: we haveshown that the addition of bigrams and trigramsselected via WLLR to a unigram-based classifiersignificantly improves its performance.The role of dependency relations.
In the previ-ous subsection we see that dependency relationsdo not contribute to overall performance on topof bigrams and trigrams.
There are two plausi-ble reasons.
First, dependency relations are simplynot useful for polarity classification.
Second, thehigher-order n-grams and the dependency-basedfeatures capture essentially the same informationand so using either of them would be sufficient.To test the first hypothesis, we train a clas-sifier using only 10000 unigrams and 10000dependency-based features (both selected accord-ing to WLLR).
For Dataset A, the classifierachieves an accuracy of 87.1%, which is statis-tically indistinguishable from our baseline result.On the other hand, the accuracy for Dataset B is83.5%, which is significantly better than the cor-responding baseline (82.7%) at the p = .1 level.These results indicate that dependency informa-tion is somewhat useful for the task when bigramsand trigrams are not used.
So the first hypothesisis not entirely true.So, it seems to be the case that the dependencyrelations do not provide useful knowledge for po-larity classification only in the presence of bigramsand trigrams.
This is somewhat surprising, sincethese n-grams do not capture the non-local depen-dencies (such as those that may be present in cer-tain SV or VO relations) that should intuitively beuseful for polarity classification.To better understand this issue, we again exam-ine a number of test documents.
Our initial in-vestigation suggests that the problem might havestemmed from the fact that MINIPAR returns de-pendency relations in which all the verb inflectionsare removed.
For instance, given the sentence Mycousin Paul really likes this long movie, MINIPARwill return the VO relation (like, movie).
To seewhy this can be a problem, consider another sen-tence I like this long movie.
From this sentence,MINIPAR will also extract the VO relation (like,movie).
Hence, this same VO relation is cap-turing two different situations, one in which theauthor himself likes the movie, and in the other,the author?s cousin likes the movie.
The over-generalization resulting from these ?stemmed?
re-lations renders dependency information not usefulfor polarity classification.
Additional experimentsare needed to determine the role of dependency re-lations when stemming in MINIPAR is disabled.617The role of objective information.
Resultsfrom the previous subsection suggest that ourmethod for extracting objective materials and re-moving them from the reviews is not effective interms of improving performance.
To determine thereason, we examine the n-grams and the depen-dency relations that are extracted from the non-reviews.
We find that only in a few cases do theseextracted objective materials appear in our set of25000 features obtained in Section 4.2.
This ex-plains why our method is not as effective as weoriginally thought.
We conjecture that more so-phisticated methods would be needed in order totake advantage of objective information in polar-ity classification (e.g., Koppel and Schler (2005)).5 ConclusionsWe have examined two problems in document-level sentiment analysis, namely, review identifi-cation and polarity classification.
We first foundthat review identification can be achieved withvery high accuracies (97-99%) simply by trainingan SVM classifier using unigrams as features.
Wethen examined the role of several linguistic knowl-edge sources in polarity classification.
Our re-sults suggested that bigrams and trigrams selectedaccording to the weighted log-likelihood ratio aswell as manually tagged term polarity informa-tion are very useful features for the task.
On theother hand, no further performance gains are ob-tained by incorporating dependency-based infor-mation or filtering objective materials from the re-views using our proposed method.
Nevertheless,the resulting polarity classifier compares favorablyto state-of-the-art sentiment classification systems.ReferencesC.
Cardie, J. Wiebe, T. Wilson, and D. Litman.
2004.
Low-level annotations and summary representations of opin-ions for multi-perspective question answering.
In New Di-rections in Question Answering.
AAAI Press/MIT Press.K.
Dave, S. Lawrence, and D. M. Pennock.
2003.
Miningthe peanut gallery: Opinion extraction and semantic clas-sification of product reviews.
In Proc.
of WWW, pages519?528.A.
Esuli and F. Sebastiani.
2005.
Determining the semanticorientation of terms through gloss classification.
In Proc.of CIKM, pages 617?624.M.
Gamon, A. Aue, S. Corston-Oliver, and E. K. Ringger.2005.
Pulse: Mining customer opinions from free text.In Proc.
of the 6th International Symposium on IntelligentData Analysis, pages 121?132.V.
Hatzivassiloglou and K. McKeown.
1997.
Predictingthe semantic orientation of adjectives.
In Proc.
of theACL/EACL, pages 174?181.M.
Hu and B. Liu.
2004.
Mining and summarizing customerreviews.
In Proc.
of KDD, pages 168?177.T.
Joachims.
1999.
Making large-scale SVM learning prac-tical.
In Advances in Kernel Methods - Support VectorLearning, pages 44?56.
MIT Press.S.-M. Kim and E. Hovy.
2004.
Determining the sentiment ofopinions.
In Proc.
of COLING, pages 1367?1373.M.
Koppel and J. Schler.
2005.
Using neutral examples forlearning polarity.
In Proc.
of IJCAI (poster).D.
Lin.
1998.
Dependency-based evaluation of MINIPAR.In Proc.
of the LREC Workshop on the Evaluation of Pars-ing Systems, pages 48?56.H.
Liu, H. Lieberman, and T. Selker.
2003.
A model of tex-tual affect sensing using real-world knowledge.
In Proc.of Intelligent User Interfaces (IUI), pages 125?132.S.
Morinaga, K. Yamanishi, K. Tateishi, and T. Fukushima.2002.
Mining product reputations on the web.
In Proc.
ofKDD, pages 341?349.T.
Mullen and N. Collier.
2004.
Sentiment analysis usingsupport vector machines with diverse information sources.In Proc.
of EMNLP, pages 412?418.K.
Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000.Text classification from labeled and unlabeled documentsusing EM.
Machine Learning, 39(2/3):103?134.B.
Pang and L. Lee.
2004.
A sentimental education: Senti-ment analysis using subjectivity summarization based onminimum cuts.
In Proc.
of the ACL, pages 271?278.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machine learning tech-niques.
In Proc.
of EMNLP, pages 79?86.F.
Peng, D. Schuurmans, and S. Wang.
2003.
Language andtask independent text categorization with simple languagemodels.
In HLT/NAACL: Main Proc.
, pages 189?196.A.-M. Popescu and O. Etzioni.
2005.
Extracting productfeatures and opinions from reviews.
In Proc.
of HLT-EMNLP, pages 339?346.E.
Riloff, J. Wiebe, and W. Phillips.
2005.
Exploiting sub-jectivity classification to improve information extraction.In Proc.
of AAAI, pages 1106?1111.P.
Turney.
2002.
Thumbs up or thumbs down?
Semantic ori-entation applied to unsupervised classification of reviews.In Proc.
of the ACL, pages 417?424.C.
Whitelaw, N. Garg, and S. Argamon.
2005.
Using ap-praisal groups for sentiment analysis.
In Proc.
of CIKM,pages 625?631.J.
M. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin.2004.
Learning subjective language.
Computational Lin-guistics, 30(3):277?308.T.
Wilson, J. M. Wiebe, and P. Hoffmann.
2005.
Recogniz-ing contextual polarity in phrase-level sentiment analysis.In Proc.
of EMNLP, pages 347?354.Y.
Yang and J. O. Pedersen.
1997.
A comparative study onfeature selection in text categorization.
In Proc.
of ICML,pages 412?420.J.
Yi, T. Nasukawa, R. Bunescu, and W. Niblack.
2003.Sentiment analyzer: Extracting sentiments about a giventopic using natural language processing techniques.
InProc.
of the IEEE International Conference on Data Min-ing (ICDM).H.
Yu and V. Hatzivassiloglou.
2003.
Towards answer-ing opinion questions: Separating facts from opinions andidentifying the polarity of opinion sentences.
In Proc.
ofEMNLP, pages 129?136.618
