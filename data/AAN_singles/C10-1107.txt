Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 949?957,Beijing, August 2010Bringing Active Learning to LifeInes RehbeinComputational LinguisticsSaarland UniversityJosef RuppenhoferComputational LinguisticsSaarland University{rehbein|josefr|apalmer}@coli.uni-sb.deAlexis PalmerComputational LinguisticsSaarland UniversityAbstractActive learning has been applied to dif-ferent NLP tasks, with the aim of limit-ing the amount of time and cost for humanannotation.
Most studies on active learn-ing have only simulated the annotationscenario, using prelabelled gold standarddata.
We present the first active learningexperiment for Word Sense Disambigua-tion with human annotators in a realisticenvironment, using fine-grained sense dis-tinctions, and investigate whether AL canreduce annotation cost and boost classifierperformance when applied to a real-worldtask.1 IntroductionActive learning has recently attracted attention ashaving the potential to overcome the knowledgeacquisition bottleneck by limiting the amount ofhuman annotation needed to create training datafor statistical classifiers.
Active learning has beenshown, for a number of different NLP tasks, to re-duce the number of manually annotated instancesneeded for obtaining a consistent classifier perfor-mance (Hwa, 2004; Chen et al, 2006; Tomanek etal., 2007; Reichart et al, 2008).The majority of such results have been achievedby simulating the annotation scenario using prela-belled gold standard annotations as a stand-in forreal-time human annotation.
Simulating annota-tion allows one to test different parameter set-tings without incurring the cost of human anno-tation.
There is, however, a major drawback: wedo not know whether the results of experimentsperformed using hand-corrected data carry over toreal-world scenarios in which individual humanannotators produce noisy annotations.
In addi-tion, we do not know to what extent error-proneannotations mislead the learning process.
A sys-tematic study of the impact of erroneous annota-tion on classifier performance in an active learn-ing (AL) setting is overdue.
We need to know a)whether the AL approach can really improve clas-sifier performance and save annotation time whenapplied in a real-world scenario with noisy data,and b) whether AL works for classification taskswith fine-grained or complex annotation schemesand a low inter-annotator agreement.In this paper we bring active learning to life inthe context of frame semantic annotation of Ger-man texts within the SALSA project (Burchardtet al, 2006).
Specifically, we apply AL methodsfor learning to assign semantic frames to predi-cates, following Erk (2005) in treating frame as-signment as a Word Sense Disambiguation task.Under our fine-grained annotation scheme, anno-tators have to deal with a high level of ambigu-ity, resulting in low inter-annotator agreement forsome word senses.
This fact, along with the po-tential for wrong annotation decisions or possi-ble biases from individual annotators, results inan annotation environment in which we get noisydata which might mislead the classifier.
A sec-ond characteristic of our scenario is that there is nogold standard for the newly annotated data, whichmeans that evaluation is not straightforward.
Fi-nally, we have multiple annotators whose deci-949sions on particular instances may diverge, raisingthe question of which annotations should be usedto guide the AL process.
This paper thus investi-gates whether active learning can be successfullyapplied in a real-world scenario with the particularchallenges described above.Section 2 of the paper gives a short overviewof the AL paradigm and some related work, andSection 3 discusses the multi-annotator scenario.In Section 4 we present our experimental designand describe the data we use.
Section 5 presentsresults, and Section 6 concludes.2 Active LearningThe active learning approach aims to reduce theamount of manual annotation needed to createtraining data sufficient for developing a classifierwith a given performance.
At each iteration ofthe AL cycle, the actual knowledge state of thelearner guides the learning process by determin-ing which instances are chosen next for annota-tion.
The main goal is to advance the learningprocess by selecting instances which provide im-portant information for the machine learner.In a typical active learning scenario, a small setof manually labelled seed data serves as the ini-tial training set for the classifier (learner).
Basedon the predictions of the classifier, a large poolof unannotated instances is queried for the nextinstance (or batch of instances) to be presentedto the human annotator (sometimes called the or-acle).
The underlying active learning algorithmcontrolling the learning process tries to select themost informative instances in order to get a strongboost in classifier performance.
Different meth-ods can be used for determining informativity ofinstances.
We use uncertainty sampling (Cohn etal., 1995) in which ?most informative?
instancesare those for which the classifier has the lowestconfidence in its label predictions.
The rough in-tuition behind this selection method is that it iden-tifies instance types which have yet to be encoun-tered by the classifier.
The learning process pro-ceeds by presenting the selected instances to thehuman annotator, who assigns the correct label.The newly-annotated instances are added to theseed data and the classifier is re-trained on the newdata set.
The newly trained classifier now picksthe next instances, based on its updated knowl-edge, and the process repeats.
If the learning pro-cess can provide precisely that information whichthe classifier still needs to learn, a smaller numberof instances should suffice to achieve the same ac-curacy as on a larger training set of randomly se-lected training examples.Active learning has been applied to a num-ber of natural language processing tasks likePOS tagging (Ringger et al, 2007), NER (Lawsand Schu?tze, 2008; Tomanek and Hahn, 2009),syntactic parsing (Osborne and Baldridge, 2004;Hwa, 2004), Word Sense Disambiguation (Chenet al, 2006; Chan and Ng, 2007; Zhu and Hovy,2007; Zhu et al, 2008) and morpheme gloss-ing for language documentation (Baldridge andPalmer, 2009).
While most of these studies suc-cessfully show that the same classification accu-racy can be achieved with a substantially smallerdata set, these findings are mostly based on simu-lations using gold standard data.For our task of Word Sense Disambiguation(WSD), mixed results have been achieved.
ALseems to improve results in a WSD task withcoarse-grained sense distinctions (Chan and Ng,2007), but the results of (Dang, 2004) raise doubtsas to whether AL can successfully be applied toa fine-grained annotation scheme, where Inter-Annotator Agreement (IAA) is low and thus theconsistency of the human annotations decreases.In general, AL has been shown to reduce the costof annotation when applied to classification taskswhere a single human annotator predicts labels fornew data points with a reasonable consistency andaccuracy.
It is not clear whether the same settingscan be applied to a multi-annotator environmentwhere IAA is low.3 Active Learning in a realistic taskincluding multiple annotatorsAnother possible difference between active learn-ing simulations and real-world scenarios is themulti-annotator environment.
In such a setting,two or more annotators assign labels to the sameinstances, which are then merged to check for con-flicting decisions from different annotators.
Thisis standard practise in many annotation projectsdoing fine-grained semantic annotation with a950high level of ambiguity, and it necessitates that allannotators work on the same data set.Replicating an active learning simulation onhand-corrected data, starting with a fixed set ofseed data and fixed parameter settings, using thesame algorithm, will always result in the sametraining set selected from the pool.
Human anno-tators, however, will assign different labels to thesame instances, thus influencing the selection ofthe next instance from the pool.
This means thatindividual annotators might end up with very dif-ferent sets of annotated data, depending on factorslike their interpretation of the annotation guide-lines, an implicit bias towards a particular label,or simply errors made during annotation.There is not much work addressing this prob-lem.
(Donmez and Carbonell, 2008) considermodifications of active learning to accommodatevariability of annotators.
(Baldridge and Palmer,2009) present a real-world study with human an-notators in the context of language documenta-tion.
The task consists of producing interlin-ear glossed text, including morphological andgrammatical analysis, and can be described asa sequence labelling task.
Annotation cost ismeasured as the actual time needed for annota-tion.
Among other settings, the authors comparethe performance of two annotators with differentgrades of expertise.
The classifier trained on thedata set created by the expert annotator in an ac-tive learning setting does obtain a higher accuracyon the gold standard.
For the non-expert annota-tor, however, the active learning setting resultedin a lower accuracy than for a classifier trained ona randomly selected data set.
This finding sug-gests that the quality of annotation needs to behigh enough for active learning to actually work,and that annotation noise is a problem for AL.There are two problems arising from this:1.
It is not clear whether active learning willwork when applied to noisy data2.
It is not straightforward to apply active learn-ing to a real-world scenario, where low IAAasks for multiple annotatorsIn our experiment we address these questionsby systematically investigating the impact of an-notation noise on classifier performance and onthe composition of the training set.
The next sec-tion presents the experimental design and the dataused in our experiment.4 Experimental DesignIn the experiment we annotated 8 German cau-sation nouns, namely Ausgang, Anlass, Ergeb-nis, Resultat, Grund, Konsequenz, Motiv, Quelle(outcome, occasion, effect, result, reason, con-sequence, motive, source of experience).
Thesenouns were chosen because they exhibit a rangeof difficulty in terms of the number of senses theyhave in our annotation scheme.
They all encodesubtle distinctions between different word senses,but some of them are clearly easier to disam-biguate than others.
For instance, although Aus-gang has 9 senses, they are easier to distinguishfor humans than the 4 senses of Konsequenz.Six annotators participated in the experiment.While all annotators were trained, having at leastone year experience in frame-semantic annota-tion, one of the annotators is an expert with severalyears of training and working experience in theBerkeley FrameNet Project.
This annotator alsodefined the frames (word senses) used in our ex-periment.Prior to the experiment, all annotators weregiven 100 randomly chosen sentences.
Afterannotating the training data, problematic caseswere discussed to make sure that the annotatorswere familiar with the fine-grained distinctionsbetween word senses in the annotation scheme.The data sets used for training were adjudicatedby two of the annotators (one of them being theexpert) and then used as a gold standard to testclassifier performance in the active learning pro-cess.4.1 Data and SetupFor each lemma we extracted sentences from theWahrig corpus1 containing this particular lemma.The annotators had to assign word senses to 300instances for each target word, split into 6 pack-ages of 50 sentences each.
This resulted in 2,400annotated instances per annotator (14,400 anno-tated instances in total).
The annotation was done1The Wahrig corpus includes more than 113 mio.
sen-tences from German newspapers and magazines coveringtopics such as politics, science, fashion, and others.951Anlass Motiv Konsequenz Quelle Ergebnis / Resultat Ausgang GrundOccasion (37) Motif (47) Causation (32) Relational nat feat.
(3) Causation (4/10) Outcome (67) Causation (24)Reason (63) Reason(53) Level of det.
(6) Source of getting (14) Competitive score(12/36) Have leave (4) Reason (58)Response (61) Source of exp.
(14) Decision (11/6) Portal (21) Death (1)MWE1 (1) Source of info.
(56) Efficacy (2/3) Outgoing goods (4) Part orientation.
(0)Well (6) Finding out (24/23) Ostomy (0) Locale by owner(3)Emissions source (7) Mathematics (1/0) Origin (5) Surface earth (0)Operating result (36/5) Tech output (7) Bottom layer (0)Outcome (10/17) Process end (2) Soil (1)Departing (1) CXN1 (0)CXN2 (0)MWE1 (0)MWE2 (10)MWE3 (0)MWE4 (3)MWE5 (0)MWE6 (0)Fleiss?
kappa for the 6 annotators for the 150 instances annotated in the random setting0.67 0.79 0.55 0.77 0.63 / 0.59 0.82 0.43Table 1: 8 causation nouns and their word senses (numbers in brackets give the distribution of wordsenses in the gold standard (100 sentences); CXN: constructions, MWE: multi-word expressions; notethat Ergebnis and Resultat are synonyms and therefore share the same set of frames.
)using a Graphical User Interface where the sen-tence was presented to the annotator, who couldchoose between all possible word senses listed inthe GUI.
The annotators could either select theframe by mouse click or use keyboard shortcuts.For each instance we recorded the time it tookthe annotator to assign an appropriate label.
Toease the reading process the target word was high-lighted.As we want to compare time requirementsneeded for annotating random samples and sen-tences selected by active learning, we had to con-trol for training effects which might speed up theannotation.
Therefore we changed the annotationsetting after each package, meaning that the firstannotator started with 50 sentences randomly se-lected from the pool, then annotated 50 sentencesselected by AL, followed by another 50 randomlychosen sentences, and so on.
We divided the an-notators into two groups of three annotators each.The first group started annotating in the randomsetting, the second group in the AL setting.
Thecomposition of the groups was changed for eachlemma, so that each annotator experienced all dif-ferent settings during the annotation process.
Theannotators were not aware of which setting theywere in.Pool data For the random setting we randomlyselected three sets of sentences from the Wahrigcorpus which were presented for annotation to allsix annotators.
This allows us to compare annota-tion time and inter-annotator agreement betweenthe annotators.
For the active learning setting werandomly selected three sets of 2000 sentenceseach, from which the classifier could pick new in-stances during the annotation process.
This meansthat for each trial the algorithm could select 50 in-stances out of a pool of 2000 sentences.
On anygiven AL trial each annotator uses the same poolas all the other annotators.
In an AL simulationwith fixed settings and gold standard labels thiswould result in the same subset of sentences se-lected by the classifier.
For our human annotators,however, due to different annotation decisions theresulting set of sentences is expected to differ.Sampling method Uncertainty sampling is astandard sampling method for AL where new in-stances are selected based on the confidence of theclassifier for predicting the appropriate label.
Dur-ing early stages of the learning process when theclassifier is trained on a very small seed data set,it is not beneficial to add the instances with thelowest classifier confidence.
Instead, we use a dy-namic version of uncertainty sampling (Rehbeinand Ruppenhofer, 2010), based on the confidenceof a maximum entropy classifier2, taking into ac-count how much the classifier has learned so far.In each iteration one new instance is selected fromthe pool and presented to the oracle.
After anno-tation the classifier is retrained on the new dataset.
The modified uncertainty sampling results ina more robust classifier performance during earlystages of the learning process.2http://maxent.sourceforge.net952Anlass Motiv Konsequenz Quelle Ergebnis Resultat Ausgang GrundR U R U R U R U R U R U R U R UA1 8.6 9.6 5.9 6.6 10.7 10.5 6.0 4.8 10.5 7.4 10.1 9.6 6.4 10.0 10.2 11.1A2 4.4 5.7 4.8 5.9 8.2 9.2 4.9 4.9 6.4 4.4 11.7 8.5 5.1 7.7 9.0 9.3A3 9.9 9.2 6.8 6.7 6.8 8.3 7.4 6.1 9.4 7.6 9.0 12.3 7.5 8.5 11.7 10.2A4 5.8 4.9 3.6 3.6 9.9 11.3 4.8 3.5 7.9 7.1 9.7 11.1 3.6 4.1 9.9 9.4A5 3.0 3.5 3.0 2.6 4.8 4.9 3.8 3.0 6.8 4.8 6.7 6.1 3.1 3.5 6.3 6.0A6 5.4 6.3 5.3 4.7 6.7 8.6 5.4 4.6 7.8 6.1 8.7 9.0 6.9 6.6 9.3 8.5?
6.2 6.5 4.9 5.0 7.8 8.8 5.4 4.5 8.1 6.2 9.3 9.4 5.4 6.7 9.4 9.1sl 25.8 27.8 27.8 26.0 24.2 25.8 24.9 26.5 25.7 25.2 29.0 35.9 25.5 27.9 26.8 29.7Table 2: Annotation time (sec/instance) per target/annotator/setting and average sentence length (sl)5 ResultsThe basic idea behind active learning is to se-lect the most informative instances for annotation.The intuition behind ?more informative?
is thatthese instances support the learning process, so wemight need fewer annotated instances to achievea comparable classifier performance, which coulddecrease the cost of annotation.
On the otherhand, ?more informative?
also means that theseinstances might be more difficult to annotate, so itis only fair to assume that they might need moretime for annotation, which increases annotationcost.
To answer the question of whether AL re-duces annotation cost or not we have to check a)how long it took the annotators to assign labelsto the AL samples compared to the randomly se-lected instances, and b) how many instances weneed to achieve the best (or a sufficient) perfor-mance in each setting.
Furthermore, we want toinvestigate the impact of active learning on thedistribution of the resulting training sets and studythe correlation between the performance of theclassifier trained on the annotated data and thesefactors: the difficulty of the annotation task (as-sessed by IAA), expertise and individual proper-ties of the annotators.5.1 Does AL speed up the annotation processwhen working with noisy data?Table 2 reports annotation times for each annota-tor and target for random sampling (R) and uncer-tainty sampling (U).
For 5 out of 8 targets the timeneeded for annotating in the AL setting (averagedover all annotators) was higher than for annotat-ing the random samples.
To investigate whetherthis might be due to the length of the sentencesin the samples, Table 2 shows the average sen-tence length for random samples and AL samplesfor each target lemma.
Overall, the sentences se-lected by the classifier during AL are longer (26.2vs.
28.1 token per sentence), and thus may takethe annotators more time to read.3 However, wecould not find a significant correlation (Spearmanrank correlation test) between sentence length andannotation time, nor between sentence length andclassifier confidence.The three target lemmas which took longer toannotate in the random setting are Ergebnis (re-sult), Grund (reason) and Quelle (source of expe-rience).
This observation cannot be explained bysentence length.
While sentence length for Ergeb-nis is nearly the same in both settings, for Grundand Quelle the sentences picked by the classi-fier in the AL setting are significantly longer andtherefore should have taken more time to anno-tate.
To understand the underlying reason for thiswe have to take a closer look at the distribution ofword senses in the data.5.2 Distribution of word senses in the dataIn the literature it has been stated that AL implic-itly alleviates the class imbalance problem by ex-tracting more balanced data sets, while randomsampling tends to preserve the sense distributionpresent in the data (Ertekin et al, 2007).
We couldnot replicate this finding when using noisy datato guide the learning process.
Table 3 shows thedistribution of word senses for the target lemmaErgebnis a) in the gold standard, b) in the randomsamples, and c) in the AL samples.The variance in the distribution of word sensesin the random samples and the gold standard can3The correlation between sentence length and annotationtime is not obvious, as the annotators only have to label onetarget in each sentence.
For ambiguous sentences, however,reading time may be longer, while for the clear cases we donot expect a strong effect.953ErgebnisFrame gold (%) R (%) U (%)Causation 4.0 4.8 3.7Outcome 10.0 17.8 10.5Finding out 24.0 26.2 8.2Efficacy 2.0 0.8 0.1Decision 11.0 5.1 3.2Mathematics 1.0 1.6 0.4Operating result 36.0 24.5 66.7Competitive score 12.0 19.2 7.2Table 3: Distribution of frames (word senses) forthe lemma Ergebnis in the gold standard (100 sen-tences), in the random samples (R) and AL sam-ples (U) (150 sentences each)be explained by low inter-annotator agreementcaused by the high level of ambiguity for the tar-get lemmas.
The frame distribution in the dataselected by uncertainty sampling, however, cru-cially deviates from those of the gold standardand the random samples.
A disproportionatelyhigh 66% of the instances selected by the classi-fier have been assigned the label Operating resultby the human annotators.
This is the more sur-prising as this frame is fairly easy for humans todistinguish.The classifier, however, proved to have seri-ous problems learning this particular word senseand thus repeatedly selected more instances of thisframe for annotation.
As a result, the distributionof word senses in the training set for the uncer-tainty samples is highly skewed, having a nega-tive effect on the overall classifier performance.The high percentage of instances of the ?easy-to-decide?
frame Operating result explains why theinstances for Ergebnis took less time to annotatein the AL setting.
Thus we can conclude that an-notating the same number of instances on averagetakes more time in the AL setting, and that thiseffect is not due to sentence length.5.3 What works, what doesn?t, and whyFor half of the target lemmas (Motiv, Konsequenz,Quelle, Ausgang), we did obtain best results inthe AL setting (Table 4).
For Ausgang and Mo-tiv AL gives a substantial boost in classifier per-formance of 5% and 7% accuracy, while the gainsfor Konsequenz and Quelle are somewhat smallerwith 2% and 1%, and for Grund the highest accu-racy was reached on both the AL and the randomRandom Uncertainty50 100 150 50 100 150Anlass 0.85 0.86 0.85 0.84 0.85 0.84Motiv 0.57 0.62 0.63 0.64 0.67 0.70Konseq.
0.55 0.59 0.60 0.61 0.62 0.62Quelle 0.56 0.53 0.54 0.52 0.52 0.57Ergebnis 0.39 0.42 0.41 0.39 0.37 0.38Resultat 0.31 0.35 0.37 0.32 0.34 0.34Ausgang 0.67 0.69 0.69 0.68 0.72 0.74Grund 0.48 0.47 0.47 0.47 0.44 0.48Table 4: Avg.
classifier performance (acc.)
overall annotators for the 8 target lemmas when train-ing on 50, 100 and 150 annotated instances forrandom samples and uncertainty samplessample.Figure 1 (top row) shows the learning curvesfor Resultat, our worst-performing lemma, for theclassifier trained on the manually annotated sam-ples for each individual annotator.
The solid blackline represents the majority baseline, obtained byassigning the most frequent word sense in the goldstandard to all instances.
For both random and ALsettings, results are only slightly above the base-line.
The curves for the AL setting show how erro-neous decisions can mislead the classifier, result-ing in classifier accuracy below the baseline fortwo of the annotators, while the learning curvesfor these two annotators on the random samplesshow the same trend as for the other 4 annotators.For Konsequenz (Figure 1, middle), the classi-fier trained on the AL samples yields results overthe baseline after around 25 iterations, while inthe random sampling setting it takes at least 100iterations to beat the baseline.
For Motiv (Figure1, bottom row), again we observe far higher re-sults in the AL setting.
A possible explanation forwhy AL seems to work for Ausgang, Motiv andQuelle might be the higher IAA4 (?
0.825, 0.789,0.768) as compared to the other target lemmas.This, however, does not explain the good resultsachieved on the AL samples for Konsequenz, forwhich IAA was quite low with ?
0.554.Also startling is the fact that AL seems to workparticularly well for one of the annotators (A6,Figure 1) but not for others.
Different possible ex-planations come to mind: (a) the accuracy of theannotations for this particular annotator, (b) the4IAA was computed on the random samples, as the ALsamples do not include the same instances.9540 50 100 1500.100.200.300.40Resultat (Random Sampling)no.
of iterationsaccuracyAnnotatorA1A2A3A4A5A60 50 100 1500.100.200.300.40Resultat (Uncertainty Sampling)no.
of iterationsaccuracy0 50 100 1500.30.40.50.60.7Konsequenz (Random Sampling)no.
of iterationsaccuracy0 50 100 1500.30.40.50.60.7Konsequenz (Uncertainty Sampling)no.
of iterationsaccuracy0 50 100 1500.500.600.70Motiv (Random Sampling)no.
of iterationsaccuracy0 50 100 1500.500.600.70Motiv (Uncertainty Sampling)no.
of iterationsaccuracyFigure 1: Active learning curves for Resultat, Konsequenz and Motiv (random sampling versus uncer-tainty sampling; the straight black line shows the majority baseline)955Konsequenz A1 A2 A3 A4 A5 A6human 0.80 0.72 0.89 0.73 0.89 0.76maxent 0.60 0.63 0.67 0.60 0.63 0.64Table 5: Acc.
for human annotators against theadjudicated random samples and for the classifierinstances selected by the classifier based on theannotation decisions of the individual annotators,and (c) the distribution of frames in the annotatedtraining sets for the different annotators.To test (a) we evaluated the annotated ran-dom samples for Konsequenz for each annotatoragainst the adjudicated gold standard.
Resultsshowed that there is no strong correlation betweenthe accuracy of the human annotations and theperformance of the classifier trained on these an-notations.
The annotator for whom AL workedbest had a medium score of 0.76 only, while theannotator whose annotations were least helpfulfor the classifier showed a good accuracy of 0.80against the gold standard.Next we tested (b) the impact of the particu-lar instances in the AL samples for the individ-ual annotators on classifier performance.
We tookall instances in the AL data set from A6, whoseannotations gave the greatest boost to the clas-sifier, removed the frame labels and gave themto the remaining annotators for re-annotation.Then we trained the classifier on each of the re-annotated samples and compared classifier perfor-mance.
Results for 3 of the remaining annotatorswere in the same range or even higher than theones for A6 (Figure 2).
For 2 annotators, however,results remained far below the baseline.This again shows that the AL effect is not di-rectly dependent on the accuracy of the individualannotators, but that particular instances are moreinformative for the classifier than others.
Anothercrucial point is (c) the distribution of frames inthe samples.
In the annotated samples for A1 andA2 the majority frame for Konsequenz is Causa-tion, while in the samples for the other annotatorsResponse was more frequent.
In our test set Re-sponse also is the most frequent frame, therefore itis not surprising that the classifiers trained on thesamples of A3 to A6 show a higher performance.This means that high-quality annotations (identi-fied by IAA) do not necessarily provide the in-0 50 100 1500.30.40.50.60.7Konsequenz: Re?annotated samplesno.
of iterationsaccuracyAnnotatorA1A2A3A4A5A6Figure 2: Re-annotated instances for Konsequenz(AL samples from annotator A6)formation from which the classifier benefits most,and that in a realistic annotation task address-ing the class imbalance problem (Zhu and Hovy,2007) is crucial.6 ConclusionsWe presented the first experiment applying AL ina real-world scenario by integrating the approachin an ongoing annotation project.
The task andannotation environment pose specific challengesto the AL paradigm.
We showed that annotationnoise caused by biased annotators as well as erro-neous annotations mislead the classifier and resultin skewed data sets, and that for this particular taskno time savings are to be expected when appliedto a realistic scenario.
Under certain conditions,however, classifier performance can improve overthe random sampling baseline even on noisy dataand thus yield higher accuracy in the active learn-ing setting.
Critical features which seem to influ-cence the outcome of AL are the amount of noisein the data as well as the distribution of framesin training- and test sets.
Therefore, addressingthe class imbalance problem is crucial for apply-ing AL to a real annotation task.956AcknowledgmentsThis work was funded by the German ResearchFoundation DFG (grant PI 154/9-3 and the MMCICluster of Excellence).ReferencesBaldridge, Jason and Alexis Palmer.
2009.
How welldoes active learning actually work?
: Time-basedevaluation of cost-reduction strategies for languagedocumentation.
In Proceedings of EMNLP 2009.Burchardt, Aljoscha, Katrin Erk, Anette Frank, An-drea Kowalski, Sebastian Pado?, and Manfred Pinkal.2006.
The salsa corpus: a german corpus resourcefor lexical semantics.
In Proceedings of LREC-2006.Chan, Yee Seng and Hwee Tou Ng.
2007.
Domainadaptation with active learning for word sense dis-ambiguation.
In Proceedings of ACL-2007.Chen, Jinying, Andrew Schein, Lyle Ungar, andMartha Palmer.
2006.
An empirical study of thebehavior of active learning for word sense disam-biguation.
In Proceedings of NAACL-2006, NewYork, NY.Cohn, David A., Zoubin Ghahramani, and Michael I.Jordan.
1995.
Active learning with statistical mod-els.
In Tesauro, G., D. Touretzky, and T. Leen, ed-itors, Advances in Neural Information ProcessingSystems, volume 7, pages 705?712.
The MIT Press.Dang, Hoa Trang.
2004.
Investigations into the roleof lexical semantics in word sense disambiguation.PhD dissertation, University of Pennsylvania, Penn-sylvania, PA.Donmez, Pinar and Jaime G. Carbonell.
2008.
Proac-tive learning: Cost-sensitive active learning withmultiple imperfect oracles.
In Proceedings ofCIKM08.Erk, Katrin.
2005.
Frame assignment as word sensedisambiguation.
In Proceedings of the IWCS-6.Ertekin, S?eyda, Jian Huang, L?eon Bottou, and LeeGiles.
2007.
Learning on the border: active learn-ing in imbalanced data classification.
In Proceed-ings of CIKM ?07.Hwa, Rebecca.
2004.
Sample selection for statisti-cal parsing.
Computational Linguistics, 30(3):253?276.Laws, Florian and Heinrich Schu?tze.
2008.
Stoppingcriteria for active learning of named entity recogni-tion.
In Proceedings of Coling 2008.Osborne, Miles and Jason Baldridge.
2004.Ensemble-based active learning for parse selection.In Proceedings of HLT-NAACL 2004.Rehbein, Ines and Josef Ruppenhofer.
2010.
Theresno data like more data?
revisiting the impact ofdata size on a classification task.
In Proceedingsof LREC-07, 2010.Reichart, Roi, Katrin Tomanek, Udo Hahn, and AriRappoport.
2008.
Multi-task active learning forlinguistic annotations.
In Proceedings of ACL-08:HLT.Ringger, Eric, Peter Mcclanahan, Robbie Haertel,George Busby, Marc Carmen, James Carroll, andDeryle Lonsdale.
2007.
Active learning for part-of-speech tagging: Accelerating corpus annotation.In Proceedings of ACL Linguistic Annotation Work-shop.Tomanek, Katrin and Udo Hahn.
2009.
Reducingclass imbalance during active learning for namedentity annotation.
In Proceedings of the 5th Interna-tional Conference on Knowledge Capture, RedondoBeach, CA.Tomanek, Katrin, Joachim Wermter, and Udo Hahn.2007.
An approach to text corpus constructionwhich cuts annotation costs and maintains corpusreusability of annotated data.
In Proceedings ofEMNLP-CoNLL 2007.Zhu, Jingbo and Ed Hovy.
2007.
Active learningfor word sense disambiguation with methods for ad-dressing the class imbalance problem.
In Proceed-ings of EMNLP-CoNLL 2007.Zhu, Jingbo, Huizhen Wang, Tianshun Yao, and Ben-jamin K. Tsou.
2008.
Active learning with sam-pling by uncertainty and density for word sense dis-ambiguation and text classification.
In Proceedingsof Coling 2008.957
