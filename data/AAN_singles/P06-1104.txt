Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 825?832,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Composite Kernel to Extract Relations between Entities withboth Flat and Structured FeaturesMin Zhang         Jie Zhang       Jian Su      Guodong ZhouInstitute for Infocomm Research21 Heng Mui Keng Terrace, Singapore 119613{mzhang, zhangjie, sujian, zhougd}@i2r.a-star.edu.sgAbstractThis paper proposes a novel composite ker-nel for relation extraction.
The compositekernel consists of two individual kernels: anentity kernel that allows for entity-relatedfeatures and a convolution parse tree kernelthat models syntactic information of relationexamples.
The motivation of our method isto fully utilize the nice properties of kernelmethods to explore diverse knowledge forrelation extraction.
Our study illustrates thatthe composite kernel can effectively captureboth flat and structured features without theneed for extensive feature engineering, andcan also easily scale to include more fea-tures.
Evaluation on the ACE corpus showsthat our method outperforms the previousbest-reported methods and significantly out-performs previous two dependency tree ker-nels for relation extraction.1 IntroductionThe goal of relation extraction is to find variouspredefined semantic relations between pairs ofentities in text.
The research on relation extrac-tion has been promoted by the Message Under-standing Conferences (MUCs) (MUC, 1987-1998) and Automatic Content Extraction (ACE)program (ACE, 2002-2005).
According to theACE Program, an entity is an object or set of ob-jects in the world and a relation is an explicitlyor implicitly stated relationship among entities.For example, the sentence ?Bill Gates is chair-man and chief software architect of MicrosoftCorporation.?
conveys the ACE-style relation?EMPLOYMENT.exec?
between the entities?Bill Gates?
(PERSON.Name) and ?MicrosoftCorporation?
(ORGANIZATION.
Commercial).In this paper, we address the problem of rela-tion extraction using kernel methods (Sch?lkopfand Smola, 2001).
Many feature-based learningalgorithms involve only the dot-product betweenfeature vectors.
Kernel methods can be regardedas a generalization of the feature-based methodsby replacing the dot-product with a kernel func-tion between two vectors, or even between twoobjects.
A kernel function is a similarity functionsatisfying the properties of being symmetric andpositive-definite.
Recently, kernel methods areattracting more interests in the NLP study due totheir ability of implicitly exploring huge amountsof structured features using the original represen-tation of objects.
For example, the kernels forstructured natural language data, such as parsetree kernel (Collins and Duffy, 2001), string ker-nel (Lodhi et al, 2002) and graph kernel (Suzukiet al, 2003) are example instances of the well-known convolution kernels1 in NLP.
In relationextraction, typical work on kernel methods in-cludes: Zelenko et al (2003), Culotta and Soren-sen (2004) and Bunescu and Mooney (2005).This paper presents a novel composite kernelto explore diverse knowledge for relation extrac-tion.
The composite kernel consists of an entitykernel and a convolution parse tree kernel.
Ourstudy demonstrates that the composite kernel isvery effective for relation extraction.
It alsoshows without the need for extensive feature en-gineering the composite kernel can not only cap-ture most of the flat features used in the previouswork but also exploit the useful syntactic struc-ture features effectively.
An advantage of ourmethod is that the composite kernel can easilycover more knowledge by introducing more ker-nels.
Evaluation on the ACE corpus shows thatour method outperforms the previous best-reported methods and significantly outperformsthe previous kernel methods due to its effectiveexploration of various syntactic features.The rest of the paper is organized as follows.In Section 2, we review the previous work.
Sec-tion 3 discusses our composite kernel.
Section 4reports the experimental results and our observa-tions.
Section 5 compares our method with the1 Convolution kernels were proposed for a discrete structureby Haussler (1999) in the machine learning field.
Thisframework defines a kernel between input objects by apply-ing convolution ?sub-kernels?
that are the kernels for thedecompositions (parts) of the objects.825previous work from the viewpoint of feature ex-ploration.
We conclude our work and indicate thefuture work in Section 6.2 Related WorkMany techniques on relation extraction, such asrule-based (MUC, 1987-1998; Miller et al,2000), feature-based (Kambhatla 2004; Zhou etal., 2005) and kernel-based (Zelenko et al, 2003;Culotta and Sorensen, 2004; Bunescu andMooney, 2005), have been proposed in the litera-ture.Rule-based methods for this task employ anumber of linguistic rules to capture various rela-tion patterns.
Miller et al (2000) addressed thetask from the syntactic parsing viewpoint andintegrated various tasks such as POS tagging, NEtagging, syntactic parsing, template extractionand relation extraction using a generative model.Feature-based methods (Kambhatla, 2004;Zhou et al, 2005; Zhao and Grishman, 20052)for this task employ a large amount of diverselinguistic features, such as lexical, syntactic andsemantic features.
These methods are very effec-tive for relation extraction and show the best-reported performance on the ACE corpus.
How-ever, the problems are that these diverse featureshave to be manually calibrated and the hierarchi-cal structured information in a parse tree is notwell preserved in their parse tree-related features,which only represent simple flat path informa-tion connecting two entities in the parse treethrough a path of non-terminals and a list of basephrase chunks.Prior kernel-based methods for this task focuson using individual tree kernels to exploit treestructure-related features.
Zelenko et al (2003)developed a kernel over parse trees for relationextraction.
The kernel matches nodes from rootsto leaf nodes recursively layer by layer in a top-down manner.
Culotta and Sorensen (2004) gen-eralized it to estimate similarity between depend-ency trees.
Their tree kernels require the match-able nodes to be at the same layer counting fromthe root and to have an identical path of ascend-ing nodes from the roots to the current nodes.The two constraints make their kernel high preci-sion but very low recall on the ACE 2003 corpus.Bunescu and Mooney (2005) proposed anotherdependency tree kernel for relation extraction.2 We classify the feature-based kernel defined in (Zhao andGrishman, 2005) into the feature-based methods since theirkernels can be easily represented by the dot-products be-tween explicit feature vectors.Their kernel simply counts the number of com-mon word classes at each position in the shortestpaths between two entities in dependency trees.The kernel requires the two paths to have thesame length; otherwise the kernel value is zero.Therefore, although this kernel shows perform-ance improvement over the previous one (Culottaand Sorensen, 2004), the constraint makes thetwo dependency kernels share the similar behav-ior: good precision but much lower recall on theACE corpus.The above discussion shows that, althoughkernel methods can explore the huge amounts ofimplicit (structured) features, until now the fea-ture-based methods enjoy more success.
Onemay ask: how can we make full use of the niceproperties of kernel methods and define an effec-tive kernel for relation extraction?In this paper, we study how relation extractioncan benefit from the elegant properties of kernelmethods: 1) implicitly exploring (structured) fea-tures in a high dimensional space; and 2) the nicemathematical properties, for example, the sum,product, normalization and polynomial expan-sion of existing kernels is a valid kernel(Sch?lkopf and Smola, 2001).
We also demon-strate how our composite kernel effectively cap-tures the diverse knowledge for relation extrac-tion.3 Composite Kernel for Relation Ex-tractionIn this section, we define the composite kerneland study the effective representation of a rela-tion instance.3.1 Composite KernelOur composite kernel consists of an entity kerneland a convolution parse tree kernel.
To ourknowledge, convolution kernels have not beenexplored for relation extraction.
(1) Entity Kernel: The ACE 2003 data definesfour entity features: entity headword, entity typeand subtype (only for GPE), and mention typewhile the ACE 2004 data makes some modifica-tions and introduces a new feature ?LDC men-tion type?.
Our statistics on the ACE data revealsthat the entity features impose a strong constrainton relation types.
Therefore, we design a linearkernel to explicitly capture such features:1 2 1 21,2( , ) ( .
, .
)L E i iiK R R K R E R E== ?
(1)where 1R and 2R stands for two relation instances,Ei means the ith entity of a relation instance, and826( , )EK ?
?
is a simple kernel function over the fea-tures of entities:1 2 1 2( , ) ( .
, .
)E i iiK E E C E f E f=?
(2)where if represents the ith entity feature, and thefunction ( , )C ?
?
returns 1 if the two feature val-ues are identical and 0 otherwise.
( , )EK ?
?
re-turns the number of feature values in common oftwo entities.
(2) Convolution Parse Tree Kernel: A convo-lution kernel aims to capture structured informa-tion in terms of substructures.
Here we use thesame convolution parse tree kernel as describedin Collins and Duffy (2001) for syntactic parsingand Moschitti (2004) for semantic role labeling.Generally, we can represent a parse tree T  by avector of integer counts of each sub-tree type(regardless of its ancestors):( )T?
= (# subtree1(T), ?, # subtreei(T), ?,  #subtreen(T) )where # subtreei(T) is the occurrence number ofthe ith sub-tree type (subtreei) in T. Since thenumber of different sub-trees is exponential withthe parse tree size, it is computationally infeasi-ble to directly use the feature vector ( )T?
.
Tosolve this computational issue, Collins and Duffy(2001) proposed the following parse tree kernelto calculate the dot product between the abovehigh dimensional vectors implicitly.1 1 2 21 1 2 21 2 1 21 21 21 2( , ) ( ), ( )# ( ) # ( )( ) ( )( , )( ) ( )i ii iisubtree subtreei n N n Nn N n NK T T T Tsubtree T subtree TI n I nn n?
??
??
?=< >=== ?????
?
??
?
(3)where N1 and N2 are the sets of nodes in trees T1and T2, respectively, and ( )isubtreeI n  is a functionthat is 1 iff the subtreei occurs with root at node nand zero otherwise, and 1 2( , )n n?
is the number ofthe common subtrees rooted at n1 and n2, i.e.1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n?
= ?
?1 2( , )n n?
can be computed by the following recur-sive rules:(1) if the productions (CFP rules) at 1n  and 2nare different, 1 2( , ) 0n n?
= ;(2) else if both 1n  and 2n  are pre-terminals (POStags), 1 2( , ) 1n n ??
= ?
;(3) else, 1( )1 2 1 21( , ) (1 ( ( , ), ( , )))nc njn n ch n j ch n j?
=?
= +??
,where 1( )nc n is the child number of 1n , ch(n,j) isthe jth child of node n  and?
(0<?
<1) is the de-cay factor in order to make the kernel value lessvariable with respect to the subtree sizes.
In ad-dition, the recursive rule (3) holds because giventwo nodes with the same children, one can con-struct common sub-trees using these children andcommon sub-trees of further offspring.The parse tree kernel counts the number ofcommon sub-trees as the syntactic similaritymeasure between two relation instances.
Thetime complexity for computing this kernelis 1 2(| | | |)O N N?
.In this paper, two composite kernels are de-fined by combing the above two individual ker-nels in the following ways:1) Linear combination:1 1 2 1 2 1 2?
?
( , ) ( , ) (1 ) ( , )LK R R K R R K T T?
??
?= + ?
(4)Here, ?
( , )K ?
?
is the normalized3 ( , )K ?
?
and ?is the coefficient.
Evaluation on the developmentset shows that this composite kernel yields thebest performance when ?
is set to 0.4.2) Polynomial expansion:2 1 2 1 2 1 2?
?
( , ) ( , ) (1 ) ( , )PLK R R K R R K T T?
??
?= + ?
(5)Here, ?
( , )K ?
?
is the normalized ( , )K ?
?
, ( , )pK ?
?is the polynomial expansion of ( , )K ?
?
with de-gree d=2, i.e.
2( , ) ( ( , ) 1)pK K?
?
?
?= + , and ?
is thecoefficient.
Evaluation on the development setshows that this composite kernel yields the bestperformance when ?
is set to 0.23.The polynomial expansion aims to explore theentity bi-gram features, esp.
the combined fea-tures from the first and second entities, respec-tively.
In addition, due to the different scales ofthe values of the two individual kernels, they arenormalized before combination.
This can avoidone kernel value being overwhelmed by that ofanother one.The entity kernel formulated by eqn.
(1) is aproper kernel since it simply calculates the dotproduct of the entity feature vectors.
The treekernel formulated by eqn.
(3) is proven to be aproper kernel (Collins and Duffy, 2001).
Sincekernel function set is closed under normalization,polynomial expansion and linear combination(Sch?lkopf and Smola, 2001), the two compositekernels are also proper kernels.3  A kernel ( , )K x y  can be normalized by dividing it by( , ) ( , )K x x K y y?
.8273.2 Relation Instance SpacesA relation instance is encapsulated by a parsetree.
Thus, it is critical to understand which por-tion of a parse tree is important in the kernel cal-culation.
We study five cases as shown in Fig.1.
(1) Minimum Complete Tree (MCT): the com-plete sub-tree rooted by the nearest common an-cestor of the two entities under consideration.
(2) Path-enclosed Tree (PT): the smallest com-mon sub-tree including the two entities.
In otherwords, the sub-tree is enclosed by the shortestpath linking the two entities in the parse tree (thispath is also commonly-used as the path tree fea-ture in the feature-based methods).
(3) Context-Sensitive Path Tree (CPT): the PTextended with the 1st left word of entity 1 and the1st right word of entity 2.
(4) Flattened Path-enclosed Tree (FPT): thePT with the single in and out arcs of non-terminal nodes (except POS nodes) removed.
(5) Flattened CPT (FCPT): the CPT with thesingle in and out arcs of non-terminal nodes (ex-cept POS nodes) removed.Fig.
1 illustrates different representations of anexample relation instance.
T1 is MCT for therelation instance, where the sub-tree circled by adashed line is PT, which is also shown in T2 forclarity.
The only difference between MCT andPT lies in that MCT does not allow partial pro-duction rules (for example, NP?PP is a partialproduction rule while NP?NP+PP is an entireproduction rule in the top of T2).
For instance,only the most-right child in the most-left sub-tree[NP [CD 200] [JJ domestic] [E1-PER ?]]
of T1is kept in T2.
By comparing the performance ofT1 and T2, we can evaluate the effect of sub-treeswith partial production rules as shown in T2 andthe necessity of keeping the whole left and rightcontext sub-trees as shown in T1 in relation ex-traction.
T3 is CPT, where the two sub-trees cir-cled by dashed lines are included as the contextto T2 and make T3 context-sensitive.
This is toevaluate whether the limited context informationin CPT can boost performance.
FPT in T4 isformed by removing the two circled nodes in T2.This is to study whether and how the eliminationof single non-terminal nodes affects the perform-ance of relation extraction.T1): MCT T2): PTT3):CPT T4): FPTFigure 1.
Different representations of a relation instance in the example sentence ?
?provide bene-fits to 200 domestic partners of their own workers in New York?, where the phrase type?E1-PER?
denotes that the current node is the 1st entity with type ?PERSON?, and like-wise for the others.
The relation instance is excerpted from the ACE 2003 corpus, wherea relation ?SOCIAL.Other-Personal?
exists between entities ?partners?
(PER) and?workers?
(PER).
We use Charniak?s parser (Charniak, 2001) to parse the example sen-tence.
To save space, the FCPT is not shown here.
8284 Experiments4.1 Experimental SettingData: We use the English portion of both theACE 2003 and 2004 corpora from LDC in ourexperiments.
In the ACE 2003 data, the trainingset consists of 674 documents and 9683 relationinstances while the test set consists of 97 docu-ments and 1386 relation instances.
The ACE2003 data defines 5 entity types, 5 major relationtypes and 24 relation subtypes.
The ACE 2004data contains 451 documents and 5702 relationinstances.
It redefines 7 entity types, 7 major re-lation types and 23 subtypes.
Since Zhao andGrishman (2005) use a 5-fold cross-validation ona subset of the 2004 data (newswire and broad-cast news domains, containing 348 documentsand 4400 relation instances), for comparison, weuse the same setting (5-fold cross-validation onthe same subset of the 2004 data, but the 5 parti-tions may not be the same) for the ACE 2004data.
Both corpora are parsed using Charniak?sparser (Charniak, 2001).
We iterate over all pairsof entity mentions occurring in the same sen-tence to generate potential relation instances.
Inthis paper, we only measure the performance ofrelation extraction models on ?true?
mentionswith ?true?
chaining of coreference (i.e.
as anno-tated by LDC annotators).Implementation: We formalize relation extrac-tion as a multi-class classification problem.
SVMis selected as our classifier.
We adopt the one vs.others strategy and select the one with the largestmargin as the final answer.
The training parame-ters are chosen using cross-validation (C=2.4(SVM); ?
=0.4(tree kernel)).
In our implementa-tion, we use the binary SVMLight (Joachims,1998) and Tree Kernel Tools (Moschitti, 2004).Precision (P), Recall (R) and F-measure (F) areadopted to measure the performance.4.2 Experimental ResultsIn this subsection, we report the experiments ofdifferent kernel setups for different purposes.
(1) Tree Kernel only over Different RelationInstance Spaces: In order to better study the im-pact of the syntactic structure information in aparse tree on relation extraction, we remove theentity-related information from parse trees byreplacing the entity-related phrase types (?E1-PER?
and so on as shown in Fig.
1) with ?NP?.Table 1 compares the performance of 5 tree ker-nel setups on the ACE 2003 data using the treestructure information only.
It shows that:?
Overall the five different relation instancespaces are all somewhat effective for relationextraction.
This suggests that structured syntacticinformation has good predication power for rela-tion extraction and the structured syntactic in-formation can be well captured by the tree kernel.?
MCT performs much worse than the others.The reasons may be that MCT includes toomuch left and right context information, whichmay introduce many noisy features and causeover-fitting (high precision and very low recallas shown in Table 1).
This suggests that onlykeeping the complete (not partial) productionrules in MCT does harm performance.?
PT achieves the best performance.
This meansthat only keeping the portion of a parse tree en-closed by the shortest path between entities canmodel relations better than all others.
This maybe due to that most significant information iswith PT and including context information mayintroduce too much noise.
Although contextmay include some useful information, it is still aproblem to correctly utilize such useful informa-tion in the tree kernel for relation extraction.?
CPT performs a bit worse than PT.
In somecases (e.g.
in sentence ?the merge of company Aand company B?.
?, ?merge?
is a critical con-text word), the context information is helpful.However, the effective scope of context is hardto determine given the complexity and variabil-ity of natural languages.?
The two flattened trees perform worse than theoriginal trees.
This suggests that the single non-terminal nodes are useful for relation extraction.Evaluation on the ACE 2004 data also showsthat PT achieves the best performance (72.5/56.7/63.6 in P/R/F).
More evaluations with the entitytype and order information incorporated into treenodes (?E1-PER?, ?E2-PER?
and ?E-GPE?
asshown in Fig.
1) also show that PT performs bestwith 76.1/62.6/68.7 in P/R/F on the 2003 dataand 74.1/62.4/67.7 in P/R/F on the 2004 data.Instance Spaces P(%) R(%) FMinimum Complete Tree(MCT) 77.5 38.4 51.3Path-enclosed Tree (PT) 72.8 53.8 61.9Context-Sensitive PT(CPT) 75.9 48.6 59.2Flattened PT 72.7 51.7 60.4Flattened CPT 76.1 47.2 58.2Table 1. five different tree kernel setups on theACE 2003 five major types using the parsetree structure information only (regardless ofany entity-related information)829PTs (with Tree Struc-ture Information only)P(%) R(%) FEntity kernel only 75.1(79.5)42.7(34.6)54.4(48.2)Tree kernel only 72.5(72.8)56.7(53.8)63.6(61.9)Composite kernel 1(linear combination)73.5(76.3)67.0(63.0)70.1(69.1)Composite kernel 2(polynomial expansion)76.1(77.3)68.4(65.6)72.1(70.9)Table 2.
Performance comparison of differentkernel setups over the ACE major types ofboth the 2003 data (the numbers in parenthe-ses) and the 2004 data (the numbers outsideparentheses)(2) Composite Kernels: Table 2 compares theperformance of different kernel setups on theACE major types.
It clearly shows that:?
The composite kernels achieve significant per-formance improvement over the two individualkernels.
This indicates that the flat and the struc-tured features are complementary and the com-posite kernels can well integrate them: 1) theflat entity information captured by the entitykernel; 2) the structured syntactic connectioninformation between the two entities capturedby the tree kernel.?
The composite kernel via the polynomial ex-pansion outperforms the one via the linear com-bination by ~2 in F-measure.
It suggests that thebi-gram entity features are very useful.?
The entity features are quite useful, which canachieve F-measures of 54.4/48.2 alone and canboost the performance largely by ~7 (70.1-63.2/69.1-61.9) in F-measure when combiningwith the tree kernel.?
It is interesting that the ACE 2004 data showsconsistent better performance on all setups thanthe 2003 data although the ACE 2003 data istwo times larger than the ACE 2004 data.
Thismay be due to two reasons: 1) The ACE 2004data defines two new entity types and re-definesthe relation types and subtypes in order to re-duce the inconsistency between LDC annota-tors.
2) More importantly, the ACE 2004 datadefines 43 entity subtypes while there are only 3subtypes in the 2003 data.
The detailed classifi-cation in the 2004 data leads to significant per-formance improvement of 6.2 (54.4-48.2) in F-measure over that on the 2003 data.Our composite kernel can achieve77.3/65.6/70.9 and 76.1/68.4/72.1 in P/R/F overthe ACE 2003/2004 major types, respectively.Methods (2002/2003 data) P(%) R(%) FOurs: composite kernel 2(polynomial expansion)77.3(64.9)65.6(51.2)70.9(57.2)Zhou et al (2005):feature-based SVM77.2(63.1)60.7(49.5)68.0(55.5)Kambhatla (2004):feature-based ME(-)(63.5)(-)(45.2)(-)(52.8)Ours: tree kernel with en-tity information at node76.1(62.4)62.6(48.5)68.7(54.6)Bunescu and Mooney(2005): shortest path de-pendency kernel65.5(-)43.8(-)52.5(-)Culotta and Sorensen(2004): dependency kernel67.1(-)35.0(-)45.8(-)Table 3.
Performance comparison on the ACE2003/2003 data over both 5 major types (thenumbers outside parentheses) and 24 subtypes(the numbers in parentheses)Methods (2004 data) P(%) R(%) FOurs: composite kernel 2(polynomial expansion)76.1(68.6)68.4(59.3)72.1(63.6)Zhao and Grishman (2005):feature-based kernel69.2(-)70.5(-)70.4(-)Table 4.
Performance comparison on the ACE2004 data over both 7 major types (the numbersoutside parentheses) and 23 subtypes (the num-bers in parentheses)(3) Performance Comparison: Tables 3 and 4compare our method with previous work on theACE 2002/2003/2004 data, respectively.
Theyshow that our method outperforms the previousmethods and significantly outperforms the previ-ous two dependency kernels4.
This may be due totwo reasons: 1) the dependency tree (Culotta andSorensen, 2004) and the shortest path (Bunescuand Mooney, 2005) lack the internal hierarchicalphrase structure information, so their correspond-ing kernels can only carry out node-matchingdirectly over the nodes with word tokens; 2) theparse tree kernel has less constraints.
That is, it is4 Bunescu and Mooney (2005) used the ACE 2002 corpus,including 422 documents, which is known to have manyinconsistencies than the 2003 version.
Culotta and Sorensen(2004) used a generic ACE corpus including about 800documents (no corpus version is specified).
Since the testingcorpora are in different sizes and versions, strictly speaking,it is not ready to compare these methods exactly and fairly.Therefore Table 3 is only for reference purpose.
We justhope that we can get a few clues from this table.830not restricted by the two constraints of the twodependency kernels (identical layer and ances-tors for the matchable nodes and identical lengthof two shortest paths, as discussed in Section 2).The above experiments verify the effective-ness of our composite kernels for relation extrac-tion.
They suggest that the parse tree kernel caneffectively explore the syntactic features whichare critical for relation extraction.# of error instances Error Type2004 data 2003 dataFalse Negative 198  416False Positive 115 171Cross Type 62 96Table 5.
Error distribution of major types onboth the 2003 and 2004 data for the compos-ite kernel by polynomial expansion(4) Error Analysis: Table 5 reports the errordistribution of the polynomial composite kernelover the major types on the ACE data.
It showsthat 83.5%(198+115/198+115+62) / 85.8%(416+171/416+171+96) of the errors result from rela-tion detection and only 16.5%/14.2% of the er-rors result from relation characterization.
Thismay be due to data imbalance and sparsenessissues since we find that the negative samples are8 times more than the positive samples in thetraining set.
Nevertheless, it clearly directs ourfuture work.5 DiscussionIn this section, we compare our method with theprevious work from the feature engineeringviewpoint and report some other observationsand issues in our experiments.5.1 Comparison with Previous WorkThis is to explain more about why our methodperforms better and significantly outperforms theprevious two dependency tree kernels from thetheoretical viewpoint.
(1) Compared with Feature-based Methods:The basic difference lies in the relation instancerepresentation (parse tree vs. feature vector) andthe similarity calculation mechanism (kernelfunction vs. dot-product).
The main difference isthe different feature spaces.
Regarding the parsetree features, our method implicitly represents aparse tree by a vector of integer counts of eachsub-tree type, i.e., we consider the entire sub-treetypes and their occurring frequencies.
In this way,the parse tree-related features (the path featuresand the chunking features) used in the feature-based methods are embedded (as a subset) in ourfeature space.
Moreover, the in-between wordfeatures and the entity-related features used inthe feature-based methods are also captured bythe tree kernel and the entity kernel, respectively.Therefore our method has the potential of effec-tively capturing not only most of the previousflat features but also the useful syntactic struc-ture features.
(2) Compared with Previous Kernels: Sinceour method only counts the occurrence of eachsub-tree without considering the layer and theancestors of the root node of the sub-tree, ourmethod is not limited by the constraints (identi-cal layer and ancestors for the matchable nodes,as discussed in Section 2) in Culotta and Soren-sen (2004).
Moreover, the difference betweenour method and Bunescu and Mooney (2005) isthat their kernel is defined on the shortest pathbetween two entities instead of the entire sub-trees.
However, the path does not maintain thetree structure information.
In addition, their ker-nel requires the two paths to have the samelength.
Such constraint is too strict.5.2 Other Issues(1) Speed Issue: The recursively-defined convo-lution kernel is much slower compared to fea-ture-based classifiers.
In this paper, the speedissue is solved in three ways.
First, the inclusionof the entity kernel makes the composite kernelconverge fast.
Furthermore, we find that thesmall portion (PT) of a full parse tree can effec-tively represent a relation instance.
This signifi-cantly improves the speed.
Finally, the parse treekernel requires exact match between two sub-trees, which normally does not occur very fre-quently.
Collins and Duffy (2001) report that inpractice, running time for the parse tree kernel ismore close to linear (O(|N1|+|N2|), rather thanO(|N1|*|N2| ).
As a result, using the PC with IntelP4 3.0G CPU and 2G RAM, our system onlytakes about 110 minutes and 30 minutes to dotraining on the ACE 2003 (~77k training in-stances) and 2004 (~33k training instances) data,respectively.
(2) Further Improvement: One of the potentialproblems in the parse tree kernel is that it carriesout exact matches between sub-trees, so that thiskernel fails to handle sparse phrases (i.e.
?a car?vs.
?a red car?)
and near-synonymic grammartags (for example, the variations of a verb (i.e.go, went, gone)).
To some degree, it could possi-bly lead to over-fitting and compromise the per-831formance.
However, the above issues can behandled by allowing grammar-driven partial rulematching and other approximate matchingmechanisms in the parse tree kernel calculation.Finally, it is worth noting that by introducingmore individual kernels our method can easilyscale to cover more features from a multitude ofsources (e.g.
Wordnet, gazetteers, etc) that canbe brought to bear on the task of relation extrac-tion.
In addition, we can also easily implementthe feature weighting scheme by adjusting theeqn.
(2) and the rule (2) in calculating 1 2( , )n n?
(see subsection 3.1).6 Conclusion and Future WorkKernel functions have nice properties.
In thispaper, we have designed a composite kernel forrelation extraction.
Benefiting from the niceproperties of the kernel methods, the compositekernel could well explore and combine the flatentity features and the structured syntactic fea-tures, and therefore outperforms previous best-reported feature-based methods on the ACE cor-pus.
To our knowledge, this is the first researchto demonstrate that, without the need for exten-sive feature engineering, an individual tree ker-nel achieves comparable performance with thefeature-based methods.
This shows that the syn-tactic features embedded in a parse tree are par-ticularly useful for relation extraction and whichcan be well captured by the parse tree kernel.
Inaddition, we find that the relation instance repre-sentation (selecting effective portions of parsetrees for kernel calculations) is very importantfor relation extraction.The most immediate extension of our work isto improve the accuracy of relation detection.This can be done by capturing more features byincluding more individual kernels, such as theWordNet-based semantic kernel (Basili et al,2005) and other feature-based kernels.
We canalso benefit from machine learning algorithms tostudy how to solve the data imbalance andsparseness issues from the learning algorithmviewpoint.
In the future work, we will design amore flexible tree kernel for more accurate simi-larity measure.Acknowledgements: We would like to thankDr.
Alessandro Moschitti for his great help inusing his Tree Kernel Toolkits and fine-tuningthe system.
We also would like to thank the threeanonymous reviewers for their invaluable sug-gestions.ReferencesACE.
2002-2005.
The Automatic Content ExtractionProjects.
http://www.ldc.upenn.edu/Projects /ACE/Basili R., Cammisa M. and Moschitti A.
2005.
A Se-mantic Kernel to classify text with very few train-ing examples.
ICML-2005Bunescu R. C. and Mooney R. J.
2005.
A ShortestPath Dependency Kernel for Relation Extraction.EMNLP-2005Charniak E. 2001.
Immediate-head Parsing for Lan-guage Models.
ACL-2001Collins M. and Duffy N. 2001.
Convolution Kernelsfor Natural Language.
NIPS-2001Culotta A. and Sorensen J.
2004.
Dependency TreeKernel for Relation Extraction.
ACL-2004Haussler D. 1999.
Convolution Kernels on DiscreteStructures.
Technical Report UCS-CRL-99-10,University of California, Santa Cruz.Joachims T. 1998.
Text Categorization with SupportVecor Machine: learning with many relevant fea-tures.
ECML-1998Kambhatla N. 2004.
Combining lexical, syntactic andsemantic features with Maximum Entropy modelsfor extracting relations.
ACL-2004 (poster)Lodhi H., Saunders C., Shawe-Taylor J., CristianiniN.
and Watkins C. 2002.
Text classification usingstring kernel.
Journal of Machine Learning Re-search, 2002(2):419-444Miller S., Fox H., Ramshaw L. and Weischedel R.2000.
A novel use of statistical parsing to extractinformation from text.
NAACL-2000Moschitti A.
2004.
A Study on Convolution Kernelsfor Shallow Semantic Parsing.
ACL-2004MUC.
1987-1998. http://www.itl.nist.gov/iaui/894.02/related_projects/muc/Sch?lkopf B. and Smola A. J.
2001.
Learning withKernels: SVM, Regularization, Optimization andBeyond.
MIT Press, Cambridge, MA 407-423Suzuki J., Hirao T., Sasaki Y. and Maeda E. 2003.Hierarchical Directed Acyclic Graph Kernel:Methods for Structured Natural Language Data.ACL-2003Zelenko D., Aone C. and Richardella A.
2003.
KernelMethods for Relation Extraction.
Journal of Ma-chine Learning Research.
2003(2):1083-1106Zhao S.B.
and Grishman R. 2005.
Extracting Rela-tions with Integrated Information Using KernelMethods.
ACL-2005Zhou G.D., Su J, Zhang J. and Zhang M. 2005.
Ex-ploring Various Knowledge in Relation Extraction.ACL-2005832
