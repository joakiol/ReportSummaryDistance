Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 213?218,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational Linguistics?The red one!?
:On learning to refer to thingsbased on discriminative propertiesAngeliki Lazaridou and Nghia The Pham and Marco BaroniUniversity of Trento{angeliki.lazaridou|thepham.nghia|marco.baroni}@unitn.itAbstractAs a first step towards agents learning tocommunicate about their visual environ-ment, we propose a system that, given vi-sual representations of a referent (CAT)and a context (SOFA), identifies their dis-criminative attributes, i.e., properties thatdistinguish them (has_tail).
More-over, although supervision is only pro-vided in terms of discriminativeness ofattributes for pairs, the model learns toassign plausible attributes to specific ob-jects (SOFA-has_cushion).
Finally,we present a preliminary experiment con-firming the referential success of the pre-dicted discriminative attributes.1 IntroductionThere has recently been renewed interest in devel-oping systems capable of genuine language under-standing (Hermann et al, 2015; Hill et al, 2015).In this perspective, it is important to think of anappropriate general framework for teaching lan-guage to machines.
Since we use language pri-marily for communication, a reasonable approachis to develop systems within a genuine commu-nicative setup (Steels, 2003; Mikolov et al, 2015).Out long-term goal is thus to develop communi-ties of computational agents that learn how to uselanguage efficiently in order to achieve commu-nicative success (Vogel et al, 2013; Foerster et al,2016).Within this general picture, one fundamental as-pect of meaning where communication is indeedcrucial is the act of reference (Searle, 1969; Ab-bott, 2010), the ability to successfully talk to oth-ers about things in the external world.
A specificinstantiation of reference studied in this paper isthat of referring expression generation (Dale andis_roundis_metal is_greenmade_of_woodFigure 1: Discriminative attributes predicted byour model.
Can you identify the intended refer-ent?
See Section 6 for more informationReiter, 1995; Mitchell et al, 2010; Kazemzadehet al, 2014).
A necessary condition for achievingsuccessful reference is that referring expressions(REs) accurately distinguish the intended referentfrom any other object in the context (Dale andHaddock, 1991).
Along these lines, we presenthere a model that, given an intended referent and acontext object, predicts the attributes that discrim-inate between the two.
Some examples of the be-haviour of the model are presented in Figure 1.Importantly, and distinguishing our work fromearlier literature on generating REs (Krahmer andVan Deemter, 2012): (i) the input objects arerepresented by natural images, so that the agentmust learn to extract relevant attributes from real-istic data; and (ii) no direct supervision on the at-tributes of a single object is provided: the trainingsignal concerns their discriminativeness for objectpairs (that is, during learning, the agent might betold that has_tail is discriminative for ?CAT,SOFA?, but not that it is an attribute of cats).
Weuse this ?pragmatic?
signal since it could later bereplaced by a measure of success in actual com-munication between two agents (e.g., whether asecond agent was able to pick the correct referentgiven a RE).2 Discriminative Attribute DatasetWe generated the Discriminative AttributeDataset, consisting of pairs of (intended) referentsand contexts, with respect to which the referentsshould be identified by their distinctive attributes.213?referent, visual instancesdiscriminativecontext?attributes?CAT, SOFA ?has tail, has cushion,...?CAT, APPLE?has legs, is green, ...Table 1: Example training dataOur starting point is the Visual Attributes forConcepts Dataset (ViSA) (Silberer et al, 2013),which contains per-concept (as opposed to per-image) attributes for 500 concrete concepts (CAT,SOFA, MILK) spanning across different categories(MAMMALS, FURNITURE), annotated with 636general attributes.
We disregarded ambiguousconcepts (e.g., bat), thus reducing our working setof concepts C to 462 and the number of attributesV to 573, as we eliminated any attribute that didnot occur with concepts in C. We extracted onaverage 100 images annotated with each of theseconcepts from ImageNet (Deng et al, 2009).
Fi-nally, each image i of concept c was associatedto a visual instance vector, by feeding the imageto the VGG-19 ConvNet (Simonyan and Zisser-man, 2014), as implemented in the MatConvNettoolkit (Vedaldi and Lenc, 2015), and extractingthe second-to-last fully-connected (fc) layer as its4096-dimensional visual representation vci.We split the target concepts into training, val-idation and test sets, containing 80%, 10% and10% of the concepts in each category, respec-tively.
This ensures that (i) the intersection be-tween train and test concepts is empty, thus al-lowing us to test the generalization of the modelacross different objects, but (ii) there are instancesof all categories in each set, so that it is possibleto generalize across training and testing objects.Finally we build all possible combinations of con-cepts in the training split to form pairs of refer-ents and contexts ?cr, cc?
and obtain their (binary)attribute vectors pcrand pccfrom ViSA, result-ing in 70K training pairs.
From the latter, we de-rive, for each pair, a concept-level ?discriminative-ness?
vector by computing the symmetric differ-ence dcr,cc= (pcr?
pcc) ?
(pcc?
pcr).
Thelatter will contain 1s for discriminative attributes,0s elsewhere.
On average, each pair is associ-ated with 20 discriminative attributes.
The finaltraining data are triples of the form ?cr, cc,dcr,cc?
(the model never observes the attribute vectors ofspecific concepts), to be associated with visual in-stances of the two concepts.
Table 1 presents someexamples.Note that ViSA contain concept-level attributes,but images contain specific instances of conceptsfor which a general attribute might not hold.
Thisintroduces a small amount of noise.
For example,is_green would in general be a discriminativeattribute for apples and cats, but it is not for thesecond sample in Table 1.
Using datasets with per-image attribute annotations would solve this issue.However, those currently available only cover spe-cific classes of concepts (e.g., only clothes, or ani-mals, or scenes, etc.).
Thus, taken separately, theyare not general enough for our purposes, and wecannot merge them, since their concepts live in dif-ferent attribute spaces.3 Discriminative Attribute NetworkThe proposed Discriminative Attribute Network(DAN) learns to predict the discriminative at-tributes of referent object crand context ccwithoutdirect supervision at the attribute level, but relyingonly on discriminativeness information (e.g., forthe objects in the first row of Table 1, the gold vec-tor would contain 1 for has_tail, but 0 for bothis_green and has_legs).
Still, the model isimplicitly encouraged to embed objects into a con-sistent attribute space, to generalize across the dis-criminativeness vectors of different training pairs,so it also effectively learns to annotate objects withvisual attributes.Figure 2 presents a schematic view of DAN,focusing on a single attribute.
The model is pre-sented with two concepts ?CAT, SOFA?, and ran-domly samples a visual instance of each.
The in-stance visual vectors v (i.e., ConvNet second-to-last fc layers) are mapped into attribute vectors ofdimensionality |V | (cardinality of all available at-tributes), using weights Ma?
R4096?|V |sharedbetween the two concepts.
Intuitively, this layershould learn whether an attribute is active for aspecific object, as this is crucial for determiningwhether the attribute is discriminative for an ob-ject pair.
In Section 5, we present experimentalevidence corroborating this hypothesis.In order to capture the pairwise interactions be-tween attribute vectors, the model proceeds byconcatenating the two units associated with thesame visual attribute v across the two objects (e.g.,the units encoding information about has_tail)and pass them as input to the discriminative layer.214MdMDhas_tailattribute layervisual instancevectorsvisual instancesdiscriminativelayerreferent contextMa MaMDrepeated |V| times with sharedand MdFigure 2: Schematic representation of DAN.
Forsimplicity, the prediction process is only illus-trated for has_tailThe discriminative layer processes the two unitsby applying a linear transformation with weightsMd?
R2?h, followed by a sigmoid activationfunction, finally deriving a single value by anotherlinear transformation with weights MD?
Rh?1.The output?dvencodes the predicted degree ofdiscriminativeness of attribute v for the specificreference-context pair.
The same process is ap-plied to all attributes v ?
V , to derive the esti-mated discriminativeness vector?d, using the sameshared weights Mdand MDfor each attribute.To learn the parameters ?
of the model (i.e.
Ma,Mdand MD), given training data ?cr, cc,dcr,cc?,we minimize MSE between the gold vector dcr,ccand model-estimated?dcr,cc.
We trained the modelwith rmsprop and with a batch size of 32.
All hy-perparameters (including the hidden size h whichwas set to 60) were tuned to maximize perfor-mance on the validation set.4 Predicting DiscriminativenessWe evaluate the ability of the model to predictattributes that discriminate the intended referentfrom the context.
Precisely, we ask the model toreturn all discriminative attributes for a pair, in-dependently of whether they are positive for thereferent or for the context (given images of a catand a building, both +is_furry and ?made_of_bricks are discriminative of the cat).Test stimuli We derive our test stimuli from theVisA test split (see Section 2), containing 2000pairs.
Unlike in training, where the model waspresented with specific visual instances (i.e., sin-gle images), for evaluation we use visual concepts(CAT, BED), which we derive by averaging thevectors of all images associated to an object (i.e.,deriving CAT from all images of cats), due to lackModel Precision Recall F1DAN 0.66 0.49 0.56attribute+sym.
difference 0.64 0.48 0.55no attribute layer 0.63 0.33 0.43Random baseline 0.16 0.16 0.16Table 2: Predicting discriminative featuresof gold information on per-image attributes.Results We compare DAN against a randombaseline based on per-attribute discriminativenessprobabilities estimated from the training data andan ablation model without attribute layer.
We testmoreover a model that is trained with supervi-sion to predict attributes and then deterministicallycomputes the discriminative attributes.
Specifi-cally, we implemented a neural network with onehidden layer, which takes as input a visual in-stance, and it is trained to predict its gold attributevector, casting the problem as logistic regression,thus relying on supervision at the attribute level.Then, given two paired images, we let the modelgenerate their predicted attribute vectors and com-pute the discriminative attributes by taking thesymmetric difference of the predicted attributevectors as we do for DAN.
For the DAN and its ab-lation, we use a 0.5 threshold to deem an attributediscriminative, without tuning.The results in Table 2 confirm that, with ap-propriate supervision, DAN performs discrimina-tiveness prediction reasonably well ?
indeed, aswell as the model with similar parameter capac-ity requiring direct supervision on an attribute-by-attribute basis, followed by the symmetric differ-ence calculation.
Interestingly, allowing the modelto embed visual representations into an interme-diate attribute space has a strong positive effecton performance.
Intuitively, since DAN is eval-uated on novel concepts, the mediating attributelayer provides more high-level semantic informa-tion helping generalization, at the expense of extraparameters compared to the ablation without at-tribute layer.5 Predicting AttributesAttribute learning is typically studied in su-pervised setups (Ferrari and Zisserman, 2007;Farhadi et al, 2009; Russakovsky and Fei-Fei,2010).
Our model learns to embed visual ob-jects in an attribute space through indirect supervi-sion about attribute discriminativeness for specific<referent, context> pairs.
Attributes are never215explicitly associated to a specific concept duringtraining.
The question arises of whether discrim-inativeness pushes the model to learn plausibleconcept attributes.
Note that the idea that the se-mantics of attributes arises from their distinctivefunction within a communication system is fully inline with the classic structuralist view of linguisticmeaning (Geeraerts, 2009).To test our hypothesis, we feed DAN the sametest stimuli (visual concept vectors) as in the pre-vious experiment, but now look at activations inthe attribute layer.
Since these activations are realnumbers whereas gold values (i.e., the visual at-tributes in the ViSA dataset) are binary, we usethe validation set to learn the threshold to deeman attribute active, and set it to 0.5 without tun-ing.
Note that no further training and no extra su-pervision other than the discriminativeness signalare needed to perform attribute prediction.
The re-sulting binary attribute vector p?cfor concept c iscompared against the corresponding gold attributevector pc.Results We compare DAN to the random base-line and to an explicit attribute classifier similarto the one used in the previous experiment, i.e., aone-hidden-layer neural network trained with lo-gistic regression to predict the attributes.
We re-port moreover the best F1 score of Silberer etal.
(2013), who learn a SVM for each visual at-tribute based on HOG visual features.
Unlike inour setup, in theirs, images for the same con-cept are used both for training and to derive vi-sual attributes (our setup is ?zero-shot?
at the con-cept level, i.e., we predict attributes of conceptsnot seen in training).
Thus, despite the fact thatthey used presumably less accurate pre-CNN vi-sual features, the setup is much easier for them,and we take their performance to be an upperbound on ours.DAN reaches, and indeed surpasses, the perfor-mance of the model with direct supervision at theattribute level, confirming the power of discrimi-nativeness as a driving force in building semanticrepresentations.
The comparison with Silberer?smodel suggests that there is room for improve-ment, although the noise inherent in concept-levelannotation imposes a relatively low bound on re-alistic performance.Model Precision Recall F1DAN 0.58 0.64 0.61direct supervision 0.56 0.60 0.58Silberer et.
al.
0.70 0.70 0.70Random baseline 0.13 0.12 0.12Table 3: Predicting concept attributes6 Evaluating Referential SuccessWe finally ran a pilot study testing whether DAN?sability to predict discriminative attributes at theconcept level translates into producing featuresthat would be useful in constructing successful ref-erential expressions for specific object instances.Test stimuli Our starting point is the ReferItdataset (Kazemzadeh et al, 2014), consistingof REs denoting objects (delimited by boundingboxes) in natural images.
We filter out any ?RE,bounding box?
pair whose RE does not overlapwith our attribute set V and annotate the remainingones with the overlapping attribute, deriving dataof the form ?RE, bounding box, attribute?.For each intended referent of this type, we sampleas context another ?RE, bounding box?
pair suchthat (i) the context RE does not contain the ref-erent attribute, so that the latter is a likelydiscriminative feature; (ii) referent and contextcome from different images, so that their bound-ing boxes do not accidentally overlap; (iii) thereis maximum word overlap between referent andcontexts REs, creating a realistic referential ambi-guity setup (e.g., two cars, two objects in similarenvironments).
Finally we sample maximally 20?referent, context?
pairs per attribute, result-ing in 790 test items.
For each referent and contextwe extract CNN visual vectors from their bound-ing boxes, and feed them to DAN to obtain theirdiscriminative attributes.
Note that we used theViSA-trained DAN for this experiment as well.Results For 12% of the test ?referent, context?pairs, the discriminative attribute is con-tained in the set of discriminative attributes pre-dicted by DAN.
A random baseline estimated fromthe distribution of attributes in the ViSA datasetwould score 15% recall.
This baseline howeveron average predicts 20 discriminative attributes,whereas DAN activates, only 4.
Thus, the base-line has a trivial recall advantage.In order to evaluate whether in general the dis-criminative attributes activated by DAN wouldlead to referential success, we further sampled a216subset of 100 ?referent, context?
test pairs.
Wepresented them separately to two subjects (onea co-author of this study) together with the at-tribute that the model activated with the largestscore (see Figure 1 for examples).
Subjects wereasked to identify the intended referent based onthe attribute.
If both agreed on the same referent,we achieved referential success, since the model-predicted attribute sufficed to coherently discrim-inate between the two images.
Encouragingly,the subjects agreed on 78% of the pairs (p<0.001when comparing against chance guessing, accord-ing to a 2-tailed binomial test).
In cases of dis-agreement, the predicted attribute was either toogeneric or very salient in both objects, a behaviourobserved especially in same-category pairs (e.g.,is_round in Figure 1).7 ConcusionWe presented DAN, a model that, given a ref-erent and a context, learns to predict their dis-criminative features, while also inferring visual at-tributes of concepts as a by-product of its train-ing regime.
While the predicted discriminativeattributes can result in referential success, DANis currently lacking all other properties of refer-ence (Grice, 1975) (salience, linguistic and prag-matic felicity, etc).
We are currently working to-wards adding communication (thus simulating aspeaker-listener scenario (Golland et al, 2010))and natural language to the picture.AcknowledgmentsThis work was supported by ERC 2011 StartingIndependent Research Grant n. 283554 (COM-POSES).
We gratefully acknowledge the supportof NVIDIA Corporation with the donation of theGPUs used for this research.ReferencesBarbara Abbott.
2010.
Reference.
Oxford UniversityPress, Oxford, UK.Robert Dale and Nicholas Haddock.
1991.
Contentdetermination in the generation of referring expres-sions.
Computational Intelligence, 7(4):252?265.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the gricean maxims in the gener-ation of referring expressions.
Cognitive science,19(2):233?263.Jia Deng, Wei Dong, Richard Socher, Lia-Ji Li, andLi Fei-Fei.
2009.
Imagenet: A large-scale hierarchi-cal image database.
In Proceedings of CVPR, pages248?255, Miami Beach, FL.Ali Farhadi, Ian Endres, Derek Hoiem, and DavidForsyth.
2009.
Describing objects by their at-tributes.
In Proceedings of CVPR, pages 1778?1785, Miami Beach, FL.Vittorio Ferrari and Andrew Zisserman.
2007.
Learn-ing visual attributes.
In Proceedings of NIPS, pages433?440, Vancouver, Canada.Jakob N. Foerster, Yannis M. Assael, Nando de Fre-itas, and Shimon Whiteson.
2016.
Learningto communicate to solve riddles with deep dis-tributed recurrent q-networks.
Technical ReportarXiv:1602.02672.Dirk Geeraerts.
2009.
Theories of lexical semantics.Oxford University Press, Oxford, UK.Dave Golland, Percy Liang, and Dan Klein.
2010.A game-theoretic approach to generating spatial de-scriptions.
In Proceedings of the 2010 conferenceon empirical methods in natural language process-ing, pages 410?419.
Association for ComputationalLinguistics.Herbert P Grice.
1975.
Logic and conversation.
Syn-tax and Semantics.Karl Moritz Hermann, Tom?a?s Ko?cisk?y, EdwardGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom.
2015.
Teaching ma-chines to read and comprehend.
In Advances in Neu-ral Information Processing Systems (NIPS).Felix Hill, Antoine Bordes, Sumit Chopra, and JasonWeston.
2015.
The Goldilocks principle: Read-ing children?s books with explicit memory repre-sentations.
http://arxiv.org/abs/1511.02301.Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,and Tamara L Berg.
2014.
Referitgame: Refer-ring to objects in photographs of natural scenes.
InEMNLP, pages 787?798.Emiel Krahmer and Kees Van Deemter.
2012.
Compu-tational generation of referring expressions: A sur-vey.
Computational Linguistics, 38(1):173?218.Tomas Mikolov, Armand Joulin, and Marco Baroni.2015.
A roadmap towards machine intelligence.arXiv preprint arXiv:1511.08130.Margaret Mitchell, Kees van Deemter, and Ehud Re-iter.
2010.
Natural reference to objects in a visualdomain.
In Proceedings of the 6th international nat-ural language generation conference, pages 95?104.Association for Computational Linguistics.Olga Russakovsky and Li Fei-Fei.
2010.
Attributelearning in large-scale datasets.
In Proceedings ofECCV, pages 1?14.217John R. Searle.
1969.
Speech Acts: An Essay inthe Philosophy of Language.
Cambridge UniversityPress.Carina Silberer, Vittorio Ferrari, and Mirella Lapata.2013.
Models of semantic representation with visualattributes.
In Proceedings of ACL, pages 572?582,Sofia, Bulgaria.Karen Simonyan and Andrew Zisserman.
2014.
Verydeep convolutional networks for large-scale imagerecognition.
arXiv preprint arXiv:1409.1556.Luc Steels.
2003.
Social language learning.
In MarioTokoro and Luc Steels, editors, The Future of Learn-ing, pages 133?162.
IOS, Amsterdam.Andrea Vedaldi and Karel Lenc.
2015.
MatConvNet ?Convolutional Neural Networks for MATLAB.
Pro-ceeding of the ACM Int.
Conf.
on Multimedia.Adam Vogel, Max Bodoia, Christopher Potts, andDaniel Jurafsky.
2013.
Emergence of gricean max-ims from multi-agent decision theory.
In HLT-NAACL, pages 1072?1081.218
