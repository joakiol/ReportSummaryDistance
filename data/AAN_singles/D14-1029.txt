Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 227?232,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsReordering Model for Forest-to-String Machine TranslationMartin?CmejrekIBM Watson GroupPrague, Czech Republicmartin.cmejrek@us.ibm.comAbstractIn this paper, we present a novel exten-sion of a forest-to-string machine transla-tion system with a reordering model.
Wepredict reordering probabilities for everypair of source words with a model usingfeatures observed from the input parse for-est.
Our approach naturally deals with theambiguity present in the input parse forest,but, at the same time, takes into accountonly the parts of the input forest usedby the current translation hypothesis.
Themethod provides improvement from 0.6 upto 1.0 point measured by (Ter ?
Bleu)/2metric.1 IntroductionVarious commonly adopted statistical machinetranslation (SMT) approaches differ in the amountof linguistic knowledge present in the rules theyemploy.Phrase-based (Koehn et al., 2003) models arestrong in lexical coverage in local contexts, anduse external models to score reordering op-tions (Tillman, 2004; Koehn et al., 2005).Hierarchical models (Chiang, 2005) use lexi-calized synchronous context-free grammar rulesto produce local reorderings.
The grammatical-ity of their output can be improved by addi-tional reordering models scoring permutations ofthe source words.
Reordering model can be eitherused for source pre-ordering (Tromble and Eisner,), integrated into decoding via translation rules ex-tension (Hayashi et al., 2010), additional lexicalfeatures (He et al., ), or using external sources ofinformation, such as source syntactic features ob-served from a parse tree (Huang et al., 2013).Tree-to-string (T2S) models (Liu et al., 2006;Galley et al., 2006) use rules with syntactic struc-tures, aiming at even more grammatically appro-priate reorderings.Forest-to-string (F2S) systems (Mi et al., 2008;Mi and Huang, 2008) use source syntactic forestas the input to overcome parsing errors, and to al-leviate sparseness of translation rules.The parse forest may often represent severalmeanings for an ambiguous input that may needto be transtated differently using different word or-derings.
The following example of an ambiguousChinese sentence with ambiguous part-of-speechlabeling motivates our interest in the reorderingmodel for the F2S translation.S.
t?aol`un (0) SSS.
h`ui (1) SSS z?enmey`ang (2)discussion/NN SS meeting/NN how/VVdiscuss/VV SSSSSwill/VVThere are several possible meanings based onthe different POS tagging sequences.
We presenttranslations for two of them, together with the in-dices to their original source words:(a) NN NN VV:How2was2the0discussion0meeting1?
(b) VV VV VV:Discuss0what2will1happen1.A T2S system starts from a single parse corre-sponding to one of the possible POS sequences,the same tree can be used to predict word reorder-ings.
On the other hand, a F2S system deals withthe ambiguity through exploring translation hy-potheses for all competing parses representing thedifferent meanings.
As our example suggests, dif-ferent meanings also tend to reorder differently227id ruler1NP(t?aol`un/NN) ?
discussionr2NP(h`ui/NN) ?
meetingr3NP(x1:NP x2:NP) ?
the x1x2r4IP(x1:NP z?enmey`ang/VV) ?
how was x1r5IP(h`ui/VV z?enmey`ang/VV) ?
what will happenr6IP(t?aol`un/VV x1:IP) ?
discuss x1Table 1: Tree-to-string translation rules (withoutinternal structures).during translation.
First, the reordering model suit-able for F2S translation should allow for trans-lation of all meanings present in the input.
Sec-ond, as the process of deriving a partial transla-tion hypothesis rules out some of the meanings,the reordering model should restrict itself to fea-tures originating in the relevant parts of the inputforest.
Our work presents a novel technique satis-fying both these requirements, while leaving thedisambuiguation decision up to the model usingglobal features.The paper is organized as follows: We brieflyoverview the F2S and Hiero translation models inSection 2, present the proposed forest reorderingmodel in Section 3, describe our experiment andpresent results in Section 4.2 Translation ModelsForest-to-string translation (Mi et al., 2008) is anextension of the tree-to-string model (Liu et al.,2006; Huang et al., 2006) allowing it to use apacked parse forest as the input instead of a sin-gle parse tree.Figure 1 shows a tree-to-string translationrule (Huang et al., 2006), which is a tuple?lhs(r), rhs(r), ?
(r)?, where lhs(r) is the source-side tree fragment, whose internal nodes are la-beled by nonterminal symbols (like NP), andwhose frontier nodes are labeled by source-language words (like ?z?enmey`ang?)
or variablesfrom a finite set X = {x1, x2, .
.
.
}; rhs(r) isthe target-side string expressed in target-languagewords (like ?how was?)
and variables; and ?
(r) isa mapping from X to nonterminals.
Each variablexi?
X occurs exactly once in lhs(r) and exactlyonce in rhs(r).The Table 1 lists all rules necessary to derivetranslations (a) and (b), with their internal struc-ture removed for simplicity.Typically, an F2S system translates in two steps(shown in Figure 2): parsing and decoding.
In theIPx1:NP VPVVz?enmey`ang?
how was x1Figure 1: Tree-to-string rule r4.parsing step, the source language input is con-verted into a parse forest (A).
In the decoding step,we first convert the parse forest into a translationforest Ftin (B) by using the fast pattern-matchingtechnique (Zhang et al., 2009).
Then the decoderuses dynamic programing with beam search andcube pruning to find the approximation to the bestscoring derivation in the translation forest, andoutputs the target string.3 Forest Reordering ModelIn this section, we describe the process of ap-plying the reordering model scores.
We scorepairwise translation reorderings for every pair ofsource words similarly as described by Huang etal.
(2013).
In their approach, an external model ofordering distributions of sibling constituent pairspredicts the reordering of word pairs.
Our ap-proach deals with parse forests rather than withsingle trees, thus we have to model the scores dif-ferently.
We model ordering distributions for ev-ery pair of close relatives?nodes in the parse forestthat may occur together as frontier nodes of a sin-gle matching rule.
We further condition the distri-bution on a third node?a common ancestor of thenode pair that corresponds to the root node of thematching rule.
This way our external model takesinto acount the syntactic context of the hypothe-sis.
For example, nodes NP0, 1and NP1, 2are closerelatives, NP0, 2and IP0, 3are their common ances-tors; NP0, 1and VV2, 3are close relatives, IP0, 3istheir common ancestor; NP0, 1and VV1, 2are notclose relatives.More formally, let us have an input sentence(w0, ...,wn) and its translation hypothesis h. Forevery i and j such that 0 ?
i < j ?
n we as-sume that the translations of wiand wjare in thehypothesis h either in the same or inverted order-ing oi j?
{Inorder,Reorder}, with a probabilityPorder(oi j|h).
Conditioning on h signifies that theprobabilistic model takes the current hypothesis asa parameter.
The reordering score of the entire hy-228(A)IP0, 3NP0, 2NP0, 1t?aol`unVV0, 1NP1, 2h`uiVV1, 2IP1, 3z?enmey`angVV2, 3Rt?
(B)e4e6e3e1t?aol`une2h`uie5z?enmey`angFigure 2: Parse and translation hypergraphs.
(A) The parse forest of the example sentence.
Solid hy-peredges denote the best parse, dashed hyperedges denote the second best parse.
Unary edges were col-lapsed.
(B) The corresponding translation forest Ftafter applying the tree-to-string translation rule set Rt.Each translation hyperedge (e.g.
e4) has the same index as the corresponding rule (r4).
The forest-to-string system can produce the example translation (a) (solid derivation: r1, r2, r3, and r4) and (b) (dashedderivation: r5, r6).pothesis forder(h) is then computed asforder=?0?i< j?n?
log Porder(oi j= ohi j| h), (1)where ohi jdenotes the actual ordering used in h.The score fordercan be computed recursively bydynamic programing during the decoding.
As anexample, we show in Table 2 reordering probabil-ities retrieved in decoding of our sample sentence.
(a) If h is a hypothesis formed by a single trans-lation rule r with no frontier nonterminals, weevaluate all word pairs wiand wjcovered by hsuch that i < j.
For each such pair we find thefrontier nodes x and y matched by r such thatx spans exactly wiand y spans exactly wj.
(Inthis case, x and y match preterminal nodes, eachspanning one position).
We also find the node zmatching the root of r. Then we directly use theEquation 1 to compute the score using an exter-nal model Porder(oi j|xyz) to estimate the probabil-ity of reordering the relative nodes.
For example,when applying rule r5, we use the ordering dis-tribution Porder(o1,2|VV1, 2,VV2, 3, IP1, 3) to scorereorderings of h`ui and z?enmey`ang.
(b) If h is a hypothesis formed by a T2S rulewith one or more frontier nonterminals, we eval-uate all word pairs as follows: If both wiand wjare spanned by the same frontier nonterminal (e.g.,t?aol`un and h`ui when applying the rule r4), thescore forderhad been already computed for the un-derlying subhypothesis, and therefore was alreadyincluded in the total score.
Otherwise, we computethe word pair ordering cost.
We find the close rel-atives x and y representing each wiand wj.
If wiis matched by a terminal in r, we select x as thenode matching r and spanning exactly wi.
If wiisspanned by a frontier nonterminal in r (meaningthat it was translated in a subhypothesis), we selectx as the node matching that nonterminal.
We pro-ceed identically for wjand y.
For example, whenapplying the rule r4, the word z?enmey`ang will berepresented by the node VV2, 3, while t?aol`un andh`ui will be represented by the node NP0, 2.Note that the ordering ohi jcannot be determinedin some cases, sometimes a source word does notproduce any translation, or the translation of oneword is entirely surrounded by the translations ofanother word.
A weight corresponding to the bi-nary discount feature founknownis added to the scorefor each such case.The external model Porder(oi j|xyz) is imple-mented as a maximum entropy model.
Featuresof the model are observed from paths connectingnode z with nodes x and y as follows: First, wepick paths z ?
x and z ?
y.
Let z?be the last nodeshared by both paths (the closest common ances-tor of x and y).
Then we distinguish three types ofpath: (1) The common prefix z ?
z?
(it may havezero length), the left path z ?
x, and the right pathz ?
y.
We observe the following features on eachpath: the syntactic labels of the nodes, the produc-tion rules, the spans of nodes, a list of stop wordsimmediately preceding and following the span ofthe node.
We merge the features observed fromdifferent paths z ?
x and z ?
y.
This approach229rule word pair order probabilitya) how2was2the discussion0meeting1r3(t?aol`un,h`ui) Inorder Porder(o0,1|NP0, 1,NP1, 2,NP0, 2)r4(t?aol`un,z?enmey`ang) Reorder Porder(o0,2|NP0, 2,VV2, 3, IP0, 3)(h`ui,z?enmey`ang) Reorder Porder(o1,2|NP0, 2,VV2, 3, IP0, 3)b) discuss0what2will1happen1r5(h`ui, z?enmey`ang) Reorder Porder(o1,2|VV1, 2,VV2, 3, IP1, 3)r6(t?aol`un, h`ui) Inorder Porder(o0,1|VV0, 1, IP1, 3, IP0, 3)(t?aol`un, z?enmey`ang Inorder Porder(o0,2|VV0, 1, IP1, 3, IP0, 3)Table 2: Example of reordering scores computed for derivations (a) and (b).ignores the internal structure of each rule1, relyingon frontier node annotation.
On the other hand itis still feasible to precompute the reordering prob-abilities for all combinations of xyz.4 ExperimentIn this section we describe the setup of the exper-iment, and present results.
Finally, we propose fu-ture directions of research.4.1 SetupOur baseline is a strong F2S system (?Cmejreket al., 2013) built on large data with the full setof model features including rule translation prob-abilities, general lexical and provenance transla-tion probabilities, language model, and a vari-ety of sparse features.
We build it as follows.The training corpus consists of 16 million sen-tence pairs available within the DARPA BOLTChinese-English task.
The corpus includes a mixof newswire, broadcast news, webblog data com-ing from various sources such as LDC, HK Law,HK Hansard and UN data.
The Chinese text is seg-mented with a segmenter trained on CTB data us-ing conditional random fields (CRF).Bilingual word alignments are trained and com-bined from two sources: GIZA (Och, 2003) andmaximum entropy word aligner (Ittycheriah andRoukos, 2005).Language models are trained on the Englishside of the parallel corpus, and on monolingualcorpora, such as Gigaword (LDC2011T07) andGoogle News, altogether comprising around 10billion words.We parse the Chinese part of the training datawith a modified version of the Berkeley parser1Only to some extent, the rule still has to match the inputforest, but the reordering model decides based on the sum ofpaths observed between the root and frontier nodes.
(Petrov and Klein, 2007), then prune the ob-tained parse forests for each training sentence withthe marginal probability-based inside-outside al-gorithm to contain only 3n CFG nodes, where n isthe sentence length.We extract tree-to-string translation rules fromforest-string sentence pairs using the forest-basedGHKM algorithm (Mi and Huang, 2008; Galley etal., 2004).In the decoding step, we use larger inputparse forests than in training, we prune them tocontain 10n nodes.
Then we use fast pattern-matching (Zhang et al., 2009) to convert the parseforest into the translation forest.The proposed reordering model is trained on100, 000 automatically aligned forest-string sen-tence pairs from the parallel training data.
Thesesentences provide 110M reordering events that areused by megam (Daum?e III, 2004) to train the max-imum entropy model.The current implementation of the reorderingmodel requires offline preprocessing of the inputhypergraphs to precompute reordering probabili-ties for applicable triples of nodes (x, y, z).
Sincethe number of levels in the syntactic trees in T2Srules is limited to 4, we only need to consider suchtriples, where z is up to 4 levels above x or y.We tune on 1275 sentences, each with 4 refer-ences, from the LDC2010E30 corpus, initially re-leased under the DARPA GALE program.We combine two evaluation metrics for tun-ing and testing: Bleu (Papineni et al., 2002) andTer (Snover et al., 2006).
Both the baseline andthe reordering experiments are optimized withMIRA (Crammer et al., 2006) to maximize (Ter-Bleu)/2.We test on three different test sets: GALEWeb test set from LDC2010E30 corpus (1239sentences, 4 references), NIST MT08 Newswire230System GALE Web MT08 Newswire MT08 WebTer?Bleu2Bleu TerTer?Bleu2Bleu TerTer?Bleu2Bleu TerF2S 8.8 36.1 53.7 5.6 40.6 51.8 12.0 31.3 55.3+Reordering 8.2 36.4 52.7 4.8 41.7 50.5 11.0 31.7 53.7?
-0.6 +0.3 -1.0 -0.8 +1.1 -1.3 -1.0 +0.4 -1.6Table 3: Results.portion (691 sentences, 4 references), and NISTMT08 Web portion (666 sentences, 4 references).4.2 ResultsTable 3 shows all results of the baseline and thesystem extended with the forest reordering model.The (Ter ?
Bleu)/2 score of the baseline systemis 12.0 on MT08 Newswire, showing that it is astrong baseline.
The system with the proposed re-ordering model significantly improves the base-line by 0.6, 0.8, and 1.0 (Ter ?
Bleu)/2 points onGALE Web, MT08 Newswire, and MT08 Web.The current approach relies on frontier nodeannotations, ignoring to some extent the internalstructure of the T2S rules.
As part of future re-search, we would like to compare this approachwith the one that takes into accout the internalstructure as well.5 ConclusionWe have presented a novel reordering model forthe forest-to-string MT system.
The model dealswith the ambiguity of the input forests, but alsopredicts specifically to the current parse followedby the translation hypothesis.
The reordering prob-abilities can be precomputed by an offline pro-cess, allowing for efficient scoring in runtime.
Themethod provides improvement from 0.6 up to 1.0point measured by (Ter ?
Bleu)/2 metrics.AcknowledgmentsWe thank Ji?r??
Havelka for proofreading and help-ful suggestions.
We would like to acknowledgethe support of DARPA under Grant HR0011-12-C-0015 for funding part of this work.
The views,opinions, and/or findings contained in this articleare those of the author and should not be inter-preted as representing the official views or poli-cies, either expressed or implied, of the DARPA.ReferencesDavid Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the ACL.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research, 7.Hal Daum?e III.
2004.
Notes on CG and LM-BFGS op-timization of logistic regression.
Paper available athttp://pub.hal3.name#daume04cg-bfgs, im-plementation available at http://hal3.name/megam/.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of the HLT-NAACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of the COLING-ACL.Katsuhiko Hayashi, Hajime Tsukada, Katsuhito Sudoh,Kevin Duh, and Seiichi Yamamoto.
2010.
Hi-erarchical Phrase-based Machine Translation withWord-based Reordering Model.
In Proceedings ofthe COLING.Zhongjun He, Yao Meng, and Hao Yu.
Maximumentropy based phrase reordering for hierarchicalphrase-based translation.
In Proceedings of theEMNLP.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of the AMTA.Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.2013.
Factored soft source syntactic constraints forhierarchical machine translation.
In Proceeedings ofthe EMNLP.Abraham Ittycheriah and Salim Roukos.
2005.
A max-imum entropy word aligner for arabic-english ma-chine translation.
In Proceedings of the HLT andEMNLP.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of NAACL.231Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descriptionfor the 2005 iwslt speech translation evaluation.
InProceedings of the IWSLT.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of COLING-ACL.Haitao Mi and Liang Huang.
2008.
Forest-based trans-lation rule extraction.
In Proceedings of EMNLP.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL: HLT.Franz Joseph Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings ofACL.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of HLT-NAACL.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the AMTA.Christoph Tillman.
2004.
A unigram orientationmodel for statistical machine translation.
Proceed-ings of the HLT-NAACL.Roy Tromble and Jason Eisner.
Learning linear order-ing problems for better translation.
In Proceedingsof the EMNLP.Martin?Cmejrek, Haitao Mi, and Bowen Zhou.
2013.Flexible and efficient hypergraph interactions forjoint hierarchical and forest-to-string decoding.
InProceedings of the EMNLP.Hui Zhang, Min Zhang, Haizhou Li, and Chew LimTan.
2009.
Fast translation rule matching forsyntax-based statistical machine translation.
In Pro-ceedings of EMNLP, pages 1037?1045, Singapore,August.232
