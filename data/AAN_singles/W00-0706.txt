In: Proceedings of CoNLL-2000 and LLL-2000, pages 31-36, Lisbon, Portugal, 2000.A Comparison between Supervised Learning Algorithms for WordSense Disambiguation*Gerard  Escudero  and Lluis Mhrquez  and German R igauTALP Research Center.
LSI Department.
Universitat Polit~cnica de Catalunya (UPC)Jordi Girona Salgado 1-3.
E-08034 Barcelona.
Catalonia{escudero, lluism, g.rigau}@Isi.upc.esAbst ractThis paper describes a set of comparative exper-iments, including cross-corpus evaluation, be-tween five alternative algorithms for supervisedWord Sense Disambiguation (WSD), namelyNaive Bayes, Exemplar-based learning, SNOW,Decision Lists, and Boosting.
Two main conclu-sions can be drawn: 1) The LazyBoosting algo-rithm outperforms the other four state-of-the-art algorithms in terms of accuracy and abilityto tune to new domains; 2) The domain depen-dence of WSD systems eems very strong andsuggests that some kind of adaptation or tun-ing is required for cross-corpus application.1 In t roduct ionWord Sense Disambiguation (WSD) is the prob-lem of assigning the appropriate meaning (orsense) to a given word in a text or discourse.Resolving the ambiguity of words is a centralproblem for large scale language understandingapplications and their associate tasks (Ide andV4ronis, 1998).
Besides, WSD is one of the mostimportant open problems in NLP.
Despite thewide range of approaches investigated (Kilgar-rift and Rosenzweig, 2000) and the large effortdevoted to tackling this problem, to date, nolarge-scale broad-coverage and highly accurateWSD system has been built.One of the most successful current lines ofresearch is the corpus-based approach, in whichstatistical or Machine Learning (M L) algorithmshave been applied to learn statistical modelsor classifiers from corpora in order to per-* This research has been partially funded by the SpanishResearch Department (CICYT's project TIC98-0423-C06), by the EU Commission (NAMIC I8T-1999-12392),and by the Catalan Research Department (CIRIT'sconsolidated research group 1999SGR-150 and CIRIT'sgrant 1999FI 00773).form WSD.
Generally, supervised approaches(those that learn from previously semanticallyannotated corpora) have obtained better esultsthan unsupervised methods on small sets of se-lected ambiguous words, or artificial pseudo-words.
Many standard M L algorithms for su-pervised learning have been applied, such as:Decision Lists (Yarowsky, 1994; Agirre andMartinez, 2000), Neural Networks (Towell andVoorhees, 1998), Bayesian learning (Bruce andWiebe, 1999), Exemplar-based learning (Ng,1997), Boosting (Escudero et al, 2000a), etc.Further, in (Mooney, 1996) some of the previ-ous methods are compared jointly with DecisionTrees and Rule Induction algorithms, on a veryrestricted omain.Although some published works include thecomparison between some alternative algo-rithms (Mooney, 1996; Ng, 1997; Escudero etal., 2000a; Escudero et al, 2000b), none ofthem addresses the issue of the portability ofsupervised ML algorithms for WSD, i.e., testingwhether the accuracy of a system trained ona certain corpus can be extrapolated to othercorpora or not.
We think that the study of thedomain dependence of WSD -- in the style ofother studies devoted to parsing (Sekine, 1997;Ratnaparkhi, 1999)-- is needed to assure thevalidity of the supervised approach, and to de-termine to which extent a tuning pre-process inecessary to make real WSD systems portable.In this direction, this work compares five differ-ent M L algorithms and explores their portabilityand tuning ability by training and testing themon different corpora.2 Learn ing  A lgor i thms TestedNaive-Bayes (NB).
Naive Bayes is intendedas a simple representative of statistical learningmethods.
It has been used in its most classi-31cal setting (Duda and Hart, 1973).
That is,assuming the independence of features, it clas-sifies a new example by assigning the class thatmaximizes the conditional probability of theclass given the observed sequence of featuresof that example.
Model probabilities are esti-mated during the training process using relativefrequencies.
To avoid the effect of zero counts, avery simple smoothing technique has been used,which was proposed in (Ng, 1997).Despite its simplicity, Naive Bayes is claimedto obtain state-of-the-art accuracy on super-vised WSD in many papers (Mooney, 1996; Ng,1997; Leacock et al, 1998).Exemplar -based  Classif ier (EB).
In exem-plar, instance, or memory-based learning (Ahaet al, 1991) no generalization of training ex-amples is performed.
Instead, the examples aresimply stored in memory and the classificationof new examples is based on the most similarstored exemplars.
In our implementation, allexamples are kept in memory and the classifica-tion is based on a k-NN (Nearest-Neighbours)algorithm using Hamming distance to measurecloseness.
For k's greater than 1, the resultingsense is the weighted majority sense of the knearest neighbours --where each example votesits sense with a strength proportional to itscloseness to the test example.Exemplar-based learning is said to be thebest option for WSD (Ng, 1997).
Other au-thors (Daelemans et al, 1999) point out thatexemplar-based methods tend to be superior inlanguage learning problems because they do notforget exceptions.The SNoW Arch i tec ture  (SN).
SNoWis aSparse Network of linear separators which uti-lizes the Winnow learning algorithm 1.
In theSNo W architecture there is a winnow node foreach class, which learns to separate that classfrom all the rest.
During training, which is per-formed in an on-line fashion, each example isconsidered a positive example for the winnownode associated to its class and a negative x-ample for all the others.
A key point that allowsa fast learning is that the winnow nodes are notconnected to all features but only to those that1The Winnow algorithm (Littlestone, 1988) consistsof a linear threshold algorithm with multiplicative weightupdating for 2-class problems.are "relevant" for their class.
When classify-ing a new example, SNo W is similar to a neuralnetwork which takes the input features and out-puts the class with the highest activation.
Ourimplementation f SNo W for WSD is explainedin (Escudero et al, 2000c).SNoW is proven to perform very well inhigh dimensional NLP problems, where both thetraining examples and the target function residevery sparsely in the feature space (Roth, 1998),e.g: context-sensitive spelling correction, POStagging, PP-attachment disambiguation, etc.Decis ion Lists (DL).
In this setting, a Deci-sion List is a list of features extracted from thetraining examples and sorted by a log-likelihoodmeasure.
This measure stimates how strong aparticular feature is as an indicator of a specificsense (Yarowsky, 1994).
When testing, the deci-sion list is checked in order and the feature withthe highest weight that matches the test exam-ple is used to select the winning word sense.Thus, only the single most reliable piece of ev-idence is used to perform disambiguation.
Re-garding the details of implementation (smooth-ing, pruning of the decision list, etc.)
we havefollowed (Agirre and Martinez, 2000).Decision Lists were one of the most success-ful systems on the 1st Senseval competition forWSD (Kilgarriff and Rosenzweig, 2000).LazyBoost ing  (LB).
The main idea of boost-ing algorithms is to combine many simple andmoderately accurate hypotheses (weak classi-fiers) into a single, highly accurate classifier.The weak classifiers are trained sequentiallyand, conceptually, each of them is trained on theexamples which were most difficult to classify bythe preceding weak classifiers.
These weak hy-potheses are then linearly combined into a singlerule called the combined hypothesis.Schapire and Singer's real AdaBoost.MH al-gorithm for multiclass multi-label classifica-tion (Schapire and Singer, 1999) has been used.It constructs a combination of very simpleweak hypotheses that test the value of a singleboolean predicate and make a real-valued pre-diction based on that value.
LazyBoosting (Es-cudero et al, 2000a) is a simple modificationof the AdaBoost.MH algorithm, which consistsin reducing the feature space that is exploredwhen learning each weak classifier.
This mod-ification significantly increases the efficiency of32the learning process with no loss in accuracy.3 Set t ingA number of comparative experiments has beencarried out on a subset of 21 highly ambiguouswords of the DSO corpus, which is a semanti-cally annotated English corpus collected by Ngand colleagues (Ng and Lee, 1996).
Each wordis treated as a different classification problem.The 21 words comprise 13 nouns (age, art, body,car, child, cost, head, interest, line, point, state,thing, work) and 8 verbs (become, fall, grow, lose,set, speak, strike, tell), which frequently appearin the WSD literature.
The average number ofsenses per word is close to 10 and the numberof training examples is around 1,000.The DSO corpus contains entences from twodifferent corpora, namely Wall Street Journal(WSJ) and Brown Corpus (BC).
Therefore, it iseasy to perform experiments about the porta-bility of systems by training them on the WSJpart (A part, hereinafter) and testing them onthe BC part (B part, hereinafter), or vice-versa.Two kinds of information are used to trainclassifiers: local and topical context.
Let... " be ~ W-3 W--2 W--1 W W-i_ 1 W+2 W+3.
.
.the context of consecutive words around theword w to be disambiguated, and P?i ( -3  <i < 3) be the part-of-speech tag of wordw?i.
Attributes referring to local contextare the following 15: P-3, P-2, P- l ,  P+I,P+2, P+3, w- l ,  W-t-1 , (W-2, W-1), (W-i,W+I),(W+I ,W+2) ,  (W-3,  W--2, W--1), (W-2, W- i ,W+I ) ,(W--l, W+i , W+2), and (W+l, w+2, w+3), wherethe last seven correspond to collocations of twoand three consecutive words.
The topical con-text is formed by c l , .
.
.
,  Cm, which stand for theunordered set of open class words appearing inthe sentence 2.
Details about how the differentalgorithms translate this information into fea-tures can be found in (Escudero et al, 2000c).4 Compar ing  the  five approachesThe five algorithms, jointly with a naive Most-Frequent-sense Classifier (MFC), have beentested, by 10-fold cross validation, on 7 differentcombinations of training-test sets 3.
Accuracy2This set of attributes corresponds to that used in (Ngand Lee, 1996), with the exception of the morphology ofthe target word and the verb-object syntactic relation.3The combinations of training-test sets are called:A+B-A+B, A-I-B-A, A+B-B, A-A, B-B, A-B, and B-A,figures, micro-averaged over the 21 words andover the ten folds, are reported in table 1.
Thecomparison leads to the following conclusions:As expected, the five algorithms ignificantlyoutperform the baseline M FC classifier.
Amongthem, three groups can be observed: Ni3, DL,and SN perform similarly; LB outperforms allthe other algorithms in all experiments; and EBis somewhere in between.
The difference be-tween \[B and the rest is statistically significantin all cases except when comparing \[B to the EBapproach in the case marked with an asterisk 4.Extremely poor results are observed whentesting the portability of the systems.
Restrict-ing to LB results, it can be observed that theaccuracy obtained in A-B is 47.1%, while theaccuracy in B-B (which can be considered anupper bound for LB in B corpus) is 59.0%, thatis, that there is a difference of 12 points.
Fur-thermore, 47.1% is only slightly better than themost frequent sense in corpus B, 45.5%.Apart from accuracy figures, the comparisonbetween the predictions made by the five meth-ods on the test sets provides interesting infor-mation about the relative behaviour of the algo-rithms.
Table 2 shows the agreement rates andthe Kappa statistics 5 between all pairs of meth-ods in the A+B-A+B experiment.
Note that'DSO' stands for the annotation of DSO corpus,which is taken as the correct one.It can be observed that N B obtains the mostsimilar results with regard to M FC in agreementand Kappa values.
The agreement ratio is 74%,that is, almost 3 out of 4 times it predicts themost frequent sense.
On the other extreme, LBobtains the most similar results with regard toDSO in agreement and Kappa values, and it hasthe least similar with regard to M FC, suggestingrespectively.
In this notation, the training set is placedon the left hand side of symbol "-", while the test setis on the right hand side.
For instance, A-B means thatthe training set is corpus A and the test set is corpus B.The symbol "+" stands for set union.4Statistical tests of significance applied: McNemar'stest and 10-fold cross-validation paired Student's t-testat a confidence value of 95% (Dietterich, 1998).~The Kappa statistic (Cohen, 1960) is a better mea-sure of inter-annotator agreement which reduces the ef-fect of chance agreement.
It has been used for measur-ing inter-annotator agreement during the constructionof semantic annotated corpora (V~ronis, 1998; Ng et al,1999).
A Kappa value of 1 indicates perfect agreement,while 0.8 is considered as indicating ood agreement.33Accuracy (%)LazyBoostingA+B-A+B A+B-A A+B-B A-A B-BMFC 46.55?o.71 53.90?2.ol 39.21?i.9o 55.94?Mo 45.52?1.27Naive Bayes 61.55?1.o4 67.25?1.o7 55.85?1.81 65.86?1.11 56.80?1.12Decision Lists 61.58?o.98 67.64?0.94 55.53?1.85 67.57?1.44 56.56?1.59SNoW 60.92?1.o9 65.57?1.33 56.28?1.1o 67.12?1.16 56.13?1.23Exemplar-based 63.01?o.93 69.08?1.66 56.97?1.22 68.98?1.o6 57.36?1.6866.32?1.34 71.79?1.51 60.85~L81 71,26?1.i5 58.96?1.86A-B B-A36.40 38.7141.38 47.6643.01 48.8344.07 49.7645.32 51.1347.10 51.99"Table 1: Accuracy results (=h standard eviation) of the methods on all training-test combinationsA+B-A+BDSO MFC NB EB SN DL LBDSO - -  46.6 61.6 63.0 60.9 61.6 66.3MFC -0.19 --  73.9 60.0 55.9 64.9 54.9NB 0.24 -0.09 --  76.3 74.5 76.8 71.4EB 0.36 -0.15 0.44 - -  69.6 70.7 72.5SN 0.36 -0.17 0.44 0.44 - -  67.5 69.0DL 0.32 -0.13 0.40 0.41 0.38 --  69.9LB 0.44 -0.17 0.37 0.50 0.46 0.42 - -Table 2: Kappa statistic (below diagonal) and% of agreement (above diagonal) between allmethods in the A+B-A+B experimentthat LB is the algorithm that better learns thebehaviour of the DSO examples.In absolute terms, the Kappa values are verylow.
But, as it is suggested in (Vdronis, 1998),evaluation measures hould be computed rela-tive to the agreement between the human an-notators of the corpus and not to a theoreti-cal 100%.
It seems pointless to expect moreagreement between the system and the refer-ence corpus than between the annotators them-selves.
Contrary to the intuition that the agree-ment between human annotators should be veryhigh in the WSD task, some papers report sur-prisingly low figures.
For instance, (Ng et al,1999) reports an accuracy rate of 56.7% and aKappa value of 0.317 when comparing the anno-tation of a subset of the DSO corpus performedby two independent research groups.
From thisperspective, the Kappa value of 0.44 achievedby LB in A+B-A+B could be considered an ex-cellent result.
Unfortunately, the subset of the\[:)SO corpus studied by (Ng et al, 1999) andthat used in this report are not the same and,thus, a direct comparison is not possible.4.1 About  the  tun ing  to new domainsThis experiment explores the effect of a sim-ple tuning process consisting in adding to theoriginal training set A a relatively small sampleof manually sense-tagged examples of the newdomain B.
The size of this supervised portionvaries from 10% to 50% of the available corpusin steps of 10% (the remaining 50% is kept fortesting) 6.
This experiment will be referred toas A+%B-B T. In order to determine to whichextent the original training set contributes toaccurately disambiguating in the new domain,we also calculate the results for %B-B, that is,using only the tuning corpus for training.Figure 1 graphically presents the results ob-tained by all methods.
Each plot contains theA+%B-B and %B-B curves, and the straightlines corresponding to the lower bound MFC,and to the upper bounds B-B and A+B-B.As expected, the accuracy of all methodsgrows (towards the upper bound) as more tun-ing corpus is added to the training set.
How-ever, the relation between A+%B-B and %B-Breveals some interesting facts.
In plots (c) and(d), the contribution of the original training cor-pus is null, while in plots (a) and (b), a degrada-tion onthe  accuracy is observed.
Summarizing,these results suggest hat for NB, DL, SN, andEB methods it is not worth keeping the originaltraining examples.
Instead, a better (but dis-appointing) strategy would be simply using thetuning corpus.
However, this is not the situa-tion of LB - -plot  (d) - -  for which a moderate,but consistent, improvement of accuracy is ob-served when retaining the original training set.6Tuning examples can be weighted more highly thanthe training examples to force the learning algorithm toadapt more quickly to the new corpus.
Some experi-ments in this direction revealed that slightly better e-sults can be obtained, though the improvement was notstatistically significant.7The converse xperiment B-F%A-A is not reportedin this paper due to space limitations.
Results can befound in (Escudero et al, 2000c).34585654g 52al~ 50~ 46444240(a) Naive Bayes.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
M~S o........................................................... ~ ................. B...B .
.
.
.
.A+B-B ........A+%B-B ~-%B-B .
.
.
.
.
..56~ 54o 52~ 484644585654164442(b) Decision Lists.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
G .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
o .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
AYS:B : : i : : :  ....A+%B-B%B-B .
.
.
.
.
', , , , ,5 10 15 20 25 30 35 40 45 50 5 10 15(d )  SNoW58E::E::E.~:E.~:E.~:EZ::Z~:E.~:Z.Z..~YE::LL:::Y.:~.
'S::47724:L:;B-B .
.
.
.
.Zoi~:~ '?~5 10 15 20 25 30 35 40 45 500 20 25 30 35 40 45 50626058~ 56~ 52484644(c) Exemplar Based58MFS56 =::=:==::=Q:~=:::=:==:::a=:====::==~=::=:::=:=:Q-~,~=--~---~ A+B-B ........A+%B-B54 %B-B .......5250 ..... ~46  / o  , , ,' ' ' 44  ' ' ' ' ' ' ' ' '5 10 15 20 25 30 35 40 45 50(e) Lazygoosting.................. o................... ~ ................... ~ ................ MF$- -~- -B-B ............................................................ A~'B-B .
.
.
.
.
.
.A+%B-B ~- -%B-B .
.
.
.
./ /i /5 10 15 20 25 30 35 40 45 50Figure 1: Results of the tuning experimentWe observed that part of the poor resultsobtained is explained by: 1) corpus A andB have a very different distribution of senses,and, therefore, different a-priori biases; further-more, 2) examples of corpus A and B containdifferent information and, therefore, the learn-ing algorithms acquire different (and non inter-changeable) classification clues from both cor-pora.
The study of the rules acquired by Lazy-Boosting from WSJ and BC helped understand-ing the differences between corpora.
On the onehand, the type of features used in the rules wassignificantly different between corpora and, ad-ditionally, there were very few rules that appliedto both sets.
On the other hand, the sign of theprediction of many of these common rules wassomewhat contradictory between corpora.
See(Escudero et al, 2000c) for details.4.2 About  the tra in ing data qual i tyThe observation of the rules acquired by Lazy-Boosting could also help improving data qualityin a semi-supervised fashion.
It is known thatmislabelled examples resulting from annotationerrors tend to be hard examples to classify cor-rectly and, therefore, tend to have large weightsin the final distribution.
This observation al-lows both to identify the noisy examples anduse LazyBoosting as a way to improve the train-ing corpus.A preliminary experiment has been carriedout in this direction by studying the rules ac-quired by LazyBoosting from the training ex-amples of the word state.
The manual revi-sion, by four different people, of the 50 high-est scored rules, allowed us to identify 28 noisytraining examples.
11 of them were clear tag-ging errors, and the remaining 17 were not co-herently tagged and very difficult to judge, sincethe four annotators achieved systematic dis-agreement (probably due to the extremely finegrained sense definitions involved in these ex-amples).5 Conc lus ionsThis work reports a comparative study of fiveML algorithms for WSD, and provides some re-sults on cross corpora evaluation and domainre-tuning.Regarding portability, it seems that the per-formance of supervised sense taggers is notguaranteed when moving from one domain toanother (e.g.
from a balanced corpus, suchas BC, to an economic domain, such as WSJ).35These results imply that some kind of adap-tation is required for cross-corpus application.Consequently, it is our belief that a number ofissues regarding portability, tuning, knowledgeacquisition, etc., should be thoroughly studiedbefore stating that the supervised M k paradigmis able to resolve a realistic WSD problem.Regarding the ML algorithms tested, kazy-Boosting emerges as the best option, sinceit outperforms the other four state-of-the-artmethods in all experiments.
Furthermore, thisalgorithm shows better properties when tunedto new domains.
Future work is planned foran extensive valuation of kazyBoosting on theWSD task.
This would include taking into ac-count additional/alternative attributes, learn-ing curves, testing the algorithm on other cor-pora, etc.Re ferencesE.
Agirre and D. Martinez.
2000.
Decision Lists andAutomatic Word Sense Disambiguation.
In Pro-ceedings of the COLING Workshop on SemanticAnnotation and Intelligent Content.D.
Aha, D. Kibler, and M. Albert.
1991.
Instance-based Learning Algorithms.
Machine Learning,7:37-66.R.
F. Bruce and J. M. Wiebe.
1999.
DecomposableModeling in Natural Language Processing.
Com-putational Linguistics, 25(2):195-207.J.
Cohen.
1960.
A Coefficient of Agreement forNominal Scales.
Journal of Educational and Psy-chological Measurement, 20:37-46.W.
Daelemans, A. van den Bosch, and J. Zavrel.1999.
Forgetting Exceptions is Harmful in Lan-guage Learning.
Machine Learning, 34:11-41.T.
G. Dietterich.
1998.
Approximate StatisticalTests for Comparing Supervised ClassificationLearning Algorithms.
Neural Computation, 10(7).R.
O. Duda and P. E. Hart.
1973.
Pattern Classifi-cation and Scene Analysis.
Wiley ~: Sons.G.
Escudero, L. M~rquez, and G. Rigau.
2000a.Boosting Applied to Word Sense Disambiguation.In Proceedings of the 12th European Conferenceon Machine Learning, ECML.G.
Escudero, L. M~rquez, and G. Rigau.
2000b.Naive Bayes and Exemplar-Based Approaches toWord Sense Disambiguation Revisited.
In Pro-ceedings of the 14th European Conference on Ar-tificial Intelligence, ECALG.
Escudero, L. M~rquez, and G. Rigau.
2000c.
Onthe Portability and Tuning of Supervised WordSense Disambiguation Systems.
Research ReportLSI-00-30-R, Software Department (LSI).
Techni-cal University of Catalonia (UPC).N.
Ide and J. V@ronis.
1998.
Introduction to theSpecial Issue on Word Sense Disambiguation:The State of the Art.
Computational Linguistics,24(1):1-40.A.
Kilgarriff and J. Rosenzweig.
2000.
English SEN-SEVAL: Report and Results.
In Proceedings of the2nd International Conference on Language Re-sources and Evaluation, LREC.C.
Leacock, M. Chodorow, and G. A. Miller.
1998.Using Corpus Statistics and WordNet Relationsfor Sense Identification.
Computational Linguis-tics, 24(1):147-166.N.
Littlestone.
1988.
Learning Quickly when Ir-relevant Attributes Abound.
Machine Learning,2:285-318.R.
J. Mooney.
1996.
Comparative Experiments onDisambiguating Word Senses: An Illustration ofthe Role of Bias in Machine Learning.
In Proceed-ings of the 1st Conference on Empirical Methodsin Natural Language Processing, EMNLP.H.
T. Ng and H. B. Lee.
1996.
Integrating MultipleKnowledge Sources to Disambiguate Word Senses:An Exemplar-based Approach.
In Proceedings ofthe 3~th Annual Meeting of the ACL.H.
T. Ng, C. Lim, and S. Foo.
1999.
A Case Studyon Inter-Annotator Agreement for Word SenseDisambiguation.
In Procs.
of the ACL SIGLEXWorkshop: Standardizing Lexical Resources.H.
T. Ng.
1997.
Exemplar-Base Word Sense Disam-biguation: Some Recent Improvements.
In Procs.of the 2nd Conference on Empirical Methods inNatural Language Processing, EMNLP.A.
Ratnaparkhi.
1999.
Learning to Parse NaturalLanguage with Maximum Entropy Models.
Ma-chine Learning, 34:151-175.D.
Roth.
1998.
Learning to Resolve Natural Lan-guage Ambiguities: A Unified Approach.
In Pro-ceedings of the National Conference on ArtificialIntelligence, AAAI  '98.R.
E. Schapire and Y.
Singer.
1999.
ImprovedBoosting Algorithms Using Confidence-rated Pre-dictions.
Machine Learning, 37(3):297-336.S.
Sekine.
1997.
The Domain Dependence of Pars-ing.
In Proceedings of the 5th Conference on Ap-plied Natural Language Processing, ANLP.G.
Towell and E. M. Voorhees.
1998.
Disambiguat-ing Highly Ambiguous Words.
ComputationalLinguistics, 24(1):125-146.J.
V@ronis.
1998.
A study of polysemy judgementsand inter-annotator agreement.
In Programmeand advanced papers of the Senseval workshop,Herstmonceux Castle, England.D.
Yarowsky.
1994.
Decision Lists for Lexical Ambi-guity Resolution: Application to Accent Restora-tion in Spanish and French.
In Proceedings of the32nd Annual Meeting of the ACL.36
