Last WordsThat?s Nice .
.
.What Can You Do With It?Anja Belz?University of BrightonA regular fixture on the mid 1990s international research seminar circuit was the?billion-neuron artificial brain?
talk.
The idea behind this project was simple: in order tocreate artificial intelligence, what was needed first of all was a very large artificial brain;if a big enough set of interconnected modules of neurons could be implemented, thenit would be possible to evolve mammalian-level behavior with current computational-neuron technology.
The talk included progress reports on the current size of the artificialbrain, its structure, ?update rate,?
and power consumption, and explained how intelli-gent behavior was going to develop by mechanisms simulating biological evolution.What the talk didn?t mention was what kind of functionality the team had so farmanaged to evolve, and so the first comment at the end of the talk was inevitably ?nicework, but have you actually done anything with the brain yet?
?1In human language technology (HLT) research, we currently report a range ofevaluation scores that measure and assess various aspects of systems, in particular thesimilarity of their outputs to samples of human language or to human-produced gold-standard annotations, but are we leaving ourselves open to the same question as thebillion-neuron artificial brain researchers?Shrinking HorizonsHLT evaluation has a long history.
Spa?rck Jones?s Information Retrieval Experiment (1981)already had two decades of IR evaluation history to look back on.
It provides a fairlycomprehensive snapshot of HLT evaluation at the time, as much of HLT evaluationresearch was in the field of IR.
One thing that is striking from today?s perspectiveis the rich diversity of evaluation paradigms?user-oriented and developer-oriented,intrinsic and extrinsic2?that were being investigated and discussed on an equal footing?
NLTG, University of Brighton, Lewes Road, Brighton BN2 4GJ, UK.
E-mail: A.S.Belz@brighton.ac.uk.1 To which the answer was, in effect, ?the brain isn?t big enough yet to be used.?
The original aim of theCAM-Brain Project was to evolve behavior as complex as that of a kitten (the brain was going to control arobotic kitten, the ?Robokoneko?).
The functionality reported for the modules the brain was composed ofwas on the level of the XOR-function (de Garis et al 1999).
To date, no functionality appears to have beenreported for the brain as a whole.2 User-oriented evaluations (covered by ISO standards 9126 and 14598 on software evaluation) look at a setof requirements (available computational, financial, and other resources, acceptable processing time,maintenance cost, etc.)
of the user (embedding application or person) and assess how well differenttechnological alternatives fulfill them.
Developer-oriented evaluations focus on functionality (just onecomponent in the ISO standards) and seek to assess the quality of a system?s (or component?s) outputs.The user-oriented vs. developer-oriented distinction concerns evaluation purpose.
Another commondistinction is about evaluation methods: intrinsic evaluations assess properties of systems in their ownright, for example, comparing their outputs to reference outputs in a corpus, whereas extrinsicevaluations assess the effect of a system on something that is external to it, for example, the effect onhuman performance at a given task or the value added to an application (Spa?rck Jones 1994).?
2009 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 1in the context of academic research.
At the same time Spa?rck Jones described a lack ofconsolidation and collective progress, noting: ?there is so little control in individual testsand so much variation in method between tests that interpretations of the results of anyone test or of their relationships with those of others must be uncertain?
(page 245).These days, HLT research has many more subfields, most of which devote substan-tial research effort to evaluation.
We have far more established evaluation techniquesand comparative evaluation is the norm.
In fact, virtually all HLT subfields now havesome form of competitive evaluation.3 But it seems we have achieved comparability atthe price of diversity.
The range of evaluation methods we employ has shrunk dra-matically.
Not only is virtually all evaluation in HLT research now developer-orientedand intrinsic, but, even more narrowly, most of it is a version of one of just three basicintrinsic techniques: (i) assessment by trained assessors of the quality of system outputsaccording to different quality criteria, typically using rating scales; (ii) automatic mea-surements of the degree of similarity between system outputs and reference outputs;and (iii) human assessment of the degree of similarity between system outputs andreference outputs.4What is noticeable by its absence is any form of extrinsic evaluation.
Applicationpurpose?of the embedded component or end-to-end system?is not part of task defi-nitions, and we do not test how well components or systems fulfill (some aspect of) theapplication purpose.Tasks in Need of an ApplicationBecause application purpose does not figure in it, the intrinsic evaluation paradigmtreats tasks as generic even though this may not always be appropriate.
Kilgarriffwarned against treating the word sense disambiguation (WSD) task as generic right atthe start of the SENSEVAL evaluation competitions:[...] a task-independent set of word senses for a language is not a coherent concept.
[...]Until recently, WSD researchers have generally proceeded as if this was not the case: asif a single program?disambiguating, perhaps, in its English-language version,between the senses given in some hybrid descendant of Merriam-Webster, LDOCE,COMLEX, Roget, OALDCE and WordNet?would be relevant to a wide range of NLPapplications.
(Kilgarriff 1997, page 107)WSD may still be the most notorious ?task in need of an application?
(McCarthy andNavigli 2007), but the case of WSD points to a more general issue: in intrinsic evaluations,the output representation formalism (e.g., tag set, syntactic formalism) is fixed in theform of gold-standard reference annotations, and alternative representations are notsubject to evaluation.
There is evidence that it may be worth looking at how differentrepresentations perform.
For example, Miyao et al (2008) found significant differencesbetween different parse representations given the same parser type when evaluatingtheir effect on the performance of a biomedical IR tool.
The intrinsic set-up makesit impossible to perform such evaluations of alternative representations, because this3 Some examples are the NIST-run DUC document summarization evaluation campaign (now part of TAC),the NIST-run Open MT evaluation campaign (MT-Eval), and the academic-led SENSEVAL/SEMEVAL WSDevaluations, among many others.4 All three techniques have been used in competitive evaluations: i and iii have been used, for example, inDUC; ii in MT-Eval, DUC, and SENSEVAL/SEMEVAL.
By far the most common technique to be found inindividual research reports is ii, although iii and related types of user assessments are also used.112Belz That?s Nice .
.
.What Can You Do With It?requires an external?extrinsic?point of reference, as is provided by an embeddingsystem like the IR tool in Miyao et al?s work.If we don?t include application purpose in task definitions then not only do wenot know which applications (if indeed any) systems are good for, we also don?t knowwhether the task definition (including output representations) is appropriate for theapplication purpose we have in mind.A Closed CircleWhereas in analysis tasks evaluation typically measures the similarity between systemoutput representations and gold-standard reference representations, in tasks where theoutput is language (e.g., MT, summarization, data-to-text generation), system outputsare compared to human-produced reference texts, or directly evaluated by assessors.Methods for evaluating these evaluation methods, or ?meta-evaluation?
methods, lookin particular at the reference outputs and similarity measures they involve.
In analysis,where there are single target outputs, and similarity measures are a matter of count-ing matching brackets or tags, we can?t do much more than assess inter-annotatoragreement and perform error analysis for reference annotations.
In generation, it is thesimilarity measures that are scrutinized most.
Metrics such as BLEU and ROUGE wereconceived as surrogate measures5 (the U in BLEU stands for ?understudy?).
Surrogatemeasures in science in general need to be tested in terms of their correlation with somereliable measure which is known to be a true indicator of the condition or property (e.g.,karyotyping for chromosomal abnormalities) for which the surrogate measure (e.g.,serum testing for specific protein types) is intended to be an approximate indicator.In HLT, we test (surrogate) automatic metrics in terms of their correlation with humanratings of quality, using Pearson?s product-moment correlation coefficient, and some-times Spearman?s rank-order correlation coefficient (Lin and Och 2004).
The strongerand more significant the correlation, the better metrics are deemed to be.
The humanratings are not tested.In this set-up, clearly, there is no way in which the results of human quality assess-ment can ever be shown to be wrong.
If human judgment says a system is good, thenif an automatic measure says the system is good, it simply confirms human judgment;if the automatic measure says the system is bad, then the measure is a bad one, its re-sults are disregarded, and the system is still a good system.
This is a classic closed-circle set-up: It isn?t falsifiable, and it doesn?t include a scenario in which it would beconcluded that the initial theory was wrong.
The problem lies with treating what is butanother surrogate measure?human quality ratings?as a reliable, objective measure ofquality.
We may be justified in not accepting contradicting metric scores as evidenceagainst human quality judgments and humanlikeness assessments, but perhaps weshould pay attention if such intrinsic measures are contradicted by the results of user-performance and other extrinsic experiments.
For example, in a comparison of graphicalrepresentations of medical data with textual descriptions of the same data, Law et al(2005) found that, whereas in intrinsic assessments doctors rated the graphs more highlythan the texts, in an extrinsic diagnostic performance test they performed better with thetexts than the graphs.
Engelhardt, Bailey, and Ferreira (2006) found that subjects rated5 This term is more commonly used in biology where it refers to a laboratory measurement of biologicalactivity within the body that indirectly indicates the effect of treatment on disease state.
For example,CD4 cell counts and viral load are examples of surrogate markers in HIV infection.
?http://cancerweb.ncl.ac.uk/omd?.113Computational Linguistics Volume 35, Number 1over-descriptions as highly as concise descriptions, but performed worse at a visualidentification task with over-descriptions than with concise descriptions.
In a recent setof evaluation experiments involving 15 NLG systems, the eight intrinsic measures tested(although correlating strongly and positively with each other) either did not correlatesignificantly with the three extrinsic measures of task performance that were also tested,or were negatively correlated with them (Belz and Gatt 2008).
In parsing, Miyao et al(2008) performed an extrinsic evaluation of eight state-of-the-art parsers used as partof a biomedical IR tool.
The effect parsers had on IR quality revealed a different systemranking than the WSJ-Corpus based F-scores reported for the same parsers elsewhere.Unreliable Evidence?There is some indication that human quality judgments and measurements of similaritywith human-produced reference material may not be able to live up to the role they arecurrently assigned.
We know that agreement between annotators is notoriously difficultto achieve, particularly at the more subjective end of the spectrum (see, for example,Reidsma and op den Akker 2008).
Stable averages of human quality judgments, let alnehigh levels of agreement, are hard to achieve, as has been observed for MT (Turian, Shen,and Melamed 2003; Lin and Och 2004), text summarization (Trang Dang 2006), and NLG(Belz and Reiter 2006).
In fact, the large variance typical of human quality judgments canresult in higher agreement between automatic metrics and human judges than amongthe human judges (Burstein and Wolska 2003; Belz and Reiter 2006).Despite the evidence, it is hard to shake the assumption that similarity to ?howhumans do it?
is an indicator of quality, that the more similar HLT system outputs areto human outputs the better they are.
In fact, to some it is a matter of a priori fact thathumans cannot be outperformed in HLT by machines:[...] no-one (surely) would dispute that human performance is the ultimate criterion forautomatic language analysis.To draw an analogy with another area of computational linguistics, namelymachine translation, it would not make sense to claim that some MT system wascapable of translating language A into language B better than the best humantranslators for that language-pair: skilled human performance logically defines anupper bound for machine performance.
(Sampson and Babarczy 2003, page 63)Clearly, there do exist low-level HLT tasks that machines can perform faster and moreaccurately than humans (e.g., concordance construction, spell checking, anagram find-ing).
But there is some evidence that there are more complex HLT tasks for which thisis the case, too.
In genre classification, even ad hoc systems have matched humanperformance (Jabbari et al 2006).
In NLG, domain experts have been shown to prefersystem-generated language to alternatives produced by human experts (Reiter et al2005; Belz and Reiter 2006).
In WSD, Ide and Wilks have pointed out that ?claimed andtested success rates in the 90%+ range are strikingly higher than the inter-annotatoragreement level of 80%+, and to some this is a paradox?
; they conclude that the onlyexplanation that seems to fit the data is that the average tested person is not as good asthe tested systems at this task (Ide and Wilks 2006, page 52).Limited ConclusionsSo, current HLT evaluation practices involve a limited number of basic evaluationtechniques capable of testing for a limited range of system characteristics; because114Belz That?s Nice .
.
.What Can You Do With It?they do not involve a system-external perspective we can?t test systems for suitabilityfor application purpose and we can?t effectively meta-evaluate evaluation procedures;instead, we have to rely heavily on human judgments and annotations that we know tobe unreliable in many cases.
In addition, we tend to evaluate systems on a single corpus(failing to make use of one way in which an extrinsic perspective could be introducedinto an intrinsic evaluation set-up).
In this situation only very limited conclusions canbe drawn from evaluations.
When large companies with corporate lawyers are amongthe participants of an HLT competition, this fact must be made explicit in a prominentlydisplayed formal disclaimer:The data, protocols, and metrics employed in this evaluation [...] should not beconstrued as indicating how well these systems would perform in applications.
Whilechanges in the data domain, or changes in the amount of data used to build a system,can greatly influence system performance, changing the task protocols could indicatedifferent performance strengths and weaknesses for these same systems.
(Disclaimer,NIST Open MT Evaluation 20056)Prevailing evaluation practices guide the development of an entire field of research;flagship shared-task evaluation competitions such as MT-Eval, DUC, SEMEVAL, andCoNLL are regarded as determining the state of the art of a field?should we not expectsuch evaluations to give some indication of ?how well [...] systems would perform inapplications?
?Towards a More Extrinsic PerspectiveThe explanation routinely given for not carrying out extrinsic evaluations is that theyare too time-consuming and expensive.
There clearly is a need for radical innovation inthis area, and industry involvement and crowd-sourcing may provide ways to offsetcost.
But there are things we can do now, even with limited budgets.
For example,automatic extrinsic evaluations are possible, and avoid the cost of human participants:Kabadjov, Poesio, and Steinberger (2005) tested an anaphora resolver embedded in asummarization system; Miyao et al (2008) evaluated a parser embedded within an IRsystem.
Even evaluation experiments involving human subjects do not have to comewith an exorbitant price-tag: REG?08, a competition in the field of referring expressiongeneration which had very minimal funding, included a task-performance experimentin which the speed and accuracy with which subjects were able to identify intendedreferents was tested (Gatt, Belz, and Kow 2008).Perhaps the most immediately feasible way to bring an extrinsic perspective intocurrent HLT evaluation practices is to combine methods for extrinsic validation withcurrent intrinsic techniques.
What makes extrinsic evaluation infeasible in many casesis not the cost of a single experiment, but the fact that the experiment has to be repeatedfor every data set and for every set of systems, and that the cost of the experiment is thesame every time it is run.
In contrast, extrinsic validation involves one-off validationprocedures for evaluation metrics, reference material, and training data.
Because theyare one-off experiments that form part of the development of evaluation methods anddata resources, they can be achieved at a much lower cost than extrinsic evaluation6 http://www.nist.gov/speech/tests/mt/2005/doc/mt05eval official results release 20050801 v3.html.115Computational Linguistics Volume 35, Number 1methods that are directly applied to systems.
Extrinsic validation can potentially takemany different forms; the following are three examples:1.
Extrinsic meta-evaluation of evaluation metrics: Evaluation methods, in particularautomatic metrics, are evaluated in terms of their correlation with user-performanceand other application-specific evaluations.2.
Extrinsic evaluation of human-produced reference material: The quality of referencematerial is assessed by testing it directly in user-performance/application-specificevaluations.
Intrinsic evaluation techniques using the reference material can then beweighted in favor of more highly scoring material.3.
Extrinsic evaluation of training data: Training data is annotated with informationfrom extrinsically motivated experiments, for example, reading speed or eye-trackinginformation, or scores obtained in other user-performance/application-specific eval-uations.
Training procedures can then be weighted in favor of more highly scoringmaterial.ConclusionsScience and technology is littered with the remains of intrinsic measures discardedwhen extrinsic measures revealed them to be unreliable indicators.7 The billion-neuronbrain researchers pursued an intrinsic measure (size) without testing it against the cor-responding extrinsic measure (improved functionality), and ended up with an artificialbrain that was very, very large, but was of little actual use (and certainly didn?t fulfill itsdeclared application purpose of controlling a robot kitten).In HLT we are currently enthusiastic about evaluation to the point where it is hardto get a paper into ACL or COLING that doesn?t have evaluation results; at the same timewe consider tables of metric scores on single data sets a meaningful form of evaluation.If we think that, say, the purpose of a parser is to place brackets in text where a humanannotator thinks they should go, then we?re doing fine.
If we think it is to facilitate high-quality IR, NER, and similar tasks, then we need to evaluate extrinsically, with referenceto a range of application contexts (if the tool is intended to be a generic one) or a specificapplication context (if it is a specialized tool); then we need to stop picking the low-hanging fruit, and instead put our energies into solving the hard problem of how tosituate evaluation in a context:One of the main requirements for future NLP evaluations is thus to approach these in acomprehensive as well as systematic way, so that the specific tests done are properlysituated, especially in relation to the ends the evaluation subject is intended to serve,and the properties of the context in which it does this.
(Sparck Jones 1994, page 107)Putting greater emphasis on the extrinsic perspective in HLT research will result inimproved checks and balances for the evaluation methods we apply; it will enable us to7 Some examples: (i) The drug cholestyramine successfuly reduces blood cholesterol levels, but was foundto have no impact on overall mortality rate (Le Fanu 1999, page 341); (ii) cardiac ac/decleration andraised blood acidity were thought to be indicators of an increased risk of cerebral palsy, but theintroduction of foetal monitoring had no effect on the incidence of cerebral palsy, which remainedconstant (Le Fanu 1999, pages 255?258); (iii) commonly used in adverts by financial institutions and asthe basis for investor decisions, past performance has been shown not to be a reliable indicator of thefuture performance of mutual funds (Allen et al 2003); (iv) total harmonic distortion was thought to bean overall indicator of amplifier quality until the 1970s when, following technological improvements inaudio production, it was found not to correlate with expert listeners?
assessments at improved levels.116Belz That?s Nice .
.
.What Can You Do With It?make better predictions about how the methods we develop will perform when appliedto the purpose we develop them for; and it will mean that we have a better answerwhen we are asked ?.
.
.
but what can you do with it?
?ReferencesAllen, D., T. Brailsford, R. Bird, and R. Faff.2003.
A review of the research on the pastperformance of managed funds.
ASICREP 22, Australian Securities andInvestment Commission.Belz, A. and A. Gatt.
2008.
Intrinsicvs.
extrinsic evaluation measures forreferring expression generation.
InProceedings of the 46th AnnualMeeting of the Association forComputational Linguistics (ACL?08),pages 197?200, Columbus, OH.Belz, A. and E. Reiter.
2006.
Comparingautomatic and human evaluationof NLG systems.
In Proceedingsof the 11th Conference of the EuropeanChapter of the Association forComputational Linguistics (EACL?06),pages 313?320, Trento, Italy.Burstein, J. and M. Wolska.
2003.
Towardevaluation of writing style: Overlyrepetitious word use.
In Proceedingsof the 10th Conference of the EuropeanChapter of the Association forComputational Linguistics (EACL?03),pages 35?42, Budapest.de Garis, H., N. Eiji Nawa, A. Buller, M.Korkin, F. Gers, and M. Hough.
1999.ATR?s artificial brain (?CAM-brain?)project.
In Proceedings of the 1st Geneticand Evolutionary Computation Conference(GECCO?99), volume 2, page 1233,Orlando, FL.Engelhardt, P., K. Bailey, and F. Ferreira.2006.
Do speakers and listenersobserve the Gricean maxim of quantity?Journal of Memory and Language,54:554?573.Gatt, A., A. Belz, and E. Kow.
2008.
TheTUNA Challenge 2008: Overview andevaluation results.
In Proceedings of the5th International Natural LanguageGeneration Conference (INLG?08),pages 198?206, Salt Fork, OH.Ide, N. and Y. Wilks.
2006.
Making senseabout sense (Chapter 3).
In E. Agirreand P. Edmonds, editors, Word SenseDisambiguation: Algorithms andApplications.
Springer, Berlin,pages 47?74.Jabbari, S., B. Allison, D. Guthrie,and L. Guthrie.
2006.
Towards theOrwellian nightmare: Separation ofbusiness and personal emails.
InProceedings of the Joint 21st InternationalConference on Computational Linguisticsand 44th Annual Meeting of the Associationfor Computational Linguistics (COLING-ACL?06), pages 407?411, Sydney, Australia.Kabadjov, M. A., M. Poesio, andJ.
Steinberger.
2005.
Task-basedevaluation of anaphora resolution: Thecase of summarization.
In Proceedingsof the RANLP?05 Workshop on CrossingBarriers in Text Summarization Research,pages 18?25, Borovets, Bulgaria.Kilgarriff, A.
1997.
?I don?t believe in wordsenses.?
Computers and the Humanities,31:91?113.Law, A. S., Y.
Freer, J.
Hunter, R. H. Logie,N.
McIntosh, and J. Quinn.
2005.
Acomparison of graphical and textualpresentations of time series data to supportmedical decision making in the neonatalintensive care unit.
Journal of ClinicalMonitoring and Computing, 19:183?194.Le Fanu, J.
1999.
The Rise and Fall of ModernMedicine.
Abacus, London.Lin, C.-Y.
and F. J. Och.
2004.
ORANGE: Amethod for evaluating automaticevaluation metrics for machinetranslation.
In Proceedings of the 20thInternational Conference on ComputationalLinguistics (COLING?04), pages 501?507,Geneva.McCarthy, D. and R. Navigli.
2007.Semeval-2007 Task 10: English lexicalsubstitution task.
In Proceedingsof the 4th International Workshop onSemantic Evaluations (SemEval?07),pages 48?53, Prague.Miyao, Y., R. Saetre, K. Sagae, T. Matsuzaki,and J. Tsujii.
2008.
Task-orientedevaluation of syntactic parsers andtheir representations.
In Proceedings ofthe 46th Annual Meeting of the Associationfor Computational Linguistics (ACL?08),pages 46?54, Columbus, OH.Reidsma, D. and R. op den Akker.
2008.Exploiting ?subjective?
annotations.
InProceedings of the COLING?08 Workshop onHuman Judgements in ComputationalLinguistics, pages 8?16, Manchester, UK.Reiter, E., S. Sripada, J.
Hunter, andJ.
Yu.
2005.
Choosing words incomputer-generated weather forecasts.Artificial Intelligence, 167:137?169.117Computational Linguistics Volume 35, Number 1Sampson, G. and A. Babarczy.
2003.Limits to annotation precision.
InProceedings of the EACL?03 Workshop onLinguistically Interpreted Corpora (LINC?03),pages 61?89, Budapest.Spa?rck Jones, K. 1981.
Retrieval systemtests 1958?1978 (chapter 12).
In K. Spa?rckJones, editor, Information RetrievalExperiment.
Butterworth & Co, pages213?255, Newton, MA.Spa?rck Jones, K. 1994.
Towards better NLPsystem evaluation.
In Proceedings of theWorkshop on Human Language Technology(HLT?94), pages 102?107, Plainsboro, NJ.Trang Dang, H. 2006.
DUC 2005: Evaluationof question-focused summarizationsystems.
In Proceedings of theCOLING-ACL?06 Workshop on Task-FocusedSummarization and Question Answering,pages 48?55, Prague.Turian, J., L. Shen, and I. D. Melamed.
2003.Evaluation of machine translation and itsevaluation.
In Proceedings of MT Summit IX,pages 386?393, New Orleans.118
