Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 400?409,Honolulu, October 2008. c?2008 Association for Computational LinguisticsA Dependency-based Word Subsequence KernelRohit J. KateDepartment of Computer SciencesThe University of Texas at Austin1 University Station C0500Austin, TX 78712-0233, USArjkate@cs.utexas.eduAbstractThis paper introduces a new kernel whichcomputes similarity between two natural lan-guage sentences as the number of paths sharedby their dependency trees.
The paper gives avery efficient algorithm to compute it.
Thiskernel is also an improvement over the wordsubsequence kernel because it only countslinguistically meaningful word subsequenceswhich are based on word dependencies.
Itovercomes some of the difficulties encoun-tered by syntactic tree kernels as well.
Ex-perimental results demonstrate the advantageof this kernel over word subsequence and syn-tactic tree kernels.1 IntroductionKernel-based learning methods (Vapnik, 1998) arebecoming increasingly popular in natural languageprocessing (NLP) because they allow one to workwith potentially infinite number of features with-out explicitly constructing or manipulating them.
Inmost NLP problems, the data is present in structuredforms, like strings or trees, and this structural infor-mation can be effectively passed to a kernel-basedlearning algorithm using an appropriate kernel, likea string kernel (Lodhi et al, 2002) or a tree kernel(Collins and Duffy, 2001).
In contrast, feature-basedmethods require reducing the data to a pre-definedset of features often leading to some loss of the use-ful structural information present in the data.A kernel is a measure of similarity between ev-ery pair of examples in the data and a kernel-basedmachine learning algorithm accesses the data onlythrough these kernel values.
For example, the stringkernel (Lodhi et al, 2002; Cancedda et al, 2003)computes the similarity between two natural lan-guage strings as the number of common word sub-sequences between them.
A subsequence allowsgaps between the common words which are penal-ized according to a parameter.
Each word subse-quence hence becomes an implicit feature used bythe kernel-based machine learning algorithm.
Aproblem with this kernel is that many of these wordsubsequences common between two strings may notbe semantically expressive or linguistically mean-ingful1.
Another problem with this kernel is thatif there are long-range dependencies between thewords in a common word subsequence, then theywill unfairly get heavily penalized because of thepresence of word gaps.The syntactic tree kernel presented in (Collins andDuffy, 2001) captures the structural similarity be-tween two syntactic trees as the number of syntac-tic subtrees common between them.
However, of-ten syntactic parse trees may share syntactic sub-trees which correspond to very different semanticsbased on what words they represent in the sentence.On the other hand, some subtrees may differ syn-tactically but may represent similar underlying se-mantics.
These differences can become particularlyproblematic if the tree kernel is to be used for taskswhich require semantic processing.This paper presents a new kernel which computessimilarity between two sentences as the the numberof paths common between their dependency trees.1(Lodhi et al, 2002) use character subsequences instead ofword subsequences which are even less meaningful.400(a) A fat cat was chased by a dog.
(b) A cat with a red collar was chased two days agoby a fat dog.Figure 1: Two natural language sentences.It improves over the word subsequence kernel be-cause it only counts the word subsequences whichare linked by dependencies.
It also circumventssome of the difficulties encountered with the syntac-tic tree kernel when applied for semantic processingtasks.Although several dependency-tree-based kernelsand modifications to syntactic tree kernels have beenproposed which we briefly discuss in the RelatedWork section, to our best knowledge no previouswork has presented a kernel based on dependencypaths which offers some unique advantages.
We alsogive a very efficient algorithm to compute this ker-nel.
We present experimental results on the task ofdomain-specific semantic parsing demonstrating theadvantage of this kernel over word subsequence andsyntactic tree kernels.The following section gives some background onstring and tree kernels.
Section 3 then introducesthe dependency-based word subsequence kernel andgives an efficient algorithm to compute it.
Some ofthe related work is discussed next, followed by ex-periments, future work and conclusions.2 String and Tree Kernels2.1 Word-Subsequence KernelA kernel between two sentences measures the simi-larity between them.
Lodhi et al (2002) presented astring kernel which measures the similarity betweentwo sentences, or two documents in general, as thenumber of character subsequences shared betweenthem.
This was extended by Cancedda et al (2003)to the number of common word subsequences be-tween them.
We will refer to this kernel as the wordsubsequence kernel.Consider the two sentences shown in Figure 1.Some common word subsequences between themare ?a cat?, ?was chased by?, ?by a dog?, ?a catchased by a dog?, etc.
Note that the subsequence?was chased by?
is present in the second sentencebut it requires skipping the words ?two days ago?
orhas a gap of three words present in it.
The kerneldownweights the presence of gaps by a decay fac-tor ??
(0, 1].
If g1 and g2 are the sum totals of gapsfor a subsequence present in the two sentences re-spectively, then the contribution of this subsequencetowards the kernel value will be ?g1+g2 .
The ker-nel can be normalized to have values in the rangeof [0, 1] to remove any bias due to different sen-tence lengths.
Lodhi et al (2002) give a dynamicprogramming algorithm to compute string subse-quence kernels in O(nst) time where s and t are thelengths of the two input strings and n is the maxi-mum length of common subsequences one wants toconsider.
Rousu and Shawe-Taylor (2005) presentan improved algorithm which works faster when thevocabulary size is large.
Subsequence kernels havebeen used with success in NLP for text classification(Lodhi et al, 2002; Cancedda et al, 2003), informa-tion extraction (Bunescu and Mooney, 2005b) andsemantic parsing (Kate and Mooney, 2006).There are, however, some shortcomings of thisword subsequence kernel as a measure of similaritybetween two sentences.
Firstly, since it considers allpossible common subsequences, it is not sensitiveto whether the subsequence is linguistically mean-ingful or not.
For example, the meaningless sub-sequences ?cat was by?
and ?a was a?
will also beconsidered common between the two sentences bythis kernel.
Since these subsequences will be used asimplicit features by the kernel-based machine learn-ing algorithm, their presence can only hurt the per-formance.
Secondly, if there are long distance de-pendencies between the words of the subsequencepresent in a sentence then the subsequence will getunfairly penalized.
For example, the most importantword subsequence shared between the two sentencesshown in Figure 1 is ?a cat was chased by a dog?which will get penalized by total gap of eight wordscoming from the second sentence and a gap of oneword from the first sentence.
Finally, the kernel isnot sensitive to the relations between the words, forexample, the kernel will consider ?a fat dog?
as acommon subsequence although in the first sentence?a fat?
relates to the cat and not to the dog.2.2 Syntactic Tree KernelSyntactic tree kernels were first introduced byCollins and Duffy (2001) and were also used by401SNPNPDTANNcatPPINwithNPDTaJJredNNcollarVPAUXwasVPVBDchasedADVPNPCDtwoNNSdaysRBagoPPINbyNPDTaJJfatNNdogFigure 3: Syntactic parse tree of the sentence shown in Figure 1 (b).SNPDTAJJfatNNcatVPAUXwasVPVBDchasedPPINbyNPDTaNNdogFigure 2: Syntactic parse tree of the sentence shown inFigure 1 (a).Collins (2002) for the task of re-ranking syntacticparse trees.
They define a kernel between two treesas the number of subtrees shared between them.
Asubtree is defined as any subgraph of the tree whichincludes more than one node, with the restrictionthat entire productions must be included at everynode.
The kernel defined this way captures mostof the structural information present in the syntac-tic parse trees in the form of tree fragments whichthe kernelized learning algorithms can then implic-itly use as features.
The kernel can be computedin O(|N1||N2|) time, where |N1| and |N2| are thenumber of nodes of the two trees.
An efficient al-gorithm to compute tree kernels was given by Mos-chitti (2006a) which runs in close to linear time inthe size of the input trees.One drawback of this tree kernel, though, partic-ularly when used for any task requiring semanticprocessing, is that it may match syntactic subtreesbetween two trees even though they represent verydissimilar things in the sentence.
For example, be-tween the syntactic parse trees shown in Figures 2and 3 for the two sentences shown in Figure 1, thesyntactic tree kernel will find (NP (DT a) JJ NN) as acommon subtree but in the first sentence it represents?cat?
while in the second it represents ?collar?
and?dog?.
It will also find ?
(NP (DT a) (JJ fat) NN)?as a common subtree which again refers to ?cat?
inthe first sentence and ?dog?
in the second sentence.As another example, consider two simple sentences:(S (NP Chip) (VP (V saw) (NP Dale))) and (S (NPMary) (VP (V heard) (NP Sally))).
Even though se-mantically nothing is similar between them, the syn-tactic tree kernel will still find common subtrees (SNP VP), (VP N NP) and (S NP (VP V NP)).
Theunderlying problem is that the syntactic tree kerneltends to overlook the words of the sentences which,in fact, carry the essential semantics.
On the otherhand, although (NP (DT a) (NN cat)) and (NP (DTa) (JJ fat) (NN cat)) represent very similar conceptsbut the kernel will not capture this high level sim-ilarity between the two constituents, and will onlyfind (DT a) and (NN cat) as the common substruc-tures.
Finally, the most important similarity betweenthe two sentences is ?a cat was chased by a dog?which will not be captured by this kernel because402wascata fatchasedbydogaFigure 4: Dependency tree of the sentence shown in Fig-ure 1 (a).
(b)wascata withcollara redchasedbydoga fatagodaystwoFigure 5: Dependency tree of the sentence shown in Fig-ure 1 (b).there is no common subtree which covers it.
TheRelated Work section discusses some modificationsthat have been proposed to the syntactic tree kernel.3 A Dependency-based Word SubsequenceKernelA dependency tree encodes functional relationshipsbetween the words in a sentence (Hudson, 1984).The words of the sentence are the nodes and if aword complements or modifies another word thenthere is a child to parent edge from the first word tothe second word.
Every word in a dependency treehas exactly one parent except for the root word.
Fig-ures 4 and 5 show dependency trees for the two sen-tences shown in Figure 1.
There has been a lot ofprogress in learning dependency tree parsers (Mc-Donald et al, 2005; Koo et al, 2008; Wang et al,2008).
They can also be obtained indirectly fromsyntactic parse trees utilizing the head words of theconstituents.We introduce a new kernel which takes the wordsinto account like the word-subsequence kernel andalso takes the syntactic relations between them intoaccount like the syntactic tree kernel, however, itdoes not have the shortcomings of the two kernelspointed out in the previous section.
This kernelcounts the number of common paths between the de-pendency trees of the two sentences.
Another wayto look at this kernel is that it counts all the commonword subsequences which are linked by dependen-cies.
Hence we will call it a dependency-based wordsubsequence kernel.
Since the implicit features ituses are dependency paths which are enumerable, itis a well defined kernel.
In other words, an examplegets implicitly mapped to the feature space in whicheach dependency path is a dimension.The dependency-based word subsequence kernelwill find the common paths ?a ?
cat?, ?cat ?
was?
chased?, ?chased?
by?
dog?
among many oth-ers between the dependency trees shown in Figures 4and 5.
The arrows are always shown from child nodeto the parent node.
A common path takes into ac-count the direction between the words as well.
Alsonote that it will find the important subsequence ?a?
cat ?
was ?
chased ?
by ?
dog ?
a?
as acommon path.It can be seen that the word subsequences thiskernel considers as common paths are linguisticallymeaningful.
It is also not affected by long-range de-pendencies between words because those words arealways directly linked in a dependency tree.
Thereis no need to allow gaps in this kernel either becauserelated words are always linked.
It also won?t find?a fat?
as a common path because in the first tree?cat?
is between the two words and in the secondsentence ?dog?
is between them.
Thus it does nothave the shortcomings of the word subsequence ker-nel.
It also avoids the shortcomings of the syntac-tic tree kernel because the common paths are wordsthemselves and syntactic labels do not interfere incapturing the similarity between the two sentences.It will not find anything common between depen-dency trees for the sentences ?Chip saw Dale?
and?Mary heard Sally?.
But it will find ?a ?
cat?
as acommon path between ?a cat?
and ?a fat cat?.
Wehowever note that this kernel does not use generalsyntactic categories, unlike the syntactic tree kernel,which will limit its applicability to the tasks whichdepend on the syntactic categories, like re-rankingsyntactic parse trees.403We now give an efficient algorithm to computeall the common paths between two trees.
To ourbest knowledge, no previous work has consideredthis problem.
The key observation for this algo-rithm is that a path in a tree always has a structure inwhich nodes (possibly none) go up to a highest nodefollowed by nodes (possibly none) coming down.Based on this observation we compute two quanti-ties for every pair of nodes between the two trees.We call the first quantity common downward paths(CDP ) between two nodes, one from each tree, andit counts the number of common paths between thetwo trees which originate from those two nodes andwhich always go downward.
For example, the com-mon downward paths between the ?chased?
node ofthe tree in Figure 4 and the ?chased?
node of thetree in Figure 5 are ?chased ?
by?, ?chased ?
by?
dog?
and ?chased ?
by ?
dog ?
a?.
HenceCDP (chased, chased) = 3.
A word may occurmultiple times in a sentence so the CDP values willbe computed separately for each occurrence.
Wewill shortly give a fast recursive algorithm to com-pute CDP values.Once these CDP values are known, using thesethe second quantity is computed which we call com-mon peak paths (CPP ) between every two nodes,one from each tree.
This counts the number of com-mon paths between the two trees which peak at thosetwo nodes, i.e.
these nodes are the highest nodes inthose paths.
For example, ?was?
is the peak for thepath ?a ?
cat ?
was ?
chased?.
Since every com-mon path between the two trees has a unique highestnode, once these CPP values have been computed,the number of common paths between the two treesis simply the sum of all these CPP values.We now describe how all these values are effi-ciently computed.
The CDP values between everytwo nodes n1 and n2 of the trees T1 and T2 respec-tively, is recursively computed as follows:CDP (n1, n2) = 0 if n1.w 6= n2.wotherwise,CDP (n1, n2) =?c1?C(n1)c2?C(n2)c1.w = c2.w(1 + CDP (c1, c2))In the first equation, n.w stands for the word atthe node n. If the words are not equal then therecannot be any common downward paths originatingfrom the nodes.
In the second equation, C(n) rep-resents the set of children nodes of the node n in atree.
If the words at two children nodes are the same,then the number of common downward paths fromthe parent will include all the common downwardpaths at the two children nodes incremented with thelink from the parent to the children.
In addition thepath from parent to the child node is also a commondownward path.
For example, in the trees shownin Figures 4 and 5, the nodes with word ?was?
have?chased?
as a common child.
Hence all the commondownward paths originating from ?chased?
(namely?chased ?
by?, ?chased ?
by ?
dog?
and ?chased?
by ?
dog ?
a?)
when incremented with ?was?
chased?
become common downward paths orig-inating from ?was?.
In addition, the path ?was ?chased?
itself is a common downward path.
Since?cat?
is also a common child at ?was?, it?s commondownward paths will also be added.The CDP values thus computed are then used tocompute the CPP values as follows:CPP (n1, n2) = 0 if n1.w 6= n2.wotherwise,CPP (n1, n2) = CDP (n1, n2) +?c1, c?1?C(n1)c2, c?2?C(n2)c1.w = c2.wc?1.w = c?2.w( 1 + CDP (c1, c2) + CDP (c?1, c?2)+CDP (c1, c2) ?
CDP (c?1, c?2) )If the two nodes are not equal then the number ofcommon paths that peak at them will be zero.
Ifthe nodes are equal, then all the common downwardpaths between them will also be the paths that peakat them, hence it is the first term in the above equa-tion.
Next, the remaining paths that peak at themcan be counted by considering every pair of commonchildren nodes represented by c1 & c2 and c?1 & c?2.For example, for the common node ?was?
in Figures4 and 5, the children nodes ?cat?
and ?chased?
arecommon.
The path ?cat ?
was ?
chased?
is a paththat peaks at ?was?, hence 1 is added in the second404term.
All the downward paths from ?cat?
when in-cremented up to ?was?
and down to ?chased?
are alsothe paths that peak at ?was?
(namely ?a?
cat?was?
chased?).
Similarly, all the downward paths from?chased?
when incremented up to ?was?
and down to?cat?
are also paths that peak at ?was?
(?cat ?
was?
chased ?
by?, ?cat ?
was ?
chased ?
by ?dog?, etc.).
Hence the next two terms are present inthe equation.
Finally, all the downward paths from?cat?
when incremented up to ?was?
and down to ev-ery downward path from ?chased?
are also the pathsthat peak at ?was?
(?a ?
cat ?
was ?
chased ?by?, ?a ?
cat ?
was ?
chased ?
by ?
dog?
etc.
).Hence there is the product term present in the equa-tion.
It is important not to re-count a path from theopposite direction hence the two pairs of commonchildren are considered only once (i.e.
not reconsid-ered symmetrically).The dependency word subsequence kernel be-tween two dependency trees T1 and T2 is then sim-ply:K(T1, T2) =?n1?T1n2?T2n1.w = n2.w(1 + CPP (n1, n2))We also want to count the number of commonwords between the two trees in addition to the num-ber of common paths, hence 1 is added in the equa-tion.
The kernel is normalized to remove any biasdue to different tree sizes:Knormalized(T1, T2) =K(T1, T2)?K(T1, T1) ?K(T2, T2)Since for any long path common between twotrees, there will be many shorter paths within itwhich will be also common between the two trees,it is reasonable to downweight the contribution oflong paths.
We do this by introducing a parameter??
(0, 1] and by downweighting a path of length l by?l.
A similar mechanism was also used in the syn-tactic tree kernel (Collins and Duffy, 2001).The equations for computing CDP and CPPare accordingly modified as follows to accommodatethis downweighting.CDP (n1, n2) = 0 if n1.w 6= n2.wotherwise,CDP (n1, n2) =?c1?C(n1)c2?C(n2)c1.w = c2.w(?
+ ?
?
CDP (c1, c2))CPP (n1, n2) = 0 if n1.w 6= n2.wotherwise,CPP (n1, n2) = CDP (n1, n2) +?c1, c?1?C(n1)c2, c?2?C(n2)c1.w = c2.wc?1.w = c?2.w(?2 + ?
?
CDP (c1, c2)+?
?
CDP (c?1, c?2)+?2 ?
CDP (c1, c2) ?
CDP (c?1, c?2))This algorithm to compute all the common pathsbetween two trees has worst time complexity ofO(|T1||T2|), where |T1| and |T2| are the number ofnodes of the two trees T1 and T2 respectively.
Thisis because CDP computations are needed for everypairs of nodes between the two trees and is recur-sively computed.
Using dynamic programming theirrecomputations can be easily avoided.
The CPPcomputations then simply add the CDP values2.
Ifthe nodes common between the two trees are sparsethen the algorithm will run much faster.
Since thealgorithm only needs to store the CDP values, itsspace complexity is O(|T1||T2|).
Also note that thisalgorithm computes the number of common pathsof all lengths unlike the word subsequence kernelin which the maximum subsequence length needs tobe specified and the time complexity then dependson this length.4 Related WorkSeveral modifications to the syntactic tree kernelshave been proposed to overcome the type of prob-lems pointed out in Subsection 2.2.
Zhang et al(2007) proposed a grammar-driven syntactic treekernel which allows soft matching between the sub-trees of the trees if that is deemed appropriate bythe grammar.
For example, their kernel will be able2This analysis uses the fact that any node in a tree on averagehas O(1) number of children.405to match the subtrees (NP (DT a) (NN cat)) and(NP (DT a ) (JJ fat) (NN cat)) with some penalty.Moschitti (2006b) proposed a partial tree kernelwhich can partially match subtrees.
Moschitti etal.
(2007) proposed a tree kernel over predicate-argument structures of sentences based on the Prob-Bank labels.
Che et al (2006) presented a hy-brid tree kernel which combines a constituent anda path kernel.
We however note that the paths in thiskernel link predicates and their arguments and arevery different from general paths in a tree that ourdependency-based word subsequence kernel uses.Shen et al (2003) proposed a lexicalized syntac-tic tree kernel which utilizes LTAG-based features.Toutanova et al (2004) compute similarity betweentwo HPSG parse trees by finding similarity betweenthe leaf projection paths using string kernels.A few kernels based on dependency trees havealso been proposed.
Zelenko et al (2003) pro-posed a tree kernel over shallow parse tree represen-tations of sentences.
This tree kernel was slightlygeneralized by Culotta and Sorensen (2004) to com-pute similarity between two dependency trees.
Inaddition to the words, this kernel also incorporatesword classes into the kernel.
The kernel is basedon counting matching subsequences of children ofmatching nodes.
But as was also noted in (Bunescuand Mooney, 2005a), this kernel is opaque i.e.
it isnot obvious what the implicit features are and theauthors do not describe it either.
In contrast, ourdependency-based word subsequence kernel, whichalso computes similarity between two dependencytrees, is very transparent with the implicit featuresbeing simply the dependency paths.
Their kernel isalso very time consuming and in their more generalsparse setting it requires O(mn3) time and O(mn2)space, where m and n are the number of nodes ofthe two trees (m >= n) (Zelenko et al, 2003).Bunescu and Mooney (2005a) give a shortest pathdependency kernel for relation extraction.
Their ker-nel, however, does not find similarity between twosentences but between the shortest dependency pathsconnecting the two entities of interests in the sen-tences.
This kernel uses general dependency graphsbut if the graph is a tree then the shortest path isthe only path between the entities.
Their kernel alsouses word classes in addition to the words them-selves.5 ExperimentsWe show that the new dependency-based word sub-sequence kernel performs better than word subse-quence kernel and syntactic tree kernel on the taskof domain-specific semantic parsing.5.1 Semantic ParsingSemantic parsing is the task of converting natu-ral language sentences into their domain-specificcomplete formal meaning representations which anapplication can execute, for example, to answerdatabase queries or to control a robot.
A learn-ing system for semantic parsing induces a seman-tic parser from the training data of natural languagesentences paired with their respective meaning rep-resentations.
KRISP (Kate and Mooney, 2006)is a semantic parser learning system which usesword subsequence kernel based SVM (Cristianiniand Shawe-Taylor, 2000) classifiers and was shownto be robust to noise compared to other semanticparser learners.
The system learns an SVM classi-fier for every production of the meaning representa-tion grammar which tells the probability with whicha substring of the sentence represents the semanticconcept of the production.
Using these classifiersa complete meaning representation of an input sen-tence is obtained by finding the most probable parsewhich covers the whole sentence.
For details pleaserefer to (Kate and Mooney, 2006).The key operation in KRISP is to find the sim-ilarity between any two substrings of two naturallanguage sentences.
Word subsequence kernel wasemployed in (Kate and Mooney, 2006) to computethe similarity between two substrings.
We modi-fied KRISP so that the similarity between two sub-strings can also be computed using the syntactic treekernel and the dependency-based word subsequencekernel.
For applying the syntactic tree kernel, thesyntactic subtree over a substring of a sentence is de-termined from the syntactic tree of the sentence byfinding the lowest common ancestor of the words inthis substring and then considering the smallest sub-tree rooted at this node which includes all the wordsof the substring.
For applying the dependency-basedword subsequence kernel to two substrings of a sen-tence, the kernel computation was suitably modifiedso that the common paths between the two depen-406dency trees always begin and end with the wordspresent in the substrings.
This is achieved by in-cluding only those downward paths in computationsof CDP which end with words within the givensubstrings.
These paths relate the words within thesubstrings perhaps using words outside of these sub-strings.5.2 MethodologyWe measure the performance of KRISP obtained us-ing the three types of kernels on the GEOQUERYcorpus which has been used previously by severalsemantic parsing learning systems.
It contains 880natural language questions about the US geogra-phy paired with their executable meaning represen-tations in a functional query language (Kate et al,2005).
Since the purpose of the experiments is tocompare different kernels and not different seman-tic parsers, we do not compare the performance withother semantic parser learning systems.
The train-ing and testing was done using standard 10-foldcross-validation and the performance was measuredin terms of precision (the percentage of generatedmeaning representations that were correct) and re-call (the percentage of all sentences for which cor-rect meaning representations were obtained).
SinceKRISP assigns confidences to the meaning represen-tations it outputs, an entire range of precision-recalltrade-off can be obtained.
We measure the best F-measure (harmonic mean of precision and recall) ob-tained when the system is trained using increasingamounts of training data.Since we were not interested in the accuracy ofdependency trees or syntactic trees but in the com-parison between various kernels, we worked withgold-standard syntactic trees.
We did not have gold-standard dependency trees available for this cor-pus so we obtained them indirectly from the gold-standard syntactic trees using the head-rules from(Collins, 1999).
We however note that accurate syn-tactic trees can be obtained by training a syntac-tic parser on WSJ treebank and gold-standard parsetrees of some domain-specific sentences (Kate et al,2005).In the experiments, the ?
parameter of thedependency-based word subsequence kernel was setto 0.25, the ?
parameter of the word subsequencekernel was fixed to 0.75 and the downweighting pa-Examples Dependency Word Syntactic40 25.62 21.51 23.6580 45.30 42.77 43.14160 63.78 61.22 59.66320 72.44 70.36 67.05640 77.32 77.82 74.26792 79.79 79.09 76.62Table 1: Results on the semantic parsing task with in-creasing number of training examples using dependency-based word subsequence kernel, word subsequence ker-nel and syntactic tree kernel.rameter for the syntactic tree kernel was fixed to 0.4.These were determined through pilot experimentswith a smaller portion of the data set.
The maxi-mum length of subsequences required by the wordsubsequence kernel was fixed to 3, a longer lengthwas not found to improve the performance and wasonly increasing the running time.5.3 ResultsTable 1 shows the results.
The dependency-basedword subsequence kernel always performs betterthan the syntactic tree kernel.
All the numbersunder the dependency kernel were found statisti-cally significant (p < 0.05) over the correspond-ing numbers under the syntactic tree kernel based onpaired t-tests.
The improvement of the dependency-based word subsequence kernel over the word sub-sequence kernel is greater with less training data,showing that the dependency information is moreuseful when the training data is limited.
The per-formance converges with higher amounts of trainingdata.
The numbers shown in bold were found statis-tically significant over the corresponding numbersunder the word subsequence kernel.It may be noted that syntactic tree kernel is mostlydoing worse than the word subsequence kernel.
Webelieve this is because of the shortcomings of thesyntactic tree kernel pointed out in Subsection 2.2.Since this is a semantic processing task, the wordsplay an important role and the generalized syntacticcategories are not very helpful.6 Future WorkIn future, the dependency-based word subsequencekernel could be extended to incorporate word classes407like the kernels presented in (Bunescu and Mooney,2005a; Zelenko et al, 2003).
It should be possible toachieve this by incorporating matches between wordclasses in addition to the exact word matches in thekernel computations similar to the way in which theword subsequence kernel was extended to incorpo-rate word classes in (Bunescu and Mooney, 2005b).This will generalize the kernel and make it more ro-bust to data sparsity.The dependency-based word subsequence kernelcould be tested on other tasks which require comput-ing similarity between sentences or texts, like textclassification, paraphrasing, summarization etc.
Webelieve this kernel will help improve performance onthose tasks.7 ConclusionsWe introduced a new kernel which finds similaritybetween two sentences as the number of commonpaths shared between their dependency trees.
Thiskernel can also be looked upon as an improved wordsubsequence kernels which only counts the commonword subsequences which are related by dependen-cies.
We also gave an efficient algorithm to computethis kernel.
The kernel was shown to out-performthe word subsequence kernel and the syntactic treekernel on the task of semantic parsing.ReferencesRazvan C. Bunescu and Raymond J. Mooney.
2005a.A shortest path dependency kernel for relation extrac-tion.
In Proc.
of HLT/EMNLP-05, pages 724?731,Vancouver, BC, October.Razvan C. Bunescu and Raymond J. Mooney.
2005b.Subsequence kernels for relation extraction.
InY.
Weiss, B. Scho?lkopf, and J. Platt, editors, Advancesin Neural Information Processing Systems 18, Vancou-ver, BC.Nicola Cancedda, Eric Gaussier, Cyril Goutte, andJean Michel Renders.
2003.
Word sequence kernels.Journal of Machine Learning Research, Special Issueon Machine Learning Methods for Text and Images,3:1059?1082, February.Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.2006.
A hybrid convolution tree kernel for semanticrole labeling.
In Proc.
of COLING/ACL-06, pages 73?80, Sydney, Australia, July.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Proc.
of NIPS-2001.Michael Collins.
1999.
Head-driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Michael Collins.
2002.
New ranking algorithms for pars-ing and tagging: Kernels over discrete structures, andthe voted perceptron.
In Proc.
of ACL-2002, pages263?270, Philadelphia, PA, July.Nello Cristianini and John Shawe-Taylor.
2000.
AnIntroduction to Support Vector Machines and OtherKernel-based Learning Methods.
Cambridge Univer-sity Press.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proc.
of ACL-04, pages 423?429, Barcelona, Spain, July.Richard Hudson.
1984.
Word Grammar.
Blackwell.Rohit J. Kate and Raymond J. Mooney.
2006.
Us-ing string-kernels for learning semantic parsers.
InProc.
of COLING/ACl-06, pages 913?920, Sydney,Australia, July.Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney.2005.
Learning to transform natural to formal lan-guages.
In Proc.
AAAI-2005, pages 1062?1068, Pitts-burgh, PA, July.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Proc.of ACL-08, pages 595?603, Columbus, Ohio, June.Huma Lodhi, Craig Saunders, John Shawe-Taylor, NelloCristianini, and Chris Watkins.
2002.
Text classifica-tion using string kernels.
Journal of Machine LearningResearch, 2:419?444.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proc.
ofHLT/EMNLP-05, pages 523?530, Vancouver, BC.Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,and Suresh Manandhar.
2007.
Exploiting syntacticand shallow semantic kernels for question answer clas-sification.
In Proc.
of ACL-07, pages 776?783, Prague,Czech Republic, June.Alessandro Moschitti.
2006a.
Making tree kernels prac-tical for natural language learning.
In Proc.
of EACL-06, pages 113?120, Trento, Italy, April.Alessandro Moschitti.
2006b.
Syntactic kernels for natu-ral language learning: the semantic role labeling case.In Proc.
of HLT/NAACL-06, short papers, pages 97?100, New York City, USA, June.Juho Rousu and John Shawe-Taylor.
2005.
Efficientcomputation of gapped substring kernels on large al-phabets.
Journal of Machine Learning Research,6:1323?1344.Libin Shen, Anoop Sarkar, and Aravind Joshi.
2003.
Us-ing ltag based features in parse reranking.
In Proc.
ofEMNLP-2003, pages 89?96, Sapporo, Japan, July.408Kristina Toutanova, Penka Markova, and ChristopherManning.
2004.
The leaf projection path viewof parse trees: Exploring string kernels for HPSGparse selection.
In Proc.
EMNLP-04, pages 166?173,Barcelona, Spain, July.Vladimir N. Vapnik.
1998.
Statistical Learning Theory.John Wiley & Sons.Qin Iris Wang, Dale Schuurmans, and Dekang Lin.
2008.Semi-supervised convex training for dependency pars-ing.
In Proceedings of ACL-08: HLT, pages 532?540,Columbus, Ohio, June.D.
Zelenko, C. Aone, and A. Richardella.
2003.
Kernelmethods for relation extraction.
Journal of MachineLearning Research, 3:1083?1106.Min Zhang, Wanxiang Che, Aiti Aw, Chew Lim Tan,Guodong Zhou, Ting Liu, and Sheng Li.
2007.
Agrammar-driven convolution tree kernel for semanticrole classification.
In Proc.
of ACL-2007, pages 200?207, Prague, Czech Republic, June.409
