PROBABIL IST IC  TREE-ADJO IN ING GRAMMAR AS A FRAMEWORKFOR STAT IST ICAL  NATURAL LANGUAGE PROCESSINGPhilip Resnik*Department of Computer and Information ScienceUniversity of Pennsylvania, Philadelphia, Pennsylvania 19104, USAresnik@Iinc.cis, upenn, eduAbst ractIn this paper, I argue for the use of a probabilisticform of tree-adjoining grammar (TAG) in statisticalnatural anguage processing.
I first discuss two previ-ous statistical approaches --- one that concentrates onthe probabilities of structural operations, and anotherthat emphasizes co, occurrence relationships betweenwords.
I argue that a purely structural approach, ex-emplified by probabilistie context-free grammar, lackssufficient sensitivity to lexical context, and, conversely,that lexical co-occurence analyses require a richer no-tion of locality that is best provided by importing somenotion of structure.I then propose probabilistie TAG as a frameworkfor statistical language modelling, arguing that it pro-vides an advantageous combination of structure, local-ity, and lexical sensitivity.
Issues in the acquisition ofprobabilistie TAG and parameter stimation are brieflyconsidered.1 Probab i l i s t i c  CFGsThanks to the increased availability of text corpora,fast computers, and inexpensive off-line storagc, sta-tistical approaches to issues in natural \]mlguage pro-cessing are enjoying a surge of interest, across a widevariety of applications.
As research iu tbis area pro-gresses, the question of how to combine our existingknowledge of grammatical methods (e.g.
generativepower, eff?cient parsing) with dcvelol)ments in statisti-cal and information-theoretic methods (especially tech-niques imported from the speech-processing commu-nity) takes on increasing significance.Perhaps the most straightforward combination ofgrammatical nd statistical techniques can be found inthe probabilistic generalization of context-free gram-*This reaeardl was supported by the following remts: ARODAAL 03~9-C-0031, DARPA N00014-90-J-1863, NSF lRI 90-16592, and Ben Franklin 91S.3078C-1, I would like to thankAravind Joshi, Yves Schabes, and members ofthe CLIFF groupat Penn for helpful discussion.
(S~) S ~ NP VP (0.5)($2) S ~ S PP (0.35)($3) S ~ V NP (0.15)(VP,) VP ~ V NP (0.6)(VP2) VP ~ V PP (0.4)Figure 1: Fragment of a conlext-fl~e g~ammar.SNP VP (rl = S ~ NP VP)N VP (r2 = NP ~ N)we VP (rs = N ~ we)we V NP (r4 = VP ---* V NP)we like NP (r5 = V ---* like)we like N (r6 = NP ---, N)we like Mary (r7 = N ~ Mary)Figure 2: A context-free derivationmars.
\[Jelinek et al, 1990\] characterize a probabilisticcontext-free grammar (PCFG) as a context-free gram-mar in which each production has been assigned aprob-ability of use.
I will refer to the entire set of probabili-ties as the statistical parameters, or simply parameters,of the probabilistie grammar.For exmnple, the productions in Figure 1, togetherwith their associated parameters, might comprise afragment of a PCFG for English.
Note that for eachnonterminal symbol, the probabilities of its alternativeexpansions um to 1.
Tbis captures the fact that ina context-free derivation, each instance of a nontermi-na\] symbol must be rewritten according to exactly oneof its expansions.
Also by definition of a context-freederivation, each rewriting is independent of the con-text within which the nonterminal appears.
So, forexample, in the (leftmost) derivation of We like Mary(Figure 2), the expansions N ~ we and VP --~ V NPare independent events.The probability of n independent events isACTES DE COLING-92, NANTES, 23-28 Aot)r 1992 4 1 8 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992merely the product of the probabilities of each in-dividual event.
Therefbre the probability of acontext-frce derivation with rewritings rt, r~, .
.
.
,  rn isPr(r l )Pr(r2) .
.
.
Pr(rn).Jelinek et al use this fact to develop a set of efficientalgorithms (based on the CKY parsing algorithm) tocompute the probability of a sentence given a grammar,to find the most probable parse of a given sentence, tocompute the probability of an initial substring leadingto a sentence generated by the grarmnar, and, following\[Baker, 1979\], to estimate the statistical parameters fora given grammar using a large text corpus.Now, the definition of the probability of a deriva-tion that is used for PCFG depends crucially upon thecontext-freeness of the grammar; without it, the inde-pendence assumption that pernfits us to simply multi-ply probabilities i  compromised.
Yet experience tellsus that rule expmmions are not, in general, context-free.Consider the following simple example.
According toone frequency estimate of English usage \[~?tmeis andKucera, 1982\], he is more titan three times as likelyas we to appear as a subject pronoun.
So if we is re-placed with he in Figure 2, the new derivation (of Hehke Mary) is accorded far higher probability accordingto the PCFG, even though He like Mary is not En-glish.
The problem extends beyond such obvious casesof agreenmnt: since crushing happens to be more likelythan mashing, the probability of deriving lie's crushingpotatoes is greater than the probability correspondingderivation for He's mashing potatoes, although we ex-pect the latter to be more likely.lu short, lexical context matters.
Although probn-bilistic context-free grammar captures the fact that notall nonterminal rewritiugs are equally likely, its insen-sitivity to lexical context makes it less than adequateas a statistical model of natural anguage, t and weak-eus Jelinek et al's contention that "in an ambiguousbut appropriately chosen probabilistic CFG (PCFG),correct parses are Ifigh probability parses" (p. 2).
Inpractical terms, it also suggests that techniques for in-corporatiug lexical context-sensitivity will potentiallyimprove PCFG performance.2 Co-occur rence  Sta t i s t i csA second common approach to the corpus-based in-vestigation of natural language considers instances ofwords occurring adjacent o each other in the text, or,more generally, words occurring together within a win-dow of fixed size.
For example, \[Church and tlanks,1989\] use an information-theoretic measure, mutualinformation, to calculate the degree of association of1 Independent of the statiatical i~ue, of course, it is also gen-erally accepted that CFGs are gcneratively too weak to modelnatural ltmguage in its full generality \[Shieber, 1985\].word pairs based upon their co-occurrence throughouta large corpus.
Tile nmtual information between twoevents is defined as?
Pr(x, y)l(x; Y) = tOgp~)where, for tile purposes of corpus analysis, Pr(x) andPr(y) are the respective probabilities of words x and yappearing within the corpus, and Pr(z, y) is the proba-bility of word x being followed by word y within a win-dow of w words.
Intuitively, mutual information relatesthe actual probability of seeing x and y together (nu-merator) to the probability of seeing them together un-der the as~sumption that they were independent events(denominator).As defined here, tbe calculation of mutual infor-mation takes into account only information about co-occurrence within surface strings.
A difficulty withsuch an al~proash, however, is that cO-occurrences ofinterest may not lie sufficiently local.
Although sen-tence (1)a can witness the co-oceurence of students andspend within a small window (say, w = 2 or 3), sen-tence (1)b cannot.(1)a.
Students pend a lot of money.b.
Students who attend conferances spend a lotof money.Simply increasing the window size will not suffice, fortwo rc~sons.
First, there is no bound on the lengthof relative clauses such as the one in (1)b, hence nofixed value of w that will solve the problem.
Second,the choice of window size depends on the application- -  Church and Hanks write that "smaller window sizeswill identify fixed expressions (idioms) and other rela-tions that hold over short ranges; larger window sizeswill highlight semantic oncepts and other relationshipsthat hold over larger scales" (p. 77).
Extending thewindow size in order to capture co-occurrences such asthe one found in (1)1) may therefore undermine othergoals of the analysis.\[Brown et ul., 1991\] encounter a similar problem.Their application is statistics-driven machine transla-tion, where the statistics are calculated on the basis oftrigranm (observed triples) of words within the sourceand target corpora.
As in (1), sentences (2)a and (3)aillustrate a difficulty encountered when limiting win-dow size.
Light verb constructions are an example ofa situation in which the correct ranslation of the verbdepends on the identity of its object.
The correct wordsense of p~ndre, "to make," is chosen in (2)a, sincethe dependency signalling that word sense - -  betweenprcndre and its object, decision - -  falls within a tri-gram window and thus within the bounds of their lan-guage model.(2)a.
prendre la decisionACrEs DE COLING-92, NANTES, 23-28 ^ ofrr 1992 4 1 9 PROC.
OF COL1NG-92, NANTES, AUC;, 23-28, 1992b.
make the decision(3)a. prendre une di\]fficile dgcisionb.
*take a difficult decisionIn (3)a, however, the dependency is not sufficiently lo-cal: the model has no access to lexical relationshipsspanning a distance of more than three words.
With-out additional information about the verb's object, thesystem relies on the fact that the sense of prendre a.~ "totake" occurs with greater frequency, and tire incorrecttranslation results.Brown et al propose to solve the problem by seek-ing clues to a word's sense within a larger context.Each possible clue, or informant, is e~ site relative to theword.
For prendre, the informant of interest is "the firstnoun to the right"; other potential informants include"the first verb to the right," and so on.
A set of such po-tential informants i defined in advance, and statisticaltechniques are used to identify, for each word, whichpotential informant contribntes most to determiningthe sense of the word to use.It seems clear that in most cases, Brown et al'sinformants represent approximations to structural re-lationships uch as subject and object.
Examples (1)through (3) suggest hat attention to structure wouldhave advantages for the analysis of lexical relationships.By using co-occurrence within a structure rather thanco-occurrence within a window, it is possible to cap-ture lexical relationships without regard to irrelevantintervening material.
One way to express this notionis by saying that structural relationships permit us touse a notion of locality that is more general than sim-ple distance in the surface string.
So, for example, (1)demonstrates that the relationship between a verb andits subject is not affected by a subject relative clanseof any length.3 Hybrid ApproachesIn the previous ections I have observed that proba-billstic CFGs, though capable of capturing the relativelikelihoods of context-free derivations, require greatersensitivity to lexical context in order to model naturallanguages.
Conversely, lexica\] co-occurrence analysesseem to require a richer notion of locality, somethingthat structural relationships such as subject and objectcan provide.
In this section I briefly discuss two pro-posals that could be considered "hybrid" approaches,making use of both grammatical structure and lexicalco-occurrence,The first of these, \[Hindle, 1990\], continues in thedirection suggested at the end of the previous ection:rather than using "informants" to approximate struc-tural relationships, lexical eo-oecnrrenee is calculatedover structures.
Hindle uses co-occurrence statistics,collected over parse trees, in order to classify nounson the basis of the verb contexts in which they ap-pear.
First, a robust parser is used to obtain a parsetree (possibly partial) for each sentence.
For example,the following table contains some of the informationHindle's parser retrieved from the parse of the sen-tence "The clothes we wear, the food we eat, the airwe breathe, the watcr we drink, the land that sustainsus, and many of the prodncts we use are the result ofagricultural research.
"I verb \] subject I object Ie~t we foodbreathe we airdrink we watersust'ain land usNext, Hindle calculates a similarity measure basedon tile mutual information of verbs and their argu-ments: nouns that tend to appear as subjects and ob-jects of the sanae verbs are judged more similar.
Ac-cording to the criterion that that two nouns be recip-rocally most similar to each other, Hindle's analysisderives such plausible pairs of similar nonns as rulingand decision, battle and fight, researcher and scientist.A proposal in \[Magerman and Marcus, 1991\] re-lates to probabilistie parsing.
As in PCFG, Magermanand Marcus associate probabilities with the rules of acontext-free grammar, but these probabilities are con-ditioned on the contexts within which a rule is observedto be used.
It is in the formulation of "context" thatlexical co-occurrence plays a role, albeit an indirect one.Magerman and Marcus's Pearl parser is essentiallya bottom-up chart parser with Earley-style top-downprediction.
When a context-free rule A -~ a l .
.
.a~is proposed as an entry in ttle chart spanning inputsymbols al through ai, it is assigned a "score" basedon1.
tile rule that generated this instance of nontermi-hal symbol A, and2.
tile part-of-speech trigram centered at al.For example, given the input My first love was namedPearl, a proposed chart entry VP---+ V NP startingits span at love (i.e., a theory trying to interpretlove as a verb) would be scored on the basis of therule that generated the VP (in this ease, probablyS ---* NP VP) together with the part-of-speech trigram"adjective verb verb."
This score would be lower thanthat of a different chart entry interpreting love as anoun, since the latter would be scored using the morelikely part of speech trigram "adjective noun verb".So, although tire context-free probability favors the in-terpretation of love as the beginning of a verb phrase,ACRES DI" COLUqG-92, NANTES, 23-28 AOl3"r 1992 4 2 0 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992information about the lexical context of the word res-cues the correct interpretation, in which it is catego-rized as a noun.Although Pearl does not take into account ile re-lationships between particular words, it does representa promising combiuation of CFG-based probahilisticparsing mid corpus statistics calculated on the basis ofsimple (trigram) co-occurrences.A difficulty with the hybrid approaches, however,is that they leave unclear exactly what the statisticalmodel of tile language is.
In probabilistic ontext:i~eegrannnar and in surface-string analysis of lexical c(>occurrence, there is a precise deIinition of the eventspace -- what events go into making up a sentence.
(As discussed earlier, the events in a PCFG derivationare the rule expansions; the events in a window-baqedanalysis of surface strings call be viewed as transitionsin a finite Markov chain modelling the language.)
Theabsence of sitch a characterization in the hybrid ap-proaches makes it more difficult to identify what as-sumptions are being made, and gives such work a de-cidedly empirical flavor.4 Probab i l i s t i c  TAG4.1 Lex ica l i zed  TAGThe previous ections have demonstrated a need for awell-defned statistical framework in which both syn-tactic structure and texical co, occurrences are incor-porated.
In thLs section, I argue that a probabilis-tic form of lexicalized tree-adjoining grammar providesjust such a seamless combination.
\[begin with a briefdescription of lcxicalized tree-adjoining grammar, andthen present its probabilistic generalization a d the ad-vantages of the resulting formalism.
"l~'ec~adjoining grammar \[Joshi ctaL ,  1975\], orTAG, is a generalization of context-fl'ee grannnar thathas been proposed as a useful forlualism for the studyof natural anguages.
A tree-adjoining grammar eom-priscs two sets of elementary structures: initial treesand auxiliary trees.
(See l,'igure 3, in which initial andauxiliary trees are labelled using cr and f3, respectively.
)An auxiliary tree, by definition, has a uonterminal nodeon its frontier - tile fool node -- that matches thenonterminal symbol at its root.These elementary structures can be combined us-ing two operations, substitution and adjunction.
Thesubstitution operation corresponds to the rewriting ofa symbol in a context-free derivation: a nonterminalnode at the frontier of a tree is replaced with au initialtree having the same nonterminal symbol at its rout.So, for example, one could expand either NP in al byrewriting it as tree ccu.S NI' NP NNP VP N N Adj N /V NP pesamU people roaslcxlteat(al) (~) (cd) (ill)Figure 3: Examples of TAG elementary freesFigure 4: Ad3unetion of auxiliary tree fl into free a atnode 71, ~*sulting in derived structure 7.The adjunction operation is a generalization f sub-stitution that permits internal as well as frontier nodesto be expanded.
One adjoins an auxiliary tree fl intoa tree c~ at node r I by "splitting" ~ horizontally at r 1and then inserting ft.
In the resulting structure, 7, f)'sroot node appears in tfs original position, mid its footnode dominates all the material that r I donfinated pre-viously (see Figure 4).
For example, l"igure 5 shows T1,the result of adjoining auxiliary tree ~1 at the N nodeof e~2.
}br context-free grmnmar, each derived structureis itself a record of the operations that produced it:one Call simply read from a parse tree tile context-freerules that were used in the parse.
In contrm~t, a derivedstructure in a tree-adjoining grammar is distinct fromits derivation history.
The final parse tree encodes theslruclure of the sentence according to the grammar,but the events that constitute the derivation historythat is, the subsitutions and adjunctions that tookplace are not directly encoded.
The significance ofthis distinction will become apparent shortly.A lcxicalized tree-adjoining gra~nmar is it TAG inwhich each elemcutary structure (initial or auxiliarytree) has a lexieal item on its frontier, known as itsanchor \[Schabes, 1990\].
Another natural way to saythis is that each lexical item has associated with itNPIN/~dj NIroasted peanuti(rl)Figure 5: Result of adjoiaiu 9 fll into ol~.ACRES 1OE COLING-92, NANTES, 23-28 AOl~l" 1992 4 2 1 PROC.
Or; COLING-92, NANTJ!S.
AUG. 23-28, 1992a set of structures that, taken together, characterizethe contexts within which that item can appear.
2 Forexample, tree al in Figure 3 represents one possiblestructural context for eat; another would be an initialtree in which eat appears as an intransitive verb, andyet another would be a tree containing eat within apassive construction.4.2 Features  of  Probabi l i s t ic  TAGThree features of lexicalized TAG urake it particularlyappropriate as a probabilistic framework for naturallanguage processing.
First, since every tree is associ-ated with a lexica\] anchor, words and their associatedstructures are tightly linked.
Thus, unlike probabilisticcontext-free grammar, the probabilities associated withstructural operations are sensitive to lexical context.
Inparticular, the probability of an adjunetion step suchas the one that produced 71, above, is sensitive to thelexical anchors of the trees.
One would expect a similaradjunction of ~1 into aa (resulting in the string roastedpeople) to have extremely low probability, reflecting thelow degree of association between the lexical items in-volved.
Similarly, tree aa is far more likely than a2 tobe substituted as the subject NP in tree ~1, since peo-ple is more likely than peanuts to appear as the subjectof eat.Notice that this attention to lexical context is notacquired at the expense of the independence assump-tion for probabilities.
Just as expansion of a nonter-minal node in a CFG derivation takes place withoutregard to that node's history, substitution or adjunc-tion to a node during a TAG derivation takes placewithout regard to that node's history, a Thus it will bestraightforward to express the probability of a proba-bilistic TAG derivation as a product of probabilities,just as was the case for PCFG, yielding a well-definedstatistical model.Second, since TAG factors local dependencies fromrecursion, the problem of intervening material inwindow-based lexical co-occurrence analyses does notarise.
Notice that in tree c~t, the verb, its subject, andits object all appear together within a single elementarystructure.
The introduction of recursive substructure- -  relative clauses, adverbs, strings of adjectival mod-ifiers - -  is done entirely by means of the adjunctionoperation, as illustrated by figures 4 and 5.
This factprovides a principled treatment ofexamples such as (4):the probability of substituting cr2 as the object NP ofax (capturing an association between eat and peanuts)2LexicMized TAG is  on ly  one of meaty sudl graznmar for-ma l i sms ,  of course.3Schabes (personal communication) capture~ the generallza-tion quite nicely in obscxving that for both CFG derivation treesand TAG derivation histories, the path setA (set of possible pathsfrom root  to frontier) are regular.is independent of any other operations that might takeplace, such as the introduction (via adjunction) of anadjectival modifier.
Similarly, the probability of substi-tuting aa as the subject NP of al  is not affected by thesubsequent adjunction of a relative clause (cf.
exam-pie (1)).
Thus, in contrast o a co-occnrrence analysisbased on strings, the analysis of (4) within probabilis-tic TAG finds precisely the same relationship betweenthe verb and its arguments in (4)b and (4)c as it doesin (4)a.(4)a.
People eat peanuts.b.
People eat roasted peanuts.c.
People who are v~iting the zoo eat roastedpeanuts.Third, the notion of lexical anchor in lexicalizedTAG has been generalized to account for multi-wordlexicat entries \[Abeill6 and Schabes, 1989\].
Thus theformalism appears to satisfy the criteria of \[Smadja andMcKeown, 1990\], who write of the need for "a flexiblelexicon capable of using single word entries, multipleword entries as well as phrasal templates and a mecha-nism that would be able to gracefully merge and com-bine them with other types of constraints" (p. 256).Among "other types of constraints" are linguistic cri-teria, and here, too, TAG offers the potential for cap-turing the details of linguistic and statistical facts in auniform way \[Kroch mad Joshi, 1985\].4.3  Formal i za t ion  of  P robab i l i s t i c  TAGThere are a number of ways to express lexicalizedtree-adjoining rammar as a probabilistic grammarformalism.
4 Here I propose what appears to me tobe the most direct probabilistic generalization of lex-iealized TAG; a different treatment can be found in\[Schabes, 1992\].Definitions:Let I denote the set of initial trees in the grammar,and A the set of auxiliary trees.Each tree in a lexlcalized TAG has a (possiblyempty) subset of its frontier nodes marked asnodes at which substitution may take place.
Givena tree c~, let that subset be denoted by s(a).Adjunction may take place at any node r/ inlabelled by a nonterminal symbol, so long as *7s(~).
Denote this set of possible adjunetion odesn(.).
54 The idea of defining probabilities over derivations involvingcombinations of elementary structur~ was introduced as earlyas \[J~hi, 1973\].~Note that ~almtitution nodes and adjmmtion odes ar~ notdisting~fished in Figure 3.AcrEs DE COLING-92, NANTES, 23-28 nO~" 1992 4 2 2 PROC.
OF COLING-92, NA/CrES, AUG. 23-28, 1992Let S(a,  a ' ,  r/) denote the event of substituting treec~ * into tree c~ at node r/.Let A(a,~,O) denote the event of adjoining aux-iliary tree ~ into tree c~ at node r/, and letA(o?, none, U) denote the event in which no adjunc-tion is performed at that node.Let f~ = the set of all substitution and adjunctionevents.A probabilis~ic tree-adjoining rammar is a 5-tuple,(I, A, P1, Ps, Pa),  where I and A are defined as above,PI is a function from I to the real interval \[0,1\], andPs and PA are functions from fl to \[0,1\], such that:1.
~ Pr (a )= 1aEl2.
Voe;uaV, o ( ,  ) ~ PS(S(~,~?,'J)) = lc~EI3.
Vaelua Vires(a) ~ PA(A(ot, fl, ,)) = 1flEAU(.o.?
)Pl(c~) is interpreted as the probability that a derivationbegins with initial tree a.  Ps(S(ce, cd, r/)) denotes theprobability of substituting cd at node ~t of tree a, andPa(A(a,~,r/)) denotes the probability of adjoining flat node 7/of tree a (where PA(A(a, none,t\])) denotesthe probability that no adjunetion takes place at nodeo of a).A TAG derivation is described by the initial treewith which it starts, together with the sequence ofsubstitutions and adjunction operations that then takeplace.
Denoting each operation as op(cq,a2,~l),op E{S,A}, and denoting the initial tree with whieb timderivation starts as ~0, the probability of a TAG deriva-tion ~-= (,to, opt ( .
.
. )
.
.
.
.
.
op , ( .
.
. )
)  i~Pr(~) = P~(,~o) ~I Po~,(op,(...))l<i<_nThis definition is directly analogous to the probabilityof a context-free derivation r = ( rh .
.
.
,  r , ) ,Pr(r )  = PI(S) H P(ri),l<i<_nthough in a CFG every derivation starts with S and soPI(S) -= 1.Does probabilistic TAG, thus formalized, behave asdesired?
Returning to example (4), consider the fol-lowing derivation history for the sentence People eatroasted peanuts:(OtlS(al,aa, NP?~j) ,S(ai,a2, NPobj),A(a2,/~1, N))Notice first that, given intuitively reasonable sti-mates for the probabilities of substitution and adjune-tion, the probability of this derivation would indeedbe far greater than for the corresponding derivation ofPeanuts eat roasted people.
Thus, in contrast o PCFG,probabitistic TAG's sensitivity to lexieal context doesprovide a more accurate language model.
In addition,were this derivation to be used as an observation forthe purpose of estimating probabilities, the estimateof Ps(S(at,?t3, NPs,,bj)) would be unaffected by theevent A(a2, /!/1, N).
That  is, the model would in factcapture a relationship between the verb eat and its ob-ject peanuts, mediated by the trees that they anchor,regardless of intervening material in the surface string.4.4 Acquisition of Probabilistic TAGDespite the attractive theoretical features of proba-bilistic TAG, many practical issues remain.
Foremostamong these is the acquisition of the statistical param-eters.
\[Sehabes, 1992\] has recently adapted the Inside-Outside algorithm, used for estinaating the parametersof a probabilistic CFG \[Baker, 1979\], to probabilistieTAG.
The Inside-Outside algorithm is itself a gener-alization of tile Forward-Backward algorithm used totrain hidden Markov models \[Baum, 1972\].
It opti-mizes by starting with a set of initial pareaneters (cho-sen randomly, or uniformly, or perhaps initially set onthe basis of a priori knowledge), then iteratively col-lecting statistics over the training data (using the exist-ing parameters) and then re-estimating the parameterson the basis of those statistics.
Each re-estimation stepguarantees improved or at worst equivalent parameters,according to a maximmn-likelihood criterion.The procedure immediately raises two questions,regardless of whether the probabilistic formalism un-der cousideration is finite-state, context-free, or tree-adjoining.
First, since the algorithm re-estimates pa-rameters but does not determine the rules in the gram-mar, the structures underlying the statistics (ares,rules, trees) must bc determined in some other fashion.The starting point can be a hand-written grammar,which requires considerable effort and linguistic knowl-edge; altcrnatively, one can initially include all possiblestructures and let tim statistics weed out useless rulesby assigning them near-zero probability.
A third pos-sibility is to add an engine that hypothesizes rules onthe basis of observed ata, a~ld then let the parameterestimation operate over these hypothesized structures.Which possibility is best depends upon the applicationand the breadth of linguistic coverage needed.Second, any statistical approach to natural lan-guage nmst consider tim size of the parameter set tobe estimated.
Additional parameters can enhance thetheoretical accuracy of a statistical model (e.g., extend-ing a trigram model to 4-grams) but may also lead to anACTES DE COL1NG-92.
NANqES.
23-28 Ao(rr 1992 4 2 3 PRoc.
ov COLING-92.
NArCrES, Au6.23-28, 1992unmanageable number of parameters to store and re-trieve, much less estimate.
In addition, as the numberof parameters grows, more data are required in orderto collect accurate statistics.
For example, both (5)aand (5)b witness the same relationship between calledand son within a trigram model.(5)a.
Mary called her son.b.
Mary called her son Junior.Within a recent lexicalized TAG for English \[Abeill~el al., 1990\], however, these two instances of calledare associated with two distinct initiM trees, reflectingthe different syntactic structures of the two examples.Thus observations that would be the same for the tri-gram model can be fragmented in the TAG model.
Asa result of this fragmentation, each individual event isobserved fewer times, and so the model is more vulner-able to statistical inaccuracy resulting from low counts.The acquisition of a probababilistie tree-adjoininggrammar "from the ground up," that is, including hy-pothesizing rammatical structures as well as estimat-ing statistical parameters, i  an intended topic of futurework.5 Conc lus ions1 have argued that probabilistic 'rAG provides a seam-less framework that combines the simplicity and struc-ture of the probabilistie CFG approach with the lex-ieal sensitivity of string-based co-occurrence analyses.Within a wide variety of applications, it appears thatvarious researchers (e.g.
\[tIindle, 1990; Magerman andMarcus, 1991; Brown el al., 1991; Smadja and MeK-eown, 1990\]) are confronting issues similar to thosediscussed here.
As the resources required for statis-tical approaches to natural language continue to be-come more readily available, these issues will take onincreasing importance, and the need for a frameworkthat unifies grammatical nd statistical techniques willcontinue to grow.
Probabilistie TAG is one step towardsuch a unifying framework.References\[Abeill~ and Schabes, 1989\] Anne Abeill~ and Yves Sch-abes.
Parsing idioms iu tree adjoining grammars.In Fourth ConJerence of the European Chapter of theAssociation .\[or Computational Linguistics (EACL'89),Manchester, 1989.\[Abeill~ et al, 1990\] Anne Abeill6,Kathleen Bishop, Sharon Cote, and Yves Schabes.
Alexicaiized tree adjoining grammar for english.
Techni-call Report MS-CIS-90-24, Department of Computer andInformation Science, University of Penusylvania, April1990.\[Baker, 1979\] J.K. Baker.
Trainable grammars for speechrecognition.
In Proceedings of the Spring Conferenceo.\[ the Acoustical Society of America, pages 547-550,Boston, MA, June 1979.\[Baum, 1972\] L.E.
Bantu.
An inequality and associatedmaximization technique in statistical estimation of prob-abifistic functions of a Markov process.
Inequalities, 3:1-8, 1972.\[Brown et al, 1991\] P. Brown, S. Della Pietra, V. DellaPietra, and R. Mercer.
A statistical approach tosense disambiguation i  machine translation.
In FourthDARPA Workshop on Speech and Natural Language, Pa-cific Grove, CA, February 1991.\[Church and Hanks, 1989\] K. Church and P. Hanks.
Worda.ssociz~tion norms, nmtual information, and lexicogra-phy.
In Proceedings o.\[ the ~Tth Meeting o\] the Associ-ation \]or Computational Linguistics, 1989.
Vancouver,B.C.\[Francis and Kucera, 1982\] W. Francis and H. Kucera.
Fre-quency Analysis of English Usage.
Houghton Mifflin Co.:New York, 1982.\[Hindle, 1990\] D. Hindle.
Noun classification frmnpredicate-argument structures.
In Proceedings o.\[ the 1~HthAnnual Meeting of the Assocation of Computational Lin-guistics, Pittsburgh, Penna., pages 268-275, 1990.\[Jelinek et al, 1990\] F. Jelinek, J. D. Lafferty, and R. L.Mercer.
Basic methods of probabilistic ontext free gram-mars.
Research Report RC 16374 (#72684), IBM, York-town Heights, New York 10598, 1990.\[Joshi el al., 1975\] Aravind Joshi, Leon Levy, and M Taka-hashi.
Tree adjunct grammars.
Journal of the Computerand System Sciences, 10, 1975.\[Joshi, 1973\] Aravind Joshi.
Remarks on some aspects oflanguage structure and their relevance to pattern analy-sis.
Pattern Recognition, 5:365-381, 1973.\[Kroch and Joshi, 1985\] Anthony Kroch and Aravind K.Joshi.
Linguistic relevance of tree adjoining grammars.Technical Report MS-CIS-85-18, Department of Com-puter and Information Science, University of Pennsyl-vania, April 1985./\[Magerman d Marcus, 1991\] D. Magerman and M. Mar-cus.
Pearl: A probabilistic chart parser.
In FourthDARPA Workshop on Speech and Natural Language, Pa-cific Grove, CA, February 1991.\[Schabes, 1990\] Yves Schabes.
Mathematical and Compu-tational Aspects of Lexicalized Grammars.
PhD thesis,Univ.
of Pennsylvania, 1990.\[Schabes, 1992\] Yves Schabes.
Stochastic lexicaiized tree-adjoining grammars, 1992.
This proceedings.\[Shieber, 1985\] Stuart Shieber.
Evidence against thecontext-freeness of natural anguage.
Linguistics andPhilosophy, 8:333-343, 1985.\[Smadja nd McKeown, 1990\] F. Smadja and K. McKe-own.
Automatically extracting and representing collo-cations for language generation.
In Proceedings of theP8th Annual Meeting o\] the Assocation of ComputationalLinguistics, pages 252-259, 1990.ACRES DE COLING-92, NANTES, 23-28 Ao(rr 1992 4 2 4 Paoc.
or: COLING-92, NANTES, AUO.
23-28, 1992
