OVERVIEW OF TREC-1Donna HarmanNational Institute of  Standards and TechnologyGaithersburg, Md.
20899ABSTRACTThe first Text REtrieval Conference (TREC-I) washeld in early November 1992 and was attended by about100 people working in the 25 participating groups.
Thegoal of the conference was to bring research groupstogether to discuss their work on a new large test collec-tion.
There was a large variety of retrieval techniquesreported on, including methods using automatic thesaurii,sophisticated term weighting, natural language tech-niques, relevance feedback, and advanced pattern match-ing.
As results had been run through a common evalua-tion package, groups were able to compare theeffectiveness of different techniques, and discuss howdifferences among the systems affected performance.1.
INTRODUCTIONThere is a long history of experimentation in infor-mation retrieval.
Research started with experiments inindexing languages, uch as the Cranfield I tests \[1\], andhas continued with over 30 years of experimentationwith the retrieval engines themselves.
The Cranfield IIstudies \[2\] showed that automatic indexing was compar-able to manual indexing, and this and the availability ofcomputers created a major interest in the automaticindexing and searching of texts.
The Cranfield experi-ments also emphasized the importance of creating testcollections and using these for comparative valuation.The Cranfield collection, created in the late 1960's, con-tained 1400 documents and 225 queries, and has beenheavily used by researchers since then.
Subsequentlyother collections have been built, such as the CACMcollection \[3\], and the NPL collection \[4\].In the thirty or so years of experimentation therehave been two missing elements.
First, although someresearch groups have used the same collections, therehas been no concerted effort by groups to work with thesame data, use the same evaluation techniques, and gen-erally compare results across systems.
The importanceof this is not to show any system to be superior, but toallow comparison across a very wide variety of tech-niques, much wider than only one research group wouldtackle.
Karen Sparck Jones in 1981 \[5\] commented that:Yet the most slriking feature of the test historyof the past two decades is its lack ofconsolidation.
It is true that some very broadgeneralizations have been endorsed by succes-sive tests: for example...but there has been areal failure at the detailed level to build onetest on another.
As a result there are no expla-nations for these generalizations, and hence nomeans of knowing whether improved systemscould be designed (p. 245).This consolidation is more likely ff groups can compareresults across the same data, using the same evaluationmethod, and then meet to discuss openly how methodsdiffer.The second missing element, which has become criti-cal in the last ten years, is the lack of a realistically-sized test collection.
Evaluation using the small collec-tions currently available may not reflect performance ofsystems in large full-text searching, and certainly doesnot demonstrate any proven abilities of these systems tooperate in real-world information retrieval environments.This is a major barrier to the transfer of these laboratorysystems into the commercial world.
Additionally sometechniques such as the use of phrases and the construc-tion of automatic thesaurii seem intuitively workable, buthave repeatedly failed to show improvement in perfor-mance using the small collections.
Larger collectionsmight demonslrate he effectiveness of these procedures.The overall goal of the Text REtrieval Conference(TREC) is to address these two missing elements.
It ishoped that by providing a very large test collection, andencouraging interaction with other groups in a friendlyevaluation forum, a new thrust in information retrievalwill occur.
There is also an increased interest in thisfield within the DARPA community, and TREC isdesigned to be a showcase of the state-of-the-art inretrieval research.
NIST's goal as co-sponsor of TRECis to encourage communication and technology transferamong academia, industry, and government.The following description was excerpted from a morelengthy overview published in the conference proceed-ings \[6\].
The full proceedings also contain papers by allparticipants and results for all systems.612.
THE TASK2.1 IntroductionTREC is dcsigned to encouraage r search in informationretrieval using large data collections.
Two types ofretrieval and being examined -- retrieval using an "ad-hoc" query such as a researcher might use in a libraryenvironment, and retrieval using a "routing" query suchas a profile to filter some incoming document stream.
Itis assumed that potential users need the ability to doboth high precision and high recall searches, and are wil-ling to look at many documents and repeatedly modifyqueries in order to get high recall.
Obviously theywould like a system that makes this as easy as possible,but this ease should be reflected in TREC as added intel-ligence in the system rather than as special interfaces.Since 'IREC has been designed to evaluate system per-formance both in a routing (filtering or profiling) mode,and in an ad-hoc mode, both functions need to be tested.The test design was based on traditional informationretrieval models, and evaluation used traditional recalland precision measures.
The following diagram of thetest design shows the various components of TREC (Fig-ure 1).50TrainingTopics50TestTopicsQI 02 Q~Training Routing Ad-hocQuedes Queries Quedes-1 GigabyteTrainingDocuments(DI)I .
~1 Gigabyte TestDocuments(02)Figure 1 -- The TREC TaskThis diagram reflects the four data sets (2 sets of topicsand 2 sets of documents) that were provided to partici-pants.
These data sets (along with a set of samplerelevance judgments for the 50 training topics) wereused to construct three sets of queries.
Q1 is the set ofqueries (probably multiple sets) created to help in adjust-ing a system to this task, create better weighting algo-rithms, and in general to train the system fox testing.The results of this research were used to create Q2, therouting queries to be used against he test documents.Q3 is the set of queries created from the test topics asad-hoc queries for searching against he combined ocu-ments (both training documents and test documents).The results from searches using Q2 and Q3 were theofficial test results.
The queries could be constructedusing one of three alternative methods.
They could beconstructed automatically from the topics, with nohuman intervention.
Alternatively they could be con-structed manually from the topic, but with no "retries"after looking at the results.
The third method allowed"retries", but under eonslrained conditions.2.2 The  Par t i c ipantsThere were 25 participating systems in TREC-1, using awide range of retrieval techniques.
The participantswere able to choose from three levels of participation:Category A, full participation, Category B, full participa-tion using a reduced dataset (25 topics and 1/4 of thefull document set), and Category C for evaluation only(to allow commercial systems to protect proprietaryalgorithms).
The program committee selected onlytwenty category A and B groups to present alks becauseof limited conference time, and requested that the rest ofthe groups present posters.
All groups were asked tosubmit papers for the proceedings.Each group was provided the data and asked to turn ineither one or two sets of results for each topic.
Whentwo sets of results were sent, they could be made usingdifferent methods of creating queries (methods 1, 2, or3), or by using different parameter settings for one querycreation method.
Groups could chose to do the routingtask, the adhoc task, or both, and were requested to sub-mit the top 200 documents retrieved for each topic forevaluation.3.
THE TEST  COLLECT IONCritical to the success of TREC was the creation of thetest collection.
Like most traditional retrieval collec-tions, there are three distinct parts to this collection.The first is the documents themselves -- the training set(D1) and the test set (D2).
Both were distributed asCD-ROMs with about 1 gigabyte of data each,compressed to fit.
The training topics, the test topics62and the relevance judgments were supplied by email.These components of the test collection -- the docu-ments, the topics, and the relevance judgments, are dis-cussed in the rest of this section.3.1 The DocumentsThe documents came from the following sources.Disk 1WSJ -- Wall Street Journal (1986, 1987, 1988, 1989)AP -- AP Newswire (1989)ZIFF -- Information from Computer Select disks(Ziff-Davis Pubfishing)FR -- Federal Register (1989)DOE -- Short abstracts from Department of EnergyDisk 2WSJ -- Wall Street Journal (1990, 1991, 1992)AP -- AP Newswire (1988)ZIFF -- Information from Computer Select disks(Ziff-Davis Pubfishing)FR -- Federal Register (1988)The particular sources were selected because theyreflected the different types of documents used in theimagined TREC application.
Specifically they had avaried length, a varied writing style, a varied level ofediting and a varied vocabulary.
All participants wererequired to sign a detailed user agreement for the data inorder to protect the copyrighted source material.
Thedocuments were uniformly formatted into an SGML-Iikestructure, as can be seen in the following example.<IX)C><DOCNO> WZJ880406--0090 </DOCNO><HL> AT&T Unveils Services to Upgrade PhoneNetworks Under Global Plan </HI.,><AUTHOR> Janet Guyon (WSJ Staf0 </AUTHOR><DATELINE> NEW YORK </DATELINE><TEXT>American Telephone & Telegraph Co. introducedthe first of a new generation of phone services withbroad implications for computer and communicationsequipment markets.AT&T said it is the first national ong-distance car-tier to announce prices for specific services under aworld-wide standardization plan to upgrade phone net-works.
By announcing commercial services under theplan, which the industry calls the Integrated ServicesDigital Network, AT&T will influence volving com-munications tandards to its advantage, consultantssaid, just as International Business Machines Corp. hascreated de facto computer standards favoring its pro-ducts.</TEXT></DOC>All documents had beginning and end markers, and aunique DOCNO id field.
Additionally other fields takenfrom the initial data appeared, but these varied widelyacross the different sources.
The documents also haddifferent amounts of errors, which were not checked orcorrected.
Not only would this have been an impossibletask, but the errors in the data provided a better simula-tion of the real-world tasks.
Table 1 shows some basicdocument collection statistics.Subset of collectionSize of collection(megabytes)(disk 1)(disk 2)Number of records(disk 1)(disk 2)Median number ofterms per record(disk 1)(disk 2)Average number ofterms per record(disk 1)(disk 2)TABLE 1DOCUMENT STATISTICSWSJ AP ZIFF295 266 7251255 248 18898,736 84.930 75,18074,520 79,923 56,920182 353 181218 346 167329 375 412377 370 394FR25821126,20720,1083133151017107/3DOE190226,0878289Note that although the collection sizes are roughlyequivalent in megabytes, there is a range of documentlengths from very short documents (DOE) to very long(FR).
Also the range of document lengths within a col-lection varies.
For example, the documents from AP aresimilar in length (the median and the average length arevery close), but the WSJ and ZIFF documents have awider range of lengths.
The documents from the FederalRegister (FR) have a very wide range of lengths.What does this mean to the TREC task?
First, amajor portion of the effort for TREC-1 was spent in thesystem engineering necessary to handle the huge numberof documents.
This means that little time was left forsystem tuning or experimental runs, and therefore theTREC-1 results can best be viewed as a baseline forlater research.
The longer documents also requiredmajor adjustments o the algorithms themselves (or lossof performance).
This is particularly true for the verylong documents in FR.
Since a relevant document might63contain only one or two relevant sentences, many algo-rithms needed adjustment from working with the abstractlength documents found in the old collections.
Addition-ally many documents were composite stories, withdifferent opics, and this caused problems for most algo-rithms.3.2 The TopicsIn designing the TREC task, there was a conscious deci-sion made to provide "user need" statements rather thanmore traditional queries.
Two major issues wereinvolved in this decision.
First there was a desire toallow a wide range of query construction methods bykeeping the topic (the need statement) distinct from thequery (the actual text submitted to the system).
Thesecond issue was the ability to increase the amount ofinformation available about each topic, in particular toinclude with each topic a clear statement of what criteriamake a document relevant.
The topics were designed tomimic a real user's need, and were written by peoplewho are actual users of a retrieval system.
Although thesubject domain of the topics was diverse, some con-sideration was given to the documents to be searched.The following is one of the topics used in TREC.<top><head> Tipster Topic Description<num> Number: 066<dom> Domain: Science and Technology<title> Topic: Natural Language Processing<desc> Description: Document will identify a type ofnatural anguage processing technology which is beingdeveloped or marketed in the U.S.<narr> Narrative: A relevant document will identify acompany or institution developing or marketing anatural language processing technology, identify thetechnology, and identify one or more features of thecompany's product.<con> Concept(s):1. natural anguage processing2.
translation, language, dictionary, font3.
software applications<fac> Factor(s):<nat> Nationality: U.S.</fac><def> Definition(s):</top>3.3 The Relevance JudgmentsThe relevance judgments are of critical importance to atest collection.
For each topic it is necessary to compilea list of relevant documents; hopefully as comprehensivea list as possible.
Relevance judgments were made usinga sampling method, with the sample constructed bytaking the top 100 documents retrieved by each systemfor a given topic and merging them into a pool forrelevance assessment.
This sampling, known as pooling,proved to be an effective method.
There was little over-lap among the 25 systems in their retrieved ocuments.For example, out of a maximum of 3300 unique docu-ments (33 runs times 100 documents), over one-thirdwere actually unique.
This means that the different sys-tems were finding different documents as likely relevantdocuments for a topic.
One reason for the lack of over-lap is the very large number of documents that containmany of the same keywords as the relevant documents,but probably a larger reason is the very different sets ofkeywords in the constructed queries.
This lack of over-lap should improve the coverage of the relevance set,and verifies the use of the pooling methodology to pro-duce the sample.The merged list of results was then shown to the humanassessors.
Each topic was judged by a single assessor toinsure the best consistency of judgment and varyingnumbers of documents were judged relevant o the topics(with a median of about 250 documents).4.
PREL IMINARY RESULTSAn important element of TREC was to provide a com-mon evaluation forum.
Standard recall/precision figureswere calculated for each system and the tables andgraphs for the results are presented in the proceedings.The results of the TREC-1 conference can be viewedonly as a preliminary baseline for what can be expectedfrom systems working with large test collections.
Thereare several reasons for this.
First, the deadlines forresults were very tight, and most groups had minimaltime for experiments.
Additionally groups were workingblindly as to what constitutes a relevant document.There were no reliable relevance judgments for training,and the use of the structured topics was completely new.It can be expected that the results seen at the secondTREC conference will be much better, and also moreindicative of how well a method works.However there were some clear trends that emerged.Automatic construction of queries proved to be aseffective as manual construction of queries.
Figure 2shows a comparison of four sets of results, two usingautomatic query construction and two using manualquery construction, and it can be seen that there is rela-tively little difference between the results.6410.90.80.70.6O?
~ 0.50.40.30.20.10Adhoc Manual vs AutomaticI I I I I0.00 0.10 0.20 0.30 0.40 0.50 0.60RecallAutomatic , Automatic , Manual0.70 0.80 0.90 1.00ManualFigure 2 -- A Comparison of Adhoc Results using Different Query Construction MethodsThe two automatic systems hown used basically allthe terms in the topic as query terms, and relied onautomatic term weighting and sophisticated ranking algo-rithms for performance.
The manual systems also usedsophisticated term weighting and algorithms, but manu-ally selected which terms to include in a query.Several minor trends were also noticeable.
Systemsthat worked with subdocuments, or used local term con-text to improve term weighting, seemed particularly suc-cessful in handling the longer documents in TREC.More systems may investigate this approach in TREC-2.Also systems that attempted to expand a topic beyond itsoriginal terms (either manually or automatically) seemedto do well, although it was often hard to properly controlthis expansion (particularly for automatically expandedqueries).
These trends may continue in TREC-2 and itis expected that clearer trends will emerge as groupshave more time to work at this new task.5.
REFERENCES\[1\] Cleverdon C.W., (1962).
Report on the Testing andAnalysis of an Investigation into the ComparativeEfficiency of Indexing Systems.
College of Aeronautics,Cranfield, England, 1962.Factors Determining the Performance of Indexing Sys-terns, Vol.
1: Design, Vol.
2: Test Results.
AslibCranfield Research Project, Cranfield, England, 1966.\[3\] Fox E. (1983).
Characteristics of Two New Experi-mental Collections in Computer and Information ScienceContaining Textual and Bibliographic Concepts.
Techni-cal Report TR 83-561, Cornell University: ComputingScience Department.\[4\] Sparck Jones K. and C. Webster (1979).
Research inRelevance Weighting, British Library Research andDevelopment Report 5553, Computer Laboratory,University of Cambridge.\[5\] Sparck Jones K. (1981).
Information Retrieval Exper-iment.
London, England: Butterworths.\[6\] Harman D. "The First Text REtrieval Conference(TREC1)."
National Institute of Standards and Technol-ogy Special Publication 500-207, Gaithersburg, Md.20899 (in press, available in May 1993).\[2\] Cleverdon C.W., Mills, J. and Keen E.M. (1966).65
