Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 377?382,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsExperiments with crowdsourced re-annotation of a POS tagging data setDirk Hovy, Barbara Plank, and Anders S?gaardCenter for Language TechnologyUniversity of CopenhagenNjalsgade 140, 2300 Copenhagen{dirk|bplank}@cst.dk, soegaard@hum.ku.dkAbstractCrowdsourcing lets us collect multiple an-notations for an item from several annota-tors.
Typically, these are annotations fornon-sequential classification tasks.
Whilethere has been some work on crowdsourc-ing named entity annotations, researchershave largely assumed that syntactic taskssuch as part-of-speech (POS) tagging can-not be crowdsourced.
This paper showsthat workers can actually annotate sequen-tial data almost as well as experts.
Fur-ther, we show that the models learned fromcrowdsourced annotations fare as well asthe models learned from expert annota-tions in downstream tasks.1 IntroductionTraining good predictive NLP models typically re-quires annotated data, but getting professional an-notators to build useful data sets is often time-consuming and expensive.
Snow et al (2008)showed, however, that crowdsourced annotationscan produce similar results to annotations madeby experts.
Crowdsourcing services such as Ama-zon?s Mechanical Turk has since been successfullyused for various annotation tasks in NLP (Jha etal., 2010; Callison-Burch and Dredze, 2010).However, most applications of crowdsourcingin NLP have been concerned with classificationproblems, such as document classification andconstructing lexica (Callison-Burch and Dredze,2010).
A large part of NLP problems, however, arestructured prediction tasks.
Typically, sequencelabeling tasks employ a larger set of labels thanclassification problems, as well as complex inter-actions between the annotations.
Disagreementamong annotators is therefore potentially higher,and the task of annotating structured data thusharder.Only a few recent studies have investi-gated crowdsourcing sequential tasks; specifically,named entity recognition (Finin et al, 2010; Ro-drigues et al, 2013).
Results for this are good.However, named entities typically use only few la-bels (LOC, ORG, and PER), and the data containsmostly non-entities, so the complexity is manage-able.
The question of whether a more linguisti-cally involved structured task like part-of-speech(POS) tagging can be crowdsourced has remainedlargely unaddressed.1In this paper, we investigate how well lay anno-tators can produce POS labels for Twitter data.
Inour setup, we present annotators with one word ata time, with a minimal surrounding context (twowords to each side).
Our choice of annotatingTwitter data is not coincidental: with the short-lived nature of Twitter messages, models quicklylose predictive power (Eisenstein, 2013), and re-training models on new samples of more represen-tative data becomes necessary.
Expensive profes-sional annotation may be prohibitive for keepingNLP models up-to-date with linguistic and topicalchanges on Twitter.
We use a minimum of instruc-tions and require few qualifications.Obviously, lay annotation is generally less re-liable than professional annotation.
It is there-fore common to aggregate over multiple annota-tions for the same item to get more robust anno-tations.
In this paper we compare two aggrega-tion schemes, namely majority voting (MV) andMACE (Hovy et al, 2013).
We also show how wecan use Wiktionary, a crowdsourced lexicon, to fil-ter crowdsourced annotations.
We evaluate the an-notations in several ways: (a) by testing their ac-curacy with respect to a gold standard, (b) by eval-uating the performance of POS models trained on1One of the reviewers alerted us to an unpublished mas-ters thesis, which uses pre-annotation to reduce tagging tofewer multiple-choice questions.
See Related Work sectionfor details.377the annotations across several existing data sets,as well as (c) by applying our models in down-stream tasks.
We show that with minimal contextand annotation effort, we can produce structuredannotations of near-expert quality.
We also showthat these annotations lead to better POS taggingmodels than previous models learned from crowd-sourced lexicons (Li et al, 2012).
Finally, weshow that models learned from these annotationsare competitive with models learned from expertannotations on various downstream tasks.2 Our ApproachWe crowdsource the training section of the datafrom Gimpel et al (2011)2with POS tags.
We useCrowdflower,3to collect five annotations for eachword, and then find the most likely label for eachword among the possible annotations.
See Figure1 for an example.
If the correct label is not amongthe annotations, we are unable to recover the cor-rect answer.
This was the case for 1497 instancesin our data (cf.
the token ?:?
in the example).We thus report on oracle score, i.e., the best labelsequence that could possibly be found, which iscorrect except for the missing tokens.
Note thatwhile we report agreement between the crowd-sourced annotations and the crowdsourced anno-tations, our main evaluations are based on modelslearned from expert vs. crowdsourced annotationsand downstream applications thereof (chunkingand NER).
We take care in evaluating our modelsacross different data sets to avoid biasing ourevaluations to particular annotations.
All the datasets used in our experiments are publicly availableat http://lowlands.ku.dk/results/.x Z y@USER NOUN,NOUN,X,NOUN,-,NOUN NOUN: .,.,-,.,.,.
XI PRON,NOUN,PRON,NOUN,PRON,- PRONowe VERB,VERB,-,VERB,VERB,VERB VERBU PRON,X,-,NOUN,NOUN,PRON PRON?
= 0.9, 0.4, 0.2, 0.8, 0.8, 0.9Figure 1: Five annotations per token, supplied by 6different annotators (- = missing annotation), goldlabel y. ?
= competence values for each annotator.2http://www.ark.cs.cmu.edu/TweetNLP/3http://crowdflower.com3 Crowdsourcing Sequential AnnotationIn order to use the annotations to train models thatcan be applied across various data sets, i.e., mak-ing out-of-sample evaluation possible (see Section5), we follow Hovy et al (2014) in using the uni-versal tag set (Petrov et al, 2012) with 12 labels.Figure 2: Screen shot of the annotation interfaceon CrowdflowerAnnotators were given a bold-faced word withtwo words on either side and asked to select themost appropriate tag from a drop down menu.
Foreach tag, we spell out the name of the syntacticcategory, and provide a few example words.See Figure 2 for a screenshot of the interface.Annotators were also told that words can belongto several classes, depending on the context.
Noadditional guidelines were given.Only trusted annotators (in Crowdflower:Bronze skills) that had answered correctly on 4gold tokens (randomly chosen from a set of 20gold tokens provided by the authors) were allowedto submit annotations.
In total, 177 individualannotators supplied answers.
We paid annotatorsa reward of $0.05 for 10 tokens.
The full data setcontains 14,619 tokens.
Completion of the tasktook slightly less than 10 days.
Contributors werevery satisfied with the task (4.5 on a scale from 1to 5).
In particular, they felt instructions were clear(4.4/5), and that the pay was reasonable (4.1/5).4 Label AggregationAfter collecting the annotations, we need to aggre-gate the annotations to derive a single answer foreach token.
In the simplest scheme, we choose themajority label, i.e., the label picked by most an-notators.
In case of ties, we select the final labelat random.
Since this is a stochastic process, weaverage results over 100 runs.
We refer to this asMAJORITY VOTING (MV).
Note that in MV wetrust all annotators to the same degree.
However,crowdsourcing attracts people with different mo-378tives, and not all of them are equally reliable?even the ones with Bronze level.
Ideally, we wouldlike to factor this into our decision process.We use MACE4(Hovy et al, 2013) as our sec-ond scheme to learn both the most likely answerand a competence estimate for each of the annota-tors.
MACE treats annotator competence and thecorrect answer as hidden variables and estimatestheir parameters via EM (Dempster et al, 1977).We use MACE with default parameter settings togive us the weighted average for each annotatedexample.Finally, we also tried applying the joint learn-ing scheme in Rodrigues et al (2013), but theirscheme requires that entire sequences are anno-tated by the same annotators, which we don?t have,and it expects BIO sequences, rather than POStags.Dictionaries Decoding tasks profit from the useof dictionaries (Merialdo, 1994; Johnson, 2007;Ravi and Knight, 2009) by restricting the numberof tags that need to be considered for each word,also known as type constraints (T?ackstr?om et al,2013).
We follow Li et al (2012) in includingWiktionary information as type constraints intoour decoding: if a word is found in Wiktionary,we disregard all annotations that are not licensedby the dictionary entry.
If the word is not found inWiktionary, or if none of its annotations is licensedby Wiktionary, we keep the original annotations.Since we aggregate annotations independently(unlike Viterbi decoding), we basically use Wik-tionary as a pre-filtering step, such that MV andMACE only operate on the reduced annotations.5 ExperimentsEach of the two aggregation schemes above pro-duces a final label sequence y?
for our training cor-pus.
We evaluate the resulting annotated data inthree ways.1.
We compare y?
to the available expert annota-tion on the training data.
This tells us how similarlay annotation is to professional annotation.2.
Ultimately, we want to use structured anno-tations for supervised training, where annotationquality influences model performance on held-outtest data.
To test this, we train a CRF model(Lafferty et al, 2001) with simple orthographicfeatures and word clusters (Owoputi et al, 2013)4http://www.isi.edu/publications/licensed-sw/mace/on the annotated Twitter data described in Gim-pel et al (2011).
Leaving out the dedicated testset to avoid in-sample bias, we evaluate our mod-els across three data sets: RITTER (the 10% testsplit of the data in Ritter et al (2011) used in Der-czynski et al (2013)), the test set from Foster etal.
(2011), and the data set described in Hovy etal.
(2014).We will make the preprocessed data sets avail-able to the public to facilitate comparison.
In ad-dition to a supervised model trained on expert an-notations, we compare our tagging accuracy withthat of a weakly supervised system (Li et al, 2012)re-trained on 400,000 unlabeled tweets to adapt toTwitter, but using a crowdsourced lexicon, namelyWiktionary, to constrain inference.
We use param-eter settings from Li et al (2012), as well as theirWikipedia dump, available from their project web-site.53.
POS tagging is often the first step for furtheranalysis, such as chunking, parsing, etc.
Wetest the downstream performance of the POSmodels from the previous step on chunking andNER.
We use the models to annotate the trainingdata portion of each task with POS tags, anduse them as features in a chunking and NERmodel.
For both tasks, we train a CRF modelon the respective (POS-augmented) training set,and evaluate it on several held-out test sets.
Forchunking, we use the test sets from Foster et al(2011) and Ritter et al (2011) (with the splitsfrom Derczynski et al (2013)).
For NER, we usedata from Finin et al (2010) and again Ritter et al(2011).
For chunking, we follow Sha and Pereira(2003) for the set of features, including tokenand POS information.
For NER, we use standardfeatures, including POS tags (from the previousexperiments), indicators for hyphens, digits,single quotes, upper/lowercase, 3-character prefixand suffix information, and Brown word clusterfeatures6with 2,4,8,16 bitstring prefixes estimatedfrom a large Twitter corpus (Owoputi et al, 2013).We report macro-averages over all these data sets.6 ResultsAgreement with expert annotators Table 1shows the accuracy of each aggregation comparedto the gold labels.
The crowdsourced annotations5https://code.google.com/p/wikily-supervised-pos-tagger/6http://www.ark.cs.cmu.edu/TweetNLP/379majority 79.54MACE-EM 79.89majority+Wiktionary 80.58MACE-EM+Wiktionary 80.75oracle 89.63Table 1: Accuracy (%) of different annotations wrtgold dataaggregated using MV agree with the expert anno-tations in 79.54% of the cases.
If we pre-filter thedata using Wiktionary, the agreement becomes80.58%.
MACE leads to higher agreement withexpert annotations under both conditions (79.89and 80.75).
The small difference indicates thatannotators are consistent and largely reliable,thus confirming the Bronze-level qualificationwe required.
Both schemes cannot recover thecorrect answer for the 1497 cases where none ofthe crowdsourced labels matched the gold label,i.e.
y /?
Zi.
The best possible result either of themcould achieve (the oracle) would be matching allbut the missing labels, an agreement of 89.63%.Most of the cases where the correct label wasnot among the annotations belong to a small setof confusions.
The most frequent was mislabeling?:?
and ?.
.
.
?, both mapped to X. Annotatorsmostly decided to label these tokens as punctu-ation (.).
They also predominantly labeled your,my and this as PRON (for the former two), and avariety of labels for the latter, when the gold labelis DET.RITTER FOSTER HOVYLi et al (2012) 73.8 77.4 79.7MV 80.5 81.6 83.7MACE 80.4 81.7 82.6MV+Wik 80.4 82.1 83.7MACE+Wik 80.5 81.9 83.7Upper boundsoracle 82.4 83.7 85.1gold 82.6 84.7 86.8Table 2: POS tagging accuracies (%).Effect on POS Tagging Accuracy Usually, wedon?t want to match a gold standard, but werather want to create new annotated trainingdata.
Crowdsourcing matches our gold standardto about 80%, but the question remains how usefulthis data is when training models on it.
After all,inter-annotator agreement among professional an-notators on this task is only around 90% (Gimpelet al, 2011; Hovy et al, 2014).
In order to evalu-ate how much each aggregation scheme influencestagging performance of the resulting model, wetrain separate models on each scheme?s annota-tions and test on the same four data sets.
Table2 shows the results.
Note that the differences be-tween the four schemes are insignificant.
Moreimportantly, however, POS tagging accuracy us-ing crowdsourced annotations are on average only2.6% worse than gold using professional annota-tions.
On the other hand, performance is muchbetter than the weakly supervised approach by Liet al (2012), which only relies on a crowdsourcedPOS lexicon.POS model from CHUNKING NERMV 74.80 75.74MACE 75.04 75.83MV+Wik 75.86 76.08MACE+Wik 75.86 76.15Upper boundsoracle 76.22 75.85gold 79.97 75.81Table 3: Downstream accuracy for chunking (l)and NER (r) of models using POS.Downstream Performance Table 3 shows theaccuracy when using the POS models trainedin the previous evaluation step.
Note that wepresent the average over the two data sets usedfor each task.
Note also how the Wiktionary con-straints lead to improvements in downstream per-formance.
In chunking, we see that using thecrowdsourced annotations leads to worse perfor-mance than using the professional annotations.For NER, however, we find that some of the POStaggers trained on aggregated data produce bet-ter NER performance than POS taggers trained onexpert-annotated gold data.
Since the only dif-ference between models are the respective POSfeatures, the results suggest that at least for sometasks, POS taggers learned from crowdsourced an-notations may be as good as those learned fromexpert annotations.7 Related WorkThere is considerable work in the literature onmodeling answer correctness and annotator com-petence as latent variables (Dawid and Skene,3801979; Smyth et al, 1995; Carpenter, 2008; White-hill et al, 2009; Welinder et al, 2010; Yan et al,2010; Raykar and Yu, 2012).
Rodrigues et al(2013) recently presented a sequential model forthis.
They estimate annotator competence as la-tent variables in a CRF model using EM.
Theyevaluate their approach on synthetic and NER dataannotated on Mechanical Turk, showing improve-ments over the MV baselines and the multi-labelmodel by Dredze et al (2009).
The latter do notmodel annotator reliability but rather model labelpriors by integrating them into the CRF objective,and re-estimating them during learning.
Both re-quire annotators to supply a full sentence, whilewe use minimal context, which requires less anno-tator commitment and makes the task more flexi-ble.
Unfortunately, we could not run those mod-els on our data due to label incompatibility andthe fact that we typically do not have complete se-quences annotated by the same annotators.Mainzer (2011) actually presents an earlier pa-per on crowdsourcing POS tagging.
However, itdiffers from our approach in several ways.
It usesthe Penn Treebank tag set to annotate Wikipediadata (which is much more canonical than Twitter)via a Java applet.
The applet automatically labelscertain categories, and only presents the users witha series of multiple choice questions for the re-mainder.
This is highly effective, as it eliminatessome sources of possible disagreement.
In con-trast, we do not pre-label any tokens, but alwayspresent the annotators with all labels.8 ConclusionWe use crowdsourcing to collect POS annotationswith minimal context (five-word windows).
Whilethe performance of POS models learned fromthis data is still slightly below that of modelstrained on expert annotations, models learnedfrom aggregations approach oracle performancefor POS tagging.
In general, we find that theuse of a dictionary tends to make aggregationsmore useful, irrespective of aggregation method.For some downstream tasks, models using theaggregated POS tags perform even better thanmodels using expert-annotated tags.AcknowledgmentsWe would like to thank the anonymous review-ers for valuable comments and feedback.
This re-search is funded by the ERC Starting Grant LOW-LANDS No.
313695.ReferencesChris Callison-Burch and Mark Dredze.
2010.
Creat-ing Speech and Language Data With Amazon?s Me-chanical Turk.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk.Bob Carpenter.
2008.
Multilevel Bayesian models ofcategorical data annotation.
Technical report, Ling-Pipe.A.
Philip Dawid and Allan M. Skene.
1979.
Max-imum likelihood estimation of observer error-ratesusing the EM algorithm.
Applied Statistics, pages20?28.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incompletedata via the EM algorithm.
Journal of the Royal Sta-tistical Society.
Series B (Methodological), 39(1):1?38.Leon Derczynski, Alan Ritter, Sam Clark, and KalinaBontcheva.
2013.
Twitter part-of-speech taggingfor all: overcoming sparse and noisy data.
InRANLP.Mark Dredze, Partha Pratim Talukdar, and Koby Cram-mer.
2009.
Sequence learning from data with multi-ple labels.
In ECML/PKDD Workshop on Learningfrom Multi-Label Data.Jacob Eisenstein.
2013.
What to do about bad lan-guage on the internet.
In NAACL.Tim Finin, Will Murnane, Anand Karandikar, NicholasKeller, Justin Martineau, and Mark Dredze.
2010.Annotating named entities in Twitter data withcrowdsourcing.
In NAACL-HLT 2010 Workshop onCreating Speech and Language Data with Amazon?sMechanical Turk.Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,Josef Le Roux, Joakim Nivre, Deirde Hogan, andJosef van Genabith.
2011.
From news to comments:Resources and benchmarks for parsing the languageof Web 2.0.
In IJCNLP.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-Speech Taggingfor Twitter: Annotation, Features, and Experiments.In ACL.Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,and Eduard Hovy.
2013.
Learning whom to trustwith MACE.
In NAACL.Dirk Hovy, Barbara Plank, and Anders S?gaard.
2014.When pos datasets don t add up: Combatting samplebias.
In LREC.381Mukund Jha, Jacob Andreas, Kapil Thadani, SaraRosenthal, and Kathleen McKeown.
2010.
Corpuscreation for new genres: A crowdsourced approachto pp attachment.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk.
Associationfor Computational Linguistics.Mark Johnson.
2007.
Why doesn?t EM find goodHMM POS-taggers.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL).John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: prob-abilistic models for segmenting and labeling se-quence data.
In ICML.Shen Li, Jo?ao Grac?a, and Ben Taskar.
2012.
Wiki-lysupervised part-of-speech tagging.
In EMNLP.Jacob Emil Mainzer.
2011.
Labeling parts ofspeech using untrained annotators on mechanicalturk.
Master?s thesis, The Ohio State University.Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational linguistics,20(2):155?171.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InNAACL.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In LREC.Sujith Ravi and Kevin Knight.
2009.
Minimized Mod-els for Unsupervised Part-of-Speech Tagging.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP.
Association for Computational Lin-guistics.Vikas C. Raykar and Shipeng Yu.
2012.
Eliminat-ing Spammers and Ranking Annotators for Crowd-sourced Labeling Tasks.
Journal of Machine Learn-ing Research, 13:491?518.Alan Ritter, Sam Clark, Oren Etzioni, et al 2011.Named entity recognition in tweets: an experimentalstudy.
In EMNLP.Filipe Rodrigues, Francisco Pereira, and BernardeteRibeiro.
2013.
Sequence labeling with multiple an-notators.
Machine Learning, pages 1?17.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In NAACL.Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Per-ona, and Pierre Baldi.
1995.
Inferring ground truthfrom subjective labelling of Venus images.
Ad-vances in neural information processing systems,pages 1085?1092.Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?Evaluating non-expert annotations for natural lan-guage tasks.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing.
Association for Computational Linguistics.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, RyanMcDonald, and Joakim Nivre.
2013.
Token andtype constraints for cross-lingual part-of-speech tag-ging.
TACL, Mar(1):1?12.Peter Welinder, Steve Branson, Serge Belongie, andPietro Perona.
2010.
The multidimensional wisdomof crowds.
In NIPS.Jacob Whitehill, Paul Ruvolo, Tingfan Wu, JacobBergsma, and Javier Movellan.
2009.
Whose voteshould count more: Optimal integration of labelsfrom labelers of unknown expertise.
Advances inNeural Information Processing Systems, 22:2035?2043.Yan Yan, R?omer Rosales, Glenn Fung, Mark Schmidt,Gerardo Hermosillo, Luca Bogoni, Linda Moy, andJennifer Dy.
2010.
Modeling annotator exper-tise: Learning when everybody knows a bit of some-thing.
In International Conference on Artificial In-telligence and Statistics.382
