Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744?751,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging?Sharon GoldwaterDepartment of LinguisticsStanford Universitysgwater@stanford.eduThomas L. GriffithsDepartment of PsychologyUC Berkeleytom griffiths@berkeley.eduAbstractUnsupervised learning of linguistic structureis a difficult problem.
A common approachis to define a generative model and max-imize the probability of the hidden struc-ture given the observed data.
Typically,this is done using maximum-likelihood es-timation (MLE) of the model parameters.We show using part-of-speech tagging thata fully Bayesian approach can greatly im-prove performance.
Rather than estimatinga single set of parameters, the Bayesian ap-proach integrates over all possible parame-ter values.
This difference ensures that thelearned structure will have high probabilityover a range of possible parameters, and per-mits the use of priors favoring the sparsedistributions that are typical of natural lan-guage.
Our model has the structure of astandard trigram HMM, yet its accuracy iscloser to that of a state-of-the-art discrimi-native model (Smith and Eisner, 2005), upto 14 percentage points better than MLE.
Wefind improvements both when training fromdata alone, and using a tagging dictionary.1 IntroductionUnsupervised learning of linguistic structure is a dif-ficult problem.
Recently, several new model-basedapproaches have improved performance on a vari-ety of tasks (Klein and Manning, 2002; Smith and?This work was supported by grants NSF 0631518 andONR MURI N000140510388.
We would also like to thankNoah Smith for providing us with his data sets.Eisner, 2005).
Nearly all of these approaches haveone aspect in common: the goal of learning is toidentify the set of model parameters that maximizessome objective function.
Values for the hidden vari-ables in the model are then chosen based on thelearned parameterization.
Here, we propose a dif-ferent approach based on Bayesian statistical prin-ciples: rather than searching for an optimal set ofparameter values, we seek to directly maximize theprobability of the hidden variables given the ob-served data, integrating over all possible parame-ter values.
Using part-of-speech (POS) tagging asan example application, we show that the Bayesianapproach provides large performance improvementsover maximum-likelihood estimation (MLE) for thesame model structure.
Two factors can explain theimprovement.
First, integrating over parameter val-ues leads to greater robustness in the choice of tagsequence, since it must have high probability overa range of parameters.
Second, integration permitsthe use of priors favoring sparse distributions, whichare typical of natural language.
These kinds of pri-ors can lead to degenerate solutions if the parametersare estimated directly.Before describing our approach in more detail,we briefly review previous work on unsupervisedPOS tagging.
Perhaps the most well-known is thatof Merialdo (1994), who used MLE to train a tri-gram hidden Markov model (HMM).
More recentwork has shown that improvements can be madeby modifying the basic HMM structure (Banko andMoore, 2004), using better smoothing techniques oradded constraints (Wang and Schuurmans, 2005), orusing a discriminative model rather than an HMM744(Smith and Eisner, 2005).
Non-model-based ap-proaches have also been proposed (Brill (1995); seealso discussion in Banko and Moore (2004)).
All ofthis work is really POS disambiguation: learning isstrongly constrained by a dictionary listing the al-lowable tags for each word in the text.
Smith andEisner (2005) also present results using a diluteddictionary, where infrequent words may have anytag.
Haghighi and Klein (2006) use a small list oflabeled prototypes and no dictionary.A different tradition treats the identification ofsyntactic classes as a knowledge-free clusteringproblem.
Distributional clustering and dimen-sionality reduction techniques are typically appliedwhen linguistically meaningful classes are desired(Schu?tze, 1995; Clark, 2000; Finch et al, 1995);probabilistic models have been used to find classesthat can improve smoothing and reduce perplexity(Brown et al, 1992; Saul and Pereira, 1997).
Unfor-tunately, due to a lack of standard and informativeevaluation techniques, it is difficult to compare theeffectiveness of different clustering methods.In this paper, we hope to unify the problems ofPOS disambiguation and syntactic clustering by pre-senting results for conditions ranging from a full tagdictionary to no dictionary at all.
We introduce theuse of a new information-theoretic criterion, varia-tion of information (Meila?, 2002), which can be usedto compare a gold standard clustering to the clus-tering induced from a tagger?s output, regardless ofthe cluster labels.
We also evaluate using tag ac-curacy when possible.
Our system outperforms anHMM trained with MLE on both metrics in all cir-cumstances tested, often by a wide margin.
Its ac-curacy in some cases is close to that of Smith andEisner?s (2005) discriminative model.
Our resultsshow that the Bayesian approach is particularly use-ful when learning is less constrained, either becauseless evidence is available (corpus size is small) orbecause the dictionary contains less information.In the following section, we discuss the motiva-tion for a Bayesian approach and present our modeland search procedure.
Section 3 gives results illus-trating how the parameters of the prior affect re-sults, and Section 4 describes how to infer a goodchoice of parameters from unlabeled data.
Section 5presents results for a range of corpus sizes and dic-tionary information, and Section 6 concludes.2 A Bayesian HMM2.1 MotivationIn model-based approaches to unsupervised lan-guage learning, the problem is formulated in termsof identifying latent structure from data.
We de-fine a model with parameters ?, some observed vari-ables w (the linguistic input), and some latent vari-ables t (the hidden structure).
The goal is to as-sign appropriate values to the latent variables.
Stan-dard approaches do so by selecting values for themodel parameters, and then choosing the most prob-able variable assignment based on those parame-ters.
For example, maximum-likelihood estimation(MLE) seeks parameters ??
such that??
= argmax?P (w|?
), (1)where P (w|?)
= ?t P (w, t|?).
Sometimes, anon-uniform prior distribution over ?
is introduced,in which case ??
is the maximum a posteriori (MAP)solution for ?:??
= argmax?P (w|?
)P (?).
(2)The values of the latent variables are then taken tobe those that maximize P (t|w, ??
).In contrast, the Bayesian approach we advocate inthis paper seeks to identify a distribution over latentvariables directly, without ever fixing particular val-ues for the model parameters.
The distribution overlatent variables given the observed data is obtainedby integrating over all possible values of ?
:P (t|w) =?P (t|w, ?
)P (?|w)d?.
(3)This distribution can be used in various ways, in-cluding choosing the MAP assignment to the latentvariables, or estimating expected values for them.To see why integrating over possible parametervalues can be useful when inducing latent structure,consider the following example.
We are given acoin, which may be biased (t = 1) or fair (t = 0),each with probability .5.
Let ?
be the probability ofheads.
If the coin is biased, we assume a uniformdistribution over ?, otherwise ?
= .5.
We observew, the outcomes of 10 coin flips, and we wish to de-termine whether the coin is biased (i.e.
the value of745t).
Assume that we have a uniform prior on ?, withp(?)
= 1 for all ?
?
[0, 1].
First, we apply the stan-dard methodology of finding the MAP estimate for?
and then selecting the value of t that maximizesP (t|w, ??).
In this case, an elementary calculationshows that the MAP estimate is ??
= nH/10, wherenH is the number of heads in w (likewise, nT isthe number of tails).
Consequently, P (t|w, ??)
favorst = 1 for any sequence that does not contain exactlyfive heads, and assigns equal probability to t = 1and t = 0 for any sequence that does contain exactlyfive heads ?
a counterintuitive result.
In contrast,using some standard results in Bayesian analysis wecan show that applying Equation 3 yieldsP (t = 1|w) = 1/(1 + 11!nH !nT !210)(4)which is significantly less than .5 when nH = 5, andonly favors t = 1 for sequences where nH ?
8 ornH ?
2.
This intuitively sensible prediction resultsfrom the fact that the Bayesian approach is sensitiveto the robustness of a choice of t to the value of ?,as illustrated in Figure 1.
Even though a sequencewith nH = 6 yields a MAP estimate of ??
= 0.6(Figure 1 (a)), P (t = 1|w, ?)
is only greater than0.5 for a small range of ?
around ??
(Figure 1 (b)),meaning that the choice of t = 1 is not very robust tovariation in ?.
In contrast, a sequence with nH = 8favors t = 1 for a wide range of ?
around ??.
Byintegrating over ?, Equation 3 takes into account theconsequences of possible variation in ?.Another advantage of integrating over ?
is thatit permits the use of linguistically appropriate pri-ors.
In many linguistic models, including HMMs,the distributions over variables are multinomial.
Fora multinomial with parameters ?
= (?1, .
.
.
, ?K), anatural choice of prior is the K-dimensional Dirich-let distribution, which is conjugate to the multino-mial.1 For simplicity, we initially assume that allK parameters (also known as hyperparameters) ofthe Dirichlet distribution are equal to ?, i.e.
theDirichlet is symmetric.
The value of ?
determineswhich parameters ?
will have high probability: when?
= 1, all parameter values are equally likely; when?
> 1, multinomials that are closer to uniform are1A prior is conjugate to a distribution if the posterior has thesame form as the prior.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1?P(?| w)0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.51?P(t=1| w,?
)w = HHTHTTHHTHw = HHTHHHTHHHw = HHTHTTHHTHw = HHTHHHTHHH(a)(b)Figure 1: The Bayesian approach to estimating thevalue of a latent variable, t, from observed data, w,chooses a value of t robust to uncertainty in ?.
(a)Posterior distribution on ?
given w. (b) Probabilitythat t = 1 given w and ?
as a function of ?.preferred; and when ?
< 1, high probability is as-signed to sparse multinomials, where one or moreparameters are at or near 0.Typically, linguistic structures are characterizedby sparse distributions (e.g., POS tags are followedwith high probability by only a few other tags, andhave highly skewed output distributions).
Conse-quently, it makes sense to use a Dirichlet prior with?
< 1.
However, as noted by Johnson et al (2007),this choice of ?
leads to difficulties with MAP esti-mation.
For a sequence of draws x = (x1, .
.
.
, xn)from a multinomial distribution ?
with observedcounts n1, .
.
.
, nK , a symmetric Dirichlet(?)
priorover ?
yields the MAP estimate ?k = nk+??1n+K(?
?1) .When ?
?
1, standard MLE techniques such asEM can be used to find the MAP estimate simplyby adding ?pseudocounts?
of size ?
?
1 to each ofthe expected counts nk at each iteration.
However,when ?
< 1, the values of ?
that set one or moreof the ?k equal to 0 can have infinitely high poste-rior probability, meaning that MAP estimation canyield degenerate solutions.
If, instead of estimating?, we integrate over all possible values, we no longerencounter such difficulties.
Instead, the probabilitythat outcome xi takes value k given previous out-comes x?i = (x1, .
.
.
, xi?1) isP (k|x?i, ?)
=?P (k|?
)P (?|x?i, ?)
d?= nk + ?i?
1 + K?
(5)746where nk is the number of times k occurred in x?i.See MacKay and Peto (1995) for a derivation.2.2 Model DefinitionOur model has the structure of a standard trigramHMM, with the addition of symmetric Dirichlet pri-ors over the transition and output distributions:ti|ti?1 = t, ti?2 = t?, ?
(t,t?)
?
Mult(?
(t,t?
))wi|ti = t, ?
(t) ?
Mult(?(t))?
(t,t?)|?
?
Dirichlet(?)?(t)|?
?
Dirichlet(?
)where ti and wi are the ith tag and word.
We assumethat sentence boundaries are marked with a distin-guished tag.
For a model with T possible tags, eachof the transition distributions ?
(t,t?)
has T compo-nents, and each of the output distributions ?
(t) hasWt components, where Wt is the number of wordtypes that are permissible outputs for tag t. We willuse ?
and ?
to refer to the entire transition and out-put parameter sets.
This model assumes that theprior over state transitions is the same for all his-tories, and the prior over output distributions is thesame for all states.
We relax the latter assumption inSection 4.Under this model, Equation 5 gives usP (ti|t?i, ?)
=n(ti?2,ti?1,ti) + ?n(ti?2,ti?1) + T?
(6)P (wi|ti, t?i,w?i, ?)
=n(ti,wi) + ?n(ti) + Wti?
(7)where n(ti?2,ti?1,ti) and n(ti,wi) are the number ofoccurrences of the trigram (ti?2, ti?1, ti) and thetag-word pair (ti, wi) in the i ?
1 previously gener-ated tags and words.
Note that, by integrating outthe parameters ?
and ?, we induce dependenciesbetween the variables in the model.
The probabil-ity of generating a particular trigram tag sequence(likewise, output) depends on the number of timesthat sequence (output) has been generated previ-ously.
Importantly, trigrams (and outputs) remainexchangeable: the probability of a set of trigrams(outputs) is the same regardless of the order in whichit was generated.
The property of exchangeability iscrucial to the inference algorithm we describe next.2.3 InferenceTo perform inference in our model, we use Gibbssampling (Geman and Geman, 1984), a stochasticprocedure that produces samples from the posteriordistribution P (t|w, ?, ?)
?
P (w|t, ?
)P (t|?).
Weinitialize the tags at random, then iteratively resam-ple each tag according to its conditional distributiongiven the current values of all other tags.
Exchange-ability allows us to treat the current counts of theother tag trigrams and outputs as ?previous?
obser-vations.
The only complication is that resamplinga tag changes the identity of three trigrams at once,and we must account for this in computing its condi-tional distribution.
The sampling distribution for tiis given in Figure 2.In Bayesian statistical inference, multiple samplesfrom the posterior are often used in order to obtainstatistics such as the expected values of model vari-ables.
For POS tagging, estimates based on multi-ple samples might be useful if we were interested in,for example, the probability that two words have thesame tag.
However, computing such probabilitiesacross all pairs of words does not necessarily lead toa consistent clustering, and the result would be diffi-cult to evaluate.
Using a single sample makes stan-dard evaluation methods possible, but yields sub-optimal results because the value for each tag is sam-pled from a distribution, and some tags will be as-signed low-probability values.
Our solution is totreat the Gibbs sampler as a stochastic search pro-cedure with the goal of identifying the MAP tag se-quence.
This can be done using tempering (anneal-ing), where a temperature of ?
is equivalent to rais-ing the probabilities in the sampling distribution tothe power of 1?
.
As ?
approaches 0, even a singlesample will provide a good MAP estimate.3 Fixed Hyperparameter Experiments3.1 MethodOur initial experiments follow in the tradition begunby Merialdo (1994), using a tag dictionary to con-strain the possible parts of speech allowed for eachword.
(This also fixes Wt, the number of possiblewords for tag t.) The dictionary was constructed bylisting, for each word, all tags found for that word inthe entire WSJ treebank.
For the experiments in thissection, we used a 24,000-word subset of the tree-747P (ti|t?i,w, ?, ?)
?n(ti,wi) + ?nti + Wti?
?n(ti?2,ti?1,ti) + ?n(ti?2,ti?1) + T?
?n(ti?1,ti,ti+1) + I(ti?2 = ti?1 = ti = ti+1) + ?n(ti?1,ti) + I(ti?2 = ti?1 = ti) + T?
?n(ti,ti+1,ti+2) + I(ti?2 = ti = ti+2, ti?1 = ti+1) + I(ti?1 = ti = ti+1 = ti+2) + ?n(ti,ti+1) + I(ti?2 = ti, ti?1 = ti+1) + I(ti?1 = ti = ti+1) + T?Figure 2: Conditional distribution for ti.
Here, t?i refers to the current values of all tags except for ti, I(.
)is a function that takes on the value 1 when its argument is true and 0 otherwise, and all counts nx are withrespect to the tag trigrams and tag-word pairs in (t?i,w?i).bank as our unlabeled training corpus.
54.5% of thetokens in this corpus have at least two possible tags,with the average number of tags per token being 2.3.We varied the values of the hyperparameters ?
and?
and evaluated overall tagging accuracy.
For com-parison with our Bayesian HMM (BHMM) in thisand following sections, we also present results fromthe Viterbi decoding of an HMM trained using MLEby running EM to convergence (MLHMM).
Wheredirect comparison is possible, we list the scores re-ported by Smith and Eisner (2005) for their condi-tional random field model trained using contrastiveestimation (CRF/CE).2For all experiments, we ran our Gibbs samplingalgorithm for 20,000 iterations over the entire dataset.
The algorithm was initialized with a random tagassignment and a temperature of 2, and the temper-ature was gradually decreased to .08.
Since our in-ference procedure is stochastic, our reported resultsare an average over 5 independent runs.Results from our model for a range of hyperpa-rameters are presented in Table 1.
With the bestchoice of hyperparameters (?
= .003, ?
= 1), weachieve average tagging accuracy of 86.8%.
Thisfar surpasses the MLHMM performance of 74.5%,and is closer to the 90.1% accuracy of CRF/CE onthe same data set using oracle parameter selection.The effects of ?, which determines the probabil-2Results of CRF/CE depend on the set of features used andthe contrast neighborhood.
In all cases, we list the best scorereported for any contrast neighborhood using trigram (but nospelling) features.
To ensure proper comparison, all corporaused in our experiments consist of the same randomized sets ofsentences used by Smith and Eisner.
Note that training on setsof contiguous sentences from the beginning of the treebank con-sistently improves our results, often by 1-2 percentage points ormore.
MLHMM scores show less difference between random-ized and contiguous corpora.Value Value of ?of ?
.001 .003 .01 .03 .1 .3 1.0.001 85.0 85.7 86.1 86.0 86.2 86.5 86.6.003 85.5 85.5 85.8 86.6 86.7 86.7 86.8.01 85.3 85.5 85.6 85.9 86.4 86.4 86.2.03 85.9 85.8 86.1 86.2 86.6 86.8 86.4.1 85.2 85.0 85.2 85.1 84.9 85.5 84.9.3 84.4 84.4 84.6 84.4 84.5 85.7 85.31.0 83.1 83.0 83.2 83.3 83.5 83.7 83.9Table 1: Percentage of words tagged correctly byBHMM as a function of the hyperparameters ?
and?.
Results are averaged over 5 runs on the 24k cor-pus with full tag dictionary.
Standard deviations inmost cases are less than .5.ity of the transition distributions, are stronger thanthe effects of ?, which determines the probabilityof the output distributions.
The optimal value of.003 for ?
reflects the fact that the true transitionprobability matrix for this corpus is indeed sparse.As ?
grows larger, the model prefers more uniformtransition probabilities, which causes it to performworse.
Although the true output distributions tend tobe sparse as well, the level of sparseness depends onthe tag (consider function words vs. content wordsin particular).
Therefore, a value of ?
that accu-rately reflects the most probable output distributionsfor some tags may be a poor choice for other tags.This leads to the smaller effect of ?, and suggeststhat performance might be improved by selecting adifferent ?
for each tag, as we do in the next section.A final point worth noting is that even when?
= ?
= 1 (i.e., the Dirichlet priors exert no influ-ence) the BHMM still performs much better than theMLHMM.
This result underscores the importanceof integrating over model parameters: the BHMMidentifies a sequence of tags that have high proba-748bility over a range of parameter values, rather thanchoosing tags based on the single best set of para-meters.
The improved results of the BHMM demon-strate that selecting a sequence that is robust to vari-ations in the parameters leads to better performance.4 Hyperparameter InferenceIn our initial experiments, we experimented with dif-ferent fixed values of the hyperparameters and re-ported results based on their optimal values.
How-ever, choosing hyperparameters in this way is time-consuming at best and impossible at worst, if thereis no gold standard available.
Luckily, the Bayesianapproach allows us to automatically select valuesfor the hyperparameters by treating them as addi-tional variables in the model.
We augment the modelwith priors over the hyperparameters (here, we as-sume an improper uniform prior), and use a sin-gle Metropolis-Hastings update (Gilks et al, 1996)to resample the value of each hyperparameter aftereach iteration of the Gibbs sampler.
Informally, toupdate the value of hyperparameter ?, we sample aproposed new value ??
from a normal distributionwith ?
= ?
and ?
= .1?.
The probability of ac-cepting the new value depends on the ratio betweenP (t|w, ?)
and P (t|w, ??)
and a term correcting forthe asymmetric proposal distribution.Performing inference on the hyperparameters al-lows us to relax the assumption that every tag hasthe same prior on its output distribution.
In the ex-periments reported in the following section, we usedtwo different versions of our model.
The first ver-sion (BHMM1) uses a single value of ?
for all wordclasses (as above); the second version (BHMM2)uses a separate ?j for each tag class j.5 Inferred Hyperparameter Experiments5.1 Varying corpus sizeIn this set of experiments, we used the full tag dictio-nary (as above), but performed inference on the hy-perparameters.
Following Smith and Eisner (2005),we trained on four different corpora, consisting ofthe first 12k, 24k, 48k, and 96k words of the WSJcorpus.
For all corpora, the percentage of ambigu-ous tokens is 54%-55% and the average number oftags per token is 2.3.
Table 2 shows results forthe various models and a random baseline (averagedCorpus sizeAccuracy 12k 24k 48k 96krandom 64.8 64.6 64.6 64.6MLHMM 71.3 74.5 76.7 78.3CRF/CE 86.2 88.6 88.4 89.4BHMM1 85.8 85.2 83.6 85.0BHMM2 85.8 84.4 85.7 85.8?
< .7 .2 .6 .2Table 2: Percentage of words tagged correctlyby the various models on different sized corpora.BHMM1 and BHMM2 use hyperparameter infer-ence; CRF/CE uses parameter selection based on anunlabeled development set.
Standard deviations (?
)for the BHMM results fell below those shown foreach corpus size.over 5 random tag assignments).
Hyperparameterinference leads to slightly lower scores than are ob-tained by oracle hyperparameter selection, but bothversions of BHMM are still far superior to MLHMMfor all corpus sizes.
Not surprisingly, the advantagesof BHMM are most pronounced on the smallest cor-pus: the effects of parameter integration and sensiblepriors are stronger when less evidence is availablefrom the input.
In the limit as corpus size goes to in-finity, the BHMM and MLHMM will make identicalpredictions.5.2 Varying dictionary knowledgeIn unsupervised learning, it is not always reasonableto assume that a large tag dictionary is available.
Todetermine the effects of reduced or absent dictionaryinformation, we ran a set of experiments inspiredby those of Smith and Eisner (2005).
First, we col-lapsed the set of 45 treebank tags onto a smaller setof 17 (the same set used by Smith and Eisner).
Wecreated a full tag dictionary for this set of tags fromthe entire treebank, and also created several reduceddictionaries.
Each reduced dictionary contains thetag information only for words that appear at leastd times in the training corpus (the 24k corpus, forthese experiments).
All other words are fully am-biguous between all 17 classes.
We ran tests withd = 1, 2, 3, 5, 10, and ?
(i.e., knowledge-free syn-tactic clustering).With standard accuracy measures, it is difficult to749Value of dAccuracy 1 2 3 5 10 ?random 69.6 56.7 51.0 45.2 38.6MLHMM 83.2 70.6 65.5 59.0 50.9CRF/CE 90.4 77.0 71.7BHMM1 86.0 76.4 71.0 64.3 58.0BHMM2 87.3 79.6 65.0 59.2 49.7?
< .2 .8 .6 .3 1.4VIrandom 2.65 3.96 4.38 4.75 5.13 7.29MLHMM 1.13 2.51 3.00 3.41 3.89 6.50BHMM1 1.09 2.44 2.82 3.19 3.47 4.30BHMM2 1.04 1.78 2.31 2.49 2.97 4.04?
< .02 .03 .04 .03 .07 .17Corpus stats% ambig.
49.0 61.3 66.3 70.9 75.8 100tags/token 1.9 4.4 5.5 6.8 8.3 17Table 3: Percentage of words tagged correctly andvariation of information between clusterings in-duced by the assigned and gold standard tags as theamount of information in the dictionary is varied.Standard deviations (?)
for the BHMM results fellbelow those shown in each column.
The percentageof ambiguous tokens and average number of tags pertoken for each value of d is also shown.evaluate the quality of a syntactic clustering whenno dictionary is used, since cluster names are inter-changeable.
We therefore introduce another evalua-tion measure for these experiments, a distance met-ric on clusterings known as variation of information(Meila?, 2002).
The variation of information (VI) be-tween two clusterings C (the gold standard) and C ?
(the found clustering) of a set of data points is a sumof the amount of information lost in moving from Cto C ?, and the amount that must be gained.
It is de-fined in terms of entropy H and mutual informationI: V I(C,C ?)
= H(C)+H(C ?)?
2I(C,C ?).
Evenwhen accuracy can be measured, VI may be more in-formative: two different tag assignments may havethe same accuracy but different VI with respect tothe gold standard if the errors in one assignment areless consistent than those in the other.Table 3 gives the results for this set of experi-ments.
One or both versions of BHMM outperformMLHMM in terms of tag accuracy for all values ofd, although the differences are not as great as in ear-lier experiments.
The differences in VI are morestriking, particularly as the amount of dictionary in-formation is reduced.
When ambiguity is greater,both versions of BHMM show less confusion withrespect to the true tags than does MLHMM, andBHMM2 performs the best in all circumstances.
Theconfusion matrices in Figure 3 provide a more intu-itive picture of the very different sorts of clusteringsproduced by MLHMM and BHMM2 when no tagdictionary is available.
Similar differences hold to alesser degree when a partial dictionary is provided.With MLHMM, different tokens of the same wordtype are usually assigned to the same cluster, buttypes are assigned to clusters more or less at ran-dom, and all clusters have approximately the samenumber of types (542 on average, with a standarddeviation of 174).
The clusters found by BHMM2tend to be more coherent and more variable in size:in the 5 runs of BHMM2, the average number oftypes per cluster ranged from 436 to 465 (i.e., to-kens of the same word are spread over fewer clus-ters than in MLHMM), with a standard deviationbetween 460 and 674.
Determiners, prepositions,the possessive marker, and various kinds of punc-tuation are mostly clustered coherently.
Nouns arespread over a few clusters, partly due to a distinctionfound between common and proper nouns.
Like-wise, modal verbs and the copula are mostly sep-arated from other verbs.
Errors are often sensible:adjectives and nouns are frequently confused, as areverbs and adverbs.The kinds of results produced by BHMM1 andBHMM2 are more similar to each other than tothe results of MLHMM, but the differences are stillinformative.
Recall that BHMM1 learns a singlevalue for ?
that is used for all output distribu-tions, while BHMM2 learns separate hyperparame-ters for each cluster.
This leads to different treat-ments of difficult-to-classify low-frequency items.In BHMM1, these items tend to be spread evenlyamong all clusters, so that all clusters have simi-larly sparse output distributions.
In BHMM2, thesystem creates one or two clusters consisting en-tirely of very infrequent items, where the priors onthese clusters strongly prefer uniform outputs, andall other clusters prefer extremely sparse outputs(and are more coherent than in BHMM1).
Thisexplains the difference in VI between the two sys-tems, as well as the higher accuracy of BHMM1for d ?
3: the single ?
discourages placing low-frequency items in their own cluster, so they aremore likely to be clustered with items that have sim-7501 2 3 4 5 6 7 8 9 1011121314151617NINPUNCADJVDETPREPENDPUNCVBGCONJVBNADVTOWHPRTPOSLPUNCRPUNC(a) BHMM2Found TagsTrueTags1 2 3 4 5 6 7 8 9 1011121314151617NINPUNCADJVDETPREPENDPUNCVBGCONJVBNADVTOWHPRTPOSLPUNCRPUNC(b) MLHMMFound TagsTrueTagsFigure 3: Confusion matrices for the dictionary-free clusterings found by (a) BHMM2 and (b) MLHMM.ilar transition probabilities.
The problem of junkclusters in BHMM2 might be alleviated by using anon-uniform prior over the hyperparameters to en-courage some degree of sparsity in all clusters.6 ConclusionIn this paper, we have demonstrated that, for a stan-dard trigram HMM, taking a Bayesian approachto POS tagging dramatically improves performanceover maximum-likelihood estimation.
Integratingover possible parameter values leads to more robustsolutions and allows the use of priors favoring sparsedistributions.
The Bayesian approach is particularlyhelpful when learning is less constrained, either be-cause less data is available or because dictionaryinformation is limited or absent.
For knowledge-free clustering, our approach can also be extendedthrough the use of infinite models so that the num-ber of clusters need not be specified in advance.
Wehope that our success with POS tagging will inspirefurther research into Bayesian methods for other nat-ural language learning tasks.ReferencesM.
Banko and R. Moore.
2004.
A study of unsupervised part-of-speech tagging.
In Proceedings of COLING ?04.E.
Brill.
1995.
Unsupervised learning of disambiguation rulesfor part of speech tagging.
In Proceedings of the 3rd Work-shop on Very Large Corpora, pages 1?13.P.
Brown, V. Della Pietra, V. de Souza, J. Lai, and R. Mer-cer.
1992.
Class-based n-gram models of natural language.Computational Linguistics, 18:467?479.A.
Clark.
2000.
Inducing syntactic categories by context dis-tribution clustering.
In Proceedings of the Conference onNatural Language Learning (CONLL).S.
Finch, N. Chater, and M. Redington.
1995.
Acquiring syn-tactic information from distributional statistics.
In J.
In Levy,D.
Bairaktaris, J. Bullinaria, and P. Cairns, editors, Connec-tionist Models of Memory and Language.
UCL Press, Lon-don.S.
Geman and D. Geman.
1984.
Stochastic relaxation, Gibbsdistributions and the Bayesian restoration of images.
IEEETransactions on Pattern Analysis and Machine Intelligence,6:721?741.W.R.
Gilks, S. Richardson, and D. J. Spiegelhalter, editors.1996.
Markov Chain Monte Carlo in Practice.
Chapmanand Hall, Suffolk.A.
Haghighi and D. Klein.
2006.
Prototype-driven learning forsequence models.
In Proceedings of HLT-NAACL.M.
Johnson, T. Griffiths, and S. Goldwater.
2007.
Bayesianinference for PCFGs via Markov chain Monte Carlo.D.
Klein and C. Manning.
2002.
A generative constituent-context model for improved grammar induction.
In Proceed-ings of the ACL.D.
MacKay and L. Bauman Peto.
1995.
A hierarchical Dirich-let language model.
Natural Language Engineering, 1:289?307.M.
Meila?.
2002.
Comparing clusterings.
Technical Report 418,University of Washington Statistics Department.B.
Merialdo.
1994.
Tagging English text with a probabilisticmodel.
Computational Linguistics, 20(2):155?172.L.
Saul and F. Pereira.
1997.
Aggregate and mixed-ordermarkov models for statistical language processing.
In Pro-ceedings of the Second Conference on Empirical Methods inNatural Language Processing (EMNLP).H.
Schu?tze.
1995.
Distributional part-of-speech tagging.
InProceedings of the European Chapter of the Association forComputational Linguistics (EACL).N.
Smith and J. Eisner.
2005.
Contrastive estimation: Traininglog-linear models on unlabeled data.
In Proceedings of ACL.I.
Wang and D. Schuurmans.
2005.
Improved estimationfor unsupervised part-of-speech tagging.
In Proceedingsof the IEEE International Conference on Natural LanguageProcessing and Knowledge Engineering (IEEE NLP-KE).751
