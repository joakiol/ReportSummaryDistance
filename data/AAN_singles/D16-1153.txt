Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1462?1472,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsPhonologically Aware Neural Model for Named Entity Recognition inLow Resource Transfer SettingsAkash Bharadwaj David Mortensen Chris Dyer Jaime G. Carbonell{akashb, dmortens, cdyer, jgc}@cs.cmu.eduLanguage Technologies InstituteCarnegie Mellon UniversityAbstractNamed Entity Recognition is a well estab-lished information extraction task with manystate of the art systems existing for a va-riety of languages.
Most systems rely onlanguage specific resources, large annotatedcorpora, gazetteers and feature engineeringto perform well monolingually.
In this pa-per, we introduce an attentional neural modelwhich only uses language universal phonolog-ical character representations with word em-beddings to achieve state of the art perfor-mance in a monolingual setting using super-vision and which can quickly adapt to a newlanguage with minimal or no data.
We demon-strate that phonological character represen-tations facilitate cross-lingual transfer, out-perform orthographic representations and in-corporating both attention and phonologicalfeatures improves statistical efficiency of themodel in 0-shot and low data transfer settingswith no task specific feature engineering in thesource or target language.1 IntroductionNamed Entity Recognition (NER) (Nadeau andSekine, 2007; Marrero et al, 2013) is an informa-tion extraction task that deals with finding and clas-sifying entities in text into a fixed set of types ofinterest.
It is challenging for a variety of reasons.Named Entities (NEs) can be arbitrarily synthesized(eg.
people?s/organization?s names).
NEs are oftennot subject to uniform cross-linguistic spelling con-ventions: compare France (English) and Frantsiya(Uzbek).
NEs occur rarely in data which makes gen-eralization difficult.
Skewed class statistics necessi-tate measures to prevent models from merely favor-ing a majority class.Named entities must also be annotated in con-text (eg.
?
[New York Times]ORG?
vs.
?[NewYork]LOC?).
Lexical ambiguity (Turkey?countryvs.
bird), semantic ambiguity (?I work at the [NewYork Times]ORG?
vs. ?I read the New York Times?
)and sparsity induced by morphology add complex-ity.Despite the challenges mentioned above, compe-tent monolingual systems that rely on having suffi-cient annotated data, knowledge and resources avail-able for engineering features have been developed.A more challenging task is to design a model thatretains competence in monolingual scenarios andcan easily be transferred to a low resource languagewith minimum overhead in terms of data annotationrequirements and feature engineering.
This trans-fer setting introduces additional challenges suchas varying character usage conventions across lan-guages with same script, differing scripts, lack ofNE transliteration, varying morphology, differentlexicons and mutual non-intelligibility to name afew.We propose the following changes over priorwork (Lample et al, 2016) to address the challengesof the low-resource transfer setting.
We use:1.
Language universal phonological characterrepresentations instead of orthographic ones.2.
Attention over characters of a word whilelabeling it with an NE category.1462Figure 1: Attentional LSTM-CRF architecture.
li denotesthe encoding of a word and its left context (forward LSTM)while ri includes only right context (backward LSTM).
In-puts to word LSTMs are obtained using character LSTMsand word-embeddings.
ai denotes an attentional contextvector concatenated with li and ri.We show that using phonological character rep-resentations instead does not negatively impact per-formance on two languages: Spanish and Turkish.We then show that using global phonological repre-sentations enables model transfer from one or moresource languages to a target language with no extraeffort, even when the languages use different scripts.We demonstrate that while attention over charactersof words has marginal utility in monolingual andhigh resource settings, it greatly improves the sta-tistical efficiency of the model in 0-shot and lowresource transfer settings.
We do require a map-ping from a language?s script to phonological featurespace which is script specific and not task specific.This presents little or no overhead due to existenceof tools like PanPhon (Littell et al, 2016).2 Our ApproachFigure 1 provides a high level overview of ourmodel.
We model the words of a sentence at thetype level and the token level.
At the type level (ig-norant of sentential context), we use bidirectionalcharacter LSTMs as in figure 2 to compose charac-ters of a word to obtain its word representation andconcatenate this with a word embedding that cap-tures distributional semantics.
This can memorizeentities or capture morphological and suffixal cluesFigure 2: Type level word representations - l denotes aword prefix encoding (by forward char LSTM) while r de-notes a word suffix encoding (by backward char LSTM).that can help at a discriminative task like NER.
Wecompose type level word representations with bi-directional LSTMs to obtain token level (cognizantof sentential context) representations.
Using tokenlevel word representations along with an attentionalcontext vector for each word based on the sequenceof characters it contains, we generate score functionsused by a Conditional Random Field (CRF) for in-ference.
To facilitate transfer across languages withdifferent scripts, we use Epitran 1 and PanPhon (Lit-tell et al, 2016).Epitran is a straightforward orthography-to-IPA(International Phonetic Alphabet [language univer-sal]) system including a collection of preprocessorsand grapheme-to-phoneme mappings for a variety oflanguage-script pairs.
It converts a word from its na-tive script into a sequence of IPA characters, eachof which approximately corresponds to a phoneme.PanPhon is a database of IPA-to-phonological fea-ture vector mappings and a library for querying, ma-nipulating, and exploiting this database.
It consumesthe output of Epitran and produces feature vectors(21 dimensions) of phonological characteristics suchas whether a phoneme is articulated with (accom-panied by) vibration of the vocal folds (voiced),with the tongue in a high, low, back, or front po-sition, with the lips rounded or unrounded, withtongue tip or blade (coronal), etc.
Figure 3 depictsthe sequence of operations applied to the same NE1https://github.com/dmort27/epitran1463orthographicrepresentationEpitranIPArepresentationPanPhonfeaturevectorrepresentation>??????</?ind??a?/PanPhonfeaturevectorrepresentationorthographicrepresentationEpitranIPArepresentation<?incan>/?ind??an/TURKISH123123?ind??a?
?ind?
?anFigure 3: Use of Epitran and PanPhon to enable transferacross orthographiesin Uyghur (Perso-Arabic script) and Turkish (Latinscript), thus making the equivalence across scriptsapparent.
We concatenate the feature vectors fromPanPhon and 1-hot encodings of the correspondingIPA characters and use these as inputs to the charac-ter bi-LSTMs.This shift to IPA space is motivated by prior work(Tsvetkov et al, 2015; Tsvetkov and Dyer, 2015)which demonstrated the value of projecting ortho-graphic surface forms of words into a phonologi-cal space for detecting loan words, transliterationand cognates even in language pairs that exhibit sig-nificant typological, morphological and phonologi-cal differences.
Our underlying assumption is thatnamed entities are likely to be transliterated or re-tain pronunciation patterns across languages.
Addi-tionally, phenomena such as vowel harmony mani-fest explicitly in IPA representation and can poten-tially be helpful for NER.
Foreign named entities forexample, need not obey vowel harmony rules preva-lent in languages like Turkish.
A powerful sequencemodel such as a LSTM could be tolerant to the noisecreated by lexical aberrations, lack of spelling con-ventions, vowel raising etc.
when given a phonolog-ical representation of an NE in different languages.Our second proposed change is to incorporate at-tention over the sequence of IPA segments in a wordwhen predicting scores for its possible labels.
Atten-tion is an unsupervised alternative to convolution orfeature engineering to capture helpful localized phe-nomena like capitalization of first letter, presence ofcase markers, special characters, helpful morpho-logical suffixes etc.
or the conjunction of multiplesuch phenomena.
Such features have been the main-stay of most prior work for NER.
Most of these fea-tures are subtle and occur at the type level, whereasthe CRF performs inference at the token level.
Weshow (empirically) that attention makes the CRFmore sensitive to such useful type level phenom-ena during inference and improves the statistical ef-ficiency of the model in certain scenarios.
Havingdescribed our intuitions, we now provide mathemat-ical details of our model.2.1 Model Description2.1.1 LSTMLong Short Term Memory (LSTM) (Hochreiterand Schmidhuber, 1997) belongs to a special breedof neural networks called Recurrent Neural Net-works (RNNs) which are capable of processing se-quential input of unbounded and arbitrary length.This makes them suitable for a sequence labelingtask.
LSTMs incorporate gating functions at eachtime step to allow the network to forget, rememberand update contextual memory and mitigate prob-lems like vanishing gradient.
We use the followingimplementation:it =?
(Wxixt +Whiht?1 +Wcict?1 + bi)ct =(1?
it) ct?1+it  tanh(Wxcxt +Whcht?1 + bc)ot =?
(Wxoxt +Whoht?1 +Wcoct + bo)ht =ot  tanh(ct)where  indicates element-wise product and ?
indi-cates element-wise sigmoid function.In practice we use bi-directional LSTMs (eachwith its own parameters) to mitigate cases where theLSTM may be biased towards the last few inputs itreads.
This is done both at the word-level and thecharacter level.
Let the hidden state at time step tof the forward LSTM be denoted by ?
?ht and the cor-responding state of the backward LSTM be denotedby ?
?ht .
At the character level, for a word with Lcharacters, we only take the final hidden states ineach direction i.e.
[?
?hL; ?
?h0] and concatenate themto get a representation of the word that comprisesthese characters.
At the word level, we concatenatecorresponding forward and backward LSTM statesfor each word Xt to get [?
?ht ;?
?ht] which is the token1464level representation for the tth word in a sentence X.We use this to generate un-normalized energy/scorefunctions for the CRF layer.2.2 AttentionLet wt = [?
?ht ;?
?ht] indicate the concatenated word bi-LSTM output (of dimension d1) at step t correspond-ing to the tth word (Xt) in the input sequence X.LetMt be the matrix containing the concatenated bi-directional character LSTM outputs for each charac-ter of Xt.
It has dimensions (lt, d2) where d2 is thedimension of the concatenated bi-directional charac-ter LSTM hidden states and lt is the number of char-acters in Xt.
Let mit denote the ith row of Mt.
LetP be a parameter matrix of dimension (d1, d2) and pbe a bias vector of length d2.
We follow (Bahdanauet al, 2014) in the formulation of attention contextvector at:w?t =tanh(wt ?
P + p)?i =exp(w?t ?mit)?ltj=1 exp(w?t ?mjt )at =lt?i=1(?i ?mit)The attentional context vector at is then appendedto wt to obtain concatenated vector ut = [at; wt].We apply a linear transform U (matrix of dimension(d1 + d2, k) where k is the number of unique NERtags).
This gives us:et = ut ?
U (1)where et is a vector of un-normalized energy/scorefunctions indicating the compatibility between wordXt and each of the k possible NER labels it can begiven.
Note that each word has a separate attentioncontext vector obtained using the character LSTMhidden states generated by its constituent characters.2.3 Conditional Random FieldUnlike Hidden Markov Models, CRFs do not en-force any independence assumptions among ob-served data and directly model the likelihood of a la-beling hypothesis discriminatively.
They also modeladjacency compatibility between NER tags whichcan capture strong constraints like an ?I-label?
tagnot following an O tag without a ?B-label?
tag in be-tween them (see section 2.6).
In our model, the CRFis parametrized as follows:For a word sequence X = (x1, x2, ...xN ), let E bea matrix of dimension (k,N) where k is the numberof unique NER tags and N is the sequence length.The tth column is the vector et obtained in equation1.
Let T be the square transition matrix of size (k+2,k+2) which captures transition score between the kNER tags, the start and the end symbols.
Let Y =(y1, y2, ...yN ) be the label sequence associated withthe input word sequence, each yi being an index intothe ordered set of unique NER tags.
Let y0 be thestart symbol and yN+1 be the end symbol.
The scoreof this sequence is evaluated as:S(X,Y ) =N?i=1Eyi,i +N?j=0Tyj ,yj+1LetYX indicate the exponential space of all possi-ble labelings of this sequence X.
The partition func-tion associated with this CRF is then evaluated as:Z =?Y ?YXeS(X,Y )The probability of a specific labeling Y ?
YX :P (Y |X) = eS(X,Y )ZThe training objective is to maximize conditionallog probability of the correct labeling sequence Y ?
:log(P (Y ?|X)) = S(X,Y ?)?
log (Z) (2)The decoding criteria for an input sequence X is:Y ?
= argmaxY ?YXS(X,Y ) (3)Normally, evaluating the partition function overthe exponential space of all possible labelings wouldbe intractable.
However, as described in (Laffertyet al, 2001), this can be done efficiently for linearchain CRFs using the forward backward algorithm.2.4 Word RepresentationsThe inputs to our model are in the form of type levelword representations (figure 2).
Motivated by thedistributional hypothesis (Harris, 1954; Firth, 1957)1465we use word embeddings as inputs.
In the monolin-gual scenario, we use structured skipgram word em-beddings (Ling et al, 2015a).
For the transfer sce-nario, embeddings can optionally be trained usingtechniques like multi CCA described in (Ammar etal., 2016).
By learning a linear transformation froma shared vector space between languages, the modelmay acquire some transfer capability to the targetlanguage.We use character bi-LSTMs to handle the OutOf Vocabulary (OOV) problem as in (Ling et al,2015b).
However, just as a distributional hypothe-sis exists for words, prior work (Tsvetkov and Dyer,2015; Tsvetkov et al, 2015) suggests phonologicalcharacter representations capture inherent similari-ties between characters that are not apparent fromorthogonal one-hot orthographic character represen-tations and can serve as a language universal surro-gate for character representations.
This is also use-ful for multi-lingual named entity recognition as ex-plained in section 2.
Therefore we make use of Epi-tran and PanPhon as in figure 3 to obtain both 1-hot IPA character encodings and phonological fea-ture vectors capturing similarity between IPA char-acters.
These form the inputs to the character bi-LSTMs.
The mapping from orthographic charac-ter segments to IPA is sometimes many to one (am-biguous) and unarticulated characters (like periodsin NE abbreviations) and capitalization informationis lost by default.
Given the importance of such or-thographic features for NER and the ambiguity in-troduced, a drop in performance may be expectedby using phonological character representations.2.5 TrainingOur model parametrization is similar to (Lample etal., 2016).
The weights to be trained include the theCRF transition matrix T, the projection parametersare used to generate matrix E (P and U), the LSTMparameters and word and character embedding ma-trices.
The objective is to maximize the log prob-ability of the correct labeling sequence as given inequation 2.
This objective is fully differentiable andstandard back-propagation is used to learn weights.We use Stochastic Gradient Descent with a learningrate of 0.05 and gradients clipped at 5.0 providingbest performance.
Using Adadelta or Adam leads tofaster convergence but slightly worse performance.Word level LSTMs use a hidden layer size of 100,orthographic character LSTMs (if used) used a hid-den layer of size 25 while phonological characterLSTMs used a hidden layer of size 15 due to thesmaller phonetic alphabet.
Tuning these did not havea major effect on performance.
Dropout of 0.5 isapplied after concatenation of the word embeddingsand character LSTM outputs.
Best dropout valuewas chosen through ablation studies.
For Spanish,we use word embeddings pre-trained on the Span-ish Gigaword version 3.
For transfer experiments,we use multilingual word embeddings trained usingmulti CCA described in (Ammar et al, 2016).2.6 Entity Types and Tagging SchemesIn all of the datasets in table 1, 4 entity types areannotated:1.
Persons (PER)Real/fictional, living/dead people.
Aliases andfamily names are also annotated.
E.g.
BarackObama, the [Kennedys], Puff Daddy etc.2.
Locations (LOC)Geographical locations without a dedicatedpopulation and government.
E.g.
Nile river,Sahara desert, Mt.
Everest, Asia etc.3.
Geo-Political Entities (GPE)Geographical regions with corresponding pop-ulation and government.
Mentions can be nom-inal (e.g.
India, E.U., Britain etc.)
or adjectival(e.g.
[British] army, [French] government etc.).4.
Organizations (ORG)Names of entities with organization and man-agerial structure.
E.g.
Democratic Party,Google, JetBlue, etc.A BIO tagging scheme is used for all annotations.All non-entity tokens are annotated as ?O?.
The firsttoken of an entity span is annotated as ?B-label?
andall remaining tokens in the entity span are annotatedas ?I-label?.3 ExperimentsWe conduct four different experiments:1.
CoNLL 2002 Spanish NER (Sang., 2002) task.This demonstrates the monolingual compe-tence of phonological character representationsvs.
orthographic representations.14662.
Turkish NER using the Linguistic Data Con-sortium?s LDC2014E115 BOLT Turkish Lan-guage Pack 2.
This demonstrates the utility ofphonological character representations and at-tention in a morphologically rich, low resourcelanguage.
We compare against a state-of-the-art monolingual model (Lample et al, 2016)that uses orthographic character LSTMs.3.
Transfer Experiments from Uzbek to Turkishusing LDC2014E112 BOLT 3 data pack forUzbek and LDC2014E115 BOLT data packfor Turkish.
These two languages have sim-ilar syntax and word order (Dryer, 2013) butvary significantly in morphology, vowel har-mony and phonology, can use different scripts(Uzbek-Latin/Cyrilic, Turkish-Latin) and arenot mutually intelligible.4.
Transfer Experiments from Uzbek and Turk-ish into Uyghur using LDC2014E112 andLDC2014E115 BOLT data pack for Uzbekand Turkish respectively and Uyghur dataprovided as part of DARPA LORELEI4.
Theselanguages all have different scripts, lexicons,morphology and phonology.
Results arereported by NIST 5 on an unseen test set.3.1 ResultsTables 2 and 3 report results from monolingual ex-periments in Spanish.
In table 3, we report the per-formance of our best model against other state-of-the-art models for the Spanish CoNLL 2002 NERtask (Sang., 2002).
Our model performs marginallybetter than other benchmarks with the optimal con-figuration of hyper-parameters and using pre-trainedword embeddings.
Table 2 report ablation study re-sults, which reveal that using pre-trained word em-beddings without using character LSTMs yields avery strong baseline that already out-performs var-ious previous benchmarks.
Using character LSTMsthat compose orthographic character representationsyields a +0.91 improvement in F1 score and a further2http://opencatalog.darpa.mil/BOLT.html3BOLT contains newswire, discussion forum, social mediaand chat data4http://www.darpa.mil/program/low-resource-languages-for-emergent-incidents5https://www.nist.gov6Sparse features for character capitalization and charactertype (digit, punctuation etc.
)Language # Sentences # EntitiesSpanish 8323 18798Turkish 5065 4883Uzbek 10078 7960Uyghur 2161 2668?Table 1: Dataset Statistics.
* indicates non-gold annotationsproduced by a non-speaker linguist.PhonoCharsOrthoCharsWordVecsCap+Cat6OrthoAttnPhonoAttn F1No No Yes No No No 83.61No Yes Yes No No No 84.52No Yes Yes No Yes No 84.64No Yes Yes Yes No No 84.91No Yes Yes Yes Yes No 85.25Yes No Yes No No No 84.08Yes No Yes No No Yes 84.88Yes No Yes Yes No No 84.89Yes No Yes Yes No Yes 85.81Yes Yes Yes No No Yes 84.53Yes Yes Yes Yes No No 84.92Yes Yes Yes Yes Yes Yes 84.75Yes Yes Yes Yes No Yes 84.84Yes Yes Yes Yes Yes No 85.32Table 2: Ablation Tests on Spanish CoNLL 2002.
Bold indi-cates the best model.Model F1Carreras et al (2002)* 81.39dos Santos et al (2015) 82.21Gillick et al (2015) 81.83Gillick et al (2015)* 82.95Lample et al (2016) 85.75Yang et al (2016) 85.77Our Best 85.81Table 3: Comparison with benchmarks.
* indicates a modelthat uses external labeled dataPhonoCharsOrthoCharsWordvecsCap+CatOrthoAttnPhonoAttn F1No No Yes No No No 49.2No Yes Yes No No No 65.41No Yes Yes No Yes No 64.76No Yes Yes Yes No No 60.57No Yes Yes Yes Yes No 60.87Yes No Yes No No No 63.04Yes No Yes No No Yes 66.07Yes No Yes Yes No No 59.08Yes No Yes Yes No Yes 62.46Yes Yes Yes No No Yes 63.43Yes Yes Yes Yes No No 63.46Yes Yes Yes Yes Yes Yes 66.47Table 4: Ablation Tests on Turkish Bold indicates the besttransfer eligible (66.07) and transfer ineligible models (66.47)1467Model F1Lample et al (2016) 61.11Lample et al (2016) withpretrained embeddings 65.41Our Best model 66.47Our Best transfer-eligible model 66.07Table 5: Comparison with state-of-the-art monolingual TurkishmodelModel F1Lample et al (2016) 37.1Our best transfer model* 51.2Table 6: NIST evaluations for Uyghur.
* indicates transfer fromUzbek and Turkish+0.12 F1 with attention.
Using phonological charac-ter representations instead yields an improvement of+0.47 F1 and a further +0.8 F1 with attention.
Thus,phonological representations benefit more from at-tention applied over them to beat out orthographicrepresentations in that scenario.
Using sparse fea-tures indicating the character category (alphabet vs.number vs. punctuation/non-phonetic) and capital-ization in conjunction with with phonological char-acter representations and word embeddings with at-tention over phonological characters yields the bestconfiguration that slightly outperforms the best pub-lished models so far.
Given that many previousbenchmarks used features that rely heavily on or-thography, this is an encouraging result since onewould expect to lose some performance by usingmore abstract phonological representations as ex-plained in section 2.4.Tables 4 and 5 highlight results from monolingualexperiments on Turkish.
This dataset is much morechallenging since the annotated training courpus issignificantly smaller than the CoNLL 2002 sharedtask dataset and because Turkish is an agglutinativelanguage exhibiting sparsity inducing morphologywhich leads to huge vocabulary size.
As a compet-itive baseline, we train the LSTM CRF described in(Lample et al, 2016) due to its documented abil-ity to obtain state-of-the-art monolingual results formany languages with minimal feature engineering.Our best model from the Turkish ablation study out-performs this baseline.
We also see a stark contrastbetween the ablation study results for Turkish com-pared to Spanish.
Firstly, word embeddings aloneperform rather poorly due to the challenges of reli-ably estimating them for a large vocabulary given asmall dataset.
Character composed representationsof words provide a significant performance boost(+17.27 F1 for the best model).
Secondly, usage ofsparse character features (like capitalization) seemsto hurt performance in all but the last model in table4.
Thirdly, phonological and orthographic charac-ter representations are complementary in the case ofTurkish, unlike Spanish.
This is not too surprisingsince Turkish exhibits phonological phenomena likevowel harmony.
Lack of vowel harmony in a wordcould give-away a foreign word or a named entityfor example.
Results show that attention is helpfulas well.
We would also like to point out that theonly models in the ablation studies eligible for trans-fer are those that do not use orthographic characterrepresentations.
Among these, the model that usesphonological representation with attention and wordvectors performs the best and also outperforms thebaseline system.Our next experiments on model transfer are ar-guably the most interesting.
Tables 7 and 8 docu-ment single source (Uzbek to Turkish) transfer re-sults.
We find that using sparse character categoryand capitalization features in conjunction with atten-tion and phonological features yields the best 0-shottransfer performance (no training labels in the tar-get language).
Specifically, attention provides +6F1 in 0-shot and 5% labeled-target language datascenarios.
It is interesting to note that using mul-tilingual word embeddings for the source and tar-get languages alone accounts for very poor trans-fer.
We also find that with as little as 20% of thedata, we approach the performance of a monolingualtarget model that was trained on all the data.
Wealso notice that the transfer models retain a consis-tent advantage over a monolingually trained targetlanguage model across all data availability scenar-ios.
Lastly, we note that while attention providesa significant improvement in 0-shot and 5% dataavailability scenarios, a model without attention (orsparse features like capitalization) eventually doesbetter with more data.
This indicates that the modelis competent enough to leverage transfer via phonol-ogy alone.
This could also possibly be because at-tention patterns from Uzbek could bring in a biasthat is eventually sub-optimal for Turkish due to dif-1468PhonoCharsWordvecsCap+CatPhonoAttnUzbekSource F1Target0-shot F15%data20%data40%data60%data80%dataAlldataNo Yes No No 41.87 2.09 23.44 35 42.75 46.32 48.81 50.34Yes Yes No Yes 61.24 11.9 34.06 47.84 56.1 53.5 64.72 65.2Yes Yes No No 60.92 15.55 39.42 60.14 63.23 62.54 65.24 65.63Yes Yes Yes No 64.89 22.14 41.19 54.02 57.06 59.26 60.84 61.72Yes Yes Yes Yes 61.85 26.92 47.21 58.58 60.32 60.7 62.84 63.58Table 7: Model Transfer from Uzbek (Source) to Turkish (Target) at different target data availability thresholdsModel 0-shot 5%data20%data40%data60%data80%dataAlldataLSTM-CRF (Lample et al, 2016) 0 33.44 50.61 53.25 57.41 60 61.11S-LSTM (Lample et al, 2016) 0 15.41 39.33 42.99 51.92 51.55 56.58Table 8: Monolingual Turkish baseline at different data availability thresholdsferent morphology and phonology.
In future work,we shall perform more insightful error analysis toexplain these trends.Table 6 documents NIST evaluation results on anunseen Uyghur test set (with gold annotations) forthe best transfer model configuration jointly trainedon Turkish and Uzbek gold annotations and Uyghurtraining annotations produced by a non-speaker lin-guist (non-gold).
Since Uyghur lacks helpful type-level orthographic features such as capitalization,our transfer model in table 6 does not use anysparse features or attention but benefits from transfervia the phonological character representations we?veproposed.
Despite the noisy supervision provided inthe target language, transferring from Turkish andUzbek provides a +14.1 F1 improvement over a stateof the art monolingual model trained on the sameUyghur annotations.
It is worth pointing out thatthis transfer was achieved across 3 languages eachwith different scripts, morphology, phonology andlexicons.4 Prior WorkNER is a well-studied sequence-labeling problemfor which many different approaches have been pro-posed.
Early works had a monolingual focus andrelied heavily on feature engineering.
Approachesinclude maximum entropy models (Chieu and Ng,2003), hierarchically smoothed tries (Cucerzan andYarowsky, 1999), decision trees (Carreras et al,2002) and models incorporating syntactic, semanticand world knowledge (Wakao et al, 1996).
Eachof these models brings in a bias of its own.
Florianet al (2003) successfully tried ensembling multipleclassifiers and improved performance.Since NER is a sequence labeling problem, thereare local dependencies both among NE labels as-sociated with words and among the words them-selves, that could aid the labeling process.
To explic-itly deal with these sequential characteristics, Hid-den Markov Models (HMMs) and Conditional Ran-dom Fields (CRFs) became very popular.
(Kleinet al, 2003; Florian et al, 2003; McCallum andLi, 2003; Ratinov and Roth, 2009; Chandra et al,1981; Lin and Wu, 2009; Lample et al, 2016; Yanget al, 2016; Ma and Hovy, 2016).
CRFs even-tually became more popular because they are dis-criminative models that directly model the requiredposterior probability of a labeling sequence usingparametrized functions of features.
They do notmodel the probability of the observed sentence itself,avoid Markovian independence assumptions madeby HMMs and avoid the label bias problem.Most of the work cited so far makes use of handengineered features.
The following approaches min-imize the use of features while still maintaining amonolingual focus.
Collobert et al (2011), Turian etal.
(2010), and Ando and Zhang (2005) use unsuper-vised features in conjunction with engineered fea-tures capturing capitalization, character categoriesand gazetteer matches.
Collobert et al (2011) usea Convolutional Neural Network (CNN) over the se-quence of word embeddings.
Huang et al (2015)instead use bi-directional LSTMs over the sequenceof words, along with spelling and orthographic fea-tures.The most recent work eliminates feature engi-neering altogether and combines CRFs with LSTMs1469which can model long sequences while remember-ing appropriate past context.
Lample et al (2016)proposed an architecture that uses both character andword level LSTMs to produce score functions forCRF inference conditioned on global context.
Maand Hovy (2016) replace the character LSTMs ofLample et al (2016) with a CNN instead.
Yang etal.
(2016) follow a very similar architecture to Lam-ple et al (2016), replacing the LSTMs with GatedRecurrent Units (Cho et al, 2014).
However, Yanget al (2016) also tackle multi task and multi-lingualjoint training scenarios.Most of the models cited so far are monolin-gual either because they use hand crafted featuresand language specific resources or because of deep-seated assumptions.
For example a change in or-thography, lexicon, script, word order or additionof complex morphology makes transfer impossi-ble.
This is the central challenge that we tackle.There has been much less work catering to this sce-nario.
Kim et al (2012) use weak annotations fromWikipedia metadata and parallel data for multi lin-gual NER.
Yang et al (2016) addresses the use caseof multilingual joint training, which assumes thereis sufficient data available in all languages.
Noth-man et al (2013) also operate under the assumptionof availability of Wikipedia data.To the best of our knowledge, a scenario involvingtransfer of a model trained in one (or more) sourcelanguage(s) to another language with little or no la-beled data, different script, different morphology,different lexicon, lack of transliteration, non-mutualintelligibility etc.
has not been addressed recently.5 ConclusionIn this paper, we presented two improvements overa state-of-the-art monolingual model to addressNamed Entity Recognition in transfer settings.
Thefirst seeks to reconcile various dimensions of vari-ability between languages such as varying script,orthographic conventions, phonological phenomenaetc.
by representing words as sequences of IPA (In-ternational Phonetic Alphabet) segments consistentacross all languages, rather than sequences of char-acters specific to a particular language.
Secondly,we exploit the one-to-one mapping between inputsequence words and output labels and advocate forthe use of attention over the character/IPA sequenceof a word when predicting its label.
We show em-pirically that these two improvements 1) achieveat least state-of-the-art performance on a monolin-gual NER task in Spanish, 2) handle complex mor-phology in languages such as Turkish, Uzbek andUyghur better than state of the art, 3) provide 0-shotperformance in a transfer scenario to a related newlanguage, well above that possible using multilin-gual word embeddings alone, and 4) are capable ofgeneralizing to a new language with much less train-ing data than a monolingually trained model.
More-over, all of this is achieved without any extra featureengineering specific to the task or language, apartfrom mapping characters in that language to IPA.
Webelieve these results to be encouraging and look for-ward to replicating these results on more languagepairs in different language families and further au-tomating the process of obtaining phonological char-acter representations.6 AcknowledgementThis work is sponsored by Defense Advanced Re-search Projects Agency Information Innovation Of-fice (I2O).
Program: Low Resource Languagesfor Emergent Incidents (LORELEI).
Issued byDARPA/I2O under Contract No.
HR0011-15-C-0114.
The views and conclusions contained in thisdocument are those of the authors and should notbe interpreted as representing the official policies,either expressed or implied, of the U.S. Govern-ment.
The U.S. Government is authorized to re-produce and distribute reprints for Government pur-poses notwithstanding any copyright notation hereon.ReferencesWaleed Ammar, George Mulcaire, Yulia Tsvetkov, Guil-laume Lample, Chris Dyer, and Noah A Smith.
2016.Massively multilingual word embeddings.
arXivpreprint arXiv:1602.01925.Rie Kubota Ando and Tong Zhang.
2005.
A frameworkfor learning predictive structures from multiple tasksand unlabeled data.
The Journal of Machine LearningResearch, 6:1817?1853.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointly1470learning to align and translate.
arXiv preprintarXiv:1409.0473.Xavier Carreras, Lluis Marquez, and Llu?
?s Padro?.
2002.Named entity extraction using adaboost.
In pro-ceedings of the 6th conference on Natural languagelearning-Volume 20, pages 1?4.
Association for Com-putational Linguistics.Ashok K. Chandra, Dexter C. Kozen, and Larry J. Stock-meyer.
1981.
Alternation.
Journal of the Associationfor Computing Machinery, 28(1):114?133.Hai Leong Chieu and Hwee Tou Ng.
2003.
Named en-tity recognition with a maximum entropy approach.
InWalter Daelemans and Miles Osborne, editors, Pro-ceedings of CoNLL-2003, pages 160?163.
Edmonton,Canada.Kyunghyun Cho, Bart van Merrie?nboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014.
On the proper-ties of neural machine translation: Encoder-decoderapproaches.
arXiv preprint arXiv:1409.1259.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Silviu Cucerzan and David Yarowsky.
1999.
Languageindependent named entity recognition combining mor-phological and contextual evidence.
In Proceedings ofthe 1999 Joint SIGDAT Conference on EMNLP andVLC, pages 90?99.C?cero dos Santos, Victor Guimaraes, RJ Nitero?i, and Riode Janeiro.
2015.
Boosting named entity recogni-tion with neural character embeddings.
In Proceed-ings of NEWS 2015 The Fifth Named Entities Work-shop, page 25.Matthew S. Dryer, 2013.
Order of Subject, Object andVerb.
Max Planck Institute for Evolutionary Anthro-pology, Leipzig.John Rupert Firth.
1957.
A synopsis of linguistic theory.In In Studies in Linguistic Analysis.
Oxford: Philolog-ical Societ.Radu Florian, Abe Ittycheriah, Hongyan Jing, and TongZhang.
2003.
Named entity recognition through clas-sifier combination.
In Walter Daelemans and MilesOsborne, editors, Proceedings of CoNLL-2003, pages168?171.
Edmonton, Canada.Dan Gillick, Cliff Brunk, Oriol Vinyals, and AmarnagSubramanya.
2015.
Multilingual language processingfrom bytes.
arXiv preprint arXiv:1512.00103.Zellig S Harris.
1954.
Distributional structure.
Word,10(2-3):146?162.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Zhiheng Huang, Wei Xu, and Kai Yu.
2015.
Bidirec-tional lstm-crf models for sequence tagging.
arXivpreprint arXiv:1508.01991.Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.2012.
Multilingual named entity recognition usingparallel data and metadata from wikipedia.
In Pro-ceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics: Long Papers-Volume1, pages 694?702.
Association for Computational Lin-guistics.Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-pher D. Manning.
2003.
Named entity recognitionwith character-level models.
In Walter Daelemans andMiles Osborne, editors, Proceedings of CoNLL-2003,pages 180?183.
Edmonton, Canada.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.Guillaume Lample, Miguel Ballesteros, Sandeep Subra-manian, Kazuya Kawakami, and Chris Dyer.
2016.Neural architectures for named entity recognition.arXiv preprint arXiv:1603.01360.Dekang Lin and Xiaoyun Wu.
2009.
Phrase cluster-ing for discriminative learning.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Volume2-Volume 2, pages 1030?1038.
Association for Com-putational Linguistics.Wang Ling, Chris Dyer, Alan Black, and Isabel Trancoso.2015a.
Two/too simple adaptations of word2vec forsyntax problems.
In Proceedings of the 2015 Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies.
Association for Computational Linguis-tics.Wang Ling, Tiago Lu?
?s, Lu?
?s Marujo, Ramo?n Fernan-dez Astudillo, Silvio Amir, Chris Dyer, Alan WBlack, and Isabel Trancoso.
2015b.
Finding func-tion in form: Compositional character models foropen vocabulary word representation.
arXiv preprintarXiv:1508.02096.Patrick Littell, David Mortensen, Kartik Goyal, ChrisDyer, and Lori Levin.
2016.
Bridge-language capital-ization inference in western iranian: Sorani, kurmanji,zazaki, and tajik.
In Proceedings of the Eleventh In-ternational Conference on Language Resources andEvaluation (LREC16).Xuezhe Ma and Eduard Hovy.
2016.
End-to-end se-quence labeling via bi-directional lstm-cnns-crf.
arXivpreprint arXiv:1603.01354.Mo?nica Marrero, Julia?n Urbano, Sonia Sa?nchez-Cuadrado, Jorge Morato, and Juan Miguel Go?mez-Berb??s.
2013.
Named entity recognition: fallacies,1471challenges and opportunities.
Computer Standards &Interfaces, 35(5):482?489.Andrew McCallum and Wei Li.
2003.
Early resultsfor named entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In Proceedings of the seventh conference on Natu-ral language learning at HLT-NAACL 2003-Volume 4,pages 188?191.
Association for Computational Lin-guistics.David Nadeau and Satoshi Sekine.
2007.
A survey ofnamed entity recognition and classification.
Lingvisti-cae Investigationes, 30(1):3?26.Joel Nothman, Nicky Ringland, Will Radford, Tara Mur-phy, and James R Curran.
2013.
Learning multilin-gual named entity recognition from wikipedia.
Artifi-cial Intelligence, 194:151?175.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProceedings of the Thirteenth Conference on Compu-tational Natural Language Learning, pages 147?155.Association for Computational Linguistics.Erik F. Tjong Kim Sang.
2002.
Introduction to the conll-2002 shared task: Languageindependent named entityrecognition.
In Proc.
CoNLL.Yulia Tsvetkov and Chris Dyer.
2015.
Cross-lingualbridges with models of lexical borrowing.
JAIR.Yulia Tsvetkov, Waleed Ammar, and Chris Dyer.2015.
Constraint-based models of lexical borrowing.NAACL.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In Proceedings of the 48thannual meeting of the association for computationallinguistics, pages 384?394.
Association for Computa-tional Linguistics.Takahiro Wakao, Robert Gaizauskas, and Yorick Wilks.1996.
Evaluation of an algorithm for the recognitionand classification of proper names.
In Proceedingsof the 16th conference on Computational linguistics-Volume 1, pages 418?423.
Association for Computa-tional Linguistics.Zhilin Yang, Ruslan Salakhutdinov, and William Cohen.2016.
Multi-task cross-lingual sequence tagging fromscratch.
arXiv preprint arXiv:1603.06270.1472
