Coling 2010: Poster Volume, pages 1220?1228,Beijing, August 2010Confidence Measures for Error Discriminationin an Interactive Predictive Parsing Framework1Ricardo Sa?nchez-Sa?ez, Joan Andreu Sa?nchez and Jose?
Miguel BenedInstituto Tecnolo?gico de Informa?ticaUniversidad Polite?cnica de Valencia{rsanchez,jandreu,jbenedi}@dsic.upv.esAbstractWe study the use of Confidence Measures(CM) for erroneous constituent discrimi-nation in an Interactive Predictive Parsing(IPP) framework.
The IPP framework al-lows to build interactive tree annotationsystems that can help human correctorsin constructing error-free parse trees withlittle effort (compared to manually post-editing the trees obtained from an auto-matic parser).
We show that CMs canhelp in detecting erroneous constituentsmore quickly through all the IPP process.We present two methods for precalculat-ing the confidence threshold (globally andper-interaction), and observe that CMs re-main highly discriminant as the IPP pro-cess advances.1 IntroductionWithin the Natural Language Processing (NLP)field, we can tell apart two different usage scenar-ios for automatic systems that output or work withnatural language.
On one hand, we have the casesin which the output of such systems is expected tobe used in a vanilla fashion, that is, without val-idating or correcting the results produced by thesystem.
Within this usage scheme, the most im-portant factor of a given automatic system is thequality of the results.
Although memory and com-putational requirements of such systems are usu-ally taken into account, the ultimate aim of most1Work partially supported by the Spanish MICINN underthe MIPRCV ?Consolider Ingenio 2010?
(CSD2007-00018),MITTRAL (TIN2009-14633-C03-01), Prometeo (PROME-TEO/2009/014) research projects, and the FPU fellowshipAP2006-01363.research that relates to this scenario is to minimizethe amount of error (measured with metrics likeWord Error Rate, BLEU, F-Measure, etc.)
presentwithin the results that are being produced.The second usage scenario arises when thereexists the need for perfect and completely error-free results, for example, flawlessly translatedsentences or correctly annotated syntactic trees.In such cases, the intervention of a human valida-tor/corrector is unavoidable.
The corrector willreview and validate the results, making the suit-able modifications before the system output canbe employed.
In these kind of tasks, the most im-portant factor to be minimized is the human ef-fort that has to be applied to transform the sys-tem?s potentially incorrect output into validatedand error-free output.
Measuring user effort hasan intrinsic subjectivity that makes it hard to bequantitatized.
Given that the user effort is usuallyinversely proportional to the quality of the systemoutput, most research about problems associatedto this scenario t to minimize just the system?s er-ror rate as well.Interactive Predictive NLP SystemsOnly recently, more comparable and repro-ducible evaluation methods for Interactive NaturalLanguage Systems have started to be developed,within the context of Interactive Predictive Sys-tems (IPS).
These systems formally integrate thecorrecting user into the loop, making him part ofthe system right at its theoretical framework.
IPSsallow for human correctors to spare effort becausethe system updates its output after each individ-ual user correction, potentially fixing several er-rors at each step.
Interactive Predictive methodshave been studied and successfully used in fields1220like Handwritten Text Recognition (HTR) (Toselliet al, 2008) and Statistical Machine Translation(SMT) (Vidal et al, 2006; Barrachina et al, 2009)to ease the work of transcriptors and translators.In IPS related research the importance of thesystem base error rate per se is diminished.
In-stead, the intention is to measure how well theuser and the system work together.
For this, for-mal user simulation protocols together with newobjective effort evaluation metrics such as theWord Stroke Ratio (WSR) (Toselli et al, 2008) orthe Key-Stroke and Mouse-Ratio (KSMR) (Bar-rachina et al, 2009) started to be used as abenchmark.
These ratios reflect the amount ofuser effort (whole-word corrections in the case ofWSR; keystrokes plus mouse actions in the case ofKSMR) given a certain output.
To get the amountof user effort into context they should be measuredagainst the corresponding error ratios of compara-ble non-interactive systems: Word Error Rate forWSR and Character Error Rate for KSMR.This dichotomy in evaluating either system per-formance or user effort applies to Syntactic Pars-ing as well.
The objective of parsing is to pre-cisely determine the syntactic structure of sen-tences written in one of the several languages thathumans use.
Very bright research has been carriedout in this field, resulting in several top perform-ing completely automatic parsers (Collins, 2003;Klein and Manning, 2003; McClosky et al, 2006;Huang, 2008; Petrov, 2010).
However, these pro-duce results that are erroneous to some extent, andas such unsuitable for some applications without aprevious manual correction.
There are many prob-lems where error-free results consisting in per-fectly annotated trees are needed, such as hand-written mathematical expression recognition (Ya-mamoto et al, 2006) or construction of large newgold treebanks (de la Clergerie et al, 2008).When using automatic parsers as a baseline forbuilding perfect syntactic trees, the role of thehuman annotator is usually to post-edit the treesand correct the errors.
This manner of operat-ing results in the typical two-step process for er-ror correcting, in which the system first gener-ates the whole output and then the user verifiesor amends it.
This paradigm is rather inefficientand uncomfortable for the human annotator.
Forexample, a basic two-stage setup was employedin the creation of the Penn Treebank annotatedcorpus: a rudimentary parsing system provided askeletal syntactic representation, which then wasmanually corrected by human annotators (Marcuset al, 1994).
Additional works within this fieldhave presented systems that act as a computerizedaid to the user in obtaining the perfect annotation(Carter, 1997; Oepen et al, 2004; Hiroshi et al,2005).
Subjective measuring of the effort neededto obtain perfect annotations was reported in someof these works, but we feel that a more compara-ble metric is needed.With the objective of reducing the user effortand making the laborious task of tree annotationeasier, the authors of (Sa?nchez-Sa?ez et al, 2009a)devised an Interactive Predictive Parsing (IPP)framework.
That work embeds the human cor-rector into the automatic parser, and allows himto interact in real time within the system.
In thismanner, the system can use the readily availableuser feedback to make predictions about the partsof the trees that have not been validated by thecorrector.
The authors simulated user interactionand calculated effort evaluation metrics, establish-ing that an IPP system results in amounts slightlyabove 40% of effort reduction for a manual anno-tator compared to a two-step system.Confidence Measures in NLPAnnotating trees syntactically, even with theaid of automatic systems, generally requires hu-man intervention with a high degree of special-ization.
This fact partially justifies the shortagein large manually annotated treebanks.
Endeavorsdirected at easing the burden for the experts per-forming this task could be of great help.One approach that can be followed in reducinguser effort within an IPS is adding informationthat helps the user to locate the individual errorsin a sentence, so he can correct them in a hastierfashion.
The use of the Confidence Measure (CM)formalism goes in this direction, allowing us toassign a probability of correctness for individualerroneous constituents of a more complex outputblock of a NLP system.In fields such as HTR, SMT or AutomaticSpeech Recognition (ASR), the output sentences1221have a global probability (or score) that reflectsthe likeness of the output sentence being correct.CMs allow precision beyond the sentence level inpredicting errors: they can be used to label the in-dividual words as either correct or incorrect.
Au-tomatic systems can use CMs to help the user inidentifying the erroneous parts of the output in afaster way or to aid with the amendments by sug-gesting replacement words that are likely to becorrect.Previous research shows that CMs have beensuccessfully applied within the ASR (Wessel etal., 2001), HTR (Tarazo?n et al, 2009; Serranoet al, 2010) and SMT (Ueffing and Ney, 2007)fields.
In these works, the ability of CMs in de-tecting erroneous constituents is assessed by theclassical confidence metrics: the Confidence Er-ror Rate (CER) and the Receiver Operating Char-acteristic (ROC) (Ueffing and Ney, 2007).However, until recent advances, the use of CMsremained largely unexplored in Parsing.
Assess-ing the correctness of the different parts of a pars-ing tree can be useful in improving the efficiencyand usability of an IPP system, not only by tag-ging parts with low confidence for the user to re-view, but also by automating part of the correctionprocess itself by presenting constituents that yielda higher confidence when an error is confirmed bythe user.CMs for parsing in the form of combinationsof features calculated from n-best lists were pro-posed in (Bened??
et al, 2007).
Later on, the au-thors of (Sa?nchez-Sa?ez et al, 2009b) introduceda statistical method for calculating a CM for eachof the constituents in a parse tree.
In that work,CMs are calculated using the posterior probabilityof each tree constituent, approach which is similarto the word-graph based methods in the ASR andSMT fields.In this paper, we apply Confidence Measuresto the Interactive Predictive Parsing framework toasses how CMs are increasingly more accurate asthe user validates subtrees within the interactiveprocess.
We prove that after each correction per-formed by the user, the CMs of the remaining un-validated constituents are more helpful to detecterrors.2 Interactive Predictive ParsingIn this section we review the IPP framework(Sa?nchez-Sa?ez et al, 2009a) and its underlyingoperation protocol.
In parsing, a syntactic tree t,attached to a string x = x1 .
.
.
x|x| is composedby substructures called constituents.
A constituentcAij is defined by the nonterminal symbol (eithera syntactic label or a POS tag) A and its spanij (the starting and ending indexes which delimitthe part of the input sentence encompassed by theconstituent).Here follows a general formulation for the non-interactive syntactic parsing scenario, which willallow us to better introduce the IPP formulation.Assume that using a given parsing model G, theparser analyzes the input sentence x and producesthe most probable parse treet?
= argmaxt?TpG(t|x), (1)where pG(t|x) is the probability of the parse treet given the input string x using model G, and T isthe set of all possible parse trees for x.In the IPP framework, the manual correctorprovides feedback to the system by correcting anyof the constituents cAij from t?.
The system reactsto each of the corrections performed by the humanannotator by proposing a new t??
that takes into ac-count the correction.Within the IPP framework, the user reviews theconstituents contained in the tree to assess theircorrectness.
When the user finds an incorrect con-stituent he modifies it, setting the correct span andlabel.
This action implicitly validates what it iscalled the validated prefix tree tp.We define the validated prefix tree to be com-posed by the partially corrected constituent, allof its ancestor constituents, and all constituentswhose end span is lower than the start span of thecorrected constituent.
When the user replaces theconstituent cAij with the correct one c?Aij , the vali-dated prefix tree istp(c?Aij ) = {cBmn : m ?
i, n ?
j ,d(cBmn) ?
d(c?Aij )} ?
{cDpq : q < i }(2)1222with d(cZab) being the depth (distance from root)of constituent cZab.The validated prefix tree is parallel to the vali-dated sentence prefix commonly used in Interac-tive Machine Translation or Interactive Handwrit-ten Recognition, and is established after each useraction.This particular definition of the prefix tree de-termines the fact that the user is expected to re-view the parse tree in a preorder fashion (left-to-right depth-first).
Note that this specific explo-ration order allows us to simulate the user inter-action for the experimentation, as we will explainbelow.
Also note that other types of prefixes couldbe defined, allowing for different tree review or-ders.Within the IPP formulation, when a constituentcorrection is performed, the prefix tree tp(c?Aij ) isvalidated and a new tree t??
that takes into accountthe prefix is proposed.
Incorporating this newevidence into expression (1) yields the followingequationt??
= argmaxt?TpG(t|x, tp(c?Aij )).
(3)Given the properties of Probabilistic Context-Free Grammars (PCFG) the only subtree that ef-fectively needs to be recalculated is the one start-ing from the parent of the corrected constituent.This way, just the descendants of the newly intro-duced constituent, as well as its right hand siblings(along with their descendants) are calculated.2.1 User Interaction OperationThe IPP formulation allows for a very straightfor-ward operation protocol that is performed by themanual corrector, in which he validates or correctsthe successive output parse trees:1.
The IPP system proposes a full parse tree tfor the input sentence.2.
Then, the user finds the first incorrect con-stituent exploring the tree in a certain orderedmanner (preorder in our case, given by thetree prefix definition) and amends it, by mod-ifying its span and/or label (implicitly vali-dating the prefix tree tp).3.
The IPP system produces the most probabletree that is compatible with the validated pre-fix tree tp as shown in expression (3).4.
These steps are iterated until a final, perfectparse tree is produced by the system and val-idated by the user.It is worth noting that within this protocol, con-stituents can be automatically deleted or insertedat the end of any subtree in the syntactic struc-ture by adequately modifying the span of the left-neighbouring constituent.The IPP interaction process is similar to theones already established in HTR and SMT.
Inthese fields, the user reads the output sentencefrom left to right.
When the user finds and correctsan erroneous word, he is implicitly validating theprefix sentence up to that word.
The remainingsuffix sentence is recalculated by the system tak-ing into account the validated prefix sentence.Fig.
1 shows an example that intends to clar-ify the Interactive Predictive process.
First, thesystem provides a tentative parse tree (Fig.
1.b).Then the user, which has the correct reference tree(Fig.
1.a) in mind, notices that it has two wrongconstituents (cX23 and cZ44) (Fig.
1.c), and choosesto replace cX23 by cB22 (Fig.
1.d).
Here, cB22 cor-responds to c?Aij of expression (3).
As the userdoes this correction, the system automatically val-idates the prefix (dashed line in Fig.
1.d, tp(c?Aij )of expression (2)).
The system also invalidatesthe subtrees outside the prefix (dotted line line inFig.
1.d).
Finally, the system automatically pre-dicts a new subtree (Fig.
1.e).
Notice how cZ34changes its span and cD44 is introduced which pro-vides the correct reference parse.For further exemplification, Sa?nchez-Sa?ezet al (2010) demonstrate an IPP basedannotation tool that can be accessed athttp://cat.iti.upv.es/ipp/.Within the IPP scenario, the user has to man-ually review all the system output and correct orvalidate it, which is still a considerable amount ofeffort.
CMs can ease this work by helping to spotthe erroneous constituents.1223SB ZYba c dADC(a) Reference treeSba c dACBXYZ(b) Iteration 0: Pro-posed output tree 1Sba c dACBX Z 423 4Y(c) Iteration 0: Erro-neous constituentsYSba c dAB 22 ??
?
(d) Iteration 1:User correctedconstituentSB ZYba c dADC34(e) Iteration 1:Proposed outputtree 2Figure 1: Synthetic example of user interaction with the IPP system.3 Confidence MeasuresProbabilistic calculation of Confidence Measures(Sa?nchez-Sa?ez et al, 2009b) for all tree con-stituents can be introduced within the IPP process.The CM of each constituent is its posteriorprobability, which can be considered as a measureof the degree to which the constituent is believedto be correct for a given input sentence x.
This isformulated as followspG(cAij |x) =pG(cAij ,x)pG(x)=?t?
?T ; c?Aij ?t?
?
(cAij , c?Aij ) pG(t?|x)pG(x)(4)with ?
() being the Kronecker delta function.
Nu-merator in expression (4) stands for the probabil-ity of all parse trees for x that contain the con-stituent cAij (see Fig.
2).SA?A(i, j)?A(i, j)x1 xi?1 xi xj xj+1 x|x|Figure 2: The product of the inside and outsideprobabilities for each constituent comprises theupper part of expression (5)The posterior probability is computed with theinside ?
and outside ?
probabilities (Baker, 1979)C(tAij) = pG(cAij |x) =pG(cAij ,x)pG(x)= ?A(i, j) ?A(i, j)?S(1, |x|).
(5)It should be clear that the calculation of con-fidence measures reviewed here is generalizablefor any problem that employs PCFGs, and notjust NLP tasks.
In the experiments presented inthe following section we show that CMs are in-creasingly discriminant when used within the IPPframework to detect erroneous constituents.4 ExperimentsEvaluation of the quality of CMs within the IPPframework is done in a completely automaticfashion by simulating user interaction.
Section 4.1introduces the evaluation protocol and metricsmeasuring CM quality (i.e., their ability to de-tect incorrect constituents).
The experimentationframework and the results are discussed in sec-tion 4.2.4.1 Evaluation Methods4.1.1 IPP EvaluationA good measure of the performance of an In-teractive Predictive System is the amount of ef-fort saved by the users of such a system.
It issubjective and expensive to test an IPS with realusers, so these systems are usually evaluated us-ing automatically calculated metrics that assessthe amount of effort saved by the user.1224As already mentioned, the objective of an IPPbased system is to be employed by annotators toconstruct correct syntactic trees with less effort.Evaluation of an IPP system was previously doneby comparing the IPP usage effort (the number ofcorrections using the IPP system) against the es-timated effort required to manually post-edit thetrees after obtaining them with a traditional au-tomatic parsing system (the amount of incorrectconstituents) (Sa?nchez-Sa?ez et al, 2009a).In the case of IPP, the gold reference trees areused to simulate system interaction by a humancorrector and provide a comparable benchmark.This automatic evaluation protocol is similar tothe one presented in section 2.1:1.
The IPP system proposes a full parse tree tfor the input sentence.2.
The user simulation subsystem finds the firstincorrect constituent by exploring the tree inthe order defined by the prefix tree definition(preorder) and comparing it with the refer-ence.
When the first erroneous constituentis found, it is amended by being replaced inthe output tree by the correct one, operationwhich implicitly validates the prefix tree tp.3.
The IPP system produces the most probabletree that is compatible with the validated pre-fix tree tp.4.
These steps are iterated until a final, perfectparse tree is produced by the IPP system andvalidated against the reference by the usersimulation subsystem.In this work, metrics assessing the quality ofCM are introduced within this automatic protocol.We calculate and report them after each of the it-erations in the IPP process.4.1.2 Confidence Measure EvaluationMetricsThe CM of each tree constituent, computed asshown in expression (4) can be seen as its prob-ability of being correct.
Once all CM are calcu-lated, a confidence threshold ?
?
[0, 1] can bechosen.
Constituents are then marked using ?
: theones with a confidence above this threshold aremarked as correct, and the rest as incorrect.
Com-paring the confidence marks in the output treewith the reference, we obtain the false rejectionNf (?)
?
[0, Nc] (number of correct constituentsin the output tree wrongly marked as incorrect bytheir CM) and the true rejection Nt(?)
?
[0, Ni](number of incorrect constituents in the outputtree that are indeed detected as incorrect by theirconfidence).The amount of correct and incorrect con-stituents in each tree is Nc and Ni respectively.
Inthe ideal case of perfectly error discriminant CM,using the best threshold would yield Nf (?)
= 0and Nt(?)
= Ni.A evaluation metric that assess the ability ofCMs in telling apart correct constituents from in-correct ones is the Confidence Error Rate (CER):CER(?)
= Nf (?)
+ (Ni ?Nt(?
))Nc +Ni.
(6)The CER is the number of errors incurred by theCMs divided by the total number of constituents.The CER can be compared with the AbsoluteConstituent Error Rate (ACER), which is the CERobtained assuming that all constituents are markedas correct (the only possible assumption when CMare not available):ACER = CER(0) = NiNc +Ni.
(7)4.2 Experimental FrameworkOur experiments were carried out over the WallStreet Journal Penn Treebank (PTB) manually an-notated corpus.
Three sets were defined over thePTB: train (sections 2 to 21), test (section 23),and development (the first 346 sentences of sec-tion 24).
Before carrying out experimentation, theNoEmpties transformation was applied to all sets(Klein and Manning, 2001).We implemented the CYK-Viterbi parsing al-gorithm as the parse engine within the IPPframework.
This algorithm uses grammars inthe Chomsky Normal Form (CNF) so we em-ployed the open source Natural Language Toolkit2(NLTK) to obtain several right-factored binary2http://www.nltk.org/12250246810121416182022242628300  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 1600.10.20.30.40.50.60.70.80.91CERThresholdInteractionThr.ACERCER(a) PCFG: h=0,v=10246810121416182022242628300  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 1600.10.20.30.40.50.60.70.80.91CERThresholdInteractionThr.ACERCER(b) PCFG: h=0,v=2Figure 3: CER results over IPP system interaction.
Threshold fixed at before the interactive process.0246810121416182022242628300  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 1600.10.20.30.40.50.60.70.80.91CERThresholdInteractionThr.ACERCER(a) PCFG: h=0,v=10246810121416182022242628300  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 1600.10.20.30.40.50.60.70.80.91CERThresholdInteractionThr.ACERCER(b) PCFG: h=0,v=2Figure 4: CER results over IPP system interaction.
Threshold optimized for each step of the interactiveprocess.grammars with different markovization parame-ters from the training set (Klein and Manning,2003).The purpose of our experimentation is to de-termine if CMs can successfully discriminate er-roneous constituents from correct ones within anIPP process, that is, if they help the user to finderrors in a hastier manner.
For this we need toassess if there exists discriminant information inthe CMs corresponding to the constituents of theunvalidated part of the successive IPP-proposedtrees.With this objective in mind, we introduced aCM calculation step after each user interactionwithin the IPP process.
CMs for all constituents ineach tree were obtained as described in section 3.After each simulated interaction, we also calcu-lated the ACER and CER over all the syntacticconstituents of the whole test set.Each IPP user interaction yields a parse treewhich can be seen as the concatenation of twoparts: the validated prefix tree (which is knownto be correct because the user, or the user simula-tion subsystem in this case, has already reviewedit) and a new suffix tree which is calculated bythe IPP system based on the validated prefix, asshown in section 2.The fact that the validated prefix is alreadyknown to be correct is taken into account by theCM calculation process, and the confidence of theconstituents in the prefix tree is automatically setto their maximum score, equal to 1.
This factcauses that the CMs become more discriminantafter each interaction, because a larger part of the1226tree (the prefix) has a completely correct confi-dence.
The key point here is to measure if thisincreasingly reduced CER (CM error rate) main-tains its advantage over the also increasingly re-duced ACER (absolute constituent error rate with-out taking CMs into account) which would meanthat the CMs retain their discriminant power andcan be useful as an aid for a human annotator us-ing an IPP system.Two batches of experiments were performedand, in each of them, two different markovizationsof the vanilla PCFG were tested as the parsingmodel.In the first battery of experiments, the confi-dence threshold ?
was optimized over the devel-opment set before starting the IPP process, re-maining the same during the user interaction.
Theresults can be seen in Fig.
3, which shows theobtained baseline ACER and the CER (the con-fidence assessing metric) for the test set after eachuser interaction.
We see how CMs retain all oftheir error detection capabilities during the IPPprocess: in the h0v1 PCFG they are able to dis-cern about 25% of incorrect constituents at moststages of the IPP process, with a slight bump up to27% after about 7 user interactions; for the h0v2PCFG they are able to detect about 18% of incor-rect constituents at the first interactions, but go upto detect 27% of errors after about 7 or more in-teractions.In the second experimental setup, a differentthreshold for each interaction step was calcu-lated by performing the IPP user simulation pro-cess over the development set and optimizingthe threshold value.
The results can be seen inFig.
4.
We observe improvements in the discrim-inant ability of confidence values after 8 user in-teractions, with them being capable to detect moreerrors towards the end of each IPP session: about34% of errors for h0v1, and 49% of them for h0v2.The calculated thresholds have also been plot-ted in the aforementioned figures.
For the per-interaction threshold experimentation, we can seehow the threshold gets fine-tuned as the IPP pro-cess advances.
The lower threshold values for thelast interactions were expected due to the fact thatmore constituents have been validated and havethe maximum confidence.
This method for pre-calculating one specific threshold for each of theiterations could be useful when incorporating CMto a real IPP based annotator.5 Conclusions and Future WorkWe have proved that using Confidence Measurescan be used to discriminate incorrect constituentsfrom correct ones over an Interactive PredictiveParsing process.
We have show two methodsfor calculating the threshold used to mark con-stituents as correct/incorrect, showing the advan-tage of precalculating a specific threshold for eachof the interaction steps.Immediate future work involves implementingCMs as a visual aid in a real IPP system likethe one presented in (Sa?nchez-Sa?ez et al, 2010).Through he use of CMs, all constituents in thesuccessive trees could be color-coded accordingto their correctness confidence, so the user couldfocus and make corrections faster.Future research paths can deal with applyingCMs to improve the output of completely auto-matic parsers, for example, using them as a com-ponent of an n-best re-ranking system.Additionally, the IPP framework is also suit-able for studying and applying training algorithmswithin the Active Learning and Adaptative/OnlineParsing paradigms.
This kind of systems couldimprove their models at operating time, by incor-porating new ground truth data as it is provided bythe user.ReferencesBaker, JK.
1979.
Trainable grammars for speechrecognition.
Journal of the Acoustical Society ofAmerica, 65:132.Barrachina, S., O. Bender, F. Casacuberta, J. Civera,E.
Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Toma?s,E.
Vidal, and J.M.
Vilar.
2009.
Statistical ap-proaches to computer-assisted translation.
Compu-tational Linguistics, 35(1):3?28.Bened?
?, J.M., J.A.
Sa?nchez, and A.
Sanch??s.
2007.Confidence measures for stochastic parsing.
InProc.
of RANLP, pages 58?63, Borovets, Bulgaria,27-29 September.Carter, D. 1997.
The TreeBanker.
A tool for super-vised training of parsed corpora.
In Proc.
of EN-VGRAM Workshop, pages 9?15.1227Collins, M. 2003.
Head-driven statistical models fornatural language parsing.
Computational Linguis-tics, 29(4):589?637.de la Clergerie, E.V., O. Hamon, D. Mostefa, C. Ay-ache, P. Paroubek, and A. Vilnat.
2008.
Passage:from French parser evaluation to large sized tree-bank.
Proc.
of LREC, 100:2.Hiroshi, I., N. Masaki, H. Taiichi, T. Takenobu, andT.
Hozumi.
2005. eBonsai: An integrated environ-ment for annotating treebanks.
In Proc.
of IJCNLP,pages 108?113.Huang, L. 2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proc.
of ACL.Klein, D. and C.D.
Manning.
2001.
Parsing withtreebank grammars: Empirical bounds, theoreticalmodels, and the structure of the Penn treebank.
InProc.
of ACL, pages 338?345, Morristown, USA.ACL.Klein, D. and C.D.
Manning.
2003.
Accurate unlex-icalized parsing.
In Proc.
of ACL, volume 1, pages423?430, Morristown, USA.
ACL.Marcus, M.P., B. Santorini, and M.A.
Marcinkiewicz.1994.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.McClosky, D., E. Charniak, and M. Johnson.
2006.Effective self-training for parsing.
In Proc.
ofNAACL-HLT, pages 152?159.Oepen, S., D. Flickinger, K. Toutanova, and C.D.
Man-ning.
2004.
LinGO Redwoods.
Research on Lan-guage & Computation, 2(4):575?596.Petrov, S. 2010.
Products of Random Latent VariableGrammars.
Proc.
of NAACL-HLT.Sa?nchez-Sa?ez, R., J.A.
Sa?nchez, and J.M.
Bened??.2009a.
Interactive predictive parsing.
In Proc.
ofIWPT?09, pages 222?225, Paris, France, October.ACL.Sa?nchez-Sa?ez, R., J.A.
Sa?nchez, and J.M.
Bened??.2009b.
Statistical confidence measures for proba-bilistic parsing.
In Proc.
of RANLP, pages 388?392,Borovets, Bulgaria, September.Sa?nchez-Sa?ez, R., L.A. Leiva, J.A.
Sa?nchez, and J.M.Bened??.
2010.
Interactive predictive parsing usinga web-based architecture.
In Proc.
of NAACL-HLT,Los Angeles, United States of America, June.Serrano, N., A. Sanchis, and A. Juan.
2010.
Bal-ancing error and supervision effort in interactive-predictive handwriting recognition.
In Proc.
of IUI,pages 373?376.
ACM.Tarazo?n, L., D. Pe?rez, N. Serrano, V. Alabau,O.
Ramos Terrades, A. Sanchis, and A. Juan.
2009.Confidence Measures for Error Correction in Inter-active Transcription of Handwritten Text.
In Proc.of ICIAP, pages 567?574, Vietri sul Mare, Italy,September.
LNCS.Toselli, A.H., V. Romero, and E. Vidal.
2008.
Com-puter assisted transcription of text images and mul-timodal interaction.
In Proc.
MLMI, volume 5237,pages 296?308.
Springer.Ueffing, N. and H. Ney.
2007.
Word-level confidenceestimation for machine translation.
ComputationalLinguistics, 33(1):9?40.Vidal, E., F. Casacuberta, L.
Rodr?
?guez, J. Civera, andC.
Mart??nez.
2006.
Computer-assisted translationusing speech recognition.
IEEE TASLP, 14(3):941?951.Wessel, F., R. Schluter, K. Macherey, and H. Ney.2001.
Confidence measures for large vocabu-lary continuous speech recognition.
IEEE TSAP,9(3):288?298.Yamamoto, R., S. Sako, T. Nishimoto, andS.
Sagayama.
2006.
On-line recognition ofhandwritten mathematical expressions based onstroke-based stochastic context-free grammar.
InProc of ICFHR, pages 249?254.1228
