Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135?139,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsJoshua: An Open Source Toolkit for Parsing-based Machine TranslationZhifei Li, Chris Callison-Burch, Chris Dyer,?
Juri Ganitkevitch,+ Sanjeev Khudanpur,Lane Schwartz,?
Wren N. G. Thornton, Jonathan Weese and Omar F. ZaidanCenter for Language and Speech Processing, Johns Hopkins University, Baltimore, MD?
Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University, Germany?
Natural Language Processing Lab, University of Minnesota, Minneapolis, MNAbstractWe describe Joshua, an open sourcetoolkit for statistical machine transla-tion.
Joshua implements all of the algo-rithms required for synchronous contextfree grammars (SCFGs): chart-parsing, n-gram language model integration, beam-and cube-pruning, and k-best extraction.The toolkit also implements suffix-arraygrammar extraction and minimum errorrate training.
It uses parallel and dis-tributed computing techniques for scala-bility.
We demonstrate that the toolkitachieves state of the art translation per-formance on the WMT09 French-Englishtranslation task.1 IntroductionLarge scale parsing-based statistical machinetranslation (e.g., Chiang (2007), Quirk et al(2005), Galley et al (2006), and Liu et al (2006))has made remarkable progress in the last fewyears.
However, most of the systems mentionedabove employ tailor-made, dedicated software thatis not open source.
This results in a high bar-rier to entry for other researchers, and makes ex-periments difficult to duplicate and compare.
Inthis paper, we describe Joshua, a general-purposeopen source toolkit for parsing-based machinetranslation, serving the same role as Moses (Koehnet al, 2007) does for regular phrase-based ma-chine translation.Our toolkit is written in Java and implementsall the essential algorithms described in Chiang(2007): chart-parsing, n-gram language model in-tegration, beam- and cube-pruning, and k-best ex-traction.
The toolkit also implements suffix-arraygrammar extraction (Lopez, 2007) and minimumerror rate training (Och, 2003).
Additionally, par-allel and distributed computing techniques are ex-ploited to make it scalable (Li and Khudanpur,2008b).
We have also made great effort to ensurethat our toolkit is easy to use and to extend.The toolkit has been used to translate roughlya million sentences in a parallel corpus for large-scale discriminative training experiments (Li andKhudanpur, 2008a).
We hope the release of thetoolkit will greatly contribute the progress of thesyntax-based machine translation research.12 Joshua ToolkitWhen designing our toolkit, we applied generalprinciples of software engineering to achieve threemajor goals: Extensibility, end-to-end coherence,and scalability.Extensibility: The Joshua code is organizedinto separate packages for each major aspect offunctionality.
In this way it is clear which filescontribute to a given functionality and researcherscan focus on a single package without worryingabout the rest of the system.
Moreover, to mini-mize the problems of unintended interactions andunseen dependencies, which is common hinder-ance to extensibility in large projects, all exten-sible components are defined by Java interfaces.Where there is a clear point of departure for re-search, a basic implementation of each interface isprovided as an abstract class to minimize the worknecessary for new extensions.End-to-end Cohesion: There are many compo-nents to a machine translation pipeline.
One of thegreat difficulties with current MT pipelines is thatthese diverse components are often designed byseparate groups and have different file format andinteraction requirements.
This leads to a large in-vestment in scripts to convert formats and connectthe different components, and often leads to unten-able and non-portable projects as well as hinder-1The toolkit can be downloaded at http://www.sourceforge.net/projects/joshua, and the in-structions in using the toolkit are at http://cs.jhu.edu/?ccb/joshua.135ing repeatability of experiments.
To combat theseissues, the Joshua toolkit integrates most criticalcomponents of the machine translation pipeline.Moreover, each component can be treated as astand-alone tool and does not rely on the rest ofthe toolkit we provide.Scalability: Our third design goal was to en-sure that the decoder is scalable to large modelsand data sets.
The parsing and pruning algorithmsare carefully implemented with dynamic program-ming strategies, and efficient data structures areused to minimize overhead.
Other techniques con-tributing to scalability includes suffix-array gram-mar extraction, parallel and distributed decoding,and bloom filter language models.Below we give a short description about themain functions implemented in our Joshua toolkit.2.1 Training Corpus Sub-samplingRather than inducing a grammar from the full par-allel training data, we made use of a method pro-posed by Kishore Papineni (personal communica-tion) to select the subset of the training data con-sisting of sentences useful for inducing a gram-mar to translate a particular test set.
This methodworks as follows: for the development and testsets that will be translated, every n-gram (up tolength 10) is gathered into a map W and asso-ciated with an initial count of zero.
Proceedingin order through the training data, for each sen-tence pair whose source-to-target length ratio iswithin one standard deviation of the average, ifany n-gram found in the source sentence is alsofound in W with a count of less than k, the sen-tence is selected.
When a sentence is selected, thecount of every n-gram in W that is found in thesource sentence is incremented by the number ofits occurrences in the source sentence.
For oursubmission, we used k = 20, which resulted in1.5 million (out of 23 million) sentence pairs be-ing selected for use as training data.
There were30,037,600 English words and 30,083,927 Frenchwords in the subsampled training corpus.2.2 Suffix-array Grammar ExtractionHierarchical phrase-based translation requires atranslation grammar extracted from a parallel cor-pus, where grammar rules include associated fea-ture values.
In real translation tasks, the grammarsextracted from large training corpora are often fartoo large to fit into available memory.In such tasks, feature calculation is also very ex-pensive in terms of time required; huge sets ofextracted rules must be sorted in two directionsfor relative frequency calculation of such featuresas the translation probability p(f |e) and reversetranslation probability p(e|f) (Koehn et al, 2003).Since the extraction steps must be re-run if anychange is made to the input training data, the timerequired can be a major hindrance to researchers,especially those investigating the effects of tok-enization or word segmentation.To alleviate these issues, we extract only a sub-set of all available rules.
Specifically, we followCallison-Burch et al (2005; Lopez (2007) and usea source language suffix array to extract only thoserules which will actually be used in translating aparticular set of test sentences.
This results in avastly smaller rule set than techniques which ex-tract all rules from the training set.The current code requires suffix array rule ex-traction to be run as a pre-processing step to ex-tract the rules needed to translate a particular testset.
However, we are currently extending the de-coder to directly access the suffix array.
This willallow the decoder at runtime to efficiently extractexactly those rules needed to translate a particu-lar sentence, without the need for a rule extractionpre-processing step.2.3 Decoding Algorithms2Grammar formalism: Our decoder assumes aprobabilistic synchronous context-free grammar(SCFG).
Currently, it only handles SCFGs of thekind extracted by Heiro (Chiang, 2007), but is eas-ily extensible to more general SCFGs (e.g., (Gal-ley et al, 2006)) and closely related formalismslike synchronous tree substitution grammars (Eis-ner, 2003).Chart parsing: Given a source sentence to de-code, the decoder generates a one-best or k-besttranslations using a CKY algorithm.
Specifically,the decoding algorithm maintains a chart, whichcontains an array of cells.
Each cell in turn main-tains a list of proven items.
The parsing processstarts with the axioms, and proceeds by applyingthe inference rules repeatedly to prove new itemsuntil proving a goal item.
Whenever the parserproves a new item, it adds the item to the appro-priate chart cell.
The item also maintains back-2More details on the decoding algorithms are provided in(Li et al, 2009a).136pointers to antecedent items, which are used fork-best extraction.Pruning: Severe pruning is needed in order tomake the decoding computationally feasible forSCFGs with large target-language vocabularies.In our decoder, we incorporate two pruning tech-niques: beam and cube pruning (Chiang, 2007).Hypergraphs and k-best extraction: For eachsource-language sentence, the chart-parsing algo-rithm produces a hypergraph, which representsan exponential set of likely derivation hypotheses.Using the k-best extraction algorithm (Huang andChiang, 2005), we extract the k most likely deriva-tions from the hypergraph.Parallel and distributed decoding: We alsoimplement parallel decoding and a distributedlanguage model by exploiting multi-core andmulti-processor architectures and distributed com-puting techniques.
More details on these two fea-tures are provided by Li and Khudanpur (2008b).2.4 Language ModelsIn addition to the distributed LM mentionedabove, we implement three local n-gram languagemodels.
Specifically, we first provide a straightfor-ward implementation of the n-gram scoring func-tion in Java.
This Java implementation is able toread the standard ARPA backoff n-gram models,and thus the decoder can be used independentlyfrom the SRILM toolkit.3 We also provide a na-tive code bridge that allows the decoder to use theSRILM toolkit to read and score n-grams.
Thisnative implementation is more scalable than thebasic Java LM implementation.
We have also im-plemented a Bloom Filter LM in Joshua, followingTalbot and Osborne (2007).2.5 Minimum Error Rate TrainingJohsua?s MERT module optimizes parameterweights so as to maximize performance on a de-velopment set as measuered by an automatic eval-uation metric, such as Bleu.
The optimizationconsists of a series of line-optimizations alongthe dimensions corresponding to the parameters.The search across a dimension uses the efficientmethod of Och (2003).
Each iteration of ourMERT implementation consists of multiple weight3This feature allows users to easily try the Joshua toolkitwithout installing the SRILM toolkit and compiling the nativebridge code.
However, users should note that the basic JavaLM implementation is not as scalable as the native bridgecode.updates, each reflecting a greedy selection of thedimension giving the most gain.
Each iterationalso optimizes several random ?intermediate ini-tial?
points in addition to the one surviving fromthe previous iteration, as an approximation to per-forming multiple random restarts.
More details onthe MERT method and the implementation can befound in Zaidan (2009).43 WMT-09 Translation Task Results3.1 Training and Development DataWe assembled a very large French-English train-ing corpus (Callison-Burch, 2009) by conductinga web crawl that targted bilingual web sites fromthe Canadian government, the European Union,and various international organizations like theAmnesty International and the Olympic Commit-tee.
The crawl gathered approximately 40 millionfiles, consisting of over 1TB of data.
We convertedpdf, doc, html, asp, php, etc.
files into text, andpreserved the directory structure of the web crawl.We wrote set of simple heuristics to transformFrench URLs onto English URLs, and consideredmatching documents to be translations of eachother.
This yielded 2 million French documentspaired with their English equivalents.
We split thesentences and paragraphs in these documents, per-formed sentence-aligned them using software thatIBM Model 1 probabilities into account (Moore,2002).
We filtered and de-duplcated the result-ing parallel corpus.
After discarding 630 thousandsentence pairs which had more than 100 words,our final corpus had 21.9 million sentence pairswith 587,867,024 English words and 714,137,609French words.We distributed the corpus to the other WMT09participants to use in addition to the Europarlv4 French-English parallel corpus (Koehn, 2005),which consists of approximately 1.4 million sen-tence pairs with 39 million English words and 44million French words.
Our translation model wastrained on these corpora using the subsampling de-scried in Section 2.1.For language model training, we used themonolingual news and blog data that was as-sembled by the University of Edinburgh and dis-tributed as part of WMT09.
This data consisted4The module is also available as a standalone applica-tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/?ozaidan/zmert.
)137of 21.2 million English sentences with half a bil-lion words.
We used SRILM to train a 5-gramlanguage model using a vocabulary containing the500,000 most frequent words in this corpus.
Notethat we did not use the English side of the parallelcorpus as language model training data.To tune the system parameters we used NewsTest Set from WMT08 (Callison-Burch et al,2008), which consists of 2,051 sentence pairswith 43 thousand English words and 46 thou-sand French words.
This is in-domain data thatwas gathered from the same news sources as theWMT09 test set.3.2 Translation ScoresThe translation scores for four different systemsare reported in Table 1.5Baseline: In this system, we use the GIZA++toolkit (Och and Ney, 2003), a suffix-array archi-tecture (Lopez, 2007), the SRILM toolkit (Stol-cke, 2002), and minimum error rate training (Och,2003) to obtain word-alignments, a translationmodel, language models, and the optimal weightsfor combining these models, respectively.Minimum Bayes Risk Rescoring: In this sys-tem, we re-ranked the n-best output of our base-line system using Minimum Bayes Risk (Kumarand Byrne, 2004).
We re-score the top 300 trans-lations to minimize expected loss under the Bleumetric.Deterministic Annealing: In this system, in-stead of using the regular MERT (Och, 2003)whose training objective is to minimize the one-best error, we use the deterministic annealingtraining procedure described in Smith and Eisner(2006), whose objective is to minimize the ex-pected error (together with the entropy regulariza-tion technique).Variational Decoding: Statistical models inmachine translation exhibit spurious ambiguity.That is, the probability of an output string is splitamong many distinct derivations (e.g., trees orsegmentations).
In principle, the goodness of astring is measured by the total probability of itsmany derivations.
However, finding the best string(e.g., during decoding) is then computationally in-tractable.
Therefore, most systems use a simpleViterbi approximation that measures the goodness5Note that the implementation of the novel techniquesused to produce the non-baseline results is not part of the cur-rent Joshua release, though we plan to incorporate it in thenext release.System BLEU-4Joshua Baseline 25.92Minimum Bayes Risk Rescoring 26.16Deterministic Annealing 25.98Variational Decoding 26.52Table 1: The uncased BLEU scores on WMT-09French-English Task.
The test set consists of 2525segments, each with one reference translation.of a string using only its most probable deriva-tion.
Instead, we develop a variational approxima-tion, which considers all the derivations but stillallows tractable decoding.
More details will beprovided in Li et al (2009b).
In this system, wehave used both deterministic annealing (for train-ing) and variational decoding (for decoding).4 ConclusionsWe have described a scalable toolkit for parsing-based machine translation.
It is written in Javaand implements all the essential algorithms de-scribed in Chiang (2007) and Li and Khudanpur(2008b): chart-parsing, n-gram language modelintegration, beam- and cube-pruning, and k-bestextraction.
The toolkit also implements suffix-array grammar extraction (Callison-Burch et al,2005; Lopez, 2007) and minimum error rate train-ing (Och, 2003).
Additionally, parallel and dis-tributed computing techniques are exploited tomake it scalable.
The decoder achieves state ofthe art translation performance.AcknowledgmentsThis research was supported in part by the DefenseAdvanced Research Projects Agency?s GALE pro-gram under Contract No.
HR0011-06-2-0001 andthe National Science Foundation under grantsNo.
0713448 and 0840112.
The views and find-ings are the authors?
alone.ReferencesChris Callison-Burch, Colin Bannard, and JoshSchroeder.
2005.
Scaling phrase-based statisti-cal machine translation to larger corpora and longerphrases.
In Proceedings of ACL.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further meta-evaluation of machine translation.
InProceedings of the Third Workshop on StatisticalMachine Translation (WMT08).138Chris Callison-Burch.
2009.
A 109 word parallel cor-pus.
In preparation.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In Proceedingsof ACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of the ACL/Coling.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the International Work-shop on Parsing Technologies.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT/NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, , and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the ACL-2007 Demo and Poster Ses-sions.Philipp Koehn.
2005.
A parallel corpus for statisticalmachine translation.
In Proceedings of MT-Summit,Phuket, Thailand.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine transla-tion.
In Proceedings of HLT/NAACL.Zhifei Li and Sanjeev Khudanpur.
2008a.
Large-scalediscriminative n-gram language models for statisti-cal machine translation.
In Proceedings of AMTA.Zhifei Li and Sanjeev Khudanpur.
2008b.
A scalabledecoder for parsing-based machine translation withequivalent language model state maintenance.
In InProceedings Workshop on Syntax and Structure inStatistical Translation.Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur,and Wren Thornton.
2009a.
Decoding in joshua:Open source, parsing-based machine translation.The Prague Bulletin of Mathematical Linguistics,91:47?56.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.2009b.
Variational decoding for statistical machinetranslation.
In preparation.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment templates for statistical machinetranslation.
In Proceedings of the ACL/Coling.Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In Proceedings of EMNLP-CoLing.Robert C. Moore.
2002.
Fast and accurate sentencealignment of bilingual corpora.
In Proceedings ofAMTA.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingfor statistical machine translation.
In Proceedingsof ACL.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal smt.
In Proceedings of ACL.David A. Smith and Jason Eisner.
2006.
Minimum riskannealing for training log-linear models.
In Pro-ceedings of the ACL/Coling.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Process-ing, Denver, Colorado, September.David Talbot and Miles Osborne.
2007.
Randomisedlanguage modelling for statistical machine transla-tion.
In Proceedings of ACL.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.139
