Improving POS Tagging Using Machine-Learning TechniquesLlufs Mhrquez 1, Horacio Rodrfguez 2, Josep Carmona 1 and Josep Montol io 11 TALP Research Center.
Dep.
LSI - Universitat Polit~cnica de Catalunyac / Jo rd i  Girona 1-3.
08034 Barcelona.
Catalonialluism@l si .upc.
es2 Dep.
IMA - Universitat de GironaHoracio.
Rodriguez@ima.
udg.
esAbstractIn this paper we show how machine learningtechniques for constructing and combining sev-eral classifiers can be applied to improve theaccuracy of an existing English POS tagger(MSxquez and Rodrfguez, 1997).
Additionally,the problem of data sparseness i  also addressedby applying a technique of generating convezpseudo-data (Breiman, 1998).
Experimental re-sults and a comparison to other state-of-the-art tuggers are reported.Keywords :  POS Tagging, Corpus-based mod-eling, Decision Trees, Ensembles of Classifiers.1 Introduct ionThe study of general methods to improve theperformance in classification tasks, by the com-bination of different individual classifiers, is acurrently very active area of research in super-vised learning.
In the machine learning (ML)literature this approach is known as ensemble,stacked, or combined classifiers.
Given a classi-fication problem, the main goal is to constructseveral independent classifiers, since it has beenproven that when the errors committed by indi-vidual classifiers are uncorrelated to a sufficientdegree, and their error rates are low enough,the resulting combined classifier performs bet-ter than all the individual systems (Ali and Paz-zani, 1996; Tumer and Ghosh, 1996; Dietterich,1997).Several methods have been proposed in orderto construct ensembles of classifiers that makeuncorrelated errors.
Some of them are general,and they can be applied to any learning algo-rithln, while other are specific to particular al-gorithms.
From a different perspective, thereexist methods for constructing homogeneous en-sembles, in the sense that a unique learning al-gorithm has been used to acquire each individ-ual classifier, and heterogeneous ensembles thatcombine different ypes of learning paradigms 1.Impressive results have been obtained by ap-plying these techniques on the so-called unsta-ble learning algorithms (e.g.
induction of deci-sion trees, neural networks, rule-induction sys-tems, etc.).
Several applications to real taskshave been performed, and, regarding NLP, wefind ensembles of classifiers in context-sensitivespelling correction (Golding and Roth, 1999),text categorization (Schapire and Singer, 1998;Blum and Mitchell, 1998), and text filtering(Schapire t al., 1998).
Combination of classi-tiers have also been applied to POS tagging.
Forinstance, van Halteren (1996) combined a num-ber of similar tuggers by way of a straightfor-ward majority vote.
More recently, two parallelworks (van Halteren et al, 1998; Brill and Wu,1998) combined, with a remarkable success, theoutput of a set of four tuggers based on differentprinciples and feature modelling.
Finally, in thework by MSxquez et al (1998) the combinationof taggers is used in a bootstrapping algorithmto train a part of speech tagger from a limitedamount of training material.The aim of the present work is to improvean existing POS tagger based on decision trees(Mkrquez and Rodriguez, 1997), by using en-sembles of classifiers.
This tagger treats sepa-rately the different ypes (classes) of ambiguityby considering a different decision tree for eachclass.
This fact allows a selective constructionof ensembles of decision trees focusing on themost relevant ambiguity classes, which greatlyvary in size and difficulty.
Another goal of thepresent work is to try to alleviate the problemof data sparseness by applying a method, due1An excellent survey covering all these topics call befound in (Dietterich, 1997).53to Breiman (1998), for generating new pseudo-examples from existing data.
As we will see insection 4.2 this technique will be combined withthe construction of an ensemble of classifiers.The paper is organized as follows: we startby presenting the two versions of the POS tag-ger and their evaluation on the reference corpus(sections 2 and 3).
Sections 4 and 5 are, respec-tively, devoted to present he machine-learningimprovements and to test their implementation.Finally, section 6 concludes.2 Tree-based  TaggersDecision trees have been successfully applied toa number of different NLP problems and, in par-ticular, in POS tagging they have proven to bean efficient and compact way of capturing therelevant information for disambiguating.
See(MSxquez, 1999) for a broad survey on this is-sue.In this approach to tagging, the ambiguouswords in the training corpus are divided intoclasses corresponding to the sets of tags theycan take (i.e, 'noun-adjective', 'noun-adjective-verb', etc.).
These sets are called ambiguityclasses and a decision tree is acquired for eachof them.
Afterwards, the tree-base is applied ina particular disambiguation algorithm.Regarding the learning algorithm, we use aparticular implementation f a top-down induc-tion of decision trees (TDIDT) algorithm, be-longing to the supervised learning family.
Thisalgorithm is quite similar to the well-knownCART (Breiman et al, 1984), and C4.5 (Quin-lan, 1993), but it incorporates some particular-ities in order to better fit the domain at hand.Training examples are collected from anno-tated corpora and they consist of the targetword to be disambiguated and some informa-tion of its local context in the sentence.All words not present in the training corpusare considered unknown.
In principle, we haveto assume that they can take any tag corre-sponding to open categories (i.e., noun, propernoun, verb, adjective, adverb, cardinal, etc.
),which sum up to 20 in the Penn Treebanktagset.
In this approach, an additional ambigu-ity class for unknown words is considered, andso, they are treated exactly in the same wayas the other ambiguous words, except by thetype of information used for acquiring the trees,which is enriched with a number of morpholog-ical features.Once the tree-model has been acquired, it canbe used in many ways to disambiguate a realtext.
In the following sections, 2.1 and 2.2, wepresent wo alternatives.2.1 RTT: A Reductionistic Tree-basedTaggerRTT is a reductionistic tagger in the sense ofConstraint Grammars (Karlsson et al, 1995).In a first step a word-form frequency dictionaryprovides each input word with all possible tagswith their associated lexical probability.
Afterthat, an iterative process reduces the ambiguity(discarding low probable tags) at each step untila certain stopping criterion is satisfied.More particularly, at each step and for eachambiguous word (at a sentence level) the workperformed in parallel is: 1) The target wordis "passed" through its corresponding decisiontree; 2) The resulting probability distribution isused to multiplicatively update the probabilitydistribution of the word; and 3) The tags withvery low probabilities are filtered out.For more details, we refer the reader to(Mgrquez and Rodrfguez, 1997).2.2 STT: A Statistical Tree-basedTaggerThe aim of statistical or probabilistic tagging(Church, 1988; Cutting et al, 1992) is to as-sign the most likely sequence of tags given theobserved sequence of words.
For doing so, twokinds of information are used: the lexical prob-abilities, i.e, the probability of a particular tagconditional on the particular word, and the con-textual probabilities, which describe the proba-bility of a particular tag conditional on the sur-rounding tags.Contextual (or transition) probabilities areusually reduced to the conditioning of the pre-ceding tag (bigrams), or pair of tags (tri-grams), however, the general formulation allowsa broader definition of context.
In this way, theset of acquired statistical decision trees can beseen as a compact representation f a rich con-textual model, which can be straightforwardlyincorporated inside a statistical tagger.
Thepoint here is that the context is not restricted tothe n-1 preceding tags as in the n-gram formu-lation.
Instead, it is extended to all the contex-S4tual information used for learning the decisiontrees.The Viterbi algorithm (described for instancein (Deaose, 1988)),.
in which n-gram probabil-ities are substituted by the application of thecorresponding decision trees, allows the calcu-lation of the most-likely sequence of tags witha linear cost on the sequence length.
However,one problem appears when applying condition-ings on the right context of the target word,since the disambiguation proceeds from left toright and, so, the right hand side words may beambiguous.
Although dynamic programmingcan be used to Calculate the most likely sequenceof tags to the right (in a forward-backward ap-proach), we use a simpler approach which con-sists of calculating the contextual probabilitiesby a weighted average of all possible tags for theright context.Additionally, the already presented tagger al-lows a straightforward incorporation of n-gramprobabilities, by linear interpolation, in a back-off approach including, from most general tomost specific, unigrams, bigrams, trigrams anddecision trees.
From now on, we will refer toSTT as STT + when using n-gram information.Due to the high ambiguity of unknown words,their direct inclusion in the statistical taggerwould result in a severe decreasing of perfor-mance.
To avoid this situation, we apply thetree for unknown words in a pre-process for fil-tering low probable tags.
In this way, when en-tering to the tagger the average number of tagsper unknown word is reduced from 20 to 3.1.3 Eva luat ion  o f  the  Taggers3.1 Domain of  Appl icat ionWe have used a portion of about 1,17 Mw of theWall Street Journal (WSJ) corpus, tagged ac-cording to the Penn Treebank tag set (45 differ-ent tags).
The corpus has been randomly parti-tioned into two subsets to train (85%) and test(15%) the system.
See table 1 for some detailsabout the used corpus.The training corpus has been used to create aword form lexicon - -of  45,469 entries-- with theassociated lexical probabilities for each word.The training corpus contains 239 differentambiguity classes, with a number of examplesranging from few dozens to several thousands(with a maximum of 34,489 examples for thepreposition-adverb-particle ambiguity).
It isnoticeable that only the 36 most frequent am-biguity classes concentrate up to 90% of theambiguous occurrences of the training corpus.Table 2 contains more information about thenumber of ambiguity classes necessary to covera concrete percentage of the training corpus.Training examples for the unknown-word am-biguity class were collected from the trainingcorpus in the following way: First, the trainingcorpus is randomly divided into twenty parts ofequal size.
Then, the first part is used to extractthe examples which do not occur in the remain-ing nineteen parts, that is, taking the 95% of thecorpus as known and the remaining 5% to ex-tract the examples.
This procedure is repeatedwith each of the twenty parts, obtaining approx-imately 22,500 examples from the whole corpus.The choice of dividing by twenty is not arbi-trary.
95%-5% is the proportion that results ina percentage of unknown words very similar tothe test set (i.e., 2.25%) 2 .Finally, the test set has been used as com-pletely fresh material to test the taggers.
Allresults on tagging accuracy reported in this pa-per have been obtained against his test set.3.2 ResultsIn this experiment we used six basic discrete-valued features to disambiguate know n ambigu-ous words, which are: the part-of-speech tagsof the three preceding and two following words,and the orthography of the word to be disam-biguated.For tagging unknown words, we used 20 at-tributes that can be classified into three groups:?
Contextual information: part-of-speechtags of the two preceding and followingwords.?
Orthographic and Morphological informa-tion (about the target word): prefixes (firsttwo symbols) and suffixes (last three sym-bols); Length; Multi-word?
; Capitalized?
;Other capital letters?
; Numerical charac-ters?
; Contain dots??
Dictionary-related information: Does thetarget word contains any known word asa prefix (or a suffix)?
; Is the target word:See (M?rquez, 1999) for a discussion on the appro-priateness of this procedure.55TrainingTestS W W/S AW T/W T/AW T/DW U40,977 998,354 24.36 339,916(34.05%) 1.48 2.40 - -  - -7,167 175,412 24.47 59,440 (33.89%) 1.45 2.40 3.49 3,941 (2.25%)Total 48,144 1,173,766 24.38 399,356 (34.02%) 1.47 2.40 - -  - -Table 1: Information about the WSJ training and test corpora.
S: number of sentences; W: numberof words; W/S: average number of words per sentence; AW: number and percentage of ambiguouswords; T/W: average number of tags per word; T/AW: average number of tags per ambiguoust:nown word; T/DW: average number of tags per ambiguous word (including unknown words); andU: number and percentage of Unknown wordsClasses I 8 11 14 18 36 57 111 239 JTable 2: Number of ambiguity classes that cover the x% of the ambiguous words of the trainingcorpusthe prefix (or the suffix) of any word in thelexicon?The last group of features are inspired inthose applied by Brill (1995) when addressingunknown words.The learning algorithm 3 acquired, in aboutthirty minutes, a base of 191 trees (the otherambiguity classes had not enough examples)which required about 0,68 Mb of storage.The results of the taggers working with thistree-base is presented in table 3.
MFT standsfor a baseline most-frequent-tag tagger.
RTT,STT, and STT + stand for the basic versions ofthe taggers presented in section 2.
The over-all accuracy is reported in the first column.Columns 2, 3, and 4 contain the tagging ac-curacy on some specific groups of words: un-known words, ambiguous words (excluding un-known words) and known words which is thecomplementary of the set of unknown words.Column 5 shows the speed of each tagger 4 and,finally, the 'Memory' column reflects the size ofthe used language model (the lexicon is not con-sidered).Three main conclusions can be extracted:?
RTT and STT approaches obtain almostthe same results in accuracy, however RTTis faster.ZThe programs were implemented using PERL-5.0and they were run on a SUN UltraSparc2 machine with194Mb of RAM.4More than absolute figures what is important hereis the performance of each tagger elative to the others.5TT obtains better results when it incor-porates bigrams and trigrams, with a slighttime-space penalty.The accuracy of all taggers is comparableto the best state-of-the art taggers underthe open vocabulary assumption (see sec-tion 5.2).4 Mach ine-Learn ing-basedImprovementsOur purpose is to improve the performance ontwo types of ambiguity classes, namely:Most frequent ambiguity classes.
We fo-cused on the 26 most representative classes,which concentrate the 86% of the am-biguous occurrences.
From these, eight(24.1%) were already resolved at almost100% of accuracy, while the remaining eigh-teen (61.9%) left some room for improve-ment.
Section 4.1 explain which meth-ods have been applied to construct en-sembles for these eighteen classes plus theu n k n o w n - w o r d  ambiguity class.Ambiguity classes with few examples.
Weconsidered the set of 82 ambiguity classeswith a number of examples between 50 and3,000 and an accuracy rate lower than 95%.They agglutinate 48,322 examples (14.24%of the total ambiguous occurrences).
Sec-tion 4.2 explains the applied method toincrease the number of examples of theseclasses.56iTaggerMFTRTTSTT .STT +Overall Known Ambiguous Unknown Speed Memory92.75% 94.25% 83.40% 27.43% 2818 w/s 0 Mb96.61% 97.01% 91.36% 79.22% 426 w/s 0.68 Mb96.63% 97.02% 91.40% 79.60% ~ 321 w/s 0.68 Mb96.84% 97.21% 91.95% 80.70% !
302 w/s 0.90 MbTable 3: Tagging accuracy, speed, and storage requirement of RTT and STT taggers4.1 Ensembles of Decision TreesThe general methods for constructing ensem-bles of classifiers are based on four tech-niques: 1) Resampling the training data, e.g.Boosting (Freund and Schapire, 1995), Bagging(Breiman, 1996), and Cross-validated Commit-tees (Parmanto et al, 1996); 2) Combining dif-ferent input features (Cherkauer, 1996; Tumerand Ghosh, 1996); 3) Changing output repre-sentation, e.g.
ECOC (Dietterich and Bakiri,1995) and PWC-CC (Moreira and Mayoraz,1998); and 4) Injecting randomness (Dietterich,1998).We tested several of the preceding methodson our domain.
Below, we briefly describe thosethat reported major benefits.4 .
i .1  Bagging (BAG)From a training set of n examples, severaI sam-ples of the same size are extracted by randomlydrawing, with replacement, n times.
Such newtraining sets are called bootstrap replicates.
Ineach replicate, some examples appear multipletimes, while others do not appear.
A classifier isinduced from each bootstrap replicate and thenthey are combined in a voting approach.
Thetechnique is called bootstrap aggregation, fromwhich the acronym bagging is derived.
In ourcase, the bagging approach was performed fol-lowing the description of Breiman (1996), con-structing 10 replicates for each data set 5.4.1.2 Combining Feature SelectionCriteria, (FSC)In this case, the idea is to obtain different clas-sifiers by applying several different functions forfeature selection inside the tree induction algo-rithm.
In particular, we have selected a set ofseven functions that achieve a similar accuracy,namely: Gini Impurity Index, Information Gainand Gain Ratio, Chi-square statistic (X2), Sym-metrical Tau criterion, RLM (a distance-basedmethod), and a version of RELIEF-F which usesthe Information Gain function to assign weightsto the features.
The first five are described,for instance, in (Sestito and Dillon, 1994), RLMis due to LSpez de M?ntaras (1991), and, fi-nally, RELIEF-F is described in (Kononenko etal., 1995).
Since the applied feature selectionfunctions are based on different principles, weexpect o obtain biased classifiers with comple-mentary information.4.1.3 Combining Features (FCOMB)We have extended the basic set of six featureswith lexical information about words appear-ing in the local context of the target word, andwith the ambiguity classes of the same words.In this way, we consider information about thesurrounding words at three different levels ofspecificity: word form, POS tag, and ambiguityclass.Very similar to Brill's lexical patterns (Brill,1995), we also have included features to capturecollocational information.
Such features are ob-tained by composition of the already describedsingle attributes and they are sequences of con-tiguous words and/or POS tags (up to threeitems).The resulting features were grouped accord-ing to their specificity to generate nsemblesof eight trees 6.
The idea here is that spe-cific information (lexical attributes and colloca-tional patterns) would produce classifiers thatcover concrete cases (hopefully, with a high pre-cision), while more general information (POStags) would produce more general (but proba-bly less precise) trees.
The combination of bothtype of trees should perform better because ofthe complementarity of the information.5Several authors indicate that most of the potentialimprovement provided by bagging is obtained within thefirst ten replicates.6The features for dealing with unknown words werecombined in a similar way to create ensembles of 10 trees.For details, see (M?rquez, 1999).574.2 Generating Pseudo-Examples(CPD)Breiman (1998), describes a simple and effectivemethod for generating new pseudo-examplesfl'om existing data and incorporating them intoa tree-based learning algorithm to increase pre-diction accuracy in domains with few trainingexalnples.
We call this method CPD (standingfor generation of Convex Pseudo-Data).The method for obtaining new data from theold is similar to the process of gene combinationto create new generations in genetic algorithms.First, two examples of the same class are se-lected at random from the training set.
Then,a new example is generated from them by se-lecting attributes from one or another parentaccording to a certain probability.
This prob-ability depends on a single generation param-eter (a real number between 0 and 1), whichregulates the amount of change allowed in thecombination step.In the original paper, Breiman does not pro-pose any optimization of the generation param-eter, instead, he performs a limited amount oftrials with different values and simply reportsthe best result.
In our domain, we observeda big variance on the results depending on theconcrete values of the generation parameter.
In-stead of trying to tune it, we generate severaltraining sets using different values of the genera-tion parameter and we construct an ensemble ofdecision trees.
In this way, we make the globalclassifier independent of the particular choice,and we generally obtain a combined result whichis more accurate than any of the individuals.5 Experiments and Results5.1 Constructing and EvaluatingEnsemblesFirst, the three types of ensembles were appliedto the 19 selected ambiguity classes in order todecide which is the best in each case.
The eval-uation was performed by means of a 10-foldcross-validation ,using the training corpus.
Theobtained results confirm that all methods con-tribute to improve accuracy in almost all do-mains.
The absolute improvement is not veryimpressive but the variance is generally very lowand, so, the gain is statistically significant in themajority of cases.
Summarizing, BAG wins in 8cases, FCOMB in 9, and FSC in 2 (including theunknown-word class).These results are reported in table 4, in whichthe error rate of a single basic tree is comparedto the results of the ensembles for each ambi-guity class r. The last column presents the per-centage of error reduction for the best methodin each row.Second, CPD was applied to the 82 selectedambiguity classes, with positive results in 59cases, from which 25 were statistically signifi-cant (again in a 10-fold cross-validation exper-iment).
These 25 classes agglutinate 20,937 ex-amples and the error rate was diminished, o11average, from 20.16% to 18.17%.5.2 Tagging w i th  the  Enr iched  ModelEnsembles of classifiers were learned for the am-biguity classes explained in the previous sec-tions using the best technique in each case.These ensembles were included in the tree-base,used by the basic taggers of section 3, substi-tuting the corresponding individual trees, andboth taggers were tested again using the en-riched model.At runtime, the combination of classifiers wasdone by averaging the results of each individualdecision tree.In order to test the relative improvementof each component, the inclusion of the en-sembles is performed in three steps: 'CPD ~stands for the ensembles for infrequent ambi-guity classes, 'ENS' stands for the ensembles forfrequent ambiguity classes and unknown words,and 'CPD-~ENS' stands for the inclusion of both.Results are described in table 5.Some important conclusions are:?
The best result of each tagger is signifi-cantly better than each corresponding ba-sic version, and the accuracy consistentlygrows as more components are added.?
The relative improvement of STT + is lowerthan those of RTT and STT, suggestingthan the better the tree-based model is,the less relevant is the inclusion of n-graminformation.?
The special treatment of low frequent am-biguity classes results in a very small con-tribution, indicating that there is no much7These figures are calculated by averaging the resu|tsof the ten folds.58A-class #exsIN-RB-RP i2 VBD-VBN3 NN-VB-VBP4 VB-VBP5 JJ-NN6 NNS-VBZ I7 NN-NNP8 JJ-VBD-VBN9 NN-VBGI0 JJ-NNPii JJ-RB12 DT-IN-RB-WDT13 J JR -RBR14 NNP-NNPS-NNS15 J J -NN-RB16 J J -NN-VB17 JJ-NN-VBG18 JJ-VBGTotal19 unknown-word34,48925,88224,52217,78817,07715,29513,82411,4039,5978 7248 7228 4192 8682 8082 6252 1451.9861.980210.15422.594%exs10.16%7.63%7.23%5.24%5.03%4.51%4.07%3.36%2.83%2.57%2.57%2.48%o.85%0.83%0.77%0.63%0.59%0.58%61.93%B asic8.30%7.44%4.10%4.13%14.71%5.14%9.67%19.18%14.11%5.10%10.45%7.01%16.40%36.50%i5.31%13.32%20.30%21.11%9.35%20.87%BAG7.31%5.93%3.7o%3.62%13.30%4.37%9.10%17.91%12.53%4.5o%8.86%6.49%15.84%36.50%13.32%13.87%17.98%18.89%8.38%17,47%FSC7.79%6.64%3.84%3.94%13.50%4.59%8.37%18.05%12.93%4.56%9.75%6.84%15.28%35.14%11.83%12.99%18.79%19.39%8.61%16.86%FCOMB7.23%6.28%3.58%3.76%13.55%4.34%6.83%17.27%12.99%4.35%9.68%6.53%14.72%35.oo%12.44%12.75%18.23%19.60%8.25%17.21%BestER12.89%20.30%12.68%12.35%9.59%15.56%29.37%9.96%11.20%14.71%15.22%7.42%10.24%4.11%22.73%4.28%11.43%10.52%13.40%19.26%Table 4:classesComparative r sults (error rates) of different ensembles on the most significant ambiguityTaggerRTTRTT(cPD)RTT(ENS)RTT(cPD+ENS)STTSTT(cPD)STT(ENs)STT(cPD+ENS)STT +STT+(cPD)STT+(ENS)5TT+(CPD+ENS)Overall Known Ambig.
Unknown96.61% 97.00% 91.36% 79.21%96.66% 97.06% 91.51% 79.25%96.99% 97.30% 92.23% 83.25%97.05% 97.37% 92.48% 83.30%96.63% 97.02% 91.40% 79.60%96.69% 97.07% 91.56% 79.69%97.05% 97.36% 92.38% 83.78%97.10% 97.40% 92.51% 83.68%96.84% 97.21% 91.95% 80.70%96.88% 97.25% 92.09% 80.77%97.19% 97.48% 92.73% 84.47%97.22% 97.51% 92.81% 84.54%Speed Memory426 w/s 0.68Mb366 w/s 0.93Mb97 w/s 3.53Mb89 w/s 3.78Mb321 w/s 0.68Mb261 w/s 0.93Mb70 w/s 3.53Mb64 w/s 3.78Mb302 w/s 0.90Mb235 w/s 1.15Mb65 w/s 3.75Mb60 w/s 3.97MbTable 5: Tagging accuracy, speed, and storage requirements of enriched RTT and 5TT taggersto win from these classes, unless we wereable to fix their errors in a much greaterproportion than we really did.?
The price to pay for the enriched models isa substantial overhead in storage require-ment and speed decreasing, which in theworst case is divided by 5.In order to compare our results to others,we list in table 6 the results reported by sev-eral state-of-the-art PO5 taggers, tested onthe WSJ corpus with the open vocabulary as-sumption.
In that table, TBL stands forBrill's transformation-based error-driven tag-get (Brill, 1995), ME stands for a tagger basedon the ma?imum entropy modelling (Ratna-parkhi, 1996), SPATTER stands for a statisti-cal parser based on decision trees (Magerman,1996), IGTREE stands for the memory-basedtagger by Daelemans et al (1996), and, finally,TComb stands for a tagger that works by com-bination of a statistical trigram-based tagger,59TaggerTBLMESPATTERIGTREETCombSTT+(CPD+ENS)Train Test950 Kw 150 Kw963 Kw 193 Kw~975 Kw 47 Kw2,000 kw 200 Kw1,i00 Kw 265 KwOverall Known Unknown96.6% - -  82.2%96.5% - -  86.2%96.5% - -  - -96.4% 96.7% 90.6%97.2% - -  - -Ambig998 Kw 175 Kw 97.2% 97.5% 84.5% 92.8%Table 6: Comparison of different uggers on the WSJ corpusTBL and ME (Brill and Wu, 1998).Comparing to all the individual tuggers weobserve that our approach reports the highestaccuracy, and that it is comparable to that ofYComb obtained by the combination of threetuggers.
This is encouraging, since we have im-proved an individual POS tagger which could befurther introduced as a better component in anensemble of tuggers.Unfortunately, the performance on unknownwords is difficult to compare, since it stronglydepends on the used lexicon.
For instance,IGTREE does not include in the lexicon the num-bers appearing in the training set, and, so, anynumber in the test set is considered unknown(they report an unusually high percentage ofUnknown words: 5.5% compared to our 2.25%).The fact that numbers are very easy to rec-ognize could explain their outstanding resultson tagging unknown words.
ME also reportsa higher percentage of unknown words, 3.2%,?
while TBL says nothing about this issue.6 Conc lus ions  and  Fur ther  WorkIn this paper, we have applied several ML tech-niques for constructing ensembles of classifiersto address the most representative and/or diffi-cult cases of ambiguity within a decision-tree-based English POS tagger.
As a result, the over-all accuracy has been significantly improved.Comparing to other approaches, we see that ourtagger performs better on the WSJ corpus andunder the open vocabulary assumption, than anumber of state-of- the-art  POS tuggers, andsimilar to another approach based on the com-bination of several tuggers s.8However, it has to be said that the pure statisticalor machine-learning based approaches to POS taggingsti l l  significantly underperform some sophisticated man-ually constructed systems, such as the English shallowparser based on Constraint Grammars developed at theHelsinki University (Samuelsson and Voutilainen, 1997).The cost of this improvement has been quan-tiffed in terms of storage requirement and speedof the resulting enriched tuggers.
Of course,there exists a clear tradeoff between accuracyand efficiency which should be resolved on thebasis of the user needs.
Although all proposedtechniques are fully automatic, it has to be saidthat the construction of appropriate nsemblesrequires a significant human and computationaleffort.There are several features that should be fur-ther studied with respect o the  used methodsfor constructing the ensembles of decision trees,the way they are combined and included in thetuggers, etc.
However, we are now more inter-ested on experimenting with the inclusion of ourtagger as a component in an ensemble of pre-existing tuggers, in the style of (Brill and Wu,1998; van Halteren et al, 1998).More generally, one may think that, after allthe involved effort, the achieved improvementseems mall.
On this particular, we think thatwe are moving very close to the best achiev-able results using fully statistically-based tech-niques, and that some kind of specific humanknowledge should be jointly considered in orderto achieve the next qualitative step.
We alsothink that other issues than simply 'accuracyrates' are becoming more important in order totest and evaluate the real utility of different ap-proaches for tagging.
Such aspects, that shouldbe studied in the near future, refer to the abil-ity of adapting to new domains (tuning), thetypes of errors committed and their influenceon the task at hand, the language independenceassumption, etc.AcknowledgmentsThis research as been partially funded by theSpanish Research Department (CICYT's ITEMproject TIC96-1243-C03-02), by the EU Corn-60mission (EuroWordNet LE4003) and by theCatalan Research Department (CIRIT's con-solidated research group 1997SGR 00051, andCREL project).ReferencesK.
M. All and M. J. Pazzani.
1996.
Error Reductionthrough Learning Multiple Descriptions.
MachineLearning, 24(3): 173-202.A.
Blum and T. Mitchell.
1998.
Combining Labeledand Unlabeled Data with Co-Training.
In Pro-ceedings of the ilth Annual Conference on Com-putational Learning Theory, COLT-98, pages 92-100, Madison, Wisconsin.L.
Breiman, J. H. Friedman, R. A. Olshen, and C. J.Stone.
1984.
Classification and Regression Trees.Wadsworth International Group, Belmont, CA.L.
Breiman.
1996.
Bagging Predictors.
MachineLearning, 24(2): !23-140-L. Breiman.
1998.
I Using Convex Pseudo-Data toIncrease Prediction Accuracy.
Technical Report,Statistics Department.
University of California,Berkeley, CA.E.
Brill and J. Wu.
1998.
Classifier Combinationfor Improved Lexical Disambiguation.
I  Proceed-ings of the joint COLING-ACL'98, pages 191-195, Montreal, Canada.E.
Brill.
1995.
Transformation-based Error-drivenLearning and Natural Language Processing: ACase Study in Part-of-speech Tagging.
Compu-tational Linguistics, 21(4) :543-565.E.
Charniak.
1993.
Statistical Language Learning.The MIT Press, Cambridge, Massachusetts.K.J.
Cherkauer.
1996.
Human Expert-level Perfor-mance on a Scientific Image Analysis Task bya System Using Combined Artificial Neural Net-works.
In P. Chan, editor, Working Notes of theAAAI Workshop on Integrating Multiple LearnedModels, pages 15-21.K.
W. Church.
1988.
A Stochastic Parts Programand Noun Phrase Parser for Unrestricted Text.In Proceedings of the 1st Conference on AppliedNatural Language Processing, ANLP, pages 136-143.
ACL.D.
Cutting, J. Kupiec, J. Pederson, and P. Sibun.1992.
A Practical Part-of-speech Tagger.
In Pro-ceedings of the 3rd Conference on Applied Natu-ral Language Processing, ANLP, pages 133-140.ACL.W.
Daelemans, J .
Zavrel, P. Berck, and S. Gillis.1996: MBT: A Memory-Based Part-of-speechTagger Generator.
In Proceedings of the 4thWorkshop on Very Large ColTora , pages 14-27,Copenhagen, Denmark.S.
J. DeRose.
1988.
Grammatical Category Disam-biguatlon by Statistical Optimization.
Computa-tional Linguistics, 14:31-39.T.
G. Dietterich and G. Bakiri.
1995.
Solving Mul-ticlass Learning Problems via Error-CorrectingOutput Codes.
Journal of Artificial IntelligenceResearch, 2:263-286.T.
G. Dietterich.
1997.
Machine Learning Research:Four Current Directions.
AI Magazine, 18(4):97-136.T.
G. Dietterich.
1998.
An Experimental Compar-ison of Three Methods for Constructing Ensem-bles of Decision Trees: Bagging, Boosting, andRandomization.
Machine Learning, pages 1-22.Y.
Freund and R. E. Schapire.
1995.
A Decision-Theoretic Generalization of On-line Learningand an Application to Boosting.
In Pro-ceedings of the 2nd European Conference onComputational Learning Theory, EuroCOLT'95,Barcelona, Spain.A.
R. Golding and D. Roth.
1999.
A Winnow-basedApproach to Spelling Correction.
Machine Learn-ing, Special issue on Machine Learning and Nat-ural Language Processing.H.
van Halteren, J. Zavrel, and W. Daelemans.1998.
Improving Data Driven Wordclass Taggingby System Combination.
In Proceedings of thejoint COLING-A CL'98, pages 491-497, MontrEal,Canada.H.
van Halteren.
1996.
Comparison of TaggingStrategies, a Prelude to Democratic Tagging.
S.Hockney and N. Ide (eds.
), Clarendon Press.Research in Humanities Computing 4.
Selectedpapers for the ALLC/ACH Conference, ChristChurch, Oxford.F.
Karlsson, A. Voutilainen, J.
Heikkil?, andA.
Anttila, editors.
1995.
Constraint Grammar:A Language-Independent System for Parsing Un-restricted Text.
Mouton de Gruyter, Berlin andNew York.I.
Kononenko, E. Simec, and M. Robnik-Sikouja.1995.
Overcoming the Myopia of Inductive Learn-ing Algorithms with RELIEFF.
Applied Intelli-gence, 10:39-55.R.
LSpez de Mhntaras.
1991.
A Distance-Based At-tribute Selection Measure for Decision Tree In-duction.
Machine Learning, Kluwer Academic,6(1):81-92.D.
M. Magerman.
1996.
Learning GramnaaticalStructure Using Statistical Decision-Trees.
InProceedings of the 3rd International Colloquiumon Grammatical Inference, ICGL Springer-VerlagLecture Notes Series in Artificial Intelligence1147.L.
M~trquez and H. Rodrfguez.
1997.
AutomaticallyAcquiring a Language Model for POS Tagging Us-ing Decision Trees.
In Proceedings of the SecondConference on Recent Advances in Natural Lan-61guage Processing, RANLP, pages 27-34, TzigovChark, Bulgaria.L.
M~rquez, L. Padr5, and H. Rodrfguez.
1998.hnproving Tagging Accuracy by Voting Taggers.In Proceedings of thc 2nd Conference on Nat-ural Language Processing ~ Industrial Applica-lions, NLP+IA/TAL+AI, pages 149-155, NewBrunswick, Canada.L.
Mhrquez.
1999.
Part-of-Speech Tagging: A Ma-chine Learning Approach based on Decision Trees.Phd.
Thesis, Dep.
Llenguatges i Sistemes In-fortuities.
Universitat Polit~cnica de Catalunya.(Forthcoming)M.
Moreira and E. Mayoraz.
1998.
Improved Pair-wise Coupling Classification with Correcting Clas-sifiers.
In Proceedings of the lOth European Con-ference on Machine Learning, ECML, pages 160-171, Chemnitz, Germany.B.
Parmanto, P.W.
Munro, and H.R.
Doyle.
1996.hnproving Committee Diagnosis with ResamplingTechniques.
In M.C.
Mozer D.S.
Touretzky andM.E.
Hesselmo, editors, Advances in Neural In-formation Processing Systems, volume 8, pages882-888.
MIT Press., Cambridge, MA.J.
R. Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann Publishers, Inc.,San Mateo, CA.L.
R. Rabiner.
1990.
A Tutorial on Hidden MarkovModels and Selected Applications in Speech Recog-nition.
Readings in Speech Recognition (eds.
A.Waibel, K. F. Lee).
Morgan Kaufmann Publish-ers, Inc., San Mateo, CA.A.
Ratnaparkhi.
1996.
A Maximum Entropy Part-of-speech Tagger.
In Proceedings of the 1st Con-ference on Empirical Methods in Natural Lan-guage Processing, EMNLP'96.C.
Samuelsson and A. Voutilainen.
1997.
Compar-ing a Linguistic and a Stochastic Tagger.
In Pro-ceedings of the 35th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 246-253, Madrid, Spain.R.
E. Schapire and Y.
Singer.
1998.
BoosTexter: Asystem for multiclass multi-label text categoriza-tion.
Unpublished.
Postscript version available atAT&T Labs.R.
E. Schapire, Y.
Singer, and A. Singhal.
1998.Boosting and Rocchio applied to text filtering.
InIn Proceedings of the 21st Annual InternationalConference on Research and Development in In-formation Retrieval, SIGIR '98.S.
Sestito and T. S. Dillon.
1994.
AutomatedKnowledge Acquisition.
T. S. Dillon (ed.
), Seriesin Computer Systems Science and Engineering.Prentice Hall, New York/London.K.
Tumer and J. Ghosh.
1996.
Error Correla-tiou and Error Reduction in Ensemble Classifiers.Connection Science.
Special issue on combiningartificial neural networks: ensemble appTvache.~,8(3 and 4):385-404.62
