Statistical and Linguistic Strategies in theComputer Grading:of EssaysEllis B. PageUniversity of ConnecticutStorrs, Conn,, U.S.A.Essay tests are used in the schools and colleges ofall nations, and in major testing programs of nationaland eveh international size.
Potentially, such essaytests are an important applied field for computationallinguistics, and.should eventually provide focus formuch work.
Yet in the past, little direct attention hasbeen paid to such grading, although there are ways tobegin investigation which would not necessarily requiremuch linguistic knowledge beyond that now available.Beginning in December of 1964, Project Essay Grade(PEG), at the University of Connecticut, has investi-gated the c0mpute~lanalysis and evaluation of studentwriting.
In February, 1965, the project was givenpilot funding by the College Entrance ExaminationBoard of New York City, and in June, 1966, the United3tares Office of Education gave it much larger ~upport.Zhrough this period of preliminary investigation, icer-tainproblems have become much better understood(Daigon, 1966; Page, 1966, 1967).
~his paper discussesthese problems, relates certain major findings to date,and outlines apparently promising avenues for futurework by linguists, computer scientists, psychologists,and educators.BackgroundIt'is useful to conceptualize the field of essaygrading in two dimensions, as represented in Figure 1;Figure 1liossible Dimensiens ofEssay GradingContent11StyleA.Rating ~ (A) |I (A)SimulationB.Master I 08) II (B)AnalysisAny serious effort to grade essays must obviouslyface problems of "content" as in Column I, and of "style"as in Column II.
Yet it is obvious that these columnsare not mutually exclusive.
Similarly, the  rows are notmutually exclusive either; but their general meaningmust be mastered to understand the work to date and the ~problems of the field.
The first row refers to thesimulation of the human judgment, without great concernabout the way this judgment was produced.
The secondrow refers to the accurate, deep, "true" analysis ofthe essay.We have coined two terms to describe this differ-ence.
Since the top row is concerned with approximation,we speak of the computer-variables employed as ~ .Since the bottom row is concerned with the true intrin-sic" variables of interest, we speak of such var iab l -~as trins.
A trin, then, is a variable of intrinsicinterest to the human judge, for example, "aptness ofword choice".
Usually a trin is not directly measurableby present computer strategies.
And a prox is anyvar iab le  measured by the computer, as an approximation(or correlate) of some trin, for example, proportion ofuncommon words used by a student (where common words arediscovered by a list look-up procedure in computermemory).In the early par?
of our investigations, we concen-trated on the right column and top row of Figure i, look-ing for actuarial strategies, seeking out those proxeswhich would be of most immediate use in the simulationof the final human product, the ratings of stylisticfactors.For the first attempts, we evolved a general researchdesign, which we have more or less followed to date:(i) Samples of essays were judged by a number ofindependent experts.
For our first trial 272 essays ,written by students in Grades 8 to 12 in an Americanhigh school, and judged by at least four independentteachers.
These judgments of overall quality formed thetrins.
(2) Hypotheses were generated about the variableswhich might be associated with these judgments.
Ifmeasurable by computer, and feasible to program withinthe logistics of the study, these computer variables be-came the proxes of the study.
(3) Computer routines were written to measure theseproxes in the essays.
These were written in FJORTRAN IV,for the IBM 7040 computer, and are highly modular andmnemonic programs, fairly well documented and availableto computational linguists interested in using them oradapting them.
(4) Essays were prepared for computer input.
Inthe present stage of data processing, this means thatthey were typed by clerical workers on an ordinary key-punch.
They were punched into cards, and these cardsserved as input for the next stage.
(5) The essays were passed through the computer,under the control of the program which collected dataabout the proxes.
The output was as appears in Figure 2.PEG-L4 OutputO |w THINK IHAT |F  PEOPLE WOULD L lYE  HO\ [HE IR  lANK 8i)UK EVERYQNE NQQLO IE  8E0 IOZ I 19  S I 106 .88  26  676  O 0 i !
0 0 0 0 0 2 0 0 ,k 0 0 ZO ~ .
.
.
.
.
1CAP(N,,%T ncLv  w.  ~ A k , l ~Luz  ?
~u S ?
r .
=u= ,~k .
.
l  u ?
u ?
& u u .
u u .
v ?
.
u0I GUESS THAt  I t  IS  JUSt  WISHFULGOD FOR \ [HAT0 ~)  10E 1 E1 S 1 ot  46z E1 44t  0 t Z 1 0 0 0 ?
0 0 0 0 Z 0 0~)  102 I EL S IZ 149T t365 371 768S 0 4 26 El  0 3 0 0 0 8 0 0 32 2 Z k0 t "~ i1021- -1~-17 I "  =.
ST* =}k .
&* u .
lu*  ~, .
0 .
C,.
G .
&*  .&oJ,.
& .
'~k~IOZ2 O.
9 .
h l*  3.
E. 06. tO0.
100.
O.
3.
O.
404.
109. t3*0~ m  VUU UCmN ~UM~e ~IT I~ l  ~ i~IN L IFE  iRE  FREE * ?
THEO ~EmS~ ,~ .u ,  vex .
.oac~M,  ?
; "  lu= ~ i ?
u vz *=u ~c qu~ u u ~ ~ u u u u u z v u ~ u u ?Figure 2 shows a piece of output from PEG-IA.
LineA shows the way a sentence from the student essay is re-written in 12-character double-precision computer "words"and stored in memory.
Line B shows the summary of datafor that sentence just analyzed.
The first number isthe essay identification.
The other numbers of Line Bare some counts from that sentence.
Line C shows asummary of these counts, across sentences, for this wholeessay.
And Line D are these measures transformed in anumber of simple ways, and ready for input into the f ina lanalysis.
(6) These scores were then analyzed for their mul-tivariate relationship to the human ratings, were weightedappropriately, and were used to maximize the predictionof the expert human ratings.
This was all done by use ofstandard mult iple-regression programs.The first analyses produced results as shown inTable i.
Here it is possible to read the list of proxes(Col. A), "and their correlation, after transformation,with the human judgments of overall quality (Col. B).Col.
C shows their contribution to the total multipleregression, and Col. D indicates the test-retest relia-bil ity of the proxes themselves, as discovered from twoessays written by the same students, with about a monthbetween writings.Tab le  1"Variables Used in Project Essay Grade I-Afor a Criterion of Overall QualityA.
B.Proxes Corr.
withCriterion1.
Title present .042.
Av.
sentence l ngth .043.
Number of paragraphs .064.
Subject-verb openings - .165.
Length of essay in words .326.
Number o~" parentheses .047.
Number of apostrophes - .238.
Number of commas .349.
Number of periods --.0510.
Number of underlined words .0111.
Number of dashes .2212.
No.
colons .0213.
No.
se/nicolons .0814.
No.
quotation marks .1115.
No.
exclamation marks - .0516.
No.
question marks - ?
i417.
No.
prepositions .2518.
No.
connective words .1819.
No.
spelling errors --.21 .20.
No.
relative pronouns .1121.
No.
subordinating conjs.
--.1222.
No.
common words on Dale --.4823.
No.
sents, end punc.
pres.
--.0124.
No.
declar, sents, type A .1225.
No.
declar, sents, type B .0226.
No.
hyphens .1827.
No.
slashes - .0728.
Aver.
word length in Itrs.
.5129.
Stan.
dev.
of word length .5330.
Stan.
dev.
of sent.
length - .07C.
O.Beta wts.
Test-Ret.
Rel.
(Two essays).09 .05--.13 .63-- .
I  1 .42--.Oi .20.32 .55--.01 .21- -  .06 ?42.O9 .6 I--.05 .57.00 .22?
10  .44- -  .03 .29.06 .32.04 .27.09 .20.Ol .29.lO .27- -  .02 .24--.13 .23.11 .17.06 .18- -  .07  .65- -  .08  .14.14 .34.02 .09.07 .20- -  .02  - -  ~ .02.12 .62.30 .61.03 .48*N~r  of students jud~d was 272.
Multiple R ~ainst human criterion (~ur judas) w~ ,71 ~rbmh Essay C and Essay D (D d~a shown ~R).
~rat i~ ~r  Multiple R we~ highly significant.IThe overall accuracy of this beginning strategy wasstartling.
The proxes achieved a mult iple-correlat ion of.71 for the first set of essays analyzed and, by chance,achieved the identical coefficient for the second Set.Furthermore, the beta weightings from one set of essaysdid well in predicting the human judgments for the secondset of  essays written by the same youngsters.
All in all,the computer did a respectable, "human-expert" job ingrading essays, as is visible in Table 2.Tab le  2Which One is the Compute r ?"
Below is the intercorreMtion matr~ generated by the cross-validation of P?o \[JudtesA B C D EA 51 51 44 57B 51 53 56 61C 51 53 48 49D 44 56 48 59E ?
57 61.
49 '59Here we see the results of a cross-validation.These are correlations between judgments of 138 essaysdone by five "judges," four of them human and one ofthem the computer.
The computer judgments were thegrades given by the regression weightings based on 138other essays by other students.
This cross-validation,then,  is very conservative.
Yet, from a practicalpoint of view, the five judges are indistinguishablefrom one another.However useful such an overall rat ing m&ght be,we of course still wished greater detail in our analysis.We therefore broadened the ana lys i s - - -~ ive  traits be-lieved important in essays, adapted partly from thoseof Paul Diederich.
They may be summarized as: ideas,organization, st__~, mechanics, and creativity.
Wehad a partfcular interest in creativity, since some ?critics from the beginning have believed that the com-puter must founder on this kind of measure.
"YOU migh tgra~e mechanics all right," someone will say, "but whatabout originality?
What about the fellow who is reallydifferent?
The machine can't handle him~"Therefore, in 1966 we called together a group of32 highly qualif ied Eng l i shteachers  from the schoolsof Connecticut to see how they would handle creativityand these other traits.
Each of 256 essays wes ratedon a five-point scale on each of these five importanttraits, by eight such expert judges, each acting inde-pendently of any other judge.
The teacher ratings werethen analyzed, and it was found that the essay and thetrait contributed significant variances, as did thetrait-by-essay interaction, (perhaps the clearest demon-stration'of the ipsative profile).
To investigate eacho f  these five trait ratings, the same 30 proxes wereagain employed, with the results to be seen in Table 3.Table 3Computer Simulation of Human JudgmentsFor Five Essay Traits(30 predictors, 256 cases)A.
B. C. D. E.Hum.-Gp.
Mult.
Shrunk.
Corr.Traits Reliab.
R Mult.
R (At--~t-6~.)I.
Ideas or Content .75 .72 .68 .78II.
Organization .75 .62 .55 .64III.
Style .79 .73 .69 .77IV.
Mechanics ~85 .69 .64 .69V.
Creativity .
.72 .71 .66 .78Note:Coi.
B represents the reliabil ity of the human judg-ments of each trait, based upon the sum of eight inde,pendent ratings, August 1966.Col.
C represents the mult iple-regression coeffi-cients found in predicting the pooled human ratingswith 30 independent proxes found in the essays by thecomputer  program of PEG-IA.Col.
D presents these same coefficients, shrunkento eliminate capitalization on chance from the numberof predictor variables (cf.
McNemar, 1962, p. 184~Col.
E presents these coefficients, both shrunkenand corrected for the unreliabil ity of the human groups(cf.
McNemar, 1962, p.
153.
)In our rapidly growing knowledge, Table 3 maytemporarily say the most to us about the computer anal-ysis of important essay traits.
Column A of coursegives the titles of the five traits (more completedescriptions of the rating instructions may be suppliedon request).
Column B shows the rather low rel iabil ityof the group of eight human judges, computed by anal-ysis of variance.Here in Column B "creativity" is less reliablyjudged by these experts than are the other traits,even when eight judgments are pooled.
And mechanicsmay be the most reliably graded of these five traits.Surely, then, humans seemed to have a harder timewith c reat iv i t~wi th  mechanics.What of the computer?
Column C shows the\[rawmultiple correlations of the proxes with these ratherunre l iab le  group judgments.
These were the coeffi-cients produced by the standard regression program runby Dieter Paulus and myself.
Column D simply showsthe same coefficients after the necessary shrinking to ?avoid the Capitalization on chance which is inherentwith multiple predictors.
Finally, in order for afair comparison to be made among the traits, thecriterion's unreliabil ity should be taken into account,as in Column E. Here such difficult variables ascreativity and organization no longer seem to suffer;the computer's difficulty is apparently in the criter-ion itself, and is therefore attributable to humanlimitations, rather than to machine or program limita-tions.
Column E, then, exhibits what might be theexpectable cross-validation from a similar set ofessays, if predicting a perfectly reliable set of humanjudgments.Current and Projected ProblemsOf course, all this is a temporary reading takenin the middle of the research s t ream.
Our investigatorshave also gone on Withjother strategies.
Donald Mar-cotte (1967) has developed a phrase analyzer, and hasdiscovered that cliches, as usually listed, were largelyirrelevant to the judgment of such essays.
DieterPaulus (1967a) has studied the Curvil inearity of proxes,and concluded that much elaborate statistical optimiza-tion may be a?waste of time; and that the most majorimprovements should probab lybe  made in other ways.
Healso has studied feedback to the student writer, usingan on-line time-sharing console (Paulus, 1967b), ashas also Michael Zieky.
Another researcher, Jack H.Hiller (1967), has investigated quasi-psychologicaldimensions (including opinionation and vagueness)aspredictors of the human judgments.
Using techniquesfamiliar from automatic content analysis (cf.
Stone et al1966), he constructed lists of words and ph-{ases to ~-6 - ?-fine the variables of psychological interest, and foundthese negatively correlated, as he predicted, with writ-ing quality.
And, in May, 1967, a sizeable improvementwas made in the statistical accuracy, increasing themultiple-regression coefficient from about .71 to about.77, and improving the variance accounted for by around20%.
In other words, the newest programs apparently dobetter than the indiyidual, expert English teacher.The early strategies, then, have provided fertileground for statistical investigation of essay grading,especially in the actuarial simulation of rating ofstyle.
But what of the deeper dimensions of stylisticanalysis, and what of subject-matter ?content, as inessay questions in history, philosophy, or science?7Possible contributory linguistic strategies havebeen under more intensive study in recent months, withthe advice and help of Susumu Kuno (1964), StanleyPetrick (Keyser and Petrick, 1967), John Olney (Olneyand Londe, 1966; also see Harris, 1952) and others.
(Of course these workers are not resppnsible for errorsor misconceptions in the present paper.)
Anticipatedf~ture strategies are currently summarized in Table 4.This table is based partly on work already accomplishedin Project Essay Grade, partly on suggested minoradaptations of systems already working for others, andpartly on projected programs which are not yet appar-ently operative in any system, but which do not seemimpossibly difficult at the efficiency desired.Table 4Project Essay GradeHypothetical Complete Essay Graderi.2..4.5.INPUT and PUNCH.
Handwritten or typewritten orother raw response of the writer is convertedfor computer input.SNTORG.
Creates arrays of words and sentences asfound in prose.
This is just as performed inPEG-I.DICT.
Assignment of available syntactic roles toeach word.
This is currently done by many pro-grams, but needs an expanded dictionary, andambiguity resolver.'
At the same time, thesemantic information will be stored in the work-space for reference of other parts of program.Availabil ity of the tape-written Random HouseDictionary (Unabridged) has been promised.PARS.
A modified Kuno (1964) program seems mostpromising, and is currently being programmed forboth the 7094 and the 360 by workers at IBM.Alterations will be hecessary to accep?
wellnformed substrings.:REFER.
This is intended to identify and encode themost likely referents of pronouns and otheranaphoric expressions.
(Cf.
Olney and Londe,1966).
This process must employ both syntacticfeatures and semantic information from DICT.
(Continued)Table 4 (Continued)6.
KERNEL and STRUC.
From the rewritten string outputof (5), KERNEL would establish a set  of elemen-tary propositfons, and STRUC would encode therelationships among these elements.
This stepwould retain all the information of an essay insimplest possible units, yet would retain addi-tional information about emphasis, subordination,causal relation, etc., among these units..i0.7 .
EQUIV.
The elementary units would be augmented b.ythe semantic information in DICT.
To each wordwould be assigned a cluster of permissiblesynonyms, with weightings of semantic distance.This permits an analysis of redundance andemphasis in the essay, and permits a comparisonof the content of the student essay with that ofthe key or master essay.8.
STYLE.
Descriptions of the surface structure char-acteristics of the essay~ parts of speech,organization of themes, types and varieties ofsentence structure, grammatical dePths, tightnessof reference, etc~ information about grammaticalerrors and strengths.CONTNT.
Comparison of the agreement of student andmaster essay, through measure of kernel hits andstruc hits, these weighted by semantic distanceof~language chosen.JSCOR.
Multivariate prediction of appropriate pro-f i le for the immediate purpose .The limitations of space will permit only a fewcomments on this table, which may be seen as representinga hypothetical, ideal essay grader.
For large gradingsystems, over established substantive content, it wouldbe possible, for the key or master ~ ,  to edit by handthe output fro--m-ce--~ain ro-utln~s(especlally REFER andSTRUC).
Of course, four of the most important routineslisted in Table 4 are far from perfected in any existingprograms.
Ideally, they would assume better solutions tocertain major, stubborn problems in computational linguis-tics.
?Indeed, the steps in this hypothetical essaygrader are close to the heart of the most persistentand  troublesome problems in linguistics.
Is itnecessary that sentences be syntactically analyzedbefore mapping into deep structure?
What is the properrole of semantics in such deep structure?
How can theoutside knowledge of the reader be incorporated intothe machine analysis?
(For some discussion of this pro-blem, see Quillian, 1966).
In general, how may we in-corporate some of the intuitive richness which theliterate hu~lan brings to his reading?It is not expected that workers in essay gradingwill suddenly resolve all such questions.
They may berecognized as those which so trouble linguists as tocontribute to the recent official pessimism, in theUnited States, about the future of mechanical transla-tion.
After 15 years of effort, mechanical translationis still regarded as disappointing in quality, and vir-tually no sustained output of any machine program wouldbe ordinarily mistaken for the work of a professionalhuman t rans la tor .On the other hand, the earliest attempts at essaygrading by computer have, in a very limited way, leapedahead of machine translation.
And if the expert humanratings of high school essays may be regarded as anacceptable goal, then the machine program appears tohave reached such a goal already.
For that matter,improved performance, even superior to that of the in-dividual human expert, appears to be immediately practi-cable as well.The explanation of this advantage, of course, isthat the-problem of essay grading as attacked in thecurrent work is much easier than the problem of machinetranslation.
In translation, every nuance of the inputs t r ingshou ld  be accounted for in the output string.In essay grading, only-a certain portion of the inputtext needs to be accounted for, and the output .doesnotdepend on the existence of any large,language-generatingsystem.
High quality machine translation apparently de-mands a fair portion of the total language-manipulatingcapability of the human, but essay grading may use onlya fraction of it, and may process language in ways quitedifferent from that of the human being.
For example, ourpresent programs have to date largely ignored order andsequence in the essays, although to the human th--~rderof words is, of course , of crucial and unceasing import-ance.Since essay grading can work with such fractionalinformation, then, why pursue the deeper analysis ofTable 4?
Clearly, the purpose is not entirely the sameas it would be for the usual linguist.
At any d iscretei0time in research, what is sought is not necessarily theperfect humanoid behavior, but rather those portions ofthat  behavior which, given any current state of the art,will contribute optimally to efficient and practicableimprovements in output.
Indeed, regardless of theeventual perfection of deep linguistic behavior, for anyspecific application to essay grading, at any one moment,large portions of such available behavior may be irrele-vant, just as it seems that ordinary human languageprocessing does not usually call for our full l inguisticeffort.Yetwe regard it as eventually important to be~ble to perform these various kinds of advanced machineanalysis when required.
Therefore, the eventual usesof the ideal essay analyzer may require analytic capa-bility as deep as may be imagined.
Writing out suitablecomments for the student, for example, will in somecases~ tax any system which may be foreseen.Even approximate solutions to these problems, how-ever, though unsatisfactory for certain scientific pur-poses,  could make important contributions to the educa-tional description and evaluation of essays.
For suchevaluation is itself probabilistic, l imited by imperfectasymptotes of writer consistency and rater agreement.And such evaluation therefore does not require, to bepracticable and satisfactory, the same deterministicperfection which has continued to elude and frustrateresearchers ~ mechanical translation.
~ There is a fund,amental difference in goals, which must be realized.
Ashas been demonstrated here, the output from much cruderstatistical programs has already reached a quality nottoo remote from usefulness.
The more advanced strate-gies currently seem, at least to the present workers,bright with promise,11REFERENCESDaigon, Arthur.
Computer Grading of English Composition.The English Journal, January, 1966, 46-52.Harris, Z. S. Discourse analysis.
(4), 474-493.Language, 1952,Hiller, Jack H., Page, E. B., and Marcotte, D. R. AComputer Search for Traits of Opinionation, Vague-ness, and Specificity-Distinction in Student Essays.Paper read at the Annual Meeting of the AmericanPsychological Association, Washington, D.C.,September 2, 1967.Keyser, S. J., and Petrick, S. R. Syntactic Analysis,1966.
(In press in a forthcoming book.
~)Kuno, Susumu.
Some characteristics of the Multiple-PathSyntactic Analyzer.
Language Data Processing,Cambridge: Harvard Computation Laboratory, 1964.C6, 1-8.Marcotte, Donald.
The.
Computer Analysis of Clich~ Be-havior in Student Writing.
Paper readat  theAnnual Meeting of the American Educational ResearchAssociation, New York, February 18, 1967.McNemar, Quinn~ Psychological Statistics, 3rd ed.
NewYork: Wiley, 1962.Olney, John and Londe, D. A research plan for investi-gating English discourse structure with particularattention to anaphoric relationships.
Tech Memomm-(L)-3256.
Santa Monica, California " SystemDevelopment Corporation.
November 22, 1966.
17 p.Page, Ellis B.
The Imminence of Grading Essays byComputer.
Phi Delta Kappan, January, 1966, 238-243.Page, Ellis B. Grading Essays by Computer: ProgressReport.
Proceedings of the 1966 Invitational Con-ference on Testing Problems.
Princeton, N.J.:Educational Testing Service, 1967.
Pp.
87-100.Paulus, Dieter.
Problems of Nonlinearity in GradingEssays.
Paper read atbthe Annual Meeting of theAmerican Educational R4search Association, NewYork ,  February'16, 1967a.Paulus, Dieter.
Feedback in Project Essay Grade.
Paper?
read at the Annual'Meeting of the American Psycholog-ical Association, Washington, D.C., September 2,1967b.Quillian, M. Ross.
Semantic Memory.
Cambridge, Mass.
:Bolt Beranek and Newman, 1966.12References (Continued)Stone, Philip J., Dunphey, Dexter C., Smith, Marshall S.,and Ogilvie, Daniel M. The General Inquirer: AComputer Approach to Content Analysis.
Cambridge:M.I.T.
Press, 1966.
Pp.
651.Woods, William A.
Semantics for.a Question-AnsweringSystem.
Paper read at the Annual Meeting of theAssociation for Machine Translation and Computa-tional Linguistics.
Atlantic City, N.J. April 21,196713
