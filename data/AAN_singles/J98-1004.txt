Automatic Word Sense DiscriminationHinr ich  Sch{itze*Xerox Palo Alto Research CenterThis paper presents context-group discrimination, a disambiguation algorithm based on cluster-ing.
Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word.Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valuedspace in which closeness corresponds to semantic similarity.
Similarity in Word Space is basedon second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned tothe same sense cluster if the words they co-occur with in turn occur with similar words in atraining corpus.
The algorithm is automatic and unsupervised in both training and application:senses are induced from a corpus without labeled training instances or other external knowledgesources.
The paper demonstrates good performance of context-group discrimination for a sampleof natural and artificial ambiguous words.1.
IntroductionWord sense disambiguation is the task of assigning sense labels to occurrences of anambiguous word.
This problem can be divided into two subproblems: ense discrimi-nation and sense labeling.
Sense discrimination divides the occurrences of a word intoa number of classes by determining for any two occurrences whether they belong tothe same sense or not.
Sense labeling assigns a sense to each class, and, in combinationwith sense discrimination, to each occurrence of the ambiguous word.
This view ofdisambiguation as a two-stage process may not be completely general (for example,it may not be appropriate for the iterative process by which a lexicographer arrivesat the sense divisions of a dictionary entry), but it seems applicable to most work ondisambiguation i computational linguistics.In this paper, we will address the problem of sense discrimination as definedabove.
That is, we will not be concerned with the sense-labeling component of wordsense disambiguation.
Word sense discrimination is easier than full disambiguationsince we need only determine which occurrences have the same meaning and not whatthe meaning actually is.
Focusing solely on word sense discrimination also liberates usof a serious constraint common to other work on word sense disambiguation.
If senselabeling is part of the task, an outside source of knowledge is necessary to define thesenses.
Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996), thesauri (Yarowsky1992; Walker and Amsler 1986), bilingual corpora (Brown et al 1991; Church andGale 1991), or hand-labeled training sets (Hearst 1991; Leacock, Towell, and Voorhees1993; Niwa and Nitta 1994; Bruce and Wiebe 1994), providing information for sensedefinitions can be a considerable burden.What makes our approach unique is that, since we narrow the problem to sensediscrimination, we can dispense of an outside source of knowledge for defining senses.
* Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304Q 1998 Association for Computational LinguisticsComputational Linguistics Volume 24, Number 1We therefore call our approach automatic word sense discrimination, since we do notrequire manually constructed sources of knowledge.In many applications, word sense disambiguation must both discriminate and la-bel occurrences; for example, in order to find the correct ranslation of an ambiguousword in machine translation or the right pronunciation in a text-to-speech system.The application of interest o us is information access, i.e., making sense of and find-ing information in large text databases.
For many problems in information access,it is sufficient o solve the discrimination problem only.
In one study, we measureddocument-query similarity based on word senses rather than words and achieved aconsiderable improvement in ranking relevant documents ahead of nonrelevant doc-uments (Schi.itze and Pedersen 1995).
Since the measurement of similarity is a system-internal process, no reference to externally defined senses need be made.
Anotherpotentially beneficial application of word sense discrimination in information accessis the design of interfaces that take account of ambiguity.
If a user enters a query thatcontains an ambiguous word, a system capable of discrimination can give examples ofthe different senses of the word in the text database.
The user can then decide whichsense was intended and only documents with the intended sense would be retrieved.Again, a reference to external sense definitions is not required for this task.The algorithm we propose in this paper is context-group discrimination.
1 Context-group discrimination groups the occurrences of an ambiguous word into clusters,where clusters consist of contextually similar occurrences.
Words, contexts, and clus-ters are represented in a high-dimensional, real-valued vector space.
Context vectorscapture the information present in second-order co-occurrence.
Instead of forminga context representation from the words that the ambiguous word directly occurswith in a particular context (first-order co-occurrence), we form the context represen-tation from the words that these words in turn co-occur with in the training corpus.Second-order co-occurrence information is less sparse and more robust than first-orderinformation.In context-group discrimination, the context of each occurrence of the ambiguousword in the training corpus is represented as a context vector formed from second-order co-occurrence information.
The context vectors are then clustered into coherentgroups such that occurrences judged similar according to second-order co-occurrenceare assigned to the same cluster.
Clusters are represented by their centroids, the av-erage of their elements.
An occurrence in a test text is disambiguated by computingthe second-order representation f the relevant context, and assigning it to the clusterwhose centroid is closest o that representation.
Since the choice of representation influ-ences the formation of clusters, we will experiment with several representations in thispaper, some involving a dimensionality reduction using singular value decomposition(SVD).Context-group discrimination can be generalized to do a discrimination task thatgoes beyond the notion of sense that underlies many other contributions to the dis-ambiguation literature.
If the ambiguous word's occurrences are clustered into a largenumber n of clusters (e.g., n = 10), then the clusters can capture fine contextual distinc-tions.
Consider the example of space.
For a small number of clusters, only the senses"outer space" and "limited extent in one, two, or three dimensions" are separated.
Ifthe word's occurrences are clustered into more clusters, then finer distinctions uchas the one between "office space" and "exhibition space" are also discovered.
Notethat differences between sense entries in dictionaries are often similarly fine-grained.1 The basic idea of the algorithm was first described in Schi~tze (1992b).98Schiitze Automatic Word Sense DiscriminationI WORD ITRAINING TEXT \[VECTORS I WORD SPACE- - - -T :xxOx!
~x \[\] x i /  L.
Xx~."
'......x.
.. ....... ",TEST CONTEXT /Figure 1The basic design of context-group discrimination.
Contexts of the ambiguous word in thetraining set are mapped to context vectors in Word Space (upper dashed arrow) by summingthe vectors of the words in the context.
The context vectors are grouped into clusters (dottedlines) and represented by sense vectors, their centroids (squares).
A context of the ambiguousword ("test context") is disambiguated by mapping it to a context vector in Word Space(lower dashed arrow ending in circle).
The context is assigned to the sense with the closestsense vector (solid arrow).Even if the contextual distinctions captured by generalized context-group discrimina-tion do not line up perfectly with finer distinctions made in dictionaries, they still helpcharacterize the contextual meaning in which the ambiguous word is used in a partic-ular instance.
Such a characterization is useful for the information-access applicationsdescribed above, among others.The basic idea of context-group discrimination is to induce senses from contextualsimilarity.
There is some evidence that contextual similarity also plays a crucial role inhuman semantic ategorization.
Miller and Charles (1991) found evidence in severalexperiments hat humans determine the semantic similarity of words from the similar-ity of the contexts they are used in.
We hypothesize that, by extension, senses are alsobased on contextual similarity: a sense is a group of contextually similar occurrencesof a word.The following sections describe the disambiguation algorithm, our evaluation, andthe results of the algorithm for a test set drawn from the New York Times News Wire,and discuss the relevance of our approach in the context of other work on word sensedisambiguation.2.
Context-Group DiscriminationContext-group discrimination groups a set of contextually similar occurrences of anambiguous word into a cluster, which is then interpreted as a sense.
The particular im-plementation of this idea described here makes use of a high-dimensional, real-valuedvector space.
Context-group discrimination is a corpus-based method: all representa-tions are derived from a large text corpus.The basic design of context-group discrimination is shown in Figure 1.
Each oc-currence of the ambiguous word in the training set is mapped to a point in WordSpace (shown for one example occurrence: see dashed line from training text to WordSpace).
The mapping is based on word vectors that are looked up in Word Space(to be described below).
Once all training-text contexts have been mapped to WordSpace, the resulting point cloud is clustered into groups of points such that points areclose to each other in each group and that groups are as distant from each other as99Computational Linguistics Volume 24, Number 1possible.
The resulting clusters are delimited by dotted lines in the figure.
Each clusteris assumed to correspond to a sense of the ambiguous word (an assumption to beevaluated later).
The representative of each group is its centroid, depicted as a square.After training, a new occurrence of the ambiguous word (labeled "test context"in the figure) is disambiguated by mapping its context to Word Space (see lowerdashed line; the context's point is depicted as a circle).
The context is then assignedto the context group whose centroid is closest (solid arrow).
Finally, the context iscategorized as being a use of the sense corresponding to this context group.There are three types of entities that we need to represent: words, contexts, andsenses.
They are represented as word vectors, context vectors, and sense vectors, re-spectively.
Word vectors are derived from neighbors in the corpus, context vectors arederived from word vectors, and sense vectors are derived by way of clustering fromthe distribution of context vectors.The representational medium of a vector space was chosen because of its wideacceptance in information retrieval (IR) (see, e.g., Salton and McGill \[1983\]).
The vector-space model is arguably the most common framework in IR.
Systems based on it haveranked among the best in many evaluations of IR performance (Harman 1993).
Thesuccess of the vector-space model motivates us to use it for the representation f words.We represent words in a space in which each dimension corresponds to a word, justas documents and queries are commonly represented in this space in IR.Another approach to computing word similarity is the representation f words ina document space in which each dimension corresponds to a document (Lesk 1969;Salton 1971; Qiu and Frei 1993).
There are fewer occurrence-in-document than word-co-occurrence events, so these word representations tend to be more sparse and, ar-guably, less informative than word-based representations.
Word vectors have also beenbased on hand-encoded features (Gallant 1991) and dictionaries (Sparck-Jones 1986;Wilks et al 1990).
Corpus-based methods like the one proposed here have the ad-vantage that no manual labor is required and that a possible mismatch between ageneral dictionary and a specialized text (e.g., on chemistry) is avoided.
Finally, wordsimilarity can be computed from structural features like head-modifier relationships(Grefenstette 1994b; Ruge 1992; Dagan, Marcus, and Markovitch 1993; Pereira, Tishby,and Lee 1993; Dagan, Pereira, and Lee 1994).
Like document-based representations,structure-based representations are sparser than those based on co-occurrence.
It isdebatable whether structural features are more informative than associational features(Grefenstette 1992, 1996) or not (Schtitze and Pedersen 1997).
Approaches to wordrepresentation closely related to ours were proposed by Niwa and Nitta (1994) andBurgess and Lund (1997).
Instead of co-occurrence counts, vector entries are mutualinformation scores between the word that is to be represented and the dimensionwords, in Niwa and Nitta's approach.The algorithms for vector derivation and sense discrimination are described inwhat follows.2.1 Word VectorsA vector for word w is derived from the close neighbors of w in the corpus.
Closeneighbors are all words that co-occur with w in a sentence or a larger context.
In thesimplest case, the vector has an entry for each word that occurs in the corpus.
Theentry for word v in the vector for w records the number of times that word v occursclose to w in the corpus.
It is this representational vector space that we refer to asWOrd Space.Figure 2 gives a schematic example of two words being represented in a two-dimensional space.
The representation is based on the co-occurrence ounts of a hypo-100Schiitze Automatic Word Sense DiscriminationTable 1Co-occurrence counts for four words in ahypothetical corpus.
The words legal andclothes are interpreted as dimensions inFigure 2, judge and robe as vectors.VectorDimension judge robelegal 300 133clothes 75 200LEGAL300 JUDGE /133 f /  ROBEI I75 200 CLOTHESFigure 2The derivation of word vectors, judge and robe are represented as word vectors in atwo-dimensional space with the dimensions 'legal' and 'clothes.'
Co-occurrence data are fromTable 1.thetical corpus in Table 1.
The word judge has a value of 300 on the dimension "legal"because judge and legal co-occur 300 times with each other (see below for which wordsare selected as dimensions; a word can be a dimension of Word Space and representedas a word vector in Word Space at the same time).This vector representation captures the typical topic or subject matter of a word.For example, words like judge and law are closer to the "legal" dimension; wordslike robe and tailor are closer to the "clothes" dimension.
By looking at the amount  ofoverlap between two vectors, one can roughly determine how closely they are relatedsemantically.
This is because related meanings are often expressed by similar sets ofwords.
Semantically related words will therefore co-occur with similar neighbors andtheir vectors will have considerable overlap.This similarity can be measured by the cosine between two vectors.
The cosine isequivalent o the normalized correlation coefficient:corr( fi, ~ ) = ~iN=l ViWiwhere ff and ~ are vectors and N is the dimension of the vector space.
The value ofthe cosine is higher, the more overlap there is between the neighbors of the two wordswhose vectors are compared.
If two words occur with exactly the same neighbors101Computational Linguistics Volume 24, Number 1(perfect overlap), then the value of the cosine is 1.0.
If there is no overlap at all, thenthe value of the cosine is 0.0.
The cosine can therefore be used as a rough measure ofsemantic relatedness between words.What words should serve as the dimensions of Word Space?
We will experimentwith two strategies: a global and a local one.
The local strategy focuses on the contextsof the ambiguous words and ignores the rest of the corpus.
The global strategy is toselect he n most frequent words of the corpus as features and use them regardless ofthe word that is to be disambiguated.
(See Karov and Edelman \[1996\] for a differentapproach that selects features according to a combination ofglobal frequency and localsalience.
)For local selection, we can also use a frequency cutoff.
As an alternative, we willtest selection according to a X 2 test.
For the frequency-based selection criterion, theneighbors of the ambiguous word in the corpus are counted.
A neighbor is any wordthat occurs at a distance of at most 25 words from the ambiguous word (that is, in a 50-word window centered on the ambiguous word).
The 1,000 most frequent neighborsare chosen as the dimensions of the space.
For the x2-based criterion, a x2-measureof dependence is applied to a contingency table containing the number of contexts ofthe ambiguous word in which the candidate word occurs (N++) and does not occur(N+_), and the number of contexts without an occurrence of the ambiguous word inwhich the candidate word occurs (N_+) and does not occur (N__).X2 = N(N++N__ - N+_N_+) 2(N++ + N+_)(N_+ + N__)(N++ + N_+)(N+_ + N__)The underlying assumption i  using the x2-test is that candidate words whose occur-rence depends on whether the ambiguous word occurs will be indicative of one of thesenses of the ambiguous word and hence useful for disambiguation.
2After 1,000 words have been selected in local selection, word vectors are formedby collecting a 1,000-by-I, 000 matrix C, such that element cq records the number oftimes that words i and j co-occur in a window of size k. Column n (or, equivalently,row n) of matrix C represents word n. Note that C is symmetric since the words thatare represented asword vectors are also those that form the dimensions of the 1,000-dimensional space.
We chose a window size of k = 50 because no improvement ofdiscrimination performance was found in Schfitze (1997) for k > 50.For global selection, we choose the 20,000 most frequent words as features andthe 2,000 most frequent words as dimensions of Word Space.
A global 20, 000-by-2, 000co-occurrence matrix is derived from the corpus.Association data were extracted from the training set consisting of 17 months ofthe New York Times News Service, June 1989 through October 1990.
The size of thisset is about 435 megabytes and 60.5 million words.
Two months (November 1990 andMay 1989; 46 megabytes, 5.4 million words) were set aside as a test set.2.2 Context VectorsThe representation forwords derived above conflates enses.
For example, both sensesof the word suit ('lawsuit' and 'garment') are summed in its word vector, which willtherefore be positioned somewhere between the 'legal' and 'clothes' dimensions inFigure 2.
We need to go back to individual contexts in the corpus to acquire informationabout sense distinctions.
Contexts are represented as context vectors in Word Space.2 Candidate words are selected after a list of 930 stopwords has been removed.
This stop list was basedon the one used in the Text Data Base system (Cutting, Pedersen, and Halvorsen 1991).102Sch~tze Automatic Word Sense DiscriminationLEGALCENTROIDLAW / ~JUDGE/iJ~ISTATUTE/ I I /// /#SUIT/ / /I/~//I/1/CLOTHESFigure 3The derivation of context vectors.
A context vector is computed as the centroid of the wordsoccurring in the context.
The words in this example context are law, judge, statute, and suit.A context vector is the centroid (or sum) of the vectors of the words occurring in thecontext.
Figure 3 shows the context vector of an example context of suit containing thewords law, judge, statute, and suit.
Note that the context vector is closer to the "legal'than to the 'clothes' dimension, thus capturing that the context is a 'legal' use of suit.
(The true sum of the four vectors is longer than shown.
Since all correlation coefficientsare normalized, the length of a vector does not play a role in the computations.
)The centroid "averages" the direction of a set of vectors.
If many of the words ina context have a strong component for one of the topics (like 'legal' in Figure 3), thenthe average of the vectors, the context vector, will also have a strong component forthe topic.
Conversely, if only one or two words represent a particular topic, then thecontext vector will be weak on this component.
The context vector hence representsthe strength of different opical or semantic omponents in a context.In the computation of the context vector, we will weight a word vector accordingto its discriminating potential.
A rough measure of how well word wi discriminatesbetween different opics is the log inverse document frequency used in informationretrieval (Salton and Buckley 1990):ai = l ?g /~ /where ni is the number of documents that wi occurs in and N is the total numberof documents.
Poor discriminators of topics are words such as idea or help that arerelatively uniformly distributed and therefore have a high document frequency.
Goodcontent discriminators like automobile or China have a bursty distribution (they haveseveral occurrences in a short interval if they occur at all \[Church and Gale 1995\]),and therefore a low document frequency relative to their absolute frequency.Other algorithms for computing context vectors have been proposed by Wilks et al(1990) (based on dictionary entries), Gallant (1991) (based on hand-encoded semanticfeatures), Grefenstette (1994b) (based on light parsing), and Niwa and Nitta (1994) (acomparison of dictionary-based and corpus-based context vectors).103Computational Linguistics Volume 24, Number 1LEGALSENSE 1Cl..,,,7 ,s ~-  SENSECLOTHESFigure 4The derivation of sense vectors.
Sense vectors are derived by clustering the context vectors ofan ambiguous word (here, cl, c2, c3, c4, c5, c6, c7, and cs), and computing sense vectors as thecentroids of the resulting clusters.
The vectors SENSE 1 and SENSE 2 are the sense vectors ofclusters {cl, c2, c3, c4} and {cs, c6, c7, cs}, respectively.2.3 Sense VectorsSense representations are computed as groups of similar contexts.
All contexts of theambiguous word are collected from the corpus.
For each context, a context vector iscomputed.
This set of context vectors is then clustered into a predetermined number ofcoherent clusters or context groups using Buckshot (Cutting et al 1992), a combinationof the EM algorithm and agglomerative clustering.
The representation of a sense issimply the centroid of its cluster.
It marks the portion of the multidimensional spacethat is occupied by the cluster.We chose the EM algorithm for clustering since it is guaranteed to converge on alocally optimal solution of the clustering problem.
In our case, the solution is optimalin that the sum of the squared distances between context vectors and their centroidswill be minimal.
In other words, the centroids are optimal representatives for thecontext vectors in their cluster.One problem with the EM algorithm is that it finds a solution that is only locallyoptimal.
It is therefore important o find a good starting point since a bad startingpoint will lead to a local minimum that is not globally optimal.
Some experimentalevidence given below shows that cluster quality varies considerably depending onthe initial parameters.
In order to find a good starting point, we use group-averageagglomerative clustering (GAAC) on a sample of context vectors.
For each of the 2,000clustering experiments described below, we first choose a random sample of 50.
Thissize is roughly equal to v'~, the number of context vectors to be clustered.
SinceGAAC is of time complexity O(n2), this guarantees overall linear time complexityof the clustering procedure.
If the training set has more than 2,000 instances of theambiguous word, 2,000 context vectors are selected randomly.
The centroids of theresulting clusters are then the parameters for the first iteration of EM.
We computefive iterations of the EM algorithm for all experiments since in most cases only a few,if any, context vectors were reassigned in the fifth iteration.Both the EM algorithm and group-average agglomerative clustering are describedin more detail in the appendix.104Schfitze Automatic Word Sense DiscriminationAn example is shown in Figure 4.
The clustering step has grouped context vectorscl, c2, c3, and c4 in the first group and c5, c6, c7, and c8 in the second group.
The sensevector of the first group is the centroid labeled SENSE 1, the sense vector of the secondgroup the centroid labeled SENSE 2.The result of clustering depends on the representation of context vectors.
Forthis reason, we also investigate a transformation of the multidimensional space viaa singular value decomposition (SVD) (Golub and van Loan 1989).
SVD is a form ofdimensionality reduction that finds the major axes of variation in Word Space.
Contextvectors can then be represented by their values on these principal dimensions.
Themotivation for applying SVD here is much the same as the use of Latent SemanticIndexing (LSI) in information retrieval (Deerwester et al 1990).
LSI abstracts awayfrom the surface word-based representation and detects underlying features.
Whensimilarity is computed on these features (via cosine between SVD-reduced contextvectors), contextual similarity can be, potentially, better measured than via cosine be-tween unreduced context vectors.
The appendix defines SVD and gives an examplematrix decomposition.In this paper, the word vectors will be reduced to 100 dimensions.
The experimentsreported in Schfitze (1992b, 1997) give evidence that reduction to this dimensionalitydoes not decrease accuracy of sense discrimination.
Space requirements for contextvectors are reduced to about 1/10 and 1/20 for a 1,000-dimensional nd a 2,000-dimensional Word Space, respectively.
Although most word vectors are sparse, contextvectors are dense, since they are the sum of many word vectors.
Time efficiency isincreased on the same order of magnitude when the correlation of context vectorsand sense vectors is computed.
The computation of the SVD's in this paper took froma few minutes per word for the local feature set to about three hours for the globalfeature set.2.4 Application of Context-Group DiscriminationContext-group discrimination uses word vectors and sense vectors as follows to dis-criminate occurrences of the ambiguous word.
For an occurrence t of the ambiguousword v:?
Map t into its corresponding context vector ~ in Word Space using thevectors of the words in t's context (the lower dashed line in Figure 1).?
Retrieve all sense vectors ~j of v (the two points marked as squares in thefigure).?
Assign t to the sense j whose sense vector ~j is closest o ~ (assignmentshown as a solid arrow).This algorithm selects the context group whose sense vector is closest to the contextvector of the occurrence of the word that is to be disambiguated.
Context vectorsand sense vectors capture semantic haracteristics of the corresponding context andsense, respectively.
Consequently, the sense vector that is closest o the context vectorhas the best semantic match with the context.
Therefore, context-group discriminationcategorizes the occurrence as belonging to that sense.3.
EvaluationWe test context-group discrimination on the 10 natural ambiguous words that formedthe test set in Schfitze (1992b) and on 10 artificial ambiguous words.
Table 2 glossesthe major senses of the 20 words.105Computational Linguistics Volume 24, Number 1Table 2Number of occurrences oftest words in training and test set, percent rare senses in test set,baseline performance (all occurrences a signed to most frequent sense), and the two mainsenses of each of the 20 artificial and natural ambiguous words used in the experiment.Word Training Test.
Rare Senses Baseline Frequent Senseswide range/consulting firm 1,422 149 0% 62%heart disease/reserve board 1,197 115 0% 54%urban development/cease fire 1,582 101 0% 50%drug administration /fernando valley 1,465 122 0%wide rangeconsulting firmheart diseasereserve boardurban developmentcease fire52% drug administrationfernando valleyeconomic development /right field 1,030 88 0% 68%national park/judiciary committee 1,279 122 0% 70%japanese companies /city hall 1,569 208 0% 58%drug dealers /paine webber 1,183 104 0% 55%league baseball/square feet 1,097 143 0% 66%pete rose/nuclear power 1,245 103 0% 52%capital/s 13,015 200 2% 64%interest/s 21,374 200 4% 58%motion/s 2,705 200 0% 55%plant/s 12,833 200 0% 54%economicdevelopmentright fieldnational parkjudiciary committeejapanese companiescity halldrug dealerspaine webberleague baseballsquare feetpete rosenuclear powerstock of goodsseat of governmenta feeling of specialattentiona charge forborrowed moneymovementproposal for actiona factoryliving being106Schiitze Automatic Word Sense DiscriminationTable 2Continued.Word Training Test Rare Senses Baseline Frequent Sensesruling 5,482 200 3.5% 60% an authoritative decisionto exert control, orinfluencespace 9,136 200 0% 56% area, volumeouter spacesuit/s 7,467 200 12.5% 57% an action or processin a courta set of garmentstank/s 3,909 200 4.5% 90% a combat vehiclea receptacle for liquidstrain/s 4,271 200 1.5% 74% a line of railroad carsto teachvessel/s 1,618 144 13.9% 69% a ship or planea tube or canal (as an artery)Artificial ambiguous words or pseudowords are a convenient means of testingdisambiguation algorithms (Schtitze 1992a; Gale, Church, and Yarowsky 1992).
It istime-consuming to hand-label a large number of instances of an ambiguous word forevaluating the performance of a disambiguation algorithm.
Pseudowords circumventthis need: Two or more words, e.g., banana and door, are conflated into a new type:banana~door.
All occurrences of either word in the corpus are then replaced by the newtype.
It is easy to evaluate disambiguation performance for pseudowords ince onecan go back to the original text to decide whether a correct decision was made.To create the pseudowords shown in Table 2, all word pairs were extracted fromthe corpus, i.e., all pairs of words that occurred adjacent o each other in the corpus ina particular order.
All numbers were discarded, since numbers do not seem to involvesense ambiguity.
Pseudowords were then created by randomly drawing two pairs fromthose that had a frequency between 500 and 1,000 in the corpus.
Pseudowords weregenerated from pairs rather than simple words because pairs are less likely than wordsto be ambiguous themselves.
Pair-based pseudowords are therefore good examples ofambiguous words with two clearly distinct senses.Table 2 indicates how often the ambiguous word occurred in the training and testsets, how many instances were instances of rare senses, and the baseline performancethat is achieved by assigning all occurrences to the most frequent sense.
In the eval-uation given here, only senses that account for at least 15% of the occurrences of theambiguous word are taken into account.
Rare senses are those that account for fewerthan 15% of the occurrences.
The words in Table 2 each had two frequent senses.
Thefrequency of rare senses ranges from 0% to 13.9%, with an average of 2.1%.
Rare sensesare not eliminated from the training set.The training and test sets were taken from the New York Times News Service asdescribed above (training set: June 1989-October 1990; test set: November 1990, May1989).
If a word had more than 200 occurrences in the test set, then only the first 200occurrences were included in the evaluation.The labeling of words in the test corpus was performed by the author.
The distinc-107Computational Linguistics Volume 24, Number 1tions between the senses in Table 2 are intuitively clear.
For example, the probabilityof a context in which suit could at the same time refer to a set of garments and anaction in court is very low.
Consequently, there were fewer than five instances wherethe appropriate sense was not obvious from the immediate context.
In these cases, thesense that seemed more plausible to the author was assigned.It is important o evaluate on a test set that is separate from the training set.Context-group discrimination is based on the distribution of context vectors in thetraining set.
The distribution in the training set is often a bad model for the distributionin the test set.
In practice, the intended text of application will be from a time periodnot covered in the training set (for example, newswire text from after the date oftraining).
Word distributions can change considerably over time.
The test set wastherefore constructed to be from a time period different from the time period of thetraining set.
This is also the reason that we do not do cross-validation.
Cross-validationrespecting the constraint hat test and training sets be from different time periodswould have required a test set several times larger than the one that was available.Clustering and evaluating on the same set is also problematic because of samplingvariation.
Consider the following example.
We have a set of three context vectorsC : {C 1 : (1), c2 = (2), C 3 : (3)} in a one-dimensional space.
Contexts 1 and 2 are usesof sense 1, context 3 is a use of sense 2.
If C is used as both training and evaluation set,then average performance is 83% (with probability 0.5, we get centroids 1.5 and 3 and100% accuracy, with probability 0.5, we get centroids 1 and 2.5 and 67% accuracy).
Ifwe split C into a training set T of size 2 and a test set E of size 1, we get an averageperformance of 67% (100% for E = {cl}, 50% for E = {c2}, 50% for E = {?3}) ,  whichis lower than 83%.
This example shows that conflating training and test set can resultin artificially high performance.An advantage of context-group discrimination is that the granularity of sensedistinctions is an adjustable parameter of the algorithm.
Experiments run directly forthe senses in Table 2 will test the algorithm's ability to discriminate coarse sensedistinctions.
To test performance for fine-grained sense distinctions (e.g., 'office space'vs.
'exhibition space'), we will run two experiments, one that evaluates performance forclustering the context vectors of a word into ten clusters and an information retrievalexperiment in which the number of clusters is also large for sufficiently frequent words.The goal of the 10-cluster experiments i  to induce more fine-grained sense distinc-tions than in the 2-cluster experiments.
However, it is harder to determine the groundtruth for fine sense distinctions.
When it comes to fine distinctions, a large number ofoccurrences are indeterminate or compatible with several of the more finely individ-uated senses (cf.
Kilgarriff \[1993\]).For this reason, experiments with a large number of clusters were evaluated usingtwo indirect measures.
The first measure is accuracy for two-way discriminations, i.e.,the degree to which each of the ten clusters contained only one of the two "coarse"senses.
This evaluation is indirect because a cluster that contains, say, only 'limitedextent in one, two, or three dimensions' uses of space would be deemed 100% correct,yet it could be randomly mixed as far as fine sense distinctions are concerned (e.g.,'office space' vs. 'exhibition space').
The author inspected the data and found goodseparation of fine-grained senses in the 10-cluster experiments to the extent that theevaluation measure indicated good performance on the two-way discrimination task.However, because of the above-mentioned subjectivity of judgements for fine sensedistinctions, this is hard to quantify.Results from a second evaluation on an information retrieval task will be presentedin Section 4.2 below.
We will show that sense-based information retrieval (in which therelevance of documents to a query is determined using context-group discrimination)108Schiitze Automatic Word Sense Discriminationimproves the performance of an IR system considerably.
Since the success of sense-based retrieval depends on the accuracy of context-group discrimination, we can inferthat the algorithm reliably assigns ambiguous instances to induced senses even in thefine-grained case.4.
Experiments4.1 Word Sense DiscriminationTable 3 shows experimental results for context-group discrimination.
There were fourconditions that were varied in the experiments (as described in Section 2):?
local vs. global feature selection?
feature selection according to frequency vs .
X 2?
term representations vs. SVD-reduced representations?
number of clusters (2 vs. 10)For local feature selection, the other three parameters are varied systematically (the firsteight columns of Table 3).
For global feature selection, selection according to X 2 is notpossible, since the X 2 test presupposes an event (like the occurrence of an ambiguousword) that the occurrence of candidate words depends on.
There is no such event forglobal feature selection.
A larger number of dimensions (2,000) is used for the globalvariant of the algorithm in order to get coverage of a large range of topics that mightbe relevant for disambiguation.
We therefore apply SVD in the global feature selectioncase.
Even if word vectors are sparse, context vectors are usually not.
Clustering 2,000-dimensional vectors is computationally expensive, so that we only ran experimentswith SVD-reduced vectors for the global variant.Ten experiments with different randomly chosen initial parameters were run foreach of the 200 combinations of the different levels of Word, Representation, andClustering.
The mean percentage correctness and the standard eviation for each suchset of 10 experiments i shown in the cells of Table 3.
We give mean and deviationof the percentage of correctly labeled occurrences of all instances in the training set("total" = "t.'), of the instances of sense 1 ("$1") and of the instances of sense 2 ("$2").The bottom row of the table gives averages of the total percentage correct numbersover the 20 words covered.
The rightmost column gives averages of the means overthe 10 experiments.We analyzed the results in Table 3 via analysis of variance (ANOVA, see, forexample, Ott \[1992\]).
An ANOVA was performed for a 20 x 5 x 2 design with 10replicates.
The factors were Word, Representation (local, frequency-based, terms; local,frequency-based, SVD; local, Xa-based, terms; local, x2-based, SVD; global, frequency-based, SVD), and Clustering (coarse = 2 clusters, fine = 10 clusters).
Percentages weretransformed using the functionf(X) = 2 x sin -1 (v/X) as recommended by Winer (1971).The transformed percentages have a distribution that is close to a normal distributionas required for the application of ANOVA.We found that the effects of all three factors and all interactions was significant atthe 0.001 level.
These effects are discussed in what follows.Factor Word.
In general, performance for pseudowords i  better than for natural words.This can be explained by the fact that pseudowords have two focussed senses--the twoword pairs they are composed of.
In contrast, some of the senses of natural ambiguous109Computational Linguistics Volume 24, Number 1Table 3Results of disambiguation experiments.
Rows give total accuracy for each word ("t.') as wellas accuracy for the two senses separately ("$1", "$2").
The average in the bottom row is anaverage over total ("t.') accuracy numbers only.
Columns describe experimental conditionsand the mean ("\]~") and standard deviation ("a") of 10 replications of each experiment.
Therightmost column contains an average over the mean values of the 10 experiments.Pseudowords are abbreviated to the first words of pairs.Local Globalwide~consul.
$1$2 55 16 100 0 69 31 92 9 74 25 92 13 69 24 82 10 89 6 94 4t.
51 4 62 0 60 4 66 3 56 6 64 3 65 8 66 2 87 3 87 3heart~reserve $1 66 0 78 11 100 0 99 4.
72 0 75 12 100 0 98 2 100 0 100 0$2 100 0 90 7 100 0 100 0 100 0 94 5 98 0 100 1 100 0 100 0t.
84 0 :85  2 100 0 99 287  0 85 3 99 0 99 1 100 0 100 0urban~cease $1 86 1 87 2 96 0 97 191  4 90 8 98 0 98 1 100 0 98 2~rugffern..ocon./rightnat./jud.iap./city $171rug/paine $1!eague/square $1pete~nuclear $1:apital $1interest $1,wtion $1~lant $1ruling $1;pace $1~uit S1~ank $1~rain $1vessel $1AverageX 2 Frequency FrequencyTerms I SVD Terms I SVD SVD2 10 2 10 2 10 2 10 2 1045 16 0 0 45 47 22 22 25 26 19 30 59 37 39 17 84 2 76 8 41.481.666.488.898.293.894.1$2 78 l J 70 7 100 0 100 1 73 24 80 11 100 0 96 5 100 0 100 0 89.7t.
82 0i 79 3 98 0 99 1 I 82 10 85 3 99 0 97 2 100 0 99 1 92.0S1 89 l i 87 7 98 0 100 1 I 94 4 88 5 98 0 95 1 100 0 100 0 94.9$2 78 1 77 12 95 0 100 1 60 35 90 7 59 8 96 2 100 0 100 1 85.5t.
84 I 82 3 97 0 100 0 78 15 89 2 80 4 96 1 100 0 100 0 90.6$1 72 2 89 6 92 1 95 1 92 0 87 5 98 0 98 3 100 0 100 0 92.3$2 89 0 67 13 96 0 96 2 87 2 91 5 96 0 97 2 100 0 100 0 91.9t.
78 1 82 1 93 1 95 1 90 1 88 2 98 0 97 1 100 0 100 0 92.1S1 91 1 96 3 98 0 97 0 99 0 99 0 98 01 97 1 100 0 100 0 97.5$2 73 0 53 14 100 0 100 1 70 0 61 9 92 0i 96 4 100 0 98 2 84.3t.
85 1 83 3 98 0 98 0 90 0 87 3 96 0i 97 1 100 0 99 1 93.384 18 90 7 96 1 95 1 94 2 91 4 97 2 93 2 99 0 99 1 93.8i$2 56 10 63 15 71 23 87 4 66 17 71 10 88 5 90 5 99 0 99 1 79.0t.
73 12 79 3 86 9 92 1 82 6 83 2 93 1 92 1 99 0 99 0 87.868 6 76 9 86 1 81 9 70 18 81 14 95 0 85 4 100 0 97 3 83.9$2 86 13 86 8 100 0 99 1 68 23 87 14 100 0 98 3 100 0 100 0 92.4t.
76 9 80 2 93 0 89 5 69 19 83 3 97 0 91 2 100 0 98 2 87.654 8 77 8 66 41 96 3 32 31 77 10 56 32 90 4 100 0 100 1 74.-----8$2 60 20 94 3 100 0 99 1 91 18 94 5 100 0 96 4 100 0 99 2 93.3t.
58 16 88 1 88 14 98 2 71 13 88 1 85 11 94 3 100 0 99 l i  86.991 0 78 10 94 1 98 2 72 21 90 10 86 19 95 6 100 0 99 1 90.3$2 78 0 80 8 94 0 91 2 96 1 81 13 88 20 91 7 100 0 99 1 I 89.8t.
84 0 79 2 94 0 95 2 83 11 86 3 87 19 93 4 100 0 99 1 90.088 16 97 3 91 4 96 2 91 3 97 3 93 1 93 2 92 1 93 1 93.1$2 27 23 36 11 23 34 87 7 36 34 57 9 80 27 88 6 96 1 89 5 61.9t.
66 7 75 3 66 13 93 2 71 13 82 2 88 10 91 1 94 0 91 1; 81.782 18 77 8 95 1 86 5 96 0 93 3 94 1 91 4 96 0 89 3 89.9$2 43 37 87 4 90 6 96 2 83 1 85 3 71 35 91 4 88 1 93 31 82.7t.
66 14 81 4 93 2 90 2 90 0 90 1 84 15 91 2 93 0 91 l i  86.957 14 72 6 58 1 84 1 61 17 88 6 90 15 93 4 85 1 91 5 I 77.-----------9$2 60 15 70 10 97 0 91 8 58 20 63 16 51 24 77 7 88 13 71 151 72.6t.
58 10 71 3 76 1 87 3 59 12 77 4 73 12 86 2 86 5 82 5 i 75.5i73 20 0 0 92 4 " 0 0 91 16 0 0 54 46 2 5 70 37 0 0'  38.-----2$2 47 12 100 0 37 5 100 0 41 30 100 0 59 36 100 0 70 26 100 0 75.4t.
59 8 54 0 63 4 54 0 64 11 54 0 56 7 55 2 70 13 54 0 58.375 1 61 13 84 2 71 14 81 1 65 15 79 7 79 13 85 0 82 3 76.2$2 86 1 90 4 93 1 96 3 87 1 93 4 93 5 95 2 95 0 95 1 92.3t.
82 0 78 3 90 1 86 4 84 0 82 4 88 1 89 4 91 0 90 1 86.010 25 48 30 0 0 48 22 15 25 38 24 16 25 51 15 8 25 54 16 28.8$2 87 7 91 7 96 0 95 3 97 1 96 2 96 2 96 2 94 10 93 3 94.1t.
53 7 72 9 54 0 74 8 61 11 71 10 60 12 76 6 56 5 75 6 65.283 1 77 5 80 2 85 6 81 2 84 7 94 2 88 8 95 0 83 6 85.0$2 80 0 i 84 4 93 0 94 2 92 2 88 6 86 29 97 2 96 0 97 2 90.7t.
82 1 I 80 2 85 1 89 3 86 1 86 2 91 12\] 92 4 95 0 89 3 87.529 91 7 6 80 8 32 13 88 5 12 14 86 29i~ 31 22 92 3 28 19 48.5$2 94 15 100 0 92 95 1 100 0 87 5\] 99 2 84 1 99 2 94.9 4 99 0t.
87 13 I 90 1 90 3 92 1 95 1 91 1 87 2i 92 1 85 1 92 2 90.160 21 100 0 74 16 100 0 89 20 100 0 95 81100 1 79 19 100 0 89.7$2 40 21  0 0 12 20 0 0 18 29 0 0 8 21!
1 3 55 31 0 0 13.4t.
55 10' 74 0 58 7 74 0 69 11 74 0 72 1 '  74 0 73 8 74 0 69.784 18 86 14 100 0 99 1 85 30 90 7 20 42 94 2 30 48 79 5 76.7$2 76 14 84 9 100 0 100 0 89 3 92 5 79 17 100 0 81 9 100 0 90.1t.
79 15\] 85 2 100 0 100 0 88 11 91 2 61 14\] 98 1 65 13 93 1 .. 86 .Oi\[ 72.1 I 77.9 \[ 84.1 i 88.5 i 77.8 i 81.8 i 82.9 \[ 88.3 i 89.7 i 90.6 I\]110Schfitze Automatic Word Sense DiscriminationTable 4The Tukey W test shows significantly different performance for the fiverepresentations.
Proportions are transformed using f iX)  = 2 x sin -1 (v/X).
Therightmost column contains the accuracy A in percent that would correspond tothe average value Y in the second column (i.e., f(A) = Y).
Significant differencefor a = 0.01:0.034Average of Difference CorrespondingLevel 2 x s in- l (V 'X)  from Closest Accuracylocal, )/2, terms 2.11 0.13 76%local, frequency, terms 2.24 0.13 81%local, frequency, SVD 2.44 0.06 88%local, X 2, SVD 2.50 0.06 90%global, frequency, SVD 2.66 0.16 94%words (for example, space and interest) are composed of many different subsenses thatare hard to identify for both people and computers.The only pseudoword with poor performance is wide range/consultingfirm.
Thisis an illustrative xample of a weakness of the particular implementation of context-group discrimination chosen here.
Since we only rely on topical information, a wordcomposed of a nontopical sense, like wide range, that can occur in almost any subjectarea is disambiguated poorly.
The 'area, volume' sense of space and the 'teaching' senseof train are similarly topically amorphous and therefore hard if only topical informa-tion is considered.
The poor performance for 'plant' in the 10-cluster experiments iprobably due to the way training-set clusters were assigned to senses.
The trainingset was clustered into 20 clusters and each cluster was given a sense label.
This proce-dure introduces many misclassifications of individual instances in the training set.
Incontrast, a performance of 92% was achieved in Schiitze (1992b) by hand-categorizingthe training set, instance by instance.Note that for some experimental conditions and for some words, performanceof two-group clustering is below baseline.
In a completely unsupervised setting, wehave to make the assumption that the two induced clusters correspond to two differentsenses.
In the worst case, we will get, two clusters with identical proportions of thetwo senses and an accuracy of 50%, below the baseline of assigning all occurrencesto a sense that occurs in more than 50% of all cases.
For example, for vessel the worstcase would be two clusters each with 69% 'ship' instances and 31% 'tube' instances.Overall accuracy would be 0.5 x .69 + 0.5 x .31 = 0.5.
It could be argued that the truebaseline for unsupervised two-group clustering is 50%, not the proportion of the mostfrequent sense.Factor Representation.
A Tukey W test (Ott 1992) was performed to evaluate the factorRepresentation.
The Tukey W test determines the least significant difference betweensample means.
That is, it yields a threshold such that if two levels of a factor differby more than the threshold, then they are significantly different.
For the factor Rep-resentation, in our case, this least significant difference is 0.034 for a = 0.01.
Table 4shows that all differences are significant.
This is evidence that SVD representationsperform better than term representations and that global representations perform bet-ter than local representations.
The advantage of SVD representations is partly due tothe use of a normality assumption in clustering.
This is a poor approximation for term111Computational Linguistics Volume 24, Number 1Table 5Occurrence of selected term features in the test set.
The table shows number of wordsoccurring in the test set (averaged over the 20 ambiguous words); number of words occurringper context (averaged over contexts); proportion of words from one representation ccurringin another (averaged first over contexts, then over ambiguous words; e.g., on average 91% ofX2-selected terms were also in the set selected by local frequency); average number of contextsthat a selected term occurred in (e.g., on average a xa-selected term occurred in 8.7 contexts ofthe artificial ambiguous words, averaged over the words in a context).~2 Local Frequency Global FrequencyWords Occurring in Test Set 283.0 571.2 489.6Words per Context 6.1 11.1 9.2Term OverlapX 2 100% 91% 53%local frequency 51% 100% 68%global frequency 34% 78% 100%Average Frequency of termsartificial words 8.7 6.7 16.7natural words 39.5 22.4 17.5representations, but is more accurate for SVD-reduced representations.Why do globally selected features perform better?
Table 5 presents data on theoccurrence of selected terms in the test set that are relevant o this question.
Note firstthat locally selected features eem to do better than globally selected ones on severalmeasures.
More locally selected features occur in the test set ("words occurring in testset": 571.2 vs. 489.6), more local features occur in the individual contexts ("words percontext": 11.1 vs. 9.2), and more global features are also local features than vice versa(on a per-context basis, 78% of global features are also local features, but only 68%of local features are also global features), suggesting that local features capture moreinformation than global features.
The first two measures also show that X2-selectedfeatures uffer from sparseness.
Both the total number of features that occur in thetraining set and the number of words per context are small.
This evidence xplainswhy SVD representations that address parseness do better than term representationsfor X 2.To explain the difference in performance between local and global frequency fea-tures, we have to break down average accuracy according to artificial and naturalambiguous words.
Average accuracy for artificial ambiguous words is 89.9% (2 clus-ters) and 92.2% (10 clusters) for local features and 98.6% (2 clusters) and 98.0% (10clusters) for global features.
Average accuracy for natural ambiguous words is 76.0%(2 clusters) and 84.4% (10 clusters) for local features and 80.8% (2 clusters) and 83.1%(10 clusters) for global features.
These data show a clear split.
Performance of local andglobal features is comparable for natural ambiguous words.
Global features performclearly better for artificial ambiguous words.The last two rows of Table 5 explain this difference in behavior.
The numbers corre-spond to the average number of contexts that the selected features occur in (averagedfirst over the words in a context, then over contexts; e.g., a context with three selectedterms occurring in 10, 3, and 15 contexts of the ambiguous word in the training setwould have an average number of contexts of (10+3+15)/3 = 9.3).
These averages are11.2Schi~tze Automatic Word Sense Discriminationsmall for X 2 and local frequency in the case of artificial ambiguous words.
Clusteringcan only work well if contexts have enough elements in common so that similaritycan be determined robustly.
Apparently, there were too few elements in common forX 2 and local frequency in the case of artificial ambiguous word (and the patterns wereso sparse that even SVD was not an effective remedy).The problem is that artificial ambiguous words are much less frequent in thetraining set than natural ambiguous words (average frequencies of 1,306.9 vs. 8,231.0),so that reliable feature selection is harder for artificial ambiguous words.
With ampleinformation on natural ambiguous words available in the training set, features canbe selected that will occur densely in the test set.
The quality of feature selection forartificial ambiguous words was less successful due to smaller training set sizes.This analysis reiterates the importance of a clear separation of training and testsets.
Performance numbers will be artificially high if feature selection is done on bothtraining and test sets, avoiding the problems with feature coverage demonstrated inTable 5.Since global feature selection is simpler and as effective as local approaches, globalfeature selection is the preferred implementation of context-group discrimination inthe general case.
Note, however, that different words may have different optimal repre-sentations.
For example, local features work best for vessel.
There are similar individualdifferences for frequency vs. x2-based selection.
Frequency-based selection is best forsuit, but x2-based selection is better for vessel, at least for SVD-reduced representations.Factor Clustering.
Fine clustering is generally better than coarse clustering.
The onecase for which coarse clustering comes close to the performance of fine clustering isglobal feature selection.
But this small difference is almost entirely due to the badperformance of fine clustering for plant, which is likely to be due to insufficient hand-categorization of the training set, as explained above.That fine clustering performs better than coarse clustering is not surprising, sincemore information is used in the evaluation of fine clustering: the labeling of clus-ters in the training set.
Only coarse clustering is evaluated as strictly unsuperviseddisambiguation, since we do not have an evaluation set for fine sense distinctions.Variance.
In general, the variance of discrimination accuracy is higher for coarse clus-tering than for fine clustering.
This is not surprising, given the fact that we evaluateboth types of clustering on how well they do on a two-way distinction.
There maybe several quite different ways of dividing a set of context vectors into two groups.But if we first cluster into ten groups and assign these groups to two senses, then theresulting two-way partitions are more likely to resemble ach other (even if the initial10-group clusterings are not very similar).The experiments indicate that context-group discrimination based on globally se-lected features is the best implementation i  the general case.
The algorithm achievesabove-baseline p rformance (with a small number of exceptions for certain parametersettings).
The average performance of the SVD-based representations of 83% to 91%is satisfactory, although inferior by about 5% to 10%, to disambiguation with minimalmanual intervention (e.g., Yarowsky \[1995\]).
33 Manually supplied priming information about senses i  not the only difference between context-groupdiscrimination a d other disambiguation algorithms.
Could one of the other differences be responsiblefor the difference inperformance?
The fact hat the error rate more than doubles when the seeds inYarowsky's (1995) experiments are reduced from a sense's best collocations tojust one word per sensesuggests hat the error rate would increase further if no seeds were provided.113Computational Linguistics Volume 24, Number 14.2 Application to Information RetrievalOur principal motivation for concentrating on the discrimination subtask is to ap-ply disambiguation to information retrieval.
While there is evidence that ambiguityresolution improves the performance of IR systems (Krovetz and Croft 1992), severalresearchers have failed to achieve consistent experimental improvements for practi-cally realistic rates of disambiguation accuracy.Voorhees (1993) compared two term-expansion methods for information retrievalqueries, one in which each term was expanded with all related terms and one in whichit was only expanded with terms related to the sense used in the query.
She found thatdisambiguation did not improve the performance of term expansion.
In our study, wewill use disambiguation to eliminate document-query matches that are due to sensemismatches (that is, the word in question is used in different ypes of context in thequery and the document).
This approach decreases the number of documents that aquery matches with whereas term expansion i creases it.
Another important differencein this study is that longer queries are used.
Long queries (as they may arise in an IRsystem after relevance feedback) provide more context than the short queries Voorheesworked with in her experiments.Sanderson (1994) modified a test collection by creating pseudowords similar to theones used in this study.
He found that even unrealistically high rates of disambiguationaccuracy had little or no effect on retrieval performance.
An analysis presented inSchfitze and Pedersen (1995) suggests that the main reason for the minor effect ofdisambiguation is that most of the pseudowords created in the study had a majorsense that accounted for almost all occurrences of the pseudoword.
Creating this typeof pseudoword amounts to adding a small amount of noise to an unambiguous word,which is not expbcted to have a large effect on retrieval performance.
To some extent,actual dictionary senses have the same property: one sense often accounts for a largeproportion of occurrences.
However, this is not necessarily true when rare senses arenot taken into account and when high-frequency senses are broken up into smallergroups (the example of 'office space' vs. 'exhibition space').
Large dictionaries tendto break up high-frequency senses into such more narrowly defined subsenses.
Thesuccessful se of disambiguation in our study may be due to the fact that rare senses,which are less likely to be useful in IR, are not taken into account and that frequentsenses are further subdivided.Good evidence for the potential utility of disambiguation in information retrievalwas provided by Krovetz and Croft (1992).
They showed that there is a considerableamount of ambiguity even in technical text (which is often assumed to be less ambigu-ous than nonspecialized writing).
Many technical terms have nontechnical meaningsthat are used in addition to more specialized senses even in technical text (e.g., windowand application in computer magazines, convertible in automobile magazines \[Krovetz1997\]).
Krovetz and Croft also showed that sense mismatches (i.e., spurious matchingwords that were used in different senses in query and document) occurred significantlymore often in nonrelevant than in relevant documents.
This suggests that eliminatingspurious matches could improve the separation between onrelevant and relevantdocuments and hence the overall quality of retrieval results.In order to show that context-group discrimination is an approach to disambigua-tion that is beneficial in information retrieval, we will now summarize the experimentpresented in Schfitze and Pedersen (1995).
That experiment evaluates sense-based re-trieval, a modification of the standard vector-space model in information retrieval.
(Werefer to the standard vector-space model as word-based retrieval.)
In word-based re-trieval, documents and queries are represented asvectors in a multidimensional spacein which each dimension corresponds to a word (similar to the way that we repre-114Schi~tze Automatic Word Sense Discriminationsent word vectors in Word Space).
In sense-based retrieval, documents and queriesare also represented in a multidimensional space, but its dimensions are senses, notwords.
Words are disambiguated using context-group discrimination.
Documents andqueries that contain a word assigned to a particular sense have a nonzero value onthe corresponding dimension.The test corpus in Sch~tze and Pedersen (1995) is the Category B TREC-1 collection(about 170,000 documents from the Wall Street Journal) in conjunction with its queries51-75 (Harman 1993).
Sense-based retrieval improved average precision by 7.4% whencompared to word-based retrieval.
A combination of word-based and sense-basedretrieval increased performance by 14.4%.
The greater improvement of the combinationis probably due to discrimination errors (i.e., the fact that discrimination is less than100% correct), which are partially undone by combining sense and word evidence.Improvement was particularly high when small sets of documents were requested,for example, 16.5% (sense-based) and 19.4% (word- and sense-based combined) for arecall level of 10% of relevant documents.
This experiment suggests a high utility ofsense discrimination for information retrieval.At first sight, sense-based retrieval may seem related to term expansion.
Bothsense-based retrieval and term expansion take individual terms as the starting pointfor modifying the similarity measure that determines which documents are deemedmost closely related to the query.
However, the two approaches are actually oppositesof each other in the following sense.
Term expansion increases the number of match-ing documents for a query.
For example, if the query contains cosmonaut and expan-sion adds astronaut, then documents containing astronaut become additional nonzeromatches.
Sense-based retrieval decreases the number of matches.
For example, if theword suit occurs in the query and is disambiguated as being used in the 'legal' sense,then documents that contain suit in a different sense will no longer match with thequery.5.
DiscussionWhat distinguishes context-group discrimination from other work on disambiguationis that no outside source of information eed be supplied as input to the algorithm.Other disambiguation algorithms employ various sources of information.
Kelly andStone (1975) consider hand-constructed disambiguation rules; Lesk (1986), Krovetzand Croft (1989), Guthrie et al (1991), and Karov and Edelman (1996) use on-line dic-tionaries; Hirst (1987) constructs knowledge bases; Cottrell (1989) uses syntactic andsemantic structure ncoded in a connectionist net; Brown et al (1991) and Church andGale (1991) exploit bilingual corpora; Dagan, Itai, and Schwall (1991) use a bilingualdictionary; Hearst (1991), Leacock, Towell, and Voorhees (1993), Niwa and Nitta (1994),and Bruce and Wiebe (1994) exploit a hand-labeled training set; and Yarowsky (1992)and Walker and Amsler (1986) perform computations based on a hand-constructedsemantic ategorization f words (Roget's Thesaurus and Longman's ubject codes, re-spectively).For some of these algorithms, the expense of supplying information to the disam-biguation algorithm is relatively small.
For example, in many of the methods usinghand-labeled training sets (e.g., Hearst \[1991\]), a relatively small number of trainingexamples i sufficient.
Yarowsky has proposed an algorithm that requires as little userinput as one seed word per sense to start the training process (Yarowsky 1995).
Suchminimal user input will be a negligible burden for users in some situations.
However,consider the interactive information-access application described above.
When askedto improve their initial ambiguous information request many users will be reluctant to115Computational Linguistics Volume 24, Number 1give a seed word or a set of good features for each sense of the word.
They are morelikely to satisfy a request by the system to choose the correct sense (e.g., by mouseclick), if example contexts corresponding todifferent senses are presented without herequirement of additional user interaction.
In an application like this, it is of great ad-vantage that context-group discrimination does not require any manual interventionto induce senses.Another body of related work is the literature on word clustering in computationallinguistics (Brown et al 1992; Finch 1993; Pereira, Tishby, and Lee 1993; Grefenstette1994a) and document clustering in information retrieval (van Rijsbergen 1979; Willett1988; Sparck-Jones 1991; Cutting et al 1992).
In contrast to this earlier work, we clustercontexts or, equivalently, word tokens here, not words (or, more precisely, word types)or documents.
The straightforward extension of word-type clustering and documentclustering to word-token clustering would be to represent a token by all words it co-occurs with in its context and cluster these representations.
Such an approach basedon first-order co-occurrence is used, for example, by Hearst and Plaunt (1993) for therepresentation f tiles or document subunits that are similar to our notion of context.Instead, we use second-order co-occurrence to represent the tokens of ambiguouswords: the words that occur with the token are in turn looked up in the trainingcorpus and the words they co-occur with are used to represent the token.
Second-order representations are less sparse and more robust han first-order representations.In a cluster-based approach, the subdivision of the universe of elements into clus-ters depends on the representation.
If the representation does not capture the infor-mation crucial for distinguishing senses, then context-group discrimination performspoorly.
The clearest such example in the above experiments i  the pseudoword widerange~consulting firm.
The algorithm does not do better than the baseline of alwayschoosing the most frequent sense.
The reason is that the representation captures onlytopic information.
So a cluster will contain a group of contexts that are about thesame topic.
Unfortunately, the pair wide range can come up in text about almost anytopic.
Since there is no clear topical characterization f one sense of the pseudoword,context-group discrimination performs poorly.The reliance on topical similarity may also be the reason that performance forpseudowords i  generally better than performance for natural ambiguous words.
Allpseudowords except for wide range/consultingJirm are composed of two pairs from dif-ferent opics.
For example, heart disease and reserve board pertain to biology and finance,respectively, two clearly distinct opics.
On the other hand, the senses of some of theambiguous words have less clear associations with particular topics.
For example, onecan be trained to perform a wide variety of activities, so the 'teaching' sense of traincan be invoked in many different opics.
Part of the superior performance for pseu-dowords is due to this different opic sensitivity of natural and artificial ambiguouswords.The limitation to topical distinctions i not so much a flaw of context-group dis-crimination as a flaw of the particular implementation we have presented here.
It ispossible to integrate information in the context vectors that reflect syntactic or sub-categorization behavior of different senses, such as the output of a shallow parser asused in Pereira, Tishby, and Lee (1993).
For example, one good indicator of the twosenses of the word interest is a preposition occurring to its right.
The phrase interestin invokes the 'feeling of attention' sense, the phrase interest on, the sense 'charge onborrowed money.'
It seems plausible that performance could be improved for wordswhose senses are less sensitive to topical distinctions if such "proximity" information isintegrated.
In some recent experiments, Pedersen and Bruce (1997) have used proxim-ity features (tags of close words and the presence or absence of close functions words116Schfttze Automatic Word Sense Discriminationand content words) with some promising results.
This suggests that a combination ofthe topical features used here and proximity features may give optimal performance ofcontext-group discrimination.
4 We have used only one source of information (topicalfeatures) in the interest of simplicity, not because we see any inherent advantage oftopical features compared to a combination of multiple sources of evidence.Our justification for the basic idea of context-group discrimination, inducing sensesfrom contextual similarity, has been that its results seem to align well with the groundtruth of senses defined in dictionaries.
However, there is also some evidence thatcontextual similarity plays a crucial role in human semantic ategorization.
In onestudy, Miller and Charles (1991) found evidence that human subjects determine thesemantic similarity of words from the similarity of the contexts they are used in.
Theysummarized this result in the following hypothesis:Strong Contextual Hypothesis: Two words are semantically similarto the extent hat their contextual representations are similar.
(p. 8)A contextual representation f a word is knowledge of how that word is used.
Thehypothesis states that semantic similarity is determined by the degree of similarity ofthe sets of contexts that the two words can be used in.The hypothesis that underlies context-group discrimination is an extension of theStrong Contextual Hypothesis to senses:Contextual Hypothesis for Senses: Two occurrences of an ambiguousword belong to the same sense to the extent that their contextualrepresentations are similar.So a sense is simply a group of occurrence tokens with similar contexts.
The analogybetween the contextual hypotheses for words and senses is that both word types andword tokens are semantically similar to the extent hat their contexts are semanticallysimilar.
A group of contextually similar word tokens is a sense.
Miller and Charles'swork thus provides a justification for our framework, the induction of senses fromcontextual similarity.There are several issues that need to be addressed in future work on context-groupdiscrimination.
First, our experiments only considered words with two major senses.The algorithm also needs to be tested for words with more than two frequent sensesand for infrequent senses.
Second, our test set consisted of a relatively small num-ber of natural ambiguous words.
This is a flaw of almost all contemporary work onword sense disambiguation, but in the future more extensive test sets will be requiredto establish the general applicability of disambiguation algorithms.
Finally, the imple-mentation of context-group discrimination proposed here is based on topical similarityonly.
It will be necessary to incorporate other, more structural constraints (such as theinterest in vs. interest on case discussed above) to achieve adequate performance for awide variety of ambiguous words.Appendix A: Singular Value DecompositionA singular value decomposition factors an m-by-n matrix A into a product of threematrices:( , )A  = U diag (o1,...,o.p)V T4 See Leacock (1993) for a discussion ofproximity and topical features insupervised disambiguation.117Computational Linguistics Volume 24, Number 1Table 6Co-occurrence ounts for eight words in a five-dimensional word space.judge suit robe gangster criminal police gun baillegal 300 210 133 30 200 160 120 150clothes 75 182 200 10 5 10 20 15cop 100 75 25 250 10 140 200 160fashion 5 100 200 5 5 5 5 5pants 5 110 190 5 5 5 5 5Table 7SVD reduction to two dimensions of the matrix in Table 6.judge suit robe gangster criminal police gun baildim1 -0.47 -0.46 -0.41 -0.22 -0.31 -0.30 -0.30 -0.30dim2 0.13 -0.31 -0.69 0.41 0.05 0.25 0.33 0.28Table 8Correlation coefficients of three words before and after SVDdimensionality reduction.criminal robeWord Space SVD Space Word Space SVD Spacegangster 0.17 0.61 0.15 -0.52criminal 0.41 0.37where p = min{m, n}, U (the left matrix) is an orthonormal m-by-p matrix, V (theright matrix) is an orthonormal n-by-p matrix and diag(o.1 .
.
.
.
, o.p) is a matrix withthe diagonal elements o.1 _> o'2 > ""  _> o.p ~_ 0 (and the value zero for nondiagonalelements) (Golub and van Loan 1989).Dimensionality reduction can be based on SVD by keeping only the first k singularvalues o.1 ?
?
?
c~k and setting the remaining ones to zero.
It can be shown that the productA' = U diag(o'l .
.
.
.
.
o.k)V T is the closest approximation to A in a k-dimensional space(that is, there is no matrix of rank k with a smaller least-square distance to A than A').See Golub and van Loan (1989) and Berry (1992) for a detailed description of SVD andefficient algorithms to compute it.The benefits of dimensionality reduction for our purposes can best be explainedusing an example.
Table 6 shows co-occurrence ounts from a hypothetical corpus (e.g.,legal and robe co-occur 133 times with each other).
Note that two semantically similarwords, gangster and criminal, have a low correlation in the words they co-occur withbecause they belong to different registers (this is one of reasons that topically similarwords can have few neighbors in common).
Table 7 shows the two columns of theright matrix V of the SVD of the matrix in Table 6.
Table 7 is therefore a dimensionalityreduction of Table 6 to two dimensions.
The advantage of the reduced space is that itdirectly represents the similar topicality of gangster and criminal: their vectors are closeto each other in the space, as shown in Figure 5.
On the other hand, both words' vectors118Schiitze Automatic Word Sense DiscriminationDIMENSION 2GANGSTERCRIMINA LROBErDIMENSION 1Figure 5The vectors for robe, gangster, and criminal in the reduced SVD space.
The words gangster andcriminal are represented assemantically similar.
Both are represented assemanticallydissimilar from robe.are less correlated with a topically dissimilar word like robe in the reduced space.
Thecorrelation coefficients of the three words are shown in Table 8 for the unreduced andthe reduced space.
The correlation of the topically related words (gangster and criminal)increases from 0.17 to 0.61, whereas the correlation of both words with robe decreases.This example demonstrates the effect of SVD dimensionality reduction: topicallysimilar words are projected closer to each other in the reduced space; topically dissim-ilar words are projected to distant locations.
Part of the motivation for using SVD forword vectors is the success of latent semantic indexing (LSI) in information retrieval(Deerwester et al 1990).
LSI projects topically similar documents to close locations inthe reduced space, just as we project topically similar words to close locations.Appendix B: The EM AlgorithmThe clustering algorithm used in this paper is the EM algorithm.
The observed ata(context vectors in our case) are interpreted as being generated by hidden causes,the clusters.
The EM algorithm is an iterative procedure that, starting from an initialhypothesis of the cluster parameters, improves the estimates of the parameters in eachiteration.
We follow here the discussion and notation in Dempster, Laird, and Rubin(1977) and Ghahramani (1994).We make the assumption that each cluster j is a Gaussian source with density ~j:~j(~)  - exp\[where \]/j is the mean and Gj the covariance matrix of a;j.
We write Oj = (fij, Gj) for theparameters of cluster j.119Computational Linguistics Volume 24, Number 1Assume that we have N d-dimensional context vectors ,g = {Xl ... XN} C T4 d gen-erated by M Gaussians COl... CVM.
The EM algorithm iteratively applies the Expectationstep (E step) and the Maximization step (M step).
The E step is the estimation of pa-rameters hq where hq is the probability of event zij, the event that cluster j generatedXi (context vector i).hij = E(zij I ~i; O k) =O k is 0 at iteration k.P(~ I ~J ;0~) .p(~l~j;o k) -~j(~;)P(~j)G~ P(~ I ~,; o~)The M step computes the most likely parameters of the distribution given thecluster membership robabilities:~\]i=1 /jk+l E/N1 hq(2:i - lif)(Y:i - 11~) T~j = ~N=lhqThese are the well-known maximum-likelihood estimates for mean and variance of aGaussian.Recomputed means and variances are the parameters for the next iteration k+l.
Forreasons of computational efficiency, we chose the implementation f the EM clusteringknown as k-means or hard clustering (Duda and Hart 1973).
In each iteration, contextvectors are first assigned to the cluster with the closest mean; then cluster means arerecomputed as the centroid of all members of the cluster.
This amounts to assuming avery small fixed variance for all clusters and only re-estimating the means in each step.The initial cluster parameters are computed by applying group-average agglomerativeclustering to a sample of size v'N.Appendix C: Agglomerative ClusteringAgglomerative clustering is a clustering technique that starts by assigning each ele-ment to a different cluster and then iteratively merges clusters according to a goodnesscriterion until the desired number of clusters has been reached.
Two such goodnessmeasures give rise to single-link clustering and complete-link clustering.
Single-linkclustering in each step merges the two clusters that have two elements with the small-est distance of any two clusters.
Complete-link clustering in each step executes themerger whose resulting cluster has the smallest diameter of all possible mergers.Single-link clustering has been found in practice to produce elongated clusters (e.g.,two parallel ines) that do not correspond well to the intuitive notion of a cluster as amass of points with a center.
Complete-link clustering is strongly affected by outliersand has a time complexity cubic in the number of points to be merged and, hence, isless efficient han single-link clustering (which can be computed in quadratic time).In this paper, we chose group-average agglomerative clustering (GAAC) as ourclustering algorithm, a hybrid of single-link and complete-link clustering.
GAAC ineach iteration executes the merger that gives rise to the cluster F with the largestaverage correlation C(P):1 1C(P) - 2 IPl(\[rl- 1) ~ ~ corr(~,7~)~cF/~cP120Computational Linguistics Volume 24, Number 1Journal of the Royal Statistical Society, SeriesB, 39:1-38.Duda, Richard O. and Peter E. Hart.
1973.Pattern Classi~'cation a d Scene Analysis.John Wiley & Sons, New York.Finch, Steven Paul.
1993.
Finding Structure inLanguage.
Ph.D. thesis, University ofEdinburgh.Gale, William A., Kenneth W. Church, andDavid Yarowsky.
1992.
Work on statisticalmethods for word sense disambiguation.In Robert Goldman, Peter Norvig, EugeneCharniak, and Bill Gale, editors, WorkingNotes of the AAAI Fall Symposium onProbabilistic Approaches toNatural Language,pages 54-60, AAAI Press, Menlo Park,CA.Gallant, Stephen I.
1991.
A practicalapproach for representing context and forperforming word sense disambiguationusing neural networks.
NeuralComputation, 3(3):293-309.Ghahramani, Zoubin.
1994.
Solving inverseproblems using an EM approach todensity estimation.
In Michael C. Mozer,Paul Smolensky, David S. Touretzky, andAndreas S. Weigend, editors, Proceedingsof the 1993 Connectionist Models SummerSchool, Erlbaum Associates, Hillsdale, NJ.Golub, Gene H. and Charles F. van Loan.1989.
Matrix Computations.
The JohnsHopkins University Press, Baltimore andLondon.Grefenstette, Gregory.
1992.
Use of syntacticcontext o produce term association listsfor text retrieval.
In Proceedings ofSIGIR'92, pages 89-97.Grefenstette, Gregory.
1994a.Corpus-derived first, second andthird-order word affinities.
In Proceedingsof the Sixth Euralex International Congress,Amsterdam.Grefenstette, Gregory.
1994b.
Explorations inAutomatic Thesaurus Discovery.
KluwerAcademic Press, Boston.Grefenstette, Gregory.
1996.
Evaluationtechniques for automatic semanticextraction: Comparing syntactic andwindow-based approaches.
In BranimirBoguraev and James Pustejovsky, editors,Corpus Processing for Lexical Acquisition.MIT Press, Cambridge, MA.Guthrie, Joe A., Louise Guthrie, YorickWilks, and Homa Aidinejad.
1991.Subject-dependent co-occurrence andword sense disambiguation.
I  Proceedingsof the 29th Annual Meeting, pages 146-152,Berkeley, CA.
Association forComputational Linguistics.Harman, D. K., editor.
1993.
The First TextREtrieval Conference (TREC-1).
U.S.Department of Commerce, Washington,DC.
NIST Special Publication 500-207.Hearst, Marti A.
1991.
Noun homographdisambiguation using local context inlarge text corpora.
In Proceedings oftheSeventh Annual Conference ofthe UW Centrefor the New OED and Text Research: UsingCorpora, pages 1-22, Oxford.Hearst, Marti and Christian Plaunt.
1993.Subtopic structuring for full-lengthdocument access.
In Proceedings ofSIGIR'93, pages 59-68.Hirst, Graeme.
1987.
Semantic Interpretationand the Resolution of Ambiguity.
CambridgeUniversity Press, Cambridge.Jain, Anil K. and Richard C. Dubes.
1988.Algorithms for Clustering Data.
PrenticeHall, Englewood Cliffs, NJ.Karov, Yael and Shimon Edelman.
1996.Learning similarity-based word sensedisambiguation from sparse data.
InProceedings ofthe Fourth Workshop on VeryLarge Corpora.Kelly, Edward and Phillip Stone.
1975.Computer Recognition ofEnglish WordSenses.
North-Holland, Amsterdam.Kilgarriff, Adam.
1993.
Dictionary wordsense distinctions: An enquiry into theirnature.
Computers and the Humanities,26:365-387.Krovetz, Robert.
1997.
Homonymy andpolysemy in information retrieval.
InProceedings ofthe 35th Annual Meeting andEACL 8, pages 72-79, Morgan Kaufmann,San Francisco, CA.
Association forComputational Linguistics.Krovetz, Robert and W. Bruce Croft.
1989.Word sense disambiguation usingmachine-readable dictionaries.
InProceedings ofSIGIR '89, pages 127-136,Cambridge, MA.Krovetz, Robert and W. Bruce Croft.
1992.Lexical ambiguity and informationretrieval.
ACM Transactions on InformationSystems, 10(2):115-141.Leacock, Claudia, Geoffrey Towell, andEllen Voorhees.
1993.
Towards buildingcontextual representations of word sensesusing statistical models.
In BranimirBoguraev and James Pustejovsky, editors,Acquisition of Lexical Knowledge From Text:Workshop Proceedings, pages 10-21, Ohio.Leacock, Claudia, Geoffrey Towell, andEllen Voorhees.
1993.
Corpus-basedstatistical sense resolution.
In Proceedingsof the ARPA Workshop on Human LanguageTechnology, Morgan Kaufman, San Mateo,CA.Lesk, M. E. 1969.
Word-word association idocument retrieval systems.
AmericanDocumentation, 20(1):27-38.122Schiitze Automatic Word Sense DiscriminationLesk, Michael.
1986.
Automatic sensedisambiguation: How to tell a pine conefrom an ice cream cone.
In Proceedings ofthe 1986 SIGDOC Conference, pages 24-26,New York.
Association for ComputingMachinery.Miller, George A. and Walter G. Charles.1991.
Contextual correlates of semanticsimilarity.
Language and Cognitive Processes,6(1):1-28.Niwa, Yoshiki and Yoshihiko Nitta.
1994.Co-occurrence v ctors from corpora vs.distance vectors from dictionaries.
InProceedings ofCOLING94, pages 304-309.Ott, Lyman.
1992.
An Introduction toStatistical Methods and Data Analysis.Wadsworth, Belmont, CA.Pedersen, Ted and Rebecca Bruce.
1997.Distinguishing word senses in untaggedtext.
In Proceedings ofthe Second Conferenceon Empirical Methods in Natural LanguageProcessing, pages 197-207, Providence, RI.Pereira, Fernando, Naftali Tishby, andLillian Lee.
1993.
Distributional clusteringof English words.
In Proceedings ofthe 31stAnnual Meeting, pages 183-190,Columbus, OH.
Association forComputational Linguistics.Qiu, Yonggang and H.P.
Frei.
1993.
Conceptbased query expansion.
In Proceedings ofSIGIR "93, pages 160-169.Ruge, Gerda.
1992.
Experiments onlinguistically-based term associations.Information Processing & Management,28(3):317-332.Salton, Gerard.
1971.
Experiments inautomatic thesaurus construction forinformation retrieval.
In Proceedings IFIPCongress, pages 43-49.Salton, Gerard and Chris Buckley.
1990.Improving retrieval performance byrelevance feedback.
Journal of the AmericanSociety for Information Science,41(4):288-297.Salton, Gerard and Michael J. McGill.
1983.Introduction to Modern Information Retrieval.McGraw-Hill, New York.Sanderson, Mark.
1994.
Word sensedisambiguation a d information retrieval.In Proceedings ofSIGIR "94, pages 142-151.Schiitze, Hinrich.
1992a.
Context space.
InRobert Goldman, Peter Norvig, EugeneCharniak, and Bill Gale, editors, WorkingNotes of the AAAI Fall Symposium onProbabilistic Approaches to Natural Language,pages 113-120, AAAI Press, Menlo Park,CA.Schiitze, Hinrich.
1992b.
Dimensions ofmeaning.
In Proceedings ofSupercomputing"92, pages 787-796, Minneapolis, MN.Schtitze, Hinrich.
1997.
Ambiguity Resolutionin Language Learning.
CSLI Publications,Stanford, CA.Schiitze, Hinrich and Jan O. Pedersen.
1995.Information retrieval based on wordsenses.
In Proceedings ofthe Fourth AnnualSymposium on Document Analysis andInformation Retrieval, pages 161-175, LasVegas, NV.Schiitze, Hinrich and Jan O. Pedersen.
1997.A cooccurrence-based thesaurus and twoapplications to information retrieval.Information Processing & Management,33(3):307-318.Sparck-Jones, Karen.
1986.
Synonymy andSemantic ClassiJication.
EdinburghUniversity Press, Edinburgh.
(Publicationof Ph.D. thesis, University of Cambridge,1964.
)Sparck-Jones, Karen.
1991.
Notes andreferences on early classification work.ACM SIGIR Forum, 25(1):10-17.van Rijsbergen, C. J.
1979.
InformationRetrieval.
Second edition.
Butterworths,London.Voorhees, Ellen M. 1993.
Using WordNet odisambiguate word senses for textretrieval.
In Proceedings ofSIGIR "93, pages171-180.Walker, Donald E. and Robert A. Amsler.1986.
The use of machine-readabledictionaries in sublanguage analysis.
InRalph Grishman and Richard Kittredge,editors, Analyzing Language in RestrictedDomains: Sublanguage Description andProcessing.
L. Erlbaum Associates,Hillsdale, NJ, pages 69-84.Wilks, Yorick A., Dan C. Fass, Cheng MingGuo, James E. McDonald, Tony Plate, andBrian M. Slator.
1990.
Providing machinetractable dictionary tools.
Journal ofComputers and Translation, 2.Willett, Peter.
1988.
Recent rends inhierarchic document clustering: A criticalreview.
Information Processing &Management, 24(5):577-597.Winer, B. J.
1971.
Statistical Principles inExperimental Design.
Second edition.McGraw-Hill, New York, NY.Yarowsky, David.
1992.
Word-sensedisambiguation using statistical models ofRoget's categories trained on largecorpora.
In Proceedings ofColing-92, pages454-460, Nantes, France.Yarowsky, David.
1995.
Unsupervised wordsense disambiguation rivaling supervisedmethods.
In Proceedings ofthe 33rd AnnualMeeting, Cambridge, MA.
Association forComputational Linguistics.123
