c?
2003 Association for Computational LinguisticsA Machine Learning Approach toModeling Scope PreferencesDerrick Higgins?
Jerrold M. Sadock?University of Chicago University of ChicagoThis article describes a corpus-based investigation of quantifier scope preferences.
Followingrecent work on multimodular grammar frameworks in theoretical linguistics and a long historyof combining multiple information sources in natural language processing, scope is treated as adistinct module of grammar from syntax.
This module incorporates multiple sources of evidenceregarding the most likely scope reading for a sentence and is entirely data-driven.
The experimentsdiscussed in this article evaluate the performance of our models in predicting the most likely scopereading for a particular sentence, using Penn Treebank data both with and without syntacticannotation.
We wish to focus attention on the issue of determining scope preferences, which haslargely been ignored in theoretical linguistics, and to explore different models of the interactionbetween syntax and quantifier scope.1.
OverviewThis article addresses the issue of determining the most accessible quantifier scopereading for a sentence.
Quantifiers are elements of natural and logical languages (suchas each, no, and some in English and ?
and ?
in predicate calculus) that have certainsemantic properties.
Loosely speaking, they express that a proposition holds for someproportion of a set of individuals.
One peculiarity of these expressions is that therecan be semantic differences that depend on the order in which the quantifiers areinterpreted.
These are known as scope differences.
(1) Everyone likes two songs on this album.As an example of the sort of interpretive differences we are talking about, considerthe sentence in (1).
There are two readings of this sentence; which reading is meantdepends on which of the two quantified expressions everyone and two songs on thisalbum takes wide scope.
The first reading, in which everyone takes wide scope, simplyimplies that every person has a certain preference, not necessarily related to anyoneelse?s.
This reading can be paraphrased as ?Pick any person, and that person will liketwo songs on this album.?
The second reading, in which everyone takes narrow scope,implies that there are two specific songs on the album of which everyone is fond, say,?Blue Moon?
and ?My Way.
?In theoretical linguistics, attention has been primarily focused on the issue of scopegeneration.
Researchers applying the techniques of quantifier raising and Cooper stor-age have been concerned mainly with enumerating all of the scope readings for a?
Department of Linguistics, University of Chicago, 1010 East 59th Street, Chicago, IL 60637.
E-mail:dchiggin@alumni.uchicago.edu.?
Department of Linguistics, University of Chicago, 1010 East 59th Street, Chicago, IL 60637.
E-mail:j-sadock@uchicago.edu.74Computational Linguistics Volume 29, Number 1sentence that are possible, without regard to their relative likelihood or naturalness.Recently, however, linguists such as Kuno, Takami, and Wu (1999) have begun to turntheir attention to scope prediction, or determining the relative accessibility of differentscope readings.In computational linguistics, more attention has been paid to the factors that de-termine scope preferences.
Systems such as the SRI Core Language Engine (Moran1988; Moran and Pereira 1992), LUNAR (Woods 1986), and TEAM (Martin, Appelt,and Pereira 1986) have employed scope critics that use heuristics to decide betweenalternative scopings.
However, the rules that these systems use in making quantifierscope decisions are motivated only by the researchers?
intuitions, and no empiricalresults have been published regarding their accuracy.In this article, we use the tools of machine learning to construct a data-drivenmodel of quantifier scope preferences.
For theoretical linguistics, this model serves asan illustration that Kuno, Takami, and Wu?s approach can capture some of the clear-est generalizations about quantifier scoping.
For computational linguistics, this articleprovides a baseline result on the task of scope prediction, with which other scopecritics can be compared.
In addition, it is the most extensive empirical investigationof which we are aware that collects data of any kind regarding the relative frequencyof different quantifier scope readings in English text.1Section 2 briefly discusses treatments of scoping issues in theoretical linguistics,and Section 3 reviews the computational work that has been done on natural languagequantifier scope.
In Section 4 we introduce the models that we use to predict quantifierscoping, as well as the data on which they are trained and tested.
Section 5 combinesthe scope model of the previous section with a probabilistic context-free grammar(PCFG) model of syntax and addresses the issue of whether these two modules ofgrammar ought to be combined in serial, with information from the syntax feeding thequantifier scope module, or in parallel, with each module constraining the structuresprovided by the other.2.
Approaches to Quantifier Scope in Theoretical LinguisticsMost, if not all, linguistic treatments of quantifier scope have closely integrated it withthe way in which the syntactic structure of a sentence is built up.
Montague (1973) useda syntactic rule to introduce a quantified expression into a derivation at the point whereit was to take scope, whereas generative semantic analyses such as McCawley (1998)represented the scope of quantification at deep structure, transformationally loweringquantifiers into their surface positions during the course of the derivation.
More recentwork in the interpretive paradigm takes the opposite approach, extracting quantifiersfrom their surface positions to their scope positions by means of a quantifier-raising(QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995).
Another populartechnique is to percolate scope information up through the syntactic tree using Cooperstorage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995;Pollard and Yoo 1998).The QR approach to dealing with scope in linguistics consists in the claim thatthere is a covert transformation applying to syntactic structures that moves quantifiedelements out of the position in which they are found on the surface and raises them toa higher position that reflects their scope.
The various incarnations of the strategy that1 See Carden (1976), however, for a questionnaire-based approach to gathering data on the accessibilityof different quantifier scope readings.75Higgins and Sadock Modeling Scope PreferencesFigure 1Simple illustration of the QR approach to quantifier scope generation.follows from this claim differ in the precise characterization of this QR transforma-tion, what conditions are placed upon it, and what tree-configurational relationshipis required for one operator to take scope over another.
The general idea of QR isrepresented in Figure 1, a schematic analysis of the reading of the sentence Someonesaw everyone in which someone takes wide scope (i.e., ?there is some person x such thatfor all persons y, x saw y?
).In the Cooper storage approach, quantifiers are gathered into a store and passedupward through a syntactic tree.
At certain nodes along the way, quantifiers may beretrieved from the store and take scope.
The relative scope of quantifiers is determinedby where each quantifier is retrieved from the store, with quantifiers higher in the treetaking wide scope over lower ones.
As with QR, different authors implement thisscheme in slightly different ways, but the simplest case is represented in Figure 2, theCooper storage analog of Figure 1.These structural approaches, QR and Cooper storage, have in common that theyallow syntactic factors to have an effect only on the scope readings that are available fora given sentence.
They are also similar in addressing only the issue of scope generation,or identifying all and only the accessible readings for each sentence.
That is to say,they do not address the issue of the relative salience of these readings.Kuno, Takami, and Wu (1999, 2001) propose to model the scope of quantifiedelements with a set of interacting expert systems that basically consists of a weightedvote taken of the various factors that may influence scope readings.
This model ismeant to account not only for scope generation, but also for ?the relative strengths ofthe potential scope interpretations of a given sentence?
(1999, page 63).
They illustratethe plausibility of this approach in their paper by presenting a number of examplesthat are accounted for fairly well by the approach even when an unweighted vote ofthe factors is allowed to be taken.So, for example, in Kuno, Takami and Wu?s (49b) (1999), repeated here as (2), thecorrect prediction is made: that the sentence is unambiguous with the first quantifiednoun phrase (NP) taking wide scope over the second (the reading in which we don?tall have to hate the same people).
Table 1 illustrates how the votes of each of Kuno,Takami, and Wu?s ?experts?
contribute to this outcome.
Since the expression many ofus/you receives more votes, and the numbers for the two competing quantified expres-sions are quite far apart, the first one is predicted to take wide scope unambiguously.
(2) Many of us/you hate some of them.76Computational Linguistics Volume 29, Number 1Figure 2Simple illustration of the Cooper storage approach to quantifier scope generation.Table 1Voting to determine optimal scope readings for quantifiers, according to Kuno, Takami, andWu (1999).many of us/you some of themBaseline:?
?Subject Q:?Lefthand Q:?Speaker/Hearer Q:?Total: 4 1Some adherents of the structural approaches also seem to acknowledge the ne-cessity of eventually coming to terms with the factors that play a role in determiningscope preferences in language.
Aoun and Li (2000) claim that the lexical scope pref-erences of quantifiers ?are not ruled out under a structural account?
(page 140).
It isclear from the surrounding discussion, though, that they intend such lexical require-ments to be taken care of in some nonsyntactic component of grammar.
AlthoughKuno, Takami, and Wu?s dialogue with Aoun and Li in Language has been portrayedby both sides as a debate over the correct way of modeling quantifier scope, they arenot really modeling the same things.
Whereas Aoun and Li (1993) provide an accountof scope generation, Kuno, Takami, and Wu (1999) intend to model both scope gen-eration and scope prediction.
The model of scope preferences provided in this articleis an empirically based refinement of the approach taken by Kuno, Takami, and Wu,but in principle it is consistent with a structural account of scope generation.77Higgins and Sadock Modeling Scope Preferences3.
Approaches to Quantifier Scope in Computational LinguisticsMany studies, such as Pereira (1990) and Park (1995), have dealt with the issue ofscope generation from a computational perspective.
Attempts have also been madein computational work to extend a pure Cooper storage approach to handle scopeprediction.
Hobbs and Shieber (1987) discuss the possibility of incorporating some sortof ordering heuristics into the SRI scope generation system, in the hopes of producinga ranked list of possible scope readings, but ultimately are forced to acknowledge that?
[t]he modifications turn out to be quite complicated if we wish to order quantifiersaccording to lexical heuristics, such as having each out-scope some.
Because of therecursive nature of the algorithm, there are limits to the amount of ordering that canbe done in this manner?
(page 55).
The stepwise nature of these scope mechanismsmakes it hard to state the factors that influence the preference for one quantifier totake scope over another.Those natural language processing (NLP) systems that have managed to providesome sort of account of quantifier scope preferences have done so by using a separatesystem of heuristics (or scope critics) that apply postsyntactically to determine the mostlikely scoping.
LUNAR (Woods 1986), TEAM (Martin, Appelt, and Pereira 1986), andthe SRI Core Language Engine as described by Moran (1988; Moran and Pereira 1992)all employ scope rules of this sort.
By and large, these rules are of an ad hoc nature,implementing a linguist?s intuitive idea of what factors determine scope possibilities,and no results have been published regarding the accuracy of these methods.
Forexample, Moran (1988) incorporates rules from other NLP systems and from VanLehn(1978), such as a preference for a logically weaker interpretation, the tendency for eachto take wide scope, and a ban on raising a quantifier across multiple major clauseboundaries.
The testing of Moran?s system is ?limited to checking conformance tothe stated rules?
(pages 40?41).
In addition, these systems are generally incapable ofhandling unrestricted text such as that found in the Wall Street Journal corpus in arobust way, because they need to do a full semantic analysis of a sentence in orderto make scope predictions.
The statistical basis of the model presented in this articleoffers increased robustness and the possibility of more serious evaluation on the basisof corpus data.4.
Modeling Quantifier ScopeIn this section, we argue for an empirically driven machine learning approach tothe identification of factors relevant to quantifier scope and the modeling of scopepreferences.
Following much recent work that applies the tools of machine learning tolinguistic problems (Brill 1995; Pedersen 2000; van Halteren, Zavrel, and Daelemans2001; Soon, Ng, and Lim 2001), we will treat the prediction of quantifier scope asan example of a classification task.
Our aim is to provide a robust model of scopeprediction based on Kuno, Takami, and Wu?s theoretical foundation and to addressthe serious lack of empirical results regarding quantifier scope in computational work.We describe here the modeling tools borrowed from the field of artificial intelligencefor the scope prediction task and the data from which the generalizations are to belearned.
Finally, we present the results of training different incarnations of our scopemodule on the data and assess the implications of this exercise for theoretical andcomputational linguistics.78Computational Linguistics Volume 29, Number 14.1 Classification in Machine LearningDetermining which among multiple quantifiers in a sentence takes wide scope, givena number of different sources of evidence, is an example of what is known in machinelearning as a classification task (Mitchell 1996).
There are many types of classifiersthat may be applied to this task that both are more sophisticated than the approachsuggested by Kuno, Takami, and Wu and have a more solid probabilistic foundation.These include the naive Bayes classifier (Manning and Schu?tze 1999; Jurafsky andMartin 2000), maximum-entropy models (Berger, Della Pietra, and Della Pietra 1996;Ratnaparkhi 1997), and the single-layer perceptron (Bishop 1995).
We employ theseclassifier models here primarily because of their straightforward probabilistic inter-pretation and their similarity to the scope model of Kuno, Takami, and Wu (since theyeach could be said to implement a kind of weighted voting of factors).
In Section 4.3,we describe how classifiers of these types can be constructed to serve as a grammaticalmodule responsible for quantifier scope determination.All of these classifiers can be trained in a supervised manner.
That is, given a sam-ple of training data that provides all of the information that is deemed to be relevantto quantifier scope and the actual scope reading assigned to a sentence, these classi-fiers will attempt to extract generalizations that can be fruitfully applied in classifyingas-yet-unseen examples.4.2 DataThe data on which the quantifier scope classifiers are trained and tested is an extractfrom the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) that we havetagged to indicate the most salient scope interpretation of each sentence in context.Figure 3 shows an example of a training sentence with the scope reading indicated.The quantifier lower in the tree bears the tag ?Q1,?
and the higher quantifier bears thetag ?Q2,?
so this sentence is interpreted such that the lower quantifier has wide scope.Reversing the tags would have meant that the higher quantifier takes wide scope, andwhile if both quantifiers had been marked ?Q1,?
this would have indicated that thereis no scope interaction between them (as when they are logically independent or takescope in different conjuncts of a conjoined phrase).2The sentences tagged were chosen from the Wall Street Journal (WSJ) section ofthe Penn Treebank to have a certain set of attributes that simplify the task of design-ing the quantifier scope module of the grammar.
First, in order to simplify the codingprocess, each sentence has exactly two scope-taking elements of the sort consideredfor this project.3 These include most NPs that begin with a determiner, predeterminer,or quantifier phrase (QP)4 but exclude NPs in which the determiner is a, an, or the.
Ex-2 This ?no interaction?
class is a sort of ?elsewhere?
category that results from phrasing the classificationquestion as ?Which quantifier takes wider scope in the preferred reading??
Where there is no scopeinteraction, the answer is ?neither.?
This includes cases in which the relative scope of operators doesnot correspond to a difference in meaning, as in One woman bought one horse, or when they take scopein different propositional domains, such as in Mary bought two horses and sold three sheep.
The humancoders used in this study were instructed to choose class 0 whenever there was not a clear preferencefor one of the two scope readings.3 This restriction that each sentence contain only two quantified elements does not actually excludemany sentences from consideration.
We identified only 61 sentences with three quantifiers of the sortwe consider and 12 sentences with four.
In addition, our review of these sentences revealed that manyof them simply involve lists in which the quantifiers do not interact in terms of scope (as in, forexample, ?We ask that you turn off all cell phones, extinguish all cigarettes, and open any candy beforethe performance begins?).
Thus, the class of sentences with more than two quantifiers is small andseems to involve even simpler quantifier interactions than those found in our corpus.4 These categories are intended to be understood as they are used in the tagging and parsing of the PennTreebank.
See Santorini (1990) and Bies et al (1995) for details; the Appendix lists selected codes used79Higgins and Sadock Modeling Scope Preferences( (S(NP-SBJ(NP (DT Those) )(SBAR(WHNP-1 (WP who) )(S(NP-SBJ-2 (-NONE- *T*-1) )(ADVP (RB still) )(VP (VBP want)(S(NP-SBJ (-NONE- *-2) )(VP (TO to)(VP (VB do)(NP (PRP it) ))))))))(??
??
)(VP (MD will)(ADVP (RB just) )(VP (VB find)(NP(NP (DT-Q2 some) (NN way) )(SBAR(WHADVP-3 (-NONE- 0) )(S(NP-SBJ (-NONE- *) )(VP (TO to)(VP (VB get)(PP (IN around) (??
??
)(NP (DT-Q1 any) (NN attempt)(S(NP-SBJ (-NONE- *) )(VP (TO to)(VP (VB curb)(NP (PRP it) ))))))(ADVP-MNR (-NONE- *T*-3) ))))))))(.
.)
))Figure 3Tagged Wall Street Journal text from the Penn Treebank.
The lower quantifier takes widescope, indicated by its tag ?Q1.
?cluding these determiners from consideration largely avoids the problem of genericsand the complexities of assigning scope readings to definite descriptions.
In addi-tion, only sentences that had the root node S were considered.
This serves to excludesentence fragments and interrogative sentence types.
Our data set therefore differssystematically from the full WSJ corpus, but we believe it is sufficient to allow manygeneralizations about English quantification to be induced.
Given these restrictions onthe input data, the task of the scope classifier is a choice among three alternatives:5(Class 0) There is no scopal interaction.
(Class 1) The first quantifier takes wide scope.
(Class 2) The second quantifier takes wide scope.for annotating the Penn Treebank corpus.
The category QP is particularly unintuitive in that it does notcorrespond to a quantified noun phrase, but to a measure expression, such as more than half.5 Some linguists may find it strange that we have chosen to treat the choice of preferred scoping for twoquantified elements as a tripartite decision, since the possibility of independence is seldom treated inthe linguistic literature.
As we are dealing with corpus data in this experiment, we cannot afford toignore this possibility.80Computational Linguistics Volume 29, Number 1The result is a set of 893 sentences,6 annotated with Penn Treebank II parse trees andhand-tagged for the primary scope reading.To assess the reliability of the hand-tagged data used in this project, the data werecoded a second time by an independent coder, in addition to the reference coding.The independent codings agreed with the reference coding on 76.3% of sentences.
Thekappa statistic (Cohen 1960) for agreement was .52, with a 95% confidence intervalbetween .40 and .64.
Krippendorff (1980) has been widely cited as advocating theview that kappa values greater than .8 should be taken as indicating good reliability,with values between .67 and .8 indicating tentative reliability, but we are satisfiedwith the level of intercoder agreement on this task.
As Carletta (1996) notes, manytasks in computational linguistics are simply more difficult than the content analysisclassifications addressed by Krippendorff, and according to Fleiss (1981), kappa valuesbetween .4 and .75 indicate fair to good agreement anyhow.Discussion between the coders revealed that there was no single cause for their dif-ferences in judgments when such differences existed.
Many cases of disagreement stemfrom different assumptions regarding the lexical quantifiers involved.
For example, thecoders sometimes differed on whether a given instance of the word any correspondsto a narrow-scope existential, as we conventionally treat it when it is in the scope ofnegation, or the ?free-choice?
version of any.
To take another example, two universalquantifiers are independent in predicate calculus (?x?y[?]
??
?y?x[?
]), but in creat-ing our scope-tagged corpus, it was often difficult to decide whether two universal-likeEnglish quantifiers (such as each, any, every, and all) were actually independent in agiven sentence.
Some differences in coding stemmed from coder disagreements aboutwhether a quantifier within a fixed expression (e.g., all the hoopla) truly interacts withother operators in the sentence.
Of course, another major factor contributing to inter-coder variation is the fact that our data sentences, taken from Wall Street Journal text,are sometimes quite long and complex in structure, involving multiple scope-takingoperators in addition to the quantified NPs.
In such cases, the coders sometimes haddifficulty clearly distinguishing the readings in question.Because of the relatively small amount of data we had, we used the technique oftenfold cross-validation in evaluating our classifiers, in each case choosing 89 of the893 total data sentences from the data as a test set and training on the remaining 804.We preprocessed the data in order to extract the information from each sentence thatwe would be treating as relevant to the prediction of quantifier scoping in this project.
(Although the initial coding of the preferred scope reading for each sentence was donemanually, this preprocessing of the data was done automatically.)
At the end of thispreprocessing, each sentence was represented as a record containing the followinginformation (see the Appendix for a list of annotation codes for Penn Treebank):?
the syntactic category, according to Penn Treebank conventions, of thefirst quantifier (e.g., DT for each, NN for everyone, or QP for more than half )?
the first quantifier as a lexical item (e.g., each or everyone).
For a QPconsisting of multiple words, this field contains the head word, or ?CD?in case the head is a cardinal number.?
the syntactic category of the second quantifier?
the second quantifier as a lexical item6 These data have been made publicly available to all licensees of the Penn Treebank by means of apatch file that may be retrieved from ?http://humanities.uchicago.edu/linguistics/students/dchiggin/qscope-data.tgz?.
This file also includes the coding guidelines used for this project.81Higgins and Sadock Modeling Scope Preferences?????????????????????????????
?class: 2first cat: DTfirst head: somesecond cat: DTsecond head: anyjoin cat: NPfirst c-commands: YESsecond c-commands: NOnodes intervening: 6VP intervenes: YESADVP intervenes: NO...S intervenes: YESconj intervenes: NO, intervenes: NO: intervenes: NO...?
intervenes: YES?????????????????????????????
?Figure 4Example record corresponding to the sentence shown in Figure 3.?
the syntactic category of the lowest node dominating both quantifiedNPs (the ?join?
node)?
whether the first quantified NP c-commands the second?
whether the second quantified NP c-commands the first?
the number of nodes intervening7 between the two quantified NPs?
a list of the different categories of nodes that intervene between thequantified NPs (actually, for each nonterminal category, there is a distinctbinary feature indicating whether a node of that category intervenes)?
whether a conjoined node intervenes between the quantified NPs?
a list of the punctuation types that are immediately dominated by nodesintervening between the two NPs (again, for each punctuation tag in thetreebank there is a distinct binary feature indicating whether suchpunctuation intervenes)Figure 4 illustrates how these features would be used to encode the example in Fig-ure 3.The items of information included in the record, as listed above, are not the exactfactors that Kuno, Takami, and Wu (1999) suggest be taken into consideration in mak-ing scope predictions, and they are certainly not sufficient to determine the properscope reading for all sentences completely.
Surely pragmatic factors and real-worldknowledge influence our interpretations as well, although these are not representedhere.
This list does, however, provide information that could potentially be useful inpredicting the best scope reading for a particular sentence.
For example, information7 We take a node ?
to intervene between two other nodes ?
and ?
in a tree if and only if ?
is the lowestnode dominating both ?
and ?, ?
dominates ?
or ?
= ?, and ?
dominates either ?
or ?.82Computational Linguistics Volume 29, Number 1Table 2Baseline performance, summed over all ten test sets.Condition Correct Incorrect Percentage correctFirst has wide scope 0 64 0/64 = 0.0%Second has wide scope 0 281 0/281 = 0.0%No scope interaction 545 0 545/545 = 100.0%Total 545 345 545/890 = 61.2%about whether one quantified NP in a given sentence c-commands the other corre-sponds to Kuno, Takami, and Wu?s observation that subject quantifiers tend to takewide scope over object quantifiers and topicalized quantifiers tend to outscope ev-erything.
The identity of each lexical quantifier clearly should allow our classifiers tomake the generalization that each tends to take wide scope, if this word is found inthe data, and perhaps even learn the regularity underlying Kuno, Takami, and Wu?sobservation that universal quantifiers tend to outscope existentials.4.3 Classifier DesignIn this section, we present the three types of model that we have trained to predictthe preferred quantifier scoping on Penn Treebank sentences: a naive Bayes classifier,a maximum-entropy classifier, and a single-layer perceptron.8 In evaluating how wellthese models do in assigning the proper scope reading to each test sentence, it is im-portant to have a baseline for comparison.
The baseline model for this task is one thatsimply guesses the most frequent category of the data (?no scope interaction?)
everytime.
This simplistic strategy already classifies 61.2% of the test examples correctly, asshown in Table 2.It may surprise some linguists that this third class of sentences in which there isno scopal interaction between the two quantifiers is the largest.
In part, this may bedue to special features of the Wall Street Journal text of which the corpus consists.
Forexample, newspaper articles may contain more direct quotations than other genres.
Inthe process of tagging the data, however, it was also apparent that in a large proportionof cases, the two quantifiers were taking scope in different conjuncts of a conjoinedphrase.
This further tendency supports the idea that people may intentionally avoidconstructions in which there is even the possibility of quantifier scope interactions,perhaps because of some hearer-oriented pragmatic principle.
Linguists may also beconcerned that this additional category in which there is no scope interaction betweenquantifiers makes it difficult to compare the results of the present work with theoreticalaccounts of quantifier scope that ignore this case and concentrate on instances in whichone quantifier does take scope over another.
In response to such concerns, however,we point out first that we provide a model of scope prediction rather than scopegeneration, and so it is in any case not directly comparable with work in theoreticallinguistics, which has largely ignored scope preferences.
Second, we point out thatthe empirical nature of this study requires that we take note of cases in which thequantifiers simply do not interact.8 The implementations of these classifiers are publicly available as Perl modules at ?http://humanities.uchicago.edu/linguistics/students/dchiggin/classifiers.tgz?.83Higgins and Sadock Modeling Scope PreferencesTable 3Performance of the naive Bayes classifier, summed over all 10 test runs.Condition Correct Incorrect Percentage correctFirst has wide scope 177 104 177/281 = 63.0%Second has wide scope 41 23 41/64 = 64.1%No scope interaction 428 117 428/545 = 78.5%Total 646 244 646/890 = 72.6%4.3.1 Naive Bayes Classifier.
Our data D will consist of a vector of features (d0 ?
?
?
dn)that represent aspects of the sentence under examination, such as whether one quan-tified expression c-commands the other, as described in Section 4.2.
The fundamentalsimplifying assumption that we make in designing a naive Bayes classifier is thatthese features are independent of one another and therefore can be aggregated as in-dependent sources of evidence about which class c?
a given sentence belongs to.
Thisindependence assumption is formalized in equations (1) and (2).c?
= arg maxcP(c)P(d0 ?
?
?
dn | c) (1)?
arg maxcP(c)n?k=0P(dk | c) (2)We constructed an empirical estimate of the prior probability P(c) by simply count-ing the frequency with which each class occurs in the training data.
We constructedeach P(dk | c) by counting how often each feature dk co-occurs with the class c toconstruct the empirical estimate P?
(dk | c) and interpolated this with the empiricalfrequency P?
(dk) of the feature dk, not conditioned on the class c. This interpolatedprobability model was used in order to smooth the probability distribution, avoidingthe problems that can arise if certain feature-value pairs are assigned a probability ofzero.The performance of the naive Bayes classifier is summarized in Table 3.
For eachof the 10 test sets of 89 items taken from the corpus, the remaining 804 of the total893 sentences were used to train the model.
The naive Bayes classifier outperformedthe baseline by a considerable margin.In addition to the raw counts of test examples correctly classified, though, wewould like to know something of the internal structure of the model (i.e., what sort offeatures it has induced from the data).
For this classifier, we can assume that a featuref is a good predictor of a class c?
when the value of P(f | c?)
is significantly largerthan the (geometric) mean value of P(f | c) for all other values of c. Those featureswith the greatest ratio P(f ) ?
P(f |c?
)geom.mean(?c =c?
[P(f |c)]) are listed in Table 4.9The first-ranked feature in Table 4 shows that there is a tendency for quanti-fied elements not to interact when they are found in conjoined constituents, and thesecond-ranked feature indicates a preference for quantifiers not to interact when thereis an intervening comma (presumably an indicator of greater syntactic ?distance?
).Feature 3 indicates a preference for class 1 when there is an intervening S node,9 We include the term P(f ) in the product in order to prevent sparsely instantiated features fromshowing up as highly-ranked.84Computational Linguistics Volume 29, Number 1Table 4Most active features from naive Bayes classifier.Rank Feature Predicted Ratioclass1 There is an intervening conjunct node 0 1.632 There is an intervening comma 0 1.513 There is an intervening S node 1 1.334 The first quantified NP does not c-command the second 0 1.255 Second quantifier is tagged QP 1 1.166 There is an intervening S node 0 1.1215 The second quantified NP c-commands the first 2 1.00whereas feature 6 indicates a preference for class 0 under the same conditions.
Pre-sumably, this reflects a dispreference for the second quantifier to take wide scopewhen there is a clause boundary intervening between it and the first quantifier.
Thefourth-ranked feature in Table 4 indicates that, if the first quantified NP does notc-command the second, it is less likely to take wide scope.
This is not surprising,given the importance that c-command relations have had in theoretical discussionsof quantifier scope.
The fifth-ranked feature expresses a preference for quantified ex-pressions of category QP to take narrow scope, if they are the second of the twoquantifiers under consideration.
This may simply be reflective of the fact that class1 is more common than class 2, and the measure expressions found in QP phrasesin the Penn Treebank (such as more than three or about half ) tend not to be logicallyindependent of other quantifiers.
Finally, the feature 15 in Table 4 indicates a highcorrelation between the second quantified expression?s c-commanding the first andthe second quantifier?s taking wide scope.
We can easily see this as a translation intoour feature set of Kuno, Takami, and Wu?s claim that subjects tend to outscope ob-jects and obliques and topicalized elements tend to take wide scope.
Some of thesetop-ranked features have to do with information found only in the written medium,but on the whole, the features induced by the naive Bayes classifier seem consis-tent with those suggested by Kuno, Takami, and Wu, although they are distinct bynecessity.4.3.2 Maximum-Entropy Classifier.
The maximum-entropy classifier is a sort of log-linear model, defining the joint probability of a class and a data vector (d0 ?
?
?
dn) asthe product of the prior probability of the class c with a set of features related to thedata:10P(d0 ?
?
?
dn, c) =P(c)Zn?k=0?k (3)This classifier superficially resembles in form the naive Bayes classifier in equation (2),but it differs from that classifier in that the way in which values for each ?
are chosendoes not assume that the features in the data are independent.
For each of the 10training sets, we used the generalized iterative scaling algorithm to train this classifieron 654 training examples, using 150 examples for validation to choose the best set of10 Z in Equation 3 is simply a normalizing constant that ensures that we end up with a probabilitydistribution.85Higgins and Sadock Modeling Scope PreferencesTable 5Performance of the maximum-entropy classifier, summed over all 10 test runs.Condition Correct Incorrect Percentage correctFirst has wide scope 148 133 148/281 = 52.7%Second has wide scope 31 33 31/64 = 48.4%No scope interaction 475 70 475/545 = 87.2%Total 654 236 654/890 = 73.5%Table 6Most active features from maximum-entropy classifier.Rank Feature Predicted ?c,.25class1 Second quantifier is each 2 1.132 There is an intervening comma 0 1.013 There is an intervening conjunct node 0 1.004 First quantified NP does not c-command the second 0 0.995 Second quantifier is every 2 0.986 There is an intervening quotation mark (?)
0 0.957 There is an intervening colon 0 0.9512 First quantified NP c-commands the second 1 0.9225 There is no intervening comma 1 0.90values for the ?s.11 Test data could then be classified by choosing the class for the datathat maximizes the joint probability in equation (3).The results of training with the maximum-entropy classifier are shown in Table 5.The classifier showed slightly higher performance than the naive Bayes classifier, withthe lowest error rate on the class of sentences having no scope interaction.To determine exactly which features of the data the maximum-entropy classifiersees as relevant to the classification problem, we can simply look at the ?
values (fromequation (3)) for each feature.
Those features with higher values for ?
are weightedmore heavily in determining the proper scoping.
Some of the features with the highestvalues for ?
are listed in Table 6.
Because of the way the classifier is built, predictorfeatures for class 2 need to have higher loadings to overcome the lower prior probabil-ity of the class.
Therefore, we actually rank the features in Table 6 according to ?P?
(c)k(which we denote as ?c,k).
P?
(c) represents the empirical prior probability of a class c,and k is simply a constant (.25 in this case) chosen to try to get a mix of features fordifferent classes at the top of the list.The features ranked first and fifth in Table 6 express lexical preferences for certainquantifiers to take wide scope, even when they are the second of the two quantifiersaccording to linear order in the string of words.
The tendency for each to take widescope is stronger than for the other quantifier, which is in line with Kuno, Takami,and Wu?s decision to list it as the only quantifier with a lexical preference for scoping.Feature 2 makes the ?no scope interaction?
class more likely if a comma intervenes, and11 Overtraining is not a problem with the pure version of the generalized iterative scaling algorithm.
Forefficiency reasons, however, we chose to take the training corpus as representative of the event space,rather than enumerating the space exhaustively (see Jelinek [1998] for details).
For this reason, it wasnecessary to employ validation in training.86Computational Linguistics Volume 29, Number 1Table 7Performance of the single-layer perceptron, summed over all 10 test runs.Condition Correct Incorrect Percentage correctFirst has wide scope 182 99 182/281 = 64.8%Second has wide scope 35 29 35/64 = 54.7%No scope interaction 468 77 468/545 = 85.9%Total 685 205 685/890 = 77.0%feature 25 makes a wide-scope reading for the first quantifier more likely if there is nointervening comma.
The third-ranked feature expresses the tendency mentioned abovefor quantifiers in conjoined clauses not to interact.
Features 4 and 12 indicate that if thefirst quantified expression c-commands the second, it is likely to take wide scope, andthat if this is not the case, there is likely to be no scope interaction.
Finally, the sixth-and seventh-ranked features in the table show that an intervening quotation mark orcolon will make the classifier tend toward class 0, ?no scope interaction,?
which is easyto understand.
Quotations are often opaque to quantifier scope interactions.
The topfeatures found by the maximum-entropy classifier largely coincide with those foundby the naive Bayes model, which indicates that these generalizations are robust andobjectively present in the data.4.3.3 Single-Layer Perceptron.
For our neural network classifier, we employed a feed-forward single-layer perceptron, with the softmax function used to determine the acti-vation of nodes at the output layer, because this is a one-of-n classification task (Bridle1990).
The data to be classified are presented as a vector of features at the input layer,and the output layer has three nodes, representing the three possible classes for thedata: ?first has wide scope,?
?second has wide scope,?
and ?no scope interaction.
?The output node with the highest activation is interpreted as the class of the datumpresented at the input layer.For each of the 10 test sets of 89 examples, we trained the connection weightsof the network using error backpropagation on 654 training sentences, reserving 150sentences for validation in order to choose the weights from the training epoch with thehighest classification performance.
In Table 7 we present the results of the single-layerneural network in classifying our test sentences.
As the table shows, the single-layerperceptron has much better classification performance than the naive Bayes classifierand maximum-entropy model, possibly because the training of the network aims tominimize error in the activation of the classification output nodes, which is directlyrelated to the classification task at hand, whereas the other models do not directlymake use of the notion of ?classification error.?
The perceptron also uses a sort ofweighted voting and could be interpreted as an implementation of Kuno, Takami,and Wu?s proposal for scope determination.
This clearly illustrates that the tenabilityof their proposal hinges on the exact details of its implementation, since all of ourclassifier models are reasonable interpretations of their approach, but they have verydifferent performance results on our scope determination task.To determine exactly which features of the data the network sees as relevant tothe classification problem, we can simply look at the connection weights for eachfeature-class pair.
Higher connection weights indicate a greater correlation betweeninput features and output classes.
For one of the 10 networks we trained, some ofthe features with the highest connection weights are listed in Table 8.
Since class 0 is87Higgins and Sadock Modeling Scope PreferencesTable 8Most active features from single-layer perceptron.Rank Feature Predicted Weightclass1 There is an intervening comma 0 4.312 Second quantifier is all 0 3.773 There is an intervening colon 0 2.984 There is an intervening conjunct node 0 2.7217 The first quantified NP c-commands the second 1 1.6918 Second quantifier is tagged RBS 2 1.6919 There is an intervening S node 1 1.6120 Second quantifier is each 2 1.50simply more frequent in the training data than the other two classes, the weights forthis class tend to be higher.
Therefore, we also list some of the best predictor featuresfor classes 1 and 2 in the table.The first- and third-ranked features in Table 8 show that an intervening comma orcolon will make the classifier tend toward class 0, ?no scope interaction.?
This findingby the classifier is similar to the maximum-entropy classifier?s finding an interveningquotation mark relevant and can be taken as an indication that quantifiers in distantsyntactic subdomains are unlikely to interact.
Similarly, the fourth-ranked feature indi-cates that quantifiers in separate conjuncts are unlikely to interact.
The second-rankedfeature in the table expresses a tendency for there to be no scope interaction betweentwo quantifiers if the second of them is headed by all.
This may be related to theindependence of universal quantifiers (?x?y[?]
??
?y?x[?]).
Feature 17 in Table 8indicates a high correlation between the first quantified expression?s c-commandingthe second and the first quantifier?s taking wide scope, which again supports Kuno,Takami, and Wu?s claim that scope preferences are related to syntactic superiority re-lations.
Feature 18 expresses a preference for a quantified expression headed by mostto take wide scope, even if it is the second of the two quantifiers (since most is theonly quantifier in the corpus that bears the tag RBS).
Feature 19 indicates that thefirst quantifier is more likely to take wide scope if there is a clause boundary in-tervening between the two quantifiers, which supports the notion that the syntacticdistance between the quantifiers is relevant to scope preferences.
Finally, feature 20expresses the well-known tendency for quantified expressions headed by each to takewide scope.4.4 Summary of ResultsTable 9 summarizes the performance of the quantifier scope models we have presentedhere.
All of the classifiers have test set accuracy above the baseline, which a pairedt-test reveals to be significant at the .001 level.
The differences between the naiveBayes, maximum-entropy, and single-layer perceptron classifiers are not statisticallysignificant.The classifiers performed significantly better on those sentences annotated consis-tently by both human coders at the beginning of the study, reinforcing the view thatthis subset of the data is somehow simpler and more representative of the basic regu-larities in scope preferences.
For example, the single-layer perceptron classified 82.9%of these sentences correctly.
To further investigate the nature of the variation betweenthe two coders, we constructed a version of our single-layer network that was trained88Computational Linguistics Volume 29, Number 1Table 9Summary of classifier results.Training data Validation data Test dataBaseline ?
?
61.2%Na?
?ve Bayes 76.7% ?
72.6%Maximum entropy 78.3% 75.5% 73.5%Single-layerperceptron 84.7% 76.8% 77.0%on the data on which both coders agreed and tested on the remaining sentences.
Thisclassifier agreed with the reference coding (the coding of the first coder) 51.4% of thetime and with the additional independent coder 35.8% of the time.
The first coder con-structed the annotation guidelines for this project and may have been more successfulin applying them consistently.
Alternatively, it is possible that different individuals usedifferent strategies in determining scope preferences, and the strategy of the secondcoder may simply have been less similar than the strategy of the first coder to that ofthe single-layer network.These three classifiers directly implement a sort of weighted voting, the methodof aggregating evidence proposed by Kuno, Takami, and Wu (although the classifiers?implementation is slightly more sophisticated than the unweighted voting that is ac-tually used in Kuno, Takami, and Wu?s paper).
Of course, since we do not use exactlythe set of features suggested by Kuno, Takami, and Wu, our model should not beseen as a straightforward implementation of the theory outlined in their 1999 paper.Nevertheless, the results in Table 9 suggest that Kuno, Takami, and Wu?s suggesteddesign can be used with some success in modeling scope preferences.
Moreover, theproject undertaken here provides an answer to some of the objections that Aoun andLi (2000) raise to Kuno, Takami, and Wu.
Aoun and Li claim that Kuno, Takami, andWu?s choice of experts is seemingly arbitrary and that it is unclear how the votingweights of each expert are to be set, but the machine learning approach we employin this article is capable of addressing both of these potential problems.
Supervisedtraining of our classifiers is a straightforward approach to setting the weights andalso constitutes our approach to selecting features (or ?experts?
in Kuno, Takami, andWu?s terminology).
In the training process, any feature that is irrelevant to scopingpreferences should receive weights that make its effect negligible.5.
Syntax and ScopeIn this section, we show how the classifier models of quantifier scope determinationintroduced in Section 4 may be integrated with a PCFG model of syntax.
We com-pare two different ways in which the two components may be combined, which mayloosely be termed serial and parallel, and argue for the latter on the basis of empiricalresults.5.1 Modular DesignOur use of a phrase structure syntactic component and a quantifier scope componentto define a combined language model is simplified by the fact that our classifiers areprobabilistic and define a conditional probability distribution over quantifier scopings.The probability distributions that our classifiers define for quantifier scope structuresare conditional on syntactic phrase structure, because they are computed on the basis89Higgins and Sadock Modeling Scope Preferencesof syntactically provided features, such as the number of nodes of a certain type thatintervene between two quantifiers in a phrase structure tree.Thus, the combined language model that we define in this article assigns probabil-ities according to the pairs of structures that may be assigned to a sentence by the Q-structure and phrase structure syntax modules.
The probability of a word string w1?nis therefore defined as in equation (4), where Q ranges over all possible Q-structuresin the set Q and S ranges over all possible syntactic structures in the set S.P(w1?n) =?S?S,Q?QP(S, Q | w1?n) (4)=?S?S,Q?QP(S | w1?n)P(Q | S, w1?n) (5)Equation (5) shows how we can use the definition of conditional probability tobreak our calculation of the language model probability into two parts.
The first ofthese parts, P(S | w1?n), which we may abbreviate as simply P(S), is the probabilityof a particular syntactic tree structure?s being assigned to a particular word string.
Wemodel this probability using a probabilistic phrase structure grammar (cf.
Charniak[1993, 1996]).
The second distribution on the right side of equation (5) is the conditionalprobability of a particular quantifier scope structure?s being assigned to a particularword string, given the syntactic structure of that string.
This probability is written asP(Q | S, w1?n), or simply P(Q | S), and represents the quantity we estimated abovein constructing classifiers to predict the scopal representation of a sentence based onaspects of its syntactic structure.Thus, given a PCFG model of syntactic structure and a probabilistically definedclassifier of the sort introduced in Section 4, it is simple to determine the probabilityof any pairing of two particular structures from each domain for a given sentence.We simply multiply the values of P(S) and P(Q | S) to obtain the joint probabilityP(Q, S).
In the current section, we examine two different models of combination forthese components: one in which scope determination is applied to the optimal syn-tactic structure (the Viterbi parse), and one in which optimization is performed in thespace of both modules to find the optimal pairing of syntactic and quantifier scopestructures.5.2 The Syntactic ModuleBefore turning to the application of our multimodular approach to the problem ofscope determination in Section 5.3, we present here a short overview of the phrasestructure syntactic component used in these projects.
As noted above, we model syn-tax as a probabilistic phrase structure grammar (PCFG), and in particular, we use atreebank grammar (Charniak 1996) trained on the Penn Treebank.A PCFG defines the probability of a string of words as the sum of the probabilitiesof all admissible phrase structure parses (trees) for that string.
The probability of agiven tree is the product of the probability of all of the rule instances used in theconstruction of that tree, where rules take the form N ?
?, with N a nonterminalsymbol and ?
a finite sequence of one or more terminals or nonterminals.To take an example, Figure 5 illustrates a phrase structure tree for the sentence Su-san might not believe you, which is admissible according to the grammar in Table 10.
(Allof the minimal subtrees in Figure 5 are instances of one of our rules.)
The probability90Computational Linguistics Volume 29, Number 1Figure 5A simple phrase structure tree.Table 10A simple probabilistic phrase structure grammar.Rule ProbabilityS ?
NP VP .7S ?
VP .2S ?
V NP VP .1VP ?
V VP .3VP ?
ADV VP .1VP ?
V .1VP ?
V NP .3VP ?
V NP NP .2NP ?
Susan .3NP ?
you .4NP ?
Yves .3V ?
might .2V ?
believe .3V ?
show .3V ?
stay .2ADV ?
not .5ADV ?
always .591Higgins and Sadock Modeling Scope Preferencesof this tree, which we can indicate as ?
, can be calculated as in equation (6).P(?)
=???Rules(?)P(?)
(6)= P(S ?
NP VP) ?
P(VP ?
V VP) ?
P(VP ?
ADV VP)?
P(VP ?
V NP) ?
P(NP ?
Susan) ?
P(V ?
might)?
P(ADV ?
not) ?
P(V ?
believe) ?
P(NP ?
you) (7)= .7 ?
.3 ?
.1 ?
.3 ?
.3 ?
.2 ?
.5 ?
.3 ?
.4 = 2.268 ?
10?5 (8)The actual grammar rules and associated probabilities that we use in defining oursyntactic module are derived from the WSJ corpus of the Penn Treebank by maximum-likelihood estimation.
That is, for each rule N ?
?
used in the treebank, we add therule to the grammar and set its probability to C(N??)??C(N??)
, where C(?)
denotes the?count?
or a rule (i.e., the number of times it is used in the corpus).
A grammarcomposed in this manner is referred to as a treebank grammar, because its rules aredirectly derived from those in a treebank corpus.We used sections 00?20 of the WSJ corpus of the Penn Treebank for collecting therules and associated probabilities of our PCFG, which is implemented as a bottom-upchart parser.
Before constructing the grammar, the treebank was preprocessed usingknown procedures (cf.
Krotov et al [1998]; Belz [2001]) to facilitate the construction ofa rule list.
Functional and anaphoric annotations (basically anything following a ?-?in a node label; cf.
Santorini [1990]; Bies et al [1995]) were removed from nonterminallabels.
Nodes that dominate only ?empty categories?
such as traces were removed.In addition, unary-branching constructions were removed by replacing the mothercategory in such a structure with the daughter node.
(For example, given an instanceof the rule X ?
YZ, if the daughter category Y were expanded by the unary ruleY ?
W, our algorithm would induce the single rule X ?
WZ.)
Finally, we discardedall rules that had more than 10 symbols on the right-hand side (an arbitrary limit ofour parser implementation).
This resulted in a total of 18,820 rules, of which 11,156were discarded as hapax legomena, leaving 7,664 rules in our treebank grammar.Table 11 shows some of the rules in our grammar with the highest and lowest corpuscounts.5.3 Unlabeled Scope DeterminationIn this section, we describe an experiment designed to assess the performance ofparallel and serial approaches to combining grammatical modules, focusing on thetask of unlabeled scope determination.
This task involves predicting the most likelyQ-structure representation for a sentence, basically the same task we addressed in Sec-tion 4, in comparing the performance levels of each type of classifier.
The experimentof this section differs, however, from the task presented in Section 4 in that instead ofproviding a syntactic tree from the Penn Treebank as input to the classifier, we providethe model only with a string of words (a sentence).
Our dual-component model willsearch for the optimal syntactic and scopal structures for the sentence (the pairing(??,??))
and will be evaluated based on its success in identifying the correct scopereading ?
?.Our concern in this section will be to determine whether it is necessary to searchthe space of possible pairings (?
,?)
of syntactic and scopal structures or whetherit is sufficient to use our PCFG first to fix the syntactic tree ?
, and then to choose92Computational Linguistics Volume 29, Number 1Table 11Rules derived from sections 00?20 of the Penn Treebank WSJ corpus.
?TOP?
is a special ?start?symbol that may expand to any of the symbols found at the root of a tree in the corpus.Rule Corpus countPP ?
IN NP 59,053TOP ?
S 34,614NP ?
DT NN 28,074NP ?
NP PP 25,192S ?
NP VP 14,032S ?
NP VP .
12,901VP ?
TO VP 11,598...S ?
CC PP NNP NNP VP .
2NP ?
DT ?
NN NN NN ??
2NP ?
NP PP PP PP PP PP 2INTJ ?
UH UH 2NP ?
DT ?
NN NNS 2SBARQ ?
?
WP VP .
??
2S ?
PP NP VP .
??
2a scope reading to maximize the probability of the pairing.
That is, are syntax andquantifier scope mutually dependent components of grammar, or can scope relationsbe ?read off of?
syntax?
The serial model suggests that the optimal syntactic structure??
should be chosen on the basis of the syntactic module only, as in equation (9),and the optimal quantifier scope structure ??
then chosen on the basis of ?
?, as inequation (10).
The parallel model, on the other hand, suggests that the most likelypairing of structures must be chosen in the joint probability space of both components,as in equation (11).??
= arg max??SPS(?
| w1?n) (9)??
= arg max??QPQ(?
| ?
?, w1?n) (10)??
= { ?
| (?
,?)
= arg max??S,??QPS(?
| w1?n)PQ(?
| ?
, w1?n) } (11)5.3.1 Experimental Design.
For this experiment, we implement the scoping compo-nent as a single-layer feed-forward network, because the single-layer perceptron clas-sifier had the best prediction rate among the three classifiers tested in Section 4.
Thesoftmax activation function we use for the output nodes of the classifier guaranteesthat the activations of all of the output nodes sum to one and can be interpreted asclass probabilities.
The syntactic component, of course, is determined by the treebankPCFG grammar described above.Given these two models, which respectively define PQ(?
| ?
, w1?n) and PS(?
| w1?n)from equation (11), it remains only to specify how to search the space of pairings(?
,?)
in performing this optimization to find ??.
Unfortunately, it is not feasible toexamine all values ?
?
S, since our PCFG will generally admit a huge number of93Higgins and Sadock Modeling Scope PreferencesTable 12Performance of models on the unlabeled scope prediction task, summed over all 10 test runs.Condition Correct Incorrect Percentage correctParallel modelFirst has wide scope 168 113 167/281 = 59.4%Second has wide scope 26 38 26/64 = 40.6%No scope interaction 467 78 467/545 = 85.7%Total 661 229 661/890 = 74.3%Serial modelFirst has wide scope 163 118 163/281 = 58.0%Second has wide scope 27 37 27/64 = 42.2%No scope interaction 461 84 461/545 = 84.6%Total 651 239 651/890 = 73.1%trees for a sentence (especially given a mean sentence length of over 20 words inthe WSJ corpus).12 Our solution to this search problem is to make the simplifying as-sumption that the syntactic tree that is used in the optimal set of structures (??,??
)will always be among the top few trees ?
for which PS(?
| w1?n) is the greatest.That is, although we suppose that quantifier scope information is relevant to pars-ing, we do not suppose that it is so strong a determinant as to completely over-ride syntactic factors.
In practice, this means that our parser will return the top 10parses for each sentence, along with the probabilities assigned to them, and theseare the only parses that are considered in looking for the optimal set of linguisticstructures.We again used 10-fold cross-validation in evaluating the competing models, di-viding the scope-tagged corpus into 10 test sections of 89 sentences each, and weused the same version of the treebank grammar for our PCFG.
The first model re-trieved the top 10 syntactic parses (?0 ?
?
?
?9) for each sentence and computed theprobability P(?
,?)
for each ?
?
?0 ?
?
?
?9,?
?
0, 1, 2, choosing that scopal represen-tation ?
that was found in the maximum-probability pairing.
We call this the par-allel model, because the properties of each probabilistic model may influence theoptimal structure chosen by the other.
The second model retrieved only the Viterbiparse ?0 from the PCFG and chose the scopal representation ?
for which the pair-ing (?0,?)
took on the highest probability.
We call this the serial model, because itrepresents syntactic phrase structure as independent of other components of gram-mar (in this case, quantifier scope), though other components are dependentupon it.5.3.2 Results.
There was an appreciable difference in performance between these twomodels on the quantifier scope test sets.
As shown in Table 12, the parallel modelnarrowly outperformed the serial model, by 1.2%.
A 10-fold paired t-test on the testsections of the scope-tagged corpus shows that the parallel model is significantly better(p < .05).12 Since we are allowing ?
to range only over the three scope readings (0, 1, 2), however, it is possible toenumerate all values of ?
to be paired with a given syntactic tree ?
.94Computational Linguistics Volume 29, Number 1This result suggests that, in determining the syntactic structure of a sentence, wemust take aspects of structure into account that are not purely syntactic (such as quanti-fier scope).
Searching both dimensions of the hypothesis space for our dual-componentmodel allowed the composite model to handle the interdependencies between differ-ent aspects of grammatical structure, whereas fixing a phrase structure tree purelyon the basis of syntactic considerations led to suboptimal performance in using thatstructure as a basis for determining quantifier scope.6.
ConclusionIn this article, we have taken a statistical, corpus-based approach to the modelingof quantifier scope preferences, a subject that has previously been addressed onlywith systems of ad hoc rules derived from linguists?
intuitive judgments.
Our modeltakes its theoretical inspiration from Kuno, Takami, and Wu (1999), who suggest an?expert system?
approach to scope preferences, and follows many other projects in themachine learning of natural language that combine information from multiple sourcesin solving linguistic problems.0ur results are generally supportive of the design that Kuno, Takami, and Wu pro-pose for the quantifier scope component of grammar, and some of the features inducedby our models find clear parallels in the factors that Kuno, Takami, and Wu claim tobe relevant to scoping.
In addition, our final experiment, in which we combine ourquantifier scope module with a PCFG model of syntactic phrase structure, providesevidence of a grammatical architecture in which different aspects of structure mutu-ally constrain one another.
This result casts doubt on approaches in which syntacticprocessing is completed prior to the determination of other grammatical properties ofa sentence, such as quantifier scope relations.Appendix: Selected Codes Used to Annotate Syntactic Categories in the Penn Tree-bank, from Marcus et al (1993) and Bies et al (1995)Part-of-speech tagsTag Meaning Tag MeaningCC Conjunction RB AdverbCD Cardinal number RBR Comparative adverbDT Determiner RBS Superlative adverbIN Preposition TO ?to?JJ Adjective UH InterjectionJJR Comparative adjective VB Verb in base formJJS Superlative adjective VBD Past-tense verbNN Singular or mass noun VBG Gerundive verbNNS Plural noun VBN Past participial verbNNP Singular proper noun VBP Non-3sg, present-NNPS Plural proper noun tense verbPDT Predeterminer VBZ 3sg, present-tenseverbPRP Personal pronoun WP WH pronounPRP$ Possessive pronoun WP$ Possessive WH pronoun95Higgins and Sadock Modeling Scope PreferencesPhrasal categoriesCode Meaning Code MeaningADJP Adjective phrase SBAR Clause introduced byADVP Adverb phrase a subordinatingINTJ Interjection conjunctionNP Noun phrase SBARQ Clause introduced byPP Prepositional phrase a WH phraseQP Quantifier phrase (i.e., SINV Inverted declarativemeasure/amount sentencephrase) SQ Inverted yes/noS Declarative clause question following theWH phrase in SBARQVP Verb phraseAcknowledgmentsThe authors are grateful for an AcademicTechnology Innovation Grant from theUniversity of Chicago, which helped tomake this work possible, and to JohnGoldsmith, Terry Regier, Anne Pycha, andBob Moore, whose advice and collaborationhave considerably aided the researchreported in this article.
Any remainingerrors are, of course, our own.ReferencesAoun, Joseph and Yen-hui Audrey Li.
1993.The Syntax of Scope.
MIT Press, Cambridge.Aoun, Joseph and Yen-hui Audrey Li.
2000.Scope, structure, and expert systems: Areply to Kuno et al Language,76(1):133?155.Belz, Anja.
2001.
Optimisation ofcorpus-derived probabilistic grammars.
InProceedings of Corpus Linguistics 2001,pages 46?57.Berger, Adam L., Stephen A. Della Pietra,and Vincent J. Della Pietra.
1996.
Amaximum entropy approach to naturallanguage processing.
ComputationalLinguistics, 22(1):39?71.Bies, Ann, Mark Ferguson, Karen Katz, andRobert MacIntyre.
1995.
Bracketingguidelines for Treebank II style.
Technicalreport, Penn Treebank Project, Universityof Pennsylvania.Bishop, Christopher M. 1995.
NeuralNetworks for Pattern Recognition.
OxfordUniversity Press, Oxford.Bridle, John S. 1990.
Probabilisticinterpretation of feedforwardclassification network outputs withrelationships to statistical patternrecognition.
In F. Fougelman-Soulie andJ.
Herault, editors, Neurocomputing?Algorithms, Architectures, and Applications.Springer-Verlag, Berlin, pages 227?236.Brill, Eric.
1995.
Transformation-basederror-driven learning and naturallanguage processing: A case study inpart-of-speech tagging.
ComputationalLinguistics, 21(4):543?565.Carden, Guy.
1976.
English Quantifiers:Logical Structure and Linguistic Variation.Academic Press, New York.Carletta, Jean.
1996.
Assessing agreement onclassification tasks: The kappa statistic.Computational Linguistics, 22(2):249?254.Charniak, Eugene.
1993.
Statistical languagelearning.
MIT Press, Cambridge.Charniak, Eugene.
1996.
Tree-bankgrammars.
In AAAI/IAAI, vol.
2,pages 1031?1036.Cohen, Jacob.
1960.
A coefficient ofagreement for nominal scales.
Educationaland Psychological Measurement, 20:37?46.Cooper, Robin.
1983.
Quantification andSyntactic Theory.
Reidel, Dordrecht.Fleiss, Joseph L. 1981.
Statistical Methods forRates and Proportions.
John Wiley & Sons,New York.Hobbs, Jerry R. and Stuart M. Shieber.
1987.An algorithm for generating quantifierscopings.
Computational Linguistics,13:47?63.Hornstein, Norbert.
1995.
Logical Form: FromGB to Minimalism.
Blackwell, Oxford andCambridge.Jelinek, Frederick.
1998.
Statistical Methods forSpeech Recognition.
MIT Press, Cambridge.Jurafsky, Daniel and James H. Martin.
2000.Speech and Language Processing.
PrenticeHall, Upper Saddle River, New Jersey.Krippendorff, Klaus.
1980.
Content Analysis:An Introduction to Its Methodology.
Sage96Computational Linguistics Volume 29, Number 1Publications, Beverly Hills, California.Krotov, Alexander, Mark Hepple, Robert J.Gaizauskas, and Yorick Wilks.
1998.Compacting the Penn treebank grammar.In COLING-ACL, pages 699?703.Kuno, Susumu, Ken-Ichi Takami, and YuruWu.
1999.
Quantifier scope in English,Chinese, and Japanese.
Language,75(1):63?111.Kuno, Susumu, Ken-Ichi Takami, and YuruWu.
2001.
Response to Aoun and Li.Language, 77(1):134?143.Manning, Christopher D. and HinrichSchu?tze.
1999.
Foundations of StatisticalNatural Language Processing.
MIT Press,Cambridge.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313?330.Martin, Paul, Douglas Appelt, andFernando Pereira.
1986.
Transportabilityand generality in a natural-languageinterface system.
In B. J. Grosz, K. SparckJones, and B. L. Webber, editors, NaturalLanguage Processing.
Kaufmann, Los Altos,California, pages 585?593.May, Robert.
1985.
Logical Form: Its Structureand Derivation.
MIT Press, Cambridge.McCawley, James D. 1998.
The SyntacticPhenomena of English.
University ofChicago Press, Chicago, second edition.Mitchell, Tom M. 1996.
Machine Learning.McGraw Hill, New York.Montague, Richard.
1973.
The propertreatment of quantification in ordinaryEnglish.
In J. Hintikka et al, editors,Approaches to Natural Language.
Reidel,Dordrecht, pages 221?242.Moran, Douglas B.
1988.
Quantifier scopingin the SRI core language engine.
InProceedings of the 26th Annual Meeting of theAssociation for Computational Linguistics(ACL?88), pages 33?40.Moran, Douglas B. and Fernando C. N.Pereira.
1992.
Quantifier scoping.
InHiyan Alshawi, editor, The Core LanguageEngine.
MIT Press, Cambridge,pages 149?172.Nerbonne, John.
1993.
A feature-basedsyntax/semantics interface.
InA.
Manaster-Ramer and W. Zadrozsny,editors, Annals of Mathematics and ArtificialIntelligence (Special Issue on Mathematics ofLanguage), 8(1?2):107?132.
Also publishedas DFKI Research Report RR-92-42.Park, Jong C. 1995.
Quantifier scope andconstituency.
In Proceedings of the 33rdAnnual Meeting of the Association forComputational Linguistics (ACL?95),pages 205?212.Pedersen, Ted.
2000.
A simple approach tobuilding ensembles of na?
?ve Bayesianclassifiers for word sense disambiguation.In Proceedings of the First Meeting of theNorth American Chapter of the Association forComputational Linguistics (NAACL 2000),pages 63?69.Pereira, Fernando.
1990.
Categorialsemantics and scoping.
ComputationalLinguistics, 16(1):1?10.Pollard, Carl.
1989.
The syntax-semanticsinterface in a unification-based phrasestructure grammar.
In S. Busemann,C.
Hauenschild, and C. Umbach, editors,Views of the Syntax/Semantics InterfaceKIT-FAST Report 74.
Technical Universityof Berlin, pages 167?185.Pollard, Carl and Eun Jung Yoo.
1998.
Aunified theory of scope for quantifiersand WH-phrases.
Journal of Linguistics,34(2):415?446.Ratnaparkhi, Adwait.
1997.
A simpleintroduction to maximum entropy modelsfor natural language processing.
TechnicalReport 97-08, Institute for Research inCognitive Science, University ofPennsylvania.Santorini, Beatrice.
1990.
Part-of-speechtagging guidelines for the Penn Treebankproject.
Technical Report MS-CIS-90-47,Department of Computer and InformationScience, University of Pennsylvania.Soon, Wee Meng, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machinelearning approach to coreferenceresolution of noun phrases.
ComputationalLinguistics, 27(4):521?544.van Halteren, Hans, Jakub Zavrel, andWalter Daelemans.
2001.
Improvingaccuracy in word class tagging throughthe combination of machine learningsystems.
Computational Linguistics,27(2):199?229.VanLehn, Kurt A.
1978.
Determining thescope of English quantifiers.
TechnicalReport AITR-483, Massachusetts Instituteof Technology Artificial IntelligenceLaboratory, Cambridge.Woods, William A.
1986.
Semantics andquantification in natural languagequestion answering.
In B. J. Grosz,K.
Sparck Jones, and B. L. Webber, editors,Natural Language Processing.
Kaufmann,Los Altos, California, pages 205?248.
