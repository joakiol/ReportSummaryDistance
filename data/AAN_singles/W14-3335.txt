Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 287?292,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsDependency-based Automatic Enumeration of Semantically EquivalentWord Orders for Evaluating Japanese TranslationsHideki Isozaki, Natsume KouchiOkayama Prefectural University111 Kuboki, Soja-shi, Okayama, 719-1197, Japanisozaki@cse.oka-pu.ac.jpTsutomu HiraoNTT Communication Science Laboratories2-4, Hikaridai, Seika-cho, Sorakugun, Kyoto, 619-0237, Japanhirao.tsutomu@lab.ntt.co.jpAbstractScrambling is acceptable reordering ofverb arguments in languages such asJapanese and German.
In automatic eval-uation of translation quality, BLEU isthe de facto standard method, but BLEUhas only very weak correlation with hu-man judgements in case of Japanese-to-English/English-to-Japanese translations.Therefore, alternative methods, IMPACTand RIBES, were proposed and they haveshown much stronger correlation thanBLEU.
Now, RIBES is widely used inrecent papers on Japanese-related transla-tions.
RIBES compares word order of MToutput with manually translated referencesentences but it does not regard scram-bling at all.
In this paper, we present amethod to enumerate scrambled sentencesfrom dependency trees of reference sen-tences.
Our experiments based on NTCIRPatent MT data show that the method im-proves sentence-level correlation betweenRIBES and human-judged adequacy.1 IntroductionStatistical Machine Translation has grown with anautomatic evaluation method BLEU (Papineni etal., 2002).
BLEU measures local word order by n-grams and does not care about global word order.In JE/EJ translations, this insensitivity degradesBLEU?s correlation with human judgements.Therefore, alternative automatic evaluationmethods are proposed.
Echizen-ya and Araki(2007) proposed IMPACT.
Isozaki et al.
(2010)presented the idea of RIBES.
Hirao et al.
(2011)named this method ?RIBES?
(Rank-based Intu-itive Bilingual Evaluation Score).
This version ofRIBES was defined as follows:RIBES = NKT?
P?Table 1: Meta-evaluation of NTCIR-7 JE task data(Spearman?s ?, System-level correlation)BLEU METEOR ROUGE-L IMPACT RIBES0.515 0.490 0.903 0.826 0.947where NKT (Normalized Kendall?s ? )
is definedby (?
+ 1)/2.
This NKT is used for measur-ing word order similarity between a reference sen-tence and an MT output sentence.
Thus, RIBESpenalizes difference of global word order.
P isprecision of unigrams.
RIBES is defined for eachtest sentence and averaged RIBES is used for eval-uating the entire test corpus.Table 1 is a table in an IWSLT-2012 invitedtalk (http://hltc.cs.ust.hk/iwslt/slides/Isozaki2012 slides.pdf).
METEOR was pro-posed by Banerjee and Lavie (2005).
ROUGE-Lwas proposed by Lin and Och (2004).
Accordingto this table, RIBES with ?
= 0.2 has a verystrong correlation (Spearman?s ?
= 0.947) withhuman-judged adequacy.
For each sentence,we use the average of adequacy scores of threejudges.
Here, we call this average ?Adequacy?.We focus on Adequacy because current SMTsystems tend to output inadequate sentences.Note that only single reference translations areavailable for this task although use of multiplereferences is common for BLEU.RIBES is publicly available from http://www.kecl.ntt.co.jp/icl/lirg/ribes/ andwas used as a standard quality measure in recentNTCIR PatentMT tasks (Goto et al., 2011; Gotoet al., 2013).
Table 2 shows the result of meta-evaluation at NICTR-9/10 PatentMT.
The tableshows that RIBES is more reliable than BLEUand NIST.Current RIBES has the following improve-ments.?
BLEU?s Brevity Penalty (BP) was introduced287Table 2: Meta-evaluation at NTCIR-9/10PatentMT (Spearman?s ?, Goto et al.
2011, 2013)BLEU NIST RIBESNTCIR-9 JE ?0.042 ?0.114 0.632NTCIR-9 EJ ?0.029 ?0.074 0.716NTCIR-10 JE 0.31 0.36 0.88NTCIR-10 EJ 0.36 0.22 0.79in order to penalize too short sentences.RIBES = NKT?
P??
BP?where ?
= 0.25 and ?
= 0.10.
BLEU usesBP for the entire test corpus, but RIBES usesit for each sentence.?
The word alignment algorithm in the originalRIBES used only bigrams for disambiguationwhen the same word appears twice or morein one sentence.
This restriction is now re-moved, and longer n-grams are used to get abetter alignment.RIBES is widely used in recent Annual Mee-ings of the (Japanese) Association for NLP.
In-ternational conference papers on Japanese-relatedtranslations also use RIBES.
(Wu et al., 2012;Neubig et al., 2012; Goto et al., 2012; Hayashiet al., 2013).
Dan et al.
(2012) uses RIBES forChinese-to-Japanese translation.However, we have to take ?scrambling?
intoaccount when we think of Japanese word order.Scrambling is also observed in other languagessuch as German.
Current RIBES does not regardthis fact.2 MethodologyFor instance, a Japanese sentence S1jon ga sushi-ya de o-sushi wo tabe-ta .
(John ate sushi at a sushi restaurant.
)has the following acceptable word orders.1.
jon ga sushi-ya de o-sushi wo tabe-ta .2. jon ga o-sushi wo sushi-ya de tabe-ta .3. sushi-ya de jon ga o-sushi wo tabe-ta .4. sushi-ya de o-sushi wo jon ga tabe-ta .5. o-sushi wo jon ga sushi-ya de tabe-ta .6. o-sushi wo sushi-ya de jon ga tabe-ta .The boldface short words ?ga?, ?de?, and?wo?, are case markers (?Kaku joshi?
inJapanese).tabe-tasushi-ya dejon gao-sushi woFigure 1: Dependency Tree of S1?
?ga?
is a nominative case marker that meansthe noun phrase before it is the subject of afollowing verb/adjective.?
?de?
is a locative case marker that means thenoun phrase before it is the location of a fol-lowing verb/adjective.?
?wo?
is an accusative case marker that meansthe noun phrase before it is the direct objectof a following verb.The term ?scrambling?
stands for these accept-able permutations.
These case markers explicitlyshow grammatical cases and reordering of themdoes not hurt interpretation of these sentences.
Al-most all other permutations of words are not ac-ceptable (?).?
jon ga de sushi-ya o-sushi tabe-ta wo .?
jon de sushi-ya ga o-sushi wo tabe-ta .?
jon tabe-ta ga o-sushi wo sushi-ya de .?
sushi-ya ga jon tabe-ta de o-sushi wo .Most readers unfamiliar with Japanese will notunderstand which word order is acceptable.2.1 Scrambling as Post-Order Traversal ofDepenedncy TreesHere, we describe this ?scrambling?
from theviewpoint of Computer Science.
Figure 1 showsS1?s dependency tree.
Each box indicates a ?bun-setsu?
or a grammatical chunk of words.
Each ar-row starts from a modifier (dependent) to its head.The root of S1 is ?tabe-ta?
(ate).
This verbhas three modifiers:?
?jon ga?
(John is its subject)?
?sushi-ya de?
(A sushi restaurant is its location)?
?o-sushi wo?
(Sushi is its object)It is well known that Japanese is a typical head-final language.
In order to generate a head-finalword order from this dependency tree, we shouldoutput tree nodes in post-order.
That is, we haveto output all children of a node N before the nodeN itself.288mi-taato ni kabuki wotabe-tasushi-ya dejon gao-sushi woFigure 2: Dependency Tree of S2All of the above acceptable word orders followsthis post-order.
Even in post-order traverse, prece-dence among children is not determined and thisfact leads to different permutations of children.
Inthe above example, the root ?tabe-ta?
has threechildren, and its permutation is 3!
= 6.2.2 Simple Case Marker ConstraintFigure 2 shows the dependency tree of a morecomplicated sentence S2:jon ga sushi-ya de o-sushi wo tabe-taato ni kabuki wo mi-ta .
(John watched kabuki after eating sushi at a shushirestaurant)Kabuki is a traditional Japanese drama performedin a theatre.
In this case, the root ?mi-ta?
(watched) has two children: ?ato ni?
(after it)and ?kabuki wo?
(kabuki is its object).?
?ni?
is a dative/locative case marker thatmeans the noun phrase before it is an indi-rect object or a location/time of a followingverb/adjective.In this case, we obtain 3!?2!
= 12 permutations:1.
*S1P* ato ni kabuki wo mi-ta .2. kabuki wo *S1P* ato ni mi-ta .Here, *S1P* is any of the above 3!
permutationsof S1.
If we use S1?s 3 as *S1P* in S2?s 1, we getsushi-ya de jon ga o-sushi wo tabe-taato ni kabuki wo mi-ta .However, we cannot accept all of these permu-tations equally.
For instance,kabuki wo o-sushi wo sushi-ya dejon ga tabe-ta ato ni mi-ta .is comprehensible but strange.
This strangnesscomes from the two objective markers ?wo?
be-fore the first verb ?tabe-ta.?
Which did Johneat, kabuki or sushi?
Semantically, we cannoteat kabuki (drama), and we can understand thissentence.
But syntactic ambiguity causes thisstrangeness.
Without semantic knowledge aboutkabuki and sushi, we cannot disambiguate thiscase.For readers/listeners, we should avoid suchsyntactically ambiguous sentences.
Modifiers(here, ?kabuki wo?)
of a verb (here, ?mi-ta?,watched) should not be placed before another verb(here, ?tabe-ta?, ate).In Japanese, verbs and adjectives are used sim-ilarly.
In general, adjectives are not modified by?wo?
case markers.
Therefore, we can place ?wo?case markers before adjectives.
In the followingsentences, ?atarashii?
(new) is an adjectiveand placing ?inu wo?
(A dog is the direct object)before ?atarashii?
does not make the sentenceambiguous.?
atarashii ie ni inu wo ture te itta .
((Someone) took the dog to the new house.)?
inu wo atarashii ie ni ture te itta .This idea leads to the following Simple CaseMarker Constraint:Definition 1 (Simple Case Marker Constraint)If a reordered sentence has a case marker phraseof a verb that precedes another verb before theverb, the sentence is rejected.
?wo?
case markerscan precede adjectives before the verb.This is a primitive heuristic constraint and theremust be better ways to make it more flexible.If we use Nihongo Goi Taikei (Ikehara et al.,1997), we will be able to implement such a flex-isble constraint.
For example, some verbs suchas ?sai-ta?
(bloomed) are never modified by?wo?
case marker phrases.
Therefore, the follow-ing sentence is not ambiguous at all although thewo phrase precedes ?sai-ta?.?
hana ga sai-ta ato ni sono ki wo mi-ta.
((Someone) saw the tree after it bloomed.)?
sono ki wo hana ga sai-ta ato ni mi-ta.2.3 Evaluation with scrambled sentencesAs we mentioned before, RIBES measures globalword order similarity between machine-translatedsentences and reference sentences.
It does not re-gard scrambling at all.
When the target languageallows scrambling just like Japanese, RIBESshould consider scrambling.Once we have a correct dependency tree of thereference sentence, we enumerate scrambled sen-tences by reordering children of each node.
The289number of the reordered sentences depend on thestructure of the dependency tree.Current RIBES code (RIBES-1.02.4) assumesthat every sentence has a fixed number of refer-ences, but here the number of automatically gen-erated reference sentences depends on the depen-dency structure of the original reference sentence.Therefore, we modified the code for variable num-bers of reference sentences.
RIBES-1.02.4 simplyuses the maximum value of the scores for differentreference sentences, and we followed it.Here, we compare the following four methods.?
single: We use only single reference transla-tions provided by the NTCIR organizers.?
postOrder: We generate all permutations ofthe given reference sentence generated bypost-order traversals of its dependency tree.This can be achieved by the following twosteps.
First, we enumerate all permutationsof child nodes at each node.
Then, we com-bine these permutations.
This is implementedby cartesian products of the permutation sets.?
caseMarkers: We reorder only ?case marker(kaku joshi) phrases?.
Here, a ?case markerphrase?
is post-order traversal of a subtreerooted at a case marker bunsetsu.
For in-stance, the root of the following sentence S3has a non-case marker child ?kaburi ,?
(wear) between case marker children, ?jonga?
and ?zubon wo?
(Trousers are the ob-ject).
Figure 3 shows its dependency tree.jon ga shiroi boushi wo kaburi ,kuroi zubon wo hai te iru.
(John wears a white hat and wears black trousers.
)This is implemented by removing non-casemarker nodes from the set of child nodesto be reordered in the above ?postOrder?method.
For simplicity, we do not reorderother markers such as the topic marker ?wa?here.
This is future work.?
proposed: We reorder only contiguous casemarker children of a node, and we accept sen-tences that satisfy the aforementioned Sim-ple Case Marker Constraint.
S3?s root nodehas two case marker children, but they arenot contiguous.
Therefore, we do not reorderthem.
We expect that the constraint inhibitgeneration of incomprehensible or mislead-ing sentences.hai te iru.kaburi ,jon gazubon woboushi woshiroikuroiFigure 3: Dependency Tree of S3Table 3: Distribution of the number of generatedpermutations#permutations 1 2 4 6 8 12 16 24 >24single 100 0 0 0 0 0 0 0 0proposed 70 20 7 3 0 0 0 0 0caseMarkers 64 23 4 6 2 2 0 2 0postOrder 1 17 9 11 4 12 1 12 333 ResultsWe applied the above four methods to the ref-erence sentences of human-judged 100 sentencesof NTCIR-7 Patent MT EJ task.
(Fujii et al.,2008) We applied CaboCha (Kudo and Mat-sumoto, 2002) to the reference sentences, andmanually corrected the dependency trees becauseJapanese dependency parsers are not satisfactoryin terms of sentence accuracy (Tamura et al.,2007).To support this manual correction, CaboCha?sXML output was automatically convertedto dependency tree pictures by usingcabochatrees package for LATEX.
http://softcream.oka-pu.ac.jp/wp/wp-content/uploads/cabochatrees.pdf.
Then, it is easyto find mistakes of the dependency trees.
Inaddition, CaboCha?s dependency accuracy is veryhigh (89?90%) (Kudo and Matsumoto, 2002).Therefore, it took only one day to fix dependencytrees of one hundred reference sentences.Table 3 shows distribution of the number ofword orders generated by the above methods.
Pos-tOrder sometimes generates tens of thousands ofpermutations.Figure 4 shows a sentence-level scatter plotbetween Adequacy and RIBES for the baselineMoses system.
Each ?
indicates a sentence.Arrows indicate significant improvements ofRIBES scores by the proposed method.
For in-stance, the?mark at (5.0, 0.53) corresponds to anMT output:290Adequacy0 1 2 3 4 5RIBES00.20.40.60.81Average of RIBES: 0.706?
0.719Pearson?s r: 0.607?
0.663Spearman?s ?
: 0.607?
0.670Figure 4: Scatter plot between Adequacy andRIBES for 100 human-judged sentences in theoutput of NTCIR-7?s baseline Moses system andthe effects of the proposed methodindekkusu kohna wo zu 25 ni shimesu .which is a Japanese translation of ?FIG.25 showsthe index corner.?
The reference sentence for thissentence iszu 25 ni indekkusu kohna wo shimeshite iru .In this case, RIBES is 0.53, but all of the threejudges evaluated this as 5 of 5-point scale.
Thatis, RIBES disagrees with human judges.
The pro-posed method reorders this reference sentence asfollows:indekkusu kohna wo zu 25 ni shimeshite iru .This is very close to the above MT output andRIBES is 0.884 for this automatically reorderedreference sentence.
This shows that automatic re-ordering reduces the gap between single-referenceRIBES and Adequacy.Although RIBES strongly correlates with ade-quacy at the system level (Table 1), it has onlymediocre correlation with adequacy at the sen-tence level: Spearman?s ?
is 0.607 for the baselineMoses system.
The ?proposed?
method improvesit to 0.670.We can draw similar scatter plots for each sys-tem.
Table 4 summarises such improvement ofcorrelations.
And this is the main result of thisTable 4: Improvement of sentence-level correla-tion between Adequacy and RIBES for human-judged NTCIR-7 EJ systems (MAIN RESULT)Pearson?s r Spearman?s ?single?
proposed single?
proposedtsbmt 0.466 ?
0.472 0.439 ?
0.452Moses 0.607 ?
0.663 0.607 ?
0.670NTT 0.709 ?
0.735 0.692 ?
0.727NICT-ATR 0.620 ?
0.631 0.582 ?
0.608kuro 0.555 ?
0.608 0.515 ?
0.550Table 5: Increase of averaged RIBES scoresAdeq.
RIBESsystem single proposed caseMarkers postOrdertsbmt 3.527 0.715 0.7188 0.719 0.7569moses 2.897 0.706 0.7192 0.722 0.781NTT 2.740 0.671 0.683 0.686 0.7565NICT-ATR 2.587 0.655 0.664 0.670 0.749kuro 2.420 0.629 0.638 0.647 0.752paper.
The ?proposed?
method consistently im-proves sentence-level correlation between Ade-quacy and RIBES.Table 5 shows increase of averaged RIBES, butthis increase is not always an improvement.
Weexpected that ?PostOrder?
generates not only ac-ceptable sentences but also incomprehensible ormisleading sentences.
This must be harmful to theautomatic evaluation by RIBES.
Accoding to thistable, PostOrder gave higher RIBES scores to allsystems and correlation between RIBES and Ade-quacy is lost as expected.The ranking by RIBES-1.02.4 with ?single?reference sentences completely agrees with Ad-equacy, but the weakest constraint, ?postOrder?,disagrees.
Spearman?s ?
of the two ranks is 0.800but Pearson?s r is as low as 0.256.
It generates toomany incomprehensible/misleading word orders,and they also raise RIBES scores of bad transla-tions.
On the other hand, ?proposed?
and ?case-Markers?
agree with Adequacy except the ranksof tsbmt and the baseline Moses.4 Concluding RemarksRIBES is now widely used in Japanese-relatedtranslation evaluation.
But RIBES sometimes pe-nalizes good sentences because it does not re-gard scrambling.
Once we have correct depen-dency trees of reference sentences, we can auto-matically enumerate semantically equivalent word291orders.
Less constrained reordering tend to gener-ate syntactically ambiguous sentences.
They be-come incomprehensible or misleading sentences.In order to avoid them, we introduced SimpleCase Marker Constraint and restricted permuta-tions to contiguous case marker children of verbs/adjectives.
Then, sentence-level correlation coef-ficients were improved.The proposed enumeration method is also ap-plicable to other automatic evaluation methodssuch as BLEU, IMPACT, and ROUGE-L, but wehave to modify their codes for variable numbers ofmulti-reference sentences.
We will examine themin the full paper.We hope our method is also useful for other lan-guages that have scrambling.AcknowledgementThis research was supported by NTT Communi-cation Science Laboratories.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
Meteor:An automatic metric for MT evaluation with im-proved correlation with human judgements.
In Proc.of ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for MT and Summarization, pages65?72.Han Dan, Katsuhito Sudoh, Xianchao Wu, Kevin Duh,Hajime Tsukada, and Masaaki Nagata.
2012.
Headfinalization reordering for Chinese-to-Japanese ma-chine translation.
In Proceedings of SSST-6, SixthWorkshop on Syntax, Semantics and Structure inStatistical Translation, pages 57?66.Hiroshi Echizen-ya and Kenji Araki.
2007.
Automaticevaluation of machine translation based on recursiveacquisition of an intuitive common parts continuum.In MT Summit XI, pages 151?158.Atsushi Fujii, Masao Uchimura, Mikio Yamamoto, andTakehito Usturo.
2008.
Overview of the patentmachine translation task at the NTCIR-7 workshop.In Working Notes of the NTCIR Workshop Meeting(NTCIR).Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, andBenjamin K. Tsou.
2011.
Overview of the patentmachine translation task at the NTCIR-9 workshop.In Working Notes of the NTCIR Workshop Meeting(NTCIR).Isao Goto, Masao Utiyama, and Eiichiro Sumita.
2012.Post-ordering by parsing for japanese-english statis-tical machine translation.
In Proc.
of the AnnualMeeting of the Association of Computational Lin-guistics (ACL), pages 311?316.Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, andBenjamin K. Tsou.
2013.
Overview of the patentmachine translation task at the NTCIR-10 work-shop.
In Working Notes of the NTCIR WorkshopMeeting (NTCIR).Katsuhiko Hayashi, Katsuhito Sudoh, Hajime Tsukada,Jun Suzuki, and Masaaki Nagata.
2013.
Shift-reduce word reordering for machine translation.In Proc.
of the Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages1382?1386.Tsutomu Hirao, Hideki Isozaki, Kevin Duh, KatsuhitoSudoh, Hajime Tsukada, and Masaaki Nagao.
2011.RIBES: An automatic evaluation method of trans-lation based on rank correlation (in Japanese).
InProc.
of the Annual Meeting of the Association forNatural Language Processing, pages 1115?1118.Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai,Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,Yoshifumi Ooyama, and Yoshihiko Hayashi.
1997.Goi-Taikei ?
A Japanese Lexicon (in Japanese).Iwanami Shoten.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Kat-suhito Sudoh, Hajime Tsukada, and Masaaki Na-gata.
2010.
Automatic evaluation of translationquality for distant language pairs.
In Proc.
of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 944?952.Taku Kudo and Yuji Matsumoto.
2002.
Japanesedependency analysis using cascaded chunking.
InProc.
of the Conference on Computational NaturalLanguage Learning (CoNLL).Chin-Yew Lin and Franz Josef Och.
2004.
Automaticevaluation of translation quality using longest com-mon subsequences and skip-bigram statistics.
InProc.
of the Annual Meeting of the Association ofComputational Linguistics (ACL), pages 605?612.Graham Neubig, Taro Watanabe, and Shinsuke Mori.2012.
Inducing a discriminative parser to optimizemachine translation reordering.
In Proc.
of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 843?853.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
of theAnnual Meeting of the Association of ComputationalLinguistics (ACL), pages 311?318.Akihiro Tamura, Hiroya Takamura, and Manabu Oku-mura.
2007.
Japanese dependency analysis usingthe ancestor-descendant relation.
In Proc.
of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 600?609.Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsu-jii.
2012.
Akamon: An open source toolkit fortree/forest-based statistical machine translation.
InProc.
of the Annual Meeting of the Association ofComputational Linguistics (ACL), pages 127?132.292
