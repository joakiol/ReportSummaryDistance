Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 561?569,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPStatistical Bistratal Dependency ParsingRichard JohanssonDepartment of Information Engineering and Computer ScienceUniversity of TrentoTrento, Italyjohansson@disi.unitn.itAbstractWe present an inexact search algorithm forthe problem of predicting a two-layereddependency graph.
The algorithm is basedon a k-best version of the standard cubic-time search algorithm for projective de-pendency parsing, which is used as thebackbone of a beam search procedure.This allows us to handle the complex non-local feature dependencies occurring inbistratal parsing if we model the interde-pendency between the two layers.We apply the algorithm to the syntactic?semantic dependency parsing task of theCoNLL-2008 Shared Task, and we obtaina competitive result equal to the highestpublished for a system that jointly learnssyntactic and semantic structure.1 IntroductionNumerous linguistic theories assume a multistratalmodel of linguistic structure, such as a layer ofsurface syntax, deep syntax, and shallow seman-tics.
Examples include Meaning?Text Theory(Mel?c?uk, 1988), Discontinuous Grammar (Buch-Kromann, 2006), Extensible Dependency Gram-mar (Debusmann et al, 2004), and the FunctionalGenerative Description (Sgall et al, 1986) whichforms the theoretical foundation of the Prague De-pendency Treebank (Hajic?, 1998).In the statistical NLP community, the mostwidely used grammatical resource is the PennTreebank (Marcus et al, 1993).
This is a purelysyntactic resource, but we can also include thistreebank in the category of multistratal resourcessince the PropBank (Palmer et al, 2005) andNomBank (Meyers et al, 2004) projects have an-notated shallow semantic structures on top of it.Dependency-converted versions of the Penn Tree-bank, PropBank and NomBank were used in theCoNLL-2008 Shared Task (Surdeanu et al, 2008),in which the task of the participants was to pro-duce a bistratal dependency structure consisting ofsurface syntax and shallow semantics.Producing a consistent multistratal structure isa conceptually and computationally complex task,and most previous methods have employed apurely pipeline-based decomposition of the task.This includes the majority of work on shallow se-mantic analysis (Gildea and Jurafsky, 2002, in-ter alia).
Nevertheless, since it is obvious thatsyntax and semantics are highly interdependent, ithas repeatedly been suggested that the problems ofsyntactic and semantic analysis should be carriedout simultaneously rather than in a pipeline, andthat modeling the interdependency between syn-tax and semantics would improve the quality of allthe substructures.The purpose of the CoNLL-2008 Shared Taskwas to study the feasibility of a joint analysisof syntax and semantics, and while most partici-pating systems used a pipeline-based approach tothe problem, there were a number of contribu-tions that attempted to take the interdependencebetween syntax and semantics into account.
Thetop-performing system in the task (Johansson andNugues, 2008) applied a very simple rerankingscheme by means of a k-best syntactic output,similar to previous attempts (Gildea and Juraf-sky, 2002; Toutanova et al, 2005) to improve se-mantic role labeling performance by using mul-561tiple parses.
The system by Henderson et al(2008) extended previous stack-based algorithmsfor dependency parsing by using two separatestacks to build the syntactic and semantic graphs.Llu?
?s and Ma`rquez (2008) proposed a model thatsimultaneously predicts syntactic and semanticlinks, but since its search algorithm could not takethe syntactic?semantic interdependencies into ac-count, a pre-parsing step was still needed.
In ad-dition, before the CoNLL-2008 shared task therehave been a few attempts to jointly learn syntac-tic and semantic structure; for instance, Merlo andMusillo (2008) appended semantic role labels tothe phrase tags in a constituent treebank and ap-plied a conventional constituent parser to predictconstituent structure and semantic roles.In this paper, we propose a new approximatesearch method for bistratal dependency analysis.The search method is based on a beam search pro-cedure that extends a k-best version of the stan-dard cubic-time search algorithm for projectivedependency parsing.
This is similar to the searchmethod for constituent parsing used by Huang(2008), who referred to it as cube pruning, in-spired by an idea from machine translation decod-ing (Chiang, 2007).
The cube pruning approach,which is normally used to solve the argmax prob-lem, was also recently extended to summing prob-lems, which is needed in some learning algorithms(Gimpel and Smith, 2009).We apply the algorithm on the CoNLL-2008Shared Task data, and obtain the same evalua-tion score as the best previously published systemthat simultaneously learns syntactic and semanticstructure (Titov et al, 2009).2 Bistratal Dependency ParsingIn the tradition of dependency representation ofsentence structure, starting from Tesnie`re (1959),the linguistic structure of the sentence is repre-sented as a directed graph of relations betweenwords.
In most theories, certain constraints are im-posed on this graph; the most common constrainton dependency graphs in syntax, for instance, isthat the graph should form a tree (i.e.
it should beconnected, acyclic, and every node should have atmost one incoming edge).
This assumption un-derlies almost all dependency parsing, althoughthere are also a few parsers based on slightly moregeneral problem formulations (Sagae and Tsuji,2008).In this paper, we assume a different type of con-straint: that the graph can be partitioned into twosubgraphs that we will refer to as strata or layers,where the first of the layers forms a tree.
For thesecond layer, the only assumption we make is thatthere is at most one link between any two words.However, we believe that for any interesting lin-guistic structure, the second layer will be highlydependent on the structure of the first layer.Figure 1 shows an example of a bistratal depen-dency graph such as in the CoNLL-2008 SharedTask on syntactic and semantic dependency pars-ing.
The figure shows the representation of thesentence We were expecting prices to fall.
The pri-mary layer represents surface-syntactic relations,shown above the sentence, and the secondary layerconsists of predicate?argument links (here, wehave two predicates expecting and fall).SBJROOTWe were expecting prices to fallVC IMOPRDOBJC?A1A1 A1A0Figure 1: Example of a bistratal dependencygraph.We now give a formal model of the statisticalparsing problem of prediction of a bistratal depen-dency graph.
For a given input sentence x, the taskof our algorithm is to predict a structure y?
consist-ing of a primary layer y?pand a secondary layery?s.
In a discriminative modeling framework, wemodel this prediction problem as the search for thehighest-scoring output from the candidate space Yunder a scoring function F :?y?p, y?s?
= argmax?yp,ys?
?YF (x, yp, ys)The learning problem consists of searching in themodel space for a scoring function F that mini-mizes the cost of predictions on unseen examplesaccording to a given cost function ?.
In this work,we consider linear scoring functions of the follow-ing form:F (x, yp, ys) = w ??
(x, yp, ys)where ?
(x, y) is a numeric feature representationof the tuple (x, yp, ys) and w a high-dimensionalvector of feature weights.562Based on the structural assumptions madeabove, we now decompose the feature represen-tation into three parts:?
= ?p+?i+?sHere, ?prepresents the primary layer, assumed tobe a tree, ?sthe secondary layer, and finally ?iis the representation of the interdependency be-tween the layers.
For the feature representationsof the primary and secondary layers, we employedge factorization, a decomposition widely usedin statistical dependency parsing, and assume thatall edges can be scored independently:?p(x, yp) =?f?yp?p(x, f)The representation of the interdependency be-tween the layers assumes that each secondary linkis dependent on the primary layer, but independentof other secondary links.
?i(x, yp, ys) =?f?ys?i(x, f, yp)The interdependency between layers is the bottle-neck for the search algorithm that we will presentin Section 3.
For semantic role analysis, this in-volves all features that rely on a syntactic repre-sentation, most importantly the PATH feature thatrepresents the grammatical relation between pred-icate and argument words.
For instance, in Fig-ure 1, we can represent the surface-syntactic re-lation between the tokens fall and prices as thestring IM?OPRD?OBJ?.
In this work, all interde-pendency features will be based on paths in theprimary layer.3 A Bistratal Search AlgorithmThis section presents an algorithm to approxi-mately solve the argmax problem for predictionof bistratal dependency structures.
We present thealgorithm in two steps: first, we review a k-bestversion of the standard search algorithm for pro-jective monostratal dependency parsing, based onthe work by Huang and Chiang (2005).1 In thesecond step, starting from the k-best monostratalsearch, we devise a search method for the bistratalproblem.1Huang and Chiang (2005) described an even more effi-cient k-best algorithm based on lazy evaluation, which wewill not use here since it is not obviously adaptable to thesituation where the search is inexact.3.1 Review of k-Best Dependency ParsingThe search method commonly used in dependencyparsers is a chart-based dynamic programming al-gorithm that finds the highest-scoring projectivedependency tree under an edge-factored scoringfunction.
It runs in cubic time with respect to thesentence length.
In a slightly more general for-mulation, it was first published by Eisner (1996).Starting from McDonald et al (2005), it has beenwidely used in recent statistical dependency pars-ing frameworks.The algorithm works by creating open struc-tures, which consist of a dependency link and theset of links that it spans, and closed structures,consisting of the left or right half of a completesubtree.
An open structure is created by a proce-dure LINK that adds a dependency link to connecta right-pointing and a left-pointing closed struc-ture, and a closed structure by a procedure JOINthat joins an open structure with a closed structure.Figure 2 shows schematic illustrations: a LINKoperation connects the right-pointing closed struc-ture between s and j with the left-pointing closedstructure between j + 1 and e, and a JOIN oper-ation connects an open structure between s and jwith a closed structure between j and e.es j j+1 es jFigure 2: Illustrations of the LINK and JOIN oper-ations.The search algorithm can easily be extended tofind the k best parses, not only the best one.
Ink-best parsing, we maintain a k-best list in everycell in the dynamic programming table.
To createthe k-best list of derivations for an open structurebetween the positions s and e, for instance, thereare up to |L| ?
(e ?
s) ?
k2 possible combinationsto consider if the set of allowed labels is L. Thekey observation by Huang and Chiang (2005) is tomake use of the fact that the lists are sorted.
Forevery position between s and e, we add the bestcombination to a priority queue, from which wethen repeatedly remove the front item.
For everyitem we remove, we add three successors: an itemwith a next-best left part, an item with a next-bestright part, and finally an item with a next-best edge563label.The pseudocode of the search algorithm fork-best dependency parsing is given in Algo-rithms 1 and 2.
For brevity, we omitted thecode for ADVANCE-LEFT and ADVANCE-RIGHT,which are similar to ADVANCE-EDGE, as well asADVANCE-LOWER, which resembles ADVANCE-UPPER.
The FST function used in the pseudocodereturns the first element of a tuple.The algorithm uses a priority queue with stan-dard operations ENQUEUE, which enqueues anelement, and DEQUEUE, which removes thehighest-scoring item from the queue.
With a stan-dard binary heap implementation of the priorityqueue, these two operations execute in logarithmictime.
To build the queue, we use a constant-timeTOSS operation, which appends an item to thequeue without enforcing the priority queue con-straint, and a HEAPIFY operation that constructs aconsistent priority queue in linear time.3.2 Extension to Bistratal DependencyParsingThe k-best algorithm forms the core of the inexactbistratal search algorithm.
Our method is similarto the forest reranking method by Huang (2008),although there is no forest pruning or reranking in-volved here.
Crucially, we divide the features intolocal features, which can be computed ?offline?,and nonlocal features, which must be computedduring search.
In our case, the local features are?pand ?s, while the nonlocal features are the in-terdependent features ?i.Algorithm 3 shows pseudocode for the mainpart of the bistratal search algorithm, and Algo-rithm 4 for its support functions.
The algorithmworks as follows: for every span ?s, e?, the algo-rithm first uses the LINK procedure from the k-best monostratal search to construct a k-best list ofopen structures without semantic links.
In the nextstep, secondary links are added in the procedureLINK-SECONDARY.
For brevity, we show onlythe procedures that create open structures; they arevery similar to their closed-structure counterparts.The LINK-SECONDARY procedure starts bycreating an initial candidate (FIRST-SEC-OPEN)based on the best open structure for the primarylayer.
FIRST-SEC-OPEN creates the candidatespace for secondary links for a single primaryopen structure.
To reduce search complexity, itmakes use of a problem-specific function SCOPEAlgorithm 1 k-best search algorithm for depen-dency parsing.function k-BEST-SEARCH(k)n?
length of the sentenceinitialize the table O of open structuresinitialize the table C of closed structuresfor m ?
[1, .
.
.
, n]for s ?
[0, .
.
.
, n?m]LINK(s, s + m,?, k)LINK(s, s + m,?, k)JOIN(s, s + m,?, k)JOIN(s, s + m,?, k)return C[0, n,?
]procedure LINK(s, e, dir, k)E ?
CREATE-EDGES(s,e, dir, k)q ?
empty priority queuefor j ?
[s, .
.
.
, e?
1]l ?
C[s, j,?
]r ?
C[j + 1, e,?]o?
CREATE-OPEN(E,l, r, 1, 1, 1)TOSS(q, o)HEAPIFY(q)while |O[s, e, dir]| < k and |q| > 0o?
DEQUEUE(q)if o /?
O[s, e, dir]APPEND(O[s, e, dir], o)ENQUEUE(q,ADVANCE-EDGE(o))ENQUEUE(q,ADVANCE-LEFT(o))ENQUEUE(q,ADVANCE-RIGHT(o))procedure JOIN(s, e, dir, k)q ?
empty priority queueif dir =?for j ?
[s + 1, .
.
.
, e]u?
O[s, j,?]l?
C[j, e,?]c?
CREATE-CLOSED(u, l, 1, 1)TOSS(q, c)elsefor j ?
[s, .
.
.
, e?
1]u?
O[j, e,?]l?
C[s, j,?]c?
CREATE-CLOSED(u, l, 1, 1)TOSS(q, c)HEAPIFY(q)while |C[s, e, dir]| < k and |q| > 0c?
DEQUEUE(q)if c /?
C[s, e, dir]APPEND(C[s, e, dir], c)ENQUEUE(q,ADVANCE-UPPER(c))ENQUEUE(q,ADVANCE-LOWER(c))that defines which secondary links are possiblefrom a given token, given a primary-layer context.An important insight by Huang (2008) is thatnonlocal features should be computed as early aspossible during search.
In our case, we assumethat the interdependency features are based on treepaths in the primary layer.
This means that sec-ondary links between two tokens can be addedwhen there is a complete path in the primary layerbetween the tokens.
When we create an open564Algorithm 2 Support operations for the k-bestsearch.function CREATE-EDGES(s,e, dir, k)E ?
?for l ?
ALLOWED-LABELS(s,e, dir)scoreL?
w ?
?p(s, e, dir, l)edge?
?scoreL, s, e, dir, l?APPEND(E, edge)return the top k edges in Efunction CREATE-OPEN(E,l, r, ie, il, ir)scoreL?
FST(E[ie]) + FST(l[il]) + FST(r[ir])return ?scoreL+ scoreN, E, l, r, ie, il, ir?function CREATE-CLOSED(u,l, iu, ir)scoreL?
FST(u[iu]) + FST(l[il])return ?scoreL+ scoreN, u, l, iu, il?function ADVANCE-EDGE(o)where o = (score,E, l, r, ie, il, ir)if ie= LENGTH(E)return ?elsereturn CREATE-OPEN(E,l, r, ie+ 1, il, ir)function ADVANCE-UPPER(c)where c = (u, l, iu, il)if iu= LENGTH(u)return ?elsereturn CREATE-CLOSED(u,l, iu+ 1, il)structure by adding a link between two substruc-tures, a complete path is created between the to-kens in the substructures.
We thus search for pos-sible secondary links only between the two sub-structures that are joined.Figure 3 illustrates this process.
A primary openstructure between s and e has been created byadding a link from the right-pointing closed struc-ture between s and j to the left-pointing closedstructure between j + 1 and e. We now try toadd secondary links between the two substruc-tures.
For instance, in the semantic role parsingtask described in subsection 3.3, if we know thatthere is a predicate between s and j, then we lookfor arguments between j + 1 and e, i.e.
we applythe SCOPE function to the right substructure.When computing the scores for secondary links,note that for efficiency only the interdependentpart ?ishould be computed in CREATE-SEC-EDGES; the part of the score that does not dependon the primary layer can be computed before en-tering the search procedure.es j j+1p aFigure 3: Illustration of the secondary linking pro-cess: When two substructures are connected, wecan compute the path between a predicate in theleft substructure and an argument in the right sub-structure.Algorithm 3 Search algorithm for bistratal depen-dency parsing.function BISTRATAL-SEARCH(k)n?
length of the sentenceinitialize the table O of open structuresinitialize the table C of closed structuresusing ?s, compute a table scoressfor allpossible secondary edges ?h, d, l?for m ?
[1, .
.
.
, n]for s ?
[0, .
.
.
, n?m]LINK(s, s + m,?, k)LINK-SECONDARY(s,s + m,?, k)LINK(s, s + m,?, k)LINK-SECONDARY(s,s + m,?, k)JOIN(s, s + m,?, k)JOIN-SECONDARY(s,s + m,?, k)JOIN(s, s + m,?, k)JOIN-SECONDARY(s,s + m,?, k)return FIRST(C[0, n,?
])procedure LINK-SECONDARY(s,e, dir, k)q ?
empty priority queueo?
FIRST-SEC-OPEN(O[s,e, dir], 1, k)ENQUEUE(q, o)buf ?
empty listwhile |buf | < k and |q| > 0o?
DEQUEUE(q)if o /?
bufAPPEND(buf, o)for o?
?
ADVANCE-SEC-OPEN(o, k)ENQUEUE(q,o?
)SORT(buf) to O[s, e, dir]3.3 Application on the CoNLL-2008 SharedTask TreebankWe applied the bistratal search method in Algo-rithm 3 on the data from the CoNLL-2008 SharedTask (Surdeanu et al, 2008).
Here, the primarylayer is the tree of surface-syntactic relations suchas subject and object, and the secondary layer con-tains the links between the predicate words in thesentence and their respective logical arguments,such as agent and patient.
The training corpus con-sists of sections 02 ?
21 of the Penn Treebank, andcontains roughly 1 million words.565Algorithm 4 Support operations in bistratalsearch.function FIRST-SEC-OPEN(L,iL, k)if i = LENGTH(L)return ?l?GET-LEFT(L[iL]), r ?GET-RIGHT(L[iL])for h ?
[START(l), .
.
.
, END(l)]for d ?
SCOPE(r, h)]E[h][d]?
CREATE-SEC-EDGES(h, d, L[iL], k)]IE[h][d]?
1for h ?
[START(r), .
.
.
, END(r)]for d ?
SCOPE(l, h)]E[h][d]?
CREATE-SEC-EDGES(h, d, L[iL], k)]IE[h][d]?
1return CREATE-SEC-OPEN(L, iL, E, I)function CREATE-SEC-EDGES(h,d, o, k)E ?
?for l ?
ALLOWED-SEC-LABELS(h,d)score?
w ?
?i(h, d, l, o) + scoress[h, d, l]edge?
?score, h, d, l?APPEND(E, edge)return the top k edges in Efunction CREATE-SEC-OPEN(L,iL, E, I)score?
FST(L[iL]) +?h,dFST(E[h, d, IE[h, d]])return ?score, L, iL, E, IE?function ADVANCE-SEC-OPEN(o,k)where o = ?score, L, iL, E, IE?buf ?
?if iL< LENGTH(L) and IE= [1, .
.
.
, 1]APPEND(buf, FIRST-SEC-OPEN(L, iL+ 1, k))for h, dif IE[h, d] < LENGTH(E[h, d])I?E?
COPY(IE)I?E[h, d]?
I?E[h, d] + 1APPEND(buf, CREATE-SEC-OPEN(L, iL, E, I?E))return bufTo apply the bistratal search algorithm tothe problem of syntactic?semantic parsing, aproblem-specific implementation of the SCOPEfunction is needed.
In this case, we made two as-sumptions.
First, we assumed that the identitiesof the predicate words are known a priori2.
Sec-ondly, we assumed that every argument of a givenpredicate word is either a direct dependent of thepredicate, one of its ancestors, or a direct depen-dent of one of its ancestors.
This assumption is asimple adaptation of the pruning algorithm by Xueand Palmer (2004), and it holds for the vast major-ity of arguments in the CoNLL-2008 data; in thetraining set, we measured that this covers 99.04%of the arguments of verbs and 97.55% of the argu-2Since our algorithm needs to know the positions of thepredicates, we trained a separate classifier using the LIBLIN-EAR toolkit (Fan et al, 2008) to identify the predicate words.As features for the classifier, we used the words and part-of-speech tags in a ?3 window around the word under consid-eration.ments of nouns.Figure 4 shows an example of how the SCOPEfunction works in our case.
If a predicate is con-tained in the right substructure, we find two po-tential arguments: one at the start of the left sub-structure, and one more by recursively searchingthe left structure.pa a21Figure 4: Illustration of the SCOPE function forpredicate?argument links.
If the right substructurecontains a predicate, we can find potential argu-ments in the left substructure.While the primary layer is assumed to be pro-jective in Algorithm 3, the syntactic trees in theCoNLL-2008 data have a small number of nonpro-jective links.
We used a pseudo-projective edge la-bel encoding to handle nonprojectivity (Nivre andNilsson, 2005).To implement the model, we constructed fea-ture representations ?p, ?s, and ?i.
The surface-syntactic representation ?pwas a standard first-order edge factorization using the same featuresas McDonald et al (2005).
The features in?sand?iare shown in Table 1 and are standard featuresin statistical semantic role classification.
?s?iPredicate word PathPredicate POS Path + arg.
POSArgument word Path + pred.
POSArgument POS Path + arg.
wordPred.
+ arg.
words Path + pred.
wordPredicate word + label Path + labelPredicate POS + label Path + arg.
POS + labelArgument word + label Path + pred.
POS + labelArgument POS + label Path + arg.
word + labelPred.
+ arg.
words + label Path + pred.
word + labelTable 1: Feature representation for secondarylinks.We trained the discriminative model usingthe Online Passive?aggressive algorithm (Cram-mer et al, 2006), which is an efficient onlinelearning method that can be used to train mod-els for learning problems with structured out-put spaces.
A cost function ?
is needed in thelearning algorithm; we decomposed it into a pri-566mary part ?pand a secondary part ?s.
We com-puted the primary part as the sum of link errors:?p(yp, y?p) =?l?y?pcp(l, yp), wherecp(l, yp) =0 if l ?
ypand its label is correct0.5 if l ?
ypbut its label is incorrect1 if l /?
ypIn a similar vein, we computed the secondary part?sof the cost function as #fp+#fn+0.5 ?#fl,where #fp is the number of false positive sec-ondary links, #fn the number of false negativelinks, and #fl the number of links with correctendpoints but incorrect label.The training procedure took roughly 24 hourson an 2.3 GHz AMD Athlon processor.
The mem-ory consumption was about 1 GB during training.4 ExperimentsWe evaluated the performance of our system onthe test set from the CoNLL-2008 shared task,which consists of section 23 of the WSJ part ofthe Penn Treebank, as well as a small part of theBrown corpus.
A beam width k of 4 was usedin this experiment.
Table 2 shows the results ofthe evaluation.
The table shows the three mostimportant scores computed by the official evalua-tion script: labeled syntactic dependency accuracy(LAS), labeled semantic dependency F1-measure(Sem.
F1), and the macro-averaged F1-measure, aweighted combination of the syntactic and seman-tic scores (M. F1).
Our result is competitive; weobtain the same macro F1 as the newly publishedresult by Titov et al (2009), which is the high-est published figure for a joint syntactic?semanticparser so far.
Importantly, our system clearly out-performs the system by Llu?
?s and Ma`rquez (2008),which is the most similar system in problem mod-eling, but which uses a different search strategy.System LAS Sem.
F1 M. F1This paper 86.6 77.1 81.8Titov et al (2009) 87.5 76.1 81.8H.
et al(2008) 87.6 73.1 80.5L.
& M. (2008) 85.8 70.3 78.1Table 2: Results of published joint syntactic?semantic parsers on the CoNLL-2008 test set.Since the search procedure is inexact, it is im-portant to quantify roughly how much of a detri-mental impact the approximation has on the pars-ing quality.
We studied the influence of the beamwidth parameter k on the performance of theparser.
The results on the development set can beseen in Table 3.
As can be seen, a modest increasein performance can be obtained by increasing thebeam width, at the cost of increased parsing time.k LAS Sem.
F1 M. F1 Time1 85.14 77.05 81.10 2422 85.43 77.17 81.30 3694 85.49 77.20 81.35 6258 85.58 77.20 81.40 1178Table 3: Influence of beam width on parsing accu-racy.In addition, to have a rough indication of the im-pact of search errors on the quality of the parses,we computed the fraction of sentences where thegold-standard parse had a higher score accord-ing to the model than the parse returned by thesearch3.
Table 4 shows the results of this exper-iment.
This suggests that the search errors, al-though they clearly have an impact, are not the ma-jor source of errors, even with small beam widths.k Fraction1 0.1212 0.1044 0.0968 0.090Table 4: Fraction of sentences in the developmentset where the gold-standard parse has a higherscore than the parse returned by the search pro-cedure.To investigate where future optimization effortsshould be spent, we used the built-in hprof pro-filing tool of Java to locate the bottlenecks.
Onceagain, we ran the program on the developmentset with a beam width of 4, and Table 5 showsthe three types of operations where the algorithmspent most of its time.
It turns out that 74% of thetime was spent on the computation and scoring ofinterdependency features.
To make our algorithmtruly useful in practice, we thus need to devise away to speed up or cache these computations.3To be able to compare the scores of the gold-standardand predicted parses, we disabled the automatic classifier forpredicate identification and provided the parser with gold-standard predicates in this experiment.567Operation Fractionw ?
?i0.64Queue operations 0.15Computation of ?i0.10Table 5: The three most significant bottlenecksand their fraction of the total runtime.5 DiscussionIn this paper, we have presented a new approxi-mate search method to solve the problem of jointlypredicting the two layers in a bistratal dependencygraph.
The algorithm shows competitive perfor-mance on the treebank used in the CoNLL-2008Shared Task, a bistratal treebank consisting of asurface-syntactic and a shallow semantic layer.
Inaddition to the syntactic?semantic task that wehave described in this paper, we believe that ourmethod can be used in other types of multistratalsyntactic frameworks, such as a representation ofsurface and deep syntax as in Meaning?Text The-ory (Mel?c?uk, 1988).The optimization problem that we set out tosolve is intractable, but we have shown that rea-sonable performance can be achieved with an in-exact, beam search-based search method.
This isnot obvious: it has previously been shown that us-ing an inexact search procedure when the learn-ing algorithm assumes that the search is exactmay lead to slow convergence or even divergence(Kulesza and Pereira, 2008), but this does notseem to be a problem in our case.While we used a beam search method as themethod of approximation, other methods are cer-tainly possible.
An interesting example is the re-cent system by Smith and Eisner (2008), whichused loopy belief propagation in a dependencyparser using highly complex features, while stillmaintaining cubic-time search complexity.An obvious drawback of our approach com-pared to traditional pipeline-based semantic rolelabeling methods is that the speed of the algo-rithm is highly dependent on the size of the in-terdependency feature representation ?i.
Also,extracting these features is fairly complex, and itis of critical importance to implement the featureextraction procedure efficiently since it is one ofthe bottlenecks of the algorithm.
It is plausiblethat our performance suffers from the absence ofother frequently used syntax-based features suchas dependent-of-dependent and voice.It is thus highly dubious that a joint modelingof syntactic and semantic structure is worth theadditional implementational effort.
So far, no sys-tem using tightly integrated syntactic and semanticprocessing has been competitive with the best sys-tems, which have been either completely pipeline-based (Che et al, 2008; Ciaramita et al, 2008)or employed only a loose syntactic?semantic cou-pling (Johansson and Nugues, 2008).
It has beenconjectured that modeling the semantics of thesentence would also help in syntactic disambigua-tion; however, it is likely that this is already im-plicitly taken into account by the lexical featurespresent in virtually all modern parsers.In addition, a problem that our beam searchmethod has in common with the constituent pars-ing method by Huang (2008) is that highly non-local features must be computed late.
In our case,this means that if there is a long distance between apredicate and an argument, the secondary link be-tween them will be unlikely to influence the finalsearch result.AcknowledgementsThe author is grateful for the helpful comments bythe reviewers.
This work has been funded by theLivingKnowledge project under the seventh EUframework program.ReferencesMatthias Buch-Kromann.
2006.
Discontinuous Gram-mar.
A dependency-based model of human parsingand language learning.
Ph.D. thesis, CopenhagenBusiness School.Wanxiang Che, Zhenghua Li, Yuxuan Hu, YongqiangLi, Bing Qin, Ting Liu, and Sheng Li.
2008.
Acascaded syntactic and semantic dependency pars-ing system.
In CoNLL 2008: Proceedings of theTwelfth Conference on Natural Language Learning.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Massimiliano Ciaramita, Giuseppe Attardi, FeliceDell?Orletta, and Mihai Surdeanu.
2008.
DeSRL:A linear-time semantic role labeling system.
In Pro-ceedings of the Shared Task Session of CoNLL-2008.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Schwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research, 2006(7):551?585.Ralph Debusmann, Denys Duchier, Alexander Koller,Marco Kuhlmann, Gert Smolka, and Stefan Thater.5682004.
A relational syntax-semantics interface basedon dependency grammar.
In Proceedings of the20th International Conference on ComputationalLinguistics (COLING 2004).Jason M. Eisner.
1996.
Three new probabilistic mod-els for dependency parsing: An exploration.
In Pro-ceedings of the 16th International Conference onComputational Linguistics, pages 340?345.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Kevin Gimpel and Noah A. Smith.
2009.
Cubesumming, approximate inference with non-local fea-tures, and dynamic programming without semirings.In Proceedings of the Twelfth Conference of the Eu-ropean Chapter of the Association for Computa-tional Linguistics (EACL).Jan Hajic?.
1998.
Building a syntactically annotatedcorpus: The Prague Dependency Treebank.
In Is-sues of Valency and Meaning, pages 106?132.James Henderson, Paola Merlo, Gabriele Musillo, andIvan Titov.
2008.
A latent variable model ofsynchronous parsing for syntactic and semantic de-pendencies.
In CoNLL 2008: Proceedings of theTwelfth Conference on Natural Language Learning.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the 9th InternationalWorkshop on Parsing Technologies (IWPT 2005).Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL-08: HLT, pages 586?594.Richard Johansson and Pierre Nugues.
2008.Dependency-based syntactic?semantic analysis withPropBank and NomBank.
In Proceedings of theShared Task Session of CoNLL-2008.Alex Kulesza and Fernando Pereira.
2008.
Structuredlearning with approximate inference.
In Advancesin Neural Information Processing Systems 20.Xavier Llu?
?s and Llu?
?s Ma`rquez.
2008.
A joint modelfor parsing syntactic and semantic dependencies.
InCoNLL 2008: Proceedings of the Twelfth Confer-ence on Natural Language Learning.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of 43rd AnnualMeeting of the Association for Computational Lin-guistics (ACL?05), pages 91?98.Igor A. Mel?c?uk.
1988.
Dependency Syntax: Theoryand Practice.
State University Press of New York.Paola Merlo and Gabriele Musillo.
2008.
Semanticparsing for high-precision semantic role labelling.In Proceedings of the 12th Conference on Computa-tional Natural Language Learning (CoNLL?2008).Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
The NomBank project:An interim report.
In HLT-NAACL 2004 Workshop:Frontiers in Corpus Annotation.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In Proceedings ofthe 43rd Annual Meeting of the Association forComputational Linguistics (ACL?05).Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Kenji Sagae and Jun?ichi Tsuji.
2008.
Shift?reducedependency DAG parsing.
In Proceedings of the22nd International Conference on ComputationalLinguistics (Coling 2008).Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?.
1986.The Meaning of the Sentence in Its Semantic andPragmatic Aspects.
Dordrecht:Reidel PublishingCompany and Prague:Academia.David Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), Honolulu, UnitedStates.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
TheCoNLL?2008 shared task on joint parsing of syn-tactic and semantic dependencies.
In Proceedingsof CoNLL?2008.Lucien Tesnie`re.
1959.
?Ele?ments de syntaxe struc-turale.
Klincksieck, Paris.Ivan Titov, James Henderson, Paola Merlo, andGabriele Musillo.
2009.
Online graph planarisationfor synchronous parsing of semantic and syntacticdependencies.
In Proceedings of the InternationalJoint Conferences on Artificial Intelligence.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2005.
Joint learning improves semanticrole labeling.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Lin-guistics (ACL?05), pages 589?596.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Proceedingsof the 2004 Conference on Empirical Methods inNatural Language Processing, pages 88?94.569
