Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 321?328Manchester, August 2008Improving Statistical Machine Translation usingLexicalized Rule SelectionZhongjun He1,2 and Qun Liu1 and Shouxun Lin11Key Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesBeijing, 100190, China2Graduate University of Chinese Academy of SciencesBeijing, 100049, China{zjhe,liuqun,sxlin}@ict.ac.cnAbstractThis paper proposes a novel lexicalized ap-proach for rule selection for syntax-basedstatistical machine translation (SMT).
Webuild maximum entropy (MaxEnt) mod-els which combine rich context informa-tion for selecting translation rules dur-ing decoding.
We successfully integratethe MaxEnt-based rule selection modelsinto the state-of-the-art syntax-based SMTmodel.
Experiments show that our lexical-ized approach for rule selection achievesstatistically significant improvements overthe state-of-the-art SMT system.1 IntroductionThe syntax-based statistical machine translation(SMT) models (Chiang, 2005; Liu et al, 2006;Galley et al, 2006; Huang et al, 2006) use ruleswith hierarchical structures as translation knowl-edge, which can capture long-distance reorderings.Generally, a translation rule consists of a left-hand-side (LHS) 1and a right-hand-side (RHS).
TheLHS and RHS can be words, phrases, or even syn-tactic trees, depending on SMT models.
Transla-tion rules can be learned automatically from par-allel corpus.
Usually, an LHS may correspond tomultiple RHS?s in multiple rules.
Therefore, in sta-tistical machine translation, the rule selection taskis to select the correct RHS for an LHS during de-coding.The conventional approach for rule selection isto use precomputed translation probabilities whichc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1In this paper, we use LHS and source-side interchange-ably (so are RHS and target-side).are estimated from the training corpus, as well as an-gram language model which is trained on the tar-get language.
The limitation of this method is thatit ignores context information (especially on thesource-side) during decoding.
Take the hierarchi-cal model (Chiang, 2005) as an example.
Considerthe following rules for Chinese-to-English transla-tion 2:(1) X ?
??
X1?
X2, X2in X1?
(2) X ?
??
X1?
X2, at X1?s X2?
(3) X ?
??
X1?
X2, with X2of X1?These rules have the same source-side, and all ofthem can pattern-match all the following sourcephrases:(a) ?in[??economic??]1field??s[??
]2cooperation[cooperation]2in [the economic field]1(b) ?at[??]1today??s[??meeting?
]2onat [today]1?s [meeting]2(c) ?with[??]1people??s[??support?
]2underwith [the support]2of [the people]1Given a source phrase, how does the decoderknow which rule is suitable?
In fact, rule (1) andrule (2) have different syntactic structures (the lefttwo trees of Figure 1).
Thus rule (1) can be usedfor translating noun phrase (a), and rule (2) can beapplied to prepositional phrase (b).
The weakness2In this paper, we use Chinese and English as the sourceand target language, respectively.321NPDNPPP?X1?X2X2ofX1PPLCPNP?X1?X2atX1?sX2PPLCPNP?X1?X2withX2ofX1Figure 1: Syntactic structures of the same source-side in different rules.of Chiang?s hierarchical model is that it cannotdistinguish different structures on the source-side.The linguistically syntax-based models (Liu et al,2006; Huang et al, 2006) can distinguish syntacticstructures by parsing source sentence.
However,as an LHS tree may correspond to different RHSstrings in different rules (the right two rules of Fig-ure 1), these models also face the rule selectionproblem during decoding.In this paper, we propose a lexicalized approachfor rule selection for syntax-based statistical ma-chine translation.
We use the maximum entropyapproach to combine various context features, e.g.,context words of rules, boundary words of phrases,parts-of-speech (POS) information.
Therefore, thedecoder can use rich context information to per-form context-dependent rule selection.
We builda maximum entropy based rule selection (MaxEntRS) model for each ambiguous hierarchical LHS,the LHS which contains nonterminals and corre-sponds to multiple RHS?s in multiple rules.
Weintegrate the MaxEnt RS models into the state-of-the-art hierarchical SMT system (Chiang, 2005).Experiments show that the lexicalized rule se-lection approach improves translation quality ofthe state-of-the-art SMT system, and the improve-ments are statistically significant.2 Previous Work2.1 The Selection Problem in SMTStatistical machine translation systems usuallyface the selection problem because of the one-to-many correspondence between the source and tar-get language.
Recent researches showed that richcontext information can help SMT systems per-form selection and improves translation quality.The discriminative phrasal reordering models(Xiong et al, 2006; Zens and Ney, 2006) pro-vided a lexicalized method for phrase reordering.In these models, LHS and RHS can be consid-ered as phrases and reordering types, respectively.Therefore the selection task is to select a reorder-ing type for phrases.
They use a MaxEnt modelto combine context features and distinguished twokinds of reorderings between two adjacent phrases:monotone or swap.
However, our method is moregeneric, we perform lexicalized rule selection forsyntax-based SMT models.
In these models, therules with hierarchical structures can handle re-orderings of non-adjacent phrases.
Furthermore,the rule selection can be considered as a multi-class classification task, while the phrase reorder-ing between two adjacent phrases is a two-classclassification task.Recently, word sense disambiguation (WSD)techniques improved the performance of SMT sys-tems by helping the decoder perform lexical selec-tion.
Carpuat and Wu (2007b) integrated a WSDsystem into a phrase-based SMT system, Pharaoh(Koehn, 2004a).
Furthermore, they extendedWSDto phrase sense disambiguation (PSD) (Carpuatand Wu, 2007a).
Either the WSD or PSD systemcombines rich context information to solve the am-biguity problem for words or phrases.
Their exper-iments showed stable improvements of translationquality.
These are different from our work.
Onone hand, they focus on solving the lexical am-biguity problem, and use a WSD or PSD systemto predict translations for phrases which only con-sist of words.
However, we put emphasis on ruleselection, and predict translations for hierarchicalLHS?s which consist of both words and nontermi-nals.
On the other hand, they incorporated a WSDor PSD system into a phrase-based SMT systemwith a weak distortion model for phrase reorder-ing.
While we incorporate MaxEnt RS modelsinto the state-of-the-art syntax-based SMT system,which captures phrase reordering by using a hier-archical model.322Chan et al (2007) incorporated a WSD systeminto the hierarchical SMT system, Hiero (Chi-ang, 2005), and reported statistically significantimprovement.
But they only focused on solvingambiguity for terminals of translation rules, andlimited the length of terminals up to 2.
Differentfrom their work, we consider a translation rule as awhole, which contains both terminals and nonter-minals.
Moreover, they explored features for theWSD system only on the source-side.
While wedefine context features for the MaxEnt RS modelson both the source-side and target-side.2.2 The Hierarchical ModelThe hierarchical model (Chiang, 2005; Chiang,2007) is built on a weighted synchronous context-free grammar (SCFG) .
A SCFG rule has the fol-lowing form:X ?
?
?, ?,??
(4)where X is a nonterminal, ?
is an LHS string con-sists of terminals and nonterminals, ?
is the trans-lation of ?, ?
defines a one-one correspondencebetween nonterminals in ?
and ?.
For example,(5) X ?
????
?, economic development?
(6) X ?
?
X1?
X2?
the X2of X1?Rule (5) contains only terminals, which is simi-lar to phrase-to-phrase translation in phrase-basedSMT models.
Rule (6) contains both terminalsand nonterminals, which causes a reordering ofphrases.
The hierarchical model uses the max-imum likelihood method to estimate translationprobabilities for a phrase pair ?
?, ?
?, independentof any other context information.To perform translation, Chiang uses a log-linearmodel (Och and Ney, 2002) to combine variousfeatures.
The weight of a derivation D is computedby:w(D) =?i?i(D)?i(7)where ?i(D) is a feature function and ?iis the fea-ture weight of ?i(D).
During decoding, the de-coder searches the best derivation with the lowestcost by applying SCFG rules.
However, the ruleselections are independent of context information,except the left neighboring n ?
1 target words forcomputing n-gram language model.3 Lexicalized Rule SelectionThe rule selection task can be considered as amulti-class classification task.
For a source-side,each corresponding target-side is a label.
The max-imum entropy approach (Berger et al, 1996) isknown to be well suited to solve the classificationproblem.
Therefore, we build a maximum entropybased rule selection (MaxEnt RS) model for eachambiguous hierarchical LHS.
In this section, wewill describe how to build the MaxEnt RS mod-els and how to integrate them into the hierarchicalSMT model.3.1 The MaxEnt RS ModelFollowing (Chiang, 2005), we use ?
?, ??
to repre-sent a SCFG rule extracted from the training cor-pus, where ?
and ?
are source and target strings,respectively.
The nonterminals in ?
and ?
are rep-resented by Xk, where k is an index indicatingone-one correspondence between nonterminals insource and target sides.
Let us use f(Xk) to rep-resent the source text covered by Xk, and e(Xk)to represent the translation of f(Xk).
Let C(?)
bethe context information of source text matched by?, and C(?)
be the context information of targettext matched by ?.
Under the MaxEnt model, wehave:Prs(?|?, f(Xk), e(Xk)) =(8)exp[?i?ihi(C(?
), C(?
), f(Xk), e(Xk))]???exp[?i?ihi(C(??
), C(?
), f(Xk), e(Xk))]where hiis a binary feature function, ?iis the fea-ture weight of hi.
The MaxEnt RS model com-bines rich context information of grammar rules,as well as information of the subphrases whichwill be reduced to nonterminal X during decoding.However, these information is ignored by Chiang?shierarchical model.We design three kinds of features for a rule?
?, ??:?
Lexical features, which are the words imme-diately to the left and right of ?, and boundarywords of subphrase f(Xk) and e(Xk);?
Parts-of-speech (POS) features, which arePOS tags of the source words defined in lexi-cal features.?
Length features, which are the length of sub-phrases f(Xk) and e(Xk).323Side Type Name DescriptionW?
?1The source word immediately to the left of ?W?+1The source word immediately to the right of ?WLf(Xk)The first word of f(Xk)Lexical FeaturesWRf(Xk)The last word of f(Xk)P?
?1POS of W?
?1P?+1POS of W?+1PLf(Xk)POS of WLf(Xk)POS FeaturesPRf(Xk)POS of WRf(Xk)Source-sideLength Feature LENf(Xk)Length of source subphrase f(Xk)WLe(Xk)The first word of e(Xk)Lexical FeaturesWRe(Xk)The last word of e(Xk)Target-sideLength Feature LENe(Xk)Length of target subphrase e(Xk)Table 1: Feature categories of the MaxEnt RS model.Type FeatureW??1=??
W?+1=bLexical Features WLf(X1)=??
WRf(X1)=??
WLf(X2)=??
WRf(X1)=?
?WLe(X1)=economic WRe(X1)=field WLe(X2)=cooperation WRf(X1)=cooperationP?
?1=v W?+1=wjPOS FeaturesPLf(X1)=n PRf(X1)=n PLf(X2)=vn PRf(X2)=vnLength Feature LENf(X1)=2 LENf(X2)=1 LENe(X1)=2 LENe(X2)=1Table 2: Features of rule X ?
??
X1?
X2, X2in the X1?.?
?/v ?/p ?
?/n ?
?/n ?/ude ?
?/vn b/wjstrengthenthecooperationintheeconomicfield.Figure 2: An training example for rule extraction.Table 1 shows these features in detail.These features can be easily gathered accord-ing to Chinag?s rule extraction method (Chiang,2005).
We use an example for illustration.
Fig-ure 2 is a word-aligned training example with POStags on the source side.
We can obtain a SCFGrule:(9) X ?
??
X1?
X2, X2in the X1?Where the source phrases covered by X1and X2are ???
???
and ???
?, respectively.
Table2 shows features of this rule.
Note that following(Chiang, 2005), we limit the number of nontermi-nals of a rule up to 2.
Thus a rule may have 20features at most.After extracting features from the training cor-pus, we use the toolkit implemented by Zhang(2004) to train a MaxEnt RS model for each am-biguous hierarchical LHS.
We set iteration numberto 100 and Gaussian prior to 1.3.2 Integrating the MaxEnt RS Models intothe SMT ModelWe integrate the MaxEnt RS models into the SMTmodel during the translation of each source sen-tence.
Thus the MaxEnt RS models can help thedecoder perform context-dependent rule selectionduring decoding.In (Chiang, 2005), the log-linear model com-bines 8 features: the translation probabilitiesP (?|?)
and P (?|?
), the lexical weights Pw(?|?
)and Pw(?|?
), the language model, the wordpenalty, the phrase penalty, and the glue rulepenalty.
For integration, we add two new features:?
Prs(?|?, f(Xk), e(Xk)).
This feature iscomputed by the MaxEnt RS model, whichgives a probability that the model selecting atarget-side ?
given an ambiguous source-side?, considering context information.?
Prsn= exp(1).
This feature is similar tophrase penalty feature.
In our experiments,324we find that some source-sides are not am-biguous, and correspond to only one target-side.
However, if a source-side ??
is not am-biguous, the first feature Prswill be set to 1.0.In fact, these rules are not reliable since theyusually occur only once in the training corpus.Therefore, we use this feature to reward theambiguous source-side.
During decoding, ifan LHS has multiple translations, this featureis set to exp(1), otherwise it is set to exp(0).The advantage of our integration is that we neednot change the main decoding algorithm of a SMTsystem.
Furthermore, the weights of the new fea-tures can be trained together with other features ofthe translation model.Chiang (2007) uses the CKY algorithm with acube pruning method for decoding.
This methodcan significantly reduce the search space by effi-ciently computing the top-n items rather than allpossible items at a node, using the k-best Algo-rithms of Huang and Chiang (2005) to speed upthe computation.
In cube pruning, the translationmodel is treated as the monotonic backbone ofthe search space, while the language model scoreis a non-monotonic cost that distorts the searchspace (see (Huang and Chiang, 2005) for defini-tion of monotonicity).
Similarly, in the MaxEntRS model, source-side features form a monotonicscore while target-side features constitute a non-monotonic cost that can be seen as part of the lan-guage model.For translating a source sentence F JI, the de-coder adopts a bottom-up strategy.
All derivationsare stored in a chart structure.
Each cell c[i, j] ofthe chart contains all partial derivations which cor-respond to the source phrase f ji.
For translatinga source-side span [i, j], we first select all possi-ble rules from the rule table.
Meanwhile, we canobtain features of the MaxEnt RS models whichare defined on the source-side since they are fixedbefore decoding.
During decoding, for a sourcephrase f ji, suppose the ruleX ?
?fkiX1fjt, ek?i?X1ej?t??
(10)is selected by the decoder, where i ?
k < t ?
jand k + 1 < t, then we can gather features whichare defined on the target-side of the subphrase X1from the ancestor chart cell c[k + 1, t ?
1] sincethe span [k + 1, t ?
1] has already been covered.Then the new feature scores Prsand Prsncan becomputed.
Therefore, the cost of the derivation canbe obtained.
Finally, the decoding is completedwhen the whole sentence is covered, and the bestderivation of the source sentence F JIis the itemwith the lowest cost in cell c[I, J ].4 Experiments4.1 CorpusWe carry out experiments on two translation taskswith different sizes and domains of the trainingcorpus.?
IWSLT-05: We use about 40,000 sentencepairs from the BTEC corpus with 354k Chi-nese words and 378k English words as ourtraining data.
The English part is used to traina trigram language model.
We use IWSLT-04test set as the development set and IWSLT-05test set as the test set.?
NIST-03: We use the FBIS corpus as thetraining corpus, which contains 239k sen-tence pairs with 6.9M Chinese words and8.9M English words.
For this task, we traintwo trigram language models on the Englishpart of the training corpus and the Xinhuaportion of the Gigaword corpus, respectively.NIST-02 test set is used as the developmentset and NIST-03 test set is used as the test set.4.2 TrainingTo train the translation model, we first runGIZA++ (Och and Ney, 2000) to obtain wordalignment in both translation directions.
Then theword alignment is refined by performing ?grow-diag-final?
method (Koehn et al, 2003).
We usethe same method suggested in (Chiang, 2005) toextract SCFG grammar rules.
Meanwhile, wegather context features for training the MaxEnt RSmodels.
The maximum initial phrase length is setto 10 and the maximum rule length of the source-side is set to 5.We use SRI Language Modeling Toolkit (Stol-cke, 2002) to train language models for both tasks.We use minimum error rate training (Och, 2003) totune the feature weights for the log-linear model.The translation quality is evaluated by BLEUmetric (Papineni et al, 2002), as calculated bymteval-v11b.pl with case-insensitive matching ofn-grams, where n = 4.4.3 BaselineWe reimplement the decoder of Hiero (Chiang,2007) in C++, which is the state-of-the-art SMT325System IWSLT-05 NIST-03Baseline 56.20 28.05+ MaxEnt RSSLex 56.51 28.26PF 56.95 28.78SLex+PF 56.99 28.89SLex+PF+SLen 57.10 28.96SLex+PF +SLen+TF 57.20 29.02Table 3: BLEU-4 scores (case-insensitive) on IWSLT-05 task and NIST MT-03 task.
SLex = Source-sideLexical Features, PF = POS Features, SLen = Source-side Length Feature, TF = Target-side features.system.
During decoding, we set b = 100 to prunegrammar rules, ?
= 10, b = 30 to prune X cells,and ?
= 10, b = 15 to prune S cells.
For cubepruning, we set the threshold ?
= 1.0.
See (Chi-ang, 2007) for meanings of these pruning parame-ters.The baseline system uses precomputed phrasetranslation probabilities and two trigram languagemodels to perform rule selection, independent ofany other context information.
The results areshown in the row Baseline of Table 3.
For IWSLT-05 task, the baseline system achieves a BLEU-4score of 56.20.
For NIST MT-03 task, the BLEU-4 score is 28.05 .4.4 Baseline + MaxEnt RSAs described in Section 3.2, we add two new fea-tures to integrate the MaxEnt RS models into thehierarchical model.
To run the decoder, we sharethe same pruning settings with the baseline system.Table 3 shows the results.Using all features defined in Section 3.1 to trainthe MaxEnt RS models, for IWSLT-05 task, theBLEU-4 score is 57.20, which achieves an abso-lute improvement of 1.0 over the baseline.
ForNIST-03 task, our system obtains a BLEU-4 scoreof 29.02, with an absolute improvement of 0.97over the baseline.
Using Zhang?s significancetester (Zhang et al, 2004) to perform paired boot-strap sampling (Koehn, 2004b), both improve-ments on the two tasks are statistically significantat p < 0.05.In order to explore the utility of the context fea-tures, we train the MaxEnt RS models on differentfeature sets.
We find that POS features are the mostuseful features since they can generalize over alltraining examples.
Moreover, length feature alsoyields improvement.
However, these features arenever used in the baseline.NO.
of NO.
of NO.
ofLHS H-LHS AH-LHSNIST MT-03 163,097 148,671 95,424Baseline 12,069 7,164 5,745+MaxEnt RS(All features) 12,655 10,306 9,259Table 4: Number of possible source-sides of SCFGrules for NIST-03 task and number of source-sidesof the best translation.
H-LHS = HierarchicalLHS, AH-LHS = Ambiguous hierarchical LHS.5 AnalysisTable 4 shows the number of source-sides ofthe SCFG rules for NIST-03 task.
After extract-ing grammar rules from the training corpus, thereare 163,097 source-sides match the test corpus,91.15% are hierarchical LHS?s (H-LHS, the LHSwhich contains nonterminals).
For the hierarchi-cal LHS?s, 64.18% are ambiguous (AH-LHS, theH-LHS which has multiple translations).
This in-dicates that the decoder will face serious rule se-lection problem during decoding.
We also note thenumber of the source-sides of the best translationfor the test corpus.
For the baseline system, thenumber of H-LHS only account for 59.36% of to-tal LHS?s.
However, by incorporating MaxEnt RSmodels, that proportion increases to 81.44%, sincethe number of AH-LHS increases.
The reason isthat, we use the feature Prsnto reward ambiguoushierarchical LHS?s.
This has some advantages.
Onone hand, H-LHS can capture phrase reorderings.On the other hand, AH-LHS is more reliable thannon-ambiguous LHS, since most non-ambiguousLHS?s occur only once in the training corpus.In order to know how the MaxEnt RS modelsimprove the performance of the SMT system, we326study the best translation of Baseline and Base-line+MaxEnt RS.
We find that the MaxEnt RSmodels improve translation quality in 2 ways.5.1 Better Phrase reorderingSince the SCFG rules which contain nonterminalscan capture reordering of phrases, better rule se-lection will produce better phrase reordering.
Forexample, the source sentence ?...
[????????]1?
[????
???]2...
?
is translatedas follows:?
Reference: ... the five permanent members ofthe UN Security Council ...?
Baseline: ... the [United Nations SecurityCouncil]1[five permanent members]2...?
+MaxEnt RS: ... [the five permanentmembers]2of [the UN Security Council]1...The source sentence is translated incorrectly by thebaseline system, which selects the rule(11) X ?
?
X1?
X2, the X1X2?and produces a monotone translation.
In contrast,by considering information of the subphrases X1and X2, the MaxEnt RS model chooses the rule(12) X ?
?
X1?
X2, X2of X1?and obtains a correct translation by swapping X1and X2on the target-side.5.2 Better Lexical TranslationThe MaxEnt RS models can also help the decoderperform better lexical translation than the baseline.This is because the SCFG rules contain terminals.When the decoder selects a rule for a source-side,it also determines the translations of the source ter-minals.
For example, the translations of the sourcesentence ???????????
?b?
areas follows:?
Reference I?m afraid this flight is full.?
Baseline: I?m afraid already booked for thisflight.?
+MaxEnt RS: I?m afraid this flight is full.Here, the baseline translates the Chinese phrase????
into ?booked?
by using the rule:(13) X ?
?
X1?
?, X1booked?The meaning is not fully expressed since the Chi-nese word ???
is not translated.
However, theMaxEnt RS model obtains a correct translation byusing the rule:(14) X ?
?
X1?
?, X1full ?However, we also find that some results pro-duced by the MaxEnt RS models seem to decreasethe BLEU score.
An interesting example is thetranslation of the source sentence ?????????:?
Reference1: What is the name of this street??
Reference2: What is this street called??
Baseline: What is the name of this street??
+MaxEnt RS: What?s this street called?In fact, both translations are correct.
But the trans-lation of the baseline fully matches Reference1.Although the translation produced by the MaxEntRS model is almost the same as Reference2, asthe BLEU metric is based on n-gram matching,the translation ?What?s?
cannot match ?What is?in Reference2.
Therefore, the MaxEnt RS modelachieves a lower BLEU score.6 ConclusionIn this paper, we propose a generic lexicalized ap-proach for rule selection.
We build maximum en-tropy based rule selection models for each ambigu-ous hierarchical source-side of translation rules.The MaxEnt RS models combine rich context in-formation, which can help the decoder performcontext-dependent rule selection during decod-ing.
We integrate the MaxEnt RS models intothe hierarchical SMT model by adding two newfeatures.
Experiments show that the lexicalizedapproach for rule selection achieves statisticallysignificant improvements over the state-of-the-artsyntax-based SMT system.Furthermore, our approach not only can be usedfor the formally syntax-based SMT systems, butalso can be applied to the linguistically syntax-based SMT systems.
For future work, we will ex-plore more sophisticated features for the MaxEntRS models and integrate the models into the lin-guistically syntax-based SMT systems.327AcknowledgementsWe would like to show our special thanks to HweeTou Ng, Liang Huang, Yajuan Lv and Yang Liufor their valuable suggestions.
We also appreciatethe anonymous reviewers for their detailed com-ments and recommendations.
This work was sup-ported by the National Natural Science Foundationof China (NO.
60573188 and 60736014), and theHigh Technology Research and Development Pro-gram of China (NO.
2006AA010108).ReferencesBerger, A. L., S. A. Della Pietra, and V. J. Della.1996.
A maximum entropy approach to natural lan-guage processing.
Computational Linguistics, page22(1):39?72.Carpuat, Marine and Dekai Wu.
2007a.
How phrasesense disambiguation outperforms word sense dis-ambiguation for statistical machine translation.
In11th Conference on Theoretical and MethodologicalIssues in Machine Translation, pages 43?52.Carpuat, Marine and Dekai Wu.
2007b.
Improving sta-tistical machine translation using word sense disam-biguation.
In Proceedings of EMNLP-CoNLL 2007,pages 61?72.Chan, Yee Seng, Hwee Tou Ng, and David Chiang.2007.
Word sense disambiguation improves sta-tistical machine translation.
In Proceedings of the45th Annual Meeting of the Association for Compu-tational Linguistics, pages 33?40.Chiang, David.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics, pages 263?270.Chiang, David.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, pages 33(2):201?228.Galley, Michel, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Pro-ceedings of COLING-ACL 2006, pages 961?968.Huang, Liang and David Chiang.
2005.
Better k-best parsing.
In Proceedings of the 9th InternationalWorkshop on Parsing Technologies.Huang, Liang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of the 7th Bi-ennial Conference of the Association for MachineTranslation in the Americas.Koehn, Philipp, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedingsof HLT-NAACL 2003, pages 127?133.Koehn, Philipp.
2004a.
Pharaoh: a beam search de-coder for phrase-based statistical machine translationmodels.
In Proceedings of the Sixth Conference ofthe Association for Machine Translation in the Amer-icas, pages 115?124.Koehn, Philipp.
2004b.
Statistical significance tests formachine translation evaluation.
In Proceedings ofthe 2004 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), pages 388?395.Liu, Yang, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proceedings of the 44th Annual Meet-ing of the Association for Computational Linguistics,pages 609?616.Och, Franz Josef and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of the38th Annual Meeting of the Association for Compu-tational Linguistics, pages 440?447.Och, Franz Josef and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics, pages 295?302.Och, Franz Josef.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics, pages 160?167.Papineni, K., S. Roukos, T. Ward, and W.-J.
Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics,pages 311?318.Stolcke, Andreas.
2002.
Srilm ?
an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken language Process-ing, volume 2, pages 901?904.Xiong, Deyi, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for sta-tistical machine translation.
In Proceedings of the44th Annual Meeting of the Association for Compu-tational Linguistics, pages 521?528.Zens, Richard and Hermann Ney.
2006.
Discrimina-tive reordering models for statistical machine trans-lation.
In Proceedings of the Workshop on StatisticalMachine Translation, pages 55?63.Zhang, Ying, Stephan Vogel, and Alex Waibel.
2004.Interpreting bleu/nist scores: How much improve-ment do we need to have a better system?
In Pro-ceedings of the Fourth International Conference onLanguage Resources and Evaluation, pages 2051?2054.Zhang, Le.
2004.
Maximum entropy model-ing toolkit for python and c++.
available athttp://homepages.inf.ed.ac.uk/s0450736/maxent too-lkit.html.328
