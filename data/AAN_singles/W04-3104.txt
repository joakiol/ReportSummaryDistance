A Study of Text Categorization for Model Organism DatabasesHongfang LiuUniversity of Maryland at Bal-timore Countyhfliu@umbc.eduCathy WuGeorgetown UniversityMedical Centerwuc@georgetown.eduAbstractOne of the routine tasks for model organ-ism database curators is to identify andassociate research articles to database en-tries.
Such task can be considered as textcategorization which has been studied inthe general English domain.
The task canbe decomposed into two text categoriza-tion subtasks: i) finding relevant articlesassociating with specific model organ-isms, and ii) routing the articles to spe-cific entries or specific areas.
In thispaper, we investigated the first subtaskand designed a study using existing refer-ence information available at four well-known model organism databases andinvestigated the problem of identifyingrelevant articles for these organisms.
Weused features obtained from abstract textand titles.
Additionally, we studied the de-termination power of other MEDLINE ci-tation fields (e.g., Authors,MeshHeadings, Journals).
Furthermore,we compared three supervised machinelearning techniques on predicting towhich organism the article belongs.1 IntroductionWith the accelerated accumulation of genetic in-formation associated with popularly used geneticmodel organisms for the human genome projectsuch as laboratory mouse, C. elegans, fruit fly, andSaccharomyces, model organism databases thatcontain curated genetic information specifically tothe associated organism have been initialized andevolved to provide a central place for researchersto seek condense genetic information (Flybase,2003; Blake, 2003; Misra, 2003).
At the same time,a rich amount of genetic and biomedical informa-tion associated with these model organisms arepublished through scientific literature.
One of theroutine curation tasks for the database curators is toassociate research articles to specific genetic en-tries in the databases and identify key informationmentioned in the articles.
For example, a regularpractice of mouse genetic database curators is toscan the current scientific literature, extract andenter the relevant information into databases(Blake, 2003).
In Saccharomyces Genome Data-base (SGD, Saccharomyces cerevisiae:http://www.yeastgenome.org), database curatorsare currently in the process of revising the informa-tion associated with the Description field of an en-try to ensure that the Description (which usually isa concise summary of the function and biologicalcontext of the associated entry) contains the mostup-to-date information and is written in a consis-tent style.
One of the current objectives of Worm-Base (http://www.wormbase.org) is tosystematically curate the C. elegans literature.However, manually scanning scientific articles is alabor intensive task.
Meanwhile, the outcome maybe incomplete, i.e., curators may miss some criticalpapers.
Additionally, more than 2000 completedreferences are added daily to MEDLINE alone.
Itseems impossible to be always up-to-date.The task of associating research articles withspecific entries can be decomposed into two sub-tasks: i) categorizing articles into several catego-ries where articles with the same category areabout the same model organism, and ii) associatingthe articles to specific entries or specific areas.Finding relevant articles specific to a particularmodel organism is a case of information retrieval.A simple way to retrieve relevant articles about amodel organism is to retrieve articles containingterms that represent that organism.
For example, ifthe term ?C.
elegans?
appears in a paper, mostAssociation for Computational Linguistics.Linking Biological Literature, Ontologies and Databases, pp.
25-32.HLT-NAACL 2004 Workshop: Biolink 2004,likely, the paper is relevant to C. elegans.
Anotherway is to apply supervised machine learning tech-niques on a list of category-labeled documents.
Inthis paper, we designed a study on retrieving rele-vant articles using MEDLINE reference informa-tion obtained from four model organism databasesaiming to answer the following questions:?
Can we just use keywords to retrieve relevantMEDLINE references instead of using com-plicated machine learning techniques??
How accurate is the retrieval when we usevarious MEDLINE fields such as Authors,MeshHeadings etc to retrieve articles?Which kind of feature representations hasthe best performance??
Which kind of machine learning algorithm issuitable for categorizing the articles to theappropriate categories??
How good is the MEDLINE citation informa-tion when we used category-labeled docu-ments obtained in the past to predict thecategory of new documents?In the following, we first provide backgroundinformation about applying supervised machinelearning techniques on text categorization.
We thendescribe materials and methods.
Finally, we pre-sent our results and discussions.2 Background and Related WorkUsing keywords to retrieve relevant articles is touse a list of keywords to retrieve articles contain-ing these keywords.
The main component here isto derive a list of keywords for each category.
Us-ing supervised machine learning techniques to re-trieve relevant articles requires a collection ofcategory-labeled documents.
The objective is tolearn classifiers from these category-labeleddocuments.
The construction process of a classifiergiven a list of category-labeled documents containstwo components.
The first component transferseach document into a feature representation.
Thesecond component uses a supervised learning algo-rithm to learn classification knowledge that formsa classifier.Machine learning for text categorization re-quires transforming each document into a featurerepresentation (usually a feature vector) where fea-tures are usually words or word stems in thedocument.
In our study, in addition to word orword stems in free text, we also explored otherfeatures that could be extracted from the materialwe used for the study.Several supervised learning algorithms havebeen adapted for text categorization: Na?ve Bayeslearning (Yang and Liu, 1999), neural networks(Wiener, 1995), instance-based learning (Iwayamaand Takunaga, 1995), and Support vector machine(Joachims, 1998).
Yang and Liu (1999) providedan overview and a comparative study about differ-ent learning algorithms.
In previous studies of ap-plying supervised machine learning on the problemof word sense disambiguation, we investigated andimplemented several supervised learning algo-rithms including Na?ve Bayes learning, DecisionList learning and Support Vector Machine forword sense disambiguation.
There is not much dif-ference between word sense disambiguation taskand text categorization task.
We can formulate aword sense disambiguation task as a textcategorization task by considering senses of a wordas categories (Sebastiani, 2002).
We can alsoformulate a text categorization task by consideringthere is a hidden word (e.g., TC) in the text withmultiple senses (i.e., categories).
Note that in wordsense disambiguation task, one occurrence of aword usually holds a unique sense.
While for textcategorization task, sometimes one document canbe in multiple categories.
After verifying that therewere less than 1% of documents holding multiplecategories (shown in detail in the following sec-tion), for simplicity, we applied the implementa-tions of supervised machine learning algorithm(used for word sense disambiguation) directly fortext categorization by considering the disambigua-tion of a hidden word (TC) in the context.
The fol-lowing summarizes the algorithms used in thestudy.
For detail implementations of these algo-rithms, readers can refer to (Liu, 2004).Na?ve Bayes learning (NBL) (Duda, 1973) iswidely used in machine learning due to its effi-ciency and its ability to combine evidence from alarge number of features.
An NBL classifierchooses the category with the highest conditionalprobability for a given feature vector; while thecomputation of conditional probabilities is basedon the Na?ve Bayes assumption: the presence ofone feature is independent of another when condi-tioned on the category variable.
The training of theNa?ve Bayes classifier consists of estimating theprior probabilities for different categories as wellas the probabilities of each category for each fea-ture.05001000150020002500300035004000450050001965196819711974197719801983198619891992199519982001YearNumberofCitationsYeast Fly Worm Mouse00.020.040.060.080.10.121965196819711974197719801983198619891992199519982001YearProportionYeast Fly Worm MouseThe Decision List method (DLL) (Yarowsky,1994) is equivalent to simple case statements inmost programming languages.
In a DLL classifier,a sequence of tests is applied to each feature vec-tor.
If a test succeeds, then the sense associatedwith that test is returned.
If the test fails, then thenext test in the sequence is applied.
This continuesuntil the end of the list, where a default test simplyreturns the majority sense.
Learning a decision listclassifier consists of generating and ordering indi-vidual tests based on the characteristics of thetraining data.
(a)Support vector machine (SVM) (Vapnik,1998) is a supervised learning algorithm proposedby Vladimir Vapnik and his co-workers.
For a bi-nary classification task with classes {+1, ?1},given a training set with n class-labeled instances,(x1, y1), (x2, y2), ..., (xi, yi), ?, (xn, yn), where xiis a feature vector for the ith instance and yi indi-cates the class, an SVM classifier learns a lineardecision rule, which is represented using a hyper-plane.
The tag of an unlabelled instance x is deter-mined by which side of the hyperplane x lies.
Thepurpose of training the SVM is to find a hyper-plane that has the maximum margin to separate thetwo classes.Using a list of keywords to retrieve relevant ar-ticles has been used frequently for NLP systems inthe biological domain.
For example, Iliopoulos etal.
(2001) used keywords pertinent to a biologicalprocess or a single species to select a set of ab-stracts for their system.
Supervised machine learn-ing has been used by Donaldson et al (2003) torecognize abstracts describing bio-molecular inter-actions.
The training articles in their study werecollected and judged by domain experts.
In ourstudy, we compared keywords retrieving with su-pervised machine learning algorithms.
The cate-gory-labeled training documents used in our studywere automatically obtained from model organismdatabases and MEDLINE.3 Material and Methods3.1 Model Organism DatabasesThe research done here is based on MEDLINEreferences associated with four model organisms(i.e., mouse, fly, worm and yeast) obtained fromMouse Genome Informatics (MGD, Mus musculus:http:// www.informatics.jax.org, FlyBase (Droso-phila melanogaster.
http://www.flybase.org),WormBase (Caenorhabditis elegans:http://www.wormbase.org), and SaccharomycesGenome Database (SGD, Saccharomyces cere-visiae: http://www.yeastgenome.org).
Wedownloaded literature reference information fromeach database on March 2003.
All databases pro-vide PMID (unique identifier for MEDLINE cita-tions) information except WormBase where somereferences use MEDLINEID (another unique iden-tifier for MEDLINE citations) as reference identi-fiers, some references use PMID as referenceidentifiers.
Meanwhile, about two thirds of the ref-erences in WormBase do not have reference identi-fiers to MEDLINE, which we eliminated in ourstudy since we were not able to get the MEDLINEFigure 1.
References for four organism databasesfrom 1966 to 2002.
X-axis represents years from1965 to 2003 in ascending order.
The Y axis in figure(a) represents the number of citations.
The Y-axis infigure (b) represents the proportion of each yearcomparing to the total number of citations for a spe-cific organism.
(b)citation information.
We then used e-Fetch toolsprovided by Entrez (http://entrez.nlm.nih.gov) andfetched complete MEDLINE citations in XMLformat from MEDLINE.Year Tra Te AbT  ArT Aut Jou MH1990 21563 2963 251  0 0  11 31991 24526 3394 250  0 2  21  01992 27920 4355 300 0 1 8 21993 32275 5267 391 0 1 46  01994 37542 6474 450 0 3 34 11995 44016 7178 490 0 4 12  11996 51194 7782 613 0 2 3 41997 58976 8115 593 0 4 8 101998 67091 7726 519 0 2 8 121999 74817 9057 599 0 10 15 232000 83874 9234 587 0 4 16 282001 93108 8479 362 0 4 16 3092002 101587 7688 237 0 6 7 13662003 109275 569 6 0 0 0 146Total NA 88281 5647 0 43 206 1905Finally, we obtained 31,414 MEDLINE cita-tions from Flybase, 26,046 from SGD, 3,926 fromWormBase, and 48,458 from MGD.
Figure 1 liststhe statistical information according to the publica-tion date for each organism, where X-axis repre-sents year, Y-axis in Fig.
1(a) represents thenumber of citations and Y-axis in Fig.
1(b) repre-sents the percentage of the number of citations tothe total number of citations for each organism.Note that there were 1,005 citations holding multi-ple categories (15 of them were referred by mouse,fly and yeast, 1 referred by fly, worm, and mouse,338 referred by mouse and yeast, 282 referred byfly and yeast, 310 referred by fly and mouse, 9 re-ferred by worm and yeast, 36 referred by fly andworm, 5 referred by mouse and worm).
However,comparing to the total of 109,844 citations, therewere less than 1% of citations with multiple cate-gories.
For simplicity, we defined our categoriza-tion task as a single category text categorizationtask.Table 1.
The number of citations for training(Tra), testing (Te) for each year.
Note that somefields in certain MEDLINE citations may beempty (e.g., not all references have abstracts), thenumber of these non-applicable citations for fea-3.2 MethodsWe studied Taxonomy from NCBI(http://www.ncbi.nlm.nih.gov) and UMLS knowl-edge sources (http://umlsks.nlm.nih.gov) and de-rived a list of keywords for each organism andused them to retrieve relevant articles.
If the title,the abstract or Mesh Headings of a MEDLINE ci-tation contains these keywords, we considered it asa relevant article.
Table 1 shows the list of key-words we obtained for each model organism.MEDLINE citations also contain other informa-tion such as Authors, Mesh Headings, and Journalsetc besides abstracts and titles.
Based on the intui-tions that biologists tend to use the same organismfor their research and a specific journal tend topublish papers in a limited number of areas, wealso evaluated Authors and Journals as features.Additionally, Mesh Headings which were assignedmanually by librarians to index papers representkey information of the papers, we also evaluatedthe categorization power of Mesh Headings in de-termining which organism the paper belongs to.We then combined some or all features togetherand evaluated the prediction power.3FclyeTthfofrture representations abstracts (AbT), titles (ArT),authors (Aut), Journals (Jou), and Mesh Headings(MH) for each year.MOUSE Mouse, mice, mus muscaris,mus musculus, mus spYEAST Saccharomyces, yeast, yeasts,candida robusta, oviformis,italicus, capensis, uvarum, ere-visiaeFLY drosophila, fly, fliesWORM Elegans, worm, wormsTable 2.
Keywords used to retrieve relevantarticles for four model organisms mouse,yeast, fly and worm..3 Experimentsor each year from 1990 to 2003, we trained aassifier using citations published in all previousars and tested using citations in the current year.able 2 lists the detail about the training set ande test set for each year.
We experimented thellowing feature representations: stemmed wordsom AbstractText, stemmed words from Title,Author, MeshHeading, and Journals.
Since someof the MEDLINE fields may be empty (such assome citations do not contain abstracts), Table 2also provides the number of non-applicable refer-ences each year for a given feature representationmethod.
From Table 2, we found that every cita-tion has a title.
However, there are about 6.4% ofcitations (5,647 out of 88,281) that do not haveabstracts.
For each feature representation, we ap-plied three supervised learning algorithms (i.e.,Na?ve Bayes learning, Decision List learning, Sup-port Vector Machine).For each combination of machine learning algo-rithm and feature representation, we computed theperformance using the F-measure, which is definedas 2*P*R/(P+R), where P is the precision (thenumber of citations predicted correctly to the totalnumber of citations being predicted) and R is therecall (the number of citations predicted correctlyto the total number of citations).We then sorted the feature representations ac-cording to their F-measures and gradually com-bined them into several complex featurerepresentations.
The feature vector of a complexfeature representation is formed by simply combin-ing the feature vector of its members.
For example,suppose the feature vector of feature representationusing stemmed words from the title contains anelement A and the feature vector of feature repre-sentation using stemmed words from the abstractcontains an element B, then the feature vector ofthe complex representation obtained by combiningstemmed words from title and stemmed wordsfrom abstracts will contain the two elements: Title:A and Abstract: B.
These feature representationswere then combined with the machine learningalgorithm that has the best overall performance tobuild text categorization classifiers.
Similarly, weevaluated these complex feature representationsusing citations published in all previous years astraining citations and tested using citations pub-lished in the current year.4 Results and DiscussionTable 3 shows the detail F-measure obtained foreach combination of machine learning algorithm,year, and feature representation.
Among them,Support Vector machine along with stemmedwords in abstracts achieved the best F-measure(i.e., 90.5%).
Decision list learning along withstemmed words in titles achieved the second bestF-measure (i.e., 90.1%).
Feature representationusing Mesh Headings along with Decision listlearning or Support Vector machine has the thirdbest F-measure (i.e., 88.7%).
Feature representa-tion using Author combined with Support VectorMachine has an F-measure of 71.8%.
Feature rep-resentation using Journals has the lowest F-measure (i.e., 62.1%).
From Table 3, we can seethat Support Vector Machine has the best perform-ance for almost each feature representation.Note that the results for feature representationAuthors were significantly worse for year 2002.After reviewing some citations, we found that theformat of the author field has changed since year2002 in MEDLINE citations.
The current formatresults in less ambiguity among authors.
However,we could not use the author fields of citations fromprevious years to predicate the category of docu-ments for year 2002.
Also, since a lot of citationsin years 2002 and 2003 are in-process citations(i.e., people are still working on indexing thesecitations using Mesh Headings), feature representa-tion using Mesh Headings had worse performancein these two years comparing to other years.According to the reported performance, we ex-plored the following feature representations: i).stemmed words from titles and stemmed wordsfrom abstracts, ii) Mesh Headings, stemmed wordsfrom titles, and stemmed words from abstracts, iii)Authors, Mesh Headings, stemmed words fromtitles, and stemmed words from abstracts, and iv)Journals, Authors, Mesh Headings, stemmed wordsfrom titles and stemmed words from abstracts.Figure 2 shows the performance of these featurerepresentations when using support vector machineas machine learning algorithm.
Note that the F-measures for complex feature representations thatcontain Abstract, Title, and Mesh Headings areindistinguishable.
The inclusion of addition fea-tures such as Authors or Journals does not improveF-measure visibly.
Figure 2 also includes the meas-ure for keyword retrieving, which is different fromthe measure for each complex feature representa-tion.
The performance of keyword retrieving ismeasured using the ratio of the number of citationsin each organism that contain keywords from thelist of keywords obtained for that organism to thetotal number of citations for the organism.
Themeasure for each complex feature representation isthe F-measure obtained using support vector ma.MeshHeading Journal Author AbstractText ArticleTitle YearDLL NBL SVM DLL NBL SVM DLL NBL SVM DLL NBL SVM DLL NBL SVM1990 94.3 88.8 93.7 56.4 56.1 58.5 82.6 80.9 84.7 89.1 88.7 91.8 94.5 90.4 94.41991 92.7 88.3 93.5 55.4 56.6 57.3 81.3 77.9 83.4 88.7 88.4 91.7 92.2 89.5 92.31992 92.7 88.4 92.7 55.0 59.8 57.2 76.8 71.2 78.6 88.1 88.6 91.8 91.9 89.9 91.71993 93.0 88.6 92.9 60.0 60.3 58.6 76.8 70.6 76.2 87.5 87.1 91.0 91.5 89.1 91.31994 93.0 89.7 92.9 61.1 61.9 60.6 74.0 66.8 74.2 88.9 88.9 91.0 92.3 89.7 91.51995 93.3 90.7 92.5 63.2 63.4 63.1 76.5 67.2 73.3 88.0 88.6 91.2 92.0 89.2 89.41996 92.0 88.9 90.8 64.1 64.2 63.8 75.7 65.4 74.1 85.8 87.2 90.0 90.8 88.5 87.81997 90.6 87.5 90.5 63.9 64.0 64.4 75.8 65.2 72.8 85.1 86.0 89.7 89.8 86.7 87.81998 90.6 87.6 91.2 61.1 62.1 61.8 76.0 65.4 73.7 84.1 86.4 89.8 89.7 85.6 87.91999 89.7 87.1 88.9 61.8 63.3 62.4 73.3 64.1 69.5 84.3 86.2 89.6 88.5 85.3 85.82000 88.4 84.2 88.0 60.8 61.0 62.4 74.0 63.0 71.0 83.5 85.7 88.9 87.7 84.2 86.52001 87.0 84.9 87.7 62.4 62.7 62.4 74.4 62.5 68.3 85.0 86.8 91.0 89.1 85.3 87.22002 63.8 19.9 67.3 62.7 64.5 63.6 3.8 8.8 53.9 86.2 88.2 91.7 88.7 84.8 86.42003 77.6 76.0 76.0 48.0 48.0 54.5 62.2 53.1 63.1 85.3 89.0 92.8 83.3 87.0 83.7Overall 88.7 82.1 88.8 61.3 62.1 61.8 69.3 61.6 71.8 86.0 87.2 90.5 90.1 87.0 88.5KeywordAbstractTable 3.
The F-measure of supervised text categorization study on different combination of super-vised machine learning algorithms and feature representations.
The classifiers trained using citationspublished previous years and tested using citations published in the current year.FVtNslAbstact+TitleAbstract+Title+MeshHeadingAbstract+Title+MeshHeading+AuthorAbstract+Title+MeshHeading+Author+Journal80%82%84%86%88%90%92%94%96%98%100%19901991199219931994199519961997199819992000200120022003igure 2.
The F-measure of classifiers using complex feature representations learned using Supportector Machine and the percentage of the number of citations containing keywords associated withhe corresponding organism comparing to the total number of citations associated with that organism.ote that F-measures for complex feature representations Abstract+Title+MeshHeading, Ab-tract+Title+MeshHeading+Author, and Abstract+Title+MeshHeading+Author+Journal are over-apped with each other.chine which was trained using citations from allprevious years and tested using citations in currentyear.From the study, we answered at least partiallythe questions.
We cannot just simply use keywordsto retrieve MEDLINE citations for model organismdatabases.
From Figure 2, we can see that usingkeywords to retrieve citations may miss 20% of thecitations.
However, when combining all featurerepresentation together, using citations from previ-ous years could correctly predict to which organ-ism the current year citations belong with anoverall F-Measure of 94.1%.For the supervised learning on text categoriza-tion task, different MEDLINE citation fields havedifferent power on predicting to which model or-ganism the paper belongs.
Feature representationusing stemmed words from abstracts has the moststable and highest predicting power with an overallF-measure of 90.5%.
Authors alone can predict thecategory with an overall F-measure of 71.8%.Among three supervised machine learning algo-rithms, support vector machine achieves the bestperformance.
For feature representations wherethere are only a few features in a feature vectorwith non-zero values, decision list learningachieved comparable performance with (some-times superior than) support vector machine.
Forexample, decision list learning achieved an F-measure of 90.1% when using stemmed wordsfrom titles as feature representation method, whichis superior than support vector machine (with an F-measure of 88.5%).
Consistent with our findings in(Liu, 2004), the performance of Na?ve Bayes learn-ing is very unstable.
For example, when usingstemmed words from abstracts, the performance ofNa?ve Bayes learning is comparable to the othertwo machine learning algorithms.
However, whenusing Mesh Headings as feature representationmethods, the performance of Na?ve Bayes learning(with an F-measure of 82.1%) is much worse thandecision list learning and support vector machine(with F-measures of over 88.0%).One limitation of the study is that we used onlyabstracts that are about one of the four model or-ganisms.
The evaluation would be more meaning-ful if we could include abstracts that are outside ofthese four model organisms.
However, suchevaluation would involve human experts since wecan not grantee that abstracts that are not includedin these four model organism databases are notabout one of the four model organisms.
That is alsothe reason we cannot provide F-measures when weevaluated the performance of keyword retrievingsince we cannot grantee that abstracts associatedwith one organism are not related to another organ-ism since the list of references in each organismdatabase is not complete.We could use previous published articles to-gether with their categories to predict categories ofthe current articles where the list of categories isnot limited to model organisms.
It could be othercategories such as the main themes for each para-graph in each paper.
We will conduct a serial ofstudies on text categorization in the biomedicalliterature under the condition of the availability ofcategory-labeled examples.
One future projectwould be to apply text categorization on citationinformation for the protein family classificationand annotation in Protein Information Resources(Wu, 2003).As we know, homologous genes are usuallyrepresented in text using the same terms.
Knowingto which organism the paper belongs can reducethe ambiguity of biological entity terms.
For ex-ample, if we know the paper is related to mouse,we can use entities that are specific to mouse forbiological entity tagging.
Future work will becombining text categorization with the task of bio-logical entity tagging to reduce the ambiguity ofbiological entity names.5 ConclusionIn this paper, we designed a study using existingreference information available at four well-knownmodel organism databases and investigated theproblem of identifying relevant articles for theseorganisms using MEDLINE.
We compared theresults obtained using keyword searching with su-pervised machine learning techniques.
We foundout that keyword searching retrieved about 80% ofthe citations.
When using supervised machinelearning techniques, the overall F-measure of thebest classifier is around 94.1%.
Future work wouldbe applying the supervised machine learning tech-nique to the whole MEDLINE citation to retrieverelevant articles.
Also we plan to apply text clus-tering techniques or text categorization techniquesfor the routing problem inside a specific modelorganism database (such as routing to curators in aspecific area).AcknowledgementWe thank anonymous referees for their valuable com-ments and insights.
This work was supported in part bygrant EIA-031 from the National Science Foundation.ReferencesThe FlyBase Consortium.
The Flybase database of theDrosophila genome projects and community litera-ture, Nucleic Acid Research 2003, Vol 31 (1) 172-175Blake J, Richardson J.E., Bult C.J., Kadin J.A., EppigJ.T.
MGD: the Mouse Genome Database, NucleicAcid Research, 2003, Vol 31 (1) 193-195Donaldson I, Martin J, de Bruijn B, et al PreBIND andTextomy--mining the biomedical literature for pro-tein-protein interactions using a support vector ma-chine  BMC Bioinformatics.
2003 Mar 27; 4(1): 11.Duda R, Hart P. ?Pattern Classification and SceneAnalysis?.
John Wiley and Sons, NY, 1973.Harris,T.W., Lee,R., Schwarz,E., et al WormBase: across-species database for comparative genomics.Nucleic Acids Research 2003, Vol 31, 133?137.Iliopoulos I, Enright AJ, Ouzounis CA.
Textquest:document clustering of Medline abstracts for conceptdiscovery in molecular biology, Pac Symp Biocom-put.
2001; 384-95.Iwayama M, Tokunaga T. Cluster-based text categori-zation: a comparison of category search strategies,SIGIR 1995, 273-281Joachims T. Text categorization with support vectormachines: learning with many relevant features,ECML 1998, 137-142Liu H, V. Teller, C. Friedman.
A Multi-Variable Com-parison Study of Supervised Word Sense Disam-biguation.
Submitted to JAMIAMisra S, Madeline A.C., Mungall C.J., et al Annotationof the Drosophila melanogaster euchromatic ge-nome: a systematic review, Genome Biology 2002, 3(12)Sebastiani F. Machine learning in automated text cate-gorization.
ACM Computing Surveys, 2002, Vol 34(1) 1-47Vapnik.
Statistical Learning Theory John Wiley andSons, NY, 1998Wiener ED, Pedersen JO, Weigend AS.
A neural net-work approach to topic spotting, SDAIR 1995, 317-332Wu CH, L Yeh, H Huang, L Arminski, et al The Pro-tein Information Recource Nucleic Acids Research,31: 345-347Yang Y, Liu X.
A re-examination of text categorizationmethods, SIGIR 1999, 42-49Yarowsky D. Decision lists for lexical ambiguity resolu-tion: Application to accent restoration in Spanishand French.
1994; ACL 32: 88-95
