Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2343?2349,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsUnsupervised Timeline Generation for Wikipedia History ArticlesSandro Bauer and Simone TeufelComputer LaboratoryUniversity of CambridgeCambridge, United Kingdom{sandro.bauer, simone.teufel}@cl.cam.ac.ukAbstractThis paper presents a generic approach to con-tent selection for creating timelines from in-dividual history articles for which no exter-nal information about the same topic is avail-able.
This scenario is in contrast to existingworks on timeline generation, which requirethe presence of a large corpus of news arti-cles.
To identify salient events in a given his-tory article, we exploit lexical cues about thearticle?s subject area, as well as time expres-sions that are syntactically attached to an eventword.
We also test different methods of en-suring timeline coverage of the entire histori-cal time span described.
Our best-performingmethod outperforms a new unsupervised base-line and an improved version of an existingsupervised approach.
We see our work as astep towards more semantically motivated ap-proaches to single-document summarisation.1 IntroductionWhile there has been much work on generating his-tory timelines automatically, these approaches arecommonly evaluated on events that took place in re-cent decades, as they depend on the availability oflarge numbers of articles describing the same his-torical period.
If such a rich data source is avail-able, it is possible to exploit document creationtimes, redundancy across documents, as well asback-references to earlier events in order to identifysalient events.
For instance, the start of the Iraq Warin 2003 is mentioned frequently in a general newscorpus, including in articles published years after theevent took place.
The high number of mentions sug-gests that the beginning of the Iraq War was an im-portant historical event.However, for most historical periods covered inhistory articles (e.g., Antiquity or the Middle Ages),such cues are not commonly available, as no newsarticles from these eras exist.
Generating event time-lines for arbitrary historical periods is therefore amuch harder problem, which requires methods thatrely less heavily on the types of rich, parallel anddense information contained in news clusters.To investigate this problem, we approach timelinegeneration as a special single-document summarisa-tion task.
In other words, we assume that the in-formation to be summarised is contained in a singlehistory article, and that no further mentions of spe-cific events exist externally.
This is a realistic sce-nario, for instance, for a specialist article describingthe history of music in Ancient China.We introduce a method for selecting salient con-tent in history articles of any subject area, as long asthe events in the text are roughly ordered chronolog-ically.
The hypothesis is that knowledge of an text?ssubject area can help decide which content shouldbe selected.
Another intuition is that certain com-binations of events should be avoided in a timeline.We therefore investigate ways of encouraging a bal-anced selection of content from all parts of the text.2 Related workTimeline extraction has mostly been explored ina multi-document summarisation setting using cor-pora of news articles (Tran et al, 2015; Swan andAllan, 2000; Yan et al, 2011; Chieu and Lee, 2004;2343Allan et al, 2001).
This task definition allows theexploitation of features such as document creationtimes and headlines.
The most important featureis redundancy between articles, which facilitates theidentification of salient events.A second important strand of work focuses on ex-tracting all events from a single input text and an-choring them in time.
The creation of the TimeMLspecification language (Pustejovsky et al, 2003) laidthe foundations for the TempEval series of sharedtasks (Verhagen et al, 2007; Verhagen et al, 2010;UzZaman et al, 2013), in which systems had toidentify TimeML events and temporal expressionsin free-form text.
Further subtasks included the nor-malisation of temporal expressions and the creationof links between events and temporal expressions.
Afurther shared task investigated the use of TimeMLannotation for the downstream task of question an-swering (Llorens et al, 2015).
Kolomiyets et al(2012) created a connected timeline for a text basedon TimeML annotations; a dependency parser infersdependency structures between events.
Finally, a re-cent SemEval task (Minard et al, 2015) explored therelated problem of cross-document event ordering.Here, relevant events and temporal expressions con-cerning a single target entity of interest have to beidentified in more than one input document.Chasin et al (2014) try to identify importantevents in single texts, but their approach is limited toarticles on wars and battles, and the problem is notapproached as a summarisation task.
Their methodis lightly supervised, using features such as the pres-ence of negation or past tense verbs in the sentence,and TextRank (Mihalcea and Tarau, 2004) for identi-fying salient sentences.
We use an improved versionof this system as a baseline.3 Overall approachOur problem is that of finding an optimal sequenceof events (of a given maximum length) in a giveninput article.
We follow the literature on event ex-traction and use TimeML events (Pustejovsky etal., 2003).
Most TimeML events are verbs, butsome are nominalisations such as ?invasion?
orother event-like words such as ?war?.
The use ofTimeML events, aside from the practical advan-tage that commonly-available event extraction algo-rithms exist, allows us to evaluate content selectionat the event rather than at the sentence level.We assume that there are both local and globalfactors that determine which events should be con-tained in the timeline.
Local factors reflect howimportant an event is in its own right.
Global fac-tors represent intuitions about which combinationsof events should or should not be selected.
Our ap-proach, which is unsupervised, takes into accountthe factors described in what follows.3.1 Date presenceIntuitively, we expect that many important eventshave a date attached to them, as authors tend to givethe reader this information if it is available.
Thisis true for all historical periods from prehistory on-wards, since for most events at least an approxi-mate date is known.
We considered two alternatives:The simplest approach is to only use sentences thatcontain a date, regardless of where in the sentencethe date is located.
A more sophisticated alterna-tive verifies that the date is syntactically attached tothe event, such as in ?Richelieu died in 1642?.
Toidentify such cases, we constructed a parse tree us-ing the C&C dependency parser (Clark and Curran,2007) and only considered a TimeML event to be?dated?
if it is at most two outgoing dependenciesaway from a temporal expression.
We used Heidel-Time (Stro?tgen and Gertz, 2013), a the state-of-the-art temporal expression software package, to iden-tify such temporal expressions.3.2 Lexical cuesThe key component we use to judge the importanceof any event are lexical cues about the input text?ssubject area.
Examples of such subject areas in-clude INVENTION and FOOD/DRINK.
The subjectarea of a text should give us prior knowledge aboutwhich types of events are likely to be important.
Forinstance, we would expect that a timeline describingthe history of a country should contain informationabout revolutions, invasions, elections and similarevents, whereas a timeline about science will insteadfocus on discoveries, publications, and Nobel prizes.To mine knowledge about such subject-area-specific preferences, we make use of Wikipedia asa background corpus.
Only history-specific articleswhose title starts with ?History of?
are considered.2344We start by generating sets of all Wikipedia historyarticles belonging to a given subject area, e.g.
AGPEor AINVENTION.
To do this, we make use of theWikipedia category system.
For instance, for con-structing a set of articles for the subject area FIELDOF SCIENCE, we collected all history articles thatbelong to the Wikipedia category ?History of sci-ence by discipline?.
For each subject area g, we thencalculate a preference score for each word lemma lfound in any of the articles in the corresponding listAg, using the following formula:sc(g, l) =freq(Ag ,l)freq(Ag ,?)freq(?,l)freq(?,?
)where freq(Ag, l) is the summed frequency of wordlemma l in all documents belonging to subject areag, and ?*?
stands for any.
The numerator de-notes how often lemma l appears in the subject-area-specific set of articles Ag, normalised by the totalnumber of tokens found in this set.
The denomina-tor is invariant across all subject areas.
If the ratio ishigh, lemma l is more likely to appear in articles ofsubject area g than in Wikipedia overall, suggestingthat it is typical for the given subject area.For each event e in the input text, a local impor-tance score imp(e) is calculated asimp(e) =?w?R(e)sc(g,l)1+dist(we,w)Nwhere R(e) is a window of words around the wordrepresenting the event (including the event wordwe itself), and dist(w1, w2) refers to the absolutedistance in words between two words w1 and w2.imp(e) is a weighted average of the preferencescores of all words in a window.
The intuition isthat context words of the event word can also be ex-pected to be indicative of the subject area (consider?publish a paper?)
in many cases.
1+dist(we, w) isused as a distance penalty in order to give more im-portance to words that are closer to the event wordwe.
N is a constant which normalises the score bythe sum of all distance penalties, to account for caseswhere the event word occurs at the beginning or endof a sentence.
Table 1 shows examples of words withhigh and low preference scores.3.3 Temporal coverageWe would like to avoid cases where too many eventsare selected from a small portion of the document,GPE INVENTION FOOD/DRINKabsolutism gas-works yerbaprotectorate reverse-angle hamburgerserfdom flashback saffronclub season playgame team memberseason school bearTable 1: Words with high (top) and low (bottom) preferencescores for three subject areaseven if all these events are relevant.
For instance,an article might list all a country?s elections of thepast few years, while mentioning only very impor-tant elections in earlier time periods.
In this case,knowing that elections are important in the historyof a country is not helpful, since this would lead toinsufficient coverage of the remaining events in thearticle.
We therefore take into account global factorsas well.
We experiment with two different methods:Exploiting document structure.
We selectsalient events from each section of the Wikipediaarticle in a round-robin fashion.
The algorithmoperates in a greedy fashion by selecting the mostlocally important remaining event for each section,until the desired timeline length has been reached.Integer linear program.
We use an integer linearprogram to encode the intuition that no two timelineentries should have the same year.
The ILP max-imises the following objective function for each ar-ticle (E refers to the set of all dated events):?ei?Exi ?
imp(ei)?
?ei?E?ej?Ebij ?
pen(ei, ej)subject to the constraints:bij ?
xi ?i, j ?
Exi + xj ?
bij ?
1 ?i, j ?
Exi ?
{0, 1} ?i ?
E bij ?
{0, 1} ?i, j ?
E?ei?Exi = LmaxThis is similar to the model used by McDon-ald (2007) for multi-document summarisation.
Themodel tries to find a set of locally important eventswhile discouraging the selection of events that havethe same date.
xi is a variable denoting whetherthe corresponding event ei has been selected.
bijis a variable which is 1 if and only if both events iand j have been selected.
pen(ei, ej) is a penaltyfunction that is 1 if the two events ei and ej have2345the same date, otherwise 0.
Each event was linkedto the preceding temporal expression identified byHeidelTime; this heuristic was found to work well.The last constraint ensures that not more than Lmaxevents are chosen, where Lmax is the desired time-line length for the article considered.4 EvaluationFor evaluating our algorithms, the methodology weintroduced in (Bauer and Teufel, 2015) is used,along with the accompanying Cambridge Single-Document Timeline Corpus (CSDTC, version 2.0),which has been made publicly available1.4.1 Cambridge Single-Document TimelineCorpusThe CSDTC contains 10 articles from 3 sub-ject areas: GPE (geo-political entities such ascountries and cities), FIELD OF SCIENCE andINVENTION.
To tune our algorithms, we con-structed a development set of a further 30 annotatedhistory articles from the subject areas in the CSDTCand one additional subject area (FOOD/DRINK).Due to the high annotation cost, only a single time-line creator was used.
Important events were di-rectly marked up in the source text (as opposed tothe CSDTC, where timeline entries were written byhand), and exactly one HCU2 was created per event.Using this development corpus, the window size ofwords considered for calculating local importancescores (cf.
Section 3.2) was set to 3.
We report theperformance of all algorithms on both the develop-ment set and the test set (the CSDTC).Although the number of subject areas in the twocorpora is rather small owing to the considerable an-notation effort, we believe that the resulting systemwould generalise rather well to other subject areas,were they added, as the subject areas in the corpusare very different in nature from each other.
Carewas taken when constructing the CSDTC to use aset of subject areas that is representative for human-written timelines on the Web.1The corpus is available on the first author?s website:http://www.cl.cam.ac.uk/?smb89/form.html2As opposed to the CSDTC, HCUs in the development setalways have a weight of 1, as only timeline writer was used.4.2 Evaluation based on Historical ContentUnitsThe evaluation is based on abstract (?deep?)
mean-ing units called Historical Content Units (HCUs).HCUs were derived on the basis of human-createdtimelines.
Between 32 and 80 HCUs per article wereannotated for the articles in the CSDTC.Each HCU is weighted by the number of time-line creators who expressed its semantic contentin their timelines.
Because HCUs are linked toTimeML events in the surface text, it is possible toperform automatic deep evaluation without requir-ing any manual annotation of system summaries.Algorithms are evaluated on a given input arti-cle using an adapted version of the pyramid score(Nenkova and Passonneau, 2004), which is calcu-lated as the ratio between the sum of all rewards forHCUs chosen by the algorithm normalised by themaximum possible score scoremax:score =?h?HCUswh?Cov(h,E,T )scoremaxwhere wh is the weight of HCU h (a number be-tween 1 and the number of annotators), E is the setof events in the article, T are the events in the sys-tem timeline, and the coverage score Cov(h,E, T )is a number between 0 and 1 that indicates to whatextent the events chosen by the algorithm jointly ex-press the semantic content of HCU h. The basic ver-sion of Cov(h,E, T ) is defined as follows:Cov(h,E, T ) = min(1.0,?ej?E vh,ej ?
s(T, ej))where vh,ej is an anchor weight between 0 and 1which denotes to what extent event ej expresses thesemantic content of HCU h, and s(T, e) is a helperfunction that returns 1 if the set of selected events Tincludes event e, and 0 otherwise.The coverage score for each HCU is calculatedby summing up the anchor weights of those eventsthat the algorithm has selected.
A coverage score of0 means that the events mentioned in the timelinedo not express the HCU?s semantic content at all,while a score of 1 occurs where the HCU?s contentis fully expressed by the timeline.
Scores between 0and 1 occur in a large number of cases.
For instance,an HCU may express the fact that a country was in-vaded and destroyed.
If the system timeline merelycontains a TimeML event that refers to the invasion,it is assigned a coverage score of 0.5 for this HCU,2346as it expresses only half of the HCU?s semantic con-tent.
Where the sum exceeds 1, the coverage score isset to a hard upper limit of 1.
This ensures that algo-rithms are not doubly rewarded for selecting mul-tiple TimeML events expressing the same seman-tic content.
The final formula we used to calculatecoverage scores is slightly more complex, as someTimeML events in the CSDTC have been groupedtogether into event groups.
A detailed description isgiven in the documentation of the corpus.Pyramid scores are recall-based: The evaluationassumes a maximum number of timeline entries n,and the maximum possible score is the sum of theHCU weights of the n most highly weighted HCUs.The values for n are given in the CSDTC.4.3 System and baselinesWe report the performance of two systems.
Bothsystems first remove all events that do not have adate, or whose date is too far away, as describedin Section 3.1.
Our first system (?ILP-based?)
se-lects events based on the integer linear program de-scribed, while the second system (?Round-robin?
)selects locally important events per section.We have speculated above that dates are impor-tant for our task.
We therefore compare against adate baseline which selects events randomly fromthe list of all dated events.
We also compare againstseveral modified versions of our method: To inves-tigate the influence of the parser in identifying suit-able dated events, we report the results for a simplermethod which considers all events that have a date inthe same sentence (?Round-robin, simple date crite-rion?).
Two alternative systems select locally impor-tant events from all (not only dated) events (?Round-robin, without date criterion?)
or salient dated eventsfrom the entire article without considering documentstructure (?Local importance + date criterion?
).The supervised baseline (?Chasin et al (2014)?
)was re-implemented using LibSVM (Chang and Lin,2011), and SVM parameters were tuned using gridsearch.
25 of the 30 articles were used for trainingand 5 for development.
We improved their systemby defining some of their sentence-level features atthe event level.
Probability estimates as describedby Platt (2000) were used as importance scores.System Dev TestILP-based 0.22N 0.30NRound-robin 0.20N 0.30NRound-robin w/o local importance 0.18 0.26Local importance + date criterion 0.21N 0.29Round-robin, simple date criterion 0.19 0.25Round-robin without date criterion 0.14 0.18Date baseline 0.18 0.25Chasin et al (2014) (improved) ?
0.12Random baseline 0.08 0.10Table 2: Average pyramid scores across all articles (N = signif-icantly better than the date baseline)4.4 ResultsThe results in Table 2 show that only a combina-tion of all three factors (date presence, local impor-tance, coverage) results in a statistically significantimprovement over the date baseline at ?
= 0.05 ac-cording to Wilcoxon?s signed-rank test on the testset.
Both our systems perform comparably on thetest set; removing any of the three components re-sults in lower performance.
Using a parser to iden-tify dated events has a strong positive effect (see?Round-robin, simple date criterion?).
Our systemalso outperforms the improved supervised baselineby a large margin.
The fact that a completely un-supervised system performs best is encouraging, astraining data for this task is very expensive to ob-tain.
Our results suggest that it might be worth in-vestigating other types of prior knowledge about thesemantics of an input text in further research.
Thecrucial advantage of such generic methods is that notexts on exactly the same topic are needed, which isa requirement with texts about niche topics.5 ConclusionWe have introduced an unsupervised method forthe challenging problem of timeline generation fromsingle history articles, a scenario where parallel textscannot be assumed to exist.
Our method results in asignificant improvement over a novel unsupervisedbaseline as well as an existing supervised approach.AcknowledgmentsThe first author received financial support from Mi-crosoft Research, St John?s College Cambridge andthe Cambridge University Computer Laboratory.2347ReferencesJames Allan, Rahul Gupta, and Vikas Khandelwal.
2001.Temporal Summaries of New Topics.
In Proceedingsof the 24th Annual International ACM SIGIR Confer-ence on Research and Development in Information Re-trieval, SIGIR ?01, pages 10?18, New York, NY, USA.ACM.Sandro Bauer and Simone Teufel.
2015.
A Methodologyfor Evaluating Timeline Generation Algorithms basedon Deep Semantic Units.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conferenceon Natural Language Processing of the Asian Federa-tion of Natural Language Processing, ACL 2015, July26-31, 2015, Beijing, China, Volume 2: Short Papers,pages 834?839.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Rachel Chasin, Daryl Woodward, Jeremy Witmer, andJugal Kalita.
2014.
Extracting and Displaying Tempo-ral and Geospatial Entities from Articles on HistoricalEvents.
Comput.
J., 57(3):403?426.Hai Leong Chieu and Yoong Keok Lee.
2004.
QueryBased Event Extraction Along a Timeline.
In Pro-ceedings of the 27th Annual International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, SIGIR ?04, pages 425?432, New York,NY, USA.
ACM.Stephen Clark and James R. Curran.
2007.
Wide-Coverage Efficient Statistical Parsing with CCG andLog-Linear Models.
Computational Linguistics,33(4):493?552.Oleksandr Kolomiyets, Steven Bethard, and Marie-Francine Moens.
2012.
Extracting Narrative Time-lines as Temporal Dependency Structures.
In The 50thAnnual Meeting of the Association for ComputationalLinguistics, Proceedings of the Conference, July 8-14,2012, Jeju Island, Korea - Volume 1: Long Papers,pages 88?97.Hector Llorens, Nathanael Chambers, Naushad UzZa-man, Nasrin Mostafazadeh, James Allen, and JamesPustejovsky.
2015.
SemEval-2015 Task 5: QA Tem-pEval - Evaluating Temporal Information Understand-ing with Question Answering.
In Proceedings of the9th International Workshop on Semantic Evaluation(SemEval 2015), pages 792?800, Denver, Colorado,June.
Association for Computational Linguistics.Ryan McDonald.
2007.
A Study of Global Inference Al-gorithms in Multi-document Summarization.
In Pro-ceedings of the 29th European Conference on IR Re-search, ECIR?07, pages 557?564, Berlin, Heidelberg.Springer-Verlag.Rada Mihalcea and Paul Tarau.
2004.
TextRank: Bring-ing Order into Text.
In Proceedings of the 2004 Con-ference on Empirical Methods in Natural LanguageProcessing , EMNLP 2004, A meeting of SIGDAT, aSpecial Interest Group of the ACL, held in conjunctionwith ACL 2004, 25-26 July 2004, Barcelona, Spain,pages 404?411.Anne-Lyse Minard, Manuela Speranza, Eneko Agirre,Itziar Aldabe, Marieke van Erp, Bernardo Magnini,German Rigau, and Ruben Urizar.
2015.
SemEval-2015 Task 4: TimeLine: Cross-Document Event Or-dering.
In Proceedings of the 9th International Work-shop on Semantic Evaluation (SemEval 2015), pages778?786, Denver, Colorado, June.
Association forComputational Linguistics.Ani Nenkova and Rebecca Passonneau.
2004.
Evaluat-ing Content Selection in Summarization: The PyramidMethod.
In Daniel Marcu Susan Dumais and SalimRoukos, editors, HLT-NAACL 2004: Main Proceed-ings, pages 145?152, Boston, Massachusetts, USA,May 2 - May 7.
Association for Computational Lin-guistics.J.
Platt.
2000.
Probabilistic outputs for support vectormachines and comparisons to regularized likelihoodmethods.
In Advances in Large Margin Classifiers.James Pustejovsky, Jose?
Castan?o, Robert Ingria, RoserSaur?
?, Robert Gaizauskas, Andrea Setzer, and GrahamKatz.
2003.
TimeML: Robust specification of eventand temporal expressions in text.
In Fifth Interna-tional Workshop on Computational Semantics (IWCS-5), pages 1?11.Jannik Stro?tgen and Michael Gertz.
2013.
Multilingualand Cross-domain Temporal Tagging.
Language Re-sources and Evaluation, 47(2):269?298.Russell Swan and James Allan.
2000.
TimeMine(Demonstration Session): Visualizing AutomaticallyConstructed Timelines.
In Proceedings of the 23rdAnnual International ACM SIGIR Conference on Re-search and Development in Information Retrieval, SI-GIR ?00, pages 393?, New York, NY, USA.
ACM.Giang Tran, Eelco Herder, and Katja Markert.
2015.Joint Graphical Models for Date Selection in TimelineSummarization.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Linguis-tics and the 7th International Joint Conference on Nat-ural Language Processing (Volume 1: Long Papers),pages 1598?1607, Beijing, China, July.
Associationfor Computational Linguistics.Naushad UzZaman, Hector Llorens, Leon Derczynski,James Allen, Marc Verhagen, and James Pustejovsky.2013.
SemEval-2013 Task 1: TempEval-3: EvaluatingTime Expressions, Events, and Temporal Relations.2348In Second Joint Conference on Lexical and Compu-tational Semantics (*SEM), Volume 2: Proceedings ofthe Seventh International Workshop on Semantic Eval-uation (SemEval 2013), pages 1?9, Atlanta, Georgia,USA, June.
Association for Computational Linguis-tics.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
SemEval-2007 Task 15: TempEval TemporalRelation Identification.
In Proceedings of the 4th In-ternational Workshop on Semantic Evaluations, Se-mEval ?07, pages 75?80, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Marc Verhagen, Roser Saur?
?, Tommaso Caselli, andJames Pustejovsky.
2010.
SemEval-2010 Task 13:TempEval-2.
In Proceedings of the 5th Interna-tional Workshop on Semantic Evaluation, SemEval?10, pages 57?62, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,Xiaoming Li, and Yan Zhang.
2011.
EvolutionaryTimeline Summarization: A Balanced OptimizationFramework via Iterative Substitution.
In Proceedingsof the 34th International ACM SIGIR Conference onResearch and Development in Information Retrieval,SIGIR ?11, pages 745?754, New York, NY, USA.ACM.2349
