A Semantic Feature for Relation Recognition Using A Web-based CorpusChen-Ming HungInstitute of Information ScienceAcademia Sinica, Taipei, Taiwanrglly@iis.sinica.edu.twAbstractSelecting appropriate features to representan entity pair plays a key role in the taskof relation recognition.
However, existingsyntactic features or lexical features cannotcapture the interaction between two enti-ties because of the dearth of annotated rela-tional corpus specialized for relation recog-nition.
In this paper, we propose a seman-tic feature, called the latent topic feature,which is topic-based and represents an en-tity pair at the semantic level instead of theword level.
Moreover, to address the prob-lem of insufficiently annotated corpora, wepropose an algorithm for compiling a train-ing corpus from the Web.
Experiment resultsdemonstrate that latent topic features areas effective as syntactic or lexical features.Moreover, the Web-based corpus can resolvethe problems caused by insufficiently anno-tated relational corpora.1 INTRODUCTIONRelation recognition is a challenging task becausefinding appropriate features to represent the rela-tionship between two entities is difficult and limitedby the scarcity of annotated corpora.
Prior workson relation recognition have focused on syntacticfeatures, e.g., parsing trees (Culotta and Sorensen,2004; Zelenko et al, 2003), and on lexical fea-tures, e.g., Part-Of-Speech (POS) features.
Theseapproaches show that syntactic features and lexicalfeatures outperform bag-of-words (BOW) on exist-ing annotated corpora such as the RDC corpus of theACE project.
The superior performance achieved bysyntactic and lexical features is due to their abilityto capture the grammatical relations between twoentities and the characteristics of the entities.
Forexample, (Culotta and Sorensen, 2004) add hyper-nyms of entities to features derived from WordNet.However, neither syntactic nor lexical features cancapture the interaction between two entities at thesemantic level.Another issue in the task of relation recognitionis insufficiently annotated corpora.
For example,given a pair {the U.N. body, Kosovo}, we can onlyfind three sentences containing both entities in theRDC corpus, which is commonly used corpus in therelation recognition task.
The problem of an in-sufficiently annotated corpus biases feature vectorsand distorts the prediction of entity pairs.
How-ever, (Huang et al, 2004; Hung and Chien, 2007)have shown that the Web can be used as an alterna-tive source of documents related to a given query.That is possibly because of the increasing size ofthe Web and the efficiency in commercial search en-gines, e.g., Google and Yahoo!.To resolve the above problems, we propose a se-mantic feature called the latent topic feature, whichis extracted by exploiting the Latent Dirichlet Al-location (LDA) algorithm.
Unlike syntactic fea-tures or lexical features, latent topic features repre-sent entity pairs as random mixtures of latent topics,where each topic is characterized by a distributionof words.
We prove experimentally that latent topicfeatures are as effective as syntactic features or lexi-cal features in capturing the interaction between twoentities.
The experiment results are predictable.
In434the above {the U.N. body, Kosovo} example, it maybe difficult to determine the relationship betweenU.N.
body and Kosovo straightforwardly.
However,making the right guess about the relationship is eas-ier if the U.N. body is grouped with army and gov-ernment.
Therefore, the right guess in this exampleis management.To overcome the problems caused by an insuffi-ciently annotated corpus, we exploit the Web as asource of training data for the relation recognitiontask.
Given an entity pair, documents describing theentity pair are extracted from the Web via commer-cial search engines using both entities as the query.In other words, snippets returned from the Web aretreated as documents related to the query.
Our as-sumption, which has been proved in previously pub-lished works, is that returned snippets can capturethe interaction between two entities.
After the latenttopic features extracted from returned snippets usingthe Web as the corpus, an SVM classifier is trainedas the relation recognition classifier for use in thelater experiments.The remainder of the paper is organized as fol-lows.
in Section 2, we discuss works related to fea-ture selection in the relation recognition task as wellas using the Web as a corpus.
The concept of la-tent topic features is presented in Section 3.
We alsoexplain how we represent a document in the vectorspace of a latent topic feature.
Section 4containsan evaluation of the latent topic feature.
We thenpresent our conclusions in Section 5.2 RELATED WORKIn the field of information extraction (IE), the goalof relation recognition is to find the relationshipbetween two entities.
Without considering en-tity detection, relation recognition depends heav-ily on the representation of entity pairs.
(Zelenkoet al, 2003) showed how to extract relations bycomputing the kernel functions between the ker-nels of shallow parse trees.
The kernels are de-fined over a shallow parse representation of the textand used in conjunction with a Support Vector Ma-chine (SVM) learning algorithm to extract person-affiliation and organization-location relations.
(Cu-lotta and Sorensen, 2004) extended this work to es-timate kernel functions between augmented depen-dency trees, while (Kambhatla, 2004) combined lex-ical features, syntactic features, and semantic fea-tures in a maximum entropy model.
However, thesemantic features discussed in (Kambhatla, 2004)still focus on the word level instead of the concep-tual level.LDA is an aspect model that represents docu-ments as a set of topics instead of a bag-of-words.Latent semantic indexing (LSI) (Deerwester et al,1990) and probabilistic latent semantic indexing(PLSI) (Hofmann, 1999) are also aspect models andhave been widely used in the field of informationretrieval.
LSI simply assumes that each documentis generated from single latent topic, while PLSIattempts to relax the assumption by using a mix-ture of latent topics for each document.
However,PLSI is highly dependent on training documents; inother words, it cannot handle the probability of la-tent topics in a previously unseen document.
In ad-dition, the number of parameters that must be esti-mated in PLSI grows linearly with the number oftraining documents.
(Blei et al, 2003; Blei andJordan, 2003) proposed LDA to resolve the above-mentioned limitations.
It can easily generate an un-seen document under controllable parameters.A number of works, e.g., (Huang et al, 2004),have investigated using the Web to acquire a train-ing corpus or acquire additional information not pro-vided by existing annotated corpora.
(Huang et al,2004) exploited the Web as a training corpus to traina classifier with user-defined categories.
However,it is widely recognized that when using documentson the Web users must spend a great deal of timefiltering out unrelated contents.
(Hung and Chien,2007) designed a bootstrapping method that adaptsan existing corpus with an automatic verification al-gorithm in order to control the quality of returnedsnippets in each iteration.
(Matsuo et al, 2006) usedthe Web to construct a social network system, calledPOLYPHONET, which visualizes the relationshipbetween two personal names.3 LATENT TOPIC FEATUREIn this section, we introduce the concept of using theWeb to augment an insufficiently annotated corpusfor relation recognition.
Then we apply the LDAalgorithm to the corpus to extract the latent topic435features to represent entity pairs in the corpus forrelation recognition.Figure 1: The framework of the proposed approach.3.1 Compiling a Web-based Relational CorpusFor an entity pair, E = {e1, e2}, where e1 and e2are named entities, it is difficult to find sufficientsentences to describe their relationship from exist-ing annotated corpora.
In other words, given an en-tity pair without a relation label, users cannot rec-ognize the pair.
Even with a widely used thesaurus,like WordNet, we can only obtain hypernyms or syn-onyms of given entities.
It is not possible to obtainknowledge about the interaction between two enti-ties.To capture the interaction between two entities,we send both entities, e1 AND e2, to commer-cial search engines and collect returned snippets astraining documents for an entity pair, E. Snippetsof returned search results are defined as the sur-rounding contexts of queries highlighted by com-mercial search engines.
In other words, the fulltexts of search results are not considered in thecollected corpus when filtering noisy informationin full documents.
Let R be the relation label ofentity pairs {E1, .
.
.
, EM}; then, the training cor-pus for R is the collection of all returned snippetsfor {E1, .
.
.
, EM}.
Through effective commercialsearch engines such as Google and Yahoo!, sen-tences describing the interaction between two enti-ties can easily be retrieved.
Returning to the ex-ample {the U.N. body, Kosovo}, almost two mil-lion sentences with co-occurrences of the two en-tities are retrieved by Google.
Another advantage ofusing the Web to retrieve relevant documents is theauto?
correction ability of commercial search en-gines.
The feature can correct a misspelled queryor replace an uncommon word with a synonym or acommon word which is correct, so that more relatedinformation about entity pairs can be retrieved fromthe Web as returned snippets.
For example, Googlecan automatically link the U.N. body to UnitedNations, which is used more frequently in search-ing.
Clearly, the number of returned snippets mustbe considered.
Actually, based on experiment re-sults in Section 4, we set the number as five, whichachieves the best performance.3.2 Modified LDA for the Relational CorpusLDA is an aspect model with three levels, namely,the corpus level, the document level, and the wordlevel.
Given a document, variables of the corpus itbelongs to are sampled first, after which the variableof the document is sampled once.
Finally, variablesfor words in the document are sampled.For a document d in a corpus D, the modelingprocess is as follows:1.
Sample ?
?
Dir(?|?).2.
For each word wn in d, n ?
{1, .
.
.
, N}:(a) sample zn ?
Mult(?
),(b) sample a word wn ?
p(wn|zn, ?)
from amultivariate Gaussian distribution condi-tioned on the topic zn.Note that ?
is a vector of corpus-level variableswhose dimensionality is equal to the number of la-tent topics; ?
is a variable of the document and is as-sumed to follow Dirichlet distribution for the givencorpus; and ?
is a word-level variable.
In addition,Z = {z1, z2, .
.
.
, zN} are latent factors that gener-ate the document, and zn is the latent topic that wnis generated from.
Finally, N is the length of thedocument d.An entity pair E in the relational corpus is similarto a document d in the text corpus.
In other words,the corpus DR is comprised of returned snippets forall entity pairs ER with the same relation label R.Therefore, given the parameters ?
and ?, we obtainthe distribution of entity pair E as follows:p(E|?, ?)
=?
?znp(?, zn, SE |?, ?
)d?,wherep(?, zn, SE |?, ?)
= p(?|?)NE?n=1p(zn|?
)p(wn|zn, ?
),436NE is the number of words in the returned snippetsfor E; and wn is the nth word in SE , the returnedsnippets of E. Table 1 summarizes notations used inthe paper.Table 1: Notations used in this paper.SYMBOL DESCRIPTIONR relation labelDR corpus for RERj jth entity pair in the relation label RSE returned snippets for an entity pair E|ER| number of entity pairs in the relation RNE number of words in SEwn nth word in SEzn latent topic that wn is generated fromIn Section 3.1, we discussed the advantages of us-ing the Web as a corpus to model entity pairs.
Inthe modeling process, we estimate the probability ofwn conditioned on zn, p(wn|zn, ?
), to maximize theprobability of the entire corpus of R. The probabil-ity that we try to maximize isp(DR|?, ?)
=|ER|?j=1p(ERj |?, ?
).3.3 Latent Topic FeatureIn different corpora, z obtains a different distribu-tion to maximize the likelihood of the given corpus.In this section, we describe how to exploit z as fea-tures to represent a snippet for an entity pair E, i.e.,SE .
In Section 3.2, we noted that the parameters tobe estimated in the aspect model are all probabilitiesof words in each latent topic z.
Thus, we let the ex-pected number of words generated from latent topicsbe features of each entity pair.
In other words, an en-tity pair E is represented as a feature vector whoselength is equal to the number of latent topics andwhose ith attribute is equal to?i +NE?n=1|wn| ?
p(wn|zn, ?
),where ?i is the ith prior Dirichlet parameter.In addition, because there is no solution goodenough to determine the dimensionality of the fea-ture vector or the number of latent topics, we set thenumber of topics at thirty because it probably min-imize the computation cost without significantly af-fecting the performance.4 EXPERIMENTSIn this section, we evaluate the performance of thelatent topic feature in representing entity pairsextracted from the Relation Detection and Charac-terization (RDC) corpus of the Automatic ContentExtraction 2003 model (ACE 2003)1.4.1 The RDC CorpusIn the RDC corpus, five relation types, AT , NEAR,PART , ROLE, and SOC, are defined; each rela-tion type has extended sub-relations.
Table 2 sum-marizes the relations in the RDC corpus for ACE2003.
Based on Table 2, we find that the distributionof the number relations is very unbalanced, rangingfrom 2 to 773.
In the following experiments, we onlyconsider the Role relation because it has the largestnumbers of sub-relations and it is easier to verifythe recognition results manually.
Note that a rela-tion is dropped if it has less than ten sub-relations inorder to avoid the bias of learned classifiers.
There-fore, the sub-relation founder in Role is droppedin the following experiments because it occurs lessthan ten times.
Other is also dropped because itsdefinition is unclear.Table 2: Distribution over relation types in the RDCcorpus (ACE 2003).Relations Sub-Relations(Size)AT Based-in(78) Located(773)Residence(186)NEAR Relative-location(73)PART Part-of(242) Subsidiary(172)Other(2)ROLE Affiliate-partner(34)Citizen-of(93) Client(33)Founder(6) General-Staff(460)Management(294) Member(398)Owner(41) Other(98)SOC Associate(25) Grandparent(3)Parent(23) Sibling(5)Spouse(22) Other-relative(24)Other-Personal(10)Other-Professional(88)1http://projects.ldc.upenn.edu/ace/4374.2 Setting and MeasurementWe used the package of (Chang and Lin, 2001) todesign the following experiments.
In addition, ?-SVM with a radial kernel function was used to learnthe relation classifier.
To determine the parametersin ?-SVM, i.e., ?
and ?, we observed the perfor-mance of the ?-SVM classifier by randomly select-ing 80% of the sentences in the RDC corpus as train-ing data and the remaining 20% as test data.
Inother words, we applied five-fold cross validationto build a temporary model for parameter estima-tion.
Furthermore, it is well known that parame-ters in the SVM model must be optimized manually;therefore, we estimate ?
first and then estimate ?.
?is fixed while ?
is being estimated and vice versa.After estimation, the best result is achieved at thepoint that ?
is equal to 2.5?
10?4 and ?
is equal to0.05.
We summarize the results in Figure 2.
The topgraph in Figure 2 is the accuracy curve, where fixed?
= 2.5 ?
10?4 and flexible ?
; the bottom graph isthe accuracy curve with fixed ?
= 0.05 and flexible?.Figure 2: Accuracy of five-fold cross validation us-ing bigram features.
Top: ?
with ?
= 2.5 ?
10?4.Bottom: ?
with ?
= 0.05.For each sub-relation in Role, binary classifica-tion is used in the experiments and the F-measure ofeach sub-relation is used as the metric for assessingthe performance of latent topic features.F ?
value = 2?
Precision?RecallPrecision+RecallRecall = ] of correct positive predictions] of positive examplesPrecision = ] of correct positive predictions] of positive predictions4.3 Web-based Corpus vs. Annotated CorpusWe now evaluate the performance of relational clas-sifier on a Web-based corpus and on an annotatedcorpus.
To assess the performance of on the anno-tated corpus, sentences in the RDC corpus contain-ing a co-occurrence of both given entities were ex-tracted as training data to learn a benchmark relationclassifier.
On the other hand, the Web-based corpusis compiled from snippets retrieved by using bothentities as a query.
The latent topic feature is ap-plied on both the Web-based corpus and the RDCcorpus using the procedure described in Section 4.2.In addition, to analyze the effect of the number ofreturned snippets, we increased the number of snip-pets from 3 to 45 in increments of three and thensummarized the relationship between the number ofreturned snippets and the achieved accuracy curveshown in Figure 3.
In the figure, the training datais comprised of snippets of information returned byquerying 80% of the entity pairs selected at randomin the RDC corpus.
The test data comprises snippetsreturned by querying the remaining 20% of entitypairs in the corpus.From Figure 3, we observe that using five re-turned snippets for each entity pair achieves the bestaccuracy (0.85), which is substantially higher thanthe accuracy achieved by using annotated corpus(0.69).
Note that using more returned snippets doesnot guarantee higher accuracy.
For example, when39 returned snippets are used for each entity pair,the accuracy (0.56) is almost the same as that (0.55)achieved by using only 3 returned snippets.
More-over, it is significantly less than the accuracy (0.69)achieved by using the RDC corpus.
This is rea-sonable because the greater the number of returnedsnippets, the larger the amount of noisy informationintroduced to the classifier, which degrades its per-formance.438Figure 3: Accuracy of five-fold cross validation us-ing the Web-based corpus and the annotated corpus.4.4 Latent Topic Feature vs. Other FeaturesIn this section, we compare the performance oflatent topic features with that of syntactic fea-tures and lexical features, i.e., bag ?
of ?
wordsor parts ?
of ?
speech.
Because of the superiorperformance achieved by using the Web-based cor-pus described in Section 4.3, we extracted featuresfrom the training corpus compiled from that corpusrather than the annotated corpus.Based on the results reported in Section 4.3, fivesnippets were returned by the Web-based corpus foreach entity pair.
For each sub-relation, a one-classSVM was trained to perform binary classification.Each sub-relation of Role in Table 3 is appliedwith binary classification using a one-class SVM.Table 5 summarizes the results of a comparison be-tween the latent topic feature and the featuresused by (Culotta and Sorensen, 2004).
The lat-ter depends on dependency tree kernels, which rep-resent the grammatical dependencies in a sentenceand are considered as syntactical features.
In Table5, BOW denotes bag-of-word, sparse represents asparse kernel, and contiguous represents a contigu-ous kernel.Surprisingly, for every sub sub-relation in Table3, the latent topic feature consistently achieves asignificantly higher average recall rate, but a loweraverage precision rate.
This may be due to thelatent topic feature?s ability to capture informa-tion at the semantic level precisely, but it cannotdistinguish the information at the word level eas-ily.
In other words, the latent topic feature cancapture the common semantic information, proba-bly the Role, of all sub-relations, but it cannot tellthe difference between citizen ?
of and founder.Table 4 shows the results of applying binary classifi-cation to five relations in the RDC corpus.
Althoughthe precision rate for each relation is still low, therecall rate has been increased significantly.
This de-mostrates the ability of the latent topic feature tocapture semantic information.Table 3: Binary classification results for each sub-relation of Role.Latent Topic FeatureF Prec.
Rec.Aff.-Part.
0.30 0.18 1.00Client 0.40 0.28 0.71Citizen-Of 0.62 0.47 0.91Gen.-Staff 0.78 0.64 0.99Manage.
0.56 0.39 1.00Member 0.62 0.46 0.93Owner 0.45 0.29 0.98Table 4: Binary classification results for each rela-tion in the RDC corpus.Latent Topic FeatureF Prec.
Rec.At 0.61 0.48 0.84NEAR 0.36 0.23 0.88PART 0.58 0.46 0.80ROLE 0.71 0.64 0.79SOC 0.59 0.45 0.87In Table 5, although the recall rate using thelatent topic feature is much higher than thatachieved by the other features, unfortunately, theF ?
score of the latent topic feature cannot beredeemed because of the much lower precision rate.Moreover, the latent topic feature is comparableto the sparse kernel method in a different way be-cause it has a low precision rate but a high recallrate.
Finally, the latent topic feature achieves ahigher average F-score than the bag-of-words fea-ture, which proves the assumption that the latenttopic feature can better capture the interaction be-tween two entities than features at the word level.5 CONCLUSIONWe have proposed a concept called the latent topicfeature for the task of relation recognition andevaluated it on the RDC of the ACE project.
Thefeature captures the interaction between two entities439Table 5: Comparison between Latent topicfeature and other features.AverageF Prec.
Rec.Latent Topic 0.58 0.45 0.84Sparse 0.59 0.83 0.46Contiguous 0.62 0.85 0.49BOW 0.52 0.73 0.40Sparse+BOW 0.62 0.80 0.50Cont.+BOW 0.63 0.81 0.52at the semantic level rather than at the word level.Therefore, combining the latent topic featurewith syntactic features and lexical features shouldachieve a better performance than using the featuresseparately.
In our future work, we will devise an ap-propriate way of combining latent topic featureswith syntactical and lexical features.Because of the lack of a sufficiently annotatedcorpus for relation corpus for relation recognition,we have also proposed using a Web-based corpusto train classifiers for the purpose.
Our experimentresults demonstrates that Web documents can accu-rately capture information about the interaction be-tween two named entities in the absence of an an-notated corpus.
By using a Web-based corpus, thetime cost to manually annotating a corpus for rela-tion recognition is expected to be significantly re-duced if the quality of returned snippetscan be con-trolled.ReferencesDavid M. Blei and Michael I. Jordan.
2003.
Model-ing annotated data.
In Proceedings of the 26th SIGIR,pages 127?134.
ACM Press.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: alibrary for support vector machines.A.
Culotta and J. Sorensen.
2004.
Dependency tree ker-nels for relation extraction.
Proceedings of the 42ndAnnual Meeting of the Association for ComputationalLinguistics.Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society of Information Science,41(6):391?407.Thomas Hofmann.
1999.
Probabilistic latent semanticanalysis.
In Proc.
of Uncertainty in Artificial Intelli-gence, UAI?99, Stockholm.Chien-Chung Huang, Shui-Lung Chuang, and Lee-FengChien.
2004.
Liveclassifier: creating hierarchical textclassifiers through web corpora.
In WWW ?04: Pro-ceedings of the 13th international conference on WorldWide Web, pages 184?192, New York, NY, USA.Chen-Ming Hung and Leeq-Feng Chien.
2007.
Web-based text classification in the absence of manually la-beled training documents.
J.
Am.
Soc.
Inf.
Sci.
Tech-nol., 58(1):88?96.Nanda Kambhatla.
2004.
Combining lexical, syntactic,and semantic features with maximum entropy modelsfor information extraction.
In The Companion Volumeto the Proceedings of 42st Annual Meeting of the Asso-ciation for Computational Linguistics, pages 178?181,Barcelona, Spain, July.
Association for ComputationalLinguistics.Yutaka Matsuo, Junichiro Mori, Masahiro Hamasaki,Keisuke Ishida, Takuichi Nishimura, Hideaki Takeda,Koiti Hasida, and Mitsuru Ishizuka.
2006.
Poly-phonet: an advanced social network extraction systemfrom the web.
In WWW ?06: Proceedings of the 15thinternational conference on World Wide Web, pages397?406, New York, NY, USA.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relationextraction.
J. Mach.
Learn.
Res., 3:1083?1106.440
