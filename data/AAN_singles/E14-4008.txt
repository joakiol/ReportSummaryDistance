Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38?42,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsChasing Hypernyms in Vector Spaces with EntropyEnrico SantusDept.
of Chinese and Bilingual StudiesThe Hong Kong PolytechnicUniversity, Hong Konge.santus@connect.polyu.hkAlessandro LenciCoLing Lab ?
Dept.
of  Philology,Literature, and LinguisticsUniversity of Pisa, Italyalessandro.lenci@ling.unipi.itQin LuDept.
of ComputingThe Hong Kong PolytechnicUniversity, Hong Kongcsluqin@comp.polyu.edu.hkSabine Schulte im WaldeInst.
for Natural Language ProcessingUniversity of StuttgartGermanyschulte@ims.uni-stuttgart.deAbstractIn this paper, we introduce SLQS, a newentropy-based measure for the unsupervisedidentification of hypernymy and itsdirectionality in Distributional SemanticModels (DSMs).
SLQS is assessed throughtwo tasks: (i.)
identifying the hypernym inhyponym-hypernym pairs, and (ii.
)discriminating hypernymy among varioussemantic relations.
In both tasks, SLQSoutperforms other state-of-the-art measures.1 IntroductionIn recent years, Distributional Semantic Models(DSMs) have gained much attention incomputational linguistics as unsupervisedmethods to build lexical semantic representationsfrom corpus-derived co-occurrences encoded asdistributional vectors (Sahlgren, 2006; Turneyand Pantel, 2010).
DSMs rely on theDistributional Hypothesis (Harris, 1954) andmodel lexical semantic similarity as a function ofdistributional similarity, which is mostcommonly measured with the vector cosine(Turney and Pantel, 2010).
DSMs have achievedimpressive results in tasks such as synonymdetection, semantic categorization, etc.
(Pad?
andLapata, 2007; Baroni and Lenci, 2010).One major shortcoming of current DSMs isthat they are not able to discriminate amongdifferent types of semantic relations linkingdistributionally similar lexemes.
For instance, thenearest neighbors of dog in vector spacestypically include hypernyms like animal, co-hyponyms like cat, meronyms like tail, togetherwith other words semantically related to dog.DSMs tell us how similar these words are to dog,but they do not give us a principled way to singleout the items linked by a specific relation (e.g.,hypernyms).Another related issue is to what extentdistributional similarity, as currently measuredby DSMs, is appropriate to model the semanticproperties of a relation like hypernymy, which iscrucial for Natural Language Processing.Similarity is by definition a symmetric notion (ais similar to b if and only if b is similar to a) andit can therefore naturally model symmetricsemantic relations, such as synonymy and co-hyponymy (Murphy, 2003).
It is not clear,however, how this notion can also modelhypernymy, which is asymmetric.
In fact, it isnot enough to say that animal is distributionallysimilar to dog.
We must also account for the factthat animal is semantically broader than dog:every dog is an animal, but not every animal is adog.38In this paper, we introduce SLQS, a newentropy-based distributional measure that aims toidentify hypernyms by providing a distributionalcharacterization of their semantic generality.
Weassess it with two tasks: (i.)
the identification ofthe broader term in hyponym-hypernym pairs(directionality task); (ii.)
the discrimination ofhypernymy among other semantic relations(detection task).
Given the centrality ofhypernymy, the relevance of the themes weaddress hardly needs any further motivation.Improving the ability of DSMs to identifyhypernyms is in fact extremely important in taskssuch as Recognizing Textual Entailment (RTE)and ontology learning, as well as to enhance thecognitive plausibility of DSMs as general modelsof the semantic lexicon.2 Related workThe problem of identifying asymmetric relationslike hypernymy has so far been addressed indistributional semantics only in a limited way(Kotlerman et al., 2010) or treated through semi-supervised approaches, such as pattern-basedapproaches (Hearst, 1992).
The few works thathave attempted a completely unsupervisedapproach to the identification of hypernymy incorpora have mostly relied on some versions ofthe Distributional Inclusion Hypothesis (DIH;Weeds and Weir, 2003; Weeds et al., 2004),according to which the contexts of a narrow termare also shared by the broad term.One of the first proposed measuresformalizing the DIH is WeedsPrec (Weeds andWeir, 2003; Weeds et al., 2004), whichquantifies the weights of the features f of anarrow term u that are included into the set offeatures of a broad term v:??????????
?, ??
= ?
?????????????
????????
?where Fx is the set of features of a term x, andwx(f) is the weight of the feature f of the term x.Variations of this measure have been introducedby Clarke (2009), Kotlerman et al.
(2010) andLenci and Benotto (2012).In this paper, we adopt a different approach,which is not based on DIH, but on the hypothesisthat hypernyms are semantically more generalthan hyponyms, and therefore tend to occur inless informative contexts than hypernyms.3 SLQS: A new entropy-based measureDIH is grounded on an ?extensional?
definitionof the asymmetric character of hypernymy: sincethe class (i.e., extension) denoted by a hyponymis included in the class denoted by the hypernym,hyponyms are expected to occur in a subset ofthe contexts of their hypernyms.
However, it isalso possible to provide an ?intensional?definition of the same asymmetry.
In fact, thetypical characteristics making up the ?intension?
(i.e., concept) expressed by a hypernym (e.g.,move or eat for animal) are semantically moregeneral than the characteristics forming the?intension?
of its hyponyms (e.g., bark or has furfor dog).
This corresponds to the idea thatsuperordinate terms like animal are lessinformative than their hyponyms (Murphy, 2002).From a distributional point of view, we cantherefore expect that the most typical linguisticcontexts of a hypernym are less informative thanthe most typical linguistic contexts of itshyponyms.
In fact, contexts such as bark and hasfur are likely to co-occur with a smaller numberof words than move and eat.
Starting from thishypothesis and using entropy as an estimate ofcontext informativeness (Shannon, 1948), wepropose SLQS, which measures the semanticgenerality of a word by the entropy of itsstatistically most prominent contexts.For every term wi we identify the N mostassociated contexts c (where N is a parameterempirically set to 50)1.
The association strengthhas been calculated with Local MutualInformation (LMI; Evert, 2005).
For eachselected context c, we define its entropy H(c) as:1N=50 is the result of an optimization of the modelagainst the dataset after trying the followingsuboptimal values: 5, 10, 25, 75 and 100.39????
= ??????|??
?
?????????|??????
?where p(fi|c) is the probability of the feature figiven the context c, obtained through the ratiobetween the frequency of <c, fi> and the totalfrequency of c. The resulting values H(c) arethen normalized in the range 0-1 by using theMin-Max-Scaling (Priddy and Keller, 2005):Hn(c).
Finally, for each term wi we calculate themedian entropy Ewi of its N contexts:???
= ??????
???????????
can be considered as a semantic generalityindex for the term wi: the higher ???
, the moresemantically general wi is.
SLQS is then definedas the reciprocal difference between the semanticgenerality ???
and ???
of two terms w1 and w2:??????
?, ???
= 1 ?
?????
?According to this formula, SLQS<0, if ???>???
;SLQS?0, if ???????
; and SLQS>0, if ???<??
?.SLQS is an asymmetric measure because, bydefinition, SLQS(w1,w2)?SLQS(w2,w1) (exceptwhen w1 and w2 have exactly the samegenerality).
Therefore, if SLQS(w1,w2)>0, w1 issemantically less general than w2.4 Experiments and evaluation4.1 The DSM and the datasetFor the experiments, we used a standardwindow-based DSM recording co-occurrenceswith the nearest 2 content words to the left andright of each target word.
Co-occurrences wereextracted from a combination of the freelyavailable ukWaC and WaCkypedia corpora (with1.915 billion and 820 million words, respectively)and weighted with LMI.To assess SLQS we relied on a subset ofBLESS (Baroni and Lenci, 2011), a freely-available dataset that includes 200 distinctEnglish concrete nouns as target concepts,equally divided between living and non-livingentities (e.g.
BIRD, FRUIT, etc.).
For each targetconcept, BLESS contains several relata,connected to it through one relation, such as co-hyponymy (COORD), hypernymy (HYPER),meronymy (MERO) or no-relation (RANDOM-N).2Since BLESS contains different numbers ofpairs for every relation, we randomly extracted asubset of 1,277 pairs for each relation, where1,277 is the maximum number of HYPER-relatedpairs for which vectors existed in our DSM.4.2 Task 1: DirectionalityIn this experiment we aimed at identifying thehypernym in the 1,277 hypernymy-related pairsof our dataset.
Since the HYPER-related pairs inBLESS are in the order hyponym-hypernym (e.g.eagle-bird, eagle-animal, etc.
), the hypernym ina pair (w1,w2) is correctly identified by SLQS, ifSLQS (w1,w2) > 0.
Following Weeds et al.
(2004),we used word frequency as a baseline model.This baseline is grounded on the hypothesis thathypernyms are more frequent than hyponyms incorpora.
Table 1 gives the evaluation results:SLQS WeedsPrec BASELINEPOSITIVE 1111 805 844NEGATIVE 166 472 433TOTAL 1277 1277 1277PRECISION 87.00% 63.04% 66.09%Table 1.
Accuracy for Task 1.As it can be seen in Table 1, SLQS scores aprecision of 87% in identifying the second termof the test pairs as the hypernym.
This result isparticularly significant when compared to theone obtained by applying WeedsPrec (+23.96%).As it was also noticed by Geffet and Dagan(2005) with reference to a previous similarexperiment performed on a different corpus(Weeds et al., 2004), the WeedsPrec precision inthis task is comparable to the na?ve baseline.SLQS scores instead a +20.91%.2In these experiments, we only consider the BLESSpairs containing a noun relatum.404.3 Task 2: DetectionThe second experiment aimed atHYPER test pairs from those linked by othertypes of relations in BLESS (i.e.and RANDOM-N).
To this purpose, we assumethat hypernymy is characterized by two mainproperties: (i.)
the hypernym and the hyponymare distributionally similar (in the sense of theDistributional Hypothesis), andhyponym is semantically less general than thehypernym.
We measured the first property withthe vector cosine and the second one withAfter calculating SLQS for all the pairs in ourdatasets, we set to zero all the negative values,that is to say those in whichSLQS ?
the first term is semantically moregeneral than the second one.
Then,SLQS and vector cosine by thegreater the resulting value, the greater thelikelihood that we are considering a hypernymyrelated pair, in which the first word isand the second word is a hypernym.To evaluate the performanceused Average Precision (AP; Kotlerman et al.,2010), a method derived from InformationRetrieval that combines precision, relevanceranking and overall recall, returning a value thatranges from 0 to 1.
AP=1 means that all theinstances of a relation are in the top of the rankwhereas AP=0 means they are in the bottomis calculated for the four relations we extractedfrom BLESS.
SLQS was also compared withWeedsPrec and vector cosinefrequency as baseline.
Table 2 shows the resultsHYPER COORD MEROBaseline 0.40 0.51Cosine 0.48 0.46WeedsPrec 0.50 0.35SLQS *Cosine0.59 0.27Table 2.
AP values for TThe AP values show the performancetested measures on the fouroptimal result would be obtained scoringHYPER and 0 for the other relationsdiscriminating, MERO, COORDd(ii.)
theSLQS.?
according towe combinedir product.
The-a hyponymof SLQS, we,.
AP, again using:RANDOM0.38 0.170.31 0.210.39 0.210.35 0.24ask 2.s of therelations.
The1 for.The product between SLQSgets the best performance in identifying(+0.09 in comparison todiscriminating it from COORDWeedsPrec).
It also achievesdiscriminating MERO (-0.04On the other hand, it seems to getlower precision in discriminating(+0.03 in comparison to WeedsPrecreason is that unrelated pairs might also have afairly high semantic generality difference,slightly affecting the measureFigure 1 gives a graphic depiction of theperformances.
SLQS corresponds to theline in comparison to theborders, grey fill), the vector cborders) and the baseline (grey fill).Figure 1.
AP values5 Conclusions and future workIn this paper, we have proposedasymmetric distributional measure of semanticgenerality which is able to identify theterm in a hypernym-hyponym paircombined with vector cosinehypernymy from other types of semanticrelations.
The successful performance ofthe reported experimentshyponyms and hypernymssimilar, but hyponyms tend to occur in moreinformative contexts than hypernyms.shows that an ?intensional?hypernymy can be pursued in distributionalterms.
This opens up newstudy of semantic relationsresearch, SLQS will also be tested on otherdatasets and languages.and vector cosineHYPERWeedsPrec) and in(-0.08 thanbetter results inthan WeedsPrec).a slightlyRANDOM-N).
The likely?s performance.blackWeedsPrec (blackosine (greyfor Task 2.SLQS, a newbroaderand, when, to discriminateSLQS inconfirms thatare distributionallySLQScharacterization ofpossibilities for thein DSMs.
In further41ReferencesBaroni, Marco and Lenci, Alessandro.
2010.?Distributional Memory: A general framework forcorpus-based semantics?.
ComputationalLinguistics, Vol.
36 (4).
673-721.Baroni, Marco and Lenci, Alessandro.
2011.
?Howwe BLESSed distributional semanticevaluation?.
Proceedings of the EMNLP 2011Geometrical Models for Natural LanguageSemantics (GEMS 2011) Workshop.
Edinburg, UK.1-10.Clarke, Daoud.
2009.
?Context-theoretic semanticsfor natural language: An overview?.
Proceedingsof the Workshop on Geometrical Models of NaturalLanguage Semantics.
Athens, Greece.
112-119.Evert, Stefan.
2005.
The Statistics of WordCooccurrences.
Dissertation, Stuttgart University.Geffet, Maayan and Dagan, Idan.
2005.
?TheDistributional Inclusion Hypotheses and LexicalEntailment?.
Proceedings of 43rd Annual Meetingof the ACL.
Michigan, USA.
107-114.Harris, Zellig.
1954.
?Distributional structure?.
Word,Vol.
10 (23).
146-162.Hearst, Marti A.
1992.
?Automatic Acquisition ofHyponyms from Large Text Corpora?.Proceedings of the 14th International Conferenceon Computational Linguistics.
Nantes, France.539-545.Kotlerman, Lili, Dagan, Ido, Szpektor, Idan, andZhitomirsky-Geffet, Maayan.
2010.
?DirectionalDistributional Similarity for Lexical Inference?.Natural Language Engineering, Vol.
16 (4).
359-389.Lenci, Alessandro and Benotto, Giulia.
2012.?Identifying hypernyms in distributional semanticspaces?.
SEM 2012 ?
The First Joint Conferenceon Lexical and Computational Semantics.
Montr?al,Canada.
Vol.
2.
75-79.Murphy, Gregory L.. 2002.
The Big Book of Concepts.The MIT Press, Cambridge, MA.Murphy, M. Lynne.
2003.
Lexical meaning.Cambridge University Press, Cambridge.Pad?, Sebastian and Lapata, Mirella.
2007.?Dependency-based Construction of SemanticSpace Models?.
Computational Linguistics, Vol.33 (2).
161-199.Priddy, Kevin L. and Keller, Paul E. 2005.
ArtificialNeural Networks: An Introduction.
SPIE Press -International Society for Optical Engineering,October 2005.Sahlgren, Magnus.
2006.
The Word-Space Model:Using distributional analysis to representsyntagmatic and paradigmatic relations betweenwords in high-dimensional vector spaces.
Ph.D.dissertation, Department of Linguistics, StockholmUniversity.Shannon, Claude E. 1948.
?A mathematical theory ofcommunication?.
Bell System Technical Journal,Vol.
27.
379-423 and 623-656.Turney, Peter D. and Pantel, Patrick.
2010.
?FromFrequency to Meaning: Vector Space Models ofSemantics?.
Journal of Articial IntelligenceResearch, Vol.
37.
141-188.Weeds, Julie and Weir, David.
2003.
?A generalframework for distributional similarity?.Proceedings of the 2003 Conference on EmpiricalMethods in Natural Language Processing.
Sapporo,Japan.
81-88.Weeds, Julie, Weir, David and McCarthy, Diana.2004.
?Characterising measures of lexicaldistributional similarity?.
Proceedings of COLING2004.
Geneva, Switzerland.1015-1021.42
