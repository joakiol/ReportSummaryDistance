Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 7?12,Avignon, France, 24 April 2012. c?2012 Association for Computational LinguisticsA high speed transcription interface for annotating primary linguisticdataMark Dingemanse, Jeremy Hammond, Herman Stehouwer,Aarthy Somasundaram, Sebastian DrudeMax Planck Institute for PsycholinguisticsNijmegen{mark.dingemanse, jeremy.hammond, herman.stehouwer,aarthy.somasundaram, sebastian.drude}@mpi.nlAbstractWe present a new transcription mode forthe annotation tool ELAN.
This mode isdesigned to speed up the process of creat-ing transcriptions of primary linguistic data(video and/or audio recordings of linguisticbehaviour).
We survey the basic transcrip-tion workflow of some commonly usedtools (Transcriber, BlitzScribe, and ELAN)and describe how the new transcription in-terface improves on these existing imple-mentations.
We describe the design ofthe transcription interface and explore somefurther possibilities for improvement in theareas of segmentation and computationalenrichment of annotations.1 IntroductionRecent years have seen an increasing interest inlanguage documentation: the creation and preser-vation of multipurpose records of linguistic pri-mary data (Gippert et al, 2006; Himmelmann,2008).
The increasing availability of portablerecording devices enables the collection of pri-mary data even in the remotest field sites, and theexponential growth in storage makes it possibleto store more of this data than ever before.
How-ever, without content annotation for searching andanalysis, such corpora are of limited use.
Ad-vances in machine learning can bring some mea-sure of automation to the process (Tscho?pel etal., 2011), but the need for human annotation re-mains, especially in the case of primary data fromundocumented languages.
This paper describesthe development and use of a new rapid transcrip-tion interface, its integration in an open sourcesoftware framework for multimodality research,and the possibilities it opens up for computationaluses of the annotated data.Transcription, the production of a written rep-resentation of audio and video recordings ofcommunicative behaviour, is one of the mosttime-intensive tasks faced by researchers work-ing with language data.
The resulting data is use-ful in many different scientific fields.
Estimatesfor the ratio of transcription time to data timelength range from 10:1 or 20:1 for English data(Tomasello and Stahl, 2004, p. 104), but maygo up to 35:1 for data from lesser known and en-dangered languages (Auer et al, 2010).
As in allfields of research, time is a most important limit-ing factor, so any significant improvement in thisarea will make available more data and resourcesfor analysis and model building.
The new tran-scription interface described here is designed forcarrying out high-speed transcription of linguis-tic audiovisual material, with built-in support formultiple annotation tiers and for both audio andvideo streams.Basic transcription is only the first step; fur-ther analysis often necessitates more fine-grainedannotations, for instance part of speech taggingor morpheme glossing.
Such operations are evenmore time intensive.
Time spent on further an-notations generally goes well over a 100:1 anno-tation time to media duration ratio1 (Auer et al,2010).The post-transcription work is also an areawith numerous possibilities for further reductionof annotation time by applying semi-automatedannotation suggestions, and some ongoing work1Cf.
a blog post by P.K.Austin http://blogs.usyd.edu.au/elac/2010/04/how long is a piece of string.html.7to integrate such techniques in our annotation sys-tem is discussed below.2 Semi-automatic transcription:terminology and existing toolsTranscription of linguistic primary data has longbeen a concern of researchers in linguistics andneighbouring fields, and accordingly several toolsare available today for time-aligned annotationand transcription.
To describe the different userinterfaces these tools provide, we adopt a modelof the transcription process by (Roy and Roy,2009), adjusting its terminology to also cover theuse case of transcribing sign language.
Accordingto this model, the transcription of primary linguis-tic data can be divided into four basic subtasks:1) find linguistic utterances in the audio or videostream, 2) segment the stream into short chunksof utterances, 3) play the segment, and 4) type thetranscription for the segment.Existing transcription tools implement thesefour steps in different ways.
To exemplify this wediscuss three such tools below.
All three can beused to create time-aligned annotations of audioand/or video recordings, but since they have dif-ferent origins and were created for different goals,they present the user with interfaces that differquite radically.Transcriber (Barras et al, 2001) was ?designedfor the manual segmentation and transcription oflong duration broadcast news recordings, includ-ing annotation of speech turns, topics and acousticcondition?
(Barras et al, 2001, p. 5).
It providesa graphical interface with a text editor at the topand a waveform viewer at the bottom.
All foursubtasks from the model above, FSPT, are donein this same interface.
The text editor, where Seg-menting and Typing are done, is a vertically ori-ented list of annotations.
Strengths of the Tran-scriber implementation are the top-to-bottom ori-entation of the text editor, which is in line withthe default layout of transcripts in the discipline,and the fact that it is possible to rely on only oneinput device (the keyboard) for all four subtasks.Weaknesses are the fact that it does not mark an-notation ends, only beginnings,and that it treatsthe data as a single stream and insists on a strictpartitioning, making it difficult to handle overlap-ping speech, common in conversational data (Bar-ras et al, 2001, p. 18).BlitzScribe (Roy and Roy, 2009) was devel-oped in the context of the Human Speechomeproject at the MIT Media Lab as a custom solu-tion for the transcription of massive amounts ofunstructured English speech data collected over aperiod of three years (Roy et al, 2006).
It is notavailable to the academic community, but we de-scribe it here because its user interface presentssignificant improvements over previous models.BlitzScribe uses automatic speech detection forsegmentation, and thus eliminates the first twosteps of the FSPT model, Find and Segment, fromthe user interface.
The result is a minimalist de-sign which focuses only on Playing and Typing.The main strength of BlitzScribe is this stream-lined interface, which measurably improves tran-scription speed ?
it is about four times as fast asTranscriber (Roy and Roy, 2009, p. 1649).
Weak-nesses include its monolingual, speech-centric fo-cus, its lack of a mechanism for speaker identi-fication, and its single-purpose design which tiesit to the Human Speechome project and makes itunavailable to the wider academic community.ELAN (Wittenburg et al, 2006) was developedas a multimedia linguistic annotation framework.Unlike most other tools it was built with multi-modal linguistic data in mind, supporting the si-multaneous display and annotation of multiple au-dio and video streams.
Its data model is tier-based, with multiple tiers available for annota-tions of different speakers or different modalities(e.g.
speech and gesture).
Its strengths are itssupport for multimodal data, its handling of over-lapping speech, its flexible tier structure, and itsopen source nature.
Its noted weaknesses includea steep learning curve and a user interface thatwas, as of 2007, ?not the best place to work on a?first pass?
of a transcript?
(Berez, 2007, p. 288).The new user interface we describe in this pa-per is integrated in ELAN as a separate ?Tran-scription Mode?, and was developed to combinethe strengths of existing implementations while atthe same time addressing their weaknesses.
Fig-ure 1 shows a screenshot of the new transcriptionmode.3 Description of the interfaceFrom the default Annotation Mode in ELAN, theuser can switch to several other modes, one ofwhich is Transcription Mode.
Transcription Modedisplays annotations in one or more columns.
Acolumn collects annotations of a single type.
For8Figure 1: The interface of the transcription mode, showing two columns: transcriptions and the correspondingtranslations.instance, the first column in Figure 1 displays allannotations of the type ?practical orthography?in chronological order, colour-coding for differ-ent speakers.
The second column displays cor-responding, i.e., time aligned, annotations of thetype ?literal translation?.
Beside the annotationcolumns there is a pane showing the data (videoand/or audio stream) for the selected utterance.Below it are basic playback parameters like vol-ume and rate, some essential interface settings,and a button ?Configure?
which brings up the col-umn selection dialog window.
We provide an ex-ample of this preference pane in Figure 2.The basic organisation of the TranscriptionMode interface reflects its task-oriented design:the annotation columns occupy pride of place andonly the most frequently accessed settings aredirectly visible.
Throughout, the user interfaceis keyboard-driven and designed to minimise thenumber of actions the user needs to carry out.
Forinstance, selecting a segment (by mouse or key-board) will automatically trigger playback of thatsegment (the user can play and pause using theTab key).
Selecting a grey (non-existent) field ina dependent column will automatically create anannotation.
Selection always opens up the fieldfor immediate editing.
Arrow keys as well as user-configurable shortcuts move to adjacent fields.ELAN Transcription Mode improves the tran-scription workflow by taking apart the FSPTmodel and focusing only on the last two steps:Play and Type.
In this respect it is likeBlitzScribe; but it is more advanced than that andother tools in at least two important ways.
First,it is agnostic to the type of data transcribed.
Sec-ond, it does not presuppose monolingualism andis ready for multilingual work.
It allows the dis-play of multiple annotation layers and makes foreasy navigation between them.Further, when tran-scription is done with the help of a native speakerit allows for them to provide other relevant infor-mation at the same time (such as cultural back-ground explanations) keeping primary data andmeta-data time aligned and linked.Some less prominently visible features of theuser interface design include: the ability to re-order annotation columns by drag and drop; a tog-gle for the position of the data streams (to the leftor to the right of the annotation columns); the abil-ity to detach the video stream (for instance for dis-play on a secondary monitor); the option to shownames (i.e.
participant ID?s) in the flow of anno-9Figure 2: The interface of the transcription mode; the configuration dialog.tations or to indicate them by colour-coding only;the option to keep the active annotation centered;and settings for font size and number of columns(in the ?Configure?
pane).
These features enablethe user to customise the transcription experienceto their own needs.The overall design of Transcription Modemakes the process of transcription as smooth aspossible by removing unnecessary clutter, fore-grounding the interface elements that matter, andenabling a limited degree of customisation.
Over-all, the new interface has realised significantspeedups for many people2.
User feedback in re-sponse to the new transcription mode has beenoverwhelmingly positive, e.g., the members ofmailing lists such as the Resource Network forLinguistic Diversity3.4 A prerequisite: semi-automaticsegmentationAs we noted in the introduction, the most im-portant step before transcription is that of seg-mentation (steps Find and Segment in the FSPTmodel).
Segmentation is a large task that involvessubdividing the audio or video stream in, possi-bly overlapping, segments.
The segments eachdenote a distinct period of speech or any othercommunicative act and each segment is com-2Including ourselves, Jeremy Hammond claims that:?Based on my last two field work trips, I am getting my tran-scription time down below that of transcriber (but perhapsnot by much) but still keeping the higher level of data thatELANs tiers provide - probably around 18-20 hours for anhour of somewhat detailed trilingual annotation.
?3www.rnld.orgmonly assigned to a specific speaker.
This stepcan potentially be sped up significantly by doingit semi-automatically using pattern recognitiontechniques, as pursued in the AVATecH project(Auer et al, 2010).In the AVATecH project, audio and videostreams can be sent to detection componentscalled ?recognisers?.
Some detection compo-nents accept the output of other recognisers asadditional input, next to the audio and/or videostreams, thus facilitating cascaded processing ofthese streams.
Amongst the tasks that can be per-formed by these recognisers is the segmentationof audio and video, including speaker assignment.A special challenge for the recognisers in thisproject is the requirement of language indepen-dence (in contrast to the English-only situationin the Human Speechome project that producedBlitzscribe(Roy et al, 2006)).
The recognisersshould ideally accommodate the work of fieldlinguists and other alike researchers and there-fore cannot simply apply existing language andacoustic models.
Furthermore, the conditions thatare encountered in the field are often not ideal,e.g., loud and irregular background noises such asthose from animals are common.
Nevertheless,automatic segmentation has the potential to speedup the segmentation step greatly.5 Future possibilities: computationalapproaches to data enrichmentWhile a basic transcription and translation is es-sential as a first way into the data, it is not suf-ficient for many research questions, linguistic or10otherwise.
Typically a morphological segmenta-tion of the words and a labelling of each individ-ual morph is required.
This level of annotation isalso known as basic glossing (Bow et al, 2003b;Bow et al, 2003a).Automatically segmenting the words into theirmorphological parts, without resorting to the useof pre-existing knowledge has seen a wide vari-ety of research (Hammarstro?m and Borin, 2011).Based on the knowledge-free induction of mor-phological boundaries the linguist will usuallyperform corrections.
Above all, a system mustlearn from the input of the linguist, and must in-corporate it in the results, improving the segmen-tation of words going forward.
However, it is wellknown from typological research that languagesdiffer tremendously in their morphosyntactic or-ganisation and the specific morphological meansthat are employed to construct complex meanings(Evans and Levinson, 2009; Hocket, 1954).As far as we know, there is no current morpho-logical segmentation or glossing system that dealswell with all language types, in particular inflec-tional and polysynthetic languages or languagesthat heavily employ tonal patterns to mark differ-ent forms of the same word.
Therefore, there isa need for an interactive, modular glossing sys-tem.
For each step of the glossing task, one woulduse one, or a set of complementary modules.
Wecall such modules ?annotyzers?.
They generatecontent on the basis of the source tiers and addi-tional data, e.g.
lexical data (or learnt states fromearlier passes).
Using such modules will resultin a speedup for the researcher.
We remark thatthere are existing modular NLP systems, such asGATE(Cunningham et al, 2011), however theseare tied to different workflows, i.e., they are not assuitable for the multimodal multi-participant an-notation process.Currently a limited set of such functionality isavailable in Toolbox and FLEX.
In the case ofboth Toolbox and FLEX the functionality is lim-ited to a set of rules written by the linguist (i.e.in a database-lookup approach).
Even thoughthe ELAN modules will offer support for suchrules, our focus is on the automation of machine-learning systems in order to scale the annotationprocess.Our main aim for the future is to incorporatelearning systems that support the linguists by im-proving the suggested new annotations on thebases of choices the linguist made earlier.
Thegoal there is, again, to reduce annotation time, sothat the linguist can work more on linguistic anal-ysis and less on annotating.
At the same time,a working set of annotyzers will promote morestandardised glossing, which can then be used forfurther automated research, cf.
automatic tree-bank production or similar (Bender et al, 2011).6 ConclusionsThe diversity of the world?s languages is in dan-ger.
Perhaps user interface design is not the firstthing that comes to mind in response to this sober-ing fact.
Yet in a field that increasingly works withdigital annotations of primary linguistic data, it isimperative that the basic tools for annotation andtranscription are optimally designed to get the jobdone.We have described Transcription Mode, a newuser interface in ELAN that accelerates the tran-scription process.
This interface offers several ad-vantages compared to similar tools in the softwarelandscape.
It automates actions wherever pos-sible, displays multiple parallel information andannotation streams, is controllable with just thekeyboard, and can handle sign language as wellas spoken language data.
Transcription Mode re-duces the required transcription time by providingan optimised workflow.The next step is to optimise the preceding andfollowing stages in the annotation process.
Pre-ceding the transcription stage is segmentation andspeaker labelling, which we address using auto-matic audio/video recogniser techniques that areindependent of the language that is transcribed.Following transcription, we aim to support basicglossing (and similar additional annotations basedon transcriptions) with a modular software archi-tecture.
These semi-automated steps lead to fur-ther time savings, allowing researchers to focuson the analysis of language data rather than on theproduction of annotations.The overall goal of the developments describedhere is to help researchers working with primarylanguage data to use their time more optimally.Ultimately, these improvements will lead to an in-crease in both quality and quantity of primary dataavailable for analysis.
Better data and better anal-yses for a stronger digital humanities.11ReferencesEric Auer, Peter Wittenburg, Han Sloetjes, OliverSchreer, Stefano Masneri, Daniel Schneider, andSebastian Tscho?pel.
2010.
Automatic annotationof media field recordings.
In Proceedings of theECAI 2010 Workshop on Language Technology forCultural Heritage, Social Sciences, and Humanities(LaTeCH 2010), pages 31?34.Claude Barras, Edouard Geoffrois, Zhibiao Wu, andMark Liberman.
2001.
Transcriber: Develop-ment and use of a tool for assisting speech corporaproduction.
Speech Communication, 33(1-2):5?22,January.Emily M. Bender, Dan Flickinger, Stephan Oepen, andYi Zhang.
2011.
Parser evaluation over local andnon-local deep dependencies in a large corpus.
InProceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages397?408, Edinburgh, Scotland, UK., July.
Associ-ation for Computational Linguistics.Andrea L. Berez.
2007. Review of EUDICO linguis-tic annotator (ELAN).
Language Documentation &Conservation, 1(2):283?289, December.Catherine Bow, Baden Hughes, and Steven Bird.2003a.
A four-level model for interlinear text.Cathy Bow, Baden Hughes, and Steven Bird.
2003b.Towards a general model of interlinear text.
InProceedings of EMELD Workshop 2003: Digitizingand Annotating Texts and Field Recordings.
Lans-ing MI, USA.Hamish Cunningham, Diana Maynard, KalinaBontcheva, Valentin Tablan, Niraj Aswani, IanRoberts, Genevieve Gorrell, Adam Funk, AngusRoberts, Danica Damljanovic, Thomas Heitz,Mark A. Greenwood, Horacio Saggion, JohannPetrak, Yaoyong Li, and Wim Peters.
2011.
TextProcessing with GATE (Version 6).Nicholas Evans and Stephen C. Levinson.
2009.
Themyth of language universals: Language diversityand its importance for cognitive science.
Behav-ioral and Brain Sciences, 32(05):429?448.Jost Gippert, Nikolaus P. Himmelmann, and UlrikeMosel, editors.
2006.
Essentials of language docu-mentation.
Mouton de Gruyter, Berlin / New York.Harald Hammarstro?m and Lars Borin.
2011.
Un-supervised learning of morphology.
To Appear inComputational Linguistics.Nikolaus P. Himmelmann.
2008.
Reproduction andpreservation of linguistic knowledge: Linguistics?response to language endangerment.
In Annual Re-view of Anthropology, volume 37 (1), pages 337?350.Charles F. Hocket.
1954.
Two models of grammaticaldescription.
Word 10, pages 210?234.Chris Rogers.
2010. Review of fieldworks languageexplorer (flex) 3.0.
In Language Documentation& Conservation 4, pages 1934?5275.
University ofHawai?i Press.Brandon C. Roy and Deb Roy.
2009.
Fast transcrip-tion of unstructured audio recordings.
In Proceed-ings of Interspeech 2009, Brighton, England.Deb Roy, Rupal Patel, Philip DeCamp, Rony Kubat,Michael Fleischman, Brandon C. Roy, NikolaosMavridis, Stefanie Tellex, Alexia Salata, JethranGuinness, Micheal Levit, and Peter Gorniak.
2006.The human speechome project.
In Paul Vogt, Yu-uga Sugita, Elio Tuci, and Chrystopher Nehaniv, ed-itors, Symbol Grounding and Beyond, volume 4211of Lecture Notes in Computer Science, pages 192?196.
Springer, Berlin / Heidelberg.Michael Tomasello and Daniel Stahl.
2004.
Sam-pling children?s spontaneous speech: How much isenough?
Journal of Child Language, 31(01):101?121.Sebastian Tscho?pel, Daniel Schneider, Rolf Bardeli,Peter Wittenburg, Han Sloetjes, Oliver Schreer, Ste-fano Masneri, Przemek Lenkiewicz, and Eric Auer.2011.
AVATecH: Audio/Video technology for hu-manities research.
Language Technologies for Dig-ital Humanities and Cultural Heritage, page 86.Peter Wittenburg, Hennie Brugman, Albert Russel,and Han Sloetjes.
2006.
ELAN: a professionalframework for multimodality research.
In Proceed-ings of LREC 2006.12
