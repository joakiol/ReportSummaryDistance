Experiments Adapting an Open-Domain Question Answering System tothe Geographical Domain Using Scope-Based ResourcesDaniel Ferre?s and Horacio Rodr?
?guezTALP Research CenterSoftware DepartmentUniversitat Polite`cnica de Catalunya{dferres, horacio}@lsi.upc.eduAbstractThis paper describes an approach to adaptan existing multilingual Open-DomainQuestion Answering (ODQA) system forfactoid questions to a Restricted Domain,the Geographical Domain.
The adaptationof this ODQA system involved the modifi-cation of some components of our systemsuch as: Question Processing, Passage Re-trieval and Answer Extraction.
The newsystem uses external resources like GNSGazetteer for Named Entity (NE) Classi-fication and Wikipedia or Google in orderto obtain relevant documents for this do-main.
The system focuses on a Geograph-ical Scope: given a region, or country, anda language we can semi-automatically ob-tain multilingual geographical resources(e.g.
gazetteers, trigger words, groups ofplace names, etc.)
of this scope.
Thesystem has been trained and evaluated forSpanish in the scope of the Spanish Geog-raphy.
The evaluation reveals that the useof scope-based Geographical resources isa good approach to deal with multilingualGeographical Domain Question Answer-ing.1 IntroductionQuestion Answering (QA) is the task of, givena query expressed in Natural Language (NL), re-trieving its correct answer (a single item, a textsnippet,...).
QA has become a popular task in theNL Processing (NLP) research community in theframework of different international ODQA eval-uation contests such as: Text Retrieval Confer-ence (TREC) for English, Cross-Lingual Evalua-tion Forum (CLEF) for European languages, andNTCIR for Asian languages.In this paper we describe our experiments in theadaptation and evaluation of an ODQA system toa Restricted Domain, the Geographical Domain.GeoTALP-QA is a multilingual Geographi-cal Domain Question Answering (GDQA) sys-tem.
This Restricted Domain Question Answer-ing (RDQA) system has been built over an existingODQA system, TALP-QA, a multilingual ODQAsystem that processes both factoid and definitionquestions (see (Ferre?s et al, 2005) and (Ferre?s etal., 2004)).
The system was evaluated for Spanishand English in the context of our participation inthe conferences TREC and CLEF in 2005 and hasbeen adapted to a multilingual GDQA system forfactoid questions.As pointed out in (Benamara, 2004), the Geo-graphical Domain (GD) can be considered a mid-dle way between real Restricted Domains andopen ones because many open domain texts con-tain a high density of geographical terms.Although the basic architecture of TALP-QAhas remained unchanged, a set of QA componentswere redesigned and modified and we had to addsome specific components for the GD to our QAsystem.
The basic approach in TALP-QA consistsof applying language-dependent processes on bothquestion and passages for getting a language inde-pendent semantic representation, and then extract-ing a set of Semantic Constraints (SC) for eachquestion.
Then, an answer extraction algorithmextracts and ranks sentences that satisfy the SCsof the question.
Finally, an answer selection mod-ule chooses the most appropriate answer.We outline below the organization of the paper.In the next section we present some characteris-tics of RDQA systems.
In Section 3, we presentEACL 2006 Workshop on Multilingual Question Answering - MLQA0669the overall architecture of GeoTALP-QA and de-scribe briefly its main components, focusing onthose components that have been adapted from anODQA to a GDQA.
Then, the Scope-Based Re-sources needed for the experimentation and the ex-periments are presented in Sections 4 and 5.
Insection 6 we present the results obtained over aGD corpus.
Finally, in Section 7 and 8 we describeour conclusions and the future work.2 Restricted Domain QA SystemsRDQAs present some characteristics that preventus from a direct use of ODQA systems.
The mostimportant differences are:?
Usually, for RDQA, the answers are searchedin relatively small domain specific collec-tions, so methods based on exploiting the re-dundancy of answers in several documentsare not useful.
Furthermore, a highly accu-rate Passage Retrieval module is required be-cause frequently the answer occurs in a verysmall set of passages.?
RDQAs are frequently task-based.
So, therepertory of question patterns is limited al-lowing a good accuracy in Question Process-ing with limited effort.?
User requirements regarding the quality ofthe answer tend to be higher in RDQA.
As(Chung et al, 2004) pointed out, no answeris preferred to a wrong answer.?
In RDQA not only NEs but also domain spe-cific terminology plays a central role.
Thisfact usually implies that domain specific lex-icons and gazetteers have to be used.?
In some cases, as in GD, many documents in-cluded in the collections are far to be stan-dard NL texts but contain tables, lists, ill-formed sentences, etc.
sometimes followinga more or less defined structure.
Thus, extrac-tion systems based, as our, on the linguisticstructure of the sentences have to be relaxedin some way to deal with this kind of texts.More information about RDQA systems can befound in the ACL 2004 Workshop on QA in Re-stricted Domains1 and the AAAI 2005 Worshopon Question Answering in Restricted Domains(Molla and Vicedo, 2005) .1http://acl.ldc.upenn.edu/acl2004/qarestricteddomain/3 System DescriptionGeoTALP-QA has been developed within theframework of ALIADO2 project.
The systemarchitecture uses a common schema with threephases that are performed sequentially withoutfeedback: Question Processing (QP), Passage Re-trieval (PR) and Answer Extraction (AE).
Moredetails about this architecture can be found in(Ferre?s et al, 2005) and (Ferre?s et al, 2004).Before describing these subsystems, we intro-duce some additional knowledge sources that havebeen added to our system for dealing with thegeographic domain and some language-dependentNLP tools for English and Spanish.
Our aim is todevelop a language independent system (at leastable to work with English and Spanish).
Lan-guage dependent components are only includedin the Question Pre-processing and Passage Pre-processing components, and can be easily substi-tuted by components for other languages.3.1 Additional Knowledge SourcesOne of the most important task to deal with theproblem of GDQA is to detect and classify NEswith its correct Geographical Subclass (see classesin Section 3.3).
We use Geographical scope basedKnowledge Bases (KB) to solve this problem.These KBs can be built using these resources:?
GEOnet Names Server (GNS3).
A world-wide gazetteer, excluding the USA andAntarctica, with 5.3 million entries.?
Geographic Names Information System(GNIS4).
A gazetteer with 2.0 million entriesabout geographic features of the USA.?
Grammars for creating NE aliases.
Ge-ographic NEs tend to occur in a great va-riety of forms.
It is important to take thisinto account to avoid losing occurrences.A set of patterns for expanding have beencreated.
(e.g.
<toponym> Mountains,<toponym> Range, <toponym> Chain).?
Trigger Words Lexicon.
A lexicon con-taining trigger words (including multi-wordterms) is used for allowing local disambigua-tion of ambiguous NE, both in the questionsand in the retrieved passages.2ALIADO.
http://gps-tsc.upc.es/veu/aliado3GNS.
http://earth-info.nga.mil/gns/html4GNIS.
http://geonames.usgs.gov/geonames/stategazEACL 2006 Workshop on Multilingual Question Answering - MLQA0670Working with geographical scopes avoids manyambiguity problems, but even in a scope theseproblems occur:?
Referent ambiguity problem.
This problemoccurs when the same name is used for sev-eral locations (of the same or different class).In a question, sometimes it is impossible tosolve this ambiguity, and, in this case, wehave to accept as correct all of the possible in-terpretations (or a superclass of them).
Oth-erwise, a trigger phrase pattern can be usedto resolve the ambiguity (e.g.
?Madrid?
isan ambiguous NE, but in the phrase, ?comu-nidad de Madrid?
(State of Madrid), ambigu-ity is solved).
Given a scope, we automati-cally obtain the most common trigger phrasepatterns of the scope from the GNS gazetteer.?
Reference ambiguity problem.
This prob-lem occurs when the same location can havemore than one name (in Spanish texts this fre-quently occurs as many place names occurin languages other than Spanish, as Basque,Catalan or Galician).
Our approach to solvethis problem is to group together all the ge-ographical names that refer to the same lo-cation.
All the occurrences of the geograph-ical NEs in both questions and passages aresubstituted by the identifier of the group theybelong to.We used the geographical knowledge avail-able in the GNS gazetteer to obtain this ge-ographical NEs groups.
First, for each placename in the scope-based GNS gazetteer weobtained all the NEs that have the same fea-ture designation code, latitude and longitude.For each group, we then selected an identifierchoosing one of the NE included in it usingthe following heuristics: the information ofthe GNS field ?native?
tells if a place name isnative, conventional, a variant, or, is not ver-ified.
So we decided the group representativeassigning the following order of priorities tothe names: native, conventional name, vari-ant name, unverified name.
If there is morethan one place name in the group with thesame name type we decide that the additionallength gives more priority to be cluster repre-sentative.
It is necessary to establish a set ofpriorities among the different place names ofthe group because in some retrieval engines(e.g.
web search engines) is not possible todo long queries.3.2 Language-Dependent Processing ToolsA set of general purpose NLP tools are used forSpanish and English.
The same tools are used forthe linguistic processing of both the questions andthe passages (see (Ferre?s et al, 2005) and (Ferre?set al, 2004) for a more detailed description ofthese tools).
The tools used for Spanish are:?
FreeLing, which performs tokenization, mor-phological analysis, POS tagging, lemmati-zation, and partial parsing.?
ABIONET, a NE Recognizer and Classifier(NERC) on basic categories.?
EuroWordNet, used to obtain a list of synsets,a list of hypernyms of each synset, and theTop Concept Ontology class.The following tools are used to process English:?
TnT, a statistical POS tagger.?
WordNet lemmatizer 2.0.?
ABIONET.?
WordNet 1.5.?
A modified version of the Collins parser.?
Alembic, a NERC with MUC classes.3.3 Question ProcessingThe main goal of this subsystem is to detect theQuestion Type (QT), the Expected Answer Type(EAT), the question logic predicates, and the ques-tion analysis.
This information is needed for theother subsystems.
We use a language-independentformalism to represent this information.We apply the processes described above to thethe question and passages to obtain the followinginformation:?
Lexical and semantic information for eachword: form, lemma, POS tag (Eagles or PTBtag-set), semantic class and subclass of NE,and a list of EWN synsets.?
Syntactic information: syntactic constituentstructure of the sentence and the informationof dependencies and other relations betweenthese components.EACL 2006 Workshop on Multilingual Question Answering - MLQA0671Once this information is obtained we can findthe information relevant to the following tasks:?
Environment Building.
The semantic pro-cess starts with the extraction of the semanticrelations that hold between the different com-ponents identified in the question text.
Theserelations are organized into an ontology ofabout 100 semantic classes and 25 relations(mostly binary) between them.
Both classesand relations are related by taxonomic links.The ontology tries to reflect what is neededfor an appropriate representation of the se-mantic environment of the question (and theexpected answer).
A set of about 150 ruleswas built to perform this task.
The ontologyhas been extended for the GD (see below theclasses related with this domain).ENTITYENTITY_PROPER_PLACEGEOLOGICAL_REGIONARCHIPELAGOISLANDLAND_FORMMOUNTAINSEA_FORMCAPEGULFSEAWATER_FORMRIVERPOLITICAL_REGIONCITYCONTINENTCOUNTYCOUNTRYSTATEENTITY_QUANTITYNUMERICMAGNITUDEAREALENGTHFLOWWEIGHT?
Question Classification.
Our ODQA systemuses 25 QTs.
For the GD we only used 10Question Types (see Table 1).
Only 5 QTs arecommon with the ODQA QTs, 5 QTs havebeen specially created for this domain.Question Type Expected Answer TypeCount objects NUMBERHow many people NUMBERWhat area MEASURE AREAWhat flow MEASURE FLOWWhat height MEASURE HEIGHTWhat length MEASURE LENGTHWhere action LOCATION SUBCLASSWhere location LOCATION SUBCLASSWhere quality LOCATION SUBCLASSDefault class LOCATIONTable 1: QTs and Expected Answer Types.In order to determine the QT our system usesa Prolog DCG Parser.
This parser uses thefollowing features: word form, word positionin the question, lemma and part-of-speech(POS).
A set of DCG rules was manuallyconfigured in order to ensure a sufficient cov-erage.The parser uses external information: geo-graphical NE subclasses, trigger words foreach Geographical subclass (e.g.
?poblado?
(ville)), semantically related words of eachsubclass (e.g.
?water?
related with sea andriver), and introductory phrases for eachQuestion Type (e.g.
?which extension?
is aphrase of the QT What area).?
Semantic Constraints Extraction.
Depend-ing on the QT, a subset of useful items ofthe environment has to be selected in orderto extract the answer.
Accordingly, we definethe set of relations (the semantic constraints)that are supposed to be found in the answer.These relations are classified as mandatory,(MC), (i.e.
they have to be satisfied in thepassage) or optional, (OC), (if satisfied thescore of the answer is higher).
In order tobuild the semantic constraints for each ques-tion, a set of rules has been manually built.A set of 88 rules is used.
An example of theconstraints extracted from an environment isshown in Table 2.
This example shows thequestion type predicted, the initial predicatesextracted from the question, the Environmentpredicates, the MCs and the OCs.
MCs areentity(4) and i en city(6).
The first predi-cate refers to token number 4 (?autonomia?
(state)) and the last predicate refers to tokennumber 6 (?Barcelona?
).Question ?
A que?
autonom?
?a pertenece Barcelona?
(At which state pertains Barcelona?)Q.
Type where locationPredicates city(?Barcelona?
),state(X),pertains(?Barcelona?,X)Environment action(5), participant in event(5,4),theme of event(5,6),prep(4,2),entity(4),i en proper place(6),det(4,3),qu(3)Mandatory entity(4),i en city(6)ConstraintsOptional action(5),theme of event(5,6),Constraints participant in event(5,4),prep(4,2),type of location(5,5,i en state),property(5,5,pertenecer,3,6)Table 2: Question Analysis example.EACL 2006 Workshop on Multilingual Question Answering - MLQA06723.4 Passage RetrievalWe use two different approaches for Passage Re-trieval.
The first one uses a pre-processed corpusas a document collection.
The second one uses theweb as document collection.3.4.1 Off-line Corpus RetrievalThis approach uses a pre-processed and indexedcorpus with Scope-related Geographical Informa-tion as a document collection for Passage Re-trieval.
The processed information was used forindexing the documents.
Storing this informationallows us to avoid the pre-processing step afterretrieval.
The Passage Retrieval algorithm usedis the same of our ODQA system: a data-drivenquery relaxation technique with dynamic passagesimplemented using Lucene IR engine API (See(Ferre?s et al, 2005) for more details).3.4.2 Online Web Snippet RetrievalThe other approach uses a search-engine to getsnippets with relevant information.
We expect toget a high recall with few snippets.
In our exper-iments, we chose Google as the search-engine us-ing a boolean retrieval schema that takes advan-tage of its phrase search option and the Geograph-ical KB to create queries that can retrieve highlyrelevant snippets.
We try to maximize the num-ber of relevant sentences with only one query perquestion.The algorithm used to build the queries is sim-ple.
First, some expansion methods described be-low can be applied over the keywords.
Then, stop-words (including normal stop-words and sometrigger words) are removed.
Finally, only theNouns and Verbs are extracted from the keywordslist.
The expansion methods used are:?
Trigger Words Joining (TWJ).
Uses thetrigger words list and the trigger phrase pat-tern list (automatically generated from GNS)to join trigger phrases (e.g.
?isla Conejera?
o?Sierra de los Pirineos?).?
Trigger Words Expansion (TWE).
This ex-pansion is applied to the NEs that were notdetected as a trigger phrase.
The expansionuses its location subclass to create a key-word with the pattern: TRIGGER + NE (e.g.?Conejera?
is expanded to: (?isla Conejera?OR ?Conejera?)).?
GNS Grouping Expansion (CE).
NounPhrase expansion based on the groups gen-erated from GNS Gazetteer.?
Question-based Expansion (QBE).
Thismethod appends keywords or expands thequery depending on the question type.
As anexample, in the case of a question classifiedas What length, trigger words and units as-sociated to the question class like ?longitud?
(length) and ?kilo?metros?
(kilometers) are ap-pended to the query.3.5 Answer ExtractionWe used two systems for Answer Extraction: ourODQA system (adapted for the GD) and a fre-quency based system.3.5.1 ODQA ExtractionThe linguistic process of analyzing passages issimilar to the process carried out on questions andleads to the construction of the environment ofeach sentence.
Then, a set of extraction rules areapplied following an iterative approach.
In the firstiteration all the MC have to be satisfied by at leastone of the candidate sentences.
Then, the itera-tion proceeds until a threshold is reached by re-laxing the MC.
The relaxation process of the setof semantic constraints is performed by means ofstructural or semantic relaxation rules, using thesemantic ontology.
The extraction process con-sists on the application of a set of extraction ruleson the set of sentences that have satisfied the MC.The Knowledge Source used for this process is aset of extraction rules owning a credibility score.Each QT has its own subset of extraction rules thatleads to the selection of the answer.In order to select the answer from the set of can-didates, the following scores are computed and ac-cumulated for each candidate sentence: i) the rulescore (which uses factors such as the confidenceof the rule used, the relevance of the OC satisfiedin the matching, and the similarity between NEsoccurring in the candidate sentence and the ques-tion), ii) the passage score, iii) a semantic score(see (Ferre?s et al, 2005)) , iv) the extraction rulerelaxation level score.
The answer to the questionis the candidate with the best global score.3.5.2 Frequency-Based ExtractionThis extraction algorithm is quite simple.
First,all snippets are pre-processed.
Then, we make aranked list of all the tokens satisfying the expectedEACL 2006 Workshop on Multilingual Question Answering - MLQA0673answer type of the question.
The score of each to-ken in the snippets is computed using the follow-ing formula:Score(tki) =?o?Occurrence(tki)1snippet rank(o)Finally, the top-ranked token is extracted.4 Resources for Scope-BasedExperimentsIn this section we describe how we obtained theresources needed to do experiments in the Span-ish Geography domain using Spanish.
These re-sources were: the question corpus (validation andtest), the document collection required by the off-line ODQA Passage Retrieval, and the geograph-ical scope-based resources.
Finally, we describethe experiments performed.4.1 Language and Scope based GeographicalQuestion CorpusWe obtained a corpus of Geographical questionsfrom Albayzin, a speech corpus (Diaz et al, 1998)that contains a geographical subcorpus with utter-ances of questions about the geography of Spain inSpanish.
We obtained from Albayzin a set of 6887question patterns.
We analyzed this corpus and weextracted the following type of questions: PartialDirect, Partial Indirect, and Imperative Interroga-tive factoid questions with a simple level of dif-ficulty (e.g.
questions without nested questions).We selected a set of 2287 question patterns.
As aquestion corpus we randomly selected a set of 177question patterns from the previous selection (seeTable 3).
These patterns have been randomly in-stantiated with Geographical NEs of the Albayzincorpus.
Then, we searched the answers in the Weband the Spanish Wikipedia (SW).
The results ofthis process were: 123 questions with answer inthe SW and the Web, 33 questions without answerin the SW but with answer using the Web, andfinally, 21 questions without answer (due to thefact that some questions when instantiated cannotbe answered (e.g.
which sea bathes the coast ofMadrid?)).
We divided the 123 questions with an-swer in the SW in two sets: 61 questions for devel-opment (setting thresholds and other parameters)and 62 for test.
?A que?
comunidad auto?noma pertenece el <PICO>?At which state pertains <PEAK>?
?Cua?l es el capital de <COMUNIDAD>?Which is the capital of <STATE>?
?Cua?l es la comunidad en la que desemboca el <R?IO>?What is the state in which <RIVER> flows into?
?Cua?l es la extensio?n de <COMUNIDAD>?Which is the extension of <STATE>?Longitud del r?
?o <R?IO>.Length of river <RIVER>.
?Cua?ntos habitantes tiene la <COMUNIDAD>?How many people does <STATE> has?Table 3: Some question patterns from Albayzin.4.2 Document Collection for ODQA PassageRetrievalIn order to test our ODQA Passage Retrieval sys-tem we need a document collection with sufficientgeographical information to resolve the questionsof Albayzin corpus.
We used the filtered Span-ish Wikipedia5.
First, we obtained the originalset of documents (26235 files).
Then, we selectedtwo sets of 120 documents about the Spanish ge-ography domain and the non-Spanish geographydomain.
Using these sets we obtained a set ofTopic Signatures (TS) (Lin and Hovy, 2000) forthe Spanish geography domain and another set ofTS for the non-Spanish geography domain.
Then,we used these TS to filter the documents fromWikipedia, and we obtained a set of 8851 doc-uments pertaining to the Spanish geography do-main.
These documents were pre-processed andindexed.4.3 Geographical Scope-Based ResourcesA Knowledge Base (KB) of Spanish Geographyhas been built using four resources:?
GNS: We obtained a set of 32222 non-ambiguous place names of Spain.?
Albayzin Gazetteer: a set of 758 places.?
A Grammar for creating NE aliases.
We cre-ated patterns for the summit and state classes(the ones with more variety of forms), and weexpanded this patterns using the entries of Al-bayzin.?
A lexicon of 462 trigger words.We obtained a set of 7632 groups of placenames using the grouping process over GNS.These groups contain a total of 17617 place5Spanish Wikipedia.
http://es.wikipedia.orgEACL 2006 Workshop on Multilingual Question Answering - MLQA0674names, with an average of 2.51 place names pergroup.
See in Figure 1 an example of a groupwhere the canonical term appears underlined.
{Cordillera Pirenaica, Pireneus, Pirineos, PyrenaeiMontes, Pyre?ne?es, Pyrene, Pyrenees}Figure 1: Example of a group obtained from GNS.In addition, a set of the most common triggerphrases in the domain has been obtained from theGNS gazetteer (see Table 4).Geographical ScopeSpain UKTRIGGER de NE NE TRIGGERTop-ranked TRIGGER NE TRIGGER NETrigger TRIGGER del NE TRIGGER of NEPhrases TRIGGER de la NE TRIGGER a?
NETRIGGER de las NE TRIGGER na NETable 4: Sample of the top-ranked trigger phrasesautomatically obtained from GNS gazetteer for thegeography of Spain and UK.5 ExperimentsWe have designed some experiments in order toevaluate the accuracy of the GDQA system andits subsystems (QP, PR, and AE).
For PR, weevaluated the web-based snippet retrieval usingGoogle with some variants of expansions, versusour ODQA Passage Retrieval with the corpus ofthe SW. Then, the passages (or snippets) retrievedby the best PR approach were used by the two dif-ferent Answer Extraction algorithms.
The ODQAAnswer Extractor has been evaluated taking intoaccount the answers that have a supported con-text in the set of passages (or snippets).
Finally,we evaluated the global results of the completeQA process with the different Answer Extractors:ODQA and Frequency-Based.6 ResultsThis section evaluates the behavior of our GDQAsystem over a test corpus of 62 questions and re-ports the errors detected on the best run.
We evalu-ated the three main components of our system andthe global results.?
Question Processing.
The Question Clas-sification task has been manually evaluated.This subsystem has an accuracy of 96.77%.?
Passage Retrieval.
The evaluation of thissubsystem was performed using a set of cor-rect answers (see Table 5).
We computed theanswer accuracy: it takes into account thenumber of questions that have a correct an-swer in its set of passages.Retrieval Accuracy at N passages/snippetsMode N=10 N=20 N=50 N=100Google 0.6612 0.6935 0.7903 0.8225+TWJ 0.6612 0.6774 0.7419 0.7580+TWJ+TWE 0.6612 0.6774 0.7419 0.7580+CE 0.6612 0.6774 0.7741 0.8064+QBE 0.8064 0.8387 0.9032 0.9354+TWJ+QB+CE 0.7903 0.8064 0.8548 0.8870Google+All 0.7903 0.8064 0.8548 0.8870ODQA+Wiki 0.4354 0.4516 0.4677 0.5000Table 5: Passage Retrieval results (refer to sec-tion 3.4.2 for detailed information of the differentquery expansion acronyms).?
Answer Extraction.
The evaluation ofthe ODQA Answer Extractor subsystem isshown in Table 6.
We evaluated the ac-curacy taking into account the number ofcorrect and supported answers by the pas-sages divided by the total number of ques-tions that have a supported answer in its setof passages.
This evaluation has been doneusing the results of the top-ranked retrievalconfiguration over the development set: theGoogle+TWJ+QB+CE configuration of thesnippet retriever.Accuracy at N SnippetsN=10 N=20 N=500.2439 (10/41) 0.3255 (14/43) 0.3333 (16/48)Table 6: Results of the ODQA Answer Extractionsubsystem (accuracy).In Table 7 are shown the global results of thetwo QA Answer Extractors used (ODQA andFrequency-Based).
The passages retrieved by theGoogle+TWJ+QB+CE configuration of the snip-pet retriever were used.AccuracyNum.
Snippets ODQA Freq-based10 0.1774 (11/62) 0.5645 (35/62)20 0.2580 (16/62) 0.5967 (37/62)50 0.3387 (21/62) 0.6290 (39/62)Table 7: QA results over the test set.EACL 2006 Workshop on Multilingual Question Answering - MLQA0675We analyzed the 23 questions that fail in ourbest run.
The analysis detected that 10 questionshad no answer in its set of passages.
In 5 of thesequestions it is due to have a non common ques-tion or location.
The other 5 questions have prob-lems with ambiguous trigger words (e.g.
capital)that confuse the web-search engine.
On the otherhand, 13 questions had the answer in its set of pas-sages, but were incorrectly answered.
The reasonsare mainly due to the lack of passages with the an-swer (8), answer validation and spatial-reasoning(3), multilabel Geographical NERC (1), and morecontext in the snippets (1).7 Evaluation and ConclusionsThis paper summarizes our experiments adaptingan ODQA to the GD and its evaluation in Spanishin the scope of the Spanish Geography.
Out of 62questions, our system provided the correct answerto 39 questions in the experiment with the best re-sults.Our Passage Retrieval for ODQA offers less at-tractive results when using the SW corpus.
Theproblem of using SW to extract the answers is thatit gives few documents with the correct answer,and, it is difficult to extract the answer becausethe documents contain tables, lists, ill-formed sen-tences, etc.
Our ODQA AE needs a grammati-cally well-structured text to extract correctly theanswers.
The QA system offers a low perfor-mance (33% of accuracy) when using this AE overthe web-based retrieved passages.
In some cases,the snippets are cut and we could expect a betterperformance retrieving the whole documents fromGoogle.On the other hand, web-based snippet retrieval,with only one query per question, gives good re-sults in Passage Retrieval.
The QA system withthe Frequency-Based AE obtained better resultsthan with the ODQA AE (62.9% of accuracy).Finally, we conclude that our approach withGeographical scope-based resources are notablyhelpful to deal with multilingual GeographicalDomain Question Answering.8 Future WorkAs a future work we plan to improve the AE mod-ule using a semantic analysis with extended con-texts (i.e.
more than one sentence) and addingsome spatial reasoning.
We also want to improvethe retrieval by crawling relevant documents fromweb search-engines instead of using snippets.
Thiscould be a good method to find more sentenceswith supported answers.
Finally, we expect to dotests with English in another scope.AcknowledgementsThis work has been partially supported by theEuropean Commission (CHIL, IST-2004-506909)and the Spanish Research Dept.
(ALIADO,TIC2002-04447-C02).
Daniel Ferre?s is sup-ported by a UPC-Recerca grant from UniversitatPolite`cnica de Catalunya (UPC).
TALP ResearchCenter is recognized as a Quality Research Group(2001 SGR 00254) by DURSI, the Research De-partment of the Catalan Government.ReferencesF.
Benamara.
2004.
Cooperative Question Answeringin Restricted Domains: the WEBCOOP Experiment.In Proceedings of the Workshop Question Answeringin Restricted Domains, within ACL-2004.H.
Chung, Y.
Song, K. Han, D. Yoon, J. Lee, H. Rim,and S. Kim.
2004.
A Practical QA System in Re-stricted Domains.
In Proceedings of the WorkshopQuestion Answering in Restricted Domains, withinACL-2004.J.
Diaz, A. Rubio, A. Peinado, E. Segarra, N. Prieto,and F. Casacuberta.
1998.
Development of Task-Oriented Spanish Speech Corpora.
In Procceed-ings of the First International Conference on Lan-guage Resources and Evaluation, pages 497?501,Granada, Spain, May.
ELDA.D.
Ferre?s, S. Kanaan, A. Ageno, E. Gonza?lez,H.
Rodr?
?guez, M. Surdeanu, and J. Turmo.
2004.The TALP-QA System for Spanish at CLEF 2004:Structural and Hierarchical Relaxing of SemanticConstraints.
In C. Peters, P. Clough, J. Gonzalo,G.
J. F. Jones, M. Kluck, and B. Magnini, editors,CLEF, volume 3491 of Lecture Notes in ComputerScience, pages 557?568.
Springer.D.
Ferre?s, S. Kanaan, E. Gonza?lez, A. Ageno,H.
Rodr?
?guez, M. Surdeanu, and J. Turmo.
2005.TALP-QA System at TREC 2004: Structural andHierarchical Relaxation Over Semantic Constraints.In Proceedings of the Text Retrieval Conference(TREC-2004).C-Y.
Lin and E. Hovy.
2000.
The automated acquisi-tion of topic signatures for text summarization.
InCOLING, pages 495?501.
Morgan Kaufmann.D.
Molla and J.L.
Vicedo.
2005.
AAAI-05 Work-shop on Question Answering in Restricted Domains.AAAI Press.
to appear.EACL 2006 Workshop on Multilingual Question Answering - MLQA0676
