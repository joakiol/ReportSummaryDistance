Empirical Methods for the Study ofDenotation in Nominalizationsin SpanishAina Peris?University of BarcelonaMariona Taule?
?University of BarcelonaHoracio Rodr??guez?
?Technical University of CataloniaThis article deals with deverbal nominalizations in Spanish; concretely, we focus on the deno-tative distinction between event and result nominalizations.
The goals of this work is twofold:first, to detect the most relevant features for this denotative distinction; and, second, to build anautomatic classification system of deverbal nominalizations according to their denotation.
Wehave based our study on theoretical hypotheses dealing with this semantic distinction andwe have analyzed them empirically by means of Machine Learning techniques which are thebasis of the ADN-Classifier.
This is the first tool that aims to automatically classify deverbalnominalizations in event, result, or underspecified denotation types in Spanish.
The ADN-Classifier has helped us to quantitatively evaluate the validity of our claims regarding deverbalnominalizations.
We set up a series of experiments in order to test the ADN-Classifier with differ-ent models and in different realistic scenarios depending on the knowledge resources and naturallanguage processors available.
The ADN-Classifier achieved good results (87.20% accuracy).1.
IntroductionThe last few years have seen an increasing amount of work in the semantic treatmentof unrestricted text, such as Minimal Recursive Semantics in Lingo/LKB (Copestake2007), Frame Semantics in Shalmaneser (Erk and Pado?
2006), Discourse RepresentationStructures in Boxer (Bos 2008), and the automatic learning of Semantic Grammars(Mooney 2007), but we are still a long way from representing the full meaning of textswhen not restricted to narrow domains.
Many Natural Language Processing (NLP)applications such as Question Answering, Information Extraction, Machine Readingand high-quality Machine Translation or Summarization systems, and many NLPintermediate level tasks such as Textual Entailment, Paraphrase Detection, or Word?
CLiC, Centre de Llenguatge i Computacio?/University of Barcelona, Gran Via de les Corts Catalanes 585,08007 Barcelona.
E-mail: aina.peris@ub.edu; mtaule@ub.edu.??
TALP Research Center - Technical University of Catalonia, Jordi Girona Salgado 1-3, 08034 Barcelona.E-mail: horacio@lsi.upc.edu.Submission received: 22 July 2011; revised submission received: 27 December 2011; accepted: 1 February 2012.?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 4Sense Disambiguation (WSD), have almost reached their practical upper bounds andit is difficult to move forward without using a serious semantic representation of thetext under consideration.
Given the limitations and the difficulties in obtaining anin-depth semantic representation of texts as a whole, most efforts have been focusedon partial semantic representation using less expressive semantic formalisms, such asthose that come under the umbrella of Description Logic variants, or on discarding thewhole semantic interpretation task in order to focus on smaller (and easier) subtasks.This is the case, for instance, in Semantic Role Labeling (SRL) systems, which indicatethe semantic relations that hold between a predicate and its associated participantsand properties, the relations of which are drawn from a pre-specified list of possiblesemantic roles for that predicate or class of predicates.
See Ma?rquez et al (2008) andPalmer, Gildea, and Xue (2010) for recent surveys.
Closely related to SRL is the taskof learning Selectional Restrictions for a predicate, for example, the kind of semanticclass each argument of the predicate must belong to (Mechura 2008).
In this casea predefined set of semantic classes must also be used to perform the classificationtask.
WordNet (Fellbaum 1998), VerbNet (Kipper et al 2006), PropBank (Palmer,Kingsbury, and Gildea 2005), FrameNet (Ruppenhofer et al 2006), and OntoNotes(Hovy et al 2006) are resources frequently used for this purpose.
Most of these effortsare verb-centered and reduce role labeling to the roles played by entities around apredicate instantiated as a verb.
At a finer level, there is the task of WSD, for example,assigning the most appropriate sense to each lexical unit of the text from a predefinedlexical-semantic resource.
Once again a catalogue of classes has to be used as a rangefor the assignment.1 In this case as well, despite its excessive finer granularity, WordNetis the most widely used reference.
See Navigli (2009) for a recent survey.In this line of research, there has recently been a growing interest in going beyondverb-centered hypotheses to tackle the computational treatment of deverbal nominal-izations (nouns derived from verbs), in order to move forward to the full compre-hension of texts.
Deverbal nominalizations are lexical units that contain rich semanticinformation equivalent to a clausal structure.
Many recent studies have focused onthe detection of semantic relations between pairs of nominals that belong to differentNominal Phrases (NPs), such as Task 4 of SemEval 2007 (Girju et al 2009) and Task 8of SemEval 2010 (Hendrickx et al 2010), or between nominals taking part in nouncompound constructions.
In the latter case, they take into account a predefined setof semantic relations (Girju et al 2005) or use verb paraphrases with prepositions(Task 9 of SemEval 2010 [Butnariu et al 2010a, 2010b]).
Although these works includenominalizations, they are not strictly focused on them but cover all type of nouns.Actually, most of the work studying only deverbal nominalizations is focused on theirargument structure: Some authors focus on the detection of arguments within the NPheaded by the nominalization (Hull and Gomez 2000; Lapata 2002; Gurevich et al 2006;Pado?, Pennacchiotti, and Sporleder 2008; Surdeanu et al 2008; Gurevich and Waterman2009), whereas others center their attention on detecting the implicit arguments of thenominalizations which are outside the NP (Gerber and Chai 2010; Ruppenhofer et al2010).
Among the former group, there are different approaches to the problem: Lapata(2002) and Gurevich and Waterman (2009) use probabilistic models, Hull and Gomez(2000) and Gurevich et al (2006) develop heuristic rules, Pado?, Pennacchiotti, and1 Some approaches simply discriminate between different senses for a case without assigning it to apredefined specific class, however.
Clustering techniques rather than classification are used in theseapproaches.828Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in NominalizationsSporleder (2008) work with an unsupervised SRL system, and in Surdeanu et al (2008)the work presented uses supervised SRL systems.
The kind of argument annotated isalso different in these works: Although only two, more syntactic labels (subj [subject]and obj [object]), are used to annotate the arguments in Lapata (2002), Gurevich et al(2006), and Gurevich and Waterman (2009), Pado?, Pennacchiotti, and Sporleder (2008)use FrameNet labels and Surdeanu et al (2008) use NomBank (Meyers, Reeves, andMacleod 2004)2 labels.
The interpretation of nominalizations is crucial because they arecommon in texts and an important amount of information is representedwithin them.
Inthe AnCora-ES corpus (Taule?, Mart?
?, and Recasens 2008), for instance, the semantic in-formation is mostly coded in verbs (56,590 verbal occurrences) but a significant numberof deverbal nominalizations (23,431 occurrences) also encode rich semantic information.Most of the work on this topic sets out from the denotative distinction betweennominalizations referring to an event, those that express an action or a process, andnominalizations referring to a result, those expressing the outcome of an action orprocess.
From a theoretical point of view, it is stated that this denotative distinctionmay have repercussions on the argument-taking capability of deverbal nominaliza-tions.
Despite being aware of this distinction, computational approaches focus on eventnominalizations, not taking into account the result ones or, more frequently, withoutcharacterizing the difference.
For instance, SRL systems are mostly applied to eventnominalizations (Pradhan et al 2004; Erk and Pado?
2006; Liu and Ng 2007).
Resultnominalizations are more frequent than the event types, however, at least in Spanish(1,845 event occurrences in contrast to 20,037 result occurrences in AnCora-ES).
In thepresent work, we hypothesize that result nominalizations, like event nominalizations,can take arguments; therefore, discarding result nominalizations would imply a loss ofsemantic information, equally relevant to text representation.
In this article, we focusour interest on this denotative distinction.
Concretely, we aim to determine the relevantlinguistic information required to classify deverbal nominalizations as event or resulttypes in Spanish.
In order to achieve this goal, we have built an automatic classifier ofdeverbal nominalizations?the ADN-Classifier?for Spanish, aimed at identifying thesemantic denotation of these nominal predicates (Peris et al 2010).
The ADN-Classifieris a tool that takes into account different levels of linguistic information depending onits availability, such as senses, lemmas, or syntactic and semantic information codedin the verbal and nominal lexicons (AnCora-Verb [Aparicio, Taule?, and Mart??
2008]and AnCora-Nom [Peris and Taule?
2011]) or in the AnCora-ES corpus.Therefore, this article contributes to the semantic analysis of texts focusing onSpanish deverbal nominalizations, although the proposal presented could be extendedto other Romance languages.
We base our study on theoretical hypotheses that weanalyze empirically, and as a result we have developed three new resources: 1) theADN-Classifier, the first tool that allows for the automatic classification of deverbalnouns as event or result nominalizations; 2) the AnCora-ES corpus enriched withthe annotation of deverbal nominalizations according to their semantic denotation,the only Spanish corpus that incorporates this information; and 3) AnCora-Nom, alexicon of deverbal nominalizations containing information about denotation typesand argument structure.The ADN-Classifier can be used independently in NLP tasks, such as Corefer-ence Resolution and Paraphrase Detection (Recasens and Vila 2010).
For Coreference2 In the work of Hull and Gomez (2000) it is not stated explicitly which set of arguments are used, althoughfrom their examples we infer that they are semantic roles such as those of VerbNet.829Computational Linguistics Volume 38, Number 4Resolution tasks it would be useful to have the nominalizations classified into denota-tions in order to detect coreference types.
For instance, if a nominalization has a verbalantecedent (anchor) and its denotation is of the event type, an identity coreference rela-tion could be established between them (Example (1)).
If the nominalization is of theresult type, however, the relation established between verb and noun would be a bridg-ing coreference relation (Example (2)) (Clark 1975; Recasens, Mart?
?, and Taule?
2007).
(1) En Francia los precios cayeron un 0,1% en septiembre.
La ca?
?da<event> haprovocado que la inflacio?n quedara en el 2,2%.
?In France prices fell by 0.1 % in September.
The fall<event> causedinflation to remain at 2.2 %.?
(2) La imprenta se invento?
en 1440.
El invento<result>permitio?
difundir lasideas y conocimientos con eficacia.
?The printing press was invented in 1440.
This invention<result> allowedfor ideas and knowledge to be spread efficiently.
?As for Paraphrase Detection (Androutsopoulos and Malakasiotis 2010; Madnani andDorr 2010), event nouns (but not result nouns) are paraphrases for full sentences, sothis type of information can also be useful for this task.
For instance, the sentence inExample (3) and the NP headed by an event nominalization in Example (4) are typicallyconsidered to be paraphrases.
(3) Se ha ampliado el capital de la empresa en un 20%.
?The company?s capital has been increased by 20%.?
(4) La ampliacio?n<event>del capital de la empresa en un 20%.
?The increase<event> of company?s capital by 20%.?
(5) Se han vendido muchas traducciones<result>de su u?ltimo libro.
?Many translations<result> of his latest book have been sold.?
(6) Se han vendido muchos libros traducidos de su u?ltimo t??tulo.
?Many translated editions of his latest book have been sold.
?If the nominalization, however, has a result interpretation as in Example (5)?traducciones, ?translations?
refers to the concrete object, that is, the book translated?it is impossible to have a paraphrase with a clausal structure.
This is due to the factthat result nominalizations can denote an object whereas verbs cannot denote objects.In fact, result nominalizations can only be paraphrases of other NPs denoting objects(Example (6)).The AnCora-ES corpus enriched with denotative information could be used astraining and test data for WSD systems.
The work presented in this article also providesan additional insight into the linguistic question underlying it: the characterization ofdeverbal nominalizations according to their denotation and the identification of themost useful criteria for distinguishing between these denotation types.The remainder of this article is organized as follows.
Section 2 summarizes thetheoretical approaches to the semantic denotation with which we deal here.
Section 3describes the methodology used in this work.
Section 4 presents the empirical linguisticstudy in which the initial model is established; in Section 5 the different knowledgeresources used are presented, paying special attention to the nominal lexicon, AnCora-Nom.
In Section 6, the ADN-Classifier is presented and in Section 7 the different830Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in Nominalizationsexperiments conducted are described and their results are reported.
Section 8 reviewsrelated work and, finally, our conclusions are drawn in Section 9.2.
Theoretical BackgroundIn the linguistics literature related to nominalizations, one of the most studied andcontroversial topics is the denotative distinction between event and result nominaliza-tions.
By event nominalization we mean those nouns that denote an action or processin the same way that a verb does.
In other words, as their verbal counterparts, eventnominalizations have the aspectual property of dynamicity (Example (7)).
In contrast, aresult nominalization refers to the state (Example (9)) or the concrete or abstract objectresulting from the event (Example (8)).
Both types of result nominalizations (states andobjects) lack the aspectual property of dynamicity.
(7) El proyecto americano consiste en la adaptacio?n<event>de la novelaPaper Boy.
?The American project is the adaptation< event> of the Paper Boy novel.?
(8) Esta adaptacio?n<result> cinematogra?fica ha recibido buenas cr??ticas.
?This film adaptation<result> has received good reviews.?
(9) Reforzo?
la tendencia<result> al alza del Euro de los u?ltimos d??as.
?The upward trend<result> of the euro has been reinforced in recent days.
?In Example (7), the noun adaptacio?n (?adaptation?)
denotes an event because it expressesan action in the same way that a verb does (it is equivalent to El proyecto americano con-siste en adaptar la novela Paper Boy, ?The American project consists of adapting the PaperBoy novel?).
The event interpretation is characterized as dynamic because it implies achange from ?not being adapted?
to ?being adapted.?
In contrast, in Example (8) the samenominalization is understood as a result because it denotes a specific object that is theoutcome of the action of adapting a creative work into a film.
In Example (9), the resultinterpretation is due to the fact that the verb base of tendencia, ?trend,?
denotes a state,so the noun inherits the property of stativity (non-dynamicity) and does not imply anychange.In this sense, our notions of event and result are equivalent to the complex-eventand result nominalizations, respectively, in the terminology of Grimshaw (1990)3 or theterms process and result used in Pustejovsky (1995)4 and Alexiadou (2001).
Althoughthe event vs. result distinction we make is widespread, it is true that event and resultnominalizations can also be represented in a more fine-grained way.
For instance,Eberle, Faasz, and Heid (2009) distinguish between events (messung, ?measurement?
),states (teilung, ?division?
), and objects (lieferung, ?furnished material?)
in Germannominalizations, and Balvet, Barque, and Mar?
?n (2010) propose a typology of Frenchnominalizations that contemplates four aspectual types: states (admiration, ?admiration?
),durative events (ope?ration, ?operation?
), punctual events (explosion, ?explosion?
), and objects(ba?timent, ?building?).
For English, Levi (1978) identifies four types of nominalizations:actions (parental refusal), which are equivalent to the event notion; agents ( financial3 She distinguishes a third denotative type, simple event nouns like trip in That trip took two weeks, but wediscard them because, although expressing an event, they are not derived from verbs; in this researchwe focus on deverbal nominalizations.4 This author characterizes nominalizations as dot-objects that include both process and resultmeanings.831Computational Linguistics Volume 38, Number 4analyst), which denote the agent of an action and are characterized by using a differentset of suffixes; products (human error), which denote the result of an action; and patients(student?s inventions), which denote the patient object of the action.
Also for English,Nunes (1993) defines five types of nominalizations: process nouns, that name the actionor process denoted by the base verb (The documents?
destruction by the North); resultnouns, that denote a new creation resulting from the base verb (The invention was put ondisplay); accumulated-action nouns, that name the sum total of a verb activity (The attackwas unexpected); experiential-state nouns, nominalized stative verbs or nominalizations ofa state brought about by a particular verb (Sam?s interest in maths); and experiential-stateresults, the result counterpart of the previous class (Sam has many interests).The authors working on this topic mainly differ on two issues.
On the onehand, they do not agree on how to consider (and therefore, how to represent) thesetwo denotations: as two senses of the same lexical unit (Pustejovsky 1995; Badia2002; Alonso 2004) or as two different lexical units (Grimshaw 1990; Picallo 1999;Alexiadou 2001).
Regarding this denotative distinction, several linguistic criteria havebeen proposed in order to identify each of these denotations, mostly for English,although there are some proposals for Spanish (Picallo 1999), French, Greek, Russian,and Polish (Alexiadou 2001) (see Table 2 in Section 4.1).
On the other hand, authorsdiffer on the argument-taking capacity of deverbal nominalizations: Some linguistsmaintain that only event deverbal nominalizations can take arguments (Zubizarreta1987; Grimshaw 1990), whereas others consider that both event and result nominaliza-tions can take arguments (Pustejovsky 1995; Picallo 1999; Alexiadou 2001).Authors who conceptualize event and result nominalizations as different lexicalunits justify this in different ways.
Grimshaw (1990) considers that only complex-eventnominals legitimate an argument structure, and that constitutes the main differencewith respect to result nominalizations which, according to her, lack argument structure.In Alexiadou (2001) and Picallo (1999), the idea that event and result nominalizationsare different lexical units is justified by the different functional projection of these twotypes of nominalization and by word-formation at different levels of the language,5respectively.
In contrast to Grimshaw, however, they state that both types of nominal-izations can select arguments.
In the words of Alexiadou (2001, page 69): ?Given thatthere is no lexical difference between verbs and process nouns, and between result andprocess nouns, apart from the functional domain, all can take arguments.?
Picallo alsobelieves that complements of result nominalizations are arguments when argumentalrelationships can be established between them and the nominal head.In contrast, those who consider both denotations as senses of the same lexical unitmaintain that nominalizations are underspecified lexical units (Pustejovsky 1995), unitsin which a disjunction of meaning is present (Alonso 2004), or simply lexical units withdifferent senses (Badia 2002).
Specifically, Pustejovsky accounts for the event-result ambi-guity in nominalizations bymeans of an underspecified lexical representation called dot-object, arguing that event-result nominalizations are cases of complementary polysemy:?both senses of a logically polysemous noun seem relevant for the interpretation of thenoun in the context, but one sense seems ?focused?
for purposes of a particular context?
(Pustejovsky 1995, page 31).
In relation to argument-taking capacity, both types ofnominalizations are argumental because the dot-object representation has an argumentstructure in which the nominal arguments are specified.5 In Picallo (1999) it is stated that event nominalizations are created in the syntax whereas resultnominalizations are created in the lexicon; therefore they have different derivation processes.832Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in NominalizationsAlonso (2004) argues that these nominalizations present a disjunction of meaningbecause they can update an event and a result reading in the same sentence without theunderstanding of the sentence being affected.
For instance, declaracio?n (?declaration?)
inExample (10)6 can be interpreted as an event and as a result at the same time: Only eventnominalizations can be said to have an initial moment and only result nominalizationscan be said to have five pages.
These two senses both originate in the same lexical unit,which includes both, and the context provides both meanings.
(10) La declaracio?n<event/result> que el juez tomo?
al testigo, que comenzo?
alas once, ocupa cinco folios.
?The statement<event/result> that the judge took from the witness, whichbegan at eleven, takes up five pages.
?Regarding argument-taking capacity, Alonso maintains that all nominalizations takingpart in a support-verb construction can select arguments.
So, if a result nominalizationis part of a support-verb construction, it will also have argument structure.
Followingthese authors, we also consider that result nominalizations can take arguments.Within a computational framework, there are different models that represent nom-inalizations; not all of them take into account the event and result distinction, however.For instance, in NomBank (Meyers, Reeves, and Macleod 2004) this distinction is com-pletely ignored and the authors focus on argument structure.
In contrast, Spencer andZaretskaya (1999) label each nominal sense with one of the Grimshaw semantic cate-gories (i.e., complex event, simple event, and result).
Their database contains informationabout 7,000 Russian verbs and their 5,000 corresponding nominalizations, distinguish-ing between the verbal entries that nominalize the whole event while preserving theverbal argument structure, and those that denote a concrete or abstract result of theverb.
The Nomage project (Balvet, Barque, and Mar?
?n 2010) annotates French deverbalnominalizations in the French TreeBank (Abeille?, Cle?ment, and Kinyon 2000) with oneof the four classes proposed in their work (i.e., states, durative events, punctual events, andobjects).
In the middle ground between these two positions, we find the representationmodels proposed in WordNet (Fellbaum 1998), FrameNet (Baker, Fillmore, and Lowe1998; Ruppenhofer et al 2006), and Ontonotes (Hovy et al 2006).
WordNet, possiblydue to its extremely fine granularity, usually includes among the senses correspondingto deverbal nouns one paraphrased as ?the acting of verb x?
(our event nominalization)and another paraphrased to something similar to ?the thing X-verb+ed?
(our resultnominalization).
FrameNet distinguishes between deverbal nominalizations definedas the action or process of X verb and nominalizations defined as entities.
Concerningnouns, Ontonotes is interested in the disambiguation of noun senses in order to createan ontology.
Within deverbal nouns the authors distinguish between nominalizationsenses that truly inherit the verb meaning and deverbal noun senses whose denotationis not directly related to the verb meaning.7 That is, they distinguish between twodifferent types of deverbal nouns but this distinction is not akin to the event-result one.The distinctions established in WordNet and FrameNet are more similar to the oneproposed in OntoNotes.To sum up, the event vs. result distinction in deverbal nominalizations has receivedmuch attention in linguistics literature.
It seems to be less relevant in the computational6 This example is taken from Alonso (2004).7 For instance, building in The building was made mostly of concrete and glass.833Computational Linguistics Volume 38, Number 4framework, in contrast, although some computational models do represent a semanticdistinction that is similar to the one we are analyzing (see Section 8).3.
MethodologyThe aim of the current work is twofold: first, to detect the most relevant features for thedenotative distinction between event and result nominalizations; and, second, to buildFigure 1Scheme of the methodology followed.834Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in Nominalizationsan automatic classification system for deverbal nominalizations according to their de-notation.
In Figure 1 the overall methodology for carrying out this work is schematized.In order to determine the most relevant features, the first step was to carry out alinguistic study of deverbal nominalizations (see Section 4).
This study consisted of theapplication of the linguistic criteria stated in the literature to a reduced set of nominal-izations corresponding to the occurrences extracted from a 100,000 word (henceforth100kw) subset of the AnCora-ES corpus.
As an outcome, we manually obtained (seestep 1 in Figure 1) a lexicon for this deverbal nominalization set, AnCora-Nom-R1 (Peris,Taule?, and Rodr?
?guez 2009), which allowed us to annotate the corresponding deverbaloccurrences in the corpus subset.
The nominalization classifying model (see step 2 inFigure 1) underlying these two initial resources was tested by empirical methods.
Thisfirst model of classification is based on sense distinctions?that is, the extraction offeatures was performed at sense level and the examples for learning (both positive andnegative) corresponded to different senses in the lexicon; thus, we will refer to it as thesense-based model.
We set up a series of experiments based on Machine Learning (ML)techniques in order to evaluate the consistency of the data annotated in AnCora-Nom-R1, to analyze the relevance of the attributes used in the lexical-semantic representationand to infer new useful features to distinguish between event and result nominalizations.As well as experimenting with AnCora-Nom-R1 simple features, we experimented withthe binarization8 and grouping9 of several of them to address sparseness problems.Furthermore, with these experiments the foundations of the automatic classificationsystem, the ADN-Classifier-R1, were laid (see step 3 in Figure 1).
See Section 6 for adescription of the ADN-Classifier.Once the consistency of the annotated data was corroborated and the most relevantfeatures were established, we focused on the building of the ADN-Classifier, the secondgoal of the research presented in this article.
In order to develop the final version ofthe ADN-Classifier (R3) we increased the corpus data used for learning.
Therefore, weneeded to annotate the denotation types in the whole AnCora-ES.
Because this involvesan increase in the number of nominalization occurrences to annotate (23,431 occurrencesin contrast to 3,077), we carried out this annotation automatically.
To do so, wemodifiedthe (sense-based) model of classification underlying the ADN-Classifier-R1 (see step 2in Figure 1) creating a new model able to carry out the more realistic task of classifyingthe occurrences in the whole AnCora-ES corpus.This new model (see step 5 in Figure 1) uses the following as knowledge resources:1) the AnCora-Verb lexicon to obtain the features from the verbs related to nominaliza-tions; 2) the complete AnCora-ES corpus (500kw); and 3) AnCora-Nom-R2, an extendedlexicon of nominalizations without denotation types obtained automatically (see step 4in Figure 1).
This lexicon contains a total of 1,655 lexical entries, corresponding to the1,655 nominalization types in the whole AnCora-ES.
Because we annotate the occur-rences in the AnCora-ES corpus, however, we reduce our dependence on the lexicalsource AnCora-Nom-R2 (see Section 5.3), removing the sense-specific information fromit and taking into account only the information shared by all the senses of one lemma,while maintaining the features extracted from the corpus.
In this sense, we adapted thesense-based model developed in ADN-Classifier-R1 (see step 2 in Figure 1) to obtaina new classification model that is based on lemmas (and not senses) (see step 5 in8 Binarizing a k-value categorial feature means transforming it into k binary features.9 Combining several simple features into a complex feature using a combination (for instance,a logical OR).835Computational Linguistics Volume 38, Number 4Figure 1).
This new model was used for the automatic annotation of the AnCora-EScorpus with denotation information (see step 7 in Figure 1).
Afterwards, in order toevaluate the performance of the developed model, the corpus annotation was manuallyvalidated (Peris, Taule?, and Rodr?
?guez 2010) (see step 8 in Figure 1).
Thismanual processalso guarantees the quality of the corpus annotation, leading to the final version of theAnCora-Nom lexicon (R3) containing denotation type information.At that moment, we were able to build the final version of the ADN-Classifier (R3)(see step 10 in Figure 1).
In order to do so, we set up a series of experiments leading tothe development of new sense- and lemma-based models using the resources alreadybuilt (AnCora-ES with denotation information and AnCora-Nom-R3), that is, modelslearned with more instances.
We also replicated the experiments at sense and lemmalevel with the subset of 100kw from the already enriched AnCora-ES and the subsetof 817 lexical entries from the AnCora-Nom-R3.
Specifically, we carried out a set ofnew ML experiments using the simple and binarized features from the nominal andverbal lexicons as well as additional features obtained from the AnCora-ES corpus (theso-called contextual features).
For the evaluation of the different sense- and lemma-based models derived from this set of experiments (see step 9 in Figure 1), tenfoldcross-validation was used with the pre-created resources.
These models give rise to thefinal version of the ADN-Classifier (R3) (see step 10 in Figure 1).
See Section 6 for adescription of the ADN-Classifier.4.
Previous Linguistic StudyThe aim of the corpus-based linguistic study conducted was twofold.
First, we wantedto determine which of the criteria stated in the literature were the most relevant linguis-tic features to establish the distinction between event and result denotations in Spanish.Secondly, we wanted to find other features that could be used to reinforce the semanticdistinction we are dealing with.In order to do this, we selected a sample of 817 Spanish deverbal nominalizationscorresponding to 3,077 occurrences.
These nominalizations were obtained semiauto-matically from a 100kw subset of the AnCora-ES corpus.
In this selection we took intoaccount a predefined list of ten suffixes (-a, -aje, -io?n/-cio?n/-sio?n/-o?n, -da/-do, -dura/-ura, -e,-ido, -miento/-mento, -ncia/-nza, -o/-eo?
[Santiago and Bustos 1999]) that contribute to anevent or resultmeaning and which take verbs as a basis for the derivation process.This sample corresponds to the original 3LB corpus (Civit and Mart??
2004), that canbe considered to be a preliminary version of AnCora-ES.
The set of 817 nominalizationsconsists of those occurrences in this sub-corpus.
Despite coming from different sources,the 100kw corpus and the full 500kw corpus are comparable as is shown in Table 1.In Table 1 we present some metrics for describing the whole AnCora-ES corpusand its 100kw subset.
We present the metrics we have used in three rows: degreeTable 1Descriptive content of AnCora-ES and its 100Kw subset.
In each cell the values corresponding tothe subset and the whole corpus are present.Min Max Mean Standard Deviationsense/lemma 1/1 13/13 2.19/1.86 1.54/1.31examples/lemma 1/1 239/255 19.99/14.15 30.76/26.44length sentences 4/4 149/149 39.14/39.51 10.69/12.08836Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in Nominalizationsof polysemy (number of senses per lemma), the number of examples (sentences) perlemma in the corpus, and the average length of sentences per lemma.
We depict theminimum and maximum values, the mean and the standard deviation for each metric.The values in the figure seem reasonable.
The only anomalous figures correspond tothe extremely high values of the standard deviation of the number of examples metric.It is due to the highly biased shape of the curve towards small values.
In fact, mostof the lemmas have only one example (121 for the 100kw sample) and the number oflemmas having values over the mean are very few.
As can be seen in Table 1, thereare no notable differences in the values corresponding to the whole set and the subset.Additionally we computed the ratio of lemmas containing only one example.4.1 Analyzing Features from the LiteratureIn order to determine which criteria stated in the literature were the most relevant, weselected seven criteria that satisfy two conditions: first, they are some of the most widelyused by other authors, and second, it is possible to search for them in the AnCora-EScorpus without suffering data sparseness.
These criteria and the authors who proposethem are shown schematically in Table 2.The seven criteria were analyzed by contrasting them with the behavior of the 817Spanish deverbal nominalizations in the AnCora-ES corpus.
Concretely, two graduatestudents in linguistics classified the 3,077 occurrences independently into event or resultclass.
After this first annotation, we discussed the results with them and reached anagreement in those cases in which the denotation type assigned was not the same.
Itshould be noted that the aim of this analysis was to encourage reflection on the suit-ability of these seven criteria and on finding new clues to interpret the nominalizationssemantically.During the classification procedure, we observed that these two denotations didnot allow us to account for all the data in the corpus.
On the one hand, it is notalways possible to distinguish between event and result, because the linguistic context(the sentence) is not always informative enough.
We label such cases underspecifiedtypes, resulting finally in three possible denotation values.
On the other hand, wenoticed that some nominalizations can take part in a lexicalized construction, thus,we added the attribute <lexicalized>.
In such cases, we distinguish between six typesof lexicalization according to their similarity to different word classes: nominal (e.g.,s?
?ndrome de abstinencia, ?withdrawal symptoms?
), verbal (e.g., estar de acuerdo, ?to be inagreement?
), adjectival (e.g., al alza, ?rising?
), adverbial (e.g., con cuidado, ?with care?
),prepositional (e.g., en busca de, ?in search of?
), and conjunctive (e.g., en la medida que,Table 2Linguistic criteria for distinguishing between result and event nominalizations from differentauthors.Criteria Grimshaw?90 Alexiadou?01 Picallo?99 Alonso?04 Badia?02Verbal Class - + + - +Pluralization + - - + -Determiner Type + - + + -Preposition+Agent - - + - +Internal Argument + - + - -External Arguments + - - - -Verbal Predicates + - + - +837Computational Linguistics Volume 38, Number 4?as far as?).
One of the three denotation values?event, result, underspecified?is assignedto the whole lexicalized construction only in the case of nominal lexicalizations.
It isimportant to recognize such lexicalized cases because if the nominalization takes partin a lexicalized construction other than the nominal, it does not receive a denotation(a semantic distinction that is only associated with nouns).The 3,077 occurrences were classified into 1,121 senses considering that differentdenotations associated with a lemma are different senses.
Henceforth, we refer to themas nominalization senses.
Among these 1,121 senses, 807 were annotated as result (72%),113 as event (10%), 131 as underspecified (12%), and 70 as non-nominal lexicalized noun(6%).
It is not surprising that result nominalizations are the most frequent because eventstend to be realized mostly by verbal clauses and nominalizations are more frequentlyused for the result (non-dynamic) meaning, more typical of nouns.The fact that AnCora-ES is annotated with different linguistic information levelsallowed for the evaluation of the seven morphosyntactic and semantic criteria se-lected.
Next, we briefly present each criterion, how they were applied, and the resultsobtained.104.1.1 Verbal Class.One of the most commonly used criterion to determine the denotationis the verbal class from which the nominalization is derived (Picallo 1999; Alexiadou2001; Badia 2002).
It is claimed that unergative and stative verbs give rise to resultnominalizations, and unaccusative verbs usually result in ambiguous, or what we callunderspecified, nominalizations.
Regarding transitive verbs, they give rise to either event,result, or underspecified nominalizations.
To analyze this criterion, we set out from thesemantic verbal classification proposed in AnCora-Verb.
In this verbal lexicon, eachpredicate is associated with one or more semantic classes depending on the four basictypes of events (Vendler 1967) (accomplishments which correspond to transitive verbs;achievements corresponding to unaccusative verbs; states corresponding to stativeverbs; and activities corresponding to unergative verbs) and on the diathesis alterna-tions in which the verb can occur.
We therefore looked up the verbal classes from whichthe 817 nominalizations are derived in AnCora-Verb.
This allowed us to determinewhether the claims about the relation between the nominalization denotation type andthe corresponding verbal classes are valid.In the sample analyzed, most of the nominal senses were classified as results(72%), thus, it should not surprise us that all the verbal classes have a wide per-centage of result nominalizations.
The most significant result, however, is that stativeand unergative verbs lead nearly exclusively to result nominalizations in Spanish,97% and 100%, respectively.
Regarding transitive and unaccusative verbs, they lead toevent, result, or underspecified nominalizations.
It is also interesting to remark that eventnominalizations derive mostly from transitive verbs (15%, in contrast to the 1% derivedfrom achievement verbs) and underspecified nominalizations derive from unaccusativeverbs (28%, in contrast to the 11% and 3% derived from transitive and state verbs,respectively), confirming the hypothesis stated by Picallo (1999), Alexiadou (2001), andBadia (2002).4.1.2 Pluralization.
According to Grimshaw (1990) and Alonso (2004), one of the featuresthat clearly identifies result nominalizations is their pluralization capacity because it is10 In the following criteria we do not consider lexicalized senses because the criteria do not apply to thesecomplex lexical units.838Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in Nominalizationsmore usual to quantify objects than events.
It is important to point out, however, thatit is also possible to make an event reading of a plural nominalization.
For instance, inExample (11) bombardeos (?shelling?)
refers to multiple actions of bombing, therefore, itis open to an event reading.
(11) Los bombardeos<event>de Sarajevo por parte del eje?rcito Bosnio.
?The shelling<event> of Sarajevo by the Bosnian army.
?This criterion was measured taking into account whether the 817 nominalizationsappeared in the plural in some of their occurrences in the sample analyzed.
The resultsobtained (98% of the nominalizations in the plural were classified as result and theremaining 2% as underspecified) confirmed plurality as one of the features able to detectresult nominalizations.
In contrast, the singular feature is not informative enough todiscard any of the nominal denotations (69% of the nominalizations were classified asresults, 15% as events, and 16% as underspecified type).
Therefore, in the sample analyzedall event nominalizations and most of the underspecified nominalizations appeared onlyin the singular.4.1.3 Determiner Types.
Authors such as Grimshaw (1990), Picallo (1999), and Alonso(2004) claim that event nominalizations usually appear with definite articles whereasresult nominalizations may be specified by all types of determiners.
For instance,demonstrative determiners can only specify result nominalizations because this typeof determiner is used to refer to an entity in a frame of reference.
In order to evaluatethis criterion we took into account the types of determiners combining with the nomi-nalization and also whether the noun appeared without a determiner.Because most of the nominal senses were classified as results (72%), it should notsurprise us that all types of determiners have a wide percentage of result nominaliza-tions.
A striking result observed in Table 3, however, is that indefinite articles (99%),demonstratives (100%), and quantifiers (100%) nearly always appear with result senses.In contrast, the definite article, the possessive, and the empty position can occur in allnominalization classes.
Seventy-two percent of definite articles appear with result, 13%with event, and 15% with underspecified nominalizations; 82% of possessive determinersappear with result, 10% with events, and 8% with underspecified nominalizations; and88% of nominalizations without determiner are classified as result, 5% as events, and 7%as underspecified nominalizations.
The data therefore partially confirm the hypothesesfrom the literature: Result nominalizations appear with a wider range of determiners.Although event nominalizations are not always specified by a definite article, they canalso appear with possessive determiners or without any determiner.4.1.4 Preposition Introducing the Agent.
In Spanish nominalizations derived from tran-sitive verbs are considered to be results if the agent complement is introduced by thepreposition de (?of?)
and events if the preposition used is por (?by?)
(Picallo 1999).11 Wetook into consideration agent complements introduced by prepositions appearing inthe sample analyzed.
As shown in Table 3, Prepositional Phrases (PPs) interpreted asagents in the NPs analyzed are introduced by the following prepositions: de (?of?
), entre(?between?
), por (?by?
), and por parte de (?by?).
We observed that the distribution of thefour PPs is complementary between event and result denotations: The agent nominal11 A similar claim is stated by Badia (2002) for Catalan.839Computational Linguistics Volume 38, Number 4Table 3Distribution of the denotation types according to the criteria evaluated.Criteria Values Result (%) Event (%) Underspecified (%)Verbal ClassAccomplishments 74 15 11Achievements 71 1 28States 97 ?
3Activities 100 ?
?PluralizationPlural 98 ?
2Singular 69 15 16DeterminersDefinite 72 13 15Indefinite 99 ?
1Demonstrative 100 ?
?Possessive 82 10 8Quantifier 100 ?
?No Determiner 88 5 7Preposition-Agentde ?of?
98 ?
2entre ?between?
100 ?
?por ?by?
?
100 ?por parte de ?by?
?
100 ?Internal argumentPossessive 41 38 21PPs 53 25 22Relative Pronoun 71 29 ?Relational Adjectives 97 ?
3External argumentpor ?by?
PPs ?
100 ?Relational Adjectives 100 ?
?Possessive 95 ?
5PredicatesAttributive 75 6 18Eventive 44 41 15complement introduced by de or entre occurs with result nominalizations (98% and100%, respectively) and the agent nominal complement introduced by por or por partede occurs with event nominalizations (100% both), corroborating the hypothesis putforward by Picallo (1999).4.1.5 Internal Argument.
The internal argument criterion proposed by Grimshaw (1990)and Picallo (1999) states that only event deverbal nominalizations require the presenceof an internal argument because they are more similar to verbs, whereas in result-nominalized NPs the internal argument is not needed.
Badia (2002) argues that the real-ization of this argument is not always compulsory to obtain an event interpretation of thenominalization.
To analyze this criterion, we observed those nominalized NPs in whichthe internal argument was explicit and the type of argument that realized it.
As a result,we observed that the majority of event nominalizations are complemented by an inter-nal argument (98%).
This is also the case for underspecified nominalizations in a fairlyhigh percentage (78%).
The percentage decreases considerably in result nominalizations(34%), however.
Therefore, the data seem to confirm Picallo?s and Grimshaw?s hypoth-esis.
Table 3 shows that there are four constituents that realize an internal argument:possessive determiners, PPs, genitive relative pronouns, and relational adjectives.
The840Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in Nominalizationsfirst two constituents appear in the three nominal denotation types: 41% of possessivedeterminers appear with result nominalizations, 38% with events nominalizations, and21% with underspecified nominalizations; and 53% of PPs complement result, 25% events,and 22% underspecified nominalizations.
Relative pronouns only occur with event (29%)and result (71%) denotation types, and, finally relational adjectives occur nearly exclu-sively in result nominalizations (97%).
This last fact constitutes an identification featurefor result nominalizations, as Picallo states (see next criterion).Table 3 shows the results obtained for each criterion.4.1.6 External Arguments vs. Possessors.
Grimshaw (1990) states that PPs introducedby the preposition ?by?
(by-PPs), relational adjectives, and possessive determiners inEnglish would be interpreted as external arguments (subjects) in the case of eventnominalizedNPs.
These constituents, however, would be interpreted as possessors (thatis, as non-argumental) in result nominalized NPs.
Other authors, like Picallo (1999),nevertheless, claim that possessive determiners may be argumental in result and eventnominalizations in Spanish.
Regarding relational adjectives, Picallo argues that theseconstituents can only be arguments of result nominalizations.
As seen, there is noconsensus among authors; therefore, it seemed to us to be an interesting criterion tocontrast.
We observed whether these constituents (by-PPs, relational adjectives, andpossessive determiners) were interpreted as external arguments in the nominalized NPsample.
If this was so, we also analyzed whether the fact of being external argumentsconditioned the denotation of the nominalization.12The results obtained are very clear: PPs introduced by por (?by?)
with an agentinterpretation only occur in NPs headed by event nominalizations.
Relational adjectivesare interpreted as external arguments only in NPs headed by result nominalizations.Possessive determiners with an agent interpretation are mostly (95%) constituents ofNPs headed by result nominalizations though they can also be constituents (5%) ofNPs headed by underspecified nominalizations.
Therefore, for Spanish, Grimshaw?s hy-pothesis is only partially corroborated because only by-PPs guarantee the event read-ing.
Regarding relational adjectives, Picallo?s thesis is confirmed, because this type ofconstituent mostly appears as an argument of result nominalizations.
Moreover, weobserved a preference for possessive determiners to be external arguments of resultnominalizations, which is not stated in any of the theoretical proposals.4.1.7 Verbal Predicates.
The type of verbal predicate that can be combined with nominal-izations may be an indicator for determining the denotation (Grimshaw 1990; Picallo1999; Badia 2002).
Result nominalizations tend to combine with attributive predicates,whereas event nominalizations tend to be subjects of predicates such as tener lugar (?totake place?)
or ocurrir (?to happen?)
because these predicates tend to select event typesubjects.
In order to examine this criterion, we analyzed the types of predicates com-bined with the 817 nominalization types.
We observed whether the predicates belongto the event-denoting class (tener lugar, ?to take place?
; ocurrir, ?to happen?
; comenzar,?to begin?
; acabar, ?to finish?
; durar, ?to last?
; llevar a cabo, ?to carry out?)
or if they wereattributive predicates (ser, ?to be?
; estar, ?to be?
; parecer, ?to seem?
).
Table 3 illustrates that12 The way we decided whether these constituents were external arguments consisted of paraphrasingthe nominalized NPs into clause structures in order to see if they were semantically equivalent toverbal subjects.841Computational Linguistics Volume 38, Number 4attributive predicates tend to choose result nominalizations (75%) as subjects whereaseventive predicates do not show a clear preference for any type of nominal: Forty-four percent of them combine with result, 41% with events, and 15% with underspecifiednominalizations.
These results partially confirmedwhat is stated by these three authors:result nominals combine preferentially with attributive predicates.From the corpus-based study, we conclude that the semantic distinction betweenevent and result nominalizations is not always as clear as is stated in the literature.
Thecriteria proposed in the literature are well suited to constructed examples but whenthey are applied to naturally occurring data they do not work so well: Some of themcannot be applied and sometimes we found contradictory criteria in the same example.That said, it is important to point out that these criteria are not irrefutable proofs formaking an event or a result reading, but rather indicators that can help us to strengthenour semantic intuition.
In fact, we propose the third denotation type underspecified forthose cases in which semantic intuition is insufficient and the criteria for reinforcing oneof the two main denotation types are not clear.Regarding the criteria established in the literature, the main conclusion drawnis that not all the criteria analyzed seem to hold for Spanish.
Among the evaluatedcriteria, those that appear to be most helpful for distinguishing between event and resultnominalizations are: 1) the semantic class of the verb from which the noun is derived;2) the pluralization capacity; 3) the determiner types; 4) the preposition introducing anagentive complement; and 5) the obligatory presence of an internal argument.
Thesefeatures are represented as attributes in the nominal lexical entries of the AnCora-Nomlexicon (see Section 5.3).It is interesting to note that the number of criteria found that reinforce result read-ings is significantly higher than the number of criteria found that strengthen eventreadings.
In every criterion we find features that support the identification of resultnominalizations but not event nominalizations.
To support result nominalizations thefollowing features were found: nominalizations deriving from unergative and stativeverbs; nominalizations appearing in the plural; nominalizations specified by an indeter-minate article, a demonstrative, or quantifier determiner; nominalizations with an agentcomplement introduced by de (?of?)
or entre (?between?
); the nonrealization of the inter-nal argument; and nominalizations having relational adjectives as arguments and theattributive predicate combined with them.
In order to underpin event nominalizations,however, the only unambiguous criterion found was when the preposition introducinga PP agent complement is por (?by?)
or por parte de (?by?
).13 If we take into account thatthe agent complement is mostly optional in an NP configuration, it is very difficult tofind a criterion within the NP context to support event nominalizations.We believe that there are more features to support result nominalizations becausethey are closer to non-derived nouns and, like them, admit a wide variety of config-urations: plural inflection, different types of determiners, the possibility of appearingwithout complements, and so forth.
In contrast, event nominalizations (since they arenot typical nouns because they denote an action), like verbs, do not admit this varietyof configurations: They rarely appear without complements, admit fewer types of deter-miners, and appear in the plural less frequently.
Most of the configurations they admitare also admitted by result nominalizations; this explains why there are more criteriato support result than event nominalizations.13 Literally, ?on the part of.
?842Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in NominalizationsIn fact, the remaining criteria?the fact of deriving from transitive or unaccusativeverbs; the nominalization being in the singular; the co-occurrence with a definite ar-ticle, a possessive determiner, or without any determiner; the presence of the internalargument; and the combination with typically eventive predicates?do not support anyspecific denotation.
As a result, there are several cases where it is very difficult to assigna denotation, especially when the context is not clear enough, and therefore, we needto apply the underspecified tag.In the next section, we present other indicators found in the empirical study thatprovide us with clues for the differentiation between event and result nominalizations.These indicators are data-driven and we can only guarantee that they work for Spanish.4.2 Finding New Clues to Support Event and Result DenotationsThe analysis of 3,077 nominalization occurrences, focusing on the semantic distinctionbetween those denoting an event, result, or underspecified type, has allowed us to findnew clues that strengthen these readings, especially the event reading.One of the clearest clues for detecting the event nominalizations is the possibility ofparaphrasing the NP with a clausal structure, as we saw in Section 1, Examples (3)?(6).
Another valuable clue is to check whether the nominalization admits an agentcomplement introduced by por (?by?)
or por parte de (?by?).
We use this criterion becauseit is the most informative one to support event nominalizations but it is also a veryoptional complement and is scarcely represented in the corpus.
The annotators coulduse these two tests to decide the denotation type.
Therefore, they had two extra criteriathat the data did not provide.Furthermore, we found other indicators that can help to select one denotationtype, the so-called selectors.
We identified two types of selectors:1.
External selectors: Prepositions like durante (?during?
), nouns like proceso(?process?
), adjectives like resultante (?resulting?
), verbs like empezar(?begin?
), and adverbs en v?
?a de (?on the way to?
), which are elements thatpoint to a specific denotation from outside the nominalized NP.
Forinstance, in Example (12) the preposition durante (?during?)
gives a clue tointerpret presentacio?n (?presentation?)
as an event.2.
Internal selectors: Prefixes within the nominalization that indicate aspecific denotation type; for instance, a noun with the prefix re-with areiterative meaning such as reubicacio?n (?relocation?)
in Example (13).
Thisis due to the fact that the reiterative meaning only applies to bases thatdenote actions.
(12) Durante [la presentacio?n<event>del libro], e?l abogo?
por la formacio?n delos investigadores en innovacio?n tecnolo?gica.
?During [the presentation<event> of the book], he advocated the trainingof researchers in technological innovation.?
(13) Hoy [la reubicacio?n<event>del ex ministro] no resulta fa?cil.
?Today, [the relocation<event> of the ex minister] does not seem easy.
?These new clues allow us to support our semantic classification independently fromthe literature criteria under evaluation.
The only inconvenience of these tests and theselectors is that they cannot be implemented as features in the ADN-classifier.843Computational Linguistics Volume 38, Number 45.
Knowledge ResourcesThis section presents the linguistic resources used in building the final version of theADN-Classifier (R3).
We briefly describe the AnCora-ES corpus and the AnCora-Verblexicon, and we focus in more detail on the description of the AnCora-Nom lexiconfrom which we obtain most of the features for the building of the ADN-Classifier.5.1 AnCora-ES CorpusAnCora-ES is a 500,000 word (henceforth, 500kw) Spanish corpus14 consisting of news-paper texts annotated at different linguistic levels: morphology (part of speech andlemmas), syntax (constituents and functions), semantics (verbal argument structure,thematic roles, semantic verb classes, named entities, and WordNet nominal senses),and pragmatics (coreference).15 The corpus contains 10,678 fully parsed sentences.
Aswe explained in Section 3, nominalization occurrences (23,431) were automatically an-notated with denotation types using an intermediate model of classification (see step 5in Figure 1).
This automatic annotation was then manually validated by three graduatestudents in linguistics.
These annotators were selected from a group of five, becausethey achieved an observed agreement of over 80%, corresponding to a kappa of 65% inan inter-annotator agreement test, whereas the average observed agreement was 75%corresponding to a 60% kappa.
For the purpose of annotation, the three annotators tookinto account the semantic definition we provided, the criteria presented in Section 4.1,and the semantic tests described in Section 4.2.
The inter-annotator agreement wascarried out to ensure the consistency and quality of the AnCora-ESmanual annotation.16Therefore, the AnCora-ES corpus enriched with denotation type annotation is usedfor learning the different models of the ADN-Classifier-R3.
From this resource weobtained two kinds of features:(a) The corpus versions of the features from the lexicon (see Section 5.3): thetype of determiner used in Section 4; the number (plural or singular) inwhich the nominalization occurrences appear; and the constituent type ofthe complements.
(b) The contextual features such as the tense and the semantic class of the verbthat dominates the nominalization in the sentence; the syntactic functionof the NP headed by a nominalization; and whether the noun appears in anamed entity.We use the Tgrep217 tool for the feature extraction from the corpus; this allows usto efficiently inspect the syntactic trees in a Treebank format.1814 A similar version exists for Catalan, AnCora-CA.15 AnCora-ES is the largest multilayer annotated corpus of Spanish freely available at:http://clic.ub.edu/corpus/ancora.16 For more details on the manual validation and the inter-annotator agreement test, see Peris, Taule?, andRodr?
?guez (2010).17 http://tedlab.mit.edu/ dr/TGrep2/.
Tgrep2 is an improvement of Tgrep.
Both tools are tree-basedcounterparts of the widely used string searching Unix Grep tool.18 In the following link the set of tgrep rules as well as some implemented examples are available:http://clic.ub.edu/corpus/en/documentation.844Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in Nominalizations5.2 AnCora-VerbAnCora-Verb-ES is a verbal lexicon that contains 2,830 Spanish verbs.19 In this lex-icon, each predicate is related to one or more semantic classes, depending on itssenses, basically differentiated according to the four event classes?accomplishments,achievements, states, and activities (Vendler 1967; Dowty 1979)?and on the diathe-ses alternations in which a verb can occur (Va?zquez, Ferna?ndez, and Mart??
2000).The semantic class of the verb base of the nominalization is used as a feature in theADN-Classifier.5.3 AnCora-NomThis section presents AnCora-Nom,20 a Spanish lexicon of deverbal nominalizationsthat has been iteratively used and improved as a result of the experiments reportedhere.
At present, it contains 1,655 lexical entries corresponding to 3,094 senses and3,204 frames.
These lexical entries represent the lemmas corresponding to the 23,431deverbal nominalization occurrences appearing in the annotated AnCora-ES corpus.For each of these lemmas we created a lexical entry using the information annotated inthe corpus.21 The features of each lexical entry are organized in three levels: lexical entry,sense, and frame level.
The lexical entry attributes are not extracted from the corpus butadded in order to document the lexical entry.
Sense and frame attributes, in contrast,were extracted from the AnCora-ES corpus.
Each lexical entry is organized in differentsenses, which were established taking into account the denotation type, the sense of thebase verb, and whether or not the nominalization is part of a lexicalized construction.
Inturn, each sense can also contain one or more nominal frames, depending on the verbalframe fromwhich the nominalization is derived.
Next, we detail the attributes specifiedin the three levels described above.
Figure 2 shows the full information associated withthe lexical entry aceptacio?n (?acceptance?
).5.3.1 Lexical Entry Level Attributes.
These are as follows:(a) Lemma.
In Figure 2, the value for this attribute is the noun aceptacio?n(lemma=?aceptacio?n?).
(b) The attribute language (?lng?)
codifies the language represented in the lexicalentry.
AnCora resources work with Spanish and Catalan, so the values of this attributeare ?es?
for Spanish (lng=?es?)
and ?ca?
for Catalan (lng=?ca?).
At present, AnCora-Nom only deals with Spanish nominalizations.
(c) The attribute origin indicates the type of word from which the nouns are de-rived.
In Figure 2, the value for this attribute is ?deverbal?, meaning that this lexicalentry concerns a noun derived from a verb.
At present, AnCora-Nom only containsdeverbal nouns but in the future it will include other types of nominalizations such asdeadjectivals.
(d) The attribute type refers to the word class, ?noun?
in Figure 2.19 A similar version exists for Catalan, AnCora-Verb-CA.20 We describe here AnCora-Nom-R3, the final version of the lexicon.21 For a detailed explanation of the automated extraction process see Peris and Taule?
(2011).845Computational Linguistics Volume 38, Number 4Figure 2Aceptacio?n (?acceptance?)
lexical entry in AnCora-Nom.846Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in Nominalizations5.3.2 Sense Level Attributes.
These include:(e) The attribute cousin marks whether the nominalization is morphologically de-rived from a verb (cousin=?no?, in Figure 2) or is a cousin noun (cousin=?yes?).
Cousinnouns (Meyers, Reeves, andMacleod 2004) are nouns that give rise to verbs (e.g., relacio?n[?relation?]
>relacionar [?to relate?
]), or nouns semantically related to verbs (e.g., escarnio[?mockery?]
is related to mofarse [?to make fun?]).
(f) The denotation attribute indicates the semantic interpretation of the deverbalnoun.
The possible values are: ?event,?
?result,?
and ?underspecified.?
In Figure 2, thereare two senses, the first one being result (denotation=?result?)
and the second one event(denotation=?event?).
(g) Each sense contains an identifier (?id?)
to indicate the sense number.
(h) The lexicalized attribute indicates whether or not the nominalization is partof a lexicalized construction (idiomatic expression) (Figure 2: lexicalized=?no?).
In thefirst case, two additional attributes are added: (i) the alternative-lemma, specifyingthe whole lexicalized construction of which the nominalization is part (for instance,alternative-lemma=?golpe de estado,?
[?coup d?etat?
]), and (ii) lexicalization-type, todistinguish between the six types of lexicalizations: ?nominal,?
?verbal,?
?adjectival,??adverbial,?
?prepositional,?
or ?conjunctive?
(see Section 4).
We should bear in mindthat one of the three above-mentioned denotation values is assigned to the wholelexicalized construction only in the case of nominal lexicalizations.
For instance, thelexicalized construction golpe de estado is a nominal lexicalization (lexicalization-type=?nominal?
), and therefore, it has a denotation value (denotation=?result?).
(i) The attribute originlemma specifies the verb lemma from which the noun isderived.
In Figure 2, the value for this attribute is ?aceptar?
in both senses (origin-lemma=?aceptar?).
(j) Because verbs can have different senses, the attribute originlink indicates theconcrete verbal sense of the base verb.
In Figure 2, the originlink attribute takes thesame value in both senses: ?verb.aceptar.1?
(oringinlink=?verb.aceptar.1?).
(k) Because nouns in the AnCora corpus are annotated with WordNet synsets,22 weincorporate this information in the attribute wordnetsynset.
In Figure 2, the first senseof aceptacio?n corresponds to two synsets (wordnetsynset=?16:00117820+16:10039397?
),whereas the second only corresponds to one (wordnetsynset=?16:00117820?).
It shouldbe noted that senses in AnCora-Nom are coarser grained than in WordNet: a sense cangroup together more than one WordNet synset.5.3.3 Frame Level Attributes.
These are detailed as follows:(l) The attribute type indicates the verbal frame from which the nominalization isderived.
In AnCora-Verb, each verbal sense can be realized in more than one frame:default, passive, anticausative, locative, and so forth.
In the nominal entries, we markthe corresponding verbal frames, which are the possible values for this attribute.
Inmost cases, its value is ?default?
as in Figure 2 (type=?default?).
This feature is neededto look for the corresponding verbal semantic class in AnCora-Verb.22 We used WordNet 1.6 for Spanish and WordNet offsets for identifying synsets.847Computational Linguistics Volume 38, Number 4(m) Argument (Structure).
In this complex attribute, the different arguments(argument) and the corresponding thematic roles (thematicrole) are specified.
Torepresent the arguments we follow the same annotation scheme used in AnCora-Verb.
For instance, in Figure 2, the event sense has one argument (?arg1?)
with apatient thematic role (?pat?).
This argument is realized twice (frequency=?2?)
bya prepositional phrase (constituent type =?sp?)
introduced by the preposition de(?of?)
(preposition=?de?)
and once by a possessive determiner (type=?determiner,?postype=?possessive?).
(n) The attribute referencemodifier represents the nominal complements that arenot arguments but which modify the reference of the nominalization.
Frequency isalso taken into account.
Strictly speaking, this attribute does not fit perfectly at theframe level.
We were interested in representing this information, however, and themost suitable level was the frame level because it allows for a seamless comparisonof argumental and nonargumental nominal complements.
(o) The type of determiner has proved to be a useful criterion for distinguishingbetween result and event readings, so we include this information in the attributespecifier.23 The possible values are: ?article,?
?indefinite,?
?demonstrative,?
?exclama-tive,?
?numeral,?
?interrogative,?
?possessive,?
?ordinal,?
and ?void?
when there isno determiner.
In this attribute, we also take into account the frequency with whichthe determiners are realized.
In Figure 2, the event sense is specified twice (constituentfrequency=?2?)
by an article determiner (type=?determiner,?
postype=?article?).
(p) The attribute appearsinplural indicates whether or not an occurrence of anominalization in a particular frame appears in the plural.
It is a boolean attribute.
InFigure 2, neither of the senses appear in the plural, thus, the value is ?no.?
(q) Finally, each lexical entry also contains all the examples from which the infor-mation has been extracted, specifying the corpus file, the node path, and the sentencein which each is located.6.
ADN-ClassifierAs stated previously, our goals for building the ADN-Classifier were twofold: On theone hand, to have at our disposal a tool to help us to quantitatively evaluate the validityof our claims regarding deverbal nominalizations as discussed in Section 4; and, on theother hand, to provide a classification tool able to take advantage of all the availableinformation in a specific scenario in the automatic classification of a deverbal noun.
Theaim of the task is to classify a deverbal nominalization candidate in an event, result, orunderspecified denotation type, as well as to identify whether the nominalization takespart in a lexicalized construction (idiomatic expression).
Therefore, we model the taskas a four-way classification problem.
In order to achieve these goals, some functionalrequirements on the software to be built were necessary.
Regarding the first goal, werequired that a tool be able to:1.
Use all the properties discussed in Section 4 as features for classification.23 The name of the attribute refers to the syntactic position that determiners occupy in the NP;the determiners specify the nominalization.848Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in Nominalizations2.
Tune the features: binarization, grouping the possible values,generalization, combination of features, and computation ofderived features.3.
Perform feature selection.4.
Facilitate the human interpretation of the model used by the classifier.5.
Analyze the accuracy of the individual features.6.
Use either senses or lemmas corresponding to deverbal nominalizationcandidates as units for classifying.In order to achieve the first aim, the first version of the ADN-Classifier (R1) (Peris,Taule?, and Rodr?
?guez 2009) was developed.
This is the basis for the building of theintermediate and final versions of the ADN-Classifier.
The final version is presented indetail next.
Obviously, the second goal imposes heavier constraints on the design of thetool (the ADN-Classifier-R3).
As is usual in other lexical classification tasks, such as PartOf Speech (POS) tagging or WSD, a word taken as an isolated unit is ambiguous butcan be disambiguated, or at least partially disambiguated, if enough context is takeninto account.
An additional constraint is that the nominalization candidate has to betagged as a noun.
For our classification task at least four processes are carried out: 1)tokenization; 2) segmentation at sentence level; 3) POS tagging; and 4) localization ofa nominalization candidate by means of a set of regular expressions looking for verbalnominalization endings.24In this setting, a case for classifying consists of a nominalization candidate usingthe POS-tagged sentence where it occurs as context, although this context is not alwayssufficient for disambiguation.
Additional processes could be carried out on the nomi-nalization candidate and the sentence (WSD, chunking, full parsing, SRL, linking of thenominalization candidate with the origin verb, etc.).
Each of these processes increasesthe number of possible features used for classifying but, because they are not errorfree, they could involve a decrement in the global accuracy of the preprocess step.Therefore, a careful examination of the processes, their accuracy, and the improvementof classification accuracy is needed.
For instance, performing WSD on the nominal-ization candidate could allow for the use of sense-based features and, thus, a clearimprovement in classification accuracy.
The inconvenience is that the state-of-the-artaccuracy rate of WSD is not very promising.
In recent SemEval challenges, the accuracyrate in All-Words tasks is between 60% and 70% for a baseline of 51.4% using the firstsense in WordNet, and 89% in Lexical-Sample tasks for a baseline of 78% (Chklovskiand Mihalcea 2002; Decadt et al 2004; Pradhan et al 2007).25 These figures dependon the sense inventory used for disambiguation: The All-Words task uses fine-grainedsenses (WordNet synsets) and the Lexical Sample task uses more coarse-grained senseinventory (Ontonotes senses).Therefore, we approach the problem of classification taking into account differentfeature sets which come from different knowledge resources, and we examine andevaluate the task performance when a decreasing number of knowledge resourcesare used.
Depending on the available knowledge resources and natural language24 We used 10 suffixes such as -cio?n (see Section 4).25 All these figures are for English.
To have some idea of the relative difficulty of the task for Spanishwe have measured this baseline in Ancora-ES resulting in a value of 42%, that is, an 18% drop withrespect to English.849Computational Linguistics Volume 38, Number 4Table 4Description of the scenarios used for evaluation.Scenario Knowledge Resources Features level NL Pre-Process1 AnCora-Nom+AnCora-Verb lemma POS2 AnCora-Nom+AnCora-Verb sense POS+WSD3 AnCora-Nom+AnCora-Verb lemma POS+Parsing4 AnCora-Nom+AnCora-Verb sense POS+WSD+Parsing5 AnCora-Nom lemma POS6 AnCora-Nom sense POS+WSD7 AnCora-Nom lemma POS+Parsing8 AnCora-Nom sense POS+WSD+Parsing9 ?
lemma POS+Parsing10 ?
lemma POS+Parsing+SRL(NL) processors, we designed the classification task in different scenarios, which arepresented in Table 4.
The columns include the knowledge resources used in eachscenario (column 2), whether the features used are extracted at sense or lemma level(column 3), and the NL processors that are necessary in each case.Scenario 1 in Table 4 presents the case in which the nominal lexicon (AnCora-Nom in our case) is available and the nominalization candidate is an entry in thislexicon.
The sentence where the nominalization candidate occurs is POS-tagged andno other NL processes are carried out.26 In this case, we apply the ADN-Classifier witha model learned using only features coming from the lexicon at the lemma level withAcclemma;lex accuracy.
Scenario 2 is the same as Scenario 1 but adds a WSD process tothe nominalization candidate with AccWSD accuracy.
In this case, we apply the ADN-Classifier with a model learned using only lexicon features at a sense level achiev-ing Accsense;lex accuracy.
Obviously, applying this model is only useful if Acclemma;lex?Accsense;lex outperforms the expected WSD error (1 ?
AccWSD).
Scenario 3 is the same asScenario 1 but adds constituent parsingwith Accparser accuracy.
In this case, we apply theADN-Classifier with amodel learned using lexicon and corpus features27 at lemma levelwith Acclemma;lex+corpus accuracy.
Again, thismodel is only useful if the Acclemma;lex+corpus?Acclemma;lex outperforms the expected parsing error (1 ?
Accparser).
Scenario 4 consists ofa combination of Scenarios 2 and 3.
Scenarios 5, 6, 7, and 8 reproduce Scenarios 1, 2, 3,and 4, respectively, without using the features extracted from the AnCora-Verb lexicon,so obtaining the origin verb of the candidate is not necessary.
In Scenario 9, the nominallexicon is not available or the nominalization candidate is a noun that does not occurin the nominal lexicon, and only the features extracted from the parsed tree at lemmalevel are used.
Finally, Scenario 10 is the same as Scenario 9 but adds an SRL processin order to obtain argument structure information.
Taking into account these two setsof requirements the final version of the ADN-Classifier (R3) has been built.We used ML techniques to build the ADN-Classifier.
Specifically, we used theJ48.Part rule-based classifier, the rule version of the decision-tree classifier C4.5.Rules(Quinlan 1993) as implemented in the Weka toolbox (Witten and Frank 2005).
We chosea rule-based classifier because it provides a natural representation of classification rules,thus allowing for the inspection of the model without diminishing accuracy and it26 POS-tagging implies previous tokenization and sentence segmentation steps.27 The parse tree obtained can be inspected in the same way as AnCora-ES.850Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in Nominalizationsallows us to perform a ranking of the individualized rules and a definition of a thresh-olding mechanism for performing a precision oriented classification.
Because most ofthe features are binary and the others are discrete with small range values, using morecomplex rule-based classifiers such as Cohen?s Ripper supposes no real improvementover our choice.28The ADN-Classifier therefore consists of the J48.Part classifier within the Wekatoolbox, the appropriate learned model (from the set described in Section 7.2 and listedin Table 5), and the list of features to be used.
During the exploitation phase the input tothe system consists of a table.
Each row in the table corresponds to a case for classifyingand each column to the values of the corresponding feature.
The result of the processis a column vector containing the result of classifying each instance.7.
Experiments and EvaluationIn this section we present and evaluate the experiments carried out with the ADN-Classifier.
First we present the settings of these experiments, then we focus on theexperiments themselves, and finally we evaluate the results.7.1 SettingIn order to validate the performance of the ADN-Classifier, a sequence of experimentswas conducted.
Concretely, two sets of experiments were carried out: we experimentedwith different models of the classifier structured in five dimensions (see Section 7.2) andwe applied the appropriate models in the different scenarios set out in Table 4.
We usea tenfold cross-validation method29 for the evaluation of these two sets of experiments.In order to evaluate the features selected and to carry out the classification task in eachscenario we used the models learned as described in Section 7.2.
As noted earlier, usingthe ADN-Classifier for classify involves using the J48.Part classifier within the Wekatoolbox and the appropriate learned model.7.2 ExperimentsThe experiments carried out with the ADN-Classifier-R3 are presented here.
Firstly,we describe those experiments related to the different models of the classifier andsecondly, we focus on how some of these models are applied in different scenarios.We apply the ADN-Classifier in different modes that correspond to the following fivedimensions.28 J48.Part learns first a decision tree and then builds the rules traversing all the branches of the tree.
Ripper,instead, learns the rules one by one (increasing the learning cost).
This can result in a more accurate andsmaller rule set just in the case of splitting numerical attributes; that is not our case.29 In n-fold cross-validation, the original sample is randomly partitioned into n subsamples.
Of the nsubsamples, a single subsample is retained as the validation data for testing the model, and theremaining n ?
1 subsamples are used as training data.
The cross-validation process is then repeated ntimes (the folds), with each of the n subsamples used exactly once as the validation data.
The n resultsfrom the folds can then be averaged (or otherwise combined) to produce a single estimation.
Theadvantage of this method over repeated random sub-sampling is that all observations are used forboth training and validation, and each observation is used for validation exactly once.
n is commonlyset to 10 (McLachlan, Do, and Ambroise 2004).851Computational Linguistics Volume 38, Number 4Table 5Experiments and evaluation of the models.
Legend of the model name: 1st letter(S = sense-based, L = lemma-based); 2nd letter (S = sense, L = lemma, E = corpus example);3rd letter (L = lexicon features, C = corpus features, A = all features); 4th letter (R = reducedvocabulary, F = full vocabulary) and 5th letter (R = reduced corpus, F = full corpus).Model Inst.
Att.
Rules Baseline (%) Accu.
(%) ?error (%) Red?error (%)Sense-basedmodelsSenses SSLRR 609 937 51 71.75 76.84 5.09 18.02ExamplesSSLRF 964 937 78 60.68 70.02 9.33 23.74SSLFR 1,428 1,671 84 70.86 81.72 10.85 37.25SSLFF 3,094 1,671 224 60.95 74.36 13.41 34.35SELRR 1,840 937 42 85.32 93.80 8.47 57.77SELRF 9,278 937 137 87.03 97.82 10.78 83.20SELFR 3,994 1,671 117 83.92 93.69 9.76 60.74SELFF 23,431 1,671 366 85.45 96.65 11.19 76.99SECRR 1,840 197 35 85.32 83.96 ?1.35 ?9.25SECRF 9,278 197 116 87.03 86.34 ?0.68 ?5.31SECFR 3,994 197 81 83.92 82.72 ?1.20 ?7.47SECFF 23,431 197 211 85.45 84.93 ?0.52 ?3.60SEARR 1,840 1,133 76 85.32 91.57 6.25 42.59SEARF 9,278 1,133 196 87.03 96.38 9.35 72.15SEAFR 3,994 1,867 146 83.92 91.72 7.80 48.52SEAFF 23,431 1,867 498 85.45 95.46 10.01 68.83Lemma-basedmodelsLemmas LLLRR 242 852 6 90.90 88.84 ?2.06 ?22.72ExamplesLLLRF 242 852 6 90.90 88.84 ?2.06 ?22.72LLLFR 532 1,559 14 89.84 89.66 ?0.18 ?1.85LLLFF 972 1,559 26 87.55 89.09 1.54 12.39LELRR 1,840 852 50 85.32 83.96 ?1.35 ?9.25LELRF 9,278 852 76 87.03 86.88 ?0.15 ?1.16LELFR 3,994 1,559 162 83.92 83.50 ?0.42 ?2.64LELFF 23,431 1,559 322 85.45 85.62 0.16 1.14LECRR 1,840 197 35 85.32 84.02 ?1.30 ?8.88LECRF 9,278 197 116 87.03 86.35 ?0.67 ?5.23LECFR 3,994 197 81 83.92 82.57 ?1.35 ?8.41LECFF 23,431 197 211 85.45 84.86 ?0.58 ?4.04LEARR 1,840 1,048 109 85.32 85.05 ?0.27 ?1.85LEARF 9,278 1,048 355 87.03 87.64 0.61 4.7LEAFR 3,994 1,755 236 83.92 85.27 1.35 8.41LEAFF 23,431 1,755 981 85.45 87.20 1.74 12.00?
Application level.
We distinguish between sense-based and lemma-based mod-els.
Sense-based models use the information from the AnCora-Nom-R3 lexicon at senselevel, that is, the features for learning (and classification) are associated with the specificsenses.
In contrast, in lemma-based models, when extracting features from the lexicon,we use as features for learning and classification those attributes whose values areshared by all senses of the same lemma.
Therefore, at this second level of applicationthe features are not so informative but, at the same time, we reduce our dependenceon the lexicon, which was a step that had to be taken to move towards a more realisticscenario.?
Unit of learning and classification (i.e., the instance to be classified).
Thesesense- or lemma-based models are in turn distinguished depending on whether theunit of learning and classification comes from the lexicon (sense or lemma) or from the852Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in NominalizationsAnCora-ES corpus (examples), that is, if they correspond to types or tokens.
In the firstcase all the features come from the lexicon, whereas in the latter contextual features,extracted from the corpus, can also be used.
Consequently, in sense-based models theunits used are senses (from the lexicon) or examples (from the corpus), and in lemma-based models they are lemmas (from the lexicon) or examples (from the corpus).
It hasto be taken into account that depending on this dimension, the number of instancesfor learning varies: There are more senses than lemmas in the lexicon and there aremore nominalization occurrences (examples) in the corpus than nominalization sensesor lemmas in the lexicon.?
Features involved.
The features used for both learning and classifying are ob-tained from the lexicon (lexical features) or from the corpus (contextual features).
Thedifferent models are distinguished by using only lexical features, only contextual fea-tures, or the combination of both types of features.?
Vocabulary size.
The data sets taken into account correspond to a reduced set of817 nominalization lemmas obtained from the 100kw subset of the corpus used for thefirst version of the ADN-Classifier (R1) or to the full set of 1,655 nominalization lemmasoccurring in the whole AnCora-ES (500kw).
Depending on this dimension, the numberof instances for learning also vary.?
Corpus size.
Two corpus sets are used in the different models: a reduced subsetof 100kw used for the first version of the ADN-Classifier (R1) or the full 500kw corre-sponding to the whole corpus.30 Depending on this dimension, the number of instancesfor learning also vary.In order to identify the models as presented in Table 5, we use five letters asnotation, each of which identifies one of these five dimensions.
The first letter corre-sponds to the application level: If the model is sense-based an S will be used for theidentification and an L in the case of lemma-based models.
The second letter refersto the unit of classification and is L for lemmas, S for senses, and E for examples.
Inthe third position the reference to the origin of the features involved in the model isfound: L (from the lexicon), C (from the corpus), A (from both resources, all features).In fourth place, we refer to the vocabulary size: R (reduced) stands for the reducedset of 817 nominalization lemmas and F (full) for the full set of 1,655 nominalizationlemmas.
In last place, we also designate the corpus size by an R (reduced set of100kw) or an F (full set of 500kw).
Therefore a lemma-based model that uses examplesas units of classification, uses all the features, the whole vocabulary, and the wholecorpus is identified as the LEAFF model.
In total we experimented with 32 differentmodels.31For the different scenarios described in Section 6, we applied the appropriatemodels so as not to use the noninformed features for each one.7.3 EvaluationThe classifier performance of the different models was evaluated by a tenfold cross-validation method.
Next, we focus on the results of the 32 models resulting from thefive dimensions described in Section 7.2.
Table 5 presents the overall results: the models30 For obtaining the learning curve of some of our models intermediate sizes have been used.31 Not all the combinations of values of the dimensions are allowed.853Computational Linguistics Volume 38, Number 4used (column 1), the number of instances used for learning (column 2), the number ofattributes used, and the number of rules built by the classifier (columns 3 and 4), andfinally, the baseline, the accuracy, the decrease error over the baseline (?-error), and therelative error-reduction ratio (Red-?-error) obtained by each model (columns 5, 6, 7,and 8).
The rows correspond to the different models presented.
Recall that the namesof the models are assigned according to the five dimensions presented in Section 7.2.It should be borne in mind here that in column 2, the number of instances for learningdepends on the type of unit used for learning and classification (senses in sense-basedmodels, lemmas in lemma-based models, and examples) and on the vocabulary andcorpus size.
The interaction between these three dimensions also explains why thefigures for the baseline change for each model.
The baseline is a majority baselinewhich assigns all the instances to the result class.
In general, when the unit used isfrom the lexicon, the lemma baseline increases relative to the sense baseline.
This isbecause in lemma-based models we group the senses that share all the features undera lemma; because different senses do not normally share all the features, in the end,only monosemic lemmas are in fact taken into account.
This fact, therefore, shows thatthere are more result type monosemic lemmas than event and underspecified monosemiclemmas.
Furthermore, it is worth noting that when the unit of learning and classifi-cation used are the examples from the AnCora-ES corpus and not the senses fromthe lexicon, the baseline also increases.
Therefore, it seems that proportionally resultnominalizations are more highly represented in the corpus than event and underspecifiednominalizations.
Regarding the number of features used for learning, the type of featureinvolved and the vocabulary size (when features from the lexicon are used) are the tworelevant dimensions.
Finally, it should be said that the accuracy and the other two cor-related measures are obtained by evaluating the performance of the different models bytenfold cross-validation.As can be seen in Table 5, the sense-based models (the first 16 rows) outperformthe corresponding lemma-based models (the last 16 rows).
This is explained by the factthat there are features in the lexicon coded at the sense level that cannot be recoveredat the lemma level because in lemma-based models we only use as features for theclassification those attributes whose values are shared by all senses of the same lemma,and this does not commonly happen.
At the sense level, the best results are achievedwhen the features used in the classification come exclusively from the lexicon, withthe unit of classification being senses from the lexicon (the first block of four rows)or examples from the corpus (the second block of four rows).
The contextual features(those coming from the corpus) can only be applied to models using examples from thecorpus as the unit of classification.
These features harm accuracy: When they are usedalone (the third block of rows) they yield accuracy values that are below the baselineand when they are used in combination with features obtained from the lexicon (thefourth block of rows) the accuracy decreases in relation to the models that use onlythe lexicon as the source of the features (the second block of four rows).
This showsthat there is crucial information in the lexicon that is not possible to recover from thecorpus.
Furthermore, it should be mentioned that there is a generalized improvementacross sense-based models correlated to the vocabulary and especially corpus size: Thebigger the set of vocabulary and corpus, the better the result.
This fact is also presentin lemma-based models.The sense-based models represent the upper bounds for our task.
In a realisticscenario, however, given the state-of-the-art results in WSD, we would not have accessto sense labels, so we are much more interested in the performance of lemma-basedmodels.
The best results are achieved when features from the lexicon and from the854Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in Nominalizationscorpus are combined (the last block of rows), showing that the sum of both types offeatures gives rise to positive results, which are not achieved by lexical features orcontextual features on their own.
When the features used in the classification comeexclusively from the lexicon, with the unit of classification being lemmas from thelexicon (the fifth block of four rows) or the examples from the corpus (the sixth blockof four rows), the results are negative (below the baseline) except when the vocabularyand corpus size are both the full sets (1.54% and 0.16% improvement, respectively).In these cases, the information from the lexicon is not as accurate as in sense-basedmodels.
The contextual features alone do not achieve positive results, not even with thefull vocabulary and the full corpus.
Therefore, the combination of features is needed ina realistic scenario in order to achieve good performance of the classifier.
In these cases,only when the reduced vocabulary and the reduced corpus are used are the resultsslightly negative.
From now on, we will focus on the last model (LEAFF) because evenif it has a lower accuracy than the corresponding sense-model, we expect it to exhibit amore robust behavior when tackling unseen data.An important point for the classifier to learn a model is whether or not the samplesize is large enough for accurate learning.
We performed a learning curve analysis ofthe LEAFF model for different sample sizes (from 1,000 examples to the whole setof 23,431 examples).
The results are depicted in Figure 3.
We have also plotted theconfidence intervals at 95%.
The results seem to imply that for sizes over 5,000 examplesthe accuracy tends to stabilize; we are, therefore, highly confident of our results.
Asexpected, the confidence intervals consistently diminish as the corpus grows.7.4 Precision Oriented Classifier: ThresholdAll the experiments reported so far are based on a full coverage setting.
Coverageis 100% in all cases and, therefore, accuracy and precision have the same score.Figure 3Learning curve for the LEAFF model.855Computational Linguistics Volume 38, Number 4Figure 4Precision/coverage plot for different rules thresholding of the SEAFR model.Additionally, we performed a precision oriented experiment based on a classifieraiming to achieve a high precision at the cost of a drop in coverage.
In order to do this,we scored each of the rules in the rule set from the LEAFF model of the ADN-Classifierindividually (not taking into account the order of such rules).
We sorted the rules in therule set by their individual scores, provided by the Weka toolbox, and built a classifierbased on a thresholding mechanism: Only the rules over the threshold were applied.This resulted in higher precision at the cost of lower coverage.
The LEAFF model, aspresented in Table 5, consists of 981 rules obtaining an overall accuracy of 87.20%.
Theresults of the application of the n most accurate rules, for n from 981 to 1, are depictedin Figure 4.
Note that removing the 500 least accurate rules has a small effect on thecoverage and the overall precision has risen to 94%.
An additional reduction of 200rules results in an increase of the overall precision to 96.5% at a cost of a drop in thecoverage to 90%.
Using only the 100 most accurate rules obtains a precision of 98.5%for a still useful coverage of 80%.7.5 Evaluation of ScenariosThe results of the experiments applying the scenarios described in Section 6 (see Table 4)are presented in Table 6.
The table shows the results of the ten scenarios set out in rows,and in columns we provide the scenario identification (column 1); the model appliedout of the 32 generated, following the notation in Section 7.2 (column 2); the number offeatures in the original model (column 3); the number of features in the model adaptedfor that scenario after removing noninformed features, that is, the features used in theoriginal model that do not fit in the description of a concrete scenario (column 4);and the accuracies of the original and final model (columns 5 and 6, respectively).
Ineach scenario we applied the best model of the 32 we generated taking into account856Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in NominalizationsTable 6Experiment and evaluation of scenarios.Scenario Model Initial Att.
Final Att.
Initial Acc.
(%) Final Acc.
(%)1 LELFF 1,559 1,559 85.62 85.622 SELFF 1,671 1,671 96.65 96.653 LEAFF 1,755 1,755 87.20 87.204 SEAFF 1,867 1,867 95.46 95.465 LELFF 1,559 1,416 85.62 85.566 SELFF 1,671 1,613 96.65 96.177 LEAFF 1,755 1,611 87.20 87.128 SEAFF 1,867 1,808 95.46 95.419 LECFF 197 197 84.86 84.8610 LEAFF 1,755 1,556 87.20 87.08the features that each model uses and that fit the best in each scenario according tothe hypothesized available linguistic processors.
When there is no concrete model toproject how the ADN-Classifier would perform in a concrete scenario, we selected themodel that fits approximately in that scenario and removed the noninformed features.For instance, Scenario 10 describes the case where the nominal lexicon is not availableor the nominalization candidate is a noun that does not occur in the nominal lexicon,and the features used are extracted from the parsed tree at lemma level and from theSRL process in order to obtain argument structure information.
Because we do nothave a model that perfectly fits in that scenario, we select the LEAFF (lemma basedmodel using examples from the corpus as the unit of classification and obtaining thefeatures from both lexicon and corpus, with full vocabulary and full corpus sets), andwe removed all the features from the lexicon except the ones related to the argumentstructure, simulating an SRL process.32These results show that the difference between lemma-based and sense-based mod-els shown in Table 5 is also present here.
There is a decrease in accuracy in all the casesin which features are removed, although this decrease is not statistically significant.This could be due to the large number of features available for rule learning and thepossibility of using alternate features when some of the original ones are removed.7.6 Error AnalysisThe analysis of errors focuses on the lemma-based model using lexicon and corpusinformation with the full vocabulary and the full corpus (LEAFF).
Table 7 presents theconfusion matrix of the model.
Rows correspond to manually labeled data and columnsare predictions from the classifier.
The correct predictions are in the diagonal (in bold-face).
The errors are marked in italics.The rate of error is almost equally split between the three main classes: incorrectlyclassified event nominalizations represent 31%, result nominalizations 34%, and under-specified nominalizations 32%.
Lexicalized instances,33 however, only display an errorrate of 3%.
Among event nominalizations incorrectly classified by the ADN-Classifier,32 In the same way, sense-based models simulate how the ADN-Classifier would perform with an idealWSD automatic process.33 By lexicalized nominalizations we refer here to those lexicalized nominalizations where a denotationtype is not assigned, that is, non-nominal lexicalizations.857Computational Linguistics Volume 38, Number 4Table 7Confusion matrix for the LEAFF model.ADN Result Event Underspecified Lexicalized TotalResult 18,997 575 397 54 20,023Manually Event 676 933 242 2 1,853Validated Underspecified 643 309 453 7 1,412Lexicalized 90 2 2 49 143Total 20,406 1,819 1,094 112 23,43173% (676 instances) were classified as result; 26% (242 instances) as underspecified, and amarginal 3% (2 instances) as lexicalized nominalizations.
These errors are attributable tofour main causes.
Firstly, 27% of the errors are in fact human errors,34 which means thatthe ADN-Classifier performed well.
Secondly, the annotation guidelines contain criteriathat the ADN-Classifier cannot recognize: the paraphrase criterion, the agent criterion,and the so-called selectors (see Section 4.2).
These errors represent 51% of the wronglyclassified events.
Therefore, there were cases (a total of 61) where manual annotatorshad an extra criterion that the ADN-Classifier could not use.
We thought that imple-menting the selectors as features in the ADN-Classifier would be an excessively ad hocapproach.
Thirdly, an error of 21% in event classification is explained because there are anumber of criteria, implemented as features in the ADN-Classifier, that suffer from datasparseness, and, therefore, the ADN-Classifier cannot learn them as being as relevantas they are.
For instance, a very conclusive clue for detecting event nominalizations isthat they are specified by a possessive determiner that is interpreted as an arg1-patient.And, finally, the cases in which the ADN-Classifier annotated event nominalizations aslexicalized constructions are explained by the ADN-Classifier confusing them with reallexicalized constructions in which the lemma is shared (an error rate of 1%).Among result nominalizations incorrectly classified by the ADN-Classifier, 56%(575 instances) were classified as event, 39% (397 instances) as underspecified, and 5%(54 instances) as lexicalized nominalizations.
These errors are attributable to the samefour causes set out above.
The rate of human errors is now 51%, however, meaning thatthere are event and underspecified nominalizations that were incorrectly validated.
Therate of errors explained by the selectors is now just 10% because there are more selectorsfor identifying event than for detecting result nominalizations.
And finally, an error rateof 37% is explained by those criteria that are implemented as features of the ADN-Classifier, but which suffer from data sparseness, and, therefore, despite their relevance,cannot be learned by the ADN-Classifier.
In the case of result nominalizations, thereare more criteria of this type: nominalizations deriving from unergative and stativeverb classes, relational adjectives as arguments, and temporal arguments realized bya PP introduced by de (?of?).
And finally, the cases in which the classifier annotatedresult nominalizations as lexicalized constructions are explained by the ADN-Classifierconfusing them with real lexicalized constructions in which the lemma is shared (anerror rate of 2%).34 When comparing automatic annotation with the manual validation, some cases were considered to bedoubtful.
We discussed those cases with all the annotators and decided which annotation (automatic orhuman) was the correct one.
Therefore, by human errors we mean those incorrectly classified in themanual validation process.858Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in NominalizationsAmong incorrectly classified underspecified nominalizations, 32% (309 instances)were classified as events, 67% (643 instances) as results, and a marginal 1% (7 instances)as lexicalized nominalizations.
The difficulty in identifying underspecified nominaliza-tions is to be expected, given that these are either cases with no clear contextual hintsor truly ambiguous examples.
In this case, the rate of human error is 45%.
Althoughthere are no selectors that identify underspecified nominalizations, in some cases an NPcontaining a nominalization presents contradictory criteria.
For instance, an indefinitedeterminer is a criterion that points to a result denotation and the selector durante (?dur-ing?)
typically selects an event denotation.
In these cases, the annotators were instructedto tag them as underspecified.
The ADN-Classifier could not use the selectors in itsclassification, however, and most of these cases therefore were annotated as results.
Thistype of error represents 19% of the incorrectly classified underspecified nominalizations.The agent criterion explains an error rate of 20%.
If both types of PPs (introduced by thepreposition por [?by?]
or introduced by the preposition de [?of?])
are valid for the NPthe annotators were validating, they tagged the nominalization as underspecified type.Again, human annotators had an extra criterion that the ADN-Classifier could not use.The remaining 5% is explained by the failure of the ADN-Classifier to detect a patternthat is typical of underspecified nominalizations: those derived from an achievement verbwith an arg1-patient.
And finally, the cases in which the ADN-Classifier annotatedunderspecified nominalizations as lexicalized constructions are explained by the ADN-Classifier confusing them with real lexicalized constructions in which the lemma isshared (an error rate of 1%).Most incorrectly classified lexicalized constructions (96%, 90 instances) were clas-sified as result nominalizations.
This is probably due to the fact that most nominallexicalized nominalizations are of the result type.
Therefore, the key failure of theADN-Classifier is basically in distinguishing between the different types of lexicalizedconstructions.8.
Related WorkAlthough there are several works that contemplate the computational treatment ofnominalizations, most of them are basically interested in two issues: 1) automaticallyclassifying semantic relations between nominals (Task 4 of SemEval 2007 [Girju et al2009] and Task 8 of SemEval 2010 [Hendrickx et al 2010]) or in noun compoundconstructions (Girju et al [2005] and Task 9 of SemEval 2010 [Butnariu et al 2010a,2010b]); and 2) taking advantage of verbal data to interpret, represent, and assignsemantic roles to complements of nominalizations (Hull and Gomez 2000; Lapata 2002;Pado?, Pennacchiotti, and Sporleder 2008; Gurevich andWaterman 2009).
Althoughmostof these works show a certain awareness of the linguistic distinction between eventand result nominalizations, none of them applies this distinction in their systems.
Thenotion of event appears in the work of Creswell et al (2006), in which a classifier thatdistinguishes between nominal mentions of events and non-events is presented.
Theirdistinction is not comparable to our event and result distinction for one main reason,however: they do not focus on nominalizations but on nouns in general, and thereforethe difficulty in distinguishing events from non-events among all types of nouns is lessthan distinguishing between event and result nominalizations, which, as has been seen,are highly ambiguous.
We state that it is easier because in a wide nominal domain thereare many nouns which under any circumstance can be understood as non-dynamic (infact, nouns tend to denote objects, non-events) and if Creswell et al include as seed for859Computational Linguistics Volume 38, Number 4learning these types of nouns, such as airport or electronics, it will necessarily increasethe accuracy of the automatic distinction between their two denotation types.As far as we know, the only work closely related to ours is that of Eberle, Faasz,and Ulrich (2011) who are working on German ?ung nominalizations, in which theassignment of a denotation type is also carried out.
In that paper they state that thiskind of nominalization can denote an event, a state, or an object.
Specifically, they analyzethose ?ung nominalizations derived from verbs of saying embedded in PPs introducedby the preposition nach (?to?).
According to the authors, these nominalizations candenote either an event or a proposition, which is a type of object.
They present a semanticanalysis tool (Eberle et al 2008) which disambiguates this type of nominalization on thebasis of nine criteria, which they call indicators.
The tool extracts the indicators fromthe semantic representation that it provides and computes the preferred denotationaccording to a pre-established weighting schema.
They apply this tool to a set of 100sentences where the relevant material (the nine indicators) is completely familiar to thesystem and the tool recognizes the preferred reading in 82% of cases.Because the ADN-Classifier is based on ML techniques and does not restrict thenominalizations to a specific suffix and to those derived from verbs of saying, theirwork is not directly comparable to ours.
We tried to replicate their experiment, however,selecting only those nominalizations created with the suffix -cio?n (the most productiveSpanish suffix and the nearest equivalent to ?ung in German) and which derive fromverbs of saying.
The subset obtained includes 66 types of nominalizations, comparedwith the 1,655 in our work.
We applied the LEAFF model to the 719 tokens of these66 nominalization types and we obtained an accuracy of 85.6%.
This implies a 3.6percentage point increase in accuracy with respect to the results of their work, despitethe fact that our model is not trained on this specific nominalization class and does notdispose of specially designed indicators.
We have to take this result with due cautionbecause we are dealing with two not closely related languages and considering a closebut not identical set of nominalizations.9.
ConclusionsThis article contributes to the semantic analysis of texts focusing on Spanish deverbalnominalizations.
We base our study on theoretical hypotheses that we analyze empir-ically, and as a result we have developed three new resources: the ADN-Classifier, thefirst tool that allows for the automatic classification of deverbal nouns as event or resulttypes; the AnCora-ES corpus enriched with the annotation of deverbal nominalizationsaccording to their semantic denotation, being in fact the only Spanish corpus whichincorporates this information; and the AnCora-Nom lexicon, a resource containing1,655 deverbal nominalizations linked to their occurrences in the AnCora-ES corpus.These resources could be very useful for NLP applications.
The work presented in thisarticle also provides an additional insight into the linguistic question underlying it:The characterization of deverbal nominalizations according to their denotation and theidentification of the most useful criteria to distinguish between these denotation types.We can classify our contributions in three directions:1) The study of the relevant features for the classification of a nominalizationas being of event or result type.
The set of features considered were selected fromthe linguistics literature, mostly devoted to the English language, and its relevancewas established empirically for Spanish.
From the corpus-based study, we concludedthat not all the criteria posited for English seem to port to Spanish.
Among the860Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in Nominalizationsevaluated criteria, the most relevant for distinguishing between event and result nomi-nalizations are: 1) the semantic class of the verb from which the noun is derived;2) its pluralization capacity; 3) its determiner types; 4) the preposition introducing anagentive complement; and 5) the obligatory presence of an internal argument.
Thesefeatures are represented as attributes in the nominal lexical entries of the AnCora-Nomlexicon.
Models including features coming from the lexicon outperform those that onlytake into account features from the corpus.
As expected, models working at the senselevel outperform those working at the lemma level.
When working at the lemma levelonly the combination of features from both the lexicon and the corpus provides resultsthat outperform the baseline.
It is interesting to note that the number of features used tosupport result nominalizations is significantly superior to those used to strengthen eventnominalizations.
In each criterion we find features for supporting result nominalizationsbut not event nominalizations.
As a result, the ADN-Classifier uses more features fordetecting result than event nominalizations, and therefore achieves a greater degreeof accuracy on the former than in the latter.
Furthermore, the corpus base study hasallowed us to find new clues that support denotation types, especially the event reading.The paraphrase and agent criteria, as well as the selectors, have proved very usefulto human annotators for distinguishing between an event and a result reading.
Thesecriteria are difficult to implement automatically, however.2) Lexical resources derived from this work.
We have enriched the AnCora-EScorpus with the annotation of 23,431 deverbal nominalization occurrences according totheir semantic denotation; and we have built AnCora-Nom from scratch, representingthe 1,655 nominalization types that correspond to these occurrences.3) The ADN-Classifier.
This classifier is the first tool that aims to automaticallyclassify deverbal nominalizations in event, result, or underspecified denotation types, andto identify whether the nominalization takes part in a lexicalized construction in Spanish.We set up a series of experiments in order to test the ADN-Classifier under differentmodels and in different realistic scenarios, achieving good results.
The ADN-Classifierhas helped us to quantitatively evaluate the validity of our claims regarding deverbalnominalizations.
An error analysis was performed and its conclusions can be used topursue further lines of improvements.Further work.
Two of the main sources of error found in the performance of theADN-Classifier are data sparseness of some of the features (such as PP agent) and thefact that there are criteria at the disposal of human annotators that the ADN-Classifieris unable to detect.
In order to reduce the problem of data sparseness it would beinteresting to look for some linguistic generalizations of the sparse features in order toimplement a backoff mechanism.
Another line of future work is to analyze the criteriaused by human annotators and not currently implemented either in the lexicon or in thecorpus.
Some additional features could be incorporated in the Classifier.
Among themare path-based syntactic patterns that have been applied with success to related tasks(see Gildea and Palmer 2002).We have also experimented with a meta-classifier working on the results of binaryclassifiers (one for each class).
The global accuracy of the meta-classifier was not greaterthan that of the current ADN.
We think, however, that a binary classifier for theunderspecified type (the most difficult one) could result in improvements.
Most of theconsiderations regarding the scenarios described in Table 4 are based on a crude globalevaluation of complementary NL processors such as a word sense disambiguator; for861Computational Linguistics Volume 38, Number 4example, a specific scenario can be followed when the global accuracy of the NL pro-cessor crosses a given threshold.
A more precise approach can also be adopted.
Con-sider, for instance, the WSD task instead of a simple classifier providing a globalaccuracy?a regressor can provide individual scores of accuracy for each case (de-gree of confidence, margin, probability of correct classification, etc.).
This more preciseapproach can lead to new scenarios incorporating hybrid models.The last point of future work consists in analyzing to what extent the ADN-Classifier and its models are applicable to other languages.
Concretely, because we havea similar corpus for Catalan (lacking deverbal nominalization information) we plan toapply the models learned for Spanish to this closely related Romance language.AcknowledgmentsWe are grateful to Maria Anto`nia Mart??
andMarta Recasens for their helpful advice andto David Bridgewater for the proofreading ofEnglish.
We would also like to express ourgratitude to the three anonymous reviewersfor their comments and suggestions to improvethis article.
This work was partly supportedby the projects Araknion (FFI2010-114774-E),Know2 (TIN2009-14715-C04-04), and TEXT-MESS 2.0 (TIN2009-13391-C04-04) from theSpanish Ministry of Science and Innovation,and by a FPU grant (AP2007-01028) from theSpanish Ministry of Education.ReferencesAbeille?, Anne, Lionel Cle?ment, andAlexandra Kinyon.
2000.
Building atreebank for French.
In Proceedings of theSecond International Language Resourcesand Evaluation (LREC?00), pages 87?94,Athens.Alexiadou, Artemis.
2001.
The FunctionalStructure in Nominals.
Nominalizationsand Ergativity.
John Benjamins,Amsterdam/Philadelphia, PA.Alonso, Margarita.
2004.
Las construccionescon verbos de apoyo.
Visor Libros, Madrid.Androutsopoulos, Ion and ProdromosMalakasiotis.
2010.
A survey ofparaphrasing and textual entailmentmethods.
Journal of Artificial IntelligenceResearch, 38:135?187.Aparicio, Juan, Mariona Taule?, andM.
Anto`nia Mart??.
2008.
AnCora-Verb:A lexical resource for the semanticannotation of corpora.
In Proceedingsof the Sixth International LanguageResources and Evaluation (LREC?08),pages 797?802, Marrakech.Badia, Toni.
2002.
Els complementsnominals.
In Joan Sola`, editor, Grama`ticadel Catala` Contemporani, volume 3.Empu?ries, Barcelona, pages 1591?1640.Baker, Collin F., Charles J. Fillmore, andJohn B. Lowe.
1998.
The BerkeleyFrameNet Project.
In Proceedings of the36th Annual Meeting of the Associationfor Computational Linguistics and 17thInternational Conference on ComputationalLinguistics - Volume 1, ACL?98,pages 86?90, Stroudsburg, PA.Balvet, Antonio, Lucie Barque, andRafael Mar??n.
2010.
Building a lexicon ofFrench deverbal nouns from a semanticallyannotated corpus.
In Proceedings of theSeventh Conference on International LanguageResources and Evaluation (LREC?10),pages 1408?1413, Valletta.Bos, Johan.
2008.
Wide-coverage semanticanalysis with Boxer.
In Semantics inText Processing.
STEP 2008 ConferenceProceedings, pages 277?286, Venice.Butnariu, Cristina, Su Nam Kim,Preslav Nakov, Diarmuid O?
Se?aghdha,Stan Szpakowicz, and Tony Veale.2010a.
SemEval-2010 Task 9: Theinterpretation of noun compounds usingparaphrasing verbs and prepositions.In Proceedings of the Workshop on SemanticEvaluations: Recent Achievementsand Future Directions (SEW-2010),pages 100?105, Boulder, CO.Butnariu, Cristina, Su Nam Kim, PreslavNakov, Diarmuid O?
Se?aghdha, StanSzpakowicz, and Tony Veale.
2010b.SemEval-2010 Task 9: The interpretationof noun compounds using paraphrasingverbs and prepositions.
In Proceedings ofthe 5th International Workshop on SemanticEvaluation, pages 39?44, Uppsala.Chklovski, Timothy and Rada Mihalcea.2002.
Building a sense tagged corpus withopen mind word expert.
In Proceedingsof the ACL-02 Workshop on Word SenseDisambiguation: Recent Successes andFuture Directions - Volume 8, WSD ?02,pages 116?122, Stroudsburg, PA.Civit, Montserrat and Maria Anto`nia Mart??.2004.
Building Cast3LB: A Spanish862Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in NominalizationsTreebank.
Research on Language andComputation, 2(4):549?574.Clark, Herbert H. 1975.
Bridging.
InProceedings of the 1975 Workshop onTheoretical Issues in Natural LanguageProcessing, TINLAP ?75, pages 169?174,Stroudsburg, PA.Copestake, Ann.
2007.
Semantic compositionwith (Robust) Minimal RecursionSemantics.
In Proceedings of the Workshopon Deep Linguistic Processing, DeepLP ?07,pages 73?80, Prague.Creswell, Cassandre, Matthew J. Beal,John Chen, Thomas L. Cornell, LarsNilsson, and Rohini K. Srihari.
2006.Automatically extracting nominalmentions of events with a bootstrappedprobabilistic classifier.
In Proceedings ofthe Computational Linguistics/Associationfor Computational Linguistics on MainConference Poster Sessions, COLING-ACL?06, pages 168?175, Stroudsburg, PA.Decadt, Bart, Ve?ronique Hoste, WalterDaelemans, and Antal Van Den Bosch.2004.
GAMBL, genetic algorithmoptimization of memory-based WSD.In Proceedings of the Association ofComputational Linguistics/SIGLEXSenseval-3, pages 108?112,Stroudsburg, PA.Dowty, David.
1979.Word Meaning andMontague Grammar.
Reidel, Dordrecht.Eberle, Kurt, Gertrud Faasz, and UlrichHeid.
2009.
Corpus-based identificationand disambiguation of reading indicatorsin German nominalizations.
In OnlineProceedings of the 5th Corpus LinguisticsConference.
Available at ucrel.lancs.ac.uk/publications/cl2009/.Eberle, Kurt, Gertrud Faasz, and Heid Ulrich.2011.
Approximating the disambiguationof some German nominalizations by useof weak structural, lexical, and corpusinformation.
Procesamiento del LenguajeNatural, 46:67?75.Eberle, Kurt, Ulrich Heid, Manuel Kountz,and Kerstin Eckart.
2008.
A tool for corpusanalysis using partial disambiguation andbootstrapping of the lexicon.
In TextResources and Lexical Knowledge: SelectedPapers from the 9th Conference on NaturalLanguage Processing KONVENS 2008,pages 145?158, Konvens.Erk, Katrin and Sebastian Pado?.
2006.Shalmaneser: A flexible toolbox forsemantic role assignment.
In Proceedingsof the Fifth International LanguageResources and Evaluation (LREC?06),pages 527?532, Genoa.Fellbaum, Christiane.
1998.
An ElectronicLexical Database.
The MIT Press,Cambridge, MA.Gerber, Matthew and Joyce Y. Chai.
2010.Beyond NomBank: A study of implicitargumentation for nominal predicates.In Proceedings of the ACL Conference 2010,pages 1583?1592, Uppsala.Gildea, Daniel and Martha Palmer.
2002.The necessity of parsing for predicateargument recognition.
In Proceedingsof the 40th Annual Meeting on Associationfor Computational Linguistics, ACL ?02,pages 239?246, Stroudsburg, PA.Girju, Roxana, Dan Moldovan, Marta Tatu,and Daniel Antohe.
2005.
On the semanticsof noun compounds.
Computer, Speech andLanguage, 19(4):479?496.Girju, Roxana, Preslav Nakov, Vivi Nastase,Stan Szpakowicz, Peter D. Turney, andDeniz Yuret.
2009.
Classification ofsemantic relations between nominals.Language Resources and Evaluation,43(2):105?121.Grimshaw, Jane.
1990.
Argument Structure.The MIT Press, Cambridge, MA.Gurevich, Olga, Richard Crouch, TracyHolloway King, and Valeria De Paiva.2006.
Deverbal nouns in knowledgerepresentation.
In Proceedings of FloridaArtificial Intelligence Research SocietyConference, pages 670?675, Florida.Gurevich, Olga and Scott Waterman.
2009.Mapping verbal argument preferencesto deverbals.
In Proceedings of the 2009IEEE International Conference on SemanticComputing, pages 17?24, Washington, DC.Hendrickx, Iris, Su Nam Kim, ZornitsaKozareva, Preslav Nakov, DiarmuidO?
Se?aghdha, Sebastian Pado?, MarcoPennacchiotti, Lorenza Romano, andStan Szpakowicz.
2010.
SemEval-2010Task 8: Multi-way classification ofsemantic relations between pairs ofnominals.
In Proceedings of the 5thInternational Workshop on SemanticEvaluation, pages 33?38, Uppsala.Hovy, Eduard, Mitchell Marcus, MarthaPalmer, Lance Ramshaw, and RalphWeischedel.
2006.
OntoNotes: The 90%solution.
In Proceedings of Human LanguageTechnologies?North American Chapter of theAssociation of Computational Linguistics(HLT-NAACL?
06), pages 57?60,New York, NY.Hull, Richard D. and Fernando Gomez.2000.
Semantic interpretation of deverbalnominalizations.
Natural LanguageEngineering, 6(2):139?161.863Computational Linguistics Volume 38, Number 4Kipper, K., A. Korhonen, N. Ryant, andM.
Palmer.
2006.
Extending VerbNetwith novel verb classes.
In Proceedings ofthe 5th International Conference on LanguageResources and Evaluation, pages 1027?1032,Genova.Lapata, Maria.
2002.
The disambiguation ofnominalizations.
Computational Linguistics,28(3):357?388.Levi, Judith N. 1978.
The Syntax and Semanticsof Complex Nominals.
Academic Press Inc.,New York.Liu, Chang and Hwee Tou Ng.
2007.Learning predictive structures forsemantic role labeling of NomBank.In Proceedings of the 45th Annual Meetingof the Association of ComputationalLinguistics, pages 208?215, Prague.Madnani, Nitin and Bonnie J. Dorr.
2010.Generating phrasal and sententialparaphrases: A survey of data-drivenmethods.
Computational Linguistics,36(3):341?387.Ma?rquez, Llu?
?s, Xavier Carreras, Kenneth C.Litkowski, and Suzanne Stevenson.
2008.Semantic role labeling: An introduction tothe special issue.
Computational Linguistics,34(2):145?159.McLachlan, G. J., K. A.
Do, and C. Ambroise.2004.
Analyzing Microarray Gene ExpressionData.
Wiley, Hoboken, NJ.Mechura, Michal.
2008.
Selectional Preferences,Corpora and Ontologies.
Ph.D. thesis, TrinityCollege, University of Dublin.Meyers, Adam, Ruth Reeves, and CatherineMacleod.
2004.
NP-external arguments:A study of argument sharing in English.In Proceedings of the Workshop on MultiwordExpressions: Integrating Processing(MWE?04), pages 96?103, Stroudsburg, PA.Mooney, Raymond J.
2007.
Learning forsemantic parsing.
In ComputationalLinguistics and Intelligent Text Processing:Proceedings of the 8th InternationalConference (CICLing 2007) (invitedpaper), pages 311?324, Berlin.Navigli, Roberto.
2009.
Word sensedisambiguation: A survey.
ACMComputing Surveys, 41(2):1?69.Nunes, Mary L. 1993.
Argument linkingin English derived nominals.
In RobertD.
Van Valin, editor, Advances in RoleReference Grammar.
John Benjamins,Amsterdam/Philadelphia, pages 375?432.Pado?, Sebastian, Marco Pennacchiotti,and Caroline Sporleder.
2008.
Semanticrole assignment for event nominalisationsby leveraging verbal data.
In Proceedingsof the 22nd International Conference onComputational Linguistics?Volume 1,COLING ?08, pages 665?672,Stroudsburg, PA.Palmer, Martha, Daniel Gildea, andNianwen Xue.
2010.
Semantic RoleLabeling.
Synthesis on Human LanguagesTechnologies.Morgan & ClaypoolPublishers, Toronto.Palmer, Martha, Paul Kingsbury, andDaniel Gildea.
2005.
The proposition bank:An annotated corpus of semantic roles.Computational Linguistics, 31(1):76?105.Peris, Aina and Mariona Taule?.
2011.AnCora-Nom: A Spanish lexicon ofdeverbal nominalizations.
Procesamientodel Lenguaje Natural, 46:11?19.Peris, Aina, Mariona Taule?, GemmaBoleda, and Horacio Rodr??guez.
2010.ADN-Classifier: Automatically assigningdenotation types to nominalizations.
InProceedings of the Language Resources andEvaluation Conference, pages 1422?1428,Valleta.Peris, Aina, Mariona Taule?, and HoracioRodr??guez.
2009.
Hacia un sistema declasificacio?n automa?tica de sustantivosdeverbales.
Procesamiento del LenguajeNatural, 43:23?31.Peris, Aina, Mariona Taule?, and HoracioRodr??guez.
2010.
Semantic annotation ofdeverbal nominalizations in the SpanishAnCora corpus.
In Proceedings of the NinthInternational Workshop on Treebanks andLinguistic Theories, pages 187?198, Tartu.Picallo, Carme.
1999.
La estructura delSintagma Nominal: las nominalizacionesy otros sustantivos con complementosargumentales.
In Ignacio Bosque andVioleta Demonte, editors, Grama?ticaDescriptiva de la Lengua Espan?ola,volume 1.
Espasa Calpe, Madrid,pages 363?393.Pradhan, Sameer, Honglin Sun, Wayne Ward,James H. Martin, and Dan Jurafsky.
2004.Parsing arguments of nominalizationsin English and Chinese.
In Proceedingsof Human Language Technologies?North American Chapter of the Association ofComputational Linguistics (HLT-NAACL?
04,pages 208?215, Boston, MA.Pradhan, Sameer S., Edward Loper,Dmitriy Dligach, and Martha Palmer.2007.
Semeval-2007 task 17: English lexicalsample, SRL and all words.
In Proceedingsof the 4th International Workshop on SemanticEvaluations, SemEval ?07, pages 87?92,Stroudsburg, PA.Pustejovsky, James.
1995.
The GenerativeLexicon.
The MIT Press, Cambridge, MA.864Peris, Taule?, and Rodr?
?guez Empirical Methods for the Study of Denotation in NominalizationsQuinlan, J. Ross.
1993.
C4.5: Programs forMachine Learning.
Morgan Kaufmann,San Mateo, CA.Recasens, Marta, M. Anto`nia Mart?
?, andMariona Taule?.
2007.
Text as scene:Discourse deixis and bridging relations.Revista de la Asociacio?n Espan?ola para elProcesamiento del Lenguaje Natural, 39,pages 205?212.Recasens, Marta and Marta Vila.
2010.
Onparaphrase and coreference.
ComputationalLinguistics, 36(4):639?647.Ruppenhofer, Josef, Michael Ellsworth,Miriam R. L. Petruck, Christopher R.Johnson, and Jan Scheffczyk.
2006.FrameNet II: Extended theoryand practice.
Technical report,International Computer ScienceInstitute, Berkeley, CA.
Available atframenet.icsi.berkeley.edu/book/book.pdf.Ruppenhofer, Josef, Caroline Sporleder,Roser Morante, Collin Baker, andMartha Palmer.
2010.
SemEval-2010Task 10: Linking events and theirparticipants in discourse.
In Proceedingsof the 5th International Workshop onSemantic Evaluation, pages 296?299,Uppsala.Santiago, Ramo?n and Enrique Bustos.
1999.La derivacio?n nominal.
In Ignacio Bosqueand Violeta Demonte, editors, Grama?ticaDescriptiva de la Lengua Espan?ola, volume 3.Espasa Calpe, Madrid, pages 4505?4594.Spencer, Andrew and Marina Zaretskaya.1999.
The Essex database of Russianverbs and their nominalizations.Technical report, University of Essex,Colchester.Surdeanu, Mihai, Richard Johansson,Adam Meyers, Llu?
?s Ma`rquez, andJoakim Nivre.
2008.
The CoNLL-2008shared task on joint parsing of syntacticand semantic dependencies.
In Proceedingsof the Twelfth Conference on ComputationalNatural Language Learning, CoNLL ?08,pages 159?177, Stroudsburg, PA.Taule?, Mariona, M. Anto?nia Mart?
?, andMarta Recasens.
2008.
AnCora: Multilevelannotated corpora for Catalan andSpanish.
In Proceedings of the SixthInternational Language Resources andEvaluation (LREC?08), pages 96 ?101,Marrakech.Va?zquez, Glo`ria, Ana Ferna?ndez, andMaria Anto`nia Mart??.
2000.
Clasificacio?nVerbal.
Alternancias de Dia?tesis.
Quadernsde Sintagma, 3, Edicions de la Universitatde Lleida, Lerida, Spain.Vendler, Zeno.
1967.
Linguistics in Philosophy.Cornell University Press, Ithaca, NY.Witten, Ian H. and Eibe Frank.
2005.
DataMining: Practical Machine LearningTools and Techniques.
Morgan Kaufmann,San Francisco, CA.Zubizarreta, Maria Luisa.
1987.
Levels ofRepresentation in the Lexicon and in theSyntax.
Foris, Dordrect.865
