Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1241?1251,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsApproximation Strategies for Multi-Structure Sentence CompressionKapil ThadaniDepartment of Computer ScienceColumbia UniversityNew York, NY 10025, USAkapil@cs.columbia.eduAbstractSentence compression has been shown tobenefit from joint inference involving bothn-gram and dependency-factored objec-tives but this typically requires expensiveinteger programming.
We explore insteadthe use of Lagrangian relaxation to decou-ple the two subproblems and solve themseparately.
While dynamic programmingis viable for bigram-based sentence com-pression, finding optimal compressed treeswithin graphs is NP-hard.
We recover ap-proximate solutions to this problem us-ing LP relaxation and maximum spanningtree algorithms, yielding techniques thatcan be combined with the efficient bigram-based inference approach using Lagrangemultipliers.
Experiments show that theseapproximation strategies produce resultscomparable to a state-of-the-art integerlinear programming formulation for thesame joint inference task along with a sig-nificant improvement in runtime.1 IntroductionSentence compression is a text-to-text genera-tion task in which an input sentence must betransformed into a shorter output sentence whichaccurately reflects the meaning in the inputand also remains grammatically well-formed.The compression task has received increasingattention in recent years, in part due to theavailability of datasets such as the Ziff-Davis cor-pus (Knight and Marcu, 2000) and the Edinburghcompression corpora (Clarke and Lapata, 2006),from which the following example is drawn.Original: In 1967 Chapman, who had cultivated a con-ventional image with his ubiquitous tweed jacket and pipe,by his own later admission stunned a party attended by hisfriends and future Python colleagues by coming out as ahomosexual.Compressed: In 1967 Chapman, who had cultivated aconventional image, stunned a party by coming out as ahomosexual.Following an assumption often used in compres-sion systems, the compressed output in this corpusis constructed by dropping tokens from the inputsentence without any paraphrasing or reordering.1A number of diverse approaches have beenproposed for deletion-based sentence compres-sion, including techniques that assemble the out-put text under an n-gram factorization over theinput text (McDonald, 2006; Clarke and Lapata,2008) or an arc factorization over input depen-dency parses (Filippova and Strube, 2008; Galanisand Androutsopoulos, 2010; Filippova and Altun,2013).
Joint methods have also been proposed thatinvoke integer linear programming (ILP) formu-lations to simultaneously consider multiple struc-tural inference problems?both over n-grams andinput dependencies (Martins and Smith, 2009) orn-grams and all possible dependencies (Thadaniand McKeown, 2013).
However, it is well-established that the utility of ILP for optimal infer-ence in structured problems is often outweighedby the worst-case performance of ILP solverson large problems without unique integral solu-tions.
Furthermore, approximate solutions canoften be adequate for real-world generation sys-tems, particularly in the presence of linguistically-motivated constraints such as those described byClarke and Lapata (2008), or domain-specific1This is referred to as extractive compression by Cohn andLapata (2008) & Galanis and Androutsopoulos (2010) fol-lowing the terminology used in document summarization.1241pruning strategies such as the use of sentence tem-plates to constrain the output.In this work, we develop approximate inferencestrategies to the joint approach of Thadani andMcKeown (2013) which trade the optimality guar-antees of exact ILP for faster inference by sep-arately solving the n-gram and dependency sub-problems and using Lagrange multipliers to en-force consistency between their solutions.
How-ever, while the former problem can be solvedefficiently using the dynamic programming ap-proach of McDonald (2006), there are no efficientalgorithms to recover maximum weighted non-projective subtrees in a general directed graph.Maximum spanning tree algorithms, commonlyused in non-projective dependency parsing (Mc-Donald et al, 2005), are not easily adaptable tothis task since the maximum-weight subtree is notnecessarily a part of the maximum spanning tree.We therefore consider methods to recover ap-proximate solutions for the subproblem of findingthe maximum weighted subtree in a graph, com-mon among which is the use of a linear program-ming relaxation.
This linear program (LP) ap-pears empirically tight for compression problemsand our experiments indicate that simply using thenon-integral solutions of this LP in Lagrangian re-laxation can empirically lead to reasonable com-pressions.
In addition, we can recover approxi-mate solutions to this problem by using the Chu-Liu Edmonds algorithm for recovering maximumspanning trees (Chu and Liu, 1965; Edmonds,1967) over the relatively sparse subgraph definedby a solution to the relaxed LP.
Our proposed ap-proximation strategies are evaluated using auto-mated metrics in order to address the question: un-der what conditions should a real-world sentencecompression system implementation consider ex-act inference with an ILP or approximate infer-ence?
The contributions of this work include:?
An empirically-useful technique for approx-imating the maximum-weight subtree in aweighted graph using LP-relaxed inference.?
Multiple approaches to generate good ap-proximate solutions for joint multi-structurecompression, based on Lagrangian relaxationto enforce equality between the sequentialand syntactic inference subproblems.?
An analysis of the tradeoffs incurred by jointapproaches with regard to runtime as well asperformance under automated measures.2 Multi-Structure Sentence CompressionEven though compression is typically formulatedas a token deletion task, it is evident that drop-ping tokens independently from an input sentencewill likely not result in fluent and meaningful com-pressive text.
Tokens in well-formed sentencesparticipate in a number of syntactic and seman-tic relationships with other tokens, so one mightexpect that accounting for heterogenous structuralrelationships between tokens will improve the co-herence of the output sentence.
Furthermore,much recent work has focused on the challengeof joint sentence extraction and compression, alsoknown as compressive summarization (Martinsand Smith, 2009; Berg-Kirkpatrick et al, 2011;Almeida and Martins, 2013; Li et al, 2013; Qianand Liu, 2013), in which questions of efficiencyare paramount due to the larger problems in-volved; however, these approaches largely restrictcompression to pruning parse trees, thereby im-posing a dependency on parser performance.
Wefocus in this work on a sentence-level compressionsystem to approximate the ILP-based inference ofThadani and McKeown (2013) which does not re-strict compressions to follow input parses but per-mits the generation of novel dependency relationsin output compressions.The rest of this section is organized as fol-lows: ?2.1 provies an overview of the joint se-quential and syntactic objective for compressionfrom Thadani and McKeown (2013) while ?2.2discusses the use of Lagrange multipliers to en-force consistency between the different structuresconsidered.
Following this, ?2.3 discusses a dy-namic program to find maximum weight bigramsubsequences from the input sentence, while ?2.4covers LP relaxation-based approaches for ap-proximating solutions to the problem of finding amaximum-weight subtree in a graph of potentialoutput dependencies.
Finally, ?2.5 discusses thefeatures and model training approach used in ourexperimental results which are presented in ?3.2.1 Joint objectiveWe begin with some notation.
For an input sen-tence S comprised of n tokens including dupli-cates, we denote the set of tokens in S by T ,{ti: 1 ?
i ?
n}.
Let C represent a compres-sion of S and let xi?
{0, 1} denote an indicatorvariable whose value corresponds to whether to-ken ti?
T is present in the compressed sentence1242C.
In addition, we define bigram indicator vari-ables yij?
{0, 1} to represent whether a particularorder-preserving bigram2?ti, tj?
from S is presentas a contiguous bigram inC as well as dependencyindicator variables zij?
{0, 1} corresponding towhether the dependency arc ti?
tjis present inthe dependency parse of C. The score for a givencompression C can now be defined to factor overits tokens, n-grams and dependencies as follows.score(C) =?ti?Txi?
?tok(ti)+?ti?T?{START},tj?T?{END}yij?
?bgr(?ti, tj?)+?ti?T?{ROOT},tj?Tzij?
?dep(ti?
tj) (1)where ?tok, ?bgrand ?depare feature-based scoringfunctions for tokens, bigrams and dependenciesrespectively.
Specifically, each ?v(?)
?
w>v?v(?
)where ?v(?)
is a feature map for a given vari-able type v ?
{tok, bgr, dep} and wvis the cor-responding vector of learned parameters.The inference task involves recovering the high-est scoring compression C?under a particular setof model parameters w.C?= argmaxCscore(C)= argmaxx,y,zx>?tok+ y>?bgr+ z>?dep(2)where the incidence vector x , ?xi?ti?Trepre-sents an entire token configuration over T , with yand z defined analogously to represent configura-tions of bigrams and dependencies.
?v, ??v(?
)?denotes a corresponding vector of scores for eachvariable type v under the current model parame-ters.
In order to recover meaningful compressionsby optimizing (2), the inference step must ensure:1.
The configurations x, y and z are consistentwith each other, i.e., all configurations coverthe same tokens.2.
The structural configurations y and z arenon-degenerate, i.e, the bigram configurationy represents an acyclic path while the depen-dency configuration z forms a tree.2Although Thadani and McKeown (2013) is not restrictedto bigrams or order-preserving n-grams, we limit our discus-sion to this scenario as it also fits the assumptions of McDon-ald (2006) and the datasets of Clarke and Lapata (2006).These requirements naturally rule out simple ap-proximate inference formulations such as search-based approaches for the joint objective.3AnILP-based inference solution is demonstrated inThadani and McKeown (2013) that makes use oflinear constraints over the boolean variables xi, yijand zijto guarantee consistency, as well as aux-iliary real-valued variables and constraints repre-senting the flow of commodities (Magnanti andWolsey, 1994) in order to establish structure in yand z.
In the following section, we propose an al-ternative formulation that exploits the modularityof this joint objective.2.2 Lagrangian relaxationDual decomposition (Komodakis et al, 2007) andLagrangian relaxation in general are often usedfor solving joint inference problems which aredecomposable into individual subproblems linkedby equality constraints (Koo et al, 2010; Rushet al, 2010; Rush and Collins, 2011; DeNeroand Macherey, 2011; Martins et al, 2011; Daset al, 2012; Almeida and Martins, 2013).
Thisapproach permits sub-problems to be solved sepa-rately using problem-specific efficient algorithms,while consistency over the structures produced isenforced through Lagrange multipliers via itera-tive optimization.
Exact solutions are guaranteedwhen the algorithm converges on a consistent pri-mal solution, although this convergence itself isnot guaranteed and depends on the tightness ofthe underlying LP relaxation.
The primary advan-tage of this technique is the ability to leverage theunderlying structure of the problems in inferencerather than relying on a generic ILP formulationwhile still often producing exact solutions.The multi-structure inference problem de-scribed in the previous section seems in manyways to be a natural fit to such an approach sinceoutput scores factor over different types of struc-ture that comprise the output compression.
Even ifILP-based approaches perform reasonably at thescale of single-sentence compression problems,the exponential worst-case complexity of general-purpose ILPs will inevitably pose challenges whenscaling up to (a) handle larger inputs, (b) usehigher-order structural fragments, or (c) incorpo-rate additional models.3This work follows Thadani and McKeown (2013) in re-covering non-projective trees for inference.
However, recov-ering projective trees is tractable when a total ordering of out-put tokens is assumed.
This will be addressed in future work.1243Consider once more the optimization problemcharacterized by (2) The two structural problemsthat need to be solved in this formulation arethe extraction of a maximum-weight acyclic sub-sequence of bigrams y from the lattice of allorder-preserving bigrams from S and the recov-ery of a maximum-weight directed subtree z.
Let?
(y) ?
{0, 1}ndenote the incidence vector oftokens contained in the n-gram sequence y and?
(z) ?
{0, 1}ndenote the incidence vector ofwords contained in the dependency tree z.
We cannow rewrite the objective in (2) while enforcingthe constraint that the words contained in the se-quence y are the same as the words contained inthe tree z, i.e., ?
(y) = ?
(z), by introducing avector of Lagrange multipliers ?
?
Rn.
In addi-tion, the token configuration x can be rewritten inthe form of a weighted combination of ?
(y) and?
(z) to ensure its consistency with y and z. Thisresults in the following Lagrangian:L(?,y, z) = y>?bgr+ z>?dep+ ?>tok(?
??
(y) + (1?
?)
?
?
(z))+ ?>(?(y)?
?
(z)) (3)Finding the y and z that maximize this Lagrangianabove yields a dual objective, and the dual prob-lem corresponding to the primal objective speci-fied in (2) is therefore the minimization of this ob-jective over the Lagrange multipliers ?.min?maxy,zL(?,y, z)=min?maxyy>?bgr+ (?+ ?
?
?tok)>?
(y)+ maxzz>?dep?
(?+ (?
?
1) ?
?tok)>?
(z)=min?maxyf(y,?, ?,?
)+ maxzg(z,?, ?,?)
(4)This can now be solved with the iterative subgra-dient algorithm illustrated in Algorithm 1.
In eachiteration i, the algorithm solves for y(i)and z(i)under ?
(i), then generates ?
(i+1)to penalize in-consistencies between ?
(y(i)) and ?(z(i)).
When?
(y(i)) = ?
(z(i)), the resulting primal solution isexact, i.e., y(i)and z(i)represent the optimal struc-tures under (2).
Otherwise, if the algorithm startsoscillating between a few primal solutions, the un-derlying LP must have a non-integral solution inwhich case approximation heuristics can be em-Algorithm 1 Subgradient-based joint inferenceInput: scores ?, ratio ?, repetition limit lmax,iteration limit imax, learning rate schedule ?Output: token configuration x1: ?(0)?
?0?n2: M ?
?,Mrepeats?
?3: for iteration i < imaxdo4:?y?
argmaxyf(y,?, ?,?
)5:?z ?
argmaxzg(z,?, ?,?
)6: if ?
(?y) = ?
(?z) then return ?
(?y)7: if ?
(?y) ?M then8: Mrepeats?Mrepeats?
{?
(?y)}9: if ?
(?z) ?M then10: Mrepeats?Mrepeats?
{?
(?z)}11: if |Mrepeats| ?
lmaxthen break12: M ?M ?
{?(?y),?
(?z)}13: ?(i+1)?
?(i)?
?i(?(?y)?
?
(?z))return argmaxx?Mrepeatsscore(x)ployed.4The application of this Lagrangian relax-ation strategy is contingent upon the existence ofalgorithms to solve the maximization subproblemsfor f(y,?, ?,?)
and g(z,?, ?,?).
The followingsections discuss our approach to these problems.2.3 Bigram subsequencesMcDonald (2006) provides a Viterbi-like dynamicprogramming algorithm to recover the highest-scoring sequence of order-preserving bigramsfrom a lattice, either in unconstrained form or witha specific length constraint.
The latter requires adynamic programming table Q[i][r] which repre-sents the best score for a compression of length rending at token i.
The table can be populated us-ing the following recurrence:Q[i][1] = score(S, START, i)Q[i][r] = maxj<iQ[j][r ?
1] + score(S, i, j)Q[i][R+ 1] = Q[i][R] + score(S, i, END)where R is the required number of output tokensand the scoring function is defined asscore(S, i, j) , ?bgr(?ti, tj?)
+ ?j+ ?
?
?tok(tj)so as to solve f(y,?, ?,?)
from (4).
This ap-proach requires O(n2R) time in order to identify4Heuristic approaches (Komodakis et al, 2007; Rush etal., 2010), tightening (Rush and Collins, 2011) or branch andbound (Das et al, 2012) can still be used to retrieve optimalsolutions, but we did not explore these strategies here.1244ABC D-20310 21Figure 1: An example of the difficulty of recover-ing the maximum-weight subtree (B?C, B?D)from the maximum spanning tree (A?C, C?B,B?D).the highest scoring sequence y and correspondingtoken configuration ?
(y).2.4 Dependency subtreesThe maximum-weight non-projective subtreeproblem over general graphs is not as easilysolved.
Although the maximum spanning tree fora given token configuration can be recovered ef-ficiently, Figure 1 illustrates that the maximum-scoring subtree is not necessarily found withinit.
The problem of recovering a maximum-weightsubtree in a graph has been shown to be NP-hardeven with uniform edge weights (Lau et al, 2006).In order to produce a solution to this subprob-lem, we use an LP relaxation of the relevantportion of the ILP from Thadani and McKeown(2013) by omitting integer constraints over the to-ken and dependency variables in x and z respec-tively.
For simplicity, however, we describe theILP version rather than the relaxed LP in order tomotivate the constraints with their intended pur-pose rather than their effect in the relaxed prob-lem.
The objective for this LP is given bymaxx,zx>?
?tok+ z>?dep(5)where the vector of token scores is redefined as?
?tok, (1?
?)
?
?tok?
?
(6)in order to solve g(z,?, ?,?)
from (4).Linear constraints are introduced to produce de-pendency structures that are close to the optimaldependency trees.
First, tokens in the solutionmust only be active if they have a single active in-coming dependency edge.
In addition, to avoidproducing multiple disconnected subtrees, onlyone dependency is permitted to attach to the ROOTpseudo-token.xj?
?izij= 0, ?tj?
T (7)?jzij= 1, if ti= ROOT (8)ROOTProduction was closed down at Ford last night .5?3,1= 121?3,9= 1Figure 2: An illustration of commodity values fora valid solution of the non-relaxed ILP.In order to avoid cycles in the dependency tree,we include additional variables to establish single-commodity flow (Magnanti and Wolsey, 1994) be-tween all pairs of tokens.
These ?ijvariables carrynon-negative real values which must be consumedby active tokens that they are incident to.?ij?
0, ?ti, tj?
T (9)?i?ij?
?k?jk= xj, ?tj?
T (10)These constraints ensure that cyclic structures arenot possible in the non-relaxed ILP.
In addition,they serve to establish connectivity for the de-pendency structure z since commodity can onlyoriginate in one location?at the pseudo-tokenROOT which has no incoming commodity vari-ables.
However, in order to enforce these prop-erties on the output dependency structure, thisacyclic, connected commodity structure must con-strain the activation of the z variables.?ij?
Cmaxzij?
0, ?ti, tj?
T (11)where Cmaxis an arbitrary upper bound on thevalue of ?ijvariables.
Figure 2 illustrates howthese commodity flow variables constrain the out-put of the ILP to be a tree.
However, the effectof these constraints is diminished when solving anLP relaxation of the above problem.In the LP relaxation, xiand zijare redefined asreal-valued variables in [0, 1], potentially resultingin fractional values for dependency and token indi-cators.
As a result, the commodity flow network isable to establish connectivity but cannot enforce atree structure, for instance, directed acyclic struc-tures are possible and token indicators ximay bepartially be assigned to the solution structure.
Thisposes a challenge in implementing ?
(z) which isneeded to recover a token configuration from thesolution of this subproblem.We propose two alternative solutions to addressthis issue in the context of the joint inference strat-egy.
The first is to simply use the relaxed tokenconfiguration identified by the LP in Algorithm 1,1245i.e., to set ?(z?)
= x?
where x?
and z?
represent thereal-valued counterparts of the incidence vectors xand z.
The viability of this approximation strategyis due to the following:?
The relaxed LP is empirically fairly tight,yielding integral solutions 89% of the time onthe compression datasets described in ?3.?
The bigram subproblem is guaranteed to re-turn a well-formed integral solution whichobeys the imposed compression rate, so weare assured of a source of valid?if non-optimal?solutions in line 13 of Algorithm 1.We also consider another strategy that attempts toapproximate a valid integral solution to the depen-dency subproblem.
In order to do this, we firstinclude an additional constraint in the relaxed LPwhich restrict the number of tokens in the outputto a specific number of tokens R that is given byan input compression rate.
?ixi= R (12)The addition of this constraint to the relaxed LPreduces the rate of integral solutions drastically?from 89% to approximately 33%?but it serves toensure that the resulting token configuration x?
hasat least as many non-zero elements asR, i.e., thereare at least as many tokens activated in the LP so-lution as are required in a valid solution.We then construct a subgraph G(z?)
consistingof all dependency edges that were assigned non-zero values in the solution, assigning to each edgea score equal to the score of that edge in the LP aswell as the score of its dependent word, i.e., eachzijin G(z?)
is assigned a score of ?dep(?ti, tj?)
?
?j+ (1?
?)
?
?tok(tj).
Since the commodity flowconstraints in (9)?
(11) ensure a connected z?, it istherefore possible to recover a maximum-weightspanning tree from G(z?)
using the Chu-Liu Ed-monds algorithm (Chu and Liu, 1965; Edmonds,1967).5Although the runtime of this algorithmis cubic in the size of the input graph, it is fairlyspeedy when applied on relatively sparse graphssuch as the solutions to the LP described above.The resulting spanning tree is a useful integralapproximation of z?
but, as mentioned previously,may contain more nodes than R due to fractionalvalues in x?
; we therefore repeatedly prune leaves5A detailed description of the Chu-Liu Edmonds algo-rithm for MSTs is available in McDonald et al (2005).with the lowest incoming edge weight in the cur-rent tree until exactly R nodes remain.
The result-ing tree is assumed to be a reasonable approxima-tion of the optimal integral solution to this LP.The Chu-Liu Edmonds algorithm is also em-ployed for another purpose: when the underly-ing LP for the joint inference problem is nottight?a frequent occurrence in our compressionexperiments?Algorithm 1 will not converge ona single primal solution and will instead oscillatebetween solutions that are close to the dual opti-mum.
We identify this phenomenon by countingrepeated solutions and, if they exceed some thresh-old lmaxwith at least one repeated solution fromeither subproblem, we terminate the update proce-dure for Lagrange multipliers and instead attemptto identify a good solution from the repeating onesby scoring them under (2).
It is straightforward torecover and score a bigram configuration y from atoken configuration ?(z).
However, scoring so-lutions produced by the dynamic program from?2.3 also requires the score over a correspondingparse tree; this can be recovered by constructinga dependency subgraph containing across only thetokens that are active in ?
(y) and retrieving themaximum spanning tree for that subgraph usingthe Chu-Liu Edmonds algorithm.2.5 Learning and FeaturesThe features used in this work are largely based onthe features from Thadani and McKeown (2013).?
?tokcontains features for part-of-speech(POS) tag sequences of length up to 3 aroundthe token, features for the dependency labelof the token conjoined with its POS, lexicalfeatures for verb stems and non-word sym-bols and morphological features that identifycapitalized sequences, negations and wordsin parentheses.?
?bgrcontains features for POS patterns in abigram, the labels of dependency edges in-cident to it, its likelihood under a Gigawordlanguage model (LM) and an indicator forwhether it is present in the input sentence.?
?depcontains features for the probability ofa dependency edge under a smoothed depen-dency grammar constructed from the PennTreebank and various conjunctions of the fol-lowing features: (a) whether the edge appearsas a dependency or ancestral relation in theinput parse (b) the directionality of the depen-1246dency (c) the label of the edge (d) the POStags of the tokens incident to the edge and(e) the labels of their surrounding chunks andwhether the edge remains within the chunk.For the experiments in the following section, wetrained models using a variant of the structuredperceptron (Collins, 2002) which incorporatesminibatches (Zhao and Huang, 2013) for easy par-allelization and faster convergence.6Overfittingwas avoided by averaging parameters and mon-itoring performance against a held-out develop-ment set during training.
All models were trainedusing variants of the ILP-based inference approachof Thadani and McKeown (2013).
We followedMartins et al (2009) in using LP-relaxed inferenceduring learning, assuming algorithmic separabil-ity (Kulesza and Pereira, 2007) for these problems.3 ExperimentsWe ran compression experiments over thenewswire (NW) and broadcast news transcription(BN) corpora compiled by Clarke and Lapata(2008) which contain gold compressions pro-duced by human annotators using only worddeletion.
The datasets were filtered to eliminateinstances with less than 2 and more than 110tokens for parser compatibility and divided intotraining/development/test sections following thesplits from Clarke and Lapata (2008), yielding953/63/603 instances for the NW corpus and880/78/404 for the BN corpus.
Gold dependencyparses were approximated by running the Stanforddependency parser7over reference compressions.Following evaluations in machine translationas well as previous work in sentence compres-sion (Unno et al, 2006; Clarke and Lapata, 2008;Martins and Smith, 2009; Napoles et al, 2011b;Thadani and McKeown, 2013), we evaluate sys-tem performance using F1metrics over n-gramsand dependency edges produced by parsing sys-tem output with RASP (Briscoe et al, 2006) andthe Stanford parser.
All ILPs and LPs were solvedusing Gurobi,8a high-performance commercial-grade solver.
Following a recent analysis of com-pression evaluations (Napoles et al, 2011b) whichrevealed a strong correlation between system com-pression rate and human judgments of compres-sion quality, we constrained all systems to produce6We used a minibatch size of 4 in all experiments.7http://nlp.stanford.edu/software/8http://www.gurobi.comcompressed output at a specific rate?determinedby the the gold compressions available for eachinstance?to ensure that the reported differencesbetween the systems under study are meaningful.3.1 SystemsWe report results over the following systemsgrouped into three categories of models: tokens +n-grams, tokens + dependencies, and joint models.?
3-LM: A reimplementation of the unsuper-vised ILP of Clarke and Lapata (2008) whichinfers order-preserving trigram variables pa-rameterized with log-likelihood under an LMand a significance score for token variablesinspired by Hori and Furui (2004), as well asvarious linguistically-motivated constraintsto encourage fluency in output compressions.?
DP: The bigram-based dynamic program ofMcDonald (2006) described in ?2.3.9?
LP?MST: An approximate inference ap-proach based on an LP relaxation of ILP-Dep.
As discussed in ?2.4, a maximum span-ning tree is recovered from the output of theLP and greedily pruned in order to generatea valid integral solution while observing theimposed compression rate.?
ILP-Dep: A version of the joint ILP ofThadani and McKeown (2013) without n-gram variables and corresponding features.?
DP+LP?MST: An approximate joint infer-ence approach based on Lagrangian relax-ation that uses DP for the maximum weightsubsequence problem and LP?MST for themaximum weight subtree problem.?
DP+LP: Another Lagrangian relaxation ap-proach that pairs DP with the non-integralsolutions from an LP relaxation of the maxi-mum weight subtree problem (cf.
?2.4).?
ILP-Joint: The full ILP from Thadani andMcKeown (2013), which provides an upperbound on the performance of the proposedapproximation strategies.The learning rate schedule for the Lagrangian re-laxation approaches was set as ?i, ?/(?
+ i),10while the hyperparameter ?
was tuned using the9For consistent comparisons with the other systems, ourreimplementation does not include the k-best inference strat-egy presented in McDonald (2006) for learning with MIRA.10?
was set to 100 for aggressive subgradient updates.1247Inference n-grams F1% Syntactic relations F1% Inferenceobjective technique n = 1 2 3 4 z Stanford RASP time (s)n-grams3-LM (CL08) 74.96 60.60 46.83 38.71 - 60.52 57.49 0.72DP (McD06) 78.80 66.04 52.67 42.39 - 63.28 57.89 0.01depsLP?MST 79.61 64.32 50.36 40.97 66.57 66.82 59.70 0.07ILP-Dep 80.02 65.99 52.42 43.07 72.43 67.63 60.78 0.16DP + LP?MST 79.50 66.75 53.48 44.33 64.63 67.69 60.94 0.24joint DP + LP 79.10 68.22 55.05 45.81 65.74 68.24 62.04 0.12ILP-Joint (TM13) 80.13 68.34 55.56 46.60 72.57 68.89 62.61 0.31Table 1: Experimental results for the BN corpus, averaged over 3 gold compressions per instance.
Allsystems were restricted to compress to the size of the median gold compression yielding an averagecompression rate of 77.26%.Inference n-grams F1% Syntactic relations F1% Inferenceobjective technique n = 1 2 3 4 z Stanford RASP time (s)n-grams3-LM (CL08) 66.66 51.59 39.33 30.55 - 50.76 49.57 1.22DP (McD06) 73.18 58.31 45.07 34.77 - 56.23 51.14 0.01depsLP?MST 73.32 55.12 41.18 31.44 61.01 58.37 52.57 0.12ILP-Dep 73.76 57.09 43.47 33.44 65.45 60.06 54.31 0.28DP + LP?MST 73.13 57.03 43.79 34.01 57.91 58.46 53.20 0.33joint DP + LP 72.06 59.83 47.39 37.72 58.13 58.97 53.78 0.21ILP-Joint (TM13) 74.00 59.90 47.22 37.01 65.65 61.29 56.24 0.60Table 2: Experimental results for the NW corpus with all systems compressing to the size of the goldcompression, yielding an average compression rate of 70.24%.
In both tables, bold entries show signifi-cant gains within a column under the paired t-test (p < 0.05) and Wilcoxon?s signed rank test (p < 0.01).development split of each corpus.113.2 ResultsTables 1 and 2 summarize the results from ourcompression experiments on the BN and NW cor-pora respectively.
Starting with the n-gram ap-proaches, the performance of 3-LM leads us toobserve that the gains of supervised learning faroutweigh the utility of higher-order n-gram factor-ization, which is also responsible for a significantincrease in wall-clock time.
In contrast, DP is anorder of magnitude faster than all other approachesstudied here although it is not competitive underparse-based measures such as RASP F1% whichis known to correlate with human judgments ofgrammaticality (Clarke and Lapata, 2006).We were surprised by the strong performanceof the dependency-based inference techniques,which yielded results that approached the jointmodel in both n-gram and parse-based measures.11We were surprised to observe that performance improvedsignificantly when ?
was set closer to 1, thereby emphasiz-ing token features in the dependency subproblem.
The finalvalues chosen were ?BN= 0.9 and ?NW= 0.8.The exact ILP-Dep approach halves the run-time of ILP-Joint to produce compressions thathave similar (although statistically distinguish-able) scores.
Approximating dependency-basedinference with LP?MST yields similar perfor-mance for a further halving of runtime; however,the performance of this approach is notably worse.Turning to the joint approaches, the strongperformance of ILP-Joint is expected; less sois the relatively high but yet practically reason-able runtime that it requires.
We note, how-ever, that these ILPs are solved using a highly-optimized commercial-grade solver that can uti-lize all CPU cores12while our approximationapproaches are implemented as single-processedPython code without significant effort toward op-timization.
Comparing the two approximationstrategies shows a clear performance advantagefor DP+LP over DP+LP?MST: the latter ap-proach entails slower inference due to the over-head of running the Chu-Liu Edmonds algorithmat every dual update, and furthermore, the error in-troduced by approximating an integral solution re-1216 cores in our experimental environment.1248sults in a significant decrease in dependency recall.In contrast, DP+LP directly optimizes the dualproblem by using the relaxed dependency solutionto update Lagrange multipliers and achieves thebest performance on parse-based F1outside of theslower ILP approaches.
Convergence rates alsovary for these two techniques: DP+LP has a lowerrate of empirical convergence (15% on BN and 4%on NW) when compared to DP+LP?MST (19%on BN and 6% on NW).Figure 3 shows the effect of input sentencelength on inference time and performance for ILP-Joint and DP+LP over the NW test corpus.13Thetiming results reveal that the approximation strat-egy is consistently faster than the ILP solver.
Thevariation in RASP F1% with input size indicatesthe viability of a hybrid approach which could bal-ance accuracy and speed by using ILP-Joint forsmaller problems and DP+LP for larger ones.4 Related WorkSentence compression is one of the better-studiedtext-to-text generation problems and has been ob-served to play a significant role in human summa-rization (Jing, 2000; Jing and McKeown, 2000).Most approaches to sentence compression are su-pervised (Knight and Marcu, 2002; Riezler etal., 2003; Turner and Charniak, 2005; McDon-ald, 2006; Unno et al, 2006; Galley and McK-eown, 2007; Nomoto, 2007; Cohn and Lapata,2009; Galanis and Androutsopoulos, 2010; Gan-itkevitch et al, 2011; Napoles et al, 2011a; Fil-ippova and Altun, 2013) following the release ofdatasets such as the Ziff-Davis corpus (Knight andMarcu, 2000) and the Edinburgh compression cor-pora (Clarke and Lapata, 2006; Clarke and Lap-ata, 2008), although unsupervised approaches?largely based on ILPs?have also received con-sideration (Clarke and Lapata, 2007; Clarke andLapata, 2008; Filippova and Strube, 2008).
Com-pression has also been used as a tool for documentsummarization (Daum?e and Marcu, 2002; Zajicet al, 2007; Clarke and Lapata, 2007; Martinsand Smith, 2009; Berg-Kirkpatrick et al, 2011;Woodsend and Lapata, 2012; Almeida and Mar-tins, 2013; Molina et al, 2013; Li et al, 2013;Qian and Liu, 2013), with recent work formulatingthe summarization task as joint sentence extrac-tion and compression and often employing ILP orLagrangian relaxation.
Monolingual compression13Similar results were observed for the BN test corpus.Figure 3: Effect of input size on (a) inference time,and (b) the corresponding difference in RASPF1% (ILP-Joint ?
DP+LP) on the NW corpus.also faces many obstacles common to decoding inmachine translation, and a number of approacheswhich have been proposed to combine phrasal andsyntactic models (Huang and Chiang, 2007; Rushand Collins, 2011) inter alia offer directions forfuture research into compression problems.5 ConclusionWe have presented approximate inference strate-gies to jointly compress sentences under bigramand dependency-factored objectives by exploitingthe modularity of the task and considering the twosubproblems in isolation.
Experiments show thatone of these approximation strategies produces re-sults comparable to a state-of-the-art integer linearprogram for the same joint inference task with a60% reduction in average inference time.AcknowledgmentsThe author is grateful to Alexander Rush for help-ful discussions and to the anonymous reviewersfor their comments.
This work was supportedby the Intelligence Advanced Research ProjectsActivity (IARPA) via Department of Interior Na-tional Business Center (DoI/NBC) contract num-ber D11PC20153.
The U.S. Government is autho-rized to reproduce and distribute reprints for Gov-ernmental purposes notwithstanding any copy-right annotation thereon.1414The views and conclusions contained herein are those ofthe authors and should not be interpreted as necessarily repre-senting the official policies or endorsements, either expressedor implied, of IARPA, DoI/NBC, or the U.S. Government.1249ReferencesMiguel Almeida and Andr?e F. T. Martins.
2013.
Fastand robust compressive summarization with dual de-composition and multi-task learning.
In Proceed-ings of ACL, pages 196?206, August.Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of ACL-HLT, pages 481?490.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Pro-ceedings of the ACL-COLING Interactive Presenta-tion Sessions.Yoeng-jin Chu and Tseng-hong Liu.
1965.
On theshortest arborescence of a directed graph.
ScienceSinica, 14:1396?1400.James Clarke and Mirella Lapata.
2006.
Modelsfor sentence compression: a comparison across do-mains, training requirements and evaluation mea-sures.
In Proceedings of ACL-COLING, pages 377?384.James Clarke and Mirella Lapata.
2007.
Modellingcompression with discourse constraints.
In Proceed-ings of EMNLP-CoNLL, pages 1?11.James Clarke and Mirella Lapata.
2008.
Global in-ference for sentence compression: an integer linearprogramming approach.
Journal for Artificial Intel-ligence Research, 31:399?429, March.Trevor Cohn and Mirella Lapata.
2008.
Sentence com-pression beyond word deletion.
In Proceedings ofCOLING, pages 137?144.Trevor Cohn and Mirella Lapata.
2009.
Sentence com-pression as tree transduction.
Journal of ArtificialIntelligence Research, 34(1):637?674, April.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models.
In Proceedings ofEMNLP, pages 1?8.Dipanjan Das, Andr?e F. T. Martins, and Noah A.Smith.
2012.
An exact dual decomposition algo-rithm for shallow semantic parsing with constraints.In Proceedings of the First Joint Conference on Lexi-cal and Computational Semantics (*SEM), SemEval?12, pages 209?217.Hal Daum?e, III and Daniel Marcu.
2002.
A noisy-channel model for document compression.
In Pro-ceedings of ACL, pages 449?456.John DeNero and Klaus Macherey.
2011.
Model-based aligner combination using dual decomposi-tion.
In Proceedings of ACL-HLT, pages 420?429.Jack R. Edmonds.
1967.
Optimum branchings.
Jour-nal of Research of the National Bureau of Standards,71B:233?240.Katja Filippova and Yasemin Altun.
2013.
Overcom-ing the lack of parallel data in sentence compression.In Proceedings of EMNLP, pages 1481?1491.Katja Filippova and Michael Strube.
2008.
Depen-dency tree based sentence compression.
In Proceed-ings of INLG, pages 25?32.Dimitrios Galanis and Ion Androutsopoulos.
2010.
Anextractive supervised two-stage method for sentencecompression.
In Proceedings of HLT-NAACL, pages885?893.Michel Galley and Kathleen McKeown.
2007.
Lex-icalized Markov grammars for sentence compres-sion.
In Proceedings of HLT-NAACL, pages 180?187, April.Juri Ganitkevitch, Chris Callison-Burch, CourtneyNapoles, and Benjamin Van Durme.
2011.
Learn-ing sentential paraphrases from bilingual parallelcorpora for text-to-text generation.
In Proceedingsof EMNLP, pages 1168?1179.Chiori Hori and Sadaoki Furui.
2004.
Speech summa-rization: an approach through word extraction and amethod for evaluation.
IEICE Transactions on In-formation and Systems, E87-D(1):15?25.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proceedings of ACL, pages 144?151, June.Hongyan Jing and Kathleen R. McKeown.
2000.
Cutand paste based text summarization.
In Proceedingsof NAACL, pages 178?185.Hongyan Jing.
2000.
Sentence reduction for auto-matic text summarization.
In Proceedings of theConference on Applied Natural Language Process-ing, pages 310?315.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization - step one: Sentence compres-sion.
In Proceedings of AAAI, pages 703?710.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: a probabilistic ap-proach to sentence compression.
Artificial Intelli-gence, 139(1):91?107, July.Nikos Komodakis, Nikos Paragios, and Georgios Tzir-itas.
2007.
MRF optimization via dual decomposi-tion: Message-passing revisited.
In Proceedings ofICCV, pages 1?8, Oct.Terry Koo, Alexander M. Rush, Michael Collins,Tommi Jaakkola, and David Sontag.
2010.
Dualdecomposition for parsing with non-projective headautomata.
In Proceedings of EMNLP, pages 1288?1298.Alex Kulesza and Fernando Pereira.
2007.
Structuredlearning with approximate inference.
In John C.Platt, Daphne Koller, Yoram Singer, and Sam T.Roweis, editors, NIPS.
Curran Associates, Inc.1250Hoong Chuin Lau, Trung Hieu Ngo, and Bao NguyenNguyen.
2006.
Finding a length-constrainedmaximum-sum or maximum-density subtree andits application to logistics.
Discrete Optimization,3(4):385 ?
391.Chen Li, Fei Liu, Fuliang Weng, and Yang Liu.
2013.Document summarization via guided sentence com-pression.
In Proceedings of EMNLP, pages 490?500, Seattle, Washington, USA, October.Thomas L. Magnanti and Laurence A. Wolsey.
1994.Optimal trees.
In Technical Report 290-94,Massechusetts Institute of Technology, OperationsResearch Center.Andr?e F. T. Martins and Noah A. Smith.
2009.
Sum-marization with a joint model for sentence extractionand compression.
In Proceedings of the Workshopon Integer Linear Programming for Natural Lan-gauge Processing, pages 1?9.Andr?e F. T. Martins, Noah A. Smith, and Eric P. Xing.2009.
Concise integer linear programming formu-lations for dependency parsing.
In Proceedings ofACL-IJCNLP, pages 342?350.Andr?e F. T. Martins, Noah A. Smith, Pedro M. Q.Aguiar, and M?ario A. T. Figueiredo.
2011.
Dualdecomposition with many overlapping components.In Proceedings of EMNLP, pages 238?249.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Haji?c.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof EMNLP-HLT, pages 523?530.Ryan McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In Proceed-ings of EACL, pages 297?304.Alejandro Molina, Juan-Manuel Torres-Moreno, EricSanJuan, Iria da Cunha, and Gerardo EugenioSierra Mart??nez.
2013.
Discursive sentence com-pression.
In Computational Linguistics and Intelli-gent Text Processing, volume 7817, pages 394?407.Springer.Courtney Napoles, Chris Callison-Burch, Juri Ganitke-vitch, and Benjamin Van Durme.
2011a.
Paraphras-tic sentence compression with a character-basedmetric: tightening without deletion.
In Proceedingsof the Workshop on Monolingual Text-To-Text Gen-eration, pages 84?90.Courtney Napoles, Benjamin Van Durme, and ChrisCallison-Burch.
2011b.
Evaluating sentence com-pression: pitfalls and suggested remedies.
In Pro-ceedings of the Workshop on Monolingual Text-To-Text Generation, pages 91?97.Tadashi Nomoto.
2007.
Discriminative sentence com-pression with conditional random fields.
Infor-mation Processing and Management, 43(6):1571?1587, November.Xian Qian and Yang Liu.
2013.
Fast joint compressionand summarization via graph cuts.
In Proceedingsof EMNLP, pages 1492?1502, Seattle, Washington,USA, October.Stefan Riezler, Tracy H. King, Richard Crouch, andAnnie Zaenen.
2003.
Statistical sentence condensa-tion using ambiguity packing and stochastic disam-biguation methods for lexical-functional grammar.In Proceedings of HLT-NAACL, pages 118?125.Alexander M. Rush and Michael Collins.
2011.
Ex-act decoding of syntactic translation models throughLagrangian relaxation.
In Proceedings of ACL-HLT,pages 72?82.Alexander M. Rush, David Sontag, Michael Collins,and Tommi Jaakkola.
2010.
On dual decompositionand linear programming relaxations for natural lan-guage processing.
In Proceedings of EMNLP, pages1?11.Kapil Thadani and Kathleen McKeown.
2013.
Sen-tence compression with joint structural inference.
InProceedings of CoNLL.Jenine Turner and Eugene Charniak.
2005.
Super-vised and unsupervised learning for sentence com-pression.
In Proceedings of ACL, pages 290?297.Yuya Unno, Takashi Ninomiya, Yusuke Miyao, andJun?ichi Tsujii.
2006.
Trimming CFG parse treesfor sentence compression using machine learningapproaches.
In Proceedings of ACL-COLING, pages850?857.Kristian Woodsend and Mirella Lapata.
2012.
Mul-tiple aspect summarization using integer linear pro-gramming.
In Proceedings of EMNLP, pages 233?243.David Zajic, Bonnie J. Dorr, Jimmy Lin, and RichardSchwartz.
2007.
Multi-candidate reduction: Sen-tence compression as a tool for document summa-rization tasks.
Information Processing and Manage-ment, 43(6):1549?1570, Nov.Kai Zhao and Liang Huang.
2013.
Minibatch and par-allelization for online large margin structured learn-ing.
In Proceedings of HLT-NAACL, pages 370?379, Atlanta, Georgia, June.1251
