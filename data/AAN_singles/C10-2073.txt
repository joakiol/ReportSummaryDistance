Coling 2010: Poster Volume, pages 639?646,Beijing, August 2010EM-based Hybrid Model for Bilingual Terminology Extractionfrom Comparable CorporaLianhau Lee, Aiti Aw, Min Zhang, Haizhou LiInstitute for Inforcomm Research{lhlee, aaiti, mzhang, hli}@i2r.a-star.edu.sgAbstractIn this paper, we present an unsuper-vised hybrid model which combines sta-tistical, lexical, linguistic, contextual,and temporal features in a generic EM-based framework to harvest bilingualterminology from comparable corporathrough comparable document align-ment constraint.
The model is configur-able for any language and is extensiblefor additional features.
In overall, it pro-duces considerable improvement in per-formance over the baseline method.
Ontop of that, our model has shown prom-ising capability to discover new bilin-gual terminology with limited usage ofdictionaries.1 IntroductionBilingual terminology extraction or term align-ment has been well studied in parallel corpora.Due to the coherent nature of parallel corpora,various statistical methods, like EM algorithm(Brown et.
al., 1993) have been proven to beeffective and have achieved excellent perform-ance in term of precision and recall.
The limita-tion of parallel corpora in all domains and lan-guages has led some researchers to exploreways to automate the parallel sentence extrac-tion process from non-parallel corpora(Munteanu and Marcu, 2005; Fung and Cheung,2004) before proceeding to the usual termalignment extraction using the existing tech-niques for parallel corpora.
Nevertheless, thecoverage is limited since parallel sentences innon-parallel corpora are minimal.Meanwhile, some researchers have started toexploit comparable corpora directly in a newmanner.
The motivations for such an approachare obvious: comparable corpora are abundantlyavailable, from encyclopedia to daily newspa-pers, and the human effort is reduced in eithergenerating or collecting these corpora.
If bilin-gual terminology can be extracted directly fromthese corpora, evolving or emerging terminol-ogies can be captured much faster than lexicog-raphy and this would facilitate many tasks andapplications in accessing cross-lingual informa-tion.There remain challenges in term alignmentfor comparable corpora.
The structures of texts,paragraphs and sentences can be very different.The similarity of content in two documents var-ies through they talk about the same subjectmatter.
Recent research in using transliteration(Udupa et.
al., 2008; Knight and Graehl, 1998),context information (Morin et.
al., 2007; Caoand Li, 2002; Fung, 1998), part-of-speech tag-ging, frequency distribution (Tao and Zhai,2005) or some hybrid methods (Klementiev andRoth, 2006; Sadat et.
al., 2003) have shonesome light in dealing with comparable corpora.In particular, context information seems to bepopular since it is ubiquitous and can be re-trieved from corpora easily.In this paper, we propose an EM-based hy-brid model for term alignment to address theissue.
Through this model, we hope to discovernew bilingual terminology from comparablecorpora without supervision.
In the followingsections, the model will be explained in details.6392 System ArchitectureIt is expensive and challenging to extract bilin-gual terminologies from a given set of compa-rable corpora if they are noisy with very diversetopics.
Thus the first thing we do is to derive thedocument association relationship between twocorpora of different languages.
To do this, weadopt the document alignment approach pro-posed by Vu et.
al.
(2009) to harvest compara-ble news document pairs.
Their approach is re-lying on 3 feature scores, namely Title-n-Content (TNC), Linguistic Independent Unit(LIU), and Monolingual Term Distribution(MTD).
In the nutshell, they exploit commonwords, numbers and identical strings in titlesand contents as well as their distribution in timedomain.
Their method is shown to be superiorto Tao and Zai (2005) which simply make useof frequency correlation of words.After we have retrieved comparable docu-ment pairs, we tokenize these documents withprominent monolingual noun terms foundwithin.
We are interested only in noun termssince they are more informative and more im-portantly they are more likely not to be coveredby dictionary and we hope to find their transla-tions through comparable bilingual corpora.
Weadopt the approach developed by Vu et.
al.(2008).
They first use the state-of-the-art C/NC-Value method (Frantzi and Ananiadou, 1998) toextract terms based on the global context of thecorpus, follow by refining the local terms foreach document with a term re-extraction process(TREM) using Viterbi algorithm.Figure 1.
The procedure of bilingual terminol-ogy extraction from comparable documents.After these preprocesses, we have a set ofcomparable bilingual document pairs and a setof prominent monolingual noun terms for eachmonolingual document.
The aim of our termalignment model is to discover new bilingualterminology formed from these monolingualterms across aligned document pairs (Figure.1).Like other approaches to comparable corpora,there exist many challenges in aligning bilingualterms due to the presence of noises and the sig-nificant text-structure disparity across the com-parable bilingual documents.
To overcome this,we propose using both corpus-driven and non-corpus-driven information, from which we drawvarious features and derive our hybrid model.These features are used to make initial guess onthe alignment score of term pair candidates.
Fig-ure 2 shows the overall process of our termalignment model on comparable corpora.
Thismodel is language independent and it comprisesseveral main components:?
EM algorithm?
Term alignment initialization?
Mutual information (MI) & TScore res-coringFigure 2.
Term alignment model.
D = docu-ment alignment score, L = lexical similarity, N= named entity similarity, C = context similar-ity, T = temporal similarity, R = related termsimilarity.6403 EM AlgorithmWe make two assumptions on the preprocessesthat the extracted monolingual terms are goodrepresentatives of their source documents, andthe document alignment scores derived fromdocument alignment process are good indicatorsof how well the contents of various documentsalign.
Hence, the logical implication suggeststhat the extracted terms from both well aligneddocuments could well be candidates of alignedterm pairs.By reformulating the state-of-the-art EM-based word alignment framework IBM model 1(Brown et.
al., 1993), we can derive a termalignment model easily.
In IBM word alignmentmodel 1, the task is to find word alignment byusing parallel sentences.
In the reformulatedmodel for term alignment, parallel sentences arereplaced by comparable documents, character-ized by document alignment score and their rep-resentative monolingual terms.The significant advantage over the originalIBM model 1 is the relaxation of parallel sen-tences or parallel corpora, by incorporating anadditional feature of document alignment score.We initialize the term alignment score of thecorresponding term pair candidates with thedocument alignment score to reflect the confi-dence level of document alignment.
Other thanthat, we also employ a collection of featuresimilarity score: lexical similarity, named entitysimilarity, context similarity, temporal similar-ity, and related term similarity, to term align-ment initialization.
We will explain this furtherin the next section.As we know, IBM model 1 will converge tothe global maximum regardless of the initialassignment.
This is truly good news for parallelcorpora, but not for comparable corpora whichcontains a lot of noises.
To prevent IBM model1 from overfitting, we choose to run ten itera-tions (each iteration consists of one E-step andone M-step) for each cycle of EM in both e-fand f-e directions.After each cycle of EM process, we simplyfilter off the weak term alignment pairs of bothdirections with a high threshold (0.8) and popu-late the lexicon database with the remainingpairs and use it to start another cycle of EM.The process repeats until no new term align-ment pair is found.
The EM algorithm for termalignment is shown as follow:Figure 3.
EM algorithm for e-f direction, wheree[k] = k-th aligned source document, f[k] = k-thaligned target document, e[k,i] = i-th term ine[k], f[k,j] = j-th term in f[k], a[i,j,k] = probabil-ity of alignment from f[k,j] to e[k,i], t(f|e) =probability of alignment from term e to term f.4 Term Alignment InitializationWe retrieve term alignment candidates by pair-ing all possible combinations of extractedmonolingual source terms and target termsacross the aligned document pairs.
Before eachcycle of EM, we assign an initial term align-ment score, t(f|e) to each of these term pair can-didates.
Basically, we initialize the term align-ment score t(f|e) based on document alignmentscore (D), lexical similarity (L), named entitysimilarity (N), context similarity (C), temporalsimilarity (T), and related term similarity (R).The similarity calculations of the corpus-drivenfeatures (D, C, T, R) are derived directly fromthe corpus and require limited lexical resource.The non-corpus-driven features (L, N) make useof a small word based bilingual dictionary tomeasure their lexical relevancy.
That makes ourmodel not resource-demanding and it shows thatour model can work under limited resourcecondition.All the above features contribute to the termalignment score t(f|e) independently, and weformulate their cumulative contributions as thefollowing:Initialize t(f|e).for (iteration = 1 to 10)E stepkjiallforikejkftikejkftkjiai,,,]),[|],[(]),[|],[(],,[ ?=M step),(,),(),()|(),(],,,[),(],[,],[:,,feallforfetcountfetcounteftfeallforkjiafetcountffjkfeikekji?
?====End for.641)|()|()|()|()|()|()|(,:),(efRefTefCefNefLEFDeftFfEeFE??????????
?= ??
?where,e = source termf  = target termE  = source documentF   = target documentD   = document alignment scoreL   = lexical similarityN   = named entity similarityC  = context similarityT   = temporal similarityR   = related term similarity(1)This formula allows us to extend the model withadditional features without affecting the existingconfiguration.4.1 Document Alignment Score (D)As explained in the Section 3, the relaxation onthe requirement of parallel corpora in the newEM model leads to the incorporation ofdocument alignment score.
To indicate theconfidence level of document alignment, wecredit every aligned term pair candidate formedacross the aligned documents with thecorresponding document alignment score.Although it is not necessary, documentalignment score is first normalized to the rangeof [0,1], with 1 indicates parallel alignment.4.2 Lexical Similarity (L)We design a simple lexical similarity measure-ment of two terms based on word translation.Term pairs that share more than 50% of wordtranslation pairs will be credited with lexicalsimilarity of L0, where L0 is configurable con-tribution weightage of lexical similarity.
Thisprovides us a primitive hint on term alignmentwithout resorting to exhaustive dictionarylookup.???
?=otherwiseefTifLefL W,15.0)|(,)|( 0where L0 > 1 and  TW(f|e) is word translationscore.
(2)4.3 Named Entity Similarity (N)Named entity similarity is a measure of prede-fined category membership likelihood, such asperson, location and organization.
Term pairsthat belong to the same NE categories will becredited with named entity similarity of N0,where N0 is a configurable weightage of namedentity similarity.
We use this similarity score todiscover bilingual terms of same NE categories,yet not covered by bilingual dictionary.??
?=otherwisematchcategoriesNEifNefN,1,)|( 0where N0 > 1.
(3)4.4 Context Similarity (C)We assume that terms with similar contexts arelikely to have similar meaning.
Thus, we makeuse of context similarity to measure semanticsimilarity.
Here, only k nearest content words(verbs, nouns, adjectives and adverbs) before orafter the terms within the sentence boundary areconsidered as its contexts.
The following showsthe calculation of context similarity of twoterms based on cosine similarity between theircontext frequency vectors before scaling to therange of [1, C0], where C0 is a configurable con-tribution weightage of context similarity.
Asshown in the formula, the t(f?|e?)
accounts forthe translation probability from the source con-text word to the target context word, hence thecosine similarity calculation is carried out in thetarget language domain.???????
?+=)('2)('2)(')('0)'()'()'|'()'()'()1(1)|(fcontextfecontextefcontextfecontexteffreqefreqeftffreqefreqCefCwhere C0 > 1.
(4)4.5 Temporal Similarity (T)In temporal similarity, we make use of date in-formation which is available in some corpus(e.g.
news).
We assume aligned terms are syn-chronous in time, this is especially true for com-parable news corpora (Tao and Zai, 2005).
We642use Discrete Fourier Transform (DFT) to trans-form the distribution function of a term in dis-crete time domain to a representative function indiscrete frequency domain, which is usuallyknown as ?spectrum?.
We then calculate thepower spectrum, which is defined as magnitudesquare of a spectrum.
Power spectrum is sensi-tive to the relative spacing in time (or frequencycomponent), yet invariant to the shifting in time,thus it is most suitably to be used for patternmatching of time distribution.
The temporalsimilarity is calculated based on cosine similar-ity between the power spectrums of the twoterms before scaling to the range of [1, T0],where T0 is a configurable contribution weight-age of temporal similarity.
( ) 1)(),(cos)1()|( 0 +?= kPkPineTefT fe             (5)where T0 > 1 and( )2102222)(|)}({|)()()()()()(),(cos?????=?
?===NnknNixxxkkkenonFunctionDistributinonFunctionDistributiDFTkPkvkukvkukvkuine?4.6 Related Term Similarity (R)Related terms are terms that correlate statisti-cally in the same documents and they can befound by using mutual information or t-test inthe monolingual corpus.
Basically, related termsimilarity is a measure of related term likeli-hood.
Aligned terms are assumed to have simi-lar related terms, hence related term similaritycontributes to semantic similarity.
The relatedterm similarity is calculated based on weightedcontribution from the related terms of the sourceterm before scaling to the range of [1, R0],where R0 is a configurable contribution weight-age of related terms similarity.1)|()1()|( 0 +?= efyRsimilaritRefR          (6)where R0 > 1 and?
???
?
?=Ff eReeReefvoteefvoteefyRsimilarit)(')(')'|()'|()|(?
???
??????????=FfeeReReeeReReefvoteeeMIfewefvoteeeMIfewefvote}]'{)'([)("}]'{)'([)(")"|()",(),"()"|()",(),"()'|(????????=???
???=)"()()",(log)",(,1)()"(,5.1),"(epepeepeeMIotherwisefReTriffewvote(f|e?)
is initialized to 1 before it is com-puted iteratively until it converges.
R(e) is theset of related term of e and Tr(e) is the set oftranslated term of e.5 MI & TScore RescoringWe design the MI & TScore rescoring processto enhance the alignment score t(f|e) of e-f termpairs that have significant co-occurrence fre-quencies in aligned document pairs, based onpointwise mutual information and TScore (orcommonly known as t-test) of the terms.
Byusing both measures concurrently, the associa-tion relationship of a term pair can be assumedwith higher confidence.
On top of that, the asso-ciation of a term pair can also be suggested by amuch higher TScore value alone.
In this rescor-ing process, we scale up the alignment scoret(f|e) of any term pair which is strongly associ-ated by a constant factor.
The following showsthe mathematical expressions of what has beendescribed, with M0 as the configurable scalingfactor.Rescoring condition:andfeTScoreif 5.2),({[ ?
(7))]','(6.0),()()'()()'(:)','(feMIMaxfeMIffreqffreqorefreqefreqfe = =?
?thenfeTScoreor }5),( ?0)|()|( MefTefT ?=where M0 > 1 andNfepfpepfepfeTScore2),()()(),(),(?=),( feirNumberOfPaN =6436 Experiment and EvaluationWe conduct the experiment on articles fromthree newspapers of different languages pub-lished by Singapore Press Holding (SPH),namely Straits Times1 (English), ZaoBao2 (Chi-nese) and Berita Harian3 (Malay), in June 2006.There are 3187 English articles, 4316 Chinesearticles and 1115 Malay articles.
English is cho-sen to be the source language and the remainingtwo languages as target languages.
To analyzethe effect of the quality of comparable docu-ment in our term alignment model, we preparetwo different input sets of document alignment,namely golden document alignment and auto-mated document alignment for each source-target language pair.
The former is retrieved bylinguistic experts who are requested to read thecontents of the articles in the source and the tar-get languages, and then match the articles withsimilar contents (e.g.
news coverage on samestory), while the latter is generated using unsu-pervised method proposed by Vu et.
al.
(2009),mentioned in Section 2.In both cases of document alignments, onlymonolingual noun terms extracted automaticallyby program (Vu et.
al., 2008) will be used asbasic semantic unit.
There are 23,107 uniqueEnglish noun terms, 31,944 unique Chinesenoun terms and 8,938 unique Malay noun termsextracted in overall.
In average, there are 17.3noun term tokens extracted for each Englishdocument, 16.9 for Chinese document and 13.0for Malay document.
Also note that the termalignment reference list is constructed based onthese extracted monolingual terms under theconstraints of document alignment.
In otherwords, the linguistic experts are requested tomatch the extracted terms across aligned docu-ment pairs (for both golden document alignmentand automated document alignment sets respec-tively).
The numbers of comparable documentpairs and the corresponding unique term align-ment reference pairs are shown in Table 2.1 http://www.straitstimes.com/ an English newsagency in Singapore.
Source ?
Singapore PressHoldings Ltd.2 http://www.zaobao.com/ a Chinese news agency inSingapore.
Source ?
Singapore Press Holdings Ltd.3 http://cyberita.asia1.com.sg/ a Malay news agencyin Singapore.
Source ?
Singapore Press HoldingsLtd.In the experiment, we will conduct the namedentity recognition (NER) by using the devel-oped system from the Stanford NLP Group, forEnglish, and an in-house engine, for Chinese.Currently, there is no available NER engine forMalay.Dictionary E-C C-E E-M M-EEntry 23,979 71,287 28,496 18,935Table 1.
Statistics of dictionaries, where E = English,C = Chinese, M = Malay.GoldenDocAlign AutomatedDocAlignCorpus DocAlignTermAlign RefDocAlignTermAlign RefST-ZB 90 313 899 777ST-BH 42 113 475 358Table 2.
Statistics of comparable document align-ment pairs and term alignment reference pairs.For baseline, we make use of IBM model 1,modified in the same way which has been de-scribed in the section 3, except that we treat allcomparable documents as parallel sentences, i.e.document alignment score is 1.
Precision andrecall are used to evaluate the performance ofthe system.
To achieve high precision, highthresholds are used in the system and they arekept constant throughout the experiments forconsistency.
To evaluate the capability of dis-covering new bilingual terminology, we designa novelty metric, which is the ratio of the num-ber of correct out-of-dictionary term alignmentover the total number of correct term alignment.CNNoveltyGCRecallTCPrecision ===         (8)where,C = total number of correct term alignment result.T = total number of term alignment result.G = total number of term alignment reference.N     = total number of correct term alignment resultthat are out-of-dictionary.Table 3 shows the evaluation result of termalignment using EM algorithm with incrementalfeature setting.
The particular order of setting isdue to the implementation sequences and it isnot expected to affect the result of analysis.We observe that the precision, recall andnovelty of the system are comparatively higherwhen the golden document alignment is usedinstead of the automated document alignment.644Table 3.
Performance of term alignment using EM algorithm with incremental feature setting, where D =document alignment, L = lexical similarity, R = related term similarity, M = MI & TScore rescoring, N =named entity similarity, C = context similarity, T = temporal similarity.This is expected since the golden documentalignment provides document pairs withstronger semantic bonding.
This also suggeststhat improving on the document alignmentwould further improve the term alignment re-sult.It is noteworthy observation that the imple-mented features improve the system precisionand recall under various scenarios, although thedegree of improvement varies from case to case.This shows the effectiveness of these features inthe model.On the other hand, the novelty of the systemis around 40%+ and 50%+ for ST-ZB and ST-BH respectively (except for the automateddocument alignment in ST-BH scenarios).
Thissuggests that the system can discover quite alarge percentage of the correct bilingual termi-nologies that do not exist in the lexicon initially.Compared with the baseline IBM model 1,there is an increase of 14.5% in precision,3.51% in recall and 2.9% in novelty for ST-ZB,using the golden document alignment.
For ST-BH, there is an even larger increase: 50% inprecision, 7.96% in recall and 60% in novelty.7 ConclusionWe have proposed an unsupervised EM-basedhybrid model to extract bilingual terminologyfrom comparable corpora through documentalignment constraint.
Our strategy is to makeuse of various information (corpus-driven andnon-corpus-driven) to make initial guess on thesemantic bonding of the term alignment candi-dates before subjecting them to documentalignment constraint through EM algorithm.The hybrid model allows inclusion of additionalfeatures without reconfigurations on existingfeatures, this make it practically attractive.Moreover, the proposed system can be easilydeployed in any language with minimal con-figurations.We have successfully conducted the experi-ments in English-Chinese and English-Malaycomparable news corpora.
The features em-ployed in the model have shown incrementalimprovement in performance over the baselinemethod.
In particular, the system shows im-provement in the capability to discover new bi-lingual terminology from comparable corporaeven with limited usage of dictionaries.From the experiments, we have found that thequality of comparable bilingual documents is aGoldenDocAlign AutomatedDocAlign corpora SettingPrecision Recall Novelty Precision Recall NoveltyIBM 1 75.0% 1.92%  50.0% 22.2% 0.26% 50.0%(D) 75.0% 1.92% 50.0% 22.2% 0.26% 50.0%(D,L) 81.8% 2.88% 55.6% 33.3% 0.52% 25.0%(D,L,R) 81.8% 2.88% 55.6% 33.3% 0.52% 25.0%(D,L,R,M) 78.6% 3.51% 63.6% 35.7% 0.64% 40.0%(D,L,R,M,N) 88.2% 4.79% 53.3% 35.7% 0.64% 40.0%(D,L,R,M,N,C) 89.5% 5.43% 52.9% 33.3% 0.64% 40.0%ST-ZB(D,L,R,M,N,C,T) 89.5% (17/19)5.43%(17/313)52.9%(9/17)37.5%(6/16)0.77%(6/777)16.7%(1/6)IBM 1 33.3% 0.89% 0.00% 33.3% 0.78% 0.00%(D) 33.3% 0.89% 0.00% 33.3% 0.78% 0.00%(D,L) 75.0% 5.31% 50.0% 50.0% 1.94% 0.00%(D,L,R) 75.0% 5.31% 50.0% 50.0% 1.94% 0.00%(D,L,R,M) 75.0% 5.31% 50.0% 54.5% 2.33% 0.00%(D,L,R,M,N) 75.0% 5.31% 50.0% 54.5% 2.33% 0.00%(D,L,R,M,N,C) 83.3% 8.85% 60.0% 50.0% 1.94% 0.00%ST-BH(D,L,R,M,N,C,T) 83.3% (10/12)8.85%(10/113)60.0%(6/10)50.0%(5/10)1.94%(5/258)0.00%(0/5)645major limiting factor to achieve good perform-ance.
In future, we want to explore ways to im-prove on this.ReferencesR.
Agrawal, C. Faloutsos, and A. Swami.
1993.
Effi-cient similarity search in sequence databases.
InProceedings of the 4th International Conferenceon Foundations of Data Organization and Algo-rithms.
Chicago, United States.P.
F. Brown, V. S. A. Della Pietra, V. J. Della Pietra,and R. L. Mercer.
1993.
The mathematics of sta-tistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2): 263-312.Yunbo Cao and Hang Li.
2002.
Base Noun PhraseTranslation Using Wed Data and the EM Algo-rithm, Computational Linguistics, pp.1-7.Pascale Fung, 1998.
A statistical view on bilinguallexicon extraction: From parallel corpora to non-parallel corpora.
Proceedings of AMTA, pp.1-17.Pascale Fung and Percy Cheung.
2004.
Mining Very-Non-Parallel Corpora: Parallel Sentence andLexicon Extraction via Bootstrapping and EM,Proceedings of EMNLP, pp.57-63.Alexandre Klementiev and Dan Roth, 2006.
WeaklySupervised Named Entity Transliteration and Dis-covery from Multilingual Comparable Corpora.Computational Linguistics, pp.
817-824.K.
Knight and J. Graehl.
1998.
Machine translitera-tion, Computational Linguistics, 24(4): 599-612.E.
Morin, B. Daille, K. Takeuchi, K. Kageura.
2007.Bilingual Terminology Mining ?
Using Brain, notbrawn comparable corpora, Proceedings of ACL.Dragos Stefan Munteanu and Daniel Marcu.
2005.Improving Machine Translation Performance byExploiting Non-Parallel Corpora.
ComputationalLinguistics, 31(4): 477-504.Fatiha Sadat, Masatoshi Yoshikawa, Shunsuke Ue-mura, 2003.
Learning Bilingual Translations fromComparable Corpora to Cross-Language Infor-mation Retrieval: Hybrid Statistics-based andLinguistics-based Approach.
Proceedings of ACL,vol.11, pp.57-64.Tao Tao and Chengxiang Zhai.
2005.
Mining com-parable bilingual text corpora for cross-languageinformation integration, Proceedings of ACM.Raghavendra Udupa, K. Saravanan, A. Kumaran,Jagadeesh Jagarlamudi.
2008.
Mining named en-tity transliteration equivalents from comparablecorpora, Proceedings of ACM.Thuy Vu, Aiti Aw, Min Zhang, 2008.
Term extrac-tion through unithood and termhood unification.Proceedings of IJCNLP-08, Hyderabad, India.Thuy Vu, Aiti Aw, Min Zhang, 2009.
Feature-basedMethod for Document Alignment in ComparableNews Corpora.
Proceedings of EACL-09, Athens,Greece.646
