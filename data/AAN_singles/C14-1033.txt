Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 334?344, Dublin, Ireland, August 23-29 2014.Simple or Complex?
Assessing the readability of Basque TextsItziar Gonzalez-Dios, Mar?
?a Jes?us Aranzabe, Arantza D?
?az de Ilarraza, Haritz SalaberriIXA NLP GroupUniversity of the Basque Country (UPV/EHU)itziar.gonzalezd@ehu.esAbstractIn this paper we present a readability assessment system for Basque, ErreXail, which is goingto be the preprocessing module of a Text Simplification system.
To that end we compile twocorpora, one of simple texts and another one of complex texts.
To analyse those texts, we imple-ment global, lexical, morphological, morpho-syntactic, syntactic and pragmatic features basedon other languages and specially considered for Basque.
We combine these feature types and wetrain our classifiers.
After testing the classifiers, we detect the features that perform best and themost predictive ones.1 IntroductionReadability assessment is a research line that aims to grade the difficulty or the ease of the texts.
It hasbeen a remarkable question in the educational domain during the last century and is of great importancein Natural Language Processing (NLP) during the last decade.
Classical readability formulae like Fleshformula (Flesch, 1948), Dale-Chall formula (Chall and Dale, 1995) and The Gunning FOG index (Gun-ning, 1968) take into account raw and lexical features and frequency counts.
NLP techniques, on theother hand, make possible the consideration of more complex features.Recent research in NLP (Si and Callan, 2001; Petersen and Ostendorf, 2009; Feng, 2009) has demon-strated that classical readability formulae are unreliable.
Moreover, those metrics are language specific.Readability assessment is also used as a preprocess or evaluation in Text Simplification (TS) systemse.g.
for English (Feng et al., 2010), Portuguese (Alu?
?sio et al., 2010), Italian (Dell?Orletta et al., 2011),German (Hancke et al., 2012) and Spanish (?Stajner and Saggion, 2013).
Given a text the aim of thesesystems is to decide whether a text is complex or not.
So, in case of being difficult, the given text shouldbe simplified.As far as we know no specific metric has been used to calculate the complexity of Basque texts.
Theonly exception we find is a system for the auto-evaluation of essays Idazlanen Autoebaluaziorako Sistema(IAS) (Aldabe et al., 2012) which includes metrics similar to those used in readability assessment.
IASanalyses Basque texts after several criteria focused on educational correction such as the clause numberin a sentence, types of sentences, word types and lemma number among others.
It was foreseen to usethis tool in the Basque TS system (Aranzabe et al., 2012).
The present work means to add to IAS thecapacity of evaluating the complexity of texts by means of new linguistic features and criteria.In this paper we present ErreXail, a readability assessment system for Basque, a Pre-Indo-Europeanagglutinative head-final pro-drop language, which displays a rich inflectional morphology and whoseorthography is phonemic.
ErreXail classifies the texts and decides if they should be simplified or not.This work has two objectives: to build a classifier which will be the preprocess of the TS system and toknow which are the most predictive features that differ in complex and simple texts.
The study of themost predictive features will help in the linguistic analysis of the complex structures of Basque as well.This paper is organised as follows: In section 2 we offer an overview about this topic.
We present thecorpora we gathered and its processing in section 3.
In section 4 we summarise the linguistic features weThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/334implemented and we present the experiments and their results in section 5.
The present system, ErreXail,is described in section 6 and in section 7 we compare our work with other studies.
Finally, we concludeand outline the future work (section 8).2 Related workIn the last years new methods have been proposed to assess the readability in NLP.
For English, Siand Callan (2001) use statistical models, exactly unigram language models, combined with traditionalreadability features like sentence length and number of syllables per word.
Coh-Metrix (Graesser et al.,2004) is a tool that analyses multiple characteristics and levels of language-discourse such us narrativity,word concreteness or noun overlap.
In the 3.0 version1108 indices are available.
Pitler and Nenkova(2008) use lexical, syntactic, and discourse features emphasising the importance of discourse features aswell.
Schwarm and Ostendorf (2005) combine features from statistical language models, parse features,and other traditional features using support vector machines.It is very interesting to take a look at readability systems for other languages as well.
Some readabilitymetrics take them into account special characteristics linked to languages.
For example, in Chinese thenumber of strokes is considered (Pang, 2006), in Japanese the different characters (Sato et al., 2008), inGerman the word formation (vor der Br?uck et al., 2008), in French the pass?e simple (Franc?ois and Fairon,2012) and the orthographic neighbourhood (Gala et al., 2013) and in Swedish vocabulary resources(Sj?oholm, 2012; Falkenjack et al., 2013) among many other features.
For Portuguese, Coh-metrix hasbeen adapted (Scarton and Alu?
?sio, 2010) and in Arabic language-specific formulae have been used (Al-Ajlan et al., 2008; Daud et al., 2013).
Looking at free word order, head final and rich morphologylanguages, Sinha et al.
(2012) propose two new measures for Hindi and for Bangla based on Englishformulae.
Other systems use only machine learning techniques, e.g.
for Chinese (Chen et al., 2011).The systems whose motivation is Text Simplification analyse linguistic features of the text and thenthey use machine learning techniques to build the classifiers.
These systems have been created for English(Feng et al., 2010), Portuguese (Alu?
?sio et al., 2010), Italian (Dell?Orletta et al., 2011) and German(Hancke et al., 2012).
We follow the similar methodology for Basque since we share the same aim.Readability assessment can be focused on different domains such as legal, medical, education and soon.
Interesting points about readability are presented in DuBay (2004) and an analysis of the methodsand a review of the systems is presented in Benjamin (2012) and Zamanian and Heydari (2012).3 CorporaBeing our aim to build a model to distinguish simple and complex texts and to know which are themost predictive features based on NLP techniques, we needed to collect the corpora.
We gathered textsfrom the web and compiled two corpora.
The first corpus, henceforth T-comp, is composed by 200texts (100 articles and 100 analysis) from the Elhuyar aldizkaria2, a monthly journal about science andtechnology in Basque.
T-comp is meant to be the complex corpus.
The second corpus, henceforth T-simp,is composed by 200 texts from ZerNola3, a website to popularise science among children up to 12 yearsand the texts we collected are articles.
To find texts specially written for children was really challenging.Main statistics about both corpora are presented in Table 1.Corpus Docs.
Sentences Tokens Verbs NounsT-comp 200 8593 161161 52229 59510T-simp 200 2363 39565 12203 13447Table 1: Corpora statisticsBoth corpora were analysed at various levels:1.
Morpho-syntactic analysis by Morpheus (Alegria et al., 2002)1http://cohmetrix.memphis.edu/cohmetrixpr/cohmetrix3.html (accessed January, 2014)2http://aldizkaria.elhuyar.org/ (accessed January, 2014)3http://www.zernola.net/ (accessed January, 2014)3352.
Lemmatisation and syntactic function identification by Eustagger (Aduriz et al., 2003)3.
Multi-words item identification (Alegria et al., 2004a)4.
Named entities recognition and classification by Eihera (Alegria et al., 2004b)5.
Shallow parsing by Ixati (Aduriz et al., 2004)6.
Sentence and clause boundaries determination by MuGak (Aranzabe et al., 2013)7.
Apposition identification (Gonzalez-Dios et al., 2013)This preprocess is necessary to perform the analysis of the features presented in section 4.4 Linguistic featuresIn this section we summarise the linguistic features implemented to analyse the complexity of the texts.We distinguish different groups of features: global, lexical, morphological, morpho-syntactic, syntacticand pragmatic features.
There are in total 94 features.
Most of the features we present have already beenincluded in systems for other languages but others have been specially considered for Basque.4.1 Global featuresGlobal features take into account the document as whole and serve to give an overview of the texts.
Theyare presented in Table 2.AveragesAverage of words per sentenceAverage of clauses per sentenceAverage of letters per wordTable 2: Global featuresThese features are based on classical readability formulae and in the criteria taken on the simplificationstudy (Gonzalez-Dios, 2011), namely the sentence length and the clause number per sentence.
They arealso included in IAS (Aldabe et al., 2012).4.2 Lexical featuresLexical features are based on lemmas.
We calculate the ratios of all the POS tags and different kinds ofabbreviations and symbols.
We concentrate on particular types of substantives and verbs as well.
Part oftheses ratios are shown in Table 3.
In total there are 39 ratios in this group.RatiosUnique lemmas / all the lemmasEach POS / all the wordsProper Nouns / all the nounsNamed entities / all the nounsVerbal nouns / all the verbsModal verbs / all the verbsCausative verbs / all the verbsIntransitive verbs with one arg.
(Nor verbs) / all the verbsIntransitive verbs with two arg.
(Nor-Nori verbs) / all the verbsTransitive verbs with two arg.
(Nor-Nork verbs) / all the verbsTransitive verbs with three arg.
(Nor-Nori-Nork) verbs / all the verbsAcronyms / all the wordsAbbreviations / all the wordsSymbols / all the wordsTable 3: Lexical featuresAmong those features, we want to point out the causative verbs and the intransitive or transitive verbswith one, two or three arguments (arg.)
as features related to Basque.
Causative verbs are verbs with the336suffix -arazi and they are usually translated as ?to make someone + verb?, e.g.
edanarazi, that standsfor ?to make someone drink?.
Other factitive verbs are translated without using that paraphrase likejakinarazi that means ?to notify?, lit.
?to make know?.
The transitivity classification is due to the factthat Basque verb agrees with three grammatical cases (ergative Nork, absolutive Nor and dative Nori)and therefore verbs are grouped according to the arguments they take in Basque grammars.4.3 Morphological featuresMorphological features analyse the different ways lemmas can be realised.
These features are sum-marised in Table 4 and there are 24 ratios in total.RatiosEach case ending / all the case endingsEach verb aspect / all the verbsEach verb tense / all the verbsEach verb mood / all the verbsWords with ellipsis / all the wordsEach type of words with ellipsis / all the words with ellipsisTable 4: Morphological featuresBasque has 18 case endings (absolutive, ergative, inessive, allative, genitive...), that is, 18 differentendings can be attached to the end of the noun phrases.
For example, if we attach the inessive -n tothe noun phrase etxea ?the house?, we get etxean ?at home?.
The verb features considered the formsobtained with the inflection.Verb morphology is very rich in Basque as well.
The aspect is attached to the part of the verb whichcontains the lexical information.
There are 4 aspects: puntual (aoristic), perfective, imperfective andfuture aspect.
Verb tenses are usually marked in the auxiliary verb and there are four tenses: present,past, irreal and archaic future4.
The verbal moods are indicative, subjunctive, imperative and potential.The latter is used to express permissibility or possible circumstances.Due to the typology of Basque, ellipsis5is a normal phenomenon and ellipsis can be even foundwithin a word (verbs, nouns, adjective...); for instance, dioguna which means ?what we say?.
This kindof ellipsis occurs e.g.
in English, Spanish, French and German as well but in these languages it is realisedas a sentence; but it is expressed only by a word in Basque.4.4 Morpho-syntactic featuresMorpho-syntactic features are based on the shallow parsing (chunks6) and in the apposition detection(appositions).
These features are presented in Table 5.RatiosNoun phrases (chunks) / all the phrasesNoun phrases (chunks) / all the sentencesVerb phrases / all the phrasesAppositions / all the phrasesAppositions / all the noun phrases (chunks)Table 5: Morpho-syntactic featuresContrary to the features so far presented, the morpho-syntactic features take into account mainly morethan a word.
About apposition, there are 2 types in Basque (Gonzalez-Dios et al., 2013) but we considerall the instances together in this work.4The archaic future we also take into account is not used anymore, but it can be found in old texts.
Nowadays, the aspect isused to express actions in the future.5Basque is a pro-drop language and it is very normal to omit the subject, the object and the indirect object because they aremarked in the verb.
We do not treat this kind of ellipsis in the present work.6Chunks are a continuum of elements with a head and syntactic sense that do not overlap (Abney, 1991).3374.5 Syntactic featuresSyntactic features consider average of the subordinate clauses and types of subordinate clauses.
Theyare outlined in Table 6 and there are 10 ratios in total.
The types of adverbial clauses are temporal,causal, conditional, modal, concessive, consecutive and modal-temporal.
The latter is a clause typewhich expresses manner and simultaneity of the action in reference to the main clause.RatiosSubordinate clauses / all the clausesRelative clauses / subordinate clausesCompletive clauses / subordinate clausesAdverbial clauses / subordinate clausesEach type of adverbial clause / subordinate clausesTable 6: Syntactic featuresIn this first approach we decided not to use dependency based features like dependency depth ordistance from dependent to head because dependency parsing is time consuming and slows down thepreprocessing.
Moreover, the importance of syntax is under discussion: Petersen and Ostendorf (2009)find that syntax does not have too much influence while Sj?oholm (2012) shows that dependencies arenot necessary.
Pitler and Nenkova (2008) pointed out the importance of syntax.
but Dell?Orletta etal.
(2011) demonstrate that for document classification reliable results can be found without syntax.Anyway, syntax is necessary for sentence classification.4.6 Pragmatic featuresIn our cases, the pragmatic features we examine are the cohesive devices.
These features are summed upin Table 7.
There are 12 ratios in total.RatiosEach type of conjunction / all the conjunctionsEach type of sentence connector / all the sentence connectorsTable 7: Pragmatic featuresConjunction types are additive, adversative and disjuntive.
Sentence connector types are additive,adversative, disjuntive, clarificative, causal, consecutive, concessive and modal.5 ExperimentsWe performed two experiments, the first one to build a classifier and the second one to know which arethe most predictive features.
For both tasks we used the WEKA tool (Hall et al., 2009).In the first experiment we ran 5 classifiers and evaluated their performance.
Those classifiers wereRandom Forest (Breiman, 2001), the J48 decision tree (Quinlan, 1993), K-Nearest Neighbour, IBk (Ahaet al., 1991), Na?
?ve Bayes (John and Langley, 1995) and Support Vector Machine with SMO algorithm(Platt, 1998).
We used 10 fold cross-validation, similar to what has been done in other studies.Taking into account all the features presented in section 4, the best results were obtained using SMO.This way, 89.50 % of the instances were correctly classified.
The F -measure for complex text was 0.899%, for simple texts was 0.891 % and the MAE was 0.105 %.
The results using all the features are shownin Table 8.Random Forest J48 IBk Na?
?ve Bayes SMO88.50 84.75 72.00 84.50 89.50Table 8: Classification results using all the featuresWe classified each feature type on their own as well and the best results were obtained using onlylexical features, 90.75 %.
The classification results according to their feature group are presented inTable 9.
We only present the classifiers with the best results and these are remarked in bold.338Classifier Random Forest J48 SMOGlobal 74.25 73.50 74.75Lex.
88.00 85.00 90.75Morph.
82.00 71.75 75.00Morpho-synt.
78.25 76.25 72.75Synt.
71.25 73.75 67.75Prag.
67.50 70.50 65.75Table 9: Classification results of each feature typeWe also made different combinations of feature types and the accuracy was improved.
The best com-bination group was the one formed by lexical, morphological, morpho-syntactic and syntactic featuresand they obtain 93.50 % with SMO.
Best results are show in Table 10.Feature Group Random Forest SMOGlobal+Lex 87.50 89.50Global+Lex+Morph 87.75 89.00Global+Lex+Morph+Morf-sint 89.25 89.50Global+Lex+Morph+Morph-sint+Sintax 87.25 90.25Morph+Morph-sint 84.25 82.25Morph+Morph-sint+Sintax 83.25 80.75Morph+Morof-sint+Sintax+Prag 83.75 82.00Lex+Morph 88.75 92.75Lex+Morph+Morph-sint 89.25 89.25Lex+Morph+Morph-sint+Sintax 89.75 93.50Lex+Morph+Morph-sint+Sintax+Prag 88.50 90.25Sintax+Prag 78.25 73.50Table 10: Classification results using different feature combinationsCombining the feature types, SMO is the best classifier in most of the cases but Random Forest out-performs the results when there are no lexical features.In the second experiment, we analysed which were the most predictive linguistic features in eachgroup.
We used Weka?s Information Gain (InfoGain AttributeEval) to create the ranking and we ran itfor each feature group.
In Table 11 we present the 10 most predictive features taking all the featuresgroups into account.The results of this experiment are interesting for the linguistic studies on Text Simplification.
It showsus indeed which phenomena we should work on next.
In these experiment we notice as well the relevanceof the lexical features and that syntactic features are not so decisive in document classification.The features with relevance 0 have been analysed as well.
Some of them are e.g.
the ratio of theinessive among all the case endings, the ratio of the indicative mood among all the verbal moods, theratio of the adjectives among all the words and the ratio of the ratio of the present tense among all theverbal tenses.We also performed a classification experiment with the top 10 features and J48 is the best classifier(its best performance as well).
These results are presented in Table 12.To sum up, our best results are obtained using a combination of features (Lex+Morph+Morph-sint+Sintax).
We want to remark the importance of lexical features as well, since they alone outperformall the features and 5 of them are among the top ten features.6 System overviewThe readability system for Basque ErreXail has a three-stage architecture (Figure 1).So, given a Basque written text, we follow next steps:1.
The linguistic analysis will be carried out, that is, morpho-syntactic tagging, lemmatisation, syntac-tic function identification, named entity recognition, shallow parsing, sentence and clause bound-aries determination and apposition identification will be performed.
We will use the tools presentedin section 3.339Feature and group RelevanceProper nouns / common nouns ratio (Lex.)
0.2744Appositions / noun phrases ratio (Morpho-synt.)
0.2529Appositions / all phrases ratio (Morpho-synt.)
0.2529Named entities / common nouns ratio (Lex.)
0.2436Unique lemmas / all the lemmas ratio (Lex.)
0.2394Acronyms / all the words ratio (Lex.)
0.2376Causative verbs / all the verbs ratio (Lex.)
0.2099Modal-temporal clauses / subordinate clauses ratio (Synt.)
0.2056Destinative case endings / all the case endings ratio (Morph.)
0.1968Connectors of clarification / all the connectors ratio (Prag.)
0.1957Table 11: Most predictive featuresRandom Forest J48 IBk Na?
?ve Bayes SMO87.75 88.25 72.00 83.25 87.00Table 12: Classification results using the top 10 featuresFigure 1: The architecture of system2.
Texts will be analysed according to the features and measures presented in section 4.3.
We will use the SMO Support Vector Machine as classification model, since that was the bestclassifier in the experiments exposed in section 5.
To speed up the process for Text Simplification,we will analyse only the combination of lexical, morphological, morpho-syntactic and syntactic(Lex+Morph+Morph-sint+Sintax) features.Although the first application of this system will be the preprocessing of texts for the Basque TSsystem, the system we present in this paper is independent and can be used for any other application.
Wewant to remark that this study, as it is based on other languages, could be applied to any other languageas well provided that the text could be analysed similar to us.7 DiscussionThe task of text classification has been carried out by several studies before.
Due to our small corpuswe were only able to discriminate between complex and simple texts like Dell?Orletta et al.
(2011) andHancke et al.
(2012), other studies have classified more complexity levels (Schwarm and Ostendorf,2005; Alu?
?sio et al., 2010; Franc?ois and Fairon, 2012).
In this section we are going to compare oursystem with other systems that share our same goal, namely to know which texts should be simplified.Comparing our experiment with studies that classify two grades and use SMO, Hancke et al.
(2012)obtain an accuracy of 89.7 % with a 10 fold cross-validation.
These results are very close to ours, al-though their data compiles 4603 documents and ours 400.
According to the feature type, their best typeis the morphological, obtaining 85.4 % of accuracy.
Combining lexical, language model and morpho-logical features they obtain 89.4 % of accuracy.
To analyse their 10 most predictive features, they useInformation Gain as well but we do not share any feature in common.Dell?Orletta et al.
(2011) perform three different experiments but only their first experiment is similarto our work.
For that classification experiment they use 638 documents and follow a 5 fold cross-validation process of the Euclidian distance between vectors.
Taking into account all the features theaccuracy of their system is 97.02 %.
However, their best performance is 98.12 % when they only use thecombination of raw, lexical and morpho-syntactic features.340Alu?
?sio et al.
(2010) assess the readability of the texts according to three levels: rudimentary, basicand advanced.
In total they compile 592 texts.
Using SMO, 10 fold cross-validation and standard classi-fication, they obtain 0.276 MAE taking into account all the features.
The F -measure for original texts is0.913, for natural simplification 0.483 and for strong simplification 0.732.
They experiment with featuretypes as well but they obtain their best results using all the features.
Among their highly correlated fea-tures they present the incidence of apposition in second place as we do here.
We do not have any otherfeature in common.Among other readability assessment whose motivation is TS, Feng et al.
(2010) use LIBSVM (Changand Lin, 2001) and Logistic Regression from WEKA and 10 fold cross-validation.
They assess thereadability of grade texts and obtain as best results 59.63 % with LIBSVM and 57.59 % with LogisticRegression.
Since they assess different grades and use other classifiers it is impossible to compare withour results but we find that we share predictive features.
They found out that named entity density andand nouns have predictive power as well.8 Conclusion and perspectivesIn this paper we have presented the first readability assessment system for the Basque language.
Wehave implemented 94 ratios based on linguistic features similar to those used in other languages andspecially defined for Basque and we have built a classifier which is able to discriminate between difficultand easy texts.
We have also determined which are the most predictive features.
From our experimentswe conclude that using only lexical features or a combination of features types we obtain better resultsthan using all the features.
Moreover, we deduce that we do not need to use time consuming resourceslike dependency parsing or big corpora to obtain good results.For the future, we could implement new features like word formation or word ordering both based inother languages and in neurolinguistic studies that are being carried out for Basque.
Other machine learn-ing techniques can be used, e.g.
language models and in the case of getting a bigger corpora or a gradedone, we could even try to differentiate more reading levels.
We also envisage readability assessment atsentence level in near future.AcknowledgementsItziar Gonzalez-Dios?s work is funded by a PhD grant from the Basque Government.
We thank LoreaArakistain and I?naki San Vicente from Elhuyar Fundazioa for providing the corpora.
We also wantto thank Olatz Arregi for her comments.
This research was supported by the the Basque Govern-ment (IT344-10), and the Spanish Ministry of Science and Innovation, Hibrido Sint project (MICINN,TIN2010-202181).ReferencesSteven P. Abney.
1991.
Parsing by Chunks.
In Robert C. Berwick, Steven P. Abney, and Carol Tenny, editors,Principle-Based Parsing: Computation and Psycholinguistics.
Kluwer Academic.Itziar Aduriz, Izaskun Aldezabal, I?naki Alegria, Jose Mari Arriola, Arantza D?
?az de Ilarraza, Nerea Ezeiza, andKoldo Gojenola.
2003.
Finite State Applications for Basque.
In EACL?2003 Workshop on Finite-State Methodsin Natural Language Processing., pages 3?11.Itziar Aduriz, Mar?
?a Jes?us Aranzabe, Jose Mari Arriola, Arantza D?
?az de Ilarraza, Koldo Gojenola, Maite Oronoz,and Larraitz Uria.
2004.
A cascaded syntactic analyser for Basque.
Computational Linguistics and IntelligentText Processing, pages 124?134.David W. Aha, Dennis Kibler, and Marc C. Albert.
1991.
Instance-based learning algorithms.
Machine Learning,6:37?66.Amani A Al-Ajlan, Hend S Al-Khalifa, and A Al-Salman.
2008.
Towards the development of an automaticreadability measurements for Arabic language.
In Digital Information Management, 2008.
ICDIM 2008.
ThirdInternational Conference on, pages 506?511.
IEEE.341Itziar Aldabe, Montse Maritxalar, Olatz Perez de Viaspre, and Uria Larraitz.
2012.
Automatic Exercise Generationin an Essay Scoring System.
In Proceedings of the 20th International Conference on Computers in Education,pages 671?673.I?naki Alegria, Mar?
?a Jes?us Aranzabe, Aitzol Ezeiza, Nerea Ezeiza, and Ruben Urizar.
2002.
Robustness and cus-tomisation in an analyser/lemmatiser for Basque.
In LREC-2002 Customizing knowledge in NLP applicationsworkshop, pages 1?6, Las Palmas de Gran Canaria, May.I?naki Alegria, Olatz Ansa, Xabier Artola, Nerea Ezeiza, Koldo Gojenola, and Ruben Urizar.
2004a.
Repre-sentation and treatment of multiword expressions in Basque.
In Proceedings of the Workshop on MultiwordExpressions: Integrating Processing, pages 48?55.
Association for Computational Linguistics.I?naki Alegria, Olatz Arregi, Irene Balza, Nerea Ezeiza, Izaskun Fernandez, and Ruben Urizar.
2004b.
Design anddevelopment of a named entity recognizer for an agglutinative language.
In First International Joint Conferenceon NLP (IJCNLP-04).
Workshop on Named Entity Recognition.Sandra Alu?
?sio, Lucia Specia, Caroline Gasperin, and Carolina Scarton.
2010.
Readability assessment for textsimplification.
In Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for BuildingEducational Applications, pages 1?9.
Association for Computational Linguistics.Mar?
?a Jes?us Aranzabe, Arantza D?
?az de Ilarraza, and Itziar Gonzalez-Dios.
2012.
First Approach to AutomaticText Simplification in Basque.
In Luz Rello and Horacio Saggion, editors, Proceedings of the Natural LanguageProcessing for Improving Textual Accessibility (NLP4ITA) workshop (LREC 2012), pages 1?8.Mar?
?a Jes?us Aranzabe, Arantza D?
?az de Ilarraza, and Itziar Gonzalez-Dios.
2013.
Transforming Complex Sen-tences using Dependency Trees for Automatic Text Simplification in Basque.
Procesamiento de LenguajeNatural, 50:61?68.Rebekah George Benjamin.
2012.
Reconstructing readability: Recent developments and recommendations in theanalysis of text difficulty.
Educational Psychology Review, 24(1):63?88.Leo Breiman.
2001.
Random Forests.
Machine Learning, 45(1):5?32.Jeanne Sternlicht Chall and Edgar Dale.
1995.
Readability Revisited: The New DaleChall Readability Formula.Brookline Books, Cambridge, MA.Chih-Chung Chang and Chih-Jen Lin.
2001.
Libsvm - a library for support vector machines.
The Weka classifierworks with version 2.82 of LIBSVM.Yaw-Huei Chen, Yi-Han Tsai, and Yu-Ta Chen.
2011.
Chinese readability assessment using TF-IDF and SVM.In Machine Learning and Cybernetics (ICMLC), 2011 International Conference on, volume 2, pages 705?710.IEEE.Nuraihan Mat Daud, Haslina Hassan, and Normaziah Abdul Aziz.
2013.
A Corpus-Based Readability Formulafor Estimate of Arabic Texts Reading Difficulty.
World Applied Sciences Journal, 21:168?173.Felice Dell?Orletta, Simonetta Montemagni, and Giulia Venturi.
2011.
READ-IT: assessing readability of Ital-ian texts with a view to text simplification.
In Proceedings of the Second Workshop on Speech and LanguageProcessing for Assistive Technologies, SLPAT ?11, pages 73?83, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.William H. DuBay.
2004.
The Principles of Readability.
Impact Information, pages 1?76.Johan Falkenjack, Katarina Heimann M?uhlenbock, and Arne J?onsson.
2013.
Features indicating readability inSwedish text.
In Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013),pages 27?40.Lijun Feng, Martin Jansche, Matt Huenerfauth, and No?emie Elhadad.
2010.
A comparison of features for auto-matic readability assessment.
In Proceedings of the 23rd International Conference on Computational Linguis-tics: Posters, pages 276?284.
Association for Computational Linguistics.Lijun Feng.
2009.
Automatic Readability Assessment for People with Intellectual Disabilities.
SIGACCESSAccess.
Comput., (93):84?91, January.Rudolph Flesch.
1948.
A new readability yardstick.
Journal of applied psychology, 32(3):221.342Thomas Franc?ois and C?edrick Fairon.
2012.
An AI readability formula for French as a foreign language.
InProceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Compu-tational Natural Language Learning, pages 466?477.
Association for Computational Linguistics.N?uria Gala, Thomas Franc?ois, and C?edrick Fairon.
2013.
Towards a French lexicon with difficulty measures:NLP helpnig to bridge the gap between traditional dictionaries and specialized lexicons.
In I. Kosem, J. Kallas,P.
Gantar, S. Krek, M. Langemets, and M. Tuulik, editors, Electronic lexicography in the 21st century: thinkingoutside the paper.
Proceedings of the eLex 2013 conference, 17-19 October 2013, Tallinn, Estonia., pages 132?151, Ljubljana/Tallinn.
Trojina, Institute for Applied Slovene Studies/Eesti Keele Instituut.Itziar Gonzalez-Dios, Mar?
?a Jes?us Aranzabe, Arantza D?
?az de Ilarraza, and Ander Soraluze.
2013.
DetectingApposition for Text Simplification in Basque.
In Computational Linguistics and Intelligent Text Processing,pages 513?524.
Springer.Itziar Gonzalez-Dios.
2011.
Euskarazko egitura sintaktikoen azterketa testuen sinplifikazio automatikorako: Apo-sizioak, erlatibozko perpausak eta denborazko perpausak.
Master?s thesis, University of the Basque Country(UPV/EHU).Arthur C. Graesser, Danielle S. McNamara, Max M. Louwerse, and Zhiqiang Cai.
2004.
Coh-Metrix: Analysis oftext on cohesion and language.
Behavior Research Methods, 36(2):193?202.Robert Gunning.
1968.
The technique of clear writing.
McGraw-Hill New York.Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten.
2009.
TheWEKA data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?18.Julia Hancke, Sowmya Vajjala, and Detmar Meurers.
2012.
Readability Classification for German using lexical,syntactic, and morphological features.
In COLING 2012: Technical Papers, page 10631080.George H. John and Pat Langley.
1995.
Estimating Continuous Distributions in Bayesian Classifiers.
In EleventhConference on Uncertainty in Artificial Intelligence, pages 338?345, San Mateo.
Morgan Kaufmann.Lau Tak Pang.
2006.
Chinese Readability Analysis and its Applications on the Internet.
Ph.D. thesis, The ChineseUniversity of Hong Kong.Sarah E. Petersen and Mari Ostendorf.
2009.
A machine learning approach to reading level assessment.
ComputerSpeech & Language, 23(1):89?106.Emily Pitler and Ani Nenkova.
2008.
Revisiting readability: A unified framework for predicting text quality.In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 186?195.Association for Computational Linguistics.John C. Platt.
1998.
Fast Training of Support Vector Machines using Sequential Minimal Optimization.
InBernhard Schlkopf, Christopher J.
C Burges, and Alexander J. Smola, editors, Advances in Kernel Methods-Support Vector Learning.
MIT Press.Ross Quinlan.
1993.
C4.5: Programs for Machine Learning.
Morgan Kaufmann Publishers, San Mateo, CA.Satoshi Sato, Suguru Matsuyoshi, and Yohsuke Kondoh.
2008.
Automatic Assessment of Japanese Text Readabil-ity Based on a Textbook Corpus.
In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Joseph Maegaard,Benteand Mariani, Jan Odijk, Stelios Piperidis, and Daniel Tapias, editors, Proceedings of the Sixth Interna-tional Conference on Language Resources and Evaluation (LREC?08), Marrakech, Morocco, may.
EuropeanLanguage Resources Association (ELRA).Carolina Evaristo Scarton and Sandra Maria Alu??sio.
2010.
An?alise da Inteligibilidade de textos via ferramentasde Processamento de L?
?ngua Natural: adaptando as m?etricas do Coh-Metrix para o Portugu?es.
Linguam?atica,2(1):45?61.Sarah E Schwarm and Mari Ostendorf.
2005.
Reading level assessment using support vector machines andstatistical language models.
In Proceedings of the 43rd Annual Meeting on Association for ComputationalLinguistics, pages 523?530.
Association for Computational Linguistics.Luo Si and Jamie Callan.
2001.
A statistical model for scientific readability.
In Proceedings of the tenth interna-tional conference on Information and knowledge management, pages 574?576.
ACM.Manjira Sinha, Sakshi Sharma, Tirthankar Dasgupta, and Anupam Basu.
2012.
New Readability Measures forBangla and Hindi Texts.
In Proceedings of COLING 2012: Posters, pages 1141?1150, Mumbai, India, Decem-ber.
The COLING 2012 Organizing Committee.343Johan Sj?oholm.
2012.
Probability as readability: A new machine learning approach to readability assessment forwritten Swedish.
Master?s thesis, Link?oping.Tim vor der Br?uck, Sven Hartrumpf, and Hermann Helbig.
2008.
A readability checker with supervised learningusing deep indicators.
Informatica, 32(4):429?435.Sanja?Stajner and Horacio Saggion.
2013.
Readability Indices for Automatic Evaluation of Text SimplificationSystems: A Feasibility Study for Spanish.
In Proceedings of the Sixth International Joint Conference on Nat-ural Language Processing, pages 374?382, Nagoya, Japan, October.
Asian Federation of Natural LanguageProcessing.Mostafa Zamanian and Pooneh Heydari.
2012.
Readability of texts: State of the art.
Theory and Practice inLanguage Studies, 2(1):43?53.344
