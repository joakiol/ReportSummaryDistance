Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 815?824,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsString-to-Tree Multi Bottom-up Tree TransducersNina Seemann and Fabienne Braune and Andreas MalettiInstitute for Natural Language Processing, University of StuttgartPfaffenwaldring 5b, 70569 Stuttgart, Germany{seemanna,braunefe,maletti}@ims.uni-stuttgart.deAbstractWe achieve significant improvements inseveral syntax-based machine translationexperiments using a string-to-tree vari-ant of multi bottom-up tree transducers.Our new parameterized rule extraction al-gorithm extracts string-to-tree rules thatcan be discontiguous and non-minimalin contrast to existing algorithms for thetree-to-tree setting.
The obtained modelssignificantly outperform the string-to-treecomponent of the Moses framework in alarge-scale empirical evaluation on severalknown translation tasks.
Our linguisticanalysis reveals the remarkable benefits ofdiscontiguous and non-minimal rules.1 IntroductionWe present an application of a variant of localmulti bottom-up tree transducers (`MBOTs) asproposed in Maletti (2011) to statistical machinetranslation.
`MBOTs allow discontinuities on thetarget language side since they have a sequenceof target tree fragments instead of a single treefragment in their rules.
The original approachmakes use of syntactic information on both thesource and the target side (tree-to-tree) and a cor-responding minimal rule extraction is presentedin (Maletti, 2011).
Braune et al (2013) imple-mented it as well as a decoder inside the Mosesframework (Koehn et al, 2007) and demonstratedthat the resulting tree-to-tree `MBOT system sig-nificantly improved over its tree-to-tree baselineusing minimal rules.
We can see at least two draw-backs in this approach.
First, experiments investi-gating the integration of syntactic information onboth sides generally report quality deterioration.For example, Lavie et al (2008), Liu et al (2009),and Chiang (2010) noted that translation qualitytends to decrease in tree-to-tree systems becausethe rules become too restrictive.
Second, minimalrules (i.e., rules that cannot be obtained from otherextracted rules) typically consist of a few lexi-cal items only and are thus not the most suitableto translate idiomatic expressions and other fixedphrases.
To overcome these drawbacks, we abol-ish the syntactic information for the source sideand develop a string-to-tree variant of `MBOTs.In addition, we develop a new rule extraction algo-rithm that can also extract non-minimal rules.
Ingeneral, the number of extractable rules explodes,so our rule extraction places parameterized restric-tions on the extracted rules in the same spirit asin (Chiang, 2007).
In this manner, we combine theadvantages of the hierarchical phrase-based ap-proach on the source side and the tree-based ap-proach with discontinuiety on the target side.We evaluate our new system in 3 large-scale ex-periments using translation tasks, in which we ex-pect discontinuiety on the target.
MBOTs are pow-erful but asymmetric models since discontinuietyis available only on the target.
We chose to trans-late from English to German, Arabic, and Chi-nese.
In all experiments our new system signifi-cantly outperforms the string-to-tree syntax-basedcomponent (Hoang et al, 2009) of Moses.
The(potentially) discontiguous rules of our model arevery useful in these setups, which we confirm in aquantitative and qualitative analysis.2 Related workModern statistical machine translation sys-tems (Koehn, 2009) are based on differenttranslation models.
Syntax-based systems havebecome widely used because of their ability tohandle non-local reordering and other linguisticphenomena better than phrase-based models (Ochand Ney, 2004).
Synchronous tree substitutiongrammars (STSGs) of Eisner (2003) use a singlesource and target tree fragment per rule.
In con-trast, an `MBOT rule contains a single source tree815concludes X?
(VAFINist,NP,VPPPgeschlossen)X on X ?
(NP,PP?uber NN)human rights?
(NNMenschenrechte)the X?
(NPdie NN)Figure 1: Several valid rules for our MBOT.fragment and a sequence of target tree fragments.`MBOTs can also be understood as a restriction ofthe non-contiguous STSSGs of Sun et al (2009),which allow a sequence of source tree fragmentsand a sequence of target tree fragments.
`MBOTrules require exactly one source tree fragment.While the mentioned syntax-based models usetree fragments for source and target (tree-to-tree),Galley et al (2004) and Galley et al (2006) usesyntactic annotations only on the target languageside (string-to-tree).
Further research by DeNeefeet al (2007) revealed that adding non-minimalrules improves translation quality in this setting.Here we improve statistical machine translationin this setting even further using non-minimal`MBOT rules.3 Theoretical ModelAs our translation model, we use a string-to-treevariant of the shallow local multi bottom-up treetransducer of Braune et al (2013).
We will callour variant MBOT for simplicity.
Our MBOT isa synchronous grammar (Chiang, 2006) similar toa synchronous context-free grammar (SCFG), butinstead of a single source and target fragment perrule, our rules are of the form s ?
(t1, .
.
.
, tn)with a single source string s and potentially sev-eral target tree fragments t1, .
.
.
, tn.
Besides lex-ical items the source string can contain (severaloccurrences of) the placeholder X, which links tonon-lexical leaves in the target tree fragments.
Incontrast to an SCFG each placeholder can haveseveral such links.
However, each non-lexical leafin a target tree fragment has exactly one such linkto a placeholder X.
An MBOT is simply a finitecollection of such rules.
Several valid rules aredepicted in Figure 1.The sentential forms of our MBOTs, whichoccur during derivations, have exactly the sameshape as our rules and each rule is a sententialMatching sentential forms (underlining for emphasis):concludes X?
(VAFINist,NP,VPPP geschlossen)X on X ?
(NP,PP?uber NN)Combined sentential form:concludesX on X ?
(VAFINist,NP,VPPP?uber NNgeschlossen)Figure 2: Substitution of sentential forms.form.
We can combine sentential forms with thehelp of substitution (Chiang, 2006).
Roughlyspeaking, in a sentential form ?
we can replacea placeholder X that is linked (left-to-right) tonon-lexical leaves C1, .
.
.
, Ckin the target treefragments by the source string of any sententialform ?, whose roots of the target tree fragments(left-to-right) read C1, .
.
.
, Ck.
The target treefragments of ?
will replace the respective linkedleaves in the target tree fragments of the sententialform ?.
In other words, substitution has to respectthe symbols in the linked target tree fragments andall linked leaves are replaced at the same time.
Weillustrate substitution in Figure 2, where we re-place the placeholder X in the source string, whichis linked to the underlined leaves NP and PP in thetarget tree fragments.
The rule below (also in Fig-ure 1) is also a sentential form and matches sinceits (underlined) root labels of the target tree frag-ments read ?NP PP?.
Thus, we can substitute thelatter sentential form into the former and obtainthe sentential form shown at the bottom of Fig-ure 2.
Ideally, the substitution process is repeateduntil the complete source sentence is derived.4 Rule ExtractionThe rule extraction of Maletti (2011) extracts min-imal tree-to-tree rules, which are rules containingboth source and target tree fragments, from sen-tence pairs of a word-aligned and bi-parsed paral-lel corpus.
In particular, this requires parses forboth the source and the target language sentenceswhich adds a source for errors and specificity po-tentially leading to lower translation performanceand lower coverage (Wellington et al, 2006).
Chi-ang (2010) showed that string-to-tree systems?816that1concludes2the3debate4on5human6rights7TOP[1,7]PROAV[1,1]damit1VAFIN[2,2]ist2NP[3,4]ART[3,3]die3NN[4,4]Aussprache4VP[5,7]PP[5,6]APPR[5,5]?uber5NN[6,6]Menschenrechte6VVPP[7,7]geschlossen7Figure 3: Word-aligned sentence pair with target-side parse.which he calls fuzzy tree-to-tree-systems?
gen-erally yield higher translation quality compared tocorresponding tree-to-tree systems.For efficiency reasons the rule extraction ofMaletti (2011) only extracts minimal rules, whichare the smallest tree fragments compatible with thegiven word alignment and the parse trees.
Simi-larly, non-minimal rules are those that can be ob-tained from minimal rules by substitution.
In par-ticular, each lexical item of a sentence pair oc-curs in exactly one minimal rule extracted fromthat sentence pair.
However, minimal rules areespecially unsuitable for fixed phrases consistingof rare words because minimal rules encouragesmall fragments and thus word-by-word transla-tion.
Consequently, such fixed phrases will oftenbe assembled inconsistently by substitution fromsmall fragments.
Non-minimal rules encourage aconsistent translation by covering larger parts ofthe source sentence.Here we want to develop an efficient rule ex-traction procedure for our string-to-tree MBOTsthat avoids the mentioned drawbacks.
Natu-rally, we could substitute minimal rules into eachother to obtain non-minimal rules, but perform-ing substitution for all combinations is clearly in-tractable.
Instead we essentially follow the ap-proach of Koehn et al (2003), Och and Ney(2004), and Chiang (2007), which is based on con-sistently aligned phrase pairs.
Our training corpuscontains word-aligned sentence pairs ?e,A, f?,which contain a source language sentence e, atarget language sentence f , and an alignmentA ?
[1, `e] ?
[1, `f], where `eand `fare thelengths of the sentences e and f , respectively, and[i, i?]
= {j ?
Z | i ?
j ?
i?}
is the span (closedinterval of integers) from i to i?for all positive in-tegers i ?
i?.
Rules are extracted for each pairof the corpus, so in the following let ?e,A, f?
bea word-aligned sentence pair.
A source phraseis simply a span [i, i?]
?
[1, `e] and correspond-ingly, a target phrase is a span [j, j?]
?
[1, `f].A rule span is a pair ?p, ??
consisting of a sourcephrase p and a sequence ?
= p1?
?
?
pnof (non-overlapping) target phrases p1, .
.
.
, pn.
Spansoverlap if their intersection is non-empty.
If n = 1(i.e., there is exactly one target phrase in ?)
then?p, ??
is also a phrase pair (Koehn et al, 2003).We want to emphasize that formally phrases arespans and not the substrings occuring at that span.Next, we lift the notion of consistently alignedphrase pairs to our rule spans.
Simply put, fora consistently aligned rule span ?p, p1?
?
?
pn?
werequire that it respects the alignment A in thesense that the origin i of an alignment (i, j) ?
Ais covered by p if and only if the destination jis covered by p1, .
.
.
, pn.
Formally, the rulespan ?p, p1?
?
?
pn?
is consistently aligned if forevery (i, j) ?
A we have i ?
p if andonly if j ??nk=1pk.
For example, given theword-aligned sentence pair in Figure 3, the rulespan ?
[2, 4], [2, 4] [7, 7]?
is consistently aligned,whereas the phrase pair ?
[2, 4], [2, 7]?
is not.Our MBOTs use rules consisting of a sourcestring and a sequence of target tree fragments.The target trees are provided by a parser for thetarget language.
For each word-aligned sentencepair ?e,A, f?
we thus have a parse tree t for f .
Anexample is provided in Figure 3.
We omit a for-mal definition of trees, but recall that each node ?of the parse tree t governs a (unique) target phrase.In Figure 3 we have indicated those target phrases(spans) as subscript to the non-lexical node labels.A consistently aligned rule span ?p, p1?
?
?
pn?
of?e,A, f?
is compatible with t if there exist nodes?1, .
.
.
, ?nof t such that ?kgoverns pkfor all1 ?
k ?
n. For example, given the word-alignedsentence pair and parse tree t in Figure 3, the con-sistently aligned rule span ?
[2, 4], [2, 4] [7, 7]?
isnot compatible with t because there is no node in tthat governs [2, 4].
However, for the same data, therule span ?
[2, 4], [2, 2] [3, 4] [7, 7]?
is consistentlyaligned and compatible with t. The required nodesof t are labeled VAFIN, NP, VVPP.Now we are ready to start the rule extrac-tion.
For each consistently aligned rule span?p, p1?
?
?
pn?
that is compatible with t and each se-lection of nodes ?1, .
.
.
, ?nof t such that nkgov-erns pkfor each 1 ?
k ?
n, we can extract therule e(p)?
(flat(t?1), .
.
.
,flat(t?n)), where817Initial rules forrule span ?
[3, 3], [3, 3]?:the?
(ARTdie)rule span ?
[4, 4], [4, 4]?:debate?
(NNAussprache)rule span ?
[3, 4], [3, 4]?
:the debate?
(NPdie Aussprache)rule span ?
[5, 7], [5, 6]?
:on human rights?
(PP?uber Menschenrechte)rule span ?
[3, 7], [3, 4] [5, 6]?
:the debate on human rights?
(NPdie Aussprache,PP?uber Menschenrechte)rule span ?
[2, 2], [2, 2] [7, 7]?:concludes?
(VAFINist,VVPPgeschlossen)rule span ?
[2, 4], [2, 2] [3, 4] [7, 7]?
:concludes the debate?
(VAFINist,NPdie Aussprache,VVPPgeschlossen)rule span ?
[2, 7], [2, 7]?
:concludes the debate on human rights?
(VAFINist,NPdie Aussprache,VP?uber Menschenrechte geschlossen)Figure 4: Some initial rules extracted from the word-aligned sentence pair and parse of Figure 3.?
e(p) is the substring of e at span p,1?
flat(u) removes all internal nodes from u (allnodes except the root and the leaves), and?
t?is the subtree rooted in ?
for node ?
of t.The rules obtained in this manner are called initialrules for ?e,A, f?
and t. For example, for the rulespan ?
[2, 4], [2, 2] [3, 4] [7, 7]?
we can extract onlyone initial rule.
More precisely, we have?
e([2, 4]) = concludes the debate?
t?1= (VAFIN ist)?
t?2=(NP (ART die) (NN Aussprache)),?
and t?3= (VVPP geschlossen).The function flat leaves t?1and t?3unchanged,but flat(t?2) = (NP die Aussprache).
Thus, weobtain the boxed rule of Figure 4.Clearly, the initial rules are just the start be-cause they are completely lexical in the sense thatthey never contain the placeholder X in the sourcestring nor a non-lexical leaf in any output tree frag-ment.
We introduce non-lexical rules using thesame approach as for the hierarchical rules of Chi-ang (2007).
Roughly speaking, we obtain a newrule r?
?by ?excising?
an initial rule r from anotherrule r?and replacing the removed part by?
the placeholder X in the source string,?
the root label of the removed tree fragment inthe target tree fragments, and?
linking the removed parts appropriately,so that the flatted substitution of r into r?
?can1If p = [i, i?
], then e(p) = e[i, i?]
is the substring of eranging from the i-th token to the i?-th token.Extractable rule [top] and initial rule [bottom]:the debate on human rights ?
(NPdie Aussprache,PP?uber Menschenrechte)on human rights?
(PP?uber Menschenrechte)Extractable rule obtained after excision:the debate X ?
(NPdie Aussprache,PP)Figure 5: Excision of the middle initial rule fromthe topmost initial rule.
Substituting the middlerule into the result yields the topmost rule.yield r?.
This ?excision?
process is illustrated inFigure 5, where we remove the middle initial rulefrom the topmost initial rule.
The result is dis-played at the bottom in Figure 5.
Formally, the setof extractable rules R for a given word-alignedsentence pair ?e,A, f?
with parse tree t for f isthe smallest set subject to the following two con-ditions:?
Each initial rule is in R and thus extractable.?
For every initial rule r and extractable ruler??
R, any flat rule r?
?, into which we cansubstitute r to obtain ?
with flat(?)
= r?, isin R and thus extractable.2For our running example depicted in Figure 3 wedisplay some extractable rules in Figure 6.2A rule ?
= s?
(t1, .
.
.
, tn) is flat if flat(?)
= ?, whereflat(?)
= s?
(flat(t1), .
.
.
, flat(tn)).818Source string ?the debate?
:concludes Xon human rights?
(VAFINist,NP,VP?uber Menschenrechte geschlossen)Source string ?on human rights?
:concludes the debate X?
(VAFINist,NPdie Aussprache,VPPPgeschlossen)Source string ?the debate on human rights?
:concludes X?
(VAFINist,NP,VPPPgeschlossen)Figure 6: Extractable rules obtained by excising various initial rules (see Figure 4) from the initial ruledisplayed at the bottom of Figure 4.Unfortunately, already Chiang (2007) points outthat the set of all extractable rules is generallytoo large and keeping all extractable rules leads toslow training, slow decoding, and spurious ambi-guity.
Our MBOT rules are restricted by the parsetree for the target sentence, but the MBOT modelpermits additional flexibility due to the presenceof multiple target tree fragments.
Overall, we ex-perience the same problems, and consequently, inthe experiments we use the following additionalconstraints on rules s?
(t1, .
.
.
, tn):(a) We only consider source phrases p of length atmost 10 (i.e., i??
i < 10 for p = [i, i?
]).3(b) The source string s contains at most 5 occur-rences of lexical items or X (i.e.
`s?
5).
(c) The source string s cannot have consecu-tive Xs (i.e., XX is not a substring of s).
(d) The source string contains at least one lexicalitem that was aligned in ?e,A, f?.
(e) The left-most token of the source string s can-not be X (i.e., s[1, 1] 6= X).Our implementation can easily be modified to han-dle other constraints.
Figure 7 shows extractablerules violating those additional constraints.Table 1 gives an overview on how many rulesare extracted.
Our string-to-tree variant extracts12?17 times more rules than the minimal tree-to-tree rule extraction.
For our experiments (see Sec-tion 6), we filter all rule tables on the given input.The decoding times for the minimal `MBOT andour MBOT share the same order of magnitude.5 Model FeaturesFor each source language sentence e, we want todetermine its most likely translation?f given by?f = arg maxfp(f | e) = arg maxfp(e | f) ?
p(f)3Note that this restricts the set of initial rules.for some unknown probability distributions p. Weestimate p(e | f) ?p(f) by a log-linear combinationof features hi(?)
with weights ?iscored on senten-tial forms e?
(t) of our extracted MBOTM suchthat the leaves of t read (left-to-right) f .We use the decoder provided by MBOT-Mosesof Braune et al (2013) and its standard features,which includes all the common features (Koehn,2009) and a gap penalty 1001?c, where c is thenumber of target tree fragments that contributedto t. This feature discourages rules with many tar-get tree fragments.
As usual, all features are ob-tained as the product of the corresponding rule fea-tures for the rules used to derive e?
(t) by meansof substitution.
The rule weights for the transla-tion weights are obtained as relative frequenciesnormalized over all rules with the same right- andleft-hand side.
Good-Turing smoothing (Good,1953) is applied to all rules that were extracted atmost 10 times.
The lexical translation weights areobtained as usual.6 Experimental ResultsWe considered three reasonable baselines: (i) min-imal `MBOT, (ii) non-contiguous STSSG (Sun etal., 2009), or (iii) a string-to-tree Moses system.We decided against the minimal `MBOT as a base-line since tree-to-tree systems generally get lowerBLEU scores than string-to-tree systems.
We nev-ertheless present its BLEU scores (see Table 3).Unfortunately, we could not compare to Sun etal.
(2009) because their decoder and rule extrac-tion algorithms are not publicly available.
Fur-thermore, we have the impression that their systemdoes not scale well:?
Only around 240,000 training sentences wereused.
Our training data contains between1.8M and 5.7M sentence pairs.?
The development and test set were length-819violates (b):that concludes Xon human rights?
(PROAVdamit,VAFINist,NP,VP?uber Menschenrechte geschlossen)violates (c):concludes X X?
(VAFINist,NP,VPPPgeschlossen)violates (d):X?
(NP)violates (e):Xon human rights?
(NP,PP?uber Menschenrechte)Figure 7: Showing extractable rules violating the restrictions.Systemnumber of extracted rulesEnglish-To-German English-To-Arabic English-To-Chineseminimal tree-to-tree `MBOT 12,478,160 28,725,229 10,162,325non-minimal string-to-tree MBOT 143,661,376 491,307,787 162,240,663string-to-tree Moses 14,092,729 55,169,043 17,047,570Table 1: Overview of numbers of extracted rules with respect to the different extraction algorithms.ratio filtered to sentences up to 50 characters.We do not modify those sets.?
Only rules with at most one gap were al-lowed which would be equivalent to restrictthe number of target tree fragments to 2 inour system.Hence we decided to use a string-to-tree Mosessystem as baseline (see Section 6.1).6.1 SetupAs a baseline system for our experiments we usethe syntax-based component (Hoang et al, 2009)of the Moses toolkit (Koehn et al, 2007).
Oursystem is the presented translation system basedon MBOTs.
We use the MBOT-Moses decoder(Braune et al, 2013) which ?
similar to the base-line decoder ?
uses a CYK+ chart parsing algo-rithm using a standard X-style parse tree which issped up by cube pruning (Chiang, 2007) with in-tegrated language model scoring.Our and the baseline system use linguistic syn-tactic annotation (parses) only on the target side(string-to-tree).
During rule extraction we imposethe restrictions of Section 4.
Additional glue-rulesthat concatenate partial translations without per-forming any reordering are used in all systems.For all experiments (English-to-German,English-to-Arabic, and English-to-Chinese), thetraining data was length-ratio filtered.
The wordalignments were generated by GIZA++ (Ochand Ney, 2003) with the grow-diag-final-andheuristic (Koehn et al, 2005).
The followinglanguage-specific processing was performed.
TheGerman text was true-cased and the functionaland morphological annotations were removedfrom the parse.
The Arabic text was tokenizedwith MADA (Habash et al, 2009) and translit-erated according to Buckwalter (2002).
Finally,the Chinese text was word-segmented using theStanford Word Segmenter (Chang et al, 2008).In all experiments the feature weights ?iof thelog-linear model were trained using minimum er-ror rate training (Och, 2003).
The remaining infor-mation for the experiments is presented in Table 2.6.2 Quantitative AnalysisThe overall translation quality was measured with4-gram BLEU (Papineni et al, 2002) on true-cased data for German, on transliterated data forArabic, and on word-segmented data for Chinese.Significance was computed with Gimpel?s imple-mentation (Gimpel, 2011) of pairwise bootstrapresampling with 1,000 samples.
Table 3 lists theevaluation results.
In all three setups the MBOTsystem significantly outperforms the baseline.
ForGerman we obtain a BLEU score of 15.90 whichis a gain of 0.68 points.
For Arabic we get an in-crease of 0.78 points which results in 49.10 BLEU.For Chinese we obtain a score of 18.35 BLEUgaining 0.66 points.4We also trained a vanillaphrase-based system for each language pair on thesame data as described in Table 2.To demonstrate the usefulness of the multiple4NIST-08 also shows BLEU for word-segmented output(http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/mt08_official_results_v0.html).
Best constrained system: 17.69 BLEU; bestunconstrained system: 19.63 BLEU.820English to German English to Arabic English to Chinesetraining data 7th EuroParl corpus (Koehn, 2005) MultiUN corpus (Eisele and Chen, 2010)training data size ?
1.8M sentence pairs ?
5.7M sentence pairs ?
1.9M sentence pairstarget-side parser BitPar (Schmid, 2004) Berkeley parser (Petrov et al, 2006)language model 5-gram SRILM (Stolcke, 2002)add.
LM data WMT 2013 Arabic in MultiUN Chinese in MultiUNLM data size ?
57M sentences ?
9.7M sentences ?
9.5M sentencestuning data WMT 2013 cut from MultiUN NIST 2002, 2003, 2005tuning size 3,000 sentences 2,000 sentences 2,879 sentencestest data WMT 2013 (Bojar et al, 2013) cut from MultiUN NIST 2008 (NIST, 2010)test size 3,000 sentences 1,000 sentences 1,859 sentencesTable 2: Summary of the performed experiments.Language pair System BLEUEnglish-to-GermanMoses Baseline 15.22MBOT?15.90minimal `MBOT 14.09Phrase-based Moses 16.73English-to-ArabicMoses Baseline 48.32MBOT?49.10minimal `MBOT 32.88Phrase-based Moses 50.27English-to-ChineseMoses Baseline 17.69MBOT?18.35minimal `MBOT 12.01Phrase-based Moses 18.09Table 3: Evaluation results.
The starred resultsare statistically significant improvements over thebaseline (at confidence p < 1%).target tree fragments of MBOTs, we analyzed theMBOT rules that were used when decoding thetest set.
We distinguish several types of rules.
Arule is contiguous if it has only 1 target tree frag-ment.
All other rules are (potentially) discontigu-ous.
Moreover, lexical rules are rules whose leavesare exclusively lexical items.
All other rules (i.e.,those that contain at least one non-lexical leaf)are structural.
Table 4 reports how many rules ofeach type are used during decoding for both ourMBOT system and the minimal `MBOT.
Below,we focus on analyzing our MBOT system.
Outof the rules used for German, 27% were (poten-tially) discontiguous and 5% were structural.
ForArabic, we observe 67% discontiguous rules and26% structural rules.
For translating into Chinese30% discontiguous rules were used and the struc-tural rules account to 18%.
These numbers showthat the usage of discontiguous rules tunes to thespecific language pair.
For instance, Arabic uti-lizes them more compared to German and Chi-nese.
Furthermore, German uses a lot of lexicalrules which is probably due to the fact that it is amorphologically rich language.
On the other hand,Arabic and Chinese make good use of structuralrules.
In addition, Table 4 presents a finer-grainedanalysis based on the number of target tree frag-ments.
Only rules with at most 8 target tree frag-ments were used.
While German and Arabic seemto require some rules with 6 target tree fragments,Chinese probably does not.
We conclude that thenumber of target tree fragments can be restrictedto a language-pair specific number during rule ex-traction.6.3 Qualitative AnalysisIn this section, we inspect some English-to-German translations generated by the Moses base-line and our MBOT system in order to providesome evidence for linguistic constructions that oursystem handles better.
We identified (a) the real-ization of reflexive pronouns, relative pronouns,and particle verbs, (b) the realization of verbalmaterial, and (c) local and long distance reorder-ing to be better throughout than in the baselinesystem.
All examples are (parts of) translationsof sentences from the test data.
Ungrammaticalconstructions are enclosed in brackets and markedwith a star.
We focus on instances that seem rele-vant to the new ability to use non-minimal rules.We start with an example showing the realiza-tion of a reflexive pronoun.Source: Bitcoin differs from other types of virtual currency.Reference: Bitcoin unterscheidet sich von anderen Artenvirtueller W?ahrungen.Baseline: Bitcoin [unterscheidet]?von anderen Arten [dervirtuellen W?ahrung]?.821Target tree fragmentsLanguage pair System Type Lex Struct Total 2 3 4 5 ?
6English-to-Germanour cont.
27,351 635 27,986MBOT discont.
9,336 1,110 10,446 5,565 3,441 1,076 312 52minimal cont.
55,910 4,492 60,402`MBOT discont.
2,167 7,386 9,553 6,458 2,589 471 34 1English-to-Arabicour cont.
1,839 651 2,490MBOT discont.
3,670 1,324 4,994 3,008 1,269 528 153 36minimal cont.
18,389 2,855 21,244`MBOT discont.
1,138 1,920 3,058 2,525 455 67 8 3English-to-Chineseour cont.
17,135 1,585 18,720MBOT discont.
4,822 3,341 8,163 6,411 1,448 247 55 2minimal cont.
34,275 8,820 43,095`MBOT discont.
516 4,292 4,808 3,816 900 82 6 4Table 4: Number of rules per type used when decoding test (Lex = lexical rules; Struct = structural rules;[dis]cont.
= [dis]contiguous).MBOT: Bitcoin unterscheidet sich von anderen Arten [dervirtuellen W?ahrung]?.Here the baseline drops the reflexive pronoun sich,which is correctly realized by the MBOT system.The rule used is displayed in Figure 8.differs from other?
(VVFINunterscheidet,PRFsich,APPRvon,ADJAanderen)Figure 8: Rule realizing the reflexive pronoun.Next, we show a translation in which our systemcorrectly generates a whole verbal segment.Source: It turned out that not only .
.
.Reference: Es stellte sich heraus, dass nicht nur .
.
.Baseline: [Heraus,]?nicht nur .
.
.MBOT: Es stellte sich heraus, dass nicht nur .
.
.The baseline drops the verbal constructionwhereas the large non-minimal rule of Figure 9 al-lows our MBOT to avoid that drop.
Again, the re-quired reflexive pronoun sich is realized as well asthe necessary comma before the conjunction dass.It turned out that?
(PPEREs,VVFINstellte,PRFsich,PTKZUheraus,$,,,KOUSdass)Figure 9: MBOT rule for the verbal segment.Another feature of MBOT is its power to per-form long distance reordering with the help of sev-eral discontiguous output fragments.Source: .
.
.
weapons factories now, which do not endurecompetition on the international market and .
.
.Reference: .
.
.
R?ustungsfabriken, die der internationalenKonkurrenz nicht standhalten und .
.
.Baseline: .
.
.
[Waffen in den Fabriken nun]?, die nicht einemWettbewerb auf dem internationalen Markt []?und .
.
.MBOT: .
.
.
[Waffen Fabriken nun]?, die Konkurrenz auf deminternationalen Markt nicht ertragen und .
.
.Figure 10 shows the rules which enable theMBOT system to produce the correct reordering.which do not X ?
(PRELSdie,NPNP,PTKNEGnicht,VPVP)endure X ?
(NPNP,VPertragen)competition X ?
(NPKonkurrenz PP)on the international market ?
(PPauf dem internationalen Markt)Figure 10: Long distance reordering.7 ConclusionWe present an application of a string-to-tree vari-ant of local multi bottom-up tree transducers,which are tree-to-tree models, to statistical ma-chine translation.
Originally, only minimal ruleswere extracted, but to overcome the typicallylower translation quality of tree-to-tree systemsand minimal rules, we abolish the syntactic an-notation on the source side and develop a string-to-tree variant.
In addition, we present a new pa-822rameterized rule extraction that can extract non-minimal rules, which are particularly helpful fortranslating fixed phrases.
It would be interestingto know how much can be gained when using onlyone contribution at a time.
Hence, we will explorethe impact of string-to-tree and non-minimal rulesin isolation.We demonstrate that our new system signifi-cantly outperforms the standard Moses string-to-tree system on three different large-scale transla-tion tasks (English-to-German, English-to-Arabic,and English-to-Chinese) with a gain between 0.53and 0.87 BLEU points.
An analysis of the rulesused to decode the test sets suggests that the usageof discontiguous rules is tuned to each languagepair.
Furthermore, it shows that only discontigu-ous rules with at most 8 target tree fragments areused.
Thus, further research could investigate ahard limit on the number of target tree fragmentsduring rule extraction.
We also perform a manualinspection of the obtained translations and con-firm that our string-to-tree MBOT rules can ade-quately handle discontiguous phrases, which oc-cur frequently in German, Arabic, and Chinese.Other languages that exhibit such phenomena in-clude Czech, Dutch, Russian, and Polish.
Thus,we hope that our approach can also be applied suc-cessfully to other language pairs.To support further experimentation by thecommunity, we publicly release our de-veloped software and complete tool-chain(http://www.ims.uni-stuttgart.de/forschung/ressourcen/werkzeuge/mbotmoses.html).AcknowledgementThe authors would like to express their gratitudeto the reviewers for their helpful comments andRobin Kurtz for preparing the Arabic corpus.All authors were financially supported bythe German Research Foundation (DFG) grantMA 4959 / 1-1.ReferencesOnd?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Proc.8th WMT, pages 1?44.
Association for Computa-tional Linguistics.Fabienne Braune, Nina Seemann, Daniel Quernheim,and Andreas Maletti.
2013.
Shallow local multibottom-up tree transducers in statistical machinetranslation.
In Proc.
51st ACL, pages 811?821.
As-sociation for Computational Linguistics.Timothy Buckwalter.
2002.
Arabic translit-eration.
http://www.qamus.org/transliteration.htm.Pi-Chuan Chang, Michel Galley, and Christopher D.Manning.
2008.
Optimizing Chinese word segmen-tation for machine translation performance.
In Proc.3rd WMT, pages 224?232.
Association for Compu-tational Linguistics.David Chiang.
2006.
An introduction to synchronousgrammars.
In Proc.
44th ACL.
Association for Com-putational Linguistics.
Part of a tutorial given withKevin Knight.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proc.
48th ACL, pages 1443?1452.
Association for Computational Linguistics.Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What can syntax-based MT learnfrom phrase-based MT?
In Proc.
2007 EMNLP,pages 755?763.
Association for Computational Lin-guistics.Andreas Eisele and Yu Chen.
2010.
MultiUN: A mul-tilingual corpus from United Nation documents.
InProc.
7th LREC, pages 2868?2872.
European Lan-guage Resources Association.Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In Proc.
41stACL, pages 205?208.
Association for Computa-tional Linguistics.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proc.
2004 NAACL, pages 273?280.
Associationfor Computational Linguistics.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.44th ACL, pages 961?968.
Association for Compu-tational Linguistics.Kevin Gimpel.
2011.
Code for statistical significancetesting for MT evaluation metrics.
http://www.ark.cs.cmu.edu/MT/.Irving J.
Good.
1953.
The population frequencies ofspecies and the estimation of population parameters.Biometrika, 40(3?4):237?264.823Nizar Habash, Owen Rambow, and Ryan Roth.
2009.MADA+TOKAN: A toolkit for Arabic tokenization,diacritization, morphological disambiguation, POStagging, stemming and lemmatization.
In Proc.
2ndMEDAR, pages 102?109.
Association for Computa-tional Linguistics.Hieu Hoang, Philipp Koehn, and Adam Lopez.
2009.A unified framework for phrase-based, hierarchical,and syntax-based statistical machine translation.
InProc.
6th IWSLT, pages 152?159.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.2003 NAACL, pages 48?54.
Association for Compu-tational Linguistics.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descriptionfor the 2005 IWSLT Speech Translation Evaluation.In Proc.
2nd IWSLT, pages 68?75.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proc.
45th ACL, pages 177?180.
Associa-tion for Computational Linguistics.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proc.
10thMT Summit, pages 79?86.
Association for MachineTranslation in the Americas.Philipp Koehn.
2009.
Statistical Machine Translation.Cambridge University Press.Alon Lavie, Alok Parlikar, and Vamshi Ambati.
2008.Syntax-driven learning of sub-sentential translationequivalents and translation rules from parsed parallelcorpora.
In Proc.
2nd SSST, pages 87?95.
Associa-tion for Computational Linguistics.Yang Liu, Yajuan L?u, and Qun Liu.
2009.
Improvingtree-to-tree translation with packed forests.
In Proc.47th ACL, pages 558?566.
Association for Compu-tational Linguistics.Andreas Maletti.
2011.
How to train your multibottom-up tree transducer.
In Proc.
49th ACL, pages825?834.
Association for Computational Linguis-tics.NIST.
2010.
NIST 2002 [2003, 2005, 2008] open ma-chine translation evaluation.
Linguistic Data Con-sortium.
LDC2010T10 [T11, T14, T21].Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(4):417?449.Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
41st ACL,pages 160?167.
Association for Computational Lin-guistics.Kishore Papineni, Salim Roukos, Todd Ward, andWei jing Zhu.
2002.
BLEU: a method for auto-matic evaluation of machine translation.
In Proc.40th ACL, pages 311?318.
Association for Compu-tational Linguistics.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
44th ACL, pages433?440.
Association for Computational Linguis-tics.Helmut Schmid.
2004.
Efficient parsing of highlyambiguous context-free grammars with bit vectors.In Proc.
20th COLING, pages 162?168.
Associationfor Computational Linguistics.Andreas Stolcke.
2002.
SRILM ?
an extensiblelanguage modeling toolkit.
In Proc.
7th INTER-SPEECH, pages 257?286.Jun Sun, Min Zhang, and Chew Lim Tan.
2009.
A non-contiguous tree sequence alignment-based model forstatistical machine translation.
In Proc.
47th ACL,pages 914?922.
Association for Computational Lin-guistics.Benjamin Wellington, Sonjia Waxmonsky, and I. DanMelamed.
2006.
Empirical lower bounds on thecomplexity of translational equivalence.
In Proc.44th ACL, pages 977?984.
Association for Compu-tational Linguistics.824
