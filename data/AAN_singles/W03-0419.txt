Introduction to the CoNLL-2003 Shared Task:Language-Independent Named Entity RecognitionErik F. Tjong Kim Sang and Fien De MeulderCNTS - Language Technology GroupUniversity of Antwerp{erikt,fien.demeulder}@uia.ua.ac.beAbstractWe describe the CoNLL-2003 shared task:language-independent named entity recog-nition.
We give background information onthe data sets (English and German) andthe evaluation method, present a generaloverview of the systems that have takenpart in the task and discuss their perfor-mance.1 IntroductionNamed entities are phrases that contain the namesof persons, organizations and locations.
Example:[ORG U.N. ] official [PER Ekeus ] heads for[LOC Baghdad ] .This sentence contains three named entities: Ekeusis a person, U.N. is a organization and Baghdad isa location.
Named entity recognition is an impor-tant task of information extraction systems.
Therehas been a lot of work on named entity recognition,especially for English (see Borthwick (1999) for anoverview).
The Message Understanding Conferences(MUC) have offered developers the opportunity toevaluate systems for English on the same data in acompetition.
They have also produced a scheme forentity annotation (Chinchor et al, 1999).
More re-cently, there have been other system developmentcompetitions which dealt with different languages(IREX and CoNLL-2002).The shared task of CoNLL-2003 concernslanguage-independent named entity recognition.
Wewill concentrate on four types of named entities:persons, locations, organizations and names ofmiscellaneous entities that do not belong to the pre-vious three groups.
The shared task of CoNLL-2002dealt with named entity recognition for Spanish andDutch (Tjong Kim Sang, 2002).
The participantsof the 2003 shared task have been offered trainingand test data for two other European languages:English and German.
They have used the datafor developing a named-entity recognition systemthat includes a machine learning component.
Theshared task organizers were especially interested inapproaches that made use of resources other thanthe supplied training data, for example gazetteersand unannotated data.2 Data and EvaluationIn this section we discuss the sources of the datathat were used in this shared task, the preprocessingsteps we have performed on the data, the format ofthe data and the method that was used for evaluatingthe participating systems.2.1 DataThe CoNLL-2003 named entity data consists of eightfiles covering two languages: English and German1.For each of the languages there is a training file, a de-velopment file, a test file and a large file with unanno-tated data.
The learning methods were trained withthe training data.
The development data could beused for tuning the parameters of the learning meth-ods.
The challenge of this year?s shared task wasto incorporate the unannotated data in the learningprocess in one way or another.
When the best pa-rameters were found, the method could be trained onthe training data and tested on the test data.
Theresults of the different learning methods on the testsets are compared in the evaluation of the sharedtask.
The split between development data and testdata was chosen to avoid systems being tuned to thetest data.The English data was taken from the Reuters Cor-pus2.
This corpus consists of Reuters news stories1Data files (except the words) can be found onhttp://lcg-www.uia.ac.be/conll2003/ner/2http://www.reuters.com/researchandstandards/English data Articles Sentences TokensTraining set 946 14,987 203,621Development set 216 3,466 51,362Test set 231 3,684 46,435German data Articles Sentences TokensTraining set 553 12,705 206,931Development set 201 3,068 51,444Test set 155 3,160 51,943Table 1: Number of articles, sentences and tokens ineach data file.between August 1996 and August 1997.
For thetraining and development set, ten days?
worth of datawere taken from the files representing the end of Au-gust 1996.
For the test set, the texts were from De-cember 1996.
The preprocessed raw data covers themonth of September 1996.The text for the German data was taken from theECI Multilingual Text Corpus3.
This corpus consistsof texts in many languages.
The portion of data thatwas used for this task, was extracted from the Ger-man newspaper Frankfurter Rundshau.
All three ofthe training, development and test sets were takenfrom articles written in one week at the end of Au-gust 1992.
The raw data were taken from the monthsof September to December 1992.Table 1 contains an overview of the sizes of thedata files.
The unannotated data contain 17 milliontokens (English) and 14 million tokens (German).2.2 Data preprocessingThe participants were given access to the corpus af-ter some linguistic preprocessing had been done: forall data, a tokenizer, part-of-speech tagger, and achunker were applied to the raw data.
We createdtwo basic language-specific tokenizers for this sharedtask.
The English data was tagged and chunked bythe memory-based MBT tagger (Daelemans et al,2002).
The German data was lemmatized, taggedand chunked by the decision tree tagger Treetagger(Schmid, 1995).Named entity tagging of English and Germantraining, development, and test data, was done byhand at the University of Antwerp.
Mostly, MUCconventions were followed (Chinchor et al, 1999).An extra named entity category called MISC wasadded to denote all names which are not already inthe other categories.
This includes adjectives, likeItalian, and events, like 1000 Lakes Rally, making ita very diverse category.3http://www.ldc.upenn.edu/English data LOC MISC ORG PERTraining set 7140 3438 6321 6600Development set 1837 922 1341 1842Test set 1668 702 1661 1617German data LOC MISC ORG PERTraining set 4363 2288 2427 2773Development set 1181 1010 1241 1401Test set 1035 670 773 1195Table 2: Number of named entities per data file2.3 Data formatAll data files contain one word per line with emptylines representing sentence boundaries.
At the endof each line there is a tag which states whether thecurrent word is inside a named entity or not.
Thetag also encodes the type of named entity.
Here isan example sentence:U.N. NNP I-NP I-ORGofficial NN I-NP OEkeus NNP I-NP I-PERheads VBZ I-VP Ofor IN I-PP OBaghdad NNP I-NP I-LOC.
.
O OEach line contains four fields: the word, its part-of-speech tag, its chunk tag and its named entitytag.
Words tagged with O are outside of named en-tities and the I-XXX tag is used for words inside anamed entity of type XXX.
Whenever two entities oftype XXX are immediately next to each other, thefirst word of the second entity will be tagged B-XXXin order to show that it starts another entity.
Thedata contains entities of four types: persons (PER),organizations (ORG), locations (LOC) and miscel-laneous names (MISC).
This tagging scheme is theIOB scheme originally put forward by Ramshaw andMarcus (1995).
We assume that named entities arenon-recursive and non-overlapping.
When a namedentity is embedded in another named entity, usuallyonly the top level entity has been annotated.Table 2 contains an overview of the number ofnamed entities in each data file.2.4 EvaluationThe performance in this task is measured with F?=1rate:F?
=(?2 + 1) ?
precision ?
recall(?2 ?
precision + recall)(1)lex pos aff pre ort gaz chu pat cas tri bag quo docFlorian + + + + + + + - + - - - -Chieu + + + + + + - - - + - + +Klein + + + + - - - - - - - - -Zhang + + + + + + + - - + - - -Carreras (a) + + + + + + + + - + + - -Curran + + + + + + - + + - - - -Mayfield + + + + + - + + - - - + -Carreras (b) + + + + + - - + - - - - -McCallum + - - - + + - + - - - - -Bender + + - + + + + - - - - - -Munro + + + - - - + - + + + - -Wu + + + + + + - - - - - - -Whitelaw - - + + - - - - + - - - -Hendrickx + + + + + + + - - - - - -De Meulder + + + - + + + - + - - - -Hammerton + + - - - + + - - - - - -Table 3: Main features used by the the sixteen systems that participated in the CoNLL-2003 shared tasksorted by performance on the English test data.
Aff: affix information (n-grams); bag: bag of words; cas:global case information; chu: chunk tags; doc: global document information; gaz: gazetteers; lex: lexicalfeatures; ort: orthographic information; pat: orthographic patterns (like Aa0); pos: part-of-speech tags; pre:previously predicted NE tags; quo: flag signing that the word is between quotes; tri: trigger words.with ?=1 (Van Rijsbergen, 1975).
Precision is thepercentage of named entities found by the learningsystem that are correct.
Recall is the percentage ofnamed entities present in the corpus that are foundby the system.
A named entity is correct only if itis an exact match of the corresponding entity in thedata file.3 Participating SystemsSixteen systems have participated in the CoNLL-2003 shared task.
They employed a wide variety ofmachine learning techniques as well as system com-bination.
Most of the participants have attemptedto use information other than the available train-ing data.
This information included gazetteers andunannotated data, and there was one participantwho used the output of externally trained named en-tity recognition systems.3.1 Learning techniquesThe most frequently applied technique in theCoNLL-2003 shared task is the Maximum EntropyModel.
Five systems used this statistical learningmethod.
Three systems used Maximum EntropyModels in isolation (Bender et al, 2003; Chieu andNg, 2003; Curran and Clark, 2003).
Two moresystems used them in combination with other tech-niques (Florian et al, 2003; Klein et al, 2003).
Max-imum Entropy Models seem to be a good choice forthis kind of task: the top three results for Englishand the top two results for German were obtainedby participants who employed them in one way oranother.Hidden Markov Models were employed by four ofthe systems that took part in the shared task (Flo-rian et al, 2003; Klein et al, 2003; Mayfield et al,2003; Whitelaw and Patrick, 2003).
However, theywere always used in combination with other learningtechniques.
Klein et al (2003) also applied the re-lated Conditional Markov Models for combining clas-sifiers.Learning methods that were based on connection-ist approaches were applied by four systems.
Zhangand Johnson (2003) used robust risk minimization,which is a Winnow technique.
Florian et al (2003)employed the same technique in a combination oflearners.
Voted perceptrons were applied to theshared task data by Carreras et al (2003a) andHammerton used a recurrent neural network (LongShort-Term Memory) for finding named entities.Other learning approaches were employed less fre-quently.
Two teams used AdaBoost.MH (Carreraset al, 2003b; Wu et al, 2003) and two other groupsemployed memory-based learning (De Meulder andDaelemans, 2003; Hendrickx and Van den Bosch,2003).
Transformation-based learning (Florian etal., 2003), Support Vector Machines (Mayfield et al,2003) and Conditional Random Fields (McCallumand Li, 2003) were applied by one system each.Combination of different learning systems hasproven to be a good method for obtaining excellentresults.
Five participating groups have applied sys-tem combination.
Florian et al (2003) tested dif-ferent methods for combining the results of four sys-tems and found that robust risk minimization workedbest.
Klein et al (2003) employed a stacked learn-ing system which contains Hidden Markov Models,Maximum Entropy Models and Conditional MarkovModels.
Mayfield et al (2003) stacked two learnersand obtained better performance.
Wu et al (2003)applied both stacking and voting to three learners.Munro et al (2003) employed both voting and bag-ging for combining classifiers.3.2 FeaturesThe choice of the learning approach is important forobtaining a good system for recognizing named en-tities.
However, in the CoNLL-2002 shared task wefound out that choice of features is at least as impor-tant.
An overview of some of the types of featureschosen by the shared task participants, can be foundin Table 3.All participants used lexical features (words) ex-cept for Whitelaw and Patrick (2003) who imple-mented a character-based method.
Most of the sys-tems employed part-of-speech tags and two of themhave recomputed the English tags with better tag-gers (Hendrickx and Van den Bosch, 2003; Wu et al,2003).
Othographic information, affixes, gazetteersand chunk information were also incorporated inmost systems although one group reports that theavailable chunking information did not help (Wu etal., 2003) Other features were used less frequently.Table 3 does not reveal a single feature that wouldbe ideal for named entity recognition.3.3 External resourcesEleven of the sixteen participating teams have at-tempted to use information other than the trainingdata that was supplied for this shared task.
All in-cluded gazetteers in their systems.
Four groups ex-amined the usability of unannotated data, either forextracting training instances (Bender et al, 2003;Hendrickx and Van den Bosch, 2003) or obtainingextra named entities for gazetteers (De Meulder andDaelemans, 2003; McCallum and Li, 2003).
A rea-sonable number of groups have also employed unan-notated data for obtaining capitalization features forwords.
One participating team has used externallytrained named entity recognition systems for Englishas a part in a combined system (Florian et al, 2003).Table 4 shows the error reduction of the systemsG U E English GermanZhang + - - 19% 15%Florian + - + 27% 5%Chieu + - - 17% 7%Hammerton + - - 22% -Carreras (a) + - - 12% 8%Hendrickx + + - 7% 5%De Meulder + + - 8% 3%Bender + + - 3% 6%Curran + - - 1% -McCallum + + - ?
?Wu + - - ?
?Table 4: Error reduction for the two develop-ment data sets when using extra information likegazetteers (G), unannotated data (U) or externallydeveloped named entity recognizers (E).
The lineshave been sorted by the sum of the reduction per-centages for the two languages.with extra information compared to while using onlythe available training data.
The inclusion of ex-tra named entity recognition systems seems to haveworked well (Florian et al, 2003).
Generally the sys-tems that only used gazetteers seem to gain morethan systems that have used unannotated data forother purposes than obtaining capitalization infor-mation.
However, the gain differences between thetwo approaches are most obvious for English forwhich better gazetteers are available.
With the ex-ception of the result of Zhang and Johnson (2003),there is not much difference in the German resultsbetween the gains obtained by using gazetteers andthose obtained by using unannotated data.3.4 PerformancesA baseline rate was computed for the English and theGerman test sets.
It was produced by a system whichonly identified entities which had a unique class inthe training data.
If a phrase was part of more thanone entity, the system would select the longest one.All systems that participated in the shared task haveoutperformed the baseline system.For all the F?=1 rates we have estimated sig-nificance boundaries by using bootstrap resampling(Noreen, 1989).
From each output file of a system,250 random samples of sentences have been chosenand the distribution of the F?=1 rates in these sam-ples is assumed to be the distribution of the perfor-mance of the system.
We assume that performanceA is significantly different from performance B if Ais not within the center 90% of the distribution of B.The performances of the sixteen systems on thetwo test data sets can be found in Table 5.
For En-glish, the combined classifier of Florian et al (2003)achieved the highest overall F?=1 rate.
However,the difference between their performance and thatof the Maximum Entropy approach of Chieu and Ng(2003) is not significant.
An important feature of thebest system that other participants did not use, wasthe inclusion of the output of two externally trainednamed entity recognizers in the combination process.Florian et al (2003) have also obtained the highestF?=1 rate for the German data.
Here there is no sig-nificant difference between them and the systems ofKlein et al (2003) and Zhang and Johnson (2003).We have combined the results of the sixteen sys-tem in order to see if there was room for improve-ment.
We converted the output of the systems tothe same IOB tagging representation and searchedfor the set of systems from which the best tags forthe development data could be obtained with ma-jority voting.
The optimal set of systems was de-termined by performing a bidirectional hill-climbingsearch (Caruana and Freitag, 1994) with beam size 9,starting from zero features.
A majority vote of fivesystems (Chieu and Ng, 2003; Florian et al, 2003;Klein et al, 2003; McCallum and Li, 2003; Whitelawand Patrick, 2003) performed best on the Englishdevelopment data.
Another combination of five sys-tems (Carreras et al, 2003b; Mayfield et al, 2003;McCallum and Li, 2003; Munro et al, 2003; Zhangand Johnson, 2003) obtained the best result for theGerman development data.
We have performed amajority vote with these sets of systems on the re-lated test sets and obtained F?=1 rates of 90.30 forEnglish (14% error reduction compared with the bestsystem) and 74.17 for German (6% error reduction).4 Concluding RemarksWe have described the CoNLL-2003 shared task:language-independent named entity recognition.Sixteen systems have processed English and Germannamed entity data.
The best performance for bothlanguages has been obtained by a combined learn-ing system that used Maximum Entropy Models,transformation-based learning, Hidden Markov Mod-els as well as robust risk minimization (Florian et al,2003).
Apart from the training data, this system alsoemployed gazetteers and the output of two externallytrained named entity recognizers.
The performanceof the system of Chieu et al (2003) was not signif-icantly different from the best performance for En-glish and the method of Klein et al (2003) and theapproach of Zhang and Johnson (2003) were not sig-nificantly worse than the best result for German.Eleven teams have incorporated information otherthan the training data in their system.
Four of themhave obtained error reductions of 15% or more forEnglish and one has managed this for German.
Theresources used by these systems, gazetteers and ex-ternally trained named entity systems, still require alot of manual work.
Systems that employed unanno-tated data, obtained performance gains around 5%.The search for an excellent method for taking advan-tage of the fast amount of available raw text, remainsopen.AcknowledgementsTjong Kim Sang is financed by IWT STWW as aresearcher in the ATraNoS project.
De Meulder issupported by a BOF grant supplied by the Universityof Antwerp.ReferencesOliver Bender, Franz Josef Och, and Hermann Ney.2003.
Maximum Entropy Models for Named En-tity Recognition.
In Proceedings of CoNLL-2003.Andrew Borthwick.
1999.
A Maximum Entropy Ap-proach to Named Entity Recognition.
PhD thesis,New York University.Xavier Carreras, Llu?
?s Ma`rquez, and Llu?
?s Padro?.2003a.
Learning a Perceptron-Based Named En-tity Chunker via Online Recognition Feedback.
InProceedings of CoNLL-2003.Xavier Carreras, Llu?
?s Ma`rquez, and Llu?
?s Padro?.2003b.
A Simple Named Entity Extractor usingAdaBoost.
In Proceedings of CoNLL-2003.Rich Caruana and Dayne Freitag.
1994.
Greedy At-tribute Selection.
In Proceedings of the EleventhInternational Conference on Machine Learning,pages 28?36.
New Brunswick, NJ, USA, MorganKaufman.Hai Leong Chieu and Hwee Tou Ng.
2003.
NamedEntity Recognition with a Maximum Entropy Ap-proach.
In Proceedings of CoNLL-2003.Nancy Chinchor, Erica Brown, Lisa Ferro, and PattyRobinson.
1999.
1999 Named Entity RecognitionTask Definition.
MITRE and SAIC.James R. Curran and Stephen Clark.
2003.
Lan-guage Independent NER using a Maximum En-tropy Tagger.
In Proceedings of CoNLL-2003.Walter Daelemans, Jakub Zavrel, Ko van der Sloot,and Antal van den Bosch.
2002.
MBT: Memory-Based Tagger, version 1.0, Reference Guide.
ILKTechnical Report ILK-0209, University of Tilburg,The Netherlands.Fien De Meulder and Walter Daelemans.
2003.Memory-Based Named Entity Recognition usingUnannotated Data.
In Proceedings of CoNLL-2003.Radu Florian, Abe Ittycheriah, Hongyan Jing, andTong Zhang.
2003.
Named Entity Recognitionthrough Classifier Combination.
In Proceedings ofCoNLL-2003.James Hammerton.
2003.
Named Entity Recogni-tion with Long Short-Term Memory.
In Proceed-ings of CoNLL-2003.Iris Hendrickx and Antal van den Bosch.
2003.Memory-based one-step named-entity recognition:Effects of seed list features, classifier stacking, andunannotated data.
In Proceedings of CoNLL-2003.Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-pher D. Manning.
2003.
Named Entity Recogni-tion with Character-Level Models.
In Proceedingsof CoNLL-2003.James Mayfield, Paul McNamee, and Christine Pi-atko.
2003.
Named Entity Recognition using Hun-dreds of Thousands of Features.
In Proceedings ofCoNLL-2003.Andrew McCallum and Wei Li.
2003.
Early resultsfor Named Entity Recognition with ConditionalRandom Fields, Feature Induction and Web-Enhanced Lexicons.
In Proceedings of CoNLL-2003.Robert Munro, Daren Ler, and Jon Patrick.2003.
Meta-Learning Orthographic and Contex-tual Models for Language Independent Named En-tity Recognition.
In Proceedings of CoNLL-2003.Eric W. Noreen.
1989.
Computer-Intensive Methodsfor Testing Hypotheses.
John Wiley & Sons.Lance A. Ramshaw and Mitchell P. Marcus.1995.
Text Chunking Using Transformation-BasedLearning.
In Proceedings of the Third ACL Work-shop on Very Large Corpora, pages 82?94.
Cam-bridge, MA, USA.Helmut Schmid.
1995.
Improvements in Part-of-Speech Tagging with an Application to German.In Proceedings of EACL-SIGDAT 1995.
Dublin,Ireland.Erik F. Tjong Kim Sang.
2002.
Introduction to theCoNLL-2002 Shared Task: Language-IndependentNamed Entity Recognition.
In Proceedings ofCoNLL-2002, pages 155?158.
Taipei, Taiwan.C.J.
van Rijsbergen.
1975.
Information Retrieval.Buttersworth.English test Precision Recall F?=1Florian 88.99% 88.54% 88.76?0.7Chieu 88.12% 88.51% 88.31?0.7Klein 85.93% 86.21% 86.07?0.8Zhang 86.13% 84.88% 85.50?0.9Carreras (a) 84.05% 85.96% 85.00?0.8Curran 84.29% 85.50% 84.89?0.9Mayfield 84.45% 84.90% 84.67?1.0Carreras (b) 85.81% 82.84% 84.30?0.9McCallum 84.52% 83.55% 84.04?0.9Bender 84.68% 83.18% 83.92?1.0Munro 80.87% 84.21% 82.50?1.0Wu 82.02% 81.39% 81.70?0.9Whitelaw 81.60% 78.05% 79.78?1.0Hendrickx 76.33% 80.17% 78.20?1.0De Meulder 75.84% 78.13% 76.97?1.2Hammerton 69.09% 53.26% 60.15?1.3Baseline 71.91% 50.90% 59.61?1.2German test Precision Recall F?=1Florian 83.87% 63.71% 72.41?1.3Klein 80.38% 65.04% 71.90?1.2Zhang 82.00% 63.03% 71.27?1.5Mayfield 75.97% 64.82% 69.96?1.4Carreras (a) 75.47% 63.82% 69.15?1.3Bender 74.82% 63.82% 68.88?1.3Curran 75.61% 62.46% 68.41?1.4McCallum 75.97% 61.72% 68.11?1.4Munro 69.37% 66.21% 67.75?1.4Carreras (b) 77.83% 58.02% 66.48?1.5Wu 75.20% 59.35% 66.34?1.3Chieu 76.83% 57.34% 65.67?1.4Hendrickx 71.15% 56.55% 63.02?1.4De Meulder 63.93% 51.86% 57.27?1.6Whitelaw 71.05% 44.11% 54.43?1.4Hammerton 63.49% 38.25% 47.74?1.5Baseline 31.86% 28.89% 30.30?1.3Table 5: Overall precision, recall and F?=1 rates ob-tained by the sixteen participating systems on thetest data sets for the two languages in the CoNLL-2003 shared task.Casey Whitelaw and Jon Patrick.
2003.
Named En-tity Recognition Using a Character-based Proba-bilistic Approach.
In Proceedings of CoNLL-2003.Dekai Wu, Grace Ngai, and Marine Carpuat.
2003.A Stacked, Voted, Stacked Model for Named En-tity Recognition.
In Proceedings of CoNLL-2003.Tong Zhang and David Johnson.
2003.
A RobustRisk Minimization based Named Entity Recogni-tion System.
In Proceedings of CoNLL-2003.
