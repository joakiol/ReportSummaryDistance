Learning a Stopping Criterion for Active Learning for Word SenseDisambiguation and Text ClassificationJingbo Zhu   Huizhen WangNatural Language Processing LabNortheastern UniversityShenyang, Liaoning, P.R.China, 110004Zhujingbo@mail.neu.edu.cnwanghuizhen@mail.neu.edu.cnEduard HovyUniversity of Southern CaliforniaInformation Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292-6695hovy@isi.eduAbstractIn this paper, we address the problem ofknowing when to stop the process of activelearning.
We propose a new statisticallearning approach, called minimumexpected error strategy, to defining astopping criterion through estimation of theclassifier?s expected error on futureunlabeled examples in the active learningprocess.
In experiments on active learningfor word sense disambiguation and textclassification tasks, experimental resultsshow that the new proposed stoppingcriterion can reduce approximately 50%human labeling costs in word sensedisambiguation with degradation of 0.5%average accuracy, and approximately 90%costs in text classification with degradationof 2% average accuracy.1 IntroductionSupervised learning models set their parametersusing given labeled training data, and generallyoutperform unsupervised learning methods whentrained on equal amount of training data.
However,creating a large labeled training corpus is veryexpensive and time-consuming in some real-worldcases such as word sense disambiguation (WSD).Active learning is a promising way to minimizethe amount of human labeling effort by building ansystem that automatically selects the most informa-tive unlabeled example for human annotation ateach annotation cycle.
In recent years active learn-ing  has attracted a lot of research interest, and hasbeen studied in many natural language processing(NLP) tasks, such as text classification (TC)(Lewis and Gale, 1994; McCallum and Nigam,1998), chunking (Ngai and Yarowsky, 2000),named entity recognition (NER) (Shen et al, 2004;Tomanek et al, 2007), part-of-speech tagging(Engelson and Dagan, 1999), informationextraction (Thompson et  al., 1999), statisticalparsing (Steedman et al, 2003), and word sensedisambiguation (Zhu and Hovy, 2007).Previous studies reported that active learningcan help in reducing human labeling effort.
Withselective sampling techniques such as uncertaintysampling (Lewis and Gale, 1994) and committee-based sampling (McCallum and Nigam, 1998), thesize of the training data can be significantly re-duced for text classification (Lewis and Gale,1994; McCallum and Nigam, 1998), word sensedisambiguation (Chen, et al 2006; Zhu and Hovy,2007), and named entity recognition (Shen et al,2004; Tomanek et al, 2007) tasks.Interestingly, deciding when to stop activelearning is an issue seldom mentioned issue inthese studies.
However, it is an important practicaltopic, since it obviously makes no sense tocontinue the active learning procedure until thewhole corpus has been labeled.
How to define anadequate stopping criterion remains an unsolvedproblem in active learning.
In principle, this is aproblem of estimation of classifier effectiveness(Lewis and Gale, 1994).
However, in real-worldapplications, it is difficult to know when theclassifier reaches its maximum effectivenessbefore all unlabeled examples have beenannotated.
And when the unlabeled data setbecomes very large, full annotation is almostimpossible for human annotator.In this paper, we address the issue of a stoppingcriterion for active learning, and propose a newstatistical learning approach, called minimum ex-366pected error strategy, that defines a stopping crite-rion through estimation of the classifier?s expectederror on future unlabeled examples.
The intuition isthat the classifier reaches maximum effectivenesswhen it results in the lowest expected error onremaining unlabeled examples.
This proposedmethod is easy to implement, involves smalladditional computation costs, and can be applied toseveral different learners, such as Naive Bayes(NB), Maximum Entropy (ME), and SupportVector Machines (SVMs) models.
Comparing withthe confidence-based stopping criteria proposed byZhu and Hovy (2007), experimental results showthat the new proposed stopping criterion achievesbetter performance in active learning for both theWSD and TC tasks.2 Active Learning Process and Problemof General Stopping Criterion2.1 Active Learning ProcessActive learning is a two-step semi-supervisedlearning process in which a small number of la-beled samples and a large number of unlabeledexamples are first collected in the initializationstage, and a close-loop stage of query and retrain-ing is adopted.
The purpose of active learning is tominimize the amount of human labeling effort byhaving the system in each cycle automatically se-lect for human annotation the most informativeunannotated case.Procedure: Active Learning ProcessInput: initial small training set L, and pool ofunlabeled data set UUse L to train the initial classifier C (i.e.
a classi-fier for uncertainty sampling or a set of classifiersfor committee-based sampling)Repeat?
Use the current classifier C  to label allunlabeled examples in U?
Based on active learning rules R such as un-certainty sampling or committee-based sam-pling, present m top-ranked unlabeled ex-amples to oracle H for labeling?
Augment L with the m new examples, andremove them from U?
Use L to retrain the current classifier CUntil the predefined stopping criterion SC is met.Figure 1.
Active learning processIn this work, we are interested in selective sam-pling for pool-based active learning, and focus onuncertainty sampling (Lewis and Gale, 1994).
Thekey point is how to measure the uncertainty of anunlabeled example, in order to select a new exam-ple with maximum uncertainty to augment thetraining data.
The maximum uncertainty impliesthat the current classifier has the least confidencein its classification of this unlabeled example x.The well-known entropy is a good uncertaintymeasurement widely used in active learning:( ) ( | ) log ( | )y YUM x P y x P y x?= ??
(1)where P(y|x) is the a posteriori probability.
Wedenote the output class y?Y={y1, y2, ?, yk}.
UM isthe uncertainty measurement function based on theentropy estimation of the classifier?s posteriordistribution.2.2 General Stopping CriteriaAs shown in Fig.
1, the active learning processrepeatedly provides the most informative unlabeledexamples to an oracle for annotation, and updatethe training set, until the predefined stoppingcriterion SC is met.
In practice, it is not clear howmuch annotation is sufficient for inducing aclassifier with maximum effectiveness (Lewis andGale, 1994).
This procedure can be implementedby defining an appropriate stopping criterion foractive learning.In active learning process, a general stoppingcriterion SC can be defined as:1 (0 ,ALeffectiveness CSCotherwise) ??
?= ??
(2)where ?
is a user predefined constant and the func-tion effectiveness(C) evaluates the effectiveness ofthe current classifier.
The learning process endsonly if the stopping criterion function SCAL is equalto 1.
The value of constant ?
represents a tradeoffbetween the cost of annotation and the effective-ness of the resulting classifier.
A larger ?
wouldcause more unlabeled examples to be selected forhuman annotation, and the resulting classifierwould be more robust.
A smaller ?
means the re-sulting classifier would be less robust, and lessunlabeled examples would be selected to annotate.In previous work (Shen et al, 2004; Chen et al,2006; Li and Sethi, 2006; Tomanek et al, 2007),there are several common ways to define the func-367tion effectiveness(C).
First, previous work alwaysused a simple stopping condition, namely, whenthe training set reaches desirable size.
However, itis almost impossible to predefine an appropriatesize of desirable training data guaranteed to inducethe most effective classifier.
Secondly, the learningloop can end if no uncertain unlabeled examplescan be found in the pool.
That is, all informativeexamples have been selected for annotation.However, this situation seldom occurs in real-world applications.
Thirdly, the active learningprocess can stop if the targeted performance levelis achieved.
However, it is difficult to predefine anappropriate and achievable performance, since itshould depend on the problem at hand and theusers?
requirements.2.3 Problem of Performance EstimationAn appealing solution has the active learningprocess end when repeated cycles show nosignificant performance improvement on the testset.
However, there are two open problems.
Thefirst question is how to measure the performance ofa classifier in active learning.
The second one ishow to know when the resulting classifier reachesthe highest or adequate performance.
It seemsfeasible that a separate validation set can solveboth problems.
That is, the active learning processcan end if there is no significant performanceimprovement on the validation set.
But how manysamples are required for the pregiven separatevalidation set is an open question.
Too fewsamples may not be adequate for a reasonableestimation and may result in an incorrect result.Too many samples would cause additional highcost because the separate validation set is generallyconstructed manually in advance.3 Statistical Learning Approach3.1 Confidence-based StrategyTo avoid the problem of performance estimationmentioned above, Zhu and Hovy (2007) proposeda confidence-based framework to predict the upperbound and the lower bound for a stopping criterionin active learning.
The motivation is to assume thatthe current training data is sufficient to train theclassifier with maximum effectiveness if the cur-rent classifier already has acceptably strong confi-dence on its classification results for all remainedunlabeled data.The first method to estimate the confidence ofthe classifier is based on uncertainty measurement,considering whether the entropy of each selectedunlabeled example is less than a small predefinedthreshold.
Here we call it Entropy-MCS.
Thestopping criterion SC Entropy-MCS can be defined as:1 , ( )0 ,EEntropy MCSx U UM xSCotherwise???
?
?
?= ??
(3)where ?E is a user predefined entropy threshold andthe function UM(x) evaluates the uncertainty ofeach unlabeled example x.The second method to estimate the confidenceof the classifier is based on feedback from the ora-cle when the active learner asks for true labels forselected unlabeled examples, by consideringwhether the current trained classifier couldcorrectly predict the labels or the accuracyperformance of predictions on selected unlabeledexamples is already larger than a predefinedaccuracy threshold.
Here we call it OracleAcc-MCS.
The stopping criterion SCOracleAcc-MCS can bedefined as:1 (0 ,) AOracleAcc MCSOracleAcc CSCotherwise???
?= ??
(4)where ?A is a user predefined accuracy thresholdand function OracleAcc(C) evaluates accuracy per-formance of the classifier on these selected unla-beled examples through feedback of the Oracle.3.2 Minimum Expected Error StrategyIn fact, these above two confidence-based methodsdo not directly estimate classifier performance thatclosely reflects the classifier effectiveness, becausethey only consider entropy of each unlabeledexample and accuracy on selected informativeexamples at each iteration step.
In this section wetherefore propose a new statistical learning ap-proach to defining a stopping criterion through es-timation of the classifier?s expected error on allfuture unlabeled examples, which we call minimumexpected error strategy (MES).
The motivationbehind MES is that the classifier C (a classifier foruncertainty sampling or set of classifiers for com-mittee-based sampling) with maximum effective-ness is the one that results in the lowest expected368error on whole test set in the learning process.
Thestopping criterion SC MES is defined as:1 ( )0 ,errMESError CSCotherwise??
?= ??
(5)where ?err is a user predefined expected errorthreshold and the function Error(C) evaluates theexpected error of the classifier C that closely re-flects the classifier effectiveness.
So the key pointof defining MES-based stopping criterion SC MES ishow to calculate the function Error(C) that denotesthe expected error of the classifier C.Suppose given a training set L and an inputsample x, we can write the expected error of theclassifier C as follows:( ) ( ( ) | ) ( )Error C R C x x P x dx= ?
(6)where P(x) represents the known marginal distribu-tion of x.
C(x) represents the classifier?s decisionthat is one of k classes: y?Y={y1, y2, ?, yk}.
R(yi|x)denotes a conditional loss for classifying the inputsample x into a class yi that can be defined as1( | ) [ , ] ( | )ki jjR y x i j P y x?==?
(7)where P(yj|x) is the a posteriori probability pro-duced by the classifier C.
?
[i,j] represents a zero-one loss function for every class pair {i,j} that as-signs no loss to a correct classification, and assignsa unit loss to any error.In this paper, we focus on pool-based activelearning in which a large unlabeled data pool U isavailable, as described Fig.
1.
In active learningprocess, our interest is to estimate the classifier?sexpected error on future unlabeled examples in thepool U.
That is, we can stop the active learningprocess when the active learner results in the low-est expected error over the unlabeled examples inU.
The pool U can provide an estimate of P(x).
Sofor minimum error rate classification (Duda andHart.
1973) on unlabeled examples, the expectederror of the classifier C can be rewritten as1( ) (1 max ( | ))y Yx UError C P y xU ?
?= ??
(8)Assuming N unlabeled examples in the pool U,the total time is O(N) for automatically determin-ing whether the proposed stopping criterion SCMESis satisfied in the active learning.If the pool U is very large (e.g.
more than100000 examples), it would still cause high com-putation cost at each iteration of active learning.
Agood approximation is to estimate the expectederror of the classifier using a subset of the pool, notusing all unlabeled examples in U.
In practice, agood estimation of expected error can be formedwith few thousand examples.4 EvaluationIn this section, we evaluate the effectiveness ofthree stopping criteria for active learning for wordsense disambiguation and text classification asfollows:?
Entropy-MCS ?
stopping active learningprocess when the stopping criterion functionSCEntropy-MCS defined in (3) is equal to 1, where?E=0.01, 0.001,  0.0001.?
OracleAcc-MCS ?
stopping active learningprocess when the stopping criterion functionSCOracleAcc-MCS defined in (4) is equal to 1,where ?A=0.9, 1.0.?
MES ?
stopping active learning process whenthe stopping criterion function SCMES definedin (5) is equal to 1, where ?err=0.01, 0.001,0.0001.The purpose of defining stopping criterion ofactive learning is to study how much annotation issufficient for a specific task.
To comparativelyanalyze the effectiveness of each stopping criterion,a baseline stopping criterion is predefined as whenall unlabeled examples in the pool U are learned.Comparing with the baseline stopping criterion, abetter stopping criterion not only achieves almostthe same performance, but also has needed to learnfewer unlabeled examples when the active learningprocess is ended.
In other words, for a stoppingcriterion of active learning, the fewer unlabeledexamples that have been leaned when it is met, thebigger reduction in human labeling cost is made.In the following active learning experiments, a10 by 10-fold cross-validation was performed.
Allresults reported are the average of 10 trials in eachactive learning process.4.1 Word Sense DisambiguationThe first comparison experiment is active learningfor word sense disambiguation.
We utilize amaximum entropy (ME) model (Berger et al,1996) to design the basic classifier used in activelearning for WSD.
The advantage of the ME modelis the ability to freely incorporate features from369diverse sources into a single, well-grounded statis-tical model.
A publicly available ME toolkit(Zhang et.
al., 2004) was used in our experiments.In order to extract the linguistic features necessaryfor the ME model in WSD tasks, all sentences con-taining the target word are automatically part-of-speech (POS) tagged using the Brill POS tagger(Brill, 1992).
Three knowledge sources are used tocapture contextual information: unordered singlewords in topical context, POS of neighboringwords with position information, and local colloca-tions.
These are same as the knowledge sourcesused in (Lee and Ng, 2002) for supervised auto-mated WSD tasks.The data used for comparison experiments wasdeveloped as part of the OntoNotes project (Hovyet al, 2006), which uses the WSJ part of the PennTreebank (Marcus et al, 1993).
The senses ofnoun words occurring in OntoNotes are linked tothe Omega ontology (philpot et al, 2005).
InOntoNotes, at least two human annotatorsmanually annotate the coarse-grained senses ofselected nouns and verbs in their natural sentencecontext.
In this experiment, we used several tens ofthousands of annotated OntoNotes examples,covering in total 421 nouns with an inter-annotatoragreement rate of at least 90%.
We find that 302out of 421 nouns occurring in OntoNotes areambiguous, and thus are used in the followingWSD experiments.
For these 302 ambiguousnouns, there are 3.2 senses per noun, and 172instances per noun.The active learning algorithms start with arandomly chosen initial training set of 10 labeledsamples for each noun, and make 10 queries aftereach learning iteration.
Table 1 shows theeffectiveness of each stopping criterion tested onactive learning for WSD on these ambiguousnouns?
WSD tasks.
We analyze average accuracyperformance of the classifier and averagepercentage of unlabeled examples learned wheneach stopping criterion is satisfied in activelearning for WSD tasks.
All accuracies andpercentages reported in Table 1 are macro-averages over these 302 ambiguous nouns.Stopping Criterion Average accuracyAveragepercentageall unlabeled examples learned 87.3% 100%Entropy-MCS method (0.0001) 86.8% 81.8%Entropy-MCS method (0.001) 86.8% 75.8%Entropy-MCS method (0.01) 86.8% 68.6%OracleAcc-MCS method (0.9) 86.8% 56.5%OracleAcc-MCS method (1.0) 86.8% 62.4%MES method (0.0001) 86.8% 67.1%MES method (0.001) 86.8% 58.8%MES method (0.01) 86.8% 52.7%Table 1.
Effectiveness of each stopping criterion ofactive learning for WSD on OnteNotes.Table 1 shows that these stopping criteriaachieve the same accuracy of 86.8% which iswithin 0.5% of the accuracy of the baseline method(all unlabeled examples are labeled).
It is obviousthat these stopping criteria can help reduce the hu-man labeling costs, comparing with the baselinemethod.
The best criterion is MES method(?err=0.01), following by OracleAcc-MCS method(?A=0.9).
MES method (?err=0.01) and OracleAcc-MCS method (?A=0.9) can make 47.3% and 44.5%reductions in labeling costs, respectively.
Entropy-MCS method is apparently worse than MES andOracleAcc-MCS methods.
The best of theEntropy-MCS method is the one with ?E=0.01which makes approximately 1/3 reduction inlabeling costs.
We also can see from Table 1 thatfor Entropy-MCS and MES methods, reductionrate becomes smaller as the ?
becomes smaller.4.2 Text ClassificationThe second data set is for active learning for textclassification using the WebKB corpus 1(McCallum et al, 1998).
The WebKB dataset wasformed by web pages gathered from various uni-versity computer science departments.
In the fol-lowing active learning experiment, we use fourmost populous categories: student, faculty, courseand project, altogether containing 4,199 web pages.Following previous studies (McCallum et al,1998), we only remove those words that occurmerely once without using stemming or stop-list.The resulting vocabulary has 23,803 words.
In thedesign of the text classifier, the maximum entropymodel is also utilized, and no feature selectiontechnique is used.1 See http://www.cs.cmu.edu/~textlearning370The algorithm is initially given 20 labeled ex-amples, 5 from each class.
Table 2 shows theeffectiveness of each stopping criterion of activelearning for text classification on WebKB corpus.All results reported are the average of 10 trials.Stopping Criterion Average accuracyAveragepercentageall unlabeled examples learned 93.5% 100%Entropy-MCS method (0.0001) 92.5% 23.8%Entropy-MCS method (0.001) 92.4% 22.3%Entropy-MCS method (0.01) 92.5% 21.8%OracleAcc-MCS method (0.9) 91.5% 13.1%OracleAcc-MCS method (1.0) 92.5% 24.5%MES method (0.0001) 92.1% 17.9%MES method (0.001) 92.0% 15.6%MES method (0.01) 91.5% 10.9%Table 2.
Effectiveness of each stopping criterion ofactive learning for TC on WebKB corpus.From results shown in Table 2, we can see thatMES method (?err=0.01) already achieves 91.5%accuracy in 10.9% unlabeled examples learned.The accuracy of all unlabeled examples learned is93.5%.
This situation means the approximately90% remaining unlabeled examples only makeonly 2% performance improvement.
Like theresults of WSD shown in Table 1, for Entropy-MCS and MES methods used in active learning fortext classification tasks, the correspondingreduction rate becomes smaller as the value of ?becomes smaller.
MES method (?err=0.01) canmake approximately 90% reduction in human la-beling costs and results in 2% accuracy perform-ance degradation.
The Entropy-MCS method(?E=0.01) can make approximate 80% reduction incosts and results in 1% accuracy performancedegradation.
Unlike the results of WSD shown inTable 1, the OracleAcc-MCS method (?A=1.0)makes the smallest reduction rate of 75.5%.Actually in real-world applications, the selection ofa stopping criterion is a tradeoff issue betweenlabeling cost and effectiveness of the classifier.5 DiscussionIt is interesting to investigate the impact of per-formance change on defining a stopping criterion,so we show an example of active learning forWSD task in Fig.
2.0.80.820.840.860.880.90.920.940  20  40  60  80  100  120  140  160  180  200  220AccuracyNumber of Learned ExamplesActive Learning for WSD taskrate-nFigure 2.
An example of active learning for WSDon noun ?rate?
in OntoNotes.Fig.
2 shows that the accuracy performance gen-erally increases, but apparently degrades at the it-erations ?20?, ?80?, ?170?, ?190?, and ?200?, anddoes not change anymore during the iterations[?130?-?150?]
or [?200?-?220?]
in the active learn-ing process.
Actually the first time of the highestperformance of 95% achieved is at ?450?, which isnot shown in Fig.
2.
In other words, although theaccuracy performance curve shows an increasingtrend, it is not monotonously increasing.
From Fig.2 we can see that it is not easy to automaticallydetermine the point of no significant performanceimprovement on the validation set, because pointssuch as ?20?
or ?80?
would mislead final judgment.However, we do believe that the change of per-formance is a good signal to stop active learningprocess.
So it is worth studying further how tocombine the factor of performance change with ourproposed stopping criteria of active learning.The OracleAcc-MCS method would not work ifonly one or too few informative examples arequeried at the each iteration step in the activelearning.
There is an open issue how many selectedunlabeled examples at each iteration are adequatefor the batch-based sample selection.For these stopping crieria, there is no generalmethod to automatically determine the bestthreshold for any given task.
It may therefore benecessary to use a dynamic threshold change tech-nique in which the predefined threshold can beautomatically modified if the performance is stillsignificantly improving when the stopping crite-rion is met during active learning process.3716 Conclusion and Future WorkIn this paper, we address the stopping criterion is-sue of active learning, and analyze the problemsfaced by some common ways to stop the activelearning process.
In essence, defining a stoppingcriterion of active learning is a problem of estimat-ing classifier effectiveness.
The purpose of defin-ing stopping criterion of active learning is to knowhow much annotation is sufficient for a special task.To determine this, this paper proposes a new statis-tical learning approach, called minimum expectederror strategy, for defining a stopping criterionthrough estimation of the classifier?s expected er-ror on future unlabeled examples during the activelearning process.
Experimental results on wordsense disambiguation and text classification tasksshow that new proposed minimum expected errorstrategy outperforms the confidence-based strategy,and achieves promising results.
The interestingfuture work is to study how to combine the best ofboth strategies, and how to consider performancechange to define an appropriate stopping criterionfor active learning.AcknowledgmentsThis work was supported in part by the NationalNatural Science Foundation of China under Grant(60473140), the National 863 High-tech Project(2006AA01Z154); the Program for New CenturyExcellent Talents in University(NCET-05-0287).ReferencesA.
L. Berger, S. A. Della, and V. J  Della.
1996.
Amaximum entropy approach to natural languageprocessing.
Computational Linguistics 22(1):39?71.E Brill.
1992.
A simple rule-based part of speech tag-ger.
In the Proceedings of the Third Conference onApplied Natural Language Processing.J.
Chen, A. Schein, L. Ungar, M. Palmer.
2006.
Anempirical study of the behavior of active learning forword sense disambiguation.
In Proc.
of HLT-NAACL06R.
O. Duda and P. E. Hart.
1973.
Pattern classificationand scene analysis.
New York: Wiley.S.
A. Engelson and I. Dagan.
1999.
Committee-basedsample selection for probabilistic classifiers.
Journalof Artificial Intelligence Research.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw and R.Weischedel.
2006.
Ontonotes: The 90% Solution.
InProc.
of HLT-NAACL06.Y.K.
Lee and.
H.T.
Ng.
2002.
An empirical evaluationof knowledge sources and learning algorithm forword sense disambiguation.
In Proc.
of EMNLP02D.
D. Lewis and W. A. Gale.
1994.
A sequential algo-rithm for training text classifiers.
In Proc.
of SIGIR-94M.
Li, I. K. Sethi.
2006.
Confidence-based active learn-ing.
IEEE transaction on pattern analysis and ma-chine intelligence, 28(8):1251-1261.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of English:the Penn Treebank.
Computational Linguistics,19(2):313-330A.
McCallum and K. Nigram.
1998.
Employing EM inpool-based active learning for text classification.
InProc.
of 15th ICMLG.
Ngai and D. Yarowsky.
2000.
Rule writing or anno-tation: cost-efficient resource usage for based nounphrase chunking.
In Proc.
of ACL-02A.
Philpot, E. Hovy and P. Pantel.
2005.
The OmegaOntology.
In Proc.
of ONTOLEX Workshop atIJCNLP.D.
Shen, J. Zhang, J. Su, G. Zhou and C. Tan.
2004.Multi-criteria-based active learning for named entityrecognition.
In Prof. of ACL-04.M.
Steedman, R. Hwa, S. Clark, M. Osborne, A. Sakar,J.
Hockenmaier, P. Ruhlen, S. Baker and J. Crim.2003.
Example selection for bootstrapping statisticalparsers.
In Proc.
of HLT-NAACL-03C.
A. Thompson, M. E. Califf and R. J. Mooney.
1999.Active learning for natural language parsing and in-formation extraction.
In Proc.
of ICML-99.K.
Tomanek, J. Wermter and U. Hahn.
2007.
An ap-proach to text corpus construction which cuts anno-tation costs and maintains reusability of annotateddata.
In Proc.
of EMNLP/CoNLL07L.
Zhang, J. Zhu, and T. Yao.
2004.
An evaluation ofstatistical spam filtering techniques.
ACM Transac-tions on Asian Language Information Processing,3(4):243?269.J.
Zhu, E. Hovy.
2007.
Active learning for word sensedisambiguation with methods for addressing theclass imbalance problem.
In Proc.
ofEMNLP/CoNLL07372
