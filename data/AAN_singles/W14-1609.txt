Proceedings of the Eighteenth Conference on Computational Language Learning, pages 78?86,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsLexicon Infused Phrase Embeddings for Named Entity ResolutionAlexandre Passos, Vineet Kumar, Andrew McCallumSchool of Computer ScienceUniversity of Massachusetts, Amherst{apassos,vineet,mccallum}@cs.umass.eduAbstractMost state-of-the-art approaches fornamed-entity recognition (NER) use semisupervised information in the form ofword clusters and lexicons.
Recentlyneural network-based language modelshave been explored, as they as a byprod-uct generate highly informative vectorrepresentations for words, known as wordembeddings.
In this paper we presenttwo contributions: a new form of learn-ing word embeddings that can leverageinformation from relevant lexicons toimprove the representations, and the firstsystem to use neural word embeddingsto achieve state-of-the-art results onnamed-entity recognition in both CoNLLand Ontonotes NER.
Our system achievesan F1 score of 90.90 on the test set forCoNLL 2003?significantly better thanany previous system trained on publicdata, and matching a system employingmassive private industrial query-log data.1 IntroductionIn many natural language processing tasks, suchas named-entity recognition or coreference reso-lution, syntax alone is not enough to build a highperformance system; some external source of in-formation is required.
In most state-of-the-artsystems for named-entity recognition (NER) thisknowledge comes in two forms: domain-specificlexicons (lists of word types related to the de-sired named entity types) and word representa-tions (either clusterings or vectorial representa-tions of word types which capture some of theirsyntactic and semantic behavior and allow gener-alizing to unseen word types).Current state-of-the-art named entity recogni-tion systems use Brown clusters as the form ofword representation (Ratinov and Roth, 2009;Turian et al., 2010; Miller et al., 2004; Brown etal., 1992), or other cluster-based representationscomputed from private data (Lin and Wu, 2009).While very attractive due to their simplicity, gen-erality, and hierarchical structure, Brown clustersare limited because the computational complex-ity of fitting a model scales quadratically with thenumber of words in the corpus, or the number of?base clusters?
in some efficient implementations,making it infeasible to train it on large corpora orwith millions of word types.Although some attempts have been made totrain named-entity recognition systems with otherforms of word representations, most notably thoseobtained from training neural language models(Turian et al., 2010; Collobert and Weston, 2008),these systems have historically underperformedsimple applications of Brown clusters.
A disad-vantage of neural language models is that, whilethey are inherently more scalable than Brown clus-ters, training large neural networks is still oftenexpensive; for example, Turian et al (2010) re-port that some models took multiple days or weeksto produce acceptable representations.
Moreover,language embeddings learned from neural net-works tend to behave in a ?nonlinear?
fashion, asthey are trained to encourage a many-layered neu-ral network to assign high probability to the data.These neural networks can detect nonlinear rela-tionships between the embeddings, which is notpossible in a log-linear model such as a condi-tional random field, and therefore limiting howmuch information from the embeddings can be ac-tually leveraged.Recently Mikolov et al (Mikolov et al., 2013a;78Mikolov et al., 2013b) proposed two simple log-linear language models, the CBOW model and theSkip-Gram model, that are simplifications of neu-ral language models, and which can be very effi-ciently trained on large amounts of data.
For ex-ample it is possible to train a Skip-gram modelover more than a billion tokens with a single ma-chine in less than half a day.
These embeddingscan also be trained on phrases instead of individualword types, allowing for fine granularity of mean-ing.In this paper we make the following contribu-tions.
(1) We show how to extend the Skip-Gramlanguage model by injecting supervisory train-ing signal from a collection of curated lexicons?effectively encouraging training to learn similarembeddings for phrases which occur in the samelexicons.
(2) We demonstrate that this methodoutperforms a simple application of the Skip-Gram model on the semantic similarity task onwhich it was originally tested.
(3) We show thata linear-chain CRF is able to successfully usethese log-linearly-trained embeddings better thanthe other neural-network-trained embeddings.
(4)We show that lexicon-infused embeddings let useasily build a new highest-performing named en-tity recognition system on CoNLL 2003 data(Tjong Kim Sang and De Meulder, 2003) whichis trained using only publicly available data.
(5)We also present results on the relatively under-studied Ontonotes NER task (Weischedel et al.,2011), where we show that our embeddings out-perform Brown clusters.2 Background and Related Work2.1 Language models and word embeddingsA statistical language model is a way to assignprobabilities to all possible documents in a givenlanguage.
Most such models can be classifiedin one of two categories: they can directly as-sign probabilities to sequences of word types, suchas is done in n-gram models, or they can oper-ate in a lower-dimensional latent space, to whichword types are mapped.
While most state-of-the-art language models are n-gram models, therepresentations used in models of the latter cate-gory, henceforth referred to as ?embeddings,?
havebeen found to be useful in many NLP applicationswhich don?t actually need a language model.
Theunderlying intuition is that when language modelscompress the information about the word types ina latent space they capture much of the common-alities and differences between word types.
Hencefeatures extracted from these models then can gen-eralize better than features derived from the wordtypes themselves.One simple language model that discovers use-ful embeddings is known as Brown clustering(Brown et al., 1992).
A Brown clustering is aclass-based bigram model in which (1) the prob-ability of a document is the product of the proba-bilities of its bigrams, (2) the probability of eachbigram is the product of the probability of a bi-gram model over latent classes and the probabilityof each class generating the actual word types inthe bigram, and (3) each word type has non-zeroprobability only on a single class.
Given a one-to-one assignment of word types to classes, then, anda corpus of text, it is easy to estimate these proba-bilities with maximum likelihood by counting thefrequencies of the different class bigrams and thefrequencies of word tokens of each type in the cor-pus.
The Brown clustering algorithm works bystarting with an initial assignment of word typesto classes (which is usually either one unique classper type or a small number of seed classes corre-sponding to the most frequent types in the corpus),and then iteratively selecting the pair of classes tomerge that would lead to the highest post-mergelog-likelihood, doing so until all classes have beenmerged.
This process produces a hierarchical clus-tering of the word types in the corpus, and theseclusterings have been found useful in many appli-cations (Ratinov and Roth, 2009; Koo et al., 2008;Miller et al., 2004).
There are other similar modelsof distributional clustering of English words whichcan be similarly effective (Pereira et al., 1993).One limitation of Brown clusters is their com-putational complexity, as training takes O(kV2+N)x time to train, where k is the number of baseclusters, V size of vocabulary, and N number oftokens.
This is infeasible for large corpora withmillions of word types.Another family of language models that pro-duces embeddings is the neural language mod-els.
Neural language models generally work bymapping each word type to a vector in a low-dimensional vector space and assigning probabil-ities to n-grams by processing their embeddingsin a neural network.
Many different neural lan-guage models have been proposed (Bengio et al.,2003; Morin and Bengio, 2005; Bengio, 2008;79Mnih and Hinton, 2008; Collobert and Weston,2008; Mikolov et al., 2010).
While they can cap-ture the semantics of word types, and often gen-eralize better than n-gram models in terms of per-plexity, applying them to NLP tasks has generallybeen less successful than Brown clusters (Turianet al., 2010).Finally, there are algorithms for computingword embeddings which do not use language mod-els at all.
A popular example is the CCA family ofword embeddings (Dhillon et al., 2012; Dhillon etal., 2011), which work by choosing embeddingsfor a word type that capture the correlations be-tween the embeddings of word types which occurbefore and after this type.2.2 The Skip-gram ModelA main limitation of neural language models isthat they often have many parameters and slowtraining times.
To mitigate this, Mikolov etal.
(2013a; 2013b) recently proposed a familyof log-linear language models inspired by neu-ral language models but designed for efficiency.These models operate on the assumption that, eventhough they are trained as language models, userswill only look at their embeddings, and hence allthey need is to produce good embeddings, and nothigh-accuracy language models.The most successful of these models isthe skip-gram model, which computes theprobability of each n-gram as the product ofthe conditional probabilities of each contextword in the n-gram conditioned on its centralword.
For example, the probability for the n-gram ?the cat ate my homework?
is represented asP (the|ate)P (cat|ate)P (my|ate)P (homework|ate).To compute these conditional probabilities themodel assigns an embedding to each word typeand defines a binary tree of logistic regressionclassifiers with each word type as a leaf.
Eachclassifier takes a word embedding as input andproduces a probability for a binary decision cor-responding to a branch in the tree.
Each leaf in thetree has a unique path from the root, which can beinterpreted as a set of (classifier,label) pairs.
Theskip-gram model then computes a probability of acontext word given a target word as the product ofthe probabilities, given the target word?s embed-dings, of all decisions on a path from the root tothe leaf corresponding to the context word.
Figure1 shows such a tree structured model.......A An San Diego New York City......Figure 1: A binary Huffman tree.
Circles repre-sent binary classifiers.
Rectangles represent to-kens, which can be multi-word.The likelihood of the data, then, given a set Nof n-grams, with mnbeing n-gram n?s middle-word, cneach context word, wcnithe parametersof the i-th classifier in the path from the root tocnin the tree, lcniits label (either 1 or ?1), eftheembedding of word type f , and ?
is the logisticsigmoid function, is?n?N?cn?n?i?(lcniwcniTemn).
(1)Given a tree, then, choosing embeddings emnand classifier parameters wcnito maximize equa-tion (1) is a non-convex optimization problemwhich can be solved with stochastic gradient de-scent.The binary tree used in the model is com-monly estimated by computing a Huffman codingtree (Huffman, 1952) of the word types and theirfrequencies.
We experimented with other tree esti-mation schemes but found no perceptible improve-ment in the quality of the embeddings.It is possible to extend these embeddings tomodel phrases as well as tokens.
To do so,Mikolov et al (2013b) use a phrase-building cri-terion based on the pointwise mutual informationof bigrams.
They perform multiple passes overa corpus to estimate trigrams and higher-orderphrases.
We instead consider candidate trigramsfor all pairs of bigrams which have a high PMIand share a token.2.3 Named Entity RecognitionNamed Entity Recognition (NER) is the task offinding all instances of explicitly named entitiesand their types in a given document.
While80detecting named entities is superficially simple,since most sequences of capitalized words arenamed entities (excluding headlines, sentence be-ginnings, and a few other exceptions), finding allentities is non trivial, and determining the correctnamed entity type can sometimes be surprisinglyhard.
Performing the task well often requires ex-ternal knowledge of some form.In this paper we evaluate our system on twolabeled datasets for NER: CoNLL 2003 (TjongKim Sang and De Meulder, 2003) and Ontonotes(Weischedel et al., 2011).
The CoNLL datasethas approximately 320k tokens, divided into 220ktokens for training, 55k tokens for development,and 50k tokens for testing.
While the training anddevelopment sets are quite similar, the test set issubstantially different, and performance on it de-pends strongly on how much external knowledgethe systems have.
The CoNLL dataset has fourentity types: PERSON, LOCATION, ORGANIZA-TION, AND MISCELLANEOUS.
The Ontonotesdataset is substantially larger: it has 1.6M tokenstotal, with 1.4M for training, 100K for develop-ment, and 130k for testing.
It also has eighteenentity types, a much larger set than the CoNLLdataset, including works of art, dates, cardinalnumbers, languages, and events.The performance of NER systems is commonlymeasured in terms of precision, recall, and F1 onthe sets of entities in the ground truth and returnedby the system.2.3.1 Baseline SystemIn this section we describe in detail the baselineNER system we use.
It is inspired by the systemdescribed in Ratinov and Roth (2009).Because NER annotations are commonly notnested (for example, in the text ?the US Army?,?US Army?
is treated as a single entity, insteadof the location ?US?
and the organization ?USArmy?)
it is possible to treat NER as a sequencelabeling problem, where each token in the sen-tence receives a label which depends on which en-tity type it belongs to and its position in the en-tity.
Following Ratinov and Roth (2009) we usethe BILOU encoding, where each token can eitherBEGIN an entity, be INSIDE an entity, be the LASTtoken in an entity, be OUTSIDE an entity, or be thesingle UNIQUE token in an entity.Our baseline architecture is a stacked linear-chain CRF (Lafferty et al., 2001) system: we traintwo CRFs, where the second CRF can conditionon the predictions made by the first CRF as well asfeatures of the data.
Both CRFs, following Zhangand Johnson (2003), have roughly similar features.While local features capture a lot of the cluesused in text to highlight named entities, they can-not necessarily disambiguate entity types or detectnamed entities in special positions, such as the firsttokens in a sentence.
To solve these problems mostNER systems incorporate some form of externalknowledge.
In our baseline system we use lexi-cons of months, days, person names, companies,job titles, places, events, organizations, books,films, and some minor others.
These lexicons weregathered from US Census data, Wikipedia cate-gory pages, and Wikipedia redirects (and will bemade publicly available upon publication).Following Ratinov and Roth (2009), we alsocompare the performance of our system with asystem using features based on the Brown clustersof the word types in a document.
Since, as seenin section 2.1, Brown clusters are hierarchical, weuse features corresponding to prefixes of the pathfrom the root to the leaf for each word type.More specifically, the feature templates of thebaseline system are as follows.
First for each tokenwe compute:?
its word type;?
word type, after excluding digits and lower-casing it;?
its capitalization pattern;?
whether it is punctuation;?
4-character prefixes and suffixes;?
character n-grams from length 2 to 5;?
whether it is in a wikipedia-extracted lexiconof person names (first, last, and honorifics),dates (months, years), place names (country,US state, city, place suffixes, general locationwords), organizations, and man-made things;?
whether it is a demonym.For each token?s label we have feature templatesconsidering all token?s features, all neighboringtoken?s features (up to distance 2), and bags ofwords of features of tokens in a window of size8 around each token.
We also add a feature mark-ing whether a token is the first occurrence of itsword type in a document.When using Brown clusters we add as tokenfeatures all prefixes of lengths 4, 6, 10, and 20,of its brown cluster.For the second-layer model we use all these fea-tures, as well as the label predicted for each token81Figure 2: Chain CRF model for a NER systemwith three tokens.
Filled rectangles represent fac-tors.
Circles at top represent labels, circles at bot-tom represent binary token based features.
Filledcircles indicate the phrase embeddings for each to-ken.by the first-layer model.As seen in the Experiments Section, our base-line system is competitive with state-of-the-artsystems which use similar forms of information.We train this system with stochastic gradient as-cent, using the AdaGrad RDA algorithm (Duchi etal., 2011), with both `1and `2regularization, au-tomatically tuned for each experimental setting bymeasuring performance on the development set.2.4 NER with Phrase EmbeddingsIn this section we describe how to extend our base-line NER system to use word embeddings as fea-tures.First we group the tokens into phrases, assign-ing to each token a single phrase greedily.
Weprefer shorter phrases over longer ones, sinceourembeddings are often more reliable for the shorterphrases, and since the longer phrases in our dic-tionary are mostly extracted from Wikipedia pagetitles, which are not always semantically meaning-ful when seen in free text.
We then add factorsconnecting each token?s label with the embeddingfor its phrase.Figure 2 shows how phrase embeddings areplugged into a chain-CRF based NER system.Following Turian (2010), we scale the embed-ding vector by a real number, which is a hyper-parameter tuned on the development data.
Con-necting tokens to phrase embeddings of theirneighboring tokens did not improve performancefor phrase embeddings, but it was mildly benefi-cial for token embeddings.3 Lexicon-infused Skip-gram ModelsThe Skip-gram model as defined in Section 2.2 isfundamentally trained in unsupervised fashion us-ing simply words and their n-gram contexts.
In-jecting some NER-specific supervision into theembeddings can make them more relevant to theNER task.Lexicons are a simple yet powerful way to pro-vide task-specific supervisory information to themodel without the burden of labeling additionaldata.
However, while lexicons have proven use-ful in various NLP tasks, a small amount of noisein a lexicon can severely impair the its usefulnessas a feature in log-linear models.
For example,even legitimate data, such as the Chinese last name?He?
occurring in a lexicon of person last names,can cause the lexicon feature to fire spuriouslyfor many training tokens that are labeled PERSON,and then this lexicon feature may be given low oreven negative weight.We propose to address both these problems byemploying lexicons as part of the word embeddingtraining.
The skip-gram model can be trained topredict not only neighboring words but also lexi-con membership of the central word (or phrase).The resulting embedding training will thus besomewhat supervised by tending to bring togetherthe vectors of words sharing a lexicon member-ship.
Furthermore, this type of training can effec-tively ?clean?
the influence of noisy lexicons be-cause even if ?He?
appears in the PERSON lexicon,it will have a sufficiently different context distribu-tion than labeled named person entities (e.g.
a lackof preceding honorifics, etc) that the presence ofthis noise in the lexicon will not be as problematicas it was previously.Furthermore, while Skip-gram models can betrained on billions of tokens to learn word em-beddings for over a million word types in a sin-gle day, this might not be enough data to cap-ture reliable embeddings of all relevant named en-tity phrases.
Certain sets of word types, such asnames of famous scientists, can occur infrequentlyenough that the Skip-gram model will not haveenough contextual examples to learn embeddingsthat highlight their relevant similarities.In this section we describe how to extend theSkip-gram model to incorporate auxiliary infor-mation from lexicons, or lists of related words, en-couraging the model to assign similar embeddingsto word types in similar lexicons.82New YorkThe ofstate is often referred............stateThe...New YorkUS-STATEWIKI-LOCATIONBUSINESSFigure 3: A Semi supervised Skip-gram Model.
?New York?
predicts the word ?state?.
Withlexicon-infusion, ?New York?
also predicts its lex-icon classes: US-State, Wiki-location.In the basic Skip-gram model, as seen in Sec-tion 2.2, the likelihood is, for each n-gram, a prod-uct of the probability of the embedding associatedwith the middle word conditioned on each contextword.
We can inject supervision in this model byalso predicting, given the embedding of the mid-dle word, whether it is a member of each lexicon.Figure 3 shows an example, where the word ?NewYork?
predicts ?state?, and also its lexicon classes:Business, US-State and Wiki-Location.Hence, with subscript s iterating over each lex-icon (or set of related words), and lmnsbeing a la-bel for whether each word is in the set, and wsindicating the parameters of its classifier, the fulllikelihood of the model is(2)?n ?N(?cn?n?i?(lcniwcniTemn))(?s?
(lmnswTsemn)).This is a simple modification to equation (1) thatalso predicts the lexicon memberships.
Note thatthe parameters wsof the auxiliary per-lexiconclassifiers are also learned.
The lexicons are notinserted in the binary tree with the words; instead,each lexicon gets its own binary classifier.Algorithm 1 Generating the training examples forlexicon-infused embeddings1: for all n-gram n with middle word mndo2: for all Context-word cndo3: for all Classifier, label pair (wcni,lcni)in the tree do4: Add training exampleemn, wcni, lcn5: end for6: end for7: for all Lexicon s, with label lmnsdo8: Add training example emn, ws, lmns9: end for10: end forIn practice, a very small fraction of words arepresent in a lexicon-class and this creates skewedtraining data, with overwhelmingly many negativeexamples.
We address this issue by aggressivelysub-sampling negative training data for each lex-icon class.
We do so by randomly selecting only1% of the possible negative lexicons for each to-ken.A Skip-gram model has V binary classifiers.
Alexicon-infused Skip-gram model predicts an ad-ditional K classes, and thus has V + K binaryclassifiers.
If number of classes K is large, we caninduce a tree over the classes, similarly to what isdone over words in the vocabulary.
In our trainedmodels, however, we have one million words inthe vocabulary and twenty-two lexicons, so this isnot necessary.4 ExperimentsOur phrase embeddings are learned on the combi-nation of English Wikipedia and the RCV1 Cor-pus (Lewis et al., 2004).
Wikipedia contains 8Marticles, and RCV1 contains 946K.
To get candi-date phrases we first select bigrams which havea pointwise mutual information score larger than1000.
We discard bigrams with stopwords from amanually selected list.
If two bigrams share a to-ken we add its corresponding trigram to our phraselist.
We further add page titles from the EnglishWikipedia to the list of candidate phrases, as wellas all word types.
We get a total of about 10Mphrases.
We restrict the vocabulary to the most fre-quent 1M phrases.
All our reported experimentsare on 50-dimensional embeddings.
Longer em-beddings, while performing better on the semanticsimilarity task, as seen in Mikolov et al (2013a;83Model AccuracySkip-Gram 29.89Lex-0.05 30.37Lex-0.01 30.72Table 1: Accuracy for Semantic-Syntactic task,when restricted to Top 30K words.
Lex-0.01 refersto a model trained with lexicons, where 0.01% ofnegative examples were used for training.2013b), did not perform as well on NER.To train phrase embeddings, we use a con-text of length 21.
We use lexicons derived fromWikipedia categories and data from the US Cen-sus, totaling K = 22 lexicon classes.
We use arandomly selected 0.01% of negative training ex-amples for lexicons.We perform two sets of experiments.
First, wevalidate our lexicon-infused phrase embeddingson a semantic similarity task, similar to Mikolov etal (Mikolov et al., 2013a).
Then we evaluate theirutility on two named-entity recognition tasks.For the NER Experiments, we use the base-line system as described in Section 2.3.1.
NERsystems marked as ?Skip-gram?
consider phraseembeddings; ?LexEmb?
consider lexicon-infusedembeddings; ?Brown?
use Brown clusters, and?Gaz?
use our lexicons as features.4.1 Syntactic and Semantic SimilarityMikolov et al.
(2013a) introduce a test set to mea-sure syntactic and semantic regularities for words.This set contains 8869 semantic and 10675 syn-tactic questions.
Each question consists of fourwords, such as big, biggest, small, smallest.
Itasks questions of the form ?What is the word thatis similar to small in the same sense as biggest issimilar to big?.
To test this, we compute the vec-tor X = vector(?biggest?)
?
vector(?big?)
+vector(?small?).
Next, we search for the wordclosest to X in terms of cosine distance (exclud-ing ?biggest?, ?small?, and ?big?).
This questionis considered correctly answered only if the clos-est word found is ?smallest?.
As in Mikolov etal (Mikolov et al., 2013a), we only search overwords which are among the 30K most frequentwords in the vocabulary.Table 1 depicts the accuracy on Semantic Syn-tactic Task for models trained with 50 dimensions.We find that lexicon-infused embeddings performbetter than Skip-gram.
Further, lex-0.01 performsSystem Dev TestBaseline 92.22 87.93Baseline + Brown 93.39 90.05Baseline + Skip-gram 93.68 89.68Baseline + LexEmb 93.81 89.56Baseline + Gaz 93.69 89.27Baseline + Gaz + Brown 93.88 90.67Baseline + Gaz + Skip-gram 94.23 90.33Baseline + Gaz + LexEmb 94.46 90.90Ando and Zhang (2005) 93.15 89.31Suzuki and Isozaki (2008) 94.48 89.92Ratinov and Roth (2009) 93.50 90.57Lin and Wu (2009) - 90.90Table 2: Final NER F1 scores for the CoNLL 2003shared task.
On the top are the systems presentedin this paper, and on the bottom we have base-line systems.
The best results within each area arehighlighted in bold.
Lin and Wu 2009 use massiveprivate industrial query-log data in training.the best, and we use this model for further NERexperiments.
There was no perceptible differencein computation cost from learning lexicon-infusedembeddings versus learning standard Skip-gramembeddings.4.2 CoNLL 2003 NERWe applied our models on CoNLL 2003 NER dataset.
All hyperparameters were tuned by trainingon training set, and evaluating on the developmentset.
Then the best hyperparameter values weretrained on the combination of training and devel-opment data and applied on the test set, to obtainthe final results.Table 2 shows the phrase F1 scores of all sys-tems we implemented, as well as state-of-the-art results from the literature.
Note that us-ing traditional unsupervised Skip-gram embed-dings is worse than Brown clusters.
In contrast,our lexicon-infused phrase embeddings Lex-0.01achieves 90.90?a state-of-the-art F1 score for thetest set.
This result matches the highest F1 previ-ously reported, in Lin and Wu (2009), but is thefirst system to do so without using massive privatedata.
Our result is signficantly better than the pre-vious best using public data.4.3 Ontonotes 5.0 NERSimilarly to the CoNLL NER setup, we tuned thehyperparameters on the development set.
We use84System Dev TestBaseline 79.04 79.85Baseline + Brown 79.95 81.38Baseline + Skip-gram 80.59 81.91Baseline + LexEmbd 80.65 81.82Baseline + Gaz 79.85 81.31Baseline + Gaz + Brown 80.53 82.05Baseline + Gaz + Skip-gram 80.70 82.30Baseline + Gaz + LexEmb 80.81 82.24Table 3: Final NER F1 scores for Ontonotes 5.0dataset.
The results in bold face are the best oneach evaluation set.the same list of lexicons as for CoNLL NER.Table 3 summarize our results.
We found thatboth Skip-gram and Lexicon infused embeddingsgive better results than using Brown Clusters asfeatures.
However, in this case Skip-gram embed-dings give marginally better results.
(So as not tojeopardize our ability to fairly do further researchon this task, we did not analyze the test set errorsthat may explain this.)
These are, to the best of ourknowledge, the first published performance num-bers on the Ontonotes NER task.5 ConclusionsWe have shown how to inject external supervisionto a Skip-gram model to learn better phrase em-beddings.
We demonstrate the quality of phraseembeddings on three tasks: Syntactic-semanticsimilarity, CoNLL 2003 NER, and Ontonotes 5.0NER.
In the process, we provide a new publicstate-of-the-art NER system for the widely con-tested CoNLL 2003 shared task.We demonstrate how we can plug phrase em-beddings into an existing log-linear CRF System.This work demonstrates that it is possible tolearn high-quality phrase embeddings and fine-tune them with external supervision from billionsof tokens within one day computation time.
Wefurther demonstrate that learning embeddings isimportant and key to improve NLP Tasks such asNER.In future, we want to explore employing embed-dings to other NLP tasks such as dependency pars-ing and coreference resolution.
We also want toexplore improving embeddings using error gradi-ents from NER.ReferencesRie Kubota Ando and Tong Zhang.
2005.
A high-performance semi-supervised learning method fortext chunking.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguis-tics, pages 1?9.
Association for Computational Lin-guistics.Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.2003.
A neural probabilistic language model.
Jour-nal of Machine Learning Research, 3:1137?1155.Yoshua Bengio.
2008.
Neural net language models.Scholarpedia, 3(1):3881.Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n-gram models of natural language.Computational linguistics, 18(4):467?479.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th international conference onMachine learning, pages 160?167.
ACM.Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.2011.
Multi-view learning of word embeddings viacca.
In Advances in Neural Information ProcessingSystems, pages 199?207.Paramveer Dhillon, Jordan Rodu, Dean Foster, andLyle Ungar.
2012.
Two step cca: A new spec-tral method for estimating vector models of words.arXiv preprint arXiv:1206.6403.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 999999:2121?2159.David A Huffman.
1952.
A method for the construc-tion of minimum-redundancy codes.
Proceedings ofthe IRE, 40(9):1098?1101.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.David D Lewis, Yiming Yang, Tony G Rose, and FanLi.
2004.
Rcv1: A new benchmark collection fortext categorization research.
The Journal of Ma-chine Learning Research, 5:361?397.Dekang Lin and Xiaoyun Wu.
2009.
Phrase clusteringfor discriminative learning.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 2-Volume 2, pages 1030?1038.
Association forComputational Linguistics.85Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, pages 1045?1048.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013a.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013b.
Distributed repre-sentations of words and phrases and their composi-tionality.
arXiv preprint arXiv:1310.4546.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrim-inative training.
In HLT-NAACL, volume 4, pages337?342.
Citeseer.Andriy Mnih and Geoffrey E Hinton.
2008.
A scal-able hierarchical distributed language model.
InAdvances in neural information processing systems,pages 1081?1088.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InProceedings of the international workshop on artifi-cial intelligence and statistics, pages 246?252.Fernando Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of english words.
InProceedings of the 31st annual meeting on Associa-tion for Computational Linguistics, pages 183?190.Association for Computational Linguistics.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProceedings of the Thirteenth Conference on Com-putational Natural Language Learning, pages 147?155.
Association for Computational Linguistics.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-word scale unlabeled data.
In ACL, pages 665?673.Citeseer.Erik F Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the conll-2003 shared task:Language-independent named entity recognition.
InProceedings of the seventh conference on Naturallanguage learning at HLT-NAACL 2003-Volume 4,pages 142?147.
Association for Computational Lin-guistics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 384?394.
Association forComputational Linguistics.Ralph Weischedel, Martha Palmer, Mitchell Marcus,Eduard Hovy, Sameer Pradhan, Lance Ramshaw,Nianwen Xue, Ann Taylor, Jeff Kaufman, MichelleFranchini, et al.
2011.
OntoNotes Release 4.0.
Lin-guistic Data Consortium.Tong Zhang and David Johnson.
2003.
A robustrisk minimization based named entity recognitionsystem.
In Proceedings of the seventh conferenceon Natural language learning at HLT-NAACL 2003-Volume 4, pages 204?207.
Association for Compu-tational Linguistics.86
