Linear Segmentation and Segment SignificanceMin-Yen Kan*, Judith L. Klavans** and Kathleen R. McKeown*Department of Computer Science* and Center for Research on Information Access**Columbia UniversityNew York, NY 10027, USA(min, klavans, kathy} @cs.
columbia, eduAbstractWe present a new method for discovering asegmental discourse structure of a documentwhile categorizing each segment's functionand importance.
Segments are determinedby a zero-sum weighting scheme, used onoccurrences of noun phrases andpronominal forms retrieved from thedocument.
Segment roles are thencalculated from the distribution of the termsin the segment.
Finally, we present resultsof evaluation in terms of precision andrecall which surpass earlier approaches'.IntroductionIdentification of discourse structure can beextremely useful to natural language processingapplications uch as automatic text summarizationor information retrieval (IR).
For example, asummarization agent might chose to summarizeeach discourse segment separately.
Also,segmentation of a document into blocks oftopically similar text can assist a search engine inchoosing to retrieve or highlight a segment inwhich a query term occurs.
In this paper, wepresent a topical segmentation program thatachieves a 10% increase in both precision andrecall over comparable previous work.In addition to segmenting, the system alsolabels the function of discovered discourse' This material is based upon work supported by theNational Science Foundation under Grant No.
(NSF#IRI-9618797) and by the Columbia University Centerfor Research on Information Access.segments as to their relevance towards the whole.It identifies 1) segments that contribute somedetail towards the main topic of the input, 2)segments that summarize the key points, and 3)segments that contain less important information.We evaluated our segment classification as part ofa summarization system that utilizes highlypertinent segments to extract key sentences.We investigated the applicability of thissystem on general domain news articles.
Generally,we found that longer articles, usually beyond athree-page limit, tended to have their own priorsegmentation markings consisting of headers orbullets, so these were excluded.
We thusconcentrated our work on a corpus of shorterarticles, averaging roughly 800-1500 words inlength: 15 from the Wall Street Journal in theLinguistic Data Consortium's 1988 collection, and5 from the on-line The Economist from 1997.
Weconstructed an evaluation standard from humansegmentation judgments to test our output.1 SEGMENTER: L inear  Segmentat ionFor the purposes of discourse structureidentification, we follow a formulation of theproblem similar to Hearst (1994), in which zero ormore segment boundaries are found at variousparagraph separations, which identify one or moretopical text segments.
Our segmentation is linear,rather than hierarchical (Marcu 1997 and Yaari1997), i.e.
the input article is divided into a linearsequence of adjacent segments.197Our segmentation methodology has threedistinct phases (Figure 1), which are executedsequentially.
We will describe ach of these phasesin detail.\[. "
- "~-~" -~Weigh  ~--~Score:'t \[Terms I I Terrn I I Segment I:egntsI I \[Links \] \ [Boundar ie \ ]Figure 1.
SEGMENTER Architecture1,1 Extract ing Useful TokensThe task of determining segmentation bre~sdepends fundamentally on extracting useful topicinformation from the text.
We extract threecategories of information, which reflect he topicalcontent of a text, to be referred to as terms for theremainder of the paper:"1. proper noun phrases;2. common oun phrases;3. personal and possessive pronouns.In order to find these three types of terms, we firsttag the text with part of speech (POS) information.Two methods were investigated for assigning POStags to the text: I) running a specialized taggingprogram or 2) using a simple POS table lookup.We chose to use the latter to assign tags for timeefficiency reasons (since the segmentation task isoften only a preprocessing stage), but optimizedthe POS table to favor high recall of the 3 termtypes, whenever possible 2.
The resulting systemwas faster than the initial prototype that used theformer approach by more than a magnitude, with aslight decline in precision that was not statisticallysignificant.
However, if a large system requiresaccurate tags after segmentation and the cost oftagging is not an issue, then tagging should beused instead of lookup.We based our POS table lookup on NYU's COMLEX(Grishman et al 1994).
After simplifying'COMLEX'scategories to only reflect information important o toour three term types, we flattened all multi-categorywords (i.e.
"jump" as V or N) to a single category by astrategy motivated to give high term recall (i.e.
'~jump"maps to N, because NP is a term type.
)Once POS tags have been assigned, we canretrieve occurrences of noun phrases by searchingthe document for this simple regular expression:(Adj I Noun)* NounThis expression captures a simple noun phrasewithout any complements.
More complex nounphrases uch as "proprietor of Stag's Leap WineCellars in Napa Valley" are captured as threedifferent phrases: "proprietor", Stag's Leap WineCellars" and "Napa Valley".
We deliberatelymade the regular expression less powerful tocapture as many noun phrases as possible, sincethe emphasis i  on high NP recall.After retrieving the terms, a post-processing phase combines related tokens together.For possessive pronouns, we merge ach possessivewith its appropriate personal pronoun ("my" or"mine" with 'T', etc.)
For noun phrases, wecanonicalize noun phrases according to their heads.For example, if the noun phrases "red wine" and"wine" are found in a text, we subsume theoccurrences of "red wine" into the occurrences of"wine", under the condition that there are no other"wine" headed phrases, such as "white wine".Finally, we perform thresholding to filterirrelevant words, following the guidelines et out byJusteson and Katz (1995).
We use a frequencythreshold of two occurrences to determinetopicality, and discard any pronouns or nounphrases that occur only once.1.2 Weight ing Term OccurrencesOnce extracted, terms are then evaluated to arriveat segmentation.1.2.1 lank LengthGiven a single term (noun phrase or pronominalform) and the distribution of its occurrences, welink related occurrences together.
We useproximity as our metric for relatedness.
If twooccurrences of a term occur within n sentences, welink them together as a single unit, and repeat untilno larger units can be built.
This idea is a simplerinterpretation of the notion of lexical chains.Morris and Hirst (1991) first proposed this notionto chain semantically related words together via a198thesaurus, while we chose only repetition of thesame stem word'.However, for these three categories ofterms we noticed that the linking distance differsdepending on the type of term in question, withproper nouns having the maximum allowabledistance and the pronominal forms having the least.Proper nouns generally refer to the same entity,almost regardless of the number of interveningsentences.
Common nouns often have a muchshorter scope of reference, since a single token canbe used to repeatedly refer to different instances ofits class.
Personal pronouns scope even moreclosely, as is expected of an anaphoric or referringexpression where the referent can be, by def'mition,different over an active discourse.
Any termoccurrences that were not linked were then droppedfrom further consideration.
Thus, link length orlinking distance refers to the number of sentencesallowed to intervene between two occurrences of aterm.1.2.2 Assigning WeightsAfter links are established, weighting is assigned.Since paragraph level boundaries are notconsidered in the previous tep, we now label eachparagraph with its positional relationship to eachterm's link(s).
We describe these four categoriesfor paragraph labeling and illustrate them in thefigure below.Front: a paragraph in which a link begins.During: a paragraph in which a link occurs, but isnot a front paragraph.Rear: a paragraph in which a link just stoppedoccurring the paragraph before.No link: any remaining paragraphs.paras  1 2 3 4 5 7 8sents  12345678901234567890123456789012345w ine  : Ixx l  ix21type : n f d r n f dFigure 2zt A term "wine", and its occurrences and type.We also tried to semantically cluster terms by usingMiller et al (1990)'s WordNet 1.5 with edge countingto determine relatedness, as suggested by Hearst(1997).
However, results showed only minorimprovement in precision and over a tenfold increasein execution time.Figure 2a shows the algorithm asdeveloped thus far in the paper, operating on theterm "wine".
The term appears a total of six times,as shown by the numbers in the central row.
Theseoccurrences have been grouped together into twoterm links, as joined by the "x"s. The bottom"type" line labels each paragraph with one of thefour paragraph relations.
We see that it is possiblefor a term to have multiple front or rearparagraphs, as illustrated, since a term'soccurrences might be separated between disparatelinks.Then, for each of the four categories ofparagraph labeling mentioned before, and for eachof the three term types, we assign a differentsegmentation score, listed in Table 1, whose valueswere derived by training, to be discussed in section1.2.4.i TermITypeProper NPCommon NP 10Pronouns & 1Possessives ~Table I - OverviewParagraph Type with Linkrespect o termfront \]rear during~o link Length10 8 -3 * 8!8 -3 * !413  -1 * 0of weighting and linking schemeused in SEGMENTER; star'red scores to be c.alculatlxl later.For noun phrases, we assume that the introductionof the term is a point at which a new topic maystart; this is Youmans's (1991) VocabularyManagement Profile.
Similarly, when a term is nolonger being used, as in rear paragraphs, the topicmay be closed.
This observation may not be asdirect as "vocabulary introduction", and thuspresumably not as strong a marker of topic changeas the former.
Moreover, paragraphs in which thelink persists throughout indicate that a topiccontinues; thus we see a negative score assigned toduring paragraphs.
When we apply the sameparagraph labeling to pronoun forms, the samerationale applies with some modifications.
Sincethe majority of pronoun referents occur before thepronoun (i.e.
anaphoric as opposed to cataphoric),we do not weigh the front boundary heavily, butinstead place the emphasis on the rear.1991.2.3.
Zero Sum NormalizationWhen we iterate the weighting process describedabove over each term, and total the scoresassigned, we come up with a numerical score foran indication of which paragraphs are more likelyto beh a topical boundary.
The higher thenumerical score, the higher the likelihood that theparagraph is a beginning of a new topical segment.The question then is what should the threshold be?paras  1 2 3 4 5 7 8sents  12345678901234567890123456789012345w ine  : ixxl  Ix21type : n f d r n f dscore:"  i0 -3 8 " i0 -3sum to ba lance  in zero -sum we ight ing :  +12zero  :-6 i0 -3 8 -6 i0 -3Figure 2b.
A term "wine", its links and scoreassignment to paragraphs.To solve this problem, we zero-sum the weightsfor each individual term.
To do this, we first sumthe total of all scores assigned to any front,, rearand during paragraphs that we have previouslyassigned a score to and then evenly distribute tothe remaining no link paragraphs the negative ofthis sum.
This ensures that the net sum of theweight assigned by the weighting of each termsums to zero, and thus the weighting of the entirearticle, also sums to zero.
In cases where no linkparagraphs do not exist for a term, we cannotperform zero-summing, and take the scoresassigned as is, but this is in small minority ofcases.
This process of weighting followed byzero-summing is shown by the extending the"wine" example, in Figure 2b, as indicated by thescore  and zero  lines.With respect o individual paragraphs, thesummed score results in a positive or negative total.A positive score indicates a boundary, i.e.
thebeginning of a new topical segment, whereas anegative score indicates the continuation of asegment.
This use of zero sum weighting makesthe problem of finding a threshold trivial, since thedata is normalized around the value zero.L2.4 Finding Local MaximaExamination of the output indicated that for longand medium length documents, zero-sumweighting would yield good results.
However, forthe documents we investigated, namely documentsof short length (800-1500 words), we haveobserved that multiple consecutive paragraphs, allwith a positive summed score, actually only havea single, true boundary.
In these cases, we takethe maximal valued paragraph for each of theseclusters of positive valued paragraphs as the onlysegment boundary.
Again, this only makes sensefor paragraphs of short length, where thedistribution of words would smear thesegmentation values across paragraphs.
In longerlength documents, we  do not expect thisphenomenon tooccur, and thus this process can beskipped.
After finding local maxima, we arrive atthe finalized segment boundaries.1.3 A lgor i thm Tra in ingTo come up with the weights used in thesegmentation algorithm and to establish theposition criteria used later in the segment relevancecalculations, we split our corpus of articles in foursets and perforrre.d 4-fold cross validation training,intentionally keeping the five Economist articlestogether in one set to check for domain specificity.Our training phase consisted of running thealgorithm with a range of different parametersettings to determine the optimal settings.
We trieda total of 5 x 5 x 3 x 3 = 225 group settings for thefour variables (front, rear, during weights andlinking length settings) for each of the three(common ouns, proper nouns and pronoun forms)term types.
The results of each run were comparedagainst a standard of user segmentation judgments,further discussed in Section 3.The results noted that a sizable group ofsettings (approximately 10%) seemed to producevery close to optimal results.
This group ofsettings was identical across all four crossvalidation training runs, so we believe thealgorithm is fairly robust, but we cannot safelyconclude this without constructing a more extensivetraining/testing corpus.2 SEGNIFIER: Segment SignificanceOnce segments have been determined, how can wego about using them?
As illustrated in theintroduction, segments can be utilized "as-is" byinformation retrieval and automatic summarizationapplications by treating segments as individual200documents.
However, this approach losesinformation about he cohesiveness of the text as awhole unit.
What we are searching for is aframework for processing segments both as 1)sub-documents of a whole, and as 2) independententities.
This enables us to ask a parallel set ofgeneral questions concerning 1) how segmentsdiffer from each other, and 2) how a segmentcontributes to the document as a whole.In this portion of the paper, we deal withinstances of the two questions: 1) Can we decidewhether a text segment is important?
2) How dowe decide what type of function a segment serves?These two questions are related; together, theymight be said to define the task of finding segmentsignificance.
We will show a two-stage, sequentialapproach that attempts this task in the context ofthe article itself.
Assessing segment significancewith respect o a specific query could be quitedifferent.Segment Significancesegmented !Calculate Determinetext '~ Segment ; Segment (fmrn Figure l) Importance Functionlabeled segmentsFigure 3 - SEGNIFIER Architecture2.1 Segment ImportanceInformally, segment importance is defined as thedegree to which a given segment presents keyinformation about the article as a whole.
Ourmethod for calculating this metric is given in thesection below.We apply a variant of Salton's (1989)information retrieval metric, Term Frequency *Inverse Document Frequency (TF*IDF) tO nounphrases (no pronominial tokens are used in thisalgorithm).
Intuitively, a segment containing nounphrases which are then also used in other segmentsof the document will be more central to the textthan a segment that contains noun phrases that areused only within that one segment.
-We call thismetric TF*S~, since we base the importance of a?
SF = Segment frequency (How many segments doesthe term occur in)201segment on the distribution of noun phrases withinthe document.
Note that this is not exactlyanalogous to IDF; we do not compute inversesegment frequency (ISF); this is because we arelooking for segments with noun phrases that occurthroughout a text rather that segments which arecharacterized by local noun phrases.
Higher scoresalong the TF*SF metric indicate a more centralsegment, which we equate with segmentimportance.
SEGNIFIER first calculates the TF*SFscore for each noun phrase term using the termoccurrence information and segment boundariesprovided by the segmentation program.However, segment importance cannot bederived from merely summing together each term'sTF*SF score; we must also track in which segmentsthe noun phrase occurs.
This is needed to decidethe coverage of the noun phrase in the segment.
Weillustrate segment coverage by the example of twohypothetical segments A-2 and B-2 in Figure 4.
Ifwe assert that the terms in each segment areequivalent, we can show that segment B-2 hasbetter coverage because two noun phrases in B-2taken together appear across all three segments,whereas in A-2 the noun phrase cover only twosegments.seg A-1 seg A-2 seg A-3g \] xxxxxxxxxxlxxx Y~'YYYYYlYYYYYY lseg B-I seg B-2 seg B-3Figure 4 - Segment NP CoverageTo calculate coverage, SEGNIFIER first iteratesover all the occurrences of all terms within asegment, and then increments the score.
Theincrement depends on the number of termspreviously seen that also fall in the same segment.We use a hamaonic series to determine the score:for the first occurrence of a term in some segment,I is added to the segment's coverage score; asecond occurrence adds 1/2; a third, 1/3, and soforth.We normalize both the sum of the TF*SFscores over its terms and its coverage score tocalculate the segment importance of a segment.Segment importance in our current system is givenby a sum of these two numbers; thus the range isfrom 0.0 (not important) to 2.0 (maximallyimportant).
We summarize the algorithm forcalculating segment importance in the psuedocodein Figure 5 below.for each segment {{ / /  TF 'SF  ca lcu lat ionTF_SF  = sum of TF_SF per NP term;TF_SF  = TFSF  /(max TF SF over all segments);){ // coverage calculat ionscoverage = sum of coverage per  NP  term;coverage = coverage I(max coverage over all segments);}seg iral~ortance = TF_SF ?
coverage;}Figure 5 - Segment importance psuedocode2.2 Segment FunctionsContrasting with segment importance, whichexamines the prominence of a segment versusevery other segment, we now turn to examinesegment function, which looks at the role of thesegment in discourse structure.
We currentlyclassify segments into three types:a.
Summary Segments - A summarysegment contains a summary of the article.
Weassume ither the segment functions as an overview(towards the beginning of an article) or as aconclusion (near the end of an article), so theposition of the segment within the document is oneof the determining factors.
According to ourempirical study, summary segments are segmentswith the highest segment importance out ofsegments that occur within the first and last 20% ofan article.
In addition, the importance rating mustbe among the highest 10% of all segments.b.
Anecdotal Segments - Material thatdraws a reader into the main body of the articleitself are known in the field of journalism asanecdotal leads.
Similarly, closing remarks areoften clever comments for effect, but do not conveymuch content.
In our attempts to try to detect hesesegments, we have restricted our scope to the firstand last segments of an article.Empirical evidence suggests that in thedomain of journalistic text, at least a single personis introduced uring an anecdotal segment, o relatethe interesting fact or narrative.
This person isoften not mentioned outside the segment; since thepurpose of relating the anecdote is limited in scopeto that segment.
Accordingly, SEGNIFIER looks fora proper noun phrase that occurs only within thecandidate segment, and not in other segments.
Thisfirst or last segment is then labeled as anecdotal, ifit has not been already selected as the summarysegment.
This method worked remarkably well onour data although we need to address cases wherethe anecdotal material has a more complex nature.For example, anecdotal material is also sometimeswoven throughout the texts of some documents.e.
Support Segments - These segmentsare the default segment ype.
Currently, if wecannot assign a segment as either a summary or ananecdotal segment, it is deemed to be a supportsegment.2.3 Related work  on Segment  Signif icanceThere has been a large body of work doneof assessing the importance of passages and theassignment of discourse functions to them.
Chenand Withgott (1992) examine the problem of audiosummarization in domain of speech, usinginstances emphasized speech to determine anddemarcate important phrases.
Although their workis similar to the use of terms to demarcatesegments, the nature of the problem is different.The frequency of terms in text versus emphasizedspeech in audio forces different approaches to betaken.
Singhal and Salton (1996) examineddetermining paragraph connectedness via vectorspace model similarity metrics, and this approachmay extend well to the segment level.
Consideringthe problem from another angle, discourseapproaches have focused on shorter units thanmulti-paragraph segments, but Rhetorical StructureTheory (Marcu 1997 and others) may be able toscale up to associate rhetorical functions withsegments.
Our work is a first attempt o bringthese fields together to solve the problem ofsegment importance and function.202Monte Carlo 33%HypergeometricTEXTTILINGSEGMENTERHuman Judges15Precisionav~ I S.D.29.0% I 9.2!30.6% N/Ai 28.2% 18.1 I47.0% 21.467.0%WSJI Recallavg I S.D.33.3% .0230.6% N/A33.4% 25.945.1% 24.411.4 80.4% 8.9 55.8%Table 2 - Evaluation Results on3 Evaluation3.1 Segmentation EvaluationFor the segmentation algorithm we used a web-based segmentation evaluation facility to gathersegmentation judgments.
Each of the 20 articlesin the corpus was segmented by at least fourhuman judges, and the majority opinion ofsegment boundaries was computed as theevaluation standard (Klavans et al 1998).Human judges achieved on average only62.4% agreement with the majority opinion, as seenin Table 2.
Passonneau and Litrnan (1993) showthat this surprisingly low agreement is often theresult of evaluators being divided between thosewho regard segments as more localized and thosewho prefer to split only on large boundaries.We then verified that the task was welldefined by testing for a strong correlation betweenthe markings of the human judges.
We test forinter-judge reliability using Cochran (1950)'s Q-test, also discussed in Passonneau and Litrnan(1993).
We found a very high correlation betweenjudges indicating that modeling the task was indeedfeasible; the results howed that there was less thana 0.15% chance on average that the judges'segment marks agreed by chance.
We alsocalculated Kappa (K), another correlation statisticthat corrects for random chance agreement.
Kappavalues range from -1.0, showing complete negativecorrelation to +1.0, indicating complete positivecorrelation.
Surprisingly, the calculations of Kshowed only a weak level of agreer/~nt betweenjudges (K avg = .331, S.D.= .153).
Calculationsof the significance of K showed that results weregenerally significant to the 5% level, indicating that5 Economisti Precisioni avg I S.D.132.8% 12.6l132.9% N/A18.3% 20.728.6% 26.217.2I Recallavg I S.D.33.3% .0232.9% N/A18.7% 18.522.67% 25.271.9% 4.6TotalPrecisionavg I S.D.29.8%1 9.932.0%1 N/A25.8% 18.742.6% 23.562.4% 13.5Recallavg I S.D,33.3% .0232.0% N/A29.8% 27.839.6% 25.978.2% ~7.6Precision and Recall Scalescalthough the interjudge agreement is weal  i~ isstatistically significant and observable.We computed SEGMENTER'S performanceby completing the 4-fold cross validation on thetest cases.
Examining SEGMENTER'S results how asignificant improvement over the initial algorithmof Hearst 1994 (called TEXTTILING), both inprecision and recall.
A future step could be tocompare our segmenting algorithm against othermore recent systems (such as Yaari 1997,Okumura nd Honda 1994).We present two different baselines tocompare the work against.
First, we applied aMonte Carlo simulation that segments at paragraphbreaks with a 33% probability.
We executed thisbaseline I0,000 times on each article and averagedthe scores.
A more informed baseline is producedby applying a hypergeometfic distribution, whichcalculates the probability of some number ofsuccesses by sampling without replacement.
Forexample, this distribution gives the expectednumber of red balls drawn from a sample of n ballsfrom an urn containing N total balls, where only rare red.
If we allow the number of segments, r, tobe given, we can apply this to segmentation to pickr segments from N paragraphs.
By comparing theresults in Table 3, we can see that the correctnumber of segments (r) is difficult to determine.TEXTTILING's performance falls below thehypergeomtfic baseline, but on the average,SEGMENTER outperforms it.However, notice that the performance ofthe algorithm and TEX'I'TILING quoted in this paperare low in comparison to reports by others.
Webelieve this is due to the weak level of agreementbetween judges in our training/testing evaluationcorpus.
The wide range of performance hints at the203variation which segmentation algorithms mayexperience when faced with different kinds of input.3.2.
Segment  Signif icance EvaluationAs mentioned previously, segments and segmenttype assessments have been integrated into a keysentence extraction program (Klavans et al1998).
~This ~ummary-directed s ntence extractiondiffers from similar systems in its focus on highrecall; further processing of the retrieved sentenceswould discard unimportant sentences and clauses.This system used the location of the first sentenceof the summary segment as one input feature fordeciding key sentences, along with standardfeatures uch as title words, TF*IDF weights for thewords of a sentence, and the occurrences ofcommunication verbs.
This task-based evaluationof both modules together showed that combiningsegmentation i formation yielded markedly betterresults.
In some instances only segmentation wasable to identify certain key sentences; all otherfeatures failed to find these sentences.
Overall, a3.1% improvement in recall was directly achievedby adding segment significance output, increasingthe system's recall from 39% to 42%.
Since thesystem was not built with precision as a priority,so although precision of the system dropped 3%,we believe the overall effects of adding thesegmentation information was valuable.4 Future  WorkImprovements to the current system can becategorized along the lines of the two modules.For segmentation, applying machine learningtechniques (Beeferman et al 1997) to learnweights is a high priority.
Moreover we feelshared resources for segmentation evaluationshould be established', to aid in a comprehensivecross-method study and to help alleviate theproblems of significance of small-scaleevaluations as discussed in Klavans et al(1998).'
For the purposes of our own evaluation, weconstructed web-based software tool that allows usersto annotate a document with segmentation markings.We propose initiating a distributed cross evaluation oftext segmentation work, using our system as acomponent tostore and share user-given and automaticmarkings.For judging segment function, we plan toperform a direct assessment of the accuracy ofsegment classification.
We want to expand andref'me our definition of the types of segmentfunction to include more distinctions, such as thedifference between document/segment borders(Reynar 1994).
This would help in situationswhere input consists of multiple articles or acontinuous stream, as in Kanade et al (1997).5 ConclusionIn this paper we have shown how multi-paragraphtext segmentation can model discourse structureby addressing the dual problems of computingtopical text segments and subsequently assessingtheir significance.
We have demonstrated a newalgorithm that performs linear topicalsegmentation i  an efficient manner that is basedon linguistic principles.
We achieve a 10%increase in accuracy and recall levels over priorwork (Hearst 1994, 1997).
Our evaluation corpusexhibited a weak level of agreement amongjudges, which we believe correlates with the lowlevel of performance of automatic segmentationprograms as compared to earlier published works(Hearst 1997).Additionally, we describe an originalmethod to evaluate a segment's significance: a twopart metric that combines a measure of a segment'sgenerality based on statistical approaches, and aclassification of a segment's function based onempirical observations.
An evaluation of thismetric established its utility as a means ofextracting key sentences for summarization.AcknowledgementsThe authors would like to thank Slava M. Katz, whoseinsights and insistence on quality work helped push the firstpart of the research forward.
We are also indebted to SusanLee of the University of California.
Berkeley* for providingempirical validation of the segment significance through herkey sentence xtraction system.
Thanks are also due toYaakov Yaari of Bar-llan University, for helping us huntdown additional segmentation corpora.
Finally.
we thank theanonymous reviewers and the Columbia natural languagegroup members, whose careful critiques led to a more carefulevaluation of the paper's techniques.'
who ~ supported by ~ C ~  R~an:h AsSn .204ReferencesBarzilay R. and Elhadad M. (1997) Using LexicalChains for Text Summarization.
Proceedings of theIntelligent Scaleable Text SummarizationWorkshop, ACL, Madrid.Beeferman D., Berger A. and Lafferty J.
(1997) TextSegmentation Using Exponential Models.Proceedings of the Second Conference on EmpiricalMethods in Natural Language Processing.Carletta J.
(1996) Assessing agreement onclassification tasks: the kappa statistic.Computational Linguistics, vol.
22(2), pp.
249-254Chen F. R. and Withgott M. (1992) The Use ofEmphasis to Automatically Summarize A SpokenDiscourse.
Proceedings of 1992 IEEE Int'!Conference on Acoustics, Speech and SignalProcessing, vol.
1, pp.
229-232.Cochran W. G. (1950) The comparison of percentagesin matched samples.
Biometfika vol.
37, pp.
256-266.Gfishman R., Macleod C. and Meyers A.
(1994).COMLEX Syntax: Building a ComputationalLexicon, Procedings of the 15th Int'l Conference onComputational Linguistics (COLING-94).Hearst M. A.
(1994) Multi-Paragraph Segmentation ofExpository Text, Proceedings of the 32nd AnnualMeeting of the Association for ComputationalLinguistics.Hearst M. A.
(1997) TextTiling: Segmenting Text intoMulti-paragraph Subtopic Passages, ComputationalLinguistics, voi 23(1), pp.
33-64.Justeson J. and Katz S. (1995) Technical Terminology:some linguistic properties and an algorithm foridentification in text.
Natural LanguageEngineering, vol.
1(I), pp.
9-29.Klavans J., McKeown K., Kan M. and Lee S. (1998)Resources for the Evaluation of SummarizationTechniques.
Proceedings of the 1st Int'l Conferenceon Language Resources and Evaluation, Grenada,Spain: May.
1998.Kanade T, et al (1997) Spot-lt:Segmenting NewsVideos into Topics.
Proceedings of the DigitalLibrary Initiative's Project-Wide Workshop,Carnegie Mellon University, pp.
41-55.Kozima H. (1993) Text segmentation based onsimilarity between words.
Proceedings of the 31thAnnual Meeting of the Association ofComputational Linguistics, pp.
286-288.Marcu D. (1997) The Rhetorical Parsing of NaturalLanguage Texts.
Proceedings of the 35th AnnualMeeting of the Association for ComputationalLinguistics, pp 96-103.Miller G. A., Beckwith R., Fellbaum C., Gross D. andMiller K. J.
(1990) WordNet: An on-line lexicaldatabase.
Journal of Lexicography, vol.
3, pp.
235-244.Morris J. and Hirst G. (1991) Lexical CoherenceComputed by Thesaural Relations as an Indicator ofthe Structure of Text.
Computational Linguistics,vol.
17(1), pp 21-42.Okumura M. and Honda T. (1994) Word sensedisambiguation and text segmentation based onlexical cohesion.
Procedings of the 15th Int'!Conference on Computational Linguistics(COLING-94), pp.
755-761.Passonneau R. J. and Litman D. J.
(1993) Intention-based segmentation: human reliability andcorrelation with linguistic cues.
Proceeding of the31st Annual Meeting of the Association ofComputation Linguistics, pp.
148-155.Reynar J.
(1994) An Automatic Method of FindingTopic Boundaries" Proceedings of the 32nd AnnualMeeting of the Association for ComputationalLinguistics (student session), Las Cruces, NewMexico.Salton G. (1989) Automatic text processing: thetransformation, analysis, and retrieval ofinformation by computer.
Addison-Wesley,Reading, Massachusetts.Singhal A. and Salton G. (1995) Automatic TextBrowsing Using Vector Space Model.
Proceedingsof the Dual-Use Technologies and ApplicationsConference, pp.
318-324.Yaari Y.
(1997) Segmentation of Expository Text byHierarchical Agglomerative Clustering.
RecentAdvances in NLP 1997, Bulgaria.Youmans C. G. (1991) A new tool for discourseanalysis: The vocabulary-management profile.Language, vol.
67, pp.
763-789.205
