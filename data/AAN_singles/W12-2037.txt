The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 307?315,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsCrowdsourced Comprehension:Predicting Prerequisite Structure in WikipediaPartha Pratim TalukdarMachine Learning DepartmentCarnegie Mellon Universityppt@cs.cmu.eduWilliam W. CohenMachine Learning DepartmentCarnegie Mellon Universitywcohen@cs.cmu.eduAbstractThe growth of open-access technical publica-tions and other open-domain textual informa-tion sources means that there is an increas-ing amount of online technical material thatis in principle available to all, but in prac-tice, incomprehensible to most.
We proposeto address the task of helping readers com-prehend complex technical material, by us-ing statistical methods to model the ?prereq-uisite structure?
of a corpus ?
i.e., the se-mantic impact of documents on an individualreader?s state of knowledge.
Experimental re-sults using Wikipedia as the corpus suggestthat this task can be approached by crowd-sourcing the production of ground-truth labelsregarding prerequisite structure, and then gen-eralizing these labels using a learned classifierwhich combines signals of various sorts.
Thefeatures that we consider relate pairs of pagesby analyzing not only textual features of thepages, but also how the containing corpora isconnected and created.1 Introduction and MotivationNicholas Carr has argued in his recent popular book?The Shallows?
that existing Internet technologiesencourage ?shallow?
processing of recent and pop-ular information, at the expense of ?deeper?, con-templative study of less immediately-accessible in-formation (Carr, 2011) .
While Carr?s hypothesis isdifficult to formalize rigorously, it seems intuitivelyplausible.
For instance, user-generated content fromTwitter and Facebook is mainly comprised of short,shallow snippets of information.
Most current re-search in AI (and more broadly in computer science)does not seem likely to reverse this trend: e.g., workin crowdsourcing has concentrated on tasks that canbe easily decomposed into small pieces, and muchcurrent NLP research aims at facilitating short-term?shallow?
goals, such as answering well-formulatedquestions (e.g., (Kwok et al, 2001)) and extractingconcrete facts (e.g., (Etzioni et al, 2006; Yates et al,2007; Carlson et al, 2010)).
This raises the ques-tion: what can AI do to facilitate deep, contempla-tive study?In this paper we address one aspect of this largergoal.
Specifically, we consider automation of anovel task?using AI methods to facilitate the ?deepcomprehension?
of complex technical material.
Weconjecture that the primary reason that technicaldocuments are difficult to understand is lack of mod-ularity: unlike a self-contained document written fora general reader, technical documents require cer-tain background knowledge to comprehend?whilethat background knowledge may also be available inother on-line documents, determining the proper se-quence of documents that a particular reader shouldstudy is difficult.We thus formulate the problem of comprehendingtechnical material as a probabilistic planning prob-lem, where reading a document is an operator thatwill probabilistically change the state of knowledgeK(u, t) of a user u at time t, in a manner that de-pends on u?s prior knowledge K(u, t ?
1).
Solvingthis task requires, among other things, understand-ing the effect of reading individual documents d ?specifically, the concepts that are explained by d,and the concepts that are prerequisites for compre-hending d. This paper addresses this problem.
Inparticular, we consider predicting whether one pagein Wikipedia is a prerequisite of another.More generally, we define the ?prerequisite struc-307CRFStatistical_modelGraphical_modelDiscriminative_model Markov_random_fieldParameter_learningMaximum_likelihoodHidden_Markov_ModelViterbi_algorithmrandom_variableprobability_distributionvariable_mathematicsConditional_independenceGradient_descentBaum_Welch_algorithmMarkov_chainInference dynamic_programming Expectional_maximization_algorithmFigure 1: The prerequisite structure rooted at the page ?Conditional Random Fields?, omitting nodes that wouldalready be known a typical CS graduate student.Variable (Mathematics) Random Variable Probability DistributionConditional Independence Statistical Model Graphical ModelDiscriminative Model Markov Random FieldGradient Descent Parameter Learning Maximum LikelihoodInference Dynamic Programming Viterbi AlgorithmMarkov Chain Expectation Maximization Algorithm Baum Welch AlgorithmHidden Markov Model CRFFigure 2: A plan for comprehending ?Conditional Random Fields?
(to be read left-to-right, top-to-bottom).
Horizontallines indicate breaks between independent sections of the subgraph.ture?
for a corpus as a graph, where nodes are con-cepts to comprehend, and a directed edge d ?
d?corresponds to the assertion ?understanding d?
is aprerequisite to understanding d?.
For Wikipedia, weassume a one-to-one correspondence between doc-ument titles and concepts explicated by (i.e., post-conditions of) these documents.
Figure 2 presentsa small example of a prerequisite structure, and in-dicates how it might be used to construct a plan forcomprehending a specific concept.Focusing on Wikipedia has several advantages.First, it is densely linked, and hence a document dwill likely be linked directly to any prerequisite paged?.
(However, not all hyperlinks will indicate a pre-requisite.)
Second, Wikipedia?s standardized formatmakes textual analysis easier.
Finally, there is a greatdeal of social information available about how docu-ments are used by the Wikipedia community.
Theseproperties make it easy for us to explore the infor-mativeness of different types of information with re-spect to predicting prerequisite structure.Our overall plan for producing a prerequisitestructure for a corpus is first, to use crowdsourc-ing approaches to obtain a subset of the prerequisitestructure; and second, to extrapolate this structureto the entire corpus using machine learning.
Below,we first describe datasets that we have collected,based on five technical concepts in Wikipedia fromfive different fields.
We then outline the specifics ofour procedure for annotating prerequisite structure,using Amazon?s Mechanical Turk, and demonstratethat meaningful signals about prerequisite structurecan be obtained using a classifier that exploits sev-eral sources: graph analysis of Wikipedia?s linkgraph; graph analysis of a bipartite graph relatingWikipedia pages to Wikipedians that have editedthese pages; and textual analysis.
We complete ourexperimental analysis of the prerequisite-structureprediction task by discussing and evaluating the de-gree to which prerequisite-structure prediction isdomain-independent, and the degree to which differ-ent subareas of Wikipedia (e.g., biology vs computerscience) require different predictors.After discussing related work, we return in theconcluding remarks to the overarching goal of fa-cilitating comprehension, and discuss the relation-308Target Concept #Nodes #Edges #EditsGlobal Warming 19,170 501,608 1,490,967Meiosis 19,811 444,100 880,684Newton?s Laws of Motion 15,714 436,035 795,988Parallel Postulate 14,966 363,462 858,785Public-key cryptography 16,695 371,104 1,003,181Table 1: Target concepts used in the experiments.ship of the current study to these goals.
Specifi-cally we note that facilitating comprehension alsorequires understanding a user?s goals, and her initialstate of knowledge, in addition to understanding theprerequisite structure of the corpus.
We also discussthe relationship between planning and prerequisite-structure prediction and suggest that use of appro-priately robust planning methods may lead to goodcomprehension plans, even with imperfectly pre-dicted prerequisite structure.2 ExperimentsAs discussed above, we focus in this paperon predicting prerequisite structure in Wikipedia.While most Wikipedia pages are accessible to ageneral reader, there are many pages that de-scribe technical concepts, such as ?conditionalrandom fields?, ?cloud radiative forcing?, and?Corticotropin-releasing factor?.
Most of these tech-nical pages are not self-contained: for instance,to read and comprehend the page on ?conditionalrandom fields?, one will have to first understand?graphical model?, and so on, as suggested by Fig-ure 1.
In this section, we evaluate the followingquestions:?
Can we train a statistical classifier for prereq-uisite classification in a target domain, wherethe classifier is trained on out of domain (i.e.,non-target domain) data annotated using Ama-zon Mechanical Turk service??
What are the effects of different types of signalson the performance of such a classifier??
How does out of domain training compare to indomain training?2.1 Experimental SetupFor our experiments, we choose five targets fromdiffering areas for experimentation, listed in Table 1.Several of the techniques we used are based on graphanalysis.
The full graphs associated with Wikipediaare unwieldy to use for experimentation because oftheir size: therefore, for each target concept, we ex-tracted a moderate-sized low-conductance subgraphof Wikipedia?s link graph containing the target, us-ing a variant of the PageRank-Nibble algorithm (An-dersen et al, 2006).1.
As parameters we used ?
=0.15 and  = 10?7, yielding graphs with approx-imately 15-20,000 nodes and 350-500,000 edgeseach.
We also collected the edit history for eachpage in every subgraph forming a second graph foreach sub-domain 2.
On average, each page fromthese subgraphs had been edited about 20 times, byabout 8 unique editors.
Details are given in Table 1.For classification, we used a Maximum Entropy(MaxEnt) classifier.
Given a pair of Wikipedia pagesx = (d, d?)
connected by a directed edge (hyperlink)from d to d?, the classifier will predict with probabil-ity p(+1|x) whether the main concept in page d?isa prerequisite for the main concept in page d. Theclassifier has the formp(y|x) =exp(w ?
?
(x, y))?y?
?Y exp(w ?
?
(x, y?
)), y ?
Y = {?1,+1}where ?
(x, y) is a feature function which representsthe pair of pages x = (d, d?)
in a high dimensionalspace, and w is the parameter vector of the classifierwhich is estimated from training data.
We use theMallet package3 to train and evaluate classifiers.
Forthe experiments in this paper, we shall exploit thefollowing types of features:WikiHyperlinks: Features include the randomwalk with restart (RWR) score (Tong et al,2006) of the target concept page d?startingfrom the source page d. Additional featuresinclude the PageRank score of the target andsource pages.1Specifically, we used the ?ApproximatePageRank?
methodfrom (Andersen et al, 2006) to find a set of nodes S containinga low-conductance subgraph, but did not prune S to find thelowest-conductance subgraph of it with a ?sweep?.
The versionof Wikipedia?s link graph we used was DBPedia?s version 3.7(Auer et al, 2007)2Specifically, a bipartite graph connecting pages and editors.We used a version of Wikipedia?s edit history extracted by otherresearchers (Leskovec et al, 2010), discarding edits marked as?minor?
by the editor.3Mallet package: http://mallet.cs.umass.edu/309DomainTime (s) / Worker# HITs ?Evaluation / HITMeiosis 38 3 400 0.50Public-key Cryp.
26 3 200 0.63Parallel Postulate 41 3 200 0.55Newton?s Laws 20 5 400 0.47Global Warming 14 5 400 0.56Average 27.8 - - 0.54Table 2: Statistics about the Gold-standard data preparedusing Amazon Mechanical Turk.
Also shown are theaveraged ?
statistics-based inter-annotator agreement ineach domain.
The last row corresponds to the ?
valueaveraged across all five domains.WikiEdits: This includes one feature?theanalogous RWR score on the graph of edit in-formation.WikiPageContent: Features in this categoryare derived from the contents of the twoWikipedia pages d and d?.
Examples include:the category identity of the source page; thecategory identity of the target page; whetherthe titles of d?and d are mentioned in the firstsentence of d; the name of the first section in dwhich contains a link to d?
; whether there is anyoverlap in categories between the two pages;whether d is also linked from d?
; and the log ofthe number of times d?
is linked form d. We usethe JWPL library (http://jwpl.googlecode.com)for efficient and structured access to Wikipediapages from a recent dump obtained on Jan 4,2012.2.1.1 Gold-standard Annotation fromMechanical Turk4In order to evaluate different prerequisite classi-fication systems and also to train the MaxEnt clas-sifier, we collected gold prerequisite decisions us-ing Amazon Mechanical Turk (AMT).
Since prepar-ing annotated gold data for entire graphs in Table 1would be prohibitively expensive, we used the fol-lowing strategy to sample a smaller subgraph fromthe larger domain-specific subgraph, which in turnwill be used for training and evaluation purposes.Preliminary investigation suggested that most of thepages in the prerequisite structure rooted at a target4Amazon Mechanical Turk: http://mturk.amazon.comconcept d are connected to d via many short hyper-link paths.
Hence, for each target domain, we firstselected the top 20 nodes with highest RWR scores,relative to the target concept, in the subgraph for thattarget concept (as listed in Table 1.)
We then sam-pled a total of 400 edges from these selected nodes,with outgoing edges from a node sampled with a fre-quency proportional to its RWR score.
Thus, usingthis strategy, we selected up to 400 pairs of pages(d, d?
), where each pair has a hyperlink from d to d?.Classification of a pair of hyperlinked Wikipediapages (d, d?)
into one of the four following classesconstituted a Human Intelligence Task (HIT): (1) d?is a prerequisite of d; (2) d is a prerequisite of d?
; (3)the two pages are unrelated; (4) Don?t know.
Sub-sequently, based on the feedback from the workers,a fifth option was also added: the two concepts arerelated, but they don?t have any prerequisite relation-ship between them.
Based on the available workersand turnaround time, the number of assignments perHIT (i.e., number of unique workers assigned to aparticular HIT) was either 3 or 5; and the numberof HITs used was either 200 or 400.
Depending onthe hardness of domain and availability of workersopting to work on a domain, reward per HIT assign-ment was varied from $0.02 (for Global Warmingand Newton?s Laws) to $0.08 (for Public-key Cryp-tography, Meiosis and Parallel Postulate).
This datacollection stage spanning all five domains was com-pleted in about a week at a total cost of $278.
Statis-tics about the data are presented in Table 25.Starting with the AMT data collected as above,we next created a binary-labeled training dataset,where each instance corresponds to a pair of pages.We ignored all ?Don?t Know?
labels, treated option(1) above as vote for the corresponding prerequisiteedge, and treated all other options as votes against.We then assigned the final label for a node pair usingmajority vote (breaking ties arbitrarily).2.1.2 Consistency of labelsIn contrast to standard setup of gold data prepara-tion where a single annotator is guaranteed to pro-vide feedback on every instance, the situation incase of Mechanical Turk-based annotation is differ-ent, as the workers are at liberty to choose the HITs(or instances) they want to work on.
This makes5The dataset is available upon request from the authors.3104050607080Meiosis Public Key Para.
Postulate Newton?s Law Global Warming AveragePerformance Comparison for Prerequisite PredictionAccuracyRandom Baseline MaxEnt ClassifierFigure 3: Comparison of performance between the Max-Ent classifier (right bar in each group) against a randombaseline (left bar in each group) in all five domains.
Onaverage, the MaxEnt classifier results in an 8.6% absoluteimprovement in accuracy.standard ?
statistics-based inter-annotator computa-tion (Fleiss, 1981) inapplicable in the current set-ting.
We circumvented this problem by first select-ing all workers with at least 100 feedbacks, and thencomputing pairwise ?
statistics between all pairs ofthese frequent workers.
These ?
statistics were aver-aged across each domain, and also averaged acrossall domains.
The results, also shown in Table 2,show moderate agreement (recall that ?
= 0 indi-cates no correlation).
We are encouraged to observethat moderate level of agreement is possible even inthis setting, where there is no control over workerbackground and quality.
We next explore whetherthis level of agreement is sufficient to train statisti-cal classifiers.2.2 Prerequisite ClassificationIn this section, we explore whether it is possible totrain a MaxEnt classifier to determine prerequisitestructure in a target domain, with the training per-formed in ?leave one domain out?
manner, wherethe training data originates from domains other thanthe target domain.
For example, for classifications inthe target domain, say ?Global Warming?, we trainthe classifier with annotated data from the remainingfour domains (or whatever domains are available).We note that training on ?out of domain?, if it issuccessful, has several benefits.
First, a successfultraining strategy in this setup removes any need tohave labeled data in each target domain of interest,which is particularly relevant as labeled data is ex-pensive to prepare.
Second, a classifier trained justonce can be repeatedly used across multiple domainswithout requiring retraining.Accuracies of MaxEnt classifiers trained using the?leave one domain out?
strategy are shown in Fig-ure 3; we report the test accuracy on each target do-main, as well as the average across domains.
Perfor-mance of a random classifier is presented as a base-line.
Classes in the train and test sets were balancedby oversampling the minority class.
From Figure 3,we observe that it is indeed possible to train pre-requisite classifiers in an out of domain setting, us-ing data from the Amazon Mechanical Turk service;on average, the classifier outperforms the randombaseline with 8.6% absolute improvement in classi-fication accuracy.
We also experimented with otherrule-based classifiers6, and in all cases, the trainedMaxEnt classifier outperformed these baselines.
Al-though more sophisticated training strategies andmore clever feature engineering would likely yieldfurther improvements, we find it encouraging thateven a relatively straightforward classification tech-nology along with a basic set of features was able toachieve significant improvement in performance onthe novel task of prerequisite prediction.2.3 Feature Ablation ExperimentsThe MaxEnt classifier evaluated in the previoussection had access to all three types of features:WikiEdits, WikiHyperLinks, and WikiPageContent,as described in the beginning of this section.
In or-der to evaluate the contribution of each such sig-nal, we created ablated versions of the full Max-Ent classifier which uses only one of these threesubsets.
We call these thee variants: MaxEnt-WikiEdits, MaxEnt-WikiHyperLinks, and MaxEnt-WikiPageContent, respectively.
Average accuraciesacross all five domains comparing these three vari-ants, in comparison to the Random baseline andthe full classifier (MaxEnt-Full, as in previous sec-tion) are presented in Table 3.
From this, we ob-serve that all three variants perform better than therandom baseline, with maximum gains achievedby the MaxEnt-WikiPageContent classifier, whichuses page content-based features exclusively.
We6For example, classify d?as a prerequisite for d if d?islinked from the first paragraph in d.311System AccuracyRandom 50.22MaxEnt-WikiEdits 51.62MaxEnt-WikiHyperlinks 52.70MaxEnt-WikiPageContent 57.84MaxEnt-Full 58.82Table 3: Comparison of accuracies (averaged across allfive domains) of the full MaxEnt classifier with its ablatedversions which use a subset of the features, and also therandom baseline.
The full classifier, which exploits allthree types of signals (viz., WikiEdits, WikiHyperlinks,and WikiPageContent) achieves the highest performance.DomainWiki- Wiki- WikiPage-AllEdits HyperLinks ContentMeiosis 5.4 2.4 0.3 1Public-key-0.7 -1.8 15.1 17.1Crypto.Parallel3.1 6.1 11.7 14.7PostulateNewton?s-0.2 6.2 3.9 3.9LawsGlobal-7.7 0.1 5.8 6.8WarmingTable 4: Accuracy gains (absolute) relative to the Ran-dom baseline achieved by the full MaxEnt classifier aswell as its ablated versions trained with three differentsubsets of the full classifier.
Positive gains are marked inbold.also note that the full classifier MaxEnt-Full, isable to effectively combine three types of signalsimproving performance even further.
In Table 4,we present a per-domain breakdown of the gainsachieved by these four classifiers over the randombaseline.
From this, we observe that the MaxEnt-WikiEdits classifier outperforms the random base-line only in 2 out of 5 domains.
This might be dueto the fact that the MaxEnt-WikiEdits uses uses onlyone feature?the RWR score of the target page rela-tive to the source page on the Wikipedia edits graph.We hope that use of more discriminating featuresshould further help this classifier.
From Table 4, wealso observe that MaxEnt-WikiHyperLinks is able tooutperform the random baseline in 4 out of 5 cases,and the MaxEnt-WikiPageContent (as well as thefull classifier) outperforms the random baseline inall 5 domains, sometimes with large gains (as in thecase of Public-key Cryptography domain).4050607080Meiosis Public Key Para.
Postulate Newton?s Laws Global Warming AverageEffect of Out of Domain vs In Domain TrainingAccuracyOut of Domain Training In Domain TrainingFigure 4: Accuracy comparison of out of domain (left barin each group) and in domain training (right bar in eachgroup) for the five domains.
From this we observe thatgood generalization performance is possible even whenthere is no in domain training data available.2.4 Effect of Out of Domain TrainingAll the classifiers evaluated in previous sectionswere trained in an out of domain setting, i.e., thetraining data originated from domains outside thedomain in which the classifier is applied and eval-uated.
This has several benefits, as noted above.
Analternative and more standard way to train classi-fiers is to have the training and evaluation data befrom the same domain (below, the in-domain set-ting).
While such a classifier will require labeledtraining from each domain of interest, it is nonethe-less of interest to compare in-domain and out-of-domain learning.
If there are substantive differences,this could be used to improve prerequisite-structurepredictor in a subdomain (e.g., biology), or maysuggest alternative training methods (e.g., involvingtransfer learning).Motivated by this, for each domain, we com-pare the performances of the out-of-domain and in-domain classification performances.
The results areshown in Figure 4.
On average, we observe that theout-of-domain classifier is able to achieve 93% ofthe performance of the in-domain classifier.
We notethat this is encouraging for domain-independentprerequisite-structure prediction, as this suggeststhat for the prerequisite classification task, close tooptimal (i.e., in-domain performance) is possiblewhen the classifiers are trained in an out-of-domainsetting.3123 Related WorkWe believe the task of prerequisite structure predic-tion to be novel; however, it is clearly related to anumber of other well-studied research problems.In light of our emphasis on Wikipedia, a con-nection can be drawn between identifying prerequi-sites and measuring the semantic relatedness of con-cepts using Wikipedia?s link structure (Yeh et al,2009).
We consider here a related but narrowerquestion, namely whether an inter-page link will im-prove comprehension for a specific reader.In the area of intelligent tutoring and educationaldata mining, recent research has looked at enrichingtextbooks with authoritative web content (Agrawalet al, 2010).
Also, the problem of detecting pre-requisite structure from differential student perfor-mance on tests has been considered (e.g., (Pavliket al, 2008; Vuong et al, 2011)).
Our proposal con-siders discovering prerequisite structure from text,rather than from exercises, and relies on differentsignals.Research in adaptive hypermedia (surveyed else-where (Chen and Magoulas, 2005)) has goals similarto ours.
Most adaptive hypermedia systems operatein narrow domains, which precludes use of some ofthe crowd-based signals we consider here.
In this lit-erature, a distinction is often made between ?adapt-ability?
(the ability for a user to modify a presenta-tion of hypermedia) and ?adaptivity?
(the ability ofa system to adapt to a user?s needs.)
In this frame-work, our project focuses on adding ?adaptivity?
toexisting corpora via a prerequisite structure, and ourprinciple contribution to this area is identifying tech-niques that learn to combine textual features and so-cial, crowd-based signals in order to usefully guidecomprehension.Another related area is data-mining logs of Webusage, as surveyed by Pierrakos et al(Pierrakoset al, 2003).
Our focus here is on facilitating aparticular type of Web usage, comprehension, ratherthan more commonly-performed tasks like site nav-igation and purchasing.A number of ?open education?
resources exist, inwhich information can be organized into sharablemodules with known prerequisites between them(e.g., Connexions (Baraniuk, 2008)).
We focus hereon discovering prerequisite structure with machine-learning methods rather than simply encoding it.Similarly, a Wikimedia project7 has developed in-frastructure allowing a user to manually assembleWikipedia pages into e-books.
Our focus is on guid-ing the process of finding and ordering the sectionsof these books, not the infrastructure for generatingthem.
We also note that one widely-used way forcomplex technical concepts to be broadly commu-nicated is by writers or teams of writers, and pre-vious researchers have investigated understandinghow collaborative writers work (Noe?l and Robert,2004), and even developed tools for collaborativewriting (Zheng et al, 2006).
Our work focuses ontools to empower readers, rather than writers.4 ConclusionIn this paper, we motivated the goal of ?crowdsourc-ing?
the task of helping readers comprehend com-plex technical material, by using machine learningto predict prerequisite structure from not only docu-ment text, but also crowd-generated data such as hy-perlinks and edit logs.
While it is not immediatelyobvious that this task is feasible, our experimentssuggest that relatively reliable features to predictprerequisite structure exist, and can be successfullycombined using standard machine learning methods.To achieve the broader goal of facilitating com-prehension, predicting prerequisite structure is notenough.
Another important subproblem is using pre-dicted prerequisites to build a feasible plan.
As partof ongoing work, we are exploring use of modernoptimization methods (such as Integer Linear Pro-gramming) to compute ?reading plans?
that mini-mize a weighted linear combination of expected usereffort and probability of plan ?failure?8.We also plan to explore another major subprob-lem associated with facilitating comprehension?personalizing a reading plan.
Clearly, even if d?
isa prerequisite for d, a user interested in d need notfirst read a page explaining d?, if she already under-stands d?
; instead, a reading plan based on prereq-uisite structure should be adjusted based on what isbelieved about the user?s prior knowledge state.
In7See http://en.labs.wikimedia.org/wiki/Wiki to print, the?Wiki to Print?
project.8A plan ?failure?
means that the plan not actually satisfy allnecessary prerequisites, leading to imperfect comprehension onthe part of the reader after she executes the plan.313the context of Wikipedia comprehension, one possi-ble signal for predicting an individuals?
prior knowl-edge is the Wikipedia edit log: if we assume thateditors tend to edit things they know, the edit logindicates which concepts tend to be jointly known,and hence collaborative-filtering methods might beable to more completely predict a user?s knowledgegiven partial information about her knowledge?justas collaborative-filtering is often used now to extrap-olate user preference?s from knowledge of others?joint preferences.Besides contributing to the goal of facilitatingcomprehension, we believe that the specific problemof predicting prerequisite structure in Wikipedia isa task of substantial independent interest.
Prereq-uisite structure can be thought of as a sort of ex-planatory discourse structure, which is overlaid ona hyperlink graph; hence, scaling up our methodsand applying them to all of Wikipedia would iden-tify a canonical broad-coverage instance of such ex-planatory discourse.
This could be re-used for othertasks much as lexical resources like WordNet are:for instance, consider identifying explanatory dis-course in an external technical text (e.g., a textbook)by soft-matching it to the Wikipedia structure, us-ing existing techniques to match the external text toWikipedia (Agrawal et al, 2010; Mihalcea and Cso-mai, 2007; Milne and Witten, 2008).AcknowledgmentsThis research has been supported in part by DARPA(under contract number FA8750-09-C-0179), andGoogle.
Any opinions, findings, conclusions andrecommendations expressed in this paper are the au-thors?
and do not necessarily reflect those of thesponsors.
We are thankful to the anonymous review-ers for their constructive commentsReferencesAgrawal, R., Gollapudi, S., Kenthapadi, K., Srivastava,N., and Velu, R. (2010).
Enriching textbooks throughdata mining.
In Proceedings of the First ACM Sympo-sium on Computing for Development, page 19.
ACM.Andersen, R., Chung, F., and Lang, K. (2006).
Localgraph partitioning using pagerank vectors.
In Founda-tions of Computer Science, 2006.
FOCS?06.
47th An-nual IEEE Symposium on, pages 475?486.
IEEE.Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyga-niak, R., and Ives, Z.
(2007).
Dbpedia: A nucleusfor a web of open data.
In Aberer, K., Choi, K.-S.,Noy, N., Allemang, D., Lee, K.-I., Nixon, L., Golbeck,J., Mika, P., Maynard, D., Mizoguchi, R., Schreiber,G., and Cudr-Mauroux, P., editors, The Semantic Web,volume 4825 of Lecture Notes in Computer Science,pages 722?735.
Springer Berlin / Heidelberg.Baraniuk, R. (2008).
Challenges and opportunities forthe open education movement: A Connexions casestudy, pages 229?246.
MIT Press, Cambridge, Mas-sachusetts.Carlson, A., Betteridge, J., Kisiel, B., Settles, B., Hr-uschka Jr, E., and Mitchell, T. (2010).
Toward an ar-chitecture for never-ending language learning.
In Pro-ceedings of the Conference on Artificial Intelligence(AAAI), pages 1306?1313.Carr, N. (2011).
The shallows: What the Internet is doingto our brains.
WW Norton & Co Inc.Chen, S. and Magoulas, G. (2005).
Adaptable and adap-tive hypermedia systems.
IRM Press.Etzioni, O., Banko, M., and Cafarella, M. (2006).
Ma-chine reading.
In Proceedings of the National Confer-ence on Artificial Intelligence.Fleiss, J.
(1981).
The measurement of interrater agree-ment.
Statistical methods for rates and proportions,2:212?236.Kwok, C., Etzioni, O., and Weld, D. (2001).
Scalingquestion answering to the web.
ACM Transactions onInformation Systems (TOIS), 19(3):242?262.Leskovec, J., Huttenlocher, D., and Kleinberg, J.
(2010).Governance in social media: a case study of thewikipedia promotion process.
In AAAI InternationalConference on Weblogs and Social Media (ICWSM?10).
AAAI.Mihalcea, R. and Csomai, A.
(2007).
Wikify!
: linkingdocuments to encyclopedic knowledge.
In CIKM, vol-ume 7, pages 233?242.Milne, D. and Witten, I.
(2008).
Learning to link withwikipedia.
In Proceeding of the 17th ACM conferenceon Information and knowledge management, pages509?518.
ACM.Noe?l, S. and Robert, J.
(2004).
Empirical study oncollaborative writing: What do co-authors do, use,and like?
Computer Supported Cooperative Work(CSCW), 13(1):63?89.Pavlik, P., Cen, H., Wu, L., and Koedinger, K. (2008).Using item-type performance to covariance to improvethe skill acquisition of an existing tutor.
In Proc.
ofthe 1st International Conference on Educational DataMining.Pierrakos, D., Paliouras, G., Papatheodorou, C., and Spy-ropoulos, C. (2003).
Web usage mining as a tool for314personalization: A survey.
User Modeling and User-Adapted Interaction, 13(4):311?372.Tong, H., Faloutsos, C., and Pan, J.-Y.
(2006).
Fast ran-dom walk with restart and its applications.
In Proceed-ings of the Sixth International Conference on DataMining, ICDM ?06.Vuong, A., Nixon, T., and Towle, B.
(2011).
A methodfor finding prerequisites within a curriculum.
In Proc.of the 4th International Conference on EducationalData Mining.Yates, A., Cafarella, M., Banko, M., Etzioni, O., Broad-head, M., and Soderland, S. (2007).
Textrunner: Openinformation extraction on the web.
In Proceedings ofHuman Language Technologies: The Annual Confer-ence of the North American Chapter of the Associationfor Computational Linguistics: Demonstrations, pages25?26.
Association for Computational Linguistics.Yeh, E., Ramage, D., Manning, C., Agirre, E., and Soroa,A.
(2009).
Wikiwalk: random walks on wikipediafor semantic relatedness.
In Proceedings of the 2009Workshop on Graph-based Methods for Natural Lan-guage Processing, pages 41?49.
Association for Com-putational Linguistics.Zheng, Q., Booth, K., and McGrenere, J.
(2006).
Co-authoring with structured annotations.
In Proceedingsof the SIGCHI conference on Human Factors in com-puting systems, pages 131?140.
ACM.315
