Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 170?177,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsModeling User Satisfaction with Hidden Markov ModelsKlaus-Peter Engelbrecht, Florian G?dde, Felix Hartard, Hamed Ketabdar, SebastianM?llerDeutsche Telekom Laboratories, Quality & Usability Lab, TU Berlin,Ernst-Reuter-Platz 7, 10587 Berlin, Germany{Klaus-Peter.Engelbrecht,Florian.Goedde,Hamed.Ketabdar,Sebastian.Moeller}@telekom.deFelix.Hartard@Berlin.deAbstractModels for predicting judgments aboutthe quality of Spoken Dialog Systemshave been used as overall evaluationmetric or as optimization functions inadaptive systems.
We describe a newapproach to such models, using HiddenMarkov Models (HMMs).
The user?sopinion is regarded as a continuousprocess evolving over time.
We presentthe data collection method and resultsachieved with the HMM model.1 IntroductionSpoken Dialog Systems (SDSs) are now widelyused, and are becoming more complex as a resultof the increased solidity of advanced techniques,mainly in the realm of natural languageunderstanding (Steimel et al 2008).
At the sametime, the evaluation of such systems increasinglydemands for testing the entire system, ascomponents for speech recognition, languageunderstanding and dialog management areinteracting more deeply.
For example, the systemmight search for web content on the basis ofmeaning extracted from an n-best list, andgenerate the reply and speech recognitiongrammars depending on the content found(Wootton et al 2007).
The performance of singlecomponents strongly depends on each othercomponent in this case.While performance parameters become lessmeaningful in such a system, the system?soverall quality, which can only be measured byasking the user (Jekosch 2005), gains interest forthe evaluation.
Typically, users fill outquestionnaires after the interaction, which covervarious perceptional dimensions such asefficiency, dialog smoothness, or the overallevaluation of the system (Hone and Graham,2001; ITU-T Rec.
P.851, 2003; M?ller 2005a).Judgments of the system?s overall quality can beused to compare systems with respect to a singlemeasure, which however comprises all relevantaspects of the interaction.
Thus, the complexityof the evaluation task is reduced.In addition, user simulation is increasinglyused to address the difficulty of foreseeing allpossible problems a user might encounter withthe system (e.g.
Ai and Weng, 2008; Engelbrechtet al, 2008a; Chung, 2004; L?pez-C?zar et al,2003).
In order to evaluate results from suchsimulations, some approaches utilize predictionmodels of user judgments (e.g.
Ai and Weng,2008; Engelbrecht et al, 2008a).Currently, prediction models for userjudgments are based on the PARADISEframework introduced by Walker et al (1997).PARADISE assumes that user satisfactionjudgments describe the overall quality of thesystem, and are causally related to task successand dialog costs, i.e.
efficiency and quality of thedialog.
Therefore, a linear regression functioncan be trained with interaction parametersdescribing dialog costs and task success aspredictors, and satisfaction ratings as the target.The resulting equation can then be used topredict user satisfaction with unseen dialogs.In follow-up studies, it could be shown thatsuch models are to some degree generalizable(Walker et al, 2000).
However, also limitationsof the models in predicting judgments for otheruser groups, or for systems with different levelsof ASR performance, were reported (Walker etal., 1998).
In the same study, prediction170functions for user satisfaction were proposed toserve as optimization function in a systemadapting its dialog strategy during the interaction.This idea is taken up by Rieser and Lemon(2008).The prediction accuracy of PARADISEfunctions typically lies around an R2 of 0.5,meaning that 50% of the variance in thejudgments is explained by the model.
While thisnumber is not absolutely satisfying, it could beshown that mean values for groups of dialogs(e.g.
with a specific system configuration) can bepredicted more accurately than single dialogswith the same models (Engelbrecht and M?ller,2007).
Low R2 for the predictions of ratings ofindividual dialogs seems to be due to inter-raterdifferences at least to some degree.
Suchdifferences have been described, and mayconcern the actual perception of the judged issue(Guski, 1999), or the way the perception isdescribed by the participant (Okun and Weir,1990; Engelbrecht et al, 2008b)We have tested the PARADISE frameworkextensively, using different classifier models andinteraction parameters.
Precise and generalmodels are hard to achieve, even if the set ofparameters describing the interaction is widelyextended (M?ller et al, 2008).
In an effort toimprove such prediction models, we developedtwo ideas:?
Predict the distribution of ratings whichcan be expected for a representative groupof users given the same stimulus.
Thistakes into account that in most cases therelevant user characteristics determiningthe judgment cannot be tracked, or evenare unknown.?
Consider the time relations betweenevents by modeling user opinion as avariable evolving over the course of thedialog.
This way, time relations like co-occurrence of events, which affect qualityperception, attention, or memory can bemodeled most effectively.In this paper, we present a new modelingapproach considering these ideas.
In Section 2,we introduce the topology of the model.Following this, we report how training data forthe model were obtained from user tests inSection 3.
Evaluation results are presented inSection 4 and discussed in Section 5, before weconclude with some remarks on follow-upresearch.2 Modeling Judgments with HMMsHidden Markov Models (HMMs) are often usedfor classifying sequential stochastic processes,e.g.
in computational linguistics or bio-informatics.
An HMM models a sequence ofevents as a sequence of states, in which eachstate emits certain symbols with some probability.In addition, the transitions between states areprobabilistic.
The model is defined by a set ofstate symbols, a set of emission symbols, theprobabilities for the initial state, the statetransition matrix, and the emission matrix.
Thetransition matrix contains the probabilities fortransitions from each state to each other state oritself.
The emission matrix contains theprobabilities for each emission symbol to occurat each state.While the sequence of emissions can beobserved, the state sequence is hidden.
However,given an emission sequence, standard algorithmsdefined for the HMM allow to calculate theprobability of each state at each point in thesequence.
The probability for the model to be ina state is dependent on the previous state and theemissions observed at the current state.As illustrated by Figure 1, the development ofthe users?
opinions can be modelled as an HMM.The user judgment about the dialog is modelledas states, each state representing a specificjudgment (think of it as ?emotional states?).
Aprediction is made at each dialog turn.
In themodel depicted, the user judgment can either be?bad?
or ?good?.
Each judgment has aprobabilistic relation to the current events in thedialog.
In the picture, the events are described inthe form of understanding errors andconfirmation types, i.e.
there are two featureswhich can take a number of different values,each with a certain probability.Although the judgments do not ?emit?
theevents at each turn (the causal relation isopposite), the probabilistic relation between themcan be captured and evaluated with the HMMand the associated algorithms.Apart from the dialog events, the currentjudgment is also determined by the previousjudgment.
For example, we expect that thejudgments are varying smoothly, i.e.
theprobability for a transition becomes lower withincreasing (semantic) distance between the statelabels.Although events in previous turns cannotimpact the current judgment given this modeltopology, it is possible to incorporate dialog171history by creating features with a time lag.
E.g.,a feature could represent the understanding errorin the previous turn.
Also, simultaneity ofdifferent events affecting the quality perceptioncan be evaluated by calculating probabilities foreach judgment given the observed combinationof features.
If the features are interacting (i.e.
theprobability of one feature changes in dependenceof another feature), this is modelled by directlyspecifying the emission probabilities for eachcombination of features.
We call this a layer ofemissions.
Additional layers with other featurescan be created.
In this case, the likelihood ofeach judgment given probabilities from eachlayer can be calculated by multiplication of theprobabilities from each layer.For the calculation of state probabilities, wecan use forward recursion (Rabiner, 1989).
Thealgorithm proceeds through the observedsequence, and at each step calculates theprobability for each state given the probabilitiesof the observation, the probabilities of each stateat the previous step, and the transitionprobabilities.Figure 1.
Topology of an HMM to model userjudgments (?good?
or ?bad?)
in theirprobabilistic relation to dialog events (error andconfirmation strategy) and the previous rating.3 Data CollectionIn order to train the model, data is needed inwhich the association between dialog events andjudgments at each turn is represented.
Therefore,we conducted an experiment in which usersinteracted with an SDS and judged theinteraction after each dialog turn.
In addition, allusers followed the same five dialogs, which waspossible by using the Wizard-of-Oz technique(Fraser and Gilbert 1991) and relatively strictscenario descriptions.As the interactions were predefined, care wastaken that a representative sample of relevantusability problems were passed in theinteractions.
Such problems were collected in abrainstorming session, and compriseunderstanding errors, task failure, promptwording issues (such as incomplete information),missing control options, or not being able toinput one of the criteria specified in the scenariodescription.
As further features of the dialog,which might impact the quality judgment, thenumber of concepts which can be specified, aswell as the confirmation strategy, should varyalong the interactions.We then designed a dialog system with aconsistent strategy, which however allowed forthe quality-relevant issues mentioned above tooccur in the dialogs.
The system was derivedfrom the BoRIS restaurant information system(M?ller 2005b), which enables the search ofrestaurants in the German city Bochumaccording to the criteria day, time, location, typeof cuisine and pricing.
For our system, wesimplified the dialog strategy to system initiative,however, at the first turn, the users could sayboth day and time.
If both were recognized,explicit confirmation was requested.
If just oneconstraint was understood, confirmation was im-plicit.
The last constraint specified (pricing) wasnot confirmed, and instead the restaurantinformation was output directly.
If no restaurantwas found according to the specified criteria, theuser could change one of the constraints.
Table 1shows an example of a dialog with the system.We also reformulated some prompts to fit thechanged dialog strategy, and partly toincorporate potential wording problems.
Promptswere recorded with a non-professional speaker,S: Welcome to the telephone-based restaurantinformation system for Bochum.
When would youlike to eat?U: Tomorrow night.S: So you want to eat tonight?U: No.S: On which day would you like to eat?
?.U: Medium pricingS: Unfortunately, I could not find a restaurantmatching your query.
Would you like to changetime, location, cuisine or pricing?U: Yes, cuisine.S: Which type of cuisine are you searching for?U: Italian.S: The possible restaurants are: {name, address}Table 1.
Example dialog with the BoRISrestaurant information system, version as usedin the experiment.172using high-quality audio equipment.
During theinteractions, the wizard simply replayed theprompt foreseen at the current state of thepredefined dialog script.
In addition to theforeseen prompts, the wizard had at hand no-input and help prompts in case the user wouldbehave unexpectedly.25 users (13 females, 12 males), recruitedfrom the campus, but not all students,participated in the experiment.
Participants wereaged between 20 and 46 years (M=26.5;STD=6.6).
Ratings were given on a 5-point scale,where the points were labeled ?bad?, ?poor?,?fair?, ?good?, and ?excellent?.
Ratings wereinput through a number pad attached to the scale.Each participant rehearsed the procedure with atest dialog.
Before the experiment, all users filledout a questionnaire measuring their technicalaffinity.As the data collected in the described experi-ment are all needed to train the prediction modelfor as many combinations of feature values aspossible, we conducted a second experiment togenerate test data.
For this test, we asked 17 per-sons from our lab to conduct two dialogs withthe system mock-up.
The test setup was the sameas in the previous experiment, except that newdialogs were created without particularrequirements or restrictions.In both experiments, not all users behaved aswe hoped.
Therefore, not all of the predefineddialog scripts were judged by all participants(N=15?23 for training corpus, N=9?13 for testcorpus; N: number of valid dialogs).
For onedialog script in the training corpus, the deviatinginteractions were all equal (N=9), sodistributions of ratings per turn could becalculated for comparison with the predicteddistributions for this dialog.
For the training andcalculation of initial state probabilities, all dia-logs in the training corpus were used.The model derived from the data includes fivepossible states (one for each rating).
For a list offeatures annotated in the dialogs see Figure 2.4 ResultsIn order to evaluate the modeling approach, wefirst searched for the best model given thetraining data from the first experiment.
We thenapplied this model to the test data from thesecond experiment in order to evaluate the modelaccuracy given unseen data.
Afterwards, weexamined if another model trained on thetraining set can predict the test set better, i.e.
we?optimized?
the model on the test data.
Finally,we cross-check how well the model optimized onthe test data performs on the training data, whichgives a glimpse at how much the model is biasedtowards the test data.As the criterion for the optimization, we deter-mined the mean squared error (MSE), andaveraged across all dialog script in the corpus onwhich the model was optimized.
For each dialogscript, all 5 probabilities (ratings ?bad?
to?excellent?)
at each dialog turn were taken intoaccount, i.e.
the squared prediction errors wereadded.
If rate is the rating, thenAs this measure, in the particular way weapplied it here, is not easily comparable to otherresults, we add two pictures illustrating theaccuracy represented either by a rather low or bya rather high MSE.
In addition, we report themean absolute error (MAEmax) of the models inpredicting the most likely rating at each state(mean rating if two ratings with equal probability)and the baseline performance when theunconditional distribution of ratings is predicted.We first optimized a model on the trainingdata, meaning that we selected parameters,trained the HMM with these parameters on thetraining data and then predicted results for all 6dialog scripts contained in the training set (top ofFeature ValuesunderstandingerrorsPA:PA (partially correct)PA:FA (failed)PA:IC (incorrect)confirmationstrategyexplicitimplicitnonesystem speechactask for 2 constraintsask for 1 constraintask for selection of a constraintprovide infouser speech actprovide inforepeat infoconfirmmeta communicationno-inputcontextualappropriateness(Grice?smaxims)mannerqualityquantityrelevancetask success successfailureTable 2.
Annotated dialog features.
[ ]nratepratepMSEnturn ratepredempdial?
?= =?= 1512)()(173Table 3).
The optimized model was chosen as theone returning the smallest MSE (mean of alltasks).
The best model included understandingerrors interacting with confirmation type at eachturn, and interacting with task success.
As weanalyzed the prediction results, we found thatwhenever the system changed from asking twoconstraints at a time to just one (which is done inorder to avoid multiple errors in a row), thepredictions were too positive.
We thereforeintroduced a new feature, which is annotatedwhenever the system asks for a single constraintwhich has been asked in a more complexquestion before (?dummy?).
In the modeloptimized on training data, this parameter wasincluded on a separate feature layer.
That is, thisfeature impacts quality perception independentof the other features?
values.We then used this model to predict the testdata collected in the second experiment (top ofTable 4).
As expected, the MSE clearly increases;however, this was partly due to the difference inthe sample of participants.
As in the secondexperiment participants were recruited from ourlab, their technical affinity was relatively high.Therefore, we retrained the HMM with onlythose 50% of the users from the training set whogot the highest score on the technical affinityquestionnaire.
With this model, the prediction oftest data improved.In a next step, we optimized the model on thetest set meaning that we searched for theparameter combination achieving the best resulton the two test dialogs.
However, the model wasstill trained on the training data from the firstexperiment.
As expected, the MSE could beimproved.
However, only minor changes in thefeature configuration are necessary: Still, errorsand confirmation type are interacting on thesame layer.
However, task success is included asindependent variable on a second layer, andinstead, the error in the previous turn determinesthe impact of errors and confirmation on theratings.
Again, we tested if the prediction can bePredicted: training dialogsDial 1Dial 2Dial 3Dial 4Dial 5Dial 6Mean (basel.
)Optimized on trainingdialogsLayer 1: Error, Confirm, Task SuccessLayer 2: DummyMSE: 0.0185 0.0307 0.0166 0.0216 0.0333 0.0477 0.0281 (0.1201)MAEmax: 0.7000 0.5714 0.2857 0.0556 0.3636 0.3333 0.3849 (0.6167)Optimized on test dialogsLayer 1: Errors, Errors_lag, ConfirmationLayer 2: TaskSuccessMSE: 0.0272 0.0358 0.0247 0.0374 0.0400 0.0574 0.0371 (0.1201)MAEmax: 0.5000 0.4286 0.4286 0.3889 0.4545 0.3333 0.4223 (0.6167)Number of valid dialogs (N): 22 15 23 17 17 9Table 3.
Evaluation of predictions of training dialogs (mean squared error and mean absolute errorin predicting the most probable state at each turn).
Baseline results are given in brackets.
The featurecombinations with which results were obtained are also reported.Predicted: test dialogs Dial 1 Dial 2 Mean (baseline)Optimized on training dialogs Layer 1: Error, Confirm, Task SuccessLayer 2: DummyMSE: 0.1039 0.0429 0.0734 (0.1583)MAEmax: 0.4444 0.6250 0.5347 (0.6944)Optimized on training dialogs (tah) Layer 1: Error, Confirm, Task SuccessLayer 2: DummyMSE: 0.0957 0.0387 0.0672 (0.1636)MAEmax: 0.3333 0 0.1667 (0.6944)Optimized on test dialogs (rf) Layer 1: Errors, Errors_lag, ConfirmLayer 2: TaskSuccessMSE: 0.0789 0.0349 0.0569 (0.1583)MAEmax: 0.4444 0.6250 0.5347 (0.6944)Optimized on test dialogs (tah; rf) Layer 1: Errors, ConfirmMSE: 0.0860 0.0374 0.0617 (0.1636)MAEmax: 0.3333 0 0.1667 (0.6944)Number of valid dialogs (N): 9 13Table 4.
Evaluation of predictions of training dialogs (tah=model trained on users with hightechnical affinity; rf=user speech act feature exclude from analysis)174improved by considering differences between theusers?
technical affinity.
However, repeating theprocedure for only those users with hightechnical affinity did not improve the result thistime.
Concerning the parameters, error andconfirmation type were confirmed to besignificant predictors of quality judgments.
Thedummy parameter created to improve theaccuracy on training data was not proven usefulfor the prediction of the test set ratings.In order to cross-check the validity of themodel optimized on test data, we finallypredicted the ratings of the 6 dialogs from thetraining set with the same model (bottom ofTable 3).
As can be seen, the prediction is worsethan that from the model optimized on thetraining set.
However, the quality of theprediction is still reasonable, showing that thetwo datasets do not demand for completelydifferent models.
All predictions are above thebaseline.5 DiscussionIn the previous section, we presented resultsachieved with our models in terms of MSE.
Inorder to gain meaning to the values of MSE, weadded the mean absolute error of predicting themost probable judgment at each state.
A closerlook at the relation between MSE and MAEmaxreveals that both measures are not strictlycorrelated (see e.g.
the first two models in Table4).
While the MSE measures the distance at eachmeasurement point in the distribution, theMAEmax is a rough indicator of the similarity ofthe shape of the predicted and observedprobability curves.
The results for MAEmax arepromising, as predictions of test data are in therange of predictions of training data and betterthan the baseline.
Also, predictions made fromparticipants with high technical affinity achievebetter results on the test data in all cases, whichwas expected, but not found for the MSE results.Figure 2 presents examples of predictionresults graphically.
We chose one example of anaverage, and one of a relatively bad prediction, toallow extrapolation to other results presented.The pictures show that even a relatively highMSE corresponds to a fair quality of theprediction.
The probability curves are mostlysimilar, mainly smoother than the observedprobability distributions.
Sometimes thepredictions are too optimistic, however, usuallythe change in judgments is predicted, just not theextent of this change.
We can only hypothesizeFigure 2.
Examples of predictions on testdata made with the model, illustrating themeaning of MSE values.
Depicted are twodialogs (columns) with 9 (left) and 8 (right)turns (rows).
For each turn, the empirical(solid line) and predicted (dotted line) ratingdistributions are given.
Left: MSE=0.0957;N(emp)=9.
Right: MSE=0.0349;N(emp)=13.175about the reasons for the participants to judge therespective dialog worse than predicted by themodel.
A possible reason is that users moreeasily decrease their judgments when the dialoghas a longer history of problematic situations.According to our data, the users were relativelyforgiving and increased their judgments if thedialog went well, even if previously errors hadoccurred.
However, the errors might not really beforgot, and be reflected in the judgment of laterproblems and errors.
Unfortunately, for reasonsof data scarcity, the wider dialog history cannotbe considered in the models.Another source of prediction error might bethe sample size available for the predicteddialogs.
If sample size (N) and MSE values arecompared among the dialogs, it can be observedthat both values are correlated.
This might be dueto less smooth probability distribution curves iffew ratings are available at each turn.
While thecurves depicted in Figure 2 are sometimes spiky,with increasing sample size normal distributionshould be more likely.
This might to somedegree explain the clearly higher MSE for the testdata predictions despite the relatively small errorin predicting the most probable ratings.6 ConclusionIn this paper, we presented a new approach to theprediction of user judgments about SDSs, usingHMMs.
The approach allows predicting theusers?
judgments at each step of a dialog.
Inpredicting the distribution of ratings of manyusers, the approach takes into accountdifferences between the users?
judgmentbehaviors.
This increases the usefulness of themodel for a number of applications.
E.g., inadaptive systems, the decision process can takeinto account differences between the users whichcannot be attributed to user characteristics knownto the system.
If the model is applied toautomatically generated dialogs, e.g.
in theMeMo workbench (Engelbrecht et al, 2008a), amore detailed prediction of user satisfaction isenabled, allowing analysis on a turn-by-turnbasis.In addition, the approach facilitates theanalysis of models and features affecting thequality ratings, as results can be compared to theempirical ratings with more detail.
We hope togain further insight into the relations betweeninteraction parameters and user judgments byrunning simulations under different assumptionsof relations between these entities.A drawback of the approach is the generationof training data.
The models presented in thispaper cannot be assumed to be general, and inparticular are lacking important parametersreflecting the timing in the dialogs.
Therefore, asa next step the acquisition of judgments shouldbe improved to be less disruptive for theinteraction.
In addition, it would be interesting tofind a method for deriving the correctdistributions of ratings at each dialog turn from acorpus of different dialogs, e.g.
by groupingsituations which are comparable.
At the moment,we are also investigating if judgments can beacquired after the interactions without a loss invalidity.After all, the results we achieved with themodel suggest that HMMs are suitable formodeling the users?
quality perception of dialogswith SDSs.
Further research on the topic willhopefully show if the dialog history has to beconsidered to a wider degree than in our presentmodels.Concerning dialog features and their relationto the judgments, the role of understanding errorsin combination with the confirmation type couldbe established so far.
More rich data are neededto work towards a general model for judgmentpredictions, including all relevant parameters.
Ifjudgments can be acquired after the interactions,we will be able to easily get the data needed for abetter (and maybe complete) model.
In any case,we are confident that the approach taken willallow a deeper analysis of the quality judgmentprocess, which will enable progress by moreanalytical methods, such as formulating andtesting hypotheses about this process.ReferencesHua Ai, Fuliang Weng.
2008.
User Simulation asTesting for Spoken Dialog Systems.
Proc.
of the 9thSIGdial Workshop on Discourse and Dialogue,Columbus, Ohio.Grace Chung.
2004.
Developing a flexible spokendialog system using simulation.
Proc.
of the 42ndAnnual Meeting on Association for ComputationalLinguistics, Barcelona, Spain.Klaus-Peter Engelbrecht, Sebastian M?ller.
2007.Pragmatic Usage of Linear Regression Models forthe Prediction of User Judgments.
Proc.
of 8thSIGdial Workshop on Discourse and Dialogue,Antwerp, Belgium.Klaus-Peter Engelbrecht, Michael Kruppa, SebastianM?ller, Michael Quade.
2008a.
MeMo Workbench176for Semi-Automated Usability Testing.
Proc.
of 9thInterspeech, Brisbane, Australia.Klaus-Peter Engelbrecht, Sebastian M?ller, RobertSchleicher, Ina Wechsung.
2008b.
Analysis ofPARADISE Models for Individual Users of aSpoken Dialog System.
Proc.
of ESSV 2008,Frankfurt/Main, Germany.Klaus-Peter Engelbrecht, Felix Hartard, FlorianG?dde, Sebastian M?ller.
2009.
A Closer Look atQuality Judgments of Spoken Dialog Systems,submitted to Interspeech 2009.Norman M. Fraser, G. Nigel Gilbert.
1991.Simulating speech systems.
Computer Speech andLanguage, 5:81?99.Rainer Guski.
1999.
Personal and Social Variables asCo-determinants of Noise Annoyance.
Noise &Health, 3:45-56.Kate S. Hone, Robert Graham.
2001.
SubjectiveAssessment of Speech-system Interface Usability.Proc.
of EUROSPEECH, Aalborg, Denmark.ITU-T Rec.
P.851, 2003.
Subjective QualityEvaluation of Telephone Services Based on SpokenDialogue Systems, InternationalTelecommunication Union, Geneva, Switzerland.Ute Jekosch.
2005.
Voice and Speech QualityPerception.
Assessment and Evaluation, Springer,Berlin, Germany.Ram?n L?pez-C?zar, ?ngel de la Torre, Jos?
C.Segura and Antonio J. Rubio.
2003.
Assessment ofDialogue Systems by Means of a New SimulationTechnique.
Speech Communication, 40(3):387-407.Sebastian M?ller.
2005a.
Perceptual QualityDimensions of Spoken Dialog Systems: A Reviewand New Experimental Results, Proc.
of ForumAcusticum, Budapest, Hungary.Sebastian M?ller.
2005b.
Quality of Telephone-basedSpoken Dialog Systems.
Springer, New York.Sebastian M?ller, Klaus-Peter Engelbrecht, RobertSchleicher.
2008.
Predicting the Quality andUsability of Spoken Dialogue Services, SpeechCommunication, 50:730-744.Morris A. Okun, Renee M. Weir.
1990.
Toward aJudgment Model of College Satisfaction.Educational Psychological Review, 2(1):59-76.Lawrence R. Rabiner.
1989.
A tutorial on HMM andselected applications in speech recognition.
Proc.IEEE, 77(2):257-286.Verena Rieser, Oliver Lemon.
2008.
AutomaticLearning and Evaluation of User-CenteredObjective Functions for Dialogue SystemOptimisation.
Proc.
of LREC'08, Marrakech,Morocco.Bernhard Steimel, Oliver Jacobs, Norbert Pfleger,Sebastian Paulke.
2008.
Testbericht VOICE Award2008: Die besten deutschsprachigenSprachapplikationen.
Initiative Voice Business,Bad Homburg, Germany.Marilyn A. Walker, Diane J. Litman, Candace A.Kamm, Alicia Abella.
1997.
PARADISE: AFramework for Evaluating Spoken DialogueAgents.
Proc.
of ACL/EACL 35th Ann.
Meeting ofthe Assoc.
for Computational Linguistics, Madrid,Spain.Marilyn A. Walker, Diane J. Litman, Candace A.Kamm, Alicia Abella.
1998.
Evaluating SpokenDialog Agents with PARADISE: Two CaseStudies.
Computer Speech and Language, 12:317-347.Marilyn Walker, Candace Kamm, Diane Litman.2000.
Towards Developing General Models ofUsability with PARADISE.
Natural LanguageEngineering, 6(3-4):363-377.Craig Wootton, Michael McTear, Terry Anderson.2007.
Utilizing Online Content as DomainKnowledge in a Multi-Domain Dynamic DialogueSystem.
Proc.
of Interspeech 2007, Antwerp,Belgium.177
