Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
698?707, Prague, June 2007. c?2007 Association for Computational LinguisticsExploiting Wikipedia as External Knowledge for Named Entity RecognitionJun?ichi Kazama and Kentaro TorisawaJapan Advanced Institute of Science and Technology (JAIST)Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan{kazama, torisawa}@jaist.ac.jpAbstractWe explore the use of Wikipedia as externalknowledge to improve named entity recog-nition (NER).
Our method retrieves the cor-responding Wikipedia entry for each can-didate word sequence and extracts a cate-gory label from the first sentence of the en-try, which can be thought of as a definitionpart.
These category labels are used as fea-tures in a CRF-based NE tagger.
We demon-strate using the CoNLL 2003 dataset that theWikipedia category labels extracted by sucha simple method actually improve the accu-racy of NER.1 IntroductionIt has been known that Gazetteers, or entity dic-tionaries, are important for improving the perfor-mance of named entity recognition.
However, build-ing and maintaining high-quality gazetteers is verytime consuming.
Manymethods have been proposedfor solving this problem by automatically extractinggazetteers from large amounts of texts (Riloff andJones, 1999; Thelen and Riloff, 2002; Etzioni et al,2005; Shinzato et al, 2006; Talukdar et al, 2006;Nadeau et al, 2006).
However, these methods re-quire complicated induction of patterns or statisticalmethods to extract high-quality gazetteers.We have recently seen a rapid and successfulgrowth of Wikipedia (http://www.wikipedia.org),which is an open, collaborative encyclopedia onthe Web.
Wikipedia has now more than 1,700,000articles on the English version (March 2007) andthe number is still increasing.
Since Wikipediaaims to be an encyclopedia, most articles are aboutnamed entities and they are more structured than rawtexts.
Although it cannot be used as gazetteers di-rectly since it is not intended as a machine readableresource, extracting knowledge such as gazetteersfrom Wikipedia will be much easier than from rawtexts or from usual Web texts because of its struc-ture.
It is also important that Wikipedia is up-dated every day and therefore new named entities areadded constantly.
We think that extracting knowl-edge from Wikipedia for natural language process-ing is one of the promising ways towards enablinglarge-scale, real-life applications.
In fact, manystudies that try to exploit Wikipedia as a knowl-edge source have recently emerged (Bunescu andPas?ca, 2006; Toral and Mun?oz, 2006; Ruiz-Casadoet al, 2006; Ponzetto and Strube, 2006; Strube andPonzetto, 2006; Zesch et al, 2007).As a first step towards such approach, we demon-strate in this paper that category labels extractedfrom the first sentence of a Wikipedia article, whichcan be thought of as the definition of the entity de-scribed in the article, are really useful to improve theaccuracy of NER.
For example, ?Franz Fischler?
hasthe article with the first sentence, ?Franz Fischler(born September 23, 1946) is an Austrian politi-cian.?
We extract ?politician?
from this sentenceas the category label for ?Franz Fischler?.
We usesuch category labels as well as matching informa-tion as features of a CRF-based NE tagger.
In ourexperiments using the CoNLL 2003 NER dataset(Tjong et al, 2003), we demonstrate that we canimprove performance by using the Wikipedia fea-tures by 1.58 points in F-measure from the baseline,and by 1.21 points from the model that only usesthe gazetteers provided in the CoNLL 2003 dataset.Our final model incorporating all features achieved88.02 in F-measure, which means a 3.03 point im-provement over the baseline, which does not use any698gazetteer-type feature.The studies most relevant to ours are Bunescu andPas?ca (2006) and Toral and Mun?oz (2006).Bunescu and Pas?ca (2006) presented a method ofdisambiguating ambiguous entities exploiting inter-nal links in Wikipedia as training examples.
Thedifference however is that our method tries to useWikipedia features for NER, not for disambiguationwhich assumes that entity regions are already found.They also did not focus on the first sentence of anarticle.
Also, our method does not disambiguateambiguous entities, since accurate disambiguationis difficult and possibly introduces noise.
There aretwo popular ways for presenting ambiguous entitiesin Wikipedia.
The first is to redirect users to a dis-ambiguation page, and the second is to redirect usersto one of the articles.
We only focused on the secondcase and did not utilize disambiguation pages in thisstudy.
This method is simple but works well becausethe article presented in the second case represents inmany cases the major meaning of the ambiguous en-tities and therefore that meaning frequently appearsin a corpus.Toral and Mun?oz (2006) tried to extract gazetteersfrom Wikipedia by focusing on the first sentences.However, their way of using the first sentence isslightly different.
We focus on the first noun phraseafter be in the first sentence, while they used all thenouns in the sentence.
By using these nouns andWordNet, they tried to map Wikipedia entities to ab-stract categories (e.g., LOC, PER ORG, MISC) usedin usual NER datasets.
We on the other hand use theobtained category labels directly as features, sincewe think the mapping performed automatically bya CRF model is more precise than the mapping byheuristic methods.
Finally, they did not demonstratethe usefulness of the extracted gazetteers in actualNER systems.The rest of the paper is organized as follows.
Wefirst explain the structure of Wikipedia in Section2.
Next, we introduce our method of extracting andusing category labels in Section 3.
We then showthe experimental results on the CoNLL 2003 NERdataset in Section 4.
Finally, we discuss the pos-sibility of further improvement and future work inSection 5.2 Wikipedia2.1 Basic structureAn article in Wikipedia is identified by a uniquename, which can be obtained by concatenating thewords in the article title with underscore ?
?.
For ex-ample, the unique name for the article, ?David Beck-ham?, is David Beckham.
We call these uniquenames ?entity names?
in this paper.Wikipedia articles have many useful structures forknowledge extraction such as headings, lists, inter-nal links, categories, and tables.
These are markedup by using the Wikipedia syntax in source files,which authors edit.
See the Wikipedia entry iden-tified by How to edit a page for the details of themarkup language.We describe two important structures, redirec-tions and disabiguation pages, in the following sec-tions.2.2 RedirectionSome entity names in Wikipedia do not have a sub-stantive article and are only redirected to an arti-cle with another entity name.
This mechanism iscalled ?redirection?.
Redirections are marked upas ?#REDIRECT [[A B C]]?
in source files, where?[[...]]?
is a syntax for a link to another article inWikipedia (internal links).
If the source file has sucha description, users are automatically redirected tothe article specified by the entity name in the brackes(A B C for the above example).
Redirections areused for several purposes regarding ambiguity.
Forexample, they are used for spelling resolution suchas from ?Apples?
to ?Apple?
and abbreviation res-olution such as from ?MIT?
to ?Massachusetts In-stitute of Technology?.
They are also used in thecontext of more difficult disambiguations describedin the next section.2.3 Disambiguation pagesSome authors make a ?disambiguation?
page for anambiguous entity name.1 A disambiguation pagetypically enumerates possible articles for that name.For example, the page for ?Beckham?
enumerates?David Beckham (English footballer)?, ?Victoria1We mean by ?ambiguous?
the case where a name canbe used to refer to several difference entities (i.e., articles inWikipedia).699Beckham (English celebrity and wife of David)?,?Brice Beckham (American actor)?, and so on.Most, but not all, disambiguation pages have a namelike Beckham (disambiguation) and are some-times used with redirection.
For example, Beck-ham is redirected to Beckham (disambiguation)in the above example.
However, it is also possiblethat Beckham redirects to one of the articles (e.g,David Beckham).
As we mentioned, we did notutilize the disambiguation pages and relied on theabove case in this study.2.4 DataSnapshots of the entire contents of Wikipedia areprovided in XML format for each language version.We used the English version at the point of Febru-ary 2007, which includes 4,030,604 pages.2 We im-ported the data into a text search engine3 and used itfor the research.3 MethodIn this section, we describe our method of extractingcategory labels fromWikipedia and how to use thoselabels in a CRF-based NER model.3.1 Generating search candidatesOur purpose here is to find the corresponding en-tity in Wikipedia for each word sequence in a sen-tence.
For example, given the sentence, ?Rare JimiHendrix song draft sells for almost $17,000?, wewould like to know that ?Jimi Hendrix?
is describedin Wikipedia and extract the category label, ?mu-sician?, from the article.
However, considering allpossible word sequences is costly.
We thus restrictedthe candidates to be searched to the word sequencesof no more than eight words that start with a wordcontaining at least one capitalized letter.43.2 Finding category labelsWe converted a candidate word sequence to aWikipedia entity name by concatenating the wordswith underscore.
For example, a word sequence2The number of article pages is 2,954,255 including redirec-tion pages3We used HyperEstraier available athttp://hyperestraier.sourceforge.net/index.html4Words such as ?It?
and ?He?
are not considered as capital-ized words here (we made a small list of stop words).
?Jimi Hendrix?
is converted to Jimi Hendrix.
Next,we retrieved the article corresponding to the entityname.5 If the page for the entity name is a redirec-tion page, we followed redirection until we find anon-redirection page.Although there is no strict formatting rule inWikipedia, the convention is to start an article witha short sentence defining the entity the article de-scribes.
For example, the article for Jimi Hendrixstarts with the sentence, ?Jimi Hendrix (November27, 1942, Seattle, Washington - September 18, 1970,London, England) was an American guitarist, singerand songwriter.?
Most of the time, the head noun ofthe noun phrase just after be is a good category la-bel.
We thus tried to extract such head nouns fromthe articles.First, we eliminated unnecessary markup suchas italics, bold face, and internal links from thearticle.
We also converted the markup for inter-nal links like [[Jimi Hendrix|Hendrix]] toHendrix, since the part after |, if it exists, rep-resents the form to be displayed in the page.
Wealso eliminated template markup, which is enclosedby {{ and }}, because template markup sometimescomes at the beginning of the article and makesthe extraction of the first sentence impossible.6 Wethen divided the article into lines according to thenew line code, \n, <br> HTML tags, and a verysimple sentence segmentation rule for period (.
).Next, we removed lines that match regular expres-sion /?\s*:/ to eliminate the lines such as:This article is about the tree and its fruit.For the consumer electronics corporation,see Apple Inc.These sentences are not the content of the article butoften placed at the beginning of an article.
Fortu-nately, they are usually marked up using :, which isfor indentation.After the preprocessing described above, we ex-tracted the first line in the remaining lines as the firstsentence from which we extract a category label.5There are pages for other than usual articles in theWikipedia data.
They are distinguished by a namespace at-tribute.
To retrieve articles, we only searched in namespace 0,which is for usual articles.6Templates are used for example to generate profile tablesfor persons.700We then performed POS tagging and phrase chunk-ing.
TagChunk (Daume?
III and Marcu, 2005)7 wasused as a POS/chunk tagger.
Next, we extracted thefirst noun phrase after the first ?is?, ?was?, ?are?, or?were?
in the sentence.
Basically, we extracted thelast word in the noun phrase as the category label.However, we used the second noun phrase when thefirst noun phrase ended with ?one?, ?kind?, ?sort?,or ?type?, or it ended with ?name?
followed by ?of?.These rules were for treating examples like:Jazz is [a kind]NP [of]PP [music]NP characterizedby swung and blue notes.In these cases, we would like to extract the headnoun of the noun phrase after ?of?
(e.g., ?music?in instead of ?kind?
for the above example).
How-ever, we would like to extract ?name?
itself when thesentence was like ?Ichiro is a Japanese given name?.We did not utilize Wikipedia?s ?Category?
sec-tions in this study, since a Wikipedia article can havemore than one category, and many of them are notclean hypernyms of the entity as far as we observed.We will need to select an appropriate category fromthe listed categories in order to utilize the Categorysection.
We left this task for future research.3.3 Using category labels as featuresIf we could find the category label for the candidateword sequence, we annotated it using IOB2 tags inthe same way as we represent named entities.
InIOB2 tagging, we use ?B-X?, ?I-X?, and ?O?
tags,where ?B?, ?I?, and ?O?
means the beginning of anentity, the inside of an entity, and the outside of en-tities respectively.
Suffix X represents the categoryof an entity.8 In this case, we used the extracted cat-egory label as the suffix.
For example, if we foundthat ?Jimi Hendrix?
was in Wikipedia and extracted?guitarist?
as the category label, we annotated thesentence, ?Rare Jimi Hendrix song draft sells for al-most $17,000?, as:RareO JimiB-guitarist HendrixI-guitarist songO draftOforO almostO $17,000O .ONote that we adopted the leftmost longest match ifthere were several possible matchings.
These IOB2tags were used in the same way as other features7http://www.cs.utah.edu/?hal/TagChunk/8We use bare ?B?, ?I?, and ?O?
tags if we want to representonly the matching information.in our NE tagger using Conditional Random Fields(CRFs) (Lafferty et al, 2001).
For example, we useda feature such as ?the Wikipedia tag is B-guitaristand the NE tag is B-PER?.4 ExperimentsIn this section, we demonstrate the usefulness of theextracted category labels for NER.4.1 Data and settingWe used the English dataset of the CoNLL 2003shared task (Tjong et al, 2003).
It is a corpus ofEnglish newspaper articles, where four entity cate-gories, PER, LOC, ORG, and MISC are annotated.It consists of training, development, and testing sets(14,987, 3,466, and 3,684 sentences, respectively).We concatenated the sentences in the same docu-ment according to the document boundary markersprovided in the dataset.9 This generated 964 doc-uments for the training set, 216 documents for thedevelopment set, and 231 documents for the test-ing set.
Although automatically assigned POS andchunk tags are also provided in the dataset, we usedTagChunk (Daume?
III and Marcu, 2005)10 to assignPOS and chunk tags, since we observed that accu-racy could be improved, presumably due to the qual-ity of the tags.11We used the features summarized in Table 1 as thebaseline feature set.
These are similar to those usedin other studies on NER.
We omitted features whosesurface part described in Table 1 occurred less thantwice in the training corpus.Gazetteer files for the four categories, PER(37,831 entries), LOC (10,069 entries), ORG (3,439entries), and MISC (3,045 entries), are also providedin the dataset.
We compiled these files into onegazetteer, where each entry has its entity category,and used it in the same way as the Wikipedia featuredescribed in Section 3.3.
We will compare featuresusing this gazetteer with those using Wikipedia inthe following experiments.9We used sentence concatenation because we found it im-proves the accuracy in another study (Kazama and Torisawa,2007).10http://www.cs.utah.edu/?hal/TagChunk/11This is not because TagChunk overfits the CoNLL 2003dataset (TagChunk is trained on the Penn Treebank (Wall StreetJournal), while the CoNLL 2003 data are taken from the Reuterscorpus).701Table 1: Baseline features.
The value of a node fea-ture is determined from the current label, y0, and asurface feature determined only from x.
The valueof an edge feature is determined by the previous la-bel, y?1, the current label, y0, and a surface feature.Used surface features are the word (w), the down-cased word (wl), the POS tag (pos), the chunk tag(chk), the prefix of the word of length n (pn), thesuffix (sn), the word form features: 2d - cp (theseare based on (Bikel et al, 1999))Node features:{?
?, x?2, x?1, x0, x+1, x+2} ?
y0x = w, wl, pos, chk, p1, p2, p3, p4, s1, s2, s3, s4, 2d,4d, d&a, d&-, d&/, d&,, d&., n, ic, ac, l, cpEdge features:{?
?, x?2, x?1, x0, x+1, x+2} ?
y?1 ?
y0x = w, wl, pos, chk, p1, p2, p3, p4, s1, s2, s3, s4, 2d,4d, d&a, d&-, d&/, d&,, d&., n, ic, ac, l, cpBigram node features:{x?2x?1, x?1x0, x0x+1} ?
y0x = wl, pos, chkBigram edge features:{x?2x?1, x?1x0, x0x+1} ?
y?1 ?
y0x = wl, pos, chkWe used CRF++ (ver.
0.44)12 as the basis of ourimplementation of CRFs.
We implemented scaling,which is similar to that for HMMs (see for instance(Rabiner, 1989)), in the forward-backward phase ofCRF training to deal with long sequences due tosentence concatenation.13 We used Gaussian reg-ularization to avoid overfitting.
The parameter ofthe Gaussian, ?2, was tuned using the developmentset.14 We stopped training when the relative changein the log-likelihood became less than a pre-definedthreshold, 0.0001, for at least three iterations.4.2 Category label findingTable 2 summarizes the statistics of category labelfinding for the training set.
Table 3 lists examplesof the extracted categories.
As can be seen, wecould extract more than 1,200 distinct category la-bels.
These category labels seem to be useful, al-12http://chasen.org/?taku/software/CRF++13We also replaced the optimization module in the originalpackage with that used in the Amis maximum entropy estima-tor (http://www-tsujii.is.s.u-tokyo.ac.jp/amis) since we encoun-tered problems with the provided module in some cases.
Al-though this Amis module implements BLMVM (Benson andMore?, 2001), which supports the bounding of weights, we didnot use this feature in this study (i.e., we just used it as the re-placement for the L-BFGS optimizer in CRF++).14We tested 15 points: {0.01, 0.02, 0.04, .
.
.
, 163.84, 327.68}.Table 2: Statistics of category label finding.search candidates (including duplication) 256,418candidates having Wikipedia article 39,258(articles found by redirection) 9,587first sentence found 38,949category label extracted 23,885(skipped ?one?)
544(skipped ?kind?)
14(skipped ?sort?)
1(skipped ?type?)
41(skipped ?name of?)
463distinct category labels 1,248Table 3: Examples of category labels (top 20).category frequency # distinct entitiescountry 2598 152city 1436 284name 1270 281player 578 250day 564 131month 554 15club 537 167surname 515 185capital 454 79state 416 60term 369 78form 344 40town 287 97cricketer 276 97adjective 260 6golfer 229 88world 221 24team 220 52organization 214 38second 212 1though there is no guarantee that the extracted cate-gory label is correct for each candidate.4.3 Feature comparisonWe compared the following features in this experi-ment.Gazetteer Match (gaz m) This feature representsthe matching with a gazetteer entry by using?B?, ?I?, and ?O?
tags.
That is, this is thegazetteer version of wp m below.Gazetteer Category Label (gaz c) This featurerepresents the matching with a gazetteer entryand its category by using ?B-X?, ?I-X?, and?O?
tags, where X is one of ?PER?, ?LOC?,?ORG?, and ?MISC?.
That is, this is thegazetteer version of wp c below.Wikipedia Match (wp m) This feature representsthe matching with a Wikipedia entity by using?B?, ?I?, and ?O?
tags.702Table 4: Statistics of gazetteer and Wikipedia fea-tures.
Rows ?NEs (%)?
show the number of matchesthat also matched the regions of the named entities inthe training data, and the percentage of such namedentities (there were 23,499 named entities in total inthe training data).Gazetteer Match (gaz m)matches 12,397NEs (%) 6,415 (27.30%)Wikipedia Match (wp m)matches 27,779NEs (%) 16,600 (70.64%)Wikipedia Category Label (wp c)matches 18,617NEs (%) 11,645 (49.56%)common with gazetteer match 5,664Wikipedia Category Label (wp c) This featurerepresents the matching with a Wikipediaentity and its category in the way describedSection in 3.3.
Note that this feature onlyfires when the category label is successfullyextracted from the Wikipedia article.For these gaz m, gaz c, wp m, and wp c, we gener-ate the node features, the edge features, the bigramnode features, and the bigram edge features, as de-scribed in Table 1.Table 4 shows how many matches (the leftmostlongest matches that were actually output) werefound for gaz m, wp m, and wp c. We omit-ted the numbers for gaz c, since they are sameas gaz m. We can see that Wikipedia had morematches than the gazetteer, and covers more namedentities (more than 70% of the NEs in the trainingcorpus).
The overlap between the gazetteer matchesand the Wikipedia matches was moderate as the lastrow indicates (5,664 out of 18,617 matches).
Thisindicates that Wikipedia has many entities that arenot listed in the gazetteer.We then compared the baseline model (baseline),which uses the feature set in Table 1, with the fol-lowing models to see the effect of the gazetteer fea-tures and the Wikipedia features.
(A): + gaz m This uses gaz m in addition to thefeatures in baseline.
(B): + gaz m, gaz c This uses gaz m and gaz c inaddition to the features in baseline.
(C): + wp m This uses wp m in addition to the fea-tures in baseline.
(D): + wp m, wp c This uses wp m and wp c inaddition to the features in baseline.
(E): + gaz m, gaz c, wp m, wp c This usesgaz m, gaz c, wp m, and wp c in addition tothe features in baseline.
(F): + gaz m, gaz c, wp m, wp c (word comb.
)This model uses the combination of words(wl) and gaz m, gaz c, wp m, or wp c,in addition to the features of model (E).More specifically, these features are the nodefeature, wl0 ?
x0 ?
y0, the edge feature,wl0 ?
x0 ?
y?1 ?
y0, the bigram node feature,wl?1 ?
wl0 ?
x?1 ?
x0 ?
y0, and the bigramedge feature, wl?1?wl0?x?1?x0?y?1?y0,where x is one of gaz m, gaz c, wp m, andwp c. We tested this model because we thoughtthese combination features could alleviate theproblem by incorrectly extracted categoriesin some cases, if there is a characteristiccorrelation between words and incorrectlyextracted categories.Table 5 shows the performance of these mod-els.
The results for (A) and (C) indicate that thematching information alone does not improve ac-curacy.
This is because entity regions can be iden-tified fairly correctly if models are trained using asufficient amount of training data.
The category la-bels, on the other hand, are actually important forimprovement as the results for (B) and (D) indicate.The gazetteer model, (B), improved F-measure by1.47 points from the baseline.
TheWikipedia model,(D), improved F-measure by 1.58 points from thebaseline.
The effect of the gazetteer feature, gaz c,and the Wikipedia features, wp c, did not differmuch.
However, it is notable that the Wikipedia fea-ture, which is obtained by our very simple method,achieved such an improvement easily.The results for model (E) show that we can im-prove accuracy further, by using the gazetteer fea-tures and the Wikipedia features together.
Model (E)achieved 87.67 in F-measure, which is better thanthose of (B) and (D).
This result coincides with thefact that the overlap between the gazetteer feature703Table 5: Effect of gazetteer and Wikipedia features.dev evalmodel (best ?2) category P R F P R Fbaseline (20.48)PER 90.29 92.89 91.57 87.19 91.34 89.22LOC 93.32 92.81 93.07 88.14 88.25 88.20ORG 85.36 83.07 84.20 82.25 78.93 80.55MISC 92.21 84.71 88.30 79.58 75.50 77.49ALL 90.42 89.38 89.90 85.17 84.81 84.99(A): + gaz m (81.92)PER 90.60 92.56 91.57 87.90 90.72 89.29LOC 92.84 93.20 93.02 88.26 88.37 88.32ORG 85.54 82.92 84.21 82.37 79.05 80.68MISC 92.15 85.25 88.56 78.73 75.93 77.30ALL 90.41 89.45 89.92 85.33 84.76 85.04(B): + gaz m, gaz c (163.84)PER 92.45 94.41 93.42 90.78 91.96 91.37LOC 94.43 94.07 94.25 89.98 89.33 89.65ORG 86.68 85.38 86.03 82.43 81.34 81.88MISC 92.47 85.25 88.71 79.50 76.78 78.12ALL 91.77 90.84 91.31 86.74 86.17 86.46(C): + wp m (163.84)PER 90.84 92.56 91.69 87.77 90.11 88.92LOC 92.63 93.03 92.83 87.23 88.07 87.65ORG 86.19 83.74 84.95 81.77 79.65 80.70MISC 91.69 84.92 88.18 79.04 75.21 77.08ALL 90.49 89.53 90.01 84.85 84.58 84.71(D): + wp m, wp c (163.84)PER 91.57 94.41 92.97 90.13 92.02 91.06LOC 94.78 93.96 94.37 89.41 89.63 89.52ORG 87.36 85.01 86.17 82.70 82.00 82.35MISC 91.87 84.60 88.09 81.34 76.35 78.77ALL 91.68 90.63 91.15 86.71 86.42 86.57(E): + gaz m, gaz c, wp m,wp c (40.96)PER 93.32 95.49 94.39 92.28 93.14 92.71LOC 94.91 94.39 94.65 90.69 90.47 90.58ORG 88.27 86.95 87.60 83.08 83.68 83.38MISC 93.14 85.36 89.08 81.33 76.92 79.06ALL 92.65 91.65 92.15 87.79 87.55 87.67(F): + gaz m, gaz c, wp m,wp c (word comb.)
(5.12)PER 93.38 95.66 94.50 92.52 93.26 92.89LOC 94.88 94.77 94.83 91.25 90.71 90.98ORG 88.67 86.95 87.80 83.61 84.17 83.89MISC 93.56 85.03 89.09 81.63 77.21 79.36ALL 92.82 91.77 92.29 88.21 87.84 88.0270470727476788082848688100  200  300  400  500  600  700  800  900  1000Ftraining size (documents)baseline+wp_m+wp_m, wp_cFigure 1: Relation between the training size and theaccuracy.and the Wikipedia feature was not so large.
If weconsider model (B) a practical baseline, we can saythat the Wikipedia features improved the accuracy inF-measure by 1.21 points.We can also see that the effect of the gazetteerfeatures and the Wikipedia features were consistentirrespective of categories (i.e., PER, LOC, ORG, orMISC) and performance measures (i.e., precision,recall, or F-measure).
This indicates that gazetteer-type features are reliable as features for NER.The final model, (F), achieved 88.02 in F-measure.
This is greater than that of the baseline by3.03 points, showing the usefulness of the gazetteertype features.4.4 Effect of training sizeWe observed in the previous experiment that thematching information alone was not useful.
How-ever, the situation may change if the size of the train-ing data becomes small.
We thus observed the effectof the training size for the Wikipedia features wp mand wp c (we used ?2 = 10.24).
Figure 1 showsthe result.
As can be seen, the matching informationhad a slight positive effect when the size of trainingdata was small.
For example, it improved F-measureby 0.8 points from the baseline at 200 documents.However, the superiority of category labels over thematching information did not change.
The effect ofcategory labels became greater as the training sizebecame smaller.
Its effect compared with the match-ing information alone was 3.01 points at 200 docu-ments, while 1.91 points at 964 documents (i.e., thewhole training data).Table 6: Breakdown of improvements and errors.
(B) ?
(E) num.
g?
?
w?
g?
?
w g ?
w?
g ?
winc ?
inc 442 219 123 32 68inc ?
cor 102 28 56 3 15cor?
inc 56 28 13 7 8cor?
cor 5,342 1,320 1,662 723 1,6374.5 Improvement and error analysisWe analyze the improvements and the errors causedby using the Wikipedia features in this section.We compared the output of (B) and (E) for the de-velopment set.
There were 5,942 named entities inthe development set.
We assessed how the labelingfor these entities changed between (B) and (E).
Notethat the labeling for 199 sentences out of total 3,466sentences was changed.
Table 6 shows the break-down of the improvements and the errors.
?inc?
inthe table means that the model could not label theentity correctly, i.e., the model could not find the en-tity region at all, or it assigned an incorrect categoryto the entity.
?cor?
means that the model could labelthe entity correctly.
The column, ?inc ?
cor?, forexample, has the numbers for the entities that werelabeled incorrectly by (B) but labeled correctly by(E).
We can see from the column, ?num?, that thenumber of improvements by (E) exceeded the num-ber of errors introduced by (E) (102 vs. 56).
Table6 also shows how the gazetteer feature, gaz c, andthe Wikipedia feature, wp c, fired in each case.
Wemean that the gazetteer feature fired by using ?g?,and that the Wikipedia feature fired by using ?w?.?g??
and ?w??
mean that the feature did not fire.
Asis the case for other machine learning methods, itis difficult to find a clear reason for each improve-ment or error.
However, we can see that the numberof g?
?
w exceeded those of other cases in the caseof ?inc ?
cor?, meaning that the Wikipedia featurecontributed the most.Finally, we show an example of case inc ?cor in Figure 2.
We can see that ?Gazzetta delloSport?
in the sentence was correctly labeled as anentity of ?ORG?
category by model (E), because theWikipedia feature identified it as a newspaper en-tity.1515Note that the category label, ?character?, for ?Atalanta?
inthe sentence was not correct in this context, which is an examplewhere disambiguation is required.
The final recognition wascorrect in this case presumably because of the information fromgaz c feature.705O O O O B-LOCSentence No.
584UEFA came down heavily on Belgian club Standard Liege on Friday for " disgraceful behaviour " in an Intertoto final match against Karlsruhe of Germany .B-ORGO O O O O O B-ORG O O O O O O O O O O O O O O B-LOC O O OB-bodyO O O OB-countryO B-club I-club O O O O O O O O OB-competitionO O O B-city OB-countryOB-ORGO O O O B-MISC O B-ORGI-ORGO O O O O O O O O B-MISC O O O B-ORG O B-LOC OB-ORGO O O O B-MISC O B-ORGI-ORGO O O O O O O O O B-LOC O O O B-ORG O B-LOC OB-ORGO O O O B-MISC O B-ORGI-ORGO O O O O O O O O B-MISC O O O B-ORG O B-LOC OSentence No.
591ATHLETICS - HARRISON , EDWARDS TO MEET IN SARAJEVO .O O O O O O O O O OO O O O O O O O O OO O B-PER O B-PER O O O B-LOC OO O B-PER O B-LOC O O O B-LOC OO O B-PER O B-ORG O O O B-LOC OSentence No.
596Edwards was quoted as saying : " What type of character do we show by going to the IAAF Grand Prix Final in Milan where there is a lot of money to make but refusing to make the trip to Sarajevo as a humanitarian gesture ?
"B-PER O O O O O O O O O O O O O O O O OB-ORGB-MISCI-MISCO OB-PERO O O O O O O O O O O O O O O O B-LOC O O O O O OO O O O O O O O O O O O O O O O O OB-2003I-2003I-2003I-2003OB-cityO O O O O O O O O O O O O O O O B-city O O O O O OB-PER O O O O O O O O O O O O O O O O OB-MISCI-MISCI-MISCI-MISCOB-LOCO O O O O O O O O O O O O O O O B-LOC O O O O O OB-PER O O O O O O O O O O O O O O O O OB-ORGI-ORGI-ORGI-ORGI-ORGI-ORGO O O O O O O O O O O O O O O O B-LOC O O O O O OB-PER O O O O O O O O O O O O O O O O OB-ORGI-ORGI-ORGI-ORGOB-LOCO O O O O O O O O O O O O O O O B-LOC O O O O O OSentence No.
604SOCCER - MILAN 'S LENTINI MOVES TO ATALANTA .O O O O O O O O OO O B-missile O O O O O OO O B-ORG O B-PER O O B-ORG OO O B-PER O B-PER O O B-ORG OO O B-ORG O B-PER O O B-ORG OSentence No.
607The Gazzetta dello Sport said the deal would cost Atalanta around $ 600,000 .O O O B-ORG O O O O O B-ORG O O O OO B-newspaper I-newspaper I-newspaper O O O O O B-character O O O OO B-ORG I-ORG I-ORG O O O O O B-ORG O O O OO B-LOC O B-ORG O O O O O B-ORG O O O OO B-ORG I-ORG I-ORG O O O O O B-ORG O O O OSentence No.
610The move toBergamo-basedAtalanta reunites Lentini , who fell out withex-Milancoach Fabio Capello last season , with his former coach at Torino , Emiliano Mondonico .O O O O B-ORG O B-PER O O O O O O O B-PER O O O O O O O O O B-LOC O B-PER O OO O O OB-characterOB-townO O O O O O OB-coachesI-coachesO O O O O O O OB-businessOB-managerI-manager OO O O B-MISC B-ORG O B-PER O O O O OB-MISCO B-PER I-PER O O O O O O O O B-ORG O B-PER I-PER OO O O B-MISC B-ORG O B-PER O O O O O O O B-PER I-PER O O O O O O O O B-LOC O B-PER I-PER OO O O B-MISC B-ORG OB-ORGO O O O O O O B-PER I-PER O O O O O O O O B-LOC O B-PER I-PER OSentence No.
653Did not bat : Dharmasena , Vaas , Muralitharan .O O O O O O O O O OO O O O O O B-cricketer O B-cricketer OO O O O B-PER O B-PER O B-PER OO O O O B-PER O B-LOC O B-LOC O- gaz_c- wp_c- correct- (B)- (C)-  gaz_c- wp_c-  correct-  (B)-  (E)Figure 2: An example of improvement caused by Wikipedia feature.5 Discussion and Future WorkWe have empirically shown that even category la-bels extracted from Wikipedia by a simple methodsuch as ours really improves the accuracy of aNER model.
The results indicate that structuresin Wikipedia are suited for knowledge extraction.However, the results also indicate that there is roomfor improvement, considering that the effects ofgaz c and wp c were similar, while the matchingrate was greater for wp c. An issue, which weshould treat, is the disambiguation of ambiguousentities.
Our method worked well although it wasvery simple, presumably because of the followingreason.
(1) If a retrieved page is a disambiguationpage, we cannot extract a category label and criticalnoise is not introduced.
(2) If a retrieved page is nota disambiguation page, it will be the page describ-ing the major meaning determined by the agreementof many authors.
The extracted categories are use-ful for improving accuracy because the major mean-ing will be used frequently in the corpus.
How-ever, it is clear that disambiguation techniques arerequired to achieve further improvements.
In ad-dition, if Wikipedia grows at the current rate, it ispossible that almost all entities become ambiguousand a retrieved page is a disambiguation page mostof the time.
We will need a method for finding themost suitable article from the articles listed in a dis-ambiguation page.An interesting point in our results is thatWikipedia category labels improved accuracy, al-though they were much more specific (more than1,200 categories) than the four categories of theCoNLL 2003 dataset.
The correlation between aWikipedia category label and a category label ofNER (e.g., ?musician?
to ?PER?)
was probablylearned by a CRF tagger.
However, the merit ofusing such specific Wikipedia labels will be muchgreater when we aim at developing NER systems formore fine-grained NE categories such as proposedin Sekine et al (2002) or Shinzato et al (2006).We thus would like to inv tig te the effect of theWikipedia feature for NER with such fine-grainedcategories as well.
Disambiguation techniques willbe important again in that case.
Although the impactof ambiguity will be small as long as the target cat-egories are abstract and an incorrectly extracted cat-egory is in the same abstract category as the correctone (e.g., extracting ?footballer?
instead of ?crick-eter?
), such mis-categorization is critical if it is nec-essary to distinguish footballers from cricketers.6 ConclusionWe tried to exploit Wikipedia as external knowledgeto improve NER.
We extracted a category label fromthe first sentence of a Wikipedia article and used itas a feature of a CRF-based NE tagger.
The experi-ments using the CoNLL 2003 NER dataset demon-strated that category labels extracted by such a sim-ple method really improved accuracy.
However, dis-ambiguation techniques will become more impor-tant as Wikipedia grows or if we aim at more fine-grained NER.
We thus would like to incorporate adisambiguation technique into our method in futurework.
Exploiting Wikipedia structures such as dis-ambiguation pages and link structures will be thekey in that case as well.ReferencesS.
J. Benson and J. J.
More?.
2001.
A limited mem-ory variable metric method for bound constraint min-imization.
Technical Report ANL/MCS-P909-0901,Argonne National Laboratory.D.
M. Bikel, R. L. Schwartz, and R. M. Weischedel.1999.
An algorithm that learns what?s in a name.
Ma-chine Learning, 34(1-3):211?231.706R.
Bunescu and M. Pas?ca.
2006.
Using encyclopedicknowledge for named entity disambiguation.
In EACL2006.H.
Daume?
III and D. Marcu.
2005.
Learning as searchoptimization: Approximate large margin methods forstructured prediction.
In ICML 2005.O.
Etzioni, M. Cafarella, D. Downey, A. M. Popescu,T.
Shaked, S. Soderland, D. S. Weld, and A. Yates.2005.
Unsupervised named-entity extraction from theweb ?
an experimental study.
Artificial IntelligenceJournal.J.
Kazama and K. Torisawa.
2007.
A new perceptron al-gorithm for sequence labeling with non-local features.In EMNLP-CoNLL 2007.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In ICML 2001,pages 282?289.D.
Nadeau, Peter D. Turney, and Stan Matwin.
2006.Unsupervised named-entity recognition: Generatinggazetteers and resolving ambiguity.
In 19th CanadianConference on Artificial Intelligence.S.
P. Ponzetto and M. Strube.
2006.
Exploiting semanticrole lebeling, WordNet and Wikipedia for coreferenceresolution.
In NAACL 2006.L.
R. Rabiner.
1989.
A tutorial on hidden Markov mod-els and selected applications in speech recognition.Proceedings of the IEEE, 77(2):257?286.E.
Riloff and R. Jones.
1999.
Learning dictionaries forinformation extraction by multi-level bootstrapping.In 16th National Conference on Artificial Intelligence(AAAI-99).M.
Ruiz-Casado, E. Alfonseca, and P. Castells.
2006.From Wikipedia to semantic relationships: a semi-automated annotation approach.
In Third EuropeanSemantic Web Conference (ESWC 2006).S.
Sekine, K. Sudo, and C. Nobata.
2002.
Extendednamed entity hierarchy.
In LREC ?02.K.
Shinzato, S. Sekine, N. Yoshinaga, and K. Tori-sawa.
2006.
Constructing dictionaries for named en-tity recognition on specific domains from the Web.
InWeb Content Mining with Human Language Technolo-gies Workshop on the 5th International Semantic Web.M.
Strube and S. P. Ponzetto.
2006.
WikiRelate!
com-puting semantic relatedness using Wikipedia.
In AAAI2006.P.
P. Talukdar, T. Brants, M. Liberman, and F. Pereira.2006.
A context pattern induction method for namedentity extraction.
In CoNLL 2006.M.
Thelen and E. Riloff.
2002.
A bootstrapping methodfor learning semantic lexicons using extraction patterncontext.
In EMNLP 2002.E.
F. Tjong, K. Sang, and F. De Meulder.
2003.
Intro-duction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In CoNLL2003.A.
Toral and R. Mun?oz.
2006.
A proposal to automat-ically build and maintain gazetteers for named entityrecognition by using Wikipedia.
In EACL 2006.T.
Zesch, I. Gurevych, and M. Mo?hlha?user.
2007.
Ana-lyzing and accessing Wikipedia as a lexical semanticresource.
In Biannual Conference of the Society forComputational Linguistics and Language Technology.707
