Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1103?1113, Dublin, Ireland, August 23-29 2014.A Dependency Edge-based Transfer Model for Statistical MachineTranslationHongshen Chen??
Jun Xie?Fandong Meng??
Wenbin Jiang?Qun Liu??
?Key Laboratory of Intelligent Information ProcessingInstitute of Computing Technology, Chinese Academy of Sciences?University of Chinese Academy of Sciences{chenhongshen,xiejun,mengfandong,jiangwenbin}@ict.ac.cn?CNGL, School of Computing, Dublin City Universityqliu@computing.dcu.ieAbstractPrevious models in syntax-based statistical machine translation usually resort to some kindsof synchronous procedures, few of these works are based on the analysis-transfer-generationmethodology.
In this paper, we present a statistical implementation of the analysis-transfer-generation methodology in rule-based translation.
The procedures of syntax analysis, syntaxtransfer and language generation are modeled independently in order to break the synchronousconstraint, resorting to dependency structures with dependency edges as atomic manipulatingunits.
Large-scale experiments on Chinese to English translation show that our model exhibitsstate-of-the-art performance by significantly outperforming the phrase-based model.
The statis-tical transfer-generation method results in significantly better performance with much smallermodels.1 IntroductionResearches in statistical machine translation have been flourishing in recent years.
Statistical translationmethods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002;Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004;Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004;Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013).
Compared with word-based andphrase-based methods, syntax-based models perform better in long distance reordering and enjoy highergeneralization capability by leveraging the hierarchical structures in natural languages, and achieve thestate-of-the-art performance in these years.Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generationprocedures which directly model the structural correspondence between two languages.
In contrast,the analysis-transfer-generation methodology in rule-based translation solves the machine translationproblem in a more divided scheme, where the processing procedures of analysis, structural transfer andlanguage generation are modeled separately.
The analysis-transfer-generation strategy can tolerate highernon-isomorphism between languages if with a more general transformation unit and it can facilitateelaborating engineering of each processing procedure, however, there isn?t a statistical transfer modelthat shows the comparable performance with the current state-of-the-art SMT model so far.In this paper, we propose a novel statistical analysis-transfer-generation model for machine transla-tion, to integrate the advantages of the transfer-generation scheme and the statistical modeling.
Theprocedures of transfer and generation are modeled on dependency structures with dependency edgesas atomic manipulating units.
First, the source sentence is parsed by a dependency parser.
Then, thesource dependency structure is transferred into a target structure by translation rules, which composedof the source and target edges.
Last, the target sentence is finally generated from the target edges whichare used as intermediate syntactic structures.
By directly modeling the edge, the most basic unit in thedependency tree, which definitely describe the modifying relationship and positional relation betweenwords, our model alleviates the non-isomorphic problem and shows the flexibility of reordering.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1103E?ob?m?j?nti?nji?n?f?b?
?nqu?nzh?nlu?
sh?n?m?n?f?b??ob?m?
ji?n?
*:sh?n?m?n?nsubj advmoddobjf?b?f?b?
sh?n?m?n?
?nqu?nnnobamaissuewillissueissue*sh?n?m?n?zh?nlu?nna statement ofsecuritya statement ofstrategyf?b?/VV??ji?n?/AD?sh?n?m?n?/NN???nqu?n/NN??zh?nlu?/NN?
?nsubjadvmoddobjnn nn?ob?m?/NN??
?obama today will issue a statement of security strategyD ?
?
??
?j?nti?n/NT??tmodj?nti?ntomodf?b?todayissue??????????????
?Figure 1: (a)An example of labeled Chinese dependency tree aligned with the corresponding Englishsentence.
(b) Examples of the transfer rules extracted from the tree.
?*?
denotes a variable.
All the innernodes are treated as variables.
The label on the target side of a rule denotes whether the head and thedependent are adjacent or not.The rest of the paper is organized as follows, we first describe the dependency edge-based transfermodel (Section 2).
Then, we present our rule acquisition algorithm (Section 3), the decoding and targetsentence generation process (Section 4).
Finally, large-scale experiments (Section 5) on Chinese-to-English translation show that our edge-based transfer model gains state-of-the-art performance by sig-nificantly outperforming the phrase-based model (Koehn et al., 2003) by averaged +1.34 BLEU points onthree test sets.
To the best of our knowledge, this is the first transfer-generation-based statistical machinetranslation model that achieves the state-of-the-art performance.2 Dependency Edge-based Transfer Model2.1 Edges in Dependency TreesGiven a sentence, its dependency tree is a directed acyclic graph with words in the sentence as nodes.An example dependency tree is shown in Figure 1 (a).
An edge in the tree represents a dependencyrelationship between a pair of words, a head and a dependent.
When a nominal dependent acts as asubject and modifies a verbal head, they usually have a fixed relative position.
In Figure 1 (a), ?`aob?am?a?modifies ?f?ab`u?.
The grammatical relation label nsubj (Chang et al., 2009) between them denotes that anoun phrase acts as the subject of a clause.
?`aob?am?a?
is on the left of ?f?ab`u?.Based on the above observations, we take the edge as the elementary structure of a dependency treeand regard a dependency tree to be a set of edges.Definition 1.
An source side edge is a 4-tuple e = ?H,D,P,R?, where H is the head, D is the depen-dent, P denotes the relative position between H and D, left or right, R is the grammatical relation label.In Figure 1 (b), the upper sides of transfer rules are source side edges extracted from the dependencytree.1104f?b?/VVji?n?/AD sh?n?m?n?/NNnsubjadvmoddobj?ob?m?/NN j?nti?n/NTtmodTransferselect the leftadjacent edge?
?
?
?extend to the leftwilladjacentobamaissuenon-adjacenttodaynon-adjacent?
?
?willadjacentobamaissuenon-adjacenttodaynon-adjacent??
?H1:obama today will issueH2:today obama will issueextend to the right*adjacent?
?
?
?a statement of security strategyadjacentissue?willwill issuewilladjacentobamaissuenon-adjacenttodaynon-adjacent?
?
?willadjacentobamaissuenon-adjacenttodaynon-adjacent??
?H1:obama today will issue a statement of security strategyH2:today obama will issue a statement of security strategy?adjacent?*GenerationAnalysis?ob?m?
j?nti?n ji?n?
f?b?
?nqu?n zh?nlu?
sh?n?m?n?f?b??ob?m?
ji?n?nsubj advmodf?b?obamaissuewillnon-adjacentissueadjacentj?nti?ntomodf?b?todayissuenon-adjacent*:sh?n?m?n?dobjf?b?issue*adjacentFigure 2: An example partial generation of translation.
The same set of rules generate two target hy-potheses with the same words and different word order.
Assume the sub-tree rooted at ?sh?engm??ng?
hasbeen translated to the corresponding target sentence fragment.2.2 Transfer RulesA transfer rule of our model represents the reordering and relative positions of edges between languagepairs.
For example, in Figure 1 (b), the first rule shows that when a nominal subject modifies a verb, thetarget side keeps the same position relations.
?obama?
is also on the left of ?issue?, the same with thesource side relative position.
The 5-th and 6-th rules show the inversion relations between the source andthe target.
Formally, a transfer rule can be defined as a triple ?e, f,?
?, where e is an edge extracted fromthe source dependency tree, f is a target edge.
?
denotes one-to-one correspondence between variablesin e and f .Figure 1 (b) are part of transfer rules extracted from the word aligned sentence in Figure 1 (a).
Thetarget edge denotes whether the target dependent is on the left or the right side of the target head, the1105label on the edge indicates whether the target head and the target dependent are adjacent or not.
Ifthe dependent is an internal node(contrast with the leaf nodes in the dependency tree), then it will beregarded as a substitution node.
The dependent in the 4-th transfer rule is an internal node and the itscorresponding target side is a substitution variable.Figure 2 shows a partial transfer-generation of our model which involves three phases.
First, analysis.Given a source language sentence, we obtain its dependency tree using a dependency parser.
We assumethat the sub-tree of the substitution node has been translated.
Second, transfer.
For each internal node,we transfer the source side edges between the head and all its dependents into the target sides.
In thesecond block of Figure 2, we transfer four edges into the target sides.
Third, generation, correspondingto the third block of Figure 2.
We generate the target sentence with the target side edges starting from thetarget head, ?issue?.
We first try to concatenate the edges to the left.
First, we select a target side edgethat is on the left side of ?issue?
and adjacent to it to form a consecutive phrase.
Edge 3 is selected and ?toissue?
is generated.
Then, we enumerate all possible left concatenations of the other edges that are notadjacent to ?issue?.
The two sequences(1,2,3 and 2,1,3) of the edges are generated, corresponding to thetwo hypotheses.
After that, we extend the two hypotheses to the right.
The internal node ?sh?engm??ng?
isa substitution node, so the candidate translation of the sub-tree rooted at ?sh?engm??ng?
is concatenated tothe two hypotheses.
Finally, we generate the two candidate translations of the input sentence.3 Acquisition of Transfer RulesTransfer rules can be extracted automatically from a word-aligned corpus, which is a set of triples?T, S,A?, where T is a source dependency tree, S is a target side sentence and A is an alignment relationbetween T and S. Following the dependency-to-string model (Xie et al., 2011), we extract transfer rulesfrom each triple ?T, S,A?
by three steps:1.
Tree Annotation: Label each node in the dependency tree with the alignment information2.
Edges Identification: Identify acceptable edges from the annotated dependency tree3.
Rule induction: Induce a set of lexicalized and un-lexicalized transfer rules from the acceptableedges.3.1 Tree AnnotationGiven a triple ?T, S,A?
as Figure 3 shows, we define two attributes for every node in T: node span andsub-tree span:Definition 2.
Given a node n, its node span nsp(n) is a set of consecutive indexes of the target wordsaligned with the node n.For example, nsp(?anqu?an)={7-8}, which corresponds to the target word ?of?
and ?security?.Definition 3.
A node span nsp(n) is consistent if for any other node n?in the dependency tree, nsp(n)and nsp(n?)
are not overlapping.For example, nsp(zh`anlu`e) is consistent, while nsp(?anqu?an) is not consistent for it corresponds to thesame word ?of?
with nsp(sh?engm?
?ng).Definition 4.
Given a sub-tree T?rooted at n, the sub-tree span tsp(n) of n is a consecutive target wordindexes from the lower bound of the nsp of all the nodes in T?to the upper bound of those spans.For example, tsp(sh?engm?
?ng)={5-9},which corresponds to the target phrase ?a statement of securitystrategy?.Definition 5.
A sub-tree span tsp(n) is consistent if for any other node n?that is not in the sub-treerooted at n in the dependency tree, tsp(n) and nsp(n?)
are not overlapping.For example, tsp(sh?engm?
?ng) is consistent, even though nsp(sh?engm?
?ng) is not consistent, whiletsp(?anqu?an) is not consistent for ?sh?engm??ng?
is not a node in sub-tree rooted at ??anqu?an?
and ?
?anqu?an?corresponds to the same word ?of ?
with nsp(sh?engm?
?ng) .1106f?b?/VVji?n?/AD sh?n?m?n?/NN?nqu?n/NN zh?nlu?/NNnsubj advmod dobjnn nn?ob?m?/NNobama today will issue a statement of security strategyj?nti?n/NTtmod1 2 3 4 5 6 7 8 9{1-1}/{1-1} {2-2}/{2-2} {3-3}/{3-3}{4-4}/{1-9}{5-7}/{5-9}{7-8}/{7-8} {9-9}/{9-9}Figure 3: An example of annotated dependency tree.
Each node is annotated with two spans, the formeris node span and the latter is sub-tree span.
The gray edge is not acceptable.
It is different from Figure1, because ??anqu?an?
aligned with two words in Figure 3.
?of?
in the target side is aligned with both??anqu?an?
and ?sh?engm??ng?
which makes the gray edge un-acceptable.3.2 Acceptable Edges IdentificationWe identify the edges from the annotated dependency tree that are acceptable for rule induction.For an acceptable edge, its node span of the head nsp(head) and the sub-tree span of the dependenttsp(dependent) satisfy the following properties:1. nsp(head) and tsp(dependent) are consistent.2.
nsp(head) and tsp(dependent) are non-overlapping.For example, tsp(?anqu?an) and nsp(sh?engm?
?ng) are neither consistent nor non-overlapping.
So thegray edge between head ?sh?engm??ng?
and dependent ??anqu?an?
is not an acceptable edge.
nsp(f?ab`u)and tsp(sh?engm?
?ng) are consistent and the two spans are non-overlapping.
Thus, the edge between head?f?ab`u?
and dependent ?sh?engm??ng?
is an acceptable edge.3.3 Transfer Rule InductionFrom each acceptable source side edge, we induce a set of lexicalized and un-lexicalized transfer rules.We induce a lexicalized transfer rule from an acceptable edge by the following procedures:1. extract the source side edge and mark the internal nodes as substitution sites.
This form the input ofa transfer rule.2.
extract the position information according to nsp(head) and tsp(dependent), whether they are adja-cent or not and whether tsp(dependent) is on the left side or the right side of nsp(head).In Figure 4, the first transfer rule is lexicalized rule, it is induced from the edge between ?f?ab`u?
and?`aob?am?a?.In addition to the lexicalized rules described above, we also generalized the rules by replacing theword in an source side edge with a wild card and the part of speech of the word.
For example, the rulein Figure 4 can be generalized in two ways.
The generalized versions of the rule apply to ?`aob?am?a?modifying any verb and ?f?ab`u?
modifying any noun, respectively.
The generalized rules are also called1107Generalize head?ob?m?nsubjf?b?obamaissuenon-adjacent?ob?m?nsubj*:VVobama*non-adjacentGeneralizedependent*:NNnsubjf?b?*issuenon-adjacent??
?Figure 4: Generalization of transfer rule.un-lexicalized rules for the loss of word information.
The single node translations of the generalizedwords are also extracted.The unaligned words of the target side is handled by extending nsp(head) and tsp(dependent) on bothleft and right directions.
We do this process similar with the method of Och and Ney (2004).
We mightobtain m(m ?
1) extended rules from an acceptable edge.
The frequency of each rule is divided by m.We take the extracted rule set as observed data and make use of relative frequency estimator to obtainthe translation probabilities P (t|s) and P (s|t).4 Decoding and GenerationWe follow Och and Ney (2002), using a general log-linear model to score the sentence generated by eachconcatenation of the target edges.
Let c be concatenations that concatenate the target edges to generatethe target sentence e. The probability of e is defined as?P (c) ?
?i?i(c)?i(1)where ?i(c) are features defined on concatenations and ?iare feature weights.
In our experiments ofthis paper, thirteen features are used as follows:?
Transfer rules translation probabilities P (t|s) and P (s|t), and lexical translation probabilitiesPlex(t|s) and Plex(s|t);?
Bilingual phrases probabilities Pbp(t|s) and Pbp(s|t), and bilingual phrases lexical translation prob-abilities Pbplex(t|s) and Pbplex(s|t);?
Transfer rule penalty exp(?1);?
Bilingual phrase penalty exp(?1);?
Pseudo translation rule penalty exp(?1);?
Target word penalty exp(|e|);?
Language model Plm(e).Our decoder is based on a bottom-up chart-based beam-search algorithm.
We regard the decodingprocess as the composition of the target side edges.
For a given source language sentence, we obtain its1108f?b?j?nti?n?ob?m?obama today to issuef?b?sh?n?m?n?
?nqu?n zh?nlu?issue a statement of security strategyji?n?Figure 5: Two examples of the phrases incorporated in our model.dependency tree T with an external dependency parser.
Each node in T is traversed in post-order.
Foreach internal node and root node n, we do the transfer-generation translation as the following procedures:1.
Extract all the source side edges including the lexicalized and generalized edges between n and allits dependents using the same way we extract the source side edges of the transfer rules.2.
Transfer the source side edges into target side edges.
For a generalized rule, we restore it to a lex-icalized rule by combining it with the single word translation.
For no matched edges, we constructthe pseudo translation rule according to the word order of the source head-dependent relation.3.
Generate the target sentence by bi-directional extension from an adjacent target edge.
We firstgroup all the target edges by their heads.
For each group, we generate translation hypotheses withthe following procedures:(a) Select an adjacent target edge as the starting position;(b) Extend to the left side and enumerate all possible permutations of the target edges directingleft;(c) Extend to the right side and enumerate all possible permutations of the target edges directingright.Considering that in dependency trees, a head may relate to more than 4 edges which results inmassive search space.
We reduce the time complexity by using the maximum distortion limit.
Thedistortion is defined as (ai?
bi?1?
1), where aidenotes the start position of the source side edgethat is translated into the ith target side edge and bi?1denotes the end position of the source sideedge translated into the (i ?
1)th target side edge.When we reach the root node, the candidate translations of the input sentence are generated.In our model, only the adjacent target edge of a transfer rule can be regarded as a consecutive phraseand its corresponding source side length is only 2.
As we start extending the target sentence fromthe target head, it is quite natural to incorporate the bilingual phrases to make the target sentences beextended from the phrases as well as the single target head word.
Due to the flexibility of our model,we can incorporate not only the syntactic phrases which are phrases covering a whole sub-tree, but alsothe non-syntactic phrases as the fixed dependency structures in Shen et al.
(2008) which are consecutivephrases covering the head.
Figure 5 shows two examples of the phrases incorporated in our model.We prune the search space in several ways.
First, beam threshold ?, items with a score worse than ?times of the best score in the same span will be discarded; second, beam size b, items with a score worsethan the bth best item will be discarded.
For our experiments, we set ?
= 10?3and b = 300; Third,we also prune rules for the same edge with a fixed rule limit (r = 200), which denotes the maximumnumber of rules we keep.5 ExperimentsIn this section, the performance of our model is evaluated by comparing with phrase-based model (Koehnet al., 2003), on the NIST Chinese-to-English translation tasks.
We also present the influence of the1109mt02-tune mt03 mt04 mt050 30.81 30.03 32.44 30.091 33.4 32.07 34.55 31.772 34.39 32.7 35.4 32.593 34.04 32.69 35.46 32.544 33.59 31.75 35.15 32.17303132333435360 1 2 3 4maximum distortion limitBLEU(%)mt02-tunemt03mt04mt05Figure 6: Effect of different maximum distortion limits on developmentset (mt02) and three tests(mt03,04,05).
The performance of all the setsare consistent.maximum distortion limit to our model.
We take open source phrase-based system Moses (with defaultconfiguration)1as our baseline system.5.1 Experimental SettingOur training corpus consists of 1.25M sentence pairs from LDC data, including LDC2002E18, LD-C2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.To obtain the dependency trees of the source side, we parse the source sentences with Stanford Parser(Klein and Manning, 2003) into projective dependency structures with nodes annotated by POS tags andedges by dependency labels.To obtain the word alignments, we run GIZA++ (Och and Ney, 2003) on the corpus in both directionsand apply ?grow-diag-and?
refinement (Koehn et al., 2003).
We extract the phrases covering no morethan 10 nodes of the fixed structures.We use SRILM (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smooth-ing on the Xinhua portion of the Gigaword corpus.We use NISTMTEvaluation test set 2002 as our development set, 2003-2005 NIST datasets as testsets.The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric2.We make use of the minimum error rate training algorithm (Och, 2003) in order to maximize theBLEU score of the development set.The statistical significance test is performed by sign-test (Collins et al., 2005).5.2 Influence of Maximum Distortion LimitFigure 6 gives the performance of our system with different maximum distortion limits in terms ofuncased BLEU of three NIST test sets.
The performance of different distortion limit are consistent onboth development set and three test sets.
Maximum distortion limit 2 gets the best performances.
A lowdistortion limit may cause the target sentence been translated more close to the sequence of the source,especially when the distortion limit equals to 0, none of the reordering is allowed, while a high distortionlimit may lead the good translations be flooded by too many ambiguities when enumerating the possiblesequences of the target non-adjacent dependents.
We choose 2 as the maximum distortion limit in thenext experiments.1http://www.statmt.org/moses/2ftp://jaguar.ncsl.nist.glv/mt/resources/mteval-v11b.pl.1110System Rule # MT03 MT04 MT05 AverageMoses 44.49M 32.03 32.83 31.81 32.22DEBT 30.7M 32.7* 35.4* 32.59* 33.56Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets.?DEBT?
denotes our edge-based transfer model.
The ?*?
denotes that the results are significantly betterthan the baseline system (p<0.01).5.3 Performance of Our ModelTabel 1 illustrates the translation results of our experiments.
We (DEBT) surpass the baseline over +1.34BLEU points on average.
Our model significant outperforms the baseline phrase-based model, withp < 0.01 on statistical significance test sign-test (Collins et al., 2005).We also list the statistical number of rules extracted from the training corpus.
The number of ourtransfer rules is only 69.0% of the rules extracted by Moses, thus, the total rules in our model is 31%smaller than Moses.6 Related WorkTransfer-based MT systems usually take a parse tree in the source language and translate it into a parsetree in the target language with transfer rules.
Both our model and some of those previous works ac-quired transfer rules automatically from word-aligned corpus (Richardson et al., 2001; Carbonell et al.,2002; Lavoie et al., 2002; Lin, 2004).
Gimpel and Smith (2009) and Gimpel and Smith (2014) usedquasi-synchronous dependency grammar for MT and they are similar to our idea of doing transfer ofdependency syntax in a non-synchronous setting.
They do the translation as monolingual lattice parsing.As dependency-based system, Lin (2004) used path as the transfer unit and regarded the translationproblem with minimal path covering.
Quirk et al.
(2005) and Xiong et al.
(2007) used treelets to modelthe source dependency tree using synchronous grammars.
Quirk et al.
(2005) projected the source depen-dency structure into target side by word alignment and faced the problem of non-isomorphism betweenlanguages.
Xiong et al.
(2007) directly modeled the treelet to the corresponding target string to alleviatethe problem.
Xie et al.
(2011) directly specified the ordering information in head-dependents rules thatrepresent the source side as head-dependents relations and the target side as string.Differently, our model uses a much simpler elementary structure, edge, which consist of only a headand a dependent.
As a transfer-generation model, we transfer an edge in the source dependency tree intotarget side and incorporate the position information on the target edge , which alleviate non-isomorphismproblem and incorporate ordering among different target edges simultaneously.
Moreover, our decodingmethod is quite different from previous dependency tree-based works.
After parsing a given sourcelanguage sentence, we transfer and generate the target sentence fragments recursively on each internalnode of the dependency tree bottom-up.7 Conclusions and Future WorkIn this paper, we present a novel dependency edge-based transfer model using dependency trees on thesource side for machine translation.
We directly transfer the edges in source dependency tree into thetarget sides and then generate the target sentences by beam-search.
With the concise transfer rules,our model is compatible with both the syntactic and non-syntactic phrases.
Although the generationprocess of our model seems relatively simple, it still exhibits a good performance and outperforms thephrase-based model on large scale experiments.
For the first time, a statistical transfer model shows acomparable performance with the state-of-the-art translation models.Since the translation procedure is divided into three phases and each phase can be modeled indepen-dently, we would like to take further steps focusing on modeling the target language generation processspecifically to ensure a better grammatical translation with the help of natural language generation meth-ods.1111AcknowledgmentsThe authors were supported by National Key Technology R&D Program (No.
2012BAH39B03), CASAction Plan for the Development of Western China (No.
KGZD-EW-501), and Sino-Thai Scientific andTechnical Cooperation (No.
60-625J).
Sincere thanks to the anonymous reviewers for their thoroughreviewing and valuable suggestions.ReferencesPeter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer.
1993.
The mathematics ofstatistical machine translation: Parameter estimation.
Computational linguistics, 19(2):263?311.Jaime Carbonell, Katharina Probst, Erik Peterson, Christian Monson, Alon Lavie, Ralf Brown, and Lori Levin.2002.
Automatic rule learning for resource-limited mt.
In Machine Translation: From Research to Real Users,pages 1?10.
Springer.Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D Manning.
2009.
Discriminative reorderingwith chinese grammatical relations features.
In Proceedings of the Third Workshop on Syntax and Structure inStatistical Translation, pages 51?59.
Association for Computational Linguistics.David Chiang.
2005.
A hierarchical phrase-based model for statistical machine translation.
In Proceedingsof the 43rd Annual Meeting on Association for Computational Linguistics, pages 263?270.
Association forComputational Linguistics.Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005.
Clause restructuring for statistical machine transla-tion.
In Proceedings of the 43rd annual meeting on association for computational linguistics, pages 531?540.Association for Computational Linguistics.Yuan Ding and Martha Palmer.
2004.
Synchronous dependency insertion grammars: A grammar formalism forsyntax based statistical mt.
In Workshop on Recent Advances in Dependency Grammars (COLING), pages90?97.Kevin Gimpel and Noah A Smith.
2009.
Feature-rich translation by quasi-synchronous lattice parsing.
In Pro-ceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1,pages 219?228.
Association for Computational Linguistics.Kevin Gimpel and Noah A Smith.
2014.
Phrase dependency machine translation with quasi-synchronous tree-to-tree features.
Computational Linguistics.Jonathan Graehl and Kevin Knight.
2004.
Training tree transducers.
In Daniel Marcu Susan Dumais and SalimRoukos, editors, HLT-NAACL 2004: Main Proceedings, pages 105?112, Boston, Massachusetts, USA, May 2 -May 7.
Association for Computational Linguistics.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.
Statistical syntax-directed translation with extended domainof locality.
In Proceedings of AMTA, pages 66?73.Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.
Statistical phrase-based translation.
In Proceedingsof the 2003 Conference of the North American Chapter of the Association for Computational Linguistics onHuman Language Technology - Volume 1, NAACL ?03, pages 48?54, Stroudsburg, PA, USA.
Association forComputational Linguistics.Benoit Lavoie, Michael White, and Tanya Korelsky.
2002.
Learning domain-specific transfer rules: an experimentwith korean to english translation.
In Proceedings of the 2002 COLING workshop on Machine translation inAsia-Volume 16, pages 1?7.
Association for Computational Linguistics.Dekang Lin.
2004.
A path-based transfer model for machine translation.
In Proceedings of Coling 2004, pages625?630, Geneva, Switzerland, Aug 23?Aug 27.
COLING.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine translation.In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meetingof the Association for Computational Linguistics, pages 609?616.
Association for Computational Linguistics.Daniel Marcu and William Wong.
2002.
A phrase-based, joint probability model for statistical machine transla-tion.
In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume10, pages 133?139.
Association for Computational Linguistics.1112Fandong Meng, Jun Xie, Linfeng Song, Yajuan L?u, and Qun Liu.
2013.
Translation with source constituencyand dependency trees.
In Proceedings of the 2013 Conference on Empirical Methods in Natural LanguageProcessing, pages 1066?1076, Seattle, Washington, USA, October.
Association for Computational Linguistics.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL-08: HLT, pages192?199, Columbus, Ohio, June.
Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2002.
Discriminative training and maximum entropy models for statisticalmachine translation.
In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,ACL ?02, pages 295?302, Stroudsburg, PA, USA.
Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A systematic comparison of various statistical alignment models.Computational linguistics, 29(1):19?51.Franz Josef Och and Hermann Ney.
2004.
The alignment template approach to statistical machine translation.Computational linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate training in statistical machine translation.
In Proceedings of the41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160?167.
Association forComputational Linguistics.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
Dependency treelet translation: Syntactically informedphrasal SMT.
In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics(ACL?05), pages 271?279, Ann Arbor, Michigan, June.
Association for Computational Linguistics.Stephen Richardson, William Dolan, Arul Menezes, and Jessie Pinkham.
2001.
Achieving commercial-qualitytranslation with example-based methods.
In Proceedings of MT Summit VIII, pages 293?298.
Santiago DeCompostela, Spain.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
A new string-to-dependency machine translation algorithmwith a target dependency language model.
In Proceedings of ACL-08: HLT, pages 577?585, Columbus, Ohio,June.
Association for Computational Linguistics.Andreas Stolcke.
2002.
Srilm-an extensible language modeling toolkit.
In Proceedings of ICSLP, volume 30,pages 901?904.Jun Xie, Haitao Mi, and Qun Liu.
2011.
A novel dependency-to-string model for statistical machine translation.In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages216?226, Stroudsburg, PA, USA.
Association for Computational Linguistics.Deyi Xiong, Qun Liu, and Shouxun Lin.
2007.
A dependency treelet string correspondence model for statisticalmachine translation.
In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07,pages 40?47, Stroudsburg, PA, USA.
Association for Computational Linguistics.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedings of the 39thAnnual Meeting on Association for Computational Linguistics, pages 523?530.
Association for ComputationalLinguistics.1113
