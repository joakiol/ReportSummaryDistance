Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 145?149,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsThe University of Maryland Statistical Machine Translation System forthe Fourth Workshop on Machine TranslationChris Dyer?
?, Hendra Setiawan?, Yuval Marton?
?, and Philip Resnik??
?UMIACS Laboratory for Computational Linguistics and Information Processing?Department of LinguisticsUniversity of Maryland, College Park, MD 20742, USA{redpony,hendra,ymarton,resnik} AT umd.eduAbstractThis paper describes the techniques weexplored to improve the translation ofnews text in the German-English andHungarian-English tracks of the WMT09shared translation task.
Beginning with aconvention hierarchical phrase-based sys-tem, we found benefits for using word seg-mentation lattices as input, explicit gen-eration of beginning and end of sentencemarkers, minimum Bayes risk decoding,and incorporation of a feature scoring thealignment of function words in the hy-pothesized translation.
We also exploredthe use of monolingual paraphrases to im-prove coverage, as well as co-training toimprove the quality of the segmentationlattices used, but these did not lead to im-provements.1 IntroductionFor the shared translation task of the Fourth Work-shop on Machine Translation (WMT09), we fo-cused on two tasks: German to English and Hun-garian to English translation.
Despite belonging todifferent language families, German and Hungar-ian have three features in common that complicatetranslation into English:1. productive compounding (especially ofnouns),2. rich inflectional morphology,3.
widespread mid- to long-range word orderdifferences with respect to English.Since these phenomena are poorly addressed withconventional approaches to statistical machinetranslation, we chose to work primarily towardmitigating their negative effects when construct-ing our systems.
This paper is structured as fol-lows.
In Section 2 we describe the baseline model,Section 3 describes the various strategies we em-ployed to address the challenges just listed, andSection 4 summarizes the final translation system.2 Baseline systemOur translation system makes use of a hierarchicalphrase-based translation model (Chiang, 2007),which we argue is a strong baseline for theselanguage pairs.
First, such a system makes useof lexical information when modeling reorder-ing (Lopez, 2008), which has previously beenshown to be useful in German-to-English trans-lation (Koehn et al, 2008).
Additionally, sincethe decoder is based on a CKY parser, it can con-sider all licensed reorderings of the input in poly-nomial time, and German and Hungarian may re-quire quite substantial reordering.
Although suchdecoders and models have been common for sev-eral years, there have been no published results forthese language pairs.The baseline system translates lowercased andtokenized source sentences into lowercased targetsentences.
The features used were the rule transla-tion relative frequency P (e?|f?
), the ?lexical?
trans-lation probabilities Plex(e?|f?)
and Plex(f?
|e?
), a rulecount, a target language word count, the target(English) language model P (eI1), and a ?pass-through?
penalty for passing a source languageword to the target side.1 The rule feature valueswere computed online during decoding using thesuffix array method described by Lopez (2007).1The ?pass-through?
penalty was necessary since the En-glish language modeling data contained a large amount ofsource-language text.1452.1 Training and development dataTo construct the translation suffix arrays used tocompute the translation grammar, we used the par-allel training data provided.
The preprocessedtraining data was filtered for length and alignedusing the GIZA++ implementation of IBM Model4 (Och and Ney, 2003) in both directions and sym-metrized using the grow-diag-final-andheuristic.
We trained a 5-gram language modelfrom the provided English monolingual trainingdata and the non-Europarl portions of the paralleltraining data using modified Kneser-Ney smooth-ing as implemented in the SRI language modelingtoolkit (Kneser and Ney, 1995; Stolcke, 2002).
Wedivided the 2008 workshop ?news test?
sets intotwo halves of approximately 1000 sentences eachand designated one the dev set and the other thedev-test set.2.2 Automatic evaluation metricSince the official evaluation criterion for WMT09is human sentence ranking, we chose to minimizea linear combination of two common evaluationmetrics, BLEU and TER (Papineni et al, 2002;Snover et al, 2006), during system developmentand tuning:TER ?
BLEU2Although we are not aware of any work demon-strating that this combination of metrics correlatesbetter than either individually in sentence ranking,Yaser Al-Onaizan (personal communication) re-ports that it correlates well with the human evalua-tion metric HTER.
In this paper, we report uncasedTER and BLEU individually.2.3 Forest minimum error trainingTo tune the feature weights of our system, we useda variant of the minimum error training algorithm(Och, 2003) that computes the error statistics fromthe target sentences from the translation searchspace (represented by a packed forest) that are ex-actly those that are minimally discriminable bychanging the feature weights along a single vectorin the dimensions of the feature space (Machereyet al, 2008).
The loss function we used was thelinear combination of TER and BLEU described inthe previous section.3 Experimental variationsThis section describes the experimental variantsexplored.3.1 Word segmentation latticesBoth German and Hungarian have a large numberof compound words that are created by concate-nating several morphemes to form a single ortho-graphic token.
To deal with productive compound-ing, we employ word segmentation lattices, whichare word lattices that encode alternative possiblesegmentations of compound words.
Doing so en-ables us to use possibly inaccurate approaches toguess the segmentation of compound words, al-lowing the decoder to decide which to use duringtranslation.
This is a further development of ourgeneral source-lattice approach to decoding (Dyeret al, 2008).To construct the segmentation lattices, we de-fine a log-linear model of compound word seg-mentation inspired by Koehn and Knight (2003),making use of features including number of mor-phemes hypothesized, frequency of the segmentsas free-standing morphemes in a training corpus,and letters in each segment.
To tune the modelparameters, we selected a set of compound wordsfrom a subset of the German development set,manually created a linguistically plausible seg-mentation of these words, and used this to selectthe parameters of the log-linear model using a lat-tice minimum error training algorithm to minimizeWER (Macherey et al, 2008).
We reused the samefeatures and weights to create the Hungarian lat-tices.
For the test data, we created a lattice of ev-ery possible segmentation of any word 6 charac-ters or longer and used forward-backward pruningto prune out low-probability segmentation paths(Sixtus and Ortmanns, 1999).
We then concate-nated the lattices in each sentence.Source Condition BLEU TERGermanbaseline 20.8 60.7lattice 21.3 59.9Hungarianbaseline 11.0 71.1lattice 12.3 70.4Table 1: Impact of compound segmentation lat-tices.To build the translation model for lattice sys-tem, we segmented the training data using the one-best split predicted by the segmentation model,146and word aligned this with the English side.
Thisvariant version of the training data was then con-catenated with the baseline system?s training data.3.1.1 Co-training of segmentation modelTo avoid the necessity of manually creating seg-mentation examples to train the segmentationmodel, we attempted to generate sets of trainingexamples by selecting the compound splits thatwere found along the path chosen by the decoder?sone-best translation.
Unfortunately, the segmen-tation system generated in this way performedslightly worse than the one-best baseline and sowe continued to use the parameter settings derivedfrom the manual segmentation.3.2 Modeling sentence boundariesIncorporating an n-gram language model proba-bility into a CKY-based decoder is challenging.When a partial hypothesis (also called an ?item?
)has been completed, it has not yet been determinedwhat strings will eventually occur to the left ofits first word, meaning that the exact computationmust deferred, which makes pruning a challenge.In typical CKY decoders, the beginning and endsof the sentence (which often have special charac-teristics) are not conclusively determined until thewhole sentence has been translated and the proba-bilities for the beginning and end sentence proba-bilities can be added.
However, by this point it isoften the case that a possibly better sentence be-ginning has been pruned away.
To address this,we explicitly generate beginning and end sentencemarkers as part of the translation process, as sug-gested by Xiong et al (2008).
The results of doingthis are shown in Table 2.Source Condition BLEU TERGermanbaseline 21.3 59.9+boundary 21.6 60.1Hungarianbaseline 12.3 70.4+boundary 12.8 70.4Table 2: Impact of modeling sentence boundaries.3.3 Source language paraphrasesIn order to deal with the sparsity associated witha rich source language morphology and limited-size parallel corpora (bitexts), we experimentedwith a novel approach to paraphrasing out-of-vocabulary (OOV) source language phrases inour Hungarian-English system, using monolingualcontextual similarity rather than phrase-table piv-oting (Callison-Burch et al, 2006) or monolin-gual bitexts (Barzilay and McKeown, 2001; Dolanet al, 2004).
Distributional profiles for sourcephrases were represented as context vectors overa sliding window of size 6, with vectors definedusing log-likelihood ratios (cf.
Rapp (1999), Dun-ning (1993)) but using cosine rather than city-block distance to measure profile similarity.The 20 distributionally most similar sourcephrases were treated as paraphrases, consideringcandidate phrases up to a width of 6 tokens and fil-tering out paraphrase candidates with cosine simi-larity to the original of less than 0.6.
The two mostlikely translations for each paraphrase were addedto the grammar in order to provide mappings toEnglish for OOV Hungarian phrases.This attempt at monolingually-derived source-side paraphrasing did not yield improvements overbaseline.
Preliminary analysis suggests that theapproach does well at identifying many contentwords in translating extracted paraphrases of OOVphrases (e.g., a kommunista part vezetaje ?
,leader of the communist party or a ra tervezett?until the planned to), but at the cost of more fre-quently omitting target words in the output.3.4 Dominance featureAlthough our baseline hierarchical system permitslong-range reordering, it lacks a mechanism toidentify the most appropriate reordering for a spe-cific sentence translation.
For example, when themost appropriate reordering is a long-range one,our baseline system often also has to considershorter-range reorderings as well.
In the worstcase, a shorter-range reordering has a high proba-bility, causing the wrong reordering to be chosen.Our baseline system lacks the capacity to addresssuch cases because all the features it employs areindependent of the phrases being moved; these aremodeled only as an unlexicalized generic nonter-minal symbol.To address this challenge, we included what wecall a dominance feature in the scoring of hypothe-sis translations.
Briefly, the premise of this featureis that the function words in the sentence hold thekey reordering information, and therefore functionwords are used to model the phrases being moved.The feature assesses the quality of a reordering bylooking at the phrase alignment between pairs of147function words.
In our experiments, we treatedthe 128 most frequent words in the corpus as func-tion words, similar to Setiawan et al (2007).
Dueto space constraints, we will discuss the details inanother publication.
As Table 3 reports, the use ofthis feature yields positive results.Source Condition BLEU TERGermanbaseline 21.6 60.1+dom 22.2 59.8Hungarianbaseline 12.8 70.4+dom 12.6 70.0Table 3: Impact of alignment dominance feature.3.5 Minimum Bayes risk decodingAlthough during minimum error training we as-sume a decoder that uses the maximum derivationdecision rule, we find benefits to translating usinga minimum risk decision rule on a test set (Kumarand Byrne, 2004).
This seeks the translation E ofthe input lattice F that has the least expected loss,measured by some loss function L:E?
= arg minE?EP (E|F)[L(E,E?)]
(1)= arg minE?
?EP (E|F)L(E,E?)
(2)We approximate the posterior distributionP (E|F) and the set of possible candidate transla-tions using the unique 500-best translations of asource lattice F .
If H(E,F) is the decoder?s pathweight, this is:P (E|F) ?
exp?H(E,F)The optimal value for the free parameter ?mustbe experimentally determined and depends on theranges of the feature functions and weights used inthe model, as well as the amount and kind of prun-ing using during decoding.2 For our submission,we used ?
= 1.
Since our goal is to minimizeTER?BLEU2 we used this as the loss function in (2).Table 4 shows the results on the dev-test set forMBR decoding.2If the free parameter ?
lies in (1,?)
the distribution issharpened, if it lies in [0, 1), the distribution is flattened.Source Decoder BLEU TERGermanMax-D 22.2 59.8MBR 22.6 59.4HungarianMax-D 12.6 70.0MBR 12.8 69.8Table 4: Performance of maximum derivation vs.MBR decoders.4 ConclusionTable 5 summarizes the impact on the dev-test setof all features included in the University of Mary-land system submission.ConditionGerman HungarianBLEU TER BLEU TERbaseline 20.8 60.7 11.0 71.1+lattices 21.3 59.9 12.3 70.4+boundary 21.6 60.1 12.8 70.4+dom 22.2 59.8 12.6 70.0+MBR 22.6 59.4 12.8 69.8Table 5: Summary of all featuresAcknowledgmentsThis research was supported in part by theGALE program of the Defense Advanced Re-search Projects Agency, Contract No.
HR0011-06-2-001, and the Army Research Laboratory.Any opinions, findings, conclusions or recommen-dations expressed in this paper are those of the au-thors and do not necessarily reflect the view of thesponsors.
Discussions with Chris Callison-Burchwere helpful in carrying out the monolingual para-phrase work.ReferencesRegina Barzilay and Kathleen McKeown.
2001.
Ex-tracting paraphrases from a parallel corpus.
In InProceedings of ACL-2001.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine trans-lation using paraphrases.
In Proceedings NAACL-2006.D.
Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.B.
Dolan, C. Quirk, and C. Brockett.
2004.
Unsu-pervised construction of large paraphrase corpora:148exploiting massively parallel news sources.
In Pro-ceedings of the 20th International Conference onComputational Linguistics of the Association forComputational Linguistics, Geneva, Switzerland.T.
Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Lin-guistics, 19(1):61?74.Chris Dyer, Smaranda Muresan, and Philip Resnik.2008.
Generalizing word lattice translation.
In Pro-ceedings of ACL-08: HLT.
Association for Compu-tational Linguistics, June.R.
Kneser and H. Ney.
1995.
Improved backing-offfor m-gram language modeling.
In Proceedings ofIEEE Internation Conference on Acoustics, Speech,and Signal Processing, pages 181?184.P.
Koehn and K. Knight.
2003.
Empirical methodsfor compound splitting.
In Proceedings of the EACL2003.Philipp Koehn, Abhishek Arun, and Hieu Hoang.2008.
Towards better machine translation quality forthe German-English language pairs.
In ACL Work-shop on Statistical Machine Translation.S.
Kumar and W. Byrne.
2004.
Minimum Bayes-riskdecoding for statistical machine translation.
In Pro-cessings of HLT-NAACL.Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 976?985.Adam Lopez.
2008.
Tera-scale translation modelsvia pattern matching.
In Proceedings of COLING,Manchester, UK.Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,and Jakob Uszkoreit.
2008.
Lattice-based minimumerror rate training for statistical machine translation.In Proceedings of EMNLP, Honolulu, HI.F.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.F.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proceedings of the 41st An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 160?167, Sapporo, Japan,July.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In Proceedings of the 40th AnnualMeeting of the ACL, pages 311?318.Reinhard Rapp.
1999.
Automatic identification ofword translations from unrelated English and Ger-man corpora.
In Proceedings of the 37th AnnualConference of the Association for ComputationalLinguistics., pages 519?525.Hendra Setiawan, Min-Yen Kan, and Haizhao Li.2007.
Ordering phrases with function words.
InProceedings of ACL.S.
Sixtus and S. Ortmanns.
1999.
High quality wordgraphs using forward-backward pruning.
In Pro-ceedings of ICASSP, Phoenix, AZ.Matthew Snover, Bonnie J. Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
2006.
Astudy of translation edit rate with targeted human an-notation.
In Proceedings of Association for MachineTranslation in the Americas.A.
Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Intl.
Conf.
on Spoken LanguageProcessing.Deyi Xiong, Min Zhang, Ai Ti Aw, Haitao Mi, QunLiu, and Shouxun Lin.
2008.
Refinements in BTG-based statistical machine translation.
In Proceed-ings of IJCNLP 2008.149
