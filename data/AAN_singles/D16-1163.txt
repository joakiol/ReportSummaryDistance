Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1568?1575,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsTransfer Learning for Low-Resource Neural Machine TranslationBarret Zoph* Deniz Yuret Jonathan May and Kevin KnightInformation Sciences Institute Computer Engineering Information Sciences InstituteUniversity of Southern California Koc?
University Computer Science Departmentbarretzoph@gmail.com dyuret@ku.edu.tr University of Southern California{jonmay,knight}@isi.eduAbstractThe encoder-decoder framework for neuralmachine translation (NMT) has been showneffective in large data scenarios, but is muchless effective for low-resource languages.
Wepresent a transfer learning method that signifi-cantly improves BLEU scores across a rangeof low-resource languages.
Our key idea isto first train a high-resource language pair(the parent model), then transfer some of thelearned parameters to the low-resource pair(the child model) to initialize and constraintraining.
Using our transfer learning methodwe improve baseline NMT models by an av-erage of 5.6 BLEU on four low-resource lan-guage pairs.
Ensembling and unknown wordreplacement add another 2 BLEU which bringsthe NMT performance on low-resource ma-chine translation close to a strong syntax basedmachine translation (SBMT) system, exceed-ing its performance on one language pair.
Ad-ditionally, using the transfer learning modelfor re-scoring, we can improve the SBMT sys-tem by an average of 1.3 BLEU, improvingthe state-of-the-art on low-resource machinetranslation.1 IntroductionNeural machine translation (NMT) (Sutskever et al,2014) is a promising paradigm for extracting trans-lation knowledge from parallel text.
NMT sys-tems have achieved competitive accuracy rates un-der large-data training conditions for language pairsThis work was carried out while all authors were at USC?sInformation Sciences Institute.
*This author is currently at Google Brain.Language Train Test SBMT NMTSize Size BLEU BLEUHausa 1.0m 11.3K 23.7 16.8Turkish 1.4m 11.6K 20.4 11.4Uzbek 1.8m 11.5K 17.9 10.7Urdu 0.2m 11.4K 17.9 5.2Table 1: NMT models with attention are outperformed by stan-dard string-to-tree statistical MT (SBMT) when translating low-resource languages into English.
Train/test bitext corpus sizesare English token counts.
Single-reference, case-insensitiveBLEU scores are given for held-out test corpora.such as English?French.
However, neural methodsare data-hungry and learn poorly from low-countevents.
This behavior makes vanilla NMT a poorchoice for low-resource languages, where paralleldata is scarce.
Table 1 shows that for 4 low-resourcelanguages, a standard string-to-tree statistical MTsystem (SBMT) (Galley et al, 2004; Galley et al,2006) strongly outperforms NMT, even when NMTuses the state-of-the-art local attention plus feed-input techniques from Luong et al (2015a).In this paper, we describe a method for substan-tially improving NMT results on these languages.Our key idea is to first train a high-resource lan-guage pair, then use the resulting trained network(the parent model) to initialize and constrain trainingfor our low-resource language pair (the child model).We find that we can optimize our results by fixingcertain parameters of the parent model and lettingthe rest be fine-tuned by the child model.
We re-port NMT improvements from transfer learning of5.6 BLEU on average, and we provide an analysisof why the method works.
The final NMT system1568approaches strong SBMT baselines in all four lan-guage pairs, and exceeds SBMT performance in oneof them.
Furthermore, we show that NMT is an ex-ceptional re-scorer of ?traditional?
MT output; evenNMT that on its own is worse than SBMT is con-sistently able to improve upon SBMT system outputwhen incorporated as a re-scoring model.We provide a brief description of our NMT modelin Section 2.
Section 3 gives some background ontransfer learning and explains how we use it to im-prove machine translation performance.
Our mainexperiments translating Hausa, Turkish, Uzbek, andUrdu into English with the help of a French?Englishparent model are presented in Section 4.
Section 5explores alternatives to our model to enhance under-standing.
We find that the choice of parent languagepair affects performance, and provide an empiricalupper bound on transfer performance using an arti-ficial language.
We experiment with English-onlylanguage models, copy models, and word-sortingmodels to show that what we transfer goes beyondmonolingual information and that using a transla-tion model trained on bilingual corpora as a parentis essential.
We show the effects of freezing, fine-tuning, and smarter initialization of different com-ponents of the attention-based NMT system duringtransfer.
We compare the learning curves of transferand no-transfer models, showing that transfer solvesan overfitting problem, not a search problem.
Wesummarize our contributions in Section 6.2 NMT BackgroundIn the neural encoder-decoder framework for MT(Neco and Forcada, 1997; Castan?o and Casacu-berta, 1997; Sutskever et al, 2014; Bahdanau etal., 2014; Luong et al, 2015a), we use a recurrentneural network (encoder) to convert a source sen-tence into a dense, fixed-length vector.
We then useanother recurrent network (decoder) to convert thatvector to a target sentence.
In this paper, we usea two-layer encoder-decoder system (Figure 1) withlong short-term memory (LSTM) units (Hochreiterand Schmidhuber, 1997).
The models were trainedto optimize maximum likelihood (via a softmaxlayer) with back-propagation through time (Werbos,1990).
Additionally, we use an attention mecha-nism that allows the target decoder to look back atFigure 1: The encoder-decoder framework for neural machinetranslation (NMT) (Sutskever et al, 2014).
Here, a source sen-tence C B A (presented in reverse order as A B C) is trans-lated into a target sentence W X Y Z.
At each step, an evolvingreal-valued vector summarizes the state of the encoder (blue,checkerboard) and decoder (red, lattice).
Not shown here arethe attention connections present in our model used by the de-coder to access encoder states.the source encoder, specifically the local attentionmodel from Luong et al (2015a).
In our model wealso use the feed-input input connection from Luonget al (2015a) where at each timestep on the decoderwe feed in the top layer?s hidden state into the lowestlayer of the next timestep.3 Transfer LearningTransfer learning uses knowledge from a learnedtask to improve the performance on a related task,typically reducing the amount of required trainingdata (Torrey and Shavlik, 2009; Pan and Yang,2010).
In natural language processing, transferlearning methods have been successfully applied tospeech recognition, document classification and sen-timent analysis (Wang and Zheng, 2015).
Deeplearning models discover multiple levels of repre-sentation, some of which may be useful across tasks,which makes them particularly suited to transferlearning (Bengio, 2012).
For example, Cires?an etal.
(2012) use a convolutional neural network to rec-ognize handwritten characters and show positive ef-fects of transfer between models for Latin and Chi-nese characters.
Ours is the first study to apply trans-fer learning to neural machine translation.There has also been work on using data frommultiple language pairs in NMT to improve perfor-mance.
Recently, Dong et al (2015) showed thatsharing a source encoder for one language helpsperformance when using different target decoders1569Decoder Hausa Turkish Uzbek UrduNMT 16.8 11.4 10.7 5.2Xfer 21.3 17.0 14.4 13.8Final 24.0 18.7 16.8 14.5SBMT 23.7 20.4 17.9 17.9Table 2: Our method significantly improves NMT results forthe translation of low-resource languages into English.
Resultsshow test-set BLEU scores.
The ?NMT?
row shows results with-out transfer, and the ?Xfer?
row shows results with transfer.
The?Final?
row shows BLEU after we ensemble 8 models and useunknown word replacement.for different languages.
In that paper the authorsshowed that using this framework improves perfor-mance for low-resource languages by incorporatinga mix of low-resource and high-resource languages.Firat et al (2016) used a similar approach, employ-ing a separate encoder for each source language,a separate decoder for each target language, anda shared attention mechanism across all languages.They then trained these components jointly acrossmultiple different language pairs to show improve-ments in a lower-resource setting.There are a few key differences between our workand theirs.
One is that we are working with trulysmall amounts of training data.
Dong et al (2015)used a training corpus of about 8m English words forthe low-resource experiments, and Firat et al (2016)used from 2m to 4m words, while we have at most1.8m words, and as few as 0.2m.
Additionally, theaforementioned previous work used the same do-main for both low-resource and high-resource lan-guages, while in our case the datasets come fromvastly different domains, which makes the taskmuch harder and more realistic.
Our approachonly requires using one additional high-resourcelanguage, while the other papers used many.
Ourapproach also allows for easy training of new low-resource languages, while Dong et al (2015) and Fi-rat et al (2016) do not specify how a new languageshould be added to their pipeline once the models aretrained.
Finally, Dong et al (2015) observe an aver-age BLEU gain on their low-resource experiments of+1.16, and Firat et al (2016) obtain BLEU gains of+1.8, while we see a +5.6 BLEU gain.The transfer learning approach we use is simpleand effective.
We first train an NMT model on aRe-scorer SBMT DecoderHausa Turkish Uzbek UrduNone 23.7 20.4 17.9 17.9NMT 24.5 21.4 19.5 18.2Xfer 24.8 21.8 19.5 19.1LM 23.6 21.1 17.9 18.2Table 3: Our transfer method applied to re-scoring output n-best lists from the SBMT system.
The first row shows theSBMT performance with no re-scoring and the other 3 rowsshow the performance after re-scoring with the selected model.Note: the ?LM?
row shows the results when an RNN LM trainedon the large English corpus was used to re-score.large corpus of parallel data (e.g., French?English).We call this the parent model.
Next, we initialize anNMT model with the already-trained parent model.This new model is then trained on a very small par-allel corpus (e.g., Uzbek?English).
We call this thechild model.
Rather than starting from a random po-sition, the child model is initialized with the weightsfrom the parent model.A justification for this approach is that in scenar-ios where we have limited training data, we need astrong prior distribution over models.
The parentmodel trained on a large amount of bilingual datacan be considered an anchor point, the peak of ourprior distribution in model space.
When we train thechild model initialized with the parent model, we fixparameters likely to be useful across tasks so thatthey will not be changed during child model train-ing.
In the French?English to Uzbek?English ex-ample, as a result of the initialization, the Englishword embeddings from the parent model are copied,but the Uzbek words are initially mapped to randomFrench embeddings.
The parameters of the Englishembeddings are then frozen, while the Uzbek em-beddings?
parameters are allowed to be modified,i.e.
fine-tuned, during training of the child model.Freezing certain transferred parameters and fine tun-ing others can be considered a hard approximation toa tight prior or strong regularization applied to someof the parameter space.
We also experiment withordinary L2 regularization, but find it does not sig-nificantly improve over the parameter freezing de-scribed above.Our method results in large BLEU increases fora variety of low resource languages.
In one of the1570Language Pair Role Train Dev TestSize Size SizeSpanish?English child 2.5m 58k 59kFrench?English parent 53m 58k 59kGerman?English parent 53m 58k 59kTable 4: Data used for a low-resource Spanish?English task.Sizes are English-side token counts.four language pairs our NMT system using trans-fer beats a strong SBMT baseline.
Not only dothese transfer models do well on their own, they alsogive large gains when used for re-scoring n-best lists(n = 1000) from the SBMT system.
Section 4 de-tails these results.4 ExperimentsTo evaluate how well our transfer method works weapply it to a variety of low-resource languages, bothstand-alone and for re-scoring a strong SBMT base-line.
We report large BLEU increases across theboard with our transfer method.For all of our experiments with low-resource lan-guages we use French as the parent source languageand for child source languages we use Hausa, Turk-ish, Uzbek, and Urdu.
The target language is al-ways English.
Table 1 shows parallel training dataset sizes for the child languages, where the languagewith the most data has only 1.8m English tokens.For comparison, our parent French?English modeluses a training set with 300 million English tokensand achieves 26 BLEU on the development set.
Ta-ble 1 also shows the SBMT system scores along withthe NMT baselines that do not use transfer.
There isa large gap between the SBMT and NMT systemswhen our transfer method is not used.The SBMT system used in this paper is a string-to-tree statistical machine translation system (Gal-ley et al, 2006; Galley et al, 2004).
In this systemthere are two count-based 5-gram language mod-els.
One is trained on the English side of theWMT 2015 English?French dataset and the other istrained on the English side of the low-resource bi-text.
Additionally, the SBMT models use thousandsof sparsely-occurring, lexicalized syntactic features(Chiang et al, 2009).For our NMT system, we use development sets forHausa, Turkish, Uzbek, and Urdu to tune the learn-Parent BLEU ?
PPL ?none 16.4 15.9French?English 31.0 5.8German?English 29.8 6.2Table 5: For a low-resource Spanish?English task, we exper-iment with several choices of parent model: none, French?English, and German?English.
We hypothesize that French?English is best because French and Spanish are similar.ing rate, parameter initialization range, dropout rate,and hidden state size for all the experiments.
Fortraining we use a minibatch size of 128, hidden statesize of 1000, a target vocabulary size of 15K, anda source vocabulary size of 30K.
The child modelsare trained with a dropout probability of 0.5, as inZaremba et al (2014).
The common parent modelis trained with a dropout probability of 0.2.
Thelearning rate used for both child and parent mod-els is 0.5 with a decay rate of 0.9 when the de-velopment perplexity does not improve.
The childmodels are all trained for 100 epochs.
We re-scalethe gradient when the gradient norm of all param-eters is greater than 5.
The initial parameter rangeis [-0.08, +0.08].
We also initialize our forget-gatebiases to 1 as specified by Jo?zefowicz et al (2015)and Gers et al (2000).
For decoding we use a beamsearch of width 12.4.1 Transfer ResultsThe results for our transfer learning method appliedto the four languages above are in Table 2.
The par-ent models were trained on the WMT 2015 (Bojaret al, 2015) French?English corpus for 5 epochs.Our baseline NMT systems (?NMT?
row) all receivea large BLEU improvement when using the transfermethod (the ?Xfer?
row) with an average BLEU im-provement of 5.6.
Additionally, when we use un-known word replacement from Luong et al (2015b)and ensemble together 8 models (the ?Final?
row)we further improve upon our BLEU scores, bringingthe average BLEU improvement to 7.5.
Overall ourmethod allows the NMT system to reach competi-tive scores and outperform the SBMT system in oneof the four language pairs.1571Figure 2: Our NMT model architecture, showing six blocks of parameters, in addition to source/target words and predictions.During transfer learning, we expect the source-language related blocks to change more than the target-language related blocks.Language Pair Parent Train Size BLEU ?
PPL ?Uzbek?English None 1.8m 10.7 22.4French?English 1.8m 15.0 (+4.3) 13.9French?
?English None 1.8m 13.3 28.2French?English 1.8m 20.0 (+6.7) 10.9Table 6: A better match between parent and child languages should improve transfer results.
We devised a child language calledFrench?, identical to French except for word spellings.
We observe that French transfer learning helps French?
(13.3?20.0) morethan it helps Uzbek (10.7?15.0).4.2 Re-scoring ResultsWe also use the NMT model with transfer learn-ing as a feature when re-scoring output n-best lists(n = 1000) from the SBMT system.
Table 3 showsthe results of re-scoring.
We compare re-scoringwith transfer NMT to re-scoring with baseline (i.e.non-transfer) NMT and to re-scoring with a neurallanguage model.
The neural language model is anLSTM RNN with 2 layers and 1000 hidden states.
Ithas a target vocabulary of 100K and is trained usingnoise-contrastive estimation (Mnih and Teh, 2012;Vaswani et al, 2013; Baltescu and Blunsom, 2015;Williams et al, 2015).
Additionally, it is trained us-ing dropout with a dropout probability of 0.2 as sug-gested by Zaremba et al (2014).
Re-scoring with thetransfer NMT model yields an improvement of 1.1?1.6 BLEU points above the strong SBMT system; wefind that transfer NMT is a better re-scoring featurethan baseline NMT or neural language models.In the next section, we describe a number of ad-ditional experiments designed to help us understandthe contribution of the various components of ourtransfer model.5 AnalysisWe analyze the effects of using different parent mod-els, regularizing different parts of the child model,and trying different regularization techniques.5.1 Different Parent LanguagesIn the above experiments we use French?English asthe parent language pair.
Here, we experiment withdifferent parent languages.
In this set of experimentswe use Spanish?English as the child language pair.A description of the data used in this section is pre-sented in Table 4.Our experimental results are shown in Table 5,where we use French and German as parent lan-guages.
If we just train a model with no transfer ona small Spanish?English training set we get a BLEUscore of 16.4.
When using our transfer method weget Spanish?English BLEU scores of 31.0 and 29.8via French and German parent languages, respec-tively.
As expected, French is a better parent thanGerman for Spanish, which could be the result ofthe parent language being more similar to the childlanguage.
We suspect using closely-related parentlanguage pairs would improve overall quality.1572????????????????????
???
???
???
???
????????????????????????????????????????????????????????????????????????????????????????
?Figure 3: Uzbek?English learning curves for the NMT atten-tion model with and without transfer learning.
The training per-plexity converges to a similar value in both cases.
However, thedevelopment perplexity for the transfer model is significantlybetter.5.2 Effects of having Similar Parent LanguageNext, we look at a best-case scenario in which theparent language is as similar as possible to the childlanguage.Here we devise a synthetic child language (calledFrench?)
which is exactly like French, except its vo-cabulary is shuffled randomly.
(e.g., ?internationale?is now ?pomme,?
etc).
This language, which looksunintelligible to human eyes, nevertheless has thesame distributional and relational properties as ac-tual French, i.e.
the word that, prior to vocabu-lary reassignment, was ?roi?
(king) is likely to sharedistributional characteristics, and hence embeddingsimilarity, to the word that, prior to reassignment,was ?reine?
(queen).
French should be the ideal par-ent model for French?.The results of this experiment are shown in Ta-ble 6.
We get a 4.3 BLEU improvement with anunrelated parent (i.e.
French?parent and Uzbek?child), but we get a 6.7 BLEU improvement witha ?closely related?
parent (i.e.
French?parent andFrench??child).
We conclude that the choice of par-ent model can have a strong impact on transfer mod-els, and choosing better parents for our low-resourcelanguages (if data for such parents can be obtained)could improve the final results.5.3 Ablation AnalysisIn all the above experiments, only the target inputand output embeddings are fixed during training.
Inthis section we analyze what happens when different?????????????????
???
???
???
???
????????????????????????????????????????????????????????????????????????
?Figure 4: Uzbek?English learning curves for the transfermodel with and without dictionary-based assignment of Uzbekword types to French word embeddings (from the parentmodel).
Dictionary-based assignment enables faster improve-ment in early epochs.
The model variants converge, showingthat the unaided model is able to untangle the initial randomUzbek/French word-type mapping without help.parts of the model are fixed, in order to determine thescenario that yields optimal performance.
Figure 2shows a diagram of the components of a sequence-to-sequence model.
Table 7 shows the effects of al-lowing various components of the child NMT modelto be trained.
We find that the optimal setting fortransferring from French?English to Uzbek?Englishin terms of BLEU performance is to allow all of thecomponents of the child model to be trained exceptfor the input and output target embeddings.Even though we use this setting for our mainexperiments, the optimum setting is likely to belanguage- and corpus-dependent.
For Turkish, ex-periments show that freezing attention parameters aswell gives slightly better results.
For parent-childmodels with closely related languages we expectfreezing, or strongly regularizing, more componentsof the model to give better results.5.4 Learning CurveIn Figure 3 we plot learning curves for both a trans-fer and a non-transfer model on training and devel-opment sets.
We see that the final training set per-plexities for both the transfer and non-transfer modelare very similar, but the development set perplexityfor the transfer model is much better.The fact that the two models start from and con-verge to very different points, yet have similar train-ing set performances, indicates that our architecture1573Source Source Target Attention Target Input Target Output Dev DevEmbeddings RNN RNN Embeddings Embeddings BLEU ?
PPL ??
?
?
?
?
?
0.0 112.61 ?
?
?
?
?
7.7 24.71 1 ?
?
?
?
11.8 17.01 1 1 ?
?
?
14.2 14.51 1 1 1 ?
?
15.0 13.91 1 1 1 1 ?
14.7 13.81 1 1 1 1 1 13.7 14.4Table 7: Starting with the parent French?English model (BLEU =24.4, PPL=6.2), we randomly assign Uzbek word types to Frenchword embeddings, freeze various parameters of the neural network model (?
), and allow Uzbek?English (child model) trainingto modify other parts (1).
The table shows how Uzbek?English BLEU and perplexity vary as we allow more parameters to bere-trained.and training algorithm are able to reach a good min-imum of the training objective regardless of the ini-tialization.
However, the training objective seemsto have a large basin of models with similar perfor-mance and not all of them generalize well to the de-velopment set.
The transfer model, starting with andstaying close to a point known to perform well on arelated task, is guided to a final point in the weightspace that generalizes to the development set muchbetter.5.5 Dictionary InitializationUsing the transfer method, we always initializeinput language embeddings for the child modelwith randomly-assigned embeddings from the par-ent (which has a different input language).
A smartermethod might be to initialize child embeddings withsimilar parent embeddings, where similarity is mea-sured by word-to-word t-table probabilities.
To getthese probabilities we compose Uzbek?English andEnglish?French t-tables obtained from the Berke-ley Aligner (Liang et al, 2006).
We see from Fig-ure 4 that this dictionary-based assignment resultsin faster improvement in the early part of the train-ing.
However the final performance is similar to ourstandard model, indicating that the training is ableto untangle the dictionary permutation introduced byrandomly-assigned embeddings.5.6 Different Parent ModelsIn the above experiments, we use a parent modeltrained on a large French?English corpus.
Onemight hypothesize that our gains come from exploit-Transfer Model BLEU ?
PPL ?None 10.7 22.4French?English Parent 14.4 14.3English?English Parent 5.3 55.8EngPerm?English Parent 10.8 20.4LM Xfer 12.9 16.3Table 8: Transfer for Uzbek?English NMT with parent modelstrained only on English data.
The English?English parent learnsto copy English sentences, and the EngPerm?English learns toun-permute scrambled English sentences.
The LM is a 2-layerLSTM RNN language model.ing the English half of the corpus as an additionallanguage model resource.
Therefore, we exploretransfer learning for the child model with parentmodels that only use the English side of the French?English corpus.
We consider the following parentmodels in our ablative transfer learning scenarios:?
A true translation model (French?English Par-ent)?
A word-for-word English copying model(English?English Parent)?
A model that unpermutes scrambled English(EngPerm?English Parent)?
(The parameters of) an RNN language model(LM Xfer)The results, in Table 8, show that transfer learningdoes not simply import an English language model,but makes use of translation parameters learnedfrom the parent?s large bilingual text.15746 ConclusionOverall, our transfer method improves NMT scoreson low-resource languages by a large margin and al-lows our transfer NMT system to come close to theperformance of a very strong SBMT system, evenexceeding its performance on Hausa?English.
Inaddition, we consistently and significantly improvestate-of-the-art SBMT systems on low-resource lan-guages when the transfer NMT system is used for re-scoring.
Our experiments suggest that there is stillroom for improvement in selecting parent languagesthat are more similar to child languages, provideddata for such parents can be found.AcknowledgmentsThis work was supported by ARL/ARO (W911NF-10-1-0533), DARPA (HR0011-15-C-0115), and theScientific and Technological Research Council ofTurkey (TU?BI?TAK) (grants 114E628 and 215E201).ReferencesD.
Bahdanau, K. Cho, and Y. Bengio.
2014.
Neural ma-chine translation by jointly learning to align and trans-late.
In Proc.
ICLR.P.
Baltescu and P. Blunsom.
2015.
Pragmatic neural lan-guage modelling in machine translation.
In Proc.
HLT-NAACL.Y.
Bengio.
2012.
Deep learning of representations forunsupervised and transfer learning.
JMLR, 27.O.
Bojar, R. Chatterjee, C. Federmann, B. Haddow,M.
Huck, C. Hokamp, P. Koehn, V. Logacheva,C.
Monz, M. Negri, M. Post, C. Scarton, L. Specia,and M. Turchi.
2015.
Findings of the 2015 workshopon statistical machine translation.
In Proc.
WMT.M.
A. Castan?o and F. Casacuberta.
1997.
A connec-tionist approach to machine translation.
In Proc.
Eu-rospeech.D.
Chiang, K. Knight, and W. Wang.
2009.
11,001 newfeatures for statistical machine translation.
In Proc.HLT-NAACL.D.
C. Cires?an, U. Meier, and J. Schmidhuber.
2012.Transfer learning for Latin and Chinese characterswith deep neural networks.
In Proc.
IJCNN.D.
Dong, H. Wu, W. He, D. Yu, and H. Wang.
2015.Multi-task learning for multiple language translation.In Proc.
ACL-IJCNLP.O.
Firat, K. Cho, and Y. Bengio.
2016.
Multi-way, mul-tilingual neural machine translation with a shared at-tention mechanism.
In Proc.
NAACL-HLT.M.
Galley, M. Hopkins, K. Knight, and D. Marcu.
2004.What?s in a translation rule?
In Proc.
HLT-NAACL.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,W.
Wang, and I. Thayer.
2006.
Scalable inference andtraining of context-rich syntactic translation models.In Proc.
ACL-COLING.F.
A. Gers, J. Schmidhuber, and F. Cummins.
2000.Learning to forget: Continual prediction with LSTM.Neural computation, 12(10).S.
Hochreiter and J. Schmidhuber.
1997.
Long short-term memory.
Neural Computation, 9(8).R.
Jo?zefowicz, W. Zaremba, and I. Sutskever.
2015.
Anempirical exploration of recurrent network architec-tures.
In Proc.
ICML.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In Proc.
HLT-NAACL.M.
Luong, H. Pham, and C. Manning.
2015a.
Effectiveapproaches to attention-based neural machine transla-tion.
In Proc.
EMNLP.T.
Luong, I. Sutskever, Q.
Le, O. Vinyals, andW.
Zaremba.
2015b.
Addressing the rare word prob-lem in neural machine translation.
In Proc.
ACL.A.
Mnih and Y. W. Teh.
2012.
A fast and simple algo-rithm for training neural probabilistic language mod-els.
In Proc.
ICML.R.
Neco and M. Forcada.
1997.
Asynchronous transla-tions with recurrent neural nets.
In Proc.
InternationalConference on Neural Networks.S.
J. Pan and Q. Yang.
2010.
A survey on transfer learn-ing.
IEEE Transactions on Knowledge and Data En-gineering, 22(10).I.
Sutskever, O. Vinyals, and Q. V. Le.
2014.
Sequenceto sequence learning with neural networks.
In Proc.NIPS.L.
Torrey and J. Shavlik.
2009.
Transfer learning.
InE.
Soria, J. Martin, R. Magdalena, M. Martinez, andA.
Serrano, editors, Handbook of Research on Ma-chine Learning Applications and Trends: Algorithms,Methods, and Techniques.
IGI Global.A.
Vaswani, Y. Zhao, V. Fossum, and D. Chiang.
2013.Decoding with large-scale neural language models im-proves translation.
In Proc.
EMNLP.D.
Wang and T. Fang Zheng.
2015.
Transfer learn-ing for speech and language processing.
CoRR,abs/1511.06066.P.
J. Werbos.
1990.
Backpropagation through time: whatit does and how to do it.
Proc.
IEEE, 78(10).W.
Williams, N. Prasad, D. Mrva, T. Ash, and T. Robin-son.
2015.
Scaling recurrent neural network languagemodels.
In Proc.
ICASSP.W.
Zaremba, I. Sutskever, and O. Vinyals.
2014.Recurrent neural network regularization.
CoRR,abs/1409.2329.1575
