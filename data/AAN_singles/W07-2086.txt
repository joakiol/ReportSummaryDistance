Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 390?393,Prague, June 2007. c?2007 Association for Computational LinguisticsUMND1: Unsupervised Word Sense Disambiguation Using ContextualSemantic RelatednessSiddharth PatwardhanSchool of ComputingUniversity of UtahSalt Lake City, UT 84112.sidd@cs.utah.eduSatanjeev BanerjeeLanguage Technologies Inst.Carnegie Mellon UniversityPittsburgh, PA 15217.banerjee@cs.cmu.eduTed PedersenDept.
of Computer ScienceUniversity of MinnesotaDuluth, MN 55812.tpederse@d.umn.eduAbstractIn this paper we describe an unsuper-vised WordNet-based Word Sense Disam-biguation system, which participated (asUMND1) in the SemEval-2007 Coarse-grained English Lexical Sample task.
Thesystem disambiguates a target word by usingWordNet-based measures of semantic relat-edness to find the sense of the word thatis semantically most strongly related to thesenses of the words in the context of the tar-get word.
We briefly describe this system,the configuration options used for the task,and present some analysis of the results.1 IntroductionWordNet::SenseRelate::TargetWord1 (Patwardhanet al, 2005; Patwardhan et al, 2003) is an unsuper-vised Word Sense Disambiguation (WSD) system,which is based on the hypothesis that the intendedsense of an ambiguous word is related to thewords in its context.
For example, if the ?financialinstitution?
sense of bank is intended in a context,then it is highly likely the context would containrelated words such as money, transaction, interestrate, etc.
The algorithm, therefore, determinesthe intended sense of a word (target word) in agiven context by measuring the relatedness of eachsense of that word with the words in its context.The sense of the target word that is most relatedto its context is selected as the intended sense ofthe target word.
The system uses WordNet-based1http://senserelate.sourceforge.netmeasures of semantic relatedness2 (Pedersen etal., 2004) to measure the relatedness between thedifferent senses of the target word and the words inits context.This system is completely unsupervised and re-quires no annotated data for training.
The lexicaldatabase WordNet (Fellbaum, 1998) is the only re-source that the system uses to measure the related-ness between words and concepts.
Thus, our systemis classified under the closed track of the task.2 System DescriptionOur WSD system consists of a modular framework,which allows different algorithms for the differentsubtasks to be plugged into the system.
We dividethe disambiguation task into two primary subtasks:context selection and sense selection.
The contextselection module tries to select words from the con-text that are most likely to be indicative of the senseof the target word.
The sense selection module thenuses the set of selected context words to choose oneof the senses of the target word as the answer.Figure 1 shows a block schematic of the system,which takes SemEval-2007 English Lexical Sampleinstances as input.
Each instance is a made up ofa few English sentences, and one word from thesesentences is marked as the target word to be dis-ambiguated.
The system processes each instancethrough multiple modules arranged in a sequentialpipeline.
The final output of the pipeline is the sensethat is most appropriate for the target word in thegiven context.2http://wn-similarity.sourceforge.net390InstancePreprocessingFormat FilterTarget SenseContext SelectionPostprocessingSense Selection Relatedness MeasureFigure 1: System Architecture2.1 Data PreparationThe input text is first passed through a format fil-ter, whose task is to parse the input XML file.
Thisis followed by a preprocessing step.
Each instancepassed to the preprocessing stage is first segmentedinto words, and then all compound words are iden-tified.
Any sequence of words known to be a com-pound in WordNet is combined into a single entity.2.2 Context SelectionAlthough each input instance consists of a largenumber of words, only a few of these are likely tobe useful for disambiguating the target word.
Weuse the context selection algorithm to select a subsetof the context words to be used for sense selection.By removing the unimportant words, the computa-tional complexity of the algorithm is reduced.In this work, we use the NearestWords contextselection algorithm.
This algorithm algorithm se-lects 2n + 1 content words surrounding the targetword (including the target word) as the context.
Astop list is used to identify closed-class non-contentwords.
Additionally, any word not found in Word-Net is also discarded.
The algorithm then selects ncontent words before and n content words follow-ing the target word, and passes this unordered set of2n + 1 words to the Sense Selection module.2.3 Sense Selection AlgorithmThe sense selection module takes the set of wordsoutput by the context selection module, one of whichis the target word to be disambiguated.
For each ofthe words in this set, it retrieves a list of senses fromWordNet, based on which it determines the intendedsense of the target word.The package provides two main algorithms forSense Selection: the local and the global algorithms,as described in previous work (Banerjee and Peder-sen, 2002; Patwardhan et al, 2003).
In this work,we use the local algorithm, which is faster and wasshown to perform as well as the global algorithm.The local sense selection algorithm measures thesemantic relatedness of each sense of the target wordwith the senses of the words in the context, and se-lects that sense of the target word which is most re-lated to the context word-senses.
Given the 2n + 1context words, the system scores each sense of thetarget word.
Suppose the target word t has T senses,enumerated as t1, t2, .
.
.
, tT .
Also, suppose w1, w2,.
.
.
, w2n are the words in the context of t, each hav-ing W1, W2, .
.
.
, W2n senses, respectively.
Then foreach ti a score is computed asscore(ti) =2n?j=1maxk=1 to Wj(relatedness(ti, wjk))where wjk is the kth sense of word wj .
The sense tiof target word t with the highest score is selected asthe intended sense of the target word.The relatedness between two word senses is com-puted using a measure of semantic relatedness de-fined in the WordNet::Similarity software package(Pedersen et al, 2004), which is a suite of Perl mod-ules implementing a number WordNet-based mea-sures of semantic relatedness.
For this work, weused the Context Vector measure (Patwardhan andPedersen, 2006).
The relatedness of concepts iscomputed based on word co-occurrence statisticsderived from WordNet glosses.
Given two WordNetsenses, this module returns a score between 0 and 1,indicating the relatedness of the two senses.Our system relies on WordNet as its sense inven-tory.
However, this task used OntoNotes (Hovy etal., 2006) as the sense inventory.
OntoNotes wordsenses are groupings of similar WordNet senses.Thus, we used the training data answer key to gen-erate a mapping between the OntoNotes senses ofthe given lexical elements and their correspondingWordNet senses.
We had to manually create themappings for some of the WordNet senses, whichhad no corresponding OntoNotes senses.
The senseselection algorithm performed all of its computa-tions with respect to the WordNet senses, and finallythe OntoNotes sense corresponding to the selectedWordNet sense of the target word was output as the391answer for each instance.3 Results and AnalysisFor this task, we used the freely available Word-Net::SenseRelate::TargetWord v0.10 and the Word-Net::Similarity v1.04 packages.
WordNet v2.1 wasused as the underlying knowledge base for these.The context selection module used a window sizeof five (including the target word).
The semantic re-latedness of concepts was measured using the Con-text Vector measure, with configuration options asdefined in previous research (Patwardhan and Ped-ersen, 2006).
Since we always predict exactly onesense for each instance, the precision and recall val-ues of all our experiments were always the same.Therefore, in this section we will use the name ?ac-curacy?
to mean both precision and recall.3.1 Overall Results, and BaselinesThe overall accuracy of our system on the test datais 0.538.
This represents 2,609 correctly disam-biguated instances, out of a total of 4,851 instances.As baseline, we compare against the random al-gorithm where for each instance, we randomly pickone of the WordNet senses for the lexical elementin that instance, and report the OntoNotes senseid itmaps to as the answer.
This algorithm gets an ac-curacy of 0.417.
Thus, our algorithm gets an im-provement of 12% absolute (29% relative) over thisrandom baseline.Additionally, we compare our algorithm againstthe WordNet SenseOne algorithm.
In this algorithm,we pick the first sense among the WordNet sensesof the lexical element in each instance, and reportits corresponding OntoNotes sense as the answer forthat instance.
This algorithm leverages the fact that(in most cases) the WordNet senses for a particularword are listed in the database in descending orderof their frequency of occurrence in the corpora fromwhich the sense inventory was created.
If the newtest data has a similar distribution of senses, then thisalgorithm amounts to a ?majority baseline?.
Thisalgorithm achieves an accuracy of 0.681 which is15% absolute (27% relative) better than our algo-rithm.
Although this seemingly na?
?ve algorithm out-performs our algorithm, we choose to avoid usingthis information in our algorithms because it repre-sents a large amount of human supervision in theform of manual sense tagging of text, whereas ourgoal is to create a purely unsupervised algorithm.Additionally, our algorithms can, with little change,work with other sense inventories besides WordNetthat may not have this information.3.2 Results Disaggregated by Part of SpeechIn our past experience, we have found that av-erage disambiguation accuracy differs significantlybetween words of different parts of speech.
For thegiven test data, we separately evaluated the noun andverb instances.
We obtained an accuracy of 0.399for the noun targets and 0.692 for the verb targets.Thus, we find that our algorithm performs much bet-ter on verbs than on nouns, when evaluated using theOntoNotes sense inventory.
This is different fromour experience with SENSEVAL data from previousyears where performance on nouns was uniformlybetter than that on verbs.
One possible reason for thebetter performance on verbs is that the OntoNotessense inventory has, on average, fewer senses perverb word (4.41) than per noun word (5.71).
How-ever, additional experimentation is needed to morefully understand the difference in performance.3.3 Results Disaggregated by Lexical ElementTo gauge the accuracy of our algorithm on differentwords (lexical elements), we disaggregated the re-sults by individual word.
Table 1 lists the accuracyvalues over instances of individual verb lexical ele-ments, and Table 2 lists the accuracy values for nounlexical elements.
Our algorithm gets all instancescorrect for 13 verb lexical elements, and for none ofthe noun lexical elements.
More generally, our al-gorithm gets an accuracy of 50% or more on 45 outof the 65 verb lexical elements, and on 15 out of the35 noun lexical elements.
For nouns, when the ac-curacy results are viewed in sorted order (as in Table2), one can observe a sudden degradation of resultsbetween the accuracy of the word system.n ?
0.443?
and the word source.n ?
0.257.
It is unclear whythere is such a jump; there is no such sudden degra-dation in the results for the verb lexical elements.4 ConclusionsThis paper describes our system UMND1, whichparticipated in the SemEval-2007 Coarse-grained392Word Accuracy Word Accuracyremove 1.000 purchase 1.000negotiate 1.000 improve 1.000hope 1.000 express 1.000exist 1.000 estimate 1.000describe 1.000 cause 1.000avoid 1.000 attempt 1.000affect 1.000 say 0.969explain 0.944 complete 0.938disclose 0.929 remember 0.923allow 0.914 announce 0.900kill 0.875 occur 0.864do 0.836 replace 0.800maintain 0.800 complain 0.786believe 0.764 receive 0.750approve 0.750 buy 0.739produce 0.727 regard 0.714propose 0.714 need 0.714care 0.714 feel 0.706recall 0.667 examine 0.667claim 0.667 report 0.657find 0.607 grant 0.600work 0.558 begin 0.521build 0.500 keep 0.463go 0.459 contribute 0.444rush 0.429 start 0.421raise 0.382 end 0.381prove 0.364 enjoy 0.357see 0.296 set 0.262promise 0.250 hold 0.250lead 0.231 prepare 0.222join 0.222 ask 0.207come 0.186 turn 0.048fix 0.000Table 1: Verb Lexical Element AccuraciesEnglish Lexical Sample task.
The system is basedon WordNet::SenseRelate::TargetWord, which is afreely available unsupervised Word Sense Disam-biguation software package.
The system usesWordNet-based measures of semantic relatedness toselect the intended sense of an ambiguous word.
Thesystem required no training data and using WordNetas its only knowledge source achieved an accuracyof 54% on the blind test set.AcknowledgmentsThis research was partially supported by a NationalScience Foundation Early CAREER Developmentaward (#0092784).ReferencesS.
Banerjee and T. Pedersen.
2002.
An Adapted Lesk Al-gorithm for Word Sense Disambiguation Using Word-Net.
In Proceedings of the Third International Con-Word Accuracy Word Accuracypolicy 0.949 people 0.904future 0.870 drug 0.870space 0.857 capital 0.789effect 0.767 condition 0.765job 0.692 bill 0.686area 0.676 base 0.650management 0.600 power 0.553development 0.517 chance 0.467exchange 0.459 order 0.456part 0.451 president 0.446system 0.443 source 0.257network 0.218 state 0.208share 0.192 rate 0.186hour 0.167 plant 0.109move 0.085 point 0.080value 0.068 defense 0.048position 0.044 carrier 0.000authority 0.000Table 2: Noun Lexical Element Accuraciesference on Intelligent Text Processing and Computa-tional Linguistics, pages 136?145, Mexico City, Mex-ico, February.C.
Fellbaum, editor.
1998.
WordNet: An electronic lexi-cal database.
MIT Press.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
OntoNotes: The 90% Solu-tion.
In Proceedings of the Human Language Tech-nology Conference of the North American Chapter ofthe ACL, pages 57?60, New York, NY, June.S.
Patwardhan and T. Pedersen.
2006.
Using WordNet-based Context Vectors to Estimate the Semantic Relat-edness of Concepts.
In Proceedings of the EACL 2006Workshop on Making Sense of Sense: Bringing Com-putational Linguistics and Psycholinguistics Together,pages 1?8, Trento, Italy, April.S.
Patwardhan, S. Banerjee, and T. Pedersen.
2003.
Us-ing Measures of Semantic Relatedness for Word SenseDisambiguation.
In Proceedings of the Fourth In-ternational Conference on Intelligent Text Processingand Computational Linguistics, pages 241?257, Mex-ico City, Mexico, February.S.
Patwardhan, T. Pedersen, and S. Banerjee.
2005.SenseRelate::TargetWord - A Generalized Frameworkfor Word Sense Disambiguation.
In Proceedings ofthe Twentieth National Conference on Artificial In-telligence (Intelligent Systems Demonstrations), pages1692?1693, Pittsburgh, PA, July.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.WordNet::Similarity - Measuring the Relatedness ofConcepts.
In Human Language Technology Confer-ence of the North American Chapter of the Associationfor Computational Linguistics Demonstrations, pages38?41, Boston, MA, May.393
