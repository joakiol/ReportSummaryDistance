Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 117?124, New York City, June 2006. c?2006 Association for Computational LinguisticsUnsupervised Grammar Induction by Distribution and AttachmentDavid J. BrooksSchool of Computer ScienceUniversity of BirminghamBirmingham, B15 2TT, UKd.j.brooks@cs.bham.ac.ukAbstractDistributional approaches to grammar in-duction are typically inefficient, enumer-ating large numbers of candidate con-stituents.
In this paper, we describe asimplified model of distributional analy-sis which uses heuristics to reduce thenumber of candidate constituents underconsideration.
We apply this model toa large corpus of over 400000 words ofwritten English, and evaluate the resultsusing EVALB.
We show that the perfor-mance of this approach is limited, provid-ing a detailed analysis of learned structureand a comparison with actual constituent-context distributions.
This motivates amore structured approach, using a processof attachment to form constituents fromtheir distributional components.
Our find-ings suggest that distributional methodsdo not generalize enough to learn syntaxeffectively from raw text, but that attach-ment methods are more successful.1 IntroductionDistributional approaches to grammar induction ex-ploit the principle of substitutability: constituents ofthe same type may be exchanged with one anotherwithout affecting the syntax of the surrounding con-text.
Reversing this notion, if we can identify ?sur-rounding context?
by observation, we can hypothe-size that word sequences occurring in that contextwill be constituents of the same type.
Thus, distri-butional methods can be used to segment text intoconstituents and classify the results.
This work fo-cuses on distributional learning from raw text.Various models of distributional analysis havebeen used to induce syntactic structure, but mostuse probabilistic metrics to decide between candi-date constituents.
We show that the efficiency ofthese systems can be improved by exploiting someproperties of probable constituents, but also that thisreliance on probability is problematic for learningfrom text.
As a consequence, we propose an exten-sion to strict distributional learning that incorporatesmore information about constituent boundaries.The remainder of this paper describes our expe-riences with a heuristic system for grammar induc-tion.
We begin with a discussion of previous dis-tributional approaches to grammar induction in Sec-tion 2 and describe their implications in Section 3.We then introduce a heuristic distributional systemin Section 4, which we analyze empirically againsta treebank.
Poor system performance leads us to ex-amine actual constituent-context distributions (Sec-tion 5), the implications of which motivate a morestructured extension to our learning system, whichwe describe and analyze in Section 6.2 Previous approachesDistributional methods analyze text by alignment,aiming to find equivalence classes covering substi-tutable units.
We align common portions of textstermed contexts, leaving distinct contiguous word-sequences, termed expressions.
An expression andits context form an alignment pattern, which is de-117fined as:Cleft | Expression | Cright (AP1)From this alignment pattern, we can extract context-free grammar rules:NT ?
Expression1 ?
... ?
Expressionn (1)While the definition of expression is straightfor-ward, the definition of context is problematic.
Wewould like as much context as possible, but word-sequence contexts become less probable as theirlength increases, making learning harder.
Therefore,simple models of context are preferred, although theprecise definition varies between systems.Distributional approaches to grammar inductionfall into two categories, depending on their treat-ment of nested structure.
The first category cov-ers Expectation-Maximization (EM) systems.
Thesesystems propose constituents based on analysis oftext, then select a non-contradictory combinationof constituents for each sentence that maximizes agiven metric, usually parsing probability.
EM hasthe advantage that constituent probabilities are onlycompared when constituents compete, which re-moves the inherent bias towards shorter constituents,which tend to have higher probability.
However, EMmethods are more susceptible to data sparsity issuesassociated with raw text, because there is no gener-alization during constituent proposal.Examples of EM learning systems are ContextDistribution Clustering (CDC) (Clark, 2001) andConstituent-Context Model (CCM) (Klein, 2005,Chapter 5), which avoid the aforementioned data-sparsity issues by using a part-of-speech (POS)tagged corpus, rather than raw text.
AlignmentBased Learning (ABL) (van Zaanen, 2000) is theonly EM system applied directly to raw text.
ABLuses minimal String-Edit Distance between sen-tences to propose constituents, from which the mostprobable combination is chosen.
However, ABL isrelatively inefficient and has only been applied tosmall corpora.The second category is that of incremental learn-ing systems.
An incremental system analyzes a cor-pus in a bottom-up fashion: each time a new con-stituent type is found, it is inserted into the corpusto provide data for later learning.
This has the ad-vantage of easing the data-sparsity issues describedabove because infrequent sequences are clusteredinto more frequent non-terminal symbols.
However,in incremental systems, constituents are compareddirectly, which can lead to a bias towards shorterconstituents.The EMILE system (Adriaans, 1999) learns shal-low languages in an incremental manner, and hasbeen applied to natural language under the assump-tion that such languages are shallow.
Shallownessis the property whereby, for any constituent type ina language, there exist well-supported minimal unitsof that type.
EMILE aligns complete sentences only,attempting to isolate minimal units, which are thenused to process longer sequences.
This method isefficient because alignment is non-recursive.
How-ever, as a consequence, EMILE offers only a limitedtreatment of nested and recursive structures.A more comprehensive approach to learningnested structure is found in the ADIOS sys-tem (Solan et al, 2003).
ADIOS enumerates all pat-terns of a given length, under the condition that eachsequence must have non-empty contexts and expres-sions.
These patterns are ranked using an informa-tion gain metric, and the best pattern at each iterationis rewritten into the graph, before pattern scanningbegins again.
ADIOS learns context-sensitive equiv-alence classes, but does not induce grammars, andhas not been formally evaluated against treebanks.Grammar induction systems are evaluated usingstandard metrics for parser evaluation, and in par-ticular, the EVALB algorithm1.
The above sys-tems have been evaluated with respect to the ATIStreebank.
Compared with supervised parsers, thesesystems perform relatively poorly, with the strictlyunsupervised EMILE and ABL systems recovering16.8% and 35.6% of constituent structure respec-tively.
The partially-supervised systems of CDC andCCM perform better, with the latter retrieving 47.6%of the constituent structure in ATIS.
However, thestrictly unsupervised systems of ABL, EMILE andADIOS have not been evaluated on larger corpora,in part due to efficiency constraints.1There are known issues with parser evaluation, although adiscussion of these issues is outside the scope of this paper, andthe reader is referred to (Klein, 2005, Chapter 2).
We assumethe standard evaluation for comparison with previous work.1183 Issues for distributional learningThere are many issues with distributional learning,especially when learning from raw text.
First, previ-ous systems hypothesize and select constituents ac-cording to the probability of their contexts: ABL,EMILE and CCM use the probability of proposedequivalence classes, or the equivalent context prob-ability; ADIOS uses an information gain metric,again favouring probable contexts.
However, whenlearning from raw text, this preference for hypothe-ses with more probable contexts means that open-class words will seldom be considered as contexts.In POS-based learners, it is possible to align open-class POS contexts.
These contexts are demonstra-bly important despite low word probabilities, whichsuggests that selecting contexts on the basis of prob-ability will be limited in success.The second problem relates to word-senses.Alignment proceeds by matching orthographictypes, but these types can have numerous associatedsyntactic senses.
For example, ?to?
plays two distinctroles: infinitive marker or preposition.
If we alignusing the orthographic type, we will often misalignwords, as seen in the following alignment:I gave it to the man in the grey jacketJohn agreed to see me in 20 minutesHere, we are (mis)aligning a prepositional ?to?, withan infinitive marker.
The result would be a correctlyidentified noun-phrase, ?the man?, and an incorrectstructure, contradicting both the verb-group ?to see?and the noun-phrase ?me?.
This problem does notaffect POS-based learning systems, as POS tags areunambiguously assigned.Finally, grammar induction systems are typicallyinefficient, which prohibits training over large cor-pora.
Distributional analysis is an expensive proce-dure, and must be performed for large numbers ofword sequences.
Previous approaches have tendedto enumerate all alignment patterns, of which thebest are selected using probabilistic metrics.
How-ever, given the preference for probable alignments,there is considerable wasted computation here, andit is on this issue that we shall focus.4 A heuristic approach to alignmentRather than enumerating all word sequences in acorpus, we propose a heuristic for guiding distribu-tional systems towards more favourable alignmentpatterns, in a system called Directed Alignment.
Inthis system, we define context as the ordered pairof left- and right-context for a given constituent,?Cleft ?
Cright?, where Cleft and Cright are single-units.
The atomic units of this system are words, butlearned constituents may also act as context-units.The probability of a pattern depends primarily onits contexts, since they are common to all matchingsequences.
We can reduce the task of finding proba-ble alignments to simply finding probable context-pairs.
However, we can reduce this further: fora context-pair to be probable, its components mustalso be probable.
Therefore, rather than enumerat-ing all patterns in the corpus, we direct the alignmentprocedure towards patterns where Cleft and Crightare probable.The first stage of direction creates an index for thecorpus, compiling a list of unit types, where unitsare initially words.
From this list of types, the mostprobable 1% are selected as context-units.
Thesecontext-units are the only types allowed to fill theroles Cleft and Cright in alignment patterns.Alignments are created directly from the context-unit index.
For each context-unit token cu in theindex, we locate cu in the corpus and create analignment pattern, such that cu is the left context(Cleft).
Next, we scan the sequence of words fol-lowing cu, extending the alignment pattern until an-other context-unit cu?
is found, or a fixed lengththreshold is exceeded.
If cu?
is found, it fills the roleof right context (Cright), and the completed align-ment pattern is cached; otherwise, the pattern is dis-regarded.Direction permits two forms of valid expressionsin the context ?cu ?
cu??:1.
nc1 .
.
.
ncn, where each nci is a non-context2.
c1 .
.
.
cn, where each ci is a context-unitThe first of these forms allows us to examine non-nested alignments.
The second allows us to analyzenested alignments only after inner constituents havebeen learned.
These constraints reduce the numberof constituents under consideration at any time toa manageable level.
As a result, we can scan verylarge numbers of alignment patterns with relativelylittle overhead.119As an example, consider the following sequence,with context units underlined:put the whole egg , all the seasonings and vegeta-bles into the bowl and process for 10 seconds untilsmoothly pured .This would be broken into non-recursive expres-sions2:(put) the (whole egg) , all the (seasonings) and (veg-etables) into the (bowl) and (process) for (10 sec-onds) until (smoothly pureed) .These expressions will be replaced by non-terminalunit representing the class of expressions, such thateach class contains all units across the corpus thatoccur in the same context:NT0 the NT1 , all the NT2 and NT3 into the NT2and NT4 for NT5 until NT6 .Following this generalization nested structures canbe discovered using the same process.This approach has some interesting parallels withchunking techniques, most notably that of function-word phrase identification (Smith and Witten, 1993).This similarity is enforced by disallowing nestedstructures.
Unlike chunking systems, however, thiswork will also attempt to recover nested structuresby means of incremental learning.4.1 Selecting alignment patternsThe direction process extracts a set of candidatealignments, and from this set we select the bestalignment to rewrite as an equivalence class.
Previ-ous approaches offer a number of metrics for rank-ing constituents, based around constituent or contextprobability (ABL and CCM), Mutual Information(CDC), and information gain (ADIOS).
We have im-plemented several of these metrics, but our expe-riences suggest that context probability is the mostsuccessful.The probability of an alignment is effectively thesum of all path probabilities through the alignment:P (Cleft, Cright) = ?P (pathleft,right) (2)where each pathleft,right is a unique word sequencestarting with left and ending with right, under the2For clarity, we have shown all alignments for the given sen-tence simultaneously.
However, the learning process is incre-mental, so each alignment would be proposed during a distinctlearning iteration.constraints on expressions described above.
There isan important practical issue here: probability sumssuch as that in Equation 2 do not decrease when ex-pressions are replaced with equivalence classes.
Toalleviate this problem, we rewrite the units when up-dating the distribution, but discard paths that matchthe current alignment.
This prevents looping whileallowing the rewritten paths to contribute to nestedstructures.4.2 Generalizing expression classesThe model outlined above is capable of learningstrictly context-sensitive constituents.
While thisdoes allow for nested constituents, it is problematicfor generalization.
Consider the following equiva-lence classes, which are proposed relatively early inDirected Alignment:the NT1 ofthe NT2 inHere, the non-terminals have been assigned on thebasis of context-pairs: NT1 is defined by ?the?
of?and NT2 is defined by ?the ?
in?.
These types aredistinct, although intuitively they account for simplenoun-phrases.
If we then propose an alignment pat-tern with NT1 as Cleft, it must be followed by ?of?,which removes any possibility of generalizing ?of?and ?in?.We alleviate this problem by generalizing equiv-alence classes, using a simple clustering algorithm.For each new alignment, we compare the set of ex-pressions with all existing expression classes, rank-ing the comparisons by the degree of overlap withthe current alignment.
If this degree of overlap ex-ceeds a fixed threshold, the type of the existing classis assumed; otherwise, a new class is created.4.3 Experiments, results and analysisTo evaluate our algorithm, we follow the standardapproach of comparing the output of our systemwith that of a treebank.
We use the EVALB algo-rithm, originally designed for evaluating supervisedparsing systems, with identical configuration to thatof (van Zaanen, 2000).
However, we apply our algo-rithms to a different corpus: the written sub-corpusof the International Corpus of English, Great BritainComponent (henceforth ICE-GB), with punctuationremoved.
This consists of 438342 words, in 22815sentences.
We also include a baseline instantiation120System UP UR F1 CBFWB 30.0 11.0 16.0 0.36DA 23.3 8.0 11.9 0.30DAcluster 23.6 8.1 12.0 0.30Table 1: EVALB results after 500 iterations of Di-rected Alignment applied to ICE-GB, showing bothcontext-sensitive (DA) and clustered (DAcluster)alignment.
The columns represent Unlabeled Preci-sion, Unlabeled Recall, Unlabeled F-Score and theproportion of sentence with crossing brackets re-spectively.of our algorithm, which chunks text into expres-sions between function words, which we refer to asFunction-Word Bracketing (FWB).Table 1 summarizes the EVALB scores for two500-iteration runs of Directed Alignment over ICE-GB: DA is the standard context-sensitive version ofthe algorithm; DAcluster is the version with contextclustering.
FWB precision is relatively low, withonly 30% of proposed structures appearing in thetreebank.
Recall is even lower, with only 11% ofstructure retrieved.
This is unsurprising, as no nestedconstructions are considered.In comparison, both versions of Directed Align-ment perform significantly worse, with DAclusterbeing only fractionally better than standard DA.
Ex-periments over more learning iterations suggest thatthe performance of DA converges on FWB, withfew nested constituents discovered.
Both variantsof the system produce very poor performance, withvery little nested structure recovered.
While theseresults seem discouraging, it is worth investigatingsystem performance further.Table 2, summarizes the success of the algorithmat discovering different types of constituent.
Notethat these results are unlabeled, so we are examiningthe proportion of each type of constituent in ICE-GB that has been identified.
Here, Directed Align-ment exhibits the most success at identifying non-clauses, of which the primary source of success isshort sentence fragments.
Around 10% of noun-phrases (NP), verb-phrases (VP) and subordinate-phrases (SUBP) were recovered, this limited suc-cess reflects the nature of the constituents: all threehave relatively simple constructions, whereby a sin-gle word represents the constituent.
In contrast, con-Recall (%)Category Frequency FWB DA DAclusterNP 117776 11.81 10.83 10.79CL 28641 0.50 1.21 1.14VP 50280 20.88 9.58 9.89PP 42134 0.10 0.67 0.73SUBP 7474 1.10 11.05 11.15NONCL 1919 4.27 22.98 22.98Table 2: Constituent retrieval results for Function-Word Bracketing (FWB) and Directed Alignment(DA and DAcluster), categorized by gold-type(a) DA, top 5 noun-matches of271Learned Recall PrecisionNT0 4.61 84.53NT5 1.58 93.44NT7 1.36 87.14NT4 1.09 75.10NT10 0.82 84.54(b) DAcluster , top 5 noun-matches of 135Learned Recall PrecisionNT0 6.93 87.09NT4 6.48 89.91NT8 2.62 40.48NT11 0.86 68.60NT10 0.58 16.95Table 3: The top five expression classes to match N(noun) in ICE-GB, ranked by recall.stituent types that comprise multiple units, such asprepositional-phrases (PP), are seldom recovered.4.3.1 Class generalizationDuring learning in DAcluster, we induce gener-alized classes using the expression clustering algo-rithm.
This generalization can be evaluated, com-paring induced classes with those in the treebank us-ing precision and recall.
Table 2(a) shows the topfive proposed classes matching the type noun (N)in ICE-GB during 500 iterations of context-sensitiveDirected Alignment.
There are 271 types matchingnoun, and as can be seen, the top five account fora very small proportion of all nouns, some 9.46%(recall).Table 2(b) shows the same analysis for DirectedAlignment with class generalization.
For nounmatches, we can see that there are far fewer pro-posed classes (135), and that those classes are muchmore probable, the top five accounting for 17.47%121(a) Noun Phrases (frequency=123870)LEFT START END RIGHTSYMB REC SYMB REC SYMB REC SYMB RECPREP 0.36 ART 0.29 N 0.53 PUNC 0.36V 0.19 PRON 0.29 PRON 0.19 V 0.18#STA# 0.12 N 0.2 N 2 0.11 AUX 0.13CONJ 0.11 N 1 0.06 PUNC 0.06 CONJ 0.09PUNC 0.09 ADJ 0.06 NUM 0.04 PREP 0.07(b) Verb Phrases (frequency=50693)Left Start End RightSYMB REC SYMB REC SYMB REC SYMB RECPRON 0.32 V 0.68 V 0.98 PREP 0.20N 0.26 AUX 0.29 PUNC 0.01 ART 0.16PTCL 0.11 AUX 1 0.02 AUX 0.00 PRON 0.14PUNC 0.06 V 1 0.00 V 2 0.00 ADV 0.13CONJ 0.05 ADV 0.00 ADV 0.00 ADJ 0.09(c) Prepositional Phrases (frequency=45777)Left Start End RightSYMB REC SYMB REC SYMB REC SYMB RECN 0.46 PREP 0.96 N 0.63 PUNC 0.56V 0.23 PREP 1 0.02 N 2 0.12 CONJ 0.09ADV 0.05 ADV 0.01 PUNC 0.08 PREP 0.09PUNC 0.05 NUM 0.00 PRON 0.05 V 0.07ADJ 0.04 ADV 1 0.00 NUM 0.03 AUX 0.05Table 4: The five most frequent left/start/end/rightPOS contexts for NP, VP and PP constituents.of nouns in ICE-GB.
The algorithm seems to beachieving some worthwhile generalization, whichis reflected in a slight increase in EVALB scoresfor DAcluster.
However, this increase is not a sig-nificant one, suggesting that this generalization isnot sufficient to support distributional learning.
Wemight expect this: attempting to cluster based onthe low-frequency and polysemous words in expres-sions seems likely to produce unreliable clusters.5 A closer look at distributional contextsThe results discussed so far seem discouraging forthe approach.
However, there are good reasons whythese results are so poor, and why we can expectlittle improvement in the current formulation.
Wecan show some of these reasons by examining ac-tual constituent-context distributions.Table 4 shows an analysis of the constituenttypes NP, VP and PP in ICE-GB, against the fivemost frequent POS tags3 occurring as left-context,constituent-start, constituent-end, and right-context.We distinguish the following POS categories as be-ing primarily functional, as they account for the ma-jority of context-units considered by Directed Align-ment: prepositions (PREP), articles (ART), aux-3The same trends can be shown for words, but a POS analy-sis is preferred for clarity and brevity.iliaries (AUX), sentence-starts (#STA#), pronouns(PRON), conjunctions (CONJ), particles (PTCL)and punctuation (PUNC).From Table 4, we can see that noun-phrases andverb-phrases are relatively well-suited to our ap-proach.
First, both types have strong functionalleft- and right-contexts: 58% of NP left-contexts and50% of NP right-contexts are members of our func-tional POS; similarly, 43% of VP left-contexts and49% of VP right-contexts are functional.
This meansthat a probability-based model of context, such asours, will find relatively strong support for thesetypes.
Second, both NP and VP have minimal unittypes: nouns and pronouns for NP; verbs for VP.
Asa consequence, these types tend to carry more proba-bility mass, since shorter sequences tend to be morefrequent.
We should expect our system to performreasonably on NP and VP as a result.In contrast, prepositional-phrases are much lessamenable to distributional analysis.
First, PP tendto be longer, since they contain NP, and this hasobvious repercussions for alignment probabilities.More damagingly, PP contexts are dominated byopen-class words - the top 74% of PP left-contextsare nouns, verbs and adverbs.
Therefore, a purelyprobabilistic distributional approach cannot accountfor prepositional-phrases, since learning data is toosparse.
Previous approaches have relied upon open-class generalization to reduce this problem, but thesemethods suffer from the same problems of data spar-sity, and as such are not reliable enough to resolvethe issue.6 AttachmentWe have seen that strictly probabilistic distribu-tional analysis is not sufficient to learn constituentsfrom raw text.
If we are to improve upon this, wemust find a way to identify constituents from theircomponent parts, as well as by contextual analy-sis.
The constituent-context distributions in Table 4give us some clues as to where to start: both noun-phrases and prepositional-phrases show very signif-icant constituent-starts, with articles and pronounsstarting 58% of NP, and prepositions starting 94%of all PP.
These functional types would be identifiedas contexts in Directed Alignment, but the strong re-lation to their containing constituents would be ig-122nored.One method for achieving such an internal rela-tionship might be to attach contexts to the expres-sions with which they co-occur, and we propose us-ing such a method here.
However, this requires thatwe have some criterion for deciding when and howexpressions should be attached to their contexts.
Weuse a measure based on STOP arguments (Collins,1999), which allows us to condition the decision toinsert a constituent boundary on the evidence we seefor doing so.
For raw text, the only boundaries thatare explicitly marked are at the start and end of sen-tences, and it is this information we use to decidewhen to attach contexts to expressions4 .
In otherwords, if a context is likely to start a sentence, weassume it is also likely to start a constituent at otherpositions within a sentence.In order to calculate the likelihood of a particu-lar context word w occurring at the start or end of asentence, we simply use the bigram probabilities be-tween w and the special symbols START and END,which denote the start and end of a sentence respec-tively.
From these probabilities, we calculate MutualInformation MI(START,w) and MI(w,END).We prefer MI because it describes the strength ofthe relation between w and these special symbolswithout bias towards more probable words.
Fromthese MI values, we calculate a Directional Prefer-ence (DP) for the context word:dp(w) = MI(w,END) ?
MI(START,w) (3)This yields a number representing whether w ismore likely to start or end a sentence.
This num-ber will be zero if we are equally likely to see w atthe start or end of a sentence, negative if w is morelikely to start a sentence, and positive if w is is morelikely to end a sentence.Using DP, we can decide how to attach an expres-sion to its contexts.
For a given alignment, we con-sider the possibility of attaching the expression toneither context, the left-context, or the right-context,by comparing the DP for the left- and right-contexts.If the left-context shows a strong tendency to startsentences, and the right-context does not show a4For this method to work, we assume that our corpus is seg-mented into sentences.
This is not the case for speech, but forlearning from text it seems a reasonable assumption.System UP UR F1 CBDASTOP 33.6 14.1 19.8 0.42Table 5: EVALB results after 500 iterations of Di-rected Alignment with STOP attachment applied toICE-GB (DASTOP ).Category Frequency Recall (%)NP 117776 18.11VP 50280 9.78PP 42134 18.19CL 28641 2.97SUBP 7474 12.82NONCL 1919 22.62Table 6: Constituent retrieval results for DASTOP ,categorized by gold-typestrong tendency to end sentences (i.e.
there is anoverall DP is negative), we attach the expression toits left-context; if the reverse situation is true, we at-tach the expression to its right context.
Should thedifference between these DP fall below a threshold,neither context is preferred, and the expression re-mains unattached.Let us consider a specific example of attachment.The first alignment considered by the system (whenapplied to ICE-GB) is:the NT1 ofHere, we need to compare the likelihood of seeing aconstituent start with ?the?
with with the likelihoodof seeing a constituent end with ?of?.
Intuitively,?the?
occurs frequently at the start of a sentence, andnever at the end.
Consequently, it has a high neg-ative DP.
Meanwhile ?of?
has a small negative DP.In combination, there is a high negative DP, so weattach the expression to the left-context, ?the?.6.1 Experimental AnalysisWe applied Directed Alignment with attachmentbased on STOP arguments (DASTOP ) to ICE-GBas before, running for 500 iterations.
These resultsare shown in Table 5.
The results are encouraging.Unlabeled precision increased by almost 50%, from23.6% for DAcluster to 33.6%.
Likewise, system re-call increased dramatically, from 8.1% to 14.1%, upsome 75%.
Crossing-brackets increased slightly, butremained relatively low at 0.42.Table 6 shows the breakdown of EVALB scores123for the major non-terminal types, as before.
Theimprovement in EVALB scores is attributableto a marked increase in success at identifyingprepositional-phrases, with a lesser increase innoun-phrase identification.6.2 DiscussionThe attachment procedure described above is moresuccessful at discovering nested constituents thandistributional methods.
There are good reasons whythis should be the case.
First, attachment compressesthe corpus, removing the bias towards shorter se-quences.
Indeed, the algorithm seems capable ofretrieving complex constituents of up to ten wordsin length during the first 500 iterations.Second, the STOP-conditioning criterion, whilesomewhat ad hoc in relation to distributional meth-ods, allows us to assess where constituent bound-aries are likely to occur.
As such, this can be seenas a rudimentary method for establishing argumentrelations, such as those observed in (Klein, 2005,Chapter 6).Despite these improvements, the attachment pro-cess also makes some systematic mistakes.
Some ofthese may be attributed to discrepancies between thesyntactic theory used to annotate the treebank andthe attachment process.
For example, verbs are rou-tinely attached to their subjects before objects, con-tradicting the more traditional interpretation presentin treebanks.
Some of the remaining mistakes canbe attributed to the misalignment, due to the ortho-graphic match problem described in Section 3.7 Future WorkThe major problem when applying distributionalmethods to raw text is that of orthographic match-ing, which causes misalignments between alterna-tive senses of a particular word-form.
To reduce thisproblem, context-units must be classified in someway to disambiguate these different senses.
Suchclassification could be used as a precursor to align-ment in the system we have described.In addition, to better evaluate the quality of at-tachment, dependency representations and treebankscould be used, which do not have an explicit order onattachment.
This would give a more accurate evalu-ation where subject-verb attachment is concerned.8 ConclusionsWe have presented an incremental grammar induc-tion system that uses heuristics to improve the effi-ciency of distributional learning.
However, in testsover a large corpus, we have shown that it is capableof learning only a small subset of constituent struc-ture.
We have analyzed actual constituent-contextdistributions to explain these limitations.
This anal-ysis provides the motivation for a more structuredlearning method, which incorporates knowledge ofverifiable constituent boundaries - the starts andends of sentences.
This improved system performssignificantly better, with a 75% increase in recallover distributional methods, and a significant im-provement at retrieving structures that are problem-atic for distributional methods alone.ReferencesPieter Adriaans.
1999.
Learning shallow context-freelanguages under simple distributions.
Technical Re-port PP-1999-13, Institute for Logic, Language, andComputation, Amsterdam.Alexander Clark.
2001.
Unsupervised induction ofstochastic context free grammars with distributionalclustering.
In Proceedings of the Fifth Confer-ence on Natural Language Learning, pages 105?112,Toulouse, France, July.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Dan Klein.
2005.
The Unsupervised Learning of Natu-ral Language Structure.
Ph.D. thesis, Department ofComputer Science, Stanford University, March.Tony C. Smith and Ian H. Witten.
1993.
Language in-ference from function words.
Working Paper Series1170-487X-1993/3, Department of Computer Science,University of Waikato, Hamilton, New Zealand, Au-gust.Zach Solan, David Horn, Eytan Ruppin, and ShimonEdelman.
2003.
Unsupervised efficient learning andrepresentation of language structures.
In R. Altermanand D. Kirsch, editors, Proceedings of the 25th Con-ference of the Cognitive Science Society, Hillsdale, NJ.Erlbaum.Menno van Zaanen.
2000.
Learning structure usingAlignment Based Learning.
In Proceedings of theThird Annual Doctoral Research Colloquium (CLUK),pages 75?82.124
