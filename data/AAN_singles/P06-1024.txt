Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 185?192,Sydney, July 2006. c?2006 Association for Computational LinguisticsLearning More Effective Dialogue Strategies Using Limited DialogueMove FeaturesMatthew Frampton and Oliver LemonHCRC, School of InformaticsUniversity of EdinburghEdinburgh, EH8 9LW, UKM.J.E.Frampton@sms.ed.ac.uk, olemon@inf.ed.ac.ukAbstractWe explore the use of restricted dialoguecontexts in reinforcement learning (RL)of effective dialogue strategies for infor-mation seeking spoken dialogue systems(e.g.
COMMUNICATOR (Walker et al,2001)).
The contexts we use are richerthan previous research in this area, e.g.
(Levin and Pieraccini, 1997; Scheffler andYoung, 2001; Singh et al, 2002; Pietquin,2004), which use only slot-based infor-mation, but are much less complex thanthe full dialogue ?Information States?
ex-plored in (Henderson et al, 2005), forwhich tractabe learning is an issue.
Weexplore how incrementally adding richerfeatures allows learning of more effectivedialogue strategies.
We use 2 user simu-lations learned from COMMUNICATORdata (Walker et al, 2001; Georgila et al,2005b) to explore the effects of differ-ent features on learned dialogue strategies.Our results show that adding the dialoguemoves of the last system and user turnsincreases the average reward of the auto-matically learned strategies by 65.9% overthe original (hand-coded) COMMUNI-CATOR systems, and by 7.8% over a base-line RL policy that uses only slot-statusfeatures.
We show that the learned strate-gies exhibit an emergent ?focus switch-ing?
strategy and effective use of the ?givehelp?
action.1 IntroductionReinforcement Learning (RL) applied to the prob-lem of dialogue management attempts to find op-timal mappings from dialogue contexts to sys-tem actions.
The idea of using Markov Deci-sion Processes (MDPs) and reinforcement learn-ing to design dialogue strategies for dialogue sys-tems was first proposed by (Levin and Pierac-cini, 1997).
There, and in subsequent work suchas (Singh et al, 2002; Pietquin, 2004; Schefflerand Young, 2001), only very limited state infor-mation was used in strategy learning, based al-ways on the number and status of filled informa-tion slots in the application (e.g.
departure-city isfilled, destination-city is unfilled).
This we refer toas low-level contextual information.
Much priorwork (Singh et al, 2002) concentrated only onspecific strategy decisions (e.g.
confirmation andinitiative strategies), rather than the full problemof what system dialogue move to take next.The simple strategies learned for low-level def-initions of state cannot be sensitive to (sometimescritical) aspects of the dialogue context, such asthe user?s last dialogue move (DM) (e.g.
request-help) unless that move directly affects the status ofan information slot (e.g.
provide-info(destination-city)).
We refer to additional contextual infor-mation such as the system and user?s last di-alogue moves as high-level contextual informa-tion.
(Frampton and Lemon, 2005) learned fullstrategies with limited ?high-level?
information(i.e.
the dialogue move(s) of the last user utter-ance) and only used a stochastic user simulationwhose probabilities were supplied via common-sense and intuition, rather than learned from data.This paper uses data-driven n-gram user simula-tions (Georgila et al, 2005a) and a richer dialoguecontext.On the other hand, increasing the size of thestate space for RL has the danger of makingthe learning problem intractable, and at the veryleast means that data is more sparse and state ap-proximation methods may need to be used (Hen-derson et al, 2005).
To date, the use of verylarge state spaces relies on a ?hybrid?
super-vised/reinforcement learning technique, where thereinforcement learning element has not yet beenshown to significantly improve policies over thepurely supervised case (Henderson et al, 2005).185The extended state spaces that we propose arebased on theories of dialogue such as (Clark, 1996;Searle, 1969; Austin, 1962; Larsson and Traum,2000), where which actions a dialogue participantcan or should take next are not based solely onthe task-state (i.e.
in our domain, which slots arefilled), but also on wider contextual factors suchas a user?s dialogue moves or speech acts.
Infuture work we also intend to use feature selec-tion techniques (e.g.
correlation-based feature sub-set (CFS) evaluation (Rieser and Lemon, 2006))on the COMMUNICATOR data (Georgila et al,2005a; Walker et al, 2001) in order to identify ad-ditional context features that it may be effective torepresent in the state.1.1 MethodologyTo explore these issues we have developed a Re-inforcement Learning (RL) program to learn di-alogue strategies while accurate simulated users(Georgila et al, 2005a) converse with a dialoguemanager.
See (Singh et al, 2002; Scheffler andYoung, 2001) and (Sutton and Barto, 1998) for adetailed description of Markov Decision Processesand the relevant RL algorithms.In dialogue management we are faced with theproblem of deciding which dialogue actions it isbest to perform in different states.
We use (RL) be-cause it is a method of learning by delayed rewardusing trial-and-error search.
These two proper-ties appear to make RL techniques a good fit withthe problem of automatically optimising dialoguestrategies, because in task-oriented dialogue of-ten the ?reward?
of the dialogue (e.g.
successfullybooking a flight) is not obtainable immediately,and the large space of possible dialogues for anytask makes some degree of trial-and-error explo-ration necessary.We use both 4-gram and 5-gram user sim-ulations for testing and for training (i.e.
trainwith 4-gram, test with 5-gram, and vice-versa).These simulations also simulate ASR errors sincethe probabilities are learned from recognition hy-potheses and system behaviour logged in theCOMMUNICATOR data (Walker et al, 2001) fur-ther annotated with speech acts and contexts by(Georgila et al, 2005b).
Here the task domain isflight-booking, and the aim for the dialogue man-ager is to obtain values for the user?s flight infor-mation ?slots?
i.e.
departure city, destination city,departure date and departure time, before makinga database query.
We add the dialogue moves ofthe last user and system turns as context featuresand use these in strategy learning.
We comparethe learned strategies to 2 baselines: the originalCOMMUNICATOR systems and an RL strategywhich uses only slot status features.1.2 OutlineSection 2 contains a description of our basic ex-perimental framework, and a detailed descriptionof the reinforcement learning component and usersimulations.
Sections 3 and 4 describe the experi-ments and analyse our results, and in section 5 weconclude and suggest future work.2 The Experimental FrameworkEach experiment is executed using the DIPPERInformation State Update dialogue manager (Boset al, 2003) (which here is used to track and up-date dialogue context rather than deciding whichactions to take), a Reinforcement Learning pro-gram (which determines the next dialogue actionto take), and various user simulations.
In sections2.3 and 2.4 we give more details about the rein-forcement learner and user simulations.2.1 The action set for the learnerBelow is a list of all the different actions that theRL dialogue manager can take and must learn tochoose between based on the context:1.
An open question e.g.
?How may I help you??2.
Ask the value for any one of slots 1...n.3.
Explicitly confirm any one of slots 1...n.4.
Ask for the nth slot whilst implicitly confirm-ing1 either slot value n?
1 e.g.
?So you wantto fly from London to where?
?, or slot valuen + 15.
Give help.6.
Pass to human operator.7.
Database Query.There are a couple of restrictions regardingwhich actions can be taken in which states: anopen question is only available at the start of thedialogue, and the dialogue manager can only tryto confirm non-empty slots.2.2 The Reward FunctionWe employ an ?all-or-nothing?
reward functionwhich is as follows:1.
Database query, all slots confirmed: +1002.
Any other database query: ?751Where n = 1 we implicitly confirm the final slot andwhere n = 4 we implicitly confirm the first slot.
This actionset does not include actions that ask the nth slot whilst im-plicitly confirming slot value n ?
2.
These will be added infuture experiments as we continue to increase the action andstate space.1863.
User simulation hangs-up: ?1004.
DIPPER passes to a human operator: ?505.
Each system turn: ?5To maximise the chances of a slot value be-ing correct, it must be confirmed rather than justfilled.
The reward function reflects the fact thata successful dialogue manager must maximise itschances of getting the slots correct i.e.
they mustall be confirmed.
(Walker et al, 2000) showedwith the PARADISE evaluation that confirmingslots increases user satisfaction.The maximum reward that can be obtained fora single dialogue is 85, (the dialogue managerprompts the user, the user replies by filling all fourof the slots in a single utterance, and the dialoguemanager then confirms all four slots and submits adatabase query).2.3 The Reinforcement Learner?s ParametersWhen the reinforcement learner agent is initial-ized, it is given a parameter string which includesthe following:1.
Step Parameter: ?
= decreasing2.
Discount Factor: ?
= 13.
Action Selection Type = softmax (alternativeis ?-greedy)4.
Action Selection Parameter: temperature =155.
Eligibility Trace Parameter: ?
= 0.96.
Eligibility Trace = replacing (alternative isaccumulating)7.
Initial Q-values = 25The reinforcement learner updates its Q-valuesusing the Sarsa(?)
algorithm (see (Sutton andBarto, 1998)).
The first parameter is the step-parameter ?
which may be a value between 0 and1, or specified as decreasing.
If it is decreasing, asit is in our experiments, then for any given Q-valueupdate ?
is 1k where k is the number of times thatthe state-action pair for which the update is be-ing performed has been visited.
This kind of stepparameter will ensure that given a sufficient num-ber of training dialogues, each of the Q-values willeventually converge.
The second parameter (dis-count factor) ?
may take a value between 0 and 1.For the dialogue management problem we set it to1 so that future rewards are taken into account asstrongly as possible.Apart from updating Q-values, the reinforce-ment learner must also choose the next actionfor the dialogue manager and the third parameterspecifies whether it does this by ?-greedy or soft-max action selection (here we have used softmax).The fifth parameter, the eligibility trace param-eter ?, may take a value between 0 and 1, and thesixth parameter specifies whether the eligibilitytraces are replacing or accumulating.
We used re-placing traces because they produced faster learn-ing for the slot-filling task.
The seventh parameteris for supplying the initial Q-values.2.4 N-Gram User SimulationsHere user simulations, rather than real users, inter-act with the dialogue system during learning.
Thisis because thousands of dialogues may be neces-sary to train even a simple system (here we trainon up to 50000 dialogues), and for a proper explo-ration of the state-action space the system shouldsometimes take actions that are not optimal for thecurrent situation, making it a sadistic and time-consuming procedure for any human training thesystem.
(Eckert et al, 1997) were the first touse a user simulation for this purpose, but it wasnot goal-directed and so could produce inconsis-tent utterances.
The later simulations of (Pietquin,2004) and (Scheffler and Young, 2001) were tosome extent ?goal-directed?
and also incorporatedan ASR error simulation.
The user simulations in-teract with the system via intentions.
Intentionsare preferred because they are easier to generatethan word sequences and because they allow er-ror modelling of all parts of the system, for exam-ple ASR error modelling and semantic errors.
Theuser and ASR simulations must be realistic if thelearned strategy is to be directly applicable in areal system.The n-gram user simulations used here (see(Georgila et al, 2005a) for details and evaluationresults) treat a dialogue as a sequence of pairs ofspeech acts and tasks.
They take as input the n?1most recent speech act-task pairs in the dialoguehistory, and based on n-gram probabilities learnedfrom the COMMUNICATOR data (automaticallyannotated with speech acts and Information States(Georgila et al, 2005b)), they then output a userutterance as a further speech-act task pair.
Theseuser simulations incorporate the effects of ASR er-rors since they are built from the user utterancesas they were recognized by the ASR componentsof the original COMMUNICATOR systems.
Notethat the user simulations do not provide instanti-ated slot values e.g.
a response to provide a des-tination city is the speech-act task pair ?
[provideinfo] [dest city]?.
We cannot assume that two suchresponses in the same dialogue refer to the same187destination cities.
Hence in the dialogue man-ager?s Information State where we record whethera slot is empty, filled, or confirmed, we only up-date from filled to confirmed when the slot valueis implicitly or explicitly confirmed.
An additionalfunction maps the user speech-act task pairs to aform that can be interpreted by the dialogue man-ager.
Post-mapping user responses are made up ofone or more of the following types of utterance:(1) Stay quiet, (2) Provide 1 or more slot values,(3) Yes, (4) No, (5) Ask for help, (6) Hang-up, (7)Null (out-of-domain or no ASR hypothesis).The quality of the 4 and 5-gram user sim-ulations has been established through a varietyof metrics and against the behaviour of the ac-tual users of the COMMUNICATOR systems, see(Georgila et al, 2005a).2.4.1 Limitations of the user simulationsThe user and ASR simulations are a fundamen-tally important factor in determining the nature ofthe learned strategies.
For this reason we shouldnote the limitations of the n-gram simulations usedhere.
A first limitation is that we cannot be surethat the COMMUNICATOR training data is suffi-ciently complete, and a second is that the n-gramsimulations only use a window of n moves inthe dialogue history.
This second limitation be-comes a problem when the user simulation?s cur-rent move ought to take into account somethingthat occurred at an earlier stage in the dialogue.This might result in the user simulation repeating aslot value unnecessarily, or the chance of an ASRerror for a particular word being independent ofwhether the same word was previously recognisedcorrectly.
The latter case means we cannot sim-ulate for example, a particular slot value alwaysbeing liable to misrecognition.
These limitationswill affect the nature of the learned strategies.
Dif-ferent state features may assume more or less im-portance than they would if the simulations weremore realistic.
This is a point that we will return toin the analysis of the experimental results.
In fu-ture work we will use the more accurate user sim-ulations recently developed following (Georgila etal., 2005a) and we expect that these will improveour results still further.3 ExperimentsFirst we learned strategies with the 4-gram usersimulation and tested with the 5-gram simula-tion, and then did the reverse.
We experimentedwith different feature sets, exploring whether bet-ter strategies could be learned by adding limitedcontext features.
We used two baselines for com-parison:?
The performance of the original COMMUNI-CATOR systems in the data set (Walker et al,2001).?
An RL baseline dialogue manager learnedusing only slot-status features i.e.
for eachof slots 1?
4, is the slot empty, filled or con-firmed?We then learned two further strategies:?
Strategy 2 (UDM) was learned by adding theuser?s last dialogue move to the state.?
Strategy 3 (USDM) was learned by addingboth the user and system?s last dialoguemoves to the state.The possible system and user dialogue moveswere those given in sections 2.1 and 2.4 respec-tively, and the reward function was that describedin section 2.2.3.1 The COMMUNICATOR data baselineWe computed the scores for the original hand-coded COMMUNICATOR systems as was doneby (Henderson et al, 2005), and we call this the?HLG05?
score.
This scoring function is basedon task completion and dialogue length rewards asdetermined by the PARADISE evaluation (Walkeret al, 2000).
This function gives 25 points foreach slot which is filled, another 25 for each thatis confirmed, and deducts 1 point for each sys-tem action.
In this case the maximum possiblescore is 197 i.e.
200 minus 3 actions, (the sys-tem prompts the user, the user replies by filling allfour of the slots in one turn, and the system thenconfirms all four slots and offers the flight).
Theaverage score for the 1242 dialogues in the COM-MUNICATOR dataset where the aim was to filland confirm only the same four slots as we haveused here was 115.26.
The other COMMUNICA-TOR dialogues involved different slots relating toreturn flights, hotel-bookings and car-rentals.4 ResultsFigure 1 tracks the improvement of the 3 learnedstrategies for 50000 training dialogues with the 4-gram user simulation, and figure 2 for 50000 train-ing dialogues with the 5-gram simulation.
Theyshow the average reward (according to the func-tion of section 2.2) obtained by each strategy overintervals of 1000 training dialogues.Table 1 shows the results for testing the strate-gies learned after 50000 training dialogues (thebaseline RL strategy, strategy 2 (UDM) and strat-egy 3 (USDM)).
The ?a?
strategies were trainedwith the 4-gram user simulation and tested with188Features Av.
Score HLG05 Filled Slots Conf.
Slots Length4 ?
5 gram = (a)RL Baseline (a) Slots status 51.67 190.32 100 100 ?9.68RL Strat 2, UDM (a) + Last User DM 53.65** 190.67 100 100 ?9.33RL Strat 3, USDM (a) + Last System DM 54.9** 190.98 100 100 ?9.025 ?
4 gram = (b)RL Baseline (b) Slots status 51.4 190.28 100 100 ?9.72RL Strat 2, UDM (b) + Last User DM 54.46* 190.83 100 100 ?9.17RL Strat 3, USDM (b) + Last System DM 56.24** 191.25 100 100 ?8.75RL Baseline (av) Slots status 51.54 190.3 100 100 ?9.7RL Strat 2, UDM (av) + Last User DM 54.06** 190.75 100 100 ?9.25RL Strat 3, USDM (av) + Last System DM 55.57** 191.16 100 100 ?8.84COMM Systems 115.26 84.6 63.7 ?33.1Hybrid RL *** Information States 142.6 88.1 70.9 ?16.4Table 1: Testing the learned strategies after 50000 training dialogues, average reward achieved per dia-logue over 1000 test dialogues.
(a) = strategy trained using 4-gram and tested with 5-gram; (b) = strategytrained with 5-gram and tested with 4-gram; (av) = average; * significance level p < 0.025; ** signifi-cance level p < 0.005; *** Note: The Hybrid RL scores (here updated from (Henderson et al, 2005))are not directly comparable since that system has a larger action set and fewer policy constraints.the 5-gram, while the ?b?
strategies were trainedwith the 5-gram user simulation and tested withthe 4-gram.
The table also shows average scoresfor the strategies.
Column 2 contains the averagereward obtained per dialogue by each strategy over1000 test dialogues (computed using the functionof section 2.2).The 1000 test dialogues for each strategy weredivided into 10 sets of 100.
We carried out t-testsand found that in both the ?a?
and ?b?
cases, strat-egy 2 (UDM) performs significantly better thanthe RL baseline (significance levels p < 0.005and p < 0.025), and strategy 3 (USDM) performssignificantly better than strategy 2 (UDM) (signif-icance level p < 0.005).
With respect to averageperformance, strategy 2 (UDM) improves over theRL baseline by 4.9%, and strategy 3 (USDM) im-proves by 7.8%.
Although there seem to be onlynegligible qualitative differences between strate-gies 2(b) and 3(b) and their ?a?
equivalents, theformer perform slightly better in testing.
This sug-gests that the 4-gram simulation used for testingthe ?b?
strategies is a little more reliable in fillingand confirming slot values than the 5-gram.The 3rd column ?HLG05?
shows the averagescores for the dialogues as computed by the re-ward function of (Henderson et al, 2005).
This isdone for comparison with that work but also withthe COMMUNICATOR data baseline.
Using theHLG05 reward function, strategy 3 (USDM) im-proves over the original COMMUNICATOR sys-tems baseline by 65.9%.
The components makingup the reward are shown in the final 3 columnsof table 1.
Here we see that all of the RL strate-gies are able to fill and confirm all of the 4 slotswhen conversing with the simulated COMMUNI-CATOR users.
The only variation is in the aver-age length of dialogue required to confirm all fourslots.
The COMMUNICATOR systems were of-ten unable to confirm or fill all of the user slots,and the dialogues were quite long on average.
Asstated in section 2.4.1, the n-gram simulations donot simulate the case of a particular user goal ut-terance being unrecognisable for the system.
Thiswas a problem that could be encountered by thereal COMMUNICATOR systems.Nevertheless, the performance of all the learnedstrategies compares very well to the COMMUNI-CATOR data baseline.
For example, in an averagedialogue, the RL strategies filled and confirmed allfour slots with around 9 actions not including of-fering the flight, but the COMMUNICATOR sys-tems took an average of around 33 actions per di-alogue, and often failed to complete the task.With respect to the hybrid RL result of (Hen-derson et al, 2005), shown in the final row of thetable, Strategy 3 (USDM) shows a 34% improve-ment, though these results are not directly compa-rable because that system uses a larger action setand has fewer constraints (e.g.
it can ask ?how mayI help you??
at any time, not just at the start of adialogue).Finally, let us note that the performance of theRL strategies is close to optimal, but that there issome room for improvement.
With respect to theHLG05 metric, the optimal system score would be197, but this would only be available in rare caseswhere the simulated user supplies all 4 slots in the189-120-100-80-60-40-20020400  5  10 15 20 25 30 35 40 45 50AverageRewardNumber of Dialogues (Thousands)Training With 4-gramBaselineStrategy 2Strategy 3Figure 1: Training the dialogue strategies with the4-gram user simulationfirst utterance.
With respect to the metric we haveused here (with a ?5 per system turn penalty), theoptimal score is 85 (and we currently score an av-erage of 55.57).
Thus we expect that there arestill further improvments that can be made to morefully exploit the dialogue context (see section 4.3).4.1 Qualitative AnalysisBelow are a list of general characteristics of thelearned strategies:1.
The reinforcement learner learns to query thedatabase only in states where all four slotshave been confirmed.2.
With sufficient exploration, the reinforce-ment learner learns not to pass the call to ahuman operator in any state.3.
The learned strategies employ implicit confir-mations wherever possible.
This allows themto fill and confirm the slots in fewer turns thanif they simply asked the slot values and thenused explicit confirmation.4.
As a result of characteristic 3, which slotscan be asked and implicitly confirmed at thesame time influences the order in which thelearned strategies attempt to fill and confirmeach slot, e.g.
if the status of the third slot is?filled?
and the others are ?empty?, the learnerlearns to ask for the second or fourth slot-120-100-80-60-40-20020400  5  10 15 20 25 30 35 40 45 50AverageRewardNumber of Dialogues (Thousands)Training With 5-gramBaselineStrategy 2Strategy 3Figure 2: Training the dialogue strategies with the5-gram user simulationrather than the first, since it can implicitlyconfirm the third while it asks for the secondor fourth slots, but it cannot implicitly con-firm the third while it asks for the first slot.This action is not available (see section 2.1).4.2 Emergent behaviourIn testing the UDM strategy (2) filled and con-firmed all of the slots in fewer turns on aver-age than the RL baseline, and strategy 3 (USDM)did this in fewer turns than strategy 2 (UDM).What then were the qualitative differences be-tween the three strategies?
The behaviour of thethree strategies only seems to really deviate whena user response fails to fill or confirm one or moreslots.
Then the baseline strategy?s state has notchanged and so it will repeat its last dialoguemove, whereas the state for strategies 2 (UDM)and 3 (USDM) has changed and as a result, thesemay now try different actions.
It is in such circum-stances that the UDM strategy seems to be moreeffective than the baseline, and strategy 3 (USDM)more effective than the UDM strategy.
In figure 3we show illustrative state and learned action pairsfor the different strategies.
They relate to a sit-uation where the first user response(s) in the di-alogue has/have failed to fill a single slot value.NB: here ?emp?
stands for ?empty?
and ?fill?
for?filled?
and they appear in the first four state vari-ables, which stand for slot states.
For strategy 2(UDM), the fifth variable represents the user?s last190dialogue move, and the for strategy 3 (USDM), thefifth variable represents the system?s last dialoguemove, and the sixth, the user?s last dialogue move.BASELINE STRATEGYState:[emp,emp,emp,emp]Action: askSlot2STRATEGY 2 (UDM)State:[emp,emp,emp,emp,user(quiet)]Action: askSlot3State:[emp,emp,emp,emp,user(null)]Action: askSlot1STRATEGY 3 (USDM)State:[emp,emp,emp,emp,askSlot3,user(quiet)]Action: askSlot3State:[emp,emp,emp,emp,askSlot3,user(null)]Action: giveHelpState:[emp,emp,emp,emp,giveHelp,user(quiet)]Action: askSlot3State:[emp,emp,emp,emp,giveHelp,user(null)]Action: askSlot3Figure 3: Examples of the different learned strate-gies and emergent behaviours: focus switching(for UDM) and giving help (for USDM)Here we can see that should the user responsescontinue to fail to provide a slot value, the base-line?s state will be unchanged and so the strategywill simply ask for slot 2 again.
The state for strat-egy 2 (UDM) does change however.
This strategyswitches focus between slots 3 and 1 depending onwhether the user?s last dialogue move was ?null?
or?quiet?
NB.
As stated in section 2.4, ?null?
meansout-of-domain or that there was no ASR hypothe-sis.
Strategy 3 (USDM) is different again.
Knowl-edge of the system?s last dialogue move as wellas the user?s last move has enabled the learner tomake effective use of the ?give help?
action, ratherthan to rely on switching focus.
When the user?slast dialogue move is ?null?
in response to the sys-tem move ?askSlot3?, then the strategy uses the?give help?
action before returning to ask for slot 3again.
The example described here is not the onlyexample of strategy 2 (UDM) employing focusswitching while strategy 3 (USDM) prefers to usethe ?give help?
action when a user response failsto fill or confirm a slot.
This kind of behaviour instrategies 2 and 3 is emergent dialogue behaviourthat has been learned by the system rather than ex-plicitly programmed.4.3 Further possibilities for improvementover the RL baselineFurther improvements over the RL baseline mightbe possible with a wider set of system actions.Strategies 2 and 3 may learn to make more ef-fective use of additional actions than the baselinee.g.
additional actions that implicitly confirm oneslot whilst asking another may allow more of theswitching focus described in section 4.1.
Otherpossible additional actions include actions that askfor or confirm two or more slots simultaneously.In section 2.4.1, we highlighted the fact that then-gram user simulations are not completely real-istic and that this will make certain state featuresmore or less important in learning a strategy.
Thushad we been able to use even more realistic usersimulations, including certain additional contextfeatures in the state might have enabled a greaterimprovement over the baseline.
Dialogue lengthis an example of a feature that could have madea difference had the simulations been able to sim-ulate the case of a particular goal utterance beingunrecognisable for the system.
The reinforcementlearner may then be able to use the dialogue lengthfeature to learn when to give up asking for a par-ticular slot value and make a partially completedatabase query.
This would of course require areward function that gave some reward to partiallycomplete database queries rather than the all-or-nothing reward function used here.5 Conclusion and Future WorkWe have used user simulations that are n-grammodels learned from COMMUNICATOR data toexplore reinforcement learning of full dialoguestrategies with some ?high-level?
context infor-mation (the user and and system?s last dialoguemoves).
Almost all previous work (e.g.
(Singhet al, 2002; Pietquin, 2004; Scheffler and Young,2001)) has included only low-level informationin state representations.
In contrast, the explo-ration of very large state spaces to date relies on a?hybrid?
supervised/reinforcement learning tech-nique, where the reinforcement learning elementhas not been shown to significantly improve poli-cies over the purely supervised case (Henderson etal., 2005).We presented our experimental environment,the reinforcement learner, the simulated users,and our methodology.
In testing with the sim-ulated COMMUNICATOR users, the new strate-gies learned with higher-level (i.e.
dialogue move)information in the state outperformed the low-level RL baseline (only slot status information)191by 7.8% and the original COMMUNICATOR sys-tems by 65.9%.
These strategies obtained morereward than the RL baseline by filling and con-firming all of the slots with fewer system turns onaverage.
Moreover, the learned strategies showinteresting emergent dialogue behaviour such asmaking effective use of the ?give help?
action andswitching focus to different subtasks when the cur-rent subtask is proving problematic.In future work, we plan to use even more realis-tic user simulations, for example those developedfollowing (Georgila et al, 2005a), which incorpo-rate elements of goal-directed user behaviour.
Wewill continue to investigate whether we can main-tain tractability and learn superior strategies as weadd incrementally more high-level contextual in-formation to the state.
At some stage this maynecessitate using a generalisation method such aslinear function approximation (Henderson et al,2005).
We also intend to use feature selectiontechniques (e.g.
CFS subset evaluation (Rieser andLemon, 2006)) on in order to determine whichcontextual features this suggests are important.We will also carry out a more direct comparisonwith the hybrid strategies learned by (Hendersonet al, 2005).
In the slightly longer term, we willtest our learned strategies on humans using a fullspoken dialogue system.
We hypothesize that thestrategies which perform the best in terms of taskcompletion and user satisfaction scores (Walker etal., 2000) will be those learned with high-level di-alogue context information in the state.AcknowledgementsThis work is supported by the ESRC and the TALKproject, www.talk-project.org.ReferencesJohn L. Austin.
1962.
How To Do Things With Words.Oxford University Press.Johan Bos, Ewan Klein, Oliver Lemon, and TetsushiOka.
2003.
Dipper: Description and formalisationof an information-state update dialogue system ar-chitecture.
In 4th SIGdial Workshop on Discourseand Dialogue, Sapporo.Herbert H. Clark.
1996.
Using Language.
CambridgeUniversity Press.Weiland Eckert, Esther Levin, and Roberto Pieraccini.1997.
User modeling for spoken dialogue systemevaluation.
In IEEE Workshop on Automatic SpeechRecognition and Understanding.Matthew Frampton and Oliver Lemon.
2005.
Rein-forcement Learning Of Dialogue Strategies UsingThe User?s Last Dialogue Act.
In IJCAI workshopon Knowledge and Reasoning in Practical DialogueSystems.Kallirroi Georgila, James Henderson, and OliverLemon.
2005a.
Learning User Simulations for In-formation State Update Dialogue Systems.
In In-terspeech/Eurospeech: the 9th biennial conferenceof the International Speech Communication Associ-ation.Kallirroi Georgila, Oliver Lemon, and James Hender-son.
2005b.
Automatic annotation of COMMUNI-CATOR dialogue data for learning dialogue strate-gies and user simulations.
In Ninth Workshop on theSemantics and Pragmatics of Dialogue (SEMDIAL:DIALOR).James Henderson, Oliver Lemon, and KallirroiGeorgila.
2005.
Hybrid Reinforcement/SupervisedLearning for Dialogue Policies from COMMUNI-CATOR data.
In IJCAI workshop on Knowledge andReasoning in Practical Dialogue Systems,.Staffan Larsson and David Traum.
2000.
Informationstate and dialogue management in the TRINDI Dia-logue Move Engine Toolkit.
Natural Language En-gineering, 6(3-4):323?340.Esther Levin and Roberto Pieraccini.
1997.
Astochastic model of computer-human interactionfor learning dialogue strategies.
In Eurospeech,Rhodes,Greece.Olivier Pietquin.
2004.
A Framework for Unsuper-vised Learning of Dialogue Strategies.
Presses Uni-versitaires de Louvain, SIMILAR Collection.Verena Rieser and Oliver Lemon.
2006.
Using ma-chine learning to explore human multimodal clarifi-cation strategies.
In Proc.
ACL.Konrad Scheffler and Steve Young.
2001.
Corpus-based dialogue simulation for automatic strategylearning and evaluation.
In NAACL-2001 Work-shop on Adaptation in Dialogue Systems, Pittsburgh,USA.John R. Searle.
1969.
Speech Acts.
Cambridge Uni-versity Press.Satinder Singh, Diane Litman, Michael Kearns, andMarilyn Walker.
2002.
Optimizing dialogue man-agement with reinforcement learning: Experimentswith the NJFun system.
Journal of Artificial Intelli-gence Research (JAIR).Richard Sutton and Andrew Barto.
1998.
Reinforce-ment Learning.
MIT Press.Marilyn A. Walker, Candace A. Kamm, and Diane J.Litman.
2000.
Towards Developing General Mod-els of Usability with PARADISE.
Natural Lan-guage Engineering, 6(3).Marilyn A. Walker, Rebecca J. Passonneau, andJulie E. Boland.
2001.
Quantitative and Qualita-tive Evaluation of Darpa Communicator Spoken Di-alogue Systems.
In Meeting of the Association forComputational Linguistics, pages 515?522.192
