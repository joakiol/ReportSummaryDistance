Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 124?132,Portland, OR, USA, 24 June 2011. c?2011 Association for Computational LinguisticsA Study of Academic Collaboration in Computational Linguistics withLatent Mixtures of AuthorsNikhil Johri, Daniel RamageDepartment of Computer ScienceStanford UniversityStanford, CA, USADaniel A. McFarlandSchool of EducationStanford UniversityStanford, CA, USA{njohri2,dramage,dmcfarla,jurafsky}@stanford.eduDaniel JurafskyDepartment of LinguisticsStanford UniversityStanford, CA, USAAbstractAcademic collaboration has often been atthe forefront of scientific progress, whetheramongst prominent established researchers, orbetween students and advisors.
We suggest atheory of the different types of academic col-laboration, and use topic models to computa-tionally identify these in Computational Lin-guistics literature.
A set of author-specifictopics are learnt over the ACL corpus, whichranges from 1965 to 2009.
The models aretrained on a per year basis, whereby only pa-pers published up until a given year are usedto learn that year?s author topics.
To determinethe collaborative properties of papers, we use,as a metric, a function of the cosine similarityscore between a paper?s term vector and eachauthor?s topic signature in the year precedingthe paper?s publication.
We apply this metricto examine questions on the nature of collabo-rations in Computational Linguistics research,finding that significant variations exist in theway people collaborate within different sub-fields.1 IntroductionAcademic collaboration is on the rise as single au-thored work becomes less common across the sci-ences (Rawlings and McFarland, 2011; Jones et al,2008; Newman, 2001).
In part, this rise can be at-tributed to the increasing specialization of individualacademics and the broadening in scope of the prob-lems they tackle.
But there are other advantages tocollaboration, as well: they can speed up produc-tion, diffuse knowledge across authors, help trainnew scientists, and are thought to encourage greaterinnovation.
Moreover, they can integrate scholarlycommunities and foster knowledge transfer betweenrelated fields.
But all collaborations aren?t the same:different collaborators contribute different material,assume different roles, and experience the collabo-ration in different ways.
In this paper, we presenta new frame for thinking about the variation in col-laboration types and develop a computational metricto characterize the distinct contributions and roles ofeach collaborator within the scholarly material theyproduce.The topic of understanding collaborations has at-tracted much interest in the social sciences over theyears.
Recently, it has gained traction in computerscience, too, in the form of social network analysis.Much work focuses on studying networks formedvia citations (Radev et al, 2009; White and Mccain,1998), as well as co-authorship links (Nascimentoet al, 2003; Liu et al, 2005).
However, these worksfocus largely on the graphical structure derived frompaper citations and author co-occurrences, and lesson the textual content of the papers themselves.
Inthis work, we examine the nature of academic col-laboration using text as a primary component.We propose a theoretical framework for determin-ing the types of collaboration present in a docu-ment, based on factors such as the number of es-tablished authors, the presence of unestablished au-thors and the similarity of the established authors?past work to the document?s term vector.
These col-laboration types attempt to describe the nature of co-authorships between students and advisors (e.g.
?ap-prentice?
versus ?new blood?)
as well as those solelybetween established authors in the field.
We presenta decision diagram for classifying papers into thesetypes, as well as a description of the intuition behindeach collaboration class.124We explore our theory with a computationalmethod to categorize collaborative works into theircollaboration types using an approach based on topicmodeling, where we model every paper as a la-tent mixture of its authors.
For our system, we useLabeled-LDA (LLDA (Ramage et al, 2009)) to trainmodels over the ACL corpus for every year of thewords best attributed to each author in all the papersthey write.
We use the resulting author signaturesas a basis for several metrics that can classify eachdocument by its collaboration type.We qualitatively analyze our results by examin-ing the categorization of several high impact papers.With consultation from prominent researchers andtextbook writers in the field, we demonstrate that oursystem is able to differentiate between the varioustypes of collaborations in our suggested taxonomy,based only on words used, at low but statisticallysignificant accuracy.
We use this same similarityscore to analyze the ACL community by sub-field,finding significant deviations.2 Related WorkIn recent years, popular topic models such as La-tent Dirichlet Allocation (Blei et al, 2003) havebeen increasingly used to study the history of sci-ence by observing the changing trends in term basedtopics (Hall et al, 2008), (Gerrish and Blei, 2010).In the case of Hall et al, regular LDA topic mod-els were trained over the ACL anthology on a peryear basis, and the changing trends in topics werestudied from year to year.
Gerrish and Blei?s workcomputed a measure of influence by using DynamicTopic Models (Blei and Lafferty, 2006) and study-ing the change of statistics of the language used in acorpus.These models propose interesting ideas for utiliz-ing topic modeling to understand aspects of scien-tific history.
However, our primary interest, in thispaper, is the study of academic collaboration be-tween different authors; we therefore look to learnmodels for authors instead of only documents.
Pop-ular topic models for authors include the Author-Topic Model (Rosen-Zvi et al, 2004), a simpleextension of regular LDA that adds an additionalauthor variable over the topics.
The Author-TopicModel learns a distribution over words for eachtopic, as in regular LDA, as well as a distributionover topics for each author.
Alternatively, LabeledLDA (Ramage et al, 2009), another LDA variation,offers us the ability to directly model authors as top-ics by considering them to be the topic labels for thedocuments they author.In this work, we use Labeled LDA to directlymodel probabilistic term ?signatures?
for authors.
Asin (Hall et al, 2008) and (Gerrish and Blei, 2010),we learn a new topic model for each year in the cor-pus, allowing us to account for changing author in-terests over time.3 Computational MethodologyThe experiments and results discussed in this paperare based on a variation of the LDA topic model runover data from the ACL corpus.3.1 DatasetWe use the ACL anthology from years 1965 to 2009,training over 12,908 papers authored by over 11,355unique authors.
We train our per year topic mod-els over the entire dataset; however, when evaluatingour results, we are only concerned with papers thatwere authored by multiple individuals as the otherpapers are not collaborations.3.2 Latent Mixture of AuthorsEvery abstract in our dataset reflects the work, tosome greater or lesser degree, of all the authors ofthat work.
We model these degrees explicitly us-ing a latent mixture of authors model, which takesits inspiration from the learning machinery of LDA(Blei et al, 2003) and its supervised variant La-beled LDA (Ramage et al, 2009).
These modelsassume that documents are as a mixture of ?topics,?which themselves are probability distributions overthe words in the vocabulary of the corpus.
LDAis completely unsupervised, assuming that a latenttopic layer exists and that each word is generatedfrom one underlying topic from this set of latent top-ics.
For our purposes, we use a variation of LDA inwhich we assume each document to be a latent mix-ture of its authors.
Unlike LDA, where each docu-ment draws a multinomial over all topics, the latentmixture of authors model we use restricts a docu-ment to only sample from topics corresponding to125its authors.
Also, unlike models such as the Author-Topic Model (Rosen-Zvi et al, 2004), where au-thors are modeled as distributions over latent top-ics, our model associates each author to exactly onetopic, modeling authors directly as distributions overwords.Like other topic models, we will assume a genera-tive process for our collection of D documents froma vocabulary of size V .
We assume that each docu-ment d has Nd terms and Md authors from a set ofauthors A.
Each author is described by a multino-mial distribution ?a over words V , which is initiallyunobserved.
We will recover for each document ahidden multinomial ?
(d) of length Md that describeswhich mixture of authors?
best describes the doc-ument.
This multinomial is in turn drawn from asymmetric Dirichlet distribution with parameter ?restrict to the set of authors ?
(d) for that paper.
Eachdocument?s words are generated by first picking anauthor zi from ?
(d) and then drawing a word fromthe corresponding author?s word distribution.
For-mally, the generative process is as follows:?
For each author a, generate a distribution ?a overthe vocabulary from a Dirichlet prior ??
For each document d, generate a multinomial mix-ture distribution ?
(d) ?
Dir(?.1?(d))?
For each document d,?
For each i ?
{1, ..., Nd}?
Generate zi ?
{?
(d)1 , ..., ?
(d)Md} ?Mult(?(d))?
Generate wi ?
{1, ..., V } ?Mult(?zi)We use Gibbs sampling to perform inference inthis model.
If we consider our authors as a labelspace, this model is equivalent to that of LabeledLDA (Ramage et al, 2009), which we use for in-ference in our model, using the variational objec-tive in the open source implementation1.
After in-ference, our model discovers the distribution overterms that best describes that author?s work in thepresence of other authors.
This distribution servesas a ?signature?
for an author and is dominated bythe terms that author uses frequently across collabo-rations.
It is worth noting that this model constrainsthe learned ?topics?
to authors, ensuring directly in-terpretable results that do not require the interpreta-1http://nlp.stanford.edu/software/tmt/tion of a latent topic space, such as in (Rosen-Zvi etal., 2004).To imbue our model with a notion of time, wetrain a separate LLDA model for each year in thecorpus, training on only those papers written beforeand during the given year.
Thus, we have separate?signatures?
for each author for each year, and eachsignature only contains information for the specificauthor?s work up to and including the given year.Table 1 contains examples of such term signaturescomputed for two authors in different years.
The topterms and their fractional counts are displayed.4 Studying CollaborationsThere are several ways one can envision to differen-tiate between types of academic collaborations.
Wefocus on three factors when creating collaborationlabels, namely:?
Presence of unestablished authors?
Similarity to established authors?
Number of established authorsIf an author whom we know little about is presenton a collaborative paper, we consider him or her tobe a new author.
We threshold new authors by thenumber of papers they have written up to the pub-lication year of the paper we are observing.
De-pending on whether this number is below or above athreshold value, we consider an author to be estab-lished or unestablished in the given year.Similarity scores are measured using the trainedLLDA models described in Section 3.2.
For anygiven paper, we measure the similarity of the pa-per to one of its (established) authors by calculatingthe cosine similarity of the author?s signature in theyear preceding the paper?s publication to the paper?sterm-vector.Using the aforementioned three factors, we definethe following types of collaborations:?
Apprenticeship Papers are authored by one ormore established authors and one or more un-established authors, such that the similarity ofthe paper to more than half of the establishedauthors is high.
In this case, we say that thenew author (or authors) was an apprentice of126Philipp Koehn, 2002 Philipp Koehn, 2009 Fernando Pereira, 1985 Fernando Pereira, 2009Terms Counts Terms Counts Terms Counts Terms Countsword 3.00 translation 69.78 grammar 14.99 type 40.00lexicon 2.00 machine 34.67 phrase 10.00 phrase 30.89noun 2.00 phrase 26.85 structure 7.00 free 23.14similar 2.00 english 23.86 types 6.00 grammar 23.10translation 1.29 statistical 19.51 formalisms 5.97 constraint 23.00purely 0.90 systems 18.32 sharing 5.00 logical 22.41accuracy 0.90 word 16.38 unification 4.97 rules 21.72Table 1: Example term ?signatures?
computed by running a Labeled LDA model over authors in the ACL corpus on aper year basis: top terms for two authors in different years are shown alongside their fractional counts.the established authors, continuing in their lineof work.?
New Blood Papers are authored by one estab-lished author and one or more unestablished au-thors, such that the similarity of the paper to theestablished author is low.
In this case, we saythat the new author (or authors) provided newideas or worked in an area that was dissimilar tothat which the established author was workingin.?
Synergistic Papers are authored only by es-tablished authors such that it does not heavilyresemble any authors?
previous work.
In thiscase, we consider the paper to be a product ofsynergy of its authors.?
Catalyst Papers are similar to synergisticones, with the exception that unestablished au-thors are also present on a Catalyst Paper.
Inthis case, we hypothesize that the unestablishedauthors were the catalysts responsible for get-ting the established authors to work on a topicdissimilar to their previous work.The decision diagram in Figure 1 presents an easyway to determine the collaboration type assigned toa paper.5 Quantifying CollaborationsFollowing the decision diagram presented in Figure1 and using similarity scores based on the valuesreturned by our latent author mixture models (Sec-tion 3.2), we can deduce the collaboration type toassign to any given paper.
However, absolute cate-gorization requires an additional thresholding of au-thor similarity scores.
To avoid the addition of anarbitrary threshold, instead of directly categorizingpapers, we rank them based on the calculated sim-ilarity scores on three different spectra.
To facili-tate ease of interpretation, the qualitative exampleswe present are drawn from high PageRank papers ascalculated in (Radev et al, 2009).5.1 The MaxSim ScoreTo measure the similarity of authors?
previous workto a paper, we look at the cosine similarity betweenthe term vector of the paper and each author?s termsignature.
We are only interested in the highest co-sine similarity score produced by an author, as ourcategories do not differentiate between papers thatare similar to one author and papers that are sim-ilar to multiple authors, as long as high similarityto any single author is present.
Thus, we chooseour measure, the MaxSim score, to be defined as:maxa?estcos(asig, paper)We choose to observe the similarity scores onlyfor established authors as newer authors will nothave enough previous work to produce a stable termsignature, and we vary the experience threshold byyear to account for the fact that there has been a largeincrease in the absolute number of papers publishedin recent years.Depending on the presence of new authors andthe number of established authors present, each pa-per can be placed into one of the three spectra: theApprenticeship-New Blood spectrum, the Synergyspectrum and the Apprenticeship-Catalyst spectrum.Apprenticeship and Low Synergy papers are thosewith high MaxSim scores, while low scores indicateNew Blood, Catalyst or High Synergy papers.5.2 ExamplesThe following are examples of high impact papersas they were categorized by our system:127Figure 1: Decision diagram for determining the collaboration type of a paper.
A minimum of 1 established author isassumed.5.2.1 Example: Apprenticeship PaperImprovements in Phrase-Based Statistical Ma-chine Translation (2004)by Richard Zens and Hermann NeyThis paper had a high MaxSim score, indicating highsimilarity to established author Hermann Ney.
Thiscategorizes the paper as an Apprenticeship Paper.5.2.2 Example: New Blood PaperThumbs up?
Sentiment Classification usingMachine Learning Techniques (2002)by Lillian Lee, Bo Pang and ShivakumarVaithyanathanThis paper had a low MaxSim score, indicatinglow similarity to established author Lillian Lee.This categorizes the paper as a New Blood Pa-per, with new authors Bo Pang and ShivakumarVaithyanathan.
It is important to note here that newauthors do not necessarily mean young authors orgrad students; in this case, the third author on thepaper was experienced, but in a field outside ofACL.5.2.3 Example: High Synergy PaperCatching the Drift: Probabilistic ContentModels, with Applications to Generation andSummarization (2003)by Regina Barzilay and Lillian LeeThis paper had low similarity to both establishedauthors on it, making it a highly synergistic paper.Synergy here indicates that the work done on thispaper was mostly unlike work previously done byeither of the authors.5.2.4 Example: Catalyst PaperAnswer Extraction (2000)by Steven Abney, Michael Collins, Amit SinghalThis paper had a very low MaxSim score, as wellas the presence of an unestablished author, makingit a Catalyst Paper.
The established authors (froman ACL perspective) were Abney and Collins, whileSinghal was from outside the area and did not havemany ACL publications.
The work done in this pa-per focused on information extraction, and was un-like that previously done by either of the ACL estab-lished authors.
Thus, we say that in this case, Sing-hal played the role of the catalyst, getting the othertwo authors to work on an area that was outside oftheir usual range.5.3 Evaluation5.3.1 Expert AnnotationTo quantitatively evaluate the performance ofour system, we prepared a subset of 120 papersfrom among the highest scoring collaborative papersbased on the PageRank metric (Radev et al, 2009).Only those papers were selected which had at least a128single established author.
One expert in the field wasasked to annotate each of these papers as being ei-ther similar or dissimilar to the established authors?prior work given the year of publication, the title ofthe publication and its abstract.We found that the MaxSim scores of papers la-beled as being similar to the established authorswere, on average, higher than those labeled as dis-similar.
The average MaxSim score of papers anno-tated as low MaxSim collaboration types (High Syn-ergy, New Blood or Catalyst papers) was 0.15488,while that of papers labeled as high MaxSim types(Apprentice or Low Synergy papers) had a meanMaxSim score of 0.21312.
The MaxSim scores ofthe different sets were compared using a t-test, andthe difference was found to be statistically signifi-cant with a two-tailed p-value of 0.0041.Framing the task as a binary classification prob-lem, however, did not produce very strong results.The breakdown of the papers and success rates (asdetermined by a tuned threshold) can be seen in Ta-ble 3.
The system had a relatively low success rate of62.5% in its binary categorization of collaborations.5.3.2 First Author PredictionStudies have suggested that authorship order,when not alphabetical, can often be quantified andpredicted by those who do the work (Sekercioglu,2008).
Through a survey of all authors on a sam-ple of papers, Slone (1996) found that in almost allmajor papers, ?the first two authors are said to ac-count for the preponderance of work?.
We attemptto evaluate our similarity scores by checking if theyare predictive of first author.Though similarity to previous work is only a smallcontributor to determining author order, we find thatusing the metric of cosine similarity between authorsignatures and papers performs significantly betterat determining the first author of a paper than ran-dom chance.
Of course, this feature alone isn?t ex-tremely predictive, given that it?s guaranteed to givean incorrect solution in cases where the first authorof a paper has never been seen before.
To solve theproblem of first author prediction, we would haveto combine this with other features.
We chose twoother features - an alphabetical predictor, and a pre-dictor based on the frequency of an author appearingas first author.
Although we don?t show the regres-Predictor Feature AccuracyRandom Chance 37.35%Author Signature Similarity 45.23%Frequency Estimator 56.09%Alphabetical Ordering 43.64%Table 2: Accuracy of individual features at predicting thefirst author of 8843 paperssion, we do explore these two other features and findthat they are also predictive of author order.Table 2 shows the performance of our predictionfeature alongside the others.
The fact that it beatsrandom chance shows us that there is some infor-mation about authorial efforts in the scores we havecomputed.6 ApplicationsA number of questions about the nature of collabo-rations may be answered using our system.
We de-scribe approaches to some of these in this section.6.1 The Hedgehog-Fox ProblemFrom the days of the ancient Greek poetArchilochus, the Hedgehog-Fox analogy hasbeen frequently used (Berlin, 1953) to describe twodifferent types of people.
Archilochus stated that?The fox knows many things; the hedgehog one bigthing.?
A person is thus considered a ?hedgehog?if he has expertise in one specific area and focusesall his time and resources on it.
On the other hand,a ?fox?
is a one who has knowledge of severaldifferent fields, and dabbles in all of them instead offocusing heavily on one.We show how, using our computed similarityscores, one can discover the hedgehogs and foxesof Computational Linguistics.
We look at the top100 published authors in our corpus, and for eachauthor, we compute the average similarity score theauthor?s signature has to each of his or her papers.Note that we start taking similarity scores into ac-count only after an author has published 5 papers,thereby allowing the author to stablize a signaturein the corpus and preventing the signature from be-ing boosted by early papers (where author similaritywould be artificially high, since the author was new).We present the authors with the highest averagesimilarity scores in Table 4.
These authors can be129Collaboration Type True Positives False Positives AccuracyNew Blood, Catalyst or High Synergy Papers 43 23 65.15%Apprentice or Low Synergy Papers 32 22 59.25%Overall 75 45 62.50%Table 3: Evaluation based on annotation by one expertconsidered the hedgehogs, as they have highly sta-ble signatures that their new papers resemble.
Onthe other hand, Table 5 shows the list of foxes, whohave less stable signatures, presumably because theymove about in different areas.Author Avg.
Sim.
ScoreKoehn, Philipp 0.43456Pedersen, Ted 0.41146Och, Franz Josef 0.39671Ney, Hermann 0.37304Sumita, Eiichiro 0.36706Table 4: Hedgehogs - authors with the highest averagesimilarity scoresAuthor Avg.
Sim.
ScoreMarcus, Mitchell P. 0.09996Pustejovsky, James D. 0.10473Pereira, Fernando C. N. 0.14338Allen, James F. 0.14461Hahn, Udo 0.15009Table 5: Foxes - authors with the lowest average similar-ity scores6.2 Similarity to previous work by sub-fieldsBased on the different types of collaborations dis-cussed in, a potential question one might ask iswhich sub-fields are more likely to produce appren-tice papers, and which will produce new blood pa-pers.
To answer this question, we first need to deter-mine which papers correspond to which sub-fields.Once again, we use topic models to solve this prob-lem.
We first filter out a subset of the 1,200 highestpage-rank collaborative papers from the years 1980to 2007.
We use a set of topics built by running astandard LDA topic model over the ACL corpus, inwhich each topic is hand labeled by experts based onthe top terms associated with it.
Given these topic-term distributions, we can once again use the cosinesimilarity metric to discover the highly associatedTopic ScoreStatistical Machine Translation 0.2695Prosody 0.2631Speech Recognition 0.2511Non-Statistical Machine Translation 0.2471Word Sense Disambiguation 0.2380Table 6: Topics with highest MaxSim scores (papers aremore similar to the established authors?
previous work)Topic ScoreQuestion Answering 0.1335Sentiment Analysis 0.1399Dialog Systems 0.1417Spelling Correction 0.1462Summarization 0.1511Table 7: Topics with lowest MaxSim scores (papers areless similar to the established authors?
previous work)topics for each given paper from our smaller sub-set, by choosing topics with cosine similarity abovea certain threshold ?
(in this case 0.1).Once we have created a paper set for each topic,we can measure the ?novelty?
for each paper by look-ing at their MaxSim score.
We can now find the av-erage MaxSim score for each topic.
This averagesimilarity score gives us a notion of how similar tothe established author (or authors) a paper in the subfield usually is.
Low scores indicate that new bloodand synergy style papers are more common, whilehigher scores imply more non-synergistic or appren-ticeship style papers.
This could indicate that topicswith lower scores are more open ended, while thosewith higher scores require more formality or train-ing.
The top five topics in each category are shownin Tables 6 and 7.
The scores of the papers fromthe two tables were compared using a t-test, and thedifference in the scores of the two tables was foundto be very statistically significant with a two-tailed pvalue << 0.01.1307 Discussion and Future WorkOnce we have a robust way to score different kindsof collaborations in ACL, we can begin to use thesescores as a quantitative tool to study phonemena inthe computational linguistics community.
With ourcurrent technique, we discovered a number of nega-tive results; however, given that our accuracy in bi-nary classification of categories is relatively low, wecannot state for sure whether these are true negativeresults or a limitation of our model.7.1 Tentative Negative ResultsAmong the questions we looked into, we found thefollowing results:?
There was no signal indicating that authorswho started out as new blood authors were anymore or less likely to survive than authors whostarted out as apprentices.
Survival was mea-sured both by the number of papers eventuallypublished by the author as well as the year ofthe author?s final publication; however, calcu-lations by neither measure correlated with theMaxSim scores of the authors?
early papers.?
Each author in the corpus was labeled for gen-der.
Gender didn?t appear to differentiate howpeople collaborated.
In particular, there was nodifference between men and women based onhow they started their careers.
Women and menare equally likely to begin as new blood authorsas they are to begin as apprentices.?
On a similar note, established male authors areequally likely to partake in new blood or ap-prentice collaborations as their female counter-parts.?
No noticeable difference existed between aver-age page rank scores of a certain categorizationof collaborative papers (e.g.
high synergy pa-pers vs. low synergy papers).It is difficult to conclusively demonstrate negativeresults, particularly given that our MaxSim scoresare by themselves not particularly strong discrimi-nators in the binary classification tasks.
We considerthese findings to be tentative and an opportunity toexplore in the future.8 ConclusionNot everything we need to know about academiccollaborations can be found in the co-authorshipgraph.
Indeed, as we have argued, not all typesof collaborations are equal, as embodied by differ-ing levels of seniority and contribution from eachco-author.
In this work, we have taken a first steptoward computationally modeling these differencesusing a latent mixture of authors model and ap-plied it to our own field, Computational Linguistics.We used the model to examine how collaborativeworks differ by authors and subfields in the ACL an-thology.
Our model quantifies the extent to whichsome authors are more prone to being ?hedgehogs,?whereby they heavily focus on certain specific ar-eas, whilst others are more diverse with their fieldsof study and may be analogized with ?foxes.
?We also saw that established authors in certainsubfields have more deviation from their previouswork than established authors in different subfields.This could imply that the former fields, such as?Sentiment Analysis?
or ?Summarization,?
are moreopen to new blood and synergistic ideas, while otherlatter fields, like ?Statistical Machine Translation?or ?Speech Recognition?
are more formal or re-quire more training.
Alternatively, ?Summarization?or ?Sentiment Analysis?
could just still be youngerfields whose language is still evolving and being in-fluenced by other subareas.This work takes a first step toward a new way ofthinking about the contributions of individual au-thors based on their network of areas.
There aremany design parameters that still exist in this space,including alternative text models that take into ac-count richer structure and, hopefully, perform bet-ter at discriminating between the types of collabo-rations we identified.
We intend to use the ACL an-thology as our test bed for continuing to work on tex-tual models of collaboration types.
Ultimately, wehope to apply the lessons we learn on modeling thisfamiliar corpus to the challenge of answering large-scale questions about the nature of collaboration asembodied by large scale publication databases suchas ISI and Pubmed.131AcknowledgmentsThis research was supported by NSF grant NSF-0835614 CDI-Type II: What drives the dynamic cre-ation of science?
We thank our anonymous review-ers for their valuable feedback and the members ofthe Stanford Mimir Project team for their insightsand engagement.ReferencesIsaiah Berlin.
1953.
The hedgehog and the fox: An essayon Tolstoy?s view of history.
Simon & Schuster.David M. Blei and John D. Lafferty.
2006.
Dynamictopic models.
In Proceedings of the 23rd internationalconference on Machine learning, ICML ?06, pages113?120, New York, NY, USA.
ACM.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022.Sean M. Gerrish and David M. Blei.
2010.
A language-based approach to measuring scholarly impact.
In Pro-ceedings of the 26th International Conference on Ma-chine Learning.David Hall, Daniel Jurafsky, and Christopher D. Man-ning.
2008.
Studying the history of ideas usingtopic models.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,EMNLP ?08, pages 363?371, Stroudsburg, PA, USA.Association for Computational Linguistics.B.
F. Jones, S. Wuchty, and B. Uzzi.
2008.
Multi-university research teams: Shifting impact, geography,and stratification in science.
Science, 322:1259?1262,November.Xiaoming Liu, Johan Bollen, Michael L. Nelson, andHerbert Van de Sompel.
2005.
Co-authorship net-works in the digital library research community.
In-formation Processing & Management, 41(6):1462 ?1480.
Special Issue on Infometrics.Mario A. Nascimento, Jo?rg Sander, and Jeffrey Pound.2003.
Analysis of sigmod?s co-authorship graph.
SIG-MOD Rec., 32:8?10, September.M.
E. J. Newman.
2001.
From the cover: The struc-ture of scientific collaboration networks.
Proceedingsof the National Academy of Science, 98:404?409, Jan-uary.Dragomir R. Radev, Pradeep Muthukrishnan, and VahedQazvinian.
2009.
The acl anthology network cor-pus.
In Proceedings of the 2009 Workshop on Textand Citation Analysis for Scholarly Digital Libraries,NLPIR4DL ?09, pages 54?61, Stroudsburg, PA, USA.Association for Computational Linguistics.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009.
Labeled lda: a super-vised topic model for credit attribution in multi-labeledcorpora.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing:Volume 1 - Volume 1, EMNLP ?09, pages 248?256.Craig M. Rawlings and Daniel A. McFarland.
2011.
In-fluence flows in the academy: Using affiliation net-works to assess peer effects among researchers.
SocialScience Research, 40(3):1001 ?
1017.Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, andPadhraic Smyth.
2004.
The author-topic model for au-thors and documents.
In Proceedings of the 20th con-ference on Uncertainty in artificial intelligence, UAI?04, pages 487?494.Cagan H. Sekercioglu.
2008.
Quantifying coauthor con-tributions.
Science, 322(5900):371.RM Slone.
1996.
Coauthors?
contributions to majorpapers published in the ajr: frequency of undeservedcoauthorship.
Am.
J.
Roentgenol., 167(3):571?579.Howard D. White and Katherine W. Mccain.
1998.
Visu-alizing a discipline: An author co-citation analysis ofinformation science.
Journal of the American Societyfor Information Science, 49:1972?1995.132
