IIIiIi1IIIIIIIIIIIAlgor i thms for  Ontological  Mediat ionAlistair E. Campbell !
and Stuart C. Shapiro 2l,XDepartment of Computer ScienceAnd 2Center for Cognitive ScienceState University of New York at Buffalo226 Bell Hall, Box 602000Buffalo, New York 14260-2000aec@cs, buffalo, edu, shapiro@cs, buffalo, eduAbstractWe lay the foundation for ontological mediation as amethod for resolving communication difficulties result-ing from different ontologies.
The notion of hierarchicalrelations enables a theory of orientadon or direction ofontoiogies to be presented.
We describe an ontologcialmediator as being able to think about (or conceptualize)concepts from ontologies and find equivalences betweenthem.
Algorithms for finding the meanings of unfamiliarwords by asking questions are introduced and evaluatedexperimentally.1 IntroductionClearly, in order for communication between computa-tional agents to be truly successful, each agent must beable to understand what the other says.
Presently, this in-volves deciding ahead of time on the following:I. a syntax and semantics for the language in whichthey communicate (a popular one is KIF (Gene-sereth, 1995)).
andII.
an ontology, or domain conceptualization that setsforth the terminology they may use, along with rela-tions that hold between the concepts that these termsdenote.One way to make sure that both of these things happenis to develop a single ontology with a single set of termsfor each domain, and require that all communicating par-ties use only that ontology in their dialogue.
We call thisthe single ontology proposal.However.
the reality is that various agents can andoften do use different erms to denote elements in acommon domain, and this presents a pervasive problem:Words that are not in one agent's ontology will be com-pletely unintelligible when presented by another agent,even if they have agreed on a common language of com-munication (an interlingua) head of time, and even iftheir ontologies are similar, even significantly overlap-ping.This problem often occurs because the agents' ontolo-gies are designed for different purposes.
We should re-ject the single ontology proposal because it is impossi-ble to implement: even the designers of the ontologiesthemselves cannot agree on terminology.
Worse yet, they102often cannot agree on a taxonomization f the domaininto represented concepts.
For example, notice the differ-ences between upper levels of the CYC (Lenat and Guha,1990; Lenat, 199S), and Penman (Bateman et al, 1990)ontologies shown in figure 1.CYC ~-t~ M  p p v~pFigure 1: CYC and Penman Upper LevelsMoreover, useful knowledge resources designed be-fore a standard ontology is adopted will not be able toparticipate in information interchange without he assis-tance of some sort of translator or mediator to facilitatedialogue with other agents.
Since these tools are expen-sive to develop and maintain, this effectively eliminateslegacy systems as competitive knowledge resources atlarge.Also, without unacceptable forced compliance to thestandard ontologies, anyone can create new and poten-tially useful knowledge agents with which communica-tion is impossible ven if they do use some conventionallanguage and communication protocols.Instead we advocate an approach where agent design-ers are free to use whatever ontology makes sense forthem.
and when problems of terminology arise, they areI!IIl|IIIIIIiIIIIsolved by an ontologkal mediator.1.1 VocabularyLet's suppose that agent A and agent B want to commu-nicate about some domain.
They have decided on an in-terlingua, a common communication language, and eachhas adopted an ontology, or domain conceptualization.This means that hey have an established vocabulary fromwhich neither may stray.But how crucial is it that both agents have exactly thesame vocabulary?
People don't have exactly the samevocabulary, et we communicate v ry well most of thetime.
When misunderstandings occur, they are often eas-ily cleared up.
Legacy systems and most current knowl-edge resources are incapable of clearing up miscommuni-cations because they lack the intelligence to do so.
Worktoward giving information agents this capability is pro-gressing, but in the interim, machines can't communi-cate.1.2 MediationOne promising approach to this problem is to build spe-cialized agents which facilitate communication betweencommunicants who have adopted ifferent ontologies, oreven no formal ontology at all.
Indeed, given that agentshave adopted an interlingua and communication protocol,they can try to communicate.
The mediator then tries torepair miscommunications as they occur.We are concerned not with the process of detectingmisunderstandings, butrather with ways to resolve com-munication problems.
We focus on the problem of agentshaving different vocabularies.
In that context, it is possi-ble for a speaker (S) to use a word (W) unfamiliar to alistener (L).2 MediatorWe have designed an ontological mediator, an agent ca-pable of reasoning about he ontoiogies of two communi-"cating agents, or communicants, learning about what Wmeans for S, and looking for an ontological translation(W') that means for L, the same thing in the domain thatW means for S.3 FundamentalsBefore proceeding with a discussion of algorithms for on-tological mediation, we first set forth some assumptionsand definitions, and make some clarifying remarks.3.1 Common words mean the same thing.We make the following simplifying assumption:Rule 1 If two agents are communicating about he samedomain, then if both of them know some word, then itmeans the same thing to both of them.The rationale for this assumption is that when agentsare communicating, each implicitly assumes that a wordused by the other means the same thing as it does to it.103People don't go around wondering whether each wordthey hear really means what they think it does, and theircommunication with other people is usually free of error.Of course, this assumption can lead to problems whencommon words really don't mean the same thing.
Thenit becomes the agents' duty to detect miscommunicadon.Work is being done in this area (see, for example (McRoy,1996)) but this is not the focus of our current research.We are more concerned with using mediation techniquesto find correspondences b tween concepts in ontologies.This presupposes detection, since the agents have calleda mediator to help them.3.20ntologiesThe word "ontology" is used by many researchers tomean a variety of similar but distinct hings.
Withoutmaking a strong or precise statement as to what ontolo-gies should be necessarily, we present some issues withrespect to ontologies that our research addresses.3.2.1 Words vs. ConceptsContrary to many ontology designers, who do not seemto distinguish between word (or symbol) and concept, wetake an ontology to be an organization of an agent's con-cepts by some set of ontological relations.
A concept is aparticular agent's conceptualization of an element of thedomain of discourse, and each concept can be denoted byone or more words.
This way, words can be shared be-tween agents, but concepts cannot.
Naturally, we requirea mapping between words and concepts to support rea-soning about agents" concepts.
For a given agent, we cur-rently assume a 1-1, onto mapping between concepts andwords.
Presently.
we do not have algorithms that give aproper treatment of polysemy or synonomy of words forontological mediation.3.2.2 ConceptsIf an ontological mediator isto find words in one ontologythat have the same meaning as words in another ontology,the mediator must be thinking about he concepts in thoseontologies.
The notion of a "concept" is very slippery,and frequently means different things to different people.Therefore, for the purpose of describing these algorithmsand their underlying theory, we make the following defi-nitions.!.
For any agent A and domain element O, ifA knowsabout or can think about O, then there exists a men-tal representation C in A's mind, which represents O.We write \[\[C\]\] A = O,2.
Concept: The mental entity C which exists in themind of an agent and serves to represent some do-main element for that agent.3.
OM-Concept: The mental entity C' which exists inthe mind of the ontological mediator that is thinkingabout C. that is.
thinking about some concept in themind of another agent, and how that concept mightfit into the agent's ontology.IiIII_IIIIIIIIII!IIINote one important implication of the distinction:The "'domain" of thought for an ontological mediatoris not the same as the communicants' domain.
Rather,the OM's domain is that of concepts in the communi-cants' ontologies.
While the communicants are "thinkingabout" elements of their own domain, the OM is think-ing about hose concepts invoked by the communicant'sthinking.
Thus, whenever agent A uses a word W, it ex-presses ome concept C, which in turn represents somedomain entity O forA.
Therefore, the first time OM hearsA use W, OM builds in its own mind an ore-concept C torepresent that concept.
Hence \[\[C'~oM = C, and of courseClIA = O.3.3 Ontological RelationsAn ontological relation is simply any relation commonlyused in the organization of ontoiogies.
Whether a rela-tion is truly ontological is a matter of opinion, but for ex-ample, some kind ofsubclass/superclass relation pair isalmost always used to form a taxonomic hierarchy.3.3.1 Hierarchical generalizer,s, and specializersA hierarchical ontological relation is any ontological re-lation that organizes concepts into a hierarchy, taxonomy,or similar structure.
Hierarchical relations are related tobut distinct from transitive relations.
For example, thetransitive relation ancestor is related to the hierarchicalrelation parent.The hierarchical ontological relations are important forontological mediation because they form the hierarchiesorganizing the concepts in the ontology.
When a relationis hierarchical, we can think of it as having an directionor orientation, either as a generalizer, elating a conceptto concepts above it (e.g., its "superconcepts"), and mov-ing "up" the hierarchy, or as a specializer, elating acon-cept to concepts below it (its "subconcepts"), and mov-ing "'down".
For example, directSuperClass is a hierar-chical generalizer, while directSubClass is a hierarchicalspecializer.The "up" and "down" directions are merely conven-tions, of course, in that they relate to the way we tendto draw pictures of hierarchies as trees.
We start at someroot concept or concepts and fan out via some hierarchi-cal specializer.
How do we know that directSubClassis the specializer (down direction) and that dircctSuper.Class is the generalizer (up direction)?
We expect fan-out with specializers, that is, specializers tend to relateseveral subconcepts to a single superconcepts.
For a pairof hierarchical relations R and R ~ (the converse of R), weexamine the sets of concepts X = {xl3yR(x,y)} and Y ={YI3xR(x,Y)} ?
lflY\] > IXI then R is a specializer, other-wise R is a generalizer.If R is a hierarchical relation, then R ~ is its converse.i.e.. R(CI ,Cz) - R*(C2,Ct ).
It follows naturally that ifRis a generalizer, then R ~ is a specializer, and vice versa.We say that a concept P is a "parent" (with respect to R)ofanotherconcept CifR(C, P) for some hierarchical gen-eralizer R. Likewise, we say that a concept C is a "'child"104of P if R(P, C) for some hierarchical specializer R.3.4 Relation notationBy convention, R(X,Y) means that Y bears the R re-lation to X, for example, we say subclass(animal, dog)to mean that dog is a subclass of animal.
We choosethis convention to reflect the question-asking approachwhere questions are asked of the domain and answersare given in the range.
For example, in "What are thesubclasses of animal?"
we have the question in termsof a relation: subclass(animal, ?x), or functionally, as insubclass(animal) =?x.3.5 Tangled HierarchiesFor many ontologies, the taxonomic hierarchy is struc-tured as a tree (or as a forest), where any given conceptcan have at most one superconcept.
Other ontologiescan be tangled hierarchies with multiple inheritance.
Thetechniques of ontological mediation presented here do al-low for mediation with tangled hierarchies.4 A lgor i thmsIn this section, we discuss various algorithms for onto-logical mediation.
We define word(C,A) to be the wordthat agent A uses to express concept C, and concept(W,A)to be the ore-concept representing the concept that W ex-presses forA, if one exists, undefined otherwise.
Also, letknows(A, W) be true if and only if concept(W,A) is de-fined, false otherwise.We define the following operations:?
Ontology(A) : return the set of ore-concepts that OMcurrently uses to represent concepts in A's ontology.?
Agent(C) : returns a representation f the agent hatC is an ore-concept for.
This representation is usedto direct questions to the agent.The following algorithm exists in support of ontologi-cal mediation algorithms by asking questions of the com-municants as needed to establish OM's knowledge ofontological relationships.
Evaluate takes a relation R.and an ore-concept C. and returns a set of om-conceptssuch that Agent(C) believes R(~C~Age,~(c), ~C~A~,~(c~ )for each om-concept C' in the set.
Results are cached sothat multiple calls to evaluate the same question do notresult in multiple queries issued.Algorithm EvaluaCe(R,C):  sec of om-concepti.
let A~AEen~(C )2.
Build a query Q in A's inter l ingua toask ''What bears relacion R toword(C,Agent(C))? '
'3.
Issue Q to Agent(C}.
The response cothe query wil l  be a set of words S.4.
let Answer~{}5. for V6 S do6.
assert R(C,concep~(V,A))7. lee Answer~Anwswer+concep~(V,A)IIIIIIIIIIIIIiIIII8.
end for9.
return AnswerThe first two algorithms below each take as argumentsa word W used by agent S and not known by agent L, aridreturn a set of ore-concepts representing possible onto-logical translations.
More formally, when X is the ore-concept for which word(~X\]oM,S) = W, given any ore-concept Y in the set returned by the algorithm, there isreason to believe that \[~X\]oM~s = ~Y\]oM\]I,.4.1 Recursive over one relation (MedTax)The first algorithm explores an ontology along one hierar-chical relation, given by parameterR.
It is called MedTaxbecause an obvious choice for R is either SubClass orSuperClass, which will result in exploration of the tax-onomic hierarchies of the ontologies.Algorithm MedTax (W,S,L,R): set ofom-conceptI.
Zet2.
for P E EvaluaTe(R, concept(W,S) do3.
if knows(i, word(P,S)) then4.
let Q (-- (\] + concep~(word(P,S), L)5. else6.
let O +- QU MedTax(word(P,S),S,L,R)7. end if8.
end for9I0.
for P E Q doZl.
for C E EvaluaTe(Rl, P) do12.
if not knows(S,word(C,L) then13.
F+-F+C14.
end if15.
end for16.
end for17.
return F4.2 Multiple relations(MedOnt)We can extend this algorithm to handle multiple hierar-chical ontological relations, such as ParCWhole.
Now,each hierarchical ontological relation forms its own hier-arch), in which the unknown word is situated in the lis-tener's ontology.Again, we find the translation of a word used by S butunknown to L by starting at the unknown word in thespeaker's ontology, then crawling up (or down) the hier-archies of the speaker to points where ontological trans-lations of the word at those points has been made al-ready, (or is easy to make immediately because the lis-tener knows the word) then crawl back down (or up) thelistener's hierarchies.Algorithm MedOnc (W,S,L):set of om-concepcl.lec C ~ {}1052 for each relat ion3.R 6 HierarchicalRelaTions do4.
let G+-GUMedTax(W,S,L,R)5.end for6.return GNote that MedOnt is a union-tbrming algorithm,rather than an intersection-forming one.
That is, it re-turns ore-concepts that are found by exploring via one ormore hierarchical relations, rather than restricted to hav-ing been found through every relation.
It returns a set ofcandidates for ontological translation, and does not cal-culate which is the best one.4.3 Choosing the best candidate (MedCount)This algorithm, unlike the previous algorithms, returns apair: (1) the single ore-concept representing the listener'sconcept which the mediator believes to be equivalent tothe speaker's concept expressed by an unknown word W,and (2) a measure of the mediator's confidence in this on-tological translation.We introduce the notation A =r  B to mean that conceptA is known by OM to be equivalent to concept B with con-fidence measure Y.Algorithm:123456789.i0.Ii.12.131415.16.MedCouat (W, S, L) :om-concept x Realif knows(L,W) thenreturn (concept(W,L), i)end i fif concept(W,S) =r X thenreturn (X,Y)end i flet AllCandidates ~ {}for R E HierarchicalRelations dolet CandidaTes +- MedTax(W,S,L,R)let CandidaTesByRelaTions +-CandidatesByRelat ions ?
Candidateslet AllCandidates +-AllCandidaTes U Candidatesend forchoose C E AllCandidaTes such that thenumber of sets in CandidatesByRelat ionsthat contain C is maximized.let Y+- the number of sets inwhich C occurs.asser~ concepT(W,S)~vCreturn (C,Y)5 Exper iments  w i th  WordNetThe WordNet (Miller et al, 1993; Miller.
1995) lexicalontology organizes concepts called "synsets," which aresets of words considered synonymous in a certain con-text.
Primarily we are interested in some of WordNet'shierarchies, including the taxonomic hierarchy:IIIIIIIIIIiIIII5.1 VariablesSince WordNet is such a large ontology, we controlledtwo independent binary variables in the experiment, Syn-onyms, and AllowAllSenses.
These are explained below.5.1.1 SynonymsOne approach to WordNet is to consider each synset as aseparate mental concept in the mind of the agent who usesWordNet as its ontology.
When the agent expresses thatconcept, he uses one or more of the words in the synseLIf  so the agent supports ynonomy.
However, decidingwhich synonym to use is difficult o say the least, and maybe a reason why many if not most ontologies don't sup-port synonomy.$.1.2 AHowAESensesThe agent playing the role of WordNet receives queriesfrom the ontological mediator, then in turn makes an ap-propriate access to its WordNet component.
Each queryreturns a sequence of zero or more groups of output,one for each relevant synset the word was in.
If AI-IowAllSenses was not set, the agent only reported the in-formation from the first block, ignoring the others.
Con-versely, if AllowAilSenses was set, then the agent re-ported information from all synsets.5.2 ExperimentWe devised two agents, appropriately named "AMERI-CAW" and "BRITISH" because they were constructedto use the corresponding dialect of the English language.Both agents use the WordNet ontology, but are restrictedfrom using words strictly from the other's dialect (theypretend not to know them}.
The dialect restrictionscome from the Cambridge Encyclopedia of the EnglishLanguage, (Crystal, 1995, p. 309).
Naturally we onlyused word pairs where both words exist in WordNet inthe same synset.
We chose 57 word pairs where bothwords were present in WordNet and members of thesame snyset, for example, ( lift, elevator),(patience, solitaire), (holiday,vacation), (draughts, checkers).We then tested the MedCount algorithm mediatingfrom an American speaker to a British listener, and thenvice versa from a British speaker to an American listener.There were four Hierarchical relations used: SubClass,Superclass, PartOf, and HasPart.When the mediator returns the correct word from theword pair, that is called a success.
When the mediator re-turns some other word, that is called an error, and whenthe mediator can not find any word for an ontologicaltranslation that is called a miss.Table 1 summarizes the performance of the HedCoua1:algorithm under combinations of AllowAIISenses (Sen)and Synonyms (Syn), showing the numbers of successes,errors, misses, success rate (Success~57 x 100%), the av-erage certainty over all successes (Cer).
and average CPUtime.
when the speaker is "'BRITISH" and the listener is"AMERICAN."
Table 2 gives the same data for when thespeaker is"'AMERICAN" and the listener is "BRITISH".106Sen Syn Suc Err Mis Rat Cer CPUOff Off 28 2 27 49% .85 0.97sOff On 33 3 21 58% .79 2.40sOn Off 39 5 13 68% .82 3.03sOn On 40 7 10 70% .85 6.82sTable h British Speaker/American ListenerSen Syn Sue Err Mis Rat Cer CPUOff Off 19 2 36 33% .85 1.03sOff On 35 3 19 61% .78 2.38sOn Off 4 7 46 7% .81 2.20sOn On 42 4 !
1 74% .82 5.22sTable 2: American Speaker/British Listener6 Ana lys i sThe first remarkable difference between an Americanspeaker vs. a British speaker is that the success rate plum-mets when Synonyms i  turned off.
This reflects a biasin WordNet tbr putting the American words first in thesynsets.
If the British word is at the end.
it will not bereported when Synonyms i  on, thus it will not be found.and the miss rate increases.Another eason for seemingly low success rates evenwith both Synonyms and AllowAllSenses on is due toa sort of polysemy inherent in dealing with WordNet.While WordNet isn't really polysemous in its underly-ing data structure since synsets provide a crisp distinc-tion internally, any agent--human or machine--that usesthe ordinary external interface to WordNet makes queriesusing single words that may have multiple senses (mean-ings) in WordNet, and thereby may uncover data on morethan just one concept.\[t stands to reason that an agent would perform on-tological mediation more correctly if that agent weresophisticated enough to understand that WordNet's re-sposes (or the responses of any source that recognizesterms as synonymous) may include multiple distinctsynsets, that each synset contains multiple synonymousterms, and that these should be organized as one concept,not many.
While this sophistication is the subject of on-going research, presently the Ontolgical Mediator dealswith single terms only.
and cannot distinguish among on-tology data for multiple word senses.
Thus errors occurwhen there are too many translation candidates and thewrong one is picked.7 D iscuss ion  and  Future  WorkThe Ontological Mediator asks appropriate questions of aspeaker and listener to find words in the listener's ontol-ogy it believes mean the same as words in the speaker's.We have demonstrated that ontological mediation is apromising technique for assisting other agents with com-munication.
After successfully testing algorithms onIIIIIIIIIIII1IIIIIImostly identical ontol0gies we are are prepared to pro-ceed to mediation tasks involving agents with greatercontrasting ontologies.
We expect that since many of themisses and errors are due to WordNet's polysemous na-ture, performance will improve when dealing with non-polysemous ontoiogies.Long response times are due mainly to the size anddensity of the WordNet ontology.
The ontological me-diator running t, ledCount: must explore a sizable por-tion of each agent's ontology to arrive at its conclusion.Even though much of this exploration i volves commonwords, OM still must establish many equivalences be-tween ore-concepts that are expressed by the same word.Because WordNet is inherently a polysemous ontology,OM must explore several dead ends.
For example, indiscovering (successfully) that "pushcart" is synonymouswith "stroller," OM must look at senses of the word "car-tinge" which then brings in all the parts of a typewriter.Work on pruning this sort of search is being considered.Meanwhile we plan to apply ontological mediation al-gorithms to other ontoiogies including the Unified Med-ical Language System (UMLS) (Humphreys and Lind-berg, 1993).
Mediating between two different ontolo-gies, UMLS and WordNet will lead to new ideas for onto-logical mediation algorithms.
Another experiment couldinvolve human subjects, for example, those searching adatabase and are looking for just the fight keyword to findsome target.
We expect these xperiments to lead to morerobust ontological mediation algorithms.8 AcknowledgementsThis research was supported inpart by Rome Laboratoryand the Defense Advanced Research Projects Agency un-der USAF contract F30602-93-C-0177.ReferencesI.
A. Bateman, R. T. Kasper, J. D. Moore, and R. A.Whitney.
1990.
A general organization of knowledgefor natural language processing: The penman uppermodel.
Technical report, USC/Information SciencesInstitute.David Crystal.
1995.
The Cambridge Enc3'clopedia ofthe English Language.
Cambridge University Press.Michael R. Genesereth.
1995.
KnowledgeInterchange Formal Available at URL:http://logic.stanford.edu/kif.htmi, March.B.
L. Humphreys and D. A.
B. Lindberg.
1993.
The umlsproject: Making the conceptual connection betweenusers and the information they need.
Bulletin of theMedical Library Association, 81 (2): 170.Doug Lenat and R.V.
Guha.
1990.
Building LargeKnowlede-Based Systems: Representation a d Infer-ence in the CYC Project.
Addison-Wesley.Doug Lenat.
1995.
Cyc: A large-scale investmentin knowledge infrastructure.
Communications of theACM.
38(11):33-38.
Nov.107Susan McRoy, editor.
1996.
AAAI-96 Workshop on De-tecting, Preventing, and Repairing Human.MachineMiscommunication.George A. Miller, Richard Beckwith, Chris-tiane Fellbaum.
Derek Gross, and KatherineMiller.
1993.
Introduction to WordNet: AnOn-line Lexical Database.
Available at URL:http://clarity.princeton.edu:80/" wn/.George A. Miller.
1995.
WordNet: A Lexical Databasefor English.
Communications of A CM, 38(11):39-41,November.
