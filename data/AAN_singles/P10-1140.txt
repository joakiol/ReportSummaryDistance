Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1376?1385,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsGenerating Fine-Grained Reviews of Songs From Album ReviewsSwati Tata and Barbara Di EugenioComputer Science DepartmentUniversity of Illinois, Chicago, IL, USA{stata2 | bdieugen}@uic.eduAbstractMusic Recommendation Systems oftenrecommend individual songs, as opposedto entire albums.
The challenge is to gen-erate reviews for each song, since only fullalbum reviews are available on-line.
Wedeveloped a summarizer that combines in-formation extraction and generation tech-niques to produce summaries of reviews ofindividual songs.
We present an intrinsicevaluation of the extraction components,and of the informativeness of the sum-maries; and a user study of the impact ofthe song review summaries on users?
de-cision making processes.
Users were ableto make quicker and more informed deci-sions when presented with the summary ascompared to the full album review.1 IntroductionIn recent years, the personal music collection ofmany individuals has significantly grown due tothe availability of portable devices like MP3 play-ers and of internet services.
Music listeners arenow looking for techniques to help them man-age their music collections and explore songs theymay not even know they have (Clema, 2006).Currently, most of those electronic devices followa Universal Plug and Play (UPNP) protocol (UPN,2008), and can be used in a simple network, onwhich the songs listened to can be monitored.
Ourinterest is in developing a Music RecommendationSystem (Music RS) for such a network.Commercial web-sites such as Amazon (www.amazon.com) and Barnes and Nobles (www.bnn.com) have deployed Product Recommen-dation Systems (Product RS) to help customerschoose from large catalogues of products.
MostProduct RSs include reviews from customers whobought or tried the product.
As the number ofreviews available for each individual product in-creases, RSs may overwhelm the user if they makeall those reviews available.
Additionally, in somereviews only few sentences actually describe therecommended product, hence, the interest in opin-ion mining and in summarizing those reviews.A Music RS could be developed along the linesof Product RSs.
However, Music RSs recom-mend individual tracks, not full albums, e.g.
seewww.itunes.com.
Summarizing reviews be-comes more complex: available data consists ofalbum reviews, not individual song reviews (www.amazon.com, www.epinions.com).
Com-ments about a given song are fragmented all overan album review.
Though some web-sites likewww.last.fm allow users to comment on indi-vidual songs, the comments are too short (a fewwords such as ?awesome song?)
to be counted asa full review.In this paper, after presenting related work andcontrasting it to our goals in Section 2, we discussour prototype Music RS in Section 3.
We devoteSection 4 to our summarizer, that extracts com-ments on individual tracks from album reviewsand produces a summary of those comments foreach individual track recommended to the user.In Section 5, we report two types of evaluation: anintrinsic evaluation of the extraction components,and of the coverage of the summary; an extrinsicevaluation via a between-subject study.
We foundthat users make quicker and more informed deci-sions when presented with the song review sum-maries as opposed to the full album review.2 Related WorkOver the last decade, summarization has becomea hot topic for research.
Quite a few systems weredeveloped for different tasks, including multi-document summarization (Barzilay and McKe-own, 2005; Soubbotin and Soubbotin, 2005; Nas-tase, 2008).1376What?s not to get?
Yes, Maxwell, and Octopus are abit silly!
........?Something?
and ?Here Comes The Sun?
are two ofGeorge?s best songs ever (and ?Something?
may bethe single greatest love song ever).
?Oh Darling?
isa bluesy masterpiece with Paul screaming............?Come Together?
contains a great riff, but he ended upgetting sued over the lyrics by Chuck Berry......Figure 1: A sample review for the album ?Abbey Road?Whereas summarizing customer reviews canbe seen as multi-document summarization, anadded necessary step is to first extract the mostimportant features customers focus on.
Hence,summarizing customer reviews has mostly beenstudied as a combination of machine learningand NLP techniques (Hu and Liu, 2004; Ga-mon et al, 2005).
For example, (Hu and Liu,2004) use associative mining techniques to iden-tify features that frequently occur in reviewstaken from www.epinions.com and www.amazon.com.
Then, features are paired to thenearest words that express some opinion on thatfeature.
Most work on product reviews focuseson identifying sentences and polarity of opinionterms, not on generating a coherent summary fromthe extracted features, which is the main goalof our research.
Exceptions are (Carenini et al,2006; Higashinaka et al, 2006), whose focus wason extracting domain specific ontologies in orderto structure summarization of customer reviews.Summarizing reviews on objects different fromproducts, such as restaurants (Nguyen et al,2007), or movies (Zhuang et al, 2006), has alsobeen tackled, although not as extensively.
Weare aware of only one piece of work that focuseson music reviews (Downie and Hu, 2006).
Thisstudy is mainly concerned with identifying de-scriptive patterns in positive or negative reviewsbut not on summarizing the reviews.2.1 Summarizing song reviews is differentAs mentioned earlier, using album reviews forsong summarization poses new challenges:a) Comments on features of a song are embed-ded and fragmented within the album reviews, asshown in Figure 1.
It is necessary to correctly mapfeatures to songs.b) Each song needs to be identified each time itis referred to in the review.
Titles are often ab-breviated, and in different ways, even in the samereview ?
e.g.
see Octopus for Octopus?s Gardenin Figure 1.
Additionally, song titles need not benoun phrases and hence NP extraction algorithmsmiss many occurrences, as was shown by prelimi-nary experiments we ran.c) Reviewers focus on both inherent features suchas lyrics, genre and instruments, but also on people(artist, lyricist, producer etc.
), unlike in productreviews where manufacturer/designer are rarelymentioned.
This variety of features makes itharder to generate a coherent summary.3 SongRecommend: Prototype Music RSFigure 2 shows the interface of our prototype Mu-sic RS.
It is a simple interface dictated by our fo-cus on the summarization process (but it was in-formed by a small pilot study).
Moving from win-dow to window and from top to bottom:a) The top leftmost window shows different de-vices on which the user listens to songs.
Thesedevices are monitored with a UPNP control point.Based on the messages received by the controlpoint, the user activities, including the metadataof the song, are logged.b) Once the user chooses a certain song on one ofthe devices (see second window on top), we dis-play more information about the song (third topwindow); we also identify related songs from theinternet, including: other songs from the same al-bum, popular songs of the artist and popular songsof related artists, as obtained from Yahoo Music.c) The top 25 recommendations are shown in thefourth top window.
We use the SimpleKMeansClustering (Mitchell, 1997) to identify and rankthe top twenty-five songs which belong to thesame cluster and are closest to the given song.Closeness between two songs in a cluster is mea-sured as the number of attributes (album, artist etc)of the songs that match.d) When the user clicks on More Info for one ofthe recommended songs, the pop-up, bottom win-dow is displayed, which contains the summary ofthe reviews for the specific song.4 Extraction and SummarizationOur summarization framework consists of the fivetasks illustrated in Figure 3.
The first two taskspertain to information extraction, the last three torepackaging the information and generating a co-1377Figure 2: SongRecommend InterfaceFigure 3: Summarization Pipelineherent summary.
Whereas the techniques we usefor each individual step are state-of-the-art, our ap-proach is innovative in that it integrates them intoan effective end-to-end system.
Its effectiveness isshown by the promising results obtained both viathe intrinsic evaluation, and the user study.
Ourframework can be applied to any domain wherereviews of individual components need to be sum-marized from reviews of collections, such as re-views of different hotels and restaurants in a city.Our corpus was opportunistically col-lected from www.amazon.com andwww.epinions.com.
It consists of 1350album reviews across 27 albums (50 reviewsper album).
50 randomly chosen reviews wereused for development.
Reviews have noise, sincethe writing is informal.
We did not clean it, forexample we did not correct spelling mistakes.This corpus was annotated for song titles and songfeatures.
Feature annotation consists of markinga phrase as a feature and matching it with the songto which the feature is attributed.
Note that wehave no a priori inventory of features; what countsas features of songs emerged from the annotation,since annotators were asked to annotate for nounphrases which contain ?any song related term orterms spoken in the context of a song?.
Further,they were given about 5 positive and 5 negative1378What?s not to get?
Yes, <songid=3>Maxwell</song>, and <songid=5>Octopus</song> are a bit silly!
.....................<song id=2>?Something?</song> and <songid=7>?Here Comes The Sun?</song> are two of<feature id=(2,7)>George?s</feature> best songsever (and <song id=2>?Something?</song> may be......<song id=4>?Oh Darling?</song> is a <featureid=4>bluesy masterpiece</feature> with <featureid=4>Paul</feature> screaming...........<song id=1>?Come Together?</song> contains agreat <feature id=1>riff</feature>, but ...Figure 4: A sample annotated reviewexamples of features.
Figure 4 shows annotationsfor the excerpt in Figure 1.
For example inFigure 4, George, Paul, bluesy masterpiece andriff have been marked as features.
Ten randomlychosen reviews were doubly annotated for songtitles and features.
The Kappa co-efficient ofagreement on both was excellent (0.9), hence therest of the corpus was annotated by one annotatoronly.
The two annotators were considered to be inagreement on a feature if they marked the samehead of phrase and attributed it to the same song.We will now turn to describing the componenttasks.
The algorithms are described in full in (Tata,2010).4.1 Title ExtractionSong identification is the first step towards sum-marization of reviews.
We identify a string ofwords as the title of a song to be extracted froman album review if it (1) includes some or all thewords in the title of a track of that album, and (2)this string occurs in the right context.
Constraint(2) is necessary because the string of words cor-responding to the title may appear in the lyrics ofthe song or anywhere else in the review.
The stringMaxwell?s Silver Hammer counts as a title only insentence (a) below; the second sentence is a versein the lyrics:a.
Then, the wild and weird ?Maxwell?s SilverHammer.?b.
Bang, Bang, maxwell?s silver hammer camdown on her head.Similar to Named Entity Recognition (Schedl etal., 2007), our approach to song title extractionis based on n-grams.
We proceed album by al-bum.
Given the reviews for an album and the listof songs in that album, first, we build a lexicon ofall the words in the song titles.
We also segmentthe reviews into sentences via sentence boundarydetection.
All 1,2,3,4-grams for each sentence (theupper-bound 4 was determined experimentally) inthe review are generated.
First, n-grams that con-tain at least one word with an edit distance greaterthan one from a word in the lexicon are filteredout.
Second, if higher and lower order n-gramsoverlap at the same position in the same sentence,lower order n-grams are filtered out.
Third, then-grams are merged if they occur sequentially ina sentence.
Fourth, the n-grams are further fil-tered to include only those where (i) the n-gram iswithin quotation marks; and/or (ii) the first char-acter of each word in the n-gram is upper case.This filters n-grams such as those shown in sen-tence (b) above.
All the n-grams remaining at thispoint are potential song titles.
Finally, for eachn-gram, we retrieve the set of IDs for each of itswords and intersect those sets.
This intersectionalways resulted in one single song ID, since songtitles in each album differ by at least one contentword.
Recall that the algorithm is run on reviewsfor each album separately.4.2 Feature ExtractionOnce the song titles are identified in the album re-view, sentences with song titles are used as an-chors to (1) identify segments of texts that talkabout a specific song, and then (2) extract the fea-ture(s) that the pertinent text segment discusses.The first step roughly corresponds to identify-ing the flow of topics in a review.
The second stepcorresponds to identifying the properties of eachsong.
Both steps would greatly benefit from ref-erence resolution, but current algorithms still havea low accuracy.
We devised an approach that com-bines text tiling (Hearst, 1994) and domain heuris-tics.
The text tiling algorithm divides the text intocoherent discourse units, to describe the sub-topicstructure of the given text.
We found the relativelycoarse segments the text tiling algorithm providessufficient to identify different topics.An album review is first divided into seg-ments using the text tiling algorithm.
Let[seg1, seg2, ..., segk] be the segments obtained.The segments that contain potential features of asong are identified using the following heuristics:Step 1: Include segi if it contains a song title.1379These segments are more likely to contain featuresof songs as they are composed of the sentencessurrounding the song title.Step 2: Include segi+1 if segi is included andsegi+1 contains one or more feature terms.Since we have no a priori inventory of features(the feature annotation will be used for evalua-tion, not for development), we useWordNet (Fell-baum, 1998) to identify feature terms: i.e., thosenouns whose synonyms, direct hypernym or di-rect hyponym, or the definitions of any of those,contain the terms ?music?
or ?song?, or any formof these words like ?musical?, ?songs?
etc, for atleast one sense of the noun.
Feature terms excludethe words ?music?, ?song?, the artist/band/albumname as they are likely to occur across album re-views.
All feature terms in the final set of seg-ments selected by the heuristics are taken to befeatures of the song described by that segment.4.3 Sentence Partitioning and RegenerationAfter extracting the sentences containing the fea-tures, the next step is to divide the sentences intotwo or more ?sub-sentences?, if necessary.
Forexample, ?McCartney?s bouncy bass-line is espe-cially wonderful, and George comes in with an ex-cellent, minimal guitar solo.?
discusses both fea-tures bass and guitar.
Only a portion of the sen-tence describes the guitar.
This sentence canthus be divided into two individual sentences.
Re-moving parts of sentences that describe anotherfeature, will have no effect on the summary asa whole as the portions that are removed will bepresent in the group of sentences that describe theother feature.To derive n sentences, each concerning a singlefeature f , from the original sentence that coveredn features, we need to:1.
Identify portions of sentences relevant to eachfeature f (partitioning)2.
Regenerate each portion as an independent sen-tence, which we call f -sentence.To identify portions of the sentence relevant to thesingle feature f , we use the Stanford Typed De-pendency Parser (Klein and Manning, 2002; deMarnee and Manning, 2008).
Typed Dependen-cies describe grammatical relationships betweenpairs of words in a sentence.
Starting from the fea-ture term f in question, we collect all the nouns,adjectives and verbs that are directly related to itin the sentence.
These nouns, adjectives and verbs1.
?Maxwell?
is a bit silly.2.
?Octopus?
is a bit silly.3.
?Something?
is George?s best song.4.
?Here Comes The Sun?
is George?s best song.5.
?Something?
may be the single greatest love song.6.
?Oh!
Darling?
is a bluesy masterpiece.7.
?Come Together?
contains a great riff.Figure 5: f -sentences corresponding to Figure 1become the components of the new f -sentence.Next, we need to adjust their number and forms.This is a natural language generation task, specifi-cally, sentence realization.We use YAG (McRoy et al, 2003), a templatebased sentence realizer.
clause is the main tem-plate used to generate a sentence.
Slots in a tem-plate can in turn be templates.
The grammati-cal relationships obtained from the Typed Depen-dency Parser such as subject and object identifythe slots and the template the slots follows; thewords in the relationship fill the slot.
We use amorphological tool (Minnen et al, 2000) to ob-tain the base form from the original verb or noun,so that YAG can generate grammatical sentences.Figure 5 shows the regenerated review from Fig-ure 1.YAG regenerates as many f -sentences from theoriginal sentence, as many features were containedin it.
By the end of this step, for each feature fof a certain song si, we have generated a set off -sentences.
This set alo contains every originalsentence that only covered the single feature f .4.4 Groupingf -sentences are further grouped, by sub-featureand by polarity.
As concerns sub-feature group-ing, consider the following f -sentences for thefeature guitar:a. George comes in with an excellent, minimalguitar solo.b.
McCartney laid down the guitar lead for thistrack.c.
Identical lead guitar provide the rhythmicbasis for this song.The first sentence talks about the guitar solo, thesecond and the third about the lead guitar.
Thisstep will create two subgroups, with sentence a inone group and sentences b and c in another.
We1380Let [fx-s1, fx-s2, ...fx-sn] be the set of sentences forfeature fx and song SyStep 1: Find the longest common n-gram (LCN) be-tween fx-si and fx-sj for all i 6= j: LCN(fx-si, fx-sj)Step 2: If LCN(fx-si, fx-sj) contains the feature termand is not the feature term alone, fx-si and fx-sj arein the same group.Step 3: For any fx-si, if LCN(fx-si, fx-sj) for all i andj, is the feature term, then fx-si belongs to the defaultgroup for the feature.Figure 6: Grouping sentences by sub-featuresidentify subgroups via common n-grams betweenf -sentences, and make sure that only n-grams thatare related to feature f are identified at this stage,as detailed in Figure 6.
When the procedure de-scribed in Figure 6 is applied to the three sentencesabove, it identifies guitar as the longest pertinentLCN between a and b, and between a and c; andguitar lead between b and c (we do not take intoaccount linear order within n-grams, hence gui-tar lead and lead guitar are considered identical).Step 2 in Figure 6 will group b and c together sinceguitar lead properly contains the feature term gui-tar.
In Step 3, sentence a is sentence fx-si suchthat its LCN with all other sentences (b and c) con-tains only the feature term; hence, sentence a isleft on its own.
Note that Steps 2 and 3 ensurethat, among all the possible LNCs between pair ofsentences, we only consider the ones containingthe feature in question.As concerns polarity grouping, different re-views may express different opinions regarding aparticular feature.
To generate a coherent sum-mary that mentions conflicting opinions, we needto subdivide f -sentences according to polarity.We use SentiWordNet (Esuli and Sebastiani,2006), an extension of WordNet where each senseof a word is augmented with the probability ofthat sense being positive, negative or neutral.
Theoverall sentence score is based on the scores of theadjectives contained in the sentence.Since there are a number of senses for eachword, an adjective ai in a sentence is scored as thenormalized weighted scores of each sense of theadjective.
For each ai, we compute three scores,positive, as shown in Formula 1, negative and ob-Example: The lyrics are the bestAdjectives in the sentence: bestSenti-wordnet Scores of best:Sense 1 (frequency=2):positive = 0.625, negative =0 , objective = 0.375Sense 2 (frequency=1):positive = 0.75, negative = 0, objective = 0.25Polarity Scores Calculation:positive(best) = 2?0.625+1?0.75(2+1) = 0.67negative(best) = 2?0+1?0(2+1) = 0objective(best) = 2?0.375+1?0.25(2+1) = 0.33Since the sentence contains only the adjective best, itspolarity is positive, from:Max (positive(best), negative(best), objective(best))Figure 7: Polarity Calculationjective, which are computed analogously:pos(ai) =freq1 ?
pos1 + ... + freqn ?
posn(freq1 + .... + freqn)(1)ai is the ith adjective, freqj is the frequency ofthe jth sense of ai as given by Wordnet, and posjis the positive score of the jth sense of ai, as givenby SentiWordnet.
Figure 7 shows an example ofcalculating the polarity of a sentence.For an f -sentence, three scores will be com-puted, as the sum of the corresponding scores(positive, negative, objective) of all the adjectivesin the sentence.
The polarity of the sentence is de-termined by the maximum of these three scores.4.5 Selection and OrderingFinally, the generation of a coherent summary in-volves selection of the sentences to be included,and ordering them in a coherent fashion.
This stephas in input groups of f -sentences, where eachgroup pertains to the feature f , one of its subfea-tures, and one polarity type (positive, negative, ob-jective).
We need to select one sentence from eachsubgroup to make sure that all essential conceptsare included in the summary.
Note that if there arecontrasting opinions on one feature or subfeatures,one sentence per polarity will be extracted, result-ing in potentially inconsistent opinions on that fea-ture to be included in the review (we did not ob-serve this happening frequently, and even if it did,it did not appear to confuse our users).Recall that at this point, most f -sentences havebeen regenerated from portions of original sen-1381tences (see Section 4.3).
Each f -sentence in asubgroup is assigned a score which is equivalentto the number of features in the original sentencefrom which the f -sentence was obtained.
The sen-tence which has the lowest score in each subgroupis chosen as the representative for that subgroup.If multiple sentences have the lowest score, onesentence is selected randomly.
Our assumption isthat among the original sentences, a sentence thattalks about one feature only is likely to express astronger opinion about that feature than a sentencein which other features are present.We order the sentences by exploiting a musicontology (Giasson and Raimond, 2007).
We haveextended this ontology to include few additionalconcepts that correspond to features identified inour corpus.
Also, we extended each of the classesby adding the domain to which it belongs.
Weidentified a total of 20 different domains for allthe features.
For example, [saxophone,drums] be-longs to the domain Instrument, and [tone, vocals]belong to the domain Sound.
We also identifiedthe priority order in which each of these domainsshould appear in the final summary.
The order-ing of the domains is such that first we present thegeneral features of the song (e.g.
Song) domain,then present more specific domains (e.g.
Sound,Instrument).
f?sentences of a single domain formone paragraph in the final summary.
However, fea-tures domains that are considered as sub-domainsof another domain are included in the same para-graph, but are ordered next to the features of theparent domain.
The complete list of domains is de-scribed in (Tata, 2010).
f -sentences are groupedand ordered according to the domain of the fea-tures.
Figure 8 shows a sample summary when theextracted sentences are ordered via this method.
?The Song That Jane Likes?
is cute.
The songhas some nice riffs by Leroi Moore.
?The SongThat Jane Likes?
is also amazing funk number.The lyrics are sweet and loving.The song carries a light-hearted tone.
It hasa catchy tune.
The song features some nice ac-cents.
?The Song That Jane Likes?
is beautifulsong with great rhythm.
The funky beat willsurely make a move.It is a heavily acoustic guitar-based song.Figure 8: Sample summary5 EvaluationIn this section we report three evaluations, twointrinsic and one extrinsic: evaluation of the songtitle and feature extraction steps; evaluation of theinformativeness of summaries; and a user study tojudge how summaries affect decision making.5.1 Song Title and Feature ExtractionThe song title extraction and feature extraction al-gorithms (Sections 4.1 and 4.2) were manuallyevaluated on 100 reviews randomly taken from thecorpus (2 or 3 from each album).
This relativelysmall number is due to the need to conduct theevaluation manually.
The 100 reviews contained1304 occurrences of song titles and 898 occur-rences of song features, as previously annotated.1294 occurrences of song titles were correctlyidentified; additionally, 123 spurious occurrenceswere also identified.
This results in a precision of91.3%, and recall of 98%.
The 10 occurrences thatwere not identified contained either abbreviationslikeDr.
forDoctor or spelling mistakes (recall thatwe don?t clean up mistakes).Of the 898 occurrences of song features, 853were correctly identified by our feature extractionalgorithm, with an additional 41 spurious occur-rences.
This results in a precision of 95.4% and arecall of 94.9%.
Note that a feature (NP) is con-sidered as correctly identified, if its head noun isannotated in a review for the song with correct ID.As a baseline comparison, we implemented thefeature extraction algorithm from (Hu and Liu,2004).
We compared their algorithm to ours on 10randomly chosen reviews from our corpus, for atotal of about 500 sentences.
Its accuracy (40.8%precision, and 64.5% recall) is much lower thanours, and than their original results on product re-views (72% precision, and 80% recall).5.2 Informativeness of the summariesTo evaluate the information captured in the sum-mary, we randomly selected 5 or 6 songs from 10albums, and generated the corresponding 52 sum-maries, one per song ?
this corresponds to a test setof about 500 album reviews (each album has about50 reviews).
Most summary evaluation schemes,for example the Pyramid method (Harnly et al,2005), make use of reference summaries writ-ten by humans.
We approximate those gold-standard reference summaries with 2 or 3 critic re-views per album taken from www.pitchfork.1382com, www.rollingstone.com and www.allmusic.com.First, we manually annotated both critic reviewsand the automatically generated summaries forsong titles and song features.
302, i.e., 91.2%of the features identified in the critic reviews arealso identified in the summaries (recall that a fea-ture is considered as identified, if the head-noun ofthe NP is identified by both the critic review andthe summary, and attributed to the same song).
64additional features were identified, for a recall of82%.
It is not surprising that additional featuresmay appear in the summaries: even if only one ofthe 50 album reviews talks about that feature, it isincluded in the summary.
Potentially, a thresholdon frequency of feature mention could increase re-call, but we found out that even a threshold of twosignificantly affects precision.In a second evaluation, we used our FeatureExtraction algorithm to extract features from thecritic reviews, for each song whose summaryneeds to be evaluated.
This is an indirect evalu-ation of that algorithm, in that it shows it is not af-fected by somewhat different data, since the criticreviews are more formally written.
375, or 95%of the features identified in the critic reviews arealso identified in the summaries.
55 additionalfeatures were additionally identified, for a recallof 87.5%.
These values are comparable, even ifslightly higher, to the precision and recall of themanual annotation described above.5.3 Between-Subject User StudyOur intrinsic evaluation gives satisfactory results.However, we believe the ultimate measure of sucha summarization algorithm is an end-to-end eval-uation to ascertain whether it affects user behav-ior, and how.
We conducted a between-subjectuser study, where users were presented with twodifferent versions of our Music RS.
For each ofthe recommended songs, the baseline version pro-vides only whole album reviews, the experimentalversion provides the automatically generated songfeature summary, as shown in Figure 2.
The in-terface for the baseline version is similar, but thesummary in the bottom window is replaced by thecorresponding album review.
The presented re-view is the one among the 50 reviews for that al-bum whose length is closest to the average lengthof album reviews in the corpus (478 words).Each user was presented with 5 songs in suc-cession, with 3 recommendations each (only thetop 3 recommendations were presented among theavailable 25, see Section 3).
Users were asked toselect at least one recommendation for each song,namely, to click on the url where they can listen tothe song.
They were also asked to base their selec-tion on the information provided by the interface.The first song was a test song for users to get ac-quainted with the system.
We collected compre-hensive timed logs of the user actions, includingclicks, when windows are open and closed, etc.After using the system, users were administered abrief questionnaire which included questions on a5-point Likert Scale.
18 users interacted with thebaseline version and 21 users with the experimen-tal version (five additional subjects were run buttheir log data was not properly saved).
All userswere students at our University, and most of them,graduate students (no differences were found dueto gender, previous knowledge of music, or educa-tion level).Our main measure is time on task, the total timetaken to select the recommendations from song 2to song 5 ?
this excludes the time spent listen-ing to the songs.
A t-test showed that users inthe experimental version take less time to maketheir decision when compared to baseline subjects(p = 0.019, t = 2.510).
This is a positive result,because decreasing time to selection is important,given that music collections can include millionsof songs.
However, time-on-task basically repre-sents the time it takes users to peruse the reviewor summary, and the number of words in the sum-maries is significantly lower than the number ofwords in the reviews (p < 0.001, t = 16.517).Hence, we also analyzed the influence of sum-maries on decision making, to see if they haveany effects beyond cutting down on the numberof words to read.
Our assumption is that the de-fault choice is to choose the first recommenda-tion.
Users in the baseline condition picked thefirst recommendation as often as the other two rec-ommendations combined; users in the experimen-tal condition picked the second and third recom-mendations more often than the first, and the dif-ference between the two conditions is significant(?2 = 8.74, df = 1, p = 0.003).
If we examinebehavior song by song, this holds true especiallyfor song 3 (?2 = 12.3, df = 1, p < 0.001) andsong 4 (?2 = 5.08, df = 1, p = 0.024).
Wespeculate that users in the experimental condition1383are more discriminatory in their choices, becauseimportant features of the recommended songs areevident in the summaries, but are buried in the al-bum reviews.
For example, for Song 3, only oneof the 20 sentences in the album review is aboutthe first recommended song, and is not very posi-tive.
Negative opinions are much more evident inthe review summaries.The questionnaires included three commonquestions between the two conditions.
The ex-perimental subjects gave a more positive assess-ment of the length of the summary than the base-line subjects (p = 0.003, t = ?3.248, df =31.928).
There were no significant differenceson the other two questions, feeling overwhelmedby the information provided; and whether the re-view/summary helped them to quickly make theirselection.A multiple Linear Regression with, as predic-tors, the number of words the user read beforemaking the selection and the questions, and timeon task as dependent variable, revealed only one,not surprising, correlation: the number of wordsthe user read correlates with time on task (R2 =0.277, ?
= 0.509, p = 0.004).Users in the experimental version were alsoasked to rate the grammaticality and coherence ofthe summary.
The average rating was 3.33 forgrammaticality, and 3.14 for coherence.
Whereasthese numbers in isolation are not too telling, theyare at least suggestive that users did not find thesesummaries badly written.
We found no signifi-cant correlations between grammaticality and co-herence of summaries, and time on task.6 Discussion and ConclusionsMost summarization research on customer reviewsfocuses on obtaining features of the products, butnot much work has been done on presenting themas a coherent summary.
In this paper, we describeda system that uses information extraction and sum-marization techniques in order to generate sum-maries of individual songs from multiple albumreviews.
Whereas the techniques we have usedare state-of-the-art, the contribution of our work isintegrating them in an effective end-to-end system.We first evaluated it intrinsically as concerns infor-mation extraction, and the informativeness of thesummaries.
Perhaps more importantly, we also ranan extrinsic evaluation in the context of our proto-type Music RS.
Users made quicker decisions andtheir choice of recommendations was more variedwhen presented with song review summaries thanwith album reviews.
Our framework can be ap-plied to any domain where reviews of individualcomponents need to be summarized from reviewsof collections, such as travel reviews that covermany cities in a country, or different restaurantsin a city.ReferencesRegina Barzilay and Kathleen McKeown.
2005.
Sen-tence fusion for multidocument news summariza-tion.
Computational Linguistics, 31(3):297?328.Giuseppe Carenini, Raymond Ng, and Adam Pauls.2006.
Multi-document summarization of evaluativetext.
In Proceedings of EACL.Oscar Clema.
2006.
Interaction Design for Recom-mender Systems.
Ph.D. thesis, Universitat PompeuFabra, Barcelona, July.Marie-Catherine de Marnee and Christopher D. Man-ning.
2008.
Stanford Typed Dependencies Manual.http://nlp.stanford.edu/software/dependencies manual.pdf.J.
Stephen Downie and Xiao Hu.
2006. Review min-ing for music digital libraries: Phase ii.
In Proceed-ings of the 6th ACM/IEEE-CS Joint Conference onDigital Libraries, pages 196?197, Chapel Hill, NC,USA.Andrea Esuli and Fabrizio Sebastiani.
2006.
Senti-WordNet: A publicly available lexical resource foropinion mining.
In Proceedings of LREC-06, the 5thConference on Language Resources and Evaluation,Genova, IT.Christiane Fellbaum, editor.
1998.
WordNet: an elec-tronic lexical database.
MIT Press.Michael Gamon, Anthony Aue, Simon Corston-Oliver,and Eric Ringger.
2005.
Pulse: Mining customeropinions from free text.
In Advances in Intelli-gent Data Analysis VI, volume 3646/2005 of Lec-ture Notes in Computer Science, pages 121?132.Springer Berlin / Heidelberg.Frederick Giasson and Yves Raimond.
2007.
Mu-sic ontology specification.
Working draft, February.http://pingthesemanticweb.com/ontology/mo/.Aaron Harnly, Ani Nenkova, Rebecca Passonneau, andOwen Rambow.
2005.
Automation of summaryevaluation by the Pyramid method.
In Proceedingsof the Conference on Recent Advances in NaturalLanguage Processing.Marti A. Hearst.
1994.
Multi-paragraph segmentationof expository text.
In Proceedings of the 32nd Meet-ing of the Association for Computational Linguis-tics, Las Cruces, NM, June.1384Ryuichiro Higashinaka, Rashmi Prasad, and MarilynWalker.
2006.
Learning to Generate NaturalisticUtterances Using Reviews in Spoken Dialogue Sys-tems.
In COLING-ACL06, Sidney, Australia.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of KDD,Seattle, Washington, USA, August.Dan Klein and Christopher D. Manning.
2002.
Fastexact inference with a factored model for naturallanguage parsing.
In Advances in Neural Informa-tion Processing Systems 15, pages 3?10.Susan McRoy, Songsak Ukul, and Syed Ali.
2003.
Anaugmented template-based approach to text realiza-tion.
In Natural Language Engineering, pages 381?420.
Cambridge Press.Guido Minnen, John Carroll, and Darren Pearce.
2000.Robust, applied morphological generation.
In Pro-ceedings of the 1st International Natural LanguageGeneration Conference.Tom Mitchell.
1997.
Machine Learning.
McGrawHill.Vivi Nastase.
2008.
Topic-driven multi-documentsummarization with encyclopedic knowledge andspreading activation.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing.Patrick Nguyen, Milind Mahajan, and Geoffrey Zweig.2007.
Summarization of multiple user reviews inthe restaurant domain.
Technical Report MSR-TR-2007-126, Microsoft, September.Markus Schedl, Gerhard Widmer, Tim Pohle, andKlaus Seyerlehner.
2007.
Web-based detection ofmusic band members and line-up.
In Proceedings ofthe Australian Computer Society.M.
Soubbotin and S. Soubbotin.
2005.
Trade-Off Be-tween Factors Influencing Quality of the Summary.InDocument Understanding Workshop (DUC), Van-couver, BC, Canada.Swati Tata.
2010.
SongRecommend: a Music Recom-mendation System with Fine-Grained Song Reviews.Ph.D.
thesis, University of Illinois, Chicago, IL.2008.
UPnP Device Architecture Version 1.0.
(www.upnp.org).Li Zhuang, Feng Jing, and Xiaoyan Zhu.
2006.
Moviereview mining and summarization.
In Conferenceon Information and Knowledge Management, Ar-lington, Virginia, USA.1385
