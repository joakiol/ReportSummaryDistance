Proceedings of NAACL-HLT 2013, pages 190?200,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsTopic Segmentation with a Structured Topic ModelLan DuDepartment of ComputingMacquarie UniversitySydney, Australialan.du@mq.edu.auWray BuntineCanberra Research LabNational ICT AustraliaCanberra, Australiawray.buntine@nicta.com.auMark JohnsonDepartment of ComputingMacquarie UniversitySydney, Australiamark.johnson@mq.edu.auAbstractWe present a new hierarchical Bayesian modelfor unsupervised topic segmentation.
This newmodel integrates a point-wise boundary sam-pling algorithm used in Bayesian segmenta-tion into a structured topic model that can cap-ture a simple hierarchical topic structure latentin documents.
We develop an MCMC infer-ence algorithm to split/merge segment(s).
Ex-perimental results show that our model out-performs previous unsupervised segmentationmethods using only lexical information onChoi?s datasets and two meeting transcriptsand has performance comparable to those pre-vious methods on two written datasets.1 IntroductionDocuments are usually comprised of topically co-herent text segments, each of which contains somenumber of text passages (e.g., sentences or para-graphs) (Salton et al 1996).
Within each topicallycoherent segment, one would expect that the wordusage demonstrates more consistent lexical distri-butions (known as lexical cohesion (Eisenstein andBarzilay, 2008)) than that across segments.
A linearpartition of texts into topic segments may reveal in-formation about, for example, themes of segmentsand the overall thematic structure of the text, andcan subsequently be useful for text analysis tasks,such as information retrieval (e.g., passage retrieval(Salton et al 1996)), document summarisation anddiscourse analysis (Galley et al 2003).In this paper we consider how to automaticallyfind a topic segmentation.
It involves identifyingthe most prominent topic changes in a sequenceof text passages, and splits those passages into asequence of topically coherent segments (Hearst,1997; Beeferman et al 1999).
This task can be castas an unsupervised machine learning problem: plac-ing topic boundaries in unannotated text.Although a variety of cues in text can be used fortopic segmentation, such as cue phases (Beefermanet al 1999; Reynar, 1999; Eisenstein and Barzi-lay, 2008)) and discourse information (Galley et al2003), in this paper, we focus on lexical cohesionand use it as the primary cue in developing an un-supervised segmentation model.
The effectivenessof lexical cohesion has been demonstrated by Text-Tiling (Hearst, 1997), c99 (Choi, 2000), MinCut(Malioutov and Barzilay, 2006), PLDA (Purver etal., 2006), Bayesseg (Eisenstein and Barzilay, 2008),TopicTiling (Riedl and Biemann, 2012), etc.Our work uses recent progress in hierarchi-cal topic modelling with non-parametric Bayesianmethods (Du et al 2010; Chen et al 2011; Du etal., 2012a), and is based on Bayesian segmentationmethods (Goldwater et al 2009; Purver et al 2006;Eisenstein and Barzilay, 2008) using topic mod-els.
This can also be viewed as a multi-topic exten-sion of hierarchical Bayesian segmentation (Eisen-stein, 2009), although our use of hierarchies is usedto improve the performance of linear segmentation,rather than develop hierarchical segmentation.Recently, topic models are increasingly used invarious text analysis tasks including topic segmen-tation.
Previous work (Purver et al 2006; Misraet al 2008; Sun et al 2008; Misra et al 2009;Riedl and Biemann, 2012) has shown that using190topic assignments or topic distributions instead ofword frequency can significantly improve segmen-tation performance.
Here we consider more ad-vanced topic models that model dependencies be-tween (sub-)sections in a document, such as struc-tured topic models (STMs) presented in (Du et al2010; Du et al 2012b).
STMs treat each text asa sequence of segments, each of which is a set oftext passages (e.g., a paragraph or sentence).
Textpassages in a segment share the same prior distribu-tion on their topics.
The topic distributions of seg-ments in a single document are then encouraged tobe similar via a hierarchical prior.
This gives a sub-stantial improvement in modelling accuracy.
How-ever, instead of explicitly learning the segmentation,STMs just leverage the existing structure of docu-ments from the given segmentation.Given a sequence of text passages, how can weautomatically learn the segmentation?
The wordboundary sampling algorithm introduced in (Gold-water et al 2009) uses point-wise sampling of wordboundaries after phonemes in an utterance.
Simi-larly, the segmentation method of PLDA (Purveret al 2006) samples segment boundaries, but alsojointly samples a topic model.
This is different toother topic modelling approaches that run LDA asa precursor to a separate segmentation step (Misraet al 2009; Riedl and Biemann, 2012).
While con-ceptually similar to PLDA, our non-parametric ap-proach built on STM required new methods to im-plement, but the resulting improvement by the stan-dard segmentation scores is substantial.This paper presents a new hierarchical Bayesianunsupervised topic segmentation model, integratinga point-wise boundary sampling algorithm with astructured topic model.
This new model takes ad-vantage of the high modelling accuracy of structuredtopic models (Du et al 2010) to produce a topicsegmentation based on the distribution of latent top-ics.
We show that this model provides high qualitysegmentation performance on Choi?s dataset, as wellas two sets of meeting transcripts and written texts.In the following sections we describe our topicsegmentation model and an MCMC inference al-gorithm for the non-parametric split/merge pro-cess.
The rest of the paper is organised as follows.
InSection 2 we review recent related work in the topicsegmentation literature.
Section 3 presents the newtopic segmentation model, followed by the deriva-tion of a sampling algorithm in Section 4.
We reportthe experimental results by comparing several re-lated topic segmentation methods in Section 5.
Sec-tion 6 concludes the paper.2 Related WorkWe are interested in unsupervised topic segmenta-tion in either written or spoken language.
There is alarge body of work on unsupervised topic segmen-tation of text based on lexical cohesion.
It can becharacterised by how lexical cohesion is modelled.One branch of this work represents the lexical co-hesion in a vector space by exploring the word co-occurrence patterns, e.g., TF or TF-IDF.
Work fol-lowing this line includes TextTiling (Hearst, 1997),which calculates the cosine similarity between twoadjacent blocks of words purely based on the wordfrequency; C99 (Choi, 2000), an algorithm basedon divisive clustering with a matrix-ranking scheme;LSeg (Galley et al 2003), which uses a lexicalchain to identify and weight word repetitions; U00(Utiyama and Isahara, 2001), a probalistic approachusing dynamic programming to find a segmenta-tion with a minimum cost; MinCut (Malioutov andBarzilay, 2006), which casts segmentation as a graphcut problem, and APS (Kazantseva and Szpakowicz,2011), which uses affinity propagation to learn clus-tering for segmentation.The other branch of this work characterises thelexical cohesion using topic models, to which themodel introduced in Section 3 belongs.
Lexical co-hesion in this line of research is modelled by aprobabilistic generative process.
PLDA presented byPurver et al(2006) is an unsupervised topic mod-elling approach for segmentation.
It chains a set ofLDAs (Blei et al 2003) by assuming a Markovstructure on topic distributions.
A binary topic shiftvariable is attached to each text passage (i.e., an ut-terance in (Purver et al 2006)).
It is sampled to in-dicate whether the jth text passage shares the topicdistribution with the (j ?
1)th passage.Using a similar Markov structure, SITS (Nguyenet al 2012) chains a set of HDP-LDAs (Teh et al2006).
Unlike PLDA, SITS assumes each text pas-sage is associated with a speaker identity that is at-tached to the topic shift variable as supervising in-191formation.
SITS further assumes speakers have dif-ferent topic change probabilities that work as pri-ors on topic shift variables.
Instead of assumingdocuments in a dataset share the same set of top-ics, Bayesseg (Eisenstein and Barzilay, 2008) treatswords in a segment generated from a segment spe-cific multinomial language model, i.e., it assumeseach segment is generated from one topic, and alater hierarchical extension (Eisenstein, 2009) as-sumes each segment is generated from one topic orits parents.
Other methods using as input the outputof topic models include (Sun et al 2008), (Misra etal., 2009), and (Riedl and Biemann, 2012).In this paper we take a generative approach ly-ing between PLDA and SITS.
In contrast to PLDA,which uses a flat topic model (i.e., LDA), we assumeeach text has a latent topic structure that can reflectthe topic coherence pattern, and the model adapts itsparameters to the segments to further improve per-formance.
Unlike SITS that targets analysing multi-party meeting transcripts, where speaker identitiesare available, we are interested in more general textsand assume each text has a specific topic changeprobability, since (1) the identity information is notalways available for all kinds of texts (e.g., continu-ous broadcast news transcripts (Allan et al 1998)),(2) even for the same author, topic change probabil-ities for his/her different articles might be different.3 Segmentation with Topic ModelsIn documents, topically coherent segments usuallyencapsulate a set of consecutive passages that aresemantically related (Wang et al 2011).
However,the topic boundaries between segments are often un-available a priori.
Thus we treat all passage bound-aries (e.g., sentence boundaries, paragraph bound-aries or pauses between utterances) as possible topicboundaries.
To recover the topic boundaries we de-velop a structured topic segmentation model by inte-grating ideas from the segmented topic model (Du etal., 2010, STM) and Bayesian segmentation models.The basic idea of our model is that each docu-ment consists of a set of segments where text pas-sages in the same segment are generated from thesame topic distribution, called segment level topicdistribution.
The segment level topic distribution isdrawn from a topic distribution associated with thewhole document, called document level topic distri-bution.
The relationships between the levels is man-aged using Bayesian non-parametric methods and asignificant change in segment level topic distributionindicates a segment change.Our unsupervised topic segmentation model isbased on the premise that using a hierarchical topicmodel like the STM with a point-wise segmentsampling algorithm should allow better detectionof topic boundaries.
We believe that (1) segmentchange should be associated with significant changein the topic distribution, (2) topic cohesion can bereflected in document topic structure, (3) the log-likelihood of a topically coherent segment is typi-cally higher than an incoherent segment (Misra etal., 2008).Assume we have a corpus of D documents, eachdocument d consists of a sequence of Ud text pas-sages, and each passage u contains a set of Nd,uwords denoted by wd,u that are from a vocabularyW .
Our model consists of:Modelling topic boundary: We assume eachdocument has its own topic shift probabilitypid, a Beta distributed random variable, i.e.,pid?Beta(?0, ?1).
Then, we associate a bound-ary indicator variable ?d,u with u, like thetopic shift variable in PLDA and SITS.
?d,uis Bernoulli distributed with parameter pid, i.e.,?d,u?Bernoulli(pid).
It indicates whether there is atopic boundary after text passage u or not.
To sample?d,u, we use a point-wise sampling algorithm.
Con-sequently, a sequence of ?
?s defines a set of seg-ments, i.e., a topic segmentation of d. For example,let a ?
vector ?
= (0, 0, 1, 0, 1, 0, 0, 1)1, it givesus three segments, which are {1, 2, 3}, {4, 5} and{6, 7, 8}.Modelling topic structure: Following the idea ofthe STM, we assume each document d is associatedwith a document level topic distribution ?d, whichis drawn from a Dirichlet distribution with param-eter ?
; and text passages in topic segment s in dare generated from ?d,s, a segment level topic dis-tribution.
The number of segments Sd can be com-puted as Sd=1 +?Ud?1u=1 ?d,u.
Then, a Pitman-Yor1The last 1 in ?
is the document boundary that is know apriori.
This means one does not need to sample it.192DK??????
?UswzSUNFigure 1: The topic segmentation modelprocess with a discount parameter a and a concen-tration parameter b is used to link ?d and ?d,s by?d,s?PYP(a, b, ?d), which forms a simple topichierarchy.
The idea here is that topics discussed insegments can be variants of topics of the wholedocument.
Du et al(2010) have shown that thistopic structure can significantly improve the mod-elling accuracy, which should contribute to more ac-curate segmentation.
This generative process is dif-ferent from PLDA.
PLDA does not assume the docu-ment level topic distribution and each time generatesthe segment level topic distribution directly from aDirichlet distribution.The complete probabilistic generative process,shown as a graph in Figure 1 is as follows:1.
For each topic k ?
{1, .
.
.
, K}, draw a word distribution?k ?
DirichletW (?).2.
For each document d ?
{1, .
.
.
, D},(a) Draw topic shift probability pid ?
Beta(?0, ?1).
(b) Draw ?d ?
DirichletK (?).
(c) For each text passage (except last) u ?
{1, .
.
.
, Ud ?
1}, draw ?d,u ?
Bernoulli(pid).
(d) Compute Sd the number of segments as 1 +?Ud?1u=1 ?d,u.
(e) For each segment s ?
{1, .
.
.
, Sd}, draw ?d,s ?PYP(a, b, ?d).
(f) For each text passage u ?
{1, .
.
.
, Ud},i.
Set segment sd,u = 1 +?u?1v=1 ?d,v .ii.
For each word index n ?
{1, .
.
.
, Nd,u},A.
Draw topic zd,u,n ?
DiscreteK(?d,sd,u).B.
Draw word wd,u,n ?
DiscreteK(?zd,u,n).where sd,u indicates which segment text passage ubelongs to.
We assume the dimensionality of theDirichlet distribution (i.e., the number of topics) isknown and fixed, and word probabilities are param-eterized with a K ?
Wmatrix ?
= (?1, .
.
.
, ?K).In future work we plan to investigate replace theTable 1: List of statisticsMk,w total number of words with topic k.Mk a vector of Mk,w.nd,s,k total number of words with topic k in segments in document d.Nd,s total number of words in segment s.td,s,k table count of topic k in the CRP for segments in document d.td,s a vector of td,s,k for segment s in d.Td,s total table count in segment s.cd,1 total number of topic boundaries in d.cd,0 total number of non-topic boundaries in d.Dirichlet prior ?
on ?
with a Pitman-Yor prior (Pit-man and Yor, 1997) to make the model fully non-parametric, like SITS.4 Posterior InferenceIn this section we develop a collapsed Gibbs sam-pling algorithm to do an approximate inferenceby integrating out some latent variables (i.e., ??s,?
?s and pid?s).
The hierarchy in our model can bewell explained with the Chinese restaurant franchisemetaphor introduced in (Teh et al 2006).
For easierunderstanding, terminologies of the Chinese Restau-rant Process (CRP) will be used throughout this sec-tion, i.e., customers, dishes and restaurants, corre-spond to words, topics, and segments respectively.Statistics used are listed in Table 1.To integrate out the ?d,s?s generated from thePYP, we use the technique presented in (Chen etal., 2011), which computes the joint posterior forthe PYP by summing out all the possible seatingarrangements for a sequence of customers (Teh,2006).
In this technique an auxiliary binary variable,called table indicator (?d,u,n), is introduced to fa-cilitate computing table count td,s,k for topic k. Thismethod has two effects: (1) faster mixing of the sam-pler, and (2) elimination of the need for dynamicmemory to store the populations/counts of each ta-ble in the CRP.
In the CRP each word wd,u,n in topick (i.e., where zd,u,n=k) contributes a count to nd,s,kfor u ?
s; and, if wd,u,n, as a customer, also opensa new table to the CRP, it leads to increasing td,s,kby one.
In this case, ?d,u,n=1 indicates wd,u,n is thefirst customer on the table, called table head.
Thus,td,s,k =?u?sNd,u?n=1?d,u,n1zd,u,n=k .
(1)Note the two constraints on these two counts, i.e.,nd,s,k?td,s,k?0 and td,s,k=0 iff nd,s,k=0 (2)193can be replaced be a simpler constraint in the tableindicator representation.The sampler we develop is an MCMC sampleron the space ?
= {z, ?,?}
where z defines thetopic assignments of words, ?
maintains the neededCRP configuration (from which t is derived) and ?defines the segmentation.
Moreover, it is not a tra-ditional Gibbs sampler changing one variable at atime, but is a block Gibbs sampler where two dif-ferent kinds of blocks are used.
The first block is(zd,u,n, ?d,u,n) (for each word wd,u,n), which canbe sampled with a table indicator variant of a hier-archical topic sampler (Du et al 2010), describedin Section 4.1.
This corresponds to Equation (6) in(Purver et al 2006).
The second kind of block isa boundary indicator ?d,u together with a particularconstrained set of table counts designed to handlesplitting and merging, which corresponds to Equa-tion (7) in (Purver et al 2006).
Sampling this sec-ond kind of block is harder in our non-parametricmodel requiring a potentially exponential summa-tion, a problem we overcome using symmetric poly-nomials, shown in Section 4.2.4.1 Sampling TopicsOne step in our model is to sample the assignmentsof topics to words conditioned on all ??s.
As dis-cussed in Section 3, given the sequence of ?d,u?s,?d, one can figure out which segment s text passageu belongs to.
Thus, conditioned on a set of segmentss given by ?, the joint posterior distribution ofw, zand ?
is computed as p(z,w, ?
|?, ?, a, b, ?
)=?dBetaK(?+?s td,s)BetaK (?
)?kBetaW (?
+Mk)BetaW (?
)?d?s?s(b|a)Td,s(b)Nd,s?kSnd,s,ktd,s,k,a(nd,s,ktd,s,k)?1, (3)where BetaK(?)
is a K-dimension Beta function,(x|y)n the Pochhammer symbol2, and Snt,a the gen-eralised Stirling number of the second kind (Hsuand Shiue, 1998)3 precomputed in a table so cost-2The Pochhammer symbol (x|y)n denotes the rising facto-rial with a specified increment, i.e., y.
It is defined as (x|y)n =x(x+ y)...(x+ (n?
1)y).3A Stirling number of the second kind is used to studythe number of ways of partitioning a set of n objects intok nonempty subsets.
The generalised version given by Hsuand Shiue (1998) has a linear recursion which in our case isSn+1m,a = Snm?1,a + (n?ma)Snm,a.ing O(1) to use (Buntine and Hutter, 2012).Eq (3)is an indicator variant of Eq (1) in (Du et al 2010)with applying Theorem 1 in (Chen et al 2011).Given the current segmentation and topic assign-ments for all other words, using Bayes rule, we canderive the following two conditionals from Eq (3):1.
The joint probability of assigning topic k to wordwd,u,n and wd,u,n being a table head, p(zd,u,n =k, ?d,u,n = 1 |??
)=?wi,j,n +Mk,wi,j,n?w(?w +Mk,w)?k +?s td,s,k?k ?k +?s,k td,s,kb+ aTd,sb+Nd,sSnd,s,k+1td,s,k+1,aSnd,s,ktd,s,k,atd,s,k + 1nd,s,k + 1(4)2.
The joint probability of assigning k to wd,u,nand wd,u,n not being a table head, p(zd,u,n =k, ?d,u,n = 0 |??
)=?wi,j,l +Mk,wi,j,l?w ?w +Mk,w1b+Nd,sSnd,s,k+1td,s,k,aSnd,s,ktd,s,k,and,s,k + 1?
td,s,knd,s,k + 1(5)where ??
= {z?zd,u,n ,w, ??
?d,u,n ,?,?, a, b,?
}.From the two conditionals, we develop a blockedGibbs sampling algorithm for (zd,u,n, ?d,u,n).4.2 Sampling Segmentation BoundariesIn our model, each segment corresponds to aChinese restaurant in the CRP.
Sampling topicboundaries corresponds to splitting/merging restau-rant(s).
This is different from the split-merge processproposed by Jian and Neal (2004), where one actu-ally splits/merges table(s).
To our knowledge, therehas been no method developed to split/merge restau-rant(s).
We tried different approximations, suchas the minimum-path-assumption (Wallach, 2008),which in our case assumes one table for each topick, and all words in k are placed in the same ta-ble.
Although this simplifies the split-merge pro-cess, it yielded poor results.
We instead developed anovel approximate block Gibbs sampling algorithmusing symmetric polynomials.
Its segmentation per-formance worked well in our development dataset.For simplicity, we consider a passage u in doc-ument d, and assume: (1) If ?d,u=1, there are twosegments, sl and sr; sl ends at text passage u, and srstarts at text passage u+1.
(2) If ?d,u=0, there is one194segment, sm, where u is is somewhere in the middleof sm.
The split-merge choice we sample is one tomany, for a given split pair (sl, sr) we consider a setof merged states sm (represented by different possi-ble table counts).
Then, to compute the Gibbs prob-ability for splitting/merging restaurant(s), we con-sider the probability of the single split, the probabil-ity of the corresponding set of merges, and then if amerge is selected, we have to sample from the set ofmerges.
These are as follows:Splitting: split sm into sr and sl by placing aboundary after u.
Since passages have a fixed orderin each document, all the words are put into sr andsl based on which passages they belong to.
Then,given all the topic assignments, we first sample alltable indicators ?d,u?,n, for n ?
{1, ..., Nd,u?}
andu?
?
sm using Bernoulli sampling without replace-ment.
It runs as follows: 1) sample ?d,u?,n accordingto probability td,sm,k/nd,sm,k; 2) decrease td,sm,k if?d,u?,n = 1, otherwise, just decrease nd,sm,k.
Us-ing the sampled ?d,u?,n?s we compute the inferred ta-ble counts td,s,k (from Eq (1)) and customer countsnd,s,k respectively for segments s=sl and sr andtopics k. The computation may result in the follow-ing cases: for a given topic k,(I) Both sl and sr have nd,s,k>0 and td,s,k?1, whichmeans both segments have words assigned to k andwords being labelled with table head.
Accordingto constraints (2), after splitting, restaurants corre-sponding to sl and sr are valid.
We do not make anychange on table counts.
(II) Either sl or sr has nd,s,k=0 and td,s,k=0.
In thiscase, for example, all the words assigned to k in smare in sl after splitting, and all those labelled withtable head should also be in sl.
sr has no words as-signed to k. Thus, there is no need to change tablecounts.
(III) Either sl or sr has nd,s,k>0 and td,s,k=0.
Both seg-ments have words assigned to k, but those labelledwith table head only exist in one segment.
For in-stance, if they only exist in sl then sr has no tablehead, which means the restaurant of sr has customerseating a dish, but no tables serving that dish.
Thus,we set td,sr,k=1 to make the constraints (2) satisfied.The Gibbs probability for splitting a segment isp(?d,u = 1 |???)
?
?1 + cd,1?0 + ?1 + cd,0 + cd,1(6)BetaK(?+Sd?s=1td,s) ?s?
{sl,sr}(b|a)Td,s(b)Nd,s?kSnd,s,ktd,s,k,a ,where ???
= {z,w, ?,??
?d,u ,?, a, b, ?0, ?1}.Merging: remove the boundary after u, and mergesr and sl to one segment sm.
For this case, bothsr and sl satisfy constraints (2) for all k?s, and setnd,sm,k=nd,sr,k + nd,sl,k.
The following cases areconsidered: for a topic k(I) Both sl and sr have nd,s,k>0 and td,s,k>1.
Wecompute td,sm,k using Eq (7).
Thus table countsbefore and after merging are equal.
(II) Either sl or sr has nd,s,k=0 and td,s,k=0.
Similarto the above case, we use Eq (7).
(III) Both sl and sr have nd,s,k>0, and either of themhas td,s,k=1 or both.
We have to choose betweenEq (7) and Eq (8), i.e., to decide whether a tableshould be removed or not.td,sm,k = td,sl,k + td,sr,k (7)td,sm,k = td,sl,k + td,sr,k ?
1 (8)Note that choosing Eq (8) means we need to de-crease the table count td,sm,k by one.
The idea hereis that we sample to decide whether the remove tablewas added due to splitting case (III) or not.
Clearly,we have a one-to-many split-merge choice.
To com-pute the probability of a set of possible merges,we use elementary symmetric polynomials as fol-lows: letKS be a set of topic-segment combinationsthat satisfy the condition in merging case (III), for(k, s) ?
KS , we sample either Eq (7) or Eq (8).Let T = {td,s,k : (k, s) ?
KS} be the set of tablecounts affected by the changes of Eq (7) or Eq (8).The Gibbs probability for merging two segments isp(?d,u = 0 |????)
=?Tp(?d,u = 0, T |????)
(9)?
?T(?0 + cd,0?0 + ?1 + cd,0 + cd,1BetaK(?+Sd?s=1td,s)(b|a)Td,sm(b)Nd,sm?kSnd,sm,ktd,sm,k,a),where ????
= {z,w, t ?
T ,??
?d,u ,?, a, b, ?0, ?1}.This is converted to a sum on |T | booleans withindependent terms and evaluated recursively inO(|T |2) by symmetric polynomials.
If a merge ischosen, one then samples according to the terms inthe sum using a similar recursion.5 ExperimentsTo demonstrate the effectiveness of our model (de-noted by TSM) in topic segmentation tasks, we195evaluate it on three different kinds of corpora4: aset of synthetic documents, two meeting transcriptsand two sets of text books (see Tables 2 and 3);and compare TSM with the following methods: twobaselines (the Random algorithm that places topicboundaries uniformly at random, and the Even al-gorithm that places a boundary after every mth textpassage, where m is the average gold-standard seg-ment length (Beeferman et al 1999)), C99, MinCut,Bayesseg, APS (Kazantseva and Szpakowicz, 2011),and PLDA.Metrics: We evaluated the segmentation perfor-mance with PK (Beeferman et al 1999) and Win-dowDiff (WDr) (Pevzner and Hearst, 2002), whichare two common metrics used in topic segmenta-tion.
Both move a sliding window of fixed size kover the document, and compare the inferred seg-mentation with the gold-standard segmentation foreach window.
The window size is usually set tothe half of the average gold-standard segment size(Pevzner and Hearst, 2002).
In addition, we alsoused an extended WindowDiff proposed by Lam-prier et al(2007), denoted by WDe.
One problemof WDr is that errors near the two ends of a text arepenalised less than those in the middle.
To solve theproblem WDe adds k fictive text passages at the be-ginning and the end of the text when computing thescore.
We evaluated all the methods with the sameJava code for the three metrics.Parameter Settings: In order to make all themethods comparable, we chose for each methodthe parameter settings that give the gold-standardnumber of segments5.
Specifically, we used a11 ?
11 rank mask for C99, as suggested byChoi (2000), the configurations included in the code(http://groups.csail.mit.edu/rbg/code)for Bayesseg and manually tuned parameters forMinCut.
For APS, a greedy approach was used tosearch parameter settings that can approximatelygive the gold-standard number of segments.
ForPLDA, two randomly initialised Gibbs chains wereused.
Each chain ran for 75,000 burn-in iterations,then 1000 samples were drawn at a lag of 25 fromeach chain.
For TSM, 10 randomly initialised4For preprocessing, we only removed stop words.5The segments learnt by those methods will differ, but justthe segment count will be the same as the gold-standard count.Table 2: The Choi?s datasetRange of n 3-11 3-5 6-8 9-11#docs 400 100 100 100DocLenmean 69.7 39.3 69.6 98.6std 8.2 2.6 2.9 3.5SegLenmean 7 4 7 10std 2.57 0.84 0.87 1.03Table 3: Real dataset statisticsICSI Election Fiction Clinical# doc 25 4 84 227DocLenmean 994.5 144.3 325.0 139.5std 354.5 16.4 230.1 110.4SegLenmean 188 7 22 35std 219.1 8.9 23.8 41.7Gibbs chains were used.
Each chain ran for 30,000iterations with 25,000 for burn-in, then 200 sampleswere drawn.
The concentration parameter b in TSMwas sampled using the Adaptive-Reject samplingscheme introduced in (Du et al 2012b), the dis-count parameter a = 0.2, and ?0 = ?1 = 0.1.
Toderive the final segmentation for PLDA and TSM,we first estimated the marginal probabilities ofplacing boundaries after text passages from the totalof 2000 samples.
These probabilities were thenthresholded to give the gold-standard number ofsegments.
Precisely, we apply a small amount ofGaussian smoothing to the marginal probabilities(except for Choi?s dataset), like Puerver et al(2006)does.
Finally, we used a symmetric Dirichlet priorin PLDA and STM, the one on topic distributions is?
= 0.1, the other on word distributions ?
= 0.01.5.1 Evaluation on Choi?s DatasetChoi?s dataset (Choi, 2000) is commonly used inevaluating topic segmentation methods.
It consistsof 700 documents, each being a concatenation of 10segments.
Each segment is the first n sentences ofa randomly selected document from the Brown cor-pus, s.t.
3 ?
n ?
11.
Those documents are dividedinto 4 subsets with different range of n, as shown inTable 2.
We ran PLDA and STM with 50 topics.
Re-sults in Table 4 show that our model significantlyoutperforms all the other methods on the four sub-sets over all the metrics.
Furthermore, comparing toother published results, this also outperforms (Misraet al 2009) (see their table 2), and (Riedl and Bie-mann, 2012) (they report an average of 1.04 and 1.06in Tables 1 and 2, whereas TSM averages 0.93).
Thisgives TSM the best reported results to date.196Table 4: Comparison on Choi?s datasets with WD and PK (%)3-11 3-5 6-8 9-11WDr WDe PK WDr WDe PK WDr WDe PK WDr WDe PKRandom 51.7 49.1 48.7 51.4 50.0 48.4 52.5 49.9 49.2 52.4 48.9 49.2Even 49.1 46.7 49.0 46.3 45.8 46.3 38.8 37.3 38.8 30.0 28.6 30.0MinCut 30.4 29.8 26.7 41.6 41.5 37.3 28.2 27.4 25.5 23.6 22.7 21.6APS 40.7 38.8 38.4 32.0 30.6 31.8 34.4 32.6 32.7 34.5 32.2 33.2C99 13.5 12.3 12.3 11.3 10.2 10.8 10.2 9.3 9.8 8.9 8.1 8.6Bayesseg 11.6 10.9 10.9 11.8 11.5 11.1 7.7 7.2 7.3 6.1 5.7 5.7PLDA 2.4 2.2 1.8 4.0 3.9 3.3 3.6 3.5 2.7 3.0 2.8 2.0TSM 0.8 0.8 0.6 1.3 1.3 1.0 1.4 1.4 0.9 1.9 1.8 1.2Table 5: Comparison on the meeting transcripts and written texts with WD and PK (%)ICSI Election Fiction ClinicalWDr WDe PK WDr WDe PK WDr WDe PK WDr WDe PKRandom 46.3 41.7 44.1 51.0 49.7 45.1 51.0 48.7 47.5 45.9 38.5 44.1Even 48.3 43.0 46.4 56.0 55.1 51.2 48.1 45.9 46.3 49.2 42.0 48.8C99 42.9 37.4 39.9 43.1 41.5 37.0 48.1 45.1 42.1 39.7 31.9 38.7MinCut 40.6 36.9 36.9 43.6 43.3 39.0 40.5 39.7 37.1 38.2 36.2 36.8APS 58.2 49.7 54.6 47.7 36.8 40.6 48.0 45.8 45.1 39.9 32.8 39.6Bayesseg 32.4 29.7 26.7 41.1 41.3 34.1 33.7 32.8 27.8 35.0 28.8 34.0PLDA 32.6 28.8 29.4 40.6 41.1 32.0 43.0 41.3 36.1 37.3 32.1 32.4TSM 30.2 26.8 25.8 38.1 38.9 31.3 40.8 38.7 32.5 34.5 29.1 30.6Note the lexical transitions in these concatenateddocuments are very sharp (Malioutov and Barzi-lay, 2006).
The sharp transitions lead to significantchange in segment level topic distributions, whichfurther implies the variance of these distributions islarge.
In TSM, a large variance causes a small con-centration parameter b.
We observed that the sam-pled b?s (about 0.1) are indeed small for the four sub-sets, which shows there is no topic sharing amongsegments.
Therefore, TSM is able to recognise thesegments are unrelated text.5.2 Evaluation on Meeting TranscriptsWe applied our model to segmenting the two meet-ing transcripts, which are the ICSI meeting tran-scripts (Janin et al 2003) and the 2008 presidentialelection debates (Boydstun et al 2011).
The ICSImeeting has 75 transcripts, we used the 25 annotatedtranscripts provided by Galley et al(2003) for eval-uation.
For the election debates, we used the fourannotated debates used in (Nguyen et al 2012).
Thestatistics are shown in Table 3.
PLDA and TSM weretrained with 10 topics on the ICSI and 50 on theElection.
In this set of experiments, we show thatour model is robust to meeting transcripts.00.20.40.6p(l=1) TSM0 100 200 300 400 500 600 700 80000.20.40.6 PLDAUtterance position in sequencep(l =1)Figure 2: Probability of a topic boundary, compared withgold-standard segmentation (shown in red and at the topof each diagram) on one ICSI transcript.As shown in Table 5, topic modelling based meth-ods (i.e., Bayesseg, PLDA and TSM) outperformthose using either TF or TF-IDF, which is consistentwith previously reported results (Misra et al 2009;Riedl and Biemann, 2012).
Among the topic modelbased methods, TSM achieves the best results on allthe three metrics.
On the ICSI transcripts, TSM per-forms 6.8%, 9.7% and 3.4% better than Bayessegon the WDr, WDe and PK metrics respectively.
Fig-ure 2 shows an example of how the inferred topicboundary probabilities at utterances compare withthe gold-standard boundaries on one ICSI meetingtranscript.
The gold-standard segmentation is {77,95, 189, 365, 508, 609, 860}, TSM and PLDA in-fer {85, 96, 188, 363, 499, 508, 860} and {96, 136,197Table 6: Sampled concentration parametersChoi ICSI Election Fiction Clinicalb 0.1 5.2 5.4 18.4 4.8203, 226, 361, 508, 860} respectively.
Both modelsmiss the boundary after the 609th utterance, but put aboundary after the 508th utterance.
Note the bound-aries placed by TSM are always within 10 utteranceswith respect to the gold standard.Although TSM still performs the best on the de-bates, all the methods have relatively worse perfor-mance than on the ICSI meeting transcripts.
Nguyenet al(2012) pointed out that the ICSI meetings arecharacterised by pragmatic topic changes, in con-trast, the debates are characterised by strategic topicchanges with strong rewards for setting the agenda,dodging a question, etc.
Thus, considering the prop-erties of debates might further improve the segmen-tation performance.5.3 Evaluation on Written TextsWe further tested TSM on two written text datasets,Clinical (Eisenstein and Barzilay, 2008) and Fiction(Kazantseva and Szpakowicz, 2011).
The statisticsare shown in Table 3.
Each document in the Clinicaldataset is a chapter of a medical textbook.
Sectionbreaks are selected to be the true topic boundaries.For the Fiction dataset, each document is a fictiondownloaded from Project Gutenberg, the true topicboundaries are chapter breaks.
We trained PLDAand TSM with 25 topics on the Fiction and 50 on theClinical.
Results are shown in Table 5.
TSM com-pares favourably with Bayesseg and outperforms theother methods on the Clinical dataset, but it does notperform as well as Bayesseg on the Fiction dataset.In fiction books, the topic boundaries betweensections are usually blurred by the authors for rea-sons of continuity (Reynar, 1999).
We observed thatthe sampled concentration (or inverse variance) pa-rameter b in TSM is about 18.4 on Fiction, but 4.8 onClinical, as shown in Table 6.
This means the vari-ance of segment level topic distributions ?
learnt byTSM is not large for the fiction, so chapter breaksmay not necessarily indicate topic changes.
For ex-ample, there is a document in the Fiction datasetwhere gold-standard topic boundaries are placed af-ter each block of text.
In contrast, Bayesseg assumeseach segment has its own distribution over words,i.e., one topic per segment, which means topics arenot shared among segments.
We hypothesize thatfor certain kinds of documents where the change intopic distribution is subtle, such as fiction, assumingone topic per segment can capture subtle changes inword usage.
This is an area for future investigation.6 ConclusionIn this paper, we have presented a hierarchicalBayesian model for unsupervised topic segmen-tation.
This new model takes advances of bothBayesian segmentation and structured topic mod-elling.
It uses a point-wise boundary sampling al-gorithm to sample a topic segmentation, while con-currently building a structured topic model.
Wehave developed a novel approximation to com-pute the Gibbs probabilities of spliting/merging seg-ment(s).
Our model shows prominent segmentationperformance on both written or spoken texts.In future work, we would like to make the modelfully nonparametric and investigate the effects ofadding different cues in texts, such as cue phrases,pronoun usage, prosody, etc.
Currently, our modeluses marginal boundary probabilities to generatethe final segmentation.
Instead, we could develop aMetropolis-Hasting sampling algorithm to move oneboundary at a time, given the gold-standard numberof segments.
To further study the effectiveness ofour model, we would like to compare it with othermethods, like SITS (Nguyen et al 2012) and to runon more datasets, like email (Joty et al 2010).
Forexample, in order to compare with SITS, one canmake an assumption that each document just has onespeaker.AcknowledgmentsThe authors would like to thank all the anony-mous reviewers for their valuable comments.This research was supported under AustralianResearch Council?s Discovery Projects fundingscheme (project numbers DP110102506 andDP110102593).
NICTA is funded by the AustralianGovernment as represented by the Departmentof Broadband, Communications and the DigitalEconomy and the Australian Research Councilthrough the ICT Centre of Excellence program.198ReferencesJ.
Allan, J. Carbonell, G. Doddington, J. Yamron, andY.
Yang.
1998.
Topic detection and tracking pi-lot study: Final report.
In Proceedings of theDARPA Broadcast News Transcription and Under-standing Workshop, pages 194?218.Doug Beeferman, Adam Berger, and John Lafferty.1999.
Statistical models for text segmentation.
Mach.Learn., 34(1-3):177?210.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alcation.
J. Mach.
Learn.Res., 3:993?1022.A.E.
Boydstun, C. Phillips, and R.A. Glazier.
2011.
Itsthe economy again, stupid: Agenda control in the 2008presidential debates.W.
Buntine and M. Hutter.
2012.
A Bayesian reviewof the Poisson-Dirichlet process.
Technical ReportarXiv:1007.0296v2, ArXiv, Cornell.Changyou Chen, Lan Du, and Wray Buntine.
2011.Sampling for the Poisson-Dirichlet process.
In Euro-pean Conference on Machine Learning and Principlesand Practice of Knowledge Discovery in Database,pages 296?311.Freddy Y. Y. Choi.
2000.
Advances in domain inde-pendent linear text segmentation.
In Proceedings ofthe 1st North American chapter of the Association forComputational Linguistics conference, NAACL 2000,pages 26?33.Lan Du, Wray Buntine, and Huidong Jin.
2010.
Asegmented topic model based on the two-parameterPoisson-Dirichlet process.
Mach.
Learn., 81(1):5?19.Lan Du, Wray Buntine, and Huidong Jin.
2012a.
Mod-elling sequential text with an adaptive topic model.In Proceedings of the 2012 Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning, pages535?545.Lan Du, Wray Buntine, Huidong Jin, and ChangyouChen.
2012b.
Sequential latent Dirichlet alcation.Knowledge and Information Systems, 31(3):475?503.Jacob Eisenstein and Regina Barzilay.
2008.
Bayesianunsupervised topic segmentation.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP?08, pages 334?343.Jacob Eisenstein.
2009.
Hierarchical text segmentationfrom multi-scale lexical cohesion.
In Human Lan-guage Technologies: Conference of the North Amer-ican Chapter of the Association of Computational Lin-guistics, pages 353?361.
The Association for Compu-tational Linguistics.Michel Galley, Kathleen R. McKeown, Eric Fosler-Lussier, and Hongyan Jing.
2003.
Discourse segmen-tation of multi-party conversation.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics, pages 562?569.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2009.
A Bayesian framework for word segmen-tation: Exploring the effects of context.
Cognition,112(1):21?53.Marti A. Hearst.
1997.
TextTiling: segmenting textinto multi-paragraph subtopic passages.
Comput.
Lin-guist., 23(1):33?64.Leetsch C. Hsu and Peter Jau-Shyong Shiue.
1998.
Aunified approach to generalized Stirling numbers.
Adv.Appl.
Math., 20:366?384, April.Sonia Jain and Radford Neal.
2004.
A split-mergeMarkov chain Monte Carlo procedure for the Dirichletprocess mixture model.
Journal of Computational andGraphical Statistics, 13(1):158?182.A.
Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,N.
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,and C. Wooters.
2003.
The ICSI Meeting Corpus.
InProceedings of 2003 IEEE International Conferenceon Acoustics, Speech, and Signal (ICASSP ?03), pages364?367.Shafiq Joty, Giuseppe Carenini, Gabriel Murray, andRaymond T. Ng.
2010.
Exploiting conversation struc-ture in unsupervised topic segmentation for emails.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 388?398.Anna Kazantseva and Stan Szpakowicz.
2011.
Lineartext segmentation using affinity propagation.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 284?293.Sylvain Lamprier, Tassadit Amghar, Bernard Levrat, andFrederic Saubion.
2007.
On evaluation methodologiesfor text segmentation algorithms.
In Proceedings ofthe 19th IEEE International Conference on Tools withArtificial Intelligence - Volume 02, ICTAI ?07, pages19?26.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and the 44th annual meeting ofthe Association for Computational Linguistics, ACL-44, pages 25?32.Hemant Misra, Olivier Cappe, and Francois Yvon.
2008.Using LDA to detect semantically incoherent docu-ments.
In Proceedings of CoNLL-08, pages 41?48.Hemant Misra, Franc?ois Yvon, Joemon M. Jose, andOlivier Cappe.
2009.
Text segmentation via topicmodeling: an analytical study.
In Proceedings of the18th ACM conference on Information and knowledgemanagement, CIKM ?09, pages 1553?1556.Viet-An Nguyen, Jordan Boyd-Graber, and PhilipResnik.
2012.
SITS: A hierarchical nonparametric199model using speaker identity for topic segmentation inmultiparty conversations.
In Proceedings of the 50thAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 78?87.Lev Pevzner and Marti A. Hearst.
2002.
A critique andimprovement of an evaluation metric for text segmen-tation.
Comput.
Linguist., 28(1):19?36.J.
Pitman and M. Yor.
1997.
The two-parameter Poisson-Diriclet distribution derived from a stable subordina-tor.
Annals Probability, 25:855?900.Matthew Purver, Thomas L. Griffiths, Konrad P. Ko?rding,and Joshua B. Tenenbaum.
2006.
Unsupervised topicmodelling for multi-party spoken discourse.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and the 44th annual meeting ofthe Association for Computational Linguistics, ACL-44, pages 17?24.Jeffrey C. Reynar.
1999.
Statistical models for topic seg-mentation.
In Proceedings of the 37th Annual Meet-ing of the Association for Computational Linguistics,pages 357?364.Martin Riedl and Chris Biemann.
2012.
How text seg-mentation algorithms gain from topic models.
In Pro-ceedings of the 2012 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies.Gerard Salton, Amit Singhal, Chris Buckley, and MandarMitra.
1996.
Automatic text decomposition using textsegments and text themes.
In Proceedings of the theseventh ACM conference on Hypertext, pages 53?65.Qi Sun, Runxin Li, Dingsheng Luo, and Xihong Wu.2008.
Text segmentation with LDA-based Fisher ker-nel.
In Proceedings of ACL-08: HLT, Short Papers,pages 269?272.Y.
W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006.Hierarchical Dirichlet processes.
Journal of the Amer-ican Statistical Association, 101(476):1566?1581.Y.
W. Teh.
2006.
A Bayesian interpretation of interpo-lated Kneser-Ney.
Technical Report TRA2/06, Schoolof Computing, National University of Singapore.Masao Utiyama and Hitoshi Isahara.
2001.
A statisticalmodel for domain-independent text segmentation.
InProceedings of 39th Annual Meeting of the Associa-tion for Computational Linguistics, pages 499?506.H.M.
Wallach.
2008.
Structured topic models for lan-guage.
doctoral dissertation, Univ.
of Cambridge.Hongning Wang, Duo Zhang, and ChengXiang Zhai.2011.
Structural topic model for latent topical struc-ture analysis.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 1526?1535.200
