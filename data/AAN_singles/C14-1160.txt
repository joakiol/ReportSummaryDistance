Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1694?1705, Dublin, Ireland, August 23-29 2014.Combining Natural and Artificial Examples to Improve ImplicitDiscourse Relation IdentificationChlo?
BraudALPAGE, Univ Paris Diderot& INRIA Paris-Rocquencourt75013 Paris - Francechloe.braud@inria.frPascal DenisMAGNET, INRIA Lille Nord-Europe59650 Villeneuve d?Ascq - Francepascal.denis@inria.frAbstractThis paper presents the first experiments on identifying implicit discourse relations (i.e., relationslacking an overt discourse connective) in French.
Given the little amount of annotated data forthis task, our system resorts to additional data automatically labeled using unambiguous connec-tives, a method introduced by (Marcu and Echihabi, 2002).
We first show that a system trainedsolely on these artificial data does not generalize well to natural implicit examples, thus echoingthe conclusion made by (Sporleder and Lascarides, 2008) for English.
We then explain these ini-tial results by analyzing the different types of distribution difference between natural and artificialimplicit data.
This finally leads us to propose a number of very simple methods, all inspired fromwork on domain adaptation, for combining the two types of data.
Through various experimentson the French ANNODIS corpus, we show that our best system achieves an accuracy of 41.7%,corresponding to a 4.4% significant gain over a system solely trained on manually labeled data.1 IntroductionAn important bottleneck for automatic discourse understanding is the proper identification of implicitrelations between discourse units.
What makes these relations difficult is that they lack strong surfacecues like a discourse marker.
This point is illustrated in the French examples (1) and (2).1In (1), theconnective mais (but) triggers a relation of contrast, whereas in (2), there is no explicit connective tosignal the explanation relation, and the relation has to be inferred through other ways (in this case, acausal relation between having injured players and loosing).
(1) La hulotte est un rapace nocturne, mais elle peut vivre le jour.The tawny owl is a nocturnal bird of prey, but it can live in the daytime.
(2) L?
?quipe a perdu lamentablement hier.
Elle avait trop de bless?s.The team lost miserably yesterday.
It had too many injured players.Implicit relations are very widespread in naturally-occurring data.
Thus, they make up between 39.5%and 54% of the annotated examples in the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008),depending on the relation types used.2A quick look at other discourse corpora suggests that the problemis as pervasive (if not more) in other languages.
The French ANNODIS corpus does not annotate thedistinction between explicit and implicit relations, but a projection of a French connective lexicon on thedata gives a proportion of 47.4 to 71% of implicit relations, depending on the set of relations.3For theGerman discourse corpus of (Gastel et al., 2011), (Versley, 2013) report 65% of implicit relations.In this paper, we tackle the problem of automatically identifying implicit discourse relations in French.To date, the large majority of studies on this task have focused on English, and to a lesser extent onGerman.
Performance remain relatively low compared to explicit relations, due to the lack of strongThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organisers.
License details: http://creativecommons.org/licenses/by/4.0/1All our examples are taken from the ANNODIS corpus: http://redac.univ-tlse2.fr/corpus/annodis/.2The former count does not include AltLex, EntRel and NoRel as implicit examples, whereas the latter does.3The first count does not include attribution, e-elaboration and frame examples.1694predictors.
Because it relies on more complex, interacting factors, the identification of implicit relationsrequires a lot of data.
But the available annotated for French is scarce: while the PDTB contains about40, 000 examples, the French ANNODIS only has about 3, 000 examples.
An additional challenge forbuilding such a system for French compared to English is the lack of external lexical ressources (e.g.,semantic verb classification, polarity database).A natural approach to deal with the lack of annotated implicit data is to resort to additional dataautomatically obtained from explicit examples in which the connective is removed (Marcu and Echihabi,2002).
Provided that one could reliably identify discourse connectives, this approach makes it possible tocreate large amounts of additional implicit data from raw texts.
Unfortunately, (Sporleder and Lascarides,2008) show that a system trained on this type of artificially generated data does not generalize well,leading to important performance degradation compared to a system solely trained on natural data.The central question we address in this paper is how to better leverage the large amount of automat-ically generated data.
We first show that the bad generalization performance of the system trained onartificial data lies in important distribution differences between the two datasets.
This analysis in turnleads us to investigate various simple schemes for combining natural and artificial data methods inspiredfrom the field of domain adaptation.
Our best combined system yields a significant improvement of 4.4%over a system solely trained on the available manually annotated data.The rest of this paper is organized as follows.
Section 2 summarizes previous works on implicitrelation identification.
In section 3, we describe the problems introduced by the use of artificial data andthe methods we develop to deal with them.
In section 4, we give a description of the data used, and insection 5, we detail our feature set.
Our experiments are then summarized in section 6.2 Related WorkTo date, there have been only a few attempts at building full document-based discourse parsers.
On theRST-DT (Carlson et al., 2001), the best performing system is (Joty et al., 2013), who report an F1scoreof 55.71 for labeled stuctures (with 23 relations).
On the same corpus, (Sagae, 2009) and (Hernault etal., 2010) report F1scores of 44.5 and 47.3, respectively.
On the PDTB, the parser of Lin et al.
(2010)obtains an F1score of 33 (16 explicit relations, 11 implicit relations).
On the ANNODIS corpus, Mulleret al.
(2012) reports F1scores of 36.1 (17 relations) and 46.8 (4 relations).These still modest performance are due to wrong attachment decisions, as well as to errors in relationlabeling.
Most of these latter errors are mostly imputable to wrong classifications of implicit relations.Thus, the current best accuracy performance on explicit PDTB relations are 94.15% on 4 relations (Pitlerand Nenkova, 2009), and 86.77% on 16 relations (Lin et al., 2010).
By contrast, the best identificationsystem for implicit PDTB relations obtains an accuracy of 65.4% on 4 relations in (Pitler et al., 2009), anddown to 40.2% for 11 of the level 2 relations of PDTB (Lin et al., 2009).
For German, Versley (2013)?sstudy on implicit relations reports 42.5 in F1for 5 relations and 18.7 for 21 relations.
For French, Mulleret al.
(2012) report an accuracy score of 63.6% for their relation labeling system (over 17 relations), butthey do not provide separate scores for explicit vs. implicit relations.This performance drop reflects the difficulty of identifying a rhetorical relation in the absence of anexplicit discourse marker.
As shown by (Park and Cardie, 2012), the identification of implicit relationsrelies on more diverse and noisy predictors from syntax (in the form of prediction rules) and (lexical)semantics (e.g., polarity, semantic classes and fine-grained semantic tags for verbs).
Unfortunately, mostof the semantic resources used to derive features for English (polarity database, Inquirer tags) are notavailable for French.
Zhou et al.
(2010) try to predict the implicit connectives annotated in the PDTBas a way of predicting the relation, a method only possible with this corpus.
They obtain results lowerthan those reported by (Park and Cardie, 2012).
In another context, Sporleder (2008) shows that usingWordNet is less effective than lemmatisation for capturing semantic generalization, and (Wang et al.,2010) use tree kernels in order to better capture important syntactic information.
In another context,Sporleder (2008) shows that using WordNet is less effective than lemmatisation for capturing semanticgeneralization, and (Wang et al., 2010) use tree kernels in order to better capture important syntacticinformation.1695Another set of studies we directly build upon explore the idea that many connectives unambiguouslytrigger a unique relation, thus allowing to construct massive amount of (artificially) labelled implicitexamples from raw data.
Marcu and Echihabi (2002) were the first to use this method: they were mainlyinterested in showing that a removed connective could be recovered from its linguistic context.
In turn,they only tested their approach on examples that were also generated automatically, and not on manuallyannotated implicit examples.
In this setting, they report an accuracy of 49.7 (6 classes), significantlyabove luck.
Reusing the same approach, Sporleder and Lascarides (2008) then showed that a systemtrained on a large amount of artificial examples (72000 examples) performs much worse than the samesystem trained on a much smaller amount of natural examples (1, 051 examples) implicit examples, withaccuracies of 25.8 and 40.3, respectively.Marcu and Echihabi?s (2002) original approach was based on the idea of finding pairs of semanticallyrelated words that together trigger a relation (such as ?nocturne/jour?
(?nocturnal/daytime?)
in example1 of contrast).
Interestingly, Pitler et al.
(2009) showed that word pairs extracted from artificial data arenot helpful for implicit relation identification and, moreover, that the most informative word pairs are notsemantically related.
Blair-Goldensohn et al.
(2007) showed that, for cause and contrast at least, resultscan be enhanced by improving the quality of the artificial data.
Finally, Wang et al.
(2012) propose afirst approach that exploits both natural and artificial data.
Specifically, they select the most informativetraining points among natural and artificial examples, both coming from the PDTB or the RST DT.
Theydefine deterministic rules for identifying so-called ?typical?
examples of a relation, the ?seed?
sets thatare then expanded using a simple clustering algorithm.
They report performance results well over thoseof (Pitler et al., 2009), but using a different evaluation protocole.4Also, their method is not easyto repoduce, especially for French, where we can not define the same deterministic rules as some ofthese depend on polarity information, for which we do not have external resources.
Furthermore, theirapproach only extracts 1 to 5% of the data as seed examples, which would represent too few examples onour corpus.
Finally, we are interested in finer-grained relations, thus more difficult to discriminate usingthese kind of rules.3 Proposed ApproachOur approach builds upon and extends the method of (Marcu and Echihabi, 2002) and (Sporleder andLascarides, 2008) by investigating different strategies for combining natural and artificial examples ofimplicit discourse relations.
These different combination schemes are inspired from domain adaptationand are motivated by the fact that artificial and natural examples follow different probability distributions.3.1 Distribution DifferencesMost machine learning algorithms are based on the assumption that data from training and test samplesare independently and identically distributed (i.e., the i.i.d.
sampling assumption).
Yet, it seems that theuse of artificial data clearly undermines this assumption.
There is indeed no guarantee that our artificialexamples should follow a distribution similar to that of the manual examples.
This leads to the problemof learning from non-iid data, a problem that has attracted growing attention these last years in machinelearning and NLP (Sogaard, 2013), (Hand, 2006).In this particular context, we have two sets of data with the same output space (i.e., the discourserelations), and the same kind of inputs space (i.e., spans of text).
But our data samples can differ in anumber of ways.
Following the terminology in (Moreno-Torres et al., 2012), we may encounter all thedifferent kinds of shift that can appear in a classification problem.Prior Probability Shift This shift describes changes in the marginal distribution of the output (i.e., therelations).
The artificial data do not have the same class distribution as the natural ones (see section 4).Neither do they have the same distribution as the natural explicit, because of the automatic extraction.This problem can be easily handled by resampling artificial data (see section 4).4Wang et al.
(2012) only use the first annotated relation and ignore the Entity relation, whereas Pitler et al.
(2009) keep allthe annotations and map Entity examples to the Expansion class.1696Covariate Shift This shift describes changes in the marginal distribution of the input (i.e., the pairs ofspans of text).
Artifical examples are originally explicit examples minus their connective, so it is rea-sonnable to think that these examples will have a different distribution from the natural implicit examples.Moreover, it is possible that, by removing the connective, we have made these examples semanticallyunfelicitous or even ungrammatical.
Segmentation is another issue, since it is automatic and based onheuristics (see section 4).
For example, artificial examples can not be multi-sentential whereas it can bethe case for natural ones.Concept Shift This shift describes changes in the joint distribution of inputs and outputs.
Considerfor instance the occurrences of relations within inter- and intra-sentential contexts.
The proportion ofinter-sentential examples in natural and artificial datasets is the same for contrast (57.1%), it is similarfor result (resp.
45.7% and 39.8%), but very different for continuation (resp.
70% and 96.5%) and forexplanation (resp.
21.4% and 53.0%).
Moreover, the extraction method is prone to errors, and it maybe the case that we wrongly identify a word form as a discourse connective.
Thus, we may produceexamples annotated with a wrong relation or that do not involve any discourse relation at all.
Finally,deleting a connective can make the discourse ackward or even incoherent (Asher and Lascarides, 2003).We can actually witness this with example (1).
As shown by (Sporleder and Lascarides, 2008), deletingthe connective can also change the inferred relation.
They found examples of explanation in which animplicit relation becomes the only one inferable after removing the explicit marker.The deletion canalso change the inferred relation (Sporleder and Lascarides, 2008).
We found an even worse effect inour French corpus.
In example (3), the connective puisqu(e) (because) triggers an explanation, thus theevents are ordered following the causal law.
The cause, ?migrer?
(?migrate?
), comes before the effect,?deviennent?
(?becomes?).
But when we delete the connective, the order of the events seems to bereversed.
Keeping the first clause as the first argument, we then obtain a result relation in this sentence.
(3) Les Amorrites deviennent ?
la p?riode suivante de s?rieux adversaires des souverains d?Ur,puisqu?ils commencent alors ?
migrer en grand nombre vers la M?sopotamie.In the next period, Amorrites become severe opponents of the sovereigns of Ur, because they thenbegin to migrate in large numbers to Mesopotamia.3.2 Methods Inspired by Domain AdaptationA way to deal with all the distribution differences observed is to reframe our problem within the frame-work of domain adaptation.
Informally, the task of domain adaptation is to port some system from onedomain, the source, to another, the target.
Informally, we have a distribution Dsfor the source data anda distribution Dtfor the target data.
The goal of the classifier is to build a good approximation of Dt.
Ifone uses data following the distribution Dsin order to build this approximation, then the performancewill depend of the similarity between Dsand Dt.
If these distributions are too dissimilar, the approxi-mation will be bad and so will be the performance.
It is the case in particular when the domains (e.g.,text genres) are different.
The goal of domain adaptation is precisely to deal with data from differentdistributions (Jiang, 2008), (Mansour et al., 2009).
We are not exactly in the same setting, but we canregard the artificial data as the source, and the natural data, on which we evaluate, as the target.As a first step, we decided to investigate the simplest domain adaptation methods there is, such asthose described in (Daum?
III, 2007).
These methods either combine directly the data or the modelsbuilt on each set of data.
Performance of all these systems will be compared to the base systems trainedon only one set of data, in section 6.Data combination The first possibility is to combine the data.
The first model is trained on all naturaland artificial data together (UNION).
This method does not allow us to control the importance of the twosets of data nor to evaluate their influence on the system.
We thus refine it in two ways.
First, we onlyadd to the manual data randomly selected samples from the artificial data (ARTSUB).
Alternatively, wekeep all the artificial examples but reweight (or, equivalently, duplicate) the manual examples (NATW).Both these schemes allow us to avoid a massive imbalance between the two kinds of data.1697Model combination The second strategy consists in combining the models.
A first set of methodsinvolve adding new features.
That is, we train a model on the artificial data, then run it on the naturalexamples.
We use these predictions as new attributes for the natural model (ADDPRED).
The parameterassociated to the attribute therefore measures the importance to be given to the predictions made by themodel trained on artificial data.
We propose a variation of this method by adding the probabilities of eachprediction as supplementary attributes (ADDPROB).
The intuition is that even if the classifier is wrong, itcould still be consistent in its errors.
Yet another model combination consists in using the parameters ofthe artificial model as initial values for the manual model parameters (ARTINIT).
This method allows togive an initial information to the natural model rather than a random intialization.
Finally, we also builda model by linearly interpolating the two basic models (LININT).In addition to these combination schemes, we also add a method to automatically select examplesamong the artificial set based on the confidence of the artificial model.
Its aim is to filter out noisyexamples, our hypothesis being that the more confident the model, the less noisy the example.4 DataIn this work, we choose to focus on 4 relations, contrast, result, continuation and explanation, eachof which can be either explicit or implicit.
These are the same as the relations used in (Sporleder andLascarides, 2008), allowing for easy comparison across languages, with the exception of the relationsummary which does not appear in the ANNODIS corpus.
Although it is difficult to map these relationsonto the relation set of the PDTB, we can say that our relations are closer to level 2 and level 3 (i.e.,fine-grained) PDTB relations than level 1 (i.e., coarse-grained) ones.4.1 Manually Annotated Data: ANNODISOur natural implicit examples are taken from the ANNODIS corpus, which is to date the only availableFrench corpus annotated at the discourse level.
Its annotations are based on the SDRT framework (Asherand Lascarides, 2003).
It consists of 86 newspaper and Wikipedia articles.
3, 339 examples have beenannotated using 17 relations.
In way of comparison, note that the PDTB has roughly 12 times moreannotated relations than ANNODIS.
Documents are segmented in Elementary Discourse Units (EDUs)which can be clauses, prepositionnal phrases and some adverbials and parentheticals if the span of textdescribes an event.
The relations link EDUs and complex segments, adjacent or not.
The connectives arenot annotated, which means that the examples of implicit relations had to be extracted automatically.The corpus has been pre-processed using the MELt tagger (Denis and Sagot, 2009) for POS-tagging,lemmatization and morphological markings.
Then, the documents have been parsed using the the MST-Parser (McDonald and Pereira, 2006) trained for French by (Candito et al., 2010).
In order to identifyimplicit examples, we used the French lexicon of connectives (LexConn) developed by Roze et al.
(2012).We simply matched all possible connective forms associated with the annotated relations (discarding ?,which is too ambiguous).
We did not add constraints on the connective position, as we wanted to besure to exclude all explicit examples, this method led us to miss a few implicit examples.
Out of 1, 108examples annotated with one of the 4 relations considered, 494 were found to be implicit (see table 2).4.2 Automatically Annotated DataThe artificial data are automatically extracted from raw data using heuristic rules.
We use LexConn tomine explicit instances in the corpus Est R?publicain composed of newspaper articles (9M sentences),with the same pre-processings as ANNODIS.
LexConn contains 329 connectives, among them, 131 areunambiguous for our 4 relations.
We grouped pragmatic relations (i.e., the relation is between speechacts) and non pragmatic relations (i.e., the relation is between facts) relations, assuming they involve thesame kind of predictors, and the 3 contrastive relations, as only one type of contrast is annotated in ANN-ODIS.
We did not take into account 3 connectives corresponding to unknown part-of-speech.
Our firstevaluation led us to delete 6 connectives, very ambiguous between discourse and non discourse readings,such as ?maintenant?
(?now?).
We eventually settled on 122 connectives, among which 100 were seenin the corpus in a configuration matching one of our pre-defined patterns.
As a comparison, (Sporleder1698and Lascarides, 2008) only had 50 such connectives.
We finally use 122 connectives, among which 100were seen in a correct configuration in the corpus.
As a comparison, 50 were used in (Sporleder andLascarides, 2008).Position Part-of-speech Patterns ExamplesInter-sentential All POS A1.
C(,) A2.
A1.
Malheureusement(,) A2A1.
Surtout, A2.Adv.A1.
beg-A2(,) C(,) end-A2.
A1.
beg-A2, de plus, end-A2.A1.
beg-A2(,) en outre(,) end-A2.A1.
A2, C. A1.
A2, remarque.Intra-sentential All POS A1, C(,) A2.
A1, de plus(,) A2.A1(,) donc(,) A2.SC and Prep.
C A1, A2.
Preuve que A1, A2.Puisque A1, A2.Adv.A1, beg-A2(,) C (,) end-A2.
A1, beg-A2, de plus, end-A2.A1, beg-A2(,) en outre(,) A2.A1, A2, C. A1, A2, r?flexion faite.Table 1: Defined patterns with some examples.
?A1?
stands for the first argument, ?A2?
for the secondand ?C?
stands for the connective ; ?beg?
and ?end?
stand resp.
for the beginning and the end of anargument ; ?(x)?
indicates that ?x?
is not necessary, depending on the connective form.
Some patternsare only possible for some sets of connectives based on their part-of-speech (Subordinating Conjunction(SC), Preposition (Prep.
), Averbials (Adv.
)).The heuristic used to extract the examples has two main steps.
First, we search forms used in discoursereadings using patterns (see table 1) that were manually defined for each connective based on its position,its part-of-speechand the punctuation around it.
Second, we identify the connectives arguments using thesame information.
We make the same simplifying assumptions as in the previous studies: an argumentcovers at most one sentence, and we have at most 2 EDUs within a sentence.
As additional constraint,we also require the presence of a verb in each relation argument.
When two connectives occur in thesame segment, it is possible that one modifies the other.
In turn, a naive extraction could produce twoexamples with different relations but the same arguments.
To avoid the creation of spurious examples,we extract two examples in these cases only if one is inter- and the other intra-sentential according to ourextraction patterns.Natural dataset Artificial datasetRelation Explicit Implicit Available Training Testcontrast 100 42 252 793 23 409 2 926result 52 110 50 297 23 409 2 926continuation 404 272 29 261 23 409 2 926explanation 58 70 59 909 23 409 2 926All 614 494 392 260 93 636 11 704Table 2: Number of examples in our corpora, for the natural dataset, only the implicit examples are used.This simple method allows to quickly generate a large amount of data.
In total, we extracted 392, 260examples (see table 2).
This initial dataset was rebalanced in a way to keep the maximum number ofavailable examples (thus dealing with the prior probability shift).
We used 80% of the data as trainingset, and 10% the development and test set.
Note that there are some important differences in the labeldistributions between natural and artificial data.
For instance, the most represented relation in the naturaldata (continuation) is the least represented in the artificial data.
This is because the connectives thattrigger this relation are highly ambiguous between discourse and non-discourse readings.
Finally, thismethod generates some noise: out of 250 random examples, we found 37 errors in span boudaries and169918 cases in which the connective form does not have a discourse reading.5 FeaturesWe adapted various features used in previous studies.
The lack of ressources for French prevented usfrom using them all, especially the semantic ones.
These features correspond to surface information andothers more linguistic.
As a comparison, (Marcu and Echihabi, 2002) only used pairs of words.Sporleder and Lascarides (2008) used various linguistic features but no syntaxic ones.
(Wang et al.,2012) used semantic, syntaxic and lexical information.
We used lexico-syntaxic information.
Finally,note that our goal is to evaluate the efficiency of data combinations.
Thus we did not try to optimize thisfeature set, as it would have introduced another parameter in our model.Indication of syntactic complexity: we compute the number of nominal, verbal, prepositional, adjec-tival and adverbial phrases.Information concerning the heads of the arguments: we keep the lemma of negative element linkedto the head, we also get some temporal/aspectual information (number of auxiliaries dependent of thehead, tense, person, number of the auxiliaries), information about the heads dependents (if an object, aby-object or a modifier is present ; if a preposition dependent of the head, subject or object is present ;part-of-speech of the modifiers and prepositional dependents of the head, subject and object) and somemorphological information (tense and person of the head if verbal, gender if non verbal, number of thehead, precise part-of-speech, ?VPP?, and simplified,?V?).
We also add features pairing the tenses forverbal heads and the heads numbers.Position: we add a feature indicating if the example is inter or intra-sentential.Indication of thematic continuity: we compute general lemma overlap and lemma overlap for openclass words.6 ExperimentsOur main objective is to assess whether one can use the artificial data to improve the performance of asystem solely based on data manually annotated only available in small amount.
We therefore test themethods described in section 3.We experimented with a maximum entropy classifier from the MegaM5package, in multiclass clas-sification, with a maximum of 100 iterations.
We did not try to optimize the regularization parameterwhich is then equal to 1.We rebalance the corpus of manually annotated data to a maximum of 70 examples per relation.6Wehave too few annotated examples to be able to construct a separate test set sufficiently large to makestatistical significance test.
Thus, we decided to make a stratified nested cross-validation.
It has beenshown that this method provides an estimate of the error that is very close to that one could obtainon an independent evaluation set ((Varma and Simon, 2006), (Scheffer, 1999)), as it prevents us fromoptimizing our hyper-parameters and performing evaluation on the same data.
Specifically, there are twocross-validation loops: the inner loop is used for tuning the hyper-parameters (as described in section6.2) and the outer loop estimates the generalization error.
The data are first split into N folds.
We takethe fold k (with 1 ?
k ?
N ) as the current evaluation set.
The N ?
1 other folds are used as trainingdata and split into M folds used for model fitting.
The best model is then evaluated on the fold k.Finally, we report performance on the N folds.
We used two 5-fold cross-validation in order to selectand evaluate the best models for the systems described in section 3.2.
We have no guarantee to select thebest models at each test step, but this procedure allows to evaluate the stability of the system with respectto the hyper-parameters (i.e.
the chosen values should not be too scattered), the overfitting (i.e.
inner and5http://www.umiacs.umd.edu/~hal/megam/version0_3/6Our focus is on the methodology of data combination, so we left for future work the issue of dealing with the highlyimbalanced relation distribution of the natural data.
Incidentally, note that this setting prevents us from getting a system solelyperforming well on highly frequent relations.1700outer estimations should be close) and the stability of the models (i.e.
variance in the predictive capacity,between the results on the outer folds).As in the previous studies, we report performance using micro-averaged accuracy and F1score perrelation.
In order to evaluate the statistical significance of our results, we use the Student?s t-test (with p-value< 0.05) which has been proved to work with very small sample (see (de Winter, 2013)) if the effectsize (computed using the Cohen coefficient) and the correlation between the sample are large enough,while, as noted in (de Winter, 2013), the Wilcoxon signed rank test (that we initially tried) could lead tooverestimated p-value with such small sample.
The results of the most relevant systems are presented intable 3.Without selection With selectionNATONLY ARTONLY ADDPRED ARTINIT ADDPRED+SELEC NATW+selecAccuracy 37.3 23.0 39.3 40.1 41.7?41.3contrast 15.0 23.2 16.0 16.9 20.8 19.2result 47.6 15.7 50.6 45.9 51.0 48.3continuation 28.1 32.1 31.9 34.0 31.2 32.4explanation 47.9 22.4 46.7 52.2 53.9 53.4Table 3: Most relevant systems, with or without selection of examples, overall accuracy and F1score perrelation,?corresponds to a significant improvement over NATONLY.6.1 Basic ModelsIn the first set of experiments, we trained two classifiers.
The first one is trained on the natural implicitdata (NATONLY, 252 examples), and the second one on the artificial implicit data (ARTONLY, 93, 636examples).
We test both models on natural implicit data.The overall accuracy of the NATONLY model is 37.3 with F1score ranging from 15.0 for contrastto 47.9 for explanation.
The performance on contrast is fairly low, probably because this relation isthe least frequent in our training set.
Note that the overall accuracy obtained is quite close to the 40.3obtained for English by (Sporleder and Lascarides, 2008).The overall accuracy of the ARTONLY model is 47.8 when evaluated on the same type of data, thatis, artificial ones (11, 704 test examples), but only 23.0 when evaluated on natural data.
This significantdrop in performance has been observed in the previous studies on English.
It can be attributed to thedistribution differences described in section 3.
We can observe that the use of the artificial data lowersthe F1score for result and explanation while, for contrast, F1score is raised by about 10 points.6.2 Models with CombinationsIn this section, we present the results for the systems using both natural and artificial data.
We eitherdirectly combine the data or use the data to build separate models that are then combined.
Some of thesemodels use hyper-parameters.
When weighting the natural examples, we test weights c ?
[0.5, 1, 5]and c ?
[10; 2000] with an increment of 10 until 100, of 50 until 1000 and of 500 until 2000.
Whenadding random subsets of artificial data, we add each time k times the number of natural examplesartificial examples with k ?
[0.1; 600] with an increment of 0.1 until 1, of 10 until 100 and of 50 until600.
Finally, when taking a linear interpolation of the models, we build a new model by weighting theartificial model by ?
?
[0.1; 0.9] with increments of 0.1.In general, we observe that most of the systems lead to similar or higher accuracy than NATONLY, butnone of the improvements is statistically significant.
The best system is ARTINIT (accuracy 40.1, p-valueof 0.18 and a small effect size, 0.39).
Two other systems get an accuracy score better than 39, that is AD-DPRED (39.3) and LININT (39.3), but not significantly better than NATONLY.
The system ADDPROB,similar to ADDPRED, leads to lower accuracy, showing that adding the probabilities decrease the per-formance.
For these systems, the scores on each of the outer folds are close7, specially for ADDPRED,7ARTINIT : standard deviation (sd) = 0.074, mean = 40.1 ; ADDPRED : sd = 0.037, ADDPROB sd = 0.061, mean ' 391701revealing a high model stability.
The other systems allow to evaluate the impact of the artificial data onthe final results.The only method leading to lower results is when training on the union of the data sets (UNION), theaccuracy (22.6) is similar to ARTONLY.
This was expected, as the natural data are about 372 times lessnumerous than the artificial ones, the new model is thus more influenced by the latter.
Note that Wang etal.
(2012) also experiment this setting but do not observe such a gap, maybe because their artificial dataare based on manually annotated explicit examples, which are likely to be less noisy.When directly combining the data, either by adding random subsets of the artificial data (ARTSUB,accuracy 34.5) or by weighting the natural examples (NATW, accuracy 38.9), we observe, on the in-ner folds, an inverse trend.
As expected, the accuracy increases as the influence of the artificial datadecreases, that is, decreasing the coefficients for ARTSUB and increasing the weights for NATW.
Ob-serving the results in the inner folds reveals a same trend about the relative importance of the two kindsof data: natural data have to be around 2.5 times more important than the artificial ones.
We also ob-serve this effect with LININT, with the mean of the choosen ?
values equals to 0.3.
We also note thatthe variance for the values of the hyper-parameter for ARTSUB is pretty high, probably caused by therandomness of the subsamples selection.
It is a bit lower for NATW and LININT showing that thesemethods are more robust.
Nevertheless, the strategy does not give an a priori good value for the hyper-parameter but restricts the space of values (1020 plus or minus 272 for NATW and 0.3 plus or minus 0.18for LININT).6.3 Models with Automatic Selection of ExamplesPrevious experiments showed that adding artificial data mostly improves the performance but still notsignificantly.
We assume that a lot of the artificial data are noisy, which could hurt the systems.
Themethod of selection of examples thus aims at eliminating potentially noisy examples.
The artificialmodel is used on the training set, and we keep the examples that are predicted with a probability higherthan a threshold s ?
[0.3; 0.85] with an increment of 0.1 until 0.5 and of 0.05 until 0.85.
If the modelis confident enough about its prediction, the example might not correspond to noise, that is, a wordform that does not have a discourse readings and/or a segmentation error.
We also check whether theconnective is redundant.
For each threshold, we rebalance the data based on the least represented relation(+SELEC systems).The automatic selection of examples allows to improve previous results.
The accuracy of the AR-TONLY model moves from 23.0 to 25.0 with selection, and the system UNION move from 22.6 to 40.1with selection.The best results are obtained when we use artificial data to create new features but when we add onlythe relation predicted by the artificial model (ADDPRED+SELEC).
With this system, we observe a cleartendency toward significance (accuracy 41.7 with a large effect size, 0.756, and a high correlation, 0.842).The F1scores for all classes are improved : 20.8 for contrast, 51.0 for result, 31.2 for continuation and53.9 for explanation.
Two other systems get an accuracy over 40: NATW+SELEC (accuracy 41.3, witha trend toward significance8) and UNION+SELEC (no significantly higher than NATONLY).
We note thatADDPRED corresponds to the best baseline in (Daum?
III and Marcu, 2006), which shows the relevanceof dealing with the distributions differences in our data through domain adaptation methods.The automatic selection step allows a more important weight on the informations provided by theartificial data.
For LININT+SELEC, the best results are obtained with an almost equal influence of thetwo models.
In the same way, the mean of the choosen values for the coefficient for NATW+SELECis much lower, and it increases a lot for ARTSUB+SELEC allowing for larger subsamples.
Even if thechoosen values are widly scattered, these observations tend to prove that the selection improves thequality of our artificial corpus.
Regarding the choosen values for the thresholds, the mean over all thesystems is 0.7, with a variable standard deviation but always greater than 0.1.
This deviation is prettyhigh, this hyper-parameter probably needs a better optimisation, by repeating the inner loop for example,but these experiments will allow to reduce the search space.8p-value = 0.077, large effect size, 0.68 and high correlation, 0.671702The automatic selection of examples leads to one system, namely ADDPRED+SELEC, significantlyimproving the accuracy of NATONLY.
This shows that the artificial data, when rightly integrated, canthus be used to improve a system identifying implicit relations, especially if their influence is low, themodel is driven towards the good distribution.6.4 Effects on the Identification of the RelationsLooking at the F1score per relation, we observed that these systems have dissimilar behaviors.
A largerinfluence of the artificial model allows improvements for contrast: the best result for this relation is ob-tained when only the artificial data are used for training (at best, 28.8 F1score with ARTONLY+SELEC).The identification of the relation continuation seems to be also improved by the influence of the artificialdata.
We can observe it with the linear interpolation of the models: the mean of the F1score increaseswith the increasing of the ?
coefficient for these relations.
For continuation, however, the best mean F1is obtained with ?
= 0.8, this relation needs a certain degree of influence from the natural data.
Somesupport for this proposition can be found in the fact that the best result for this relation is obtained withNATW (at best, 44.7 F1score).
For the other relations, a large weight on the artificial data clearly de-creases the F1score.
However, the identification of explanation is improved when we add the predictionsof the artificial model (at best, ADDPRED+SELEC, 53.9 F1score).
Improvement is fairly low for result(at best, 51.0 with ADDPRED+SELEC).The relation contrast might take advantage of less noisy artificial data as most of the examples areextracted using the connective mais (but) always in discourse readings.
For explanation, predictions ofthe artificial model could be quiet coherent as most of the artificial examples correspond to the pragmaticrelation explanation?.
Moreover, if we look at the feature distribution (850 features overall), we observea gap of more than 30% for 2 and 5 features for result and explanation that is not observed for contrastand continuation, the relations that make the most of the artificial data.7 ConclusionWe have presented the first system that identifies implicit discourse relations for French.
This kindof relation is difficult to identify because of the lack of specific predictors.
In the previous studieson English, the performance on this task are fairly low despite the use of complex features, probablybecause of a lack of manually annotated data.
To deal with this issue, even more crucial for French,our system also resorts to additional data, automatically annotated using discourse connectives.
Thesenew data, however, do not generalize well to natural implicit data, because of distribution differences.We thus test methods inspired by domain adaptation in order to combine natural and artificial data.We add an automatic selection of examples among the artificial data to deal with noise generated bythe method of automatic annotation.
We manage to get significant improvement over a system solelytrained using available data manually annotated by using automatic selection and the addition of featurescorresponding to the predictions of the artificial model.In future work, we will explore more sophisticated methods to deal with data samples that followdifferent distributions.
We will also explore ways to deal with imbalanced data and use our methods onall the relations annotated in our French corpus.
Finally, we will test these methods on English corpora,in order to compare their efficiency with previous studies.ReferencesStergos Afantenos, Nicholas Asher, Farah Benamara, Myriam Bras, C?cile Fabre, Lydia-Mai Ho-Dac, AnneLe Draoulec, Philippe Muller, Marie-Paule P?ry-Woodley, Laurent Pr?vot, Josette Rebeyrolles, Ludovic Tanguy,Marianne Vergez-Couret, and Laure Vieu.
2012.
An empirical resource for discovering cognitive principles ofdiscourse organisation: the annodis corpus (regular paper).
In Proceedings of LREC.Nicholas Asher and Alex Lascarides.
2003.
Logics of Conversation.
Cambridge University Press.Sasha Blair-Goldensohn, Kathleen R. McKeown, and Owen C. Rambow.
2007.
Building and refining rhetorical-semantic relation models.
In Proceedings of NAACL HLT.1703Marie Candito, Joakim Nivre, Pascal Denis, and Enrique Henestroza Anguiano.
2010.
Benchmarking of statisticaldependency parsers for french.
In Proceedings of ICCL (posters).Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2001.
Building a discourse-tagged corpus in theframework of rhetorical structure theory.
In Proceedings of the Second SIGdial Workshop on Discourse andDialogue.Hal Daum?
III and Daniel Marcu.
2006.
Domain adaptation for statistical classifiers.
Journal of Artificial Intelli-gence Research.Hal Daum?
III.
2007.
Frustratingly easy domain adaptation.
In Proceedings of ACL.Joost C.F.
de Winter.
2013.
Using the student?s t-test with extremely small sample sizes.
Practical Assessment,Research & Evaluation.Pascal Denis and Beno?t Sagot.
2009.
Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art POS tagging with less human effort.
In Proceedings of PACLIC.Anna Gastel, Sabrina Schulze, Yannick Versley, and Erhard Hinrichs.
2011.
Annotation of implicit discourserelations in the t?ba-d/z treebank.
GSCL.David J.
Hand.
2006.
Classifier technology and the illusion of progress.
Statistical Science.Hugo Hernault, Helmut Prendinger, David A. duVerle, and Mitsuru Ishizuka.
2010.
Hilda: A discourse parserusing support vector machine classification.
Dialogue and Discourse.Jing Jiang.
2008.
A literature survey on domain adaptation of statistical classifiers.
Available from:http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/da_survey.html.Shafiq R. Joty, Giuseppe Carenini, Raymond T. Ng, and Yashar Mehdad.
2013.
Combining intra- and multi-sentential rhetorical parsing for document-level discourse analysis.
In Proceedings of ACL.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.
Recognizing implicit discourse relations in the penn discoursetreebank.
In Proceedings of EMNLP.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A PDTB-styled end-to-end discourse parser.
Technicalreport, National University of Singapore.Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh.
2009.
Domain adaptation: Learning bounds andalgorithms.
In Proceedings of COLT.Daniel Marcu and Abdessamad Echihabi.
2002.
An unsupervised approach to recognizing discourse relations.
InProceedings of ACL.Ryan McDonald and Fernando Pereira.
2006.
Online learning of approximate dependency parsing algorithms.
InProceedings of EACL.Jose G. Moreno-Torres, Troy Raeder, Roc?o Alaiz-Rodr?guez, Nitesh V. Chawla, and Francisco Herrera.
2012.
Aunifying view on dataset shift in classification.
Pattern Recognition.Joonsuk Park and Claire Cardie.
2012.
Improving implicit discourse relation recognition through feature setoptimization.
In Proceedings of SIGDIAL.Emily Pitler and Ani Nenkova.
2009.
Using syntax to disambiguate explicit discourse connectives in text.
InProceedings of the ACL-IJCNLP (Short Papers).Emily Pitler, Annie Louis, and Ani Nenkova.
2009.
Automatic sense prediction for implicit discourse relations intext.
In Proceedings of ACL-IJCNLP.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.2008.
The penn discourse treebank 2.0.
In Proceedings of LREC.Charlotte Roze, Laurence Danlos, and Philippe Muller.
2012.
Lexconn: A french lexicon of discourse connectives.Discours.Kenji Sagae.
2009.
Analysis of discourse structure with syntactic dependencies and data-driven shift-reduceparsing.
Proceedings of IWPT.1704Tobias Scheffer.
1999.
Error Estimation and Model Selection.
Ph.D. thesis, Technischen Universitet Berlin,School of Computer Science.Anders Sogaard.
2013.
Semi-supervised learning and domain adaptation in natural language processing.
Morgan& Claypool.Caroline Sporleder and Alex Lascarides.
2008.
Using automatically labelled examples to classify rhetoricalrelations, an assessment.
Natural Language Engineering.Caroline Sporleder.
2008.
Lexical models to identify unmarked discourse relations: Does Wordnet help?
Lexical-Semantic Resources in Automated Discourse Analysis.Sudhir Varma and Richard Simon.
2006.
Bias in error estimation when using cross-validation for model selection.BMC bioinformatics.Yannick Versley.
2013.
Subgraph-based classification of explicit and implicit discourse relations.
In Proceedingsof IWCS.WenTing Wang, Jian Su, and Chew Lim Tan.
2010.
Kernel based discourse relation recognition with temporalordering information.
In Proceedings of ACL.Xun Wang, Sujian Li, Jiwei Li, and Wenjie Li.
2012.
Implicit discourse relation recognition by selecting typicaltraining examples.
In Proceedings of COLING (Technical Papers).Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian Su, and Chew Lim Tan.
2010.
Predicting discourse connec-tives for implicit discourse relation recognition.
In Proceedings of COLING (Posters).1705
