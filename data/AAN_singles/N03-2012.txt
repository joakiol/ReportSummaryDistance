DETECTION OF AGREEMENT vs.
DISAGREEMENT IN MEETINGS:TRAINING WITH UNLABELED DATADustin Hillard and Mari OstendorfUniversity of Washington, EE{hillard,mo}@ee.washington.eduElizabeth ShribergSRI International and ICSIees@speech.sri.comAbstractTo support summarization of automaticallytranscribed meetings, we introduce a classifierto recognize agreement or disagreement utter-ances, utilizing both word-based and prosodiccues.
We show that hand-labeling efforts canbe minimized by using unsupervised trainingon a large unlabeled data set combined withsupervised training on a small amount of data.For ASR transcripts with over 45% WER, thesystem recovers nearly 80% of agree/disagreeutterances with a confusion rate of only 3%.1 IntroductionMeetings are an integral component of life in most or-ganizations, and records of meetings are important forhelping people recall (or learn for the first time) whattook place in a meeting.
Audio (or audio-visual) record-ings of meetings offer a complete record of the inter-actions, but listening to the complete recording is im-practical.
To facilitate browsing and summarization ofmeeting recordings, it is useful to automatically annotatetopic and participant interaction characteristics.
Here, wefocus on interactions, specifically identifying agreementand disagreement.
These categories are particularly im-portant for identifying decisions in meetings and infer-ring whether the decisions are controversial, which can beuseful for automatic summarization.
In addition, detect-ing agreement is important for associating action itemswith meeting participants and for understanding socialdynamics.
In this study, we focus on detection using bothprosodic and language cues, contrasting results for hand-transcribed and automatically transcribed data.The agreement/disagreement labels can be thought ofas a sort of speech act categorization.
Automatic classifi-cation of speech acts has been the subject of several stud-ies.
Our work builds on (Shriberg et al, 1998), whichshowed that prosodic features are useful for classifyingspeech acts and lead to increased accuracy when com-bined with word based cues.
Other studies look at predic-tion of speech acts primarily from word-based cues, usinglanguage models or syntactic structure and discourse his-tory (Chu-Carroll, 1998; Reithinger and Klesen, 1997).Our work is informed by these studies, but departs signif-icantly by exploring unsupervised training techniques.2 ApproachOur experiments are based on a subset of meeting record-ings collected and transcribed by ICSI (Morgan et al,2001).
Seven meetings were segmented (automatically,but with human adjustment) into 9854 total spurts.
Wedefine a ?spurt?
as a period of speech by one speaker thathas no pauses of greater than one half second (Shriberg etal., 2001).
Spurts are used here, rather than sentences,because our goal is to use ASR outputs and unsuper-vised training paradigms, where hand-labeled sentencesegmentations are not available.We define four categories: positive, backchannel, neg-ative, and other.
Frequent single-word spurts (specifi-cally, yeah, right, yep, uh-huh, and ok) are separated outfrom the ?positive?
category as backchannels because ofthe trivial nature of their detection and because they mayreflect encouragement for the speaker to continue morethan actual agreement.
Examples include:Neg: (6%) ?This doesn?t answer the question.
?Pos: (9%) ?Yeah, that sounds great.
?Back: (23%) ?Uh-huh.
?Other: (62%) ?Let?s move on to the next topic.
?The first 450 spurts in each of four meetings werehand-labeled with these four categories based on listeningto speech while viewing transcripts (so a sarcastic ?yeah,right?
is labeled as a disagreement despite the positivewording).
Comparing tags on 250 spurts from two label-ers produced a kappa coefficient (Siegel and Castellan,1988) of .6, which is generally considered acceptable.Additionally, unlabeled spurts from six hand-transcribedtraining meetings are used in unsupervised training ex-periments, as described later.
The total number of au-tomatically labeled spurts (8094) is about five times theamount of hand-labeled data.For system development and as a control, we use hand-transcripts in learning word-based cues and in training.We then evaluate the model with both hand-transcribedwords and ASR output.
The category labels from thehand transcriptions are mapped to the ASR transcripts,assigning an ASR spurt to a hand-labeled reference ifmore than half (time wise) of the ASR spurt overlaps thereference spurt.Feature Extraction.
The features used in classificationinclude heuristic word types and counts, word-based fea-tures derived from n-gram scores, and prosodic features.Simple word-based features include: the total num-ber of words in a spurt, the number of ?positive?
and?negative?
keywords, and the class (positive, negative,backchannel, discourse marker, other) of the first wordbased on the keywords.
The keywords were chosen basedon an ?effectiveness ratio,?
defined as the frequency of aword (or word pair) in the desired class divided by the fre-quency over all dissimilar classes combined.
A minimumof five occurrences was required and then all instanceswith a ratio greater than .6 were selected as keywords.Other word-based features are found by computing theperplexity (average log probability) of the sequence ofwords in a spurt using a bigram language model (LM)for each of the four classes.
The perplexity indicates thegoodness of fit of a spurt to each class.
We used bothword and class LMs (with part-of-speech classes for allwords except keywords).
In addition, the word-based LMis used to score the first two words of the spurt, whichoften contain the most information about agreement anddisagreement.
The label of the most likely class for eachtype of LM is a categorical feature, and we also computethe posterior probability for each class.Prosodic features include pause, fundamental fre-quency (F0), and duration (Baron et al, 2002).
Featuresare derived for the first word alone and for the entirespurt.
Average, maximum and initial pause duration fea-tures are used.
The F0 average and maximum featuresare computed using different methods for normalizing F0relative to a speaker-dependent baseline, mean and max.For duration, the average and maximum vowel durationfrom a forced alignment are used, both unnormalized andnormalized for vowel identity and phone context.
Spurtlength in terms of number of words is also used.Classifier design and feature selection.
The overallapproach to classifying spurts uses a decision tree clas-sifier (Breiman et al, 1984) to combine the word basedand prosodic cues.
In order to facilitate learning of cuesfor the less frequent classes, the data was upsampled (du-plicated) so that there were the same number of trainingpoints per class.
The decision tree size was determinedusing error-based cost-complexity pruning with 4-foldcross validation.
To reduce our initial candidate featureset, we used an iterative feature selection algorithm thatinvolved running multiple decision trees (Shriberg et al,2000).
The algorithm combines elements of brute-forcesearch (in a leave-one-out paradigm) with previously de-termined heuristics for narrowing the search space.
Weused entropy reduction of the tree after cross-validationas a criterion for selecting the best subtree.Unsupervised training.
In order to train the modelswith as much data as possible, we used an unsupervisedclustering strategy for incorporating unlabeled data.
Fourbigram models, one for each class, were initialized bydividing the hand transcribed training data into the fourclasses based upon keywords.
First, all spurts which con-tain the negative keywords are assigned to the negativeclass.
Backchannels are then pulled out when a spurt con-tains only one word and it falls in the backchannel wordlist.
Next, spurts are selected as agreements if they con-tain positive keywords.
Finally, the remaining spurts areassociated with the ?other?
class.The keyword separation gives an initial grouping; fur-ther regrouping involves unsupervised clustering using amaximum likelihood criterion.
A preliminary languagemodel is trained for each of the initial groups.
Then, byevaluating each spurt in the corpus against each of thefour language models, new groups are formed by asso-ciating spurts with the language model that produces thelowest perplexity.
New language models are then trainedfor the reorganized groups and the process is iterated un-til there is no movement between groups.
The final classassignments are used as ?truth?
for unsupervised trainingof language and prosodic models, as well as contributingfeatures to decision trees.3 Results and DiscussionHand-labeled data from one meeting is held out for testdata, and the hand-labeled subset of three other meet-ings are used for training decision trees.
Unlabeled spurtstaken from six meetings, different from the test meeting,are used for unsupervised training.
Performance is mea-sured in terms of overall 3-way classification accuracy,merging the backchannel and agreement classes.
Theoverall accuracy results can be compared to the ?chance?rate of 50%, since testing is on 4-way upsampled data.In addition, we report the confusion rate between agree-ments and disagreements and their recovery (recall) rate,since these two classes are most important for our appli-cation.Results are presented in Table 1 for models using onlyword-based cues.
The simple keyword indicators usedin a decision tree give the best performance on hand-transcribed speech, but performance degrades dramati-cally on ASR output (with WER > 45%).
For all othertraining conditions, the degradation in performance forthe system based on ASR transcripts is not as large,though still significant.
The system using unsupervisedtraining clearly outperforms the system trained only on asmall amount of hand-labeled data.
Interestingly, whenHand Transcriptions ASR TranscriptionsOverall A/D A/D Overall A/D A/DFeatures Accuracy Confusion Recovery Accuracy Confusion RecoveryKeywords 82% 2% 87% 61% 7% 53%Hand Trained LM 71% 13% 74% 64% 10% 67%Unsupervised LM 78% 10% 81% 67% 14% 70%All word based 79% 8% 83% 71% 3% 78%Table 1: Results for detection with different classifiers using word based features.the keywords are used in combination with the languagemodel, they do provide some benefit in the case wherethe system uses ASR transcripts.The results in Table 2 correspond to models using onlyprosodic cues.
When these models are trained on only asmall amount of hand-labeled data, the overall accuracyis similar to the system using keywords when operatingon the ASR transcript.
Performance is somewhat betterthan chance, and use of hand vs. ASR transcripts (and as-sociated word alignments) has little impact.
There is asmall gain in accuracy but a large gain in agree/disagreerecovery from using the data that was labeled via the un-supervised language model clustering technique.
Unfor-tunately, when the prosody features are combined withthe word-based features, there is no performance gain,even for the case of errorful ASR transcripts.Transcripts Overall A/D A/DTrain/Test Accuracy Confusion RecoveryHand/Hand 62% 17% 62%Unsup./Hand 66% 13% 72%Hand/ASR 62% 16% 61%Unsup./ASR 64% 14% 75%Table 2: Results for classifiers using prosodic features.4 ConclusionIn summary, we have described an approach for au-tomatic recognition of agreement and disagreement inmeeting data, using both prosodic and word-based fea-tures.
The methods can be implemented with a smallamount of hand-labeled data by using unsupervised LMclustering to label additional data, which leads to signifi-cant gains in both word-based and prosody-based classi-fiers.
The approach is extensible to other types of speechacts, and is especially important for domains in whichvery little annotated data exists.
Even operating on ASRtranscripts with high WERs (45%), we obtain a 78% rateof recovery of agreements and disagreements, with a verylow rate of confusion between these classes.
Prosodicfeatures alone provide results almost as good as the word-based models on ASR transcripts, but no additional ben-efit when used with word-based features.
However, thegood performance from prosody alone offers hope forperformance gains given a richer set of speech acts withmore lexically ambiguous cases (Bhagat et al, 2003).AcknowledgmentsThis work is supported in part by the NSF under grants 0121396and 0619921, DARPA grant N660019928924, and NASA grantNCC 2-1256.
Any opinions, conclusions or recommendationsexpressed in this material are those of the authors and do notnecessarily reflect the views of these agencies.ReferencesD.
Baron et al 2002.
Automatic punctuation and disfluencydetection in multi-party meetings using prosodic and lexicalcues.
In Proc.
ICSLP, pages 949?952.S.
Bhagat, H. Carvey, and E. Shriberg.
2003.
Automaticallygenerated prosodic cues to lexically ambiguous dialog actsin multi-party meetings.
In ICPhS.L.
Breiman et al 1984.
Classification And Regression Trees.Wadsworth International Group, Belmont, CA.J.
Chu-Carroll.
1998.
A statistical model for discourse actrecognition in dialogue interactions.
In Applying MachineLearning to Discourse Processing.
Papers from the 1998AAAI Spring Symposium, pages 12?17.N.
Morgan et al 2001.
The meeting project at ICSI.
InProc.
Conf.
on Human Language Technology, pages 246?252, March.N.
Reithinger and M. Klesen.
1997.
Dialogue act classificationusing language models.
In Proc.
Eurospeech, pages 2235?2238, September.E.
Shriberg et al 1998.
Can prosody aid the automatic classi-fication of dialog acts in conversational speech?
Languageand Speech, 41(3?4), pages 439?487.E.
Shriberg et al 2000.
Prosody-based automatic segmentationof speech into sentences and topics.
Speech Communication,32(1-2):127?154, September.E.
Shriberg et al 2001.
Observations on overlap: Findings andimplications for automatic processing of multi-party conver-sation.
In Proc.
Eurospeech, pages 1359?1362.S.
Siegel and J. Castellan.
1988.
Nonparametric Statistics Forthe Behavioral Sciences.
McGraw-Hill Inc., New York, NY,second edition edition.
