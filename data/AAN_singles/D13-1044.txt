Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 458?467,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAutomatic Feature Engineering for Answer Selection and ExtractionAliaksei SeverynDISI, University of Trento38123 Povo (TN), Italyseveryn@disi.unitn.itAlessandro MoschittiQatar Computing Research Institue5825 Doha, Qataramoschitti@qf.org.qaAbstractThis paper proposes a framework for automat-ically engineering features for two importanttasks of question answering: answer sentenceselection and answer extraction.
We representquestion and answer sentence pairs with lin-guistic structures enriched by semantic infor-mation, where the latter is produced by auto-matic classifiers, e.g., question classifier andNamed Entity Recognizer.
Tree kernels ap-plied to such structures enable a simple way togenerate highly discriminative structural fea-tures that combine syntactic and semantic in-formation encoded in the input trees.
We con-duct experiments on a public benchmark fromTREC to compare with previous systems foranswer sentence selection and answer extrac-tion.
The results show that our models greatlyimprove on the state of the art, e.g., up to 22%on F1 (relative improvement) for answer ex-traction, while using no additional resourcesand no manual feature engineering.1 IntroductionQuestion Answering (QA) systems are typicallybuilt from three main macro-modules: (i) search andretrieval of candidate passages; (ii) reranking or se-lection of the most promising passages; and (iii) an-swer extraction.
The last two steps are the most in-teresting from a Natural Language Processing view-point since deep linguistic analysis can be carriedout as the input is just a limited set of candidates.Answer sentence selection refers to the task of se-lecting the sentence containing the correct answeramong the different sentence candidates retrieved bya search engine.Answer extraction is a final step, required forfactoid questions, consisting in extracting multi-words constituting the synthetic answer, e.g., BarackObama for a question: Who is the US president?The definition of rules for both tasks is conceptuallydemanding and involves the use of syntactic and se-mantic properties of the questions and its related an-swer passages.For example, given a question from TREC QA1:Q: What was Johnny Appleseed?s realname?and a relevant passage, e.g., retrieved by a searchengine:A: Appleseed, whose real name was JohnChapman, planted many trees in the early1800s.a rule detecting the semantic links between JohnnyAppleseed?s real name and the correct answerJohn Chapman in the answer sentence has tobe engineered.
This requires the definition ofother rules that associate the question patternreal name ?
(X) with real name is(X) ofthe answer sentence.
Although this can be done byan expert NLP engineer, the effort for achieving thenecessary coverage and a reasonable accuracy is notnegligible.An alternative to manual rule definition is the useof machine learning, which often shifts the problem1We use it as our running example in the rest of the paper.458to the easier task of feature engineering.
Unfortu-nately, when the learning task is semantically dif-ficult such as in QA, e.g., features have to encodecombinations of syntactic and semantic properties.Thus their extraction modules basically assume theshape of high-level rules, which are, in any case, es-sential to achieve state-of-the-art accuracy.
For ex-ample, the great IBM Watson system (Ferrucci etal., 2010) uses a learning to rank algorithm fed withhundreds of features.
The extraction of some of thelatter requires articulated rules/algorithms, which,in terms of complexity, are very similar to thoseconstituting typical handcrafted QA systems.
Animmediate consequence is the reduced adaptabilityto new domains, which requires a substantial re-engineering work.In this paper, we show that tree kernels (Collinsand Duffy, 2002; Moschitti, 2006) can be applied toautomatically learn complex structural patterns forboth answer sentence selection and answer extrac-tion.
Such patterns are syntactic/semantic structuresoccurring in question and answer passages.
To makesuch information available to the tree kernel func-tions, we rely on the shallow syntactic trees enrichedwith semantic information (Severyn et al 2013b;Severyn et al 2013a), e.g., Named Entities (NEs)and question focus and category, automatically de-rived by machine learning modules, e.g., questionclassifier (QC) or focus classifier (FC).More in detail, we (i) design a pair of shallowsyntactic trees (one for the question and one for theanswer sentence); (ii) connect them with relationalnodes (i.e., those matching the same words in thequestion and in the answer passages); (iii) label thetree nodes with semantic information such as ques-tion category and focus and NEs; and (iv) use the NEtype to establish additional semantic links betweenthe candidate answer, i.e., an NE, and the focus wordof the question.
Finally, for the task of answer ex-traction we also connect such semantic informationto the answer sentence trees such that we can learnfactoid answer patterns.We show that our models are very effective in pro-ducing features for both answer selection and ex-traction by experimenting with TREC QA corporaand directly comparing with the state of the art,e.g., (Wang et al 2007; Yao et al 2013).
The re-sults show that our methods greatly improve on bothtasks yielding a large improvement in Mean AveragePrecision for answer selection and in F1 for answerextraction: up to 22% of relative improvement in F1,when small training data is used.
Moreover, in con-trast to the previous work, our model does not relyon external resources, e.g., WordNet, or complexfeatures in addition to the structural kernel model.The reminder of this paper is organized as fol-lows, Sec.
2 describes our kernel-based classifiers,Sec.
3 illustrates our question/answer relationalstructures also enriched with semantic information,Sec.
4 describes our model for answer selection andextraction, Sec.
5 illustrates our comparative exper-iments on TREC data, Sec.
6 reports on our erroranalysis, Sec.
7 discusses the related work, and fi-nally, Sec.
8 derives the conclusions.2 Structural Kernels for classificationThis section describes a kernel framework where theinput question/answer pairs are handled directly inthe form of syntactic/semantic structures.2.1 Feature vector approach to object pairclassificationA conventional approach to represent a ques-tion/answer pairs in linear models consists in defin-ing a set of similarity features {xi} and computingthe simple scalar product h(x) = w ?
x =?iwixi,where w is the model weight vector learned on thetraining data.
Hence, the learning problem boilsdown to estimating individual weights of each ofthe similarity features xi.
Such features often en-code various types of lexical, syntactic and semanticsimilarities shared between a question and its can-didate.
Previous work used a rich number of distri-butional semantic, knowledge-based, translation andparaphrase resources to build explicit feature vectorrepresentations.
One evident potential downside ofusing feature vectors is that a great deal of structuralinformation encoded in a given text pair is lost.2.2 Pair Classification using Structural KernelsA more versatile approach in terms of the inputrepresentation relies on kernels.
A typical ker-nel machine, e.g., SVM, classifies a test input xusing the following prediction function: h(x) =?i ?iyiK(x,xi), where ?i are the model parame-ters estimated from the training data, yi are target459variables, xi are support vectors, andK(?, ?)
is a ker-nel function.
The latter can measure the similaritybetween question and answer pairs.We define each question/answer pair x as a triplecomposed of a question treeT q and answer sentencetree T s and a similarity feature vector v , i.e., x =?T q,T s, v?.
Given two triples xi and xj , we definethe following kernel:K(xi,xj) = KTK(T iq,Tjq)+ KTK(T is,Tjs)+ Kv(v i, vj),(1)where KTK computes a structural kernel, e.g., treekernel, and Kv is a kernel over feature vectors, e.g.,linear, polynomial, gaussian, etc.
Structural kernelscan capture the structural representation of a ques-tion/answer pair whereas traditional feature vectorscan encode some sort of similarity, e.g., lexical, syn-tactic, semantic, between a question and its candi-date answer.We prefer to split the kernel computation over aquestion/answer pair into two terms since tree ker-nels are very efficient and there are no efficientgraph kernels that can encode exhaustively all graphfragments.
It should be noted that the tree kernelsum does not capture feature pairs.
Theoretically,for such purpose, a kernel product should be used.However, our experiments revealed that using theproduct is actually worse in practice.
In contrast,we solve the lack of feature pairing by annotatingthe trees with relational tags which are supposedto link the question tree fragments with the relatedfragments from the answer sentence.Such relational information is very important toimprove the quality of the pair representation as wellas the implicitly generated features.
In the next sec-tion, we show simple structural models that we usedin our experiments for question and answer pair clas-sification.2.3 Partial Tree KernelsThe above framework can use any kernel forstructural data.
We use the Partial Tree Kernel(PTK) (Moschitti, 2006) to compute KTK(?, ?)
as itis the most general convolution tree kernel, whichat the same time shows rather good efficiency.
PTKcan be effectively applied to both constituency anddependency parse trees.
It generalizes the syntactictree kernel (STK) (Collins and Duffy, 2002), whichmaps a tree into the space of all possible tree frag-ments constrained by the rule that sibling nodes can-not be separated.
In contrast, the PTK fragmentscan contain any subset of siblings, i.e., PTK allowsfor breaking the production rules in syntactic trees.Consequently, PTK generates an extremely rich fea-ture space, which results in higher generalizationability.3 Relational StructuresThis section introduces relational structures de-signed to encode syntactic and shallow semanticproperties of question/answer pairs.
We first define asimple to construct shallow syntactic tree represen-tation derived from a shallow parser.
Next, we in-troduce a relational linking scheme based on a plainsyntactic matching and further augment it with ad-ditional semantic information.3.1 Shallow syntactic treeOur shallow tree structure is a two-level syntactichierarchy built from word lemmas (leaves), part-of-speech tags that organized into chunks identified bya shallow syntactic parser (Fig.
1).
We defined asimilar structure in (Severyn and Moschitti, 2012)for answer passage reranking, which improved onfeature vector baselines.This simple linguistic representation is suitablefor building a rather expressive answer sentence se-lection model.
Moreover, the use of a shallow parseris motivated by the need to generate text spans toproduce candidate answers required by an answerextraction system.3.2 Tree pairs enriched with relational linksIt is important to establish a correspondence be-tween question and answer sentence aligning relatedconcepts from both.
We take on a two-level ap-proach, where we first use plain lexical matching toconnect common lemmas from the question and itscandidate answer sentence.
Secondly, we establishsemantic links between NEs extracted from the an-swer sentence and the question focus word, whichencodes the expected lexical answer type (LAT).
Weuse the question categories to identify NEs that have460Figure 1: Shallow tree representation of the example q/a pair from Sec.
1.
Dashed arrows (red) indicate the treefragments (red dashed boxes) in the question and its answer sentence linked by the relational REL tag, which isestablished via syntactic match on the word lemmas.
Solid arrows (blue) connect a question focus word name with therelated named entities of type Person corresponding to the question category (HUM) via a relational tag REL-HUM.Additional ANS tag is used to mark chunks containing candidate answer (here the correct answer John Chapman).higher probability to be correct answers following amapping defined in Table 1.Next, we briefly introduce our tree kernel-basedmodels for building question focus and categoryclassifiers.Lexical Answer Type.
Question Focus representsa central entity or a property asked by a question(Prager, 2006).
It can be used to search for semanti-cally compatible candidate answers, thus greatly re-ducing the search space (Pinchak, 2006).
While sev-eral machine learning approaches based on manualfeatures and syntactic structures have been recentlyexplored, e.g.
(Quarteroni et al 2012; Damljanovicet al 2010; Bunescu and Huang, 2010), we opt forthe latter approach where tree kernels handle auto-matic feature engineering.To build an automatic Question Focus detector weuse a tree kernel approach as follows: we (i) parseeach question; (ii) create a set of positive trees bylabeling the node exactly covering the focus withFC tag; (iii) build a set of negative trees by labelingany other constituent node with FC; (iii) we trainthe FC node classifier with tree kernels.
At the testtime, we try to label each constituent node with FCgenerating a set of candidate trees.
Finally, we selectthe tree and thus the constituent associated with thehighest SVM score.Question classification.
Our question classificationmodel is simpler than before: we use an SVM multi-classifier with tree kernels to automatically extractthe question class.
To build a multi-class classifierwe train a binary SVM for each of the classes andapply a one-vs-all strategy to obtain the predictedTable 1: Expected Answer Type (EAT) ?
named entitytypes.EAT Named Entity typesHUM PersonLOCATION LocationENTITY Organization, Person, MiscDATE Date, Time, NumberQUANTITY Number, PercentageCURRENCY Money, Numberclass.
We use constituency trees as our input repre-sentation.Our question taxonomy is derived from theUIUIC dataset (Li and Roth, 2002) which defines6 coarse and 50 fine grain classes.
In particular,our set of question categories is formed by adopt-ing 3 coarse classes: HUM (human), LOC (loca-tion), ENTY (entities) and replacing the NUM (nu-meric) coarse class with 3 fine-grain classes: CUR-RENCY, DATE, QUANTITY2.
This set of questioncategories is sufficient to capture the coarse seman-tic answer type of the candidate answers found inTREC.
Also using fewer question classes results ina more accurate multi-class classifier.Semantic tagging.
Question focus word specifiesthe lexical answer type capturing the target informa-tion need posed by a question, but to make this pieceof information effective, the focus word needs tobe linked to the target candidate answer.
The focusword can be lexically matched with words present in2This class is composed by including all the fine-grainclasses from NUMERIC coarse class except for CURRENCYand DATE.461the answer sentence, or the match can be establishedusing semantic information.
Clearly, the latter ap-proach is more appealing since it helps to alleviatethe lexical gap problem, i.e., it improves the cover-age of the na?ive string matching of words between aquestion and its answer.Hence, we propose to exploit a question focusalong with the related named entities (according tothe mapping from Table 1) of the answer sentenceto establish relational links between the tree frag-ments.
In particular, once the question focus andquestion category are determined, we link the fo-cus word wfocus in the question, with all the namedentities whose type matches the question class (Ta-ble 1).
We perform tagging at the chunk level anduse a relational tag typed with a question class, e.g.,REL-HUM.
Fig.
1 shows an example q/a pair wherethe typed relational tag is used in the shallow syntac-tic tree representation to link the chunk containingthe question focus name with the named entities ofthe corresponding type Person, i.e., Appleseed andJohn Chapman.4 Answer Sentence Selection and AnswerKeyword ExtractionThis section describes our approach to (i) answersentence selection used to select the most promisinganswer sentences; and (ii) answer extraction whichreturns the answer keyword (for factoid questions).4.1 Answer Sentence SelectionWe cast the task of answer sentence selection asa classification problem.
Considering a supervisedlearning scenario, we are given a set of questions{qi}Ni=1 where each question qi is associated witha list of candidate answer sentences {(ri, si)}Ni=1,with ri ?
{?1,+1} indicating if a given candidateanswer sentence si contains a correct answer (+1)or not (?1).
Using this labeled data, our goal is tolearn a classifier model to predict if a given pair ofa question and an answer sentence is correct or not.We train a binary SVM with tree kernels3 to train ananswer sentence classifier.
The prediction scores ob-tained from a classifier are used to rerank the answercandidates (pointwise reranking), s.t.
the sentencesthat are more likely to contain correct answers will3disi.unitn.it/moschitti/Tree-Kernel.htmbe ranked higher than incorrect candidates.
In addi-tion to the structural representation, we augment ourmodel with basic bag-of-word features (unigram andbigrams) computed over lemmas.4.2 Answer Sentence ExtractionThe goal of answer extraction is to extract a text spanfrom a given candidate answer sentence.
Such spanrepresents a correct answer phrase for a given ques-tion.
Different from previous work that casts the an-swer extraction task as a tagging problem and applya CRF to learn an answer phrase tagger (Yao et al2013), we take on a simpler approach using a kernel-based classifier.In particular, we rely on the shallow tree represen-tation, where text spans identified by a shallow syn-tactic parser serve as a source of candidate answers.Algorithm 1 specifies the steps to generate trainingdata for our classifier.
In particular, for each ex-ample representing a triple ?a, Tq, Ts?
composed ofthe answer a, the question and the answer sentencetrees, we generate a set of training examples E withevery candidate chunk marked with an ANS tag (oneat a time).
To reduce the number of generated exam-ples for each answer sentence, we only consider NPchunks, since other types of chunks, e.g., VP, ADJP,typically do not contain factoid answers.
Finally, anoriginal untagged tree is used to generate a positiveexample (line 8), when the answer sentence containsa correct answer, and a negative example (line 10),when it does not contain a correct answer.At the classification time, given a question and acandidate answer sentence, all NP nodes of the sen-tence are marked with ANS (one at a time) as thepossible answer, generating a set of tree candidates.Then, such trees are classified (using the kernel fromEq.
1) and the one with the highest score is selected.If no tree is classified as positive example we do notextract any answer.5 ExperimentsWe provide the results on two related yet differenttasks: answer sentence selection and answer extrac-tion.
The goal of the former is to learn a modelscoring correct question and answer sentence pairsto bring in the top positions sentences containing thecorrect answers.
Answer extraction derives the cor-462Algorithm 1 Generate training data for answer ex-traction1: for all ?a, Tq, Ts?
?D do2: E ?
?3: for all chunk ?
extract chunks(Ts) do4: if not chunk == NP then5: continue6: T ?s ?
tagAnswerChunk(Ts, chunk)7: if contains answer(a, chunk) then8: label?
+19: else10: label?
?111: e?
build example(Tq, T ?s, label)12: E ?
E ?
{e}13: return Erect answer keywords, i.e., a text span such as multi-words or constituents, from a given sentence.5.1 Semantic AnnotationWe briefly describe the experiments of training auto-matic question category and focus classifiers, whichare more extensively described in (Severyn et al2013b).Question Focus detection.
We used three datasetsfor training and evaluating the performance of ourfocus detector: SeCo-600 (Quarteroni et al 2012),Mooney GeoQuery (Damljanovic et al 2010) andthe dataset from (Bunescu and Huang, 2010).
TheSeCo dataset contains 600 questions.
The MooneyGeoQuery contains 250 question targeted at ge-ographical information in the U.S.
The first twodatasets are very domain specific, while the datasetfrom (Bunescu and Huang, 2010) is more genericcontaining the first 2,000 questions from the answertype dataset from Li and Roth annotated with fo-cus words.
We removed questions with implicit andmultiple focuses.Question Classification.
We used the UIUICdataset (Li and Roth, 2002) which contains 5,952factoid questions 4 to train a multi-class questionclassifier.Table 2 summarizes the results of question focusand category classification.4We excluded questions from TREC to ensure there is nooverlap with the data used for testing models trained on TRECQA.Table 2: Accuracy (%) of focus (FC) and question classi-fiers (QC) using PTK.TASK SET PTKFCMOONEY 80.5SECO-600 90.0BUNESCU 96.9QCUIUIC 85.9TREC 11-12 78.15.2 Answer Sentence SelectionWe used the train and test data from (Wang et al2007) to enable direct comparison with previouswork on answer sentence selection.
The trainingdata is composed by questions drawn from TREC8-12 while questions from TREC 13 are used fortesting.
The data provided for training comes astwo sets: a small set of 94 questions (TRAIN) thatwere manually curated for errors5 and 1,229 ques-tions from the entire TREC 8-12 that contain at leastone correct answer sentence (ALL).
The latter setrepresents a more noisy setting, since many answersentences are marked erroneously as correct as theysimply match a regular expression.
Table 3 summa-rizes the data used for training and testing.Table 4 compares our kernel-based structuralmodel with the previous state-of-the-art systems foranswer sentence selection.
In particular, we com-pare with four most recent state of the art answersentence reranker models (Wang et al 2007; Heil-man and Smith, 2010; Wang and Manning, 2010;Yao et al 2013), which report their performance onthe same questions and candidate sets from TREC13 as provided by (Wang et al 2007).Our simple shallow tree representation (Severynand Moschitti, 2012) delivers state-of-the-art ac-curacy largely improving on previous work.
Fi-nally, augmenting the structure with semantic link-ing (Severyn et al 2013b) yields additional im-provement in MAP and MRR.
This suggests theutility of using supervised components, e.g., ques-tion focus and question category classifiers coupledwith NERs, to establish semantic mapping betweenwords in a q/a pair.5In TREC correct answers are identified by regex matchingusing the provided answer pattern files463Table 3: Summary of TREC data for answer extractionused in (Yao et al 2013).data questions candidates correctTRAIN 94 4718 348ALL 1229 53417 6410TEST 89 1517 284Table 4: Answer sentence reranking on TREC 13.System MAP MRRWang et al(2007) 0.6029 0.6852Heilman & Smith (2010) 0.6091 0.6917Wang & Manning (2010) 0.5951 0.6951Yao et al(2013) 0.6319 0.7270+ WN 0.6371 0.7301shallow tree (S&M, 2012) 0.6485 0.7244+ semantic tagging 0.6781 0.7358It is worth noting that our kernel-based classifieris conceptually simpler than approaches in the previ-ous work, as it relies on the structural kernels, e.g.,PTK, to automatically extract salient syntactic pat-terns relating questions and answers.
Our modelonly includes the most basic feature vector (uni- andbi-grams) and does not rely on external sources suchas WordNet.5.3 Answer ExtractionOur experiments on answer extraction replicate thesetting of (Yao et al 2013), which is the most recentwork on answer extraction reporting state-of-the-artresults.Table 5 reports the accuracy of our model in re-covering correct answers from a set of candidate an-swer sentences for a given question.
Here the fo-cus is on the ability of an answer extraction systemto recuperate as many correct answers as possiblefrom each answer sentence candidate.
The set ofextracted candidate answers can then be used to se-lect a single best answer, which is the final outputof the QA system for factoid questions.
Recall (R)encodes the percentage of correct answer sentencesfor which the system correctly extracts an answer(for TREC 13 there are a total of 284 correct answersentences), while Precision (P) reflects how manyanswers extracted by the system are actually correct.Clearly, having a high recall system, allows for cor-rectly answering more questions.
On the other hand,a high precision system would attempt to answer lessquestions (extracting no answers at all) but get themright.We compare our results to a CRF model of (Yao etal., 2013) augmented with WordNet features (with-out forced voting) 6.
Unlike the CRF model whichobtains higher values of precision, our system actsas a high recall system able to recover most of theanswers from the correct answer sentences.
Havinghigher recall is favorable to high precision in answerextraction since producing more correct answers canhelp in the final voting scheme to come up with asingle best answer.
To solve the low recall problemof their CRF model, Yao et al(2013) apply fairlycomplex outlier resolution techniques to force an-swer predictions, thus aiming at increasing the num-ber of extracted answers.To further boost the number of answers producedby our system we exclude negative examples (an-swer sentences not containing the correct answer)from training, which slightly increases the numberof pairs with correctly recovered answers.
Never-theless, it has a substantial effect on the number ofquestions that can be answered correctly (assumingperfect single best answer selection).
Clearly, oursystem is able to recover a large number of answersfrom the correct answer sentences, while low pre-cision, i.e., extracting answer candidates from sen-tences that do not contain a correct answer, can beovercome by further applying various best answerselection strategies, which we explore in the nextsection.5.4 Best Answer SelectionSince the final step of the answer extraction moduleis to select for each question a single best answerfrom a set of extracted candidate answers, an answerselection scheme is required.We adopt a simple majority voting strategy, wherewe aggregate the extracted answers produced by ouranswer extraction model.
Answers sharing simi-lar lemmas (excluding stop words) are grouped to-gether.
The prediction scores obtained by the an-6We could not replicate the results obtained in (Yao et al2013) with the forced voting strategy.
Thus such result is notincluded in Table 5.464Table 5: Results on answer extraction.
P/R - precisionand recall; pairs - number of QA pairs with a correctly ex-tracted answer, q - number of questions with at least onecorrect answer extracted, F1 sets an upper bound on theperformance assuming the selected best answer amongextracted candidates is always correct.
*-marks the set-ting where we exclude incorrect question answer pairsfrom training.set P R pairs q F1Yao et al(2013) 25.7 23.4 73 33 -+ WN 26.7 24.3 76 35 -TRAIN 29.6 64.4 183 58 65.2TRAIN* 15.7 71.8 204 66 74.1Yao et al(2013) 35.2 35.1 100 38 -+ WN 34.5 34.7 98 38 -ALL 29.4 74.6 212 69 77.5ALL* 15.8 76.7 218 73 82.0Table 6: Results on finding the best answer with voting.system set P R F1Yao et al(2013)TRAIN55.7 43.8 49.1+ forced 54.5 53.9 54.2+ WN 55.2 53.9 54.5this work 66.2 66.2 66.2Yao et al(2013)ALL67.2 50.6 57.7+ forced 60.9 59.6 60.2+ WN 63.6 62.9 63.3this work 70.8 70.8 70.8swer extraction classifier are used as votes to decideon the final rank to select the best single answer.Table 6 shows the results after the majority vot-ing is applied to select a single best answer for eachcandidate.
A rather na?
?ve majority voting schemealready produces satisfactory outcome demonstrat-ing better results than the previous work.
Our vot-ing scheme is similar to the one used by (Yao et al2013), yet it is much simpler since we do not per-form any additional hand tuning to account for theweight of the ?forced?
votes or take any additionalsteps to catch additional answers using outlier detec-tion techniques applied in the previous work.6 Discussion and Error AnalysisThere are several sources of errors affecting the fi-nal performance of our answer extraction system: (i)chunking, (ii) named entity recognition and seman-tic linking, (iii) answer extraction, (iv) single bestanswer selection.Chunking.
Our system uses text spans identified bya chunker to extract answer candidates, which makesit impossible to extract answers that lie outside thechunk boundaries.
Nevertheless, we found this tobe a minor concern since for 279 out of total 284candidate sentences from TREC 13 the answers arerecoverable within the chunk spans.Semantic linking.
Our structural model relies heav-ily on the ability of NER to identify the relevant en-tities in the candidate sentence that can be furtherlinked to the focus word of the question.
Whileour answer extraction model is working on all theNP chunks, the semantic tags from NER serve as astrong cue for the classifier that a given chunk hasa high probability of containing an answer.
Typicaloff-the-shelf NER taggers have good precision andlow recall, s.t.
many entities as potential answers aremissed.
In this respect, a high recall entity linkingsystem, e.g., linking to wikipedia entities (Ratinovet al 2011), is required to boost the quality of can-didates considered for answer extraction.
Finally,improving the accuracy of question and focus clas-sifiers would allow for having more accurate inputrepresentations fed to the learning algorithm.Answer Extraction.
Our answer extraction modelacts as a high recall system, while it suffers fromlow precision in extracting answers for many incor-rect sentences.
Improving the precision without sac-rificing the recall would ease the successive task ofbest answer selection, since having less incorrect an-swer candidates would result in a better final per-formance.
Introducing additional constraints in theform of semantic tags to allow for better selection ofanswer candidates could also improve our system.Best Answer Selection.
We apply a na?
?ve majorityvoting scheme to select a single best answer froma set of extracted answer candidates.
This step hasa dramatic impact on the final performance of theanswer extraction system resulting in a large dropof recall, i.e., from 82.0 to 70.8 before and after vot-ing respectively.
Hence, a more involved model, i.e.,465performing joint answer sentence re-ranking and an-swer extraction, is required to yield a better perfor-mance.7 Related WorkTree kernel methods have found many applicationsfor the task of answer reranking which are reportedin (Moschitti, 2008; Moschitti, 2009; Moschitti andQuarteroni, 2008; Severyn and Moschitti, 2012).However, their methods lack the use of importantrelational information between a question and a can-didate answer, which is essential to learn accuraterelational patterns.
In this respect, a solution basedon enumerating relational links was given in (Zan-zotto and Moschitti, 2006; Zanzotto et al 2009) forthe textual entailment task but it is computationallytoo expensive for the large dataset of QA.
A few so-lutions to overcome computational issues were sug-gested in (Zanzotto et al 2010).In contrast, this paper relies on structures directlyencoding the output of question and focus classifiersto connect focus word and good candidate answerkeywords (represented by NEs) of the answer pas-sage.
This provides more effective relational infor-mation, which allows our model to significantly im-prove on previous rerankers.
Additionally, previouswork on kernel-based approaches does not target an-swer extraction.One of the best models for answer sentence selec-tion has been proposed in (Wang et al 2007).
Theyuse the paradigm of quasi-synchronous grammar tomodel relations between a question and a candidateanswer with syntactic transformations.
(Heilmanand Smith, 2010) develop an improved Tree EditDistance (TED) model for learning tree transforma-tions in a q/a pair.
They search for a good sequenceof tree edit operations using complex and com-putationally expensive Tree Kernel-based heuristic.
(Wang and Manning, 2010) develop a probabilisticmodel to learn tree-edit operations on dependencyparse trees.
They cast the problem into the frame-work of structured output learning with latent vari-ables.
The model of (Yao et al 2013) has reportedan improvement over the Wang?s et al(2007) sys-tem.
It applies linear chain CRFs with features de-rived from TED and WordNet to automatically learnassociations between questions and candidate an-swers.Different from previous approaches that use tree-edit information derived from syntactic trees, ourkernel-based learning approach also use tree struc-tures but with rather different learning methods, i.e.,SVMs and structural kernels, to automatically ex-tract salient syntactic patterns relating questions andanswers.
In (Severyn et al 2013c), we have shownthat such relational structures encoding input textpairs can be directly used within the kernel learningframework to build state-of-the-art models for pre-dicting semantic textual similarity.
Furthermore, se-mantically enriched relational structures, where au-tomatic have been previously explored for answerpassage reranking in (Severyn et al 2013b; Sev-eryn et al 2013a).
This paper demonstrates that thismodel also works for building a reranker on the sen-tence level, and extends the previous work by apply-ing the idea of automatic feature engineering withtree kernels to answer extraction.8 ConclusionsOur paper demonstrates the effectiveness of han-dling the input structures representing QA pairs di-rectly vs. using explicit feature vector representa-tions, which typically require substantial feature en-gineering effort.
Our approach relies on a kernel-based learning framework, where structural kernels,e.g., tree kernels, are used to handle automatic fea-ture engineering.
It is enough to specify the desiredtype of structures, e.g., shallow, constituency, de-pendency trees, representing question and its can-didate answer sentences and let the kernel learningframework learn to use discriminative tree fragmentsfor the target task.An important feature of our approach is that itcan effectively combine together different types ofsyntactic and semantic information, also generatedby additional automatic classifiers, e.g., focus andquestion classifiers.
We augment the basic struc-tures with additional relational and semantic infor-mation by introducing special tag markers into thetree nodes.
Using the structures directly in the ker-nel learning framework makes it easy to integrateadditional relational constraints and semantic infor-mation directly in the structures.The comparison with previous work on a public466benchmark from TREC suggests that our approachis very promising as we can improve the state of theart in both answer selection and extraction by a largemargin (up to 22% of relative improvement in F1 foranswer extraction).
Our approach makes it relativelyeasy to integrate other sources of semantic informa-tion, among which the use of Linked Open Data canbe the most promising to enrich the structural repre-sentation of q/a pairs.To achieve state-of-the-art results in answer sen-tence selection and answer extraction, it is sufficientto provide our model with a suitable tree structureencoding relevant syntactic information, e.g., usingshallow, constituency or dependency formalisms.Moreover, additional semantic and relational infor-mation can be easily plugged in by marking treenodes with special tags.
We believe this approachgreatly eases the task of tedious feature engineeringthat will find its applications well beyond QA tasks.AcknowledgementsThis research is partially supported by the EU?s7th Framework Program (FP7/2007-2013) (#288024LIMOSINE project) and an Open Collaborative Re-search (OCR) award from IBM Research.
The firstauthor is supported by the Google Europe Fellow-ship 2013 award in Machine Learning.ReferencesRazvan Bunescu and Yunfeng Huang.
2010.
Towards ageneral model of answer typing: Question focus iden-tification.
In CICLing.Michael Collins and Nigel Duffy.
2002.
New RankingAlgorithms for Parsing and Tagging: Kernels over Dis-crete Structures, and the Voted Perceptron.
In ACL.Danica Damljanovic, Milan Agatonovic, and HamishCunningham.
2010.
Identification of the question fo-cus: Combining syntactic analysis and ontology-basedlookup through the user interaction.
In LREC.David Ferrucci, Eric Brown, Jennifer Chu-Carroll, JamesFan, David Gondek, Aditya Kalyanpur, Adam Lally,J.
William Murdock, Eric Nyberg, John Prager, NicoSchlaefer, and Chris Welty.
2010.
Building watson:An overview of the deepqa project.
AI Magazine,31(3).Michael Heilman and Noah A. Smith.
2010.
Treeedit models for recognizing textual entailments, para-phrases, and answers to questions.
In NAACL.Xin Li and Dan Roth.
2002.
Learning question classi-fiers.
In COLING.A.
Moschitti and S. Quarteroni.
2008.
Kernels on Lin-guistic Structures for Answer Extraction.
In ACL.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.
InECML.Alessandro Moschitti.
2008.
Kernel methods, syntax andsemantics for relational text categorization.
In CIKM.Alessandro Moschitti.
2009.
Syntactic and semantic ker-nels for short text pair categorization.
In EACL.Christopher Pinchak.
2006.
A probabilistic answer typemodel.
In In EACL.John M. Prager.
2006.
Open-domain question-answering.
Foundations and Trends in InformationRetrieval, 1(2):91?231.Silvia Quarteroni, Vincenzo Guerrisi, and Pietro LaTorre.
2012.
Evaluating multi-focus natural languagequeries over data services.
In LREC.L.
Ratinov, D. Roth, D. Downey, and M. Anderson.2011.
Local and global algorithms for disambiguationto wikipedia.
In ACL.Aliaksei Severyn and Alessandro Moschitti.
2012.Structural relationships for large-scale learning of an-swer re-ranking.
In SIGIR.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013a.
Building structures from classifiersfor passage reranking.
In CIKM.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013b.
Learning adaptable patterns forpassage reranking.
In CoNLL.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013c.
Learning semantic textual similar-ity with structural representations.
In ACL.Mengqiu Wang and Christopher D. Manning.
2010.Probabilistic tree-edit models with structured latentvariables for textual entailment and question answer-ing.
In ACL.Mengqiu Wang, Noah A. Smith, and Teruko Mitaura.2007.
What is the jeopardy model?
a quasi-synchronous grammar for qa.
In EMNLP.Xuchen Yao, Benjamin Van Durme, Peter Clark, andChris Callison-Burch.
2013.
Answer extraction as se-quence tagging with tree edit distance.
In NAACL.F.
M. Zanzotto and A. Moschitti.
2006.
AutomaticLearning of Textual Entailments with Cross-Pair Sim-ilarities.
In COLING.F.
M. Zanzotto, M. Pennacchiotti, and A. Moschitti.2009.
A Machine Learning Approach to RecognizingTextual Entailment.
Natural Language Engineering,Volume 15 Issue 4, October 2009:551?582.F.
M. Zanzotto, L. Dell?Arciprete, and A. Moschitti.2010.
Efficient graph kernels for textual entail-ment recognition.
FUNDAMENTA INFORMATICAE,2010.467
