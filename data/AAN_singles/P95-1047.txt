Acqu i r ing  a Lex icon  f rom Unsegmented  SpeechCar l  de  MarckenMIT  Artif icial Intel l igence Laboratory545 Technology Square, NE43-804Cambridge,  MA, 02139, USAcgdemarc@ai .mit .eduAbst rac tWe present work-in-progress on the ma-chine acquisition of a lexicon from sen-tences that are each an unsegmented phonesequence paired with a primitive represen-tation of meaning.
A simple exploratoryalgorithm is described, along with the di-rection of current work and a discussionof the relevance of the problem for childlanguage acquisition and computer speechrecognition.1 In t roduct ionWe are interested in how a lexicon of discrete wordscan be acquired from continuous peech, a prob-lem fundamental both to child language acquisitionand to the automated induction of computer speechrecognition systems; see (Olivier, 1968; Wolff, 1982;Cartwright and Brent, 1994) for previous computa-tional work in this area.
For the time being, we ap-proximate the problem as induction from phone se-quences rather than acoustic pressure, and assumethat learning takes place in an environment wheresimple semantic representations of the speech intentare available to the acquisition mechanism.For example, we approximate he greater problemas that of learning from inputs likePhon.
Input: /~raebltslne~ b~W t/Sem.
Input: { BOAT A IN RABBIT THE BE }(The rabbit's in a boat.
)where the semantic nput is an unordered set of iden-tifiers corresponding to word paradigms.
Obviouslythe artificial pseudo-semantic representations makethe problem much easier: we experiment with themas a first step, somewhere between learning language"from a radio" and providing an unambiguous tex-tual transcription, as might be used for training aspeech recognition system.Our goal is to create a program that, after train-ing on many such pairs, can segment a new phoneticutterance into a sequence of morpheme identifiers.Such output could be used as input to many gram-mar acquisition programs.2 A S imple  P ro to typeWe have implemented a simple algorithm as an ex-ploratory effort.
It maintains a single dictionary, aset of words.
Each word consists of a phone sequenceand a set of sememes (semantic symbols).
Initially,the dictionary is empty.
When presented with anutterance, the algorithm goes through the followingsequence of actions:?
It attempts to cover ("parse") the utterancephones and semantic symbols with a sequenceof words from the dictionary, each word offset acertain distance into the phone sequence, withwords potentially overlapping.?
It then creates new words that account for un-covered portions of the utterance, and adjustswords from the parse to better fit the utterance.?
Finally, it reparses the utterance with the olddictionary and the new words, and adds the newwords to the dictionary if the resulting parsecovers the utterance well.Occasionally, the program removes rarely-usedwords from the dictionary, and removes words whichcan themselves be parsed.
The general operation ofthe program should be made clearer by the follow-ing two examples.
In the first, the program startswith an empty dictionary, early in the acquisitionprocess, and receives the simple utterance/nina/{NINA } (a child's name).
Naturally, it is unable toparse the input.Utterance:Words:Unparsed:Mismatched:Phones Sememes/nina/ { JINA }/nina/ { NINA }From the unparsed portion of the sentence, theprogram creates a new word, /nina/ { NINA }.
Itthen reparsesPhones SememesUtterance: /nina/ { NINA }Words: /nine/ { sISA }Unparsed:Mismatched:311Having successfully parsed the input, it adds thenew word to the dictionary.
Later in the acquisitionprocess, it encounters the sentence you kicked off~he sock, when the dictionary contains (among otherwords) /yu /  { YOU }, /~a/ { THE }, and /rsuk/ {SOCK }.Utterance:Words:Unparsed:Mismatched:Phones Sememes/yukIkt~f~sak/ { KiCK YOU OFFSOCK THE }/y./  { YOU }I~1  { THE }/rs~k/ { sock }kIkt~f { KICK OFF }rThe program creates the new word /kIkt~f/ {KICK OFF } to account for the unparsed portion ofthe input, and/suk/{ SOCK} to fix the mismatchedphone.
It reparses,PhonesUtterance: /yukIkt3f5~sak/Words: !yu//klkt~f//a~//s~k//rs~k/ unusedUnparsed:Mismatched:Sememes{ KICK YOU OFFSOCK THE }{ You }{ KICK OFF }{ THE }{ SOCK }{ SOCK }On this basis, it adds/kIkt~f/{ KICK OFF } and/sak/ { SOCK } to the dictionary.
/rsuk/ { SOCK}, not used in this analysis, is eventually discardedfrom the dictionary for lack of use.
/klkt~f/{ KICKOFF } is later found to be parsable into two sub-words, and also discarded.One can view this procedure as a variant of theexpectation-maximization (Dempster et al, 1977)procedure, with the parse of each utterance as thehidden variables.
There is currently no preferencefor which words are used in a parse, save to mini-mize mismatches and unparsed portions of the input,but obviously a word grammar could be learned inconjunction with this acquisition process, and usedas a disambiguation step.3 Tests  and  Resu l tsTo test the algorithm, we used 34438 utterancesfrom the Childes database of mothers' peech to chil-dren (MacWhinney and Snow, 1985; Suppes, 1973).These text utterances were run through a publiclyavailable text-to-phone engine.
A semantic dictio-nary was created by hand, in which each root wordfrom the utterances was mapped to a correspond-ing sememe.
Various forms of a root ("see", "saw","seeing") all map to the same sememe, e.g., SEE .Semantic representations for a given utterance aremerely unordered sets of sememes generated by tak-ing the union of the sememe for each word in theutterance.
Figure 1 contains the first 6 utterancesfrom the database.We describe the results of a single run of the al-gorithm, trained on one exposure to each of the34438 utterances, containing a total of 2158 differ-ent stems.
The final dictionary contains 1182 words,where some entries are different forms of a com-mon stem.
82 of the words in the dictionary havenever been used in a good parse.
We eliminate thesewords, leaving 1100.
Figure 2 presents ome entriesin the final dictionary, and figure 3 presents all 21(2%) of the dictionary entries that might be reason-ably considered mistakes.Phones/yu//~/ // .
s t /It,ll/d.//e//It//ax// in//wi/Sememes Phones Sememes{ YOU } /bik/ { BEAK }{ THE } /we/ { wAY }{ WHAT } /hi/ { HEY }{ TO } /brik/ { BREAK }{ DO } /f, vg3/ { FINGER }{ A } Ikisl { KISS }{ IT } /tap/ { TOP }{ I } /k~ld/ { CALL }{ IS } l~gz/ { EGG }{ WE } /eng/ { THING }Figure 2: Dictionary entries.
The left 10 are the10 words used most frequently in good parses.
Theright 10 were selected randomly from the 1100 en-tries./ i v /{  BE }/z~/ { YOU }/ i v /{  DO }Hi, .
/{ SHE BE }/shappin/ { HAPPEN }/t I { NOT }/skatt/ { BOB SCOTT }/nidahz/ { NEEDLE BE }IsAmOl { SOMETHING }Innpi~/{ sNooPy }I*oI { WILL }I""I { AT ZOO }/don/ { DO }/ sd f /{  YOU }/~/{ BE }/smAd/ { MUD }/~r~/{ BE }Idontl { DO NOT }/watarOiz/ { WHAT BE THESE }/wathappind/ { WHAT HAPPEN}/dran^63wiz/ { DROWN OTHERWISE }Figure 3: All of the significant dictionary errors.Some of them, like /J'iz/ are conglomerations thatshould have been divided.
Others, l ike / t / ,  /wo/,and /don/ demonstrate how the system compen-sates for the morphological irregularity of Englishcontractions.
The / I~/problem is discussed in thetext; misanalysis of the role o f / I~ /  also manifestsitself on something.The most obvious error visible in figure 3 is thesuffix -ing (/I~/), which should be have an empty se-meme set.
Indeed, such a word is properly hypothe-sized but a special mechanism prevents emanticallyempty words from being added to the dictionary.Without this mechanism, the system would chance312Sentencethis is a book.what do you see in the book?how many rabbits?how many?one rabbit.what is the rabbit doing?Phones/bIslzebuk//watduyusilnb~buk//hat~menirabhlts//hatlmeni//w^nrabblt//watlzb~rabbItdulD /Sememes{ THIS BE A'B00K ){ WHAT DO YOU SEE IS THE BOOK }{ HOW MANY RABBIT }{ HOW MANY }{ ONE RABBIT }{ WHAT BE THE RABBIT DO }Figure 1: The first 6 utterances from the Childes database used to test the algorithm.upon a new word like ring,/rig/, use the / I~/{}  toaccount for most of the sound, and build a new word/ r /{  RINa } to cover the rest; witness omething infigure 3.
Most other semantically-empty affixes (plu-ra l / s / fo r  instance) are also properly hypothesizedand disallowed, but the dictionary learns multipleentries to account for them (/eg/ "egg" and /egz/"eggs").
The system learns synonyms ("is", "was","am", .
.
. )
and homonyms ("read", "red" ; "know","no") without difficulty.Removing the restriction on empty semantics, andalso setting the semantics of the function words a,an, the, that and of to {}, the most common emptywords learned are given in figure 4.
The ring prob-lem surfaces: among other words learned are now/k /{  CAR } and/br /{  BRI/IG }.
To fix such prob-lems, it is obvious more constraint on morphemeorder must be incorporated into the parsing pro-cess, perhaps in the form of a statistical grammaracquired simultaneously with the dictionary.Word Sourcel~v/ {} -~,,gI~ I  {} the/o /{}  ?/ r /{} uo./yo.
,Is/{) plur~ -~I t /  {) is/'sWord Source/wo/{}  ?/el {} a/an/{} ../~, , /{} o//z/ {} plural -sFigure 4: The most common semantically emptywords in the final dictionary.4 Cur rent  D i rec t ionsThe algor i thm described above is extremely simple,as was the input fed to it.
In part icular,?
The input was phonetical ly oversimplified, eachword pronounced the same way each t ime it oc-curred, regardless of environment.
There wasno phonological noise and no cross-word effects.?
The semantic  representations were not onlynoise free and unambiguous,  but correspondeddirectly to the words in the utterance.To better investigate more realistic formulationsof the acquisition problem, we are extending ourcoverage to actual phonetic transcriptions of speech,by allowing for various phonological processes andnoise, and by building in probabilistic models ofmorphology and syntax.
We are further reducingthe information present in the semantic input byremoving all function word symbols and mergingvarious content symbols to encompass several wordparadigms.
We hope to transition to phonemic in-put produced by a phoneme-based speech recognizerin the near future.Finally, we are instituting an objective test mea-sure: rather than examining the dictionary directly,we will compare segmentation and morpheme-labeling to textual transcripts of the input speech.5 AcknowledgementsThis research is supported by NSF grant 9217041-ASC and AR.PA under the ttPCC program.ReferencesTimothy Andrew Cartwright and Michael R. Brent.1994.
Segmenting speech without a lexicon: Evi-dence for a bootstrapping model of lexical acqui-sition.
In Proc.
of the 16th Annual Meeting of theCognitive Science Society, IIillsdale, New Jersey.A.
P. Dempster, N. M. Liard, and D. B. Rubin.
1977.Maximum liklihood from incomplete data via theEM algorithm.
Journal of the Royal StatisticalSociety, B(39):1-38.B.
MacWhinney and C. Snow.
1985.
The child lan-guage data exchange system.
Journal of ChildLanguage, 12:271-296.Donald Cort Olivier.
1968.
Stochastic Grammarsand Language Acquisition Mechanisms.
Ph.D.thesis, Harvard University, Cambridge, Mas-sachusetts.Patrick Suppes.
1973.
The semantics of children'slanguage.
American Psychologist.J.
Gerald Wolff.
1982.
Language acquisition, datacompression and generalization.
Language andCommunication, 2(1):57-89.313
