Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 55?63,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsThe PaGe 2008 Shared Task on Parsing German?Sandra Ku?blerDepartment of LinguisticsIndiana UniversityBloomington, IN, USAskuebler@indiana.eduAbstractThe ACL 2008 Workshop on Parsing Germanfeatures a shared task on parsing German.
Thegoal of the shared task was to find reasonsfor the radically different behavior of parserson the different treebanks and between con-stituent and dependency representations.
Inthis paper, we describe the task and the datasets.
In addition, we provide an overview ofthe test results and a first analysis.1 IntroductionGerman is one of the very few languages for whichmore than one syntactically annotated resource ex-ists.
Other languages for which this is the case in-clude English (with the Penn treebank (Marcus etal., 1993), the Susanne Corpus (Sampson, 1993),and the British section of the ICE Corpus (Wallisand Nelson, 2006)) and Italian (with ISST (Mon-tegmagni et al, 2000) and TUT (Bosco et al,2000)).
The three German treebanks are Negra(Skut et al, 1998), TIGER (Brants et al, 2002), andTu?Ba-D/Z (Hinrichs et al, 2004).
We will concen-trate on TIGER and Tu?Ba-D/Z here; Negra is an-notated with an annotation scheme very similar toTIGER but is smaller.
In contrast to other languages,these two treebanks are similar on many levels:Both treebanks are based on newspaper text, bothuse the STTS part of speech (POS) tagset (Thie-len and Schiller, 1994), and both use an annotation?I am very grateful to Gerald Penn, who suggested thisworkshop and the shared task, took over the biggest part of theworkshop organization and helped with the shared task.scheme based on constituent structure augmentedwith grammatical functions.
However, they differ inthe choices made in the annotation schemes, whichmakes them ideally suited for an investigation ofhow these decisions influence parsing accuracy indifferent parsers.On a different level, German is an interestinglanguage for parsing because of the syntactic phe-nomena in which the language differs from English,the undoubtedly most studied language in parsing:German is often listed as a non-configurational lan-guage.
However, while the word order is freerthan in English, the language exhibits a less flexibleword order than more typical non-configurationallanguages.
A short overview of German word orderphenomena is given in section 2.The structure of this paper is as follows: Section2 discusses three characteristics of German word or-der, section 3 provides a definition of the shared task,and section 4 gives a short overview of the treebanksand their annotation schemes that were used in theshared task.
In section 5, we give an overview of theparticipating systems and their results.2 German Word OrderIn German, the order of non-verbal phrases is rela-tively free, but the placement of the verbal elementsis determined by the clause type.
Thus, we willfirst describe the placement of the finite verb, thenwe will explain phrasal ordering, and finally we willlook at discontinuous constituents.552.1 Verb PlacementIn German, the clause type determines the place-ment of finite verbs: In non-embedded declarativeclauses, as in (1a), the finite verb is in second posi-tion (V2).
In yes/no questions, as in (1b), the finiteverb is the clause-initial constituent (V1), and in em-bedded clauses, as in (1c), it appears clause finally(Vn).
(1) a. DerTheMannmanhathasdastheAutocargekauft.bought?The man has bought the car.?b.
HatHasdertheMannmandastheAutocargekauft?bought?Has the man bought the car??a.
dassthatdertheMannmandastheAutocargekauftboughthat.has?.
.
.
that the man has bought the car.
?All non-finite verbs appear at the right peripheryof the clause (cf.
2), independently of the clausetype.
(2) DerTheMannmansollteshoulddastheAutocargekauftboughthaben.have?The man should have bought the car.
?2.2 Flexible Phrase OrderingApart from the fixed placement of the verbs, the or-der of the non-verbal elements is flexible.
In (3), anyof the four complements and adjuncts of the mainverb (ge)geben can be in sentence-initial position,depending on the information structure of the sen-tence.
(3) a. DasTheKindchildhathasdemtheMannmangesternyesterdaydentheBallballgegeben.given?The child has given the ball to the man yes-terday.?b.
Dem Mann hat das Kind gestern den Ballgegeben.c.
Gestern hat das Kind dem Mann den Ballgegeben.d.
Den Ball hat das Kind gestern dem Manngegeben.In addition, the ordering of the elements that oc-cur between the finite and the non-finite verb formsis also free so that there are six possible lineariza-tions for each of the examples in (3a-d).One exception to the free ordering of non-verbalelements is the ordering of pronouns.
If the pro-nouns appear to the right of the finite verb in V1and V2 clauses, they are adjacent to the finite verbin fixed order.
(4) GesternYesterdayhathassieshesieher/themihmhimgegeben.given.
?Yesterday, she gave her/them to him.
?In (4), three pronouns are present.
Althoughthe pronoun sie is ambiguous between nomina-tive/accusative singular and nominative/accusativeplural, the given example is unambiguous with re-spect to case since the nominative precedes the ac-cusative, which in turn precedes the dative.Due to the flexible phrase ordering, the grammat-ical functions of constituents in German, unlike inEnglish, cannot be deduced from the constituents?location in the constituent tree.
As a consequence,parsing approaches to German need to be based ontreebank data which contain a combination of con-stituent structure and grammatical functions ?
forparsing and evaluation.
For English, in contrast,grammatical functions are often used internally inparsers but suppressed in the final parser output.2.3 Discontinuous ConstituentsAnother characteristic of German word order is thefrequency of discontinuous constituents.
The sen-tence in (5) shows an extraposed relative clause thatis separated from its head noun das Buch by the non-finite verb gelesen.
(5) DerTheMannmanhathasdastheBuchbookgelesen,read,daswhichichIihmhimempfohlenrecommendedhabe.have?The man read the book that I recommended tohim.
?56In German, it is also possible to partially frontVPs, such as in sentence (6).
This sentence is takenfrom the Tu?Ba-D/Z treebank.
(6) Fu?rFordentheBerlinerBerlinJobjobqualifiziertqualifiedhathassichhimselfZimmermannZimmermannauchalsodurchbyseinenhisBlickviewfu?rsfor thefinanziellfinanciallyMachbare.doable?Zimmermann qualified for the job in Berlinpartially because of his view for what is finan-cially feasible.
?Here, the canonical word order would be Zimmer-mann hat sich auch durch seinen Blick f?urs finanziellMachbare fu?r den Berliner Job qualifiziert.Such discontinuous structures occur frequently inthe TIGER and Tu?Ba-D/Z treebanks and are handleddifferently in the two annotation schemes, as will bediscussed in more detail in section 4.3 Task DefinitionIn this section, we give the definition of the sharedtask.
We provided two subtasks: parsing constituentstructure and parsing the dependency representa-tions.
Both subtasks involved training and testing ondata from the two treebanks, TIGER and Tu?Ba-D/Z.The dependency format was derived from the con-stituent format so that the sentences were identicalin the two versions.
The participants were giventraining sets, development sets, and test sets of thetwo treebanks.
The training sets contained 20894sentences per treebank, the development and testset consisted of 2611 sentences each.
The test setscontained gold standard POS labels.
In these sets,sentence length was restricted to a maximum of 40words.
Since for some sentences in both treebanks,the annotation consists of more than one tree, alltrees were joined under a virtual root node, VROOT.Since some parsers cannot assign grammaticalfunctions to part of speech tags, these grammati-cal functions were provided for the test data as at-tached to the POS tags.
Participants were asked toperform a test without these functions if their parserwas equipped to provide them.
Two participants didsubmit these results, and in both cases, these resultswere considerably lower.Evaluation for the constituent version consistedof the PARSEVAL measures precision, recall, andF1 measure.
All these measures were calculated oncombinations of constituent labels and grammaticalfunctions.
Part of speech labels were not consideredin the evaluation.
Evaluation for the dependencyversion consisted of labeled and unlabeled attach-ment scores.
For this evaluation, we used the scriptsprovided by the CoNLL shared task 2007 on depen-dency parsing (Nivre et al, 2007).4 The TreebanksThe two treebanks used for the shared task werethe TIGER Corpus, (Brants et al, 2002) version2, and the Tu?Ba-D/Z treebank (Hinrichs et al,2004; Telljohann et al, 2006), version 3.
Bothtreebanks use German newspapers as their datasource: the Frankfurter Rundschau newspaper forTIGER and the ?die tageszeitung?
(taz) newspaperfor Tu?Ba-D/Z.
The average sentence length isvery similar: In TIGER, sentences have an averagelength of 17.0, and in Tu?Ba-D/Z, 17.3.
This canbe regarded as an indication that the complexity ofthe two texts is comparable.
Both treebanks usethe same POS tagset, STTS (Thielen and Schiller,1994), and annotations based on phrase structuregrammar, enhanced by a level of predicate-argumentstructure.4.1 The Constituent DataDespite all the similarities presented above, theconstituent annotations differ in four important as-pects: 1) TIGER does not allow for unary branch-ing whereas Tu?Ba-D/Z does; 2) in TIGER, phraseinternal annotation is flat whereas Tu?Ba-D/Z usesphrase internal structure; 3) TIGER uses crossingbranches to represent long-distance relationshipswhereas Tu?Ba-D/Z uses a pure tree structure com-bined with functional labels to encode this informa-tion.
The two treebanks also use different notions ofgrammatical functions: Tu?Ba-D/Z defines 36 gram-matical functions covering head and non-head in-formation, as well as subcategorization for comple-ments and modifiers.
TIGER utilizes 51 grammati-cal functions.
Apart from commonly accepted gram-matical functions, such as SB (subject) or OA (ac-cusative object), TIGER grammatical functions in-57Figure 1: TIGER annotation with crossing branches.Figure 2: TIGER annotation with resolved crossing branches.clude others, e.g.
RE (repeated element) or RC (rel-ative clause).
(7) BeimAt theMu?nchnerMunichGipfelSummitistisdiethesprichwo?rtlicheproverbialbayerischeBavarianGemu?tlichkeit?Gemu?tlichkeit?vonbyeinemaBildpictureverdra?ngtsupplantedworden,been,daswhichimin theWortsinneliteral senseanofeinenaPolizeistaatpolice stateerinnert.reminds?At the Munich Summit, the proverbial Bavar-ian ?Gemu?tlichkeit?
was supplanted by an im-age that is evocative of a police state.
?Figure 1 shows a typical tree from the TIGERtreebank for sentence (7).
The syntactic categoriesare shown in circular nodes, the grammatical func-tions as edge labels in square boxes.
A majorphrasal category that serves to structure the sen-tence as a whole is the verb phrase (VP).
It con-tains non-finite verbs (here: verdra?ngt worden) aswell as their complements and adjuncts.
The subjectNP (die sprichwo?rtliche bayerische Gemu?tlichkeit)is outside the VP and, depending on its linear po-sition, leads to crossing branches with the VP.
Thishappens in all cases where the subject follows thefinite verb as in Figure 1.
Notice also that the PPsare completely flat.
An additional crossing branchresults from the direct attachment of the extraposedrelative clause (the lower S node with function RC)to the noun that it modifies.As mentioned in the previous section, TIGERtrees must be transformed into trees without crossingbranches prior to training PCFG parsers.
The stan-dard approach for this transformation is to re-attachcrossing non-head constituents as sisters of the low-est mother node that dominates all the crossing con-stituent and its sister nodes in the original TIGERtree.
Figure 2 shows the result of this transformation58Figure 3: Tu?Ba-D/Z annotation without crossing branches.of the tree in Figure 1.
Crossing branches not onlyarise with respect to the subject at the sentence levelbut also in cases of extraposition and fronting of par-tial constituents.
As a result, approximately 30% ofall TIGER trees contain at least one crossing branch.Thus, tree transformations have a major impact onthe type of constituent structures that are used fortraining probabilistic parsing models.Figure 3 shows the Tu?Ba-D/Z annotation for sen-tence (8), a sentence with a very similar structure tothe TIGER sentence shown in Figure 1.
Crossingbranches are avoided by the introduction of topo-logical structures (here: VF, LK, MF, VC, NF, andC) into the tree.
Notice also that compared to theTIGER annotation, Tu?Ba-D/Z introduces more inter-nal structure into NPs and PPs.
In Tu?Ba-D/Z, long-distance relationships are represented by a pure treestructure and specific functional labels.
Thus, theextraposed relative clause is attached to the matrixclause directly, but its functional label ON-MOD ex-plicates that it modifies the subject ON.
(8) InInBremenBremensindarebisherso farnuronlyFaktenfactsgeschaffenproducedworden,been,diewhichjederanymodernenmodernStadtplanungcity planningentgegenstehen.contradict?In Bremen, so far only such attempts havebeen made that are opposed to any modern cityplanning.
?4.2 The Dependency DataThe constituent representations from both treebankswere converted into dependencies.
The conver-sion aimed at finding dependency representationsfor both treebanks that are as similar to each otheras possible.
Complete identity is impossible be-cause the treebanks contain different levels of dis-tinction for different phenomena.
The conversion isbased on the original formats of the treebanks in-cluding crossing branches.
The target dependencyformat was defined based on the dependency gram-mar by Foth (2003).
For the conversion, we usedpre-existing dependency converters for TIGER trees(Daum et al, 2004) and for Tu?Ba-D/Z trees (Vers-ley, 2005).
The dependency representations of thetrees in Figures 1 and 3 are shown in Figures 4 and5.
Note that the long-distance relationships are con-verted into non-projective dependencies.5 Submissions and ResultsThe shared task drew submissions from 3 groups:the Berkeley group, the Stanford group, and theVa?xjo?
group.
Four more groups or individuals hadregistered but did not submit any data.
The submit-ted systems and results are described in detail in pa-pers in this volume (Petrov and Klein, 2008; Raf-ferty and Manning, 2008; Hall and Nivre, 2008).
Allthree systems submitted results for the constituenttask.
For the dependency task, the Va?xjo?
group hadthe only submission.
For this reason, we will con-centrate on the analysis of the constituent results andwill mention the dependency results only shortly.59Beim M. Gipfel ist die sprichw.
bayer.
Gem.
von einem Bild verdra?ngt worden, das im Worts.
an einen P.staat erinnert.PPATTRPNDETATTRATTRSUBJPPDETPNAUXAUX SUBJPPPNOBJPDETPNRELFigure 4: TIGER dependency annotation.In Bremen sind bisher nur Fakten geschaffen worden, die jeder modernen Stadtplanung entgegenstehen.PNPP ADVADVSUBJAUXAUXSUBJDETATTR OBJDRELFigure 5: Tu?Ba-D/Z dependency annotation.5.1 Constituent EvaluationThe results of the constituent analysis are shownin Table 1.
The evaluation was performed with re-gard to labels consisting of a combination of syn-tactic labels and grammatical functions.
A subjectnoun phrase, for example, is only counted as correctif it has the correct yield, the correct label (i.e.
NPfor TIGER and NX for Tu?Ba-D/Z), and the correctgrammatical function (i.e.
SB for TIGER and ONfor Tu?Ba-D/Z).
The results show that the Berke-ley parser reaches the best results for both treebanks.The other two parsers compete for second place.
ForTIGER, the Va?xjo?
parser outperforms the Stanfordparser, but for Tu?Ba-D/Z, the situation is reversed.This gives an indication that the Va?xjo?
parser seemsbetter suited for the flat annotations in TIGER whilethe Stanford parser is better suited for the more hier-archical structure in Tu?Ba-D/Z.
Note that all parsersreach much higher F-scores for Tu?Ba-D/Z.A comparison of howwell suited two different an-notation schemes are for parsing is a surprisinglydifficult task.
A first approach would be to com-pare the parser performance for specific categories,such as for noun phrases, etc.
However, this isnot possible for TIGER and Tu?Ba-D/Z.
On the onehand, the range of phenomena described as nounphrases, for example, is different in the two tree-banks.
The most obvious difference in annotationschemes is that Tu?Ba-D/Z annotates unary branch-ing structures while TIGER does not.
As a conse-quence, in Tu?Ba-D/Z, all pronouns and substitut-ing demonstratives are annotated as noun phrases; inTIGER, they are attached directly to the next highernode (cf.
the relative pronouns, POS tag PRELS, inFigures 1 and 3).
Ku?bler (2005) and Maier (2006)suggest a method for comparing such different an-notation schemes by approximating them stepwiseso that the decisions which result in major changescan be isolated.
They come to the conclusion thatthe differences between the two annotation schemesis a least partially due to inconsistencies introducedinto TIGER style annotations during the resolutionof crossing branches.
However, even this methodcannot give any indication which annotation schemeprovides more useful information for systems thatuse such parses as input.
To answer this question, anin vivo evaluation would be necessary.
It is, how-ever, rather difficult to find systems into which aparser can be plugged in without too many modi-fications of the system.On the other hand, it is a well-known fact that60TIGER Tu?Ba-D/Zsystem precision recall F-score precision recall F-scoreBerkeley 69.23 70.41 69.81 83.91 84.04 83.97Stanford 58.52 57.63 58.07 79.26 79.22 79.24Va?xjo?
67.06 63.40 65.18 76.44 74.79 75.60Table 1: The results of the constituent parsing task.TIGER Tu?Ba-D/Zsystem GF precision recall F-score precision recall F-scoreBerkeley SB/ON 74.46 78.31 76.34 78.33 77.08 77.70OA 60.08 66.61 63.18 58.11 65.81 61.72DA/OD 49.28 41.72 43.19 59.46 44.72 51.05Stanford SB/ON 64.40 63.11 63.75 71.16 77.76 74.31OA 45.52 45.91 45.71 47.23 51.28 49.17DA/OD 12.40 9.82 10.96 24.42 8.54 12.65Va?xjo?
SB/ON 75.33 73.00 74.15 72.37 69.53 70.92OA 57.01 57.65 57.33 58.07 57.55 57.81DA/OD 55.45 37.42 44.68 63.75 20.73 31.29Table 2: The results for subjects, accusative objects, and dative objects.the PARSEVALmeasures favor annotation schemeswith hierarchical structures, such as in Tu?Ba-D/Z,in comparison to annotation schemes with flat struc-tures (Rehbein and van Genabith, 2007).
Here,TIGER and Tu?Ba-D/Z differ significantly: in TIGER,phrases receive a flat annotation.
Prepositionalphrases, for example, do not contain an explicitlyannotated noun phrase.
Tu?Ba-D/Z phrases, in con-trast, are more hierarchical; preposition phrases docontain a noun phrase, and non phrases distinguishbetween pre- and post-modification.
For this reason,the evaluation presented in Table 1 must be takenwith more than a grain of salt as a comparison of an-notation schemes.
However, it seems safe to followKu?bler et al (Ku?bler et al, 2006) in the assump-tion that the major grammatical functions, subject(SB/ON), accusative object (OA), and dative object(DA/OD) are comparable.
Again, this is not com-pletely true because in the case of one-word NPs,these functions are attached to the POS tags andthus are given in the input.
Another solution, whichwas pursued by Rehbein and van Genabith (2007),is the introduction of new unary branching nodes inthe tree in cases where such grammatical functionsare originally attached to the POS tag.
We refrainedfrom using this solution because it introduces fur-ther inconsistencies (only a subset of unary branch-ing nodes are explicitly annotated), which make itdifficult for a parser to decide whether to group suchphrases or not.
The evaluation shown in Table 2 isbased on all nodes which were annotated with thegrammatical function in question.The results presented in Table 2 show that thedifferences between the two treebanks are incon-clusive.
While the Stanford parser performs con-sistently better on Tu?Ba-D/Z, the Berkeley parserhandles accusative objects better in TIGER, and theVa?xjo?
parser subjects and dative objects.
The resultsindicate that the Berkeley parser profits from theTIGER annotation of accusative objects, which aregrouped in the verb phrase while Tu?Ba-D/Z groupsall objects in their fields directly without resorting toa verb phrase.
However, this does not explain whythe Berkeley parser cannot profit from the subjectattachment on the clause level in TIGER to the samedegree.5.2 Dependency EvaluationThe results of the dependency evaluation for theVa?xjo?
system are shown in Table 3.
The results are61TIGER Tu?Ba-D/ZUAS 92.63 91.45LAS 90.80 88.64precision recall precision recallSUBJ 90.20 89.82 88.99 88.55OBJA 77.93 82.19 77.18 82.71OBJD 57.00 44.02 67.88 45.90Table 3: The results of the dependency evaluation.important for the comparison of constituent and de-pendency parsing since in the conversion to depen-dencies, most of the differences between the anno-tation schemes, and as a consequence, the prefer-ence of the PARSEVAL measures have been neu-tralized.
Therefore, it is interesting to see that theresults for TIGER are slightly better than the resultsfor Tu?Ba-D/Z, both for unlabeled (UAS) and la-beled attachment scores.
The reasons for these dif-ferences are unclear: either the TIGER texts are eas-ier to parse, or the (original annotation and) conver-sion from TIGER is more consistent.
Another sur-prising fact is that the dependency results are clearlybetter than the constituent ones.
This is partly dueto the fact that the dependency representation is of-ten less informative than then constituent representa-tion.
One example for this can be found in coordina-tions: In dependency representations, the scope am-biguity in phrases like young men and women is notresolved.
This gives parsers fewer opportunities togo wrong.
However, this cannot explain all the dif-ferences.
Especially the better performance on themajor grammatical functions cannot be explained inthis way.A closer look at the grammatical functions showsthat here, precision and recall are higher than forconstituent parses.
This is a first indication that de-pendency representation may be more appropriatefor languages with freer word order.
A compari-son between the two treebanks is inconclusive: forthe accusative object, the results are similar betweenthe treebanks.
For subjects, the results for TIGERare better while for dative objects, the results forTu?Ba-D/Z are better.
This issue requires closer in-vestigation.6 ConclusionThis is the first shared task on parsing German,which provides training and test sets from both ma-jor treebanks for German, TIGER and Tu?Ba-D/Z.For both treebanks, we provided a constituent and adependency representation.
It is our hope that thesedata sets will spark more interest in the comparisonof different annotation schemes and their influenceon parsing results.
The evaluation of the three par-ticipating systems has shown that for both treebanks,the use of a latent variable grammar in the Berkeleysystem is beneficial.
However, many questions re-main unanswered and require further investigation:To what extent do the evaluation metrics distort theresults?
Does a measure exist that is neutral towardsthe differences in annotation?
Is the dependency for-mat better suited for parsing German?
Are the dif-ferences between the dependency results of the twotreebanks indicators that TIGER provides more im-portant information for dependency parsing?
Or canthe differences be traced back to the conversion al-gorithms?AcknowledgmentsFirst and foremost, we want to thank all the peopleand organizations that generously provided us withtreebank data and without whom the shared taskwould have been literally impossible: Erhard Hin-richs, University of Tu?bingen (Tu?Ba-D/Z), and HansUszkoreit, Saarland University and DFKI (TIGER).Secondly, we would like to thank Wolfgang Maierand Yannick Versley who performed the data con-versions necessary for the shared task.
Additionally,Wolfgang provided the scripts for the constituentevaluation.ReferencesCristina Bosco, Vincenzo Lombardo, D. Vassallo, andLeonardo Lesmo.
2000.
Building a treebank for Ital-ian: a data-driven annotation scheme.
In Proceedingsof the 2nd International Conference on Language Re-sources and Evaluation, LREC-2000, Athens, Greece.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In Erhard Hinrichs and Kiril Simov, edi-tors, Proceedings of the First Workshop on Treebanks62and Linguistic Theories (TLT 2002), pages 24?41, So-zopol, Bulgaria.Michael Daum, Kilian Foth, and Wolfgang Menzel.2004.
Automatic transformation of phrase treebanksto dependency trees.
In Proceedings of the 4th In-ternational Conference on Language Resources andEvaluation, LREC-2004, Lisbon, Portugal.Kilian Foth.
2003.
Eine umfassende Dependenzgram-matik des Deutschen.
Technical report, FachbereichInformatik, Universita?t Hamburg.Johan Hall and Joakim Nivre.
2008.
A dependency-driven parser for German dependency and con-stituency representations.
In Proceedings of the ACLWorkshop on Parsing German, Columbus, OH.Erhard Hinrichs, Sandra Ku?bler, Karin Naumann, HeikeTelljohann, and Julia Trushkina.
2004.
Recent de-velopments in linguistic annotations of the Tu?Ba-D/Ztreebank.
In Proceedings of the Third Workshopon Treebanks and Linguistic Theories, pages 51?62,Tu?bingen, Germany.Sandra Ku?bler, ErhardW.
Hinrichs, andWolfgangMaier.2006.
Is it really that difficult to parse German?In Proceedings of the 2006 Conference on Empiri-cal Methods in Natural Language Processing, EMNLP2006, pages 111?119, Sydney, Australia.Sandra Ku?bler.
2005.
How do treebank annotationschemes influence parsing results?
Or how not to com-pare apples and oranges.
In Proceedings of the Inter-national Conference on Recent Advances in NaturalLanguage Processing, RANLP 2005, pages 293?300,Borovets, Bulgaria.WolfgangMaier.
2006.
Annotation schemes and their in-fluence on parsing results.
In Proceedings of the ACL-2006 Student Research Workshop, Sydney, Australia.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.S.
Montegmagni, F. Barsotti, M. Battista, N. Calzolari,O.
Corazzari, A. Zampolli, F. Fanciulli, M. Massetani,R.
Raffaelli, R. Basili, M. T. Pazienza, D. Saracino,F.
Zanzotto, N. Mana, F. Pianesi, and R. Delmonte.2000.
The Italian syntactic-semantic treebank: Ar-chitecture, annotation, tools and evaluation.
In Pro-ceedings of the Workshop on Linguistically InterpretedCorpora LINC-2000, pages 18?27, Luxembourg.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 shared task on dependencyparsing.
In Proceedings of the CoNLL 2007 SharedTask.
Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, EMNLP-CoNLL 2007, Prague,Czech Republic.Slav Petrov and Dan Klein.
2008.
Parsing German withlanguage agnostic latent variable grammars.
In Pro-ceedings of the ACL Workshop on Parsing German,Columbus, OH.Anna Rafferty and Christopher Manning.
2008.
Parsingthree German treebanks: Lexicalized and unlexical-ized baselines.
In Proceedings of the ACL Workshopon Parsing German, Columbus, OH.Ines Rehbein and Josef van Genabith.
2007.
Treebankannotation schemes and parser evaluation for German.In Proceedings of the 2007 Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning, EMNLP-CoNLL, pages 630?639, Prague, Czech Republic.Geoffrey Sampson.
1993.
The SUSANNE corpus.ICAME Journal, 17:125 ?
127.Wojciech Skut, Thorsten Brants, Brigitte Krenn, andHans Uszkoreit.
1998.
A linguistically interpretedcorpus of German newspaper texts.
In ESSLLIWorkshop on Recent Advances in Corpus Annotation,Saarbru?cken, Germany.Heike Telljohann, Erhard W. Hinrichs, Sandra Ku?bler,and Heike Zinsmeister, 2006.
Stylebook forthe Tu?bingen Treebank of Written German (Tu?Ba-D/Z).
Seminar fu?r Sprachwissenschaft, Universita?tTu?bingen, Germany.Christine Thielen and Anne Schiller.
1994.
Ein kleinesund erweitertes Tagset fu?rs Deutsche.
In Helmut Feld-weg and Erhard Hinrichs, editors, Lexikon & Text,pages 215?226.
Niemeyer, Tu?bingen.Yannick Versley.
2005.
Parser evaluation across texttypes.
In Proceedings of the Fourth Workshop on Tree-banks and Linguistic Theories, TLT 2005, pages 209?220, Barcelona, Spain.Sean Wallis and Gerald Nelson.
2006.
The British com-ponent of the International Corpus of English.
Release2.
CD-ROM.
London: Survey of English Usage, UCL.63
