Towards Answering Opinion Questions: Separating Facts from Opinionsand Identifying the Polarity of Opinion SentencesHong YuDepartment of Computer ScienceColumbia UniversityNew York, NY 10027, USAhongyu@cs.columbia.eduVasileios HatzivassiloglouDepartment of Computer ScienceColumbia UniversityNew York, NY 10027, USAvh@cs.columbia.eduAbstractOpinion question answering is a challenging taskfor natural language processing.
In this paper, wediscuss a necessary component for an opinion ques-tion answering system: separating opinions fromfact, at both the document and sentence level.
Wepresent a Bayesian classifier for discriminating be-tween documents with a preponderance of opinionssuch as editorials from regular news stories, anddescribe three unsupervised, statistical techniquesfor the significantly harder task of detecting opin-ions at the sentence level.
We also present a firstmodel for classifying opinion sentences as positiveor negative in terms of the main perspective be-ing expressed in the opinion.
Results from a largecollection of news stories and a human evaluationof 400 sentences are reported, indicating that weachieve very high performance in document classi-fication (upwards of 97% precision and recall), andrespectable performance in detecting opinions andclassifying them at the sentence level as positive,negative, or neutral (up to 91% accuracy).1 IntroductionNewswire articles include those that mainly presentopinions or ideas, such as editorials and letters tothe editor, and those that mainly report facts such asdaily news articles.
Text materials from many othersources also contain mixed facts and opinions.
Formany natural language processing applications, theability to detect and classify factual and opinion sen-tences offers distinct advantages in deciding what in-formation to extract and how to organize and presentthis information.
For example, information extrac-tion applications may target factual statements ratherthan subjective opinions, and summarization sys-tems may list separately factual information and ag-gregate opinions according to distinct perspectives.At the document level, information retrieval systemscan target particular types of articles and even utilizeperspectives in focusing queries (e.g., filtering or re-trieving only editorials in favor of a particular policydecision).Our motivation for building the opinion detec-tion and classification system described in this pa-per is the need for organizing information in thecontext of question answering for complex ques-tions.
Unlike questions like ?Who was the firstman on the moon??
which can be answered witha simple phrase, more intricate questions such as?What are the reasons for the US-Iraq war??
requirelong answers that must be constructed from multi-ple sources.
In such a context, it is imperative thatthe question answering system can discriminate be-tween opinions and facts, and either use the appro-priate type depending on the question or combinethem in a meaningful presentation.
Perspective in-formation can also help highlight contrasts and con-tradictions between different sources?there will besignificant disparity in the material collected for thequestion mentioned above between Fox News andthe Independent, for example.Fully analyzing and classifying opinions involvestasks that relate to some fairly deep semantic andsyntactic analysis of the text.
These include not onlyrecognizing that the text is subjective, but also de-termining who the holder of the opinion is, what theopinion is about, and which of many possible posi-tions the holder of the opinion expresses regardingthat subject.
In this paper, we are presenting threeof the components of our opinion detection and or-ganization subsystem, which have already been in-tegrated into our larger question-answering system.These components deal with the initial tasks of clas-sifying articles as mostly subjective or objective,finding opinion sentences in both kinds of articles,and determining, in general terms and without refer-ence to a specific subject, if the opinions are positiveor negative.
The three modules of the system dis-cussed here provide the basis for ongoing work forfurther classification of opinions according to sub-ject and opinion holder and for refining the originalpositive/negative attitude determination.We review related work in Section 2, and thenpresent our document-level classifier for opinion orfactual articles (Section 3), three implemented tech-niques for detecting opinions at the sentence level(Section 4), and our approach for rating an opinionas positive or negative (Section 5).
We have evalu-ated these methods using a large collection of newsarticles without additional annotation (Section 6)and an evaluation corpus of 400 sentences anno-tated for opinion classifications (Section 7).
Theresults, presented in Section 8, indicate that weachieve very high performance (more than 97%) atdocument-level classification and respectable per-formance (86?91%) at detecting opinion sentencesand classifying them according to orientation.2 Related WorkMuch of the earlier research in automated opiniondetection has been performed by Wiebe and col-leagues (Bruce and Wiebe, 1999; Wiebe et al, 1999;Hatzivassiloglou and Wiebe, 2000; Wiebe, 2000;Wiebe et al, 2002), who proposed methods for dis-criminating between subjective and objective text atthe document, sentence, and phrase levels.
Bruceand Wiebe (1999) annotated 1,001 sentences as sub-jective or objective, and Wiebe et al (1999) de-scribed a sentence-level Naive Bayes classifier usingas features the presence or absence of particular syn-tactic classes (pronouns, adjectives, cardinal num-bers, modal verbs, adverbs), punctuation, and sen-tence position.
Subsequently, Hatzivassiloglou andWiebe (2000) showed that automatically detectedgradable adjectives are a useful feature for subjec-tivity classification, while Wiebe (2000) introducedlexical features in addition to the presence/absenceof syntactic categories.
More recently, Wiebe et al(2002) report on document-level subjectivity classi-fication, using a k-nearest neighbor algorithm basedon the total count of subjective words and phraseswithin each document.Psychological studies (Bradley and Lang, 1999)found measurable associations between words andhuman emotions.
Hatzivassiloglou and McKeown(1997) described an unsupervised learning methodfor obtaining positively and negatively oriented ad-jectives with accuracy over 90%, and demonstratedthat this semantic orientation, or polarity, is a con-sistent lexical property with high inter-rater agree-ment.
Turney (2002) showed that it is possibleto use only a few of those semantically orientedwords (namely, ?excellent?
and ?poor?)
to labelother phrases co-occuring with them as positive ornegative.
He then used these phrases to automati-cally separate positive and negative movie and prod-uct reviews, with accuracy of 66?84%.
Pang et al(2002) adopted a more direct approach, using super-vised machine learning with words and n-grams asfeatures to predict orientation at the document levelwith up to 83% precision.Our approach to document and sentence classi-fication of opinions builds upon the earlier workby using extended lexical models with additionalfeatures.
Unlike the work cited above, we do notrely on human annotations for training but only onweak metadata provided at the document level.
Oursentence-level classifiers introduce additional crite-ria for detecting subjective material (opinions), in-cluding methods based on sentence similarity withina topic and an approach that relies on multiple clas-sifiers.
At the document level, our classifier uses thesame document labels that the method of (Wiebe etal., 2002) does, but automatically detects the wordsand phrases of importance without further analy-sis of the text.
For determining whether an opin-ion sentence is positive or negative, we have usedseed words similar to those produced by (Hatzivas-siloglou and McKeown, 1997) and extended them toconstruct a much larger set of semantically orientedwords with a method similar to that proposed by(Turney, 2002).
Our focus is on the sentence level,unlike (Pang et al, 2002) and (Turney, 2002); weemploy a significantly larger set of seed words, andwe explore as indicators of orientation words fromsyntactic classes other than adjectives (nouns, verbs,and adverbs).3 Document ClassificationTo separate documents that contain primarily opin-ions from documents that report mainly facts, we ap-plied Naive Bayes1, a commonly used supervisedmachine-learning algorithm.
This approach pre-supposes the availability of at least a collection of ar-ticles with pre-assigned opinion and fact labels at thedocument level; fortunately, Wall Street Journal ar-ticles contain such metadata by identifying the typeof each article as Editorial, Letter to editor, Businessand News.
These labels are used only to provide thecorrect classification labels during training and eval-uation, and are not included in the feature space.
Weused as features single words, without stemming orstopword removal.
Naive Bayes assigns a document to the class  that maximizes  	by applyingBayes?
rule  	 and assuming con-ditional independence of the features.Although Naive Bayes can be outperformed intext classification tasks by more complex methodssuch as SVMs, Pang et al (2002) report similar per-formance for Naive Bayes and other machine learn-ing techniques for a similar task, that of distinguish-ing between positive and negative reviews at thedocument level.
Further, we achieved such high per-formance with Naive Bayes (see Section 8) that ex-ploring additional techniques for this task seemedunnecessary.4 Finding Opinion SentencesWe developed three different approaches to clas-sify opinions from facts at the sentence level.
Toavoid the need for obtaining individual sentence an-notations for training and evaluation, we rely in-stead on the expectation that documents classifiedas opinion on the whole (e.g., editorials) will tend tohave mostly opinion sentences, and conversely doc-uments placed in the factual category will tend tohave mostly factual sentences.
Wiebe et al (2002)report that this expectation is borne out 75% of thetime for opinion documents and 56% of the time forfactual documents.4.1 Similarity ApproachOur first approach to classifying sentences as opin-ions or facts explores the hypothesis that, within agiven topic, opinion sentences will be more simi-lar to other opinion sentences than to factual sen-1Using the Rainbow implementation, available from www.cs.cmu.edu/?mccallum/bow/rainbow.tences.
We used SIMFINDER (Hatzivassiloglou etal., 2001), a state-of-the-art system for measuringsentence similarity based on shared words, phrases,and WordNet synsets.
To measure the overall simi-larity of a sentence to the opinion or fact documents,we first select the documents that are on the sametopic as the sentence in question.
We obtain topicsas the results of IR queries (for example, by search-ing our document collection for ?welfare reform?
).We then average its SIMFINDER-provided similari-ties with each sentence in those documents.
Thenwe assign the sentence to the category for which theaverage is higher (we call this approach the ?score?variant).
Alternatively, for the ?frequency?
variant,we do not use the similarity scores themselves butinstead we count how many of them, for each cate-gory, exceed a predetermined threshold (empiricallyset to 0.65).4.2 Naive Bayes ClassifierOur second method trains a Naive Bayes classifier(see Section 3), using the sentences in opinion andfact documents as the examples of the two cate-gories.
The features include words, bigrams, andtrigrams, as well as the parts of speech in each sen-tence.
In addition, the presence of semantically ori-ented (positive and negative) words in a sentence isan indicator that the sentence is subjective (Hatzi-vassiloglou and Wiebe, 2000).
Therefore, we in-clude in our features the counts of positive and neg-ative words in the sentence (which are obtained withthe method of Section 5.1), as well as counts ofthe polarities of sequences of semantically orientedwords (e.g., ?++?
for two consecutive positively ori-ented words).
We also include the counts of partsof speech combined with polarity information (e.g.,?JJ+?
for positive adjectives), as well as features en-coding the polarity (if any) of the head verb, themain subject, and their immediate modifiers.
Syn-tactic structure was obtained with Charniak?s statis-tical parser (Charniak, 2000).
Finally, we used asone of the features the average semantic orientationscore of the words in the sentence.4.3 Multiple Naive Bayes ClassifiersOur designation of all sentences in opinion or factualarticles as opinion or fact sentences is an approxima-tion.
To address this, we apply an algorithm usingmultiple classifiers, each relying on a different sub-set of our features.
The goal is to reduce the trainingset to the sentences that are most likely to be cor-rectly labeled, thus boosting classification accuracy.Given separate sets of features   		  ,we train separate Naive Bayes classifiers ,		 corresponding to each feature set.
Assum-ing as ground truth the information provided by thedocument labels and that all sentences inherit thestatus of their document as opinions or facts, wefirst train on the entire training set, then use theresulting classifier to predict labels for the trainingset.
The sentences that receive a label different fromthe assumed truth are then removed, and we train on the remaining sentences.
This process is re-peated iteratively until no more sentences can be re-moved.
We report results using five feature sets,starting from words alone and adding in bigrams, tri-grams, part-of-speech, and polarity.5 Identifying the Polarity of OpinionSentencesHaving distinguished whether a sentence is a fact oropinion, we separate positive, negative, and neutralopinions into three classes.
We base this decisionon the number and strength of semantically orientedwords (either positive or negative) in the sentence.We first discuss how such words are automaticallyfound by our system, and then describe the methodby which we aggregate this information across thesentence.5.1 Semantically Oriented WordsTo determine which words are semantically ori-ented, in what direction, and the strength of theirorientation, we measured their co-occurrence withwords from a known seed set of semantically ori-ented words.
The approach is based on the hypothe-sis that positive words co-occur more than expectedby chance, and so do negative words; this hypothe-sis was validated, at least for strong positive/negativewords, in (Turney, 2002).
As seed words, we usedsubsets of the 1,336 adjectives that were manuallyclassified as positive (657) or negative (679) byHatzivassiloglou and McKeown (1997).
In earlierwork (Turney, 2002) only singletons were used asseed words; varying their number allows us to testwhether multiple seed words have a positive effectin detection performance.
We experimented withseed sets containing 1, 20, 100 and over 600 positiveand negative pairs of adjectives.
For a given seed setsize, we denote the set of positive seeds as ADJ and the set of negative seeds as ADJ  .
We then cal-culate a modified log-likelihood ratio  ff POSfifor a word   with part of speech POSfi (fl can beadjective, adverb, noun or verb) as the ratio of itscollocation frequency with ADJ  and ADJ  within asentence, ffi   POSfi"!$#%& Freq (')+* POS, * ADJ - /.10Freq 2' all * POS , * ADJ - Freq 2'3)ffi* POS, * ADJ 4 5.60Freq 2' all * POS , * ADJ 4 78where Freq ffi all  POSfi  ADJ represents the collo-cation frequency of all words  all of part of speechPOS fi with ADJ  and 9 is a smoothing constant( 9;: 	=< in our case).
We used Brill?s tagger (Brill,1995) to obtain part-of-speech information.5.2 Sentence Polarity TaggingAs our measure of semantic orientation across anentire sentence we used the average per word log-likelihood scores defined in the preceding section.To determine the orientation of an opinion sentence,all that remains is to specify cutoffs >/ and >? sothat sentences for which the average log-likelihoodscore exceeds >5 are classified as positive opinions,sentences with scores lower than >? are classifiedas negative opinions, and sentences with in-betweenscores are treated as neutral opinions.
Optimal val-ues for >  and >  are obtained from the training datavia density estimation?using a small, hand-labeledsubset of sentences we estimate the proportion ofsentences that are positive or negative.
The valuesof the average log-likelihood score that correspondto the appropriate tails of the score distribution arethen determined via Monte Carlo analysis of a muchlarger sample of unlabeled training data.6 DataWe used the TREC2 8, 9, and 11 collections, whichconsist of more than 1.7 million newswire arti-cles.
The aggregate collection covers six differ-ent newswire sources including 173,252 Wall Street2http://trec.nist.gov/.Journal (WSJ) articles from 1987 to 1992.
Someof the WSJ articles have structured headings thatinclude Editorial, Letter to editor, Business, andNews (2,877, 1,695, 2,009 and 3,714 articles, re-spectively).
We randomly selected 2,000 articles3from each category so that our data set was approx-imate evenly divided between fact and opinion ar-ticles.
Those articles were used for both documentand sentence level opinion/fact classification.7 Evaluation Metrics and Gold StandardFor classification tasks (i.e., classifying betweenfacts and opinions and identifying the semantic ori-entation of sentences), we measured our system?sperformance by standard recall and precision.
Weevaluated the quality of semantically oriented wordsby mapping the extracted words and labels to an ex-ternal gold standard.
We took the subset of our out-put containing words that appear in the standard, andmeasured the accuracy of our output as the portionof that subset that was assigned the correct label.A gold standard for document-level classificationis readily available, since each article in our WallStreet Journal collection comes with an article typelabel (see Section 6).
We mapped article types Newsand Business to facts, and article types Editorial andLetter to the Editor to opinions.
We cannot auto-matically select a sentence-level gold standard dis-criminating between facts and opinions, or betweenpositive and negative opinions.
We therefore askedhuman evaluators to classify a set of sentences be-tween facts and opinions as well as determine thetype of opinions.Since we have implemented our methods in anopinion question answering system, we selected fourdifferent topics (gun control, illegal aliens, socialsecurity, and welfare reform).
For each topic, werandomly selected 25 articles from the entire com-bined TREC corpus (not just the WSJ portion); thesewere articles matching the corresponding topicalphrase given above as determined by the Lucenesearch engine.4 From each of these documents werandomly selected four sentences.
If a documenthappened to have less than four sentences, additional3Except for Letters to Editor, for which we included all1,695 articles available.4http://www.jguru.com/faq/Lucene.Label A B AgreementFact 123 16 46%Opinion 258 65 77%Uncertain 19 1 33%Breakdown of opinion labelsPositive 33 4 29%Negative 131 27 51%No orientation 45 6 26%Mixed orientation 8 0 0%Uncertain orientation 41 1 7%Table 1: Statistics of gold standards A and B.documents from the same topic were retrieved tosupply the missing sentences.
The resulting  <  :$:sentences were then interleaved sothat successive sentences came from different top-ics and documents and divided into ten 50-sentenceblocks.
Each block shares ten sentences with thepreceding and following block (the last block is con-sidered to precede the first one), so that 100 of the400 sentences appear in two blocks.
Each of ten hu-man evaluators (all with graduate training in com-putational linguistics) was presented with one blockand asked to select a label for each sentence amongthe following: ?fact?, ?positive opinion?, ?negativeopinion?, ?neutral opinion?, ?sentence contains bothpositive and negative opinions?, ?opinion but cannotdetermine orientation?, and ?uncertain?.5Since we have one judgment for 300 sentencesand two judgments for 100 sentences, we createdtwo gold standards for sentence classification.
Thefirst (Standard A) includes the 300 sentences withone judgment and a single judgment for the remain-ing 100 sentences.6 The second standard (StandardB) contains the subset of the 100 sentences for whichwe obtained identical labels.
Statistics of these twostandards are given in Table 1.
We measured thepairwise agreement among the 100 sentences thatwere judged by two evaluators, as the ratio of sen-tences that receive a label  from both evaluatorsdivided by the total number of sentences receivinglabel  from any evaluator.
The agreement across5The full instructions can be viewed online at http://www1.cs.columbia.edu/?hongyu/research/emnlp03/opinion-eval-instructions.html.6In order to assign a unique label, we arbitrarily chose thefirst evaluator for those sentences.F-measureNews vs. Editorial 0.96News+Business vs. Editorial+Letter 0.97Table 2: Document-level fact/opinion classificationby Naive Bayes algorithm.the 100 sentences for all seven choices was 55%;if we group together the five subtypes of opinionsentences, the overall agreement rises to 82%.
Thelow agreement for some labels was not surprisingbecause there is much ambiguity between facts andopinions.
An example of an arguable sentence is ?Alethal guerrilla war between poachers and wardensnow rages in central and eastern Africa?, which onerater classified as ?fact?
and another rater classifiedas ?opinion?.Finally, for evaluating the quality of extractedwords with semantic orientation labels, we used twodistinct manually labeled collections as gold stan-dards.
One set consists of the previously described657 positive and 679 negative adjectives (Hatzi-vassiloglou and McKeown, 1997).
We also usedthe ANEW list which was constructed during psy-cholinguistic experiments (Bradley and Lang, 1999)and contains 1,031 words of all four open classes.As described in (Bradley and Lang, 1999), humansassigned valence scores to each score according todimensions such as pleasure, arousal, and domi-nance; following heuristics proposed in psycholin-guistics7 we obtained 284 positive and 272 negativewords from the valence scores.8 Results and DiscussionDocument Classification We trained our Bayesclassifier for documents on 4,000 articles from theWSJ portion of our combined TREC collection, andevaluated on 4,000 other articles also from the WSJpart.
Table 2 lists the F-measure scores (the har-monic mean of precision and recall) of our Bayesianclassifier for document-level opinion/fact classifica-tion.
The results show the classifier achieved 97%F-measure, which is comparable or higher than the93% accuracy reported by (Wiebe et al, 2002),who evaluated their work based on a similar set ofWSJ articles.
The high classification performance7http://www.sci.sdsu.edu/CAL/wordlist/.Variant Class Standard A Standard BScore Fact   0.61,0.34    1.00,0.27 Opinion   0.30,0.49    0.16,0.64 FrequencyFact   0.82,0.32    0.89,0.19 Opinion   0.17,0.55    0.28,0.55 Table 3:   Recall, precision  of similarity classifier.is also consistent with a high inter-rater agreement(kappa=0.95) for document-level fact/opinion anno-tation (Wiebe et al, 2002).
Note that we trained andevaluated only on WSJ articles for which we can ob-tain article class metadata, so the classifier may per-form less accurately when used for other newswirearticles.Sentence Classification Table 3 shows the re-call and precision of the similarity-based approach,while Table 4 lists the recall and precision of naiveBayes (single and multiple classifiers) for sentence-level opinion/fact classification.
In both cases, theresults are better when we evaluate against Stan-dard B, containing the sentences for which two hu-mans assign the same label; obviously, it is easier forthe automatic system to produce the correct label inthese more clear-cut cases.Our Naive Bayes classifier has a higher recall andprecision (80?90%) for detecting opinions than forfacts (around 50%).
While words and n-grams hadlittle performance effect for the opinion class, theyincreased the recall for the fact class around five foldcompared to the approach by Wiebe et al (1999).In general, the additional features helped the classi-fier; the best performance is achieved when words,bigrams, trigrams, part-of-speech, and polarity areincluded in the feature set.
Further, using multipleclassifiers to automatically identify an appropriatesubset of the data for training slightly increases per-formance.Polarity Classification Using the method of Sec-tion 5.1, we automatically identified a total of 39,652(65,773), 3,128 (4,426), 144,238 (195,984), and22,279 (30,609) positive (negative) adjectives, ad-verbs, nouns, and verbs, respectively.
Extracted pos-itive words include inspirational, truly, luck, andachieve.
Negative ones include depraved, disas-trously, problem, and depress.
Figure 1 plots theFeatures Class Standard A Standard BSingle Multiple Single MultipleFeatures from (Wiebe et al, 1999) Fact   0.03,0.38    0.03,0.38    0.06,1.00    0.06,1.00 Opinion   0.97,0.69    0.97,0.69    1.00,0.80    1.00,0.80 Words only Fact   0.14,0.39    0.12,0.42    0.28,0.42    0.28,0.45 Opinion   0.90,0.69    0.92,0.69    0.90,0.82    0.91,0.83 Words and Bigrams Fact   0.15,0.39    0.12,0.43    0.16,0.25    0.16,0.25 Opinion   0.89,0.69    0.92,0.69    0.87,0.79    0.87,0.79 Words, Bigrams, and Trigrams Fact   0.18,0.44    0.13,0.41    0.26,0.50    0.26,0.50 Opinion   0.89,0.70    0.91,0.69    0.93,0.82    0.93,0.82 Words, Bigrams, Trigrams,and Part-of-SpeechFact   0.17,0.42    0.13,0.40    0.18,0.49    0.27,0.44 Opinion   0.89,0.70    0.91,0.69    0.92,0.70    0.85,0.84 Words, Bigrams, Trigrams,Part-of-Speech, and PolarityFact   0.15,0.43    0.13,0.42    0.44,0.50    0.44,0.53 Opinion   0.91,0.69    0.92,0.70    0.88,0.86    0.91,0.86 Table 4:   Recall, precision  of opinion/fact sentence classification using different features and either asingle or multiple (data cleaning) classifiers.Figure 1: Recall and precision (1,336 manually la-beled positive and negative adjectives as gold stan-dard) of extracted adjectives using 1, 20, and 100positive and negative adjective pairs as seeds.recall and precision of extracted adjectives by us-ing randomly selected seed sets of 1, 20, and 100pairs of positive and negative adjectives from the listof (Hatzivassiloglou and McKeown, 1997).
Both re-call and precision increase as the seed set becomeslarger.
We obtained similar results with the ANEWlist of adjectives (Section 7).
As an additional ex-periment, we tested the effect of ignoring sentenceswith negative particles, obtaining a small increase inprecision and recall.We subsequently used the automatically extractedpolarity score for each word to assign an aggregateParts-of-speech Used A BAdjectives 0.49 0.55Adverbs 0.37 0.46Nouns 0.54 0.52Verbs 0.54 0.52Adjectives and Adverbs 0.55 0.84Adjectives, Adverbs, and Verbs 0.68 0.90Adjectives, Adverbs, Nouns,and Verbs 0.62 0.74Table 5: Accuracy of sentence polarity tagging ongold standards A and B for different sets of parts-of-speech.polarity to opinion sentences.
Table 5 lists the accu-racy of our sentence-level tagging process.
We ex-perimented with different combinations of part-of-speech classes for calculating the aggregate polarityscores, and found that the combined evidence fromadjectives, adverbs, and verbs achieves the highestaccuracy (90% over a baseline of 48%).
As in thecase of sentence-level classification between opin-ion and fact, we also found the performance to behigher on Standard B, for which humans exhibitedconsistent agreement.9 ConclusionsWe presented several models for distinguishing be-tween opinions and facts, and between positive andnegative opinions.
At the document level, a fairlystraightforward Bayesian classifier using lexical in-formation can distinguish between mostly factualand mostly opinion documents with very high pre-cision and recall (F-measure of 97%).
The task ismuch harder at the sentence level.
For that case,we described three novel techniques for opinion/factclassification achieving up to 91% precision and re-call on the detection of opinion sentences.
We alsoexamined an automatic method for assigning polar-ity information to single words and sentences, accu-rately discriminating between positive, negative, andneutral opinions in 90% of the cases.Our work so far has focused on characterizingopinions and facts in a generic manner, without ex-amining who the opinion holder is or what the opin-ion is about.
While we have found presenting in-formation organized in separate opinion and factclasses useful, our goal is to introduce further analy-sis of each sentence so that opinion sentences can belinked to particular perspectives on a specific sub-ject.
We intend to cluster together sentences fromthe same perspective and present them in summaryform as answers to subjective questions.AcknowledgmentsWe wish to thank Eugene Agichtein, Sasha Blair-Goldensohn, Roy Byrd, John Chen, Noemie El-hadad, Kathy McKeown, Becky Passonneau, andthe anonymous reviewers for valuable input on ear-lier versions of this paper.
We are grateful to thegraduate students at Columbia University who par-ticipated in our evaluation of sentence-level opin-ions.
This work was supported by ARDA underAQUAINT project MDA908-02-C-0008.
Any opin-ions, findings, or recommendations are those ofthe authors and do not necessarily reflect ARDA?sviews.ReferencesM.
M. Bradley and P. J. Lang.
1999.
Affective normsfor English words (ANEW): Stimuli, instruction man-ual and affective ratings.
Technical Report C-1, TheCenter for Research in Psychophysiology, Universityof Florida, Gainesville, Florida.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A case studyin part of speech tagging.
Computational Linguistics.Rebecca Bruce and Janyce Wiebe.
1999.
Recognizingsubjectivity: A case study in manual tagging.
NaturalLanguage Engineering, 5(2).Eugene Charniak.
2000.
A maximum-entropy?inspiredparser.
In Proceedings of NAACL-2000.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proceedings of the 35th Annual Meetingof the ACL and the 8th Conference of the EuropeanChapter of the ACL, pages 174?181, Madrid, Spain,July.
Association for Computational Linguistics.Vasileios Hatzivassiloglou and Janyce Wiebe.
2000.
Ef-fects of adjective orientation and gradability on sen-tence subjectivity.
In Conference on ComputationalLinguistics (COLING-2000).Vasileios Hatzivassiloglou, Judith Klavans, Melissa Hol-combe, Regina Barzilay, Min-Yen Kan, and KathleenMcKeown.
2001.
SIMFINDER: A flexible clusteringtool for summarization.
In Proceedings of the Work-shop on Summarization in NAACL-01.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumps up?
Sentiment classification using ma-chine learning techniques.
In Proceedings of the 2002Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP-02), Philadelphia, Penn-sylvania.Peter Turney.
2002.
Thumps up or thumbs down?
Se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proceedings of the 40th AnnualMeeting of the Association for ComputationalLinguis-tics, Philadelphia, Pennsylvania.Janyce Wiebe, Rebecca Bruce, and Thomas O?Hara.1999.
Development and use of a gold standard dataset for subjectivity classifications.
In Proceedings ofthe 37th Annual Meeting of the Association for Com-putational Linguistics (ACL-99), pages 246?253.Janyce Wiebe, Theresa Wilson, Rebecca Bruce, MatthewBell, and M. Martin.
2002.
Learning subjectivelanguage.
Technical Report TR-02-100, Departmentof Computer Science, University of Pittsburgh, Pitts-burgh, Pennsylvania.Janyce Wiebe.
2000.
Learning subjective adjectivesfrom corpora.
In Proceedings of the 17th NationalConference on Artificial Intelligence (AAAI-2000),Austin, Texas.
