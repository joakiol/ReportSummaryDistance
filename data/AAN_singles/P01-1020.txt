A machine learning approachto the automatic evaluation of machine translationSimon Corston-Oliver, Michael Gamon and Chris BrockettMicrosoft ResearchOne Microsoft WayRedmond WA 98052, USA{simonco, mgamon, chrisbkt}@microsoft.comAbstractWe present a machine learningapproach to evaluating the well-formedness of output of a machinetranslation system, using classifiers thatlearn to distinguish human referencetranslations from machine translations.This approach can be used to evaluatean MT system, tracking improvementsover time; to aid in the kind of failureanalysis that can help guide systemdevelopment; and to select amongalternative output strings.
The methodpresented is fully automated andindependent of source language, targetlanguage and domain.1 IntroductionHuman evaluation of machine translation (MT)output is an expensive process, oftenprohibitively so when evaluations must beperformed quickly and frequently in order tomeasure progress.
This paper describes anapproach to automated evaluation designed tofacilitate the identification of areas forinvestigation and improvement.
It focuses onevaluating the wellformedness of output anddoes not address issues of evaluating contenttransfer.Researchers are now applying automatedevaluation in MT and natural languagegeneration tasks, both as system-internalgoodness metrics and for the assessment ofoutput.
Langkilde and Knight (1998), forexample, employ n-gram metrics to selectamong candidate outputs in natural languagegeneration, while Ringger et al (2001) use n-gram perplexity to compare the output of MTsystems.
Su et al (1992), Alshawi et al (1998)and Bangalore et al (2000) employ string editdistance between reference and output sentencesto gauge output quality for MT and generation.To be useful to researchers, however,assessment must provide linguistic informationthat can guide in identifying areas where work isrequired.
(See Nyberg et al, 1994 for usefuldiscussion of this issue.
)The better the MT system, the more itsoutput will resemble human-generated text.Indeed, MT might be considered a solvedproblem should it ever become impossible todistinguish automated output from humantranslation.
We have observed that in generalhumans can easily and reliably categorize asentence as either machine- or human-generated.Moreover, they can usually justify theirdecision.
This observation suggests thatevaluation of the wellformedness of outputsentences can be treated as a classificationproblem: given a sentence, how accurately canwe predict whether it has been translated bymachine?
In this paper we cast the problem ofMT evaluation as a machine learningclassification task that targets both linguisticfeatures and more abstract features such as n-gram perplexity.2 DataOur corpus consists of 350,000 aligned Spanish-English sentence pairs taken from publishedcomputer software manuals and online helpdocuments.
We extracted 200,000 Englishsentences for building language models toevaluate per-sentence perplexity.
From theremainder of the corpus, we extracted 100,000aligned sentence pairs.
The Spanish sentences inthis latter sample were then translated by theMicrosoft machine translation system, whichwas trained on documents from this domain(Richardson et al, 2001).
This yielded a set of200,000 English sentences, one half of whichwere English reference sentences, and the otherhalf of which were MT output.
(The Spanishsentences were not used in building orevaluating the classifiers).
We split the 200,000English sentences 90/10, to yield 180,000sentences for training classifiers and 20,000sentences that we used as held-out test data.Training and test data were evenly dividedbetween reference English sentences andSpanish-to-English translations.3 FeaturesThe selection of features used in ourclassification task was motivated by failureanalysis of system output.
We were particularlyinterested in those linguistic features that couldaid in qualitative analysis, as we discuss insection 5.
For each sentence we automaticallyextracted 46 features by performing a syntacticparse using the Microsoft NLPWin naturallanguage processing system (Heidorn, 2000) andlanguage modeling tools.
The features extractedfall into two broad categories:(i) Perplexity measures were extracted using theCMU-Cambridge Statistical Language ModelingToolkit (Clarkson and Rosenfeld, 1997).
Wecalculated two sets of values: lexicalized trigramperplexity, with values discretized into decilesand part of speech (POS) trigram perplexity.
Forthe latter we used the following sixteen POStags: adjective, adverb, auxiliary, punctuation,complementizer, coordinating conjunction,subordinating conjunction, determiner,interjection, noun, possessor, preposition,pronoun, quantifier, verb, and other.
(ii) Linguistic features fell into severalsubcategories: branching properties of the parse;function word density, constituent length, andother miscellaneous featuresWe employed a selection of features toprovide a detailed assessment of the branchingproperties of the parse tree.
The linguisticmotivation behind this was twofold.
First, it hadbecome apparent from failure analysis that MTsystem output tended to favor right-branchingstructures over noun compounding.
Second, wehypothesized that translation from languageswhose branching properties are radicallydifferent from English (e.g.
Japanese, or a verb-second language like German) might pollute theEnglish output with non-English characteristics.For this reason, assessment of branchingproperties is a good candidate for a language-pair independent measure.
The branchingfeatures we employed are given below.
Indicesare scalar counts; other measures are normalizedfor sentence length.?
number of right-branching nodes acrossall constituent types?
number of right-branching nodes forNPs only?
number of left-branching nodes acrossall constituent types?
number of left-branching nodes for NPsonly?
number of premodifiers across allconstituent types?
number of premodifiers within NPs only?
number of postmodifiers across allconstituent types?
number of postmodifiers within NPsonly?
branching index across all constituenttypes, i.e.
the number of right-branchingnodes minus number of left-branchingnodes?
branching index for NPs only?
branching weight index: number oftokens covered by right-branchingnodes minus number of tokens coveredby left-branching nodes across allcategories?
branching weight index for NPs only?
modification index, i.e.
the number ofpremodifiers minus the number ofpostmodifiers across all categories?
modification index for NPs only?
modification weight index: length intokens of all premodifiers minus lengthin tokens of all postmodifiers across allcategories?
modification weight index for NPs only?
coordination balance, i.e.
the maximallength difference in coordinatedconstituentsWe considered the density of function words,i.e.
the ratio of function words to content words,because of observed problems in WinMToutput.
Pronouns received special attentionbecause of frequent problems detected in failureanalysis.
The density features are:?
overall function word density?
density of determiners/quantifiers?
density of pronouns?
density of prepositions?
density of punctuation marks,specifically commas and semicolons?
density of auxiliary verbs?
density of conjunctions?
density of different pronoun types: Wh,1st, 2nd, and 3rd person pronounsWe also measured the following constituentsizes:?
maximal and average NP length?
maximal and average AJP length?
maximal and average PP length?
maximal and average AVP length?
sentence lengthOn a lexical level, the presence of out ofvocabulary (OOV) words is frequently causedby the direct transfer of source language wordsfor which no translation could be found.
Thetop-level syntactic template, i.e.
the labels of theimmediate children of the root node of asentence, was also used, as was subject-verbdisagreement.
The final five features are:?
number of OOV words?
the presence of a word containing a non-English letter, i.e.
an extended ASCIIcharacter.
This is a special case of theOOV problem.?
label of the root node of the sentence(declarative, imperative, question, NP,or "FITTED" for non-spanning parses)?
sentence template, i.e.
the labels of theimmediate children of the root node.?
subject-verb disagreement4 Decision TreesWe used a set of automated tools to constructdecision trees (Chickering et al, 1997) based onthe features extracted from the reference andMT sentences.
To avoid overfitting, wespecified that nodes in the decision tree shouldnot be split if they accounted for fewer than fiftycases.
In the discussion below we distinguish theperplexity features from the linguistic features.4.1 Decision trees built using alltraining dataTable 1 gives the accuracy of the decision trees,when trained on all 180,000 training sentencesand evaluated against the 20,000 held-out testsentences.
Since the training data and test datacontain an even split between reference humantranslations and machine translations, thebaseline for comparison is 50.00%.
As Table 1shows, the decision trees dramaticallyoutperform this baseline.
Using only perplexityfeatures or only linguistic features yieldsaccuracy substantially above this baseline.Combining the two sets of features yields thehighest accuracy, 82.89%.Features used Accuracy (%)All features 82.89Perplexity features only 74.73Linguistic features only 76.51Table 1 Accuracy of the decision treesNotably, most of the annotated featureswere selected by the decision tree tools.
Twofeatures were found not to be predictive.
Thefirst non-selected feature is the presence of aword containing an extended ASCII character,suggesting that general OOV features weresufficient and subsume the effect of thisnarrower feature.
Secondly, subject-verbdisagreement was also not predictive, validatingthe consistent enforcement of agreementconstraints in the natural language generationcomponent of the MT system.
In addition, onlyeight of approximately 5,200 observed sentencetemplates turned out to be discriminatory.For a different use of perplexity inclassification, see Ringger et al (2001) whocompare the perplexity of a sentence using alanguage model built solely from referencetranslations to the perplexity using a languagemodel built solely from machine translations.The output of such a classifier could be used asan input feature in building decision trees.Effect of training data size66676869707172737475767778798081828384010,00020,00030,00040,00050,00060,00070,00080,00090,000100,000110,000120,000130,000140,000150,000160,000170,000180,000Training casesAvgbestaccuracyAll features Perplexity only Linguistic onlyFigure 1 Accuracy with varying amounts of training data4.2 Varying the amount of training dataFor our experiments, we had access to severalhundred thousand sentences from the targetdomain.
To measure the effect of reducing thesize of the training data set on the accuracy ofthe classifier, we built classifiers using samplesof the training data and evaluating against thesame held-out sample of 20,000 sentences.
Werandomly extracted ten samples containing thefollowing numbers of sentences: {1,000, 2,000,3,000, 4,000, 5,000, 6,000, 12,000, 25,000,50,000, 100,000, 150,000}.
Figure 1 shows theeffect of varying the size of the training data.The data point graphed is the average accuracyover the ten samples at a given sample size, witherror bars showing the range from the leastaccurate decision tree at that sample size to themost accurate.As Figure 1 shows, the models built usingonly perplexity features do not benefit fromadditional training data.
The models built usinglinguistic features, however, benefitsubstantially, with accuracy leveling off after150,000 training cases.
With only 2,000 trainingcases, the classifiers built using all featuresrange in accuracy from 75.06% to 78.84%,substantially above the baseline accuracy of50%.5 DiscussionAs the results in section 4 show, it is possible tobuild classifiers that can distinguish humanreference translations from the output of amachine translation system with high accuracy.We thus have an automatic mechanism that canperform the task that humans appear to do withease, as noted in section 1.
The best result, aclassifier with 82.89% accuracy, is achieved bycombining perplexity calculations with a set offiner-grained linguistic features.
Even with asfew as 2,000 training cases, accuracy exceeded75%.
In the discussion below we consider theadvantages and possible uses of this automaticevaluation methodology.5.1 Advantages of the approachOnce an appropriate set of features has beenselected and tools to automatically extract thosefeatures are in place, classifiers can be built andevaluated quickly.
This overcomes the twoproblems associated with traditional manualevaluation of MT systems: manual evaluation isboth costly and time-consuming.
Indeed, anautomated approach is essential when dealingwith an MT system that is under constantdevelopment in a collaborative researchenvironment.
The output of such a system maychange from day to day, requiring frequentfeedback to monitor progress.The methodology does not crucially rely onany particular set of features.
As an MT systemmatures, more and more subtle cues might benecessary to distinguish between human andmachine translations.
Any linguistic feature thatcan be reliably extracted can be proposed as acandidate feature to the decision tree tools.The methodology is also not sensitive to thedomain of the training texts.
All that is neededto build classifiers for a new domain is asufficient quantity of aligned translations.5.2 Possible applications of theapproachThe classifiers can be used for evaluating asystem overall, providing feedback to aid insystem development, and in evaluatingindividual sentences.Evaluating an MT system overallEvaluating the accuracy of the classifier againstheld-out data is equivalent to evaluating thefluency of the MT system.
As the MT systemimproves, its output will become more like thehuman reference translations.
To measureimprovement over time, we would hold the setof features constant and build and evaluate newclassifiers using the human referencetranslations and the output of the MT system at agiven point in time.
Using the same set offeatures, we expect the accuracy of theclassifiers to go down over time as the MToutput becomes more like human translations.Feedback to aid system developmentOur primary interest in evaluating an MT systemis to identify areas that require improvement.This has been the motivation for using linguisticfeatures in addition to perplexity measures.From the point of view of system development,perplexity is a rather opaque measure.
This canbe viewed as both a strength and a weakness.
Onthe one hand, it is difficult to tune a system withthe express goal of causing perplexity toimprove, rendering perplexity a particularlygood objective measurement.
On the other hand,given a poor perplexity score, it is not clear howto improve a system without additional failureanalysis.We used the DNETVIEWER tool (Heckermanet al, 2000), a visualization tool for viewingdecision trees and Bayesian networks, to explorethe decision trees and identify problem areas inour MT system.
In one visualization, shown inFigure 2, DNETVIEWER allows the user to adjusta slider to see the order in which the featureswere selected during the heuristic search thatguides the construction of decision trees.
Themost discriminatory features are those whichcause the MT translations to look most awful, orare characteristics of the reference translationsthat ought to be emulated by the MT system.
Forthe coarse model shown in Figure 2, the distancebetween pronouns (nPronDist) is the highestpredictor, followed by the number of secondperson pronouns (n2ndPersPron), the number offunction words (nFunctionWords), and thedistance between prepositions (nPrepDist).Using DNETVIEWER we are able to explorethe decision tree, as shown in Figure 3.
Viewingthe leaf nodes in the decision tree, we see aprobability distribution over the possible statesof the target variable.
In the case of the binaryclassifier here, this is the probability that asentence will be a reference translation.
InFigure 3, the topmost leaf node shows thatp(Human translation) is low.
We modifiedDNETVIEWER so that double-clicking on the leafnode would display reference translations andMT sentences from the training data.
We displaya window showing the path through the decisiontree, the probability that the sentence is areference translation given that path, and thesentences from the training data identified by thefeatures on the path.
This visualization allowsthe researcher to view manageable groups ofsimilar problem sentences with a view toidentifying classes of problems within thegroups.
A goal for future research is to selectadditional linguistic features that will allow us topinpoint problem areas in the MT system andthereby further automate failure analysis.Figure 2 Using the slider to view the best predictorsFigure 3 Examining sentences at a leaf node in the decision treeFigure 4 Examining sentences at a leaf node in the decision treeDecision trees are merely one form ofclassifier that could be used for the automatedevaluation of an MT system.
In preliminaryexperiments, the accuracy of classifiers usingsupport vector machines (SVMs) (Vapnik, 1998;Platt et al, 2000) exceeded the accuracy of thedecision tree classifiers by a little less than onepercentage point using a linear kernel function,and by a slightly greater margin using apolynomial kernel function of degree three.
Weprefer the decision tree classifiers because theyallow a researcher to explore the classificationsystem and focus on problem areas andsentences.
We find this method for exploring thedata more intuitive than attempting to visualizethe location of sentences in the high-dimensional space of the corresponding SVM.Evaluating individual sentencesIn addition to system evaluation and failureanalysis, classifiers could be used on a per-sentence basis to guide the output of an MTsystem by selecting among multiple candidatestrings.
If no candidate is judged sufficientlysimilar to a human reference translation, thesentence could be flagged for human post-editing.6 ConclusionWe have presented a method for evaluating thefluency of MT, using classifiers based onlinguistic features to emulate the human abilityto distinguish MT from human translation.
Thetechniques we have described are system- andlanguage-independent.
Possible applications ofour approach include system evaluation, failureanalysis to guide system development, andselection among alternative possible outputs.We have focused on structural aspects of atext that can be used to evaluate fluency.
A fullevaluation of MT quality would of course needto include measurements of idiomaticity andtechniques to verify that the semantic andpragmatic content of the source language hadbeen successfully transferred to the targetlanguage.AcknowledgementsOur thanks go to Eric Ringger and MaxChickering for programming assistance with thetools used in building and evaluating thedecision trees, and to Mike Carlson for help insampling the initial datasets.
Thanks also toJohn Platt for helpful discussion on parametersetting for the SVM tools, and to the membersof the MSR NLP group for feedback on the usesof the methodology presented here.ReferencesAlshawi, H., S. Bangalore, and S. Douglas.
1998.Automatic acquisition of hierarchical transductionmodels for machine translation.
In Proceedings ofthe 36th Annual Meeting of the Association forComputational Linguistics, Montreal Canada, Vol.I: 41-47.Bangalore, S., O. Rambow, and S. Whittaker.
2000.Evaluation Metrics for Generation.
In Proceedingsof the International Conference on NaturalLanguage Generation (INLG 2000), MitzpeRamon, Israel.
1-13.Chickering, D. M., D. Heckerman, and C. Meek.1997.
A Bayesian approach to learning Bayesiannetworks with local structure.
In Geiger, D. and P.Punadlik Shenoy (Eds.
), Uncertainty in ArtificialIntelligence: Proceedings of the ThirteenthConference.
80-89.Clarkson, P. and R. Rosenfeld.
1997.
StatisticalLanguage Modeling Using the CMU-CambridgeToolkit.
Proceedings of Eurospeech97.
2707-2710.Heckerman, D., D. M. Chickering, C. Meek, R.Rounthwaite, and C. Kadie.
2000.
Dependencynetworks for inference, collaborative filtering anddata visualization.
Journal of Machine LearningResearch 1:49-75.Heidorn, G. E., 2000.
Intelligent writing assistance.In R. Dale, H. Moisl and H. Somers (Eds.
).Handbook of Natural Language Processing.
NewYork, NY.
Marcel Dekker.
181-207.Langkilde, I., and K. Knight.
1998.
Generation thatexploits corpus-based statistical knowledge.
InProceedings of the 36th Annual Meeting of theAssociation for Computational Linguistics, and17th International Conference on ComputationalLinguistics, Montreal, Canada.
704-710.Nyberg, E. H., T. Mitamura, and J. G. Carbonnell.1994.
Evaluation Metrics for Knowledge-BasedMachine Translation.
In Proceedings of the 15thInternational Conference on ComputationalLinguistics, Kyoto, Japan (Coling 94).
95-99.Platt, J., N. Cristianini, J. Shawe-Taylor.
2000.
Largemargin DAGs for multiclass classification.
InAdvances in Neural Information ProcessingSystems 12, MIT Press.
547-553.Richardson, S., B. Dolan, A. Menezes, and J.Pinkham.
2001.
Achieving commercial-qualitytranslation with example-based methods.Submitted for review.Ringger, E., M. Corston-Oliver, and R. Moore.
2001.Using Word-Perplexity for Automatic Evaluationof Machine Translation.
Manuscript.Su, K., M. Wu, and J. Chang.
1992.
A newquantitative quality measure for machinetranslation systems.
In Proceedings of COLING-92, Nantes, France.
433-439.Vapnik, V. 1998.
Statistical Learning Theory, Wiley-Interscience, New York.
