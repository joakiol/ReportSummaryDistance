THE IPS SYSTEMEric Wehrli*Laboratoire d'analyse t de technologic du langageUniversity of Geneva1211 Geneva 4wehrli@uni2a.unige.chAbst rac tThe IPS system is a large-scale interactiveGB-based parsing system (English, French) un-der development at the University of Geneva.This paper starts with an overview of the sys-tem, discussing some of its basic features aswell as its general architecture.
We then turnto a more detailed iscussion of the "right cor-ner" parsing strategy developed for this project.Combining top down and bottom up features,this strategy is consistent with an incrementalinterpretation f sentences.1 Overv iew o f  the  IPS  pro jec tThe IPS (Interactive Parsing System) researchproject, at the Linguistics Departement of theUniversity of Geneva, aims at developing a large,interactive, parsing model based on Chomsky'sGovernment and Binding (henceforth GB) lin-guistic theory t. This project, which focusesboth on English and French, has theoretical aswell as practical goals, including the following:?
to show the feasibility and the soundnessof a GB-based approach to natural languageparsing;*The research described in this paper has been sup-ported in part by n grant from the Swiss national sci-ence foundation (grant no 11-25362.88).
I am grateful toRobin Clark, Paola Merlo~ Mira Ramluckun and MartinKay for helpful comments and discussion on earlier draftsof this paper.C\], Chonmky (1981, 1988) for a discussion of 6Btheory.
Issues related to the use of Gl\] theory in naturallanguage parsing delign are discussed inBerwick (1887),Berwick et ol.
(1991) mad Wehdi (1988),?
to demonstrate he advantages of an inter-active approach to natural language pro-cessing;a to develop robust, large-scale parsers uit-able for NLP applications (e.g.
transla-tion).The IPS parser is interactive in the sensethat it can request on-line information fromthe user.
Typically, interaction will be used tosolve ambiguities that the parser cannot han-die, for instance when the resolution of an am-biguity depends on contextual or extra-linguisticknowledge 2.
The interactive f ature is seen as away to increase the reliability of the parser - -difllcult decisions are left to the user--  as well asa way to simplify the grammar, since many adhoc features that would be necessary if the pars-er had to solve all problems by itself can now bedispensed with.
In addition, the interactive ca-pabillty is also useful as a development tool, inthe sense that on-line user interaction can sup-plement modules which have not yet been de-veloped (e.g.
semantic and pragmatic compo-nents).Other important features of the IPS parserinclude:?
Modular architecture, i.e.
the parsingmechanism is decomposed into modules,which roughly correspond to some of thecomponents ofa standard GB grammar (e.g.X, chains, O, etc.).?
The parsing strategy is left-to-right, datadriven, with parallel treatment of alterna-2For arguments in favor of interactive systems (usu-ally in the context of machine translation, though) seein particular Kay (11180), Melby et al (1980), Tomita(1984), Wehrli (1990), Zajac (1988).Acres DE COLING-92, NANTES, 23-28 AOt3"r 1992 8 7 0 PROC.
OF COL1NG-92, NANTES, AUO.
23-28, 1992fives.
The nonodeterminism of the parser isrestricted by a selection mechaafism.a Use of structure-sharing techniques to cutdown the number of explicit representationsof alternatives.2 Arch i tec tureThe IPS parser tries to associate with an inputsentence a set of syntactic structures.
Thesestructures correspond to GB S-structures, i.e.surface structures enriched with traces of movedelements and other empty categories.
In ourimplementation, GB grammatical modules cor-respond to particular processes.
While some ofthe modules function as generators (2, chainmodules, coordination module) in the sense thatthey increase the set of structures hypothesizedat a given point, others are used as filters (Casemodule, 0-module) in the sense that their actiontends to reduce the set of structures.
The mod-ules apply as soon as possible at each step in theparsing process, triggered by particular data orby specific calls from other modules.Alternatives are considered concurrently(pseudo-parallelism), and a small number ofheuristics are used to restrict he size of the hy-pothesis et.
To give an example, one heuristicgives precedence to attachments satisfying for-real selectional features over (cf.
(3), (7)) otherkinds of attachments.
Thus, if an incoming verbform can be attached either as a complement toan auxiliary or as a main verb, preference willbe given to the former a.User interaction -wtfich is an optional featurein the IPS system- can be used to select alterna-tives, mostly in case of attachment ambiguities,but occasionally also for other types of ambigui-ty (lexical, thematic, ere).
Alternatives are thendisplayed (in an abbreviated manner) and theuser is asked to make a selection.aNotice that this heuristic might explain garden pathsentences such as "l'invitd quail a dit des folies" (the gueJthe has told inzanitiez),in which readers wrongly interpretdit as past participle selected by the auxiliary verb a.3 The X modu leThe central module of the IPS system is themodule, which acts as the main generator of thesystem, and determines the syntactic structuresof constituents.
We assume the X schema in (1):(1) XP --~ SpecX--+ X Complwhere X is a lexical or a functional category,Spee and Compl are lists (possibly empty)of maximal projections (YP).As indicated in (1), maximal projections areof order 2 (XP) and lexicai categories of order0 (I).
For typographical reasons, categories oforder 1 (~) are noted ' in the illustrations be-low.
The set of lexical categories include N, V,?
and p, the set of functional categories includesD(eterminer), T(ense) et C(omplementizer).
Wealso assume the DP hypothesis (cf.
Abney 1987,Clark 1990a), and, as a consequence, the strongparallelism between DP and TP structures, as il-lustrated in (2):(2)TP DPDP ~ DP -DT VP D NPLexical categories as well as functional cate+gories can select other lexical or funrtional pro-jectioos.
Tiros, u determiner cyst select a pro-jection of category Vp or NP, as in (3) and (4),respectively, corresponding to the structures (5)and (6)(3) \[each, D, \[+definite\], \[__\[D,\[ .
.
.
.
rail\] ...... \](4) \[each, D, \[+definite\], \[__\[N,\[singular\]\] .....(5)a, each five men.b.
ivy \[D' each \[DP \[I)' five\[MP iN' menJJJl\]\](6)a. each student.AcrEs DE COLING-92, NANTES, 23-28 AOlJq' 1992 8 7 1 PROC.
OF COLING-92, NAbrrES.
AUG. 23-28, 1992b.
\[ DP \[ D' each \[ NP I" N' student\]\]\]\]Similarly, auxiliaries can select projections oftype UP, and most prepositions projections oftype DP.
Some examples of selection features as-sociated with auxiliary verbs are given in (7),with the corresponding structures in (8) and (9):(7)a.b.e.(a)a.b.
{have, V, \[+aux\], {__IV,{+ pastpar ticiplel\] .
.
.
.
\]\[be, V, \[+aux\], [__\[V,\[past participle\]\] "a~\]\[be, V, \[+aux\], [__IV,\[present participle\]\] .
.
.
.
\]the men have arrived.\[ \[ \[ the \[ \[ TP DP D' NPhave \[ VP \[ V' arrived\]\]\]\]men\] \ ] \ ] \ ]  \[N' T'(9)a. the men must have been being cheated.b.
\[ TP \[ l)P \[ v' the \[ ~ \[ s '  men\ ] \ ] I l l  \[T'must \[Vp Iv '  have \[Vp Iv '  been \[Vp Iv 'being \[ VP \[ V' cheated \[ DP e \ ] i \ ] \ ] \ ] \ ] \ ] \ ] \ ] \ ] \ ] \ ]The following is a summary  of the fundamen-tal properties of constituent structures :?
Any  lexical category X projects to a maxi-mal projection Ip.a Attachments are restricted to maxlmal pro-jections (XP).?
All constituents have the same architecture.4 The IPS parsing strategyIn our implementat ion f the X module,  we dis-t inguish three types of action: projection, at-tachment o the left (specifiers) and at tachmentto the riglit.
The parsing strategy is left to right,parallel,  combilting a bot tom up approach withtop down filtering, as we shall see below.The mot ivat ion for this part icular  strategyis to maximize at each step the interpretat ionof const i tuents in order to faci l itate the selec-t ion mechanism as well as user interact ion dis-cussed in section 2.
Interest ingly enough, thisrequirement seems to coincide with psycholin-guistic evidence suggesting that sentence pars-ing proceeds in an incremental  fashion, tryingto integrate incoming items into maximal ly  in-terpreted structures ~.The basic idea is that  the parser must besensitive to incoming words.
However, str ict lybot tom up strategies are known to have someundesirable features.
In part icular,  they tendto generate numerous locally well-formed struc-tures which turn out to be incompat ib le with theoverall structure.
Furthermore,  they restrict at-tachment to complete constituents,  which meansthat when applied to r ight branching languagessuch as French or English, assembling the finalstructure does not start much before the end ofthe sentence is reached.To illustrate these problems, consider the fol-lowing examples :(10)a.
Who could the children have invited ?b.
John must have given the students severalof his books.In sentence (10a), when the parser gets to theword have, it tries to combine it with the leftcontext, say \[ DP the children\], leading to thenew constituent \[ the children have\].
Al- TIPthough this new constituent is perfectly well-formed locally, it is not compatible with themodal  could.Sentence (10b) i l lustrates the second andmore serious problem.
If node at tachment  isl imited to complete nodes, the combinat ion ofthe subject John and the rest of the structure(the whole verb phrase, which is a ?
in our sys-tem) wil l  not occur before the last word of thesentence is read.The use of a more sophisticated strategy, suchas the left corner strategy, addresses the firstproblem quite successfully ~.
However, it fails tosolve the second problem, since at tachments  arel imited to complete const i tuents in the standardleft corner parser (~.
In an a t tempt  to overcomethis problem, and taking advantage of the clear4 For a detailed iscussion attd review of the psycholin-guistic evidence for incremental parsing see Tanenhaus etal.
(1985), Frazier (1987) and Crocker (1992).SSee Aho and Unman (1972), Pereira and Shieber(1997) for a discussion of left corner parshig.6Gibson (1991) proposes to relax this latter require-ment~ and to allow attachments of incomplete con-stituents.
However, the generality of his proposal stillremains unclear.ACTES DE COLING-92, NANTES, 23-28 AoL~r 1992 8 7 2 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992bias for right branching structures in languagessuch as French or English (cf.
structure (gb)),we developed a strategy dubbed "right corner",which is based on the following principles:(11) "R ight  corner"  s t rategy :constituents can be attached as soon as theyhave been hypothesized;attach inconfing items to maximally ex-panded constituents in the left context;constituents specify a list of active attach-ment sites on their right edge;all attachments are considered in parallel;Notice that tiffs strategy is clearly bottom-up (actions are triggered by incoming material).However, it differs from other bottom-up strategies in some important ways.
First of all, thecombination of a new item with its left contextis not done through a sequence of "reduce" op-erations (as in a shift-reduce parser).
More gen-erally, the attachment of (most) incoming itemsis made directly to some subconstituent of thetop node, i.e.
to some attachment site speci-fied in the active node list of a constituent inthe left context.
This is in sharp contrast withstandard bottom-up arsers (including the leftcorner parser), for which reduction to the startsymbol cannot occur before the end of the sen-tence is reached.Although the right corner strategy requiressignificantly more complex data structures (con-stituents must specify all their potential attach-ment sitesT"), it has the advantage of being com-pntationally and psycholingulstically more ade-quate than other bottom up strategies.
Regard-ing the latter point, the right corner strategyseems consistent with the (trivial) observations(i) that the analyzer is data driven but (it) al-though still incomplete, left context constituentsare maximally specified, as they would in a top-down parser.A detailed example will illustrate tiffs strate-gy.
Consider the following sentence :rAttachment sites for a given constituent correspondto the list of X nodea on its right edge.
For efficiencyreasons~ they are put together in a stack associated withthe constituent.
(12) Jotm has bought some flowers.When the parser reads tim word some, tileleft context include, among many constituents,structure (13) :(13) \[ ~l' Johit has \[ Vl' boughtJJTile word some triggers a DP projection, withsome as its head.
Considering the left context,the parser aids structure (13), the stack of at-tachment sites of which contains the verb bought.The newly created DP projection combine withthe TP structure as a complement of the verbbought.
We now have an updated TP constituen-t, as in (14), with vm updated stack of activenodes, with at the top the DP constituent justattached.
(14) I: "rP John has bought some\]The parser cent now read the following wordflowers, which can attach to the left contextstructure (14), as a complement of the deter-miner some.The right corner strategy takes care of attach-ments to the right.
In the case of projections orof attachments to the left (specifiers), the usualbottom up procedure applies.
Typically projec-tion is triggered by inherent features (\[+tense\]verbs will trigger a T(ense) projection, propernouns a DP projection, etc.).
As for left attach-ment, it occurs when the current constituent canfind a left context constituent which can func-tion as a possible specifier.
The attachment ofthe specifier to the current constituent deter-mines a new constituent which may in turn finda specifier in its left context (iterative attach-meat) as it happens in the possessive construc-tion (e.g.
John's little brother's cat).5 Concluding remarksTile right coruer parsing strategy discussed inthis paper has been developed to satisfy tileparticular needs of our on-line interactive pars-ing model.
By (i) pursuing concurrently all thepossible analyses and (it) trying to integrate in-coming items into fully developed constituents,ACIDS DE COLING-92, Nar, rrEs, 23-28 Ao~-r 1992 8 7 3 PR()c. OF COLING-92, NANTES, AUG. 23-28, 1992this scheme, at each step in the parsing pro-cess, provides the filtering components, includ-ing user-interaction, with struct/tres that are asmuch interpreted as possible.
Not only does thismake the selection process much more reliable, itis also consistent with psycholinguistic evidencefor incremental sentence parsing.Although still under development, the IPSparser, which uses a lexical database exceed-ing 80,000 entries, has a fairly broad gram-maticai coverage including simple and complexsentences, complex determiners and possessives,yes/no and wh-interrogatives, relatives, passive,some comparatives as well as some cases of coor-dination (of same category).
The bYench versionalso handles typical Romance constructions suchas clitics and causatives.ReferencesAbney, S., 1987.
The English Noun Phrase in itsSentential Aspect, unpublished MIT Ph.D.thesis.Aho, A. et ,I.D.
UUman, 1972.
The Theo-ry of Parsing, Translation and Compiling,Prentice-Hail, Englewood Cliffs, NJ.Berwick, R., 1987.
"Prlnciple-based parsing",technical report, MIT AI-lab.Berwiek, It., S. Abney and C. Tenny (eds.)
1991.Principle-Based Parsing: Computation andPsychollnguistics, Kinwer Academic Pub-lishers, Dordrecht.Chomsky, N., 1981.
Lectures on Governmentand Binding, Foris Publications, Dordrecht.Chomsky, N., 1986.
Knowledge of Language: ItsOrigin, Nature and Use, Praeger Publisher-s, New York.Clark, It., 1990a.
"(Some) Determiners and Par-titives", ms., Uuiversity of Geneva.Clark, It., 1990b.
"The Auxiliary System of En-glish", ms., University of Geneva.Crocker, M., 1992.
A Logical Model of Compe-tence and Performance in the Human Sen-tence Processor, PhD dissertation, Univer-sity of Edinburgh.Frazier, L. 1987.
"Syntactic Processing: Evi-dence from Dutch," Natural Language andLinguistic Theory 5, pp.
515-559.Gibson, E., 1991.
A Computational Theoryof Human Linguistic Processing: Memo-ry Limitations and Processing BreakdownPhD dissertation, Carnegie Mellon Univer-sity.Kay, M., 1980.
"The Proper Place of Men andMachines in Language Translation", CSL-80-11, Xerox Paio Alto Research Center.Melby~ A.~ M. Smith and J. Peterson(1980).
"ITS: Interactive translation sys-tem" in Proceedings of the 8th Internation-al Conference on Computational Linguistics(COLING-80).Pereira, F. and S. Shieber (1987).
Prolog andNatural Language Analysis, CSLI LecturesNotes 10, Chicago Uuiversity Press.Tanenhaus, M., G. Carlson and M. Seidenberg(1985).
"Do Listeners compute LinguisticRepresentations?"
in D. Dowty, L. Kart-tunen and A. Zwicky (eds.
), Natural Lan-guage Processing: PsychologicaJ, Computa-tional and Theoretical Perspectives, Cam-bridge University Press.Tomita, M.
(1984).
"Disambiguating grammat-ically ambiguous sentences by asking,"Proceedings of the lOth InternationalConference on Computational Linguistics(COLING-84).Wehrll, E., 1988.
"Parsing with a GB gram-mar," in U. Reyle and C. Itohrer (eds.
),Natural Language Parsing and LinguisticTheories, Reidel, Dordrecht.Wehrll, E., 1990.
"STS: An Experimental Sen-tence Translation System," Proceedings ofthe 13th Internation Conference on Com-putational Linguistics (COLING-90).Zajac, It.
(1988).
"Interactive translation: anew approach~" Proceedings of the 12th In-ternational Conference on ComputationalLinguistics (COLING-88).Acres DE COLING-92.
NAMES, 23-28 AOUT I992 8 7 4 PROC.
OF COLING-92.
NANTES.
AUO.
23-28.
1992
