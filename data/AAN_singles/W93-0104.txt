Internal and External Evidence in the Identification andSemantic Categorization of Proper NamesDavid D. McDonaldmcdonald@cs.brandeis.eduAbstractWe describe the proper name recognition and classification facility ("PNV') of the SPARSERnatural language understanding system.
PNF has been used very successfully in the analysis ofunrestricted texts in several sublanguages taken from online news sources.
It makes its categoriza-tions on the basis of 'external' evidence from the context of the phrases adjacent to the name as wellas 'internal' evidence within the sequence ofwords and characters.
A semantic model of each nameand its components is maintained and used for subsequent reference.We describe PNF's operations of delimiting, classifying, and semantically recording thestructure of a name; we situate PNF with respect to the related parsing mechanisms within Sparser;and finally we work through an extended example that is typical of the sorts of text we have appliedPNF to.1 Introduction: Internal versus External EvidenceIt has long been appreciated by people working with proper names in unrestricted written textsthat any adequate treatment requires the use of a grammar.
We know at this point that the correctidentification and semantic ategorization f names requires an analysis based on the patterningof orthographic and lexical classes of elements that is analogous to what one finds in the othercontent-rich, syntactic structure-poor phrase types in the 'periphery' of the language such asnumbers, dates, citations, etc.
The point of this paper will be to argue that this grammar must becontext sensitive, and that it should incorporate a rich semantic model of names and theirrelationships to individuals.The context-sensitivity requirement derives from the fact that the classification of a propername involves two complementary kinds of evidence, which we will term 'internal' and'external'.
Internal evidence is derived from within the sequence of words that comprise thename.
This can be definitive criteria, such as the presence of known 'incorporation terms'("Ltd.", "G.m.b.H.")
that indicate companies; or heuristic criteria such as abbreviations orknown first names often indicating people.
Name-internal evidence is the only criteria consid-ered in virtually all of the name recognition systems that are reported as part of state of the artinformation extraction systems (see e.g.
Rau 1991, Alshawi 1992, DARPA 1992), most of32which depend on large (~20,000 word) gazetteers and lists of known names for their relativelyhigh performance.By contrast, external evidence is the classificatory criteria provided by the context in whicha name appears.
The basis for this evidence is the obvious observation that names are just waysto refer to individuals of specific types (people, churches, rock groups, etc.
), and that thesetypes have characteristic properties and participate in charact~'isfic events.
The presence of theseproperties or events in the immediate context of a proper name can be used to provide confirm-ing or cfiterial evidence for a name's category.
External evidence is analyzed in PNF in terms ofsubstitution contexts and operationalized in terms of context-sensitive rewrite rules.External evidence is a necessity for high accuracy performance.
One obvious reason is thatpredefined word lists can never be complete.
Another is that in many instances, especially thoseinvolving subsequent references, external evidence will override internal evidence.
In the finalconsideration it is always the way a phrase is used--the attributions and predications it is partof--that make it a proper name of a given sort; without he consideration of external evidencethis definitive criteria is missed, resulting in mistakes and confusion in the state of the parser.
(Relying solely on name lists has led to some funny errors, for example mistaking the foodcompany Sara Lee for a person.
Even some external evidence such as a title can be inadequate,if considered apart from the wider context of use, as in General Mills--both actual mistakesmade by an otherwise quite reasonable program some years ago (Masand & Duffey 1985).
)An additional reason, and one with considerable engineering utility from the point of viewof the grammar writer, is that the inclusion of external evidence into the mix of available analy-sis tools reduces the demands on the judgements one requires of internal evidence.
It canprovide a weaker (less specific) categorization about which it can be more certain, which canthen be refined as external evidence becomes available.
Lacking definitive internal evidence onecan initially label a segment simply as a 'name', and then later strengthen the judgement when,e.g., the segment is found to be adjacent to an age phrase or a title and context-sensitive rewriterules are triggered to re-label it as a person and to initiate the appropriate semantic processes.This kind of staged analysis is a requirement when the conclusions from internal evidenceare ambiguous.
It is not uncommon, for example, to have the names of a person and a companyin the same news article both share a word, i.e.
when the company is named after its founder.
Asubsequent reference using just that word cannot be definitively categorized on internal evidencealone, and must wait for the application of external evidence from the context.
(In the event hatthe context is inadequate, as when it involves a predication ot in the grammar, such 'name'segments can be left to default judgements by statistical heuristics operating after a first pass bythe parser, and the stronger categorizations then tested for coherency as the parse is resumed.
Inthe case of a company and a person with the same name, a well edited publication is unlikely touse the ambiguous word to refer to the founder without prefixing it with "Mr." or "Ms." asneeds be, so a word with both person and company denotations but without external evidencecan be assumed with some assurance to be referring to the company.
)In this paper we will describe in some detail the proper name facility of the SPARSERnatural language understanding system: "PNF", with particular attention to how it uses externalevidence and deploys its semantic model of names and their referents to handle ambiguities suchas the one noted just above.In a blind test of an earlier implementation f PNF in the context of "Who's News" articlesfrom the the Wall Street Journal, it performed at nearing 100% in the scored sentences in the33sublanguage for which a full grammar had been prepared.
We are currently testing a newimplementation a more diverse set of texts.Space will not permit a comparison of this algorithm with other approaches to propernames beyond occasional remarks and references.
As far as we know this is the only treatmentof proper names that makes essential use of context-sensitive rewrite rules, however the FUNESsystem of Sam Coates-Stephens (1992) is very similar to this work in making essential use ofexternal evidence, and Coates-Stephens's extensive research into proper names is an importantcontribution to the field; we have adopted some of his terminology as noted below.2 An overview of the procedure: Delimit, Classify, RecordThe goal of the proper name facility in Sparser (PNF) is to form and interpret full phrasalconstituents--noun phrases--.-that fit into the rest of a text's parse and contribute to the analysisof the entire text just like any other kind of constituent.
That is, PNF is operating as a compon-ent in a larger natural language comprehension system, and not as a standalone facility intendedfor name spotting, indexing, or other tasks based on skimming.
This integration is essential tothe way the PNF makes its decisions; it would not operate with anything like the same level ofperformance if it were independent, since there would then be no source of external evidence.To form full constituents for use in a language comprehension system we must (1) delimitthe sequence of words that make up each name, i.e.
identify its boundaries; (2) classify or cate-gorize the resulting constituent based on the kind of individual it names; and (3) record thename and the individual it denotes in the discourse model as our interpretation of the constit-uent's meaning.For other parts of Sparser's grammar, these three actions are done with one integratedmechanism uch as they would be in any other system.
Constituents are initiated bottom up bythe terminal rules of Sparser's lexicalized grammar, and then compositions of adjacent constit-uents are checked for and nonterrninal nodes introduced in accordance with Sparser's moderate-ly complex control structure that permits adeterministic parse and monotonic semantic interpre-tation.
The rules these operations deploy (essentially standard productions, though their modeof operation is more like that of a categorial grammar) both delimit and classify (label) constit-uents in one action: the categories are given by the producfion's lefthand sides, and the newconstituents' boundaries by the sequence of (typically binary) daughter constituents on therules' fighthand sides, with the new constituent's denotation given by an interpretation functionincluded irectly with the rule and applied as the rule completes.This normal mode of operations has not proved workable for proper names, and the reasonhas to do with the central problem with names from the point of view of a grarnrnar, namely thatin unrestricted texts the set of words that names can be comprised of cannot be completelyknown in advance.
The set is unbounded, growing at an apparently constant rate with the sizeof one's corpus, while the growth of other classes of content words tapers off asymptotic.ally(Liberman 1989).
This means that we cannot have a lexicalized grammar for proper names incethe bulk of the names we will encounter will be based on words that are undefined at the timethe grammar is written.Complicating the picture is the fact that virtually any normal word can do double duty aspart of a name (" ...
Her name was equally preposterous.
April Wednesday, she called herself,34and herpress card bore this out.
"MacLean 1976 pg.68).
This means that one either introducesa massive and arbitrary ambiguity into one's normal vocabulary, allowing any word to be partof a name, or one looks for another means of parsing proper names, which is the course thatwas taken with SPARSER'S PNF, where we separate out the three actions into distinct andlargely independent operations.
In the remainder of this section we will sketch the proceduresfor delimiting, classifying, and recording proper names; then in the following section we willgo into detail with an example once the parsing machinery that the procedures draw on has beenintroduced.Looking first at the kind of formal mechanisms used, the delimit operation is based on asimple state machine, rather than the application of context free rewrite rules as done in the restof the grammar.
This reflects that fact that the internal constituent s ructure of a proper name isfar more flat than hierarchical, and consequently should be treated as a Kleene Star structure in aregular grammar (<name-word>+).
The binary branching tree that one would get with context-free rules would be an artifact of the rule application machinery rather than reflect he grammarof names.In essence, the delimitation algorithm simply treats as a group any contiguous sequence ofcapitalized words (including 'sequences' of length one).
This is virtually always the correctthing to do as the example below illustrates, though the exceptions have to be treated carefullyas discussed later.
"The Del Fuegos, 0 Positive, and We Saw the Wolf wiU perform acoustic sets inAmnesty International USA Group 133"s Seventh Annual Benefit Concert at 8p.m.
on Friday, March 19, at the First Parish Unitarian Universalist Church inArlington Center."
(Arlington Advocate, 3/18/93)A sequence is terminated atthe first non-capitalized word 1 or comma; other punctuation ishandled case by case, e.g.
"&" is taken to extend sequences and periods are terminators unlessthey are part of an abbreviation.Classifying a proper name is a two-step rocess.
First, Sparser's regular parsing routinesare applied within the delimited word sequence.
This introduces any information the grammarhas about words or phrases it knows.
This information supplies the basis for the bulk of thestructure within a proper name, and provides the name-internal evidence on which the classifi-cation will be based.
For Sparser it includes:?
embedded references to cities or countries, e.g.
"Cambridge Savings Bank"?
open class 'keywords' like "Church" or "Bank" (following Coates-Stephens term-inology), and the incorporation-terms used by companies of various countries whengiving their full legal names ("Inc." in the U.S.A., "'P.T."
in Indonesia,"G.m.b.H."
in Germany, etc.
).It is reasonable todepend upon the existence of mixed-case text, since the number of online sources thatsupply uppercase only is rapidly diminishing and will probably disappear once all of the Model-33 Teletypesand other 6-bit data entry terminals in the world are finally junked.
In any event, o handle all-uppercase textswithin Sparser's design it is only the delimitation algorithm that must be changed, and a good approximationof the needed segmentation s i dependendy available from the distribution offunction words and punctuationin any text.
In the example above these are the commas, the appostropbe-s, "in", "at", and "on"; a mistakewould be made in "We Saw the Wolf' \[sic\] which will be problematic without external context in any event.35?
the relatively closed class of stylized modifiers used with people like "Jr.", "Sr.","Mr.", "Dr.".?
items used for heuristic lassification judgements (the items above are definitive)?
such as abbreviations (astrong indicator that the name refers to a person or a com-pany based on a person's name), or punctuation like "&" or ambiguous modifierslike "II" (which invariably means 'the second', but may be used with LimitedPartnerships as well as people).The parsing stage will reveal when the capitalized word -based elimitation has exceeded itsproper scope.
One such case is of course when a proper name appears just after the capitalizedword at the start of a sentence: "An Abitibi spokesman said...".
This is handled by defining allthe closed-class grammatical functional words as such so that they will be seen during thisembedded parse, and resegmenting the word sequence to exclude them.Another, more interesting case is where we have a sequence of modifiers prefixed to aproper name that are themselves proper names, e.g.
"Giant Group said ... seeking to block agroup led by Giant Chairman Burt Sugarman from acquiring ...".
In this situation there is nohope for correctly separating the names unless the grammar includes rules for such companiesand rifles, in which case they will appear to the classifier as successive dges with the appro-priate labels so that it can know to appreciate hem for what they are and to leave them out.
(It isperhaps a matter of judgement to hold that a person's title is not a part of their name, but thatpolicy appears to be the most consistent overall since it permits the capitalized premodifier ver-sion of a title of employment (e.g.
"Chairman") and its predicative lowercase version (as in anappositive) to be understood as the same kind of relationship semanticallyma different one thanthe relationship between aperson and their conventional title such as "Mr.".)
It is important toappreciate that all of these considerations only make sense when one is analyzing proper namesin the context of a larger system that already has grammars and semantic models for titles andemployment s atus and such; and they are hard to justify in an application that is simply namespotting.In practice, the operations of delimiting and classifying are often interleaved, since theclassification of an initially delimited segment can aid in the determination of whether thesegment needs to be extended, as when distinguishing between a list of names and a compoundname incorporating commas, e.g.
"... a string of companies - including Bosch, Continental ndVarta -have announced co-operative agreements ..." (The Financial Times, 5/16/90); v.s.
"HEALTH-CARE FIRM FOLDS: Wood, Lucksinger & Epstein is dissolving its practice."
(WallStreet Journal 2/26/91).
We will describe this process in the extended example at the end of thepaper.Once the words of the sequence have been parsed and edges introduced into the chartreflecting the grammar's analysis, the second part of the classification process is initiated as astate machine is passed over that region of the chart to arrive at the most certain classificationpossible given just this name-internal evidence.
If no specific conclusion can be reached, thenthe sequence will be covered with an edge that is simply given the category 'name', and it willbe up to external evidence to improve on that judgement as will be described later.
If a conclu-sion is made as to the kind of entity being named then the edge will be labeled with the appro-priate semantic ategory such as 'person', 'company', newspaper', etc.The recording process now takes over to provide a denotation for the edge in the discoursemodel.
Before this denotation is established, the representation f the name is just a label and a36designated sequence of words and edges internal to the name (e.g., edges over an embeddedreference to a city or region).
What we are providing now is a structured representation f thename qua name--a unique instance of one of the defined classes of names that reifies thatspecific pattern of words and embedded references.Including names as actual entities in the semantic model, rather than just treating them asephemeral pointers to the individuals they name and only using them momentarily during theinterpretation process, provides us with an elegant treatment of the ambiguity that is intrinsic tonames as representational entities.
Real names, unlike the hypothetical 'rigid designators' enter-tained by philosophers, may refer to any number of individuals according to the contingent factsof the actual world.
We capture this by making the denotation of the lexico-syntactic name--theedge in the chartmbe a semantic individual of type 'name' rather than (the representation f)aconcrete individual.
The name object in turn may be then associated in the discourse model withany number of particular individuals of various types: people, companies, places, etc.
accordingto the facts in the world.
Thus the ambiguity of names is taken not to be a linguistic fact but apragmatic fact involving different individuals having the same name.The structure that the semantic model imposes on names is designed to facilitate under-standing subsequent references tothe individuals that the names name.
The type of name struc-ture used predicts the kinds of reduced forms of the name that one can expect o be used.
Thisdesign criteria was adopted because, again, the overarching purpose of PTF is to contribute tothe thorough understanding of extended unrestricted texts, and this means that it is not enoughjust to notice that a given name has occurred somewhere in an article (which is easy to do byjust attending to the cases where the full company name is given with the 'incorporation term'that well edited newspapers will always provide when a company is introduced into a text, e.g.
"Sumitomo Electric Industries, Ltd.").
Instead, PTF must be able to recognize that the sameindividual is being talked about later when it sees, e.g., "Sumitomo Electric" (or "thecompany"), as well as to distinguish it from subsequent references to other companies thatshare part of the name: "Sumitomo Wiring Systems"; or to correctly deduce a subsidiary rela-tionship "'Sumitomo Electric International (Singapore)".
Similarly, people and companies orlocations that share name elements should be appreciated assuch: "the Suzuki Motors Company... Osamu Suzuki, the president of the company".To facilitate subsequent reference, not only does each proper name receive adenotation asan entirety, but the words that comprise it are also given denotations which are related, seman-tically, to the roles the words have each played in that name and in the names of other particularindividuals.
Thus the word "Suzuki", for example, is taken to always denote the same semanticobject, prosaically printed as #<name-word "suzuki">.
In turn this individual is related to (atleast) two other individualsmto he car company by way of the relation 'first-word-in-name',and to its president by the relation 'family-name'.373 The setting for the processIn order to supply the external evidence needed to accurately categorize proper names andunderstand them semantically, a language understanding system must include grammars (andtheir attendant semantic models) for properties and event-types that are characteristically associ-ated with them, and they should have as broad a coverage as possible.SPARSER has been applied to understanding ews articles about people changing their jobs(particularly the Wall Street Journal's "Who's News" column), and with a lesser competence toarticles on corporate joint ventures and quarterly earnings.
As a result, it has quite strongsemantic grammars for some of the very most frequent properties of companies and people inbusiness news texts: the parent-subsidiary relationship between companies, age, titles, and fora few of the more common event-types (via its primary grammars).A complementary consideration is such relatively mundane things as what approach will betaken to punctuation, capitalization, or abbreviations.
For SPARSER, since it is designed towork with well-edited news text written by professional journalists, punctuation is retained andthere are grammar rules that appreciate he (sometimes heuristic) information that it can provide.The whitespace between words is also noted (newlines, tabs, different numbers of spaces) sinceit provides relatively reliable vidence for paragraphs, tables, header fields, etc., which in turncan provide useful external evidence.Additionally, SPARSER is designed to handle a constant, unrestricted stream of text, dayafter day, and this has led to a way to treat unknown words that allows it to look at their proper-ties, and from that possibly form them into proper names, without being required to give them along-term representation which would eventually cause the program to run out of memory.To illustrate how these aspects work, and at the same time establish the implementationsetting in which proper name processing takes place, we will now describe the lower levels ofSPARSER's operation, starting with its tokenizer and populating the terminal positions of thechart.3.1 TokenizingThe tokenizer transduces characters to objects representing words, punctuation, digit sequences,or numbers of spaces.
It is conservatively designed, just grouping contiguous equences ofalphabetic haracters or digits and punctuation and passing them them all through to be theterminals of the chart, where even the simplest compounds are assembled by sets of rules thatare easily changed and experimented with.
For example, rather than conclude inside thetokenizer that the character sequence "$47.2 million" is an instance of money, it just passesthrough six tokens, including the space.A word is 'known' if it is mentioned in any of the rules of the grammar.
2 A known wordhas a permanent representation, and the tokenizer finds and returns this obj~ect when it delimitsthe designated sequence of characters.
The 'token-hood' of this word type is represented bytheword object filling particular places in the chart.Note that since this is a lexicalized semantic grammar, words have preterminal c tegories like 'flOe' or 'head-of-CO-phrase' (e.g.
"company", '~firm", "'enterprise") or are often treated just as literals, e.g.
all theprepositions or words like "basea P'.38The tokenizer separates the identity of a word from its capitalization.
A word is defined bythe identity of its characters.
The pattern of upper and lowercase l tters that happens to occur ina given instance is a separate matter, and is represented in the chart rather than with the word?Thus when the chart is populated with terminals, each position records the word that startsthere, its capitalization, and the kind of whitespace that preceded it, all given as separate fieldsof the position object.
The scan is done incrementally in step with the rest of the SPARSER'Sactions.3.2 Word- t r iggered  operationsSparser's processing is organized into layers.
Tokenizing and populating the terminals of thechart is the first level, then comes a set of special operations that are triggered by words or theirproperties (e.g.
ending in "ed" or consisting solely of digits).
The application of phrasestructure rules is the next layer, and finally there is the application of heuristics to try spanninggaps caused by unknown words.
Semantic interpretation is done continuously as part of whathappens when a rule completes and an edge is formed.
We will not describe the last two layers(see McDonald 1992 for a description of the phrase structure algorithm), but will brieflydescribe the word-level operations since it includes triggering PNF.Operations triggered by the identify of a word include forming initials and known abbre-viations, and particularly the recognition of multi-word fixed phrases which we call"polywords" following Becker (1975).
Polywords are immutable sequences of words that arenot subject o syntactically imposed morphological changes (plurals, tense) and that can only bedefined as a group.
Polywords are a natural way of predefining entities that have fixed, multi-word names uch as the countries of the world, the states of the US, major cities, etc.
Instancesof this relatively closed class of individuals are a valuable kind of evidence in the classificationof proper names.When PNF finishes the recognition and classification of a new name, it adds to the gram-mar a polyword rule for the sequence of words in the name, with the recorded name-object asthe polyword's denotation, so that the process can be short-circuited the next time the name isseen.
Note that this does not stop PNF from triggering and running its delimiting operation thenext time that sequence is seen; it only speeds up the classification and recording.
If we allowedthe polyword operation to take precedence, we would never see the longer word sequences thatembed known names ("New York Port Authority"), or we would have to resort to a morecomplex algorithm.We have a particularly fast algorithm for checking for polywords when the first word of thesequence has been seen.
There are also special rules that allow paired punctuation tobe grouped(parentheses, quotations, etc.)
even if the words separating them are not all known.
This isparticularly useful for picking up nicknames embedded within a person's name since it willoften be an unknown word in quotations inside parentheses ("Richard M. ("tricky Dick")Nixon").
Subsidiaries of companies are often marked for their geographical rea in the sameway, e.g.
"manufactured by UNIVERSAL FLUID HEADS (Aust.)
PTY.
LTD." (from the nameplate on a camera tripod).One can deliberately define acapitalization-sensitive version of a word, e.g.
to syntactically distinguish titlesin pro-head position from those in appositives or elsewhere.
In such cases there is a distinct word object witha link to the case-nouu:al version of the word.39The first check at the word-level is for actions triggered by a word's properties, particularlyhere the properties of its characters.
This is how compound numbers are formed (42,357.25)triggering off words that are sequences of digits, and it is how PNF is triggered.
Every time achart position is reached that indicates that the following word is capitalized, PNF is called.PNF then takes over the process of scanning the successive terminals of the chart, until it scansa word that is not capitalized, deliberately calling other SPARSER mechanisms like polywordrecognition or phrase structure rewrite rules as needed.When PNF is finished, its results are given in an edge it constructs over the sequence ofcapitalized words and selected punctuation, with the label on the edge dictating how it will fitinto SPARSER's later processing layers and the referent field of the edge pointing to the nameobject hat records it in the discourse history.
Since Sparser uses a semantic grammar, the labelis the constituent's classification--a semantic ategory like 'person'.
There is also conventionallabel (always NP for a name) included with the edge for default or heuristic rules of phrasalformation; see McDonald (submitted) for the details of this two label system.4 Walking through an exampleIn this final section of this paper we will look at the processing of the following paragraph-initial noun phrase from the Wall Street Journal of 10/27/89, article #34:"An industry analyst, Robert B. Morris III in Goldman, Sachs & Co.' sSan Francisco office, said... "The capitalization of the very first word "An" triggers PNF, whose delimitation processstops immediately with the next word since it is lowercase.
The classification pass through the(one word) sequence then shows it to be a grammatical function word, and classification appliesthe rule 'single word sequences consisting solely of a non-preposition function word arc not tobe treated as names'.
PNF is then finished; the article reading of "An" will have been introducedinto the chart during classification; and the scan moves on.
4As the parse moves forward, the title phrase "an industrial analyst" is recognized and thecomma fter it is marked as possibly indicating an appositive (or also a list of titles, though thisis less likely).PNF is triggered again by the capitalization of "Robert", and the delimitation process takesit up to the word "in".
Running the regular ules of the grammar within that sequence uncoversthe abbreviation and the generation-indicator "HI" for 'the third'.
We do not maintain any listsof the common first names of people or such, so consequently both "Robert" and "Morris" areFortunately we have yet to see a company whose name was "The"--one wonders how journalists would dealwith it.
Of course there are companies like Next Inc. and On Technology, which, like the names of racehorses or boats, add spice to the grammarian's life by overloading the interpretations of closed-class words.The only consistent treatment we have arrived at for these ("On" referring to the company does occur insentence-initial position) is to treat he words as ambiguous and to introduce two edges into the chart, one foreach reading.
We only do this if the full name of the company appeared earlier in the article, however, whenthe preposition will have received its denotation as an element of a name and the basis of the ambiguity beenestablished.40seen as unknown words.
The abbreviation and generation-indicator areenough, however, toallow the sequence to be reliably classified as the name of a person.Given that classification, an edge is constructed over the sequence and given the label'person', and the recording process constructs a name object for the edge's denotation.
Thepattern given by the classifier is 'name - initial - name - generation-indicator', which is clearenough for the name subtype 'person's name with generation' to be instantiated.
This objecttakes a sequence of first names or initials, a last name (the word at the end before "III"), andthen the "I Ir '  in a slot that also gets words like "Junior".
Let us call this new name-individualName- 1.5Part of the recording is the creation of denotations for the words "Robert" and "Morris".Individuals are created for them of type 'single word element of a name', and rules are added tothe grammar so that the next time we see them in the grammar we will be taken directly to thosesame individuals.
By warrant of forming the interpretation f the whole sequence as Name-1,we can now attribute properties to the names (semantic objects) 'Robert' and 'Morr is ' - -'Robert' is the first name of Name-1 and 'Morris' is the last name.
The policy of letting wordslike "Morris" denote name objects with semantic links to the name they are part of (with thatname in turn linked to the person whose name it is) provides a very direct way to understandsubsequent references involving just part of the original name (e.g.
"Mr. Morris") as we cantrace it directly to the person just by following those links.
(Of course the links will also take usto anyone else the world model knows of who has that same last name, hence the need for agood discourse model that appreciates the context set up by the article being processed.
)Moving on, the next point where PNF comes into play is at the word "Goldman".
It is seenas a one word sequence because of the comma just after it, and is an unknown word.
Not beinga function word, it is spanned with an edge labeled just 'name' and recorded with just a newsingle-word name individual as its denotation.
Given the significance of commas for namepatterns, we also at this point make a note (set a variable) that this comma is preceded by aname.PNF immediately resumes with "Sachs & Co.", stopping the delimitation process when itrecognizes the "'" and "s" tokens as constituting an appostrophe-s, which is a definitive markerfor the end of a noun phrase.
Dunng the delimitation process, the abbreviation "Co." will alsohave been recognized and expanded, and the "&" noted and appreciated as being a punctuationmark that can appear in names.
Punctuation is always handled uring the course of delimitationfor just these reasons.The presence of the "&" and the word "Company" are definitive markers of companies andthe classification process will start the assembly of a pattern to send off to be recorded.
In thiscase however, as noted earlier, there is what amounts to an interaction between classificationand delimitation.
Part of what the classifier knows about companies i the profusion of caseswhere the name of the company is a sequence of words separated by commas (law firms,advertising agencies, any sort of partnership tends to use this name pattern).
Appreciating this,the process looks for the contextual note about he preceding comma.
Finding it, it observes thatthe name in front of the comma is not itself classified as a company (which would haveindicated a list of companies rather than a single name), and it proceeds to assimilate the edgeover "Goldman" and the comma into the name it is already assembling.
Had there been stillThere is no interesting limit on the number of 'first names' aperson can have, so we have not yet found itprofitable to have any more structure in that field than simply an ordered sequence; consider "M.A.K.H alliday" , "(Prince) Charles Philip Arthur George".41more 'stranded' elements of the name followed by commas, this would have been noted as welland those elements added in.Occasionally the name of a company like this is given with "and" instead of the specialpunctuation character.
Had that happened here, the fact that the "and" preceded the word"company" would have been sufficient evidence to take the whole sequence as the name of asingle company, however ff there had been no such internal evidence within any of the elementsof the conjunction, they would have been grouped together as unconnected names panned by asingle edge labeled 'name', leaving it to external evidence from the context to supply a strongercategorization (both as category and whether they were one name or several), as we can seewith the next capitalized word sequence that PNF delimits, "San Francisco".With access to a good gazetteer we could have already defined San Francisco as the nameof a city using a polyword.
Alternatively, without needing any word list we can conclude that itis a location, and probably acity, just by looking at its context: the word "office".As said earlier, the availability of mechanisms that use external evidence like this allowsPNF to make a weak analysis that can be strengthened later.
In this case it will see "SanFrancisco" as a sequence of two unknown words.
Without any internal evidence to base itsjudgement on, it can only (1) accept he sequence as a phrase and span it with an edge, indicat-ing that the words have more relationship to each other than either individually has to its neigh-bors, and (2) give this edge the neutral label 'name'.After PNF is done, the phrase structure component of Sparser takes over.
Sparser's rewriterule facility includes context-sensitive as well as context-free productions, including for thiscase the rulename -> locat ion  / ~of f i ce"That is, an edge labeled 'name' can be respanned with a new edge with the label 'location' whenthe name edge appears just in front of the word "office".
Context sensitive rules are handledwith the same machinery as context free rules, e.g.
the trigger pattern here is the same as that ofa CF rule with the righthand side 'name' + "office".
The difference is simply that instead ofcovering the whole fighthand side with a new edge we just respan the one indicated constituent.Similarly, if the person in this example had the name "Robert Morris" (rather than "RobertB.
Morris llr'), where there would have been no available internal evidence to indicate itsclassification during the operations of PNF, we would later have applied either of two contextfree rules: one working forwards from the definitively recognized title, the other backwardsfrom the pp 'in-company'.name -> person  / t i t le  " , "  _ _name -> person  / in -companyA repertoire of such context-sensitive rules or their equivalent is needed if a proper nameclassification facility is expected to work well with the open-ended set of name words found inactual texts; Sparser used a set of roughly 30 rules to handle the names in the blind test on theWho's News column mentioned earlier.425 ReferencesAlshawi, Hiyan (ed.)
(1992) The Core Language Engine, MIT Press.Becker, Joe (1975) "The Phrasal Lexicon", in Schank & Webber (eds.)
Proc.
TINLAP-1,ACM, 60-63.Coates-Stephens, Sam (1992) "The Analysis and Acquisition of Proper Names for theUnderstanding ofFree Text", Computers in the Humanities.
(DARPA) Defence Advanced Research Projects Agency (1992) Proceedings of the FourthMessage Understanding Conference: MUC-4 , June 1992, Morgan Kaufmann.Liberman, Mark (1989) Panel presentation atthe 27th Annual Meeting of the ACL, Vancouver.MacLean, Alistair (1976) The Golden Gate, Fawcett Publications, Greenwich Connecticut.Masand, Bdj M. & Roger D. Duffey (1985) "A Rapid Prototype of an Information Extractorand its Application to Database Table Generation", working paper, Brattle ResearchCorporation, Cambridge, MA.McDonald, David D. (1992) "An Efficient Chart-based Algorithm for Partial-Parsing ofUnrestricted Texts", proceedings ofthe 3d Conference on Applied Natural LanguageProcessing (ACL), Trento, Italy, April 1992, pp.
193-200.
(submitted) "The Interplay of Syntactic and Semantic Node Labels in Partial Parsing",manuscript submitted to the International Workshop on Parsing Technologies, August1993.Rau, Lisa F. (1991) "Extracting Company Names from Text", proc.
Seventh Conference onArtificial Intelligence Applications, February 24-28, 1992, IEEE, 189-194.43
