Does tagging help parsing?A case study on finite state parsingAtro VoutflainenResearch Unit for Multflingual Language TechnologyDepartment ofGeneral LinguisticsP.O.
Box 4 (Keskuskatu 8,7th floor)FIN-00014 University of Helsinki, F~,~l~n~\[?
A~zo.
Vou~ilaJ~enQllJ~g.
hels~g\]TA.
I iAbstract.
The usefulness of POS taggere for syntactic parsing is a question little ad-dressed in the literature.
Because taggers reduce ambiguity from the parser's input, pars-ing is commonly supposed to become faster, and the result less ambiguous.
On the otherh~nd, tagging errors probably reduce the parser's recognition rate, so this drawback mayoutweigh the possible advantages.
This paper empirically investigates these issues usingtwo di~erent rule-based morphological d sambiguators as preprocessor fa wide-coveragefinite-state parser of English.
With these rule-based taggers, the parser's output becomesless ambiguous without aconsiderable penalty to recognition rate.
Parsing speed increasesslightly, not decisively.1 In t roduct ion1.1 The parsing prob lemMost application-oriented natural-language parsers are computer prograTrLq that try to auto-matically ass~n a syntactic structure onto input sentences, by using a formal anguage model,e.g.
a large gr~mm~.
In a sense, a syntactic parser tries to choose from a multitude of imag-inable syntactic sentence analyses the alternative that is the most appropriate for the inputsentence.
So syntactic parsing can be ~iewed as a massive disambiguation task where the mainproblems are:(i) Determln~n~ the correct sentence A~lysis, especially in the case of long sentences, mayrequire a lot of time and memory (computational problem)(ii) Deciding between possible analyses may be dlmcult due to (i) the inherent syntactic ambi-guity of the sentence or (ii) the shortages in the specificity of the parser's language model(linguistic problem)(iii) Assigning a syntactic analysis to an acceptable sentence may fail because the parser's lan-guage model does not account for the syntactic structures that emerge as the sentence(linguistic problem)1.2 A possible part ia l  solut ionA solution often proposed to these problems is use of a tagger as a front-end of the parser.
Nowwhat is a tagger, and how are they supposed to help parsers?What  are taggers7 Program~ variously known as part-of-speech taggers, POS taggers ormorphological taggers typically contain three analytic stages:25- The token@er identifies words, punctuation marks and sentence boundaries.- The ls~ca/ana/~/ser assigns possible analyses to each word.
Some words can receive multipleanalyses; for instance the sentence Those girls sing well can be analysed as follows:Those_DET_Pgirls_Nplsing_Vpres_Vinf_Vimp_Vsbjwell_ADV_N_Vpres_Vinf_Wmp_VsbjHere Those gets two alternative analyses (a dete~,,,iner and a pronoun analysis), while tvei!gets as many as six alternative analyses.- The d~sambiguator resolves lexical ambiguity by removing analyses that seem superfluousaccording to the language model used by the program; for instance:Those_DETg i r l s_Nplsing_Vpreswell_ADVIn this case all words became fully disambiguated, but, to reduce the risk of removingthe correct analysis, some of the most r\]if~cult ambiguities can optionally be left pendingin many tagging program~ (so other modules e.g.
the parser, can choose between them ifnecessary).The design of the disambiguator is the hardest subproblem in tagging, and several solutionshave been proposed for improving the reliability of the disambiguator's language model (data-driven models automatically generated from pretagged texts; linguistic models with manuallydeveloped isambiguation grAmmArs; hybrid models).Why are taggers upposed to be useful?
As for the supposed usefulness of taggers forparsing, the following assumptions are often made:(i) The tagger apidly decreases the ambiguity of the sentence, as a consequence of which thecomputation~IIy heavier syntactic parser has to deal with less ambiguity.
So the computa.tional problem should become smal/er and parsing faster.
(ii) The tagger resolves ome ambiguities not addressed by the syntactic parser's languagemodel, so the parser is expected to have a better chance to find the correct analysis withthe tagger than without it.At the first flush, these assumptions may seem self-evidently true.
However, also scepticalviews are possible.
One could argue that taggers do not give any real advantage for maturesyntactic parsers, e.g.
for the following reasons:(i) taggers resolve mainly local, 'easy' ambiguities that would be resolved by the parser inany case with very little extra computational load, so it is questionable whether a maturesyntactic parser would gain anything in terms of speed or accuracy ?~om using a tagger,(fi) taggers make so many mlspredictions that the possible galne in parsing time, or averagenumber of syntactic parses, is more than counteracted by the decrease in the parser's recall:for many sentences no good parse is available if correct POS tags have been lost, and26(iii) even if there were a tagger with a satisfactory recall and precision, maldng it would takeso much effort, either in the form of annotating big training corpora, or writing rules, thatthe same effort would be more beneficially spent on developing the parser itself.So the assumed relevance of tagging for parsing seems to be an open, empirical question.What does the literature say on the subject?1.3 Earl ier studiesUIinIn inThe computational linguistics literature seems to contain very few evaluations about usingtaggers in parsing.
Three studies are examined next.- A well-known paper on using statistical taggers in statistical parsing is one by Charniaket ai.
{1\].
In their experiments, they use two kinds of statistical tagger: the single taggeroutputs fully disambiguated text, while the multiple tagger leaves the hardest ambiguitiesunresolved to decrease the number of its mlspredictions.
Contrary to expectations, theirexperiments suggest that the statistical parser is no better in resolving the hardest morpho-logical ambiguities than the single tagger, so passing the most difficult ambiguities on to theaw~ecUy more informed syntactic language model does not seem practically motivated.- A paper by Wauschkuhn \[17\] eYamlnes the use of a statistical HMM tagger for Germanas a front-end of a syntactic parser that uses a hand-coded grammar.
The experimentssuggest that a tagger educes the ambiguity of the syntactic parser's output, but only with aconsiderable penalty in terms of the poorer ecognition rate of the parser.
The experimentsgive the impression that taggers, at least statistical ones, are not particularly useful forimproving a parser's accuracy.- Results more favourable for tagging are reported by Ofiazer and Kuru~z \[7\].
They reportthat a rule-based tagger of Turkish (that uses a human disambiguator asits final component)improved the speed of a LFG parser of Turkish using a non-trivial grammar by a factor of2.38, while the average number of parses per sentence fell from 5.78 to 3.30.
However, theydo not report he figures for sentence recognition: how many sentences got a parse with thetagger and how many without.
Also their test data was rather small: 80 sentences with anaverage sentence l ngth of 5.7 words only.
Several questions remain open:?
How much does the fully automatic part of the tagger improve performance??
What is the behaviour of the systems with longer sentences??
How is the parser's recognition rate affected??
How much would the tagger benefit a parser with a more mature grammar?1.4 Structure of  this paperIn this paper, we present experiments with two taggers and one parser.
The parser is a re-ductionistic dependency-oriented finite-state parser of English that represents utterances withmorphological nd syntactic tags.
The parser consists of the following components: ( i )  a to-keniser; (ii) a morphological nalyser; (iii) a simple lookup program for introducing all possiblesyntactic analyses as alternatives for each word and word-boundary; and (iv) a finite-state syn-tactic parser (actually a syntactic disambiguator) that discards those sentence readings thatviolate the parser's grammar.This setup contains no specific module for resolving morphological mbiguities that arisein lexical analysis.
The syntactic grammar actually suffices for resolving many morphologicalambiguities as a side-effect of proper syntactic parsing.27However, amorphological disambiguator can optionally be included in the setup, directly af-ter morphological nalysis.
The disambiguators referred to in this paper are linguistic onstraint-based systems.
Ambiguities occurring in unspecified contexts are not resolved, so these disam-biguators can produce ambiguous output.In this paper we report experiments where the modular setup uses the following disamhigua-tion modules:- No disambiguation.
Only the 6-ite state syntactic parser is used for ambiguity resolution.- A small disambiguator whose 149 rules were written during one day.
This module discardsover 70% of all extra morphological readings.- A mature disambiguator whose 3,500 rules were written in the course of several years.
Thismodule discards about 95% of all extra morphological readings with a ml,lmal error rate.The data is new to the system, and it consists of three corpora, each with 200 sentences.
Inthe experiments, we consider the following issues:- Syntactic ambiguity before finite state disambiguatio~ how is the ambiguity rate of thesyntactic disambiguator's input reduced by different morphological disambiguators?- The parser's recognition rate how many sentences does the finite state parser recognisewith different morphological disambiguators?- Multiple analyses: how much syntactic ambiguity isproduced by the different setups?- Parsing time: how much does the use of different disambiguators affect parsing time?2 The  f in i te  s ta te  parserThe finite state parser outlined in this section is described in greater detail in Tapauainen \[11\]and Voutilainen \[14\].2.1 Grammat ica l  representat ionLet us describe the syntactic representation with an example.
The parser produces the followinganalysis for the sentence The man who is fond of sin~ng this aria killed his .~er  (somemorphological information is deleted for readability):@@the DET @>N @man N @SUBJ @<who PRON @SUBJ @be V @MV N<@ @fond A @SC Qof PREP @N< @sing PCP1 ~mv P<<@ @this DET @>N @aria N @obj @>kill V ~MV MAINC@ @he PRON @>N @father N @OBJ @@fullstop @@28mm\[\]mm\[\]mU\[\]mm\[\]m\[\]The representation co,~i~_~ts of base-forms and various Buds of tags.
"@@" indicates entenceboundaries; the centre-embedded finite clause "who is fond of singing this aria" is flanked bythe clause boundary tags @< and ~> and its function is postmodifying, as indicated withthe second tag N<Q of "be", the main verb (@MV) of the clause.
The pronoun "who" is thesubject (~S .UBJ) of this clause, and the adjective "fond" is the subject complement (@sc)that is followed by the postmodifying (@N<) prepositional phrase starting with "oi ~', whosecomplement is the no,~S-lte main verb (~nv) "sing" that has the noun "aria" as its object(~obj) (note that the lower case is reserved for functions in nonfinite clauses).The matrix clause "The man killed his father" is a finite main clause (MAINC~) whose mainverb (@MV) is "kill".
The subject (QSUBJ) of the finite clause is the noun "man", while thenoun "father" is the object in the finite clause (@OBJ).
The word "father" has one premodifier(~N) ,  namely the genitive pronoun "he".This representation is designed to follow the principle of surface-syntacticity: distinctionsnot motivated by surface grammatical phenomena, e.g.
many attachment and coordinationproblems, are avoided by making the syntactic representation sufficiently underspecific in thedescription of grammatically (if not semantically) unresolvable distinctions.2.2 .An~dysls routineThe tokeniser identifies words and punctuation marks.
The morphological nalyser contains arule-based lexicon and a guesser that assign one or more morphological nalyses to each word,cf.
the analysis of the word-form "tries""~:l;z"i es ) ""try" <SV0> V PRES SG3 VFIN"try" N N0M PLThe next step is the introduction of alternative syntactic and word-boundary descriptors witha simple lookup program.
After this stage, the sentence "Pete tries."
looks liice this:(@@ pete <*> <Proper> N NON SG (:OR Q~h @>N @>>P @SUBJ @subj @OBJ @obj@IOBJ @iobj @SC @sc @0C @oc ~ULPP ~P<< @ADVL)(:0a Q @/ @< @>) (:0a(try <SV0> <SV> <P/for> V PRES SG3 VFIN(:OR (~RV MAINC@) (@MV PAREN@) (~MV SUBJ@) (@MV OBJ@) (@MV obj@)(QMV IOBJ@) (0MV SC@) (Gg~, ec@) (@MV 0C@) (@MV oc@) (@MV APP@)C~mV pc<Q) (@Hv ~VLQ) (~ ~<@)))Ctry N N0M PL (:OR q~h @>N @>>P @SUBJ @subj @0BJ @obj @IOBJ @iobj @SC @sc@OC @oc @APP QP<< ~XDVL)))(:0R @ @/ @< Q>) @fullstop @@)compact representation contains 16 ?
4,14 ?
16 ?
4 ---- 57, 344 different sentence readings.Long sentences asily get 10 s?
-  101??
different sentence readings at this stage, i.e.
the ambiguityproblem with this syntactic representation is considerable.The final stage in this setup is resolution of syntactic ambiguities: those sentence readingsthat violate even one syntactic rule in the gr,--mar are discarded; the rest are proposed asparses of the sentence.29B\[\]\[\]B2.3 Rule formal ism IGrammar rules are basically extended regular expressions.
A typical rule is the implication rulewhereby contextual requirements can be expressed for a distributional (or functional) category.For instance the following partial rule (taken from Voutllainen \[12\]) about a syntactic formcategory, namely prepositional phrases,PassVChain..PostmodC1..NH-q..PREP =><De~erred> ?
_ ,<Defer red> ?
_ ,<Defer red> ?
_ ;@ Coord,..PrepComp,states a number of alternative contexts in which the expression (given left of the arrow) occurs.The underscore shows the position of the expression with regard to the required alternativecontexts, expressed as regular expressions.
The parser interprets this kind of rule in the followingway: whenever a string satisfying the expression left of the arrow is detected, the parser checkswhether any of the required contextual expressions are found in the input sentence reading.If a contextual licence is found, the sentence reading is accepted by the rule, otherwise thesentence-reading s rejected.Another typical rule is the "nowhere" predicate with which the occurrence of a given regularexpression can be forbidden.
For instance, the predicate nowhere(VF IN  .. VF IN) ;  forbidsthe occurrence of two ilnite verbs in the same finite clause.These ilnite-state rules express partial facts about the language, and they are independentof each other in the sense that no particular application order is expected.
A sentence readingis accepted by the parser only if it is accepted by each individual rule.2.4 The  grammarThe syntactic grammar contains some 2,600 finite-state rules each of which have been tested andcorrected against a manually parsed corpus of about 250,000 words (over 10,000 unambiguouslyparsed sentences).
Each rule in the grammar accepts virtually all parses in this corpus (i.e.
arule may disagree with at most one or two sentences in the corpus, usually when the sentencecontaln.q a little-used construction).The rules are not much restricted by engineering consideratious; linguistic truth has beenmore important.
This shows e.g.
in the non-locality of many of the rules: the description ofmany syntactic phenomena seems to require reference to contextual elements in the scope of a?
iiuite clause, often even in the scope of the whole sentence, and this kind of globaUty has beenpracticed even though this probably results in bigger processing requirements for the iinitestate disambiguator (many disambiguating decisions have to be delayed e.g.
until the end ofthe clause or Sentence, therefore more alternatives have to be kept longer 'alive' than might bethe case "with very local rules).Many rules are lexicalised in the sense that some element in the rule is a word (rather thana tag).
Though a small purely feature-based grammar may seem more appe~llng aestheticallyor computation_Ally, many useful lexico-grammatical generalisations would be lost if referenceto words were not allowed.30To sum up: the finite state disambiguator's task is facilitated by using a reasonably re-solvable surface-syntactic grammatical representation, but the parser's task remains computa-tionally rather demanding because of (i) the high initial ambiguity of the input, especially inthe case of long sentences, (fi) considerably high number of rules and rule automata, and (iii)the non-locality of the rules.
The finite state syntactic disamhiguator is clearly faced with acomputationally and linguistically very demanding task.3 Morpho log ica l  d i sAmbiguators8.1 Mature disAmbiguatorThe mature disamhiguator is an early version of a system presently known as EngCG-2 (Samu-elsson and Voutflainen \[8\]).
EngCG-2 uses a grP-mmAr Of 3,500 rules according to the ConstraintGrammar framework (Karlsson et al, eds., \[4\]).
The rules are pattern-action statements hat,depending on rule type, select a morphological reading as correct (by discarding other eadings)or discard a morphological reading as incorrect, when the ambiguity-forml-g morphologicalan_~!ysis occurs in a context specified with the context~conditions f the constraint.
Context-conditions can refer to tags and words in any sentence position; also certain types of word/tagsequences can be used in context-conditions.An evaluation and comparison of EngCG-2 to a state-of-the-art statistical tagger is reportedin (Samuelsson and Voutilalnen \[8\]).
In similar cir~lm~tances, the error rate of EngCG-2 was anorder of magnitude smaller than that of the statistical tagger.
On a 266 MI-/z Pentium nmnln~Linux, EngCG-2 tags around 4,000 words per second.
13.2 Small disambiguatorTo determine the benefit of using a rule set developed in a short time, one long day was spent onwriting a constraint gr~trnrnar Of149 rules for disambiguating frequent and obviously resolvableambiguities.
As the grammarian's empirical basis, a manually disambiguated benchmark corpusof about 300,000 words was used.The small grammar was tested agaln~t a held-out manually disambiguated (and severaltimes proofread) corpus of 114,388 words with 87,495 superfluous morphological nalyses.
Afterthe 149 rules were applied to this corpus, there were still 24,458 superfluous analyses, i.e.about 72% of all extra readings were discarded, and the output word contained an average of1.21 alternative morphological nalyses.
Of the 63,037 discarded readings, 79 were analysedas contextually egitimate, i.e.
of the predictions made by the new tagger, almost 99.9% werecorrect.4 Exper imentsThis section reports the application of the following three setups to new text data:(i) Nodi~.
the finite state parser is used as such.i Information about esting and licensing the present version of the EngCG-2 tagger is given at thefoUowing U\]~: hCtp://In~, coaexor, fi/analysers, html.31(ii) Small: a morphological disambiguation module with 149 rules is used before the finite stateparser.
(fii) Eng: a morphological disambiguation module with 3,500 rules is used before the finite stateparser.Three text corpora were used as test data:(i) Data 1:200 10-word sentences from The Wall Street Journal(ii) Data 2:200 15-word sentences from The Wall Street Journal(iii) Data 3:200 20-word sentences from The Wall Street JournalIn the word count, punctuation marks were excluded.
The data is new to system.The machine used in the tests is Sun SparcStation 10/30, with 64 MB of RAM.In the statistics below, the term 'recognition rate' is used.
Recognition rate indicates thepercentage of sentences that get at least one analysis, correct or incorrect, from the parser.
Theparser's correctness rate rema~nq to be determined later (but cf.
Section 4.2 above).4.1 Statistics on input AmbiguityBefore going to detailed examinations, some statistics on input ambiguity are given.
The fol-lowing table indicates how many readings each word received on an average after possiblemorphological disambiguation and introduction of syntactic ambiguities.
The ambiguity ratesare given for morphology and syntax separately.SmaU EngI N?dismor syn \[mor Isyn \]morlsyn \[lDatallZ.r4 122.8111.19 Ila.0311.04113.91 IData2t1.78 23.4811.21 I 6.r011.04114.33 IData311.
'T7 23.0911.23 11n.eS11.0S114.x31For instance, after EngCG-2 dissmbiguation, words in Data 2 received an ,average 1.04morphological analyses and 14.33 syntactic analyses.At the word level, syntactic ambiguity decreases quite considerably even using the smalldisambiguator, from about 23 syntactic readings per word to some 16.5 syntactic readings perword.
Use of the EngCC~2 disambiguator does not contribute much to further decrease ofsyntactic ambiguity.
Overall, syntactic ambiguity at the word level remain.~ quite large, about14 analyses per word.However, if we consider the ambiguity rate of the finite state parser's input at the sentencelevel (which is the more common way of looking at ambiguity at the level of syntax), thingslook more worrying.
The following table (next page) presents syntactic ambiguity rates at thesentence level for Data 1 (the 10-word sentences).When no morphological disambiguation is done, a typical ambiguity rate is 1017 sentencereadings per input sentence; even after EngCG-2 disambiguation, the typical ambiguity rate isstill about 10 zs sentence readings.32ReadingsiNodis \[SmaU\[Eng I1O lz 410 is 1 8 910~" \[i 16 271O l~ 14 32 48'10 lu 31 52 491017 49 50 3310 ~s 50 23 17I0 I" ~ 127lO ~?
5 210 r' 17 210 ~z \[2 110 z3 '1 1I0 ~ ,14.2 Analysis of data 1All three setups were able to parse Data 1 in the time allowed.
Here are some statistics on thedifferent setups:.
INodb parses 2% (4) \[2.5% (5) 3.5% (7)_1 parses \]14.5% (29) 119.5% (39) 37% (74)1-5 parses 159.5% (119)\[64% (128) 80% (160)1-20 parses\]89.5% (179)J91'5% (i'83) 95.5% (191)0-10 sec \[6% (12) \[20.5% (41) 40.5% (81)0-100 sec 161% (122) i76% (152) 90% (180)Morphological disambiguation did not considerably affect he recognition rate of the parser.Without morphological disambiguation, the parser gave analyses for 98% of all sentences; theuse of the EngCG-2 disambignator decreased the recognition rate only by 1.5%.
Consideringthat the known strength of the EngCG-2 disambiguator is h/gh recall, the small loss in thenumber of parses does not seem particularly surpr'~dng.The number of parses decreased even when the small disambiguator was used.
The decreasewas considerable with EngCG-2, e.g.
the rate of sentences receiving 1-5 parses rose from about60% to 80%.
The somewhat unexpected syntactic disambiguating power of the morphologicaldisambiguators is probably due to the lexical nature of the disambiguation grammar (manyconstraints refer to words, not only to tags).
Lexical information has been argued to be animportant part of a successful POS tagger (cf.
e.g.
Church \[2\]).Generally, parsing was rather slow, considering the shortness of the sentences.
Disambigua-tion certainly had a positive impact on parsing time, e.g.
the ratio of sentences parsed in lessthan ten seconds rose from 6% to about 40%.4.3 Analysis of data 2Nodis and Small were in trouble due to excessively slow parsing.
The first 9 sentences wereparsed by all three setups.
Here are the relevant statistics.33111192 1092 15 L53 3 1172 14 335 8 97 4 443 133 426 32 355 10 i1074 6 291 3 34 3 !345 2 418 1 171 11 776 14 741 2 92 r2 !927 12 1878 4 428 14 4298 13 3061 2 792 !2 4799 81 ~021 34 16052 428The general trend seems to agree with experiences from Data 1: the number of parses aswell as parsing time generally decreases when more morphological disambiguation is carriedout (however, note the curious exception in the case of sentence 3: parsing was faster withno disambiguation than with small disambiguation).
Because of the scarcity of the data, morespecific omparisons can not be made.The setup with EngCG-2 disambiguation parsed all 200 sentences of Data 2.
Because theother setups did not do this in the time allowed, no comparisons could be made.
It may howeverbe interesting to make two observations about the number of parses received.
Consider thefollowing table.li 13.s% (7) J Ile  (38) I-s 1~.8.u% (117) t\[!
:20 186% (172) JOf all sentences, 96.5% got at least one parse, i.e.
the slightly greater length of the inputsentences does not seem to considerably affect the parser's coverage (the recognition rate wasthe same in Data 1).The ambiguity rate increases cousidexably.
For instance, only 28.5% of all sentences in Data2 (compared to the 80% of Data 1) received 1-5 parses.4.4 Analysis of data  3In the analysis of the 20-word sentences, even the setup using the EngCG-2 dissmbiguator wasin trouble: within the time allowed, the system analysed only 25 sentences.
All of them receivedat least one parse.5 D iscuss ion  and  conc lus ionWe have investigated the use of two rule-based morphological disambiguators - the grammar ofone developed over several years, the grammar of the other written in one day - for facilitatingsyntactic parsing in a nontrivial finite-state parser, paying special attention to the followingissues:34mmm)mmImmmnmmmmmmmUmmmmnnmraiNm- the possible negative ffects of morphological disambiguation to the parser's recognitionrate,- whether morphological dis~mhiguation can contribute to resolution of syntactic ambiguity,and- the effect of morphological dis~mhiguation  parsing timeOn the basis of empirical tests, two encouraging observations can be made.
Firstly, the parser'srecognition rate was not considerably impaired by either disambiguator.
The argued strengthof rule-based disambiguators is their high recall (partly at the expense of precision); the presentobservations seem to support he argument.
Secondly, especially the EngCG-2 disambiguatorcontributed tosyntactic disambigustion (i.e.
the number of parses decreased considerably).
Thisis probably due to the lexicalised nature of the disambiguation gramme,.Our observations about the effect of morphological disambiguation parsing time aresomewhat mixed.
Obviously, both disambiguators made parsing faster; for example, the EngCG-2 disambiguator made the parsing of  the 200 10-word sentences about six times faster thanwith no morphological disambiguation.
However, these experiments do not encourage use ofmorphological di~mhiguators if they are expected to m~kp arsing possible; a parser troubledby long sentences should be improved with other techniques as well.
One possible xtension ofthe present study is to apply computationa/ly cheap CG-style disambiguation techniques alsoto syntactic ambigu/ties before using the finite-state parser.AcknowledgementsThe pkrsing sof~,w=e used in these experiments was written by Pasi Tapanainen (\[9, 10\]).References1.
E. Charniak, G. Carroll, J. Adcock, A. Cassandra, Y. Gotoh, J. Katz, M. Littman and J. McCann.1996.
'l~ggers for parsers.
Artificial Int~lligentz, Vol.
85, No.
1-2.2.
K. W. Church.
1992.
Current Practice in Part of Speech Tagging and Suggestions for the Future.In Si,,,,,,ons (eel.
), S~r~O= Fr=ci: In Honor of H~ Ku~era, Mi~igan $1at~: Studies.
Mlcl~an.13-48.3.
F. Karlsson.
1990.
Constraint Gr~mmA~ asa Framework for Parsing Running Text.
In H.
Karlgren(ed.
), Proc.
Coling'90.
Helsi-i,i~4.
F. Karlsson, A. Voutilainen, J. Heikkil~ and A. Anttila (eds.).
1995.
Co~traint Grammar.
ALanguage-Independent Sgstem for Parsing Unrestricted Tezt.
Berlin and New Yorl~ Mouton deOruyter.5.
K. Koskenniemi.
1983.
Two-level Morphology.
A General Computational Model for Word.formProduction and Generation.
Publications 11, Department of General Linguistics, University ofHelsinki.6.
K. Koskenniemi.
1990.
Finite-state parsing and disambiguation.
Proc.
Coling'gO.
Hel~in~ Finland.7.
Oflazer, K. and I. Kuru~z.
1994.
Tagging and morphological disamb'~tion f Turkish text.
Procs.ANLP-94.8.
C. Samuelsson a d A. Voutilalnen.
1997.
Comparing a linguistic and a stochastic tagger.
Proc.EACL-ACL97.. ACL, Madrid.9.
P. Tspanainsn.
1992.
".~rellisiin automaatteihin perustuva luonnollisen kielen j~sennin" (A finitestate parser of natural language).
Licentiate thesis.
Dept.
Computer Science, University of Helsinki.10.
P. Tapanainen.
1996.
The Constraint Grammar Parser CG-~.
Dept.
General Linguistics, Univer-sity of Helsinki.3511.
P. Tapanainen.
1997.
Applying a finite-state intersection grammar, in Emmanuel Roche andYves Schabes, editors, Fin~e.atate language proceuing.
A Bradford Book, MIT Press, Cambridge,Massachusetts.
311-327.12.
A. Voutflalnen.
1994.
Three 8tudiea o~ gmmmar-baJed jut/ace parsing of unrestricted Bnglish tezt.
(Doctoral dissertation.).
Publications 24, Dept.
General Linguistics, Un/versity of Helsinki.13.
A. Voutilainen.
1995.
A syntax-based part of speech analyser.
Proc.
BAC/~'95.
Pages 157-164.14.
A. Voutilai~en.
1997.
The design of a (finite-state) parsing gr*mm~r, in \]~m .
.
.
.
~el Roche andYves Schabes, editors, Finite-state language proceuing.
A Bradford Book, MIT Press, Cambridge,Massachusetts.15.
A. Voutila|-en and P. Tapansinen.
1993.
Ambiguity Resolution in s Reductionistic Parser.
Proc.EACI,'g3.
ACL, Utrecht.
Pages 394-403.16.
A. Voutilainen and T. J~awinen.
1995.
Specifying ashallow grammatical representation f rparsingpurposes.
Proc.
BAOL'95.
ACL, Dublin.17.
O. Wauschkuhn 1995.
The influence of tagging on the results of partial parsing in German cor-pora.
Proc.
Fourth International Workshop on Paraing te~hnoloyie& Prague/Karlovy Vary, CzechRepublic, September 1995.
Pages 260-270.36mm\[\]mmmmmm\[\]m\[\]m\[\]\[\]\[\]\[\]\[\]\[\]m\[\]m\[\]\[\]m\[\]\[\]\[\]mm\[\]
