POLLy: A Conversational System that uses a Shared Representation toGenerate Action and Social LanguageSwati GuptaDepartment of ComputerScience, Regent Court211 Portobello StreetUniversity of SheffieldSheffield, UKs.gupta@dcs.shef.ac.ukMarilyn A. WalkerDepartment of ComputerScience, Regent Court211 Portobello StreetUniversity of SheffieldSheffield, UKm.walker@dcs.shef.ac.ukDaniela M. RomanoDepartment of ComputerScience, Regent Court211 Portobello StreetUniversity of SheffieldSheffield, UKd.romano@dcs.shef.ac.ukAbstractWe present a demo of our conversationalsystem POLLy (POliteness in LanguageLearning) which uses a common planningrepresentation to generate actions to be per-formed by embodied agents in a virtual en-vironment and to generate spoken utter-ances for dialogues about the steps in-volved in completing the task.
In order togenerate socially appropriate dialogue,Brown and Levinson?s theory of politenessis used to constrain the dialogue generationprocess.1 IntroductionResearch in Embodied Conversational Agents(ECAs) has explored embedding ECAs in domain-specific Virtual Environments (VE) where usersinteract with them using different modalities, in-cluding Spoken Language.
However, in order tosupport dialogic interaction in such environments,an important technical challenge is the synchroni-zation of the ECA Spoken Interaction module withthe ECA non-verbal actions in the VE.
We proposean approach that uses a common high level repre-sentation which is broken down to simpler levels togenerate the agents?
verbal interaction and theagents?
non-verbal actions synchronously for task-oriented applications that involve performing someactions to achieve a goal, while talking about theactions using natural language.In previous work, Bersot et al(1998) present aconversational agent called Ulysses embedded in acollaborative VE which accepts spoken input fromthe user and enables him or her to navigate withinthe VE.
They use a ?reference resolver?
whichmaps the entities mentioned in utterances to geo-metric objects in the VE and to actions.Figure 1.
A user interacting with the AgentsMax, a VR based conversational agent by Koppet al(2003) allows multimodal conversationalabilities for task-oriented dialogues in virtual con-struction tasks.
It builds on a database of utterancetemplates which contain the verbal part, aug-mented with accompanying gestures and theircross-modal affiliation.
In order to deal with thevagueness of language in specifying spatial rela-tions in virtual space, the K2 system (Takenobu etal 2003) proposed a bilateral symbolic and numericrepresentation of locations, to bridge the gap be-tween language processing (a symbolic system),and animation generation (a continuous system).K2 extracts a user?s goal from the utterance and967translates it into animation data.
The FearNot!demonstrator by Paiva et al(2005) provides train-ing to kids against bullying via virtual drama inwhich one virtual character plays the role of abully and the other plays the role of victim, whoasks the child for advice.
FearNot!
?s spoken inter-action is template-based where the incoming textfrom the child is matched against a set of languagetemplates.
The information about the character?saction is defined in a collection which contains theutterance to be spoken as well as the animation.Eichner et al(2007) describe an application inwhich life-like characters present MP3 players in avirtual showroom.
An XML scripting language isused to define the content of the presentation aswell as the animations of the agents.
A more ex-pressive agent, Greta, developed by Pelachaud et al(Poggi et al 2005) is capable of producing sociallyappropriate gestures and facial expressions, andused is in an evaluation of gesture and politenessas reported in Rehm and Andr?
(2007).Since these ECAs function in scenarios wherethey interact with the world, other agents, and theuser, they must be ?socially intelligent?
(Dauten-hahn, 2000) and exhibit social skills.
Our work isbased on the hypothesis that the relevant socialskills include the ability to communicate appropri-ately, according to the social situation, by buildingon theories about the norms of human social be-haviour.
We believe that an integral part of suchskills is the correct use of politeness (Brown &Levinson, 1987; Walker et al1997).
For instance,note the difference in the effect of requesting thehearer to clean the floor by saying ?You must cleanthe spill on the floor now!?
and ?I know I?m askingyou for a big favour but could you kindly clean thespill on the floor?
?According to Brown and Levinson (1987)(henceforth B&L), choices of these different formsare driven by sociological norms among humanspeakers.
Walker et al(1997) were the first to pro-pose and implement B&L?s theory in ECAs toprovide interesting variations of character and per-sonality in an interactive narrative application.Since then B&L?s theory has been used in manyconversational applications e.g.
animated presenta-tion teams (Andr?
et al2000; Rehm & Andr?,2007), real estate sales (Cassell & Bickmore, 2003),and tutorials (Johnson et al 2004; Johnson et al2005; Porayska-Pomsta 2003; Wang et al2003).Rehm & Andr?
(2007) show that gestures are usedconsistently with verbal politeness strategies andspecific gestures can be used to mitigate facethreats.
Work in literary analysis has also arguedfor the utility of B&L?s theory, e.g.
Culpeper(1996) argues that a notion of ?impoliteness?
indramatic narratives creates conflict by portrayingverbal events that are inappropriate in real life.Thus impoliteness often serves as a key to movethe plot forward in terms of its consequences.This demo presents our Conversational SystemPOLLy which produces utterances with a sociallyappropriate level of politeness as per the theory ofBrown and Levinson.
We have implementedPOLLy in a VE for the domain of teaching Englishas a second language (ESL).
It is rendered in ourVE RAVE at Sheffield University as well as on anormal computer screen, as explained in section 3.Figure 1 shows a user interacting with POLLy inRAVE.
Since RAVE is not portable, we will dem-onstrate POLLy on the computer screen where theuser will be able to verbally communicate with theagents and the agents will respond with computa-tionally generated utterances with an appropriatelevel of politeness as per a given situation.2 POLLy?s ArchitecturePOLLy uses a shared representation for generatingactions to be performed by the ECAs in the virtualdomain on one hand, and on the other, for generat-ing dialogues to communicate about the actions tobe performed.
It consists of three components: AVirtual Environment (VE), a Spoken LanguageGeneration (SLG) system and a Shared AI Plan-ning Representation for VE and SLG as illustratedin Figure 2.A classic STRIPS-style planner called Graph-Plan (Blum & Furst, 1997) produces, given a goale.g.
cook pasta, a plan of the steps involved in do-ing so (Gupta et al, 2007).
POLLy then allocatesthis plan to the Embodied Conversational Agents(ECA) in the VE as a shared collaborative plan toachieve the cooking task with goals to communi-cate about the plan via speech acts (SAs), neededto accomplish the plan collaboratively, such as Requests, Offers, Informs, Acceptances and rejections(Grosz, 1990; Sidner, 1994; Walker, 1996).
It alsoallocates this plan to the SLG which generatesvariations of the dialogue based on B&L?s theoryof politeness that realizes this collaborative plan asin (Andr?
et al2000;Walker et al 1997).968Figure 2: POLLY?s ArchitectureThe SLG (Gupta et al, 2007) is based on astandard architecture (Dale & Reiter, 1995) withthree components: Content planning, utteranceplanning and surface realization.
See Figure 2.
Thepoliteness strategies are implemented through acombination of content selection and utteranceplanning.
The linguistic realizer RealPro is usedfor realization of the resulting utterance plan (La-voie & Rambow, 1997), which takes a dependencystructure called the Deep-Syntactic Structure(DSyntS) as input and realizes it as a sentencestring.
The Content Planner interfaces to the AIPlanner, selecting content from the preconditions,steps and effects of the plan.
According to B&L,direct strategies are selected from the steps of theplan, while realizations of preconditions and negat-ing the effects of actions are techniques for imple-menting indirect strategies.
The content plannerextracts the components of the utterances to becreated, from the plan and assigns them their re-spective categories, for example, lexeme get/addunder category verb, knife/oil under direct objectetc and sends them as input to the Utterance Plan-ner.
The Utterance Planner then converts the utter-ance components to the lexemes of DSyntS nodesto create basic DsyntS for simple sentences, whichare then transformed to create variations as perB&L?s politeness strategies, with the ?politenessmanipulator script?.
For realizing these B&Lstrategies, transformations to add lexical itemssuch as ?please?, ?if you don?t mind?, and ?mate?were added to the DSyntS  to make a sentence lessor more polite.Some example dialogues are shown in section3.
In the VE, the human English language learneris able to interact with the Embodied Conversa-tional Agent and plays the part of one of the agentsin order to practice politeness real-time.2.1 Brown and Levinson?s theoryB&L?s theory states that speakers in conversationattempt to realize their speech acts (SAs) to avoidthreats to one another?s face, which consists of twocomponents.
Positive face is the desire that at leastsome of the speaker?s and hearer?s goals anddesires are shared by other speakers.
Negative faceis the want of a person that his action beunimpeded by others.
Utterances that threaten theconversants?
face are called Face Threatening Acts(FTAs).
B&L predict a universal of language usagethat the choice of linguistic form can bedetermined by the predicted Threat ?
as a sum of 3variables: P: power that the hearer has over thespeaker; D: social distance between speaker &hearer; and R: a ranking of imposition of thespeech act.
Linguistic strategy choice is madeaccording to the value of the Threat ?.
We followWalker et al?s (1997) four part classification ofstrategy choice.The Direct strategy is used when ?
is low andexecutes the SA in the most direct, clear and969unambiguous way.
It is usually carried out either inurgent situations (Please Help!
), or where the facethreat is small as in ?I have chopped thevegetables?
or if the speaker has power over thehearer, ?Did you finish your homework today?
?The Approval strategy (Positive Politeness) isused for the next level of threat ?
- this strategy isoriented towards the need for the hearer tomaintain a positive self-image.
Positive politenessis primarily based on how the speaker approachesthe hearer, by treating him as a friend, a personwhose wants and personality traits are liked, for ex.by using friendly markers ?Friend, would youclose the door?
?The Autonomy Strategy (Negative Politeness)is used for high face threats, when the speaker maybe imposing on the hearer, intruding on their spaceor violating their freedom of action.
These facethreats can be mitigated by using hedges, ?Iwonder if you would mind closing the door forme,?
or by minimizing imposition, ?I just want toask you if you could close the door.
?The Indirect Strategy (Off Record) is thepolitest strategy and is used when ?
is greatest.
Itusually has more than one attributable intention sothat the speaker removes himself from anyimposition.
For ex.
using metaphor and irony,rhetorical questions, understatement, hints etc.
Forinstance, ?Its cold in here,?
which implies a requestto close the door.2.2 Example DialoguesHere are some example dialogues that illustrate thedifference in the politeness strategies used in dis-course contexts of varying power.
Two MicrosoftAgents, Merlin and Genie are involved in a con-versation while cooking and cleaning in a kitchentogether.
Consider the difference in the degree ofpoliteness in each situation.Conversation 1: This is an example conversa-tion in which Merlin is Genie?s boss.Agent (Speech act: Politeness strategy): UtteranceMerlin: Would you mind washing the dishes?
(Approval: RequestAct)Genie: Sure, I?ll wash the dishes.
(Direct:AcceptRequest)Genie: I?m wondering whether I should boil thepasta.
(Autonomy:Offer)Merlin: Yeah you can.
(Direct: AcceptOffer)Merlin: You?ve burnt the vegetables.
(Direct: Inform)Genie: Yeah.
(Direct: AcceptInform)Genie: The oven is dirty.
(Indirect: RequestAct)Merlin: I?m sorry I can?t clean the oven.
(Approval: RejectRequest)Genie: Ok. (Direct: AcceptReject)Genie: If you don?t mind, can I clean the burner?
(Autonomy: Offer)Merlin: No, forget it.
(Direct: RejectOffer)Merlin: I must wipe the counter now.
(Direct: Offer)Genie: Alright, if you insist.
(Autonomy: AcceptOffer)Genie: Do you know that you?ve broken the baking dishmate?
(Approval: Inform)Merlin: Yes.
(Direct: AcceptInform)Merlin: I?ll cook the vegetables.
(Direct: Offer)Genie: No please don?t bother yourself.
(Autonomy: RejectOffer)Genie: The vegetables are healthy.
(Indirect: RequestAct)Merlin: Alright, I?ll add the vegetables to the pasta.
(Direct: AcceptRequest)Merlin: Could you please add the cheese to the pasta forme?
(Approval: RequestAct)Genie: Please don?t mind but I can?t do that.
(Autonomy: RejectRequest)Merlin: Alright.
(Direct: AcceptReject)Conversation 2: In this case Genie and Merlinare colleagues at an equal level in hierarchy.Agent (Speech act: Politeness strategy): UtteranceMerlin: Could you possibly clean the oven forme?
(Approval:RequestAct)Genie: Sure.
(Direct:AcceptRequest)Genie: I?ll clean the burner.
(Direct:Offer)Merlin: Ok. (Direct:AcceptOffer)Merlin: You?ve burnt the vegetables.
(Direct:Inform)Genie: Yeah.
(Direct:AcceptInform)Genie: Would you mind washing the dishes?
(Approval:RequestAct)Merlin: I?m sorry but I can?t wash the dishes.
(Approval:RejectRequest)Genie: Alright.
(Direct:AcceptReject)Genie: I must boil the pasta.
(Direct:Offer)Merlin: No, thanks.
(Direct:RejectOffer)Merlin: I can wipe the counter.
(Direct:Offer)Genie: Yeah you can.
(Direct:AcceptOffer)Genie: You?ve broken the baking dish.
(Direct:Inform)Merlin: Yes.
(Direct:AcceptInform)Merlin: I?ll cook the vegetables.
(Direct:Offer)Genie: No, forget it.
(Direct:RejectOffer)Merlin: Could you please add the vegetables to the pasta?
(Approval:RequestAct)Genie: Please don?t mind but I can?t do that.
(Approval:RejectRequest)Merlin: Ok. (Direct:AcceptReject)Genie: Will you please wipe the table mate?
(Approval:RequestAct)Merlin: Sure.
(Direct:AcceptRequest)9703 Virtual EnvironmentWe rendered POLLy with Microsoft Agent Char-acters (Microsoft, 1998) in our Virtual Environ-ment RAVE at Sheffield University as well as on adesktop computer screen.
RAVE consists of a 3-dimensional visualisation of computer-generatedscenes onto a 10ft x 8ft screen and a complete 3Dsurround sound system driven by a dedicated com-puter.
Since Microsoft Agents are 2D, they are notrendered 3D, but a life size image of the charactersis visible to the users on the screen to make themappear believable.
Figure 1 showed a user interact-ing with POLLy in RAVE.
The MS Agent packageprovides libraries to program control using variousdeveloping environments like the .NET frameworkand visual studio and includes a voice recognizerand a text-to-speech engine.
It also provides con-trols to embed predefined animations which makethe characters?
behaviour look more interesting andbelievable (Cassell & Th?risson, 1999).
We haveprogrammed MS agent in Visual C++ and haveembedded these animations like gesturing in a di-rection, looking towards the other agents, blinking,tilting the head, extending arms to the side, raisingeyebrows, looking up and down etc while theagents speak and listen to the utterances and hold-ing the hand to the ear, extending the ear, turningthe head left or right etc when the agents don?t un-derstand what the user says or the user doesn?tspeak anything.The Agents share the AI plan to collaborate onit together to achieve the cooking task.
Goals tocommunicate about the plan are also allocated tothe agents as speech acts (SAs) such as Requests,Offers, Informs, Acceptances and Rejections,needed to accomplish the plan collaboratively.While interacting with the system using a highquality microphone, the user sees one or twoagents on the screen and plays the part of the sec-ond or the third agent, as per the role given tohim/her.When we extend this to a real-time immersiveVirtual Reality environment, a Virtual Kitchen inthis case, the ECAs will actually perform the taskof cooking a recipe together in the virtual kitchenwhile conversing about the steps involved in doingso, as laid out by the AI plan.This setup makes it possible to design a 2x2x2experiment to test three conditions: Interactivity,i.e.
whether the user only sees the agents interact-ing on the screen vs. the user interacts with theagents by playing a role; immersiveness of the en-vironment, i.e.
rendering in RAVE vs. rendering ona desktop computer; and culture, i.e.
the differencebetween the perception of politeness by peoplefrom different cultures as in (Gupta et al, 2007).We are now in the process of completing the de-sign of this experiment and running it.4 ConclusionWe presents a demo of our conversational systemPOLLy which implements MS Agent characters ina VE and uses an AI Planning based shared repre-sentation for generating actions to be performed bythe agents and utterances to communicate aboutthe steps involved in performing the action.
Theutterances generated by POLLy are socially appro-priate in terms of their politeness level.
The userwill be given a role play situation and he/she willbe able to have a conversation with the agents on adesktop computer, where some dialogic utteranceswould be allocated to the user.
An evaluation ofPOLLy (Gupta et al 2007; Gupta et al 2008)showed that (1) politeness perceptions of POLLy?soutput are generally consistent with B&L?s predic-tions for choice of form for discourse situation, i.e.utterances to strangers or a superior person need tobe very polite, preferably autonomy oriented (2)our indirect strategies which should be the politestforms, are the rudest (3) English and Indian speak-ers of English have different perceptions of polite-ness (4) B&L implicitly state the equality of the P& D variables in their equation (?
= P + D + R),whereas we observe that not only their weights aredifferent as they appear to be subjectively deter-mined, but they are also not independent.ReferencesAndr?, E., Rist, T., Mulken, S.v., Klesen, M., & Baldes,S.
2000.
The automated design of believable dia-logues for animated presentation teams.
In EmbodiedConversational Agents (pp.
220?255).
MIT Press.Bersot, O., El-Guedj, P.O., God?ereaux, C. and Nugues.P.
1998.
A conversational agent to help navigation &collaboration in virtual worlds.
Virtual Real-ity,3(1):71?82.Blum, A., Furst, M. 1997.
Fast Planning Through Plan-ning Graph Analysis.
Artificial Intelligence 90.Cassell, J. and Th?risson, K.R.
1999.
The Power of aNod and a Glance: Envelope vs.
Emotional Feedback971in Animated Conversational Agents.
Applied ArtificialIntelligence 13: 519-538.Cassell, J. and Bickmore, Timothy W. Negotiated Col-lusion.
2003.
Modeling Social Language and its Re-lationship Effects in Intelligent Agents.
User Model.User-Adapt.Interact.
13(1-2):89-132.Culpeper, J.
1996.
(Im)politeness in dramatic dialogue.Exploring the Language of Drama: From text to con-text.
Routledge, London.Dale, R. and Reiter, E. 1995.
Building Natural Lan-guage Generation Systems.
Studies in Natural Lan-guage Processing.
Cambridge University Press.Dautenhahn, K. 2000.
Socially Intelligent Agents: TheHuman in the Loop (Papers from the 2000 AAAI FallSymposium).
The AAAI Press, Technical Report.Eichner, T., Prendinger, H., Andr?, E. and Ishizuka, M.2007.
Attentive presentation agents.
Proc.
7th Inter-national Conference on Intelligent Virtual Agents(IVA-07), Springer LNCS 4722. pp 283-295.Grosz, B.J., Sidner, C.L.
1990.
Plans for discourse.
In:Cohen, P.R., Morgan, J.L., Pollack, M.E.
(eds.)
In-tentions in Communication, MIT Press, Cambridge.Gupta, S., Walker, M.A., Romano, D.M.
2007.
HowRude are You?
: Evaluating Politeness and Affect inInteraction.
Affective Computing & Intelligent Inter-action (ACII-2007).Gupta , S., Walker, M.A., Romano, D.M.
2008 (to bepublished).
Using a Shared Representation to Gener-ate Action and Social Language for a Virtual Dia-logue Environment.
AAAI Spring Symposium onEmotion, Personality and Social Behavior.Johnson, L.W.
and Rizzo, P. and Bosma, W.E.
and Ghi-jsen, M. and van Welbergen, H. 2004.
Generatingsocially appropriate tutorial dialog.
In: ISCA Work-shop on Affective Dialogue Systems.
pp.
254-264.Johnson, L., Mayer, R., Andr?, E., & Rehm, M. 2005.Cross-cultural evaluation of politeness in tactics forpedagogical agents.
Proc.
of the 12th Int.
Conf.
onArtificial Intelligence in Education.Kopp, S., Jung, B., Lessmann, N. and Wachsmuth, I.2003.
Max ?
A multimodal assistant in virtual realityconstruction.
KI Zeitschift (German Magazine of Ar-tificial Intelligence), Special Issue on EmbodiedConversational Agents, vol.4, pp.11?17.Lavoie, B., and Rambow, O.
1997.
RealPro ?
a fast,portable sentence realizer.
In Proc.
Conference onApplied Natural Language Processing (ANLP?97).Microsoft.
1998.
Developing for Microsoft Agent.
Mi-crosoft Press.op den Akker, H.J.A.
and Nijholt, A.
2000.
Dialoguesfor Embodied Agents in Virtual Environments.
In:Natural Language Processing - NLP 2000, 2nd Int.Conf.
pp.
358-369.
LNAI 1835.Paiva, A., Dias, J., & Aylett, R.S.
2005.
Learning byfeeling: evoking empathy with synthetic characters.Applied Artificial Intelligence: 19 (3-4), 235-266.Poggi, I., Pelachaud, C., de Rosis, F., Carofiglio, V., DeCarolis, B.
2005.
GRETA.
A Believable EmbodiedConversational Agent.
in O.
Stock and M. Zanca-rano, eds, Multimodal Intelligent InformationPresentation, Kluwer.Prendinger, Helmut and Ishizuka, Mitsuru.
2001.
Let'stalk!
Socially intelligent agents for language conver-sation training.
IEEE Transactions on xSystems,Man, and Cybernetics - Part A: Systems and Hu-mans, Vol.
31, No.
5, pp 465-471.Porayska-Pomsta, K. 2003.
Influence of SituationalContext on Language Production: Modelling Teach-ers' Corrective Responses.
PhD Thesis.
School of In-formatics, University of Edinburgh.Rehm, M. and Andr?, E. 2007.
Informing the Design ofAgents by Corpus Analysis.
Conversational Informat-ics, Edited by T. Nishida.Sidner, C.L.
1994.
An artificial discourse language forcollaborative negotiation.
In: Proc.
12th NationalConf.
on AI, pp.
814?819.Takenobu, T., Tomofumi, K., Suguru, S., Manabu, O.2003.
Bridging the Gap between Language and Ac-tion.
IVA 2003, LNAI 2792, pp.
127-135.Traum, D., Rickel, J., Gratch, J., Marsella, S. 2003.
Ne-gotiation over Tasks in Hybrid Human-Agent Teamsfor Simulation-Based Training.
Proceedings of the2nd Int.
Joint Conf.
on Autonomous Agents and Mul-tiagent Systems.Walker, M.A.
1996.
The effect of resource limits andtask complexity on collaborative planning in dia-logue.
Artificial Intelligence Journal 85, 1?2.Walker, M., Cahn, J. and Whittaker, S. J.
1997.
Improv-ing linguistic style: Social and affective bases foragent personality.
In Proc.
Autonomous Agents'97.96?105.
ACM Press.Wang, N., Johnson, W.L., Rizzo, P., Shaw,E., & Mayer,R.
2005.
Experimental evaluation of polite interac-tion tactics for pedagogical agents.
Proceedings ofIUI ?05.
ACM Press.Watts, Richard J. Ide, S. and Ehlich, K. 1992.
Introduc-tion, in Watts, R, Ide, S. and Ehlich, K.
(eds.
), Po-liteness in Language: Studies in History, Theory andPractice.
Berlin: Mouton de Gruyter, pp.1-17.972
