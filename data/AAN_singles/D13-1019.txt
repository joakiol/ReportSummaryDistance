Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 182?192,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsJoint Learning of Phonetic Units and Word Pronunciations for ASRChia-ying Lee, Yu Zhang, James GlassComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of TechnologyCambridge, MA 02139, USA{chiaying,yzhang87,jrg}@csail.mit.eduAbstractThe creation of a pronunciation lexicon re-mains the most inefficient process in develop-ing an Automatic Speech Recognizer (ASR).In this paper, we propose an unsupervisedalternative ?
requiring no language-specificknowledge ?
to the conventional manual ap-proach for creating pronunciation dictionar-ies.
We present a hierarchical Bayesian model,which jointly discovers the phonetic inven-tory and the Letter-to-Sound (L2S) mappingrules in a language using only transcribeddata.
When tested on a corpus of spontaneousqueries, the results demonstrate the superior-ity of the proposed joint learning scheme overits sequential counterpart, in which the la-tent phonetic inventory and L2S mappings arelearned separately.
Furthermore, the recogniz-ers built with the automatically induced lexi-con consistently outperform grapheme-basedrecognizers and even approach the perfor-mance of recognition systems trained usingconventional supervised procedures.1 IntroductionModern automatic speech recognizers require a fewessential ingredients such as a signal representationof the speech signal, a search component, and typ-ically a set of stochastic models that capture 1) theacoustic realizations of the basic sounds of a lan-guage, for example, phonemes, 2) the realization ofwords in terms of these sounds, and 3) how wordsare combined in spoken language.
When creatinga speech recognizer for a new language the usualrequirements are: first, a large speech corpus withword-level annotations; second, a pronunciation dic-tionary that essentially defines a phonetic inventoryfor the language as well as word-level pronuncia-tions, and third, optional additional text data thatcan be used to train the language model.
Giventhese data and some decision about the signal rep-resentation, e.g., centi-second Mel-Frequency Cep-stral Coefficients (MFCCs) (Davis and Mermelstein,1980) with various derivatives, as well as the natureof the acoustic and language model such as 3-stateHMMs and n-grams, iterative training methods canbe used to effectively learn the model parameters forthe acoustic and language models.
Although the de-tails of the components have changed through theyears, this basic ASR formulation was well estab-lished by the late 1980?s, and has not really changedmuch since then.One of the interesting aspects of this formulationis the inherent dependence on the dictionary, whichdefines both the phonetic inventory of a language,and the pronunciations of all the words in the vo-cabulary.
The dictionary is arguably the cornerstoneof a speech recognizer as it provides the essentialtransduction from sounds to words.
Unfortunately,the dependency on this resource is a significant im-pediment to the creation of speech recognizers fornew languages, since they are typically created byexperts, whereas annotated corpora can be relativelymore easily created by native speakers of a language.The existence of an expert-derived dictionary inthe midst of stochastic speech recognition models issomewhat ironic, and it is natural to ask why it con-tinues to receive special status after all these years.Why can we not learn the inventory of sounds of alanguage and associated word pronunciations auto-matically, much as we learn our acoustic model pa-rameters?
If successful, we would move one stepforward towards breaking the language barrier that182limits us from having speech recognizers for all lan-guages of the world, instead of the less than 2% thatcurrently exist.In this paper, we investigate the problem of infer-ring a pronunciation lexicon from an annotated cor-pus without exploiting any language-specific knowl-edge.
We formulate our approach as a hierarchi-cal Bayesian model, which jointly discovers theacoustic inventory and the latent encoding schemebetween the letters and the sounds of a language.We evaluate the quality of the induced lexicon andacoustic model through a series of speech recogni-tion experiments on a conversational weather querycorpus (Zue et al 2000).
The results demonstratethat our model consistently generates close perfor-mance to recognizers that are trained with expert-defined phonetic inventory and lexicon.
Comparedto grapheme-based recognizers, our model is capa-ble of improving the Word Error Rates (WERs) byat least 15.3%.
Finally, the joint learning frameworkproposed in this paper is proven to be much moreeffective than modeling the acoustic units and theletter-to-sound mappings separately, as shown in a45% WER deduction our model achieves comparedto a sequential approach.2 Related WorkVarious algorithms for learning sub-word based pro-nunciations were proposed in (Lee et al 1988;Fukada et al 1996; Bacchiani and Ostendorf, 1999;Paliwal, 1990).
In these previous approaches, spo-ken samples of a word are gathered, and usuallyonly one single pronunciation for the word is de-rived based on the acoustic evidence observed in thespoken samples.
The major difference between ourwork and these previous works is that our modellearns word pronunciations in the context of lettersequences.
More specifically, our model learns letterpronunciations first and then concatenates the pro-nunciation of each letter in a word to form the wordpronunciation.
The advantage of our approach isthat pronunciation knowledge learned for a particu-lar letter in some arbitrary word can subsequently beused to help learn the letter?s pronunciation in otherwords.
This property allows our model to potentiallylearn better pronunciations for less frequent words.The more recent work by Garcia and Gish (2006)and Siu et al(2013) has made extensive useof self-organizing units for keyword spotting andother tasks for languages with limited linguisticresources.
Others who have more recently ex-plored the unsupervised space include (Varadarajanet al 2008; Jansen and Church, 2011; Lee andGlass, 2012).
The latter work introduced a non-parametric Bayesian inference procedure for auto-matically learning acoustic units that is most similarto our current work except that our model also infersword pronunciations simultaneously.The concept of creating a speech recognizer fora language with only orthographically annotatedspeech data has also been explored previously bymeans of graphemes.
This approach has been shownto be effective for alphabetic languages with rela-tively straightforward grapheme to phoneme trans-formations and does not require any unsupervisedlearning of units or pronunciations (Killer et al2003; Stu?ker and Schultz, 2004).
As we explain inlater sections, grapheme-based systems can actuallybe regarded as a special case of our model; therefore,we expect our model to have greater flexibilities forcapturing pronunciation rules of graphemes.3 ModelThe goal of our model is to induce a word pronunci-ation lexicon from spoken utterances and their cor-responding word transcriptions.
No other language-specific knowledge is assumed to be available, in-cluding the phonetic inventory of the language.
Toachieve the goal, our model needs to solve the fol-lowing two tasks:?
Discover the phonetic inventory.?
Reveal the latent mapping between the lettersand the discovered phonetic units.We propose a hierarchical Bayesian model forjointly discovering the two latent structures froman annotated speech corpus.
Before presenting ourmodel, we first describe the key latent and observedvariables of the problem.Letter (lmi ) We use lmi to denote the ith let-ter observed in the word transcription of themth training sample.
To be sure, a train-ing sample involves a speech utterance and its183corresponding text transcription.
The letter se-quence composed of lmi and its context, namelylmi?
?, ?
?
?
, lmi?1, lmi , lmi+1, ?
?
?
, lmi+?, is denoted as ~lmi,?.Although lmi is referred to as a letter in this paper,it can represent any character observed in the textdata, including space and symbols indicating sen-tence boundaries.
The set of unique characters ob-served in the data set is denoted as G. For notationsimplicity, we use L?
to denote the set of letter se-quences of length 2?
+ 1 that appear in the datasetand use ~l?
to denote the elements in L?.
Finally,P(~l?)
is used to represent the parent of ~l?, which isa substring of ~l?
with the first and the last characterstruncated.Number of Mapped Acoustic Units (nmi ) Eachletter lmi in the transcriptions is assumed to bemapped to a certain number of phonetic units.
Forexample, the letter x in the word fox is mapped to2 phonetic units /k/ and /s/, while the letter e in theword lake is mapped to 0 phonetic units.
We denotethis number as nmi and limit its value to be 0, 1 or 2in our model.
The value of nmi is always unobservedand needs to be inferred by the our model.Identity of the Acoustic Unit (cmi,p) For each pho-netic unit that lmi maps to, we use cmi,p, for 1 ?
p ?nmi , to denote the identity of the phonetic unit.
Notethat the phonetic inventory that describes the dataset is unknown to our model, and the identities ofthe phonetic units are associated with the acousticunits discovered automatically by our model.Speech Feature xmt The observed speech data inour problem are converted to a series of 25 ms 13-dimensional MFCCs (Davis and Mermelstein, 1980)and their first- and second-order time derivatives ata 10 ms analysis rate.
We use xmt ?
R39 to denotethe tth feature frame of the mth utterance.3.1 Generative ProcessWe present the generative process for a single train-ing sample (i.e., a speech utterance and its corre-sponding text transcription); to keep notation sim-ple, we discard the index variable m in this section.For each li in the transcription, the model gener-ates ni, given ~li,?, from the 3-dimensional categori-cal distribution ?~li,?(ni).
Note that for every unique~li,?
letter sequence, there is an associated ?~li,?(ni)lj1?
p ?
ni?0ci, p?0K?cdi,p?1 ?
i ?
Lmnixt1 ?
m ?
M?l2,n,p?
?
?l,n,pG ?
{(n,p) | 0 ?
n ?
2, 1 ?
p ?
n}?l1,n,pG ?GG ?G?1?2i-2 ?
j ?
i+2Figure 1: The graphical representation of the pro-posed hierarchical Bayesian model.
The shaded cir-cle denotes the observed text and speech data, andthe squares denote the hyperparameters of the priorsin our model.
See Sec.
3 for a detailed explanationof the generative process of our model.distribution, which captures the fact that the numberof phonetic units a letter maps to may depend on itscontext.
In our model, we impose a Dirichlet distri-bution prior Dir(?)
on ?~li,?
(ni).If ni = 0, li is not mapped to any acoustic unitsand the generative process stops for li; otherwise,for 1 ?
p ?
ni, the model generates ci,p from:ci,p ?
pi~li,?,ni,p (1)where pi~li,?,ni,p is a K-dimensional categorical dis-tribution, whose outcomes correspond to the pho-netic units discovered by the model from the givenspeech data.
Eq.
1 shows that for each combinationof~li,?, ni and p, there is an unique categorical distri-bution.
An important property of these categoricaldistributions is that they are coupled together suchthat their outcomes point to a consistent set of pho-netic units.
In order to enforce the coupling, we con-struct pi~li,?,ni,p through a hierarchical process.?
?
Dir(?)
(2)pi~li,?,ni,p ?
Dir(???)
for ?
= 0 (3)pi~li,?,ni,p ?
Dir(??pi~li,?
?1,ni,p) for ?
?
1 (4)184To interpret Eq.
2 to Eq.
4, we envision thatthe observed speech data are generated by a K-component mixture model, of which the componentscorrespond to the phonetic units in the language.
Asa result, ?
in Eq.
2 can be viewed as the mixtureweight over the components, which indicates howlikely we are to observe each acoustic unit in thedata overall.
By adopting this point of view, wecan also regard the mapping between li and the pho-netic units as a mixture model, and pili,ni,p1 repre-sents how probable li is mapped to each phoneticunit given ni and p. We apply a Dirichlet distribu-tion prior parametrized by ?0?
to pili,ni,p as shownin Eq.
3.
With this parameterization, the mean ofpili,ni,p is the global mixture weight ?, and ?0 con-trols how similar pili,ni,p is to the mean.
More specif-ically, for large ?0  K, the Dirichlet distributionis highly peaked around the mean; on the contrary,for ?0  K, the mean lies in a valley.
The parame-ters of a Dirichlet distribution can also be viewed aspseudo-counts for each category.
Eq.
4 shows thatthe prior for pi~li,?,ni,p is seeded by pseudo-countsthat are proportional to the mapping weights overthe phonetic units of li in a shorter context.
In otherwords, the mapping distribution of li in a shortercontext can be thought of as a back-off distributionof li?s mapping weights in a longer context.Each component of the K-dimensional mixturemodel is linked to a 3-state Hidden Markov Model(HMM).
These K HMMs are used to model thephonetic units in the language (Jelinek, 1976).
Theemission probability of each HMM state is modeledby a diagonal Gaussian Mixture Model (GMM).
Weuse ?c to represent the set of parameters that definethe cth HMM, which includes the state transitionprobability and the GMM parameters of each stateemission distribution.
The conjugate prior of ?c isdenoted as H(?0)2.Finally, to finish the generative process, for eachci,p we use the corresponding HMM ?ci,p to gen-erate the observed speech data xt, and the genera-tive process of the HMM determines the duration,1An abbreviation of pi~li,0,ni,p2H(?0) includes a Dirichlet prior for the transition probabil-ity of each state, and a Dirichlet prior for each mixture weightof the three GMMs, and a normal-Gamma distribution for themean and precision of each Gaussian mixture in the 3-stateHMM.di,p, of the speech segment.
The complete genera-tive model, with ?
set to 2, is depicted in Fig.
1; Mis the total number of transcribed utterances in thecorpus, and Lm is the number of letters in utterancem.
The shaded circles denote the observed data, andthe squares denote the hyperparameters of the priorsused in our model.
Lastly, the unshaded circles de-note the latent variables of our model, for which wederive inference algorithms in the next section.4 InferenceWe employ Gibbs sampling (Gelman et al 2004) toapproximate the posterior distribution of the latentvariables in our model.
In the following sections, wefirst present a message-passing algorithm for block-sampling ni and ci,p, and then describe how weleverage acoustic cues to accelerate the computa-tion of the message-passing algorithm.
Note that theblock-sampling algorithm for ni and ci,p can be par-allelized across utterances.
Finally, we briefly dis-cuss the inference procedures for ?~l?
, pi~l?,n,p, ?, ?c.4.1 Block-sampling ni and ci,pTo understand the message-passing algorithm in thisstudy, it is helpful to think of our model as a sim-plified Hidden Semi-Markov Model (HSMM), inwhich the letters represent the states and the speechfeatures are the observations.
However, unlike ina regular HSMM, where the state sequence is hid-den, in our case, the state sequence is fixed to be thegiven letter sequence.
With this point of view, wecan modify the message-passing algorithms of Mur-phy (2002) and Johnson and Willsky (2013) to com-pute the posterior information required for block-sampling ni and ci,p.Let L(xt) be a function that returns the indexof the letter from which xt is generated; also, letFt = 1 be a tag indicating that a new phone segmentstarts at t+ 1.
Given the constraint that 0 ?
ni ?
2,for 0 ?
i ?
Lm and 0 ?
t ?
Tm, the backwardsmessages Bt(i) and B?t (i) for the mth training sam-ple can be defined and computed as in Eq.
5 andEq.
7.
Note that for clarity we discard the index vari-able m in the derivation of the algorithm.185Bt(i) , p(xt+1:T |L(xt) = i, Ft = 1)=min{L,i+1+U}?j=i+1B?t (j)j?1?k=i+1p(nk = 0|~li,?
)=min{L,i+1+U}?j=i+1B?t (j)j?1?k=i+1?~li,?
(0) (5)B?t (i) , p(xt+1:T |L(xt+1) = i, Ft = 1)=T?t?d=1p(xt+1:t+d|~li,?
)Bt+d(i) (6)=T?t?d=1{K?ci,1=1?~li,?(1)pi~li,?,1,1(ci,1)p(xt+1:t+d|?ci,1)+d?1?v=1K?ci,1K?ci,2?~li,?(2)pi~li,?,2,1(ci,1)pi~li,?,2,2(ci,2)?
p(xt+1:t+v|?ci,1)p(xt+v+1:t+d|?ci,2)}Bt+d(i)(7)We use xt1:t2 to denote the segment consisting ofxt1 , ?
?
?
, xt2 .
Our inference algorithm only allowsup to U letters to emit 0 acoustic units in a row.
Thevalue of U is set to 2 for our experiments.
Bt(i)represents the total probability of all possible align-ments between xt+1:T and li+1:L. B?t (i) containsthe probability of all the alignments between xt+1:Tand li+1:L that map xt+1 to li particularly.
Thisalignment constraint between xt+1 and li is explic-itly shown in the first term of Eq.
6, which representshow likely the speech segment xt+1:t+d is generatedby li given li?s context.
This likelihood is simplythe marginal probability of p(xt+1:t+d, ni, ci,p|~li,?
)with ni and ci,p integrated out, which can be ex-panded and computed as shown in the last three rowsof Eq.
7.
The index v specifies where the phoneboundary is between the two acoustic units that liis aligned with when ni = 2.
Eq.
8 to Eq.
10 arethe boundary conditions of the message passing al-gorithm.
B0(0) carries the total probably of all pos-sible alignments between l1:L and x1:T .
Eq.
9 spec-ifies that at most U letters at the end of an sentencecan be left unaligned with any speech features, whileEq.
10 indicates that all of the speech features in anutterance must be assigned to a letter.Algorithm 1 Block-sample ni and ci,p from Bt(i)and B?t (i)1: i?
02: t?
03: while i < L ?
t < T do4: nexti ?
SampleFromBt(i)5: if nexti > i+ 1 then6: for k = i+ 1 to k = nexti ?
1 do7: nk ?
08: end for9: end if10: d, ni, ?ci,p?, v ?
SampleFromB?t (nexti)11: t?
t+ d12: i?
nexti13: end whileB0(0) =min{L,U+1}?j=1B?0(j)j?1?k=1?~li,?
(0) (8)BT (i) ,????
?1 if i = L?Lj=i+1 ?~li,?
(0) if L?
U ?
i < L0 if i < L?
U(9)Bt(L) ,{1 if t = T0 otherwise(10)Given Bt(i) and B?t (i), ni and ci,p for each letterin the utterance can be sampled using Alg.
1.
TheSampleFromBt(i) function in line 4 returns a ran-dom sample from the relative probability distribu-tion composed by entries of the summation in Eq.
5.Line 5 to line 9 check whether li (and maybe li+1)is mapped to zero phonetic units.
nexti points tothe letter that needs to be aligned with 1 or 2 phonesegments starting from xt.
The number of phoneticunits that lnexti maps to and the identities of theunits are sampled in SampleFromB?t (i).
This sub-routine generates a tuple of d, ni, ?ci,p?
as well asv (if ni = 2) from all the entries of the summationshown in Eq.
73.3We use ?ci,p?
to denote that ?ci,p?may consist of two num-bers, ci,1 and ci,2, when ni = 2.1864.2 Heuristic Phone Boundary EliminationThe variables d and v in Eq.
7 enumerate throughevery frame index in a sentence, treating each fea-ture frame as a potential boundary between acous-tic units.
However, it is possible to exploit acousticcues to avoid checking feature frames that are un-likely to be phonetic boundaries.
We follow the pre-segmentation method described in Glass (2003) toskip roughly 80% of the feature frames and greatlyspeed up the computation of B?t (i).Another heuristic applied to our algorithm to re-duce the search space for d and v is based on theobservation that the average duration of phoneticunits is usually no longer than 300 ms. Therefore,when computing B?t (i), we only consider speechsegments that are shorter than 300 ms to avoid align-ing letters to speech segments that are too long to bephonetic units.4.3 Sampling ?~l?
, pi~l?,ni,p, ?
and ?cSampling ?~l?
To compute the posterior distribu-tion of ?~l?
, we count how many times~l?
is mappedto 0, 1 and 2 phonetic units from nmi .
More specifi-cally, we define N~l?
(j) for 0 ?
j ?
2 as follows:N~l?
(j) =M?m=1Lm?i=1?
(nmi , j)?(~lmi,?,~l?
)where we use ?(?)
to denote the discrete Kroneckerdelta.
With N~l?
, we can simply sample a new valuefor ?~l?
from the following distribution:?~l?
?
Dir(?
+N~l?
)Sampling pi~l?,n,p and ?
The posterior distribu-tions of pi~l?,n,p and ?
are constructed recursively dueto the hierarchical structure imposed on pi~l?,n,p and?.
We start with gathering counts for updating thepi variables at the lowest level, i.e., pi~l2,n,p given that?
is set to 2 in our model implementation, and thensample pseudo-counts for the pi variables at higherhierarchies as well as ?.
With the pseudo-counts, anew ?
can be generated, which allows pi~l?,n,p to bere-sampled sequentially.More specifically, we define C~l2,n,p(k) to be thenumber of times that ~l2 is mapped to n units andthe unit in position p is the kth phonetic unit.
Thisvalue can be counted from the current values of cmi,pas follows.C~l2,n,p(k) =M?m=1Lm?i=1?(~li,2,~l2)?
(nmi , n)?
(cmi,p, k)To derive the posterior distribution of pi~l1,n,p an-alytically, we need to sample pseudo-counts C~l1,n,p,which is defined as follows.C~l1,n,p(k) =?~l2?U~l1C~l2,n,p(k)?i=1I[?i <?2pi~l1,n,p(k)i+ ?2pi~l1,n,p(k)](11)We use U~l1 = {~l2|P(~l2) = ~l1} to denote the set of~l2 whose parent is~l1 and ?i to represent random vari-ables sampled from a uniform distribution between0 and 1.
Eq.
11 can be applied recursively to com-pute C~l0,n,p(k) and C ,n,p(k), the pseudo-counts thatare applied to the conjugate priors of pi~l0,n,p and ?.With the pseudo-count variables computed, new val-ues for ?
and pi~l?,n,p can be sampled sequentially asshown in Eq.
12 to Eq.
14.?
?
Dir(?
+ C ,n,p) (12)pi~l?,n,p ?
Dir(???
+ C~l?,n,p) for ?
= 0 (13)pi~l?,n,p ?
Dir(??pi~l?
?1,n,p + C~l?,n,p) for ?
?
1(14)5 Experimental SetupTo test the effectiveness of our model for joint learn-ing phonetic units and word pronunciations from anannotated speech corpus, we construct speech rec-ognizers out of the training results of our model.The performance of the recognizers is evaluated andcompared against three baselines: first, a grapheme-based speech recognizer; second, a recognizer builtby using an expert-crafted lexicon, which is referredto as an expert lexicon in the rest of the paper forsimplicity; and third, a recognizer built by discover-ing the phonetic units and L2S pronunciation rulessequentially without using a lexicon.
In this section,we provide a detailed description of the experimen-tal setup.187?
?
?0 ?1 ?2 ?0 ?
K?0.1?3 ?10?100 1 0.1 0.2 * 2 100Table 1: The values of the hyperparameters of ourmodel.
We use ?a?D to denote aD-dimensional vec-tor with all entries being a.
*We follow the proce-dure reported in (Lee and Glass, 2012) to set up theHMM prior ?0.5.1 DatasetAll the speech recognition experiments reported inthis paper are performed on a weather query dataset,which consists of narrow-band, conversational tele-phone speech (Zue et al 2000).
We follow the ex-perimental setup of McGraw et al(2013) and splitthe corpus into a training set of 87,351 utterances, adev set of 1,179 utterances and a test set of 3,497 ut-terances.
A subset of 10,000 utterances is randomlyselected from the training set.
We use this subset ofdata for training our model to demonstrate that ourmodel is able to discover the phonetic compositionand the pronunciation rules of a language even fromjust a few hours of data.5.2 Building a Recognizer from Our ModelThe values of the hyperparameters of our model arelisted in Table 1.
We run the inference procedure de-scribed in Sec.
4 for 10,000 times on the randomlyselected 10,000 utterances.
The samples of ?~l?
andpi~l?n,p from the last iteration are used to decode nmiand cmi,p for each sentence in the entire training set byfollowing the block-sampling algorithm describedin Sec.
4.1.
Since cmi,p is the phonetic mapping oflmi , by concatenating the phonetic mapping of ev-ery letter in a word, we can obtain a pronunciationof the word represented in the labels of discoveredphonetic units.
For example, assume that word wappears in sentence m and consists of l3l4l5 (thesentence index m is ignored for simplicity).
Also,assume that after decoding, n3 = 1, n4 = 2 andn5 = 1.
A pronunciation ofw is then encoded by thesequence of phonetic labels c3,1c4,1c4,2c5,1.
By re-peating this process for each word in every sentencefor the training set, a list of word pronunciations canbe compiled and used as a stochastic lexicon to builda speech recognizer.In theory, the HMMs inferred by our model can bedirectly used as the acoustic model of a monophonespeech recognizer.
However, if we regard the ci,plabels of each utterance as the phone transcriptionof the sentence, then a new acoustic model can beeasily re-trained on the entire data set.
More conve-niently, the phone boundaries corresponding to theci,p labels are the by-products of the block-samplingalgorithm, which are indicated by the values of d andv in line 10 of Alg.
1 and can be easily saved duringthe sampling procedure.
Since these data are readilyavailable, we re-build a context-independent modelon the entire data set.
In this new acoustic model,a 3-state HMM is used to model each phonetic unit,and the emission probability of each state is modeledby a 32-mixture GMM.Finally, a trigram language model is built by usingthe word transcriptions in the full training set.
Thislanguage model is utilized in all speech recogni-tion experiments reported in this paper.
Finite StateTransducers (FSTs) are used to build all the recog-nizers used in this study.
With the language model,the lexicon and the context-independent acousticmodel constructed by the methods described in thissection, we can build a speech recognizer fromthe learning output of the proposed model withoutthe need of a pre-defined phone inventory and anyexpert-crafted lexicons.5.2.1 Pronunciation Mixture Model RetrainingMcGraw et al(2013) presented the Pronuncia-tion Mixture Model (PMM) for composing stochas-tic lexicons that outperform pronunciation dictionar-ies created by experts.
Although the PMM frame-work was designed to incorporate and augment ex-pert lexicons, we found that it can be adapted to pol-ish the pronunciation list generated by our model.In particular, the training procedure for PMMs in-cludes three steps.
First, train a L2S model froma manually specified expert-pronunciation lexicon;second, generate a list of pronunciations for eachword in the dataset using the L2S model; and finally,use an acoustic model to re-weight the pronuncia-tions based on the acoustic scores of the spoken ex-amples of each word.To adapt this procedure for our purposes, we sim-ply plug in the word pronunciations and the acous-tic model generated by our model.
Once we ob-tain the re-weighted lexicon, we re-generate forced188phone alignments and retrain the acoustic model,which can be utilized to repeat the PMM lexicon re-weighting procedure.
For our experiments, we it-erate through this model refining process until therecognition performance converges.5.2.2 Triphone ModelConventionally, to train a context-dependentacoustic model, a list of questions based on thelinguistic properties of phonetic units is requiredfor growing decision tree classifiers (Young et al1994).
However, such language-specific knowledgeis not available for our training framework; there-fore, our strategy is to compile a question list thattreats each phonetic unit as a unique linguistic class.In other words, our approach to training a context-dependent acoustic model for the automatically dis-covered units is to let the decision trees grow fullybased on acoustic evidence.5.3 BaselinesWe compare the recognizers trained by followingthe procedures described in Sec.
5.2 against threebaselines.
The first baseline is a grapheme-basedspeech recognizer.
We follow the procedure de-scribed in Killer et al(2003) and train a 3-stateHMM for each grapheme, which we refer to as themonophone grapheme model.
Furthermore, we cre-ate a singleton question set (Killer et al 2003), inwhich each grapheme is listed as a question, to traina triphone grapheme model.
Note that to enforcebetter initial alignments between the graphemes andthe speech data, we use a pre-trained acoustic modelto identify the non-speech segments at the beginningand the end of each utterance before starting trainingthe monophone grapheme model.Our model jointly discovers the phonetic inven-tory and the L2S mapping rules from a set of tran-scribed data.
An alternative of our approach is tolearn the two latent structures sequentially.
We fol-low the training procedure of Lee and Glass (2012)to learn a set of acoustic models from the speechdata and use these acoustic models to generate aphone transcription for each utterance.
The phonetranscriptions along with the corresponding wordtranscriptions are fed as inputs to the L2S modelproposed in Bisani and Ney (2008).
A stochasticlexicon can be learned by applying the L2S modelunit(%) MonophoneOur model 17.0Oracle 13.8Grapheme 32.7Sequential model 31.4Table 2: Word error rates generated by the fourmonophone recognizers described in Sec.
5.2 andSec.
5.3 on the weather query corpus.and the discovered acoustic models to PMM.
Thistwo-stage approach for training a speech recognizerwithout an expert lexicon is referred to as the se-quential model in this paper.Finally, we compare our system against a rec-ognizer trained from an oracle recognition system.We build the oracle recognizer on the same weatherquery corpus by following the procedure presentedin McGraw et al(2013).
This oracle recognizer isthen applied to generate forced-aligned phone tran-scriptions for the training utterances, from whichwe can build both monophone and triphone acous-tic models.
The expert-crafted lexicon used in theoracle recognizer is also used in this baseline.
Notethat for training the triphone model, we compose asingleton question list (Killer et al 2003) that hasevery expert-defined phonetic unit as a question.
Weuse this singleton question list instead of a more so-phisticated one to ensure that this baseline and oursystem differ only in the acoustic model and the lex-icon used to generate the initial phone transcriptions.We call this baseline the oracle baseline.6 Results and Analysis6.1 Monophone SystemsTable 2 shows the WERs produced by the fourmonophone recognizers described in Sec.
5.2 andSec.
5.3.
It can be seen that our model outper-forms the grapheme and the sequential model base-lines significantly while approaching the perfor-mance of the supervised oracle baseline.
The im-provement over the sequential baseline demonstratesthe strength of the proposed joint learning frame-work.
More specifically, unlike the sequential base-line, in which the acoustic units are discovered in-dependently from the text data, our model is able toexploit the L2S mapping constraints provided by theword transcriptions to cluster speech segments.189By comparing our model to the grapheme base-line, we can see the advantage of modeling thepronunciations of a letter using a mixture model,especially for a language like English which hasmany pronunciation irregularities.
However, evenfor languages with straightforward pronunciationrules, the concept of modeling letter pronunciationsusing mixture models still applies.
The main dif-ference is that the mixture weights for letters oflanguages with simple pronunciation rules will besparser and spikier.
In other words, in theory, ourmodel should always perform comparable to, if notbetter than, grapheme recognizers.Last but not least, the recognizer trained with theautomatically induced lexicon performs similarly tothe recognizer initialized by an oracle recognitionsystem, which demonstrates the effectiveness of theproposed model for discovering the phonetic inven-tory and a pronunciation lexicon from an annotatedcorpus.
In the next section, we provide some in-sights into the quality of the learned lexicon andinto what could have caused the performance gapbetween our model and the conventionally trainedrecognizer.6.2 Pronunciation EntropyThe major difference between the recognizer that istrained by using our model and the recognizer thatis seeded by an oracle recognition system is thatthe former uses an automatically discovered lexicon,while the latter exploits an expert-defined pronun-ciation dictionary.
In order to quantify, as well asto gain insights into, the difference between thesetwo lexicons, we define the average pronunciationentropy, H?
, of a lexicon as follows.H?
?
?1|V |?w?V?b?B(w)p(b) log p(b) (15)where V denotes the vocabulary of a lexicon, B(w)represents the set of pronunciations of a word wand p(b) stands for the weight of a certain pronun-ciation b.
Intuitively, we can regard H?
as an in-dicator of how much pronunciation variation thateach word in a lexicon has on average.
Table 3shows that the H?
values of the lexicon induced byour model and the expert-defined lexicon as well asOur model PMM iterations(Discovered lexicon) 0 1 2H?
(bit) 4.58 3.47 3.03WER (%) 17.0 16.6 15.9Oracle PMM iterations(Expert lexicon) 0 1 2H?
(bit) 0.69 0.90 0.92WER (%) 13.8 12.8 12.4Table 3: The upper-half of the table shows the aver-age pronunciation entropies, H?
, of the lexicons in-duced by our model and refined by PMM as wellas the WERs of the monophone recognizers builtwith the corresponding lexicons for the weatherquery corpus.
The definition of H?
can be found inSec.
6.2.
The first row of the lower-half of the ta-ble lists the average pronunciation entropies, H?
, ofthe expert-defined lexicon and the lexicons gener-ated and weighted by the L2P-PMM framework de-scribed in McGraw et al(2013).
The second row ofthe lower-half of the table shows the WERs of therecognizers that are trained with the expert-lexiconand its PMM-refined versions.their respective PMM-refined versions4.
In Table 3,we can see that the automatically-discovered lexi-con and its PMM-reweighted versions have muchhigher H?
values than their expert-defined counter-parts.
These higher H?
values imply that the lexiconinduced by our model contains more pronunciationvariation than the expert-defined lexicon.
Therefore,the lattices constructed during the decoding processfor our recognizer tend to be larger than those con-structed for the oracle baseline, which explains theperformance gap between the two systems in Table 2and Table 3.As shown in Table 3, even though the lexiconinduced by our model is noisier than the expert-defined dictionary, the PMM retraining frameworkconsistently refines the induced lexicon and im-proves the performance of the recognizers5.
To thebest of our knowledge, we are the first to applyPMM to lexicons that are created by a fully unsu-4We build the PMM-refined version of the expert-definedlexicon by following the L2P-PMM framework describedin McGraw et al(2013).5The recognition results all converge in 2 ?
3 PMM retrain-ing iterations.190pronunciationspronunciation probabilitiesOur model 1 PMM 2 PMM93 56 87 39 19 0.125 - -93 56 61 87 73 99 0.125 - -11 56 61 87 73 99 0.125 0.400 0.41993 20 75 87 17 27 52 0.125 0.125 0.12455 93 56 61 87 73 84 19 0.125 0.220 0.21093 26 61 87 49 0.125 0.128 0.14063 83 86 87 73 53 19 0.125 - -93 26 61 87 61 0.125 0.127 0.107Table 4: Pronunciation lists of the word Burma pro-duced by our model and refined by PMM after 1 and2 iterations.pervised method.
Therefore, in this paper, we pro-vide further analysis on how PMM helps enhancethe performance of our model.We compare the pronunciation lists for the wordBurma generated by our model and refined itera-tively by PMM in Table 4.
The first column of Ta-ble 4 shows all the pronunciations of Burma dis-covered by our model, to which our model assignsequal probabilities to create a stochastic list6.
Asdemonstrated in the third and the fourth columns ofTable 4, the PMM framework is able to iterativelyre-distribute the pronunciation weights and filter outless-likely pronunciations, which effectively reducesboth the size and the entropy of the stochastic lexi-con generated by our model.
The benefits of usingthe PMM to refine the induced lexicon are twofold.First, the search space constructed during the recog-nition decoding process with the refined lexicon ismore constrained, which is the main reason why thePMM is capable of improving the performance ofthe monophone recognizer that is trained with theoutput of our model.
Secondly, and more impor-tantly, the refined lexicon can greatly reduce the sizeof the FST built for the triphone recognizer of ourmodel.
These two observations illustrate why thePMM framework can be an useful tool for enhancingthe lexicon discovered automatically by our model.6.3 Triphone SystemsThe best monophone systems of the grapheme base-line, the oracle baseline and our model are used to6It is also possible to assign probabilities proportional to thedecoding scores of the word tokens.Unit(%) TriphoneOur model 13.4Oracle 10.0Grapheme 15.7Table 5: Word error rates of the triphone recogniz-ers.
The triphone recognizers are all built by us-ing the phone transcriptions generated by their bestmonohpone system.
For the oracle initialized base-line and for our model, the PMM-refined lexiconsare used to build the triphone recognizers.generate forced-aligned phone transcriptions, whichare used to train the triphone models described inSec.
5.2.2 and Sec.
5.3.
Table 5 shows the WERsof the triphone recognition systems.
Note that if amore conventional question list, for example, a listthat contains rules to classify phones into differentbroad classes, is used to build the oracle triphonesystem, the WER can be reduced to 6.5%.
However,as mentioned earlier, in order to gain insights intothe quality of the induced lexicon and the discoveredphonetic set, we compare our model against an ora-cle triphone system that is built by using a singletonquestion set.By comparing Table 2 and Table 5, we can seethat the grapheme triphone improves by a large mar-gin compared to its monophone counterpart, whichis consistent with the results reported in (Killer etal., 2003).
However, even though the graphemebaseline achieves a great performance gain withcontext-dependent acoustic models, the recognizertrained using the lexicon learned by our model andsubsequently refined by PMM still outperforms thegrapheme baseline.
The consistently better perfor-mance our model achieves over the grapheme base-line demonstrates the strength of modeling the pro-nunciation of each letter with a mixture model thatis presented in this paper.Last but not least, by comparing Table 2 andTable 5, it can be seen that the relative perfor-mance gain achieved by our model is similar tothat obtained by the oracle baseline.
Both Table 2and Table 5 show that even without exploiting anylanguage-specific knowledge during training, ourrecognizer is able to perform comparably with therecognizer trained using an expert lexicon.
The abil-ity of our model to obtain such similar performance191further supports the effectiveness of the joint learn-ing framework proposed in this paper for discover-ing the phonetic inventory and the word pronuncia-tions from simply an annotated speech corpus.7 ConclusionWe present a hierarchical Bayesian model for si-multaneously discovering acoustic units and learn-ing word pronunciations from transcribed spoken ut-terances.
Both monophone and triphone recogniz-ers can be built on the discovered acoustic units andthe inferred lexicon.
The recognizers trained withthe proposed unsupervised method consistently out-performs grapheme-based recognizers and approachthe performance of recognizers trained with expert-defined lexicons.
In the future, we plan to apply thistechnology to develop ASRs for more languages.AcknowledgementsThe authors would like to thank Ian McGraw andEkapol Chuangsuwanich for their advice on thePMM and recognition experiments presented in thispaper.
Thanks to the anonymous reviewers for help-ful comments.
Finally, the authors would like tothank Stephen Shum for proofreading and editingthe early drafts of this paper.ReferencesMichiel Bacchiani and Mari Ostendorf.
1999.
Joint lexi-con, acoustic unit inventory and model design.
SpeechCommunication, 29:99 ?
114.Maximilian Bisani and Hermann Ney.
2008.
Joint-sequence models for grapheme-to-phoneme conver-sion.
Speech Communication, 50(5):434?451, May.Steven B. Davis and Paul Mermelstein.
1980.
Com-parison of parametric representations for monosyllabicword recognition in continuously spoken sentences.IEEE Trans.
on Acoustics, Speech, and Signal Pro-cessing, 28(4):357?366.Toshiaki Fukada, Michiel Bacchiani, Kuldip Paliwal, andYoshinori Sagisaka.
1996.
Speech recognition basedon acoustically derived segment units.
In Proceedingsof ICSLP, pages 1077 ?
1080.Alvin Garcia and Herbert Gish.
2006.
Keyword spottingof arbitrary words using minimal speech resources.
InProceedings of ICASSP, pages 949?952.Andrew Gelman, John B. Carlin, Hal S. Stern, and Don-ald B. Rubin.
2004.
Bayesian Data Analysis.
Textsin Statistical Science.
Chapman & Hall/CRC, secondedition.James Glass.
2003.
A probabilistic framework forsegment-based speech recognition.
Computer Speechand Language, 17:137 ?
152.Aren Jansen and Kenneth Church.
2011.
Towards un-supervised training of speaker independent acousticmodels.
In Proceedings of INTERSPEECH, pages1693 ?
1696.Frederick Jelinek.
1976.
Continuous speech recogni-tion by statistical methods.
Proceedings of the IEEE,64:532 ?
556.Matthew J. Johnson and Alan S. Willsky.
2013.
Bayesiannonparametric hidden semi-markov models.
Journalof Machine Learning Research, 14:673?701, February.Mirjam Killer, Sebastian Stu?ker, and Tanja Schultz.2003.
Grapheme based speech recognition.
In Pro-ceeding of the Eurospeech, pages 3141?3144.Chia-ying Lee and James Glass.
2012.
A nonparamet-ric Bayesian approach to acoustic model discovery.
InProceedings of ACL, pages 40?49.Chin-Hui Lee, Frank Soong, and Biing-Hwang Juang.1988.
A segment model based approach to speechrecognition.
In Proceedings of ICASSP, pages 501?504.Ian McGraw, Ibrahim Badr, and James Glass.
2013.Learning lexicons from speech using a pronunciationmixture model.
IEEE Trans.
on Speech and AudioProcessing, 21(2):357?366.Kevin P. Murphy.
2002.
Hidden semi-Markov mod-els (hsmms).
Technical report, University of BritishColumbia.Kuldip Paliwal.
1990.
Lexicon-building methods for anacoustic sub-word based speech recognizer.
In Pro-ceedings of ICASSP, pages 729?732.Man-hung Siu, Herbert Gish, Arthur Chan, WilliamBelfield, and Steve Lowe.
2013.
Unsupervised train-ing of an HMM-based self-organizing unit recgonizerwith applications to topic classification and keyworddiscovery.
Computer, Speech, and Language.Sebastian Stu?ker and Tanja Schultz.
2004.
A graphemebased speech recognition system for Russian.
In Pro-ceedings of the 9th Conference Speech and Computer.Balakrishnan Varadarajan, Sanjeev Khudanpur, and Em-manuel Dupoux.
2008.
Unsupervised learning ofacoustic sub-word units.
In Proceedings of ACL-08:HLT, Short Papers, pages 165?168.Steve J.
Young, J.J. Odell, and Philip C. Woodland.
1994.Tree-based state tying for high accuracy acoustic mod-elling.
In Proceedings of HLT, pages 307?312.Victor Zue, Stephanie Seneff, James Glass, Joseph Po-lifroni, Christine Pao, Timothy J. Hazen, and Lee Het-herington.
2000.
Jupiter: A telephone-based con-versational interface for weather information.
IEEETrans.
on Speech and Audio Processing, 8:85?96.192
