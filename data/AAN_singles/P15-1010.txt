Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 95?105,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsSENSEMBED: Learning Sense Embeddingsfor Word and Relational SimilarityIgnacio Iacobacci, Mohammad Taher Pilehvar and Roberto NavigliDepartment of Computer ScienceSapienza University of Rome{iacobacci,pilehvar,navigli}@di.uniroma1.itAbstractWord embeddings have recently gainedconsiderable popularity for modelingwords in different Natural LanguageProcessing (NLP) tasks including seman-tic similarity measurement.
However,notwithstanding their success, wordembeddings are by their very natureunable to capture polysemy, as differentmeanings of a word are conflated into asingle representation.
In addition, theirlearning process usually relies on massivecorpora only, preventing them from takingadvantage of structured knowledge.
Weaddress both issues by proposing a multi-faceted approach that transforms wordembeddings to the sense level and lever-ages knowledge from a large semanticnetwork for effective semantic similaritymeasurement.
We evaluate our approachon word similarity and relational similar-ity frameworks, reporting state-of-the-artperformance on multiple datasets.1 IntroductionThe much celebrated word embeddings representa new branch of corpus-based distributional se-mantic model which leverages neural networks tomodel the context in which a word is expected toappear.
Thanks to their high coverage and theirability to capture both syntactic and semantic in-formation, word embeddings have been success-fully applied to a variety of NLP tasks, such asWord Sense Disambiguation (Chen et al, 2014),Machine Translation (Mikolov et al, 2013b), Re-lational Similarity (Mikolov et al, 2013c), Se-mantic Relatedness (Baroni et al, 2014) andKnowledge Representation (Bordes et al, 2013).However, word embeddings inherit two im-portant limitations from their antecedent corpus-based distributional models: (1) they are unable tomodel distinct meanings of a word as they conflatethe contextual evidence of different meanings of aword into a single vector; and (2) they base theirrepresentations solely on the distributional statis-tics obtained from corpora, ignoring the wealthof information provided by existing semantic re-sources.Several research works have tried to addressthese problems.
For instance, basing their workon the original sense discrimination approach ofReisinger and Mooney (2010), Huang et al (2012)applied K-means clustering to decompose wordembeddings into multiple prototypes, each denot-ing a distinct meaning of the target word.
How-ever, the sense representations obtained are notlinked to any sense inventory, a mapping that con-sequently has to be carried out either manually,or with the help of sense-annotated data.
Anotherline of research investigates the possibility of tak-ing advantage of existing semantic resources inword embeddings.
A good example is the RelationConstrained Model (Yu and Dredze, 2014).
Whencomputing word embeddings, this model replacesthe original co-occurrence clues from text corporawith the relationship information derived from theParaphrase Database1(Ganitkevitch et al, 2013,PPDB), an automatically extracted dataset of para-phrase pairs.However, none of these techniques have simul-taneously solved both above-mentioned issues,i.e., inability to model polysemy and reliance ontext corpora as the only source of knowledge.
Wepropose a novel approach, called SENSEMBED,which addresses both drawbacks by exploiting se-mantic knowledge for modeling arbitrary wordsenses in a large sense inventory.
We evaluate ourrepresentation on multiple datasets in two stan-dard tasks: word-level semantic similarity and re-lational similarity.
Experimental results show thatmoving from words to senses, while making use1http://paraphrase.org/#/download95of lexical-semantic knowledge bases, makes em-beddings significantly more powerful, resulting inconsistent performance improvement across tasks.Our contributions are twofold: (1) we proposea knowledge-based approach for obtaining contin-uous representations for individual word senses;and (2) by leveraging these representations andlexical-semantic knowledge, we put forward asemantic similarity measure with state-of-the-artperformance on multiple datasets.2 Sense EmbeddingsWord embeddings are vector space models (VSM)that represent words as real-valued vectors in alow-dimensional (relative to the size of the vo-cabulary) semantic space, usually referred to asthe continuous space language model.
The con-ventional way to obtain such representations is tocompute a term-document occurrence matrix onlarge corpora and then reduce the dimensional-ity of the matrix using techniques such as singu-lar value decomposition (Deerwester et al, 1990;Bullinaria and Levy, 2012, SVD).
Recent predic-tive techniques (Bengio et al, 2003; Collobert andWeston, 2008; Mnih and Hinton, 2007; Turian etal., 2010; Mikolov et al, 2013a) replace the con-ventional two-phase approach with a single super-vised process, usually based on neural networks.In contrast to word embeddings, which ob-tain a single model for potentially ambiguouswords, sense embeddings are continuous repre-sentations of individual word senses.
In order tobe able to apply word embeddings techniques toobtain representations for individual word senses,large sense-annotated corpora have to be available.However, manual sense annotation is a difficultand time-consuming process, i.e., the so-calledknowledge acquisition bottleneck.
In fact, thelargest existing manually sense annotated datasetis the SemCor corpus (Miller et al, 1993), whosecreation dates back to more than two decadesago.
In order to alleviate this issue, we lever-aged a state-of-the-art Word Sense Disambigua-tion (WSD) algorithm to automatically generatelarge amounts of sense-annotated corpora.In the rest of Section 2, first, in Section 2.1, wedescribe the sense inventory used for SENSEM-BED.
Section 2.2 introduces the corpus and thedisambiguation procedure used to sense annotatethis corpus.
Finally in Section 2.3 we discusshow we leverage the automatically sense-taggeddataset for the training of sense embeddings.2.1 Underlying sense inventoryWe selected BabelNet2(Navigli and Ponzetto,2012) as our underlying sense inventory.
The re-source is a merger of WordNet with multiple otherlexical resources, the most prominent of whichis Wikipedia.
As a result, the manually-curatedinformation in WordNet is augmented with thecomplementary knowledge from collaboratively-constructed resources, providing a high coverageof domain-specific terms and named entities and arich set of relations.
The usage of BabelNet as ourunderlying sense inventory provides us with theadvantage of having our sense embeddings read-ily applicable to multiple sense inventories.2.2 Generating a sense-annotated corpusAs our corpus we used the September-2014 dumpof the English Wikipedia.3This corpus comprisestexts from various domains and topics and pro-vides a suitable word coverage.
The unprocessedtext of the corpus includes approximately threebillion tokens and more than three million uniquewords.
We only consider tokens with at least fiveoccurrences.As our WSD system, we opted for Babelfy4(Moro et al, 2014), a state-of-the-art WSD andEntity Linking algorithm based on BabelNet?s se-mantic network.
Babelfy first models each con-cept in the network through its corresponding ?se-mantic signature?
by leveraging a graph randomwalk algorithm.
Given an input text, the algo-rithm uses the generated semantic signatures toconstruct a subgraph of the semantic network rep-resenting the input text.
Babelfy then searchesthis subgraph for the intended sense of each con-tent word using an iterative process and a densesubgraph heuristic.
Thanks to its use of Babel-Net, Babelfy inherently features multilinguality;hence, our representation approach is equally ap-plicable to languages other than English.
In orderto guarantee high accuracy and to avoid bias to-wards more frequent senses, we do not considerthose judgements made by Babelfy while backingoff to the most frequent sense, a case that happenswhen a certain confidence threshold is not met bythe algorithm.
The disambiguated items with highconfidence correspond to more than 50% of all the2http://www.babelnet.org/3http://dumps.wikimedia.org/enwiki/4http://www.babelfy.org/96bankn1bankn2numbern4numbern3hoodn1hoodn12(geographical) (financial) (phone) (acting) (gang) (convertible car)upstreamr1commercial bankn1callsn1appearingv6torturesn5taillightsn1downstreamr1financial institutionn1dialledv1minor rolesn1vengeancen1grillen2runsv6national bankn1operatorn20stage productionn1badguyn1bumpern2confluencen1trust companyn1telephone networkn1supporting rolesn1brutala1fascian2rivern1savings bankn1telephonyn1leading rolesn1executionn1rear windown1streamn1bankingn1subscribern2stage showsn1murdersn1headlightsn1Table 1: Closest senses to two senses of three ambiguous nouns: bank, number, and hoodcontent words.
As a result of the disambiguationstep, we obtain sense-annotated data comprisingaround one billion tagged words with at least fiveoccurrences and 2.5 million unique word senses.2.3 Learning sense embeddingsThe disambiguated text is processed with theWord2vec (Mikolov et al, 2013a) toolkit5.
We ap-plied Word2vec to produce continuous represen-tations of word senses based on the distributionalinformation obtained from the annotated corpus.For each target word sense, a representation iscomputed by maximizing the log likelihood of theword sense with respect to its context.
We optedfor the Continuous Bag of Words (CBOW) archi-tecture, the objective of which is to predict a singleword (word sense in our case) given its context.The context is defined by a window, typically withthe size of five words on each side with the para-graph ending barrier.
We used hierarchical soft-max as our training algorithm.
The dimension-ality of the vectors were set to 400 and the sub-sampling of frequent words to 10?3.As a result of the learning process, we obtainvector-based semantic representations for each ofthe word senses in the automatically-annotatedcorpus.
We show in Table 1 some of the closestsenses to six sample word senses: the geographi-cal and financial senses of river, the performanceand phone number senses of number, and the gangand car senses of hood.6As can be seen, sense em-beddings can capture effectively the clear distinc-tions between different senses of a word.
Addi-tionally, the closest senses are not necessarily con-strained to the same part of speech.
For instance,the river sense of bank has the adverbs upstreamand downstream and the ?move along, of liquid?sense of the verb run among its closest senses.5http://code.google.com/p/word2vec/6We follow Navigli (2009) and show the nthsense of theword with part of speech x as wordxn.Synset Description Synonymous senseshoodn1rough or violent youth hoodlumn1, goonn2, thugn1hoodn4photography equipment lens hoodn1hoodn9automotive body parts bonnetn2, cowln1, cowlingn1hoodn12car with retractable top convertiblen1Table 2: Sample initial senses of the noun hood(leftmost column) and their synonym expansion(rightmost column).3 Similarity MeasurementThis Section describes how we leverage the gen-erated sense embeddings for the computation ofword similarity and relational similarity.
We startthe Section by explaining how we associate aword with its set of corresponding senses andhow we compare pairs of senses in Sections 3.1and 3.2, respectively.
We then illustrate our ap-proach for measuring word similarity, togetherwith its knowledge-based enhancement, in Section3.3, and relational similarity in Section 3.4.
Here-after, we refer to our similarity measurement ap-proach as SENSEMBED.3.1 Associating senses with wordsIn order to be able to utilize our sense embeddingsfor a word-level task such as word similarity mea-surement, we need to associate each word with itsset of relevant senses, each modeled by its corre-sponding vector.
Let Swbe the set of senses asso-ciated with the word w. Our objective is to coveras many senses as can be associated with the wordw.
To this end we first initialize the set Swby theword senses of the word w and all its synonymousword senses, as defined in the BabelNet sense in-ventory.
We show in Table 2 some of the sensesof the noun hood and the synonym expansion forthese senses.
We further expand the set Swby re-peating the same process for the lemma of word w(if not already in lemma form).973.2 Vector comparisonFor comparing vectors, we use the Tanimoto dis-tance.
The measure is a generalization of Jaccardsimilarity for real-valued vectors in [-1, 1]:T ( ~w1, ~w2) =~w1?
~w2?
~w1?2+ ?
~w2?2?
~w1?
~w2(1)where ~w1?
~w2is the dot product of the vectors~w1and ~w2and ?
~w1?
is the Euclidean norm of~w1.
Rink and Harabagiu (2013) reported consis-tent improvements when using vector space met-rics, in particular the Tanimoto distance, on theSemEval-2012 task on relational similarity (Jur-gens et al, 2012) in comparison to several othermeasures that are designed for probability distri-butions, such as Jensen-Shannon divergence andHellinger distance.3.3 Word similarityWe show in Algorithm 1 our procedure for mea-suring the semantic similarity of a pair of inputwords w1and w2.
The algorithm also takes asits inputs the similarity strategy and the weightedsimilarity parameter ?
(Section 3.3.1) along witha graph vicinity factor flag (Section 3.3.2).3.3.1 Similarity measurement strategyWe take two strategies for calculating the similar-ity of the given words w1and w2.
Let Sw1andSw2be the sets of senses associated with the tworespective input wordsw1andw2, and let ~sibe thesense embedding vector of the sense si.
In the firststrategy, which we refer to as closest, we followthe conventional approach (Budanitsky and Hirst,2006) and measure the similarity of the two wordsas the similarity of their closest senses, i.e.
:Simclosest(w1, w2) = maxs1?Sw1s2?Sw2T (~s1, ~s2) (2)However, taking the similarity of the closestsenses of two words as their overall similarity ig-nores the fact that the other senses can also con-tribute to the process of similarity judgement.
Infact, psychological studies suggest that humans,while judging semantic similarity of a pair ofwords, consider different meanings of the twowords and not only the closest ones (Tversky,1977; Markman and Gentner, 1993).
For instance,the WordSim-353 dataset (Finkelstein et al, 2002)contains the word pair brother-monk.
Despite hav-ing the religious devotee sense in common, theAlgorithm 1 Word SimilarityInput: Two words w1and w2Str, the similarity strategyVic, the graph vicinity factor flag?
parameter for the weighted strategyOutput: The similarity between w1and w21: Sw1?
getSenses(w1), Sw2?
getSenses(w2)2: if Str is closest then3: sim?
-14: else5: sim?
06: end if7: for each s1?
Sw1and s2?
Sw2do8: if Vic is true then9: tmp?
T?
(~s1,~s2)10: else11: tmp?
T (~s1,~s2)12: end if13: if Str is closest then14: sim?
max (sim, tmp)15: else16: sim?
sim + tmp??
d(s1) ?
d(s2)17: end if18: end fortwo words are assigned the similarity judgementof 6.27, which is slightly above the middle pointin the similarity scale [0,10] of the dataset.
Thisclearly indicates that other non-synonymous, yetstill related, senses of the two words have alsoplayed a role in the similarity judgement.
Addi-tionally, the relatively low score reflects the factthat the religious devotee sense is not a dominantmeaning of the word brother.We therefore put forward another similaritymeasurement strategy, called weighted, in whichdifferent senses of the two words contribute totheir similarity computation, but the contributionsare scaled according to their relative importance.To this end, we first leverage sense occurrence fre-quencies in order to estimate the dominance ofeach specific word sense.
For each word w, wefirst compute the dominance of its sense s ?
Swby dividing the frequency of s by the overall fre-quency of all senses associated with w, i.e., Sw:d(s) =freq(s)?s??Swfreq(s?
)(3)We further recognize that the importance of aspecific sense of a word can also be triggered by98the word it is being compared with.
We modelthis by biasing the similarity computation towardscloser senses, by increasing the contribution ofcloser senses through a power function with pa-rameter ?.
The similarity of a pair of words w1and w2according to the weighted strategy is com-puted as:Simweighted(w1, w2) =?s1?Sw1?s2?Sw2d(s1) d(s2) T (~s1, ~s2)?
(4)where the ?
parameter is a real-valued constantgreater than one.
We show in Section 4.1.3 howwe tune the value of this parameter.3.3.2 Enhancing similarity accuracyOur similarity measurement approach takes ad-vantage of lexical knowledge at two different lev-els.
First, as we described in Sections 2.2 and2.3, we use a knowledge-based disambiguationapproach, i.e., Babelfy, which exploits BabelNet?ssemantic network.
Second, we put forward amethodology that leverages the relations in Babel-Net?s graph for enhancing the accuracy of similar-ity judgements, to be discussed next.As a distributional vector representation tech-nique, our sense embeddings can potentially sufferfrom inaccurate modeling of less frequent wordsenses.
In contrast, our underlying sense inven-tory provides a full coverage of all its concepts,with relations that are taken from WordNet andWikipedia.
In order to make use of the com-plementary information provided by our lexicalknowledge base and to obtain more accurate sim-ilarity judgements, we introduce a graph vicin-ity factor, that combines the structural knowledgefrom BabelNet?s semantic network and the distri-butional representation of sense embeddings.
Tothis end, for a given sense pair, we scale thesimilarity judgement obtained by comparing theircorresponding sense embeddings, based on theirplacement in the network.
Let E be the set of allsense-to-sense relations provided by BabelNet?ssemantic network, i.e., E = {(si, sj) : si?
sj}.Then, the similarity of a pair of words with thegraph vicinity factor in formulas 2 and 4 is com-puted by replacing T with T?, defined as:T?
(~s1, ~s2) ={T (~s1, ~s2)?
?, if (s1, s2) ?
ET (~s1, ~s2)?
?
?1, otherwise(5)We show in Section 4.1.3 how we tune the pa-rameter ?.
This procedure is particularly help-ful for the case of less frequent word senses thatdo not have enough contextual information to al-low an effective representation.
For instance, theSimLex-999 dataset (Hill et al, 2014), which weuse as our tuning dataset (see Section 4.1.3), con-tains the highly-related pair orthodontist-dentist.We observed that the intended sense of the nounorthodontist occurs only 70 times in our anno-tated corpus.
As a result, the obtained represen-tation was not accurate, resulting in a low similar-ity score for the pair.
The two respective sensesare, however, directly connected in the BabelNetgraph.
Hence, the graph vicinity factor scales upthe computed similarity value for the word pair.3.4 Relational similarityRelational similarity evaluates the correspondencebetween relations (Medin et al, 1990).
The taskcan be viewed as an analogy problem in which,given two pairs of words (wa, wb) and (wc, wd),the goal is to compute the extent to which the rela-tions of wato wband wcto wdare similar.
Senseembeddings are suitable candidates for measuringthis type of similarity, as they represent relationsbetween senses as linear transformations.
Giventhis property, the relation between a pair of wordscan be obtained by subtracting their correspond-ing normalized embeddings.
Following Zhila et al(2013), the relational similarity between two pairsof word (wa, wb) and (wc, wd) is accordingly cal-culated as:ANALOGY( ~wa, ~wb, ~wc, ~wd) =T ( ~wb?
~wa, ~wd?
~wc)(6)We show the procedure for measuring the rela-tional similarity in Algorithm 2.
The algorithmfirst finds the closest senses across the two wordpairs: s?aand s?bfor the first pair and s?cand s?dfor the second.
The analogy vector representa-tions are accordingly computed as the differencebetween the sense embeddings of the correspond-ing closest senses.
Finally, the relational similarityis computed as the similarity of the analogy vec-tors of the two pairs.4 ExperimentsWe evaluate our sense-enhanced semantic repre-sentation on multiple word similarity and related-ness datasets (Section 4.1), as well as the relationalsimilarity framework (Section 4.2).99Algorithm 2 Relational SimilarityInput: Two pairs of words wa, wband wc, wdOutput: The degree of analogy between the twopairs1: Swa?
getSenses(wa), Swb?
getSenses(wb)2: (s?a, s?b)?
argmaxsa?Swasb?SwbT (~sa, ~sb)3: Swc?
getSenses(wc), Swd?
getSenses(wd)4: (s?c, s?d)?
argmaxsc?Swcsd?SwdT (~sc, ~sd)5: return: T (~sb??
~sa?, ~sd??
~sc?
)4.1 Word similarity experimentWord similarity measurement is one of the mostpopular evaluation methods in lexical semantics,and semantic similarity in particular, with numer-ous evaluation benchmarks and datasets.
Given aset of word pairs, a system?s task is to provide sim-ilarity judgments for each pair, and these judge-ments should ideally be as close as possible tothose given by humans.4.1.1 DatasetsWe evaluate SENSEMBED on standard word simi-larity and relatedness datasets: the RG-65 (Ruben-stein and Goodenough, 1965) and the WordSim-353 (Finkelstein et al, 2002, WS-353) datasets.Agirre et al (2009) suggested that the originalWS-353 dataset conflates similarity and related-ness and divided the dataset into two subsets, eachcontaining pairs for just one type of associationmeasure: similarity (the WS-Sim dataset) and re-latedness (the WS-Rel dataset).We also evaluate our approach on the YP-130dataset, which was created by Yang and Powers(2005) specifically for measuring verb similarity,and also on the Stanford?s Contextual Word Sim-ilarities (SCWS), a dataset for measuring word-in-context similarity (Huang et al, 2012).
In theSCWS dataset each word is provided with the sen-tence containing it, which helps in pointing out theintended sense of the corresponding target word.Finally, we also report results on the MENdataset which was recently introduced by Bruniet al (2014).
MEN contains two sets of Englishword pairs, together with human-assigned similar-ity judgments, obtained by crowdsourcing usingAmazon Mechanical Turk.4.1.2 Comparison systemsWe compare the performance of our similaritymeasure against twelve other approaches.
As re-gards traditional distributional models, we reportthe best results computed by Baroni et al (2014)for PMI-SVD, a system based on Pointwise Mu-tual Information (PMI) and SVD-based dimen-sionality reduction.
For word embeddings, we re-port the results of Pennington et al (2014, GloVe)and Collobert and Weston (2008).
GloVe is an al-ternative way for learning embeddings, in whichvector dimensions are made explicit, as opposedto the opaque meaning of the vector dimensionsin Word2vec.
The approach of Collobert and We-ston (2008) is an embeddings model with a deeperarchitecture, designed to preserve more complexknowledge as distant relations.
We also show re-sults for the word embeddings trained by Baroniet al (2014).
The authors first constructed a mas-sive corpus by combining several large corpora.Then, they trained dozens of different Word2vecmodels by varying the system?s training parame-ters and reported the best performance obtained oneach dataset.As representatives for graph-based similaritytechniques, we report results for the state-of-the-art approach of Pilehvar et al (2013) which isbased on random walks on WordNet?s seman-tic network.
Moreover, we present results forthe graph-based approach of Zesch et al (2008),which compares a pair of words based on the pathlengths on Wiktionary?s semantic network.We also compare our word similarity measureagainst the multi-prototype models of Reisingerand Mooney (2010) and Huang et al (2012), andagainst the approaches of Yu and Dredze (2014)and Chen et al (2014), which enhance word em-beddings with semantic knowledge derived fromPPDB and WordNet, respectively.
Finally, we re-port results for word embeddings, as our baseline,obtained using the Word2vec toolkit on the samecorpus that was annotated and used for learningour sense embeddings (cf.
Section 2.3).4.1.3 Parameter tuningRecall from Sections 3.3.1 and 3.3.2 that our al-gorithm has two parameters: the ?
parameter forthe weighted strategy and the ?
parameter for thegraph vicinity factor.
We tuned these two parame-ters on the SimLex-999 dataset (Hill et al, 2014).We picked SimLex-999 since there are not manycomparison systems in the literature that report re-100MeasureDatasetRG-65 WS-Sim WS-Rel YP-130 MEN AveragePilehvar et al (2013) 0.868 0.677 0.457 0.710 0.690 0.677Zesch et al (2008) 0.820 ?
?
0.710 ?
?Collobert and Weston (2008) 0.480 0.610 0.380 ?
0.570 ?Word2vec (Baroni et al, 2014) 0.840 0.800 0.700 ?
0.800 ?GloVe 0.769 0.666 0.559 0.577 0.763 0.737ESA 0.749 ?
?
?
?
?PMI-SVD 0.738 0.659 0.523 0.337 0.726 0.695Word2vec 0.732 0.707 0.476 0.343 0.665 0.644SENSEMBEDclosest0.894 0.756 0.645 0.734 0.779 0.769SENSEMBEDweighted0.871 0.812 0.703 0.639 0.805 0.794Table 3: Spearman correlation performance on five word similarity and relatedness datasets.sults on the dataset.
We found the optimal valuesfor ?
and ?
to be 8 and 1.6, respectively.4.1.4 ResultsTable 3 shows the experimental results on fivedifferent word similarity and relatedness datasets.We report the Spearman correlation performancefor the two strategies of our approach as well aseight other comparison systems.
SENSEMBEDproves to be highly reliable on both similarity andrelatedness measurement tasks, obtaining the bestperformance on most datasets.
In addition, our ap-proach shows itself to be equally suitable for verbsimilarity, as indicated by the results on YP-130.The rightmost column in the Table shows theaverage performance weighted by dataset size.Between the two similarity measurement strate-gies, weighted proves to be the more suitable,achieving the best overall performance on threedatasets and the best mean performance of 0.794across the two strategies.
This indicates that ourassumption of considering all senses of a word insimilarity computation was beneficial.We report in Table 4 the Spearman correlationperformance of four approaches that are similarto SENSEMBED: the multi-prototype models ofReisinger and Mooney (2010) and Huang et al(2012), and the semantically enhanced models ofYu and Dredze (2014) and Chen et al (2014).
Weprovide results only on WS-353 and SCWS, sincethe above-mentioned approaches do not reporttheir performance on other datasets.
As we cansee from the Table, SENSEMBED outperforms theother approaches on the WS-353 dataset.
How-ever, our approach lags behind on SCWS, high-lighting the negative impact of taking the closestMeasure WS-353 SCWSHuang et al (2012) 0.713 0.628Reisinger and Mooney (2010) 0.770 ?Chen et al (2014) ?
0.662Yu and Dredze (2014) 0.537 ?Word2vec 0.694 0.642SENSEMBEDclosest0.714 0.589SENSEMBEDweighted0.779 0.624Table 4: Spearman correlation performance of themulti-prototype and semantically-enhanced ap-proaches on the WordSim-353 and the Stanford?sContextual Word Similarities datasets.senses as the intended meanings.
In fact, on thisdataset, SENSEMBEDweightedprovides better per-formance owing to its taking into account othersenses as well.
The better performance of themulti-prototype systems can be attributed to theircoarse-grained sense inventories which are auto-matically constructed by means of Word Sense In-duction.4.2 Relational similarity experimentDataset and evaluation.
We take as our bench-mark the SemEval-2012 task on Measuring De-grees of Relational Similarity (Jurgens et al,2012).
The task provides a dataset comprising 79graded word relations, 10 of which are used fortraining and the rest for test.
The task evaluatedthe participating systems in terms of the Spear-man correlation and the MaxDiff score (Louviere,1991).101ModelSetting DatasetStrategy Vicinity Expansion RG-65 WS-Sim WS-Rel YP-130 MEN AverageWord2vec ?
?
0.732 0.707 0.476 0.343 0.665 0.644Word2vecexp?
?
X 0.700 0.665 0.326 0.621 0.655 0.632SENSEMBEDclosest0.825 0.693 0.488 0.492 0.712 0.690X 0.844 0.714 0.562 0.681 0.743 0.728X X 0.894 0.756 0.645 0.734 0.779 0.769weighted0.877 0.776 0.639 0.446 0.783 0.762X 0.864 0.783 0.665 0.591 0.773 0.761X X 0.871 0.812 0.703 0.639 0.805 0.794Table 6: Spearman correlation performance of word embeddings (Word2vec) and SENSEMBED on dif-ferent semantic similarity and relatedness datasets.Measure MaxDiff SpearmanCom 45.2 0.353PairDirection 45.2 ?RNN-1600 41.8 0.275UTD-LDA ?
0.334UTD-NB 39.4 0.229UTD-SVM 34.7 0.116PMI baseline 33.9 0.112Word2vec 43.2 0.288SENSEMBEDclosest45.9 0.358Table 5: Spearman correlation performance of dif-ferent systems on the SemEval-2012 Task on Re-lational Similarity.Comparison systems.
We compare our resultsagainst six other systems and the PMI baselineprovided by the task organizers.
As for systemsthat use word embeddings for measuring rela-tional similarity, we report results for RNN-1600(Mikolov et al, 2013c) and PairDirection (Levyand Goldberg, 2014).
We also report results forUTD-NB and UTD-SVM (Rink and Harabagiu,2012), which rely on lexical pattern classificationbased on Na?
?ve Bayes and Support Vector Ma-chine classifiers, respectively.
UTD-LDA (Rinkand Harabagiu, 2013) is another system presentedby the same authors that casts the task as a selec-tional preferences one.
Finally, we show the per-formance of Com (Zhila et al, 2013), a system thatcombines Word2vec, lexical patterns, and knowl-edge base information.
Similarly to the wordsimilarity experiments, we also report a baselinebased on word embeddings (Word2vec) trained onthe same corpus and with the same settings asSENSEMBED.Results.
Table 5 shows the performance of dif-ferent systems in the task of relational similarityin terms of the Spearman correlation and MaxDiffscore.
A comparison of the results for Word2vecand SENSEMBED shows the advantage gained bymoving from the word to the sense level.
Amongthe comparison systems, Com attains the clos-est performance.
However, we note that the sys-tem is a combination of several methods, whereasSENSEMBED is based on a single approach.4.3 AnalysisIn order to analyze the impact of the differentcomponents of our similarity measure, we carriedout a series of experiments on our word similar-ity datasets.
We show in Table 6 the experimen-tal results in terms of Spearman correlation.
Per-formance is reported for the two similarity mea-surement strategies, i.e., closest and weighted, andfor different system settings with and without theexpansion procedure (cf.
Section 3.1) and graphvicinity factor (cf.
Section 3.3.2).
As our com-parison baseline, we also report results for wordembeddings, obtained using the Word2vec toolkiton the same corpus and with the same configura-tion (cf.
Section 2.3) used for learning the senseembeddings (Word2vec in the Table).
The right-most column in the Table reports the mean perfor-mance weighted by dataset size.
Word2vecexpisthe word embeddings system in which the simi-larity of the two words is determined in terms ofthe closest word embeddings among all the corre-sponding synonyms obtained with the expansionprocedure (cf.
Section 3.1).A comparison of word and sense embeddingsin the vanilla setting (with neither the expansionprocedure nor graph vicinity factor) indicates theconsistent advantage gained by moving from word102to sense level, irrespective of the dataset and thesimilarity measurement strategy.
The consistentimprovement shows that the semantic informationprovided more than compensates for the inher-ently imperfect disambiguation.
Moreover, the re-sults indicate the consistent benefit gained by in-troducing the graph vicinity factor, highlightingthe fact that our combination of the complemen-tary knowledge from sense embeddings and infor-mation derived from a semantic network is bene-ficial.
Finally, note that the expansion procedureleads to performance improvement in most casesfor sense embeddings.
In direct contrast, the stepproves harmful in the case of word embeddings,mainly due to their inability to distinguish individ-ual word senses.5 Related WorkWord embeddings were first introduced by Ben-gio et al (2003) with the goal of statistical lan-guage modeling, i.e., learning the joint probabil-ity function of a sequence of words.
The initialmodel was a Multilayer Perceptron (MLP) withtwo hidden layers: a shared non-linear and a reg-ular hidden hyperbolic tangent one.
Collobertand Weston (2008) deepened the original neuralmodel by adding a convolutional layer and an ex-tra layer for modeling long-distance dependen-cies.
A significant contribution was later made byMikolov et al (2013a), who simplified the originalmodel by removing the hyperbolic tangent layerand hence significantly speeding up the trainingprocess.
Other related work includes GloVe (Pen-nington et al, 2014), which is an effort to make thevector dimensions in word embeddings explicit,and the approach of Bordes et al (2013), whichtrains word embeddings on the basis of relation-ship information derived from WordNet.Several techniques have been proposed fortransforming word embeddings to the sense level.Chen et al (2014) leveraged word embeddings inWord Sense Disambiguation and investigated thepossibility of retrofitting embeddings with the re-sulting disambiguated words.
Guo et al (2014)exploited parallel data to automatically generatesense-annotated data, based on the fact that dif-ferent senses of a word are usually translated todifferent words in another language (Chan andNg, 2005).
The automatically-generated sense-annotated data was later used for training sense-specific word embeddings.
Huang et al (2012)adopted a similar strategy by decomposing eachword?s single-prototype representation into mul-tiple prototypes, denoting different senses of thatword.
To this end, they first gathered the contextfor all occurrences of a word and then used spher-ical K-means to cluster the contexts.
Each clusterwas taken as the context for a specific meaning ofthe word and hence used to train embeddings forthat specific meaning (i.e., word sense).
However,these techniques either suffer from low coverageas they can only model word senses that occur inthe parallel data, or require manual interventionfor linking the obtained representations to an ex-isting sense inventory.
In contrast, our approachenables high coverage and is readily applicable forthe representation of word senses in widely-usedlexical resources, such as WordNet, Wikipedia andWiktionary, without needing to resort to additionalmanual effort.6 Conclusions and Future WorkWe proposed an approach for obtaining continu-ous representations of individual word senses, re-ferred to as sense embeddings.
Based on the pro-posed sense embeddings and the knowledge ob-tained from a large-scale lexical resource, i.e., Ba-belNet, we put forward an effective technique,called SENSEMBED, for measuring semantic sim-ilarity.
We evaluated our approach on multipledatasets in the tasks of word and relational simi-larity.
Two conclusions can be drawn on the ba-sis of the experimental results: (1) moving fromword to sense embeddings can significantly im-prove the effectiveness and accuracy of the rep-resentations; and (2) a meaningful combination ofsense embeddings and knowledge from a semanticnetwork can further enhance the similarity judge-ments.
As future work, we intend to utilize oursense embeddings to perform WSD, as was pro-posed in Chen et al (2014), in order to speed upthe process and train sense embeddings on largeramounts of sense-annotated data.AcknowledgmentsThe authors gratefully acknowledgethe support of the ERC StartingGrant MultiJEDI No.
259234.103ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.
AStudy on Similarity and Relatedness Using Distribu-tional and WordNet-based Approaches.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 19?27, Boulder, Colorado.Marco Baroni, Georgiana Dinu, and Germ?anKruszewski.
2014.
Don?t count, predict!
Asystematic comparison of context-counting vs.context-predicting semantic vectors.
In Proceedingsof the 52nd Annual Meeting of the Associationfor Computational Linguistics, volume 1, pages238?247, Baltimore, Maryland.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A Neural Probabilistic Lan-guage Model.
The Journal of Machine Learning Re-search, 3:1137?1155.Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.2013.
Translating Embeddings for Modeling Multi-relational Data.
In Advances in Neural InformationProcessing Systems, volume 26, pages 2787?2795.Elia Bruni, Nam Khanh Tran, and Marco Baroni.
2014.Multimodal Distributional Semantics.
Journal ofArtificial Intelligence Research, 49(1):1?47.Alexander Budanitsky and Graeme Hirst.
2006.Evaluating WordNet-based measures of Lexical Se-mantic Relatedness.
Computational Linguistics,32(1):13?47.John A. Bullinaria and Joseph P. Levy.
2012.
Ex-tracting Semantic Representations from Word Co-occurrence Statistics: Stop-lists, Stemming andSVD.
Behavior Research Methods, 44:890?907.Yee Seng Chan and Hwee Tou Ng.
2005.
ScalingUp Word Sense Disambiguation via Parallel Texts.In Proceedings of the 20th National Conference onArtificial Intelligence - Volume 3, pages 1037?1042,Pittsburgh, Pennsylvania.Xinxiong Chen, Zhiyuan Liu, and Maosong Sun.2014.
A unified model for word sense represen-tation and disambiguation.
In Proceedings of the2014 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 1025?1035,Doha, Qatar.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th International Conference onMachine Learning, pages 160?167, Helsinki, Fin-land.Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of American Society for Information Science,41(6):391?407.Lev Finkelstein, Gabrilovich Evgeniy, Matias Yossi,Rivlin Ehud, Solan Zach, Wolfman Gadi, and Rup-pin Eytan.
2002.
Placing Search in Context: TheConcept Revisited.
ACM Transactions on Informa-tion Systems, 20(1):116?131.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The ParaphraseDatabase.
In Proceedings of Human LanguageTechnologies: The 2013 Annual Conference of theNorth American Chapter of the Association forComputational Linguistics, pages 758?764, Atlanta,Georgia.Jiang Guo, Wanxiang Che, Haifeng Wang, and TingLiu.
2014.
Learning Sense-specific Word Embed-dings By Exploiting Bilingual Resources.
In Pro-ceedings of COLING 2014, the 25th InternationalConference on Computational Linguistics: Techni-cal Papers, pages 497?507, Dublin, Ireland.Felix Hill, Roi Reichart, and Anna Korhonen.
2014.SimLex-999: Evaluating semantic models with(genuine) similarity estimation.
arXiv preprintarXiv:1408.3456.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving WordRepresentations Via Global Context And MultipleWord Prototypes.
In Proceedings of 50th AnnualMeeting of the Association for Computational Lin-guistics, volume 1, pages 873?882, Jeju Island,South Korea.David A. Jurgens, Peter D. Turney, Saif M. Moham-mad, and Keith J. Holyoak.
2012.
Semeval-2012task 2: Measuring degrees of relational similarity.In Proceedings of the First Joint Conference on Lex-ical and Computational Semantics - Volume 1: Pro-ceedings of the Main Conference and the SharedTask, and Volume 2: Proceedings of the Sixth Inter-national Workshop on Semantic Evaluation, pages356?364, Montreal, Canada.Omer Levy and Yoav Goldberg.
2014.
Linguisticregularities in sparse and explicit word representa-tions.
In Proceedings of the Eighteenth Confer-ence on Computational Natural Language Learning,pages 171?180, Ann Arbor, Michigan.Jordan Louviere.
1991.
Best-Worst Scaling: A Modelfor the Largest Difference Judgments.
Working pa-per, University of Alberta.Arthur B. Markman and Dedre Gentner.
1993.
Struc-tural alignment during similarity comparisons.
Cog-nitive Psychology, 25(4):431 ?
467.Douglas L. Medin, Robert L. Goldstone, and DedreGentner.
1990.
Similarity involving attributes andrelations: Judgments of similarity and difference arenot inverses.
Psychological Science, 1(1):64?69.104Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient Estimation of Word Repre-sentations in Vector Space.
CoRR, abs/1301.3781.Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.2013b.
Exploiting Similarities among Lan-guages for Machine Translation.
arXiv preprintarXiv:1309.4168.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013c.
Linguistic Regularities in Continuous SpaceWord Representations.
In Proceedings of HumanLanguage Technologies: The 2013 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 746?751,Atlanta, Georgia.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T. Bunker.
1993.
A Semantic Concordance.
InProceedings of the Workshop on Human LanguageTechnology, pages 303?308, Princeton, New Jersey.Andriy Mnih and Geoffrey Hinton.
2007.
Three NewGraphical Models for Statistical Language Mod-elling.
In Proceedings of the 24th InternationalConference on Machine Learning, pages 641?648,Corvallis, Oregon.Andrea Moro, Alessandro Raganato, and Roberto Nav-igli.
2014.
Entity Linking meets Word Sense Dis-ambiguation: a Unified Approach.
Transactionsof the Association for Computational Linguistics(TACL), 2:231?244.Roberto Navigli and Simone Paolo Ponzetto.
2012.BabelNet: The Automatic Construction, Evaluationand Application of a Wide-Coverage MultilingualSemantic Network.
Artificial Intelligence, 193:217?250.Roberto Navigli.
2009.
Word Sense Disambiguation:A survey.
ACM Computing Surveys, 41(2):1?69.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
GloVe: Global Vectors forWord Representation.
In Proceedings of the Em-pirical Methods in Natural Language Processing(EMNLP), volume 12, pages 1532?1543, Doha,Qatar.Mohammad Taher Pilehvar, David A. Jurgens, andRoberto Navigli.
2013.
Align, Disambiguate andWalk: a Unified Approach for Measuring SemanticSimilarity.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguis-tics, pages 1341?1351, Sofia, Bulgaria.Joseph Reisinger and Raymond J. Mooney.
2010.Multi-Prototype Vector-Space Models of WordMeaning.
In Proceedings of Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, pages 109?117, Los Angeles,California.Bryan Rink and Sanda Harabagiu.
2012.
UTD: De-termining relational similarity using lexical patterns.In *SEM 2012: The First Joint Conference on Lexi-cal and Computational Semantics ?
Volume 1: Pro-ceedings of the main conference and the shared task,and Volume 2: Proceedings of the Sixth Interna-tional Workshop on Semantic Evaluation (SemEval2012), pages 413?418, Montreal, Canada.Bryan Rink and Sanda Harabagiu.
2013.
The Impactof Selectional Preference Agreement on SemanticRelational Similarity.
In Proceedings of the 10thInternational Conference on Computational Seman-tics (IWCS) ?
Long Papers, pages 204?215, Pots-dam, Germany.Herbert Rubenstein and John B. Goodenough.
1965.Contextual Correlates of Synonymy.
Communica-tions of the ACM, 8(10):627?633.Joseph Turian, Lev Ratinov, and Yoshua Bengio.2010.
Word Representations: A Simple and GeneralMethod for Semi-supervised Learning.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 384?394, Up-psala, Sweden.Amos Tversky.
1977.
Features of similarity.
Psycho-logical Review, 84:327?352.Dongqiang Yang and David M. W. Powers.
2005.Measuring semantic similarity in the taxonomy ofwordnet.
In Proceedings of the Twenty-eighth Aus-tralasian Conference on Computer Science, vol-ume 38, pages 315?322, Darlinghurst, Australia.Mo Yu and Mark Dredze.
2014.
Improving Lexi-cal Embeddings with Semantic Knowledge.
In Pro-ceedings of the 52nd Annual Meeting of the Associa-tion for Computational Linguistics, volume 2, pages545?550, Baltimore, Maryland.Torsten Zesch, Christof M?uller, and Iryna Gurevych.2008.
Using Wiktionary for Computing Seman-tic Relatedness.
In Proceedings of the 23rd Na-tional Conference on Artificial Intelligence, vol-ume 2, pages 861?866, Chicago, Illinois.Alisa Zhila, Wen-tau Yih, Christopher Meek, GeoffreyZweig, and Tomas Mikolov.
2013.
Combining Het-erogeneous Models for Measuring Relational Sim-ilarity.
In Proceedings of Human Language Tech-nologies: The 2013 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 1000?1009, Atlanta, Geor-gia.105
