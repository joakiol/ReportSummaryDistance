The Noisy Channel and the Braying DonkeyRoberto Basili (*), Maria Teresa Pazienza (*), Paola Velardi (')(*) Dept.
of EL Engineering, University of Tor Vergata, Roma (ITALY),\[ rbas, pazienza } Otowx l .
ccd.
utovrm, i t(~) Istituto di Informatica, University of Ancona, (ITALY), v, Xa*anvax2.,inoca.
itAbstractThe title of this paper playfully contraststwo rather different approaches tolanguageanalysis.
The "Noisy Channel" 's are thepromoters of statistically based approachesto language l arning.
Many of these studiesare based on the Shannons's NoisyChannel model.
The "Braying Donkey" 'sare those oriented towards theoreticallymotivated language models.
They areinterested in any type of languageexpressions ( uch as the famous "DonkeySentences"), regardless of their frequency inreal language, because the focus is thestudy of human communication.In the past few years, we supported amorebalanced approach.
While our major concernis applicability to real NLP systems, wethink that, after aLl, quantitative methods inComputational Linguistic should provide notonly practical tools for languageprocessing, but also some linguistic insight.Since, for sake of space, in this paper wecannot give any complete account of ourresearch, we will present examples of"linguistically appealing", automaticallyacquired, lexical data (selectional restrictionsof words) obtained trough an integrated useof knowledge-based and statisticaltechniques.
We discuss the pros and cons ofadding symbolic knowledge to the corpuslinguistic recipe.1.
The "Noisy Channel" 'sAll the researchers in the field ofComputational Linguistics, no matter whattheir specific interest may be, must havenoticed the impetuous advance of thepromoter of statistically based methods inlinguistics.
This is evident not only becauseof the growing number of papers in manyComputational Linguistic conferences andjournals, but also because of the manyspecific initiatives, such as workshops,special issues, and interest groups.An historical account of this "empiricalrenaissance" is provide in \[Church andMercer, 1993\].
The general motivations are:availability of large on-line texts, on oneside, emphasis on scalability and concretedeliverables, on the other side.We agree on the claim, supported by theauthors, that statistical methods potentiallyoutperform knowledge based methods interms of coverage and human cost.
Thehuman cost., however, is not zero.
Moststatistically based methods either ely on amore or less shallow level of linguistic pre-processing, or they need non trivial humanintervention for an initial estimate of theparameters (training).
This applies inparticular to statistical methods based onShannon's Noisy Channel Model (n-grammodels).
As far as coverage is concerned~so far no method described in literaturecould demonstrate an adequate coverage ofthe linguistic phenomena being studied.
Forexample, in collocational analysis,statistically refiable associations are obtainedonly for a small fragment of the corpus.
Theproblem of" "low counts" (i.e.
linguisticpatterns that were never, or rarely found)has not been analyzed appropriately inmostpapers, as convincingly demonstrated in\[Dunning, 1993\].In addition, there are other performancefigures, such as adequacy, accuracy and"linguistic appeal" of the acquiredknowledge for a given application, forwhich the supremacy of statistics is notentirely demonstrated.
Our major objectionto purely statistically based approaches i infact that hey treat language xpressions likestings of signals.
At its extreme, thisperspective may lead to results that by nomeans have practical interest, but give nocontribution to the study of language.212...and the "Braying Donkey"'sOn the other side of the barricade, there arethe supporters of more philosophical, andtheoretically sound, models of language.
Wehope these scholars will excuse us forcategorising their very serious work undersuch a funny label.
Our point was toplayfully emphasise that the principalinterest in human models of languagecommunication motivated the study of ratherodd language xpressions, like the famous"Donkey Sentences "1.
The importance ofthese sentences i not their frequency inspoken, or written language (which isprobably close to zero), but the specificlinguistic phenomena they represent.The supporters of theoretically basedapproaches cannot be said to ignore theproblem of applicability and scalability butthis is not a priority in their research.
Someof these studies rely on statistical analyses togain evidence of some phenomenon, or tosupport empirically a theoretical framework,but the depth of the lexical model positedeventually makes a truly automatic learningimpossible or at least difficult on a vastscale.The ensign of this approach is Pustejovsky,who defined a theory of lexical semanticsmaking use of a rich knowledgerepresentation framework, called the qualiastructure.
Words in the lexicon are proposedto encode all the important aspects ofmeaning, ranging from their argumentstructure, primitive decomposition, andconceptual organisation.
The theory ofqualia has been presented inseveral papers,but the reader may refer to \[Pustejovsky andBoguraev, 1993\], for a rather complete andrecent account of this research.
Pustejovskyconfronted with the problem of automaticacquisition more extensively in \[Pustejovskyet al 1993\].
The experiment described,besides producing limited results (asremarked by the author itself), is hardlyreproducible on a large scale, since itpresupposes the identification of anappropriate conceptual schema thatgeneralises the semantics of the word beingstudied.The difficulty to define sealable methods forlexical acquisition is an obvious drawbackof using a rich lexical model.
Admittedly,corpus research is seen by many authors inthis area, as a tool to fine-tune lexicalstructures and support theoreticalhypothesis.3.
Adding semantics to thecorpus statistics recipe...Indeed, the growing literature in lexicalstatistics demonstrates that much can bedone using purely statistical methods.
Thisis appealing, since the need for heavyhuman intervention precluded to NLPtechniques a ubstantial impact on real worldapplications.
However, we should notforget hat one of the ultimate objectives o fComputational Linguistic is to acquire somedeeper insight of human communication.Knowledge-based, or syraboHc, techniquesshould not be banished as impractical, sinceno computational system can ever learnanything interesting if it does not embedsome, though primitive, semantic model 2.In the last few years, we promoted a moreintegrated use of statistically and knowledgebased models in language l arning.
Thoughour major concern is applicability andscalability in NLP systems, we do notbelieve that the human can be entirely keptout of the loop.
However, his/hercontribution i defining the semantic bias ofa lexical earning system should ideally bereduced to a limited amount of timeconstrained, well understood, actions, to beperformed by easily founded professionals.Similar constraints are commonly acceptedwhen customising Information Retrieval andDatabase systems.Since we are very much concerned withsealability asia\[ with what we call linguisticappeal, our effort has been to demonstratethat "some" semantic knowledge can bemodelled at the price of limited humanintervention, resulting in a higherinformative power of the linguistic dataextracted from corpora.
With purelystatistical pproaches, the aequked lexicalinformation has no finguisfic ontent per seuntil a human analyst assigns the correctinterpretation to the data.
Semanticmodelling can be more or less coarse, but inany case it provides a means to categoriselwhere the poor donkey brays since it is beated allthe time..2this is often referred to as the semantic bias inMachine Learning22language phenomena r ther that sinking thelinguist in milfions of data (collocations, n-grams, or the like), and it supports a morefinguistically oriented large scale languageanalysis.
Finally, symbolic computation,unlike for statistical computation, addspredictive value to the data, and ensuresstatistically reliable data even for relativelysmall corpora.Since in the past 3-4 years all our researchwas centred on finding a better balancebetween shallow (statistically based) anddeep (knowledge based) methods for lexicallearning, we cannot give for sake of brevityany complete account of the methods andalgorithms that we propose.
The interestreader is referred to \[Basili et al 1993 b andc\], for a summary of ARIOSTO, anintegrated tool for extensive acquisition oflexieal knowledge from corpora that weused to demonstrate and validate ourapproach.The learning algorithms that we def'med,acquire some useful type of lexicalknowledge (disambiguation cues, selectionalrestrictions, word categories) through thestatistical processing of syntactically andsemantically tagged collocations.
Thestatistical methods are based ondistributional analysis (we defined ameasure called mutual conditionedplausibility, a derivation of the well knownmutual information), and cluster analysis (aCOBWEB-l ike algorithm for wordclassification is presented in \[Basili et al1993,a\]).
The knowledge based methodsare morphosyntactic processing \[Basili et al1992b\] and some shallow level of semanticcategorisation.Since the use of syntactic processing incombination with probability calculus israther well established in corpus linguistics,we will not discuss here the particularmethods, and measures, that we defined.Rather, we will concentrate on semanticcategorisation, since this aspect more closelyrelates to the focus of this workshop: Whatknowledge can be represented symbolicallyand how can it be obtained on a large scale ?The title of the workshop, Combingsymbolic and statistical approaches..,presupposes that, indeed, one suchcombination is desirable, and this was notso evident in the literature so far.However, the what-and-how issue raisedby the workshop organisers i a crucial one.It seems there is no way around: the moresemantics, the less coverage.
Is that so true?We think that in part, it is, but notcompletely.
For example, categorizingcollocations via semantic tagging, as wepropose, add predictive power to thecollected collocadons, ince it is possible toforecast the probability of collocations thathave not been detected in the trainingcorpus.
Hence the coverage is, generallyspeaking, higher.In the next section we will discuss theproblem of finding the best source forsemantic ategorization.
There are manyopen issues here, that we believe anintersting matter of discussion for theworkshop.In the last section we (briefly) present anexample of very useful type of lexicalknowledge that can be extracted by the useof semantic ategorization i combinationwith statistical methods.4.
Sources of semant iccategorizationWe first presented the idea of addingsemantic tags in corpus analysis in \[B'asili etal.
1991 and 1992a\], but othercontemporaneous and subsequent papersintroduced some notion of semanticcategorisation in corpus analysis.
\[Boggesset al 1991\] used rather fine-tunedseleetional restrictions to classify word pairsand triples detected by an n-gram modelbased part of speech tagger.
\[Grishman1992\] generalises automatically acquiredword triples using a manually prepared fullword taxonomy.
More recently, the idea ofusing some kind of semantics seems to gaina wider popularity.
\[Resnik and Hearst,1993\] use Wordnet categories to tagsyntactic associations detected by a shallowparser.
\[Utsuro et al, 1993\] categorisewords using the "Bunrui Goi Hyou"(Japanese) thesaurus.In ARIOSTO, we initially used handassigned semantic ategories for two italiancorpora, since on-line thesaura arenotcurrently available in Italian.
For anEnglish corpus, we later used Wordnet.We mark with semantic tags all the wordsthat are included at least in one collocationextracted from each application corpus.In defining semantic tags, we pursued twocontrasting requirements: portability andreduced manual cost, on one side, and thevalue-added to the data by the semantic23markers, on the other side.
The compromisewe conformed to is to select about 10-15"naive" tags, that mediate at best betweengenerality and domain-appropriateness.Hand tagging was performed on acommercial and a legal domain (hereafterCD and LD), both in Italian.
Examples oftags in the CD are: MACHINE (grindstone,tractor, engine), BY_PRODUCT (wine,milk, juice).
In the LD, examples are:DOCUMENT (law, comma, invoice)andREALESTATE (field, building, house).There are categories incommon between thetwo domains, such as HUMAI~ENTITY,PLACE, etc.
The appropriate level ofgenerality for categories, is roughly selectedaccording to the criterion that words in adomain should be evenly distributed amongcategories.
For example, BY_PRODUCTis not at the same level as HUMAN EN1TYin a domain general classification, but in theCD there is a very large number of words inthis class.For what concerns ambiguous words,many subtle ambiguities are eliminatedbecause of the generality of the tags.
Sinceall verbs are either ACTs or STATEs, onehas no choices in classifying an ambiguousverb like make.
This is obviously asimplification, and we will see later itsconsequences.
On the other side, manyambiguous senses of make are not found ina given domain.
For example, in thecommercial domain, make essentially isused in the sense of manufacturing.Despite the generality of the tags used, weexperiment that, while the categorisation fanimates and concrete ntities is relativelysimple, words that do not relate with bodilyexperience, such as abstract entities and themajority of verbs, pose hard problems.An alternative to manual classification isusing on-line thesaura, such as Roget's andWordnet categories in English 3.
Weexperimented Wordnet on our Englishdomain (remote sensing abstracts, RSD).The use of domain-general categories, uchas those found in thesaura, has its evidentdrawbacks, namely that the categorisationprinciples used by the linguists are inspiredby philosophical concerns and personalintuitions, while the purpose of a typehierarchy in a NLP system is more practical,for example xpressing at the highest level3There is an on-going European initiative totranslate Wordnet in other languages, among whielaIuflianof generality the selectional constrains ofwords in a given domain.
For one suchpractical objective, asuitable categorizationpnnciple is similarity in words usage.Though Wordnet categories rely also on astudy of collocations in corpora (the Browncorpus), word similarity in contexts is onlyone of the classification pdncipia adopted,surely not prevailing.
For example, thewords resource, archive and file are used inthe RSD almost interchangeably (e.g.access, use, read .from resource, archive,file).
However, resource and archive haveno common supertyp?
in Wordnet.Another pro.blem is over-ambiguity.
Givena specific application, Wordnet tags createmany unnecessary ambiguity.
For example,we were rather surprised to find the wordhigh classified as a PERSON (--soprano)and as an ORGANIZATION (=highschool).
"this wide-speetnma classification isvery useful on a purely linguistic ground,but renders the classification unusable as itis, for most practical applications.
In theRSD, we had 5311 different words ofwhich 2796 are not classified in Wordnetbecause they are technical terms, propernouns and labels.
For the "known" words,the avergae ambiguity in Wordnet is 4.76senses per word.
In order to reduce part ofthe ambiguity, we (manually) selected 14high-level Wordnet nodes, like for example:COGNIT ION,  ART IFACT,ABSTRACTION, PROPERTY, PERSON,that seemed appropriate for the domain.
Thisreduced the average ambiguity to 1.67,which is still a bit too high (soprano ?
), i.e.it does not reflect the ambiguity actuallypresent in the domain.
There is clearly theneed of using some context-drivendisambiguafion method to automaticallyreduce the ambiguity of Wordnet ags.
Forexample, we are currently experimenting analgorithm to automatically select fromWordnet the "best level" categories for agiven corpus, and eliminate part of theunwanted ambiguity.
The algorithm is basedon the Machine Learning method for wordcategorisation, i spired by the well knownstudy on basic-level categories \[Rosch,1978\], presented in \[Basili et al 1993a\].Other methods that seem applicable to theproblem at hand have been presented in theliterature \[Yarowsky 1992\].24.5.
Producing wine, statements,and data: on the acquisition ofselectionai restrictions in sublanguagesSince our objective is to show that addingsemantics to the standard corpus linguisticsrecipe (collocations +statistics) renders theacquired ata more linguistically appealing,this section is devoted to the linguisticanalysis of a case-based lexicon.
Thealgorithm to acquire the lexicon,implemented in the ARIOSTQLEX system,has been extensively described in \[Basili etal, 1993c\].
In short, the algorithms worksas follows:First, collocations extracted from theapplication corpus are clustered according tothe semantic and syntactic tag 4 of one orboth the co-occurring content words.
Theresult are what we call clustered associationd a t a .
For  example ,V_prep_N(sell ,  to,shareholder) andV_prep_N(assign, totax-payer), occurringwith frequency f /and.t2 respectively, aremerged into a unique associationV_prep_N(ACT, to, HUMAN ENTITY)with frequency fl+f2.
The statisticallyrelevant conceptual associations arepresented to a linguist, that can replacesyntactic patterns with the underlyingconceptual relation (e.g.
\[ACT\]->(beneficiary)->\[HUMAl~ENTITY\]).These coarse grained selectional restrictionsare later used in AIOSTO LEX for a morerefined lexical acquisition phase.
We haveshown in \[Basili et al 1992a\] that in sublanguages there are many unintuitive waysof relating concepts to each other, thatwould have been very hard to find withoutthe help of an automatic procedure.Then, for each content word w, we acquireall the collocations in which it participates.We select among ambiguous patterns usinga preference method escribed in \[Basili etal, 1993 b, d\].
The detected collocations fora word w are then generalised using thecoarse grained selectional restrictions4We did not discuss of syntactic tags for brevity.Our (not-so) shallow parser detects productive pairsand triples like verb subject and direct object (N_Vand V N, respectively), prepositional triplesbetween non adjacent words (N_prep_N, V_lxeP_~,etc.acquired during the previous phase.
Forexample, the following collocationsincluding the word measurement in theRSD:V prep_N(deriveJrom, easurement),V prep_N(determineJrom, measurement)and V_prep_N( infer, from, measurement)let the ARIOSTO_LEX system learn thefollowing selectional restriction:\[COGNrNONI<-(/igurative_source)<-\[measurement\],where COGNITION is a Wordnet categoryfor the verbs determine, infer and derive,and f igurative_source is one of theconceptual relations used.
Notice that theuse of conceptual relations is not strictlynecessary, though it adds semantic value tothe data.
One could simply store thesyntactic subeategorization f each wordalong with lhe semantic restriction on theaccompanying word in a collocation, e.g.something like: measurement.
(V_prep_N.from, COGNITION(V)).
It is also possibleto cluster, for each verb or verbal noun, allthe syntactic subcategorization frames forwhich there is an evidence in the corpus.
Inthis case, lexieal acquisition is entirelyautomatic.The selectional restrictions extensivelyacquired by ARIOSTQLEX are a usefultype of lexical knowledge that could be usedvirtually in any NLP system.
Importantly,the linguistic material acquired islinguistically appealing since it providesevidence for a systematic study of sublanguages.
From a cross analysis of wordsusage in three different domains we gainedevidence that many linguistic patterns do notgeneralise across ub languages.
Hence, theapplication corpus is an ideal source forlexical acqu~ition.25In fig.
1 we show one of the screen out ofARIOSTO_LEX.
The word shown ismeasurement, very frequent in the RSD, aspresented to the linguist.
Three windowsshow, respectively, the lexical entry thatARIOSTQLEX proposes to acquire, a listof accepted patterns for which only oneexample was found (lexical patterns aregeneralized only when at least two similarpatterns are found), and a list of rejectedpatterns.
The linguist can modify or acceptany of these choices.
Each acquire dselectional restriction is represented asfollows:pre semJex(word, conceptual relation,semantic tag 5, direction, SE, CF)the first four arguments identify theselectional restriction and the direction of theconceptual relation, Le.:\[measurement\]<-(OBJ)<-\[COGNITION\](e.g.
calculate, setup, compare...ameasurement)\[measurement\] ->(INSTRUMENT)->\[ NSTRUM NTAL1TY\](e.g.
measurement from satellite, aircraft,radar)SE and CF are two statistical measures ofthe semantic expectation and confidence ofthe acquired selectional restriction (see theaforementioned papers for details).ARIOSTO_LEX provides the linguist withseveral facilities to inspect and validate theacquired lexicon, such as examples ofphrases from which a selectional restrictionwas derived, and other nice gadgets.
Forexample, the central window in Figure 1(opened only on demand) shows theConceptual Graph of the acquired entry.The Conceptual Graph includes the extendedWordnet labels for each category.One very interesting matter for linguisticanalysis is provided by a cross-comparisonof words, as used in the three domains.Many words, particularly verbs, exhibitcompletely different patterns of usage.
Hereare some examples:The verb produrre (to produce) is relativelyfrequent in all the three domains, but exhibitvery different selectional restrictions.
In theRSD (Remote Sensing) we found thepattem:produce->(agent)->\[ORGANIZATION,PERSON\]->(source)->\[INSTRUMENTALITY\]or :5 Labels of Wordnet classes are sometimes denotedby abbreviations, e.g.
CGN = 'cognition,knowledge'.26produce->(theme)-> COGNITIVE_CONTENT->(source)->INSTRUMENTALITYe.g.
the satellite produced an image withhigh accuracy, the NASA produced thedata..in the CD (commercial) we found:produce->(obj)-> ARTIFACT->(agent)-> HUMAN_ENTITY->(instruraent)->MACHINE 6e.g.
: la d/tta produce vino con macchinaripropri (*the company produces wine withowned machinery)and in the LD (legal):produce->agent)->HUMAI~ENTITY->(theme)-> DOCUMENTe.g.
: il contn'buente d ve produrre ladichiarazione (the tax payer must produce astatement)It is interesting to see which company theword "ground" keeps in the three domains.The RSD is mostly concerned with itsphysical properties, ince we find patternslike:measure->(obj)-> PROPERTY/A'ITR/BUTE<-(characteristic)<- ground(e.g.
to measure the feature, emissivity,image ,surface of ground)In the CD, terreno (=ground) is the directobject of physical ACTs such as cultivate,reclaim, plough, etc.
But is also found inpatterns like:BYPRODUCT ->(source)-> terreno(e.g.
patate, carote ed altn'prodotti delterreno = potatoes, carrots and other groundproducts)in the LD, terreno is a real estate, object oftransactions, and taxable as such.
ThegeneraJised pattern is:6MACHINE is the same as the Wordnet classINSTRUMENTALITY.
Notice that we usedWordnet categories for our English corpus only laterin our research.
Perhaps we could fmd a Wordnettag name for each of our previous manually assignedtags in the two Italian domains, but this would beonly useful for presentation purposes.
In fact, sincethere is not as yet an Italian version of Wordnet(though it will be available soon), we cannotclassify automaO~lly.TRANSACTION->(obj)-> terreno(vendere, acquistare, permutare terreno =sell, buy, exchange aground)AMOUNT <-(source)<- terreno(e.g.
costo, rendita, di terreno= price,revenue of ( =deriving from the ownershipo~ ground)And what is managed in the three domains?In the RSD,  one managesCOGNIT IVECONTENT,  such as image,information, data etc.
The manager is ahuman ORGANIZATION, but also anARTIFACT (a system, an archive).In the CD, the pattern is:manage ->(agent)->HUMAb~ENTITYor->(theme)->ACT->(location)-> BUILDINGmanage ->(agent)->HUMAI',~ENTITy->(obj)->BUILDING(e.g.
gestire la vendita di alimentari nelnegozio.. = to manage the sale of food inshops )Finally, in the LD, the pattern is:manage->(agent)->HUMAN_ENTITY->(obj)->\[AMOUNT,ABSTRACTION\](e.g.
gestione di tributi, fondi, credito,debito etc.
= management oftaxes, funding,credit, debit)\[ x:;r~-;T4s~- t~(~,ur=~.~,~- ; , -~-~, -d~i~/C;TTb0b ?
; .
.Y~=: - - , ' ) .
. '
, ,pr  w.sore.
I e~(measur~er .
t ,  th4m,  A, " ( " .
~.~,'_//, n I I ,  *.
| lOOO"," .
11000"," *" ) .Fe .sv~.
.
I vx(messw'we~nt, thyme,SO, " ( "  ,G .V .N,n l  1. "
.
?O00",  ' .
?000",  " - *  ) .p rv .~m,  l vMmns4arvm*~t,  U'~mm, ~,  * ( "  ,G .V .H,n l  I ,  " .11000 ?
, * .
r iO00 ' ,  ' *"  ) .pre.~,4~n.
1oaCmea~ur'm;nnt, ype.o?,CGl~, ",'."
,G. I ( .P .N,  o f ,  " .
10000 ?
, * .
10000", " - "  ) .rw.e.
r,n'a.
I a ,~( qt4~ S~ rm~.ir.t ~ aoJoc?., PA, " ) "  ,G. f / .
r i ,n!
I ,  " .22noo" ,  ' .~2000", "-" ) .I'u" ~.~.um.
| *~(men mr,.r~*r.t,  anJm=~., Pit, * ) " ,  G.H.P_I/,OF, " .3f lOf lO' ,"  .32000",  " - '  ) .pre .se~,  l t~(~l~sure~ent .~:  |ec t  ,PRo " ) "  ,G_V.R.ni  l o " .2000" , " .32000" , ' -  " ) .pr e_ |era.
I e <~ me~sur e~er.t.
: : .
|ec  t .
PRo " ) ' ,G .
t J .N .n !
I .
".~OOO" ~".
13000"," - "  ) .i F~v~v~v~(nrvvs~.~`t* .
.~r~t~u~vc;~)~*G~P~M~ni~2~2~+~)~w'v .vun_  lva{~v~' , r .v r .
t ,  :nw'v?~vr  Iv ,  (Cn.
~TTR. "
)  ?
.G .& .H , .
t  | ,  ?.2~000", ?.20000 ?, '-  ?)
.
: r r4 .
I tinier,.
1 ex( .~sur  ~.~nt  ~ ~ns~rl~.~lr.t ~ | NS, " ) '  ,G_H,P.NI - f rm'  ~ ' .4000o ~ " .
3~000" , , ,  ) .pre_li~.~o.le~(~eas~,a'e~cent.character~st,l~,AT'TR Ira, .
.
.
.
.
.
.~"~- .~.
-  :~-"  .
.
.
.
.
.
,~m,r~l \])re.
l |r .
.~G.
le~( :eas~'4~ent~charact4r fs t ta , t~, "  r ~U|T 1iro_ I I~c .
1 o<(.~aour ecent ,  I n .~'ur=|n t ,  ZH$, ".~" ,I r .v , surm.
t : -  )~_1 It.
~a.
1 e,4(amam~-~ir~t t,~aarm4r~.l, " ( "  ~ G.N.P.I,re_ I !~r.<_ 1 e < (~.~ m ~" eJc *n t ,l~u,-rmse, .1," ( " ,  G_i',/_P. '
:**~"e. 1 !r..c c~.
1 *,'.
(tea suree eat ,  pu?
'pOSe, ,~, " (  ",  G.V_P.p" e_ I l e .~ .
I e x ( .~  sur ~4n , .
purpose,  .I.
?
( ?
,  G./~I..P.G.
N. V(a.ea sur ~0r.41 ~t ,~1 | .
?cmdtaC t ,  1)G. N.~'( mea sur ~ll~.en t ,n l  1 ,COrr t lpO~,  1)i G. N. V(m, vsur~n~er.t, ~t * I ~ ,*~b lv ,  | )G.N.V(m.vsstm't4.~r.t,n~ I ,  : .w.Hve,  1 )G.N.V(mesc.urcm~nt,~l~ : ,  pot  ~Qrm~ 1)G_N.VOu~:rar~er .
t ,  nl  : ~ r~-ov iml ~ | }~.N_~'(maa~u~'#mqnt,nl I , re la te ,  1)G,N.~.
'(me~sure~r.ent ,n l  1 ~r esp?nle.~., 1)G.N..~( r lc l t  CEll t I t ,  n t | .
"sUit\] SUr Ile~ert, 2)\] G_N_.~( ~unphot rca, i t er  ~ n ~ I ,mval;ur ore!mr1 t ~ 2 )G_N_V(mvvsucwr.entm,I 1~ ub~,;~ i n .2 )G. N. V{ meac, ur e~r.er, t ,  n ~.
~., ShO..,, ~ ).~: G_N.H(&Cr.~e?.fc3ounCier',nl t~measur'onmn~;3)G.H.H(pntltnna, n|  1 ~aea~'~.ent ,  3):~' G.
N,.
'((me teosa t .
rd I ,measur~ent ,  3)~::i} U_N.H(Sa ,e I I I re .n |  I ,  ~easur linen, ,4)?
~\['\] G. N.P, ff(!nv~ vat  ~l~.~t L, f rur l ,  V~ t~ I I | i v ,  4 )?
\]..,-t G. ,d.V{a~:a=ur?~r.L, "~?
| ,  ".~ke.
~ ).
.
).
.
).
- )...%G_N,~'/f.
l l qutc ,n t l ,mets~.
.u '~ent )  2 - CgCG.~/_t/(~,r~il ,aelsu~'o~er.t)  - taxG.H_tl(~.aa~ssur~a;mt~H | ,r.ee3.
~) ; - C~CG.t/ .t J(mvWlurC,.~eot,rdl~itartd;rd) t o C/;CG.f/ .N(mu,nl I ,mea~ar,i:~lr.t) 1 - CRCG_~/_ff(nel~.bo~'_~EE el  l.mn~Su~',wa.nt) !
* CRCG.
r l _Nfsecc l~l ,n i l .~easurt r .ent )  !
- CRCG.
.q .
t l ( s i r .g le ,n i i ,~eas~r~er~, )  ; - CRCi ~ .N_N* :uV~.~ .r~ | I ,~ .vvv~'e .
.v , t  : 3 * CAC!
: I~.H.Hl:~...v_~.~-~hv#~)vl,r*i |,mva~u/'~Jrn~ir.~.)
~ -i: T~ x _ ..I I I( themo ) ~.-- ( 'aCt ,  h.u'~an act |on ,  I'~man act iv i ty ' : * ,"8Create# hat.urn|  s~l~,~rv~lt~ ~lC l~t~f~c  d lSC l~11n4" :?
\ ]( type_Of ) ( - -  ( ' lOs t rac~lon* : , ,"?antent ,  c?gr .
l t i ve  ?oee:er.t.
mental ot~Ject ' : * ,*COOn1 t tof b kr, owleduv" : ?
)( ob jec t  ) - ->  \ [ 'nature :  ob jec t ' : ' ,"~r~per ty '  : ,  )(F Ig .
lo? )
- - : )  ( ' l e t ,  human aCt!On, I'N.man act~v l tv ' : ' ,":Ogn1,1On, k?.ovledge" : -  \]( Instr.,amvnt ) - - )  ( ' t .
s t ru ' tq l r ,  t t l l ty ' : .
\ ]( Icc|L~lon ) - ->  r ' l oc* t ' l on ' : .
\ ](m*nnt r )  - ->  ( *?ogn l t ' i on , ) .e .ov ledge ' : .
\ ]( chr? }
- - : ,  \ [%t t r ibutv ' : - \ ]" )  i vlns t ruaent  3.~00 -- )  Ins t r~ent  4.00~ .-- )  In ,  f remont 4.000-> In l l t rume.t  S.~00*:* t r  ca.o G.O~-:.
etn_ lac t...~ '3 .~- )  ?in_ loc l .
Or,.\]o:R?CP.C- OrS:!
-Ct )  Z-i11!
IFig.1 The screenout for the lexical entry of the word "measurement"27It is interesting to see how little in commonthese patterns have.
Clearly, thisinformation could not he derived fromdictionaries or thesaura.
Though thecategories used to cluster patterns of use arevery high level (especially for verbs), stillthey capture very well the specificphenomena of each sublanguage.6.
Conc lud ing  remarksIn this paper we supported the idea that"some amount" of symbofic knowledge(high-level semantic markers) can be addedto the standard lexical statistics recipe withseveral advantages, among whichcategorization, predictive power, andlinguitic appeal of the acquired knowledge.For sake of space, we could not provide allthe evidence (algorithms, data andperformance evaluation) to support ourarguments.
We briefly discussed, and gaveexamples, of our system for the semi-automatic acquisition, on a large scale, ofselectionl restrictions.
ARIOSTO_LEX hasits merits and limitations.
The merit is that itacquires extensively, with limited manualcost, a very useful type of semanticknowledge, usable virtually in any NLPsystem.
We demonstrated with severalexamples that selectional restrictions do notgeneralize across sublanguages, andacquiring them by hand is often inintuitiveand very time-consuming.The limitation is that the choice of theappropriate conceptual types is non trivial,even when selecting very high-level tags.On the other hand, selecting categories fromon-line thesaura poses many problems,particularly because the categorizationprincipia adopted, may not be adequate forthe practical purposes of a NLP system.References\[Basili el.
al .
1991\] R. Basili, M.T.
Pazienza,P.Velardi, Using word association for syntacticdisambiguation, i  "Trends in Artificial Intelligence",Ardizzone.
Gaglio, Sorbello Eds.. in Lecture Notes inAI.
Springer Vedag, 1991.\[Basili et al 1992a\] Basili, R., Pazienza, M.T..Velardi.
P., Computational Lexicons: the NeatExamples and the Odd Exemplars, Proc.
of Third Int.Con?
on Applied Natural Language Processing, Trento,Italy, 1-3 April, 1992.\[Basili et al 1992 b\] Besili, R., M.T.
Pazienza, P.Velardl, A shallow syntactic analyzer to extract wordassociations from corpora, Literary and LinguisticComputing, 1992, vol.
7, n. 2, 114-124.\[Basili et al 1993 a\] Basili, R., Pazienza, M.T.,Velsrdi, Hierarch/cal clust~ing of verbs, ACL-SIGLEXWorkshop on I.~xical Acquisition, Columbus Ohio,June, 1993.\[Basili at al, 1993b\] Baalli, R., M.T.
Pazienza, P.Velardi, What can be learned from raw texts ?,forthcoming on Journal of Machine Translation.\[Besili et al 1993c\] Besili, R., M.T.
Pazienza, P.Velardi, Acquisition of selectional patterns,fo~ming on Journal of Machine Translation.\[Basili et al 1993d\] Besili, R., M.T.
Pazienza, P.Velardi, Semi-automatic extraction of linguisitcinformation for syntactic disambiguation, AppliedArtificial Intelligence, vol.
4, 1993.\[Boggus et al 1992\] L. Boggess, R. Agarwal, R.Davis, Disambiguation of Prepositional Phrases inAutomatically Labeled Technical Texts, Proc.
of AAAI1991\[Church and ME'reef, 1993\] K. Church and L. Mercer,Introduction to the sepuial issue on ComputationalLinguistics using large corpora, ComputationalLinguistics, vol.
19. n.1, 1993\[Dunning, t993\] T. Dunning, Accurate methods forstatistical surprise and coincidence, ComputationalLinguistics, voL 19. n.l, 1993\[Grishman, 1992\] R. Grishman, Acquisition ofselectional patterns, Proc.
of COLING-92, Nantes,1992\[Yarowsky 1992\] D. Yarowsky, Word-sensedisambiguation using statistical models of Roger'scategories trained on large corpora, INoc.
of COLING-92, Nantes, 1992\[Pustejovsky and Boguraev, 1993\] J. Pustejovsky andB.
Boguraev, Lexical Knowledge representation a dnatural language processing, Artificial Intelligence,voi.63, n. 1-2, pp.
193-224, October 1993\[Pustejovski et al 1993\] J. Pustejovsky, S. Bergler,and P. Anick, Lexical semantic techniques for corpusanalysis, Computational Linguistics, vol.
19, n.2.1993\[Rosch, 1978 \] E. Rosch, Principle of categorization,in Cognition and Categorization, Erlbaum 1978.\[Resnik and Hearst.
1993\] P. Reanik and M. Hearst,Structural ambiguity and conceptual relations, ACLworkshop on very large corpora, Ohio StateUniversity, June 22, 1993\[Utsuro et al 1993\] T. Utsuro, Y. Matsumoto.
M.Nagao, Verbal case frame acquisition from bilingualcorpora, proc.
of UCAI-9328
