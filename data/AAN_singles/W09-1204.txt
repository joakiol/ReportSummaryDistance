Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 31?36,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsHybrid Multilingual Parsing with HPSG for SRLYi ZhangLanguage TechnologyDFKI GmbH, Germanyyzhang@coli.uni-sb.deRui WangComputational LinguisticsSaarland University, Germanyrwang@coli.uni-sb.deStephan OepenInformaticsUniversity of Oslo, Norwayoe@ifi.uio.noAbstractIn this paper we present our syntactic and se-mantic dependency parsing system submittedto both the closed and open challenges of theCoNLL 2009 Shared Task.
The system ex-tends the system of Zhang, Wang, & Uszko-reit (2008) in the multilingual direction, andachieves 76.49 average macro F1 Score on theclosed joint task.
Substantial improvementsto the open SRL task have been observed thatare attributed to the HPSG parses with hand-crafted grammars.
?1 IntroductionThe CoNLL 2009 shared task (Hajic?
et al, 2009)continues the exploration on learning syntactic andsemantic structures based on dependency notationsin previous year?s shared task.
The new additionto this year?s shared task is the extension to mul-tiple languages.
Being one of the leading compe-titions in the field, the shared task received sub-missions from systems built on top of the state-of-the-art data-driven dependency parsing and se-mantic role labeling systems.
Although it wasoriginally designed as a task for machine learningapproaches, CoNLL shared tasks also feature an?open?
track since 2008, which encourages the useof extra linguistic resources to further improve the?We are indebted to our DELPH-IN colleagues, specifi-cally Peter Adolphs, Francis Bond, Berthold Crysmann, andMontserrat Marimon for numerous hours of support in adapt-ing their grammars and the PET software to parsing the CoNLLdata sets.
The first author thanks the German Excellence Clus-ter of Multimodal Computing and Interaction for the supportof the work.
The second author is funded by the PIRE PhDscholarship program.
Participation of the third author in thiswork was supported by the University of Oslo, as part of its re-search partnership with the Center for the Study of Languageand Information at Stanford University.
Our deep parsing ex-perimentation was executed on the TITAN HPC facilities at theUniversity of Oslo.performance.
This makes the task a nice testbed forthe cross-fertilization of various language process-ing techniques.As an example of such work, Zhang et al (2008)have shown in the past that deep linguistic parsingoutputs can be integrated to help improve the per-formance of the English semantic role labeling task.But several questions remain unanswered.
First, theintegration only experimented with the semantic rolelabeling part of the task.
It is not clear whethersyntactic dependency parsing can also benefit fromgrammar-based parsing results.
Second, the Englishgrammar used to achieve the improvement is one ofthe largest and most mature hand-crafted linguisticgrammars.
It is not clear whether similar improve-ments can be achieved with less developed gram-mars.
More specifically, the lack of coverage ofhand-crafted linguistic grammars is a major concern.On the other hand, the CoNLL task is also a goodopportunity for the deep processing community to(re-)evaluate their resources and software.2 System ArchitectureThe overall system architecture is shown in Figure 1.It is similar to the architecture used by Zhang et al(2008).
Three major components were involved.The HPSG parsing component utilizes several hand-crafted grammars for deep linguistic parsing.
Theoutputs of deep parsings are passed to the syntacticdependency parser and semantic role labeler.
Thesyntactic parsing component is composed of a mod-ified MST parser which accepts HPSG parsing re-sults as extra features.
The semantic role labeler iscomprised of a pipeline of 4 sub-components (pred-icate identification is not necessary in this year?stask).
Comparing to Zhang et al (2008), this archi-tecture simplified the syntactic component, and putsmore focus on the integration of deep parsing out-puts.
While Zhang et al (2008) only used seman-31SyntacticDependencyParsingMST ParserERGGGJaCYSRG[incr tsdb()]PETHPSG ParsingArgument IdentificationArgument ClassificationPredicate ClassificationSemanticRoleLabelingMRSHPSG Syn.Syn.Dep.Figure 1: Joint system architecture.tic features from HPSG parsing in the SRL task, weadded extra syntactic features from deep parsing tohelp both tasks.3 HPSG Parsing for the CoNLL DataDELPH-IN (Deep Linguistic Processing withHPSG) is a repository of open-source software andlinguistic resources for so-called ?deep?
grammat-ical analysis.1 The grammars are rooted in rela-tively detailed, hand-coded linguistic knowledge?including lexical argument structure and the linkingof syntactic functions to thematic arguments?andare intended as general-purpose resources, applica-ble to both parsing and generation.
Semantics inDELPH-IN is cast in the Minimal Recursion Seman-tics framework (MRS; Copestake, Flickinger, Pol-lard, & Sag, 2005), essentially predicate ?
argumentstructures with provision for underspecified scopalrelations.
For the 2009 ?open?
task, we used theDELPH-IN grammars for English (ERG; Flickinger,2000), German (GG; Crysmann, 2005), Japanese(JaCY; Siegel & Bender, 2002), and Spanish (SRG;Marimon, Bel, & Seghezzi, 2007).
The grammarsvary in their stage of development: the ERG com-prises some 15 years of continuous development,whereas work on the SRG only started about fiveyears ago, with GG and JaCY ranging somewhereinbetween.3.1 Overall SetupWe applied the DELPH-IN grammars to the CoNLLdata using the PET parser (Callmeier, 2002) running1See http://www.delph-in.net for background.it through the [incr tsdb()] environment (Oepen &Carroll, 2000), for parallelization and distribution.Also, [incr tsdb()] provides facilities for (re-)trainingthe MaxEnt parse selection models that PET uses fordisambiguation.The two main challenges in applying DELPH-IN resources to parsing CoNLL data were (a) mis-matches in basic assumptions, specifically tokeniza-tion and the inventory of PoS tags provided as part ofthe input, and (b) the need to adapt the resources fornew domains and genres?in particular in terms ofparse disambiguation?as the English and Spanishgrammars at least had not been previously appliedto the corpora used in the CoNLL shared task.The importance of the first of these two aspectsis often underestimated.
A detailed computationalgrammar, inevitably, comes with its own assump-tions about tokenization?the ERG, for example, re-jects the conventional assumptions underlying thePTB (and derived tools).
It opts for an analysis ofpunctuation akin to affixation (rather than as stand-alone tokens), does not break up contracted negatedauxiliaries, and splits hyphenated words like ill-advised into two tokens (the hyphen being part ofthe first component).
Thus, a string like Don?t you!in the CoNLL data is tokenized as the four-elementsequence ?do, n?t, you, !
?,2 whereas the ERG analy-sis has only two leaf nodes: ?don?t, you!
?.Fortunately, the DELPH-IN toolchain recentlyincorporated a mechanism called chart mapping(Adolphs et al, 2008), which allows one to mapflexibly from ?external?
input to grammar-internalassumptions, while keeping track of external tokenidentities and their contributions to the final analysis.The February 2009 release of the ERG already hadthis machinery in place (with the goal of supportingextant, PTB-trained PoS taggers in pre-processinginput to the deep parser), and we found that only atiny number of additional chart mapping rules wasrequired to ?fix up?
CoNLL-specific deviations fromthe PTB tradition.
With the help of the original de-velopers, we created new chart mapping configura-tions for the German and Japanese grammars (with17 and 16 such accomodation rules, respectively) ina similar spirit.
All four DELPH-IN grammars in-2Note that the implied analogy to a non-contracted variant islinguistically mis-leading, as ?Do not you!
is ungrammatical.32clude an account of unknown words, based on un-derspecified ?generic?
lexical entries that are acti-vated from PoS information.The Japenese case was interesting, in thatthe grammar assumes a different pre-processor(ChaSen, rather than Juman), such that not only to-ken boundaries but also PoS tags and morphologicalfeatures had to be mapped.
From our limited ex-perience to date, we found the chart mapping ap-proach adequate in accomodating such discrepan-cies, and the addition of this extra layer of inputprocessing gave substantial gains in parser cover-age (see below).
For the Spanish data, on the otherhand, we found it impossible to make effective useof the PoS and morphological information in theCoNLL data, due to more fundamental discrepan-cies (e.g.
the treatment of enclitics and multi-wordexpressions).3.2 Retraining Disambiguation ModelsThe ERG includes a domain-specific parse selectionmodel (for tourism instructions); GG only a stubmodel trained on a handful of test sentences.
Foruse on the CoNLL data, thus, we had to train newparse selections models, better adapted to the sharedtask corpora.
Disambiguation in PET is realized byconditional MaxEnt models (Toutanova, Manning,Flickinger, & Oepen, 2005), usually trained on fullHPSG treebanks.
Lacking this kind of training ma-terial, we utilized the CoNLL dependency informa-tion instead, by defining an unlabeled dependencyaccuracy (DA) metric for HPSG analyses, essen-tially quantifying the degree of overlap in head ?dependent relations against the CoNLL annotations.Calculating DA for HPSG trees is similar to theprocedure commonly used for extracting bi-lexicaldependencies from phrase structure trees, in a senseeven simpler as HPSG analyses fully determineheadeness.
Taking into account the technical com-plication of token-level mismatches, our DA met-ric loosely corresponds to the unlabeled attachmentscore.
To train CoNLL-specific parse selection mod-els, we parsed the development sections in 500-bestmode (using the existing models) and then mechani-cally ?annotated?
the HPSG analyses with maximumDA as preferred, all others as dis-preferred.
In otherwords, this procedure constructs a ?binarized?
em-pirical distribution where estimation of log-linearGrammar Coverage TimeERG 80.4% 10.06 sGG 28.6% 3.41 sJaCY 42.7% 2.13 sSRG 7.5% 0.80 sTable 1: Performance of the DELPH-IN grammars.model parameters amounts to adjusting conditionalprobabilities towards higher DA values.3Using the [incr tsdb()] MaxEnt experimentationfacilities, we trained new parse selection modelsfor English and German, using the first 16,000 sen-tences of the English training data and the full Ger-man training corpus; seeing that only inputs that (a)parse successfully and (b) have multiple readings,with distinct DA values are relevant to this step, thefinal models reflect close to 13,000 sentences for En-glish, and a little more than 4,000 items for German.Much like in the SRL component, these experimentsare carried out with the TADM software, using ten-fold cross-validation and exact match ranking accu-racy (against the binarized training distribution) tooptimize estimation hyper-parameters3.3 Deep Parsing FeaturesHPSG parsing coverage and average cpu time perinput for the four languages with DELPH-IN gram-mars are summarized in Table 1.
The PoS-basedunknown word mechanism was active for all gram-mars but no other robustness measures (which tendto lower the quality of results) were used, i.e.
onlycomplete spanning HPSG analyses were accepted.Parse times are for 1-best parsing, using selectiveunpacking (Zhang, Oepen, & Carroll, 2007).HPSG parsing outputs are available in several dif-ferent forms.
We investigated two types of struc-tures: syntactic derivations and MRS meaningrep-resentations.
Representative features were extractedfrom both structures and selectively used in the sta-tistical syntactic dependency parsing and semanticrole labeling modules for the ?open?
challenge.3We also experimented with using DA scores directly as em-pirical probabilities in the training distribution (or some func-tion of DA, to make it fall off more sharply), but none ofthese methods seemed to further improve parse selection per-formance.33Deep Semantic Features Similar to Zhang et al(2008), we extract a set of features from the seman-tic outputs (MRS) of the HPSG parses.
These fea-tures represent the basic predicate-argument struc-ture, and provides a simplified semantic view on thetarget sentence.Deep Syntactic Dependency Features A HPSGderivation is a tree structure.
The internal nodes arelabeled with identifiers of grammar rules, and leaveswith lexical entries.
The derivation tree providescomplete information about the actual HPSG anal-ysis, and can be used together with the grammar toreproduce complete feature structure and/or MRS.Given that the shared task adopts dependency rep-resentation, we further map the derivation trees intotoken-token dependencies, labeled by correspondingHPSG rules, by defining a set of head-finding rulesfor each grammar.
This dependency structure is dif-ferent from the dependencies in CoNLL dataset, andprovides an alternative HPSG view on the sentences.We refer to this structure as the dependency back-bone (DB) of the HPSG anaylsis.
A set of featureswere extracted from the deep syntactic dependencystructures.
This includes: i) the POS of the DB par-ent from the predicate and/or argument; ii) DB la-bel of the argument to its parent (only for AI/AC);iii) labeled path from predicate to argument in DB(only for AI/AC); iv) POSes of the predicate?s DBdependents4 Syntactic Dependency ParsingFor the syntactic dependency parsing, we use theMST Parser (McDonald et al, 2005), which is agraph-based approach.
The best parse tree is ac-quired by searching for a spanning tree which max-imizes the score on either a partially or a fully con-nected graph with all words in the sentence as nodes(Eisner, 1996; McDonald et al, 2005).
Based on ourexperience last year, we use the second order settingof the parser, which includes features over pairs ofadjacent edges as well as features over single edgesin the graph.
For the projective or non-projectivesetting, we compare the results on the developmentdatasets of different languages.
According to theparser performance, we decide to use non-projectiveparsing for German, Japanese, and Czech, and useprojective parsing for the rest.For the Closed Challenge, we first considerwhether to use the morphological features.
We findthat except for Czech, parser performs better with-out morphological features on other languages (En-glish and Chinese have no morphological features).As for the other features (i.e.
lemma and pos) givenby the data sets, we also compare the gold standardfeatures and P-columns.
For all languages, the per-formance decreases in the following order: trainingwith gold standard features and evaluating with thegold standard features, training with P-columns andevaluating with P-columns, training with gold stan-dard features and testing with P-columns.
Conse-quently, in the final submission, we take the secondcombination.The goal of the Open Challenge is to see whetherusing external resources can be helpful for the pars-ing performance.
As we mentioned before, ourdeep parser gives us both the syntactic analysis ofthe input sentences using the HPSG formalism andalso the semantic analysis using MRS as the repre-sentation.
However, for the syntactic dependencyparsing, we only extract features from the syntac-tic HPSG analyses and feed them into the MSTParser.
Although, when parsing with gold standardlemma and POS features, our open system outper-forms the closed system on out-domain tests (for En-glish), when parsing with P-columns there is no sub-stantial improvement observed after using the HPSGfeatures.
Therefore, we did not include it in the finalsubmission.5 Semantic Role LabelingThe semantic role labeling component used in thesubmitted system is similar to the one describedby Zhang et al (2008).
Since predicates are indi-cated in the data, the predicate identification mod-ule is removed from this year?s system.
Argumentidentification, argument classification and predicateclassification are the three sub-components in thepipeline.
All of them are MaxEnt-based classifiers.For parameter estimation, we use the open sourceTADM system (Malouf, 2002).The active features used in various steps of SRLare fine tuned separately for different languages us-ing development datasets.
The significance of fea-ture types varies across languages and datasets.34ca zh cs en de ja esSYN Closed 82.67 73.63 75.58 87.90 84.57 91.47 82.69ood - - 71.29 81.50 75.06 - -SRLClosed 67.34 73.20 78.28 77.85 62.95 64.71 67.81ood - - 77.78 67.07 54.87 - -Open - - - 78.13 (?0.28) 64.31 (?1.36) 65.95 (?1.24) 68.24 (?0.43)ood - - - 68.11 (?1.04) 58.42 (?3.55) - -Table 2: Summary of System Performance on Multiple LanguagesIn the open challenge, two groups of extra fea-tures from HPSG parsing outputs, as described inSection 3.3, were used on languages for which wehave HPSG grammars, that is English, German,Japanese, and Spanish.6 Result AnalysisThe evaluation results of the submitted system aresummarized in Table 2.
The overall ranking ofthe system is #7 in the closed challenge, and #2in the open challenge.
While the system achievesmediocre performance, the clear performance dif-ference between the closed and open challenges ofthe semantic role labeler indicates a substantial gainfrom the integration of HPSG parsing outputs.
Themost interesting observation is that even with gram-mars which only achieve very limited coverage, no-ticeable SRL improvements are obtained.
Con-firming the observation of Zhang et al (2008), thegain with HPSG features is more significant on out-domain tests, this time on German as well.The training of the syntactic parsing models forall seven languages with MST parser takes about100 CPU hours with 10 iterations.
The dependencyparsing takes 6 ?
7 CPU hours.
The training and test-ing of the semantic role labeler is much more effi-cient, thanks to the use of MaxEnt models and theefficient parameter estimation software.
The train-ing of all SRL models for 7 languages takes about 3CPU hours in total.
The total time for semantic rolelabeling on test datasets is less than 1 hour.Figure 2 shows the learning curve of the syntacticparser and semantic role labeler on the Czech andEnglish datasets.
While most of the systems con-tinue to improve when trained on larger datasets, anexception was observed with the Czech dataset onthe out-domain test for syntactic accuracy.
In mostof the cases, with the increase of training data, theout-domain test performance of the syntactic parserand semantic role labeler improves slowly relativeto the in-domain test.
For the English dataset, theSRL learning curve climbs more quickly than thoseof syntactic parsers.
This is largely due to the factthat the semantic role annotation is sparser than thesyntactic dependencies.
On the Czech dataset whichhas dense semantic annotation, this effect is not ob-served.7 ConclusionIn this paper, we described our syntactic parsing andsemantic role labeling system participated in bothclosed and open challenge of the (Joint) CoNLL2009 Shared Task.
Four hand-written HPSG gram-mars of a variety of scale have been applied to parsethe datasets, and the outcomes were integrated asfeatures into the semantic role labeler of the sys-tem.
The results clearly show that the integration ofHPSG parsing results in the semantic role labelingtask brings substantial performance improvement.The conclusion of Zhang et al (2008) has been re-confirmed on multiple languages for which we hand-built HPSG grammars exist, even where grammati-cal coverage is low.
Also, the gain is more signifi-cant on out-of-domain tests, indicating that the hy-brid system is more robust to cross-domain varia-tion.ReferencesAdolphs, P., Oepen, S., Callmeier, U., Crysmann, B.,Flickinger, D., & Kiefer, B.
(2008).
Some fine pointsof hybrid natural language parsing.
In Proceedingsof the 6th International Conference on Language Re-sources and Evaluation.
Marrakech, Morocco.Burchardt, A., Erk, K., Frank, A., Kowalski, A., Pado?, S.,& Pinkal, M. (2006).
The SALSA corpus: a Germancorpus resource for lexical semantics.
In Proceedingsof the 4th International Conference on Language Re-sources and Evaluation.
Genoa, Italy.35606570758085900.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Accuracy(%)Training Corpus Size (English)SynSRLSyn-oodSRL-ood69707172737475767778790.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Accuracy(%)Training Corpus Size (Czech)SynSRLSyn-oodSRL-oodFigure 2: Learning curves of syntactic dependency parser and semantic role labeler on Czech and English datasetsCallmeier, U.
(2002).
Preprocessing and encoding tech-niques in PET.
In S. Oepen, D. Flickinger, J. Tsujii, &H. Uszkoreit (Eds.
), Collaborative language engineer-ing.
A case study in efficient grammar-based process-ing.
Stanford, CA: CSLI Publications.Copestake, A., Flickinger, D., Pollard, C., & Sag, I.
A.(2005).
Minimal Recursion Semantics.
An introduc-tion.
Journal of Research on Language and Computa-tion, 3(4), 281 ?
332.Crysmann, B.
(2005).
Relative clause extrapositionin German.
An efficient and portable implementation.Research on Language and Computation, 3(1), 61 ?82.Flickinger, D. (2000).
On building a more efficient gram-mar by exploiting types.
Natural Language Engineer-ing, 6 (1), 15 ?
28.Hajic?, J., Ciaramita, M., Johansson, R., Kawahara, D.,Mart?
?, M. A., Ma`rquez, L., Meyers, A., Nivre, J., Pado?,S., S?te?pa?nek, J., Stran?a?k, P., Surdeanu, M., Xue, N.,& Zhang, Y.
(2009).
The CoNLL-2009 shared task:Syntactic and semantic dependencies in multiple lan-guages.
In Proceedings of the 13th Conference onComputational Natural Language Learning.
Boulder,CO, USA.Hajic?, J., Panevova?, J., Hajic?ova?, E., Sgall, P., Pa-jas, P., S?te?pa?nek, J., Havelka, J., Mikulova?, M., &Z?abokrtsky?, Z.
(2006).
Prague Dependency Treebank2.0 (Nos.
Cat.
No.
LDC2006T01, ISBN 1-58563-370-4).
Philadelphia, PA, USA: Linguistic Data Consor-tium.Kawahara, D., Kurohashi, S., & Hasida, K. (2002).
Con-struction of a Japanese relevance-tagged corpus.
InProceedings of the 3rd International Conference onLanguage Resources and Evaluation (pp.
2008?2013).Las Palmas, Canary Islands.Malouf, R. (2002).
A comparison of algorithms for max-imum entropy parameter estimation.
In Proceedingsof the 6th conferencde on natural language learning(CoNLL 2002) (pp.
49?55).
Taipei, Taiwan.Marimon, M., Bel, N., & Seghezzi, N. (2007).
Test suiteconstruction for a Spanish grammar.
In T. H. King &E. M. Bender (Eds.
), Proceedings of the Grammar En-gineering Across Frameworks workshop (p. 250-264).Stanford, CA: CSLI Publications.Oepen, S., & Carroll, J.
(2000).
Performance profiling forparser engineering.
Natural Language Engineering, 6(1), 81 ?
97.Palmer, M., Kingsbury, P., & Gildea, D. (2005).
TheProposition Bank: An Annotated Corpus of SemanticRoles.
Computational Linguistics, 31(1), 71?106.Palmer, M., & Xue, N. (2009).
Adding semantic rolesto the Chinese Treebank.
Natural Language Engineer-ing, 15(1), 143?172.Siegel, M., & Bender, E. M. (2002).
Efficient deep pro-cessing of Japanese.
In Proceedings of the 3rd work-shop on asian language resources and internationalstandardization at the 19th international conferenceon computational linguistics.
Taipei, Taiwan.Surdeanu, M., Johansson, R., Meyers, A., Ma`rquez, L.,& Nivre, J.
(2008).
The CoNLL-2008 shared task onjoint parsing of syntactic and semantic dependencies.In Proceedings of the 12th Conference on Computa-tional Natural Language Learning.
Manchester, UK.Taule?, M., Mart?
?, M. A., & Recasens, M. (2008).
An-Cora: Multilevel Annotated Corpora for Catalan andSpanish.
In Proceedings of the 6th International Con-ference on Language Resources and Evaluation.
Mar-rakesh, Morroco.Toutanova, K., Manning, C. D., Flickinger, D., & Oepen,S.
(2005).
Stochastic HPSG parse selection using theRedwoods corpus.
Journal of Research on Languageand Computation, 3(1), 83 ?
105.Zhang, Y., Oepen, S., & Carroll, J.
(2007).
Efficiency inunification-based n-best parsing.
In Proceedings of the10th International Conference on Parsing Technolo-gies (pp.
48 ?
59).
Prague, Czech Republic.Zhang, Y., Wang, R., & Uszkoreit, H. (2008).
Hy-brid Learning of Dependency Structures from Hetero-geneous Linguistic Resources.
In Proceedings of theTwelfth Conference on Computational Natural Lan-guage Learning (CoNLL 2008) (pp.
198?202).
Manch-ester, UK.36
