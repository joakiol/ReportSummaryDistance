Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1459?1469,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA Sense-Based Translation Model for Statistical Machine TranslationDeyi Xiong and Min Zhang?Provincial Key Laboratory for Computer Information Processing TechnologySoochow University, Suzhou, China 215006{dyxiong, minzhang}@suda.edu.cnAbstractThe sense in which a word is used deter-mines the translation of the word.
In thispaper, we propose a sense-based transla-tion model to integrate word senses intostatistical machine translation.
We builda broad-coverage sense tagger based ona nonparametric Bayesian topic modelthat automatically learns sense clusters forwords in the source language.
The pro-posed sense-based translation model en-ables the decoder to select appropriatetranslations for source words according tothe inferred senses for these words us-ing maximum entropy classifiers.
Ourmethod is significantly different from pre-vious word sense disambiguation reformu-lated for machine translation in that the lat-ter neglects word senses in nature.
We testthe effectiveness of the proposed sense-based translation model on a large-scaleChinese-to-English translation task.
Re-sults show that the proposed model sub-stantially outperforms not only the base-line but also the previous reformulatedword sense disambiguation.1 IntroductionOne of very common phenomena in language isthat a plenty of words have multiple meanings.In the context of machine translation, such dif-ferent meanings normally produce different targettranslations.
Therefore a natural assumption is thatword sense disambiguation (WSD) may contributeto statistical machine translation (SMT) by provid-ing appropriate word senses for target translationselection with context features (Carpuat and Wu,2005).
?Corresponding authorThis assumption, however, has not been em-pirically verified in the early days.
Carpuat andWu (2005) adopt a standard formulation of WSD:predicting word senses that are defined on anontology for ambiguous words.
As they applyWSD to Chinese-to-English translation, they pre-dict word senses from a Chinese ontology HowNetand project the predicted senses to English glossesprovided by HowNet.
These glosses, used as thesense predictions of their WSD system, are inte-grated into a word-based SMT system either tosubstitute for translation candidates of their trans-lation model or to postedit the output of their SMTsystem.
They report that WSD degenerates thetranslation quality of SMT.In contrast to the standard WSD formulation,Vickrey et al (2005) reformulate the task of WSDfor SMT as predicting possible target translationsrather than senses for ambiguous source words.They show that such a reformulated WSD can im-prove the accuracy of a simplified word translationtask.Following this WSD reformulation for SMT,Chan et al (2007) integrate a state-of-the-artWSD system into a hierarchical phrase-based sys-tem (Chiang, 2005).
Carpuat and Wu (2007) alsouse this reformulated WSD and further adapt it tomulti-word phrasal disambiguation.
They both re-port that the redefined WSD can significantly im-prove SMT.Although this reformulated WSD has provedhelpful for SMT, one question is not answeredyet: are pure word senses useful for SMT?
Theearly WSD for SMT (Carpuat and Wu, 2005)uses projected word senses while the reformu-lated WSD sidesteps word senses.
In this pa-per we would like to re-investigate this questionby resorting to word sense induction (WSI) thatis related to but different from WSD.1We use1We will discuss the relation and difference between WSIand WSD in Section 2.1459WSI to obtain word senses for large-scale data.With these word senses, we study in particular: 1)whether word senses can be directly integrated toSMT to improve translation quality and 2) whetherWSI-based model can outperform the reformu-lated WSD in the context of SMT.In order to incorporate word senses into SMT,we propose a sense-based translation model thatis built on maximum entropy classifiers.
We use anonparametric Bayesian topic model basedWSI toinfer word senses for source words in our training,development and test set.
We collect training in-stances from the sense-tagged training data to trainthe proposed sense-based translation model.
Spe-cially,?
Instead of predicting target translations forambiguous source words as the previous re-formulated WSD does, we first predict wordsenses for ambiguous source words.
The pre-dicted word senses together with other con-text features are then used to predict possibletarget translations for these words.?
Instead of using word senses defined by aprespecified sense inventory as the standardWSD does, we incorporate word senses thatare automatically learned from data into oursense-based translation model.We integrate the proposed sense-based transla-tion model into a state-of-the-art SMT system andconduct experiments on Chines-to-English trans-lation using large-scale training data.
Resultsshow that automatically learned word senses areable to improve translation quality and the sense-based translation model is better than the previousreformulated WSD.The remainder of this paper proceeds as fol-lows.
Section 2 introduces how we obtain wordsenses for our large-scale training data via a WSI-based broad-coverage sense tagger.
Section 3presents our sense-based translation model.
Sec-tion 4 describes how we integrate the sense-basedtranslation model into SMT.
Section 5 elaboratesour experiments on the large-scale Chinese-to-English translation task.
Section 6 introduces re-lated studies and highlights significant differencesfrom them.
Finally, we conclude in Section 7 withfuture directions.2 WSI-Based Broad-Coverage SenseTaggerIn order to obtain word senses for any sourcewords, we build a broad-coverage sense taggerthat relies on the nonparametric Bayesian modelbased word sense induction.
We first describeWSI, especially WSI based on the HierarchicalDirichlet Process (HDP) (Teh et al, 2004), a non-parametric version of Latent Dirichlet Allocation(LDA) (Blei et al, 2003).
We then elaborate howwe use the HDP-based WSI to predict sense clus-ters and to annotate source words in our train-ing/development/test sets with these sense clus-ters.2.1 Word Sense InductionBefore we introduce WSI, we differentiate wordtype from word token.
A word type refers to aunique word as a vocabulary entry while a wordtoken is an occurrence of a word type.
Take thefirst sentence of this paragraph as an example, ithas 11 word tokens but 9 word types as there aretwo word tokens of the word type ?we?
and twotokens of the word type ?word?.Word sense induction is a task of automaticallyinducing the underlying senses of word tokensgiven the surrounding contexts where the wordtokens occur.
The biggest difference from wordsense disambiguation lies in that WSI does notrely on a predefined sense inventory.
Such a pre-specified list of senses is normally assumed byWSD which predicts senses of word tokens usingthis given inventory.
From this perspective, WSIcan be treated as a clustering problem while WSDa classification one.Various clustering algorithms, such as k-means,have been previously used for WSI.
Recently, wehave also witnessed that WSI is cast as a topicmodeling problem where the sense clusters of aword type are considered as underlying topics(Brody and Lapata, 2009; Yao and Durme, 2011;Lau et al, 2012).
We follow this line to tailor atopic modeling framework to induce word sensesfor our large-scale training data.In the topic-based WSI, surrounding context ofa word token is considered as a pseudo documentof the corresponding word type.
A pseudo docu-ment is composed of either a bag of neighboringwords of a word token, or the Part-to-Speech tagsof neighboring words, or other contextual infor-mation elements.
In this paper, we define a pseudo1460document as ?N neighboring words centered ona given word token.
Table 1 shows examples ofpseudo documents for a Chinese word ?w?nglu??(network).
These two pseudo documents are ex-tracted from a sentence listed in the first row of Ta-ble 1.
Here we set N = 5.
We can extract as manypseudo documents as the number of word tokensof a given word type that occur in training data.The collection of all these extracted pseudo docu-ments of the given word type forms a corpus.
Wecan induce topics on this corpus for each pseudodocument via topic modeling approaches.Figure 1(a) shows the LDA-based WSI for agiven word type W .
The outer plate representsreplicates of pseudo documents which consist ofN neighboring words centered on the tokens ofthe given word type W .
wj,iis the i-th word ofthe j-th pseudo document of the given word typeW .
sj,iis the sense assigned to the word wj,i.The conventional topic distribution ?jfor the j-th pseudo document is taken as the the distribu-tion over senses for the given word type W .
TheLDA generative process for sense induction is asfollows: 1) for each pseudo document Dj, draw aper-document sense distribution ?jfrom a Dirich-let distribution Dir(?
); 2) for each item wj,iin thepseudo document Dj, 2.1) draw a sense clustersj,i?
Multinomial(?j); and 2.2) draw a wordwj,i?
?sj,iwhere ?sj,iis the distribution ofsense sj,iover words drawn from a Dirichlet dis-tribution Dir(?
).As LDA needs to manually specify the num-ber of senses (topics), a better idea is to let thetraining data automatically determine the numberof senses for each word type.
Therefore we re-sort to the HDP, a natural nonparametric gener-alization of LDA, for the inference of both senseclusters and the number of sense clusters follow-ing Lau et al (2012) and Yao and Durme (2011).The HDP for WSI is shown in Figure 1(b).
TheHDP generative process for word sense inductionis as follows: 1) sample a base distribution G0from a Dirichlet process DP(?,H) with a con-centration parameter ?
and a base distribution H;2) for each pseudo document Dj, sample a dis-tribution Gj?
DP(?0, G0); 3) for each itemwj,iin the pseudo document Dj, 3.1) sample asense cluster sj,i?
Gj; and 3.2) sample a wordwj,i?
?sj,i.
Here G0is a global distributionover sense clusters that are shared by all Gj.
Gjisa per-document sense distribution over these sensewj,i?
?jsj,ij ?
[1, J]?kk ?
[1,K]?G0Gjsj,ij ?
[1, J]wj,iH?
?0(a) (b)i ?
[1, Nj ]i ?
[1, Nj ]Figure 1: Graphical model representations of (a)Latent Dirichlet Allocation for WSI, (b) Hierar-chical Dirichlet Process for WSI.clusters, which has its own document-specific pro-portions of these sense clusters.
The hyperparam-eter ?, ?0in the HDP are both concentration pa-rameters which control the variability of senses inthe global distribution G0and document-specificdistribution Gj.The HDP/LDA-based WSI complies with thedistributional hypothesis that states that words oc-curring in the same contexts tend to have similarmeanings.
We want to extend this hypothesis tomachine translation by building sense-based trans-lation model upon the HDP-based word sense in-duction: words with the same meanings tend to betranslated in the same way.2.2 Word Sense TaggingWe adopt the HDP-based WSI to automaticallypredict word senses and use these predicted sensesto annotate source words.
We individually build aHDP-based WSI model per word type and trainthese models on the training data.
The sense for aword token is defined as the most probable senseaccording to the per-document sense distributionGjestimated for the corresponding pseudo doc-ument that represents the surrounding context ofthe word token.
In particular, we take the follow-ing steps.1461t?
t?x?ng w?gu?
w?nglu?
y?ny?ng zh?
zh?y?
f?ngf?n h?ik?
g?ngj?
?
qu?b?o w?nglu?
?nqu?n?Pseudo Documents for word ?w?nglu??t?
t?x?ng w?gu?
w?nglu?
y?ny?ng zh?
zh?y?
f?ngf?n h?ik?f?ngf?n h?ik?
g?ngj?
?
qu?b?o w?nglu?
?nqu?n?Table 1: Examples of pseudo documents extracted from a Chinese sentence (written in Chinese Pinyin).?
Data preprocessing We preprocess thesource side of our bilingual training data aswell as development and test set by removingstop words and rare words.?
Training Data Sense Annotation From thepreprocessed training data, we extract allpossible pseudo documents for each sourceword type.
The collection of these extractedpseudo documents is used as a corpus to traina HDP-based WSI model for the source wordtype.
In this way, we can train as many HDP-based WSI models as the number of wordtypes kept after preprocessing.
The sensewith the highest probability output by theHDP-based WSI model for each pseudo doc-ument is used as the sense cluster to label thecorresponding word token.?
Test/Dev Data Sense Annotation From thepreprocessed test data, we can also extractpseudo documents for each source word typethat occur in the test/dev set.
Using thetrained HDP-based WSI model that corre-spond to the source word type in question, wecan obtain the best sense assignment for eachpseudo document of the word type, whichin turn is used to annotate the correspondingword token in the test/dev data.3 Sense-Based Translation ModelIn this section we present our sense-based transla-tion model and describe the features that we use aswell as the training process of this model.3.1 ModelThe sense-based translation model estimates theprobability that a source word c is translated into atarget phrase e?
given contextual information, in-cluding word senses that are obtained using theHDP-based WSI as described in the last section.We allow the target phrase e?
to be either a phraseof length up to 3 words or NULL so that we cancapture both multi-word and null translations.
Theessential component of the model is a maximumentropy (MaxEnt) based classifier that is used topredict the translation probability p(e?|C(c)).
TheMaxEnt classifier can be formulated as follows.p(e?|C(c)) =exp(?i?ihi(e?, C(c)))?e??exp(?i?ihi(e?
?, C(c)))(1)where his are binary features, ?is are weights ofthese features, C(c) is the surrounding context ofc.We define two groups of binary features: 1) lex-icon features and 2) sense features.
All these fea-tures take the following form.h(e?, C(c)) ={1, if e?
= 2 and C(c).?
= ?0, else(2)where 2 is a placeholder for a possible targettranslation (up to 3 words or NULL), ?
is the nameof a contextual (lexicon or sense) feature for thesource word c, and the symbol ?
represents thevalue of the feature ?.We extract both the lexicon and sense featuresfrom a ?k-word window centered on the word c.The lexicon features are defined as the precedingk words, the succeeding k words and the word citself: {c?k, ..., c?1, c, c1, ..., ck}.
The sense fea-tures are defined as the predicted senses for thesewords: {sc?k, ..., sc?1, sc, sc1, ..., sck}.As we also use these neighboring words to pre-dict word senses in the HDP-based WSI, the infor-mation provided by the lexicon and sense featuresmay overlap.
This is not a issue for the MaxEntclassifier as it can deal with arbitrary overlappingfeatures (Berger et al, 1996).
One may also won-der whether the sense features can contribute toSMT new information that can NOT be obtainedfrom the lexicon features.
First, we believe thatthe senses induced by the HDP-based WSI pro-vide a different view of data than that of the lex-icon features.
Second, the sense features containsemantic distributional information learned by theHDP across contexts where lexical words occur.Third, we empirically investigate this doubt bycomparing two MaxEnt-based translation models1462in Section 5.
One model only uses the lexicon fea-tures while the other integrates both the lexiconand sense features.
The former model can be con-sidered as a reformulated WSD for SMT as we de-scribed in Section 1.Given a source sentence {ci}I1, the proposedsense-based translation model Mscan be denotedasMs=?ci?W(e?i|C(ci)) (3)where W is a set of words for which we buildMaxEnt classifiers (see the next subsection for thediscussion on how we build MaxEnt classifiers forour sense-based translation model).3.2 TrainingThe training of the proposed sense-based transla-tion model is a process of estimating the featureweights ?s in the equation (1).
There are twostrategies that we can use to obtain these weights.We can either build an all-in-one MaxEnt clas-sifier that integrates all source word types c andtheir possible target translations e?
or build multi-ple MaxEnt classifiers.
If we train the all-in-oneclassifier, we have to predict millions of classes(target translations of length up to 3 words).
Thisis normally intractable in practice.
Therefore wetake the second strategy: building multiple Max-Ent classifiers with one classifier per source wordtype.In order to train these classifiers, we have to col-lect training events from our word-aligned bilin-gual training data where source words are anno-tated with their corresponding sense clusters pre-dicted by the HDP-based WSI as described inSection 2.
A training event for a source word cconsists of all contextual elements in the form ofC(c).?
= ?
defined in the last subsection and thetarget translation e?.
Using these collected events,we can train our multiple classifiers.
In prac-tice, we do not build MaxEnt classifiers for sourcewords that occur less than 10 times in the train-ing data and run the MaxEnt toolkit in a parallelmanner in order to expedite the training process.4 Decoding with Sense-BasedTranslation ModelThe sense-based translation model describedabove is integrated into the log-linear translationmodel of SMT as a sense-based knowledge source.The weight of this model is tuned by the minimumsource sentences HDP-basedWSIsense-taggedsource sentencesMaxEntclassifierssense-basedtranslation modeldecodertarget sentencesothermodelsFigure 2: Architecture of SMT system with thesense-based translation model.error rate training (MERT) (Och, 2003) togetherwith other models such as the language model.Figure 2 shows the architecture of the SMTsystem enhanced with the sense-based translationmodel.
Before we translate a source sentence, weuse the HDP-based WSI models trained on thetraining data to predict senses for word tokens oc-curring in the source sentence as discussed in Sec-tion 2.2.
Note that the HDP-based WSI does notpredict senses for all words due to the followingtwo reasons.?
We do not train HDP-based WSI models forword types for which we extract more than Tpseudo documents.2?
In the test/dev set, there are some words thatare unseen in the training data.
These un-seen words, of course, do not have their HDP-based WSI models.For these words, we set a default sense (i.e.
sc=s1).Sense tagging on test sentences can be done ina preprocessing step.
Once we get sense clus-ters for word tokens in test sentences, we loadpre-trained MaxEnt classifiers of the correspond-ing word types.
During decoding, we keep wordalignments for each translation rule.
Whenever anew source word c is translated, we find its trans-lation e?
via the kept word alignments.
We thencalculate the translation probability p(e?|C(c)) ac-cording to the equation (1) using the correspond-ing loaded classifier.
In this way, we can easilycalculate the sense-based translation model score.2we set T = 20, 000.14635 ExperimentsIn this section, we carried out a series of ex-periments on Chinese-to-English translation us-ing large-scale bilingual training data.
In order tobuild the proposed sense-based translation model,we annotated the source part of the bilingual train-ing data with word senses induced by the HDP-based WSI.
With the trained sense-based transla-tion model, we would like to investigate the fol-lowing two questions:?
Do word senses automatically induced by theHDP-basedWSI improve translation quality??
Does the sense-based translation model out-perform the reformulated WSD for SMT?5.1 SetupOur baseline system is a state-of-the-art SMTsystem which adapts Bracketing TransductionGrammars (Wu, 1997) to phrasal translation andequips itself with a maximum entropy basedreordering model (Xiong et al, 2006).
We usedLDC corpora LDC2004E12, LDC2004T08,LDC2005T10, LDC2003E14, LDC2002E18,LDC2005T06, LDC2003E07, LDC2004T07 asour bilingual training data which consists of3.84M bilingual sentences, 109.5M English wordtokens and 96.9M Chinese word tokens.
We ranGiza++ on the training data in two directionsand applied the ?grow-diag-final?
refinementrule (Koehn et al, 2003) to obtain word align-ments.
From the word-aligned data, we extractedweighted phrase pairs to generate our phrasetable.
We trained a 5-gram language model on theXinhua section of the English Gigaword corpus(306 million words) using the SRILM toolkit(Stolcke, 2002) with the modified Kneser-Neysmoothing (Chen and Goodman, 1996).We trained our HDP-based WSI models via theC++ HDP toolkit3(Wang and Blei, 2012).
Weset the hyperparameters ?
= 0.1 and ?0= 1.0following Lau et al (2012).We extracted pseudodocuments from a ?10-word window centered onthe corresponding word token for each word typefollowing Brody and Lapata (2009).
As describedin Section 2.2, we preprocessed the source partof our bilingual training data by removing stopwords and infrequent words that occurs less than3http://www.cs.cmu.edu/?chongw/resource.htmlTraining Test# Word Types 67,723 4,348# Total Pseudo Documents 27.73M 11,777# Avg Pseudo Documents 427.79 2.71# Total Senses 271,770 24,162# Avg Senses 4.01 5.56Table 2: Statistics of the HDP-based word senseinduction on the training and test data.10 times in the training data.
From the prepro-cessed data, we extracted pseudo documents foreach word type to train a HDP-based WSI modelper word type.
Note that we do not build WSImodels for highly frequent words that occur morethan 20,000 times in order to expedite the HDPtraining process.We trained our MaxEnt classifiers with the off-the-shelf MaxEnt tool.4We performed 100 iter-ations of the L-BFGS algorithm implemented inthe training toolkit on the collected training eventsfrom the sense-annotated data as described in Sec-tion 3.2.
We set the Gaussian prior to 1 to avoidoverfitting.
On average, we obtained 346 classes(target translations) per source word type with themaximum number of classes being 256,243.
Ittook an average of 57.5 seconds for training aMaxent classifier.We used the NIST MT03 evaluation test data asour development set, and the NIST MT05 as thetest set.
We evaluated translation quality with thecase-insensitive BLEU-4 (Papineni et al, 2002)and NIST (Doddington, 2002).
In order to al-leviate the impact of MERT (Och, 2003) insta-bility, we followed the suggestion of Clark et al(2011) to run MERT three times and report aver-age BLEU/NIST scores over the three runs for allour experiments.5.2 Statistics and Examples of Word SensesBefore we present our experiment results of thesense-based translation model, we study somestatistics of the HDP-based WSI on the trainingand test data.
We show these statistics in Table 2.There are 67,723 and 4,348 unique word types inthe training and test data after the preprocessingstep.
For these word types, we extract 27.73M and11,777 pseudo documents from the training andtest set respectively.
On average, there are 427.794http://homepages.inf.ed.ac.uk/lzhang10/maxenttoolkit.html1464System BLEU(%) NISTSTM (?5w) 34.64 9.4346STM (?10w) 34.76 9.5114STM (?15w) - -Table 4: Experiment results of the sense-basedtranslation model (STM) with lexicon and sensefeatures extracted from a window of size varyingfrom ?5 to ?15 words on the development set.pseudo documents per word type in the trainingdata and 2.71 in the test set.
The HDP-basedWSI learns 271,770 word senses in total using thepseudo documents collected from the training dataand infers 24,162 word senses using the pseudodocuments extracted from the test set.
There are4.01 different senses per word type in the trainingdata and 5.56 in the test set on average.Table 3 illustrates six different senses of theword ???
(operate)?
learned by the HDP-basedWSI in the training data.
We also show the mostprobable 10 words for each sense cluster.
Sense s1represents the operations of company or organi-zation, sense s2denotes country/institution/inter-nation operations, sense s3refers to market opera-tions, sense s4corresponds to business operations,sense s5to public facility operations, and finallys6to economy operations.5.3 Impact of Window Size k used in MaxEntClassifiersOur first group of experiments were conducted toinvestigate the impact of the window size k ontranslation performance in terms of BLEU/NISTon the development set.
We extracted both the lex-icon and sense features from a ?k-word windowfor our MaxEnt classifiers.
We varied k from 5to 15.
Experiment results are shown in Table 4.We achieve the best performance when k = 10.This suggests that a ?10-word window context issufficient for predicting target translations for am-biguous source words.
We therefore set k = 10for all experiments thereafter.5.4 Effect of the Sense-Based TranslationModelOur second group of experiments were carried outto investigate whether the sense-base translationmodel is able to improve translation quality bycomparing the system enhanced with our sense-based translation model against the baseline.
Wealso studied the impact of word senses induced bySystem BLEU(%) NISTBase 33.53 9.0561STM (sense) 34.15 9.2596STM (sense+lexicon) 34.73 9.4184Table 5: Experiment results of the sense-basedtranslation model (STM) against the baseline.System BLEU(%) NISTBase 33.53 9.0561Reformulated WSD 34.16 9.3820STM 34.73 9.4184Table 6: Comparison results of the sense-basedtranslation model vs. the reformulated WSD forSMT.the HDP-based WSI on translation performanceby enforcing the sense-based translation model touse only sense features.
Table 5 shows the experi-ment results.
From the table, we can observe that?
Our sense-based translation model achievesa substantial improvement of 1.2 BLEUpoints over the baseline.
This indicates thatthe sense-based translation model is able tohelp select correct translations for ambiguoussource words.?
If we only integrate sense features intothe sense-based translation model, we canstill outperform the baseline by 0.62 BLEUpoints.
This suggests that automatically in-duced word senses alone are indeed useful formachine translation.5.5 Comparison to Word SenseDisambiguationAs we mentioned in Section 3.1, our sense-basedtranslation model can be degenerated to a reformu-lated WSD model for SMT if we only use lexiconfeatures in MaxEnt classifiers.
This allows us todirectly compare our method against the reformu-lated WSD for SMT.
Table 6 shows the compari-son result.From the table, we can find that the sense-based translation model outperforms the reformu-lated WSD by 0.57 BLEU points.
This suggeststhat the HDP-based word sense induction is bet-ter than the reformulated WSD in the context ofSMT.
Furthermore, as the reformulated WSD isa degenerated version of our sense-based transla-tion model which only uses the lexicon features,1465s1s2s3??
(operate) ??
(operate) ??
(operate)??
(facility) ??
(satellite) ??
(market)??
(plan) ??
(system) ??
(enterprise)??
(foundation) ??
(country) ??
(competition)??
(project) ??
(supply) ??
(assets)??
(company) ??
(inter-nation) ??
(profit)??
(structure) ??
(institution) ??
(cause)??
(service) ??
(proceed) ??
(cost)??
(organization) ??
(center) ??
(capital)??
(supply) ??
(cooperate) ??
(business)s4s5s6??
(cost) ??
(city) ??
(lie)??
(share price) ??
(process) ??
(photograph)27000 ???
(tap-water) 119???
(Kosovo) ??
(factory) DPRK??
(extra) ??
(car) ??
(insurance)??
(wage) ??
(railway) ??
(overspend)??
(dollar) ??
(sewage) ??
(position)??
(commerce) ???
(office) ??
(economy)??
(income) ??
(break-even) ???
(competitor)???
(railway administration) ??
(component) ??
(balance)Table 3: Six different senses learned for the word ????
from the training data.the sense features used in our model do providenew information that can not be obtained by thelexicon features.6 Related WorkIn this section we introduce previous studies thatare related to our work.
For ease of comparison,we roughly divide them into 4 categories: 1) WSDfor SMT, 2) topic-based WSI, 3) topic model forSMT and 4) lexical selection.WSD for SMT As we mentioned in Section1, WSD has been successfully reformulated andadapted to SMT (Vickrey et al, 2005; Carpuat andWu, 2007; Chan et al, 2007).
Rather than predict-ing word senses for ambiguous words, the refor-mulated WSD directly predicts target translationsfor source words with context information.
Oursense-based translation model also predicts targettranslations for SMT.
The significant difference isthat we predict word senses automatically learnedfrom data and incorporate these predicted sensesinto SMT.
Our experiments show that such wordsenses are able to improve translation quality.Topic-based WSI Topic-based WSI can beconsidered as the foundation of our work as weuse it to obtain broad-coverage word senses to an-notate our large-scale training data.
Brody and La-pata (2009)?s work is the first attempt to approachWSI via topic modeling.
They adapt LDA to wordsense induction by building one topic model perword type.
According to them, there are 3 sig-nificant differences between topic-based WSI andgeneric topic modeling.?
First, the goal of topic-based WSI is to di-vide contexts of a word type into differentcategories, each representing a sense cluster.However generic topic models aim at topicdistributions of documents.?
Second, generic topic modeling exploreswhole documents for topic inference whiletopic-based WSI uses much smaller units ina document (e.g., surrounding words of a tar-get word) for word sense induction.?
Finally, the number of induced word sensesin WSI is usually less than 10 while the num-ber of inferred topics in generic topic model-ing is tens or hundreds.As LDA-based WSI needs to manually spec-ify the number of word senses, Yao and Durme(2011) propose HDP-based WSI that is capable of1466determining the number of senses for each wordtype according to training data.
Lau et al (2012)adopt the HDP-based WSI for novel sense de-tection and empirically show that the HDP-basedWSI is better than the LDA-based WSI.
We followthem to set the hyperparameters of HDP for train-ing and incorporate automatically induced wordsenses into SMT in our work.Topic model for SMT Generic topic modelsare also explored for SMT.
Zhao and Xing (2007)propose a bilingual topic model and integrate atopic-specific lexicon translation model into SMT.Tam et al (2007) also explore a bilingual topicmodel for translation and language model adapta-tion.
Foster and Kunh (2007) introduce a mixturemodel approach for translation model adaptation.Xiao et al (2012) propose a topic-based similar-ity model for rule selection in hierarchical phrase-based translation.
Xiong and Zhang (2013) em-ploy a sentence-level topic model to capture co-herence for document-level machine translation.The difference between our work and these pre-vious studies on topic model for SMT lies in thatwe adopt topic-based WSI to obtain word sensesrather than generic topics and integrate inducedword senses into machine translation.Lexical selection Our work is also related tolexical selection in SMT where appropriate targetlexical items for source words are selected by astatistical model with context information (Banga-lore et al, 2007; Mauser et al, 2009).
The refor-mulated WSD discussed before can also be con-sidered as a lexical selection model.
The signif-icant difference from these studies is that we per-form lexical selection using automatically inducedword senses by the HDP on the source side.7 ConclusionWe have presented a sense-based translationmodel that integrates word senses into machinetranslation.
We capitalize on the broad-coverageword sense induction system that is built on thenonparametric Bayesian HDP to learn sense clus-ters for words in the source language.
We gen-erate pseudo documents for word tokens in thetraining/test data for the HDP-based WSI systemto infer topics.
The most probable topic inferredfor a pseudo document is taken as the sense ofthe corresponding word token.
We incorporatethese learned word senses as translation evidencesinto maximum entropy classifiers which form thefoundation of the proposed sense-based translationmodel.We carried out a series of experiments to vali-date the effectiveness of the sense-based transla-tion by comparing the model against the baselineand the previous reformulated WSD.
Our experi-ment results show that?
The sense-based translation model is able tosubstantially improve translation quality interms of both BLEU and NIST.?
The sense-based translation model is alsobetter than the previous reformulated WSDfor SMT.?
Word senses automatically induced by theHDP-based WSI on large-scale training dataare very useful for machine translation.
Tothe best of our knowledge, this is the first at-tempt to empirically verify the positive im-pact of word senses on translation quality.Comparing with macro topics of documents in-ferred by LDA with bag of words from the wholedocuments, word senses inferred by the HDP-based WSI can be considered as micro topics.
Inthe future, we would like to explore both the microand macro topics for machine translation.
Addi-tionally, we also want to induce sense clusters forwords in the target language so that we can buildsense-based language model and integrate it intoSMT.
We would like to investigate whether auto-matically learned senses of proceeding words arehelpful for predicting succeeding words.AcknowledgementThe work was sponsored by the National Natu-ral Science Foundation of China under projects61373095 and 61333018.
We would like to thankthree anonymous reviewers for their insightfulcomments.ReferencesSrinivas Bangalore, Patrick Haffner, and Stephan Kan-thak.
2007.
Statistical Machine Translation throughGlobal Lexical Selection and Sentence Reconstruc-tion.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages152?159, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.1467Adam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A Maximum Entropy Ap-proach to Natural Language Processing.
Computa-tional Linguistics, 22(1):39?71.David M. Blei, Andrew Y. Ng, Michael I. Jordan,and John Lafferty.
2003.
Latent Dirichlet Al-location.
Journal of Machine Learning Research,3:993?1022.Samuel Brody and Mirella Lapata.
2009.
BayesianWord Sense Induction.
In Proceedings of the12th Conference of the European Chapter of theACL (EACL 2009), pages 103?111, Athens, Greece,March.
Association for Computational Linguistics.Marine Carpuat and Dekai Wu.
2005.
Word SenseDisambiguation vs. Statistical Machine Transla-tion.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics (ACL?05), pages 387?394, Ann Arbor, Michi-gan, June.
Association for Computational Linguis-tics.Marine Carpuat and Dekai Wu.
2007.
ImprovingStatistical Machine Translation Using Word SenseDisambiguation.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 61?72.Yee Seng Chan, Hwee Tou Ng, and David Chiang.2007.
Word Sense Disambiguation Improves Sta-tistical Machine Translation.
In Proceedings of the45th Annual Meeting of the Association of Com-putational Linguistics, pages 33?40, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Stanley F. Chen and Joshua Goodman.
1996.
AnEmpirical Study of Smoothing Techniques for Lan-guage Modeling.
In Proceedings of the 34th An-nual Meeting on Association for Computational Lin-guistics, ACL ?96, pages 310?318, Stroudsburg, PA,USA.
Association for Computational Linguistics.David Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL?05), pages263?270, Ann Arbor, Michigan, June.
Associationfor Computational Linguistics.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better Hypothesis Testing forStatistical Machine Translation: Controlling for Op-timizer Instability.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages176?181, Portland, Oregon, USA, June.George Doddington.
2002.
Automatic Evaluationof Machine Translation Quality Using N-gram Co-occurrence Statistics.
In Proceedings of the Sec-ond International Conference on Human LanguageTechnology Research, HLT ?02, pages 138?145, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.George Foster and Roland Kuhn.
2007.
Mixture-Model Adaptation for SMT.
In Proc.
of the SecondWorkshop on Statistical Machine Translation, pages128?135, Prague, Czech Republic, June.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of the 2003 Human Language TechnologyConference of the North American Chapter of theAssociation for Computational Linguistics, pages58?54, Edmonton, Canada, May-June.Jey Han Lau, Paul Cook, Diana McCarthy, David New-man, and Timothy Baldwin.
2012.
Word SenseInduction for Novel Sense Detection.
In Proceed-ings of the 13th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 591?601, Avignon, France, April.
Associationfor Computational Linguistics.Arne Mauser, Sa?sa Hasan, and Hermann Ney.
2009.Extending Statistical Machine Translation with Dis-criminative and Trigger-Based Lexicon Models.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages210?218, Singapore, August.
Association for Com-putational Linguistics.Franz Josef Och.
2003.
Minimum Error Rate Train-ing in Statistical Machine Translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan, July.
Association for ComputationalLinguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.Yik-Cheung Tam, Ian R. Lane, and Tanja Schultz.2007.
Bilingual LSA-based adaptation for statis-tical machine translation.
Machine Translation,21(4):187?207.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,and David M. Blei.
2004.
Hierarchical Dirichletprocesses.
Journal of the American Statistical Asso-ciation, 101.David Vickrey, Luke Biewald, Marc Teyssier, andDaphne Koller.
2005.
Word-Sense Disambiguationfor Machine Translation.
In HLT/EMNLP.
The As-sociation for Computational Linguistics.C.
Wang and D. M. Blei.
2012.
A Split-Merge MCMCAlgorithm for the Hierarchical Dirichlet Process.ArXiv e-prints, January.1468Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, andShouxun Lin.
2012.
A Topic Similarity Model forHierarchical Phrase-based Translation.
In Proceed-ings of the 50th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 750?758, Jeju Island, Korea, July.
As-sociation for Computational Linguistics.Deyi Xiong and Min Zhang.
2013.
A Topic-Based Co-herence Model for Statistical Machine Translation.In Proceedings of the Twenty-Seventh AAAI Confer-ence on Artificial Intelligence (AAAI-13), Bellevue,Washington, USA, July.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Max-imum Entropy Based Phrase Reordering Model forStatistical Machine Translation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 521?528,Sydney, Australia, July.Xuchen Yao and Benjamin Van Durme.
2011.
Non-parametric Bayesian Word Sense Induction.
InProceedings of TextGraphs-6: Graph-based Meth-ods for Natural Language Processing, pages 10?14,Portland, Oregon, June.
Association for Computa-tional Linguistics.Bin Zhao and Eric P. Xing.
2007.
HM-BiTAM:Bilingual Topic Exploration, Word Alignment, andTranslation.
In Proc.
NIPS 2007.1469
