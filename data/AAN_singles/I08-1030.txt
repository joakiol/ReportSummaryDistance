Chinese Unknown Word Translation by Subword Re-segmentationRuiqiang Zhang1,2 and Eiichiro Sumita1,21National Institute of Information and Communications Technology2ATR Spoken Language Communication Research Laboratories2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0288, Japan{ruiqiang.zhang, eiichiro.sumita}@{nict.go.jp, atr.jp}AbstractWe propose a general approach for trans-lating Chinese unknown words (UNK) forSMT.
This approach takes advantage ofthe properties of Chinese word compositionrules, i.e., all Chinese words are formedby sequential characters.
According to theproposed approach, the unknown word isre-split into a subword sequence followedby subword translation with a subword-based translation model.
?Subword?
is aunit between character and long word.
Wefound the proposed approach significantlyimproved translation quality on the test dataof NIST MT04 and MT05.
We also foundthat the translation quality was further im-proved if we applied named entity transla-tion to translate parts of unknown words be-fore using the subword-based translation.1 IntroductionThe use of phrase-based translation has led to greatprogress in statistical machine translation (SMT).Basically, the mechanism of this approach is re-alized by two steps:training and decoding.
In thetraining phase, bilingual parallel sentences are pre-processed and aligned using alignment algorithms ortools such as GIZA++ (Och and Ney, 2003).
Phrasepairs are then extracted to be a phrase translation ta-ble.
Probabilities of a few pre-defined features arecomputed and assigned to the phrase pairs.
The fi-nal outcome of the training is a translation table con-sisting of source phrases, target phrases, and listsof probabilities of features.
In the decoding phase,the translation of a test source sentence is made byreordering the target phrases corresponding to thesource phrases, and searching for the best hypothesisthat yields the highest scores defined by the searchcriterion.However, this mechanism cannot solve unknownword translation problems.
Unknown words (UNK)point to those unseen words in the training or non-existing words in the translation table.
One strat-egy to deal with translating unknown words is to re-move them from the target sentence without transla-tion on assumption of fewer UNKs in the test data.Of course, this simple way produces a lower qualityof translations if there are a lot of UNKs in the testdata, especially for using a Chinese word segmenterthat produces many UNKs.
The translation of UNKsneed to be solved by a special method.The translation of Chinese unknown words seemsmore difficult than other languages because Chineselanguage is a non-inflected language.
Unlike otherlanguages (Yang and Kirchhoff, 2006; Nie?len andNey, 2000; Goldwater and McClosky, 2005), Chi-nese UNK translation cannot use information fromstem and inflection analysis.
Using machine translit-eration can resolve part of UNK translation (Knightand Graehl, 1997).
But this approach is effective fortranslating phonetically related unknown words, notfor other types.
No unified approach for translatingChinese unknown words has been proposed.In this paper we propose a novel statistics-basedapproach for unknown word translation.
This ap-proach uses the properties of Chinese word compo-sition rules ?
Chinese words are composed of oneor more Chinese characters.
We can split longer un-known words into a sequence of smaller units: char-acters or subwords.
We train a subword based trans-lation model and use the model to translate the sub-225word sequence.
Thus we get the translation of theUNKs.
We call this approach ?subword-based un-known word translation?.In what follows, section 2 reviews phrase-basedSMT.
section 3 describes the dictionary-based CWS,that is the main CWS in this work.
Section 4 de-scribes our named entity recognition approach.
Sec-tion 5 describes the subword-based approach forUNK translation.
Section 7 describes the experi-ments we conducted to evaluate our subword ap-proach for translating Chinese unknown words.
Sec-tion 8 describes existing methods for UNK transla-tions for other languages than Chinese.
Section 9briefly summarizes the main points of this work.2 Phrase-based statistical machinetranslationPhrase-based SMT uses a framework of log-linearmodels (Och, 2003) to integrate multiple features.For Chinese to English translation, source sentenceC is translated into target sentence E using a proba-bility model:P?
(E|C) =exp(?Mi=1 ?i fi(C, E))?E?
exp(?Mi=1 ?i fi(C, E?))?
= {?M1 , }(1)where fi(C, E) is the logarithmic value of the i-thfeature, and ?i is the weight of the i-th feature.
Thecandidate target sentence that maximizes P(E|C) isthe solution.Obviously, the performance of such a model de-pends on the qualities of its features.
We used thefollowing features in this work.?
Target language model: an N-gram languagemodel is used.?
Phrase translation model p(e| f ): gives theprobability of the target phrases for each sourcephrase.?
Phrase inverse probability p( f |e): the probabil-ity of a source phrase for a given target phrase.It is the coupled feature of the last one.?
Lexical probability lex(e| f , a): the sum of thetarget word probabilities for the given sourcewords and the alignment of the phrase pairs.?
Lexical inverse probability lex( f |e, a): the sumof the source word probabilities for the giventarget words and alignment.?
Target phrase length model #(p): the number ofphrases included in the translation hypothesis.?
Target word penalty model: the number ofwords included in the translation hypothesis.?
Distance model #(w): the number of words be-tween the tail word of one source phrase andthe head word of the next source phrase.In general, the following steps are used to get theabove features.1.
Data processing: segment Chinese words andtokenize the English.2.
Word alignment: apply two-way word align-ment using GIZA++.3.
Lexical translation: calculate word lexicalprobabilities.4.
Phrase extraction: extract source target bilin-gual pairs by means of union, intersection, et.al.5.
Phrase probability calculation: calculate phrasetranslation probability.6.
Lexical probability: generate word lexicalprobabilities for phrase pairs.7.
Minimal error rate training: find a solution tothe ?
?s in the log-linear models.3 Dictionary-based Chinese wordsegmentationFor a given Chinese character sequence,C = c0c1c2 .
.
.
cN , the problem of word seg-mentation is addressed as finding a word se-quence, W = wt0wt1wt2 .
.
.wtM , where the words,wt0 ,wt1 ,wt2 , .
.
.
,wtM , are pre-defined by a providedlexicon/dictionary, which satisfywt0 = c0 .
.
.
ct0 , wt1 = ct0+1 .
.
.
ct1wti = cti?1+1 .
.
.
cti , wtM = ctM?1+1 .
.
.
ctMti > ti?1, 0 ?
ti ?
N, 0 ?
i ?
M226This word sequence is found by maximizing thefunction below,W = arg maxWP(W |C)= arg maxWP(wt0wt1 .
.
.wtM )(2)We applied Bayes?
law in the above derivation.P(wt0wt1 .
.
.wtM ) is a language model that can be ex-panded by the chain rule.
If trigram LMs are used,it is approximated asP(w0)P(w1|w0)P(w2|w0w1) ?
?
?
P(wM |wM?2wM?1)where wi is a shorthand for wti .Equation 2 indicates the process of the dictionary-based word segmentation.
Our CWS is based on it.We used a beam search algorithm because we foundthat it can speed up the decoding.
Trigram LMs wereused to score all the hypotheses, of which the onewith the highest LM scores is the final output.As the name indicates, the word segmentation re-sults by the dictionary-based CWS are dependenton the size and contents of the lexicon.
We willuse three lexicons in order to compare effects oflexicon size to the translations.
The three lexiconsdenoted as Character, Subword and Hyperword arelisted below.
An example sentence, ????????
(HuangYingChun lives in Beijing City), isgiven to show the segmentation results of using thelexicons.?
Character: Only Chinese single charac-ters are included in the lexicon.
Thesentence is split character by character.?/?/?/?/?/?/?/??
Subword: A small amount of most frequentwords (10,000) are added to the lexicon.Choosing the subwords are described in sec-tion 5.
?/?/?/?/?/??/??
Hyperword: A big size of lexicon is used, con-sisting of 100,000 words.
?/?/?/?/?/??
?4 Named entity recognition (NER)Named entities in the test data need to be treatedseparately.
Otherwise, a poor translation qualitywas found by our experiments.
We define fourTable 1: NER accuracytype Recall Precision F-scorenr 85.32% 93.41% 89.18%ns 87.80% 90.46% 89.11%nt 84.50% 87.54% 85.99%all 84.58% 90.97% 87.66%types of named entities: people names (nr), orga-nization names (nt), location names (ns), and nu-merical expressions (nc) such as calendar, time, andmoney.
Our NER model is built according to con-ditional random fields (CRF) methods (Lafferty etal., 2001), by which we convert the problem of NERinto that of sequence labeling.
For example, we canlabel the last section?s example as, ?
?/B nr?/I nr?/I nr ?/O ?/O ?/B nt ?/I nt ?/I nt?, where?B?
stands for the first character of a NE; ?I?, otherthan the first character of a NE; ?O?, isolated char-acter.
?nr?
and ?nt?
are two labels of NE.We use the CRF++ tools to train the models fornamed entity recognition1.
The performance of ourNER model was shown in Table 4.
We use thePeking University (PKU) named entity corpus totrain the models.
Part of the data was used as testdata.We stick to the results of CWS if there are ambi-guities in the segmentation boundary between CWSand NER.The NER was used only on the test data in transla-tions.
It was not used on the training data due to theconsideration of data sparseness.
Using NER willgenerate more unknown words that cannot be founda translation in the translation table.
That is why weuse a subword-based translation approach.5 Subword-based translation model forUNK translationWe found there were two reasons accounting forproducing untranslatable words.
The first is thesize of lexicon.
We proposed three size of lexi-cons in section 3, of which the Hyperword type uses100,000 words.
Because of a huge lexical size, someof the words cannot be learned by SMT training be-cause of limited training data.
The CWS choosesonly one candidate segmentation from thousands in1http://chasen.org/?taku/software/CRF++/227splitting a sentence into word sequences.
Therefore,the use of a candidate will block other candidates.Hence, many words in the lexicon cannot be fullytrained if a large lexicon is used.
The second is ourNER module.
The NER groups a longer sequence ofcharacters into one entity that cannot be translated.We have analyzed this points in the last section.Therefore, in order to translate unknown words,our approach is to split longer unknown words intosmaller pieces, and then translate the smaller piecesby using Character or Subword models.
Finally, weput the translations back to the Hyperword models.We call this method subword-based unknown wordtranslation regardless of whether a Character modelor Subword model is used.As described in Section 3, Characters CWS usesonly characters in the lexicon.
So there is no tricksfor it.
But for the Subword CWS, its lexicon is asmall subset of the Hyperword CWS.
In fact, we usethe following steps for generating the lexicon.
In thebeginning, we use the Hyperword CWS to segmentthe training data.
Then, we extract a list of uniquetokens and calculate their counts from the results ofsegmentation.
Next, we sort the list as the decreas-ing order of the counts, and choose N most frequentwords from the top of the list.
We restrict the lengthof subwords to three.
We use the N words as thelexicon for the subword CWS.
N can be changed.Section 7.4 shows its effect to translations.
The sub-word CWS uses a trigram language model to disam-biguate.
Refer to (Zhang et al, 2006) for detailsabout selecting the subwords.We applied Subword CWS to re-segment thetraining data.
Finally, we can train a subword-basedSMT translation model used for translating the un-known words.
Training this subword translationmodel was done in the same way as for the Hyper-word translation model that uses the main CWS, asdescribed in the beginning of Section 2.6 Named entity translationThe subword-based UNK translation approach canbe applied to all the UNKs indiscriminately.
How-ever, if we know an UNK is a named entity, wecan translate this UNK more accurately than usingthe subword-based approach.
Some unknown wordscan be translated by named entity translation if theyare correctly recognized as named entity and fit atranslation pattern.
For example, the same wordswith different named entities are translated differ-ently in the context.
The word, ??
?, is translatedinto ?nine?
for measures and money, ?September?for calendar, and ?jiu?
for Chinese names.As stated in Section 4, we use NER to recognizefour types of named entities.
Correspondingly, wecreated the translation patterns to translate each typeof the named entities.
These patterns include pat-terns for translating numerical expressions, patternsfor translating Chinese and Japanese names, and pat-terns for translating English alphabet words.
The us-ages are described as follows.Numerical expressions are the largest proportionof unknown words.
They include calendar-relatedterms (days, months, years), money terms, mea-sures, telephone numbers, times, and addresses.These words are translated using a rule-based ap-proach.
For example, ??????
?, is translatedinto ?at 3:15?.Chinese and Japanese names are composed oftwo, three, or four characters.
They are translatedinto English by simply replacing each character withits spelling.
The Japanese name, ?????
?, istranslated into ?Shinzo Abe?.English alphabets are encoded in different Chi-nese characters.
They are translated by replacing theChinese characters with the corresponding Englishletters.We use the above translation patterns to translatethe named entities.
Using translation patterns pro-duce almost correct translation.
Hence, we put thenamed entity translation to work before we apply thesubword translation model.
The subword translationmodel is used when the unknown words cannot betranslated by named entity translation.7 SMT experiments7.1 DataWe used LDC Chinese/English data for training.
Weused two test data of NIST MT04 and NIST MT05.The statistics of the data are shown in Table 6.
Weused about 2.4 million parallel sentences extractedfrom LDC data for training.
Experiments on boththe MT04 and MT05 test data used the same transla-tion models on the same training data, but the min-228Table 2: Statistics of data for MT experimentsChinese EnglishMT Training Sentences 2,399,753words 49,546,231 52,746,558MT04 LDC2006E43 Test Sentences 1,788Words 49,860MT05 LDC2006E38 Test Sentences 1,082Words 30,816Table 3: Statistics of unknown words of test data using different CWSHyperword+Named entities Hyperword Subwords CharactersNumerics People Org.
Loc.
otherMT04 460 146 250 230 219 650 18 2MT05 414 271 311 146 323 680 23 2imum error rate training was different.
The MT04and MT05 test data were also used as developmentdata for cross experiments.We used a Chinese word segmentation tool,Achilles, for doing word segmentation.
Its wordsegmentation accuracy was higher than the stanfordword segmenter (Tseng et al, 2005) in our labora-tory test (Zhang et al, 2006).The average length of a sentence for the test dataMT04 and MT05 after word segmentation is 37.5by using the Subword CWS, and 27.9 by using theHyperword CWS.Table 6 shows statistics of unknown words inMT04 and MT05 using different word segmenta-tion.
Obviously, character-based and subword-basedCWS generated much fewer unknown words, butsentences are over-segmented.
The CWS of Hy-perword generated many UNKs because of usinga large size of lexicon.
However, if named entityrecognition was applied upon the segmented resultsof the Hyperword, more UNKs were produced.
Takean example for MT04.
There are 1,305 UNKs inwhich numeric expressions amount to 35.2%, peo-ple names at 11.2%, organization names at 19.2%,location names at 17.6%, and others at 16.8%.
Anal-ysis of these numbers helps to understand the distri-bution of unknown words.7.2 Effect of the various CWSAs described in section 3, we used three lexiconsize for the dictionary-based CWS.
Therefore, wehad three CWS denoted as: Character, Subword andHyperword.
We used the three CWS in turn to doword segmentation to the training data, and thenbuilt the translation models respectively.
We testedthe performance of each of the translation modelson the test data.
The results are shown on Table 4.The translations are evaluated in terms of BLEUscore (Papineni et al, 2002).
This experiment wasjust testing the effect of the three CWS.
Therefore,all the UNKs of the test data were not translated,simply removed from the results.We found the character-based CWS yielded thelowest BLEU scores, indicating the translation qual-ity of this type is the worst.
The Hyperword CWSachieved the best results.
If we relate it to Ta-ble 6, we found while the Hyperword CWS pro-duced many more UNKs than the Character andSubword CWS, its translation quality was improvedinstead.
The fact proves the quality of transla-tion models play a more important role than theamount of unknown word translation.
Using theHyperword CWS can generate a higher quality oftranslation models than the Character and SubwordCWS.
Therefore, we cannot use the character andsubword-based CWS in Chinese SMT system due totheir overall poor performance.
But we found their229Table 4: Compare the translations by different CWS (BLEUscores)MT04 MT05Character 0.253 0.215Subword 0.265 0.229Hyperword 0.280 0.236Table 5: Effect of subword and named entity translation(BLEU)MT04 MT05Baseline(Hyperword) 0.280 0.236Baseline+Subword 0.283 0.244Baseline+NER 0.283 0.242Baseline+NER+Subword 0.285 0.246usage for UNK translation.7.3 Effect of subword translation for UNKsThe experiments in this section show the effect ofusing the subword translation model for UNKs.
Wecompared the results of using subword translationwith those of without using it.
We also used namedentity translation together with the subword trans-lation.
Thus, we could compare the effect of sub-word translation under conditions of with or withoutnamed entity translation.
We listed four kinds of re-sults to evaluate the performance of our approach inTable 5 where the symbols indicate:?
Baseline: this is the results made by the Hyper-word CWS of Table 4.
No subword translationfor UNKs and named entity translations wereused.
Unknown words were simply removedfrom the output.?
Baseline+Subword: the results were made un-der the same conditions as the first except all ofthe UNKs were extracted, re-segmented by thesubword CWS and translated by the subwordtranslation models.
However, the named entitytranslation was not used.?
Baseline+NER: this experiment did not usesubword-based translation for UNKs.
But weused named entity translation.
Part of UNKswas labeled with named entities and translatedby pattern match of section 6.?
Baseline+NER+Subword: this experimentused the named entity translation and thesubword-based translation.
The differencefrom the second one is that some UNKs weretranslated by the translation patterns of sec-tion 6 at first and the remaining UNKs weretranslated using the subword model (the sec-ond one translated all of the UNKs using thesubword model).The results of our experiments are shown in Ta-ble 5.
We found the subword models improvedtranslations in all of the experiments.
Using thesubword models on the MT04 test data improvedtranslations in terms of BLEU scores from 0.280to 0.283, and from 0.236 to 0.244 on the MT05test data.
While only small gains of BLEU wereachieved by UNK translation, this improvement issufficient to prove the effectiveness of the subwordmodels, given that the test data had only a low pro-portion of UNKs.The BLEU scores of ?Baseline+NER?
is higherthan that of ?Baseline?, that proves using named en-tity translation improved translations, but the effectof using named entity translation was worse than us-ing the subword-based translation.
This is becausethe named entity translation is applicable for thenamed entities only.
However, the subword-basedtranslation is used for all the UNKs.When we applied named entity translation totranslate some of recognized named entities fol-lowed by using the subword models, we foundBLEU gains over using the subword modelsuniquely, 0.2% for MT04 and 0.2% for MT05.
Thisexperiment proves that the best way of using thesubword models is to separate the UNKs that canbe translated by named entity translation from thosethat cannot, and let the subword models handletranslations of those not translated.Analysis using the bootstrap tool created byZhang et al (Zhang et al, 2004) showed that theresults made by the subword translations were sig-nificantly better than the ones not using it.7.4 Effect of changing the size of subwordlexiconWe have found a significant improvement by usingthe subword models.
The essence of the approach230Table 6: BLEU scores for changing the subword lexicon sizesubword size MT04 MT05character 0.280 0.23710K 0.283 0.24420K 0.283 0.240is to split unknown words into subword sequencesand use subword models to translate the subwordsequences.
The choices are flexible in choosing thenumber of subwords in the subword lexicon.
If adifferent subword list is used, the results of the sub-word re-segmentation will be changed.
Will choos-ing a different subword list have a large impact onthe translation of UNKs?
As shown in Table 6, weused three classes of subword lists: character, 10Ksubwords and 20K subwords.
The ?character?
classused only single-character words, about 5,000 char-acters.
The other two classes, ?10K?
and ?20K?,used 10,000 and 20,000 subwords.
The method forchoosing the subwords was described in Section 5.We have used ?10K?
in the previous experiments.We did not use named entity translation for this ex-periment.We found that using ?character?
as the subwordunit brought in nearly no improvement over thebaseline results.
Using 20K subwords yielded bet-ter results than the baseline but smaller gains thanthat of using the 10K subwords for MT05 data.
Itproves that using subword translation is an effectiveapproach but choosing a right size of subword lexi-con is important.
We cannot propose a better methodfor finding the size.
We can do more experimentsrepeatedly to find this value.
We found the size of10,000 subwords achieved the best results for ourexperiments.8 Related workUnknown word translation is an important problemfor SMT.
As we showed in the experiments, appro-priate handling of this problem results in a signifi-cant improvement of translation quality.
As we haveknown, there exists some methods for solving thisproblem.
While these approaches were not proposedin aim to unknown word translation, they can beused for UNK translations indirectly.Most existing work focuses on named entitytranslation (Carpuat et al, 2006) because named en-tities are the large proportion of unknown words.
Wealso used similar methods for translating named en-tities in this work.Some used stem and morphological analysis forUNKs such as (Goldwater and McClosky, 2005).Morphological analysis is effective for inflectivelanguages but not for Chinese.
Using unknownword modeling such as backoff models was pro-posed by (Yang and Kirchhoff, 2006).Other proposed methods include paraphras-ing (Callison-Burch et al, 2006) and translitera-tion (Knight and Graehl, 1997) that uses the featureof phonetic similarity.
However, This approach doesnot work if no phonetic relationship is found.Splitting compound words into translatable sub-words as we did in this work have been usedby (Nie?len and Ney, 2000) and (Koehn and Knight,2003) for languages other than Chinese where de-tailed splitting methods are proposed.
We usedforward maximum match method to split unknownwords.
This splitting method is relatively simple butworks well for Chinese.
The splitting for Chinese isnot as complicated as those languages with alphabet.9 Discussion and conclusionWe made use of the specific property of Chinese lan-guage and proposed a subword re-segmentation tosolve the translation of unknown words.
Our ap-proach was tested under various conditions such asusing named entity translation and varied subwordlexicons.
We found this approach was very effective.We are hopeful that this approach can be applied intolanguages that have similar features as Chinese, forexample, Japanese.While the work was done on a SMT systemwhich is not the state-of-the-art 2, the idea of usingsubword-based translation for UNKs is applicable toany systems because the problem of UNK transla-tion has to be faced by any system.AcknowledgementThe authors would like to thank Dr.Michael Paul forhis assistance in this work, especially for evaluatingmethods and statistical significance test.2The BLEU score of the top one system is about 0.35 forMT05 (http://www.nist.gov/speech/tests/mt/).231ReferencesChris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine translationusing paraphrases.
In HLT-NAACL-2006.Marine Carpuat, Yihai Shen, Xiaofeng Yu, and DekaiWu.
2006.
Toward Integrating Word Sense and EntityDisambiguation into Statistical Machine Translation.In Proc.
of the IWSLT.Sharon Goldwater and David McClosky.
2005.
Improv-ing statistical MT through morphological analysis.
InProceedings of the HLT/EMNLP.Kevin Knight and Jonathan Graehl.
1997.
Machinetransliteration.
In Proc.
of the ACL.Philipp Koehn and Kevin Knight.
2003.
Empirical meth-ods for compound splitting.
In EACL-2003.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: probabilistic modelsfor segmenting and labeling sequence data.
In Proc.
ofICML-2001, pages 591?598.Sonja Nie?len and Hermann Ney.
2000.
Improving smtquality with morpho-syntactic analysis.
In Proc.
ofCOLING.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.F.
J. Och.
2003.
Minimum error rate training for statisti-cal machine trainslation.
In Proc.
ACL.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of the 40th ACL, pages 311?318,Philadelphia, USA.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for Sighan bake-off 2005.
In Proceedings of the Fourth SIGHAN Work-shop on Chinese Language Processing, Jeju, Korea.Mei Yang and Katrin Kirchhoff.
2006.
Phrase-basedbackoff models for machine translation of highly in-flected languages.
In EACL-2006.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.
In-terpreting bleu/nist scores: How much improvementdo we need to have a better system?
In Proceedings ofthe LREC.Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.2006.
Subword-based tagging by conditional randomfields for chinese word segmentation.
In Proceedingsof the HLT-NAACL.232
