Coling 2010: Poster Volume, pages 1167?1175,Beijing, August 2010Bridging Topic Modeling and Personalized SearchWei Song Yu Zhang Ting Liu Sheng LiSchool of Computer ScienceHarbin Institute of Technology{wsong, yzhang, tliu, lisheng}@ir.hit.edu.cnAbstractThis work presents a study to bridge topicmodeling and personalized search.
Aprobabilistic topic model is used to extracttopics from user search history.
Thesetopics can be seen as a roughly summaryof user preferences and further treated asfeedback within the KL-Divergence re-trieval model to estimate a more accuratequery model.
The topics more relevantto current query contribute more in updat-ing the query model which helps to dis-tinguish between relevant and irrelevantparts and filter out noise in user searchhistory.
We designed task oriented userstudy and the results show that: (1) Theextracted topics can be used to clusterqueries according to topics.
(2) The pro-posed approach improves ranking qual-ity consistently for queries matching userpast interests and is robust for queries notmatching past interests.1 IntroductionThe majority of queries submitted to search en-gines are short and ambiguous and the users ofsearch engines often have different search intentseven when they submit the same query (Janse andSaracevic, 2000)(Silverstein and Moricz, 1999).The ?one size fits all?
approach fails to optimizeeach individual?s specific information need.
Per-sonalized search has be viewed as a promisingdirection to solve the ?data overload?
problem,and aims to provide different search results ac-cording to the specific preference of an individ-ual(Pitkow and Breuel, 2002).
Information re-trieval (IR) communities have developed modelsfor context sensitive search and related applica-tions (Shen and Zhai, 2005a)(White and Chen,2009).The search context includes a broad range of in-formation types such as a user?s background, hispersonal desktop index, browser history and eventhe context information of a group of similar users(Teevan, 2009).
In this paper, we exploit the usersearch history of an individual which contains thepast submitted queries, results returned and theclick through information.
As described in (Tanand Zhai, 2006), search history is one of the mostimportant forms of search context.
When dealingwith search history, distinguishing between rele-vant and irrelevant parts is important.
The searchhistory may contain a lot of noisy informationwhich can harm the performance of personaliza-tion (Dou and Wen, 2007).
Hence, we need tosort out relevant and irrelevant parts to optimizesearch personalization.In this paper, we propose a topic model basedapproach to study users?
preferences.
The maincontribution of this work is modeling user searchhistory with topics for personalized search.
Ourapproach mainly consists of two steps: topic ex-traction and relevance feedback.
We assume thata user?s search history is governed by the underly-ing hidden properties and apply probabilistic La-tent Semantic Indexing (pLSI) (Hofmann, 1999)to extract topics from user search history.
Eachtopic indexes a unigram language model.
Wemodel these extracted topics as feedback in theKL-Divergence retrieval framework.
The task isto estimate a more accurate query model basedon the evidence from user feedback.
We distin-1167guish relevant parts from irrelevant parts in searchhistory by focusing on the relevance between top-ics and query.
The closer a topic is to the cur-rent query, the more it contributes in updating thequery model, which in turn is used to rerank thedocuments in results set.2 Related Work2.1 Personalized IRPersonalized search is an active ongoing researchdirection.
Based on different representations ofuser profile, we classify approaches as follows:Taxonomy based methods: this approachmaps user interests to an existing taxonomy.ODP1 is widely used for this purpose.
Forexample, by exploiting the user search history,(Speretta and Gauch, 2005) modeled user interestas a weighted concept hierarchy created from thetop 3 level of ODP.
(Havelivala, 2002) proposedthe ?topic sensitive pagerank?
algorithm by cal-culating a set of PageRanks for each web page onthe top 16 ODP categories.
(Qiu and Cho, 2006)further improved this approach by building usermodels from user click history.
In recent stud-ies, (Xu S. and Yu, 2008) used ODP categoriesfor exploring folksonomy for personalized search.
(Dou and Wen, 2007) proposed a method that rep-resent user profile as a weighting vector of 67 pre-defined topic categories provided by KDD Cup-2005.
Taxonomy based methods rely on a pre-defined taxonomy and may suffer from the granu-larity problem.Content based methods: this category ofmethods use traditional text presentation modelsuch as vector space model and language modelto express user preference.
Rich content infor-mation such as user search history, browser his-tory and indexes of desktop documents are ex-plored.
The user profiles are built in the forms ofterm vectors or term probability distributions.
Forexample, (Sugiyama and M., 2004) representeduser profiles as vectors of distinct terms and ac-cumulated past preferences.
(Teevan and Horvitz,2005) constructed a rich user model based on bothsearch-related information, such as previously is-sued queries, and other information such as doc-1Open Directory Project, http://dmoz.org/uments and emails a user had read and created.
(Shen and Zhai, 2005b) used browsing historiesand query sessions to construct short term indi-vidual models for personalized search.Learning to rank methods: (Eugene and Su-san, 2005) and (Eugene and Zheng, 2006) incor-porated user feedback into the ranking process in alearning to rank framework.
They leveraged mil-lions of past user interaction with web search en-gine to construct implicit feedback features.
How-ever, this approach aims to satisfy majority ofusers rather than individuals.2.2 Probabilistic Topic ModelsProbabilistic topic models have become populartools for unsupervised analysis of document col-lection.
Topic models are based upon the ideathat documents are mixtures of topics, wherea topic is a probability distribution over words(Steyvers and Griffiths, 2007).
These topics areinterpretable to a certain degree.
In fact, one ofthe most important applications of topic modelsis to find out semantic lexicons from a corpus.One of the most popular topic models, the prob-abilistic Latent Semantic Indexing Model (pLSI),was introduced by Hofmann (Hofmann, 1999)and quickly gained acceptance in a number of textmodeling applications.
In this study, pLSI is usedto discover the underlying topics in user searchhistory.
Though pLSI is argued that it is not acomplete generative model, we used it because itdoes not need to generate unseen documents inour case and the model is much easier to be es-timated compared with sophisticated models suchas LDA(David M. Blei and Jordan, 2003).2.3 Model based Relevance FeedbackOur work is also related to language model based(pseudo) relevance feedback (Zhai and Lafferty,2001b) and shares the similar idea with (Tan B.and Zhai, 2007).
The differences are: (1) Thefeedback source is user search history rather thantop ranked documents for a query.
(2) We makeuse of user implicit feedback rather than explicitfeedback.
(3) The topics in search history couldbe extracted offline and updated periodically.
Ad-ditionally, these topics provide an informative pic-ture of user search history.1168Table 1: An illustration of topics extracted from auser?s search history.
Terms with highest proba-bilities are listed below each topic.Topic 2 Topic 3 Topic 9 Topic 16climb movie swim cup0.032 0.091 0.044 0.027setup download ticket world0.022 0.078 0.032 0.022equipment dvd notice team0.020 0.061 0.019 0.016practice watch travel brazil0.009 0.060 0.016 0.011player cinema hotel storm0.006 0.038 0.008 0.0073 Proposed Approach3.1 Main IdeaA user?s search history usually covers multipletopics.
It is crucial to distinguish between rele-vant and irrelevant parts for optimizing personal-ization.
We propose a topic model based methodto achieve that goal.
First, we construct a doc-ument collection revealing user intents accordingto the user?s past activities.
A probabilistic topicmodel is applied on this collection to extract la-tent topics.
Then the extracted topics are used asfeedback.
The query model is updated by high-lighting the topics highly relevant to current query.Finally, the search results are reranked accordingto the relevance to the updated query model.
Ta-ble 1 shows 4 topics extracted from a user?s searchhistory.
Each topic is a unigram language model.The terms with higher probabilities belonging toeach topic are listed.
We can predict that the userhas interests in both movie and football.
However,when the user submits a query about world cup,the topic 16 is given higher preference for esti-mating a more accurate query model.3.2 Topic Extraction from Search HistoryIndividual?s search history consists of all the pastquery units.
Each query unit includes query text,returned search results (with title, snippets andURLs) and click through information.
Here, weconcatenated the title and snippet of each searchresult to form a document being considered as awhole.
The whole search history can be seen as acollection of documents.
Obviously, many doc-uments in the collection may fail to satisfy theuser?s information need and are uncertain for dis-covering the user?s preferences.
Therefore, thefirst task is to select proper documents in searchhistory as the preference collection for topic dis-covery.3.2.1 Preference CollectionAn intuitive solution is to use the documentsthat are clicked by the user.
The assumption isthat a user clicks on a result only if he is interestedin the document.
However, user click is sparse inreal search environments and the documents notclicked by the user may also be relevant to theuser?s information need.
We assumed that the userhad only one search intent for a submitted query.To enhance this coherence within a query unit, wecreated only one super-document for a query unitas follows: if a query unit had clicked documents,then we concatenated these document to form apreferred document.
Otherwise, we selected thetop n documents from the search results and con-catenated them as a preferred document.
That ismotivated by the idea of pseudo relevance feed-back (Lavrenko and Croft, 2001) and used here foralleviating data sparsity.
Pseudo relevance feed-back is sensitive to the number of feedback docu-ments.
In this work, n is set to 3, because the aver-age clicks for a query is not more than 3.
By thisway, we got a preference collection whose size isthe same as the number of past queries.3.2.2 Topic ExtractionGiven the collection of preferred documents,we applied pLSI on this collection to extractunderlying topics.
We define the collection asC={d1,d2,.
.
.
,dM}, where di corresponds to theith query unit, and M is the size of the collection.Each query unit is viewed as a mixture of differ-ent topics.
It is reasonable in reality.
For exam-ple, a news document about ?play basketball withobama?
might be seen as a mixture of topics ?pol-itics?
and ?sports?.Modeling: The basic idea of pLSI is to treatthe words in each document as being generatedfrom a mixture model where the component mod-els are topic word distributions.
Let k be the num-1169ber of topics which is assumed known and fixed.
?j is the word distribution for topic j.
We extracttopics from collection C using a simple proba-bilistic mixture model as described in (Zhai andYu, 2004).
A word w within document d can beviewed as generated from a mixture model:pd(w) = ?Bp(w|?B)+(1 ?
?B)k?j=1pid,jp(w|?j)(1)where ?B is the background model for all the doc-uments.
The background model is used to drawcommon words across all the documents and leadto more discriminative and informative topic mod-els, since ?B gives high weights to non-topicalwords.
?B is the probability that a term is gen-erated from the background model which is set tobe a constant.
To draw more discriminative topicmodels, we set ?B to 0.95.
Parameter pid,j indi-cates the probability that topic j is assigned to thespecific document d, where?kj=1 pid,j=1.Parameter estimation: The parameters wehave to estimate including the background model?B , {?j} and {pid,j}.
?B is maximum likelihoodestimated (MLE) using all available text in ourdata set so that it is a fixed distribution.
The otherparameters to be estimated are {?j} and {pid,j}.The log-likelihood of document d is:log p(d) =?w?Vc(w, d) log[?Bp(w|?B)+(1 ?
?B)k?j=1pid,jp(w|?j)](2)The log-likelihood of the whole collection C is:log(C) =?d?C?w?Vc(w, d) log[?Bp(w|?B)+(1 ?
?B)k?j=1pid,jp(w|?j)](3)The Expectation-Maximization (EM) algorithm(Dempster and Rubin, 1977) is used to find agroup of parameters maximizing equation (3).The updating formulas are:,,1( ) ( ),,( ) ( ), ' '' 1, ,( 1),, ,' 1(E-Step:( | )( )( | ) (1 ) ( | )( | )( )( | )M-Step:( , )(1 ( )) ( )( , )(1 ( )) ( ')B Bd w kB B B d j jjm md j jd w km md j jjd w d wm w Vd j kd w d ww Vjp wp z Bp w p wp wp z jp wc w d p z B p z jc w d p z B p z jp?
??
?
?
?
??
??
?
?==+ ?
?== =+ ?= =?
= ==?
= =????
?, ,1), ' , ''( , )(1 ( )) ( )( | )( ', )(1 ( )) ( )d w d wm d Cjd w d wd C w Vc w d p z B p z jwc w d p z B p z j?+ ??
??
= == ?
= =??
?where c(w, d) denotes the number of times woccurs in d. A hidden variable zd,w is introducedfor the identity of each word.
p(zd,w = B) isthe probability that the word w in document d isgenerated by the background model.
p(zd,w = j)denotes the probability that the word w in docu-ment d is generated using topic j given that w isnot generated from the background model.
Infor-mally, the EM algorithm starts with randomly as-signing values to the parameters to be estimatedand then alternates between E-Step and M-Stepiteratively until it yields a local maximum of thelog likelihood.Interpretation: As shown in equation (1), aword can be viewed as a mixture of topics.
Fromthe updating formulas, we can see that the domi-nant topic of a word depends on both itself and thecontext.
The word tends to have the same topicwith the document containing it.
While the prob-ability of assigning topic j to document d is es-timated by aggregating all the fractions of wordsgenerated by topic j in document d. We can ex-plain it in a more intuitive way with in our applica-tion.
As we know, the queries are usually ambigu-ous.
A classic example is ?apple?
which may re-fer to a kind of fruit, apple Inc, apple electric prod-ucts, etc.
Therefore, it is reasonable to assumethat each word belongs to multiple latent seman-tic properties.
If a returned result contains ?ap-ple?
and other words like ?computer?, ?ipod?
,etc.
The word ?apple?
in this result tends to havethe same topic distributions with ?computer?
and1170?ipod?.
If the user clicks the result, we can predictthat the user?s real preference about query ?ap-ple?
is related to electric products having a highprobability.
Further, if ?apple?
occurs frequentlyin many documents related to electric products,it obtains a higher probability in this topic.
Asa result, we not only know user?s interest in elec-tric products, but also find a preference to ?apple?brand.Since a document?s topic depends on the wordsit contains, two documents with similar word dis-tributions have similar topic distributions.
In otherwords, each topic is like a bridge connectingqueries with similar intents.
In summary, the topicextraction process plays a role in our applicationfor finding user preference, highlighting discrimi-native words and connecting queries with similarintents.3.3 Topics as FeedbackThe topics extracted from search history are con-sidered as a kind of feedback.
Since topic mod-els actually are extensions of language models,we use such feedback within the KL-Divergenceretrieval model (Xu and Croft, 1999)(Zhai andLafferty, 2001b) that is a principled frameworkto model feedback in the language modeling ap-proach.
In this framework, feedback is treated asupdating the query language model based on extraevidence obtained from the feedback sources.
Theinformation retrieval task is to rank documents ac-cording to the KL divergence D(?q||?d) betweena query language model ?q and a document lan-guage model ?d.
The KL divergence is defined as:D(?q||?d) =?w?Vp(w|?q) logp(w|?q)p(w|?d)(4)where V denotes the vocabulary.
We estimatethe document model ?d using Dirichlet estimation(Zhai and Lafferty, 2001a):p(w|?d) =c(w, d) + ?p(w|?C)|d| + ?
(5)where |d| is document length, p(w|?C) is collec-tion language model which is estimated using thewhole data collection.
?
is the Dirichlet prior thatis set to 20 in this work.
The updated query modelis defined as:p(w|?q) = ?pml(w|?q)+(1 ?
?
)k?j=1p(w|?j)p(z = j|q)(6)where pml(w|?q) is the MLE query model.
{?j}represents a set of extracted topics each of whichis a unigram language model.
?
is used to bal-ance the two components.
z is a hidden variableover topics.
The task is to estimate the multino-mial topic distribution p(z|q) for query q. SincepLSI does not properly provide a prior, we esti-mate p(z = j|q) as:p(z = j|q) = p(q, z = j)?kj?=1 p(q, z = j?)?
sim(?q, ?j)?kj?=1 sim(?q, ?j?
)(7)Since the query text is usually very short, it isnot easy to make a decision based on query textalone.
Instead, we concatenate all the availabledocuments in returned result set to form a super-document.
A language model is estimated for it.We convert both the document language modeland topic models into weighted term vectors anduse cosine similarity as the sim function.
p(z|q)plays an import role here as it determines the con-tribution of topics.
The topics with higher similar-ity with current query contributes more in updat-ing query model.
This scheme helps to filter outnoisy information in search history.4 Evaluation and Discussion4.1 Data CollectionTo the best of our knowledge, there is no publiccollection with enough content information anduser implicit feedback.
We decided to carry outa data collection.
Due to the difficulty to de-scribe and evaluate user interests implicitly, wepredefined some user interests and implementeda search system to collect user interactions.The predefined interests belong to 5 big cate-gories namely Entertainment, Computer & Inter-net, Sports, Health and Social life.
Each inter-est is a kind of user preference such as ?movies?1171Table 2: An example of predefined user interestsand taskscategory Enterntainmentinterest moviestask1 search for a brief introductionof your favorite movietask2 search for an introduction ofan actor or actress you liketask3 search for movies about?artificial intelligence?Table 3: Statistics of the data collectionuser 1 2 3 4 5#queries 218 256 177 206 311#big category 5 5 5 5 5#interest 25 25 25 25 25#tasks 100 100 100 100 100avg.#relevant 4.17 4.22 3.89 4.12 3.24resultsavg.#clicked 2.37 2.21 2.71 1.98 2.42resultsand ?outdoor sports?.
For each interest, we de-signed several tasks each of which had a goal.
Ta-ble 2 illustrates an example of a predefined userinterest and related tasks.
The volunteers wereasked to find out the information need accordingto the tasks.
Though we defined these interestsand tasks, we did not impose any constraint onthe queries.
The volunteers could choose and re-formulate any query they thought good for find-ing the desired information.
But we did try to in-crease the possibility that a user might issue am-biguous queries by designing tasks like ?searchfor movies about artificial intelligence?
which wascategorized to interest ?movies?, but also relatedto computer science.To collect the user interaction with search en-gine, we implemented a Lucene based search sys-tem on Tianwang terabyte corpus(Yan and Peng,2005).
Five volunteers were asked to submitqueries to this system to find information satisfy-ing the tasks of each interest.
The system recordedusers?
activities including submitted queries, re-turned search results (with title, snippet and URL)and users?
click through information.
When theuser finished a task, he clicked a button to tell thesystem termination of the session containing allthe queries and activities related to this task.
Afterfinishing all the tasks, the volunteers were asked tojudge the top 20 results?
relevance (relevant or notrelevant) for each query according to the searchtarget.
Each volunteer submitted 233 queries onaverage.
Table 3 presents some statistics of thiscollection.4.2 Evaluating Topic ExtractionIt is not easy to assess the quality of topics, be-cause topic extraction is an unsupervised processand difficult to give a standard answer.
Therefore,we view the topic extraction as a clustering prob-lem that is to organize queries into clusters.
Togroup queries into clusters through extracted top-ics, we use j?
= argmaxjpid,j to assign a query tothe j?th topic.
Each topic corresponds to a cluster.All the queries are divided into k clusters.
Basedon the data collection, we setup the golden an-swers according to the predefined interests.
Weview all the queries belonging to a predefined in-terest(which includes multiple tasks) form a clus-ter which helps us to build a golden answer with25 clusters in tatal.One purpose of making use of topics in searchhistory is to find more relevant parts and reducethe noise.
We hope that the extracted topics arecoherent.
That is, a cluster should contain as manyqueries as possible belonging to a single inter-est.
To evaluate coherence, we adopt purity (Zhaoand Karypis, 2001), a commonly used metric forevaluating clustering.
The higher the purity is,the better the system performs.
We compare ourmethod (denoted as PLSI) against the k-means al-gorithm(denoted as K-Means) on the preferencecollection.Figure 1 shows the overall purity with differ-ent number of topics.
Our method gained betterperformance than k-means algorithm consistently.It is effective to discover and organize user inter-ests.
Besides, as illustrated in Table 1, our methodis able to give higher probability to discriminativewords of each topic that provides a clear pictureof user search history.
This leads to an emergenceof novel approaches for personalized browsing.11720.20.30.40.50.60.70.810 20 30 40 50 60 70 80 90 100Number of topicsPurityPLSI K-MeansFigure 1: Average purity over 5 users gained byboth PLSI and K-Means with different number oftopics(clusters).4.3 Evaluating Result Reranking4.3.1 MetricTo quantify the ranking quality, the Dis-counted Cumulative Gain (DCG) (Jarvelin andKekakainen, 2000) is used.
DCG is a metric thatgives higher weights to highly ranked documentsand incorporates different relevance levels by giv-ing them different gain values.DCG(i) ={G(1), if i = 1DCG(i?
1) + G(i)log(i) , otherwiseIn our work, we use G(i) = 1 for the results la-beled as relevant by a user and G(i) = 0 for theresults that are not relevant.
The average normal-ized DCG (NDCG)over all the test queries is se-lected to show the performance.4.3.2 SystemsWe evaluated the performance of following sys-tems:PLSI: The proposed method.
The history modelwas a weighted interpolation over topics extractedfrom the preference collection described in ses-sion 3.2.1.PSEUDO: From each query unit, we selectedtop n documents as pseudo feedback.
The lan-guage history model was estimated on all thesedocuments.PLSI-PSEUDO: Top n documents from eachquery unit were concatenated to form a preferreddocument.
The history model was constructedbased on topics extracted from these preferreddocuments.HISTORY: The history language model was es-timated based on all the documents in search his-tory.TB: It was based on(Tan and Zhai, 2006)whichbuilt a unit language model for every past queryand the history model was a weighted interpola-tion of past unit language models.ORIGINAL: The default search system.The first 5 systems provided schemes to smooththe query model.
They estimated the query mod-els by utilizing different types of feedback (im-plicit feedback or pseudo feedback) and weight-ing methods (topic modeling or simple languagemodeling).
The updated query model was an in-terpolation between MLE query model and his-tory language model.
The interpolation parameterwas set to 0.5, and n was set to 3.4.3.3 Performance ComparisonTo evaluate the performance on a test query, wefocus on two conditions:1. the test query matches some past interests.We want to check the ability of systems tofind relevant information from noisy data.2.
the test query does not match any of past in-terests.
We are interested in the robustness ofthe systems.For the first case, the users were asked to se-lect at most 2 queries they submitted for eachtask.
These queries were used as test queries.The other queries were used to simulate the users?search history.
In total we got 400 queries fortesting.
Figure 2 demonstrates the performanceof these systems over all test queries.
PLSIoutperformed all other systems consistently thatshows topic model based methods help to esti-mate a more accurate query model and the userimplicit feedback is better evidence.
The PLSI-PSEUDO also performed well that indicates thetop documents is useful for revealing the topicof queries, even though they do not satisfy userneed on occasion.
TB also gained better perfor-mance than PSEUDO and HISTORY.
It indicates11730.490.50.510.520.530.540.550.5610 20 30 40 50 60 70 80 90 100Number of topicsNDCGPLSI PLSI-PSEUDO PSEUDOHISTORY TB ORIGINAL0.510.5140.5180.5220.5260.5310 20 30 40 50 60 70 80 90 100Number of topicsNDCGPLSI PLSI-PSEUDO PSEUDOHISTORY TB ORIGINALFigure 2: The overall average performance of sys-tems, when each test query matches some userpast interestshighlighting relevant parts in search history helpsto improve the retrieval performance, when thequery matches some of user past interests.
Com-pared with default system, both HISTORY andPSEUDO improved a lot which proves that thecontext in search history is reliable feedback.For the second case, each user was asked tohold out 5 interests from his collection for test-ing and the other interests were used as searchhistory.
The users selected queries from the heldout interests as test queries.
These queries didnot match each user?s past interests.
We got 244test queries.
As figure 3 shows, though systemsstill performed better against ORIGINAL, the im-provements were not significant.
PLSI still gainedthe best performance.
It has better ability to al-leviate the effect of noise.
HISTORY and PLSIare more robust than PLSI-PSEUDOwhich seemssensitive to the number of topics in this case.In both cases, HISTORY gained moderate per-formance but quite robust.
It is still a very strongbaseline, though noisy information is not filteredout.
PLSI performed best in both cases.
PLSI-PSEUDO outperformed PSEUDO when the testqueries matched user past interests and gainedcomparable results in second case.
It shows thatmodeling user search history as a mixture of top-ics and weighting topics according to relevancebetween topics and query help to update a betterquery model.
However, it is necessary to deter-mine if a query matches past interests that helpsto optimize personalized search strategies.0.490.50.510.520.530.540.550.5610 20 30 40 50 60 70 80 90 100Number of topicsNDCGPLSI PLSI-PSEUDO PSEUDOHISTORY TB ORIGINAL0.510.5140.5180.5220.5260.5310 20 30 40 50 60 70 80 90 100Number of topicsNDCGPLSI PLSI-PSEUDO PSEUDOHISTORY TB ORIGINALFigure 3: The overall average performance of sys-tems, when each test query does not match anyuser past interest.5 Conclusion and Future WorkIn this paper, we have proposed a topic modelbased method for personalized search.
This ap-proach has some advantages: first, it provides aprincipled way to combine topic modeling andpersonalized search; second, it is able to find userpreferences in an unsupervised way and gives aninformative summary of user search history; third,it explores the underlying relationship betweendifferent query units via topics that helps to filterout the noise and improve ranking quality.In future, we plan to do a large scale study byleveraging the already built search system or busi-ness search engines.
Also, we will try to add moreinformation to extend the existing model.
Besides,it is necessary to design methods for determin-ing whether a submitted query matches the userpast interests that is crucial to apply our algorithmadaptively and selectively.AcknowledgementsThis research is supported by the National Nat-ural Science Foundation of China under GrantNo.
60736044, by the National High Technol-ogy Research and Development Program of ChinaNo.
2008AA01Z144, by Key Laboratory OpeningFunding of MOE-Microsoft Key Laboratory ofNatural Language Processing and Speech, HarbinInstitute of Technology, HIT.KLOF.2009020.
Wethank the anonymous reviewers and FikaduGemechu for their useful comments and help.1174ReferencesDavid M. Blei, Andrew Y. Ng and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Dempster, A.P., Laird N.M. and D.B.
Rubin.
1977.Maximum likelihood from incomplete data via theem algorithm.
Journal of Royal Statist.
Soc.
B,39:1?38.Dou, Z., Su R. and J. Wen.
2007.
A large-scale evalu-ation and analysis of personalized search strategies.Proc.
WWW, pages 581?590.Eugene, A., Eric B. and D. Susan.
2005.
Improvingweb search ranking by incorporating user behaviorinformation.
Proc.SIGIR, pages 19?26.Eugene, A. and Zijian Zheng.
2006.
Identifying bestbet web search results by mining past user behavior.Proc.SIGKDD, pages 902?908.Havelivala, T.H.
2002.
Topic-sensitive pagerank.Proc.
WWW, pages 517?526.Hofmann, T. 1999.
Probabilistic latent semantic in-dexing.
Proc.SIGIR, pages 50?57.Janse, B.J., Spink A. Bateman J. and T. Saracevic.2000.
Real life, real users, and real needs: a studyand analysis of user queries on the web.
InformationProcessing and Management, 26(2):207?222.Jarvelin, K. and J. Kekakainen.
2000.
Ir evaluationmethods for retrieving highly relevant documents.Proc.SIGIR, pages 41?48.Lavrenko, V. and W. Croft.
2001.
Relevance basedlanguage models.
Proc.SIGIR, pages 120?127.Pitkow, J., Schutze H. Cass T. Cooley R. Turnbull D.Edmonds A. Adar E. and T. Breuel.
2002.
Person-alized search.
Commun,ACM, 45(9):50?55.Qiu, F. and J. Cho.
2006.
Automatic identification ofuser interest for personalized search.
Proc.WWW,pages 727?736.Shen, X., Tan B. and C. Zhai.
2005a.
Context-sensitive information retrieval using implicit feed-back.
Proc.
SIGIR, pages 43?50.Shen, X., Tan B. and C. Zhai.
2005b.
Implicit usermodeling for personalized search.
Proc.
CIKM,pages 824?831.Silverstein, C., Marais H. Henzinger M. andM.
Moricz.
1999.
Analysis of a very large websearch engine query log.
SIGIR Forum, 33(1):6?12.Speretta, M. and S. Gauch.
2005.
Personalized searchbased on user search histories.
Proc.
WI?05, pages622?628.Steyvers, M. and T. Griffiths.
2007.
Probabilistic topicmodels.
Handbook of Latent Semantic Analysis.Erlbaum, Hillsdale, NJ.Sugiyama, K., Hatano K. and Yoshkawa.
M. 2004.Personalized search based on user search histories.Proc.
WWW, pages 675?684.Tan, B., Shen X. and C. Zhai.
2006.
Mining long-term search history to improve search accuracy.Proc.SIGKDD, pages 718?723.Tan B., Atulya Velivelli, Fang H. and C. Zhai.
2007.Term feedback for information retrieval with lan-guage models.
Proc.SIGIR, pages 263?270.Teevan, J., Dumais S.T.
and E. Horvitz.
2005.
Per-sonalizing search via automated analysis of interestsand activities.
Proc.SIGKDD, pages 449?456.Teevan, J., Morris M.R.
Bush S. 2009.
Discover-ing and using groups to improve personalization.Proc.WSDM, pages 15?24.White, R.W., Bailey P. and L. Chen.
2009.
Pre-dicting user interest from contextual information.Proc.SIGIR, pages 363?370.Xu, Jinxi and W. Croft.
1999.
Cluster-based languagemodels for distributed retrieval.
Proc.SIGIR, pages254?261.Xu S., Bao, S. Fei B. Su Z. and Y. Yu.
2008.
Exploringfolksonomy for personalized search.
Proc.SIGIR,pages 155?162.Yan, H., Li J. Zhu j. and B. Peng.
2005.
Tian-wang search engine at trec 2005: Terabyte track.Proc.TREC.Zhai, C. and J. Lafferty.
2001a.
A study of smooth-ing methods for language models applied to ad hocinformation retrieval.
Proc.SIGIR, pages 334?342.Zhai, Chengxiang and John Lafferty.
2001b.
Model-based feedback in the language modeling approachto information retrieval.
Proc.CIKM, pages 403?410.Zhai, C., Velivelli A. and B. Yu.
2004.
A cross-collection mixture model for comparative text min-ing.
Proc.SIGKDD, pages 743?748.Zhao, Y. and G. Karypis.
2001.
Criterion functionsfor document clustering: Experiments and analysis.Technical Report TR #01?40, Department of Com-puter Science, University of Minnesota, Minneapo-lis, MN.1175
