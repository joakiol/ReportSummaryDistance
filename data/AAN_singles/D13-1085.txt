Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 863?868,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsMicroblog Entity Linking by Leveraging Extra PostsYuhang Guo, Bing Qin?, Ting Liu , Sheng LiResearch Center for Social Computing and Information RetrievalSchool of Computer Science and TechnologyHarbin Institute of Technology, China{yhguo, bqin?, tliu, sli}@ir.hit.edu.cnAbstractLinking name mentions in microblog posts toa knowledge base, namely microblog entitylinking, is useful for text mining tasks on mi-croblog.
Entity linking in long text has beenwell studied in previous works.
However fewwork has focused on short text such as mi-croblog post.
Microblog posts are short andnoisy.
Previous method can extract few fea-tures from the post context.
In this paper wepropose to use extra posts for the microblogentity linking task.
Experimental results showthat our proposed method significantly im-proves the linking accuracy over traditionalmethods by 8.3% and 7.5% respectively.1 IntroductionMicroblogging services (e.g.
Twitter) are attractingmillions of users to share and exchange their ideasand opinions.
Millions of new microblog posts aregenerated on such open broadcasting platforms ev-ery day 1.
Microblog provides a fruitful and instantchannel of global information publication and acqui-sition.A necessary step for the information acquisitionon microblog is to identify which entities a post isabout.
Such identification can be challenging be-cause the entity mention may be ambiguous.
Let?sbegin with a real post from Twitter.
(1) No excuse for floods tax, says AbbottURL?Corresponding author1See http://blog.twitter.com/2011/06/ 200-million-tweets-per-day.html.This post is about an Australia political lead-er, Tony Abbot, and his opinion on flood taxpolicy.
To understand that this post mentionsTony Abbot is not trivial because the name Ab-bot can refer to many people and organization-s.
In the Wikipedia page of Abbott, there list-s more than 20 Abbotts, such as baseball playerJim Abbott, actor Bud Abbott and companyAbbott Laboratories, etc..Given a knowledge base (KB) (e.g.
Wikipedia),entity linking is the task to identify the referent KBentity of a target name mention in plain text.
Mostcurrent entity linking techniques are designed forlong text such as news/blog articles (Mihalcea andCsomai, 2007; Cucerzan, 2007; Milne and Witten,2008; Han and Sun, 2011; Zhang et al 2011; Shenet al 2012; Kulkarni et al 2009; Ratinov et al2011).
Entity linking for microblog posts has notbeen well studied.Comparing with news/blog articles, microblogposts are:short each post contains no more than 140 charac-ters;fresh the new entity-related content may have notbeen included in the knowledge base;informal acronyms and spoken language writingstyle are common.Due to these properties, few feature can be ex-tracted from a post.
Without enough features, pre-vious entity linking methods may fail.
In order toovercome the feature sparseness, we turn to anotherproperty of microblog:863redundancy For each day, over 340M short mes-sages are posted in twitter.
Similar informationmay be posted in different expressions.For example, we find the following post,(2) Julia Gillard and Tony Abbott onthe flood levy just after 8.30am on@612brisbane!The content of post (2) is highly related to post(1).
In contrast to the confusing post (1), the textin post (2) explicitly indicates that the Abbott hererefers to the Australian political leader.
This inspiresus to bridge the confusing post and the knowledgebase with other posts.In this paper, we approach the microblog entitylinking by leveraging extra posts.
A straightforwardmethod is to expand the post context with similarposts, which we call Context-Expansion-based Mi-croblog Entity Linking (CEMEL).
In this method,we first construct a query with the given post andthen search for it in a collection of posts.
From thesearch result, we select the most similar posts for thecontext expansion.
The disambiguation will benefitfrom the extra posts if, hopefully, they are relatedto the given post in content and include explicit fea-tures for the disambiguation.Furthermore, we propose a Graph-based Mi-croblog Entity Linking (GMEL) method.
In contrastto CEMEL, the extra posts in GMEL are not directlyadded into the context.
Instead, they are representedas nodes in a graph, and weighted by their similaritywith the target post.
We use an iterative algorithmin this graph to propagate the entity weights throughthe edges between the post nodes.We conduct experiments on real microblog da-ta which we harvested from Twitter.
Current enti-ty linking corpus, such as the TAC-KBP data (M-cNamee and Dang, 2009), mainly focuses on longtext.
And few microblog entity linking corpus ispublicly available.
In this work, we manually anno-tated a microblog entity linking corpus.
This corpusinherit the target names from TAC-KBP2009.
So itis comparable with the TAC-KBP2009 corpus.Experimental results show that the performanceof previous methods drops on microblog posts com-paring with on long text.
Both of CEMEL andGMEL can significantly improve the performanceover baselines, which means that entity linking sys-tem on microblog can be improved by leveraging ex-tra posts.
The results also show that GMEL outper-forms CEMEL significantly.We summarize our contributions as follows.?
We propose a context-expansion-based and agraph-based method for microblog entity link-ing by leveraging extra posts.?
We annotate a microblog entity linking corpuswhich is comparable to an existing long textcorpus.?
We show the inefficiency of previous methodon the microblog corpus and our method cansignificantly improve the results.2 Task definationThe microblog entity linking task is that, for a namemention in a microblog post, the system is to find thereferent entity of the name in a knowledge base, orreturn a NIL mark if the entity is absence from theknowledge base.
This definition is close to the en-tity linking task in the TAC-KBP evaluation (Ji andGrishman, 2011) except for the context of the targetname is microblog post whereas in TAC-KBP thecontext is news article or web log.Several related tasks have been studied on mi-croblog posts.
In Meij et al(2012)?s work, theylink a post, rather than a name mention in the post,to relevant Wikipedia concepts.
Guo et al(2013a)and Liu et al(2013) define entity linking as to firstdetect all the mentions in a post and then link thementions to the knowledge base.
In contrast, ourdefinition (as well as the TAC-KBP definition) fo-cuses on a concerned name mention across differentposts/documents.3 MethodA typical entity linking system can be broken downinto two steps:candidate generation This step narrows down thecandidate entity range from any entity in theworld to a limited set.candidate ranking This step ranks the candidatesand output the top ranked entity as the result.864Figure 1: An example of the GMEL graph.
p1 .
.
.
p4 arepost nodes and c1 .
.
.
c3 are candidate entity nodes.
Eachpost node is connected to the corresponding candidate n-odes from the knowledge base.
The edges between thenodes are weighted by the similarity between them.In this paper, we use the candidate generationmethod described in Guo et al2013).
For the candi-date ranking, we use a Vector Space Model (VSM)and a Learning to Rank (LTR) as baselines.
VSMis an unsupervised method and LTR is a supervisedmethod.
Both of them have achieved the state-of-the-art performances in the TAC-KBP evaluations.The major challenge in microblog entity linkingis the lack of context in the post.
An ideal solu-tion is to expand the context with the posts whichcontain the same entity.
However, automaticallyjudging whether a name mention in two documentsrefers to the same entity, namely cross document co-reference, is not trivial.
Here our solution is to rankthe posts by their possibility of co-reference to thetarget one and select the most possible co-referentposts for the expansion.CEMEL is based on the assumption that, given aname and two posts where the name is mentioned,the higher similarity between the posts the high-er possibility of their co-reference and that the co-referent posts may contains useful features for thedisambiguation.
However, two literally similar postsmay not be co-referent.
If such non co-referent postis expanded to the context, noises may be included.Take the following post as an example.
(3) AG Abbott says that bullets havecrossed the border from Mexico toTexas at least four times.
URLThis post is similar to post (1) because they bothcontains ?says?
and ?URL?.
But the Abbott in post(3) refers to the Texas Attorney General Greg Ab-bott.
In this mean, the expanded context in post (3)could mislead the disambiguation for post (1).
Suchnoise can be controlled by setting a strict number ofposts to expand the context or weighting the contri-bution of this post to the target one.Our CEMEL method consists of the followingsteps: First we construct a query with the terms fromthe target post.
Second we search for the query in amicroblog post collection using a common informa-tion retrieval model such as the vector space model.Note that here we limit the searched posts must con-tain the target name mention.
Then we expand thetarget post with top N similar posts and use a typicalentity linking method (such as VSM and LTR) withthe expanded context.Figure 1 illustrates the graph of GMEL.
Each n-ode of this graph represents an candidate entity (e.g.c1 .
.
.
c3) or a post of the given target name (e.g.p1 .
.
.
p4) In this graph, each node represents an en-tity or a post of the given target name.
Between eachpair of post nodes, each pair of entity nodes and eachpost node and its candidate entity nodes, there is anedge.
The edge is weighted by the similarity be-tween the two linked nodes.
Entity nodes are labeledby themselves and candidate nodes are initialized asunlabeled nodes.
For the edges between post nodepairs and entity node pairs, we use cosine similari-ty.
For the edges between a post node and its can-didate entity nodes, we use the score given by tra-ditional entity linking methods.
We use an iterativealgorithm on this graph to propagate the labels fromthe entity nodes to the post nodes.
We adapt LabelPropagation (LP) (Zhu and Ghahramani, 2002) andModified Adsorption (MAD) (Talukdar and Pereira,2010) for the iteration over the graph.4 Experiment4.1 Data AnnotationTill now, few microblog entity linking data is pub-licly available.
In this work, we manually annotatea data set on microblog posts2.
We collect 15.6 mil-lion microblog posts in Twitter dated from January23 to February 8, 2011.
In order to compare with ex-isting entity linking on long text, we select a subsetof target names from TAC-KBP2009 and inherit theknowledge base in the TAC-KBP evaluation.
The2We published this data so that researchers can reproduceour results.865Figure 2: Percentage of the co-reference posts in the topN similar postsFigure 3: Impact of expansion post number in CEMELTAC-KBP2009 data set includes 513 target names.We search for all the target names in the post col-lection and get 26,643 matches.
We randomly sam-ple 120 posts for each of the top 30 most frequentlymatched target names and filter out non-English andoverly short (i.e.
less than 3 words) posts.
Thenwe get 2,258 posts for 25 target names and manual-ly link the target name mentions in the posts to theTAC-KBP knowledge base.In order to evaluate the assumption in CEMEL:similar posts tend to co-reference, we randomly s-elect 10 posts for 5 target names respectively andsearch for the posts in the post collection.
Fromthe search result of each of the 50 posts, we selectthe top 20 posts and manually annotate if they co-reference with the query post.4.2 SettingsWe generate candidates with the method describedin (Guo et al 2013b) and use Vector Space Mod-el (VSM) (Varma et al 2009) and Learning to Rank(LTR) (Zheng et al 2010) as the ranking model.
WeFigure 4: Accuracy of GMEL with different rate of extrapost nodesuse Lucene and ListNet with default settings for theVSM and LTR implementation respectively.
We usebigram feature for VSM and the feature set of (Chenet al 2011) for LTR.
LTR is evaluated with 10-foldcross validation.
Given a target name, the GMELgraph includes all the evaluation posts as well as aset of extra post nodes searched from the post collec-tion with the query of the target name.
We filter outdeterminers, interjections, punctuations, emoticon-s, discourse markers and URLs in the posts with atwitter part-of-speech tagger (Owoputi et al 2013).The similarity between a post and its candidate en-tities is set with the score given by VSM or LTRand the similarity between other nodes is set with thecorresponding cosine similarity.
We employ junto3with default settings for the iterative algorithm im-plementation .4.3 ResultsFigure 2 shows the relationship between similari-ty and co-reference.
From this figure we can seethat the percentage decreases with the growth of N.When the N is up to 10, about 60% of the similarposts co-reference with the query post and the de-crease speed slows down.
The Pearson correlationcoefficient between the percentage and the numberof top N is -0.843, which shows a significant corre-lation between the two variables (with p-value 0.01under t-test).Figure 3 shows the impact of the extra post num-ber for the context expansion in CEMEL.
We can seethat the accuracies of VSM and LTR are improved3See https://github.com/parthatalukdar/junto866Figure 5: Label entropy of GMEL with different rate ofextra post nodesFigure 6: Accuracy of the systemsby CEMEL.
The improvements peak with 5-10 ex-tra posts.
Then more extra posts will pull down theaccuracy.Figure 4 shows the accuracy of GMEL.
The x-axisis the rate of the extra post number over the evalu-ation post number.
We can see that the accuracy ofMAD increases with the number of extra post nodesat first and then turns to be stable.
The accuracy ofLP increases at first and drops when more extra postsare added into the graph.Figure 5 shows the information entropy of the la-bels in LP and MAD.
The curves show that the pre-diction of LP tends to converge into a small numberof labels.
This is because LP prefers smoothing la-belings over the graph (Talukdar and Pereira, 2010).We also evaluate our baselines on TAC-KBP2009data set (LTR is trained on TAC-KBP2010 data set).The accuracy of VSM and LTR are 0.8338 and0.8372 respectively, which are comparable with thestate-of-the-art result (Hachey et al 2013).Figure 6 shows the performances of the systemson the microblog data.
We set the optimal expansionpost number of CEMEL and use MAD algorithm forGMEL with all searched extra post nodes.
From thisfigure we can see that the results of VSM and LTRbaselines are comparable and both of them are sig-nificantly lower than that on TAC-KBP2009 data.CEMEL improves the VSM and LTR baselines by4.3% and 2.7% respectively.
GMEL improves VSMand LTR by 8.3% and 7.5% respectively.
The resultsof GMEL are also significantly better than CEMEL.All of the improvements are significant under Z-testwith p < 0.05.5 ConclusionIn this paper we approach microblog entity linkingby leveraging extra posts.
We propose a context-expansion-based and a graph-based method.
Exper-imental results on our data set show that the per-formance of traditional method drops on the mi-croblog data.
The graph-based method outperform-s the context-expansion-based method and both ofthem significantly improve the accuracy of tradition-al methods.
In the graph-based method the modifiedadsorption algorithm performs better than the labelpropagation algorithm.AcknowledgmentsThis work was supported by National NaturalScience Foundation of China (NSFC) via grant61273321, 61073126, 61133012 and the National863 Leading Technology Research Project via grant2012AA011102.
We would like to thank to Wanx-iang Che, Ruiji Fu, Yanyan Zhao, Wei Song andseveral anonymous reviewers for their constructivecomments and suggestions.ReferencesZheng Chen, Suzanne Tamang, Adam Lee, and Heng Ji.2011.
A toolkit for knowledge base population.
InSIGIR, pages 1267?1268.Silviu Cucerzan.
2007.
Large-scale named entity dis-ambiguation based on Wikipedia data.
In Proceedingsof the 2007 Joint Conference on Empirical Method-s in Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages708?716, Prague, Czech Republic, June.
Associationfor Computational Linguistics.867Stephen Guo, Ming-Wei Chang, and Emre Kiciman.2013a.
To link or not to link?
a study on end-to-end tweet entity linking.
In Proceedings of the 2013Conference of the North American Chapter of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 1020?1030, Atlanta, Geor-gia, June.
Association for Computational Linguistics.Yuhang Guo, Bing Qin, Yuqin Li, Ting Liu, and ShengLi.
2013b.
Improving candidate generation for entitylinking.
In Elisabeth Mtais, Farid Meziane, MohamadSaraee, Vijayan Sugumaran, and Sunil Vadera, edi-tors, Natural Language Processing and InformationSystems, volume 7934 of Lecture Notes in ComputerScience, pages 225?236.
Springer Berlin Heidelberg.Ben Hachey, Will Radford, Joel Nothman, Matthew Hon-nibal, and James R. Curran.
2013.
Evaluating en-tity linking with wikipedia.
Artificial Intelligence,194(0):130 ?
150.
?ce:title?Artificial Intelligence,Wikipedia and Semi-Structured Resources?/ce:title?.Xianpei Han and Le Sun.
2011.
A generative entity-mention model for linking entities with knowledgebase.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Techologies, pages 945?954, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.Heng Ji and Ralph Grishman.
2011.
Knowledge basepopulation: Successful approaches and challenges.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 1148?1158, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, andSoumen Chakrabarti.
2009.
Collective annotationof wikipedia entities in web text.
In Proceedingsof the 15th ACM SIGKDD international conferenceon Knowledge discovery and data mining, KDD ?09,pages 457?466, New York, NY, USA.
ACM.Xiaohua Liu, Yitong Li, Haocheng Wu, Ming Zhou, FuruWei, and Yi Lu.
2013.
Entity linking for tweets.
InProceedings of the 51th Annual Meeting of the Asso-ciation for Computational Linguistics.
Association forComputational Linguistics.P.
McNamee and H.T.
Dang.
2009.
Overview ofthe tac 2009 knowledge base population track.
InProceedings of the Second Text Analysis Conference(TAC2009).Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.2012.
Adding semantics to microblog posts.
In Pro-ceedings of the fifth ACM international conference onWeb search and data mining, WSDM ?12, pages 563?572, New York, NY, USA.
ACM.Rada Mihalcea and Andras Csomai.
2007.
Wikify!
: link-ing documents to encyclopedic knowledge.
In CIKM?07: Proceedings of the sixteenth ACM conference onConference on information and knowledge manage-ment, pages 233?242, New York, NY, USA.
ACM.David Milne and Ian H. Witten.
2008.
Learning to linkwith wikipedia.
In CIKM ?08: Proceeding of the 17thACM conference on Information and knowledge man-agement, pages 509?518, New York, NY, USA.
ACM.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, KevinGimpel, Nathan Schneider, and Noah A. Smith.
2013.Improved part-of-speech tagging for online conver-sational text with word clusters.
In NAACL2013,pages 380?390, Atlanta, Georgia, June.
Associationfor Computational Linguistics.Lev Ratinov, Dan Roth, Doug Downey, and Mike An-derson.
2011.
Local and global algorithms for dis-ambiguation to wikipedia.
In Proceedings of the 49thAnnual Meeting of the Association for Computation-al Linguistics: Human Language Technologies, pages1375?1384, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.2012.
Linden: linking named entities with knowl-edge base via semantic knowledge.
In Proceedings ofthe 21st international conference on World Wide We-b, WWW ?12, pages 449?458, New York, NY, USA.ACM.Partha Pratim Talukdar and Fernando Pereira.
2010.Experiments in graph-based semi-supervised learningmethods for class-instance acquisition.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 1473?1481, Up-psala, Sweden, July.
Association for ComputationalLinguistics.Vasudeva Varma, Vijay Bharat, Sudheer Kovelamudi,Praveen Bysani, Santosh GSK, Kiran Kumar N, Kran-thi Reddy, Karuna Kumar, and Nitin Maganti.
2009.Iiit hyderabad at tac 2009.
In Proceedings of the Sec-ond Text Analysis Conference (TAC 2009), Gaithers-burg, Maryland, USA, November.Wei Zhang, Yan Chuan Sim, Jian Su, and Chew Lim Tan.2011.
Entity linking with effective acronym expan-sion, instance selection, and topic modeling.
In TobyWalsh, editor, IJCAI 2011, pages 1909?1914.Zhicheng Zheng, Fangtao Li, Minlie Huang, and XiaoyanZhu.
2010.
Learning to link entities with knowledgebase.
In NAACL2010, pages 483?491, Los Angeles,California, June.
Association for Computational Lin-guistics.Xiaojin Zhu and Zoubin Ghahramani.
2002.
Learn-ing from labeled and unlabeled data with label prop-agation.
Technical report, Technical Report CMU-CALD-02-107, Carnegie Mellon University.868
