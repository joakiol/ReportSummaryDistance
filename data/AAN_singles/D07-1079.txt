Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
755?763, Prague, June 2007. c?2007 Association for Computational LinguisticsWhat Can Syntax-based MT Learn from Phrase-based MT?Steve DeNeefe and Kevin KnightInformation Sciences InstituteThe Viterbi School of EngineeringUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292{sdeneefe,knight}@isi.eduWei Wang and Daniel MarcuLanguage Weaver, Inc.4640 Admiralty Way, Suite 1210Marina del Rey, CA 90292{wwang,dmarcu}@languageweaver.comAbstractWe compare and contrast the strengthsand weaknesses of a syntax-based machinetranslation model with a phrase-based ma-chine translation model on several levels.We briefly describe each model, highlight-ing points where they differ.
We include aquantitative comparison of the phrase pairsthat each model has to work with, as wellas the reasons why some phrase pairs arenot learned by the syntax-based model.
Wethen evaluate proposed improvements to thesyntax-based extraction techniques in lightof phrase pairs captured.
We also comparethe translation accuracy for all variations.1 IntroductionString models are popular in statistical machinetranslation.
Approaches include word substitutionsystems (Brown et al, 1993), phrase substitutionsystems (Koehn et al, 2003; Och and Ney, 2004),and synchronous context-free grammar systems (Wuand Wong, 1998; Chiang, 2005), all of which trainon string pairs and seek to establish connections be-tween source and target strings.
By contrast, ex-plicit syntax approaches seek to directly model therelations learned from parsed data, including modelsbetween source trees and target trees (Gildea, 2003;Eisner, 2003; Melamed, 2004; Cowan et al, 2006),source trees and target strings (Quirk et al, 2005;Huang et al, 2006), or source strings and target trees(Yamada and Knight, 2001; Galley et al, 2004).It is unclear which of these important pursuits willbest explain human translation data, as each has ad-vantages and disadvantages.
A strength of phrasemodels is that they can acquire all phrase pairs con-sistent with computed word alignments, snap thosephrases together easily by concatenation, and re-order them under several cost models.
An advan-tage of syntax-based models is that outputs tend tobe syntactically well-formed, with re-ordering influ-enced by syntactic context and function words intro-duced to serve specific syntactic purposes.A great number of MT models have been re-cently proposed, and other papers have gone over theexpressive advantages of syntax-based approaches.But it is rare to see an in-depth, quantitative studyof strengths and weaknesses of particular modelswith respect to each other.
This is important for ascientific understanding of how these models workin practice.
Our main novel contribution is a com-parison of phrase-based and syntax-based extractionmethods and phrase pair coverage.
We also add tothe literature a new method of improving that cover-age.
Additionally, we do a careful study of severalsyntax-based extraction techniques, testing whether(and how much) they affect phrase pair coverage,and whether (and how much) they affect end-to-endMT accuracy.
The MT accuracy tests are neededbecause we want to see the individual effects of par-ticular techniques under the same testing conditions.For this comparison, we choose a previously estab-lished statistical phrase-based model (Och and Ney,2004) and a previously established statistical string-to-tree model (Galley et al, 2004).
These two mod-els are chosen because they are the basis of two ofthe most successful systems in the NIST 2006 MT755evaluation1.2 Phrase-based ExtractionThe Alignment Template system (ATS) described byOch and Ney (2004) is representative of statisticalphrase-based models.
The basic unit of translationis the phrase pair, which consists of a sequence ofwords in the source language, a sequence of wordsin the target language, and a vector of feature val-ues which describe this pair?s likelihood.
Decod-ing produces a string in the target language, in or-der, from beginning to end.
During decoding, fea-tures from each phrase pair are combined with otherfeatures (e.g., re-ordering, language models) using alog-linear model to compute the score of the entiretranslation.The ATS phrase extraction algorithm learns thesephrase pairs from an aligned, parallel corpus.This corpus is conceptually a list of tuples of<source sentence, target sentence, bi-directionalword alignments> which serve as training exam-ples, one of which is shown in Figure 1.Figure 1: a phrase-based training exampleFor each training example, the algorithm identi-fies and extracts all pairs of <source sequence, tar-get sequence> that are consistent with the align-ments.
It does this by first enumerating all source-side word sequences up to a length limit L, and foreach source sequence, it identifies all target wordsaligned to those source words.
For example, in Fig-ure 1, for the source phrase ?
 , the targetwords it aligns to are felt, obliged, and do.These words, and all those between them, are theproposed target phrase.
If no words in the proposedtarget phrase align to words outside of the sourcephrase, then this phrase pair is extracted.The extraction algorithm can also look to the leftand right of the proposed target phrase for neighbor-ing unaligned words and extracts phrases.
For ex-ample, for the phrase pair ?
 ?
felt obliged,1http://www.nist.gov/speech/tests/mt/mt06eval official results.htmlthe word to is a neighboring unaligned word.
Itconstructs new target phrases by adding on con-secutive unaligned words in both directions, andextracts those in new pairs, too (e.g., ?
 ?felt obliged to).
For efficiency reasons, imple-mentations often skip this step.Figure 2 shows the complete set of phrase pairsup to length 4 that are extracted from the Figure 1training example.
Notice that no extracted phrasepair contains the character ?.
Because of the align-ments, the smallest legal phrase pair, ?
?
 ?
i felt obliged to do my, is beyond the sizelimit of 4, so it is not extracted in this example.?
?
felt?
 ?
felt obliged?
  ?
felt obliged to do ?
obliged  ?
obliged to do ?
do P ?
part P ?
?
part P ?
.
?
part .?
.
?
.. ?
.Figure 2: phrases up to length 4 extracted from theexample in Figure 1Phrase pairs are extracted over the entire train-ing corpus.
Due to differing alignments, somephrase pairs that cannot be learned from one exam-ple may be learned from another.
These pairs arethen counted, once for each time they are seen in atraining example, and these counts are used as thebasis for maximum likelihood probability features,such as p(f |e) and p(e|f).3 Syntax-based ExtractionThe GHKM syntax-based extraction method forlearning statistical syntax-based translation rules,presented first in (Galley et al, 2004) and expandedon in (Galley et al, 2006), is similar to phrase-basedextraction in that it extracts rules consistent withgiven word alignments.
A primary difference is theuse of syntax trees on the target side, rather than se-quences of words.
The basic unit of translation is thetranslation rule, consisting of a sequence of words756and variables in the source language, a syntax treein the target language having words or variables atthe leaves, and again a vector of feature values whichdescribe this pair?s likelihood.
Translation rules can:?
look like phrase pairs with syntax decoration:NPB(NNP(prime)NNP(minister)NNP(keizo)NNP(obuchi))?
B ?
?
?
D #?
carry extra contextual constraints:VP(VBD(said)x0:SBAR-C)?
?
x0(according to this rule, ?
can translate tosaid only if some Chinese sequence to theright of ?
is translated into an SBAR-C)?
be non-constituent phrases:VP(VBD(said)SBAR-C(IN(that)x0:S-C))?
?
x0VP(VBD(pointed)PRT(RP(out))x0:SBAR-C)?
?
?
x0?
contain non-contiguous phrases, effectively?phrases with holes?:PP(IN(on)NP-C(NPB(DT(the)x0:NNP))NN(issue))))?
?
x0 ?
?PP(IN(on)NP-C(NPB(DT(the)NN(issue))x0:PP))?
?
x0 ?
??
be purely structural (no words):S(x0:NP-C x1:VP)?
x0 x1?
re-order their children:NP-C(NPB(DT(the)x0:NN)PP(IN(of)x1:NP-C))?
x1 { x0Decoding with this model produces a tree in thetarget language, bottom-up, by parsing the foreignstring using a CYK parser and a binarized rule set(Zhang et al, 2006).
During decoding, features fromeach translation rule are combined with a languagemodel using a log-linear model to compute the scoreof the entire translation.The GHKM extractor learns translation rules froman aligned parallel corpus where the target side hasbeen parsed.
This corpus is conceptually a list of tu-ples of <source sentence, target tree, bi-directionalword alignments> which serve as training exam-ples, one of which is shown in Figure 3.Figure 3: a syntax-based training exampleFor each training example, the GHKM extrac-tor computes the set of minimally-sized translationrules that can explain the training example while re-maining consistent with the alignments.
This is, ineffect, a non-overlapping tiling of translation rulesover the tree-string pair.
If there are no unalignedwords in the source sentence, this is a unique set.This set, ordered into a tree of rule applications, iscalled the derivation tree of the training example.Unlike the ATS model, there are no inherent sizelimits, just the constraint that the rules be as smallas possible for the example.Ignoring the unaligned ?
for the moment, thereare seven minimal translation rules that are extractedfrom the example in Figure 3, as shown in Fig-ure 4.
Notice that rule 6 is rather large and appliesto a very limited syntactic context.
The only con-stituent node that covers both i and my is the S,so the rule rooted at S is extracted, with variablesfor every branch below this top constituent that canbe explained by other rules.
Note also that to be-757comes a part of this rule naturally.
If the alignmentswere not as constraining (e.g., if my was unaligned),then instead of this one big rule many smaller ruleswould be extracted, such as structural rules (e.g.,VP(x0:VBD x1:VP-C)?
x0 x1) and function word in-sertion rules (e.g., VP(TO(to) x0:VP-C)?
x0).1.
VBD(felt)?
?2.
VBN(obliged)?
3.
VB(do)?
4.
NN(part)?
 P5.
PERIOD(.)?
.6.
S(NP-C(NPB(PRP(I)))VP(x0:VBDVP-C(x1:VBNSG-C(VP(TO(to)VP-C(x2:VBNP-C(NPB(PRP$(my)x3:NN)))))))x4:PERIOD)?
?
x0 x1 x2 x3 x47.
TOP(x0:S)?
x0Figure 4: rules extracted from training exampleWe ignored unaligned source words in the exam-ple above.
Galley et al (2004) attach the unalignedsource word to the highest possible location, in ourexample, the S. Thus it is extracted along with ourlarge rule 6, changing the target language sequenceto ??
x0 x1 x2 x3 ?
x4?.
This treatment still re-sults in a unique derivation tree no matter how manyunaligned words are present.In Galley et al (2006), instead of a unique deriva-tion tree, the extractor computes several derivationtrees, each with the unaligned word added to a dif-ferent rule such that the data is still explained.
Forexample, for the tree-string pair in Figure 3, ?could be added not only to rule 6, but alternativelyto rule 4 or 5, to make the new rules:NN(part)?
 P ?PERIOD(.)?
?
.This results in three different derivations, onewith the ?
character in rule 4 (with rules 5 and 6as originally shown), another with the ?
characterin rule 5 (with rules 4 and 6 as originally shown),and lastly one with the ?
character in rule 6 (withrules 4 and 5 as originally shown) as in the origi-nal paper (Galley et al, 2004).
In total, ten differentrules are extracted from this training example.As with ATS, translation rules are extracted andcounted over the entire training corpus, a count ofone for each time they appear in a training example.These counts are used to estimate several features,including maximum likelihood probability featuresfor p(etree, fwords|ehead), p(ewords|fwords), andp(fwords|ewords).4 Differences in Phrasal CoverageBoth the ATS model and the GHKM model extractlinguistic knowledge from parallel corpora, but eachhas fundamentally different constraints and assump-tions.
To compare the models empirically, we ex-tracted phrase pairs (for the ATS model) and transla-tion rules (for the GHKM model) from parallel train-ing corpora described in Table 1.
The ATS modelwas limited to phrases of length 10 on the sourceside, and length 20 on the target side.
A super-set of the parallel data was word aligned by GIZAunion (Och and Ney, 2003) and EMD (Fraser andMarcu, 2006).
The English side of training data wasparsed using an implementation of Collins?
model 2(Collins, 2003).Chinese ArabicDocument IDs LDC2003E07 LDC2004T17LDC2003E14 LDC2004T18LDC2005T06 LDC2005E46# of segments 329,031 140,511# of words in foreign corpus 7,520,779 3,147,420# of words in English corpus 9,864,294 4,067,454Table 1: parallel corpora used to train both modelsTable 2 shows the total number of GHKM rulesextracted, and a breakdown of the different kindsof rules.
Non-lexical rules are those whose sourceside is composed entirely of variables ?
there areno source words in them.
Because of this, theypotentially apply to any sentence.
Lexical rules(their counterpart) far outnumber non-lexical rules.Of the lexical rules, a rule is considered a phrasalrule if its source side and the yield of its targetside contain exactly one contiguous phrase each, op-tionally with one or more variables on either sideof the phrase.
Non-phrasal rules include structuralrules, re-ordering rules, and non-contiguous phrases.These rules are not easy to directly compare to anyphrase pairs from the ATS model, so we do not focuson them here.Phrasal rules can be directly compared to ATSphrase pairs, the easiest way being to discard the758Statistic Chinese Arabictotal translation rules 2,487,110 662,037non-lexical rules 110,066 15,812lexical rules 2,377,044 646,225phrasal rules 1,069,233 406,020distinct GHKM-derived phrase pairs 919,234 352,783distinct corpus-specificGHKM-derived phrase pairs 203,809 75,807Table 2: a breakdown of how many rules theGHKM extraction algorithm produces, and howmany phrase pairs can be derived from themsyntactic context and look at the phrases containedin the rules.
The second to last line of Table 2 showsthe number of phrase pairs that can be derived fromthe above phrasal rules.
The number of GHKM-derived phrase pairs is lower than the number ofphrasal rules because some rules represent the samephrasal translation, but with different syntactic con-texts.
The last line of Table 2 shows the subset ofphrase pairs that contain source phrases found in ourdevelopment corpus.Table 3 compares these corpus-specific GHKM-derived phrase pairs with the corpus-specific ATSphrase pairs.
Note that the number of phrase pairsderived from the GHKM rules is less than the num-ber of phrase pairs extracted by ATS.
Moreover, onlyslightly over half of the phrase pairs extracted by theATS model are common to both models.
The lim-its and constraints of each model are responsible forthis difference in contiguous phrases learned.Source of phrase pairs Chinese ArabicGHKM-derived 203,809 75,807ATS 295,537 133,576Overlap between models 160,901 75,038GHKM only 42,908 769ATS only 134,636 58,538ATS-useful only 1,994 2,199Table 3: comparison of corpus-specific phrase pairsfrom each modelGHKM learns some contiguous phrase pairs thatthe phrase-based extractor does not.
Only a smallportion of these are due to the fact that the GHKMmodel has no inherent size limit, while the phrasebased system has limits.
More numerous are caseswhere unaligned English words are not added to anATS phrase pair while GHKM adopts them at a syn-tactically motivated location, or where a larger rulecontains mostly syntactic structure but happens tohave some unaligned words in it.
For example, con-sider Figure 5.
Because basic and will are un-aligned, ATS will learn no phrase pairs that translateto these words alone, though they will be learned asa part of larger phrases.Figure 5: Situation where GHKM is able to learnrules that translate into basic and will, but ATSis notGHKM, however, will learn several phrasal rulesthat translate to basic, based on the syntactic con-textNPB(x0:DTJJ(basic)x1:NN)?
x0  x1NPB(x0:DTJJ(basic)x1:NN)?
x0  ?
?
x1NPB(x0:DTJJ(basic)x1:NN)?
x0 ?
?
x1and one phrasal rule that translates into willVP(MD(will)x0:RBx1:VP-C)?
x0 ?
?
x1The quality of such phrases may vary.
For example,the first translation of  (literally: ?one?
or ?a?)
tobasic above is a phrase pair of poor quality, whilethe other two for basic and one for will are ar-guably reasonable.However, Table 3 shows that ATS was able tolearn many more phrase pairs that GHKM was not.Even more significant is the subset of these missingphrase pairs that the ATS decoder used in its best22i.e.
highest scoring759translation of the corpus.
According to the phrase-based system these are the most ?useful?
phrasepairs and GHKM could not learn them.
Since this isa clear deficiency, we will focus on analyzing thesephrase pairs (which we call ATS-useful) and the rea-sons they were not learned.Table 4 shows a breakdown, categorizing each ofthese missing ATS-useful phrase pairs and the rea-sons they were not able to be learned.
The mostcommon reason is straightforward: by extractingonly the minimally-sized rules, GHKM is unable tolearn many larger phrases that ATS learns.
If GHKMcan make a word-level analysis, it will do that, atthe expense of a phrase-level analysis.
Galley etal.
(2006) propose one solution to this problem andMarcu et al (2006) propose another, both of whichwe explore in Sections 5.1 and 5.2.Category of missing ATS-useful phrase pairs Chinese ArabicNot minimal 1,320 1,366Extra target words in GHKM rules 220 27Extra source words in GHKM rules 446 799Other (e.g.
parse failures) 8 7Total missing useful phrase pairs 1,994 2,199Table 4: reasons that ATS-useful phrase pairs couldnot be extracted by GHKM as phrasal rulesThe second reason is that the GHKM model issometimes forced by its syntactic constraints to in-clude extra words.
Sometimes this is only target lan-guage words, and this is often useful ?
the rules arelearning to insert these words in their proper context.But most of the time, source language words are alsoforced to be part of the rule, and this is harmful ?
itmakes the rules less general.
This latter case is oftendue to poorly aligned target language words (such asthe ?
in our Section 3 rule extraction example), orunaligned words under large, flat constituents.Another factor here: some of the phrase pairs arelearned by both systems, but GHKM is more specificabout the context of use.
This can be both a strengthand a weakness.
It is a strength when the syntacticcontext helps the phrase to be used in a syntacticallycorrect way, as inVP(VBD(said)x0:SBAR-C)?
?
x0where the syntax rule requires a constituent of typeSBAR-C. Conversely its weakness is seen when thecontext is too constrained.
For example, ATS caneasily learn the phrase ?
?
prime ministerand is then free to use it in many contexts.
ButGHKM learns 45 different rules, each that translatethis phrase pair in a unique context.
Figure 6 showsa sampling.
Notice that though many variations arepresent, the decoder is unable to use any of theserules to produce certain noun phrases, such as ?cur-rent Japanese Prime Minister Shinzo Abe?, becauseno rule has the proper number of English modifiers.NPB(NNP(prime) NNP(minister) x0:NNP)?
x0  ?NPB(x0:NNP NNP(prime) NNP(minister) x1:NNP)?
x0  ?
x1NPB(x0:JJ NNP(prime) NNP(minister) x1:NNP)?
x0  ?
x1NPB(NNP(prime) NNP(minister) x0:NNP)?
 ?
x0NPB(NNP(prime) NNP(minister))?
 ?NPB(NNP(prime) NNP(minister) x0:NNP x1:NNP)?
x0 x1  ?NPB(x0:DT x1:JJ JJ(prime) NN(minister))?
x0 x1  ?NPB(x0:NNP NNP(prime) NNP(minister) x1:NNP)?
x0  ?
x1NPB(x0:NNP NNP(prime) NNP(minister) x1:NNP)?
x0  ?
x1Figure 6: a sampling of the 45 rules that translate ?
to prime minister5 Coverage ImprovementsEach of the models presented so far has advantagesand disadvantages.
In this section, we consider ideasthat make up for deficiencies in the GHKM model,drawing our inspiration from the strong points of theATS model.
We then measure the effects of eachidea empirically, showing both what is gained andthe potential limits of each modification.5.1 Composed RulesGalley et al (2006) proposed the idea of composedrules.
This removes the minimality constraint re-quired earlier: any two or more rules in a parent-child relationship in the derivation tree can be com-bined to form a larger, composed rule.
This changeis similar in spirit to the move from word-based tophrase-based MT models, or parsing with a DOPmodel (Bod et al, 2003) rather than a plain PCFG.Because this results in exponential variations, asize limit is employed: for any two or more rulesto be allowed to combine, the size of the resultingrule must be at most n. The size of a rule is de-fined as the number of non-part-of-speech, non-leaf760constituent labels in a rule?s target tree.
For exam-ple, rules 1-5 shown in Section 3 have a size of 0,and rule 6 has a size of 10.
Composed rules are ex-tracted in addition to minimal rules, which meansthat a larger n limit always results in a superset ofthe rules extracted when a smaller n value is used.When n is set to 0, then only minimal rules are ex-tracted.
Table 5 shows the growth in the number ofrules extracted for several size limits.Size limit (n) Chinese Arabic0 (minimal) 2,487,110 662,0372 12,351,297 2,742,5133 26,917,088 4,824,9284 55,781,061 8,487,656Table 5: increasing the size limit of composed rulessignificantly increases the number of rules extractedIn our previous analysis, the main reason thatGHKM did not learn translations for ATS-usefulphrase pairs was due to its minimal-only approach.Table 6 shows the effect that composed rule extrac-tion has on the total number of ATS-useful phrasesmissing.
Note that as the allowed size of composedrule increases, we are able to extract an greater per-centage of the missing ATS-useful phrase pairs.Size limit (n) Chinese Arabic0 (minimal) 1,994 2,1992 1,478 1,5283 1,096 1,2104 900 1,041Table 6: number of ATS-useful phrases still missingwhen using GHKM composed rule extractionUnfortunately, a comparison of Tables 5 and 6 in-dicates that the number of ATS-useful phrase pairsgained is growing at a much slower rate than the totalnumber of rules.
From a practical standpoint, morerules means more processing work and longer de-coding times, so there are diminishing returns fromcontinuing to explore larger size limits.5.2 SPMT Model 1 RulesAn alternative for extracting larger rules calledSPMT model 1 is presented by Marcu et al (2006).Though originally presented as a separate model,the method of rule extraction itself builds upon theminimal GHKM method just as composed rules do.For each training example, the method considers allsource language phrases up to length L. For each ofthese phrases, it extracts the smallest possible syn-tax rule that does not violate the alignments.
Ta-ble 7 shows that this method is able to extract rulesthat cover useful phrases, and can be combined withsize 4 composed rules to an even better effect.
Sincethere is some overlap in these methods, when com-bining the two methods we eliminate any redundantrules.Method Chinese Arabiccomposed alone (size 4) 900 1,041SPMT model 1 alone 676 854composed + SPMT model 1 663 835Table 7: ATS-useful phrases still missing after dif-ferent non-minimal methods are appliedNote that having more phrasal rules is not the onlyadvantage of composed rules.
Here, combining bothcomposed and SPMT model 1 rules, our gain in use-ful phrases is not very large, but we do gain addi-tional, larger syntax rules.
As discussed in (Galleyet al, 2006), composed rules also allow the learningof more context, such asADJP(ADVP(RB(far)CC(and)RB(away)x0:JJ)?
?
?
x0This rule is not learned by SPMT model 1 becauseit is not the smallest rule that can explain the phrasepair, but it is still valuable for its syntactic context.5.3 Restructuring TreesTable 8 updates the causes of missing ATS-usefulphrase pairs.
Most are now caused by syntactic con-straints, thus we need to address these in some way.GHKM translation rules are affected by large,flat constituents in syntax trees, as in the primeminister example earlier.
One way to soften thisconstraint is to binarize the trees, so that wide con-stituents are broken down into multiple levels of treestructure.
The approach we take here is head-out bi-narization (Wang et al, 2007), where any constituentwith more than two children is split into partial con-stituents.
The children to the left of the head word761Category of ATS-useful phrase pairs Chinese ArabicToo large 12 9Extra target words in GHKM rules 218 27Extra source words in GHKM rules 424 792Other (e.g.
parse failures) 9 7Total missing useful phrase pairs 663 835Table 8: reasons that ATS-useful phrase pairs arestill not extracted as phrasal rules, with composedand SPMT model 1 rules in placeare binarized one direction, while the children tothe right are binarized the other direction.
The topnode retains its original label (e.g.
NPB), while thenew partial constituents are labeled with a bar (e.g.NPB).
Figure 7 shows an example.Figure 7: head-out binarization in the target lan-guage: S, NPB, and VP are binarized according tothe head wordTable 9 shows the effect of binarization on phrasalcoverage, using both composed and SPMT rules.
Byeliminating some of the syntactic constraints we al-low more freedom, which allows increased phrasalcoverage, but generates more rules.Category of missing ATS-useful phrase pairs Chinese ArabicToo large 16 12Extra target words in GHKM rules 123 12Extra source words in GHKM rules 307 591Other (e.g.
parse failures) 12 7Total missing useful phrase pairs 458 622Table 9: reasons that ATS-useful phrase pairs stillcould not be extracted as phrasal rules after bina-rization6 Evaluation of TranslationsTo evaluate translation quality of each of these mod-els and methods, we ran the ATS decoder using itsextracted phrase pairs and the syntax-based decoderusing all the rule sets mentioned above.
Table 10 de-scribes the development and test datasets used, alongwith four references for measuring BLEU.
Tun-ing was done using Maximum BLEU hill-climbing(Och, 2003).
Features used for the ATS system werethe standard set.
For the syntax-based translationsystem, we used a similar set of features.# of linesDataset Chinese ArabicDevelopment set NIST 2002 MT eval 925 696(sentences < 47 tokens)Test set NIST 2003 MT eval 919 663Table 10: development and test corporaTable 11 shows the case-insensitive NIST BLEU4scores for both our development and test decod-ings.
The BLEU scores indicate, first of all, thatthe syntax-based system is much stronger in trans-lating Chinese than Arabic, in comparison to thephrase-based system.
Also, the ideas presented herefor improving phrasal coverage generally improvethe syntax-based translation quality.
In addition,composed rules are shown to be helpful as com-pared to the minimal runs.
This is true even whenSPMT model 1 is added, which indicates that thesize 4 composed rules bring more than just improvedphrasal coverage.Chinese ArabicExperiment Dev Test Dev TestBaseline ATS 34.94 32.83 50.46 50.52Baseline GHKM (minimal only) 38.02 37.67 49.34 49.99GHKM composed size 2 40.24 39.75 50.76 50.94GHKM composed size 3 40.95 40.44 51.56 51.48GHKM composed size 4 41.36 40.69 51.60 51.71GHKM minimal + SPMT model 1 39.78 39.16 50.17 51.27GHKM composed + SPMT model 1 42.04 41.07 51.73 51.53With binarization 42.17 41.26 52.50 51.79Table 11: evaluation results (reported in case-insensitive NIST BLEU4)7 ConclusionsBoth the ATS model for phrase-based machinetranslation and the GHKM model for syntax-basedmachine translation are state-of-the-art methods.Each extraction method has strengths and weak-nesses as compared to the other, and there are sur-prising differences in phrasal coverage ?
neither ismerely a superset of the other.
We have shown thatit is possible to gain insights from the strengths ofthe phrase-based extraction model to increase both762the phrasal coverage and translation accuracy of thesyntax-based model.However, there is still room for improvement inboth models.
For syntax models, there are still holesin phrasal coverage, and other areas are needingprogress, such as decoding efficiency.
For phrase-based models, incorporating syntactic knowledgeand constraints may lead to improvements as well.8 AcknowledgmentsThe authors wish to acknowledge our colleagues atISI, especially David Chiang, for constructive criti-cism on an early draft of this document, and severalreviewers for their detailed comments which helpedus make the paper stronger.
We are also grateful toJens-So?nke Vo?ckler for his assistance in setting upan experimental pipeline, without which this workwould have been much more tedious and difficult.This research was supported under DARPA ContractNo.
HR0011-06-C-0022.ReferencesRens Bod, Remko Scha, and Khalil Sima?an, editors.
2003.Data-Oriented Parsing.
CSLI Publications, University ofChicago Press.Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,and Robert L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
ComputationalLinguistics, 19.David Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proc.
ACL 2005.Michael Collins.
2003.
Head-driven statistical models for nat-ural language parsing.
Computational Linguistics, 29(4).Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006.A discriminative model for tree-to-tree translation.
In Proc.EMNLP 2006.Jason Eisner.
2003.
Learning non-isomorphic tree mappingsfor machine translation.
In Proc.
ACL 2003.Alexander Fraser and Daniel Marcu.
2006.
Semi-supervisedtraining for statistical word alignment.
In Proc.
ACL 2006.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc.
HLT-NAACL 2004.Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu,Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006.
Scal-able inference and training of context-rich syntactic transla-tion models.
In Proc.
ACL 2006.Daniel Gildea.
2003.
Loosely tree-based alignment for ma-chine translation.
In Proc.
ACL 2003, companion volume.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.
Sta-tistical syntax-directed translation with extended domain oflocality.
In Proc.
AMTA 2006.Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.
Sta-tistical phrase-based translation.
In Proc.
HLT-NAACL 2003.Daniel Marcu, Wei Wang, Abdessamad Echihabi, and KevinKnight.
2006.
SPMT: Statistical machine translation withsyntactified target language phrases.
In Proc.
EMNLP 2006.I.
Dan Melamed.
2004.
Statistical machine translation by pars-ing.
In Proc.
ACL 2004.Franz Josef Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Computa-tional Linguistics, 29(1).Franz Josef Och and Hermann Ney.
2004.
The alignment tem-plate approach to statistical machine translation.
Computa-tional Linguistics, 30.Franz Josef Och.
2003.
Minimum error rate training in statisti-cal machine translation.
In Proc.
ACL 2003.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informed phrasalSMT.
In Proc.
ACL 2005.Wei Wang, Kevin Knight, and Daniel Marcu.
2007.
Binarizingsyntax trees to improve syntax-based machine translation ac-curacy.
In Proc.
EMNLP and CoNLL 2007.Dekai Wu and Hongsing Wong.
1998.
Machine translationwith a stochastic grammatical channel.
In Proc.
ACL 1998.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statis-tical translation model.
In Proc.
ACL 2001.Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.2006.
Synchronous binarization for machine translation.
InProc.
NAACL HLT 2006.763
