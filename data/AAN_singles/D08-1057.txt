Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 543?552,Honolulu, October 2008. c?2008 Association for Computational LinguisticsSeed and Grow: Augmenting Statistically Generated Summary Sentencesusing Schematic Word PatternsStephen Wan??
Robert Dale?
Mark Dras?
?Centre for Language TechnologyDepartment of ComputingMacquarie UniversitySydney, NSW 2113swan,madras,rdale@ics.mq.edu.auCe?cile Paris?
?ICT CentreCSIROSydney, AustraliaCecile.Paris@csiro.auAbstractWe examine the problem of content selectionin statistical novel sentence generation.
Ourapproach models the processes performed byprofessional editors when incorporating ma-terial from additional sentences to supportsome initially chosen key summary sentence,a process we refer to as Sentence Augmen-tation.
We propose and evaluate a methodcalled ?Seed and Grow?
for selecting suchauxiliary information.
Additionally, we arguethat this can be performed using schemata, asrepresented by word-pair co-occurrences, anddemonstrate its use in statistical summary sen-tence generation.
Evaluation results are sup-portive, indicating that a schemata model sig-nificantly improves over the baseline.1 IntroductionIn the context of automatic text summarisation, weexamine the problem of statistical novel sentencegeneration, with the aim of moving from the currentstate-of-the-art of sentence extraction to abstract-like summaries.
In particular, we focus on the task ofselecting content to include within a generated sen-tence.Our approach to novel sentence generation is tomodel the processes underlying summarisation asperformed by professional editors and abstractors.An example of the target output of this kind of gen-eration is presented in Figure 1.
In this example, thehuman authored summary sentence was taken verba-tim from the executive summary of a United Nationsproposal for the provision of aid addressing a partic-ular humanitarian crisis.
Such documents typicallyexceed a hundred pages.Human-Authored Summary Sentence:Repeated [poor seasonal rains]1 [in 2004]2, culminatingin [food insecurity]3, indicate [another year]4 of crisis,the scale of which is larger than last year?s and is further[exacerbated by diminishing coping assets]5 [in bothrural and urban areas]6.Key Source Sentence:The consequences of [another year]4 of [poor rains]1 on[food security]3 are severe.Auxiliary Source Sentence(s):However in addition to the needs of economic recoveryactivities for IDPs, [food insecurity]3 [over the major-ity of 2004]2 [has created great stress]5 on the poorestfamilies in the country, [both within the urban and ruralsettings]6.Figure 1: Alignment of a summary sentence to sentencesin the full document.
Phrases of similar meaning are co-indexed.To write such summaries, we assume that the hu-man abstractor begins by choosing key sentencesfrom the full document.
Then, for each key sen-tence, a set of auxiliary material is identified.
Thekey sentence is revised incorporating these auxil-iary sentences to produce the eventual summary sen-tence.To study this phenomenon, a corpus of UN docu-ments was collected and analysed.1 Each documentwas divided into two parts comprising its executivesummary, and the remainder, referred to here as thesource.
We manually aligned each executive sum-mary sentence with one or more sentences from thesource, by choosing a key sentence that provided1This corpus is described in detail in Section 5.1.543evidence for the content of the summary sentencealong with additional sentences that provided sup-porting material.We refer to the resulting corpus as the UN Con-solidated Appeals Process (UN CAP) corpus.
It isa collection of sentence alignments, each referred toas an aligned sentence tuple, which consists of:1.
A human authored summary sentence from theexecutive summary;2.
A key sentence from the source;3.
Zero or more auxiliary sentences from thesource.The key and any auxiliary sentences are referred tocollectively as the aligned source sentences.We argue that some process that combines infor-mation from multiple sentences is required if we areto generate summary sentences similar to that por-trayed in Figure 1.
This is supported by our analysisof the UN CAP corpus.
Of the 580 aligned sentencetuples, the majority, 61% of cases, appear to be ex-amples of such a process.Furthermore, the auxiliary sentences are clearlynecessary.
We found that only 30% of the open-classwords in the summary are found in the key sentence.If one selects all the open-class words from alignedsource sentences, recall increases to an upper limitof 45% without yet accounting for stemming.
Thisupper bound is consistent with the upper limit of50% found by Daume?
III and Marcu (2005) whichtakes into account stemming differences.This demonstrates that the auxiliary material isa valuable source of content which should be inte-grated into the summary sentence, allowing an im-provement in recall of up to 15% prior to account-ing for morphological, synonym and paraphrase dif-ferences.
Of course, the trick is to improve recallwithout hurting precision.
A naive addition of allwords in the aligned source sentences incurs a dropin precision from 30% to 23%.
The problem thus isone of selecting the relevant auxiliary content wordswithout introducing unimportant content.
We referto this problem of incorporating material from aux-iliary sentences to supplement a key sentence as Sen-tence Augmentation.In this paper, sentence augmentation is modelledas a noisy channel process and has two facets: con-tent selection and language modelling.
This paperfocuses on the former, in which the system mustrank text segments?in this case, words?for inclu-sion in the generated sentence.
Given a ranked se-lection of words, a language model would then orderthem appropriately, as described in work on sentenceregeneration (for example, see Soricut and Marcu(2005); Wan et al (2005)).Provided with an aligned sentence tuple, the prob-lem lies in effectively selecting words from the aux-iliary sentences to bolster those taken from the keysentence.
Given that there are on average 2.7 aux-iliary sentences per aligned sentence tuple, this ad-ditional influx of words poses a considerable chal-lenge.We begin with the premise that, for documentsof a homogeneous type (in this case, the genre isa funding proposal, and the domain is humanitarianaid), it may be possible to identify patterns in the or-ganisation of information in summaries.
For exam-ple, Figure 2 presents three summary sentences fromour corpus that share the same patterned juxtapo-sition of two concepts DisplacedPersons and Host-ingCommunities.
Documents may exhibit commonpatterns since they have a similar goal: namely, toconvince donors to give financial support.
In theabove example, the juxtaposition highlights the factthat those in need are not just those people from the?epicenter?
of the crisis but also those that look afterthem.We propose and evaluate a method called ?Seedand Grow?
for selecting content from auxiliary sen-tences.
That is, we first select the core meaning ofthe summary, given here by the key sentence, andthen we find those pieces of additional informationthat are conventionally juxtaposed with it.Such patterns are reminiscent of Schemata, the or-ganisations of propositional content introduced byMcKeown (1985).
Schemata typically involve asymbolic representation of each proposition?s se-mantics.
However, in our case, a text-to-text gener-ation scenario, we are without such representationsand so must find other means to encode these pat-terns.To alleviate the situation, we turn to word-pair co-occurrences to approximate schematic patterns.
Fig-544Sentence 1:The increased number of [internally displaced persons]1and the continued presence of refugees have fur-ther strained the scarce natural resources of [hostcommunities]2, stretching their capacity to the limit.Sentence 2:100,000 people, a significant portion of the population,remain [displaced]1, burdening the already precariousliving conditions of [host families]2 in Dili and theDistricts.Sentence 3:The current humanitarian situation in Timor-Leste ischaracterised by: An estimated [100,000 displacedpeople]1 (10% of the population) living in camps andwith [host families]2 in the districts; A total or partial de-struction of over 3,000 homes in Dili affecting at least14,000 IDPsFigure 2: Examples of the pattern ?DisplacedPersons[1],HostingCommunities[2]?.ure 2 showed that mentions of the plight of interna-tionally displaced persons are often followed by de-scriptions of the impact on the host communities thatlook after them.
In this particular example, this isrealised lexically in the co-occurrences of the wordsdisplaced and host.Corpus-based methods inspired by the notion ofschemata have been explored in the past by Lap-ata (2003) and Barzilay and Lee (2004) for order-ing sentences extracted in a multi-document sum-marisation application.
However, to our knowledge,using word co-occurrences in this manner to repre-sent schematic knowledge for the purposes of select-ing content in a statistically-generated summary sen-tence has not previously been explored.This paper seeks to determine whether or not suchpatterns exist in homogeneous data; and further-more, whether such patterns can be used to betterselect words from auxiliary sentences.
In particular,we propose the ?Seed and Grow?
approach for thistask.
The results show that even simple modellingapproaches are able to model this schematic infor-mation.In the remainder of this paper, we contrast our ap-proach to related text-to-text research in Section 2.The Content Selection model is presented in Section3.
Section 4 describes how a binary classificationmodel is used in a statistical text generation system.Section 5 describes our evaluation of the model for asummary generation task.
We conclude, in Section6, that domain-specific schematic patterns can be ac-quired and applied to content selection for statisticalsentence generation.2 Related Work2.1 Content Selection in Text-to-Text SystemsStatistical text-to-text summarisation applicationshave borrowed much from the related field of statis-tical machine translation.
In one of the first works topresent summarisation as a noisy channel approach,Witbrock and Mittal (1999) presented a conditionalmodel for learning the suitability of words from anews article for inclusion in headlines, or ?ultra-summaries?.
Inspired by this approach, and withthe intention of designing a robust statistical gener-ation system, our work is also based on the noisychannel model.
Into this, we incorporate our con-tent selection model, which includes Witbrock andMittal?s model supplemented with schema-based in-formation.Roughly, text-to-text transformations fall intothree categories: those in which information is com-pressed, conserved, and augmented.
We use thesedistinctions to organise this overview of the litera-ture.In Sentence Compression work, a single sentenceundergoes pruning to shorten its length.
Previ-ous approaches have focused on statistical syntactictransformations (Knight and Marcu, 2002).
For con-tent selection, discourse-level considerations wereproposed by Daume?
III and Marcu (2002), who ex-plored the use of Rhetorical Structure Theory (Mannand Thompson, 1988).
More recently, Clarke andLapata (2007) use Centering Theory (Grosz et al,1995) and Lexical Chains (Morris and Hirst, 1991)to identify which information to prune.
Our work issimilar in incorporating discourse-level phenomenafor content selection.
However, we look at schema-like information as opposed to chains of referencesand focus on the sentence augmentation task.The work of Barzilay and McKeown (2005) onSentence Fusion introduced the problem of convert-ing multiple sentences into a single summary sen-545tence.
Each sentence set ideally tightly clustersaround a single news event.
Thus, there is one gen-eral proposition to be realised in the summary sen-tence, identified by finding the common elements inthe input sentences.
We see this as an example ofconservation.
In our work, this general propositionis equivalent to the core information for the sum-mary sentence before the incorporation of supple-mentary material.In contrast to both compression and conservationwork, we focus on augmenting the information ina key sentence.
The closest work is that of Jingand McKeown (1999) and Daume?
III and Marcu(2005), in which multiple sentences are processed,with fragments within them being recycled to gener-ate the novel generated text.In both works, recyclable fragments are identifiedby automatic means.
Jing and McKeown (1999) usemodels that are based on ?copy-and-paste?
opera-tions learnt from the behaviour of human abstrac-tors as found in a corpus.
Daume?
III and Marcu(2005) propose a model that encodes how likely itis that different sized spans of text are skipped toreach words and phrases to recycle.While similar in task, our models differ substan-tially in the nature of the phenomenon modelled.
Inthis work, we focus on content-based considerationsthat model which words can be combined to buildup a new sentence.2.2 Schemata and Text GenerationThere exists related work from Natural LanguageGeneration (NLG) in finding material to build upsentences.
As mentioned above, our content selec-tion model is inspired by work on schemata fromNLG (McKeown, 1985).
Barzilay and Lee (2004)showed that it is possible to obtain schema-likeknowledge automatically from a corpus for the pur-poses of extracting sentences and ordering them.However, their work represents patterns at the sen-tence level, and is thus not directly comparable toour work, given our focus on sentence generation.In our system, what is required is a means to rankwords for use in generation.
Thus, we focus on com-monly occurring word co-occurrences, with the aimof encoding conventions in the texts we are trying togenerate.
In this respect, this is similar to work byLapata (2003), who builds a conditional model ofwords across adjacent sentences, focusing on wordsin particular semantic roles.
Like Barzilay and Lee(2004), this model was used to order extracted sen-tences in summaries.
In contrast, our work focuseson word patterns found within a summary sentence,not between sentences.
Additionally, our tasks dif-fer as we examine the statistical sentence generationinstead of sentence ordering.3 Linguistic Intuitions behind WordSelectionThe ?Seed and Grow?
approach proposed in this pa-per divides the word-level content selection prob-lem into two underlying subproblems.
We addressthese with two separate models, called the salienceand schematic models.
The salience model choosesthe key content for the summary sentence while theschematic model attempts to identify what else istypically mentioned given those salient pieces of in-formation.3.1 A Salience Model: Learning ?Buzzwords?There are a variety of methods for determining thesalient information in a text, and these underpinmost work in automatic text summarisation.
As anexample of a salience model trained on corpus data,Witbrock and Mittal (1999) introduced a method forscoring summary words for inclusion within newsheadlines.
In their model, headlines were treated as?ultra-summaries?.
Their model learns which wordsare typically used in headlines and encodes, at leastto some degree, which words are attention grabbing.In the domain of funding proposals, key wordsthat grab attention may amount to domain-specificbuzzwords.
Intuitively, a reader, perhaps someonein charge of allocating donations, tends to look forcertain types of key information matching donationcriteria, and so human abstract authors will targettheir summaries for this purpose.We thus adapt the Witbrock and Mittal (1999)model to identify such domain specific buzzwords(BWM, for ?buzzword model?).
For an aligned sen-tence tuple, the probability that a word is selectedbased on the salience of a word with respect to thedomain is defined as:probbwm(select = 1|w) =|summaryw||sourcew|(1)546where summaryw is the set of aligned sentence tu-ples that contain the word w in the summary sen-tence and in the source sentences.
The denomina-tor, sourcew, is the set of aligned sentence tuples thathave the word w in either the key or an auxiliary sen-tence.As is implicit in this equation, we could just usethis buzzword model to select content not only fromthe key sentence, but from the auxiliary sentencesas well.
While it is intended ultimately to find thekey content of the summary, it can also serve as analternative baseline for auxiliary content selection tocompare against the ?Seed and Grow?
model.3.2 A Schema Model: Approximation viaWord co-OccurrencesTo restate the problem at hand: the task is oneof finding elements of secondary importance thatschematically elaborate on the key information.
Wedo this by examining sample summary sentences forconventional juxtapositions of concepts.
As men-tioned in Section 1, schemata are approximated herewith patterns of word-pair co-occurrences.
Using acorpus of human-authored summaries in the domainof our application, it is thus possible to learn whatthose common combinations of words are.Roughly, the process is as follows.
To begin with,a seed set of words is chosen.
The purpose of theseed set is to represent the core proposition of thesummary sentence.In this work, this core proposition is given by thekey sentence and so the non-stopwords belonging toit are used to populate the seed set.
In the ?Seed andGrow?
approach, we check to see which words fromauxiliary sentences pair well with words in the seedset.3.2.1 Collecting Word-level PatternsEach training case in the corpus contains a singlehuman-authored summary sentence that can be usedto learn which pairs of words conventionally occurin a summary.
For each summary sentence, stop-words are removed.
Then, each pairing of words inthe sentence is used to update a pair-wise word co-occurrence frequency table.
When looking up andstoring a frequency, the order of words is ignored.3.2.2 Scoring Word-Pair Co-occurrenceStrengthFor any two words, w1 from the seed set and w2 froman auxiliary sentence, the word-pair co-occurrenceprobability is defined as follows:probco-oc(w1,w2)= freq(w1,w2)freq(w1)+ freq(w2)?
freq(w1,w2)(2)where f req(w1,w2) is a lookup in the word-pair co-occurrence frequency table.
This table stores co-occurrence word pairs occurring in the summarysentence.3.2.3 Combining a Set of Co-occurrence ScoresEach auxiliary word now has a series of scores,one for each comparison with a seed word.
To rankeach auxiliary word, these need to be combined intoa single score for sorting.When combining the set of co-occurrence scores,one might want to account for the fact that each pair-ing of a seed word with an auxiliary word mightnot contribute equally to the overall selection of thatauxiliary word.
Intuitively, a word in the seed set,derived from the key sentence, may only make aminor contribution to the core meaning of the sum-mary sentence.
For example, words that are part ofan adjunct phrase in the key sentence might not begood candidates to elaborate upon.
Thus, one mightwant to weight these seed words lower, to reducetheir influence on triggering schematically associ-ated words.To allow for this, a seed weight vector is main-tained, storing a weight per seed word.
Differentweighting schemes are possible.
For example, ascheme might indicate the salience of a word.
Inaddition to the buzzword model (BWM) describedearlier, one might employ a standard vector spaceapproach (Salton and McGill, 1983) from Informa-tion Retrieval, which uses term frequency scoresweighted with an inverse document frequency fac-tor, or tf-idf.
We also implement the case in which allseed words are treated equally using binary weights,where 1 indicates the presence of a seed word, and0 indicates its absence.
In the evaluations describedin Section 5, we refer to these three seed weightingschemes as bwm and tf-idf, and binary respectively.547To find the probability of selecting an auxiliaryword using the schematic word-pair co-occurrencemodel (WCM), an averaged probability is foundby normalising the sum of the weighted probabili-ties, where weights are provided by one of the threeschemes above:probwcm(wi) =1Z?|seed|?k=0weightsk ?probco-oc(wi,wk) (3)where seed is the set of seed words and wk is the kthword in that set.
The vector, weights, stores the seedweights.
The normalisation factor for the weightedaverage, Z, is the number of auxiliary words.Finally, since the WCM model only serves to se-lect words from the auxiliary sentences, words fromthe key sentence must be given scores as well.
Forthese words, the scoring is as follows:probwcm(w) =1Z(1|seed| + probwcm(w))(4)where Z is a normalisation across the set of seedwords.4 Combining Buzzwords and Word-PairCo-Occurrence Models for GenerationAs mentioned above, the noisy channel approachis used for producing the augmented sentence.
Al-though the focus of this paper is on Content Selec-tion, an overview of the end-to-end generation pro-cess is presented for completeness.Sentence augmentation is essentially a text-to-textprocess: A key sentence and auxiliary material aretransformed into a single summary sentence.
Fol-lowing Witbrock and Mittal (1999), the task is tosearch for the string of words that maximises theprobability prob(summary|source).
Standardly re-formulating this probability using Bayes?
rule re-sults in the following:probcm(source|summary)?problm(summary) (5)In this paper, we are concerned with the firstfactor, probcm(source|summary), referred to as thechannel model (CM), which combines both thebuzzword (BWM) and word-pair co-occurrence(WCM) models.
An examination of differences be-tween the two approaches revealed only a 20% wordoverlap on the Jaccard metric.In order to combine multiple models, we intendto use machine learning approaches to combine theinformation in each model in a similar manner toBerger et al (1996).
We are currently exploring theuse of logistic regression methods to learn a func-tion that would treat, as features, the probabilitiesdefined by the salience and schematic content selec-tion models.
Although generation is possible usingeach content selection model in isolation, evalua-tions of the combined model are on-going and arenot presented in this paper.5 EvaluationIn this evaluation, the task is to select n words fromthe aligned source sentences for inclusion in a sum-mary.
As a gold-standard for comparison, we sim-ply examine what words were actually chosen in thesummary sentence of the aligned sentence tuple.
Weare specifically interested in open-class words, andso a stopword list of closed-class words is used tofilter the sentences in each test case.We evaluate against the set of open-class wordsin the human-authored summary sentence using re-call and precision metrics.
Recall is the size ofthe intersection of the selected and gold-standardsets, normalised by the length of the gold-standardsentence (in words).
This recall metric is similarto the ROUGE-1 metric, the unigram version ofthe ROUGE metric (Lin and Hovy, 2003) used inthe Document Understanding Conferences2 (DUC).Precision is the size of the intersection normalisedby the number of words selected.
We also report theF-measure, which is the harmonic mean of the recalland precision scores.Recall, precision and F-measure are measured atvarious values of n ranging from 1 to the number ofopen-class words in the gold-standard summary sen-tence for a particular test case.
For the purposes ofevaluation, differences in tokens due to morphologywere explored crudely via the use of Porter?s stem-ming algorithm.
However, the results from stem-ming are not that different from exact token matcheswhen examining performance on the entire data set2http://duc.nist.gov548Number of training cases 530Average words in summary sentence 27.0Average stopwords in summary sentence 10.3Average number of auxiliary sentences 2.75Word count: summary sentences 4630Word count: source sentences 21356Word type count in corpus 3800Table 1: Statistics for the UN CAP training setand so, for simplicity, these are omitted in this dis-cussion.5.1 The DataThe corpus is made up of a number of humanitar-ian aid proposals called Consolidated Appeals Pro-cess (UN CAP) documents, which are archived atthe United Nations website.3 135 documents fromthe period 2002 to 2007 were downloaded by the au-thors.
A preprocessing stage extracted text from thePDF files and segmented the documents into execu-tive summary and source sections.
These were thenautomatically segmented further into sentences.Executive summary sentences were manuallyaligned by the authors to source key and auxiliarysentences, producing a corpus of 580 aligned sen-tence tuples referred to here as the UN CAP cor-pus.
Of these, 230 tuples were paraphrase cases (i.e.without aligned auxiliary sentences).
The remaining550 cases were instances of sentence augmentation(with at least one auxiliary sentence).Of the 580 cases, 50 cases were set aside for test-ing.
The remaining 530 cases were used for train-ing.
Statistics for the training portion of the sentenceaugmentation set are provided in Table 1.In this paper, aligned sentence tuples are obtainedvia manual annotation.
Automatic constructionof these sentence-level alignments is possible andhas been explored by Jing and McKeown (1999).We also envisage using tools for scoring sentencesimilarity (for example, see Hatzivassiloglou et al(2001)) for automatically constructing them; this isthe focus of work by Wan and Paris (2008).3http://ochaonline3.un.org/humanitarianappeal/index.htm5.2 The BaselinesThree baselines were used in this work: the random,tf-idf and position baselines.
A random word selec-tor shows what performance might be achieved inthe absence of any linguistic knowledge.We also sorted all words in the aligned source sen-tences by their weighted tf-idf scores.
This baselineselects words in order until the desired word limitis reached.
This baseline is referred to as the tf-idfbaseline.Finally, we selected words based on their sen-tence order, choosing first those words from the keysentence.
When these are exhausted, auxiliary sen-tences are sorted by their sentence positions in theoriginal document.
Words from the first auxiliarysentence are then chosen.
This continues until ei-ther the desired number of words have been chosen,or no words remain.
This baseline is known as theposition baseline.5.3 Content Selection ResultsWe compare the three baselines to the two mod-els presented in Section 3.
These are the buzzwordsalience model (BWM) and the schematic word-pairco-occurrence model (WCM).We begin by presenting recall, precision and F-measure graphs when selecting from the alignedsource sentences, comprising the key and auxiliarysentences.
Figure 3 shows the results for the twomodels against the three baselines.
The two mod-els, the positional, and the tf-idf baselines performbetter than the random baseline, as measured by atwo-tailed Wilcoxon Matched Pairs Signed Rankstest (?
= 0.05).The WCM consistently out-performs the BWMon all metrics, and the differences are statisticallysignificant.
In fact, the BWM also generally per-forms worse than the position and tf-idf baselines.WCM and the position baseline both significantlyoutperform the tf-idf baseline on all metrics forlonger sentence lengths.That the position baseline and WCM should per-form similarly is not really surprising since, in ef-fect, the position baseline first chooses words fromthe key sentence and then selects auxiliary words.The difference essentially lies in how the auxiliarywords are chosen.54900.050.10.150.20.250.30.350.40  5  10  15  20  25  30RecallNumber of Open-class Words SelectedWCMBWMPositiontf.idfRandom00.050.10.150.20.250.30.350.40  5  10  15  20  25  30PrecisionNumber of Open-class Words SelectedWCMBWMPositiontf.idfRandom00.050.10.150.20.250.30  5  10  15  20  25  30FmeasureNumber of Open-class Words SelectedWCMBWMPositiontf.idfRandomFigure 3: Recall, Precision and F-measure performancefor open-class words from the entire input set (key andauxiliary).
Models presented are the Buzzword Model(BWM), the Word-Pair Co-occurrence Model (WCM)and position, tf-idf and random baselines.00.050.10.150.20.250  5  10  15  20  25  30FmeasureNumber of Open-class Words SelectedWCMpositionFigure 4: F-measure scores for content selection on justthe auxiliary sentences.
Models presented are the Word-Pair Co-occurrence model (WCM) and the position base-line.The results of Figure 3 weakly support thehypothesis that using schematic word-pair co-occurrences helps improve performance over mod-els without discourse-related features.
The graphsshow that WCM edges above the position base-line when the number of selected open-class wordsranges from 10 to 15.
Note that the average num-ber of open-class words in a human authored sum-mary sentence is 16.
The only significant differencefound was in the F-measure and precision scores for19 selected open-class words.
Nevertheless, a gen-eral trend can be observed in which WCM performsbetter than the position baseline.Ultimately, however, what we want to do is selectauxiliary content to supplement the key sentence.To examine the effect of two best performing ap-proaches, WCM and the position baseline, on thistask, were both modified so that the key sentencewords were explicitly given a zero probability.
Thus,the recall, precision and F-measure scores obtainedare based solely on the ability of either to select aux-iliary words.
The F-measure scores are presentedFigure 4.
WCM consistently outperforms the po-sition baseline for the selection of auxiliary words.Differences are significant for 6 or more selectedopen-class words.The results show that even when considering onlyexact token matches, we can improve on the re-call of open-class words, and do so without penaltyin precision.
Our working hypothesis is that suchgains are possible because the corpus has a homo-550geneous quality and key patterns are sufficiently re-peated even when the overall data set is of the or-der of hundreds of cases.
The benefit of using amodel encoding some schematic information is fur-ther shown by the performance of WCM over theposition baseline when selecting words from auxil-iary sentences.This is an interesting finding given that do-main independent methods are increasingly usedon domain-specific corpora such as financial andbiomedical texts, for which we may have access toonly a limited amount of data.
We anticipate that aswe introduce methods to account for paraphrase andsynonym differences, performance might rise fur-ther still.5.4 Testing Seed Weighting SchemesWe can also weight seed words in the ?Seed andGrow?
approach in a variety of ways.
To testwhether weighting schemes have any effect on con-tent selection performance, we examined the useof three schemes.
We were particularly interestedin those schemes that indicate the contribution ofa seed word to the core meaning of a sentence.These are the binary, tf-idf and buzzword weight-ing schemes described in Section 3.
We presentthe F-measure graph for these three variants of theschematic word-pair co-occurrence model (WCM)in Figure 5.The graphs show that there is no discernible dif-ference between the seed weighting schemes.
Noscheme significantly outperforms another.
Thus, weconclude that the choice of these particular seedweighting schemes has no effect on performance.
Infuture work, we intend to examine whether weight-ing schemes encoding syntactic information mightfare better, since such information might more accu-rately represent the contribution of a substring to themain clause of the sentence.6 Conclusions and Future WorkIn this paper, we argued a case for sentence augmen-tation, a component that facilitates abstract-like textsummarisation.
We showed that such a process canaccount for summary sentences as authored by pro-fessional editors.
We proposed the use of schemata,as approximated with a word-pair co-occurrence00.050.10.150.20.250.30  5  10  15  20  25  30FmeasureNumber of Open-class Words SelectedbinarytfidfBWMFigure 5: F-measure performance for open-class wordsfrom the entire input set (key and auxiliary).
Modelspresented are variants of the Word-Pair Co-occurrenceModel (WCM) that differ in the seed weighting schemes.model, and advocated a new schema-based ?Seedand Grow?
content selection model used for statisti-cal sentence generation.We also showed that domain-specific patterns,schematic word-pair co-occurrences in this case, canbe acquired from a limited amount of data as indi-cated by modest performance gains for content se-lection using schemata information.
We postulatethat this is particularly true when dealing with ho-mogeneous data.In future work, we intend to explore other stringmatches corresponding to variations due to para-phrases and synonymy.
We would also like to studythe effects of corpus size when learning schematicpatterns.
Finally, we are currently investigating theuse of machine learning methods to combine thebest of the Salience and Schemata models in orderto provide a single model for use in decoding.7 AcknowledgmentsWe would like to thank the reviewers for their in-sightful comments.
This work was funded by theCSIRO ICT Centre and Centre for Language Tech-nology at Macquarie University.ReferencesRegina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Daniel Marcu Su-san Dumais and Salim Roukos, editors, HLT-NAACL2004: Main Proceedings, pages 113?120, Boston,551Massachusetts, USA, May 2 - May 7.
Association forComputational Linguistics.Regina Barzilay and Kathleen R. McKeown.
2005.
Sen-tence fusion for multidocument news summarization.Computational Linguistics, 31(3):297?328.Adam L. Berger, Stephen Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22(1):39?71.James Clarke and Mirella Lapata.
2007.
Modelling com-pression with discourse constraints.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages1?11.Hal Daume?
III and Daniel Marcu.
2002.
A noisy-channelmodel for document compression.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics (ACL ?
2002), pages 449 ?
456,Philadelphia, PA, July 6 ?
12.Hal Daume?
III and Daniel Marcu.
2005.
Inductionof word and phrase alignments for automatic doc-ument summarization.
Computational Linguistics,31(4):505?530, December.Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.V.
Hatzivassiloglou, J. Klavans, M. Holcombe, R. Barzi-lay, M. Kan, and K. McKeown.
2001.
Simfinder: Aflexible clustering tool for summarization.
pages 41?49.
Association for Computational Linguistics.Hongyan Jing and Kathleen McKeown.
1999.
The de-composition of human-written summary sentences.
InResearch and Development in Information Retrieval,pages 129?136.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: a probabilistic ap-proach to sentence compression.
Artificial Intelli-gence, 139(1):91?107.Mirella Lapata.
2003.
Probabilistic text structuring: Ex-periments with sentence ordering.
In Proceedings ofthe 41st Annual Meeting of the Association for Compu-tational Linguistics, pages 545?552, Sapporo, Japan.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic evalu-ation of summaries using n-gram co-occurrence statis-tics.
In NAACL ?03: Proceedings of the 2003 Confer-ence of the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology, pages 71?78, Morristown, NJ, USA.
As-sociation for Computational Linguistics.W.
C. Mann and S. A. Thompson.
1988.
Rhetoricalstructure theory: Toward a functional theory of textorganization.
Text, 8(3):243?281.Kathleen R McKeown.
1985.
Text Generation: UsingDiscourse Strategies and Focus Constraints to Gen-erate Natural Language Text.
Cambridge UniversityPress.Jane Morris and Graeme Hirst.
1991.
Lexical cohe-sion computed by thesaural relations as an indicatorof the structure of text.
Computational Linguistics,17(1):21?48.G.
Salton and M. J. McGill.
1983.
Introduction to mod-ern information retrieval.
McGraw-Hill, New York.Radu Soricut and Daniel Marcu.
2005.
Towards de-veloping generation algorithms for text-to-text appli-cations.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL?05), pages 66?74, Ann Arbor, Michigan, June.Association for Computational Linguistics.Stephen Wan and Ce?cile Paris.
2008.
In-browser sum-marisation: Generating elaborative summaries biasedtowards the reading context.
In Proceedings of ACL-08: HLT, Short Papers, pages 129?132, Columbus,Ohio, June.
Association for Computational Linguis-tics.Stephen Wan, Robert Dale Mark Dras, and Ce?cile Paris.2005.
Towards statistical paraphrase generation: pre-liminary evaluations of grammaticality.
In Proceed-ings of The 3rd International Workshop on Paraphras-ing (IWP2005), pages 88?95, Jeju Island, South Korea.Michael J. Witbrock and Vibhu O. Mittal.
1999.
Ultra-summarization (poster abstract): a statistical approachto generating highly condensed non-extractive sum-maries.
In SIGIR ?99: Proceedings of the 22nd annualinternational ACM SIGIR conference on Research anddevelopment in information retrieval, pages 315?316,New York, NY, USA.
ACM Press.552
