Proceedings of the ACL Interactive Poster and Demonstration Sessions,pages 57?60, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSyntax-based Semi-Supervised Named Entity TaggingBehrang Mohit Rebecca HwaIntelligent Systems Program Computer Science DepartmentUniversity of Pittsburgh University of PittsburghPittsburgh, PA 15260 USA Pittsburgh, PA 15260, USAbehrang@cs.pitt.edu hwa@cs.pitt.eduAbstractWe report an empirical study on the roleof syntactic features in building a semi-supervised named entity (NE) tagger.Our study addresses two questions: Whattypes of syntactic features are suitable forextracting potential NEs to train a classi-fier in a semi-supervised setting?
Howgood is the resulting NE classifier on test-ing instances dissimilar from its trainingdata?
Our study shows that constituencyand dependency parsing constraints areboth suitable features to extract NEs andtrain the classifier.
Moreover, the classi-fier showed significant accuracy im-provement when constituency features arecombined with new dependency feature.Furthermore, the degradation in accuracyon unfamiliar test cases is low, suggestingthat the trained classifier generalizes well.1 IntroductionNamed entity (NE) tagging is the task of recogniz-ing and classifying phrases into one of many se-mantic classes such as persons, organizations andlocations.
Many successful NE tagging systemsrely on a supervised learning framework wheresystems use large annotated training resources(Bikel et.
al.
1999).
These resources may not al-ways be available for non-English domains.
Thispaper examines the practicality of developing asyntax-based semi-supervised NE tagger.
In ourstudy we compared the effects of two types of syn-tactic rules (constituency and dependency) in ex-tracting and classifying potential named entities.We train a Naive Bayes classification model on acombination of labeled and unlabeled exampleswith the Expectation Maximization (EM) algo-rithm.
We find that a significant improvement inclassification accuracy can be achieved when wecombine both dependency and constituency extrac-tion methods.
In our experiments, we evaluate thegeneralization (coverage) of this bootstrapping ap-proach under three testing schemas.
Each of theseschemas represented a certain level of test datacoverage (recall).
Although the system performsbest on (unseen) test data that is extracted by thesyntactic rules (i.e., similar syntactic structures asthe training examples), the performance degrada-tion is not high when the system is tested on moregeneral test cases.
Our experimental results suggestthat a semi-supervised NE tagger can be success-fully developed using syntax-rich features.2 Previous Works and Our ApproachSupervised NE Tagging has been studied exten-sively over the past decade (Bikel et al 1999,Baluja et.
al.
1999, Tjong Kim Sang and DeMeulder 2003).
Recently, there were increasinginterests in semi-supervised learning approaches.Most relevant to our study, Collins and Singer(1999) showed that a NE Classifier can be devel-oped by bootstrapping from a small amount of la-beled examples.
To extract potentially usefultraining examples, they first parsed the sentencesand looked for expressions that satisfy two con-stituency patterns (appositives and prepositionalphrases).
A small subset of these expressions wasthen manually labeled with their correct NE tags.The training examples were a combination of thelabeled and unlabeled data.
In their studies,57Collins and Singer compared several learningmodels using this style of semi-supervised training.Their results were encouraging, and their studiesraised additional questions.
First, are there otherappropriate syntactic extraction patterns in additionto appositives and prepositional phrases?
Second,because the test data were extracted in the samemanner as the training data in their experiments,the characteristics of the test cases were biased.
Inthis paper we examine the question of how well asemi-supervised system can classify arbitrarynamed entities.
In our empirical study, in additionto the constituency features proposed by Collinsand Singer, we introduce a new set of dependencyparse features to recognize and classify NEs.
Weevaluated the effects of these two sets of syntacticfeatures on the accuracy of the classification bothseparately and in a combined form (union of thetwo sets).Figure 1 represents a general overview of our sys-tem?s architecture which includes the followingtwo levels: NE Recognizer and NE Classifier.Section 3 and 4 describes these two levels in de-tails and section 5 covers the results of the evalua-tion of our system.Figure 1: System's architecture3 Named Entity RecognitionIn this level, the system used a group of syntax-based rules to recognize and extract potentialnamed entities from constituency and dependencyparse trees.
The rules are used to produce ourtraining data; therefore they needed to have a nar-row and precise coverage of each type of namedentities to minimize the level of training noise.The processing starts from construction of con-stituency and dependency parse trees from the in-put text.
Potential NEs are detected and extractedbased on these syntactic rules.3.1 Constituency Parse FeaturesReplicating the study performed by Collins-Singer(1999), we used two constituency parse rules toextract a set of proper nouns (along with their as-sociated contextual information).
These two con-stituency rules extracted proper nouns within anoun phrase that contained an appositive phraseand a proper noun within a prepositional phrase.3.2 Dependency Parse FeaturesWe observed that a proper noun acting as the sub-ject or the object of a sentence has a high probabil-ity of being a particular type of named entity.Thus, we expanded our syntactic analysis of thedata into dependency parse of the text and ex-tracted a set of proper nouns that act as the subjectsor objects of the main verb.
For each of the sub-jects and objects, we considered the maximumspan noun phrase that included the modifiers of thesubjects and objects in the dependency parse tree.4 Named Entity ClassificationIn this level, the system assigns one of the 4 classlabels (<PER>, <ORG>, <LOC>, <NONE>) to agiven test NE.
The NONE class is used for theexpressions mistakenly extracted by syntactic fea-tures that were not a NE.
We will discuss the formof the test NE in more details in section 5.
Theunderlying model we consider is a Na?ve Bayesclassifier; we train it with the Expectation-Maximization algorithm, an iterative parameterestimation procedure.4.1 FeaturesWe used the following syntactic and spelling fea-tures for the classification:Full NE Phrase.Individual word: This binary feature indicates thepresence of a certain word in the NE.58Punctuation pattern: The feature helps to distin-guish those NEs that hold certain patterns of punc-tuations like (?)
for U.S.A. or (&.)
for A&M.All Capitalization:  This binary feature is mainlyuseful for some of the NEs that have all capitalletters.
such as AP, AFP, CNN, etc.Constituency Parse Rule:  The feature indicateswhich of the two constituency rule is used for ex-tract the NE.Dependency Parse Rule:  The feature indicates ifthe NE is the subject or object of the sentence.Except for the last two features, all features arespelling features which are extracted from the ac-tual NE phrase.
The constituency and dependencyfeatures are extracted from the NE recognitionphase (section 3).
Depending on the type of testingand training schema, the NEs might have 0 valuefor the dependency or constituency features whichindicate the absence of the feature in the recogni-tion step.4.2 Na?ve Bayes ClassifierWe used a Na?ve Bayes classifier where each NEis represented by a set of syntactic and word-levelfeatures (with various distributions) as describedabove.
The individual words within the nounphrase are binary features.
These, along with otherfeatures with multinomial distributions, fit wellinto Na?ve Bayes assumption where each feature isdealt independently (given the class value).
In or-der to balance the effects of the large binary fea-tures on the final class probabilities, we used somenumerical methods techniques to transform someof the probabilities to the log-space.4.3 Semi-supervised learningSimilar to the work of Nigam et al (1999) ondocument classification, we used ExpectationMaximization (EM) algorithm along with our Na-?ve Bayes classifier to form a semi supervisedlearning framework.
In this framework, the smalllabeled dataset is used to do the initial assignmentsof the parameters for the Na?ve Bayes classifier.After this initialization step, in each iteration theNa?ve Bayes classifier classifies all of the unla-beled examples and updates its parameters basedon the class probability of the unlabeled and la-beled NE instances.
This iterative procedure con-tinues until the parameters reach a stable point.Subsequently the updated Na?ve Bayes classifiesthe test instances for evaluation.5 Empirical StudyOur study consists of a 9-way comparison that in-cludes the usage of three types of training featuresand three types of testing schema.5.1 DataWe used the data from the Automatic Content Ex-traction (ACE)?s entity detection track as our la-beled (gold standard) data.1For every NE that the syntactic rules extract fromthe input sentence, we had to find a matching NEfrom the gold standard data and label the extractedNE with the correct NE class label.
If the ex-tracted NE did not match any of the gold standardNEs (for the sentence), we labeled it with the<NONE> class label.We also used the WSJ portion of the Penn TreeBank as our unlabeled dataset and ran constituencyand dependency analyses2 to extract a set of unla-beled named entities for the semi-supervised clas-sification.5.2 EvaluationIn order to evaluate the effects of each group ofsyntactic features, we experimented with three dif-ferent training strategies (using constituency rules,dependency rules or combinations of both).
Weconducted the comparison study with three typesof test data that represent three levels of coverage(recall) for the system:1.
Gold Standard NEs:  This test set contains in-stances taken directly from the ACE data, and aretherefore independent of the syntactic rules.2.
Any single or series of proper nouns in the text:This is a heuristic for locating potential NEs so asto have the broadest coverage.3.
NEs extracted from text by the syntactic rules.This evaluation approach is similar to that of Col-lins and Singer.
The main difference is that wehave to match the extracted expressions to a pre-1 We only used the NE portion of the data and removed theinformation for other tracking and extraction tasks.2 We used the Collins parser (1997) to generate the constitu-ency parse and a dependency converter (Hwa and Lopez,2004) to obtain the dependency parse of English sentences.59labeled gold standard from ACE rather than per-forming manual annotations ourselves.All tests have been performed under a 5-fold crossvalidation training-testing setup.
Table 1 presentsthe accuracy of the NE classification and the sizeof labeled data in the different training-testing con-figurations.
The second line of each cell shows thesize of labeled training data and the third lineshows the size of testing data.
Each column pre-sents the result for one type of the syntactic fea-tures that were used to extract NEs.
Each row ofthe table presents one of the three testing schema.We tested the statistical significance of each of thecross-row accuracy improvements against an alphavalue of 0.1 and observed significant improvementin all of the testing schemas.Training Features Testing Data Const.
Dep.
UnionGold Standard NEs(ACE Data)76.7%66857978.5%88457982.4%1427579All Proper Nouns70.2%66887271.4%88487276.1%1427872NEs Extracted byTraining Rules78.2%66816980.3%88421785.1%1427354Table 1: Classification Accuracy, labeled training &testing data sizeOur results suggest that dependency parsing fea-tures are reasonable extraction patterns, as theiraccuracy rates are competitive against the modelbased solely on constituency rules.
Moreover, theymake a good complement to the constituency rulesproposed by Collins and Singer, since the accuracyrates of the union is higher than either model alone.As expected, all methods perform the best whenthe test data are extracted in the same manner asthe training examples.
However, if the systemswere given a well-formed named entity, the per-formance degradation is reasonably small, about2% absolute difference for all training methods.The performance is somewhat lower when classi-fying very general test cases of all proper nouns.6 Conclusion and Future WorkIn this paper, we experimented with different syn-tactic extraction patterns and different NE recogni-tion constraints.
We find that semi-supervisedmethods are compatible with both constituency anddependency extraction rules.
We also find that theresulting classifier is reasonably robust on testcases that are different from its training examples.An area that might benefit from a semi-supervisedNE tagger is machine translation.
The semi-supervised approach is suitable for non-Englishlanguages that do not have very much annotatedNE data.
We are currently applying our system toArabic.
The robustness of the syntactic-based ap-proach has allowed us to port the system to thenew language with minor changes in our syntacticrules and classification features.AcknowledgementWe would like to thank the NLP group at Pitt andthe anonymous reviewers for their valuable com-ments and suggestions.ReferencesShumeet Baluja, Vibhu Mittal and Rahul Sukthankar,1999.
Applying machine learning for high perform-ance named-entity extraction.
In Proceedings of Pa-cific Association for Computational Linguistics.Daniel Bikel, Robert Schwartz & Ralph Weischedel,1999.
An algorithm that learns what?s in a name.Machine Learning 34.Michael Collins, 1997.
Three generative lexicalizedmodels for statistical parsing.
In Proceedings of the35th Annual Meeting of the ACL.Michael Collins, and Yoram Singer, 1999.
Unsuper-vised Classification of Named Entities.
In Proceed-ings of SIGDAT.A.
P. Dempster, N. M. Laird and D. B. Rubin, 1977.Maximum Likelihood from incomplete data via theEM algorithm.
Journal of Royal Statistical Society,Series B, 39(1), 1-38.Rebecca Hwa and Adam Lopez, 2004.
On the Conver-sion of Constituent Parsers to Dependency Parsers.Technical Report TR-04-118, Department of Com-puter Science, University of Pittsburgh.Kamal Nigam, Andrew McCallum, Sebastian Thrun andTom Mitchell, 2000.
Text Classification from La-beled and Unlabeled Documents using EM.
MachineLearning 39(2/3).Erik F. Tjong Kim Sang and Fien De Meulder, 2003.Introduction to the CoNLL-2003 Shared Task: Lan-guage-Independent Named Entity Recognition.
InProceedings of CoNLL-2003.60
