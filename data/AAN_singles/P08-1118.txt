Proceedings of ACL-08: HLT, pages 1039?1047,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsFinding Contradictions in TextMarie-Catherine de Marneffe,Linguistics DepartmentStanford UniversityStanford, CA 94305mcdm@stanford.eduAnna N. Rafferty and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305{rafferty,manning}@stanford.eduAbstractDetecting conflicting statements is a foun-dational text understanding task with appli-cations in information analysis.
We pro-pose an appropriate definition of contradictionfor NLP tasks and develop available corpora,from which we construct a typology of con-tradictions.
We demonstrate that a system forcontradiction needs to make more fine-graineddistinctions than the common systems for en-tailment.
In particular, we argue for the cen-trality of event coreference and therefore in-corporate such a component based on topical-ity.
We present the first detailed breakdownof performance on this task.
Detecting sometypes of contradiction requires deeper inferen-tial paths than our system is capable of, butwe achieve good performance on types arisingfrom negation and antonymy.1 IntroductionIn this paper, we seek to understand the ways con-tradictions occur across texts and describe a systemfor automatically detecting such constructions.
As afoundational task in text understanding (Condoravdiet al, 2003), contradiction detection has many possi-ble applications.
Consider applying a contradictiondetection system to political candidate debates: bydrawing attention to topics in which candidates haveconflicting positions, the system could enable votersto make more informed choices between candidatesand sift through the amount of available informa-tion.
Contradiction detection could also be appliedto intelligence reports, demonstrating which infor-mation may need further verification.
In bioinfor-matics where protein-protein interaction is widelystudied, automatically finding conflicting facts aboutsuch interactions would be beneficial.Here, we shed light on the complex picture of con-tradiction in text.
We provide a definition of contra-diction suitable for NLP tasks, as well as a collec-tion of contradiction corpora.
Analyzing these data,we find contradiction is a rare phenomenon that maybe created in different ways; we propose a typol-ogy of contradiction classes and tabulate their fre-quencies.
Contradictions arise from relatively obvi-ous features such as antonymy, negation, or numericmismatches.
They also arise from complex differ-ences in the structure of assertions, discrepanciesbased on world-knowledge, and lexical contrasts.
(1) Police specializing in explosives defused the rock-ets.
Some 100 people were working inside the plant.
(2) 100 people were injured.This pair is contradictory: defused rockets cannot gooff, and thus cannot injure anyone.
Detecting con-tradictions appears to be a harder task than detectingentailments.
Here, it is relatively easy to identify thelack of entailment: the first sentence involves no in-juries, so the second is unlikely to be entailed.
Mostentailment systems function as weak proof theory(Hickl et al, 2006; MacCartney et al, 2006; Zan-zotto et al, 2007), but contradictions require deeperinferences and model building.
While mismatch-ing information between sentences is often a goodcue of non-entailment (Vanderwende et al, 2006),it is not sufficient for contradiction detection whichrequires more precise comprehension of the conse-quences of sentences.
Assessing event coreferenceis also essential: for texts to contradict, they must1039refer to the same event.
The importance of eventcoreference was recognized in the MUC informationextraction tasks in which it was key to identify sce-narios related to the same event (Humphreys et al,1997).
Recent work in text understanding has notfocused on this issue, but it must be tackled in a suc-cessful contradiction system.
Our system includesevent coreference, and we present the first detailedexamination of contradiction detection performance,on the basis of our typology.2 Related workLittle work has been done on contradiction detec-tion.
The PASCAL Recognizing Textual Entailment(RTE) Challenges (Dagan et al, 2006; Bar-Haimet al, 2006; Giampiccolo et al, 2007) focused ontextual inference in any domain.
Condoravdi et al(2003) first recognized the importance of handlingentailment and contradiction for text understanding,but they rely on a strict logical definition of thesephenomena and do not report empirical results.
Toour knowledge, Harabagiu et al (2006) provide thefirst empirical results for contradiction detection, butthey focus on specific kinds of contradiction: thosefeaturing negation and those formed by paraphrases.They constructed two corpora for evaluating theirsystem.
One was created by overtly negating eachentailment in the RTE2 data, producing a bal-anced dataset (LCC negation).
To avoid overtrain-ing, negative markers were also added to each non-entailment, ensuring that they did not create con-tradictions.
The other was produced by paraphras-ing the hypothesis sentences from LCC negation, re-moving the negation (LCC paraphrase): A hungerstrike was not attempted ?
A hunger strike wascalled off.
They achieved very good performance:accuracies of 75.63% on LCC negation and 62.55%on LCC paraphrase.
Yet, contradictions are not lim-ited to these constructions; to be practically useful,any system must provide broader coverage.3 Contradictions3.1 What is a contradiction?One standard is to adopt a strict logical definition ofcontradiction: sentences A and B are contradictoryif there is no possible world in which A and B areboth true.
However, for contradiction detection to beuseful, a looser definition that more closely matcheshuman intuitions is necessary; contradiction occurswhen two sentences are extremely unlikely to be truesimultaneously.
Pairs such as Sally sold a boat toJohn and John sold a boat to Sally are tagged as con-tradictory even though it could be that each sold aboat to the other.
This definition captures intuitionsof incompatiblity, and perfectly fits applications thatseek to highlight discrepancies in descriptions of thesame event.
Examples of contradiction are given intable 1.
For texts to be contradictory, they must in-volve the same event.
Two phenomena must be con-sidered in this determination: implied coreferenceand embedded texts.
Given limited context, whethertwo entities are coreferent may be probable ratherthan certain.
To match human intuitions, compatiblenoun phrases between sentences are assumed to becoreferent in the absence of clear countervailing ev-idence.
In the following example, it is not necessarythat the woman in the first and second sentences isthe same, but one would likely assume it is if the twosentences appeared together:(1) Passions surrounding Germany?s final match turnedviolent when a woman stabbed her partner becauseshe didn?t want to watch the game.
(2) A woman passionately wanted to watch the game.We also mark as contradictions pairs reporting con-tradictory statements.
The following sentences referto the same event (de Menezes in a subway station),and display incompatible views of this event:(1) Eyewitnesses said de Menezes had jumped over theturnstile at Stockwell subway station.
(2) The documents leaked to ITV News suggest thatMenezes walked casually into the subway station.This example contains an ?embedded contradic-tion.?
Contrary to Zaenen et al (2005), we arguethat recognizing embedded contradictions is impor-tant for the application of a contradiction detectionsystem: if John thinks that he is incompetent, and hisboss believes that John is not being given a chance,one would like to detect that the targeted informationin the two sentences is contradictory, even thoughthe two sentences can be true simultaneously.3.2 Typology of contradictionsContradictions may arise from a number of differentconstructions, some overt and others that are com-1040ID Type Text Hypothesis1 Antonym Capital punishment is a catalyst for more crime.
Capital punishment is a deterrent tocrime.2 Negation A closely divided Supreme Court said that juries andnot judges must impose a death sentence.The Supreme Court decided that onlyjudges can impose the death sentence.3 Numeric The tragedy of the explosion in Qana that killed morethan 50 civilians has presented Israel with a dilemma.An investigation into the strike in Qanafound 28 confirmed dead thus far.4 Factive Prime Minister John Howard says he will not beswayed by a warning that Australia faces more terror-ism attacks unless it withdraws its troops from Iraq.Australia withdraws from Iraq.5 Factive The bombers had not managed to enter the embassy.
The bombers entered the embassy.6 Structure Jacques Santer succeeded Jacques Delors as presidentof the European Commission in 1995.Delors succeeded Santer in the presi-dency of the European Commission.7 Structure The Channel Tunnel stretches from England toFrance.
It is the second-longest rail tunnel in theworld, the longest being a tunnel in Japan.The Channel Tunnel connects Franceand Japan.8 Lexical The Canadian parliament?s Ethics Commission saidformer immigration minister, Judy Sgro, did nothingwrong and her staff had put her into a conflict of in-terest.The Canadian parliament?s EthicsCommission accuses Judy Sgro.9 Lexical In the election, Bush called for U.S. troops to be with-drawn from the peacekeeping mission in the Balkans.He cites such missions as an example ofhow America must ?stay the course.
?10 WK Microsoft Israel, one of the first Microsoft branchesoutside the USA, was founded in 1989.Microsoft was established in 1989.Table 1: Examples of contradiction types.plex even for humans to detect.
Analyzing contra-diction corpora (see section 3.3), we find two pri-mary categories of contradiction: (1) those occur-ring via antonymy, negation, and date/number mis-match, which are relatively simple to detect, and(2) contradictions arising from the use of factive ormodal words, structural and subtle lexical contrasts,as well as world knowledge (WK).We consider contradictions in category (1) ?easy?because they can often be automatically detectedwithout full sentence comprehension.
For exam-ple, if words in the two passages are antonyms andthe sentences are reasonably similar, especially inpolarity, a contradiction occurs.
Additionally, littleexternal information is needed to gain broad cover-age of antonymy, negation, and numeric mismatchcontradictions; each involves only a closed set ofwords or data that can be obtained using existingresources and techniques (e.g., WordNet (Fellbaum,1998), VerbOcean (Chklovski and Pantel, 2004)).However, contradictions in category (2) are moredifficult to detect automatically because they requireprecise models of sentence meaning.
For instance,to find the contradiction in example 8 (table 1),it is necessary to learn that X said Y did nothingwrong and X accuses Y are incompatible.
Presently,there exist methods for learning oppositional terms(Marcu and Echihabi, 2002) and paraphrase learn-ing has been thoroughly studied, but successfullyextending these techniques to learn incompatiblephrases poses difficulties because of the data dis-tribution.
Example 9 provides an even more dif-ficult instance of contradiction created by a lexicaldiscrepancy.
Structural issues also create contradic-tions (examples 6 and 7).
Lexical complexities andvariations in the function of arguments across verbscan make recognizing these contradictions compli-cated.
Even when similar verbs are used and ar-gument differences exist, structural differences mayindicate non-entailment or contradiction, and distin-guishing the two automatically is problematic.
Con-sider contradiction 7 in table 1 and the followingnon-contradiction:(1) The CFAP purchases food stamps from the govern-ment and distributes them to eligible recipients.
(2) A government purchases food.1041Data # contradictions # total pairsRTE1 dev1 48 287RTE1 dev2 55 280RTE1 test 149 800RTE2 dev 111 800RTE3 dev 80 800RTE3 test 72 800Table 2: Number of contradictions in the RTE datasets.In both cases, the first sentence discusses one en-tity (CFAP, The Channel Tunnel) with a relationship(purchase, stretch) to other entities.
The second sen-tence posits a similar relationship that includes oneof the entities involved in the original relationshipas well as an entity that was not involved.
However,different outcomes result because a tunnel connectsonly two unique locations whereas more than oneentity may purchase food.
These frequent interac-tions between world-knowledge and structure makeit hard to ensure that any particular instance of struc-tural mismatch is a contradiction.3.3 Contradiction corporaFollowing the guidelines above, we annotated theRTE datasets for contradiction.
These datasets con-tain pairs consisting of a short text and a one-sentence hypothesis.
Table 2 gives the number ofcontradictions in each dataset.
The RTE datasets arebalanced between entailments and non-entailments,and even in these datasets targeting inference, thereare few contradictions.
Using our guidelines,RTE3 test was annotated by NIST as part of theRTE3 Pilot task in which systems made a 3-way de-cision as to whether pairs of sentences were entailed,contradictory, or neither (Voorhees, 2008).1Our annotations and those of NIST were per-formed on the original RTE datasets, contrary toHarabagiu et al (2006).
Because their corpora areconstructed using negation and paraphrase, they areunlikely to cover all types of contradictions in sec-tion 3.2.
We might hypothesize that rewriting ex-plicit negations commonly occurs via the substitu-tion of antonyms.
Imagine, e.g.
:H: Bill has finished his math.1Information about this task as well as data can be found athttp://nlp.stanford.edu/RTE3-pilot/.Type RTE sets ?Real?
corpus1 Antonym 15.0 9.2Negation 8.8 17.6Numeric 8.8 29.02 Factive/Modal 5.0 6.9Structure 16.3 3.1Lexical 18.8 21.4WK 27.5 13.0Table 3: Percentages of contradiction types in theRTE3 dev dataset and the real contradiction corpus.Neg-H: Bill hasn?t finished his math.Para-Neg-H: Bill is still working on his math.The rewriting in both the negated and the para-phrased corpora is likely to leave one in the space of?easy?
contradictions and addresses fewer than 30%of contradictions (table 3).
We contacted the LCCauthors to obtain their datasets, but they were unableto make them available to us.
Thus, we simulated theLCC negation corpus, adding negative markers tothe RTE2 test data (Neg test), and to a developmentset (Neg dev) constructed by randomly sampling 50pairs of entailments and 50 pairs of non-entailmentsfrom the RTE2 development set.Since the RTE datasets were constructed for tex-tual inference, these corpora do not reflect ?real-life?contradictions.
We therefore collected contradic-tions ?in the wild.?
The resulting corpus contains131 contradictory pairs: 19 from newswire, mainlylooking at related articles in Google News, 51 fromWikipedia, 10 from the Lexis Nexis database, and51 from the data prepared by LDC for the distillationtask of the DARPA GALE program.
Despite the ran-domness of the collection, we argue that this corpusbest reflects naturally occurring contradictions.2Table 3 gives the distribution of contradictiontypes for RTE3 dev and the real contradiction cor-pus.
Globally, we see that contradictions in category(2) occur frequently and dominate the RTE develop-ment set.
In the real contradiction corpus, there is amuch higher rate of the negation, numeric and lex-ical contradictions.
This supports the intuition thatin the real world, contradictions primarily occur fortwo reasons: information is updated as knowledge2Our corpora?the simulation of the LLC negation corpus,the RTE datasets and the real contradictions?are available athttp://nlp.stanford.edu/projects/contradiction.1042of an event is acquired over time (e.g., a rising deathtoll) or various parties have divergent views of anevent (e.g., example 9 in table 1).4 System overviewOur system is based on the stage architecture of theStanford RTE system (MacCartney et al, 2006), butadds a stage for event coreference decision.4.1 Linguistic analysisThe first stage computes linguistic representationscontaining information about the semantic contentof the passages.
The text and hypothesis are con-verted to typed dependency graphs produced bythe Stanford parser (Klein and Manning, 2003; deMarneffe et al, 2006).
To improve the dependencygraph as a pseudo-semantic representation, colloca-tions in WordNet and named entities are collapsed,causing entities and multiword relations to becomesingle nodes.4.2 Alignment between graphsThe second stage provides an alignment betweentext and hypothesis graphs, consisting of a mappingfrom each node in the hypothesis to a unique nodein the text or to null.
The scoring measure usesnode similarity (irrespective of polarity) and struc-tural information based on the dependency graphs.Similarity measures and structural information arecombined via weights learned using the passive-aggressive online learning algorithm MIRA (Cram-mer and Singer, 2001).
Alignment weights werelearned using manually annotated RTE developmentsets (see Chambers et al, 2007).4.3 Filtering non-coreferent eventsContradiction features are extracted based on mis-matches between the text and hypothesis.
Therefore,we must first remove pairs of sentences which do notdescribe the same event, and thus cannot be contra-dictory to one another.
In the following example, itis necessary to recognize that Pluto?s moon is not thesame as the moon Titan; otherwise conflicting diam-eters result in labeling the pair a contradiction.T: Pluto?s moon, which is only about 25 miles in di-ameter, was photographed 13 years ago.H: The moon Titan has a diameter of 5100 kms.This issue does not arise for textual entailment: el-ements in the hypothesis not supported by the textlead to non-entailment, regardless of whether thesame event is described.
For contradiction, however,it is critical to filter unrelated sentences to avoidfinding false evidence of contradiction when thereis contrasting information about different events.Given the structure of RTE data, in which thehypotheses are shorter and simpler than the texts,one straightforward strategy for detecting coreferentevents is to check whether the root of the hypothesisgraph is aligned in the text graph.
However, someRTE hypotheses are testing systems?
abilities to de-tect relations between entities (e.g., John of IBM .
.
.?
John works for IBM).
Thus, we do not filter verbroots that are indicative of such relations.
As shownin table 4, this strategy improves results on RTEdata.
For real world data, however, the assumptionof directionality made in this strategy is unfounded,and we cannot assume that one sentence will beshort and the other more complex.
Assuming twosentences of comparable complexity, we hypothe-size that modeling topicality could be used to assesswhether the sentences describe the same event.There is a continuum of topicality from the start tothe end of a sentence (Firbas, 1971).
We thus orig-inally defined the topicality of an NP by nw wheren is the nth NP in the sentence.
Additionally, weaccounted for multiple clauses by weighting eachclause equally; in example 4 in table 1, Australiareceives the same weight as Prime Minister becauseeach begins a clause.
However, this weighting wasnot supported empirically, and we thus use a sim-pler, unweighted model.
The topicality score of asentence is calculated as a normalized score acrossall aligned NPs.3 The text and hypothesis are topi-cally related if either sentence score is above a tunedthreshold.
Modeling topicality provides an addi-tional improvement in precision (table 4).While filtering provides improvements in perfor-mance, some examples of non-coreferent events arestill not filtered, such as:T: Also Friday, five Iraqi soldiers were killed and nine3Since dates can often be viewed as scene setting rather thanwhat the sentence is about, we ignore these in the model.
How-ever, ignoring or including dates in the model creates no signif-icant differences in performance on RTE data.1043Strategy Precision RecallNo filter 55.10 32.93Root 61.36 32.93Root + topic 61.90 31.71Table 4: Precision and recall for contradiction detectionon RTE3 dev using different filtering strategies.wounded in a bombing, targeting their convoy nearBeiji, 150 miles north of Baghdad.H: Three Iraqi soldiers also died Saturday when theirconvoy was attacked by gunmen near Adhaim.It seems that the real world frequency of eventsneeds to be taken into account.
In this case, attacksin Iraq are unfortunately frequent enough to assertthat it is unlikely that the two sentences present mis-matching information (i.e., different location) aboutthe same event.
But compare the following example:T: President Kennedy was assassinated in Texas.H: Kennedy?s murder occurred in Washington.The two sentences refer to one unique event, and thelocation mismatch renders them contradictory.4.4 Extraction of contradiction featuresIn the final stage, we extract contradiction featureson which we apply logistic regression to classify thepair as contradictory or not.
The feature weights arehand-set, guided by linguistic intuition.5 Features for contradiction detectionIn this section, we define each of the feature setsused to capture salient patterns of contradiction.Polarity features.
Polarity difference between thetext and hypothesis is often a good indicator of con-tradiction, provided there is a good alignment (seeexample 2 in table 1).
The polarity features cap-ture the presence (or absence) of linguistic mark-ers of negative polarity contexts.
These markers arescoped such that words are considered negated ifthey have a negation dependency in the graph or arean explicit linguistic marker of negation (e.g., sim-ple negation (not), downward-monotone quantifiers(no, few), or restricting prepositions).
If one word isnegated and the other is not, we may have a polaritydifference.
This difference is confirmed by checkingthat the words are not antonyms and that they lackunaligned prepositions or other context that suggeststhey do not refer to the same thing.
In some cases,negations are propagated onto the governor, whichallows one to see that no bullet penetrated and a bul-let did not penetrate have the same polarity.Number, date and time features.
Numeric mis-matches can indicate contradiction (example 3in table 1).
The numeric features recognize(mis-)matches between numbers, dates, and times.We normalize date and time expressions, and rep-resent numbers as ranges.
This includes expressionmatching (e.g., over 100 and 200 is not a mismatch).Aligned numbers are marked as mismatches whenthey are incompatible and surrounding words matchwell, indicating the numbers refer to the same entity.Antonymy features.
Aligned antonyms are a verygood cue for contradiction.
Our list of antonymsand contrasting words comes from WordNet, fromwhich we extract words with direct antonymy linksand expand the list by adding words from the samesynset as the antonyms.
We also use oppositionalverbs from VerbOcean.
We check whether analigned pair of words appears in the list, as well aschecking for common antonym prefixes (e.g., anti,un).
The polarity of the context is used to determineif the antonyms create a contradiction.Structural features.
These features aim to deter-mine whether the syntactic structures of the text andhypothesis create contradictory statements.
For ex-ample, we compare the subjects and objects for eachaligned verb.
If the subject in the text overlaps withthe object in the hypothesis, we find evidence for acontradiction.
Consider example 6 in table 1.
In thetext, the subject of succeed is Jacques Santer whilein the hypothesis, Santer is the object of succeed,suggesting that the two sentences are incompatible.Factivity features.
The context in which a verbphrase is embedded may give rise to contradiction,as in example 5 (table 1).
Negation influences somefactivity patterns: Bill forgot to take his wallet con-tradicts Bill took his wallet while Bill did not forgetto take his wallet does not contradict Bill took hiswallet.
For each text/hypothesis pair, we check the(grand)parent of the text word aligned to the hypoth-esis verb, and generate a feature based on its factiv-1044ity class.
Factivity classes are formed by clusteringour expansion of the PARC lists of factive, implica-tive and non-factive verbs (Nairn et al, 2006) ac-cording to how they create contradiction.Modality features.
Simple patterns of modal rea-soning are captured by mapping the text and hy-pothesis to one of six modalities ((not )possible,(not )actual, (not )necessary), according to thepresence of predefined modality markers such ascan or maybe.
A feature is produced if thetext/hypothesis modality pair gives rise to a con-tradiction.
For instance, the following pair willbe mapped to the contradiction judgment (possible,not possible):T: The trial court may allow the prevailing party rea-sonable attorney fees as part of costs.H: The prevailing party may not recover attorney fees.Relational features.
A large proportion of theRTE data is derived from information extractiontasks where the hypothesis captures a relation be-tween elements in the text.
Using Semgrex, a pat-tern matching language for dependency graphs, wefind such relations and ensure that the arguments be-tween the text and the hypothesis match.
In the fol-lowing example, we detect that Fernandez works forFEMA, and that because of the negation, a contra-diction arises.T: Fernandez, of FEMA, was on scene when Martinarrived at a FEMA base camp.H: Fernandez doesn?t work for FEMA.Relational features provide accurate information butare difficult to extend for broad coverage.6 ResultsOur contradiction detection system was developedon all datasets listed in the first part of table 5.
Astest sets, we used RTE1 test, the independently an-notated RTE3 test, and Neg test.
We focused on at-taining high precision.
In a real world setting, it islikely that the contradiction rate is extremely low;rather than overwhelming true positives with falsepositives, rendering the system impractical, we markcontradictions conservatively.
We found reasonableinter-annotator agreement between NIST and ourpost-hoc annotation of RTE3 test (?
= 0.81), show-ing that, even with limited context, humans tend toPrecision Recall AccuracyRTE1 dev1 70.37 40.43 ?RTE1 dev2 72.41 38.18 ?RTE2 dev 64.00 28.83 ?RTE3 dev 61.90 31.71 ?Neg dev 74.07 78.43 75.49Neg test 62.97 62.50 62.74LCC negation ?
?
75.63RTE1 test 42.22 26.21 ?RTE3 test 22.95 19.44 ?Avg.
RTE3 test 10.72 11.69 ?Table 5: Precision and recall figures for contradiction de-tection.
Accuracy is given for balanced datasets only.
?LCC negation?
refers to performance of Harabagiu et al(2006); ?Avg.
RTE3 test?
refers to mean performance ofthe 12 submissions to the RTE3 Pilot.agree on contradictions.4 The results on the test setsshow that performance drops on new data, highlight-ing the difficulty in generalizing from a small corpusof positive contradiction examples, as well as under-lining the complexity of building a broad coveragesystem.
This drop in accuracy on the test sets isgreater than that of many RTE systems, suggestingthat generalizing for contradiction is more difficultthan for entailment.
Particularly when addressingcontradictions that require lexical and world knowl-edge, we are only able to add coverage in a piece-meal fashion, resulting in improved performance onthe development sets but only small gains for thetest sets.
Thus, as shown in table 6, we achieve13.3% recall on lexical contradictions in RTE3 devbut are unable to identify any such contradictions inRTE3 test.
Additionally, we found that the preci-sion of category (2) features was less than that ofcategory (1) features.
Structural features, for exam-ple, caused us to tag 36 non-contradictions as con-tradictions in RTE3 test, over 75% of the precisionerrors.
Despite these issues, we achieve much higherprecision and recall than the average submission tothe RTE3 Pilot task on detecting contradictions, asshown in the last two lines of table 5.4This stands in contrast with the low inter-annotator agree-ment reported by Sanchez-Graillet and Poesio (2007) for con-tradictions in protein-protein interactions.
The only hypothesiswe have to explain this contrast is the difficulty of scientific ma-terial.1045Type RTE3 dev RTE3 test1 Antonym 25.0 (3/12) 42.9 (3/7)Negation 71.4 (5/7) 60.0 (3/5)Numeric 71.4 (5/7) 28.6 (2/7)2 Factive/Modal 25.0 (1/4) 10.0 (1/10)Structure 46.2 (6/13) 21.1 (4/19)Lexical 13.3 (2/15) 0.0 (0/12)WK 18.2 (4/22) 8.3 (1/12)Table 6: Recall by contradiction type.7 Error analysis and discussionOne significant issue in contradiction detection islack of feature generalization.
This problem is es-pecially apparent for items in category (2) requiringlexical and world knowledge, which proved to bethe most difficult contradictions to detect on a broadscale.
While we are able to find certain specific re-lationships in the development sets, these featuresattained only limited coverage.
Many contradictionsin this category require multiple inferences and re-main beyond our capabilities:T: The Auburn High School Athletic Hall of Fame re-cently introduced its Class of 2005 which includes10 members.H: The Auburn High School Athletic Hall of Fame hasten members.Of the types of contradictions in category (2), we arebest at addressing those formed via structural differ-ences and factive/modal constructions as shown intable 6.
For instance, we detect examples 5 and 6 intable 1.
However, creating features with sufficientprecision is an issue for these types of contradic-tions.
Intuitively, two sentences that have alignedverbs with the same subject and different objects (orvice versa) are contradictory.
This indeed indicatesa contradiction 55% of the time on our developmentsets, but this is not high enough precision given therarity of contradictions.Another type of contradiction where precision fal-ters is numeric mismatch.
We obtain high recall forthis type (table 6), as it is relatively simple to deter-mine if two numbers are compatible, but high preci-sion is difficult to achieve due to differences in whatnumbers may mean.
Consider:T: Nike Inc. said that its profit grew 32 percent, as thecompany posted broad gains in sales and orders.H: Nike said orders for footwear totaled $4.9 billion,including a 12 percent increase in U.S. orders.Our system detects a mismatch between 32 percentand 12 percent, ignoring the fact that one refers toprofit and the other to orders.
Accounting for con-text requires extensive text comprehension; it is notenough to simply look at whether the two numbersare headed by similar words (grew and increase).This emphasizes the fact that mismatching informa-tion is not sufficient to indicate contradiction.As demonstrated by our 63% accuracy onNeg test, we are reasonably good at detecting nega-tion and correctly ascertaining whether it is a symp-tom of contradiction.
Similarly, we handle singleword antonymy with high precision (78.9%).
Never-theless, Harabagiu et al?s performance demonstratesthat further improvement on these types is possible;indeed, they use more sophisticated techniques toextract oppositional terms and detect polarity differ-ences.
Thus, detecting category (1) contradictions isfeasible with current systems.While these contradictions are only a third ofthose in the RTE datasets, detecting such contra-dictions accurately would solve half of the prob-lems found in the real corpus.
This suggests thatwe may be able to gain sufficient traction on contra-diction detection for real world applications.
Evenso, category (2) contradictions must be targeted todetect many of the most interesting examples and tosolve the entire problem of contradiction detection.Some types of these contradictions, such as lexi-cal and world knowledge, are currently beyond ourgrasp, but we have demonstrated that progress maybe made on the structure and factive/modal types.Despite being rare, contradiction is foundationalin text comprehension.
Our detailed investigationdemonstrates which aspects of it can be resolved andwhere further research must be directed.AcknowledgmentsThis paper is based on work funded in part bythe Defense Advanced Research Projects Agencythrough IBM and by the Disruptive TechnologyOffice (DTO) Phase III Program for AdvancedQuestion Answering for Intelligence (AQUAINT)through Broad Agency Announcement (BAA)N61339-06-R-0034.1046ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, DaniloGiampiccolo, Bernardo Magnini, and Idan Szpektor.2006.
The second PASCAL recognising textual en-tailment challenge.
In Proceedings of the SecondPASCAL Challenges Workshop on Recognising Tex-tual Entailment, Venice, Italy.Nathanael Chambers, Daniel Cer, Trond Grenager,David Hall, Chloe Kiddon, Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage, Eric Yeh, andChristopher D. Manning.
2007.
Learning alignmentsand leveraging natural logic.
In Proceedings of theACL-PASCAL Workshop on Textual Entailment andParaphrasing.Timothy Chklovski and Patrick Pantel.
2004.
Verbo-cean: Mining the web for fine-grained semantic verbrelations.
In Proceedings of EMNLP-04.Cleo Condoravdi, Dick Crouch, Valeria de Pavia, Rein-hard Stolle, and Daniel G. Bobrow.
2003.
Entailment,intensionality and text understanding.
Workshop onText Meaning (2003 May 31).Koby Crammer and Yoram Singer.
2001.
Ultraconser-vative online algorithms for multiclass problems.
InProceedings of COLT-2001.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL recognising textual entailmentchallenge.
In Quinonero-Candela et al, editor, MLCW2005, LNAI Volume 3944, pages 177?190.
Springer-Verlag.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of the 5th International Conference on Lan-guage Resources and Evaluation (LREC-06).Christiane Fellbaum.
1998.
WordNet: an electronic lexi-cal database.
MIT Press.Jan Firbas.
1971.
On the concept of communicative dy-namism in the theory of functional sentence perspec-tive.
Brno Studies in English, 7:23?47.Danilo Giampiccolo, Ido Dagan, Bernardo Magnini, andBill Dolan.
2007.
The third PASCAL recognizing tex-tual entailment challenge.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Para-phrasing.Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.2006.
Negation, contrast, and contradiction in textprocessing.
In Proceedings of the Twenty-First Na-tional Conference on Artificial Intelligence (AAAI-06).Andrew Hickl, John Williams, Jeremy Bensley, KirkRoberts, Bryan Rink, and Ying Shi.
2006.
Recog-nizing textual entailment with LCC?s GROUNDHOGsystem.
In Proceedings of the Second PASCAL Chal-lenges Workshop on Recognising Textual Entailment.Kevin Humphreys, Robert Gaizauskas, and Saliha Az-zam.
1997.
Event coreference for information extrac-tion.
In Proceedings of the Workshop on OperationalFactors in Pratical, Robust Anaphora Resolution forUnrestricted Texts, 35th ACL meeting.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stAnnual Meeting of the Association of ComputationalLinguistics.Bill MacCartney, Trond Grenager, Marie-Catherine deMarneffe, Daniel Cer, and Christopher D. Manning.2006.
Learning to recognize features of valid textualentailments.
In Proceedings of the North AmericanAssociation of Computational Linguistics (NAACL-06).Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse rela-tions.
In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics.Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.2006.
Computing relative polarity for textual infer-ence.
In Proceedings of ICoS-5.Olivia Sanchez-Graillet and Massimo Poesio.
2007.
Dis-covering contradiction protein-protein interactions intext.
In Proceedings of BioNLP 2007: Biological,translational, and clinical language processing.Lucy Vanderwende, Arul Menezes, and Rion Snow.2006.
Microsoft research at rte-2: Syntactic contri-butions in the entailment task: an implementation.
InProceedings of the Second PASCAL Challenges Work-shop on Recognising Textual Entailment.Ellen Voorhees.
2008.
Contradictions and justifications:Extensions to the textual entailment task.
In Proceed-ings of the 46th Annual Meeting of the Association forComputational Linguistics.Annie Zaenen, Lauri Karttunen, and Richard S. Crouch.2005.
Local textual inference: can it be defined orcircumscribed?
In ACL 2005 Workshop on EmpiricalModeling of Semantic Equivalence and Entailment.Fabio Massimo Zanzotto, Marco Pennacchiotti, andAlessandro Moschitti.
2007.
Shallow semantics infast textual entailment rule learners.
In Proceedingsof the ACL-PASCAL Workshop on Textual Entailmentand Paraphrasing.1047
