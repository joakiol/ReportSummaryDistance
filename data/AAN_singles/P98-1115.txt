Compact ing the Penn Treebank GrammarAlexander  Krotov  and Mark  Hepp le  and Robert Gaizauskas and Yor ick  Wi lksDepartment  of Computer  Science, Sheffield University211 Portobel lo Street, Sheffield S1 4DP, UK{alexk, hepple, robertg, yorick}@dcs.shef.ac.ukAbstractTreebanks, such as the Penn Treebank (PTB),offer a simple approach to obtaining a broadcoverage grammar: one can simply read thegrammar off the parse trees in the treebank.While such a grammar is easy to obtain, asquare-root rate of growth of the rule set withcorpus size suggests that the derived grammaris far from complete and that much more tree-banked text would be required to obtain a com-plete grammar, if one exists at some limit.However, we offer an alternative xplanationin terms of the underspecification f structureswithin the treebank.
This hypothesis is ex-plored by applying an algorithm to compactthe derived grammar by eliminating redund-ant rules - rules whose right hand sides can beparsed by other rules.
The size of the result-ing compacted grammar, which is significantlyless than that of the full treebank grammar, isshown to approach a limit.
However, such acompacted grammar does not yield very goodperformance figures.
A version of the compac-tion algorithm taking rule probabilities into ac-count is proposed, which is argued to be morelinguistically motivated.
Combined with simplethresholding, this method can be used to givea 58% reduction in grammar size without signi-ficant change in parsing performance, and canproduce a 69% reduction with some gain in re-call, but a loss in precision.1 In t roduct ionThe Penn Treebank (PTB) (Marcus et al, 1994)has been used for a rather simple approachto deriving large grammars automatically: onewhere the grammar ules are simply 'read off'the parse trees in the corpus, with each localsubtree providing the left and right hand sidesof a rule.
Charniak (Charniak, 1996) reportsprecision and recall figures of around 80% fora parser employing such a grammar.
In thispaper we show that the huge size of such a tree-bank grammar (see below) can be reduced insize without appreciable loss in performance,and, in fact, an improvement in recall can beachieved.Our approach can be generalised in termsof Data-Oriented Parsing (DOP) methods (see(Bonnema et al, 1997)) with the tree depth of1.
However, the number of trees produced witha general DOP method is so large that Bonnema(Bonnema et al, 1997) has to resort to restrict-ing the tree depth, using a very domain-specificcorpus such as ATIS or OVIS, and parsing veryshort sentences of average length 4.74 words.Our compaction algorithm can be easily exten-ded for the use within the DOP framework but,because of the huge size of the derived grammar(see below), we chose to use the simplest PCFGframework for our experiments.We are concerned with the nature of the ruleset extracted, and how it can be improved, withregard both to linguistic riteria and processingefficiency.
Inwhat  follows, we report the worry-ing observation that the growth of the rule setcontinues at a square root rate throughout pro-cessing of the entire treebank (suggesting, per-haps that the rule set is far from complete).
Ourresults are similar to those reported in (Krotovet al, 1994).
1 We discuss an alternative pos-sible source of this rule growth phenomenon,partial bracketting, and suggest hat it can bealleviated by compaction, where rules that areredundant (in a sense to be defined) are elimin-ated from the grammar.Our experiments on compacting a PTB tree-1 For the complete investigation of the grammar ex-tracted from the Penn Treebank II see (Gaizauskas,1995)6992000015000i0000500000 20  40  60  80  i00Percentage  o f  the  corpusFigure 1: Rule Set Growth for Penn TreebankIIbank grammar esulted in two major findings:one, that the grammar can be compacted toabout 7% of its original size, and the rule num-ber growth of the compacted grammar stops atsome point.
The other is that a 58% reductioncan be achieved with no loss in parsing perform-ance, whereas a 69% reduction yields a gain inrecall, but a loss in precision.This, we believe, gives further support tothe utility of treebank grammars and to thecompaction method.
For example, compactionmethods can be applied within the DOP frame-work to reduce the number of trees.
Also, bypartially lexicalising the rule extraction process(i.e., by using some more frequent words as wellas the part-of-speech tags), we may be able toachieve parsing performance similar to the bestresults in the field obtained in (Collins, 1996).2 Growth  o f  the  Ru le  SetOne could investigate whether there is a fi-nite grammar that should account for any textwithin a class of related texts (i.e.
a domainoriented sub-grammar of English).
If there is,the number of extracted rules will approach alimit as more sentences are processed, i.e.
asthe rule number approaches the size of such anunderlying and finite grammar.We had hoped that some approach to a limitwould be seen using PTB II (Marcus et al,1994), which larger and more consistent forbracketting than PTB I.
As shown in Figure 1,however, the rule number growth continues un-abated even after more than 1 million part-of-speech tokens have been processed.3 Ru le  Growth  and  Par t ia lBrackettingWhy should the set of rules continue to grow inthis way?
Putt ing aside the possibility that nat-ural languages do not have finite rule sets, wecan think of two possible answers.
First, it maybe that the full "underlying rammar" is muchlarger than the rule set that has so far beenproduced, requiring a much larger tree-bankedcorpus than is now available for its extrac-tion.
If this were true, then the outlook wouldbe bleak for achieving near-complete grammarsfrom treebanks, given the resource demands ofproducing hand-parsed text.
However, the rad-ical incompleteness of grammar that this al-ternative implies seems incompatible with thepromising parsing results that Charniak reports(Charniak, 1996).A second answer is suggested by the presencein the extracted grammar of rules such as (1).
2This rule is suspicious from a linguistic point ofview, and we would expect that the text fromwhich it has been extracted should more prop-erly have been analysed using rules (2,3), i.e.
asa coordination of two simpler NPs.NP --~ DT NN CC DT NN (1)NP --~ NP CC NP (2)gP --+ DT  NN (3)Our suspicion is that this example reflects awidespread phenomenon of partial brackettingwithin the PTB.
Such partial bracketting willarise during the hand-parsing of texts, with (hu-man) parsers adding brackets where they areconfident hat some string forms a given con-stituent, but leaving out many brackets wherethey are less confident of the constituent struc-ture of the text.
This will mean that manyrules extracted from the corpus will be 'flat-ter' than they should be, corresponding prop-erly to what should be the result of using sev-eral grammar ules, showing only the top nodeand leaf nodes of some unspecified tree structure(where the 'leaf nodes' here are category sym-bols, which may be nonterminal).
For the ex-ample above, a tree structure that should prop-erly have been given as (4), has instead received2PTB POS tags are used here, i.e.
DT for determiner,CC for coordinating conjunction (e.g 'and'), NN for noun700only the partial analysis (5), from the flatter'partial-structure' ule (1).i.
NPNP CC NPDT NN DT NN(4)ii.
NP (5)DT NN CC DT NN4 Grammar  Compact ionThe idea of partiality of structure in treebanksand their grammars uggests a route by whichtreebank grammars may be reduced in size, orcompacted as we shall call it, by the eliminationof partial-structure ules.
A rule that may beeliminable as a partial-structure rule is one thatcan be 'parsed' (in the familiar sense of context-free parsing) using other rules of the grammar.For example, the rule (1) can be parsed us-ing the rules (2,3), as the structure (4) demon-strates.
Note that, although a partial-structurerule should be parsable using other rules, it doesnot follow that every rule which is so parsableis a partial-structure rule that should be elimin-ated.
There may be defensible rules which canbe parsed.
This is a topic to which we will re-turn at the end of the paper (Sec.
6).
For mostof what follows, however, we take the simplerpath of assuming that the parsability of a ruleis not only necessary, but also sufficient, for itselimination.Rules which can be parsed using other rulesin the grammar are redundant in the sense thateliminating such a rule will never have the ef-fect of making a sentence unparsable that couldpreviously be parsed.
3The algorithm we use for compacting a gram-mar is straightforward.
A loop is followedwhereby each rule R in the grammar is ad-dressed in turn.
If R can be parsed using otherrules (which have not already been eliminated)then R is deleted (and the grammar without Ris used for parsing further rules).
Otherwise R3Thus, wherever a sentence has a parse P that em-ploys the parsable rule R, it also has a further parse thatis just like P except that any use of R is replaced by amore complex substructure, i.e.
a parse of R.is kept in the grammar.
The rules that remainwhen all rules have been checked constitute thecompacted grammar.An interesting question is whether the resultof compaction is independent of the order inwhich the rules are addressed.
In general, this isnot the case, as is shown by the following rules,of which (8) and (9) can each be used to parsethe other, so that whichever is addressed firstwill be eliminated, whilst the other will remain.B --+ C (6)C --+ B (7)A -+ B B (8)A -~ C C (9)Order-independence can be shown to hold forgrammars that contain no unary or epsilon('empty') rules, i.e.
rules whose righthand sideshave one or zero elements.
The grammar thatwe have extracted from PTB II, and which isused in the compaction experiments reported inthe next section, is one that excludes uch rules.For further discussion, and for the proof of theorder independence s e (Krotov, 1998).
Unaryand sister rules were collapsed with the sisternodes, e.g.
the structure (S (NP -NULL-) (VPVB (NP (QP .
.
. )
) )  .)
will produce the fol-lowing rules: S -> VP., VP -> VB QPand QP_> .
4?
, .5 Exper imentsWe conducted a number of compaction exper-iments: 5 first, the complete grammar wasparsed as described in Section 4.
Results ex-ceeded our expectations: the set of 17,529 rulesreduced to only 1,667 rules, a better than 90%reduction.To investigate in more detail how the com-pacted grammar grows, we conducted a thirdexperiment involving a staged compaction of thegrammar.
Firstly, the corpus was split into 10%chunks (by number of files) and the rule setsextracted from each.
The staged compactionproceeded as follows: the rule set of the first10% was compacted, and then the rules for the4See (Gaizauskas, 1995) for discussion.SFor these experiments, we used two parsers: Stol-cke's BOOGIE (Stolcke, 1995) and Sekine's Apple PieParser (Sekine and Grishman, 1995).701$220001500I0005000: i0 20  40  60  80  i 00Percentage  o f  the  corpusFigure 2: Compacted Grammar Sizenext 10% added and the resulting set again com-pacted, and then the rules for the next 10% ad-ded, and so on.
Results of this experiment areshown in Figure 2.At 50% of the corpus processed the com-pacted grammar size actually exceeds the levelit reaches at 100%, and then the overall gram-mar size starts to go down as well as up.
Thisreflects the fact that new rules are either re-dundant, or make "old" rules redundant, so thatthe compacted grammar size seems to approacha limit.6 Retaining Linguistically ValidRulesEven though parsable rules are redundant inthe sense that has been defined above, it doesnot follow that they should always be removed.In particular, there are times where the flatterstructure allowed by some rule may be more lin-guistically correct, rather than simple a case ofpartial bracketting.
Consider, for example, the(linguistically plausible) rules (10,11,12).
Rules(11) and (12) can be used to parse (10), butit should not be eliminated, as there are caseswhere the flatter structure it allows is more lin-guistically correct.VP ~ VB NP PPVP ~ VB NPNP ~ NP  PPi.
VP ii.
VPVB NP VB NP PPNP PP(10)(ii)(12)(13)We believe that a solution to this problemcan be found by exploiting the date provided bythe corpus.
Frequency of occurrence data forrules which have been collected from the cor-pus and used to assign probabilities to rules,and hence to the structures they allow, so asto produce a probabilistic ontext-free grammarfor the rules.
Where a parsable rule is correctrather than merely partially bracketted, we thenexpect his fact to be reflected in rule and parseprobabilities (reflecting the occurrence data ofthe corpus), which can be used to decide whena rule that may be eliminated should be elimin-ated.
In particular, a rule should be eliminatedonly when the more complex structure allowedby other rules is more probable than the simplerstructure that the rule itself allows.We developed a linguistic compaction al-gorithm employing the ideas just described.However, we cannot present it here due tothe space limitations.
The preliminary resultsof our experiments are presented in Table 1.Simple thresholding (removing rules that onlyoccur once) was also to achieve the maximumcompaction ratio.
For labelled as well as unla-belled evaluation of the resulting parse trees weused the evalb software by Satoshi Sekine.
See(Krotov, 1998) for the complete presentation ofour methodology and results.As one can see, the fully compacted grammaryields poor recall and precision figures.
Thiscan be because collapsing of the rules often pro-duces too much substructure (hence lower pre-cision figures) and also because many longerrules in fact encode valid linguistic information.However, linguistic compaction combined withsimple thresholding achieves a 58% reductionwithout any loss in performance, and 69% re-duction even yields higher recall.7 ConclusionsWe see the principal results of our work to bethe following:* the result showing continued square-rootgrowth in the rule set extracted from thePTB II;?
the analysis of the source of this continuedgrowth in terms of partial bracketting andthe justification this provides for compac-tion via rule-parsing;?
the result that the compacted rule setdoes approach a limit at some point dur-702Full Simply thresholded Fully compacted Linguistically compactedGrammar 1 Grammar 2Recall 70.55%Precision 77.89%Recall 73.49%Precision 81.44%Grammar size 15,421reduction (as % of full) 0%Labelled evaluation70.78% 30.93% 71.55% 70.76%77.66% 19.18% 72.19% 77.21%Unlabelled evaluation73.71% 43.61%80.87% 27.04%7,278 1,12253% 93%74.72% 73.67%75.39% 80.39%4,820 6,41769% 58%Table 1: Preliminary results of evaluating the grammar compaction methoding staged rule extraction and compaction,after a sufficient amount of input has beenprocessed;?
that, though the fully compacted grammarproduces lower parsing performance thanthe extracted grammar, a 58% reduction(without loss) can still be achieved by us-ing linguistic compaction, and 69% reduc-tion yields a gain in recall, but a loss inprecision.The latter result in particular provides furthersupport for the possible future utility of thecompaction algorithm.
Our method is similarto that used by Shirai (Shirai et al, 1995), butthe principal differences are as follows.
First,their algorithm does not employ full context-free parsing in determining the redundancy ofrules, considering instead only direct composi-tion of the rules (so that only parses of depth2 are addressed).
We proved that the result ofcompaction is independent of the order in whichthe rules in the grammar are parsed in thosecases involving 'mutual parsability' (discussedin Section 4), but Shirai's algorithm will elimin-ate both rules so that coverage is lost.
Secondly,it is not clear that compaction will work in thesame way for English as it did for Japanese.Re ferencesRemko Bonnema, Rens Bod, and Remko Scha.
1997.A DOP model for semantic interpretation.
InProceedings of European Chapter of the ACL,pages 159-167.Eugene Charniak.
1996.
Tree-bank grammars.
InProceedings of the Thirteenth National Confer-ence on Artificial Intelligence (AAAI-96), pages1031-1036.
MIT Press, August.Michael Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In Proceed-ings of the 3~th Annual Meeting of the ACL.Robert Gaizauskas.
1995.
Investigations into thegrammar underlying the Penn Treebank II.
Re-search Memorandum CS-95-25, University ofSheffield.Alexander Krotov, Robert Gaizauskas, and YorickWilks.
1994.
Acquiring a stochastic ontext-freegrammar f om the Penn Treebank.
In Proceedingsof Third Conference on the Cognitive Science ofNatural Language Processing, pages 79-86, Dub-lin.Alexander Krotov.
1998.
Notes on compactingthe Penn Treebank grammar.
Technical Memo,Department of Computer Science, University ofSheffield.M.
Marcus, G. Kim, M.A.
Marcinkiewicz,R.
MacIntyre, A. Bies, M. Ferguson, K. Katz,and B. Schasberger.
1994.
The Penn Tree-bank: Annotating predicate argument structure.In Proceedings of ARPA Speech and Naturallanguage workshop.Satoshi Sekine and Ralph Grishman.
1995.
Acorpus-based probabilistic grammar with only twonon-terminals.
In Proceedings of Fourth Interna-tional Workshop on Parsing Technologies.Kiyoaki Shirai, Takenobu Tokunaga, and HozumiTanaka.
1995.
Automatic extraction of Japanesegrammar f om a bracketed corpus.
In Proceedingsof Natural Language Processing Pacific Rim Sym-posium, Korea, December.Andreas Stolcke.
1995.
An efficient probabilisticcontext-free parsing algorithm that computesprefix probabilities.
Computational Linguistics,21(2):165-201.703
