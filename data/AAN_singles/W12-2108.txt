Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 65?74,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsLanguage Identification for Creating Language-Specific Twitter CollectionsShane Bergsma?
Paul McNamee?,?
Mossaab Bagdouri?
Clayton Fink?
Theresa Wilson?
?Human Language Technology Center of Excellence, Johns Hopkins University?Johns Hopkins University Applied Physics Laboratory, Laurel, MD?Department of Computer Science, University of Maryland, College Park, MDsbergsma@jhu.edu, mcnamee@jhu.edu, mossaab@umd.edu, clayton.fink@jhuapl.edu, taw@jhu.eduAbstractSocial media services such as Twitter offer animmense volume of real-world linguistic data.We explore the use of Twitter to obtain authen-tic user-generated text in low-resource lan-guages such as Nepali, Urdu, and Ukrainian.Automatic language identification (LID) canbe used to extract language-specific data fromTwitter, but it is unclear how well LID per-forms on short, informal texts in low-resourcelanguages.
We address this question by an-notating and releasing a large collection oftweets in nine languages, focusing on confus-able languages using the Cyrillic, Arabic, andDevanagari scripts.
This is the first publicly-available collection of LID-annotated tweetsin non-Latin scripts, and should become astandard evaluation set for LID systems.
Wealso advance the state-of-the-art by evaluat-ing new, highly-accurate LID systems, trainedboth on our new corpus and on standard ma-terials only.
Both types of systems achievea huge performance improvement over theexisting state-of-the-art, correctly classifyingaround 98% of our gold standard tweets.
Weprovide a detailed analysis showing how theaccuracy of our systems vary along certain di-mensions, such as the tweet-length and theamount of in- and out-of-domain training data.1 IntroductionTwitter is an online social-networking service thatlets users send and receive short texts called tweets.Twitter is enormously popular; more than 50 mil-lion users log in daily and billions of tweets are senteach month.1 Tweets are publicly-available by de-1http://mashable.com/2011/09/08/Twitter-has-100-million-active-users/fault and thus provide an enormous and growing freeresource of authentic, unedited text by ordinary peo-ple.
Researchers have used Twitter to study how hu-man language varies by time zone (Kiciman, 2010),census area (Eisenstein et al, 2011), gender (Burgeret al, 2011), and ethnicity (Fink et al, 2012).
Twit-ter also provides a wealth of user dialog, and a vari-ety of dialog acts have been observed (Ritter et al,2010) and predicted (Ritter et al, 2011).Of course, working with Twitter is not all rosesand rainbows.
Twitter is a difficult domain becauseunlike, for example, news articles, tweets are short(limited to 140 characters), vary widely in style,and contain many spelling and grammatical errors.Moreover, unlike articles written by a particularnews organization, a corpus constructed from Twit-ter will contain tweets in many different languages.This latter point is particularly troubling becausethe majority of language-processing technology ispredicated on knowing which language is being pro-cessed.
We are pursuing a long-term effort to buildsocial media collections in a variety of low-resourcelanguages, and we need robust language identifica-tion (LID) technology.
While LID is often viewedas a solved problem (McNamee, 2005), recent re-search has shown that LID can be made arbitrarilydifficult by choosing domains with (a) informal writ-ing, (b) lots of languages to choose from, (c) veryshort texts, and (d) unbalanced data (Hughes et al,2006; Baldwin and Lui, 2010).
Twitter exhibits allof these properties.
While the problem of LID onTwitter has been considered previously (Tromp andPechenizkiy, 2011; Carter et al, 2013), these studieshave only targeted five or six western European lan-guages, and not the diversity of languages and writ-ing systems that we would like to process.65Our main contribution is the release of a large col-lection of tweets in nine languages using the Cyril-lic, Arabic, and Devanagari alphabets.
We test dif-ferent methods for obtaining tweets in a given tar-get language (?2).
We then use an online crowd-sourcing platform to have these tweets annotated byfluent speakers of that language (?3).
We generateover 18,000 triple-consensus tweets, providing thefirst publicly-available collection of LID-annotatedtweets in non-Latin scripts.
The annotated cor-pus is available online at: http://apl.jhu.edu/?paulmac/lid.html.
We anticipate our multilin-gual Twitter collection becoming a standard evalua-tion set for LID systems.We also implement two LID approaches and eval-uate these approaches against state-of-the-art com-petitors.
?4.1 describes a discriminative classifierthat leverages both the tweet text and the tweet meta-data (such as the user name, location, and landingpages for shortened URLs).
?4.2 describes an effi-cient tool based on compression language models.Both types of systems achieve a huge improvementover existing state-of-the-art approaches, includingthe Google Compact Language Detector (part of theChrome browser), and a recent LID system fromLui and Baldwin (2011).
Finally, we provide furtheranalysis of our systems in this unique domain, show-ing how accuracy varies with the tweet-length andthe amount of in-domain and out-of-domain train-ing data.
In addition to the datasets, we are releasingour compression language model tool for public use.2 Acquiring Language-Specific TweetsWe use two strategies to collect tweets in specificlanguages: (?2.1) we collect tweets by users whofollow language-specific Twitter sources, and (?2.2)we use the Twitter API to collect tweets from userswho are likely to speak the target language.2.1 Followers of Language-Specific SourcesOur first method is called the Sources method andinvolves a three-stage process.
First, Twitter sourcesfor the target language are manually identified.Sources are Twitter users or feeds who: (a) tweetin the target language, (b) have a large number offollowers, and (c) act as hubs (i.e., have a highfollowers-to-following ratio).
Twitter sources aretypically news or media outlets (e.g.
BBC News),celebrities, politicians, governmental organizations,but they may just be prominent bloggers or tweeters.Once sources are identified, we use the TwitterAPI (dev.twitter.com) to query each source forits list of followers.
We then query the user data forthe followers in batches of 100 tweets.
For userswhose data is public, a wealth of information isreturned, including the total number of tweets andtheir most recent tweet.
For users who had tweetedabove a minimum number of times.
and whosemost-recent-tweet tweet was in the character set forthe target language, we obtained their most recent100-200 tweets and added them to our collection.2While we have used the above approach to ac-quire data in a number of different languages, for thepurposes of our annotated corpus (?3), we select thesubsets of users who exclusively follow sources inone of our nine target languages (Table 1).
We alsofilter tweets that do not contain at least one charac-ter in the target?s corresponding writing system (weplan to address romanized tweets in future work).2.2 Direct Twitter-API CollectionWhile we are most interested in users who follownews articles, we also tested other methods for ob-taining language-specific tweets.
First, we used theTwitter API to collect tweets from locations wherewe expected to get some number of tweets in the tar-get language.
We call this method the Twit-API col-lection method.
To geolocate our tweets, the Twit-ter API?s geotag method allowed us to collect tweetswithin a specified radius of a given set of coordi-nates in latitude and longitude.
To gather a sam-ple of tweets in our target languages, we queriedfor tweets from cities with populations of at least200,000 where speakers of the target language areprominent (e.g., Karachi, Pakistan for Urdu; Tehran,Iran for Farsi; etc.).
We collected tweets within a ra-dius of 25 miles of the geocoordinates.
We also usedthe Search API to persistently poll for tweets fromusers identified by Twitter as being in the queriedlocation.
For Urdu, we also relied on the language-2Tromp and Pechenizkiy (2011) also manually identifiedlanguage-specific Twitter feeds, but they use tweets from thesesources directly as gold standard data, while we target the userswho simply follow such sources.
We expect our approach toobtain more-authentic and less-edited user language.66identification code returned by the API for eachtweet; we filter all our geolocated Urdu tweets thatare not marked as Urdu.We also obtained tweets through an information-retrieval approach that has been used elsewhere forcreating minority language corpora (Ghani et al,2001).
We computed the 25 most frequent uniquewords in a number of different languages (that is,words that do not occur in the vocabularies of otherlanguages).
Unfortunately, we found no way to en-force that the Twitter API return only tweets con-taining one or more of our search terms (e.g., re-turned tweets for Urdu were often in Arabic and didnot contain our Urdu search terms).
There is a lackof documentation on what characters are supportedby the search API; it could be that the API cannothandle certain of our terms.
We thus leave furtherinvestigation of this method for future work.3 Annotating Tweets by LanguageThe general LID task is to take as input some pieceof text, and to produce as output a prediction of whatlanguage the text is written in.
Our annotation andprediction systems operate at the level of individualtweets.
An alternative would have been to assumethat each user only tweets in a single language, andto make predictions on an aggregation of multipletweets.
We operate on individual tweets mainly be-cause (A) we would like to quantify how often usersswitch between languages and (B) we are also inter-ested in domains and cases where only tweet-sizedamounts of text are available.
When we do havemultiple tweets per user, we can always aggregatethe scores on individual predictions (?6 has some ex-perimental results using prediction aggregation).Our human annotation therefore also focuses onvalidating the language of individual tweets.
Tweetsverified by three independent annotators are ac-cepted into our final gold-standard data.3.1 Amazon Mechanical TurkTo access annotators with fluency in each language,we crowdsourced the annotation using Amazon Me-chanical Turk (mturk.com).
AMT is an online la-bor marketplace that allows requesters to post tasksfor completion by paid human workers.
Crowd-sourcing via AMT has been shown to provide high-quality data for a variety of NLP tasks (Snow et al,2008; Callison-Burch and Dredze, 2010), includingmultilingual annotation efforts in translation (Zaidanand Callison-Burch, 2011b), dialect identification(Zaidan and Callison-Burch, 2011a), and buildingbilingual lexicons (Irvine and Klementiev, 2010).3.2 Annotation TaskFrom the tweets obtained in ?2, we took a randomsample in each target language, and posted thesetweets for annotation on AMT.
Each tweet in thesample was assigned to a particular AMT job; eachjob comprised the annotation of 20 tweets.
The jobdescription requested workers that are fluent in thetarget language and gave an example of valid andinvalid tweets in that language.
The job instructionsasked workers to mark whether each tweet was writ-ten for speakers of the target language.
If the tweetcombines multiple languages, workers were askedto mark as the target language if ?most of the text isin [that language] excluding URLs, hash-tags, etc.
?Jobs were presented to workers as HTML pages withthree buttons alongside each tweet for validating thelanguage.
For example, for Nepali, a Worker canmark that a tweet is ?Nepali?, ?Not Nepali?, or ?Notsure.?
We paid $0.05 per job and requested that eachjob be completed by three workers.3.3 Quality ControlTo ensure high annotation quality, we follow ourestablished practices in only allowing our tasks tobe completed by workers who have previously com-pleted at least 50 jobs on AMT, and who have had atleast 85% of their jobs approved.
Our jobs also dis-play each tweet as an image; this prevents workersfrom pasting the tweet into existing online languageprocessing services (like Google Translate).We also have control tweets in each job to allowus to evaluate worker performance.
A positive con-trol is a tweet known to be in the target language;a negative control is a tweet known to be in a dif-ferent language.
Between three to six of the twentytweets in each job were controls.
The controls aretaken from the sources used in our Sources method(?2.1); e.g., our Urdu controls come from sourceslike BBC Urdu?s Twitter feed.
To further validatethe controls, we also applied our open-domain LIDsystem (?4.2) and filtered any Source tweets whose67Language Method Purity Gold TweetsArabic Sources 100% 1174Farsi Sources 100% 2512Urdu Sources 55.4% 1076Arabic Twit-API 99.9% 1254Farsi Twit-API 99.7% 2366Urdu Twit-API 61.0% 1313Hindi Sources 97.5% 1214Nepali Sources 97.3% 1681Marathi Sources 91.4% 1157Russian Sources 99.8% 2005Bulgarian Sources 92.2% 1886Ukrainian Sources 14.3% 631Table 1: Statistics of the Annotated Multilingual TwitterCorpus: 18,269 total tweets in nine languages.predicted language was not the expected language.Our negative controls are validated tweets in a lan-guage that uses the same alphabet as the target (e.g.,our negative controls for Ukrainian were taken fromour LID-validated Russian and Bulgarian sources).We collect aggregate statistics for each Workerover the control tweets of all their completed jobs.We conservatively discard any annotations by work-ers who get below 80% accuracy on either the posi-tive or negative control tweets.3.4 Dataset StatisticsTable 1 gives the number of triple-validated ?Gold?tweets in each language, grouped into those usingthe Arabic, Devanagari and Cyrillic writing sys-tems.
The Arabic data is further divided into tweetsacquired using the Sources and Twit-API methods.Table 1 also gives the Purity of the acquired re-sults; that is, the percentage of acquired tweets thatwere indeed in the target language.
The Purityis calculated as the number of triple-verified goldtweets divided by the total number of tweets wherethe three annotators agreed in the annotation (thustriply-marked either Yes, No, or Not sure).For major languages (e.g.
Arabic and Russian),we can accurately obtain tweets in the target lan-guage, perhaps obviating the need for LID.
For theUrdu sets, however, a large percentage of tweets arenot in Urdu, and thus neither collection method isreliable.
An LID tool is needed to validate the data.A native Arabic speaker verified that most of ourinvalid Urdu tweets were Arabic.
Ukrainian is themost glaringly impure language that we collected,with less than 15% of our intended tweets actuallyin Ukrainian.
Russian is widely spoken in Ukraineand seems to be the dominant language on Twitter,but more analysis is required.
Finally, Marathi andBulgarian also have significant impurities.The complete annotation of all nine languagescost only around $350 USD.
While not insignificant,this was a small expense relative to the total humaneffort we are expending on this project.
Scaling ourapproach to hundreds of languages would only coston the order of a few thousand dollars, and we areinvestigating whether such an effort could be sup-ported by enough fluent AMT workers.4 Language Identification SystemsWe now describe the systems we implementedand/or tested on our annotated data.
All the ap-proaches are supervised learners, trained from a col-lection of language-annotated texts.
At test time, thesystems choose an output language based on the in-formation they have derived from the annotated data.4.1 LogR: Discriminative LIDWe first adopt a discriminative approach to LID.Each tweet to be classified has its relevant informa-tion encoded in a feature vector, x?.
The annotatedtraining data can be represented as N pairs of la-bels and feature vectors: {(y1, x?1), ..., (yN , x?N )}.To train our model, we use (regularized) logistic re-gression (a.k.a.
maximum entropy) since it has beenshown to perform well on a range of NLP tasksand its probabilistic outputs are useful for down-stream processing (such as aggregating predictionsover multiple tweets).
In multi-class logistic regres-sion, the probability of each class takes the form ofexponential functions over features:p(y = k|x?)
= exp(w?k ?
x?
)?j exp(w?j ?
x?
)For LID, the classifier predicts the language k thathas the highest probability (this is also the class withhighest weighted combination of features, w?k ?
x?
).The training procedure tunes the weights to optimizefor correct predictions on training data, subject to atunable L2-regularization penalty on the weight vec-tor norm.
For our experiments, we train and test ourlogistic regression classifier (LogR) using the effi-cient LIBLINEAR package (Fan et al, 2008).68We use two types of features in our classifier:Character Features encode the characterN-grams in the input text; characters are thestandard information source for most LID systems(Cavnar and Trenkle, 1994; Baldwin and Lui, 2010).We have a unique feature for each unique N-gram inour training data.
N-grams of up-to-four characterswere optimal on development data.
Each featurevalue is the (smoothed) log-count of how oftenthe corresponding N-gram occurs in that instance.Prior to extracting the N-grams, we preprocess eachtweet to remove URLs, hash-tags, user mentions,punctuation and we normalize all digits to 0.Meta features encode user-provided informationbeyond the tweet text.
Similar information has pre-viously been used to improve the accuracy of LIDclassifiers on European-language tweets (Carter etal., 2013).
We have features for the tokens inthe Twitter user name, the screen name, and self-reported user location.
We also have features forprefixes of these tokens, and flags for whether thename and location are in the Latin script.
Our metafeatures also include features for the hash-tags, user-mentions, and URLs in the tweet.
We provide fea-tures for the protocol (e.g.
http), hostname, and top-level domain (e.g.
.com) of each link in a tweet.
Forshortened URLs (e.g.
via bit.ly), we query theURL server to obtain the final link destination, andprovide the URL features for this destination link.4.2 PPM: Compression-Based LIDOur next tool uses compression language models,which have been proposed for a variety of NLPtasks including authorship attribution (Pavelec et al,2009), text classification (Teahan, 2000; Frank et al,2000), spam filtering (Bratko et al, 2006), and LID(Benedetto et al, 2002).
Our method is based on theprediction by partial matching (PPM) family of al-gorithms and we use the PPM-A variant (Cleary etal., 1984).
The algorithm processes a string and de-termines the number of bits required to encode eachcharacter using a variable-length context.
It requiresonly a single parameter, the maximal order, n; weuse n = 5 for the experiments in this paper.
Giventraining data for a number of languages, the methodseeks to minimize cross-entropy and thus selects theLanguage Wikip.
AllArabic 372 MB 1058 MBFarsi 229 MB 798 MBUrdu 30 MB 50 MBHindi 235 MB 518 MBNepali 31 MB 31 MBMarathi 32 MB 66 MBRussian 563 MB 564 MBBulgarian 301 MB 518 MBUkrainian 461 MB 463 MBTable 2: Size of other PPM training materials.language which would most compactly encode thetext we are attempting to classify.We train this method both on our Twitter data andon large collections of other material.
These ma-terials include corpora obtained from news sources,Wikipedia, and government bodies.
For our ex-periments we divide these materials into two sets:(1) just Wikipedia and (2) all sources, includingWikipedia.
Table 2 gives the sizes of these sets.4.3 Comparison SystemsWe compare our two new systems with the best-available commercial and academic software.TextCat: TextCat3 is a widely-used stand-aloneLID program.
Is is an implementation of theN-gram-based algorithm of Cavnar and Trenkle(1994), and supports identification in ?about 69 lan-guages?
in its downloadable form.
Unfortunately,the available models do not support all of our targetlanguages, nor are they compatible with the standardUTF-8 Unicode character encoding.
We thereforemodified the code to process UTF-8 characters andre-trained the system on our Twitter data (?5).Google CLD: Google?s Chrome browser includesa tool for language-detection (the Google CompactLanguage Detector), and this tool is included as a li-brary within Chrome?s open-source code.
Mike Mc-Candless ported this library to its own open sourceproject.4 The CLD tool makes predictions using text4-grams.
It is designed for detecting the languageof web pages, and can take meta-data hints from thedomain of the webpage and/or the declared webpage3http://odur.let.rug.nl/vannoord/TextCat/4http://code.google.com/p/chromium-compact-language-detector/69Dataset Train Development TestArabic 2254 1171 1191Devanagari 2099 991 962Cyrillic 2243 1133 1146Table 3: Number of tweets used in experiments, by writ-ing system/classification taskencoding, but it also works on stand-alone text.5 Weuse it in its original, unmodified form.
While thereare few details in the source code itself, the train-ing data for this approach was apparently obtainedthrough Google?s internal data collections.Lui and Baldwin ?11: Lui and Baldwin (2011) re-cently released a stand-alone LID tool, which theycall langid.py.6 They compared this system tostate-of-the-art LID methods and found it ?to befaster whilst maintaining competitive accuracy.?
Weuse this system with its provided models only, asthe software readme notes ?training a model forlangid.py is a non-trivial process, due to the largeamount of computations required.?
The sources ofthe provided models are described in Lui and Bald-win (2011).
Although many languages are sup-ported, we restrict the system to only choose be-tween our data?s target languages (?5).5 ExperimentsThe nine languages in our annotated data use one ofthree different writing systems: Arabic, Devanagari,or Cyrillic.
We therefore define three classificationtasks, each choosing between three languages thathave the same writing system.
We divide our an-notated corpus into training, development and testdata for these experiments (Table 3).
For the Ara-bic data, we merge the tweets obtained via our twocollection methods (?2); for Devanagari/Cyrillic, alltweets are obtained using the Sources method.
Weensure that tweets by a unique Twitter user occurin at most only one of the sets.
The proportion ofeach language in each set is roughly the same as theproportions of gold tweets in Table 1.
All of ourTwitter-trained systems learn their models from thistraining data, while all hyperparameter tuning (such5Google once offered an online language-detection API, butthis service is now deprecated; moreover, it was rate-limited andnot licensed for research use (Lui and Baldwin, 2011).6https://github.com/saffsd/langid.pySystem Arab.
Devan.
Cyrill.Trained on Twitter Corpus:LogR: meta 79.8 74.7 82.0LogR: chars 97.1 96.2 96.1LogR: chars+meta 97.4 96.9 98.3PPM 97.1 95.3 95.8TextCat 96.3 89.1 90.3Open-Domain: Trained on Other Materials:Google CLD 90.5 N/A 91.4Lui and Baldwin ?11 91.4 78.4 88.8PPM (Wikip.)
97.6 95.8 95.7PPM (All) 97.6 97.1 95.8Trained on both Twitter and Other Materials:PPM (Wikip.+Twit) 97.9 97.0 95.9PPM (All+Twit) 97.6 97.9 96.0Table 4: LID accuracy (%) of different systems on held-out tweets.
High LID accuracy on tweets is obtainable,whether training in or out-of-domain.as tuning the regularization parameter of the LogRclassifier) is done on the development set.
Our eval-uation metric is Accuracy: what proportion of tweetsin each held-out test set are predicted correctly.6 ResultsFor systems trained on the Twitter data, both ourLogR and PPM system strongly outperform TextCat,showing the effectiveness of our implemented ap-proaches (Table 4).
Meta features improve LogRon each task.
For systems trained on external data,PPM strongly outperforms other systems, makingfewer than half the errors on each task.
We alsotrained PPM on both the relatively small number ofTwitter training samples and the much larger numberof other materials.
The combined system is as goodor better than the separate models on each task.We get more insight into our systems by seeinghow they perform as we vary the amount of train-ing data.
Figure 1 shows that with only a few hun-dred annotated tweets, the LogR system gets over90% accuracy, while performance seems to plateaushortly afterwards.
A similar story holds as wevary the amount of out-of-domain training data forthe PPM system; performance improves fairly lin-early as exponentially more training data is used, buteventually begins to level off.
Not only is PPM aneffective system, it can leverage a lot of training ma-70506070809010010  100  1000Accuracy(%)Number of training tweetsArabicDevanagariCyrillicFigure 1: The more training data the better, but accuracylevels off: learning curve for LogR-chars (note log-scale).5060708090100100  1000  10000 100000 1e+06  1e+07Accuracy(%)Number of characters of training dataArabicDevanagariCyrillicFigure 2: Accuracy of PPM classifier using varyingamounts of Wikipedia training text (also on log-scale).terials in order to obtain its high accuracy.In Figure 3, we show how the accuracy of our sys-tems varies over tweets grouped into bins by theirlength.
Performance on short tweets is much worsethan those closer to 140 characters in length.We also examined aggregating predictions overmultiple tweets by the same user.
We extracted allusers with ?4 tweets in the Devanagari test set (87users in total).
We then averaged the predictions ofthe LogR system on random subsets of a user?s testtweets, making a single decision for all tweets in asubset.
We report the mean accuracy of running thisapproach 100 times with random subsets of 1, 2, 3,and all 4 tweets used in the prediction.
Even withonly 2 tweets per user, aggregating predictions canreduce relative error by almost 60% (Table 5).Encouraged by the accuracy of our systems on an-notated data, we used our PPM system to analyzea large number of un-annotated tweets.
We trainedPPM models for 128 languages using data that in-cludes Wikipedia (February 2012), news (e.g., BBCNews, Voice of America), and standard corpora such88909294969810040  60  80  100  120  140Accuracy(%)Avgerage length of tweet (binned)ArabicDevanagariCyrillicFigure 3: The longer the tweet, the better: mean accuracyof LogR by average length of tweet, with tweets groupedinto five bins by length in characters.Number of Tweets 1 2 3 4Accuracy 97.0 98.7 98.8 98.9Table 5: The benefits of aggregating predictions by user:Mean accuracy of LogR-chars as you make predictionson multiple Devanagari tweets at a timeas Europarl, JRC-Acquis, and various LDC releases.We then made predictions in the TREC Tweets2011Corpus.7We observed 65 languages in roughly 10 milliontweets.
We calculated two other proportions usingauxiliary data:8 (1) the proportion of Wikipedia arti-cles written in each language, and (2) the proportionof speakers that speak each language.
We use theseproportions to measure a language?s relative repre-sentation on Twitter: we divide the tweet-proportionby the Wikipedia and speaker proportions.
Table 6shows some of the most over-represented Twitterlanguages compared to Wikipedia.
E.g., Indonesianis predicted to be 9.9 times more relatively com-mon on Twitter than Wikipedia.
Note these are pre-dictions only; some English tweets may be falselymarked as other languages due to English impuritiesin our training sources.
Nevertheless, the good rep-resentation of languages with otherwise scarce elec-tronic resources shows the potential of using Twitterto build language-specific social media collections.7http://trec.nist.gov/data/tweets/ This corpus, de-veloped for the TREC Microblog track (Soboroff et al, 2012), containsa two-week Twitter sample from early 2011.
We processed all tweetsthat were obtained with a ?200?
response code using the twitter-corpus-tools package.8From http://meta.wikimedia.org/wiki/List_of_Wikipedias_by_speakers_per_article71Language Num.
% of Tweets/ Tweets/Tweets Tot.
Wikip.
SpeakersIndonesian 1055 9.0 9.9 3.1Thai 238 2.0 5.7 1.9Japanese 2295 19.6 5.0 8.8Korean 446 3.8 4.0 3.2Swahili 46 0.4 3.4 0.4Portuguese 1331 11.4 3.2 2.8Marathi 58 0.5 2.9 0.4Malayalam 30 0.3 2.2 0.4Nepali 23 0.2 2.1 0.8Macedonian 61 0.5 1.9 13.9Bengali 25 0.2 1.9 0.1Turkish 174 1.5 1.7 1.1Arabic 162 1.4 1.6 0.3Chinese 346 3.0 1.4 0.2Spanish 696 5.9 1.4 0.7Telugu 39 0.3 1.4 0.3Croatian 79 0.7 1.3 6.1English 2616 22.3 1.2 2.1Table 6: Number of tweets (1000s) and % of total for lan-guages that appear to be over-represented on Twitter (vs.proportion of Wikipedia and proportion of all speakers).7 Related WorkResearchers have tackled language identification us-ing statistical approaches since the early 1990s.Cavnar and Trenkle (1994) framed LID as a textcategorization problem and made their influentialTextCat tool publicly-available.
The related problemof identifying the language used in speech signalshas also been well-studied; for speaker LID, bothphonetic and sequential information may be help-ful (Berkling et al, 1994; Zissman, 1996).
Insightsfrom LID have also been applied to related problemssuch as dialect determination (Zaidan and Callison-Burch, 2011a) and identifying the native languageof non-native speakers (Koppel et al, 2005).Recently, LID has received renewed interest as amechanism to help extract language-specific corporafrom the growing body of linguistic materials on theweb (Xia et al, 2009; Baldwin and Lui, 2010).
Workalong these lines has found LID to be far from asolved problem (Hughes et al, 2006; Baldwin andLui, 2010; Lui and Baldwin, 2011); the web in gen-eral has exactly the uneven mix of style, languages,and lengths-of-text that make the real problem quitedifficult.
New application areas have also arisen,each with their own unique challenges, such as LIDfor search engine queries (Gottron and Lipka, 2010),or person names (Bhargava and Kondrak, 2010).The multilinguality of Twitter has led to the de-velopment of ways to ensure language purity.
Rit-ter et al (2010) use ?a simple function-word-drivenfilter.
.
.
to remove non-English [Twitter] conversa-tions,?
but it?s unclear how much non-English sur-vives the filtering and how much English is lost.Tromp and Pechenizkiy (2011) and Carter et al(2013) perform Twitter LID, but only targeting sixcommon European languages.
We focus on low-resource languages, where training data is scarce.Our data and systems could enable better LIDfor services like indigenoustweets.com, whichaims to ?strengthen minority languages through so-cial media.
?8 ConclusionsLanguage identification is a key technology for ex-tracting authentic, language-specific user-generatedtext from social media.
We addressed a previouslyunexplored issue: LID performance on Twitter textin low-resource languages.
We have created andmade available a large corpus of human-annotatedtweets in nine languages and three non-Latin writ-ing systems, and presented two systems that can pre-dict tweet language with very high accuracy.9 Whilechallenging, LID on Twitter is perhaps not as diffi-cult as first thought (Carter et al, 2013), althoughperformance depends on the amount of training data,the length of the tweet, and whether we aggregateinformation across multiple tweets by the same user.Our next step will be to develop a similar approachto handle romanized text.
We also plan to developtools for identifying code-switching (switching lan-guages) within a tweet.AcknowledgmentsWe thank Chris Callison-Burch for his help with thecrowdsourcing.
The first author was supported by theNatural Sciences and Engineering Research Council ofCanada.
The third author was supported by the BOLTprogram of the Defense Advanced Research ProjectsAgency, Contract No.
HR0011-12-C-00159The annotated corpus and PPM system are available onlineat: http://apl.jhu.edu/?paulmac/lid.html72ReferencesTimothy Baldwin and Marco Lui.
2010.
Language iden-tification: The long and the short of the matter.
InProc.
HLT-NAACL, pages 229?237.Dario Benedetto, Emanuele Caglioti, and Vittorio Loreto.2002.
Language trees and zipping.
Physical ReviewLetters, 88(4):2?5.Kay Berkling, Takayuki Arai, and Etienne Barnard.1994.
Analysis of phoneme-based features for lan-guage identification.
In Proc.
ICASSP, pages 289?292.Aditya Bhargava and Grzegorz Kondrak.
2010.
Lan-guage identification of names with SVMs.
In Proc.HLT-NAACL, pages 693?696.Andrej Bratko, Gordon V. Cormack, Bogdan Filipic,Thomas R. Lynam, and Blaz Zupan.
2006.
Spamfiltering using statistical data compression models.JMLR, 6:2673?2698.John D. Burger, John Henderson, George Kim, and GuidoZarrella.
2011.
Discriminating gender on Twitter.
InProc.
EMNLP, pages 1301?1309.Chris Callison-Burch and Mark Dredze.
2010.
Creatingspeech and language data with amazon?s mechanicalturk.
In Proc.
NAACL HLT 2010 Workshop on Cre-ating Speech and Language Data with Amazon?s Me-chanical Turk, pages 1?12.Simon Carter, Wouter Weerkamp, and Manos Tsagkias.2013.
Microblog Language Identification: Overcom-ing the Limitations of Short, Unedited and IdiomaticText.
Language Resources and Evaluation Journal.
(forthcoming).William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Proc.
Symposium onDocument Analysis and Information Retrieval, pages161?175.John G. Cleary, Ian, and Ian H. Witten.
1984.
Datacompression using adaptive coding and partial stringmatching.
IEEE Transactions on Communications,32:396?402.Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.2011.
Discovering sociolinguistic associations withstructured sparsity.
In Proc.
ACL, pages 1365?1374.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
JMLR, 9:1871?1874.Clayton Fink, Jonathon Kopecky, Nathan Bos, and MaxThomas.
2012.
Mapping the Twitterverse in the devel-oping world: An analysis of social media use in Nige-ria.
In Proc.
International Conference on Social Com-puting, Behavioral Modeling, and Prediction, pages164?171.Eibe Frank, Chang Chui, and Ian H. Witten.
2000.
Textcategorization using compression models.
In Proc.DCC-00, IEEE Data Compression Conference, Snow-bird, US, pages 200?209.
IEEE Computer SocietyPress.Rayid Ghani, Rosie Jones, and Dunja Mladenic.
2001.Automatic web search query generation to create mi-nority language corpora.
In Proceedings of the 24thannual international ACM SIGIR conference on Re-search and development in information retrieval, SI-GIR ?01, pages 432?433, New York, NY, USA.
ACM.Thomas Gottron and Nedim Lipka.
2010.
A comparisonof language identification approaches on short, query-style texts.
In Proc.
ECIR, pages 611?614.Baden Hughes, Timothy Baldwin, Steven Bird, JeremyNicholson, and Andrew Mackinlay.
2006.
Reconsid-ering language identification for written language re-sources.
In Proc.
LREC, pages 485?488.Ann Irvine and Alexandre Klementiev.
2010.
Using Me-chanical Turk to annotate lexicons for less commonlyused languages.
In Proc.
NAACL HLT 2010 Workshopon Creating Speech and Language Data with Ama-zon?s Mechanical Turk, pages 108?113.Emre Kiciman.
2010.
Language differences and meta-data features on Twitter.
In Proc.
SIGIR 2010 WebN-gram Workshop, pages 47?51.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.Determining an author?s native language by mining atext for errors.
In Proc.
KDD, pages 624?628.Marco Lui and Timothy Baldwin.
2011.
Cross-domainfeature selection for language identification.
In Proc.IJCNLP, pages 553?561.Paul McNamee.
2005.
Language identification: a solvedproblem suitable for undergraduate instruction.
J.Comput.
Sci.
Coll., 20(3):94?101.D.
Pavelec, L. S. Oliveira, E. Justino, F. D. Nobre Neto,and L. V. Batista.
2009.
Compression and stylometryfor author identification.
In Proc.
IJCNN, pages 669?674, Piscataway, NJ, USA.
IEEE Press.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Unsu-pervised modeling of twitter conversations.
In Proc.HLT-NAACL, pages 172?180.Alan Ritter, Colin Cherry, and William B. Dolan.
2011.Data-driven response generation in social media.
InProc.
EMNLP, pages 583?593.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In Proc.
EMNLP, pages 254?263.Ian Soboroff, Dean McCullough, Jimmy Lin, Craig Mac-donald, Iadh Ounis, and Richard McCreadie.
2012.Evaluating real-time search over tweets.
In Proc.ICWSM.73William John Teahan.
2000.
Text classification andsegmentation using minimum cross-entropy.
In Proc.RIAO, pages 943?961.Erik Tromp and Mykola Pechenizkiy.
2011.
Graph-based n-gram language identication on short texts.
InProc.
20th Machine Learning conference of Belgiumand The Netherlands, pages 27?34.Fei Xia, William Lewis, and Hoifung Poon.
2009.
Lan-guage ID in the context of harvesting language data offthe web.
In Proc.
EACL, pages 870?878.Omar F. Zaidan and Chris Callison-Burch.
2011a.The arabic online commentary dataset: an annotateddataset of informal arabic with high dialectal content.In Proc.
ACL, pages 37?41.Omar F. Zaidan and Chris Callison-Burch.
2011b.Crowdsourcing translation: Professional quality fromnon-professionals.
In Proc.
ACL, pages 1220?1229.Marc A. Zissman.
1996.
Comparison of four ap-proaches to automatic language identification of tele-phone speech.
IEEE Transactions on Speech and Au-dio Processing, 4(1):31?44.74
