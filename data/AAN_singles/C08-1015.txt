Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 113?120Manchester, August 2008Learning Reliable Information for Dependency Parsing AdaptationWenliang Chen?, Youzheng Wu?, Hitoshi Isahara?
?Language Infrastructure Group?Spoken Language Communication Group, ATR?Machine Translation GroupNational Institute of Information and Communications Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289{chenwl, youzheng.wu, isahara}@nict.go.jpAbstractIn this paper, we focus on the adaptationproblem that has a large labeled data in thesource domain and a large but unlabeleddata in the target domain.
Our aim is tolearn reliable information from unlabeledtarget domain data for dependency pars-ing adaptation.
Current state-of-the-art sta-tistical parsers perform much better forshorter dependencies than for longer ones.Thus we propose an adaptation approachby learning reliable information on shorterdependencies in an unlabeled target datato help parse longer distance words.
Theunlabeled data is parsed by a dependencyparser trained on labeled source domaindata.
The experimental results indicatethat our proposed approach outperformsthe baseline system, and is better than cur-rent state-of-the-art adaptation techniques.1 IntroductionDependency parsing aims to build the dependencyrelations between words in a sentence.
Thereare many supervised learning methods for traininghigh-performance dependency parsers(Nivre et al,2007), if given sufficient labeled data.
However,the performance of parsers declines when we are inthe situation that a parser is trained in one ?source?domain but is to parse the sentences in a second?target?
domain.
There are two tasks(Daum?e III,2007) for the domain adaptation problem.
Thefirst one is that we have a large labeled data in thesource domain and a small labeled data in targetc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.domain.
The second is similar, but instead of hav-ing a small labeled target data, we have a large butunlabeled target data.
In this paper, we focus onthe latter one.Current statistical dependency parsers performworse while the distance of two words is becominglonger for domain adaptation.
An important char-acteristic of parsing adaptation is that the parsersperform much better for shorter dependencies thanfor longer ones (the score at length l is much higherthan the scores at length> l ).In this paper, we propose an approach by usingthe information on shorter dependencies in auto-parsed target data to help parse longer distancewords for adapting a parser.
Compared with theadaptation methods of Sagae and Tsujii (2007) andReichart and Rappoport (2007), our approach usesthe information on word pairs in auto-parsed datainstead of using the whole sentences as newly la-beled data for training new parsers.
It is difficultto detect reliable parsed sentences, but we can findrelative reliable parsed word pairs according to de-pendency length.
The experimental results showthat our approach significantly outperforms base-line system and current state of the art techniques.2 Motivation and prior workIn dependency parsing, we assign head-dependentrelations between words in a sentence.
A simpleexample is shown in Figure 1, where the arc be-tween a and hat indicates that hat is the head of a.Current statistical dependency parsers performbetter if the dependency lengthes are shorter (Mc-Donald and Nivre, 2007).
Here the length of thedependency from word wito word wjis simplyequal to |i ?
j|.
Figure 2 shows the results (F1113The  boy  saw    a       red       hat    .Figure 1: An example for dependency relations.20304050607080901000  2  4  6  8  10  12  14  16  18  20F1Dependency LengthsameDomaindiffDomainFigure 2: The scores relative to dependency length.?SameDomain?
refers to training and testing in thesame domain, and ?diffDomain?
refers to trainingand testing in two domains (domain adaptation).score)1on our testing data, provided by a deter-ministic parser, which is trained on labeled sourcedata.
Comparing two curves at the figure, we findthat the scores of diffDomain decreases muchmoresharply than the scores of sameDomain, when de-pendency length increases.
The score decreasesfrom about 92% at length 1 to 50% at 7.
Whenlengthes are larger than 7, the scores are below50%.
We also find that the score at length l is muchhigher (around 10%) than the score at length l + 1from length 1 to 7.
There is only one exception thatthe score at length 4 is a little less than the score atlength 5.
But this does not change so much and thescores at length 4 and 5 are much higher than theone at length 6.Two words (word wiand word wj) having adependency relation in one sentence can be adja-cent words (word distance = 1), neighboring words(word distance = 2), or the words with distance >2 in other sentences.
Here the distance of wordpair (word wiand word wj) is equal to |i ?
j|.
Forexample, ?a?
and ?hat?
has dependency relation inthe sentence at Figure 1.
They can also be adjacentwords in the sentence ?The boy saw a hat.?
andthe words with distance = 3 in ?I see a red beauti-ful hat.?.
This makes it possible for the word pairswith different distances to share the information.According to the above observations, we present1F1= 2 ?
precision ?
recall/(precision + recall) whereprecision is the percentage of predicted arcs of length d thatare correct and recall is the percentage of gold standard arcsof length d that are correctly predicted.an idea that the information on shorter depen-dencies in auto-parsed target data is reliable forparsing the words with longer distance for do-main adaptation.
Here, ?shorter?
is not exactlyshort.
That is to say, the information on depen-dency length l in auto-parsed data can be used tohelp parse the words whose distances are longerthan l when testing, where l can be any number.We do not use the dependencies whose lengthesare too long because the accuracies of long depen-dencies are very low.In the following content, we demonstrate ouridea with an example.
The example shows how touse the information on length 1 to help parse twowords whose distance is longer than 1.
Similarly,the information on length l can also be used to helpparse the words whose distance is longer than l.Figure 2 shows that the dependency parser per-forms best at tagging the relations between adja-cent words.
Thus, we expect that dependencies ofadjacent words in auto-parsed target data can pro-vide useful information for parsing words whosedistances are longer than 1.
We suppose that ourtask is Chinese dependency parsing adaptation.Here, we have two words ??
?JJ(large-scale)?and ???NN(exhibition)?.
Figure 3 showsthe examples in which word distances of thesetwo words are different.
For the sentences inthe bottom part, there is a ambiguity of ?JJ+ NN1 + NN2?
at ??
?JJ(large-scale)/??NN(art)/?
?NN(exhibition)?, ???JJ(large-scale)/?
?NN(culture)/?
?NN(art)/??NN(exhibition)?
and ???JJ(large-scale)/??NR(China)/?
?NN(culture)/?
?NN(art)/??NN(exhibition)?.
Both NN1 and NN2 could bethe head of JJ.
In the examples in the upper part,???JJ(large-scale)?
and ??
?NN(exhibition)?are adjacent words, for which current parserscan work well.
We use a parser to parse thesentences in the upper part.
???(exhibition)?
isassigned as the head of ???(large-scale)?.
Thenwe expect the information from the upper partcan help parse the sentences in the bottom part.Now, we consider what a learning model coulddo to assign the appropriate relation between ???(large-scale)?
and ???(exhibition)?
in thebottom part.
We provide additional informationthat ???(exhibition)?
is the possible head of ???(large-scale)?
in the auto-parsed data (the upperpart).
In this way, the learning model may use thisinformation to make correct decision.114A1)?///?A2)?///?B1)?
!
"//#$//%&'()*+,)?B3)?-./012#$345678//9 /12/#$//:B2)?;<=>?
@AB//12/#$//:Figure 3: Examples for ???(large-scale)?
and???(exhibition)?.
The upper part (A) refers tothe sentences from unlabeled data and the bottompart (B) refers to the sentences waiting for parsing.Up to now, we demonstrate how to use the in-formation on length 1.
Similarly, we can use theinformation on length 2, 3, .
.
.
.
By this way, wepropose an approach by exploiting the informationfrom a large-scale unlabeled target data for depen-dency parsing adaptation.In this paper, our approach is to use unlabeleddata for parsing adaptation.
There are several stud-ies relevant to ours as described below.CoNLL 2007(Nivre et al, 2007) organized ashared task for domain adaptation without anno-tated data in new domain.
The labeled data wasfrom the Wall Street Journal, the development datawas from biomedical abstracts, and the testing datawas from chemical abstracts and parent-child dia-logues.
Additionally, a large unlabeled corpus wasprovided.
The systems by Sagae and Tsujii (2007),Attardi et al (2007), and Dredze et al (2007) per-formed top three in the shared task.Sagae and Tsujii (2007) presented a proceduresimilar to a single iteration of co-training.
Firstly,they trained two parsers on labeled source data.Then the two parsers were used to parse the sen-tences in unlabeled data.
They selected only iden-tical parsing results produced by the two parsers.Finally, they retrained a parser on newly parsedsentences and the original labeled data.
They per-formed the highest scores for this track.
Attardiet al (2007) presented a procedure with correct-ing errors by a revision techniques.
Dredze etal.
(2007) submitted parsing results without adap-tation.
They declared that it was difficult to signif-icantly improve performance on any test domainbeyond that of a state-of-the-art parser.
Their er-ror analysis suggested that the primary cause ofloss from adaptation is because of differences inthe annotation guidelines.
Without specific knowl-edge of the target domain?s annotation standards,significant improvement can not be made.Reichart and Rappoport (2007) studied self-training method for domain adaptation (The WSJdata and the Brown data) of phrase-based parsers.McClosky et al (2006) presented a successful in-stance of parsing with self-training by using a re-ranker.
Both of them used the whole sentences asnewly labeled data for adapting the parsers, whileour approach uses the information on word pairs.Chen et al (2008) presented an approach byusing the information of adjacent words for in-domain parsing.
As Figure 2 shows, the scorecurves of sameDomain (in-domain) parsing anddiffDomain (out-domain) parsing are quite differ-ent.
Our work focuses on parsing adaptation and isbased on the fact that current parsers performmuchbetter for shorter dependencies than for longerones.
This causes that our work differs in thatwe use the information on shorter dependenciesin auto-parsed target data to help parse the wordswith longer distance for parsing adaptation.
In thispaper, ?shorter?
and ?longer?
are relative.
Lengthl is relatively shorter than length l + 1, where l canbe any number.3 The parsing approachIn this paper, we choose the model described byNivre (2003) as our parsing model.
It is a deter-ministic parser and works quite well in the shared-task of CoNLL2006(Nivre et al, 2006).3.1 The parsing modelThe Nivre (2003) model is a shift-reduce type al-gorithm, which uses a stack to store processedtokens and a queue to store remaining input to-kens.
It can perform dependency parsing in O(n)time.
The dependency parsing tree is built fromatomic actions in a left-to-right pass over the in-put.
The parsing actions are defined by four opera-tions: Shift, Reduce, Left-Arc, and Right-Arc, forthe stack and the queue.
TOP is the token on topof the stack and NEXT is next token in the queue.The Left-Arc and Right-Arc operations mean thatthere is a dependency relation between TOP andNEXT.The model uses a classifier to produce a se-quence of actions for a sentence.
In this paper,we use the SVM model.
And LIBSVM(Chang andLin, 2001) is used in our experiments.Note that the approach (see section 4)wepresent in this paper can also be applied to115other parsers, such as the parser by Yamada andMatsumoto (2003), or the one by McDonald etal.
(2006).3.2 Parsing with basic featuresThe parser is a history-based parsing model, whichrelies on features of the parsed tokens to predictnext parsing action.
We represent basic featuresbased on words and part-of-speech (POS) tags.The basic features are listed as follows:?
Lexical Features on TOP: the word of TOP, the word ofthe head of TOP, and the words of leftmost and right-most dependent of TOP.?
Lexical Features on NEXT: the word of NEXT and theword of the token immediately after NEXT in the orig-inal input string.?
POS features on TOP: the POS of TOP, the POS of thetoken immediately below TOP, and the POS of leftmostand rightmost dependent of TOP.?
POS features on NEXT: the POS of NEXT, the POS ofnext three tokens after NEXT, and the POS of the tokenimmediately before NEXT in original input string.Based on the above parsing model and basic fea-tures, we train a basic parser on annotated sourcedata.
In the following content, we call this parserBasic Parser.4 Domain adaptation with shorterdependencyThis section presents our adaptation approach byusing the information based on relative shorter de-pendencies in auto-parsed data to help parse thewords whose distances are longer.
Firstly, we usethe Basic Parser to parse all the sentences in un-labeled target data.
Then we explore reliable in-formation based on dependency relations in auto-parsed data.
Finally, we incorporate the featuresbased on reliable information into the parser to im-prove performance.4.1 Extracting word pairs from auto-parseddataIn this section, we collect word pairs from theauto-parsed data.
At first, we collect the wordpairs with length 1.
In a parsed sentence, if twowords have dependency relation and their worddistance is 1, we will add this word pair into thelist Ldepand count its frequency.
We also con-sider the direction, LA for left arc and RA forright arc.
For example, ???(large-scale)?
and???(exhibition)?
are adjacent words in the sen-tence ???(We)/??(held)/??(large-scale)/??(exhibition)/b?
and have a left dependency arcassigned by the Basic Parser.
The word pair???(large-scale)-??(exhibition)?
with ?LA?is added into Ldep.Similarly, we collect the pairs whose word dis-tances are longer than 1.
In Ldep, with length l anddirection dr(LA or RA), the pair puhas f reql(pu:dr).
For example, f req2(pu: LA) = 3 refers tothe word pair puwith left arc(LA) occurs 3 timesin the auto-parsed data when two words?
distanceis 2.
Because figure 2 shows that the accuraciesof long dependencies are low, we only collect thepairs whose distances are not larger than a prede-fined length lmax.4.2 The adaptation approachThe word pair ptis the pair < wi,wj>.4.2.1 The information on shorter distancesIf the distance of ptis d, we will use the pairswhose lengthes are less than d. It results in thewords with different distances using different setof word pairs in Ldep.
For example, if d is 5, wecan use the pairs with dependency lengthes from 1to 4 in Ldep.
The information is represented by theequation as follows:Id(pt: dr) =????????
?0 pt< Ldepf req1(pt: dr) d = 1?d?1l=1f reql(pt: dr) d > 1(1)4.2.2 Classifying into bucketsAccording to Id(pt: dr), word pairs are groupedinto different buckets as follows:Bucketd(pt: dr) =????????????????
?B0Id(pt: dr) = 0B10 < Id(pt: dr) ?
f1.
.
.Bnfn?1< Id(pt: dr) ?
fnBafn< Id(pt: dr)(2)where, f1, f2, ..., fnare the thresholds.
For exam-ple, I3(??-??
:LA) is 20, f3= 15 and f4= 25.Then it is grouped into the bucket B4.
We setf1= 2, f2= 8, and f3= 15 in the experiments.4.2.3 Parsing with the adapting FeaturesBased on the buckets of word pairs, we representnew features on labeled source data for the parser.We call these new features adapting features.
Ac-cording to different word distances between TOPand NEXT, the features are listed at Table 1.
Sowe have 8 types of the adapting features, includ-ing 2 types for distance=1, 3 types for distance=2,and 3 types for distance?3.
Each feature is format-ted as ?DistanceType:FeatureType:Bucket?, whereDistanceType is D1, D2, or D3 corresponding to116three distances, FeatureType is FB0, FB1, or FB 1corresponding to three positions.
Here, if a wordpair has two dependency directions in Ldep, we willchoose the direction having higher frequency.Then using the parsing model of Nivre (2003),we train a new parser based on the adapting fea-tures and basic features.distance FB 1 FB0 FB1=1 + +=2 + + +?3 + + +Table 1: Adapting features.
FB0 refers to thebucket of the word pair of TOP and NEXT, FB1refers to the bucket of the word pair of TOP andnext token after NEXT, and FB 1 refers to thebucket of the word pair of TOP and the token im-mediately before NEXT.
?+?
refers to this itemhaving this type of feature.4.2.4 An exampleWe show an example for representing theadapting features.
For example, we have thestring ???JJ(large-scale)/??NN(culture)/??NN(art)/??NN(exhibition)/b?.
And ???(large-scale)?
is TOP and ???(exhibition)?
isNEXT.
Because the distance of TOP and NEXT is3, we have three features.
We suppose that (FB0)the bucket of the word pair (???-???)
of TOPand NEXT is bucket B4, (FB1) the bucket of theword pair (???-b?)
of TOP and next token af-ter NEXT is bucket B0, and (FB 1) the bucket ofthe word pair (???-???
)of TOP and the to-ken immediately before NEXT is bucket B1.
Then,we have the features: ?D3:FB0:B4?, ?D3:FB1:B0?,and ?D3:FB 1:B1?.4.3 Adaptation for unknown word2The unknown word problem is an important issuefor domain adaptation(Dredze et al, 2007).
Ourapproach can work for improving performance ofparsing unknown word pairs in which there isat least one unknown word.
We collect wordpairs including unknown word pairs at Section 4.1.Then unknown word pairs in testing data are alsomapped into one of the buckets via Equation (2).So known word pairs can share the features withunknown word pairs.2An unknown word is a word that is not included in train-ing data.5 Experimental setupCoNLL 2007(Nivre et al, 2007) organized the do-main adaptation task and provided a data set inEnglish.
However, the data set had differencesbetween the annotation guidelines in source andtarget domains.
Without specific knowledge ofthe target domain?s annotation standards, signifi-cant improvement can not be made(Dredze et al,2007).
In this paper, we discussed the situation thatthe data of source and target domains were anno-tated under the same annotation guideline.
So weused a data set converted from Penn Chinese Tree-bank (CTB)3.Labeled data: the CTB(V5.0) was used in ourexperiments.
The data set was converted by thesame rules for conversion as Chen et al (2008)did.
We used files 1-270, 400-554, and 600-931as source domain training data (STrain), files 271-300 as source domain testing data (STest) and files590-596 as target domain testing data (TTest).
Weused the gold standard segmentation and POS tagsin the CTB.
The target domain data was from Sino-rama magazine, Taiwan and the source domaindata was mainly from Xinhua newswire, mainlandof China.
The genres of these two parts were quitedifferent.
Table 2 shows the statistical informationof the data sets.
Given the words of the STraindata, TTest included 30.79% unknown words.
Wealso checked the distribution of POS tags.
The dif-ference was large, too.Unlabeled data: three data sets were used inour experiments, including the PFR data (5.44Mwords), the CKIP data (5.44M words), and theSINO data (25K words).
The PFR corpus4in-cluded the documents from People?s Daily at 1998and we used about 1/3 of all sentences.
The CKIP5corpus was used for SIGHAN word segmentationbakeoff 2005.
To simplify, we used their segmen-tation.
The SINO data was the files 1001-1151 ofCTB, also from Sinorama magazine, the same asour testing target data.
We removed the annotationtags from the SINO data.
Among the three unla-beled data, the SINO data was closest to testingtarget data because they came from the same re-source.
Table 2 lists the information of data sets.From the table, we found that the PFR data was3More detailed information can be found athttp://www.cis.upenn.edu/?chinese/.4More detailed information can be found athttp://www.icl.pku.edu.5More detailed information can be found athttp://rocling.iis.sinica.edu.tw/CKIP/index.htm117Num Of Words Unknown Word RateSTrain 17983 -STest 1829 9.73TTest 1783 30.79CKIP 140k -STest 1829 11.42TTest 1783 8.63PFR 123k -STest 1829 8.58TTest 1783 15.64Table 2: The information of the data setscloser to source domain and the CKIP data wascloser to target domain.To assign POS tags for the unlabeled data, weused the package TNT (Brants, 2000) to train aPOS tagger on training data.
Because the PFR dataand the CTB used different POS standards, we didnot use the POS tags in the PFR data.We measured the quality of the parser by the un-labeled attachment score (UAS), i.e., the percent-age of tokens with correct head.
We also reportedthe accuracy of ROOT.6 ResultsIn the following content, OURS refers to our pro-posed approach.
The baseline system refers to theBasic Parser.6.1 Basic experimentsIn this section, we examined the performance ofbaseline systems and our proposed approach withdifferent unlabeled data sets.Table 3 shows the experimental results, where?OURS with SINO(GOLD)?
refers to the parserusing gold standard POS tags, and ?OURS withSINO(AUTO)?
refers to the parser using auto-assigned POS tags.
From the two results of base-line, we found that the parser performed very dif-ferently in two domains by 8.24%.With the help of SINO(AUTO), OURS pro-vided 1.11% improvement for UAS and 6.16%for ROOT.
If we used gold standard POS tags,the score was 78.40% for UAS (1.34% improve-ment), and 65.40% for ROOT (6.64% improve-ment).
By using the SINO data, our approachachieved significant improvements over baselinesystem.
It was surprised that OURS with CKIPachieved 78.30% score, just a little lower than theone with SINO(GOLD).
The reason may be thatthe size of the CKIP data was much bigger thanthe SINO data.
So we can obtain more word pairsfrom the CKIP data.
The parser achieved 0.30%Data UAS ROOTbaseline(STest) 85.30 88.21baseline(TTest) 77.06 58.76OURS with SINO(GOLD) 78.40(+1.34) 65.40OURS with SINO(AUTO) 78.17(+1.11) 64.92OURS with CKIP 78.30(+1.24) 65.87OURS with PFR 77.36(+0.30) 63.03Table 3: Basic resultslmaxSINO(GOLD) SINO(AUTO)1 77.84 77.803 78.03 77.955 78.22 78.177 78.40 78.119 78.38 78.13?
78.35 78.09Table 4: The effect of different lmaximprovement with PFR.
Even though the size ofthe SINO data was smaller, the parser performedwell with its help.These results indicated that we should collectthe unlabeled data that is closer to target domainor larger.
The improvements of OURS with CKIPand OURS with SINO were significant in one-tailpaired t-test (p < 10?5).6.2 The effect of different lmaxTable 4 shows the experimental results, where lmaxis described at Section 4.1.
With SINO(GOLD),our parser performed best at lmax= 7.
Andwith SINO(AUTO), it performed best at lmax= 5.These indicated that our approach can incorpo-rate pairs with different lengthes to improve per-formance.
We also found that the long dependen-cies were not reliable, as the curve (diffDomain)of Figure 2 showed that the scores were less than50% when lengthes were larger than 8.6.3 Comparison of other systemsIn this section, we turned to compare our approachwith other methods.
We implemented two sys-tems: SelfTrain and CoTrain.
The SelfTrain sys-tem was following to the method described byReichart and Rappoport (2007) and randomly se-lected new auto-parsed sentences.
The CoTrainsystem was similar to the learning scheme de-scribed by Sagae and Tsujii (2007).
However, wedid not use the same parsing algorithms as theones used by Sagae and Tsujii (2007).
Firstly, we118Method UAS ROOTbaseline 77.06 58.76SelfTrain 77.44 60.18CoTrain 77.57 60.81OURS 78.30 65.87Table 5: The results of several adaptation methodswith CKIPtrained a forward parser (same as our baseline sys-tem) and a backward parser.
Then the identicalparsed sentences by the two parsers were selectedas newly labeled data.
Finally, we retrained a for-ward parser with new training data.
We selectedthe sentences having about 200k words from theCKIP data as newly labeled data for the SelfTrainand CoTrain systems.Table 5 shows the experimental results.
Bothsystems provided about 0.4%-0.5% improvementover baseline system.
Our approach performedbest among all systems.
Another problem wasthat the time for training the SelfTrain and CoTrainsystems became much longer because they almostused double size of training data.7 AnalysisIn this section, we try to understand the benefit inour proposed adaptation methods.
Here, we com-pare OURS?s results with baseline?s.7.1 Improvement relative to dependencylengthWe presented an idea that using the information onshorter dependencies in auto-parsed target data tohelp parse the words with longer distance for do-main adaptation.
In this section, we investigatedhow our approach performed for parsing longerdistance words.
Figure 4 shows the improvementrelative to dependency length.
From the figure, wefound that our approach always performed betterthan baseline when dependency lengthes were 1-7.Especially, our approach achieved improvementsby 2.58% at length 3, 5.38% at 6, and 3.67% at7.
For longer ones, the improvement was not sta-ble.
One reason may be that the numbers of longerones were small.
Another reason was that parsinglong distance words was very difficult.
However,we still found that our approach did improve theperformance for longer ones, by performing betterat 8 points and worse at 5 points when length wasnot less than 8.1020304050607080901000  2  4  6  8  10  12  14  16  18  20F1Dependency LengthbaselineOURSFigure 4: Performance as a function of dependencylength7.2 Improvement relative to unknown wordsThe unknown word problem is an important is-sue for adaptation.
Our approach can partially re-lease the unknown word problem.
We listed thedata of the numbers of unknown words from 0 to8 because the number of sentences was very smallfor others.
We grouped each sentence into one ofthree classes: (Better) those where our approach?sscore increased relative to the baseline?s score,(NoChange) those where the score remained thesame, and (Worse) those where the score had a rel-ative decrease.
We added another class (NoWorse)by merging Better and NoChange.Figure 5 shows the experimental results, wherex axis refers to the number of unknown words inone sentence and y axis refers to how many per-cent the class has.
For example, for the sentenceshaving 5 unknown words, about 45.45% improved,22.73% became worse, 31.82% kept unchanged,and 77.27% did not become worse.
The NoWorsecurve showed that regardless of the number of un-known words in a sentence, there was more than60% chance that our approach did not harm the re-sult.
The Better curve and Worse curve showedthat our approach always provided better results.Our approach achieved most improvement for themiddle ones.
The reason was that parsing the sen-tence having too many unknown words was verydifficult.7.3 Improvement relative to POS pairsIn this section, we listed the improvements rela-tive to POS tags of paired words having a depen-dency relation.
Table 6 shows the accuracies ofbaseline and OURS on TOP 20 POS pairs (or-dered by the frequencies of their occurrences intesting data), where ?A1?
refers to the accuracyof baseline, ?A2?
refers to the accuracy of OURS,and ?Pairs?
is the POS pairs of dependent-head.1190.10.20.30.40.50.60.70.80.90  1  2  3  4  5  6  7  8PercentageNumber of unknown wordsBetterNoChangeWorseNoWorseFigure 5: Performance as a function of number ofunknown wordsPairs A1 A2(A2-A1) Pairs A1 A2(A2-A1)NN-VV 79.61 81.90(+2.29) DEG-NN 94.74 94.74(=)VV-VV 50.00 50.40(+0.40) CD-M 96.77 97.85(+1.08)NN-NN 86.08 87.26(+1.18) NN-P 76.92 76.92(=)AD-VV 91.01 91.01(=) JJ-NN 92.11 94.74(+2.63)P-VV 68.60 70.25(+1.65) AD-VA 98.55 98.55(=)DEC-NN 97.48 98.32(+0.84) NN-VA 78.95 84.21(+5.26)NR-VV 81.98 81.98(=) NN-DEG 96.43 94.64(-1.79)VV-DEC 74.07 73.15(-0.92) VV-VC 40.82 46.94(+6.12)NR-NN 87.85 87.85(=) AD-VC 95.92 93.88(-2.04)NN-VC 90.91 91.92(+1.01) VA-VV 60.87 67.39(+6.52)Table 6: Improvement relative to POS pairsFor example, ?NN-VV?
means that ?NN?
is thePOS of the dependent and ?VV?
is the POS of thehead.
And baseline yielded 79.61% accuracy andOURS yielded 81.90% (2.29% higher) on ?NN-VV?.
From the table, we found that our approachworked well for most POS pairs (better for elevenpairs, no change for six, and worse for three).8 ConclusionThis paper presents a simple but effective approachto adapt dependency parser by using unlabeled tar-get data.
We extract the information on shorter de-pendencies in an unlabeled data parsed by a basicparser to help parse longer distance words.
Theexperimental results show that our approach sig-nificantly outperforms baseline system and currentstate of the art adaptation techniques.There are a lot of ways in which this researchcould be continued.
First, we can apply our ap-proach to other languages because our approach isindependent on language.
Second, we can enlargethe unlabeled data set to obtain more word pairs toprovide more information for the parsers.ReferencesAttardi, Giuseppe, Felice Dell?Orletta, Maria Simi,Atanas Chanev, and Massimiliano Ciaramita.
2007.Multilingual dependency parsing and domain adap-tation using DeSR.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages1112?1118.Brants, T. 2000.
TnT?a statistical part-of-speech tag-ger.
Proceedings of the 6th Conference on AppliedNatural Language Processing, pages 224?231.Chang, C.C.
and C.J.
Lin.
2001.
LIBSVM: a libraryfor support vector machines.
Software available athttp://www.
csie.
ntu.
edu.
tw/cjlin/libsvm.Chen, W., D. Kawahara, K. Uchimoto, Y. Zhang, andH.
Isahara.
2008.
Dependency parsing with shortdependency relations in unlabeled data.
In Proceed-ings of IJCNLP 2008.Daum?e III, Hal.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of ACL 2007, Prague, CzechRepublic.Dredze, Mark, John Blitzer, Partha Pratim Taluk-dar, Kuzman Ganchev, Jo?ao Graca, and FernandoPereira.
2007.
Frustratingly hard domain adap-tation for dependency parsing.
In Proceedings ofthe CoNLL Shared Task Session of EMNLP-CoNLL2007, pages 1051?1055.McClosky, D., E. Charniak, and M. Johnson.
2006.Reranking and self-training for parser adaptation.
InProceedings of Coling-ACL, pages 337?344.McDonald, Ryan and Joakim Nivre.
2007.
Character-izing the errors of data-driven dependency parsingmodels.
In Proceedings of EMNLP-CoNLL, pages122?131.McDonald, Ryan, Kevin Lerman, and FernandoPereira.
2006.
Multilingual dependency analysiswith a two-stage discriminative parser.
In Proceed-ings of CoNLL-X, New York City, June.Nivre, J., J.
Hall, J. Nilsson, G. Eryigit, and S Mari-nov. 2006.
Labeled pseudo-projective dependencyparsing with support vector machines.
In CoNLL-X.Nivre, Joakim, Johan Hall, Sandra K?ubler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages915?932.Nivre, J.
2003.
An efficient algorithm for projectivedependency parsing.
In Proceedings of IWPT2003,pages 149?160.Reichart, Roi and Ari Rappoport.
2007.
Self-trainingfor enhancement and domain adaptation of statisticalparsers trained on small datasets.
In Proceedings ofACL, Prague, Czech Republic, June.Sagae, Kenji and Jun?ichi Tsujii.
2007.
Dependencyparsing and domain adaptation with LR models andparser ensembles.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages1044?1050.Yamada, H. and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
InProceedings of IWPT2003, pages 195?206.120
