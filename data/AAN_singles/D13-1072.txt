Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 769?779,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsExploiting language models for visual recognitionDieu-Thu LeDISI, University of TrentoPovo, 38123, Italydle@disi.unitn.itJasper UijlingsDISI, University of TrentoPovo, 38123, Italyjrr@disi.unitn.itRaffaella BernardiDISI, University of TrentoPovo, 38123, Italybernardi@disi.unitn.itAbstractThe problem of learning language modelsfrom large text corpora has been widely stud-ied within the computational linguistic com-munity.
However, little is known about theperformance of these language models whenapplied to the computer vision domain.
In thiswork, we compare representative models: awindow-based model, a topic model, a distri-butional memory and a commonsense knowl-edge database, ConceptNet, in two visualrecognition scenarios: human action recog-nition and object prediction.
We examinewhether the knowledge extracted from textsthrough these models are compatible to theknowledge represented in images.
We de-termine the usefulness of different languagemodels in aiding the two visual recognitiontasks.
The study shows that the languagemodels built from general text corpora can beused instead of expensive annotated imagesand even outperform the image model whentesting on a big general dataset.1 IntroductionComputational linguistics have created many toolsfor automatic knowledge acquisition which havebeen successfully applied in many tasks inside thelanguage domain, such as question answering, ma-chine translation, semantic web, etc.
In this paperwe ask whether such knowledge generalizes to theobserved reality outside the language domain, wherewe use well-known image datasets as a proxy for ob-served reality.In particular, we aim to determine which languagemodel yields knowledge that is most suitable for usein Computer Vision.
Therefore we test a variety oflanguage models and a linguistically mined knowl-edge base within two computer vision scenarios:Human action recognition : Recognizing<subject, verb, object> triples based onobjects (e.g., car, horse) and scenes (the placethat the actions occur, e.g., countryside, forest,office) recognized in images.
In this scenario,we only consider images with human actionsso the ?human?
subject is always present.Objects in context : Predicting the most likelyidentity of an object given its context as ex-pressed in terms of co-occurring objects.Computer vision can greatly benefit from naturallanguage processing as learning from images re-quires a prohibitively expensive annotation effort.
Amajor goal of natural language processing is to ob-tain general knowledge from text and in this paperwe test which model provides the best knowledgefor use in the visual domain.Within the two visual scenarios, we compare threestate-of-the-art language models and a knowledgebase: (1) A window-based model, which countsco-occurrence frequencies within a fixed window;(2) R-LDA (Se?aghdha, 2010), an extension of LDAthat enables generation of joint probabilities; (3)TypeDM (Baroni and Lenci, 2010), a strong Distri-butional Memory model; (4) ConceptNet (Speer andHavasi, 2013), an automatically generated semanticgraph containing concepts with their relations.We test the language models in two ways: (1) Wedirectly compare the statistics of the linguistic mod-els with statistics extracted from the visual domain.769(2) We compare the linguistic models inside the twocomputer vision applications, leading to a direct es-timation of their usefulness.To summarize, our main research questions are:(1) Is the knowledge from language compatible withthe knowledge from vision?
(2) Can the knowl-edge extracted from language help in computer vi-sion scenarios?2 Related WorkUsing high level knowledge to aid image under-standing has become a recent interest in the com-puter vision community.
Objects, actions and scenesare detected and localized in images using low-level features.
This detection and localization pro-cess is guided by reasoning and knowledge.
Suchknowledge is employed to disambiguate locationsbetween objects in (Gupta and Davis, 2008).
Fromthe defined relationships between nouns (e.g., above,below, brighter, smaller), the system constrainswhich region in an image corresponds to which ob-ject/noun.
Similarly, (Srikanth et al 2005) ex-ploit ontologies extracted from WordNet to asso-ciate words and images and image regions.
(Yuet al 2011) employ relations between scenes andobjects introducing an active model to recognizescenes through objects.
The reasoning knowledgelimits the detector to search for an object within aparticular region rather than on the whole image.Language models have also been employed togenerate descriptive sentences for images.
(Ushikuet al 2012) introduce an online learning method formulti-keyphrase estimation to generate a sentenceusing a grammar model to describe an image.
Simi-larly, from objects and scenes detected in an image,(Yang et al 2011) estimated a sentence structure togenerate a sentence description composed of a noun,verb, scene and preposition.The studies most similar to ours are (Teo et al2012) and (Lampert et al 2009).
In (Teo et al2012), the Gigaword corpus is used to extract rela-tionships between tools and actions (e.g., knife - cut,cup - drink) by counting their co-occurences.
Theserelationships are used to constrain and select themost plausible actions within a predefined set of ac-tions in cooking videos.
Instead of using this knowl-edge as a guidance during recognition, we comparedifferent language models and build a general frame-work that is able to detect unseen actions throughtheir components (verb - object - scene), hence ourmethod does not limit the number of actions in im-ages.
(Lampert et al 2009) use attributes of nouns(e.g., an animal: white, eat fish, water, etc.).
Theycan detect animals without having seen training ex-amples by manually defining the attributes of the tar-get animal.
In this work, rather than relying on man-ual definitions, our aim is to find the best languagemodels built automatically from available corpora toextract relations from natural language.Currently, human action recognition is popularand mostly studied in video using the Bag-of-Visual-Words method (Delaitre et al 2010; Everts et al2013; Kuehne et al 2012; Reddy and Shah, 2012;Wang et al 2013).
In this method one extracts smalllocal visual patches of, say, 24 by 24 pixels by 10frames at every 12th pixel at every 5th frame.
Foreach patch local gradients or local movement (opti-cal flow) histograms are calculated.
Then these localvisual features are mapped to abstract, predefined?visual words?, previously obtained using k-meansclustering on a set of random features.
While resultsare good, there are two main drawbacks with thisapproach.
First of all, human actions are semanticand more naturally recognized through their compo-nents (human, objects, scene) rather than through abag of local gradient/motion patterns.
Hence we usea component-based method for human action recog-nition.
Second, the number of possible human ac-tions is huge (the number of objects times the num-ber of verbs).
Obtaining annotated visual examplesfor each action is therefore prohibitively expensive.So we learn from language models how componentscombine into human actions.3 Two Visual Recognition ScenariosWe now describe the two computer vision scenarios:human action recognition and objects in context.3.1 Human Action RecognitionWe want to identify a human action, defined as a<subject, verb, object> triple.
We do this by recog-nizing the human, the object, and the scene and thendetermine the most likely verb based on these com-ponents.
Scenes are only used here as features for770predicting/disambiguating the human action and thefinal task is to define the human action triple.
As inmost work in human action recognition, we simplifythe problem by considering only images in whichhuman actions occur.
This means that a human isalways present, leaving the problem of predictingthe verb given the object and the scene.
While thismay seem like a strong assumption, the possibilityof having no action in the image at all is largely un-explored in computer vision due to its difficulty.		  		!	Figure 1: Human action suggestion: based on the objectsand scenes recognized in an image, the system suggeststhe most plausible actions.
The action models provide therelationships between objects - scenes - verbs3.1.1 Human action recognition frameworkOur general action recognition framework is pre-sented in Figure 1.
Given an image, an object recog-nizer will predict the probability of each object (e.g,bike, horse) presented in that image.
Furthermore,a scene recognizer will provide the probabilities ofeach scene (e.g., countryside, suburb, forest) giventhe image.
The action model is composed of theconditional probabilities that relate verbs, objectsand scenes, which have been learned from trainingimages or language corpora.
Given the object andscene probabilities recognized in the image, the ac-tion model will guide the action prediction processand finally, the system will suggest the most properactions (e.g., ride horse, drive car).
We will now de-scribe each component in detail:?
Object and scene recognizers: To train the ob-ject recognizer, we use a set of images where ob-jects have been annotated with bounding boxes (tospecify objects?
locations).
We follow the state-of-the-art method of (Uijlings et al 2013).
Themethod is based on multiple hierarchical segmen-tations to sample a limited set of high quality ob-ject locations in terms of bounding boxes.
A Bag-of-Visual-Words method (Uijlings et al 2010) isapplied to these boxes to localize and recognizeobjects.
For the scene recognizer, we trained thesame Bag-of-Visual-Words method on completeimages on a dataset annotated with 15 scenes.
Inboth cases we use Support Vector Machines tolearn the object/scene models.
We use Platt?s sig-moid function to obtain the final conditional prob-abilities P (oj |I) and P (sk|I).?
Action models: The model captures the relation-ship between Object - Scene, Verb - Scene andVerb - Object: the probability of an object given ascene P (oj |sk), a verb given an object P (vi|oj),and a verb given a scene P (vi|sk).
In one exper-iment we learn the probabilities from the train-ing images, where each image has been anno-tated with an object, a verb (of an action) and ascene.
All three probabilities are computed usingfrequency counts in the training set, for example:P (oj |sk) =#images having oj , sk#images having sk(1)We aim to replace this learning from annotatedtraining images, which are expensive to obtain,with learning from language corpora.
The de-tails of how to extract the probability distributionsfrom language models are explained in section 4.3.1.2 Component integrationTo combine these components in the framework,we use an energy-based model (Lecun et al 2006)visualized in Figure 2, which includes the image I(an observed variable) and object O, scene S, andverb V .
This energy-based formulation allows us toset different weights for energies which come fromdisparate sources (i.e.
language and vision) usingGibbs measure.Now given an image I , we can compute the scorefunction S(aij ; I) of an action aij as:S(aij ; I) = S(vi, oj ; I) =1Zexp(?
?F?FEijF),771Figure 2: An energy-based model for action recognitionwhere we define each energy function EijF to givelower energies to correct answers and higher ener-gies to incorrect ones, with S is the set of all scenes:EijF1(O, I) = ?wF1 logP (oj |I) (2)EijF2(S, I) = ?wF2 log?S?SP (oj |S)?
P (S|I)(3)EijF3(V,O) = ?wF3 logP (vi|oj) (4)EijF4(V, S) = ?wF4 log?S?SP (vi|S)?
P (S|I)(5)Let Pi be the position of the correct action in theranked list of predicted actions for a certain imageIi.
The ranked list is sorted in the order of the scoreS.
We evaluate human action recognition in terms ofthis position average over all images, which we callAverage Ranking (AR).
Therefore we use AverageRanking as our loss-function:L(wF ) = ARN =1NN?i=0Pi.
(6)Training the energy model involves finding the fac-tors w?F that minimizes the loss:w?F = argminwFL(wF ) (7)As we have only four parameters to learn in ourenergy model, we do this by performing an ex-haustive search and cross validation.
We requirewF ?
{0.0, 0.1, 0.2, ..., 0.9} and set the constraint?F?F wF = 1.
We note that the factor graph for-mulation of our framework would allow us to usemore advanced learning algorithms.
We plan to lookinto this once the model becomes more complex byadding, for example, information about the positionof the objects and the human.3.1.3 DatasetRecently, researchers have released many im-age action datasets such as the 7 everyday ac-tions (Delaitre et al 2010), the Stanford 40 actiondataset (Yao et al 2011), the PASCAL action clas-sification competition (Everingham et al 2012), andthe 89 action dataset (Le et al 2013).
The 89 actiondataset was originally created for the recognition of20 objects.
Afterwards also actions were annotated.Therefore, the actions occurring with these objectsare mostly unbiased, unlike in other action datasets.Hence we choose to use the 89 action dataset.In the 89 action dataset, every image has been an-notated with human actions, where each action iscomposed of a verb and an object.
We addition-ally annotated every image with one of the 15 scenesfrom the 15 scene dataset (Lazebnik et al 2006).3.2 Objects in ContextOur other computer vision scenario is about objectsin context.
Context is useful in visual recognition fortwo reasons: Firstly, context can significantly reducethe number of possible object categories simplifyingthe problem.
Secondly, when the object appearanceis inconclusive for its identity, context can be usedfor disambiguation.
For example, a grey rectangleon a desk may be recognized as a pen, while a greyrectangle on a table may be recognized as a knife.As the recognition systems are not always reliable,the use of context can greatly improve results.For this scenario we choose a theoretical settingin which we want to predict the identity of one ob-ject given that the identities of all other objects inthe image are known.
We believe that our main con-clusions on the linguistic models will transfer to apractical computer vision application where visualrecognition systems predict the object identities.Formally, we can describe this scenario as fol-lows: Given an image I with N objects O ={o1, o2, ?
?
?
, oN}, we want to predict the identity ofobject oi given all other objects O \ oi.
In this paperwe use a Naive Bayes assumption, leading to:P (oi|O \ oi) =P (O \ oi|oi)?
P (oi)P (O \ oi)?
P (oi)?
?oj?O\oiP (oj |oi).
(8)772In this scenario, we need conditional relationsP (oj |oi) and priors.
We obtain these from languagedata or from images directly.3.2.1 DatasetFor the objects in context scenario, we use theSUN object dataset (Xiao et al 2010), which con-tains more than 16 thousand images, more than79,000 objects whose locations are annotated us-ing polygons.
The dataset has been annotated byvarious people who could choose their own objectcategories, leading to duplicate categories such as?building?
and ?buildings?, ?person?
and ?personwalking?.
Furthermore, for some images large partsare not annotated leading to an incomplete context.We therefore cleaned the object categories (mappingfrom around 7,500 objects to over 700 unique objectcategories) and considered only images whose con-tent was sufficiently annotated.In our experiments, we used the predefined train-ing and testing parts of the SUN dataset and obtainedaround 4,500 images for learning the object relationsand 10,600 images for testing the object prediction.We obtain conditional probabilities P (oj |oi) fromfrequency counts.4 Language models & distributionextractionTo extract probability distributions from texts, weuse ConceptNet, the Window2 and 20 model,TypeDM and R-LDA.
We will now describehow we estimate the four conditional probabilitiesP (V |O), P (V |S), P (O|S), P (O|O) needed in thetwo visual scenarios for each language model.4.1 ConceptNetConceptNet (Speer and Havasi, 2013) is a large se-mantic graph containing concepts and relations be-tween them.
It includes everyday basic, culturaland scientific knowledge, which have been automat-ically extracted from Internet using predefined rules.In this work we use the most current version, Con-ceptNet 5.
As it was mined from free text usingrules, the database has uncontrolled vocabulary andcontains many false/nonesense statements.To extract relations from ConceptNet5, we firstexamine all relations in the database and define thosethat are relevant to our scenarios (Figure 3).
ForFigure 3: List of relations in ConceptNetexample, for the conditional probability of objectsgiven scenes, relations such as ?At Location?, ?Lo-cated Near?
are extracted.
For the human actionrecognition scenario, we used a list of 19 objects,15 scenes and around 5 thousand verbs for comput-ing P (V |O), P (O|S), P (V |S).
For the objects incontext scenario, we used 700 objects for comput-ing P (O|O).
Examples of relations extracted fromConceptNet are illustrated in Table 1, such as: Oil -Located near - Car, Horse - Related to - Zebra.
Fromthese relations, we define the four conditional prob-abilities using their frequency counts.
For example,to compute the conditional probability of an objectgiven a scene P (oi|sj), we extract all triples havingthe form <object, rel, scene>, where ?rel?
can be?AtLocation?, ?LocatedNear?, etc.P (oi|sj) =freq(< oi, rel, sj >)?om?O freq< om, rel, sj >(9)4.2 Window modelOne of the most famous and basic statistical model isbased on counting co-occurrences within a windowof fixed width, which follows the tradition of hy-perspace analogue to language (Lund and Burgess,1996).
We took the Window2, 20 models whichhave been built in (Bruni et al 2012) using theukWaC (1.9B tokens) and Wackypedia (820M to-kens).
As the Window2 model only looks at 2 wordson the left and right of the current one, it reflectsthe relationships between words occurring near eachother, while the Window20 searches for a broaderview of how words are related to each other.
Theweights of each pairs of words are calculated usingthe Local Mutual Information (LMI).
To computethe conditional probabilities, we use the LMI scor-773LocatedNear RelatedTo UsedFor AtLocationoil car seatbelt car horse zebra plant garden bottle store liquid horse race bus city car citychair your bottom chair school horse pony sheep baa boat fish table eat off of bike street dog cityplant everywhere muzzle dog plant green sheep cloud dog companionship chair rest bird countryside dog streettrailer car dog bark bone boat ship cow bull horse riding bus travel car street chair citysalt table horse cowboy chair table horse riding chair sitting table eat meal cat store bus citystool table carriage horse dog wolf sheep farm chair sit on boat travel car street chair storepasture cow horse fence dog cat cow milk car transportation bottle hold liquid car street bicycle storecat dog whisker cat sheep lamb table desk sheep wool boat float on water bird forest chair storehorse zebra desk chair sheep wool cat feline table put thing on table eat at car city bottle storecat household train railroad dog a wolf dog canine boat travel on water cat catch mouse table kitchen chair officehorsehair horse sheep wool cat dog plant flower chair sit cow milk chair office chair cityTable 1: Examples of relations extracted from ConceptNet 5ing function provided by the models, for example:P (vi|oj) =LMIvi,oj?vm?V LMIvm,oj(10)4.3 Distributional MemoryDistributional Memory (Baroni and Lenci, 2010)(DM) is a multi-purpose framework for semanticmodeling.
This model is more complex than theWindow models because it exploits different de-grees of lexicalization for each relation.
Distribu-tional information is extracted as a set of weighted<word-link-word> tuples obtained from a depen-dency parse of corpora.
In the Window model therelation between each word pair is decided by theirco-occurences within a sliding window, while in DMthis relation is defined by distributional properties ofthe two words.
These distributional properties arebased on a syntactic relation or lexico-syntactic pat-tern that links the two words.
For example, the tu-ple <marine, use, bomb> encodes that marine co-occurs with bomb in the corpus, and the word ?use?specifies the type of the syntagmatic link.Distributional Memory contains three differentmodels, corresponding to different ways to con-struct the weighted structure through the ?link?.
Thefirst model, LexDM is the most heavily lexicalizedmodel with the most variety of links, whereas theDepDM has the minimum degree of lexicalization,thus having the smallest number of links.
TypeDM,which was reported to achieve the best performancein different tasks including selectional preferences,is laying somewhere in the middle of the other twomodels.
It shares the same lexical information as inLexDM but use a different scoring function, whichfocuses on the variety of surface forms, rather thanthe frequency of a link.
Hence we choose the bestmodel, TypeDM, to learn the relationships betweenverbs, objects and scenes.
As in the window model,we compute conditional probabilities using the LMIscores provided by the model (Equation 10).4.4 R-LDATo model the relationships between verbs, objectsand scenes, we adapt the R-LDA model (Se?aghdha,2010) (ROOTH-LDA), which has been used for theselectional preference task in order to obtain con-ditional probabilities of two words.
Each relation mof< w1, w2 > is generated by picking up a distribu-tion over topics, then both elements of the relationmshare the same topic assignment zm, which keep twodifferentw1-topic andw2-topic distributions sharingthe same topic (Figure 4).
The models are estimatedby Gibbs sampling following (Heinrich, 2004).
It isalso noted that these models are generative, hencethey also predict the probabilities of tuples that donot occur in the corpus.Figure 4: Generative graphical model of R-LDA: model-ing the relations between two wordsTo model the relations between objects and verbs,we follow the data preparation in (Le et al 2013),using the British National Corpus (BNC) which hasbeen preprocessed and parsed using TreeTagger andMaltparser.
Verbs are heads of sentences while ob-jects are either direct or indirect objects related tothose verbs by the parser.
For the relations betweenverbs and scenes, we consider also verbs as heads774Topic 8: Topic 14: Topic 0: Topic 54:Noun Verb Noun Verb Noun Noun Noun Nounpeople 0.0208 have 0.157 year 0.0154 win 0.109 attention 0.0172 study 0.01 decision 0.02 case .0176job 0.0167 work 0.108 Cup 0.0093 have 0.099 model 0.0147 research 0.0123 view 0.0244 fact .0096work 0.0156 make 0.0262 team 0.0086 beat 0 study 0.014 work 0.0111 question 0.018 question .0096class 0.014 take 0.0244 race 0.00772 take 0.025 role 0.0139 chapter 0.0085 issue 0.0124 law .0096worker 0.0123 find 0.0194 season 0.0062 lose 0.0211 account 0.013 problem 0.0075 evidence 0.0104 decision .0092staff 0.0111 pay 0.0156 time 0.006 run 0.0152 analysis 0.0123 issue 0.006 point 0.0099 time .0067group 0.0089 say 0.0146 world 0.0058 finish 0.0135 aspect 0.012 system 0.0065 reason 0.0096 issue .0062way 0.0086 get 0.0136 game 0.0055 make 0.0127 problem 0.0106 area 0.006 statement 0.0086 evidence .00617service 0.008 leave 0.0114 champion 0.0053 lead 0.0122 effect 0.0105 process 0.006 doubt 0.008 interest .0059company 0.0076 run 0.0102 seat 0.0049 follow 0.0098 pattern 0.0103 policy 0.0055 attention 0.0076 point .0058day 0.0069 come 0.0101 match 0.0049 qualify 0.008 issue 0.0102 theory 0.00551 matter 0.00738 judge .0055number 0.00615 help 0.0088 place 0.0047 compete 0.0074 range 0.0095 way 0.0053 policy 0.006 statement .005Table 2: Random R-LDA topics with the relations between Noun-Verb (first 2 columns) and between Noun-Noun (last2 columns)of sentences while scenes are all nouns occurring inthe same sentence.
For the relations between objectsand scenes as well as objects and objects, we use allnouns to capture a general model1.
The statistics ofthe BNC corpus with their corresponding relationsare reported in Table 3.#Relations #TokensVerb - Object 3.3M 6.7MNoun - Noun 19.8M 39.7MVerb - Noun 83.4M 166.8MTable 3: The statistics of the dataset used for estimatingR-LDA models for each relation typeSamples of topics extracted through R-LDA areillustrated in Table 3.
It shows that Noun and Objectshare many similar terms in the same topic whileNoun and Verb sharing the same topics tend to gooften together (e.g., win, cup, beat, race).5 ExperimentsIn this section we want to answer our two main re-search questions: (1) Is knowledge from languagecompatible with knowledge from vision?
(2) Canwe use knowledge extracted from language in com-puter vision scenarios?5.1 Is knowledge from language and visioncompatible?In this section we compare statistics mined fromtexts with those mined from visual sources.
Ideally,1Different from objects and verbs, which can be defined ex-plicitly from the parsed corpora, scenes can only be definedfrom more restrained rules (e.g., followed by some preposi-tions), so here we take all nouns to have the most general model.Chi statistics P(V|O) P(V|S) P(O|S)R-LDA 17.8 11.6 11.9Window2 11.6 11.4 32.6Window20 11.7 11.4 23.7TypeDM 11.5 13.3 23.2ConceptNet 17.5 11.5 34.4Table 4: X 2 distance for relations between verbs, objects,scenes from different language models to image datawe want statistics from the language models to fol-low those of the image model, even though not allstatistics from images can be reliably measured dueto insufficient data.
Therefore, we measure how wellthe estimated language models fit the estimated vi-sual distributions using the the ?2-distance:X 2 =N?i=1(PIi ?
PLi)2PI i(11)where PI and PL are the probability distribution ob-tained from the image data and language models re-spectively.For the conditional probabilities P (V |O),P (V |S), and P (O|S) we compare language modelswith image statistics extracted from the 89 humanaction dataset.
Table 4 shows the results.
For the re-lations between verb and scene P (V |S), there is notmuch fluctuation among different language models.For objects and scenes P (O|S), R-LDA is closestto the image model.
This is because R-LDA is goodat measuring contextual and indirect relations bydesign, which is the case for object-scene relations.This also explains why TypeDM and Window20 are775further away from the image model, followed bythe Window2 model.
Instead, human actions arefound in language as the relation between verbs andtheir direct linguistic objects.
Indeed, TypeDM isclosest to the image model for P (V |O) as it makesexplicit use of this linguistic link.
The Window2and 20 models are almost as close to the imagemodel for P (V |O), while R-LDA is considerablyfurther away due to its contextual nature.
Finally,ConceptNet is the furthest away from the imagemodel.
To conclude, TypeDM is best for modellingdirect verb-object relations, while R-LDA is betterat capturing the more contextual object-scenerelations.To look closer at the difference between the statis-tics obtained from the image and language data,we give an example of the conditional probabil-ities of an object given a scene P (O|S) in Fig-ure 5.
We see that the distribution extracted fromlanguage (TypeDM) is much smoother and containsmore relations than the image model since it hasbeen trained on general and large text corpora.
Thedistribution from image data on the other hand ismore sparse and tailored to this specific dataset.
Forexample, given a ?store?, the probability that thereis a ?table?
is 1, given ?highway?, the probability ofa ?car?
is also 1 in the image dataset, while the high-est conditional probability of the language model isonly less than 60%.
   !"#$%!
"#%!$#%  &'()*+,)Figure 6: ?2-distances between the tested language mod-els and the image model for conditional probabilities ofobjects P (O|O).For the relations between objects and objects, weuse the SUN dataset, which is much bigger and moregeneral than the action dataset.
As shown in Fig-ure 6, R-LDA is most similar to the image model,closely followed by TypeDM and the Window20model.
All these three models are good at captur-ing broad contextual relations.
The Window2 modelhas a significantly larger distance to the image modelas it captures a narrow context of 2 words, which isapparently not enough to find co-occurences of ob-jects.
ConceptNet is the most inconsistent with thisimage data since not enough objects and their re-lations are extracted from it.
To sum up, R-LDAachieves the best performance in modeling the re-lations between objects and objects among all lan-guage models.5.2 Language Models for Visual RecognitionTo measure the performance of the two visual recog-nition scenarios, we use the position pi of the correctaction found in the ranked list for each image i.We report the average ranking over all images(ARI ) and over all objects or actions (ARO, ARA):ARI =?Ni=0 piN;ARO =?Noj=0 pjoNo(12)where N is the number of images, No is the numberof objects and pjo is the average rank of all imageshaving object j.
The average rank over all actionsARA is defined similarly to ARO.
The average rankover all image measures the performance over theimage dataset, but infrequent objects/events have lit-tle impact on this performance.
The average rankover objects or actions gives more weight to rare ex-amples.5.2.1 Human Action RecognitionWe evaluate the performance of human actionrecognition in images based on objects and scenesindividually, and then study the integration of them.The training set contains 1,104 images (for trainingthe image relations) and the test set has 710 im-ages.
First, we test how the model predicts an ac-tion knowing the actual object and/or scene appear-ing in an image (given object/scene gold standard),i.e., Ogs, Sgs and OgsSgs in the settings.
After that,we test a complete model which is based on the out-put of our object recognizer and our scene recog-nizer (Orec, Srec, OrecSrec).For each setting, we try different action models,either learnt from the training images (Image), orfrom each of the language models (TypeDM, R-LDA, Window2, Window20, ConceptNet).776Figure 5: Probability distributions of scene over object extracted from: (left) image dataset; (right) TypeDM model(as there are many <object - scene> relations, only a few are shown on the Y-axises).
The number of relations in theTypeDM is much bigger than in the image model, which shows a more general model than the image one.Table 5 presents the average ranking over all im-ages.
Results show that the action model learnt di-rectly from the training images achieves the best per-formance in all settings, even if we give more weightto infrequent actions by taking the average rankingover all actions, as presented in Table 6.
One ex-planation may be that the action dataset has a lim-ited domain of only 19 objects, while the languagemodels were learnt from broad knowledge (See Fig-ure 5).
Another possibility is that verbs used fordescribing actions in images are more specific thanverbs used in language.
For example, in languageone would ?use the car?, while in images such ac-tion would be labelled ?drive a car?.If we look at the performance of the languagemodels, TypeDM performs best by a significant mar-gin.
This makes sense, as the most powerful termfor predicting an action is obviously P (V |O), andwe saw earlier that TypeDM produces probabili-ties P (V |O) which are closest to the image model.For the same reason, the second and third bestlanguage model are the Window2 and Window20models, although their performance is significantlylower when using the predictions for objects and/orscenes.
This is somewhat surprising consideringthat TypeDM, Window2 and Window20 are all veryclose in distance to the image model.
Of course,Image TypeDM R-LDA Window2 Window20 C.NetOgs 0.3 16.1 63.4 16.4 18.3 86.1Orec 14.9 26.9 66.7 44.7 54.9 115.6Sgs 35.7 181.7 174.9 168.5 174.8 252.5Srec 46.8 250.5 348 190.2 189.8 241.2OgsSgs 0.28 10.2 15.2 13.8 13.6 81.9OrecSrec 13.6 26.9 66.7 44.7 54.9 115.6Table 5: Average rank over all images ARI of the humanaction recognition using different settings: Ogs, Orecuse only objects (gold standard and object recognizer);Sgs, Srec use only scenes,OgsSgs andOrecSrec integrateboth objects and scenes togetherthe distance is just an indication.
R-LDA performspoorly because it is much more contextual.
Finally,ConceptNet performs the worst.Another observation is that using the scene iden-tity should theoretically help in human action recog-nition: Using TypeDM, the use of the gold standardobject identity yields an average ranking over all im-ages of 16.1, while using both the scene and objectidentity yields an average ranking of 10.2, whichis significantly better.
It means that the use of thescene can disambiguate some actions (e.g.
?ride ahorse?
vs. ?feed a horse?).
However, when usingthe recognition system, using the scene does not in-crease the overall performance.
This shows that the777visual recognition system may not be strong enoughfor recognizing these 15 scenes.
Another problemmay be the limitation of 15 scenes only: while an-notating we frequently found that it was hard for nu-merous images to put them into one of the 15 scenes.So a bigger scene database may help.The main problem with most available annotatedhuman action datasets is that they are very restrictedand domain-specific.
For example, in this datasetwith 19 objects and 15 scenes, there are many photosof a person riding a motorbike on rocky mountainsas a kind of sport.
Consequently, the probabilityof ?riding?
given ?mountain?
learnt from the imagedataset is high according to the image data (78%)but is uncommon in general.
So the image datasetmight be too restricted or biased for general knowl-edge to work well.
In the next section we thereforeuse a more general dataset.Image TypeDM R-LDA Window2 Window20 C.NetARI 13.6 26.9 66.7 44.7 54.9 115.6ARA 16.4 30.8 64.7 45.3 51.9 131.7Table 6: Average rank over all images vs. actions of thehuman action recognition using the OrecSrec setting5.2.2 Objects in ContextFor every object in every image in the test set ofthe SUN database, we guess the identity of an objectgiven the identity of all other objects in the image.In total, there are 78,306 object predictions within10,652 images.As shown in Figure 7, the R-LDA model outper-forms all other models for both average rank overimages and over objects.
Interestingly, both R-LDAand TypeDM are better at predicting the correct ob-jects in images than the model learnt from the imagetraining set itself.
It shows that for many cases, therelation statistics learnt from language data can helpin visual recognition.
These language models areeven better than the information extracted from gen-eral, relatively unbiased image datasets, where an-notation is limited.
For the limited annotation, thishypothesis is further supported by looking at the av-erage rank over objects, which gives more weightto rarely occurring objects.
As seen in Figure 7, alllanguage models except ConceptNet outperform theimage model.
We conclude that language models  	 	 !"	#	!"	$% !"	#	!"	&'%Figure 7: Average rank over all images and objects usingdifferent language models and ID (image data)can aid visual models in large-scale visual recogni-tion problems which use co-occurrence of objects astheir context, especially when the annotation is lim-ited, as is often the case.6 ConclusionIn this paper, we investigated the problem of ap-plying knowledge learnt from language corpora tovisual recognition.
We compared statistics of var-ious language models mined on general corporawith statistics observed in image datasets.
It showsthat the generative R-LDA model is good at relat-ing contextual relations (e.g., object - object, ob-ject - scene), while the syntactic based distributionalmodel TypeDM is good at representing direct rela-tions such as verb - object in images.We have evaluated the performance of the lan-guage models in two visual scenarios: human actionrecognition and object prediction.
It suggests thatthe language models need some tailoring when ap-plied to restricted datasets, but for a bigger and moregeneral dataset, the language models even outper-form the model learnt from annotated images itself.This shows that language models built from avail-able text corpora can be used for visual recognitioninstead of expensive annotated image data.In the future, we want to further investigate theproblem of domain adaptation when applying gen-eral language models to a new image dataset.
Thisproblem can be integrated into the energy-basedmodel during the training phase.
We plan to extendwork on human action recognition by including therelative position between the human and object inthe images.778ReferencesMarco Baroni and Alessandro Lenci.
2010.
Distribu-tional memory: A general framework for corpus-basedsemantics.
Computational Linguistics.Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran.
2012.
Distributional semantics in tech-nicolor.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics, ACL.ACL.Vincent Delaitre, Ivan Laptev, and Josef Sivic.
2010.Recognizing human actions in still images: a studyof bag-of-features and part-based representations.
InProceedings of the British Machine Vision Conference.BMVA Press.M.
Everingham, L. Van Gool, C. K. I. Williams, J. Winn,and A. Zisserman.
2012.
The PASCAL Visual ObjectClasses Challenge 2012 (VOC2012) Results.I.
Everts, J. van Gemert, and T. Gevers.
2013.
Evaluationof color stips for human action recognition.
In CVPR.Abhinav Gupta and Larry S. Davis.
2008.
Beyondnouns: Exploiting prepositions and comparative adjec-tives for learning visual classifiers.
In Proceedings ofthe 10th European Conference on Computer Vision.Gregor Heinrich.
2004.
Parameter estimation for textanalysis.
Technical report.H.
Kuehne, D. Gehrig, T. Schultz, and R. Stiefelhagen.2012.
On-line action recognition from sparse featureflow.
In VISAPP.C.H.
Lampert, H. Nickisch, and S. Harmeling.
2009.Learning to detect unseen object classes by between-class attribute transfer.
In Conference on ComputerVision and Pattern Recognition CVPR.S.
Lazebnik, C. Schmid, and J. Ponce.
2006.
Beyondbags of features: Spatial pyramid matching for rec-ognizing natural scene categories.
In Conference onComputer Vision and Pattern Recognition CVPR, vol-ume 2, pages 2169?2178.Dieu Thu Le, Raffaella Bernardi, and Jasper Uijlings.2013.
Exploiting language models to recognize un-seen actions.
In Proceedings of the 3rd ACM con-ference on International conference on multimedia re-trieval ICMR.
ACM.Yann Lecun, Sumit Chopra, Raia Hadsell, Fu J. Huang,G.
Bakir, T. Hofman, B. Scho?lkopf, A. Smola, andB.
Taskar Eds.
2006.
A tutorial on energy-basedlearning.
In Predicting Structured Data.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instruments,and Computers.K.
Reddy and M. Shah.
2012.
Recognizing 50 humanaction categories of web videos.
In Machine Visionand Applications.Diarmuid O?.
Se?aghdha.
2010.
Latent variable mod-els of selectional preference.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics, ACL, pages 435?444.
Associationfor Computational Linguistics.Robert Speer and Catherine Havasi.
2013.
Conceptnet 5:A large semantic network for relational knowledge.
InThe People?s Web Meets NLP.
Springer Berlin Heidel-berg.Munirathnam Srikanth, Joshua Varner, Mitchell Bowden,and Dan Moldovan.
2005.
Exploiting ontologies forautomatic image annotation.
In Special Interest Groupon Information Retrieval SIGIR.
ACM.C.L.
Teo, Yezhou Yang, H. Daume, C. Fermuller, andY.
Aloimonos.
2012.
Towards a watson that sees:Language-guided action recognition for robots.
In2012 IEEE International Conference on Robotics andAutomation ICRA.J R R Uijlings, A W M Smeulders, and R J H Scha.
2010.Real-time Visual Concept Classification.
IEEE Trans-actions on Multimedia, 12.J R R Uijlings, K.E.A.
van de Sande, T. Gevers, andA.W.M.
Smeulders.
2013.
Selective search for ob-ject recognition.
International Journal of ComputerVision.Yoshitaka Ushiku, Tatsuya Harada, and Yasuo Ku-niyoshi.
2012.
Efficient image annotation for auto-matic sentence generation.
In ACM International Con-ference on Multimedia ACM MM.H.
Wang, A. Kla?ser, C. Schmid, and C. Liu.
2013.
Densetrajectories and motion boundary descriptors for ac-tion recognition.
International Journal of ComputerVision, 103:60?79.Jianxiong Xiao, J. Hays, K.A.
Ehinger, A. Oliva, andA.
Torralba.
2010.
Sun database: Large-scale scenerecognition from abbey to zoo.
In Conference onComputer Vision and Pattern Recognition CVPR.Yezhou Yang, Ching Lik Teo, Hal Daume?, III, and Yian-nis Aloimonos.
2011.
Corpus-guided sentence gener-ation of natural images.
In Conference on EmpiricalMethods in Natural Language Processing EMNLP.Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy LaiLin, Leonidas J. Guibas, and Li Fei-Fei.
2011.
Ac-tion recognition by learning bases of action attributesand parts.
In International Conference on ComputerVision ICCV.Xiaodong Yu, Cornelia Fermuller, Ching Lik Teo,Yezhou Yang, and Yiannis Aloimonos.
2011.
Ac-tive scene recognition with vision and language.
InProceedings of the 2011 International Conference onComputer Vision, International Conference on Com-puter Vision ICCV.779
