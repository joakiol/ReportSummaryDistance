The SuperARV Language Model: Investigating the Eectivenessof Tightly Integrating Multiple Knowledge SourcesWen Wang and Mary P. HarperSchool of Electrical and Computer EngineeringPurdue University1285 The Electrical Engineering BuildingWest Lafayette, IN 47907-1285fwang28,harperg@ecn.purdue.eduAbstractA new almost-parsing language model incorporat-ing multiple knowledge sources that is based uponthe concept of Constraint Dependency Grammars ispresented in this paper.
Lexical features and syn-tactic constraints are tightly integrated into a uni-form linguistic structure called a SuperARV that isassociated with a word in the lexicon.
The Super-ARV language model reduces perplexity and word er-ror rate compared to trigram, part-of-speech-based,and parser-based language models.
The relative con-tributions of the various knowledge sources to thestrength of our model are also investigated by usingconstraint relaxation at the level of the knowledgesources.
We have found that although each knowl-edge source contributes to language model quality,lexical features are an outstanding contributor whenthey are tightly integrated with word identity andsyntactic constraints.
Our investigation also suggestspossible reasons for the reported poor performanceof several probabilistic dependency grammar modelsin the literature.1 IntroductionThe purpose of a language model (LM) is to de-termine the a priori probability of a word sequencew1; : : : ; wn, P (w1; : : : ; wn).
Language modeling is es-sential in a wide variety of applications; we focus onspeech recognition in our research.
Although word-based LMs (with bigram and trigram being the mostcommon) remain the mainstay in many continuousspeech recognition systems, recent eorts have ex-plored a variety of ways to improve LM performance(Niesler and Woodland, 1996; Chelba et al, 1997;Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosen-feld, 2000; Goodman, 2001; Roark, 2001; Charniak,2001).Class-based LMs attempt to deal with data sparse-ness and generalize better to unseen word sequencesby rst grouping words into classes and then usingthese classes to compute n-gram probabilities.
Part-of-Speech (POS) tags were initially used as classesby Jelinek (1990) in a conditional probabilisticmodel (which predicts the tag sequence for a wordsequence rst and then uses it to predict the wordsequence):Pr(wN1 ) Xt1;t2;:::;tNNYi=1Pr(tijti?11 )Pr(wijti) (1)However, Jelinek?s POS LM is less eective at pre-dicting word candidates than an n-gram word-basedLM because it deletes important lexical informationfor predicting the next word.
Heeman?s (1998) POSLM achieves a perplexity reduction compared to atrigram LM by instead redening the speech recog-nition problem as determining:W; T = argmaxW;TP (W;T jA)= argmaxW;TP (W;T)P (AjW;T) argmaxW;TP (W;T)P (AjW )where T is the POS sequence tN1 associated with theword sequence W = wN1 given the speech utteranceA.
The LM P (W;T ) is a joint probabilistic modelthat accounts for both the sequence of words wN1and their tag assignments tN1 by estimating the jointprobabilities of words and tags:P (wN1 ; tN1 ) =NYi=1P (wi; tijwi?11 ; ti?11 ) (2)Johnson (2001) and Laerty et al (2001) provideinsight into why a joint model is superior to a con-ditional model.Recently, there has been good progress in devel-oping structured models (Chelba, 2000; Charniak,Association for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
238-247.Proceedings of the Conference on Empirical Methods in Natural2001; Roark, 2001) that incorporate syntactic infor-mation.
These LMs capture the hierarchical char-acteristics of a language rather than specic infor-mation about words and their lexical features (e.g.,case, number).
In an attempt to incorporate evenmore knowledge into a structured LM, Goodman(1997) has developed a probabilistic feature gram-mar (PFG) that conditions not only on structurebut also on a small set of grammatical features (e.g.,number) and has achieved parse accuracy improve-ment.
Goodman?s work suggests that integratinglexical features with word identity and syntax wouldbenet LM predictiveness.
PFG uses only a small setof lexical features because it integrates those featuresat the level of the production rules, causing a signif-icant increase in grammar size and a concomitantdata sparsity problem that preclude the addition ofricher features.
This sparseness problem can be ad-dressed by associating lexical features directly withwords.We hypothesize that high levels of word predic-tion capability can be achieved by tightly integrat-ing structural constraints and lexical features at theword level.
Hence, we develop a new dependency-grammar almost-parsing LM, SuperARV LM, whichuses enriched tags called SuperARVs.
In Section 2,we introduce our SuperARV LM.
Section 3 comparesthe performance of the SuperARV LM to other LMs.Section 4 investigates the knowledge source contribu-tions by constraint relaxation.
Conclusions appear inSection 5.2 SuperARV Language ModelThe SuperARV LM is a highly lexicalized probabilis-tic LM based on the Constraint Dependency Gram-mar (CDG) (Harper and Helzerman, 1995).
CDGrepresents a parse as assignments of dependency re-lations to functional variables (denoted roles) asso-ciated with each word in a sentence.
Consider theparse for What did you learn depicted in the whitebox of Figure 1.
Each word in the parse has a lexi-cal category and a set of feature values.
Also, eachword has a governor role (denoted G) which is as-signed a role value, comprised of a label as well as amodiee, which indicates the position of the word?sgovernor or head.
For example, the role value as-signed to the governor role of did is vp-1, where itslabel vp indicates its grammatical function and itsmodiee 1 is the position of its head what.
The needroles (denoted N1, N2, and N3) are used to ensurethe grammatical requirements (e.g., subcategoriza-tion) of a word are met, as in the case of the verbdid, which needs a subject and a base form verb (butsince the word takes no other complements, the mod-pronouncase=commonbehavior=nominaltype=interrogativesemtype=inanimateagr=3sG=np-4verbsubcat=baseverbtype=pastvoice=activeinverted=yestype=nonegapp=yesmood=whquestionsemtype=auxiliaryagr=allG=vp-1Need1=S-3Need2=S-4Need3=S-2pronouncase=commonbehavior=nominaltype=personalsemtype=humanagr=2sG=subj-21what2did3youThe SuperARV of the word "did":Category: Verb4learnverbsubcat=objvtype=infinitivevoice=activeinverted=notype=nonegapp=yesmood=whquestionsemtype=behavioragr=noneG=vp-2Need1=S-4Need2=S-1Need3=S-4Features: {verbtype=past, voice=active, inverted=yes, type=none,gapp=yes,mood=whquestion,agr=all}Role=G,         Label=vp, PX>MX,                (ModifieeCategory=pronoun)Role=Need1, Label=S,   PX<MX,                (ModifieeCategory=pronoun)Role=Need2, Label=S,   PX<MX,                (ModifieeCategory=verb)Role=Need3, Label=S,   PX=MX,                (ModifieeCategory=verb)Dependent Positional Constraints:MX[G] < PX = MX[Need3] < MX[Need1]< MX[Need2] MC}needroleconstraints}}}CF }}(R,L,UC,MC)+DCARV for vp-1 assigned to G for did:cat1=verb, subcat=base, verbtype=past, voice=active, inverted=yes, type=none,gapp=yes, mood=whquestion, semtype=auxiliary, agr=all,role1=G, label1=vp, (PX1>MX1)ARVP for vp-1 assigned to G for did and subj-2 assigned to G for you:cat1=verb, subcat=base, verbtype=past, voice=active, inverted=yes, type=none, gapp=yes,mood=whquestion, semtype=auxiliary, agr=all,role1=G, label1=vp, (PX1>MX1)cat2=pronoun, case=common, behavior=nominal, type=personal, semtype=human, agr=2s,role2=G, label2=subj, (PX2>MX2)(PX1<PX2), (MX1<MX2),(PX1=MX2),(MX1<PX2)Figure 1: An example of a CDG parse, an ARV andARVP, and the SuperARV of the word did in the sentencewhat did you learn.
Note: G represents the governorrole; the need roles, Need1, Need2, and Need3, are usedto ensure that the grammatical requirements of the wordare met.
PX and MX([R]) represent the position of aword and its modiee (for role R), respectively.iee of the role value assigned to N3 is set equal toits own position).
Including need roles also providesa mechanism for using non-headword dependenciesto constrain parse structures, which Bod (2001) hasshown contributes to improved parsing accuracy.During parsing, the grammaticality of a sentencein a language dened by a CDG is determined byapplying a set of constraints to the possible rolevalue assignments (Harper and Helzerman, 1995;Maruyama, 1990).
Originally, the constraints werecomprised of a set of hand-written rules specifyingwhich role values (unary constraints) and pairs ofrole values (binary constraints) were grammatical(Maruyama, 1990).
In order to derive the constraintsdirectly from CDG annotated sentences, we have de-veloped an algorithm to extract grammar relationsusing information derived directly from annotatedsentences (Harper et al, 2000; Harper and Wang,2001).
Using the relationship between a role value?sposition and its modiee?s position, unary and bi-nary constraints can be represented as a nite set ofabstract role values (ARVs) and abstract role valuepairs (ARVPs), respectively.
The light gray box ofFigure 1 shows an example of an ARV and an ARVP.The ARV for the governor role value of did indicatesits lexical category, lexical features, role, label, andpositional relation information.
(PX1 > MX1) in-dicates that did is governed by a word that precedesit.
Note that the constraints of a CDG can be ex-tracted from a corpus of parsed sentences.A super abstract role value (SuperARV) is an ab-straction of the joint assignment of dependencies fora word, which provides a mechanism for lexicaliz-ing CDG parse rules.
The dark gray box of Figure 1presents an example of a SuperARV for the word did.The SuperARV structure provides an explicit way toorganize information concerning one consistent set ofdependency links for a word that can be directly de-rived from its parse assignments.
SuperARVs encodelexical information as well as syntactic and semanticconstraints in a uniform representation that is muchmore ne-grained than POS.
A SuperARV can bethought of as providing admissibility constraints onsyntactic and lexical environments in which a wordmay be used.A SuperARV is formally dened as a four-tuplefor a word, hC; F , (R;L; UC;MC)+; DCi, where Cis the lexical category of the word, F = fFname1= Fvalue1, : : : ; FNamef = FV aluefg is a fea-ture vector (where Fnamei is the name of a featureand Fvaluei is its corresponding value), (R, L, UC,MC)+ is a list of one or more four-tuples, each rep-resenting an abstraction of a role value assignment,where R is a role variable, L is a functionality la-bel, UC represents the relative position relation ofa word and its dependent, MC is the lexical cat-egory of the modiee for this dependency relation,and DC represents the relative ordering of the po-sitions of a word and all of its modiees.
The fol-lowing features are used in our SuperARV LM: agr,case, vtype (e.g., progressive), mood, gapp (e.g.,gap or not), inverted, voice, behavior (e.g., mass,count), type (e.g., interrogative, relative).
Theselexical features constitute a much richer set than thefeatures used by the parser-based LMs in Section 1.Since Harper et al (1999) found that enforcing mod-iee constraints (e.g., the lexical categories of modi-ees) in parsing results in ecient pruning, we alsoinclude the modiee lexical category (MC) in our Su-perARV structure to impose modiee constraints.Words typically have more than one SuperARV toindicate dierent types of word usage.
The averagenumber of SuperARVs for words of dierent lexicalcategories vary, with verbs having the greatest Su-perARV ambiguity.
This is mostly due to the vari-ety of feature combinations and variations on com-plement types and positions.
We have observed inseveral experiments that the number of SuperARVsdoes not grow signicantly as training set size in-creases; the moderate-sized Resource Managementcorpus (Price et al, 1988) with 25,168 words pro-duces 328 SuperARVs, compared to 538 SuperARVsfor the 1 million word Wall Street Journal (WSJ)Penn Treebank set (Marcus et al, 1993), and 791 forthe 37 million word training set of the WSJ contin-uous speech recognition task.SuperARVs can be accumulated from a corpus an-notated with CDG relations and stored directly withwords in a lexicon, so we can learn their frequencyof occurrence for the corresponding word.
A Super-ARV can then be selected from the lexicon and usedto generate role values that meet their constraints.Since there are no large benchmark corpora anno-tated with CDG information1, we have developed amethodology to automatically transform constituentbracketing found in available treebanks into CDGannotations.
In addition to generating dependencystructures by headword percolation (Chelba, 2000),our transformer also utilizes a rule-based method todetermine lexical features and need role values forwords, as described by Wang et al (2001).Our SuperARV LM estimates the joint probabilityof words wN1 and their SuperARV tags tN1 :Pr(wN1 tN1 ) =NYi=1Pr(witijwi?11 ti?11 )=NYi=1Pr(tijwi?11 ti?11 )  Pr(wijwi?11 ti1)NYi=1Pr(tijwi?1i?2ti?1i?2)  Pr(wijwi?1i?2tii?2) (3)Notice we use a joint probabilistic model to enablethe joint prediction of words and their SuperARVs sothat word form information is tightly integrated atthe model level.
Our SuperARV LM does not encodethe word identity directly at the data structure levelas was done in (Galescu and Ringger, 1999) sincethis could cause serious data sparsity problems.To estimate the probability distributions in Equa-tion (3) from training data, we use recursive lin-ear interpolation among probability estimations ofdierent orders.
Representing each multiplicandin Equation (3) as the conditional probabilityP^ (xjy1; y2; : : : ; yn) where y1; y2; : : : ; yn belong to amixed set of words and SuperARVs, the recursivelinear interpolation is calculated as follows:1We have annotated a moderate-sized corpus,DARPA Naval Resource Management (Price et al, 1988),with CDG parse relations as reported in (Harper et al,2000; Harper and Wang, 2001).P^n(xjy1; y2; : : : ; yn)= (x; y1; y2; : : : ; yn)  Pn(xjy1; y2; : : : ; yn)+(1?
(x; y1; y2; : : : ; yn))  P^n?1(xjy1; y2; : : : ; yn?1)where: y1; y2; : : : ; yn is the context of order n-gram topredict x; Pn(xjy1; y2; : : : ; yn) is the order n-gram maximumlikelihood estimation.Table 1 enumerates the n-grams and their order forthe interpolation smoothing of the two distributionsin Equation (3).
The ordering was based on our hy-pothesis that n-grams with more ne-grained historyinformation should be ranked higher in the n-gramlist since that information should be more helpfulfor discerning word and SuperARVs based on theirhistory.
The SuperARV LM hypothesizes categoriesfor out-of-vocabulary words using the leave-one-outtechnique (Niesler and Woodland, 1996).Table 1: The enumeration and order of n-grams forsmoothing the distributions in Equation (3).n-grams P^ (tijwi?1i?2ti?1i?2) P^ (wijwi?1i?2tii?2)highest P^ (tijwi?1i?2ti?1i?2) P^ (wijwi?1i?2tii?2)P^ (tijwi?1ti?1i?2) P^ (wijwi?1i?2tii?1)P^ (tijwi?1i?2ti?1) P^ (wijwi?1tii?2)P^ (tijti?1i?2) P^ (wijwi?1tii?1)P^ (tijwi?1ti?1) P^ (wijwi?1ti)P^ (tijti?1) P^ (wijtii?1)lowest P^ (ti) P^ (wijti)In preliminary experiments, we compared severalalgorithms for smoothing the probability estima-tions for our SuperARV LM.
The best performancewas achieved by using the modied Kneser-Neysmoothing algorithm initially introduced in (Chenand Goodman, 1998) and adapting it by employinga heldout data set to optimize parameters, includ-ing cutos for rare n-grams, by using Powell?s search(Press et al, 1988).
Parameters are chosen to opti-mize the perplexity on a heldout set.In order to compare our SuperARV LM with aword-based LM, we must use the following equationto calculate the word perplexity (PPL):PPL = 2En (4)En  ?
1NNXi=1log2 P^ (wijwi?1i?2) ?
1NNXi=1log2Pti?2;iP^ (witijwi?1i?2ti?1i?2)P^(wi?1i?2ti?1i?2)Pti?2;i?1P^ (wi?1i?2ti?1i?2)Equation (4) is used by class-based LMs to calculateword perplexity (Heeman, 1998).
Parser-based LMsuse a similar procedure that sums over parses.The SuperARV LM is most closely related tothe almost-parsing-based LM developed by Srinivas(1997).
Srinivas?
LM, based on the notion of a su-pertag , the elementary structure of Lexicalized Tree-Adjoining Grammar, achieved a perplexity reductioncompared to a conditional POS n-gram LM (Nieslerand Woodland, 1996).
By comparison, our LM in-corporates dependencies directly on words insteadof through nonterminals, uses more lexical featuresthan the supertag LM, uses joint instead of con-ditional probability estimations, and uses modiedKneser-Ney rather than Katz smoothing.3 Evaluating the SuperARVLanguage ModelTraditionally, the LM quality in speech recognition isevaluated on two metrics: perplexity and WER, withthe former commonly selected as a less computation-ally expensive alternative.
We carried out two exper-iments, one using the Wall Street Journal Penn Tree-bank (WSJ PTB), a text corpus on which perplexitycan be measured and compared to other LMs, andthe Wall Street Journal Continuous Speech Recog-nition (WSJ CSR) task, a speech corpus on whichboth perplexity and WER can be evaluated after LMrescoring.
These two experiments compare our Su-perARV LM to a baseline trigram, a POS LM thatwas implemented using Equation (3) (where for thismodel t represents POS tags instead of SuperARVtags) and modied Kneser-Ney smoothing (as usedin the SuperARV LM), and one or more parser-basedLMs.
Additionally, we evaluate the performance of aconditional probability SuperARV LM (denoted cSu-perARV) implemented following Equation (1) ratherthan Equation (3) to evaluate the importance of us-ing joint probability estimations.For the WSJ PTB task, we compare the Super-ARV LMs to the parser LMs developed by Chelba(2000), Roark (2001), and Charniak (2001).
Al-though Srinivas (1997) developed an almost-parsingsupertag-based LM, we cannot compare his LM withthe other LMs because he used a small non-standardsubset of the WSJ PTB2 and a trainable supertagLM is unavailable.
Because none of the parser LMshas been fully trained for the WSJ CSR task, it isessential that we retrain them for comparison.
Theavailability of a trainable version of Chelba?s modelenables us to train and test on the CSR task; how-ever, because we do not have access to a trainableversion of Charniak?s or Roark?s LMs, they are notconsidered in the CSR task.
Note that for latticerescoring, however, Roark found that Chelba?s modelachieves a greater reduction on WER than his LM(Roark, 2001).3.1 Evaluating on the WSJ PTBTo evaluate the perplexity of the LMs on the WSJPTB task, we adopted the conventions of Chelba(2000), Roark (2001), and Charniak (2001) for pre-processing the data.
The vocabulary is limited tothe most common 10K words, with all words outsidethis vocabulary mapped to hUNKi.
All punctuationis removed and no case information is retained.
Allsymbols and digits are replaced by the symbol N.Sections 0-20 (929,564 words) are used as the train-ing set for collecting counts, sections 21-22 (73,760words) as the development set for tuning parameters,and sections 23-24 (82,430 words) for testing.The baseline trigram uses Katz back-o modelwith Good-Turing discounting for smoothing.
ThePOS, cSuperARV, and SuperARV LMs were imple-mented as described previously.
The results for theparser-based LMs were initially taken from the lit-erature.
The perplexity on the test set using eachLM and their interpolation with the correspondingtrigram (and the interpolation weight) are shown inthe top six rows of Table 2.As can be seen in Table 2, the SuperARV LM ob-tains the lowest perplexity of all of the LMs (andso it is depicted in bold face).
The SuperARV LMachieves the greatest perplexity reduction of 29.19%compared to the trigram, with Charniak?s interpo-lated trihead LM a close second at 24.91%.
The cSu-perARV LM is clearly inferior to the SuperARV LM,even after interpolation.
This result highlights thevalue of tight coupling of word, lexical feature, andsyntactic knowledge both at the data structure level(which is the same for the SuperARV and cSuper-ARV LMs) and at the probability model level (whichis dierent).Notice that the cSuperARV, Chelba?s, Roark?s,and Charniak?s LMs obtain an improvement in per-formance when interpolated with a trigram; whereas,2Using the same 180,000 word training and 20,000word test set as (Srinivas, 1997), our SuperARV LM ob-tains a perplexity of 92.76, compared to a perplexity of101 obtained by the supertag LM.the POS LM and the SuperARV LM do not benetfrom trigram interpolation3.
To gain more insightinto why a trigram is eectively interpolated withsome, but not all, of the LMs, we calculate the cor-relation of the trigram with each LM.
A standardcorrelation is calculated between the probabilities as-signed to each test set sentence by the trigram LMand the LM in question.
This technique has beenused in (Wang et al, 2002) to identify whether twoLMs can be eectively interpolated.Since we have access to an executable ver-sion of Charniak?s LM trained on the WSJ PTB(ftp.cs.brown.edu/pub/nlparser) and a trainable ver-sion of Chelba?s LM, we are able to calculate theircorrelations with our trigram LM.
Chelba?s LM wasretrained using more parameter reestimation itera-tions than in (Chelba, 2000) to optimize the per-formance.
Table 2 shows the correlation betweeneach of the executable LMs and the trigram LM.The POS LM has the highest correlation with thetrigram, closely followed by the SuperARV LM.
Be-cause these two LMs tightly integrate the word infor-mation jointly with the tag distribution, the trigraminformation is already represented.
In contrast, thecSuperARV LM and Chelba?s and Charniak?s parser-based LMs have much lower correlations, indicatingthey have much lower overlap with the trigram.
Be-cause the cSuperARV LM only uses weak word dis-tribution information in probability estimations, itleaves room for the trigram LM to compensate forthe lack of word knowledge.
The correlations for theparser-based LMs suggest that they capture dierentaspects of the words?
distributions in the languagethan the words themselves.3.2 Evaluating on the WSJ CSR TaskNext we compare the eectiveness of using the tri-gram word-based, POS, cSuperARV, SuperARV,and Chelba?s LMs in rescoring hypotheses generatedby a speech recognizer.
The training set of the WSJCSR task is composed of the 1987-1989 les con-taining 37,243,300 words.
The speech data for thetraining set is used for building the acoustic model;whereas, the parse trees for the training set are gen-erated following the policy that if the context-freegrammar constituent bracketing can be found in theWSJ PTB, it becomes the parse tree for the trainingsentence; otherwise, we use the corresponding tree inthe BLLIP treebank (Charniak et al, 2000).
SinceWSJ CSR is a speech corpus, there is no punctua-tion or case information.
All words outside the pro-vided vocabulary are mapped to hUNKi.
Note that3In the remaining experiments, the POS LM and theSuperARV LM are not interpolated with a trigram.PerplexityLM 3gram Model Intp (Weight) rPOS 167.14 142.55 142.55 (1.0) 0.95SuperARV 167.14 118.35 118.35 (1.0) 0.92cSuperARV 167.14 150.01 143.83 (0.65) 0.68Chelba (2000) 167.14 158.28 148.90 (0.64) N/ARoark (2001) 167.02 152.26 137.26 (0.64) N/ACharniak (2001) 167.89 130.20 126.07 (0.64) N/AChelba 167.14 153.76 147.70 (0.64) 0.73Charniak 167.14 130.20 126.03 (0.64) 0.69Table 2: Comparing perplexity results for each LM on the WSJ PTB test set.
3gram represents the word-basedtrigram LM, Intp (weight) the LM interpolated with a trigram (and the interpolation weight), and r the correlationvalue.
N/A means not available.the word-level tokenization of treebank texts diersfrom that used in the speech recognition task withthe major dierences being: numbers (e.g., \1.2%"versus \one point two percent"), dates (e.g., \Dec.20, 2001" versus \December twentieth, two thou-sand one") , currencies (e.g., \$10.25" versus \tendollars and twenty ve cents"), common abbrevia-tions (e.g., \Inc."
versus \Incorporated"), acronyms(e.g., \I.B.M."
versus \I.
B. M."), hyphenated andperiod-delimited phrases (e.g., \red-carpet" versus\red carpet"), and contractions and possessives (e.g.,\do n?t" versus \don?t").
The POS, parser-based,and SuperARV LMs are all trained using the text-based tokenization from the treebank.
Hence, duringtesting, a transformation converts the output of therecognizer to a form compatible with the text-basedtokenization (Roark, 2001) for rescoring.For testing the LMs, we use the four availableWSJ CSR evaluation sets: 1992 5K closed vocab-ulary (denoted 92-5k) with 330 utterances and 5,353words, 1993 5K closed vocabulary (93-5k) with 215utterances and 3,849 words, 1992 20K open vocabu-lary (92-20k) with 333 utterances and 5,643 words,and 1993 20K (93-20k) with 213 utterances and3,446 words.
We also employ a development set foreach vocabulary size: 93-5k-dt (513 utterances and8,635 words) and 93-20k-dt (252 utterances and 4,062words).The trigram provided by LDC for the CSR taskwas used due to its high quality.
Before evaluation,all the other LMs (i.e., the POS LM, the cSuperARVand SuperARV LMs, and Chelba?s LM) are retrainedon the training set trees for the CSR task.
Parametertuning for the LMs on each task uses the correspond-ing development set4.Perplexity Results Table 3 shows the perplexityresults for each test set with the best result for each4The interpolation weight for cSuperARV for latticerescoring was 0.63 on the 5k tasks and 0.60 on the 20ktasks, and 0.68 and 0.65 for Chelba?s LM, respectively.in bold face.
The SuperARV LM yields the lowestperplexity, with Chelba?s LM a close second.
Theperplexity reductions for the SuperARV LM overthe trigram across the test sets are 53.19%, 53.63%,34.33%, and 32.05%, which is even higher than onthe WSJ PTB task.
This is probably due to the factthat more training data was used for the CSR task(37 million words versus 1 million words).LM 92-5k 93-5k 92-20k 93-20k3gram 45.61 50.51 106.52 109.22POS 44.21 30.26 98.79 96.64cSuperARV 36.53 28.50 86.83 89.12SuperARV 21.35 23.42 69.95 74.22Chelba 23.92 25.07 77.16 79.37Table 3: Comparing perplexity results for each LM onthe WSJ CSR test sets.Rescoring Lattices Next using the same LMs, werescored the lattices generated by an acoustic recog-nizer built using HTK (Ent, 1997).
For each testset sentence, we generated a word lattice.
We tunedthe parameters of the LMs using the lattices on thecorresponding development sets to minimize WER.Lattices were rescored using a Viterbi search for eachLM.Table 4 shows the WER and sentence accuracy(SAC) after rescoring lattices using each LM, withthe lowest WER and highest SAC for each test setpresented in bold face.
We also give the latticeWER/SAC which denes the best accuracy possiblegiven perfect knowledge.
As can be seen from Table4, the SuperARV LM produces the best reductionin WER with Chelba?s LM the second best.
Whenrescoring lattices on the 92-5k, 93-5k, 92-20k, and93-20k test sets, the SuperARV LM yields a relativeWER reduction of 13.54%, 9.70%, 8.64%, and 3.12%compared to the trigram, respectively.
SAC resultsare similar: the SuperARV LM achieves an absoluteincrease on SAC of 4.24%, 6.97%, 2.7%, and 3.75%,compared to the trigram.
Note that Chelba?s LMtied once with the SuperARV LM on 93-20k SAC,but always obtained higher WER across the four testsets.
Because Chelba?s LM focuses on developing thecomplete parse structure for a word sequence, it en-forces more strict pruning based on the entire sen-tence.
As can be seen in Table 4, the cSuperARVLM, even when interpolated with a trigram LM, ob-tains a lower accuracy than our SuperARV LM.
Thisresult is consistent with the hypothesis that a con-ditional model suers from label bias (Laerty et al,2001).The WER reported by Chelba (2000) on the 93-20k test set was 13.0%.
This WER is lower thanwhat we obtained for Chelba?s retrained LM on thesame task.
This disparity is due to the fact that ahigher quality acoustic decoder was used in (Chelba,2000), which is not available to us.
We further com-pare the LMs on Dr. Chelba?s 93-20K lattices kindlyprovided by him, with the rescoring results shownin the last column of Table 4.
We observe thatChelba?s retrained LM improves his original result,but the SuperARV LM still obtains a greater accu-racy.
Sign tests show that the dierences betweenthe accuracies achieved by the SuperARV LM andthe trigram, POS, and cSuperARV LMs are statis-tically signicant.
Although there is no signicantdierence between the SuperARV LM and Chelba?sLM, the SuperARV LM has a much lower complexitythan Chelba?s LM.4 Investigating the KnowledgeSource ContributionsNext, we attempt to explain the contrast betweenthe encouraging results from our SuperARV LM andthe reported poor performance of several probabilis-tic dependency grammar models, i.e., the traditionalprobabilistic dependency grammar (PDG) LM, theprobabilistic link grammar (PLG) (Laerty et al,1992) LM, and Zeman?s probabilistic dependencygrammar model (ZPDG) (Hajic et al, 1998).
ZPDGwas evaluated on the Prague Dependency Treebank(Hajic, 1998) during the 1998 Johns Hopkins sum-mer workshop (Hajic et al, 1998) and produceda much lower parsing accuracy (under 60%) thanCollins?
probabilistic context-free grammar parser(80%) (Collins, 1996).
Fong et al (1995) evalu-ated the probabilistic link grammar LM describedin (Laerty et al, 1992) on small articial corporaand found that the LM has a greater perplexity thana standard bigram.
Additionally, only a modest im-provement on the bigram was achieved after Fongand Wu (1995) revised the model to make grammarrule learning feasible.One possible reason for their poor performance, es-pecially in the light of our SuperARV LM results, isthat these probabilistic dependency grammar mod-els do not utilize sucient knowledge to achieve ahigh level of accuracy.
The knowledge sources theSuperARV LM uses, represented as components ofthe structure shown in Figure 1, include: lexicalcategory (denoted c), lexical features (denoted f),role label or link type information (denoted L), agovernor role dependency relation constraint (R, L,UC) (denoted g), a set of need role dependency rela-tion constraints (R;L; UC)+ (denoted n), and mod-iee constraints represented as the lexical categoryof the modiee for each role (denoted m).
Table5 summarizes the knowledge sources that each ofthe probabilistic dependency grammar models uses.To determine whether the poor performance of thethree probabilistic dependency grammar models re-sults from our hypothesis that they utilize insucientknowledge, we will evaluate our SuperARV LM af-ter eliminating those knowledge sources that are notused by each of these models.
Additionally, we willevaluate the contribution of each of the knowledgesources to the predictiveness of our SuperARV LM.We use the methodology of selectively ignoring dif-ferent types of knowledge as constraints to evaluatethe knowledge source contributions to our SuperARVLM, as well as to approximate the performance ofthe other probabilistic dependency grammar models.The framework of CDG, on which our SuperARV LMis built, allows constraints to be tightened by addingmore knowledge sources or loosened by ignoring cer-tain knowledge.
The SuperARV structure inheritsthis capability from CDG; selective constraint re-laxation is implemented by eliminating one or moreknowledge source in K = fc; f; L; g; n;mg from theSuperARV structure.
We have constructed nine dif-ferent LMs based on reduced SuperARV structuresdenoted SARV-k (i.e., a SuperARV structure afterremoving k with k  K), where ?k represents thedeletion of a subset of knowledge types (e.g., f , mn,cgmn).
Each model is described next.Modiee constraints potentially hamper grammargenerality, and so we consider their impact by delet-ing them from the LM by using the SARV-m struc-ture.
Need roles are important for capturing thestructural requirements of dierent types of words(e.g., subcategorization), and we investigate their ef-fects by using the SARV-n structure.
The modelbased on SARV-L is built to investigate the im-portance of link type information.
We can investi-gate the contribution of the combination of m andn, fundamental to the enforcement of valency con-straints, by using the SARV-mn structure.
Themodel based on SARV-f is used to evaluate whetherOur HTK Lattices Chelba?s92-5k 93-5k 92-20k 93-20k 93-20k latticesLM WER(SAC) WER(SAC) WER(SAC) WER(SAC) WER(SAC)3gram 4.43(61.52) 6.91(43.26) 11.11(36.94) 14.74(30.52) 13.72(36.18)POS 3.92(64.85) 6.55(47.91) 10.58(38.14) 14.54(32.39) 13.51(37.96)cSuperARV 3.89(65.15) 6.42(48.84) 10.51(38.44) 14.45(32.86) 13.32 (38.22)SuperARV 3.83(65.76) 6.24(50.23) 10.15(39.64) 14.28(34.27) 12.87(42.02)Chelba 3.85(65.45) 6.26(49.77) 10.19(39.34) 14.36(34.27) 12.93(40.48)lattice 1.79(79.40) 2.16(73.95) 4.93(59.46) 6.65(52.11) 3.41 (68.86)accuracyTable 4: Comparing WER and SAC after rescoring lattices using each LM on WSJ CSR 5k- and 20k- test sets.knowledge PDG PLG ZPDG SARVsourceword identity X X lemma Xlexical X X Xcategory (c)lexical morpho- Xfeatures (f) logicalfeatureslink type (L) X X Xlink direction X X X X(UC)valency (n) Xmodiee X Xconstraints (m)Table 5: Knowledge sources used by the three prob-abilistic dependency grammar models compared to ourSuperARV LM.
Note link type is dened as L and linkdirection is dened as UC in the SuperARV structure.lexical features improve or degrade LM quality.
Themodel based on SARV-fmn is very similar to thestandard probabilistic dependency grammar LM, inwhich only word, POS, link type, and link di-rection information is used for probability estima-tions.
The model based on SARV-gmn uses a fea-ture augmentation of POS, and the model based onSARV-cgmn uses lexical features only.
Addition-ally, we built the model ZPDG-SARV to approxi-mate ZPDG.
Zeman?s PDG (Hajic et al, 1998) dif-fers signicantly from our original SuperARV LM inthat it ignores label information L and some lexi-cal feature information (the morphological tags donot include some lexical features having influenceon syntax, denoted syntactic lexical features, i.e.,gapp, inverted, mood, type, case, voice), and doesnot enforce valency constraints (instead, the modelonly counts the number of links associated with aword without discriminating whether the links repre-sent governing or linguistic structural requirements).Also, word identity information is not used, instead,the model uses a loose integration of a word?s lemmaand its morphological tag.
Given this analysis, webuilt the model ZPDG-SARV based on a structureincluding lexical category, morphological features,LM 92-5k 93-5k 92-20k 93-20k3gram 45.61 50.51 106.52 109.22SARV-cgmn 45.58 48.37 102.00 104.59ZPDG-SARV 45.50 47.98 101.89 104.21POS 44.21 30.26 98.79 96.64SARV-gmn 43.16 27.75 96.69 93.25SARV-fmn 45.01 27.42 96.23 93.16SARV-f 42.33 27.06 94.87 90.20SARV-mn 40.38 26.96 90.23 89.54SARV-n 35.02 26.08 87.32 88.04SARV-L 28.76 25.71 82.45 84.82SARV-m 26.86 25.58 80.24 83.12SARV 21.35 23.42 69.95 74.22Table 6: Comparing perplexity results for each LM onthe WSJ CSR test sets.
The LMs appear in decreasingorder of perplexity.and (G;UC;MC).Table 6 shows the perplexity results on the WSJCSR test sets ordered from highest to lowest foreach test set, with the best result for each in boldface.
The full SuperARV LM yields the lowest per-plexity.
We found that ignoring modiee constraints(SARV-m) increases perplexity the least, and ignor-ing link type information (SARV-L) and need roleconstraints (SARV-n) are a little worse than that.Ignoring both knowledge sources (SARV-mn) shouldresult in even greater degradation, which is veriedby the results.
However, ignoring lexical features(SARV-f) produces an even greater increase in per-plexity than relaxing both m and n. The SARV-fmn, which is closest to the traditional probabilisticdependency grammar LM, shows fairly poor qual-ity, not much better than the POS LM.
One mighthypothesize that lexical features individually con-tribute the most to the overall performance of the Su-perARV LM.
However, using this knowledge sourceby itself (SARV-cgmn) results in dramatic degrada-tion on perplexity, in fact even worse than that of thePOS LM, but still slightly better than the baselinetrigram.
However, as demonstrated by SARV-gmn,the constraints from lexical features are strength-ened by combining them with POS.
Given the de-scriptions in Table 5, we can approximate PLG bya model based on a SuperARV structure eliminat-ing f and m (which should have a quality betweenSARV-f and SARV-fmn).
It is noticeable that with-out word identity information, syntactic lexical fea-tures, and valency constraints, the ZPDG-SARV LMperforms worse than the POS-based LM and onlyslightly better than the LM based on SARV-cgmn.This suggests that ZPDG can be strengthened byincorporating more knowledge.The same ranking of the performance of the LMswas obtained for WER/SAC after rescoring the lat-tices using each LM, as shown in Table 7.
Our exper-iments with relaxed SuperARV LMs suggest likelymethods for improving PDG, PLG, and ZPDG mod-els.
The tight integration of word identity, lexicalcategory, lexical features, and structural dependencyconstraints is likely to improve their performance.Clearly the investigated knowledge sources are quitesynergistic, and their tight integration achieves thegreatest improvement on both perplexity and WER.5 ConclusionsWe have compared our SuperARV LM to a variety ofLMs and found that it achieves both perplexity andWER reductions compared to a trigram, and despitethe fact that it is an almost-parsing LM, it outper-forms (or performs comparably to) the more com-plex parser-based LMs on both perplexity and rescor-ing accuracy.
Additional experiments reveal that se-lecting a joint instead of a conditional probabilisticmodel is an important factor in the performance ofour SuperARV LM.
The SuperARV structure pro-vides a flexible framework that tightly couples a va-riety of knowledge sources without combinatorial ex-plosion.
We found that although each knowledgesource contributes to the performance of the LM, itis the tight integration of the word level knowledgesources (word identity, POS, and lexical features) to-gether with the structural information of governorand subcategorization dependencies that producesthe best level of LM performance.
We are currentlyextending the almost-parsing SuperARV LM to a fullparser-based LM.6 AcknowledgmentsThis research was supported by Intel, Purdue Re-search Foundation, and National Science Founda-tion under Grant No.
IRI 97-04358, CDA 96-17388,and BCS-9980054.
We would like to thank theanonymous reviewers for their comments and sug-gestions.
We would also like to thank Dr. Charniak,Dr.
Chelba, and Dr. Srinivas for their help with thisresearch eort.
Finally, we would like to thank YangLiu (Purdue University) for providing us with theWSJ CSR test set lattices.ReferencesR.
Bod.
2001.
What is the minimal set of fragmentsthat achieves maximal parse accuracy?
In Pro-ceedings of ACL?2001.E.
Charniak, D. Blaheta, N. Ge, K. Hall, andM.
Johnson.
2000.
BLLIP WSJ Corpus.
CD-ROM.
Linguistics Data Consortium.E.
Charniak.
2001.
Immediate-head parsing for lan-guage models.
In Proceedings of ACL?2001.C.
Chelba, F. Jelinek, and S. Khudanpur.
1997.Structure and performance of a dependency lan-guage model.
In Proceedings of Eurospeech, vol-ume 5, pages 2775{2778.C.
Chelba.
2000.
Exploiting Syntactic Structure forNatural Language Modeling.
Ph.D. thesis, JohnsHopkins University.S.
F. Chen and J. T. Goodman.
1998.
An empir-ical study of smoothing techniques for languagemodeling.
Technical report, Harvard University,Computer Science Group.M.
J. Collins.
1996.
A new statistical parser basedon bigram lexical dependencies.
In Proceedings ofACL?1996, pages 184{191.Entropic Cambridge Research Laboratory, Ltd.,1997.
HTK: Hidden Markov Model Toolkit V2.1.E.
W. Fong and D. Wu.
1995.
Learning restrictedprobabilistic link grammars.
Technical ReportHKUST-CS95-27, University of Science and Tech-nology, Clear Water Bay, Hong Kong.L.
Galescu and E. K. Ringger.
1999.
Augmentingwords with linguistic information for n-gram lan-guage models.
In Proceedings of Eurospeech.J.
Goodman.
1997.
Probabilistic feature grammars.In Proceedings of the Fourth international work-shop on parsing technologies.J.
Goodman.
2001.
A bit of progress in languagemodeling, extended version.
Technical ReportMSR-TR-2001-72, Microsoft Research, Redmond,WA.J.
Hajic, E. Brill, M. Collins, B. Hladka, D. Jones,C.
Kuo, L. Ramshaw, O. Schwartz, C. Tillmann,and D. Zeman.
1998.
Core natural languageprocessing technology applicable to multiple lan-guages { Workshop ?98.
Technical report, JohnsHopkins Univ.J.
Hajic.
1998.
Building a syntactically anno-tated corpus: The Prague Dependency Treekbank.In Issues of Valency and Meaning ( Festschriftfor Jarmila Panevova), pages 106{132.
Carolina,Charles University, Prague.92-5k 93-5k 92-20k 93-20kLM WER(SAC) WER(SAC) WER(SAC) WER(SAC)3gram 4.43(61.52) 6.91(43.26) 11.11(36.94) 14.74(30.52)SARV-cgmn 4.11(62.12) 6.78(45.12) 10.92(37.24) 14.63(31.46)ZPDG-SARV 4.11(62.44) 6.71(46.02) 10.92(37.24) 14.63(31.65)POS 3.92(64.85) 6.55(47.91) 10.58(38.14) 14.54(32.39)SARV-gmn 3.92(64.85) 6.52(47.91) 10.56(38.14) 14.51(32.39)SARV-fmn 3.92(64.85) 6.50(48.37) 10.53(38.14) 14.51(32.39)SARV-f 3.92(64.85) 6.47(48.37) 10.49(38.14) 14.45(32.86)SARV-mn 3.92(64.85) 6.44(48.37) 10.47(38.44) 14.42(32.86)SARV-n 3.89(65.15) 6.39(48.37) 10.40(38.74) 14.39(33.33)SARV-L 3.85(65.15) 6.29(48.92) 10.32(39.04) 14.39(33.33)SARV-m 3.85(65.15) 6.29(49.77) 10.24(39.34) 14.35(33.80)SARV 3.83(65.76) 6.24(50.23) 10.15(39.34) 14.28(34.27)Table 7: Comparing WER and SAC after rescoring lattices using each LM on WSJ CSR 5k- and 20k- test sets.
TheLMs appear in decreasing order of WER.M.
P. Harper and R. A. Helzerman.
1995.
Exten-sions to constraint dependency parsing for spokenlanguage processing.
Computer Speech and Lan-guage, 9:187{234.M.
P. Harper and W. Wang.
2001.
Approaches forlearning constraint dependency grammar from cor-pora.
In Proceedings of the Grammar and Nat-ural Language Processing Conference, Montreal,Canada.M.
P. Harper, S. A. Hockema, and C. M. White.1999.
Enhanced constraint dependency grammarparsers.
In Proceedings of the IASTED Interna-tional Conference on Articial Intelligence andSoft Computing.M.
P. Harper, C. M. White, W. Wang, M. T.Johnson, and R. A. Helzerman.
2000.
Eec-tiveness of corpus-induced dependency grammarsfor post-processing speech.
In Proceedings ofNAACL?2000.P.
A. Heeman.
1998.
POS tagging versus classesin language modeling.
In Proceedings of the 6thWorkshop on Very Large Corpora, Montreal.F.
Jelinek.
1990.
Self-organized language modelingfor speech recognition.
In Alex Waibel and Kai-FuLee, editors, Readings in Speech Recognition.
Mor-gan Kaufman Publishers, Inc., San Mateo, CA.M.
Johnson.
2001.
Joint and conditional estimationof tagging and parsing models.
In Proceedings ofACL?2001.J.
D. Laerty, D. Sleator, and D. Temperley.
1992.Grammatical trigrams: A probabilistic model oflink grammar.
In Proc.
of AAAI Fall Symp.
Prob-abilistic Approaches to Natural Language, Cam-bridge, MA.J.
Laerty, A. McCallum, and F. Pereira.
2001.
Con-ditional Random Fields: Probabilistic models forsegmenting and labeling sequence data.
In Pro-ceedings of ICML?2001.M.
P. Marcus, B. Santorini, and M. A.Marcinkiewicz.
1993.
Building a large anno-tated corpus of English: The Penn Treebank.Computational Linguistics, 19(2):313{330.H.
Maruyama.
1990.
Structural disambiguationwith constraint propagation.
In The Proceedingsof ACL?1990, pages 31{38.T.
R. Niesler and P. C. Woodland.
1996.
A variable-length category-based N-gram language model.
InProceedings of ICASSP, volume 1, pages 164{167.W.
H. Press, B. P. Flannery, S. A. Teukolsky, andW.
T. Vetterling.
1988.
Numerical Recipes in C.Cambridge University Press, Cambridge.P.
J.
Price, W. Fischer, J. Bernstein, and D. Pallett.1988.
A database for continuous speech recogni-tion in a 1000-word domain.
In Proceedings ofICASSP?1988, pages 651{654.B.
Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguis-tics, 27(2):249{276.R.
Rosenfeld.
2000.
Two decades of statistical lan-guage modeling: Where do we go from here?
Pro-ceedings of the IEEE, 88:1270{1278.B.
Srinivas.
1997.
Complexity of lexical descriptionsand its relevance to partial parsing.
Ph.D. thesis,University of Pennsylvania.W.
Wang and M. P. Harper.
2001.
Investigatingprobabilistic constraint dependency grammars inlanguage modeling.
Technical Report TR-ECE-01-4, Purdue University, School of Electrical En-gineering.W.
Wang, Y. Liu, and M. P. Harper.
2002.
Rescor-ing eectiveness of language models using dierentlevels of knowledge and their integration.
In Pro-ceedings of ICASSP?2002.
