Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 385?392Manchester, August 2008Word Lattice Reranking for Chinese Word Segmentation andPart-of-Speech TaggingWenbin Jiang ?
?
Haitao Mi ?
?
Qun Liu ?
?Key Lab.
of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China?Graduate University of Chinese Academy of SciencesBeijing, 100049, China{jiangwenbin,htmi,liuqun}@ict.ac.cnAbstractIn this paper, we describe a new rerank-ing strategy named word lattice reranking,for the task of joint Chinese word segmen-tation and part-of-speech (POS) tagging.As a derivation of the forest rerankingfor parsing (Huang, 2008), this strategyreranks on the pruned word lattice, whichpotentially contains much more candidateswhile using less storage, compared withthe traditional n-best list reranking.
With aperceptron classifier trained with local fea-tures as the baseline, word lattice rerank-ing performs reranking with non-local fea-tures that can?t be easily incorporated intothe perceptron baseline.
Experimental re-sults show that, this strategy achieves im-provement on both segmentation and POStagging, above the perceptron baseline andthe n-best list reranking.1 IntroductionRecent work for Chinese word segmentation andPOS tagging pays much attention to discriminativemethods, such as Maximum Entropy Model (ME)(Ratnaparkhi and Adwait, 1996), Conditional Ran-dom Fields (CRFs) (Lafferty et al, 2001), percep-tron training algorithm (Collins, 2002), etc.
Com-pared to generative ones such as Hidden MarkovModel (HMM) (Rabiner, 1989; Fine et al, 1998),discriminative models have the advantage of flexi-bility in representing features, and usually obtainsalmost perfect accuracy in two tasks.Originated by Xue and Shen (2003), the typ-ical approach of discriminative models conductsc?
2008.
Licensed to the Coling 2008 Organizing Com-mittee for publication in Coling 2008 and for re-publishing inany form or medium.segmentation in a classification style, by assign-ing each character a positional tag indicating itsrelative position in the word.
If we extend thesepositional tags to include POS information, seg-mentation and POS tagging can be performed by asingle pass under a unify classification framework(Ng and Low, 2004).
In the rest of the paper, wecall this operation mode Joint S&T.
Experimentsof Ng and Low (2004) shown that, compared withperforming segmentation and POS tagging one ata time, Joint S&T can achieve higher accuracy notonly on segmentation but also on POS tagging.Besides the usual local features such as thecharacter-based ones (Xue and Shen, 2003; Ngand Low, 2004), many non-local features relatedto POSs or words can also be employed to improveperformance.
However, as such features are gener-ated dynamically during the decoding procedure,incorporating these features directly into the clas-sifier results in problems.
First, the classifier?s fea-ture space will grow much rapidly, which is apt tooverfit on training corpus.
Second, the variance ofnon-local features caused by the model evolutionduring the training procedure will hurt the param-eter tuning.
Last but not the lest, since the cur-rent predication relies on the results of prior predi-cations, exact inference by dynamic programmingcan?t be obtained, and then we have to maintain an-best candidate list at each considering position,which also evokes the potential risk of depress-ing the parameter tuning procedure.
As a result,many theoretically useful features such as higher-order word- or POS- grams can not be utilized ef-ficiently.A widely used approach of using non-localfeatures is the well-known reranking technique,which has been proved effective in many NLPtasks, for instance, syntactic parsing and machine385v0v1v2v3v4v5v6v7C1:eC2:?
C3:UC4:/ C5:?
C6:?C7:YNNVVNNMNNNNNNNNVVNNNNFigure 1: Pruned word lattice as directed graph.
The character sequence we choose is ?e-?-U-/-?-?-Y?.
For clarity, we represent each subsequence-POS pair as a single edge, while ignore thecorresponding scores of the edges.translation (Collins, 2000; Huang, 2008), etc.
Es-pecially, Huang (2008) reranked the packed for-est, which contains exponentially many parses.Inspired by his work, we propose word latticereranking, a strategy that reranks the pruned wordlattice outputted by a baseline classifier, rather thanonly a n-best list.
Word lattice, a directed graph asshown in Figure 1, is a packed structure that canrepresent many possibilities of segmentation andPOS tagging.
Our experiments on the Penn Chi-nese Treebank 5.0 show that, reranking on wordlattice gains obvious improvement over the base-line classifier and the reranking on n-best list.Compared against the baseline, we obtain an errorreduction of 11.9% on segmentation, and 16.3%on Joint S&T.2 Word LatticeFormally, a word lattice L is a directed graph?V,E?, where V is the node set, and E is theedge set.
Suppose the word lattice is for sentenceC1:n= C1..Cn, node vi?
V (i = 1..n ?
1) de-notes the position between Ciand Ci+1, while v0before C1is the source node, and vnafter Cnisthe sink node.
An edge e ?
E departs from vbandarrives at ve(0 ?
b < e ?
n), it covers a subse-quence of C1:n, which is recognized as a possibleword.
Considering Joint S&T, we label each edgea POS tag to represent a word-POS pair.
A seriesof adjoining edges forms a path, and a path con-necting the source node and the sink node is calleddiameter, which indicates a specific pattern of seg-mentation and POS tagging.
For a diameter d, |d|denotes the length of d, which is the count of edgescontained in this diameter.
In Figure 1, the pathp?= v?0v3?
v?3v5?
v?5v7is a diameter, and|p?| is 3.2.1 Oracle Diameter in LatticeGiven a sentence s, its reference r and prunedword lattice L generated by the baseline classi-fier, the oracle diameter d?
of L is define as thediameter most similar to r. With F-measure as thescoring function, we can identify d?
using the al-gorithm depicted in Algorithm 1, which is adaptedto lexical analysis from the forest oracle computa-tion of Huang (2008).Before describe this algorithm in detail, we de-pict the key point for finding the oracle diameter.Given the system?s output y and the reference y?,using |y| and |y?| to denote word counts of themrespectively, and |y ?
y?| to denote matched wordcount of |y| and |y?|, F-measure can be computedby:F (y, y?)
=2PRP + R=2|y ?
y?||y| + |y?|(1)Here, P = |y?y?||y|is precision, and R = |y?y?||y?|is recall.
Notice that F (y, y?)
isn?t a linear func-tion, we need access the largest |y ?
y?| for eachpossible |y| in order to determine the diameter withmaximum F , or another word, we should know themaximum matched word count for each possiblediameter length.The algorithm shown in Algorithm 1 works ina dynamic programming manner.
A table nodeT [i, j] is defined for sequence span [i, j], and it hasa structure S to remember the best |yi:j?
y?i:j| foreach |yi:j|, as well as the back pointer for this bestchoice.
The for-loop in line 2 ?
14 processes foreach node T [i, j] in a shorter-span-first order.
Line3?
7 initialize T [i, j] according to the reference rand the word lattice?s edge set L ?E.
If there existsan edge e in L ?E covering the span [i, j], then we386Algorithm 1 Oracle Diameter, U la Huang (2008,Sec.
4.1).1: Input: sentence s, reference r and lattice L2: for [i, j] ?
[1, |s|] in topological order do3: if ?e ?
L ?
E s.t.
e spans from i to j then4: if e ?
label exists in r then5: T [i, j] ?
S[1]?
16: else7: T [i, j] ?
S[1]?
08: for k s.t.
T [i, k ?
1] and T [k, j] defined do9: for p s.t.
T [i, k ?
1] ?
S[p] defined do10: for q s.t.
T [k, j] ?
S[q] defined do11: n?
T [i, k ?
1] ?
S[p] + T [k, j] ?
S[q]12: if n > T [i, j] ?
S[p + q] then13: T [i, j] ?
S[p + q]?
n14: T [i, j] ?
S[p + q] ?
bp?
?k, p, q?15: t?
argmaxt2?T [1,|s|]?S[t]t+|r|16: d?
?
Tr(T [1, |s|] ?
S[t].bp)17: Output: oracle diameter: d?define T [i, j], otherwise we leave this node unde-fined.
In the first situation, we initialize this node?sS structure according to whether the word-POSpair of e is in the reference (line 4?7).
Line 8?14update T [i, j]?s S structure using the S structuresfrom all possible child-node pair, T [i, k ?
1] andT [k, j].
Especially, line 9?
10 enumerate all com-binations of p and q, where p and q each repre-sent a kind of diameter length in T [i, k ?
1] andT [k, j].
Line 12 ?
14 refreshes the structure Sof node T [i, j] when necessary, and meanwhile,a back pointer ?k, p, q?
is also recorded.
Whenthe dynamic programming procedure ends, we se-lect the diameter length t of the top node T [1, |s|],which maximizes the F-measure formula in line15, then we use function Tr to find the oracle di-ameter d?
by tracing the back pointer bp.2.2 Generation of the Word LatticeWe can generate the pruned word lattice using thebaseline classifier, with a slight modification.
Theclassifier conducts decoding by considering eachcharacter in a left-to-right fashion.
At each consid-ering position i, the classifier enumerates all can-didate results for subsequence C1:i, by attachingeach current candidate word-POS pair p to the tailof each candidate result at p?s prior position, asthe endmost of the new generated candidate.
Wegive each p a score, which is the highest, amongall C1:i?s candidates that have p as their endmost.Then we select N word-POS pairs with the high-est scores, and insert them to the lattice?s edge set.This approach of selecting edges implies that, forthe lattice?s node set, we generate a node viat eachposition i.
Because N is the limitation on the countAlgorithm 2 Lattice generation algorithm.1: Input: character sequence C1:n2: E ?
?3: for i?
1 .. n do4: cands?
?5: for l?
1 .. min(i, K) do6: w ?
Ci?l+1:i7: for t ?
POS do8: p?
?w, t?9: p ?
score?
Eval(p)10: s?
p ?
score + Best[i?
l]11: Best[i]?
max(s,Best[i])12: insert ?s, p?
into cands13: sort cands according to s14: E ?
E ?
cands[1..N ] ?
p15: Output: edge set of lattice: Eof edges that point to the node at position i, we callthis pruning strategy in-degree pruning.
The gen-eration algorithm is shown in Algorithm 2.Line 3 ?
14 consider each character Ciin se-quence, cands is used to keep the edges closing atposition i.
Line 5 enumerates the candidate wordsending with Ciand no longer than K, where Kis 20 in our experiments.
Line 5 enumerates allPOS tags for the current candidate word w, wherePOS denotes the POS tag set.
Function Eval inline 9 returns the score for word-POS pair p fromthe baseline classifier.
The array Best preserve thescore for sequence C1:i?s best labelling results.
Af-ter all possible word-POS pairs (or edges) consid-ered, line 13?
14 select the N edges we want, andadd them to edge set E.Though this pruning strategy seems relativerough ?
simple pruning for edge set while nopruning for node set, we still achieve a promisingimprovement by reranking on such lattices.
We be-lieve more elaborate pruning strategy will resultsin more valuable pruned lattice.3 RerankingA unified framework can be applied to describingreranking for both n-best list and pruned word lat-tices (Collins, 2000; Huang, 2008).
Given the can-didate set cand(s) for sentence s, the reranker se-lects the best item y?
from cand(s):y?
= argmaxy?cand(s)w ?
f(y) (2)For reranking n-best list, cand(s) is simply the setof n best results from the baseline classifier.
Whilefor reranking word lattice, cand(s) is the set ofall diameters that are impliedly built in the lattice.w ?
f(y) is the dot product between a feature vec-tor f and a weight vector w, its value is used to387Algorithm 3 Perceptron training for reranking1: Input: Training examples{cand(si), y?i}Ni=12: w?
03: for t?
1 .. T do4: for i?
1 .. N do5: y?
?
argmaxy?cand(si)w ?
f(y)6: if y?
6= y?ithen7: w?
w + f(y?i)?
f(y?
)8: Output: Parameters: wNon-local Template CommentW0T0current word-POS pairW?1word 1-gram before W0T0T?1POS 1-gram before W0T0T?2T?1POS 2-gram before W0T0T?3T?2T?1POS 3-gram before W0T0Table 1: Non-local feature templates used forrerankingrerank cand(s).
Following usual practice in pars-ing, the first feature f1(y) is specified as the scoreoutputted by the baseline classifier, and its valueis a real number.
The other features are non-localones such as word- and POS- n-grams extractedfrom candidates in n-best list (for n-best rerank-ing) or diameters (for word lattice reranking), andthey are 0 ?
1 valued.3.1 Training of the RerankerWe adopt the perceptron algorithm (Collins, 2002)to train the reranker.
as shown in Algorithm 3.
Weuse a simple refinement strategy of ?averaged pa-rameters?
of Collins (2002) to alleviate overfittingon the training corpus and obtain more stable per-formance.For every training example {cand(si), y?i}, y?idenotes the best candidate in cand(si).
For n-best reranking, the best candidate is easy to find,whereas for word lattice reranking, we should usethe algorithm in Algorithm 1 to determine the or-acle diameter, which represents the best candidateresult.3.2 Non-local Feature TemplatesThe non-local feature templates we use to train thereranker are listed in Table 1.
Notice that all fea-tures generated from these templates don?t contain?future?
words or POS tags, it means that we onlyuse current or history word- or POS- n-grams toevaluate the current considering word-POS pair.Although it is possible to use ?future?
informationin n-best list reranking, it?s not the same when wererank the pruned word lattice.
As we have to tra-verse the lattice topologically, we face difficulty inAlgorithm 4 Cube pruning for non-local features.1: function CUBE(L)2: for v ?
L ?
V in topological order do3: NBEST(v)4: return Dvsink[1]5: procedure NBEST(v)6: heap?
?7: for v?
topologically before v do8: ??
all edges from v?
to v9: p?
?Dv?,?
?10: ?p,1??score?
Eval(p,1)11: PUSH(?p,1?, heap)12: HEAPIFY(heap)13: buf ?
?14: while |heap| > 0 and |buf | < N do15: item?
POP-MAX(heap)16: append item to buf17: PUSHSUCC(item, heap)18: sort buf to Dv19: procedure PUSHSUCC(?p, j?, heap)20: p is ?vec1,vec2?21: for i?
1..2 do22: j?
?
j+ bi23: if |veci| ?
j?ithen24: ?p, j???score?
Eval(p, j?
)25: PUSH(?p, j?
?, heap)utilizing the information ahead of the current con-sidering node.3.3 Reranking by Cube PruningBecause of the non-local features such as word-and POS- n-grams, the reranking procedure is sim-ilar to machine translation decoding with inter-grated language models, and should maintain alist of N best candidates at each node of the lat-tice.
To speed up the procedure of obtaining theN best candidates, following Huang (2008, Sec.3.3), we adapt the cube pruning method from ma-chine translation (Chiang, 2007; Huang and Chi-ang 2007) which is based on efficient k-best pars-ing algorithms (Huang and Chiang, 2005).As shown in Algorithm 4, cube pruning workstopologically in the pruned word lattice, and main-tains a list of N best derivations at each node.When deducing a new derivation by attaching acurrent word-POS pair to the tail of a antecedentderivation, a function Eval is used to compute thenew derivation?s score (line 10 and 24).
We usea max-heap heap to hold the candidates for thenext-best derivation.
Line 7 ?
11 initialize heapto the set of top derivations along each deducingsource, the vector pair ?Dvhead,?
?.Here, ?
de-notes the vector of current word-POS pairs, whileDvheaddenotes the vector of N best derivationsat ?
?s antecedent node.
Then at each iteration,388Non-lexical-target InstancesCn(n = ?2..2) C?2=e, C?1=?, C0=U, C1=/, C2=?CnCn+1(n = ?2..1) C?2C?1=e?, C?1C0=?U, C0C1=U/, C1C2=/?C?1C1C?1C1=?/Lexical-target InstancesC0Cn(n = ?2..2) C0C?2=Ue, C0C?1=U?, C0C0=UU, C0C1=U/, C0C2=U?C0CnCn+1(n = ?2..1) C0C?2C?1=Ue?, C0C?1C0=U?U, C0C0C1=UU/, C0C1C2=U/?C0C?1C1C0C?1C1= U?/Table 2: Feature templates and instances.
Suppose we consider the third character ?U?
in the sequence?e?U/?
?.we pop the best derivation from heap (line 15),and push its successors into heap (line 17), untilwe get N derivations or heap is empty.
In line 22of function PUSHSUCC, j is a vector composed oftwo index numbers, indicating the two candidates?indexes in the two vectors of the deducing sourcep, where the two candidates are selected to deducea new derivation.
j?
is a increment vector, whoseith dimension is 1, while others are 0.
As non-local features (word- and POS- n-grams) are usedby function Eval to compute derivation?s score,the derivations extracted from heap may be out oforder.
So we use a buffer buf to keep extractedderivations (line 16), then sort buf and put its firstN items to Dv(line 18).4 Baseline Perceptron Classifier4.1 Joint S&T as ClassificationFollowing Jiang et al (2008), we describe segmen-tation and Joint S&T as below:For a given Chinese sentence appearing as acharacter sequence:C1:n= C1C2.. Cnthe goal of segmentation is splitting the sequenceinto several subsequences:C1:e1Ce1+1:e2.. Cem?1+1:emWhile in Joint S&T, each of these subsequences islabelled a POS tag:C1:e1/t1Ce1+1:e2/t2.. Cem?1+1:em/tmWhere Ci(i = 1..n) denotes a character, Cl:r(l ?r) denotes the subsequence ranging from Clto Cr,and ti(i = 1..m,m ?
n) denotes the POS tag ofCei?1+1:ei.If we label each character a positional tag in-dicating its relative position in an expected subse-quence, we can obtain the segmentation result ac-cordingly.
As described in Ng and Low (2004) andJiang et al (2008), we use s indicating a single-character word, while b, m and e indicating the be-gin, middle and end of a word respectively.
Withthese positional tags, the segmentation transformsto a classification problem.
For Joint S&T, weexpand positional tags by attaching POS to theirtails as postfix.
As each tag now contains bothpositional- and POS- information, Joint S&T canalso be resolved in a classification style frame-work.
It means that, a subsequence is a word withPOS t, only if the positional part of the tag se-quence conforms to s or bm?e pattern, and eachelement in the POS part equals to t. For example,a tag sequence b NN m NN e NN represents athree-character word with POS tag NN .4.2 Feature TemplatesThe features we use to build the classifier are gen-erated from the templates of Ng and Low (2004).For convenience of comparing with other, theydidn?t adopt the ones containing external knowl-edge, such as punctuation information.
All theirtemplates are shown in Table 2.
C denotes a char-acter, while its subscript indicates its position rela-tive to the current considering character(it has thesubscript 0).The table?s upper column lists the templates thatimmediately from Ng and Low (2004).
theynamed these templates non-lexical-target becausepredications derived from them can predicate with-out considering the current character C0.
Tem-plates called lexical-target in the column below areintroduced by Jiang et al (2008).
They are gener-ated by adding an additional field C0to each non-lexical-target template, so they can carry out pred-ication not only according to the context, but alsoaccording to the current character itself.Notice that features derived from the templatesin Table 2 are all local features, which means allfeatures are determined only by the training in-stances, and they can be generated before the train-ing procedure.389Algorithm 5 Perceptron training algorithm.1: Input: Training examples (xi, yi)2: ??
03: for t?
1 .. T do4: for i?
1 .. N do5: zi?
argmaxz?GEN(xi)?
(xi, z) ?
?6: if zi6= yithen7: ??
?
+?
(xi, yi)??
(xi, zi)8: Output: Parameters: ?4.3 Training of the ClassifierCollins (2002)?s perceptron training algorithmwere adopted again, to learn a discriminative clas-sifier, mapping from inputs x ?
X to outputsy ?
Y .
Here x is a character sequence, and y isthe sequence of classification result of each char-acter in x.
For segmentation, the classification re-sult is a positional tag, while for Joint S&T, it isan extended tag with POS information.
X denotesthe set of character sequence, while Y denotes thecorresponding set of tag sequence.According to Collins (2002), the functionGEN(x) generates all candidate tag sequences forthe character sequence x , the representation ?maps each training example (x, y) ?
X ?
Y toa feature vector ?
(x, y) ?
Rd, and the parametervector ?
?
Rd is the weight vector correspondingto the expected perceptron model?s feature space.For a given input character sequence x, the missionof the classifier is to find the tag sequence F (x)satisfying:F (x) = argmaxy?GEN(x)?
(x, y) ?
?
(3)The inner product ?
(x, y) ?
?
is the score of theresult y given x, it represents how much plausiblywe can label character sequence x as tag sequencey.
The training algorithm is depicted in Algorithm5.
We also use the ?averaged parameters?
strategyto alleviate overfitting.5 ExperimentsOur experiments are conducted on the Penn Chi-nese Treebank 5.0 (CTB 5.0).
Following usualpractice of Chinese parsing, we choose chapters1?260 (18074 sentences) as the training set, chap-ters 301?
325 (350 sentences) as the developmentset, and chapters 271 ?
300 (348 sentences) asthe final test set.
We report the performance ofthe baseline classifier, and then compare the per-formance of the word lattice reranking against the0.90.910.920.930.940.950.960  1  2  3  4  5  6  7  8  9  10F-measurenumber of iterationsPerceptron Learning CurvesSegmentationJoint STFigure 2: Baseline averaged perceptron learningcurves for segmentation and Joint S&T.n-best reranking, based on this baseline classifier.For each experiment, we give accuracies on seg-mentation and Joint S&T.
Analogous to the situa-tion in parsing, the accuracy of Joint S&T meansthat, a word-POS is recognized only if both thepositional- and POS- tags are correctly labelled foreach character in the word?s span.5.1 Baseline Perceptron ClassifierThe perceptron classifier are trained on the train-ing set using features generated from the templatesin Table 2, and the development set is used todetermine the best parameter vector.
Figure 2shows the learning curves for segmentation andJoint S&T on the development set.
We choosethe averaged parameter vector after 7 iterations forthe final test, this parameter vector achieves an F-measure of 0.973 on segmentation, and 0.925 onJoint S&T.
Although the accuracy on segmentationis quite high, it is obviously lower on Joint S&T.Experiments of Ng and Low (2004) on CTB 3.0also shown the similar trend, where they obtainedF-measure 0.952 on segmentation, and 0.919 onJoint S&T.5.2 Preparation for RerankingFor n-best reranking, we can easily generate n bestresults for every training instance, by a modifica-tion for the baseline classifier to hold n best can-didates at each considering point.
For word latticereranking, we use the algorithm in Algorithm 2 togenerate the pruned word lattice.
Given a traininginstance si, its n best result list or pruned wordlattice is used as a reranking instance cand(si),the best candidate result (of the n best list) or or-acle diameter (of the pruned word lattice) is thereranking target y?i.
We find the best result of then best results simply by computing each result?s390F-measure, and we determine the oracle diame-ter of the pruned word lattice using the algorithmdepicted in Algorithm 1.
All pairs of cand(si)and y?ideduced from the baseline model?s traininginstances comprise the training set for reranking.The development set and test set for reranking areobtained in the same way.
For the reranking train-ing set {cand(si), y?i}Ni=1, {y?i}Ni=1is called oracleset, and the F-measure of {y?i}Ni=1against the ref-erence set is called oracle F-measure.
We use theoracle F-measure indicating the utmost improve-ment that an reranking algorithm can achieve.5.3 Results and AnalysisThe flows of the n-best list reranking and thepruned word lattice reranking are similar to thetraining procedure for the baseline classifier.
Thetraining set for reranking is used to tune the param-eter vector of the reranker, while the developmentset for reranking is used to determine the optimalnumber of iterations for the reranker?s training pro-cedure.We compare the performance of the word lat-tice reranking against the n-best list reranking.
Ta-ble 3 shows the experimental results.
The up-per four rows are the experimental results for n-best list reranking, while the four rows below arefor word lattice reranking.
In n-best list rerank-ing, with list size 20, the oracle F-measure onJoint S&T is 0.9455, and the reranked F-measureis 0.9280.
When list size grows up to 50, the oracleF-measure on Joint S&T jumps to 0.9552, whilethe reranked F-measure becomes 0.9302.
How-ever, when n grows to 100, it brings tiny improve-ment over the situation of n = 50.
In word lat-tice reranking, there is a trend similar to that inn-best reranking, the performance difference be-tween in degree = 2 and in degree = 5 is ob-vious, whereas the setting in degree = 10 doesnot obtain a notable improvement over the perfor-mance of in degree = 5.
We also notice that evenwith a relative small in degree limitation, such asin degree = 5, the oracle F-measures for seg-mentation and Joint S&T both reach a quite highlevel.
This indicates the pruned word lattice con-tains much more possibilities of segmentation andtagging, compared to n-best list.With the setting in degree = 5, the oracle F-measure on Joint S&T reaches 0.9774, and thereranked F-measure climbs to 0.9336.
It achievesan error reduction of 16.3% on Joint S&T, and anerror reduction of 11.9% on segmentation, over then-best Ora Seg Tst Seg Ora S&T Tst S&T20 0.9827 0.9749 0.9455 0.928050 0.9903 0.9754 0.9552 0.9302100 0.9907 0.9755 0.9558 0.9305Degree Ora Seg Rnk Seg Ora S&T Rnk S&T2 0.9898 0.9753 0.9549 0.92965 0.9927 0.9774 0.9768 0.933610 0.9934 0.9774 0.9779 0.9337Table 3: Performance of n-best list reranking andword lattice reranking.
n-best: the size of the n-best list for n-best list reranking; Degree: the in de-gree limitation for word lattice reranking; Ora Seg:oracle F-measure on segmentation of n-best lists orword lattices; Ora S&T: oracle F-measure on JointS&T of n-best lists or word lattices; Rnk Seg: F-measure on segmentation of reranked result; RnkS&T: F-measure on Joint S&T of reranked resultbaseline classifier.
While for n-best reranking withsetting n = 50, the Joint S&T?s error reduction is6.9% , and the segmentation?s error reduction is8.9%.
We can see that reranking on pruned wordlattice is a practical method for segmentation andPOS tagging.
Even with a much small data rep-resentation, it obtains obvious advantage over then-best list reranking.Comparing between the baseline and the tworeranking techniques, We find the non-local infor-mation such as word- or POS- grams do improveaccuracy of segmentation and POS tagging, andwe also find the reranking technique is effective toutilize these kinds of information.
As even a smallscale n-best list or pruned word lattice can achievea rather high oracle F-measure, reranking tech-nique, especially the word lattice reranking wouldbe a promising refining strategy for segmentationand POS tagging.
This is based on this viewpoint:On the one hand, compared with the initial inputcharacter sequence, the pruned word lattice has aquite smaller search space while with a high ora-cle F-measure, which enables us to conduct moreprecise reranking over this search space to find thebest result.
On the other hand, as the structure ofthe search space is approximately outlined by thetopological directed architecture of pruned wordlattice, we have a much wider choice for feature se-lection, which means that we would be able to uti-lize not only features topologically before the cur-rent considering position, just like those depictedin Table 2 in section 4, but also information topo-logically after it, for example the next word W1orthe next POS tag T1.
We believe the pruned word391lattice reranking technique will obtain higher im-provement, if we develop more precise rerankingalgorithm and more appropriate features.6 ConclusionThis paper describes a reranking strategy calledword lattice reranking.
As a derivation of the for-est reranking of Huang (2008), it performs rerank-ing on pruned word lattice, instead of on n-bestlist.
Using word- and POS- gram information, thisreranking technique achieves an error reduction of16.3% on Joint S&T, and 11.9% on segmentation,over the baseline classifier, and it also outperformsreranking on n-best list.
It confirms that word lat-tice reranking can effectively use non-local infor-mation to select the best candidate result, from arelative small representation structure while with aquite high oracle F-measure.
However, our rerank-ing implementation is relative coarse, and it musthave many chances for improvement.
In futurework, we will develop more precise pruning al-gorithm for word lattice generation, to further cutdown the search space while maintaining the ora-cle F-measure.
We will also investigate the featureselection strategy under the word lattice architec-ture, for effective use of non-local information.AcknowledgementThis work was supported by National Natural Sci-ence Foundation of China, Contracts 60736014and 60573188, and 863 State Key Project No.2006AA010108.
We show our special thanks toLiang Huang for his valuable suggestions.ReferencesCollins, Michael.
2000.
Discriminative reranking fornatural language parsing.
In Proceedings of the17th International Conference on Machine Learn-ing, pages 175?182.Collins, Michael.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the Empirical Methods in Natural Language Pro-cessing Conference, pages 1?8, Philadelphia, USA.Fine, Shai, Yoram Singer, and Naftali Tishby.
1998.The hierarchical hidden markov model: Analysisand applications.
In Machine Learning, pages 32?41.Huang, Liang.
2008.
Forest reranking: Discrimina-tive parsing with non-local features.
In Proceedingsof the 46th Annual Meeting of the Association forComputational Linguistics.Jiang, Wenbin, Liang Huang, Yajuan Lv, and Qun Liu.2008.
A cascaded linear model for joint chineseword segmentation and part-of-speech tagging.
InProceedings of the 46th Annual Meeting of the Asso-ciation for Computational Linguistics.Lafferty, John, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the 23rd International Con-ference on Machine Learning, pages 282?289, Mas-sachusetts, USA.Ng, Hwee Tou and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?word-based or character-based?
In Proceedings ofthe Empirical Methods in Natural Language Pro-cessing Conference.Rabiner, Lawrence.
R. 1989.
A tutorial on hiddenmarkov models and selected applications in speechrecognition.
In Proceedings of IEEE, pages 257?286.Ratnaparkhi and Adwait.
1996.
A maximum entropypart-of-speech tagger.
In Proceedings of the Empir-ical Methods in Natural Language Processing Con-ference.Xue, Nianwen and Libin Shen.
2003.
Chinese wordsegmentation as lmr tagging.
In Proceedings ofSIGHAN Workshop.392
