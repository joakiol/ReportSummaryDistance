Processes that Shape Conversationand their Implications for Computational LinguisticsSusan E. BrennanDepartment of PsychologyState University of New YorkStony Brook, NY, US 11794-2500susan.brennan@sunysb.eduAbstractExperimental studies of interactive language usehave shed light on the cognitive andinterpersonal processes that shape conversation;corpora are the emergent products of theseprocesses.
I will survey studies that focus onunder-modelled aspects of interactive languageuse, including the processing of spontaneousspeech and disfluencies; metalinguistic displayssuch as hedges; interactive processes that affectchoices of referring expressions; and howcommunication media shape conversations.
Thefindings suggest some agendas forcomputational linguistics.IntroductionLanguage is shaped not only bygrammar, but also by the cognitive processing ofspeakers and addressees, and by the medium inwhich it is used.
These forces have, untilrecently, received little attention, having beenoriginally consigned to "performance" byChomsky, and considered to be of secondaryimportance by many others.
But as anyone whohas listened to a tape of herself lecturing surelyknows, spoken language is formally quitedifferent from written language.
And as thosewho have transcribed conversation areexcruciatingly aware, interactive, spontaneousspeech is especially messy and disfluent.
Thisfact is rarely acknowledged by psychologicaltheories of comprehension and production(although see Brennan & Schober, in press;Clark, 1994, 1997; Fox Tree, 1995).
In fact,experimental psycholinguists still make up mostof their materials, so that much of what we knowabout sentence processing is based on asanitized, ideal form of language that no oneactually speaks.But the field of computationallinguistics has taken an interesting turn:Linguists and computational linguists whoformerly used made-up sentences are now usingnaturally- and experimentally-generated corporaon which to base and test their theories.
One ofthe most exciting developments since the early1990s has been the focus on corpus data.Organized efforts such as LDC and ELRA haveassembled large and varied corpora of speechand text, making them widely available toresearchers and creators of natural language andspeech recognition systems.
Finally, Internetusage has generated huge corpora of interactivespontaneous text or "visible conversations" thatlittle resemble edited texts.Of course, ethnographers andsociolinguists who practice conversationanalysis (e.g., Sacks, Schegloff, & Jefferson,1974; Goodwin, 1981) have known for a longtime that spontaneous interaction is interestingin its own right, and that although conversationseems messy at first glance, it is actuallyorderly.
Conversation analysts havedemonstrated that speakers coordinate with eachother such feats as achieving a joint focus ofattention, producing closely timed turnexchanges, and finishing each another?sutterances.
These demonstrations have beencompelling enough to inspire researchers frompsychology, linguistics, computer science, andhuman-computer interaction to turn theirattention to naturalistic language data.But it is important to keep in mind that acorpus is, after all, only an artifact?a productthat emerges from the processes that occurbetween and within speakers and addressees.Researchers who analyze the textual records ofconversation are only overhearers, and there isample evidence that overhearers experience aconversation quite differently from addresseesand from side participants (Schober & Clark,1989; Wilkes-Gibbs & Clark, 1992).
With acorpus alone, there is no independent evidenceof what people actually intend or understand atdifferent points in a conversation, or why theymake the choices they do.
Conversationexperiments that provide partners with a task todo have much to offer, such as independentmeasures of communicative success as well asevidence of precisely when one partner isconfused or has reached a hypothesis about theother?s beliefs or intentions.
Task-orientedcorpora in combination with information abouthow they were generated are important fordiscourse studies.We still don't know nearly enough aboutthe cognitive and interpersonal processes thatunderlie spontaneous language use?howspeaking and listening are coordinated betweenindividuals as well as within the mind ofsomeone who is switching speaking andlistening roles in rapid succession.
Hence,determining what information needs to berepresented moment by moment in a dialogmodel, as well as how and when it should beupdated and used, is still an open frontier.
In thispaper I start with an example and identify somedistinctive features of spoken languageinterchanges.
Then I describe severalexperiments aimed at understanding theprocesses that generate them.
I conclude byproposing some desiderata for a dialog model.Two people in search of a perspectiveTo begin, consider the followingconversational interchange from a laboratoryexperiment on referential communication.
Adirector and a matcher who could not see eachanother were trying to get identical sets ofpicture cards lined up in the same order.
(1) D:ah boy this one ah boyall right it looks kinda like-on the right top there?s a square that looksdiagonalM: uh huhD: and you have sort of another like rectangleshape, the-like a triangle, angled, and on the bottom it?s uhI don?t know what that is, glass shapedM: all right I think I got itD: it?s almost like a person kind of in a weird wayM: yeah like like a monk praying or somethingD: right yeah good greatM: all right I got it(Stellmann & Brennan, 1993)Several things are apparent from this exchange.First, it contains several disfluencies orinterruptions in fluent speech.
The directorrestarts her first turn twice and her second turnonce.
She delivers a description in a series ofinstallments, with backchannels from thematcher to confirm them.
She seasons herspeech with fillers like uh, pauses occasionally,and displays her commitment (or lack thereof) towhat she is saying with displays like ah boy thisone ah boy and I don?t know what that is.
Eventhough she is the one who knows what the targetpicture is, it is the matcher who ends upproposing the description that they both end upratifying: like a monk praying or something.Once the director has ratified this proposal, theyhave succeeded in establishing a conceptual pact(see Brennan & Clark, 1996).
En route, bothpartners hedged their descriptions liberally,marking them as provisional, pending evidenceof acceptance from the other.
This example istypical; in fact, 24 pairs of partners whodiscussed this object ended up synthesizingnearly 24 different but mutually agreed-uponperspectives.
Finally, the disfluencies, hedges,and turns would have been distributed quitedifferently if this conversation had beenconducted over a different medium?throughinstant messaging, or if the partners had hadvisual contact.
Next I will consider the procesesthat underlie these aspects of interactive spokencommunication.1 Speech is disfluent, and disfluenciesbear informationThe implicit assumptions ofpsychological and computational theories thatignore disfluencies must be either that peoplearen't disfluent, or that disfluencies makeprocessing more difficult, and so theories offluent speech processing should be developedbefore the research agenda turns to disfluentspeech processing.
The first assumption isclearly false; disfluency rates in spontaneousspeech are estimated by Fox Tree (1995) and byBortfeld, Leon, Bloom, Schober, and Brennan(2000) to be about 6 disfluencies per 100 words,not including silent pauses.
The rate is lower forspeech to machines (Oviatt, 1995; Shriberg,1996), due in part to utterance length; that is,disfluency rates are higher in longer utterances,where planning is more difficult, and utterancesaddressed to machines tend to be shorter thanthose addressed to people, often becausedialogue interfaces are designed to take on moreinitiative.
The average speaker may believe,quite rightly, that machines are imperfect speechprocessors, and plan their utterances to machinesmore carefully.
The good news is that speakerscan adapt to machines; the bad news is that theydo so by recruiting limited cognitive resourcesthat could otherwise be focused on the taskitself.
As for the second assumption, if the goalis to eventually process unrestricted, naturalhuman speech, then committing to an early andexclusive focus on processing fluent utterancesis risky.
In humans, speech production andspeech processing are done incrementally, usingcontextual information from the earliestmoments of processing (see, e.g., Tanenhaus etal.
1995).
This sort of processing requires quite adifferent architecture and different mechanismsfor ambiguity resolution than one that beginsprocessing only at the end of a complete andwell-formed utterance.
Few approaches toparsing have tried to handle disfluent utterances(notable exceptions are Core & Schubert, 1999;Hindle, 1983; Nakatani & Hirschberg, 1994;Shriberg, Bear, & Dowding, 1992).The few psycholinguistic experimentsthat have examined human processing ofdisfluent speech also throw into question theassumption that disfluent speech is harder toprocess than fluent speech.
Lickley and Bard(1996) found evidence that listeners may berelatively deaf to the words in a reparandum (thepart that would need to be excised in order forthe utterance to be fluent), and Shriberg andLickley (1993) found that fillers such as um oruh may be produced with a distinctive intonationthat helps listeners distinguish them from therest of the utterance.
Fox Tree (1995) found thatwhile previous restarts in an utterance may slowa listener?s monitoring for a particular word,repetitions don?t seem to hurt, and some fillers,such as uh, seem to actually speed monitoringfor a subsequent word.What information exists in disfluencies,and how might speakers use it?
Speechproduction processes can be broken into threephases: a message or semantic process, aformulation process in which a syntactic frameis chosen and words are filled in, and anarticulation process (Bock, 1986; Bock &Levelt, 1994; Levelt, 1989).
Speakers monitortheir speech both internally and externally; thatis, they can make covert repairs at the pointwhen an internal monitoring loop checks theoutput of the formulation phase beforearticulation begins, or overt repairs when aproblem is discovered after the articulationphase via the speaker's external monitor?thepoint at which listeners also have access to thesignal (Levelt, 1989).
According toNooteboom's (1980) Main Interruption Rule,speakers tend to halt speaking as soon as theydetect a problem.
Production data from Levelt's(1983) corpus supported this rule; speakersinterrupted themselves within or right after aproblem word 69% of the time.How are regularities in disfluenciesexploited by listeners?
We have looked at thecomprehension of simple fluent and disfluentinstructions in a constrained situation where thelistener had the opportunity to developexpectations about what the speaker would say(Brennan & Schober, in press).
We tested twohypotheses drawn from some suggestions ofLevelt's (1989): that "by interrupting a word, aspeaker signals to the addressee that the word isan error," and that an editing expression like eror uh may "warn the addressee that the currentmessage is to be replaced," as with Move to theye?
uh, orange square.
We collected naturallyfluent and disfluent utterances by having aspeaker watch a display of objects; when onewas highlighted he issued a command about it,like "move to the yellow square."
Sometimes thehighlight changed suddenly; this sometimescaused the speaker to produce disfluencies.
Werecorded enough tokens of simple disfluencies tocompare the impact of three ways in whichspeakers interrupt themselves: immediately aftera problem word, within a problem word, orwithin a problem word and with the filler uh.We reasoned that if a disfluency indeedbears useful information, then we should be ableto find a situation where a target word is fasterto comprehend in a disfluent utterance than in afluent one.
Imagine a situation in which alistener expects a speaker to refer to one of twoobjects.
If the speaker begins to name one andthen stops and names the other, the way inwhich she interrupts the utterance might be anearly clue as to her intentions.
So the listenermay be faster to recognize her intentions relativeto a target word in a disfluent utterance than inan utterance in which disfluencies are absent.We compared the following types of utterances:a.
Move to the orange square     (naturally fluent)b.
Move to the |orange square    (disfluency excised)c. Move to the yellow- orange squared.
Move to the ye- orange squaree.
Move to the ye- uh, orange squaref.
Move to the orange squareg.
Move to the ye- orange squareh.
Move to the uh, orange squareUtterances c, d, and e were spontaneousdisfluencies, and f, g, and h were edited versionsthat replaced the removed material with pausesof equal length to control for timing.
Inutterances c?h, the reparandum began after theword the and continued until the interruptionsite (after the unintended color word, color wordfragment, or location where this information hadbeen edited out).
The edit interval in c?h beganwith the interruption site, included silence or afiller, and ended with the onset of the repaircolor word.
Response times were calculatedrelative to the onset of the repair, orange.The results were that listeners madefewer errors, the less incorrect information theyheard in the reparandum (that is, the shorter thereparandum), and they were faster to respond tothe target word when the edit interval before therepair was longer.
They comprehended targetwords after mid-word interruptions with fillersfaster than they did after mid-word interruptionswithout fillers (since a filler makes the editinterval longer), and faster than they did whenthe disfluency was replaced by a pause of equallength.
This filler advantage did not occur at theexpense of accuracy?unlike with disfluentutterances without fillers, listeners made nomore errors on disfluent utterances with fillersthan they did on fluent utterances.
Thesefindings highlight the importance of timing inspeech recognition and utterance interpretation.The form and length of the reparandum and editinterval bear consequences for how quickly adisfluent utterance is processed as well as forwhether the listener makes a commitment to aninterpretation the speaker does not intend.Listeners respond to pauses and fillerson other levels as well, such as to makeinferences about speakers?
alignment to theirutterances.
People coordinate both the contentand the process of conversation; fillers, pauses,and self-speech can serve as displays byspeakers that provide an account to listeners fordifficulties or delays in speaking (Clark, 1994;Clark, 1997; Clark & Brennan, 1991).
Speakerssignal their Feeling-of-Knowing (FOK) whenanswering a question by the displays they put onright before the answer (or right before theyrespond with I don?t know) (Brennan &Williams, 1995; Smith & Clark, 1993).
In theseexperiments, longer latencies, especially onesthat contained fillers, were associated withanswers produced with a lower FOK and thatturned out to be incorrect.
Thus in the followingexample, A1 displayed a lower FOK than A2:Q: Who founded the American Red Cross?A1: .....um......... Florence Nightingale?A2: ......... Clara Barton.Likewise, non-answers (e.g., I don?t know) aftera filler or a long latency were produced byspeakers who were more likely to recognize thecorrect answers later on a multiple choice test;those who produced a non-answer immediatelydid not know the answers.
Not only do speakersdisplay their difficulties and metalinguisticknowledge using such devices, but listeners canprocess this information to produce an accurateFeeling-of-Another's-Knowing, or estimate ofthe speaker?s likelihood of knowing the correctanswer (Brennan & Williams, 1995).These programs of experiments holdimplications for both the generation andinterpretation of spoken utterances.
A systemcould indicate its confidence in its message withsilent pauses, fillers, and intonation, and usersshould be able to interpret this informationaccurately.
If machine speech recognition wereconducted in a fashion more like human speechrecognition, timing would be a critical cue andincremental parses would be continually madeand unmade.
Although this approach would becomputationally expensive, it might producebetter results with spontaneous speech.2      Referring expressions are provisionaluntil ratified by addressees.Consider again the exchange in Example(1).
After some work, the director and matchereventually settled on a mutual perspective.When they finished matching the set of 12picture cards, the cards were shuffled and thetask was repeated several more times.
In thevery next round, the conversation went like this:(2) B: nine is that monk prayingA: yupLater on, referring was even more efficient:(3) A: three is the monkB: okA and B, who switched roles on each round,marked the fact that they had achieved a mutualperspective by reusing the same term, monk, inrepeated references to the same object.
Thesereferences tend to shorten over time.
In Brennanand Clark (1996), we showed that once peoplecoordinate a perspective on an object, they tendto continue to use the same terms that mark thatshared perspective (e.g., the man?s pennyloafer),even when they could use an even shorter basic-level term (e.g., the shoe, when the set of objectshas changed such that it no longer needs to bedistinguished from other shoes in the set).
Thisprocess of conceptual entrainment appears to bepartner-specific?upon repeated referring to thesame object but with a new partner, speakerswere more likely to revert to the basic levelterm, due in part to the feedback they receivedfrom their partners (Brennan & Clark, 1996).These examples depict the interpersonalprocesses that lead to conceptual entrainment.The director and matcher used many hedges intheir initial proposals and counter-proposals(e.g., it?s almost like a person kind of in a weirdway, and yeah like like a monk praying orsomething).
Hedges dropped out upon repeatedreferring.
We have proposed (Brennan & Clark,1996) that hedges are devices for signaling aspeaker's commitment to the perspective she isproposing.
Hedges serve social needs as well, byinviting counter-proposals from the addresseewithout risking loss of face due to overtdisagreements (Brennan & Ohaeri, 1999).It is worth noting that people's referringexpressions converge not only with those oftheir human partners, but also with those ofcomputer partners (Brennan, 1996; Ohaeri,1995).
In our text and spoken dialogue Wizard-of-Oz studies, when simulated computerpartners used deliberately different terms thanthe ones people first presented to them, peopletended to adopt the computers' terms, eventhough the computers had apparently"understood" the terms people had firstproduced (Brennan, 1996; Ohaeri, 1995).The impetus toward conceptualentrainment marked by repeated referringexpressions appears to be so compelling thatnative speakers of English will even producenon-idiomatic referring expressions (e.g., thechair in which I shake my body, referring to arocking chair) in order to ratify a mutually-achieved perspective with non-native speakers(Bortfeld & Brennan, 1987).Such findings hold many implicationsfor utterance generation and the design ofdialogue models.
Spoken and text dialogueinterfaces of the future should include resourcesfor collaboration, including those for negotiatingmeanings, modeling context, recognizing whichreferring expressions are likely to index aparticular conceptualization, keeping track of thereferring expressions used by a partner so far,and reusing those expressions.
This would helpsolve the ?vocabulary problem?
in human-computer interaction (Brennan, to appear).3 Grounding varies with the mediumGrounding is the process by which peoplecoordinate their conversational activities,establishing, for instance, that they understandone another well enough for current purposes.There are many activities to coordinate inconversation, each with its own cost, including:?
getting an addressee?s attention in order to beginthe conversation?
planning utterances the addressee is likely tounderstand?
producing utterances?
recognizing when the addressee does notunderstand?
initiating and managing repairs?
determining what inferences to make when thereis a delay?
receiving utterances?
recognizing the intention behind an utterance?
displaying or acknowledging this understanding?
keeping track of what has been discussed so far(common ground due to linguistic co-presence)?
determining when to take a turn?
monitoring and furthering the main purposes ortasks at hand?
serving other important social needs, such asface-management(adapted from Clark & Brennan, 1991)Most of these activities are relatively easy to dowhen interaction is face-to-face.
However, theaffordances of different media affect the costs ofcoordinating these activities.
The actual forms ofspeech and text corpora are shaped by howpeople balance and trade off these costs in thecontext of communication.In a referential communication study, Icompared task-oriented conversations in whichone person either had or didn?t have visualevidence about the other?s progress (Brennan,1990).
Pairs of people discussed many differentlocations on identical maps displayed onnetworked computer screens in adjoiningcubicles.
The task was for the matcher to get hiscar icon parked in the same spot as the cardisplayed on only the director?s screen.
In onecondition, Visual Evidence, the director couldsee the matcher?s car icon and its movements.
Inthe other, Verbal-Only Evidence, she could not.In both conditions, they could talk freely.Language-action transcripts wereproduced for a randomly chosen 10% of 480transcribed interchanges.
During each trial, the xand y coordinates of the matcher's icon wererecorded and time-stamped, as a moment-by-moment estimate of where the matcher thoughtthe target location was.
For the sample of 48trials, I plotted the distance between thematchers' icon and the target (the director's icon)over time, to provide a visible display of howtheir beliefs about the target location converged.Sample time-distance plots are shown inFigures 1 and 2.
Matchers' icons got closer to thetarget over time, but not at a steady rate.Typically, distance diminished relatively steeplyearly in the  trial, while  the  matcher  interpretedthe director's initial description and rapidlymoved his icon toward the target location.
Manyof the plots then showed a distinct elbowfollowed by a nearly horizontal region, meaningthat the matcher then paused or moved awayonly slightly before returning to park his caricon.
This suggests that it wasn?t sufficient forthe matcher to develop a reasonable hypothesisabout what the director meant by the descriptionshe presented, but that they also had to groundtheir understanding, or exchange sufficientevidence in order to establish mutual belief.
Theregion after the elbow appears to correspond tothe acceptance phase proposed by Clark &Schaefer (1989); the figures show that it wasmuch shorter when directors had visual evidencethan when they did not.
The accompanyingspeech transcripts, when synchronized with thetime-distance plots, showed that matchers gaveverbal acknowledgements when directors did nothave visual evidence and withheld them whendirectors did have visual evidence.
Matchersmade this adjustment to directors even thoughthe information on the matchers?
own screenwas the same for both conditions, whichalternated after every 10 locations for a total of80 locations discussed by each pair.Figure 1: Time-Distance Plot of Matcher-DirectorConvergence, Without Visual Evidence of theMatcher?s ProgressFigure 2: Time-Distance Plot of Matcher-DirectorConvergence, With Visual Evidence of the Matcher?sProgressThese results document the groundingprocess and the time course of how directors?and matchers?
hypotheses converge.
The processis a flexible one; partners shift the responsibilityto whomever can pay a particular cost mosteasily, expending the least collaborative effort(Clark & Wilkes-Gibbs, 1986).In another study of how media affectconversation (Brennan & Ohaeri, 1999; Ohaeri,1998) we looked at how grounding shapesconversation held face-to-face vs. via chatwindows in which people sent text messages thatappeared immediately on their partners?
screens.Three-person groups had to reach a consensusaccount of a complex movie clip they hadviewed together.
We examined the costs ofserving face-management needs (politeness) and0501001502002503003500 5 10 15 20 25 30Time (seconds)0501001502002503003500 5 10 15 20 25 30Time (seconds)looked at devices that serve these needs bygiving a partner options or seeking their input.The devices counted were hedges and questions.Although both kinds of groups recalledthe events equally well, they produced only halfas many words typing as speaking.
There weremuch lower rates of hedging (per 100 words) inthe text conversations than face-to-face, but thesame rates of questions.
We explained thesefindings by appealing to the costs of groundingover different media: Hedging requires usingadditional words, and therefore is more costly intyped than spoken utterances.
Questions, on theother hand, require only different intonation orpunctuation, and so are equally easy, regardlessof medium.
The fact that people used just asmany questions in both kinds of conversationssuggests that people in electronic or remotegroups don?t cease to care about face-management needs, as some have suggested; it?sjust harder to meet these needs when themedium makes the primary task more difficult.Desiderata for a Dialogue ModelFindings such as these hold a number ofimplications for both computational linguisticsand human-computer interaction.
First is amethodological point: corpus data and dialoguefeature coding are particularly useful when theyinclude systematic information about the tasksconversants were engaged in.Second, there is a large body ofevidence that people accomplish utteranceproduction and interpretation incrementally,using information from all available sources inparallel.
If computational language systems areever to approach the power, error recoveryability, and flexibility of human languageprocessing, then more research needs to be doneusing architectures that can support incrementalprocessing.
Architectures should not be based onassumptions that utterances are complete andwell-formed, and that processing is modular.A related issue is that timing is criticallyimportant in interactive systems.
Many modelsof language processing focus on thepropositional content of speech with littleattention to ?performance?
or ?surface?
featuressuch as timing.
(Other non-propositional aspectssuch as intonation are important as well.
)Computational dialogue systems (bothtext and spoken) should include resources forcollaboration.
When a new referring expressionis introduced, it could be marked as provisional.Fillers can be used to display trouble, andhedges, to invite input.
Dialogue models shouldtrack the forms of referring expressions used in adiscourse so far, enabling agents to use the sameterms consistently to refer to the same things.Because communication media shapeconversations and their emergent corpora, minordifferences in features of a dialogue interfacecan have major impact on the form of thelanguage that is generated, as well as oncoordination costs that language users pay.Finally, dialogue models should keep astructured record of jointly achievedcontributions that is updated and revisedincrementally.
No agent is omniscient; adialogue model represents only one agent'sestimate of the common ground so far (see Cahn& Brennan, 1999).
There are many open andinteresting questions about how to best structurethe contributions from interacting partners into adialogue model, as well as how such a modelcan be used to support incremental processes ofgeneration, interpretation, and repair.AcknowledgementsThis material is based upon work supported bythe National Science Foundation under GrantsNo.
IRI9402167, IRI9711974, and IRI9980013.I thank Michael Schober for helpful comments.ReferencesBock, J. K. (1986).
Meaning, sound, and syntax:Lexical priming in sentence production.
J. ofExperimental Psychology: Learning, Memory, &Cognition, 12, 575-586.Bock, K., & Levelt, W. J. M. (1994).
Languageproduction: Grammatical encoding.
In M.A.Gernsbacher (Ed.
), Handbook of psycholinguistics(pp.
945-984).
London: Academic Press.Bortfeld, H., & Brennan, S. E.  (1997).
Use andacquisition of idiomatic expressions in referring bynative and non-native speakers.
DiscourseProcesses, 23, 119-147.Bortfeld, H., Leon, S. D., Bloom, J. E., Schober, M.F., & Brennan, S. E. (2000).
Disfluency rates inspontaneous speech: Effects of age, relationship,topic, role, and gender.
Manuscript under review.Brennan, S. E. (1990).
Seeking and providingevidence for mutual understanding.
Unpublisheddoctoral dissertation.
Stanford University.Brennan, S. E. (1996).
Lexical entrainment inspontaneous dialog.
Proc.
1996 InternationalSymposium on Spoken Dialogue (ISSD-96) (pp.
41-44).
Acoustical Society of Japan: Phila., PA.Brennan, S. E. (to appear).
The vocabulary problem inspoken dialog systems.
In S. Luperfoy (Ed.
),Automated Spoken Dialog Systems, Cambridge,MA: MIT Press.Brennan, S. E., & Clark, H. H. (1996).
Conceptualpacts and lexical choice in conversation.
J. ofExperimental Psychology: Learning, Memory, &Cognition, 22, 1482-1493.Brennan, S. E., & Ohaeri, J. O.
(1999).
Why doelectronic conversations seem less polite?
The costsand benefits of hedging.
Proc.
Int.
Joint Conferenceon Work Activities, Coordination, andCollaboration  (WACC ?99) (pp.
227-235).
SanFrancisco, CA: ACM.Brennan, S. E., & Schober, M. F. (in press).
Howlisteners compensate for disfluencies in spontaneousspeech.
J. of Memory & Language.Brennan, S. E., & Williams, M. (1995).
The feeling ofanother?s knowing: Prosody and filled pauses ascues to listeners about the metacognitive states ofspeakers.
J. of Memory & Language, 34, 383-398.Cahn, J. E., & Brennan, S. E. (1999).
A psychologicalmodel of grounding and repair in dialog.
Proc.
AAAIFall Symposium on Psychological Models ofCommunication in Collaborative Systems (pp.
25-33).
North Falmouth, MA: AAAI.Clark, H.H.
(1994).
Managing problems in speaking.Speech Communication, 15, 243-250.Clark, H. H. (1997).
Dogmas of understanding.Discourse Processes, 23, 567-598.Clark, H. H., & Brennan, S. E. (1991).
Grounding incommunication.
In L. B. Resnick, J. Levine, & S.D.
Teasley (Eds.
), Perspectives on socially sharedcognition (pp.
127-149).Clark, H.H.
& Schaefer, E.F. (1989).
Contributing todiscourse.
Cognitive Science, 13, 259-294.Clark, H.H.
& Wilkes-Gibbs, D. (1986).
Referring as acollaborative process.
Cognition, 22, 1-39.Core, M. G., & Schubert, L. K. (1999).
A model ofspeech repairs and other disruptions.
Proc.
AAAIFall Symposium on Psychological Models ofCommunication in Collaborative Systems.
NorthFalmouth, MA: AAAI.Fox Tree, J.E.
(1995).
The effects of false starts andrepetitions on the processing of subsequent words inspontaneous speech.
J. of Memory & Language, 34,709-738.Goodwin, C. (1981).
Conversational Organization:Interaction between speakers and hearers.
NewYork: Academic Press.Hindle, D. (1983).
Deterministic parsing of syntacticnon-fluencies.
In Proc.
of the 21st Annual Meeting,Association for Computational Linguistics,Cambridge, MA, pp.
123-128.Levelt, W. J. M. (1983).
Monitoring and self-repair inspeech.
Cognition, 14, 41-104.Levelt, W. (1989).
Speaking: From intention toarticulation.
Cambridge, MA: MIT Press.Lickley, R., & Bard, E. (1996).
On not recognizingdisfluencies in dialog.
Proc.
InternationalConference on Spoken Language Processing(ICSLIP ?96), Philadelphia, 1876-1879.Nakatani, C. H., & Hirschberg, J.
(1994).
A corpus-based study of repair cues in spontaneous speech.
Jof the Acoustical Society of America, 95, 1603-1616.Nooteboom, S. G. (1980).
Speaking and unspeaking:Detection and correction of phonological and lexicalerrors in spontaneous speech.
In V. A.
Fromkin(Ed.
), Errors in linguistic performance.
New York:Academic Press.Ohaeri, J. O.
(1995).
Lexical convergence with humanand computer partners: Same cognitive process?Unpub.
Master's thesis.
SUNY, Stony Brook, NY.Ohaeri, J. O.
(1998).
Group processes and thecollaborative remembering of stories.
Unpublisheddoctoral dissertation.
SUNY, Stony Brook, NY.Oviatt, S. (1995).
Predicting spoken disfluenciesduring human-computer interaction.
ComputerSpeech and Language, 9, 19-35.Sacks, H., Schegloff, E., & Jefferson, G. (1974).
Asimplest systematics for the organization of turn-taking in conversation.
Language, 50, 696-735.Schober, M.F.
& Clark, H.H.
(1989).
Understandingby addressees and overhearers.
CognitivePsychology, 21, 211-232.Shriberg, E. (1996).
Disfluencies in Switchboard.Proceedings, International Conference on SpokenLanguage Processing, Vol.
Addendum, 11-14.Philadelphia, PA, 3-6 October.Shriberg, E., Bear, J., & Dowding, J.
(1992).Automatic detection and correction of repairs inhuman-computer dialog.
In M. Marcus (Ed.
), ProcDARPA Speech and Natural Language Workshop(pp.
419-424).
Morgan Kaufmann.Shriberg, E.E.
& Lickley, R.J. (1993).
Intonation ofclause-internal filled pauses.
Phonetica, 50, 172-179.Smith, V., & Clark, H. H. (1993).
On the course ofanswering questions.
J. of Memory and Language,32, 25-38.Stellmann, P., & Brennan, S. E. (1993).
Flexibleperspective-setting in conversation.
Abstracts of thePsychonomic Society, 34th Annual Meeting (p. 20),Washington, DC.Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard,K.
M., & Sedivy, J.
(1995).
Integration of visual andlinguistic information in spoken languagecomprehension.
Science, 268, 1632-1634.Wilkes-Gibbs, D., & Clark, H.H.
(1992).
Coordinatingbeliefs in conversation.
Journal of Memory andLanguage, 31, 183-194.
