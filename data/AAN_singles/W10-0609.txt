Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 70?78,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUsing fMRI activation to conceptual stimuli to evaluate methods forextracting conceptual representations from corporaBarry DevereuxCentre for Speech, Language and the BrainDepartment of Experimental PsychologyUniversity of Cambridgebarry@csl.psychol.cam.ac.ukColin Kelly & Anna KorhonenComputer LaboratoryUniversity of Cambridge{ck329,alk23}@cam.ac.ukAbstractWe present a series of methods for deriv-ing conceptual representations from corporaand investigate the usefulness of the fMRIdata and machine learning methodology ofMitchell et al (2008) as a basis for evaluat-ing the different models.
Within this frame-work, the quality of a semantic model is quan-tified by its ability to predict the fMRI ac-tivation associated with conceptual stimuli.Mitchell et al used a manually-acquired set ofverbs as the basis for their semantic model; inthis paper, we also consider automatically ac-quired feature-norm-like semantic representa-tions.
These models make different assump-tions about the kinds of information avail-able in corpora that is relevant to represent-ing conceptual knowledge.
Our results in-dicate that automatically-acquired representa-tions can make equally powerful predictionsabout the brain activity associated with thestimuli.1 IntroductionMitchell et al (2008) presented a novel approach forpredicting human brain activity associated with con-ceptual stimuli.
This approach represents a usefuldevelopment for interdisciplinary researchers inter-ested in lexical semantics, for several reasons.
Mostbroadly, it is useful in testing the hypothesis thatdistributional properties of words in corpora can re-veal important information about the meanings ofwords.
A strong version of this hypothesis (i.e.
thatchildren in part learn the meaning of concrete con-cept words from co-occurring words in discoursethat they are exposed to) has formed the basis ofone class of probabilistic cognitive models of con-ceptual representation (Andrews et al, 2005; An-drews et al, 2009; Steyvers, 2010).
Furthermorethis approach is useful for testing hypotheses aboutthe kind of co-occurring information that is usefulfor representing conceptual semantics.
In Mitchellet al?s work (2008), for example, they adopt the po-sition that the meaning of concrete concepts is en-coded in the brain with information associated withbasic sensory and motor activities (such as actionsinvolving changes to spatial relationships and phys-ical actions performed on objects).At a more technical level, Mitchell et al?s fMRIactivation data1 give researchers developing feature-based models of conceptual representation an im-portant benchmark for evaluation.
For these re-searchers, a key problem is the lack of a reason-able ?gold standard?
against which the quality of therepresentations generated by a computational modelmay be evaluated.
Previous research has adoptedtwo main approaches to evaluation.
Firstly, somemodels ?
especially those aiming to extract repre-sentations composed of psychologically meaningfulsemantic feature units, such as Baroni et al (2009)?
have been evaluated against features gathered inlarge scale property norming studies (e.g.
McRaeet al (2005)).2 By comparing the system outputagainst features elicited by people, this kind of eval-1fMRI data measures changes in oxygen concentrations inthe brain.
These changes are tied to cognitive processes.2In property norming studies, a group of human subjects areasked to cite features which come to mind for a given concept.These features are compiled by frequency (with a minimum fre-quency cut-off) to generate a list of features for each concept.70uation aims to test the psychological validity of com-putational methods.
Furthermore, it allows a fine-grained analysis of performance, for example by re-vealing the classes of features (part-of, taxonomic,etc) which a given model is particularly good at ex-tracting (Baroni et al, 2008).However, property norms come with importantcaveats.
One problem is that they tend to over-represent informative or salient information aboutconcepts whilst under-representing other kinds offeatures.
For example, participants report thatcamels have humps, but not that camels have hearts,even though all participants are likely to have bothpieces of information accessible in their representa-tion of the concept CAMEL.
If a model is successfulin extracting these less salient features, there is noway of evaluating their correctness using propertynorms.
A related issue is that participants can onlyreport verbalizable features, which may not repre-sent the total sum of their conceptual knowledge(Murphy, 2002; McRae et al, 2005).A second problem with using property norms asthe basis of evaluation is that there is often no directlexical match between feature terms appearing in thesystem output and the norms.
Feature norms are typ-ically normalized such that near-synonymous prop-erties (e.g.
is endangered, is an endangered species,is almost extinct, etc., for WHALE) given by differ-ent participants are mapped to the same feature la-bel (e.g.
is endangered).
As a consequence, a modelmay correctly extract endangered for WHALE, butother lexical forms of the same feature will notmatch any feature in the norms.
One solution to thisis to create an expansion set for each feature whichincludes its synonyms (Baroni et al, 2008).
How-ever, this is only a partial solution because lexicalvariation in features is not limited to synonyms.A second approach to evaluating semantic mod-els uses classification or similarity data.
For exam-ple, Andrews et al (2009) evaluated their models bycalculating cosine similarity scores between seman-tic representations and using these similarity scoresto predict behavioral data which are contingent onthe semantic similarity between pairs of concepts(e.g.
lexical substitution errors, semantic priminglatencies, word-association norms, etc).
Althoughthis approach is psychologically motivated, it evalu-ates a set of extracted features more indirectly thancomparison with norm data.
In computational lin-guistics, a similarly indirect evaluation method is tocluster the extracted representations.
This approachavoids the difficulties in evaluating individual fea-tures; however it only allows consideration alongone dimension of the data, namely the similarity be-tween pairs of concepts.fMRI data such as the Mitchell et al (2008)dataset offers an advancement over both of theseevaluation techniques.
Unlike, for example, prop-erty norming data, fMRI data offers direct insightinto how the brain is functioning in response to givenstimuli.
Its multidimensional nature makes it eas-ier to inspect what aspects of meaning a particularmodel is performing strongly or weakly on, and al-lows for better control of experimental variation.
Fi-nally, it avoids the two major issues associated withproperty norms, which we outlined above.This paper is structured as follows.
In the nextsection, we briefly describe the models which weused to extract conceptual representations for the 60concepts in the Mitchell et al (2008) dataset.
InSection 3, we outline our experimental objectives,and the framework we adopt for testing our seman-tic models.
In Section 4, we present the results ofour evaluation, which indicate above chance perfor-mance for each of the models.
Finally, we exam-ine the differences between models by investigatingfor which concepts prediction of the fMRI activity ispoorest, and discuss these differences with respect tothe differing assumptions made by the methods.2 Semantic modelsWe consider four different semantic models in thispaper, which are described briefly below.
Thesemodels were selected as we were interested in thevarious kinds of knowledge (part-of-speech, syntac-tic, and semantic) in corpora available to the extrac-tion process, and the extent to which the use of thesetypes of knowledge can affect the quality of the ex-tracted conceptual representations.2.1 Mitchell verb-based semantic modelThe first semantic model we considered was thatof Mitchell et al (2008).
This model assumes thatsensory-motor information is an important aspect ofconceptual representation, and that the information71relevant to a target concept?s representation can beestimated from the concept word?s frequency of co-occurrence with 25 sensory-motor verbs (eat, ma-nipulate, push, etc) in a very large corpus.
Our reim-plementation of this method used the co-occurrencestatistics provided by Mitchell et al3 which wereextracted from the Google n-gram corpus consistingof 1 trillion words of web text.2.2 SVD modelSecondly, we implemented a co-occurrence-basedSingular Value Decomposition (SVD) model basedon the one described by Baroni and colleagues (Ba-roni and Lenci, 2008; Baroni et al, 2009).
Thismodel combines aspects of both the HAL (Landaueret al, 1998) and LSA (Lund and Burgess, 1996)models in constructing representations for wordsbased on their co-occurrences in texts.
A word-by-word co-occurrence matrix was constructed forour corpus, storing how often each target word co-occurred with each context word.
The set of contextwords consisted of the 5,000 most frequent contentwords (i.e.
words not occurring in a stop-list of func-tion words) appearing in the corpus.
The set of targetwords consisted of the 60 concept terms appearingin the fMRI dataset, supplemented with the 10,000most frequent content words in the corpus (with theexception of the top 10 most frequent words).
Forcalculating co-occurrence frequency between targetand context words, the context window was definedby sentence boundaries: two words were consideredto co-occur if they appeared in the same sentence4.Following Baroni and Lenci (2008), the dimen-sionality of the target-word ?
context-word co-occurrence matrix was reduced to 150 columns bysingular value decomposition.
That is, the singu-lar value decomposition of the co-occurrence matrixwas computed and the 150 left singular vectors thataccounted for most of the variance, multiplied by thecorresponding singular values, were used as the 150-dimensional representation of each target term.
Sim-3http://www.cs.cmu.edu/?tom/science2008/semanticFeatureVectors.html4In Baroni et al?s implementation a context window of 5(Baroni and Lenci, 2008) or 20 (Baroni et al, 2009) wordseither side of the target word was used instead; we chose asentence-based context window as it is analogous to the contextused in our experimental method (described in the followingsection).ilarity between pairs of target words was calculatedas the cosine between their vectors, and for each ofthe 60 concept words in the experimental stimuli wechose the 200 most similar target words to act as thefeature terms extracted by the model.
The corpusused with this model was the British National Cor-pus (BNC) (Leech et al, 1994).2.3 Novel extraction methodFinally we implemented a novel extraction method,which aims to extract property-norm-like, psy-chologically meaningful features from corpus data(Kelly et al, 2010).
The method aims to extract se-mantically unconstrained feature triples of the formconcept-relation-feature , where feature is a feature(either noun or adjective) of the target concept andrelation is a verb representing the semantic relation-ship between them.
Examples of extracted triplesinclude: swan be white, swan have neck and screw-driver be tool.
The model uses a corpus parsed forgrammatical relations (GRs) using Robust AccurateStatistical Parsing (RASP) (Briscoe et al, 2006).For each sentence containing a target concept, theset of GRs for that sentence are examined to testwhether they match manually-created rules.
Theserules include prototypical feature-relation GR struc-tures connecting elements of the sentence and rep-resent dependency patterns which encode potentialsemantic relationships between the concept and can-didate feature terms occurring in the sentence.
Alarge set of candidate triples are extracted by ap-plying these rules to each sentence in the corpuscontaining a target concept, and the triples for eachconcept are ranked by their frequency of extraction.In the second stage of the method, the extractedtriples are reweighted on the basis of probabilistichigh-level semantic information obtained from hu-man property norm data.
This subsequent stage hasthe effect of increasing the weight associated withmore high-quality features and downgrading lower-quality features.
The extraction method is describedmore fully in Kelly et al (2010).
For this methodwe also used the BNC.
The top 200 triples rankedby frequency (i.e.
unweighted) and the top 200 fea-tures after reweighting with the semantic data wereused in our experiments.723 ExperimentAs mentioned above, we are primarily interested inusing the fMRI data to evaluate the quality of thedifferent methods for extracting conceptual repre-sentations from corpora (rather than being interestedin investigating methods for predicting fMRI activa-tion).
We make no attempt to build on the methoddescribed by Mitchell et al (2008), although thereare likely to be many interesting avenues throughwhich that method could be extended.5 We thereforefollowed the Mitchell et al methodology as closelyas possible, using the same multiple regression train-ing and leave-two-out cross-validation paradigms aspresented in their paper and supporting online ma-terial.
The only parameter that we varied was theextraction method (and corpus) that was used to gen-erate the feature-vectors associated with the 60 con-cepts that were used during the training phase.
Thequality of the predictions generated for the conceptsusing each semantic model can therefore be adoptedas an index of model performance.The Mitchell et al method uses co-occurrencewith a specific set of 25 manually selected verbs(eat, push, etc) that are the same for each concept.This results in 25-dimensional feature vectors for in-put into training.
However, for both the SVD modeland our triple extraction models there are no a pri-ori constraints on the number of unique features thatcan be extracted for the concepts.
For these mod-els, we selected the top 200 features associated witheach concept; therefore, across all 60 concepts in theMitchell et al dataset, there are thousands of uniquefeatures extracted which are used in the concepts?representations.
To ensure that the linear regres-sion model for each method would be fitted usingthe same number of free parameters during training(thereby maximizing the comparability of the dif-ferent methods), we reduced the dimensionality ofthe generated feature spaces for the SVD methodand the two triple-extraction methods using Prin-cipal Components Analysis (PCA).
The concept ?feature extraction frequency matrices for the threemodels were submitted to PCA, and the first 25 com-ponents (i.e.
those components which best charac-5For example, the method currently makes the simplifyingassumption that the activity in neighbouring voxels is indepen-dent.Triples (weighted) SVDPCA1 PCA2 PCA1 PCA2Highest-valued conceptshorse house coat butterflycat apartment skirt cowcow dog shirt antdog igloo pants beebeetle car dress lettuceLowest-valued conceptsknife pants car deskdoor coat watch armhammer dress horse chairsaw skirt dog knifechisel shirt fly legTable 1: Highest- and lowest-valued concepts for thefirst two components for the SVD and weighted triple-extraction methods.terized the variance of the original features) for eachmodel were selected.
In the case of the SVD model,these 25 dimensions explained 77.7% of the vari-ance in the original 3,061-dimensional vectors.
Forour unweighted extraction method, the 25 extractedcomponents explained 63.0% of the original 5,525dimensions; for the weighted method the compo-nents explained 71.5% of the original 6,567 dimen-sions.It is interesting to consider the kind of seman-tic information that is being captured by the resul-tant PCA components.
In particular, the compo-nents appear to capture meaningful distinctions be-tween stimuli.
For example, the first PCA com-ponent for our weighted triple extraction methodcan be interpreted as the concepts?
degree of ?an-imalness?
(animal stimuli have high values on thiscomponent).
Table 1 presents the five highest andlowest-valued concepts for the first two componentsfor the SVD model and the weighted triple extrac-tion model.
Concepts which overlap with respectto a specific set of semantic properties tend to havehigh or low values on a given dimension, indicatingthat that component is capturing a specific cluster ofco-occurring semantic features.
For example, PCA1for SVD can be interpreted as ?has features associ-ated with clothing?.Therefore, a key difference between the Michell73Method Feature Type POS Syntax SemanticsMitchell 25 verbs no no noSVD tuples (content-words) yes no notriple-extraction method (unweighted) feature-triples yes yes notriple-extraction method (weighted) feature-triples yes yes yesTable 2: Comparison of the information available to each model.et al model and our models is that while Mitchellet al posit that certain sensory-motor function verbscan act as important features of concepts, our modelsinstead place more importance on intrinsic semanticfeatures.Finally, Table 2 gives a summary comparison ofthe different models, in terms of whether or not eachuses part of speech (POS) data, syntactic informa-tion (i.e.
GRs), and semantic filtering (Section 2.3).It should be noted that the BNC corpus (used withthe SVD model and our triple-extraction method) is10,000 times smaller than the corpus from whichthe Mitchell et al feature vectors are derived.
Assuch the semantic representations we extract withour method need to make better use of the data avail-able in the corpus if they are to compete with theverb-based features used by Mitchell et al?s method.4 ResultsThe accuracy for each of the four methods was eval-uated using a leave-two-out validation paradigm.There are 1,770 possible pairs of concepts that canbe drawn from the set of 60 concept stimuli.
Train-ing was performed separately for each participantand for each of the 1,770 held-out pairs.
Givena particular participant and held-out pair, for eachvoxel v we fit the activation at that voxel to the setof 58 training items with multiple linear regression,using as predictor variables the elements of the 25-dimensional feature vectors associated with each ofthe 58 concepts.
Training therefore yields a set of25 ?-coefficients, which can be used to generate aprediction for the activation yv of voxel v for theheld-out word w using the equationypredv =25?i=1?v,ifi,w (1)where fi,w is the ith element of the feature vector forword w (see Mitchell et al (2008) for details).
Overall voxels, this method gives a prediction for the ac-tivation with respect to the held-out word w whichcan then be compared to the observed activation forthat stimulus.Rather than comparing the activity between pre-dicted and observed images using all voxels, wecompared images using only the 500 most stablevoxels for each participant.
For each participant, the500 most stable voxels were the voxels which gavethe most consistent pattern of activation across thesix presentations of all 60 stimuli (see Mitchell et al(2008) for details).The top row of Figure 1 presents the learned co-efficients for one feature dimension for each of thefour semantic models considered in our experiments(for these images, all voxels rather then the 500most stable voxels are used).
For the Mitchell et almethod, the coefficients presented correspond to theverb eat; for the other models the feature is the PCAcomponent that explained the most variance in theoriginal representations.
We also present the pre-dicted images for the concepts CELERY and AIR-PLANE, calculated on the coefficients learned overthe remaining 58 concepts.
Importantly, for theMitchell et al method (column (a)), the learned co-efficients for eat and the predicted images for CEL-ERY and AIRPLANE agree with those reported byMitchell et al (2008, Figure 2 & online supplemen-tary material6).We calculated similarity between predicted andobserved images using both cosine and Pearson cor-relation and the 500 most stable voxels; we reportthe results using Pearson correlation here as thismeasure consistently gave slightly better accuracies6http://www.cs.cmu.edu/?tom/science2008/featureSignaturesP1.html74(a) ?eat?
(b) PCA1/clothes (c) PCA1/clothes (d) PCA1/animals(a) celery (b) celery (c) celery (d) celery(a) airplane (b) airplane (c) airplane (d) airplaneFigure 1: Learned coefficients on a selected feature dimension (top row) and predicted activation for CELERY (middlerow) and AIRPLANE (bottom row) for four semantic models: (a) Mitchell et al (2008), (b) SVD (c) triple extractionmethod (unweighted), and (d) triple extraction method (weighted).
Warmer colours indicate higher values (i.e.
larger?-coefficients for the feature dimensions and higher predicted activation for the concepts).
PCA components have beengiven intuitive labels indicating the kind of information described by that component (see Table 1).
As in Figure 2 ofMitchell et al (2008), the figure shows just one slice in the horizontal plane (z = -12 in MNI space) for one participant(P1).
The predicted images for CELERY and AIRPLANE were generated from the feature coefficients learned on theother 58 concepts using each of the four models; the corresponding observed images for CELERY and AIRPLANE canbe found in Mitchell et al (2008) Figure 2 B.75Method P1 P2 P3 P4 P5 P6 P7 P8 P9 MeanMitchell et al (2008) 0.84 0.83 0.76 0.81 0.79 0.66 0.73 0.64 0.68 0.75SVD 0.82 0.67 0.79 0.83 0.74 0.64 0.64 0.70 0.75 0.73Triple-extraction (unweighted) 0.82 0.71 0.79 0.80 0.70 0.69 0.65 0.53 0.78 0.72Triple-extraction (weighted) 0.82 0.72 0.76 0.83 0.73 0.65 0.68 0.51 0.76 0.72Table 3: Accuracy results for the four semantic models.for each of the four models (the results are very simi-lar using the cosine measure).
Following Mitchell etal.
(2008; supplementary material), a match scorefor each held out pair w1 and w2 was calculatedas the sum of the similarities between the correctlyaligned predicted and observed images:a = sim(wpred1 , wobs1 ) + sim(wpred2 , wobs2 ) (2)Similarly a mismatch score was calculated asb = sim(wpred1 , wobs2 ) + sim(wpred2 , wobs1 ) (3)Cases where the match score is greater than the mis-match score (i.e.
a > b) count as successes for themodel (i.e.
the model correctly identifies the twopredicted images).
Otherwise there is a failure bythe model (i.e.
the model identifies the observed im-age for w1 as being w2 and vice-versa).Table 3 presents the results of the leave-two-out cross-validation evaluation, giving the propor-tion (across all 1,770 pairs) of predicted images forthe held-out pairs that were correctly matched tothe observed images.7 The original Mitchell et al(2008) model has the best mean performance, al-though across the nine participants, there is no sig-nificant difference in accuracy between any of themodels (|t(8)| < 1.49, p > 0.17, for all pairwisepaired t-tests between Mitchell et al (2008), SVD,and weighted triple extraction).That there is no difference between the perfor-mance of the Mitchell et al (2008), SVD and triple7Our results for the Mitchell et al (2008) method are simi-lar, though not identical, to those reported in that paper (wherethe reported mean accuracy across all participants is 0.77, usingcosine similarity).
Our implementation of the method for select-ing the 500 most stable voxels yields slightly different voxelsfrom those obtained by Mitchell et al (2008; see supplemen-tary material).
In any case, the same set of 500 voxels for eachparticipant were used for generating the results of each modelpresented here, and so we do not believe that this discrepancyaffects comparison of the different models.extraction methods is surprising, given the differentkinds of information that are available to the dif-ferent models.
In particular, the models that auto-matically acquire very general and semantically un-constrained feature-based representations performas well as the model which uses a set of manually-selected sensory-motor verbs, even though the rep-resentations generated for these models are derivedfrom 10,000 times less corpus data.As mentioned in our introduction, an advantage ofevaluating against the fMRI dataset is that this multi-dimensional data allows us to investigate strengthsand weaknesses of different models in a way whichis not possible using similarity or clustering-basedevaluation.
As a very simple investigation of spe-cific differences in model performance, we presentin Table 4 the pairs of concepts for which each of themodels performs most poorly on.
The Mitchell et al(2008) method appears to do poorly on pairs of con-cepts where a constituent word can be ambiguouswith respect to its part-of-speech (e.g.
SAW, BEAR).This is not surprising, given that part-of-speech datais not available in the Google n-gram corpus usedwith this method.
The performance of the Mitchellet al method might therefore be improved signifi-cantly by applying heuristics to the n-gram data tomake inferences about the correct part-of-speech ofinstances of words like SAW and BEAR.
For the SVDand weighted triple extraction methods, which bothuse the BNC corpus, there is some evidence thatthe models are performing poorly for relatively lowfrequency words8 (e.g.
CHISEL), words which aresemantically ambiguous as nouns (e.g.
ARM), andpairs which are semantically similar (e.g.
SPOON &KNIFE).
This suggests that the SVD and triple ex-traction methods may perform better with a largerand more diverse corpus.8AIRPLANE is relatively low frequency in the BNC; it maybe more sensible to use the word AEROPLANE with a Britishcorpus.76Mitchell et al SVD Triple Extraction (weighted)Pair Nr.
Pair Nr.
Pair Nr.bear saw 0 cup airplane 0 dresser chimney 0bell carrot 0 cup lettuce 0 airplane chisel 0bell saw 0 horse beetle 0 airplane hand 0knife bear 0 chisel arm 0 airplane tomato 0cup saw 1 hammer arm 1 spoon chisel 0bear tomato 1 dresser arch 1 spoon knife 0Table 4: Leave-out pairs for which each model performs least accurately, across the nine participants.
Nr.
= the numberof participants for which this leave-out pair was correctly matched.5 ConclusionThe fMRI dataset and training and evaluationmethodology presented by Mitchell et al (2008)gives researchers an interesting new framework withwhich to evaluate the quality of feature-based con-ceptual representations extracted from corpora.
Thisframework avoids some of the problems inherent inevaluating extracted representations against a ?goldstandard?
based on participant-generated propertynorms.
It also provides a rich multi-dimensionaldataset through which the strengths and weaknessesof extraction methods can be identified.We have applied this evaluation framework tofour feature extraction methods which use differentsources of information available in corpora to extractconceptual representations.
Surprisingly, in spite oftheir major differences, we did not find any signifi-cant difference in performance between the models.This finding has interesting theoretical implica-tions, given that previous research has suggestedthat aspects of meaning defined by sensory-motorverbs may have a somewhat distinctive role to playin predicting the fMRI activation associated withconceptual stimuli (Mitchell et al, 2008).
Our re-sults suggest that general feature-based representa-tions of concepts, which place no a priori distinc-tion on sensory-motor properties, may be equallycapable of predicting activation to conceptual stim-uli.
This highlights the potential for the Mitchellet al method to be used to inform both distributedand sensory-motor accounts of conceptual represen-tation (e.g.
McRae et al (1997), Cree et al (2006),Tyler et al (2000), Tyler & Moss (2001), Moss etal.
(2007), Martin & Chao (2001)), as well as pro-viding a benchmark with which to assess semanticmodel development.
In a similar vein, Murphy etal.
(2009) used a dependency-parsed corpus yieldingverb co-occurrence statistics to predict EEG9 activa-tion patterns with significant accuracy.The training and evaluation framework presentedby Mitchell et al (2008) represents just one pointin a large space of possibilities for using computa-tional modelling to predict human brain activity as-sociated with conceptual stimuli.
In these initial ex-periments, we have chosen to follow the Mitchellet al approach as closely as possible, in order tomaximize comparability with their results.
In futurework, we aim to investigate other methods for train-ing and evaluation, other corpora and other sourcesof imaging data.
Furthermore, we aim to use theevaluation results from such work to inform the de-velopment of our extraction method.AcknowledgmentsOur work was funded by the EPSRC grantEP/F030061/1, and the Royal Society UniversityResearch Fellowship, UK.
We thank Mitchell etal.
(2008) and McRae et al (2005) for making theirdata publically available.ReferencesMark Andrews, G. Vigliocco, and D. Vinson.
2005.Integrating attributional and distributional informa-tion in a probabilistic model of meaning representa-tion.
In Timo Honkela et al, editor, Proceedings ofAKRR?05, International and Interdisciplinary Confer-ence on Adaptive Knowledge Representation and Rea-9EEG measures voltages induced by neuronal firing acrossthe human scalp.77soning, pages 15?25, Espoo, Finland: Helsinki Uni-versity of Technology.Mark Andrews, Gabriella Vigliocco, and David Vinson.2009.
Integrating experiential and distributional datato learn semantic representations.
Psychological Re-view, 116(3):463?498.Marco Baroni and Alessandro Lenci.
2008.
Conceptsand properties in word spaces.
From context to mean-ing: Distributional models of the lexicon in linguis-tics and cognitive science (Special issue of the ItalianJournal of Linguistics), 20(1):55?88.Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-tors.
2008.
ESSLLI 2008 Workshop on DistributionalLexical Semantics.Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-simo Poesio.
2009.
Strudel: A corpus-based semanticmodel based on properties and types.
Cognitive Sci-ence, pages 1?33.E.
Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proceed-ings of the Interactive Demo Session of COLING/ACL-06, pages 77?80.George S. Cree, Chris McNorgan, and Ken McRae.2006.
Distinctive features hold a privileged statusin the computation of word meaning: Implicationsfor theories of semantic memory.
Journal of Experi-mental Psychology.
Learning, Memory, and Cognition,32(4):643?58.Colin Kelly, Barry Devereux, and Anna Korhonen.
2010.Acquiring human-like feature-based conceptual repre-sentations from corpora.
In Brian Murphy, Kai minKevin Chang, and Anna Korhonen, editors, Proceed-ings of the NAACL-HLT Workshop on ComputationalNeurolinguistics, Los Angeles, USA.T.K.
Landauer, P.W.
Foltz, and D. Laham.
1998.
An in-troduction to latent semantic analysis.
Discourse Pro-cesses, 25:259?284.G.
Leech, R. Garside, and M. Bryant.
1994.
CLAWS4:the tagging of the British National Corpus.
In Pro-ceedings of the 15th conference on Computationallinguistics-Volume 1, pages 622?628.
Association forComputational Linguistics.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instruments,and Computers, 28(2):203?208.Alex Martin and Linda L. Chao.
2001.
Semantic mem-ory and the brain: structure and processes.
CurrentOpinion in Neurobiology, 11(2):194?201.Ken McRae, Virginia R. de Sa, and Mark S. Seidenberg.1997.
On the nature and scope of featural representa-tions of word meaning.
Journal of Experimental Psy-chology: General, 126(2):99?130.Ken McRae, George S. Cree, Mark S. Seidenberg, andChris McNorgan.
2005.
Semantic feature productionnorms for a large set of living and nonliving things.Behavior Research Methods, 37:547?559.Tom M. Mitchell, Svetlana V. Shinkareva, Andrew Carl-son, Kai-Min Chang, Vicente L. Malave, Robert A.Mason, and Marcel A.
Just.
2008.
Predicting humanbrain activity associated with the meanings of nouns.Science, 320(5880):1191?1195.Helen E. Moss, Lorraine K. Tyler, and Kirsten I. Taylor.2007.
Conceptual structure.
In M. Gareth Gaskell, ed-itor, The Oxford handbook of psycholinguistics, pages217?234.
Oxford University Press, Oxford, UK.B.
Murphy, M. Baroni, and M. Poesio.
2009.
Eeg re-sponds to conceptual stimuli and corpus semantics.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP 2009),pages 619?627, East Stroudsburg, PA.Gregory Murphy.
2002.
The big book of concepts.
TheMIT Press, Cambridge, MA.Mark Steyvers.
2010.
Combining feature norms andtext data with topic models.
Acta Psychologica,133(3):234?243.Lorraine K. Tyler and Helen E. Moss.
2001.
Towards adistributed account of conceptual knowledge.
Trendsin Cognitive Sciences, 5(6):244?252.L.
K. Tyler, H. E. Moss, M. R. Durrant-Peatfield, and J. P.Levy.
2000.
Conceptual structure and the structure ofconcepts: A distributed account of category-specificdeficits.
Brain and Language, 75(2):195?231.78
