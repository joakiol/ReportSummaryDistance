Segmenting Sentences into Linky StringsUsing D-bigram StatisticsShiho NobesawaJunya Tsutsumi, Sun Da Jiang, Tomohisa Sano, Kengo SatoMasakazu NakanishiNakanish i  Laboratory ,  Keio Un ivers i ty3- \ ]4-1 Hiyoshi ,  Kohoku-kuYokohama 223 Japansh iho@nak.math .ke io .ac .
jpAbstractIt is obvious that segmentation takesan important role in natural languageprocessing(NLP), especially for the lan-guages whose sentences are not eas-ily separated into morphemes.
In thisstudy we propose amethod of segment-ing a sentence.
The system describedin this paper does not use any gram-matical information or knowledge inprocessing.
Instead, it uses statisticalinformation drawn from non-tagged cor-pus of the target language.
Most ofthe segmenting systems are to pick outconventional morphemes which is de-fined for human use.
However, we stilldo not know whether those conventionalmorphemes are good units for compu-tational processing.In this paper we explain our system'salgorithm and its experimental resultson Japanese, though this system is notdesigned for a particular language.1 Characteristics of JapaneseText1.1 Let ters  in  JapaneseJapanese text is composed of four kinds of charac-ters kanji, hiragana, katakana, and others suchas alphabetic haracters and numeral characters.Itiragana is used fbr Japanese words, inflectionsand flmction words, while k~takana is used forwords from foreign languages and for other spe-cial purposes.Table 1 shows examples of rates of those fourcharacters in texts (Teller and Batchelder, 1994).The bus__._=, corpus consists of a set of newspaperarticles on business ventures from Yomiuri.
Theed__:, corpus contains a series of editorial columnsfrom Asahi Shinbun.Table 1: Character Rates in Japanese Textbus__~, ed._=size(K chars) 42 275% hiragana 30.2 58.0% kanji 47.5 34.6% katakazla 19.3 4.8% num/alph 2.9 2.61.2 Morphemes  in  JapaneseSegmenting a Japanese text is a difficult task.
Aphrase "~b -C~ ~ b f~ (was studying)" call be asingle lexical unit or can be separated into as m~,nyas six elements (Teller and Batchelder, 1994):'study' 'do' particle progressive polite pastAcquiring "morphemes" from Japanese text isnot a simple task because of this flexibility.2 Linky StringsThis paper is on dividing non-separated languagesentences into meaningful strings of letters with-out using any grammar or linguistic knowledge.Instead, this system uses the statistical informa-tion between letters to select he best ways to seg-ment sentences in non-separated languages.It is not very hard to divide a sentence using acertain dictionary for that.
The problem is that a'certain dictionary' is not easily obtainable.
Therenever is a perfect dictionary which holds all thewords that exist in the lmlguage.
Moreover, build-ing a dictionary is very hard work, since there areno perfect automatic dictionary-making systems.586llowever, machine-readable dictionaries areneeded anyway.
~br this reason, we propose a newmethod for picking out meaningflfl strings.
Ourpurpose is not to segment a sentence into conven-tional morphemes.
We introduce a concept for atype of language unit for machine use.
We namedthe unit a 'linky string'.
A linky string is a seriesof letters extracted from a corpus using statisti-cal intbrmation only.
It is a series of letters whichshare a strong statistical relationship.3 L INK ING SCORE3.1 L ink ing  ScoreTo pick out linky strings, we need to find highlyconnectable letters in a sentence.
We introducethe linking score., which shows the linkability be-tween two neighbor letters in a sentence.
Thisscore is estimated using d-bigram statistics.3 .2  D-b igramThe idea of bigrams and trigrams is often used instudies on NI,P.
Wgram is the information of theassociation between certain events.
In this studywe use thed-b igram data (Tsutsumi et al, 1993),which is a kind of bigrmn data with the concept ofdistance between events (Figure l).
l)-higram isequal to bigram when d = l, thus d~bigrmn dataincludes the conventional bigrmn relation.d.bigrsm7q \[E3 DL__ I  L___I \ [ __ l  L__ J  ?
='I; I I, j JL Jr \] .
.
.~  ....Figure l: i)-bigram3.3  Ca lcu la t ionMutua l  In f i J rmat ion  w i th  D is tanceExpression (1) iv for calculating mutual intbrma-tion between two events(Nobesawa et al, 1994):l'(ai, bj, d)bj, d) = log  v ( .d / ' (b , )  (1)ai : a letterP(ai) : the possibility the letter ai appearsl?
(ai, bj, d) : the possibility ai and bj appear togetherwith the distance d in a sentenceThe parameter d shows the distance betweentwo events.
In Figure 2, the distance between "a"m~d "pen" is 1, and the distm,ce between "is" and"pen" is 2 as well.
Since the event order has ameaning, in this case the distance between "pen"and "a" is defined as -1 .ThL~ is a pen  .dM2d=3Figure 2: D-bigram ExampleAs the vahm of MI gets bigger, the stronger isthe association between the two events.L ink ing ScoreExpression (2) is tbr calculating the linking scorebetween two letters in a sentence ~.Z (2)d:-:l j= i - (d -1)dmax : max distance usedwl : the i-th letter in the sentence wg(d) : a certain weight for iV//concerning distance between lettersThe information between two remote wordshas less nmaning in a sentence when it comes tothe semantic analysis(Church and Hanks, 1989).According to the idea we put g(d) in the expres-sion so that nearer pair can be more effective incalculating the score of the sentence.|h ' i  , , I II - - '1B C @@ F G HFigure 3: Calculation of Linking ScoreA pair of far-away letters do not have strongrelation between each other, neither syntacticallynor semantically.
For this reason we use dma,, andin this paper we set tile dmax value 2 to 5 and 1.When the dma, is 1, the MI used in calculation isonly bigram data.1We made a Japanese word ">(i~" tlar the word "linky",We used it's pronunciation "UK \[ju:kei\]" in the expre~ion.~We had experiments for tinding a good value for dmaa:,587U ff L__t L_ .
.
.
.
.
!
L_.__J U U Ul tFigure 5: The Score Graph4 THE SYSTEM L~S4.1 OverviewThis system is called LSS, a "linky string segmen-tor".
This system takes a corpus made of non-separated sentences as its input and segments itinto linky strings using d-bigram statistics.Figure 4 shows the flow of LSS's processing.Input sentences to segment.Calculate the linking score ofeach pair of neighboring letters.Check the score graphto see where to segment.pick out each linky stringfound in the given corpus.Figure 4: System Processing FlowIn this paper we used a fixed score for the start-ing score, so that /S~ can decide whether the firstletter should be a one-letter linky string.4 .2  The  Score  GraphWhat  a Score Graph IsTo segment a sentence into statistically-meaningfulstrings, we use the linking scores to locate bound-aries between linking strings.
A score graph hasthe letters in a sentence on the x-axis and link-ing scores on the y-axis (Figure 5).
We get onescore graph for each sentence.
Figure 5 shows twosentences (one above and one below), each of 14letters (including an exclamation/question markas the sentence terminator).When the linking score between a pair of neigh-boring letters is high, we assume they are part ofthe same word.
When it is low, we assume thatthe letters, though neighbors, are statistically in-dependent of one another.
In a score graph, aseries of scores in the shape of mountain (ex.
: A-Band C-F part in Figure 5) becomes a linky string,and a valley (ex.
: between the letter B and C inFigure 5) is a spot to segment.Score -Graph Segment ing  A lgor i thmThe system LSS finds the valley-points in a sen-tence and segments the sentence there into strings.Following is the algorithm to find the segment-ing points in a sentence.1.
Do not segment in a mountain.2.
Segment at the valley point.3.
Cut before and after a one-lettered linky string.One-Let tered  L inky  Str ingA one-lettered linky string needs to (a) place atthe valley point, and (b) look flat 3 in the scoregraph.
In Figure 5, one-lettered linky strings areG,L,N 4 '0 'Y ,  Zand?.Mounta in  Thresho ldA linky string takes a mountain shape because ofhigh linking scores.
Note that a linky string is notequal to a morpheme in human-handmade gram-mars.
When a certain pair of morphemes occursin a corpus very often, the system recognizes thepair's high linking score and puts them togetherinto one linky string.
For example, "~" "~ "2/~::)k:~$~ (President Bush)" is often treated as a linkystring, since ":)" "9 "J ~ (Bush)" and "gk:})~YI (pres-ident)" appear next to each other very frequently.The mountains of letters are not always simplehat-shaped; most of time they have other smallermountains in them.
This means that there can beshorter strings in one linky string.
In one linkystring "7" y "5' J - :}%i~ (President Bush)", theremust be two smaller mountains, just like H-I andJ-K in the mountain H-K in Figure 5.
To controlthe size of linky strings we introduce a mounta inthreshold, which is shown in the sentence belowin Figure 5.
When the score of a valley point ishigher than the mountain threshold, the systemjudges the point isnot  a segmenting spot.
In thispaper the mountain threshold value is 5.0.3We use a constant value as a threshold.aN is a special one-lettered linky string which places atthe beginning of a sentence.588Figure 6:Bigramr---1B C ~ D  E Fb l | rh 'n  ~ lyII iI '+ ' i I I - - \ ]A S C ~ D  Ed-lolCz, am sum upThe Difference between D-bigram and'Fable 3: Outputd-bigram bigramdmaz = 5# of input sentences 302 302# of linky strings 6,145 7,098# of linky strings persentence 20.35 23.50# of over-segmented spots 454 689over-segmented spots 7.39% 9.71%4.3  CorpusLcN accepts all the non-separated sentences withlittle preparation.
All we need is a set of certainamount of the target-language corpus for training.In this paper we show the experimental resultson Japanese.
The corpus prepared for this paperis of Asahi Shinbun Newspaper.5 RESULTS5 .1  Exper imenta l  Resu l tsExper iment  Cond i t ionLK?/takes a set of non-separated sentences as itsinput and segments them into linky strings.
Forthe test corpus we chose sentences at random fromthe training corpus.into 20-25 linky strings on average 5.
And in onesentence there are only one or two spots on averagewhich break a morpheme into meaningless strings.With no linguistic knowledge, this can be said tobe quite a good result.It is hard to check whether an extracted linkystring is a right one, however, it is not that difficultto find over-segmented strings, for a linky stringneeds to hold the meaning.
We check those over-segmented linky strings according to a dictionary,Iwanami Kokugo Jiten.Table 4 shows the numbers of over-segmentedspots.
The figure is the ~mmber of over-segmentedspots, not the number of morphemes over-segmented 6.In Table 4 A and B are neighboring letters in asentence which are forced to separate.
The row"kanji hiragana" stands fdr over-segmented spotsbetween a km~ji letter and a hiragmm letter.
'Fable 2: Training Corpus Conditionlanguage: Japaneseform: non-separatedkanji-kana mixed sentencesAsahi Shinbun Newspaper corpus:of sentencesfor training corpus: 7,502# of sentencesfor test corpus: 302To see the efficacy of d-bigram, we compare theexperimental results of two data: d-bigram dataand bigram data.Exper imenta l  Resu l tsAs shown in Table 3, with d-bigrmn informationonly 7.39% of the segment spots are over-segmented.Table 3 shows that a sentence gets separatedTable 4: Over-Segmented Morphemes by Charac-ter Types and Segmentation Methodsd-bigramA B d .
.
.
.
= 5anji kanjikanji hiraganahiragana kanjihiragana hiraganakatakana katakanatotal 454bigram59 6529 4318 22333 50715 52689The ratio of over-segmented morphemes foreach part of speech is shown in Table 5.
'K' standsfor kanji, 'h' is for hiragana nd 'k' is for katakana.There was no missegmentation between katakanaand other character types.
There also was not any5'Fhe range of numbers of linky strings found in a sen-tence is 5-60 with d-bigram and 6-66 with bigram.6Thus  a morpheme gets counted twice when it is dividedinto three strings.589Table 5: Over-Segmented Morphemes in Outputwith D-bigramABnounproper nounpronounverbaux.
verbadjectiveadj.
verbadverbrentai-shiconjunctionfuncion wordsuffixcompound wordtotalK K h h k V-- - -  ~K h K h k49 19 6 49 6 1295 8 1316 161 3 12 84 10060 604 12 164 13 171 53 1 5511 117 715 151 4 51 15 1659 29 18 333 151454\ ]missegmentation concerning alphabets, numeralcharacters and other symbols.5 .2  A L inky  St r ingCharacter i s t i cs  o f  L inky  S t r ingsLinky strings in Japanese are not equal to conven-tional morphemes in Japanese.
As discussed insection 1.2, it is not easy to decide an absolutelycorrect segmenting spot in a Japanese sentence.That is one of the reasons that we decided to ex-tract linky strings, instead of conventional mor-phemes.
Itowever, if those linky strings do notkeep the meanings, it is useless.The result shows the linking score works wellenough not to segment senteces too much (Table3).
That is, we succeeded in extracting memfingfldstrings using only statistical information.
Figure7 shows some examples of extracted linky strings..~1{~ bank (s) meaatingful~Z ~ move/shift to meaningfifl?)
~T~)J action of meaningful~T ~ ~: did meaningful(?)
~=Y (?)
over-segmented(~V) q~'~\]':~ over-segmentedFigure 7: Examples of Linky Strings (1)Sometimes LS8 extracts strings that look toolong (Figure 8).
This is not a bad result, though.When a linky string contains several morphemesin it, it is something like picking out idioms.
Alinky string with several morphemes may be acompound word, or an idiom, or a fixed locution.1 7 t~a)$helpLondon Summitnuclear non-proliferation treatyat the end of 17th centuryJapan Railway Kyoro StationFigure 8: Examples of Linky Strings (2)The  Concept  of  the L inky  Str ingsGrammar-based NLP systems generally specify atarget language.
On the other hand statistically-based approachs do not need rules or knowledge.This makes a statistically-based approach suitableto nmltilingual processing.ISg is not only for Japanese.
With a corpus ofnon-separated sentences of any language, LSS canperform the same kind of segmentation.To deal with natural languages most systemsuse conventional morphemes or words as their pro-cessing units.
That is, most systems need to rec-ognize morphemes or words in sentences, and theyneed to make up a fairly-good morphological nal-ysis before the main processing.
We have beenworking for processing natural languages in lin-guistic ways, though we do not know whether it isa right way in computational linguistics.
A linkystring is extracted only with statistical informa-tion, using no grammars nor linguistic knowledge.The system does not need to behave like a nativespeaker of the target language; all it has to do ischeck statistical information, which is what com-puters are good at.
We expect that linky stringscan be a key to solve problems of NLP.Compound WordsThe results show that the system has 7.39% in-correct segmentation.
This result is based on aJapanese dictionary, and when a morpheme listedin the dictionary gets separated, we count it asover-segmented.
However, a dictionary often holdscompound words.
That is, some number of tilesegmented spots which we have counted as "over-segmented" ones are not really over-segmented.From this point of view, the percentage of over-segmentation is actually even lower.Inf lect ionsVerbs, adjectives, adverbs and auxiliary verbs areinflected in Japanese.
In the experimental result,89.7% (with d-bigram data) of over-segmented spotsbetween kanji and hiragana occurs in inflectivemorphemes.
We decided correct segmenting spots590for inflective morphemes according to a Japanesedictionary.
According to statistical information,segmenting method for inflective morphemes i  dif-ferent fl'om grammatical one.
So most of the over-segmented spots can be treated as correct seg-menting spots according to statistical information.5.3 D-bigram StatisticsAccording to Table 3, it scents that using the bi-gram method the output is apt to be more seg-mented than with the d-bigram method.This happens t.)ecause bigrmn cannot pick outlong strings.
Bigrmn does not hoht informationbetween remote (actually more than one letteraway) letters.
That makes long strings of letterseasily segmented.
When LcN checks a three-lettermorpheme ABC, with bigram data it can see thestring only as A-B and B-C.
If those strings ABand BC do not .appear often, the linking scoresget low and Lq.S decides to segment between A-B and B-C. IIowever, with d-bigram data ISS canget the information between A and C as well, thathelps to recognize tlmt A, B and C often come outtogether.
This happens frequently between twokatakana letters (Table 4), because of the usage ofkatakana letters in Japanese.This does not mean that with d-bigram methodsentences are less likely to be segmented.
As shownin Figure 9, the distribution is not so different be-tween two methods.
The x-axis shows the nmn-bers of linky strings in sentences and the y-axisshows the number of sentences with x linky strings.s~nt  oncos20\] 8 ','~ - - - -  d -b ig lam'~, .... b ig ram1 6 i14 7 .
.
.
.
.
.i _12i o ~{~ A642i 0  20  30  40  50  60st:I  h lgsFigure 9: The Number of Strings in Outlmt Sen-tencesAccording to Figure 9, the distributions of sen-tences are not so different between the methodwith d-bigram and the one with bigram.6 CONCLUSIONThis paper shows that this automatic segmentingsystem /NS is quite efficient for segmentation ofnon-separated language sentences.
/AN does notuse any grammatical information to divide inputsentences into linky strings, that is, a new refitfor NLP.
According to the results of the experi-ments, Lcx?~ can segment ahnost all the sentences'correctly', with strings keeping their meanings.This remarkable result of a statistic-based sys-tem l~ shows that d-bigram statistical informa-tion can be a key to meaningful-string extracting.This result also shows that the concept of linkystrings is an interesting concept for NLP.
We ex-pect that this linky string can he a unit for ma-chine translation systems or key word/phrase x-traction systems, and other NLP systems.References\[1\] Tsutsumi, J., Nitta, T., Ono, K. and Nobesawa,S.
A Multi-Lingual Translation System Based onA Statistical Model(written in Japanese).
JSAITechnical report, SIG-PPAI-9302-2, pages 7 12,1993.\[2\] Nobesawa, S., Tsutsumi, J., Sun D. J., Sano, T.,Sato K. and Nakanishi, M. Automatic Extractionof Linky Strings in Natural Languages (writtenin Japanese).
62nd Annual Meeting of the ANLP(NLP96), pages 181-184, 1996.\[3\] Nobesawa, S., Tsutsumi, J., Nitta, T., Ono, K.,Sun, 1).
J. and Nakanishi, M. Segmeting aJapanese Sentence into Morphemes Using Statisti-cal Information between Words.
Coling-94, pages227-233, 1994.\[4\] Teller V. and Batchelder E. O.
A ProbabilisiticAlgorithm for Segmenting Non-Kanji &q)aneseStrings.
AAAI, 1994.\[5\] Brown, P., Cocke, J., Pietra, S. D., Pietra, V. D.,Jelinek, F., Mercer, R. and l~oossin, A.
A Statisti-cal Approach to Language Translation.
Coling-88,pages 71--76, 1988.\[6\] Church, K. and Hanks, l'.
Word AssociationNorms, Mutual Information, and Lexicography.
InProceedings of the 27th Annual Conference of theassociation of Computational Linguistics, 1989.\[7\] Nishio, M., Iwabuchi, E. ~ld Mizutani, S.Japanese-Japanese Dictionary The 3rd Edition.Iwanami Shoten, 1985.591
