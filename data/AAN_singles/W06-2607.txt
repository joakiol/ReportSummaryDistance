Tree Kernel Engineering in Semantic Role Labeling SystemsAlessandro Moschitti and Daniele Pighin and Roberto BasiliUniversity of Rome, Tor Vergata{moschitti,basili}@info.uniroma2.itdaniele.pighin@gmail.comAbstractRecent work on the design of automaticsystems for semantic role labeling hasshown that feature engineering is a com-plex task from a modeling and implemen-tation point of view.
Tree kernels alleviatesuch complexity as kernel functions gener-ate features automatically and require lesssoftware development for data extraction.In this paper, we study several tree kernelapproaches for both boundary detectionand argument classification.
The compar-ative experiments on Support Vector Ma-chines with such kernels on the CoNLL2005 dataset show that very simple treemanipulations trigger automatic featureengineering that highly improves accuracyand efficiency in both phases.
Moreover,the use of different classifiers for internaland pre-terminal nodes maintains the sameaccuracy and highly improves efficiency.1 IntroductionA lot of attention has been recently devoted tothe design of systems for the automatic label-ing of semantic roles (SRL) as defined in twoimportant projects: FrameNet (Johnson and Fill-more, 2000), inspired by Frame Semantics, andPropBank (Kingsbury and Palmer, 2002) basedon Levin?s verb classes.
In general, given a sen-tence in natural language, the annotation of a pred-icate?s semantic roles requires (1) the detection ofthe target word that embodies the predicate and(2) the detection and classification of the word se-quences constituting the predicate?s arguments.
Inparticular, step (2) can be divided into two differ-ent phases: (a) boundary detection, in which thewords of the sequence are detected and (b) argu-ment classification, in which the type of the argu-ment is selected.Most machine learning models adopted for theSRL task have shown that (shallow or deep) syn-tactic information is necessary to achieve a goodlabeling accuracy.
This research brings a wideempirical evidence in favor of the linking theoriesbetween semantics and syntax, e.g.
(Jackendoff,1990).
However, as no theory provides a soundand complete treatment of such issue, the choiceand design of syntactic features for the automaticlearning of semantic structures requires remark-able research efforts and intuition.For example, the earlier studies concerning lin-guistic features suitable for semantic role labelingwere carried out in (Gildea and Jurasfky, 2002).Since then, researchers have proposed diverse syn-tactic feature sets that only slightly enhance theprevious ones, e.g.
(Xue and Palmer, 2004) or(Carreras and Ma`rquez, 2005).
A careful analy-sis of such features reveals that most of them aresyntactic tree fragments of training sentences, thusa natural way to represent them is the adoption oftree kernels as described in (Moschitti, 2004).
Theidea is to associate with each argument the mini-mal subtree that includes the target predicate withone of its arguments, and to use a tree kernel func-tion to evaluate the number of common substruc-tures between two such trees.
Such approach is inline with current research on the use of tree kernelsfor natural language learning, e.g.
syntactic pars-ing re-ranking (Collins and Duffy, 2002), relationextraction (Zelenko et al, 2003) and named entityrecognition (Cumby and Roth, 2003; Culotta andSorensen, 2004).Regarding the use of tree kernels for SRL, in(Moschitti, 2004) two main drawbacks have been49pointed out:?
Highly accurate boundary detection cannotbe carried out by a tree kernel model sincecorrect and incorrect arguments may share alarge portion of the encoding trees, i.e.
theymay share many substructures.?
Manually derived features (extended with apolynomial kernel) have been shown to be su-perior to tree kernel approaches.Nevertheless, we believe that modeling a com-pletely kernelized SRL system is useful for the fol-lowing reasons:?
We can implement it very quickly as the fea-ture extractor module only requires the writ-ing of the subtree extraction procedure.
Tra-ditional SRL systems are, in contrast, basedon the extraction of more than thirty features(Pradhan et al, 2005), which require the writ-ing of at least thirty different procedures.?
Combining it with a traditional attribute-value SRL system allows us to obtain a moreaccurate system.
Usually the combination oftwo traditional systems (based on the samemachine learning model) does not result inan improvement as their features are moreor less equivalent as shown in (Carreras andMa`rquez, 2005).?
The study of the effective structural featurescan inspire the design of novel linear fea-tures which can be used with a more efficientmodel (i.e.
linear SVMs).In this paper, we carry out tree kernel engineer-ing (Moschitti et al, 2005) to increase both ac-curacy and speed of the boundary detection andargument classification phases.
The engineeringapproach relates to marking the nodes of the en-coding subtrees in order to generate substructuresmore strictly correlated with a particular argu-ment, boundary or predicate.
For example, mark-ing the node that exactly covers the target ar-gument helps tree kernels to generate differentsubstructures for correct and incorrect argumentboundaries.The other technique that we applied to engineerdifferent kernels is the subdivision of internal andpre-terminal nodes.
We show that designing dif-ferent classifiers for these two different node typesslightly increases the accuracy and remarkably de-creases the learning and classification time.An extensive experimentation of our tree ker-nels with Support Vector Machines on the CoNLL2005 data set provides interesting insights on thedesign of performant SRL systems entirely basedon tree kernels.In the remainder of this paper, Section 2 intro-duces basic notions on SRL systems and tree ker-nels.
Section 3 illustrates our new kernels for bothboundary and classification tasks.
Section 4 showsthe experiments of SVMs with the above tree ker-nel based classifiers.2 Preliminary ConceptsIn this section we briefly define the SRL modelthat we intend to design and the kernel functionthat we use to evaluate the similarity between sub-trees.2.1 Basic SRL approachThe SRL approach that we adopt is based on thedeep syntactic parse (Charniak, 2000) of the sen-tence that we intend to annotate semantically.
Thestandard algorithm is to classify the tree node pair?p, a?, where p and a are the nodes that exactlycover the target predicate and a potential argu-ment, respectively.
If ?p, a?
is labeled with an ar-gument, then the terminal nodes dominated by awill be considered as the words constituting suchargument.
The number of pairs for each sentencecan be hundreds, thus, if we consider training cor-pora of thousands of sentences, we have to dealwith millions of training instances.The usual solution to limit such complexity is todivide the labeling task in two subtasks:?
Boundary detection, in which a single clas-sifier is trained on many instances to detectif a node is an argument or not, i.e.
if thesequence of words dominated by the targetnode constitutes a correct boundary.?
Argument classification: only the set ofnodes corresponding to correct boundariesare considered.
These can be used to train amulticlassifier that, for such nodes, only de-cides the type of the argument.
For example,we can train n classifiers in the style One-vs-All.
At classification time, for each argumentnode, we can select the argument type asso-ciated with the maximum among the n scoresprovided by the single classifiers.50We adopt this solution as it enables us to useonly one computationally expensive classifier, i.e.the boundary detection one.
This, as well as theargument classifiers, requires a feature represen-tation of the predicate-argument pair.
Such fea-tures are mainly extracted from the parse trees ofthe target sentence, e.g.
Phrase Type, PredicateWord, Head Word, Governing Category, Positionand Voice proposed in (Gildea and Jurasfky, 2002).As most of the features proposed in literatureare subsumed by tree fragments, tree-kernel func-tions are a natural way to produce them automati-cally.2.2 Tree kernel functionsTree-kernel functions simply evaluate the numberof substructures shared between two trees T1 andT2.
Such functions can be seen as a scalar productin the huge vector space constituted by all possi-ble substructures of the training set.
Thus, kernelfunctions implicitly define a large feature space.Formally, given a tree fragment space{f1, f2, ..} = F , we can define an indica-tor function Ii(n), which is equal to 1 if thetarget fi is rooted at node n and equal to0 otherwise.
Therefore, a tree-kernel func-tion K over T1 and T2 can be defined asK(T1, T2) =?n1?NT1?n2?NT2 ?
(n1, n2),where NT1 and NT2 are the sets of theT1?s and T2?s nodes, respectively and?
(n1, n2) =?|F|i=1 Ii(n1)Ii(n2).
This latteris equal to the number of common fragmentsrooted at nodes n1 and n2 and, according to(Collins and Duffy, 2002), it can be computed asfollows:1. if the productions at n1 and n2 are differentthen ?
(n1, n2) = 0;2. if the productions at n1 and n2 are thesame, and n1 and n2 have only leaf chil-dren (i.e.
they are pre-terminal symbols) then?
(n1, n2) = ?;3.
if the productions at n1 and n2 are the same,and n1 and n2 are not pre-terminal then?
(n1, n2) = ?
?nc(n1)j=1 (1 + ?
(cjn1 , cjn2)).where ?
is the decay factor to scale down the im-pact of large structures, nc(n1) is the number ofthe children of n1 and cjn is the j-th child of thenode n. Note that, as the productions are the same,nc(n1) = nc(n2).
Additionally, to map similar-ity scores in the [0,1] range, we applied a nor-Figure 1: The PAF subtree associated with A1.Figure 2: Example of CMST.malization in the kernel space, i.e.
K ?
(T1, T2) =K(T1,T2)?K(T1,T1)?K(T2,T2).Once a kernel function is defined, we need tocharacterize the predicate-argument pair with asubtree.
This allows kernel machines to generate alarge number of syntactic features related to suchpair.
The approach proposed in (Moschitti, 2004)selects the minimal subtree that includes a predi-cate with its argument.
We follow such approachby studying and proposing novel, interesting solu-tions.3 Novel Kernels for SRLThe basic structure used to characterize the predi-cate argument relation is the smallest subtree thatincludes a predicate with one of its argument.
Forexample, in Figure 1, the dashed line encloses apredicate argument feature (PAF) over the parsetree of the sentence: ?Paul delivers a talk in for-mal style?.
This PAF is a subtree that characterizesthe predicate to deliver with its argument a talk.In this section, we improve PAFs, propose dif-ferent kernels for internal and pre-terminal nodesand new kernels based on complete predicate ar-51Figure 3: Differences between PAF (a) and MPAF (b) structures.gument structures.3.1 Improving PAFPAFs have shown to be very effective for argu-ment classification but not for boundary detection.The reason is that two nodes that encode correctand incorrect boundaries may generate very sim-ilar PAFs.
For example, Figure 3.A shows twoPAFs corresponding to a correct (PAF+) and anincorrect (PAF-) choice of the boundary for A1:PAF+ from the NP vs. PAF- from the N nodes.
Thenumber of their common substructures is high, i.e.the four subtrees shown in Frame C. This preventsthe algorithm from making different decisions forsuch cases.To solve this problem, we specify which is thenode that exactly covers the argument (also calledargument node) by simply marking it with the la-bel B denoting the boundary property.
Figure 3.Bshows the two new marked PAFs (MPAFs).
Thefeatures generated from the two subtrees are nowvery different so that there is only one substructurein common (see Frame D).
Note that, each markupstrategy impacts on the output of a kernel functionin terms of the number of structures common totwo trees.
The same output can be obtained us-ing unmarked trees and redefining consistently thekernel function, e.g.
the algorithm described inSection 2.2.An alternative way to partially solve the struc-ture overlapping problem is the use of two differ-ent classifiers, one for the internal nodes and onefor the pre-terminal nodes, and combining theirdecisions.
In this way, the negative example ofFigure 3 would not be used to train the same clas-sifier that uses PAF+.
Of course, similar structurescan both be rooted on internal nodes, thereforethey can belong to the training data of the sameclassifier.
However, the use of different classi-fiers is motivated also by the fact that many ar-gument types can be found mostly in pre-terminalnodes, e.g.
modifier or negation arguments, anddo not necessitate training data extracted from in-ternal nodes.
Consequently, it is more convenient(at least from a computational point of view) touse two different boundary classifiers, hereinafterreferred to as combined classifier.3.2 Kernels on complete predicate argumentstructuresThe type of a target argument strongly depends onthe type and number of the predicate?s arguments1(Punyakanok et al, 2005; Toutanova et al, 2005).Consequently, to correctly label an argument, weshould extract features from the complete predi-cate argument structure it belongs to.
In contrast,PAFs completely neglect the information (i.e.
thetree portions) related to non-target arguments.One way to use this further information withtree kernels is to use the minimum subtree thatspans all the predicate?s arguments.
The wholeparse tree in Figure 1 is an example of such Min-imum Spanning Tree (MST) as it includes all andonly the argument structures of the predicate ?todeliver?.
However, MSTs pose some problems:?
We cannot use them for the boundary detec-tion task since we do not know the predi-cate?s argument structure yet.
However, wecan derive the MST (its approximation) fromthe nodes selected by a boundary classifier,i.e.
the nodes that correspond to potential ar-guments.
Such approximated MSTs can beeasily used in the argument type classifica-tion phase.
They can also be used to re-rankthe most probable m sequences of argumentsfor both labeling phases.?
Obviously, an MST is the same for all thearguments it includes, thus we need a wayto differentiate it for each target argument.1This is true at least for core arguments.52Again, we can mark the node that exactlycovers the target argument as shown in theprevious section.
We refer to this subtree asmarked MST (MMST).
However, for largearguments (i.e.
spread on a large part of thesentence tree) the substructures?
likelihood ofbeing part of other arguments is quite high.To address this latter problem, we can mark allnodes that descend from the target argument node.Figure 2 shows a MST in which the subtree as-sociated with the target argument (AM) has thenodes marked.
We refer to this structure as acompletely marked MST (CMST).
CMSTs maybe seen as PAFs enriched with new informationcoming from the other arguments (i.e.
the non-marked subtrees).
Note that if we consider onlythe PAF subtree from a CMST we obtain a differ-ently marked subtree which we refer to as CPAF.In the next section we study the impact of theproposed kernels on the boundary detection andargument classification performance.4 ExperimentsIn these experiments we evaluate the impact of ourproposed kernels in terms of accuracy and effi-ciency.
The accuracy improvement confirms thatthe node marking approach enables the automaticengineering of effective SRL features.
The effi-ciency improvement depends on (a) the less train-ing data used when applying two distinct type clas-sifiers for internal and pre-terminal nodes and (b) amore adequate feature space which allows SVMsto converge faster to a model containing a smallernumber of support vectors, i.e.
faster training andclassification.4.1 Experimental set upThe empirical evaluations were carried out withinthe setting defined in the CoNLL-2005 SharedTask (Carreras and Ma`rquez, 2005).
Weused as a target dataset the PropBank corpusavailable at www.cis.upenn.edu/?ace, alongwith the Penn TreeBank 2 for the gold trees(www.cis.upenn.edu/?treebank) (Marcus et al,1993), which includes about 53,700 sentences.Since the aim of this study was to design a realSRL system we adopted the Charniak parse treesfrom the CoNLL 2005 Shared Task data (availableat www.lsi.upc.edu/?srlconll/).We used Section 02, 03 and 24 from the PennTreeBank in most of the experiments.
Their char-acteristics are shown in Table 1.
Pos and Neg in-dicate the number of nodes corresponding or notto a correct argument boundary.
Rows 3 and 4 re-port such number for the internal and pre-terminalnodes separately.
We note that the latter are muchfewer than the former; this results in a very fastpre-terminal classifier.As the automatic parse trees contain errors,some arguments cannot be associated with anycovering node.
This prevents us to extract a treerepresentation for them.
Consequently, we do notconsider them in our evaluation.
In sections 2, 3and 24 there are 454, 347 and 731 such cases, re-spectively.The experiments were carried out withthe SVM-light-TK software available athttp://ai-nlp.info.uniroma2.it/moschitti/which encodes fast tree kernel evaluation (Mos-chitti, 2006) in the SVM-light software (Joachims,1999).
We used a regularization parameter (option-c) equal to 1 and ?
= 0.4 (see (Moschitti,2004)).4.2 Boundary Detection ResultsIn these experiments, we used Section 02 for train-ing and Section 24 for testing.
The results usingthe PAF and the MPAF based kernels are reportedin Table 2 in rows 2 and 3, respectively.
Columns3 and 4 show the CPU testing time (in seconds)and the F1 of the monolithic boundary classifier.The next 3 columns show the CPU time for the in-ternal (Int) and pre-terminal (Pre) node classifiers,as well as their total (All).
The F1 measures arereported in the 3 rightmost columns.
In particular,the third column refers to the F1 of the combinedclassifier.
This has been computed by summingcorrect, incorrect and not retrieved examples of thetwo distinct classifiers.We note that: first, the monolithic classifier ap-plied to MPAF improves both the efficiency, i.e.about 3,131 seconds vs. 5,179, of PAF and theF1, i.e.
82.07 vs. 75.24.
This suggests that mark-ing the argument node simplifies the generaliza-tion process.Second, by dividing the boundary classifica-tion in two tasks, internal and pre-terminal nodes,we furthermore improve the classification time forboth PAF and MPAF kernels, i.e.
5,179 vs. 1,851(PAF) and 3,131 vs. 1,471 (MPAF).
The sepa-rated classifiers are much faster, especially the pre-terminal one (about 61 seconds to classify 81,075nodes).53Section 2 Section 3 Section 24Nodes pos neg tot pos neg tot pos neg totInternal 11,847 71,126 82,973 6,403 53,591 59,994 7,525 50,123 57,648Pre-terminal 894 114,052 114,946 620 86,232 86,852 709 80,366 81,075Both 12,741 185,178 197,919 7,023 139,823 146,846 8,234 130,489 138,723Table 1: Tree nodes of the sentences from sections 2, 3 and 24 of the PropBank.
pos and neg are thenodes that exactly cover arguments and all the other nodes, respectively.Monolithic CombinedTagging strategy CPUtime F1 CPUtime F1Int Pre All Int Pre AllPAF 5,179.18 75.24 1,794.92 56.72 1,851.64 79.93 79.39 79.89MPAF 3,131.56 82.07 1,410.10 60.99 1,471.09 82.20 79.14 81.96Table 2: F1 comparison between PAF and MPAF based kernels using different classification strategies.Int, Pre and ALL are the internal, pre-terminal and combined classifiers.
The CPU time refers to theclassification time in seconds of all Section 24.Figure 4: Learning curve comparison between thePAF and MPAF F1 measures using the combinedclassifier.Third, the combined classifier approach seemsquite feasible as its F1 is almost equal to the mono-lithic one (81.96 vs. 82.07) in case of MPAF andeven superior when using PAF (79.89 vs. 75.34).This result confirms the observation given in Sec-tion 3.1 about the importance of reducing the num-ber of substructures common to PAFs associatedwith correct and incorrect boundaries.Finally, we trained the combined boundary clas-sifiers with sets of increasing size to derive thelearning curves of the PAF and MPAF models.To have more significant results, we increased thetraining set by using also sections from 03 to 07.Figure 4 shows that the MPAF approach is con-stantly over the PAF.
Consider also that the mark-ing strategy has a lesser impact on the combinedclassifier.4.3 Argument Classification ResultsIn these experiments we tested different kernelson the argument classification task.
As some ar-guments have a very small number of training in-stances in a single section, we also used Section03 for training and we continued to test on onlySection 24.The results of the multiclassifiers on 59 argu-ment types2 (e.g.
constituted by 59 binary clas-sifiers in the monolithic approach) are reported inTable 3.
The rows from 3 to 5 report the accuracywhen using the PAF,MPAF and CPAFwhereas therows from 6 to 8 show the accuracy for the com-plete argument structure approaches, i.e.
MST,MMST and CMST.More in detail, Column 2 shows the accuracy ofthe monolithic multi-argument classifiers whereasColumns 3, 4 and 5 report the accuracy of the in-ternal, pre-terminal and combined multi-argumentclassifiers, respectively.We note that:First, the two classifier approach does not im-prove the monolithic approach accuracy.
Indeed,the subtrees describing different argument typesare quite different and this property holds also forthe pre-terminal nodes.
However, we still mea-sured a remarkable improvement in efficiency.Second, MPAF is the best kernel.
This con-firms the outcome on boundary detection ex-periments.
The fact that it is more accu-rate than CPAF reveals that we need to distin-27 for the core arguments (A0...AA), 13 for the adjunctarguments (AM-*), 19 for the argument references (R-*) and20 for the continuations (C-*).54Monolithic CombinedTagging strategy Internal nodes Pre-terminals OverallPAF 75.06 74.16 85.61 75.15MPAF 77.17 76.25 85.76 77.07CPAF 76.79 75.68 85.76 76.54MST 34.80 36.52 78.14 40.10MMST 72.55 71.59 86.32 72.86CMST 73.21 71.93 86.32 73.17Table 3: Accuracy produced by different tree kernels on argument classification.
We trained on sections02 and 03 and tested on Section 24.guish the argument node from the other nodes.To explain this, suppose that two argumentnodes, NP1 and NP2, dominate the follow-ing structures: [NP1 [NP [DT NN]][PP]]and [NP2 [DT NN]].
If we mark only theargument node we obtain [NP-B [NP [DTNN]][PP]] and [NP-B [DT NN]] whichhave no structure in common.
In contrast, ifwe mark them completely, i.e.
[NP-B [NP-B[DT-B NN-B]][PP-B]] and [NP-B [DT-BNN-B]], they will share the subtree [NP-B[DT-B NN-B]].
Thus, although it may seemcounterintuitive, by marking only one node, weobtain more specific substructures.
Of course, ifwe use different labels for the argument nodes andtheir descendants, we obtain the same specializa-tion effect.Finally, if we do not mark the target argumentin the MSTs, we obtain a very low result (i.e.40.10%) as expected.
When we mark the cover-ing node or the complete argument subtree we ob-tain an acceptable accuracy.
Unfortunately, suchaccuracy is lower than the one produced by PAFs,e.g.
73.17% vs. 77.07%, thus it may seem thatthe additional information provided by the wholeargument structure is not effective.
A more care-ful analysis can be carried out by considering aCMST as composed by a PAF and the rest of theargument structure.
We observe that some piecesof information provided by a PAF are not deriv-able by a CMST (or a MMST).
For example, Fig-ure 1 shows that the PAF contains the subtree [VP[V NP]] while the associated CMST (see Figure2) contains [VP [V NP PP]].
The latter struc-ture is larger and more sparse and consequently,the learning machine applied to CMSTs (or MM-STs) performs a more difficult generalization task.This problem is emphasized by our use of the ad-juncts in the design of MSTs.
As adjuncts tend tobe the same for many predicates they do not pro-vide a very discriminative information.5 Discussions and ConclusionsThe design of automatic systems for the labelingof semantic roles requires the solution of complexproblems.
Among others, feature engineering ismade difficult by the structural nature of the data,i.e.
features should represent information con-tained in automatic parse trees.
This raises twoproblems: (1) the modeling of effective features,partially solved in the literature work and (2) theimplementation of the software for the extractionof a large number of such features.A system completely based on tree kernels al-leviate both problems as (1) kernel functions au-tomatically generate features and (2) only a pro-cedure for subtree extraction is needed.
Althoughsome of the manual designed features seem to besuperior to those derived with tree kernels, theircombination seems still worth applying.In this paper, we have improved tree kernelsby studying different strategies: MPAF and thecombined classifier (for internal and pre-terminalnodes) highly improve efficiency and accuracy inboth the boundary detection and argument classi-fication tasks.
In particular, MPAF improves theold PAF-based tree kernel of about 8 absolute per-cent points in the boundary classification task, andwhen used along the combined classifier approachthe speed of the model increases of 3.5 times.
Incase of argument classification the improvement isless evident but still consistent, about 2%.We have also studied tree representations basedon complete argument structures (MSTs).
Ourpreliminary results seem to suggest that additionalinformation extracted from other arguments is noteffective.
However, such findings are affected bytwo main problems: (1) We used adjuncts in thetree representation.
They are likely to add morenoise than useful information for the recognitionof the argument type.
(2) The traditional PAFcontains subtrees that cannot be derived by the55MMSTs, thus we should combine these structuresrather than substituting one with the other.In the future, we plan to extend this study asfollows:First, our results are computed individually forboundary and classification tasks.
Moreover, inour experiments, we removed arguments whosePAF or MST could not be extracted due to errorsin parse trees.
Thus, we provided only indicativeaccuracy to compare the different tree kernels.
Afinal evaluation of the most promising structuresusing the CoNLL 2005 evaluator should be carriedout to obtain a sound evaluation.Second, as PAFs and MSTs should be com-bined to generate more information, we are go-ing to carry out a set of experiments that com-bine different kernels associated with differentsubtrees.
Moreover, as shown in (Basili and Mos-chitti, 2005; Moschitti, 2006), there are other treekernel functions that generate different fragmenttypes.
The combination of such functions with themarking strategies may provide more general andeffective kernels.Third, once the final set of the most promisingkernels is established, we would like to use all theavailable CoNLL 2005 data.
This would allow usto study the potentiality of our approach by exactlycomparing with literature work.Next, our fast tree kernel function along withthe combined classification approach and the im-proved tree representation make the learning andclassification much faster so that the overall run-ning time is comparable with polynomial kernels.However, when these latter are used with SVMsthe running time is prohibitive when very largedatasets (e.g.
millions of instances) are targeted.Exploiting tree kernel derived features in a moreefficient way is thus an interesting line of futureresearch.Finally, as CoNLL 2005 has shown that themost important contribution relates on re-rankingpredicate argument structures based on one singletree (Toutanova et al, 2005) or several trees (Pun-yakanok et al, 2005), we would like to use treekernels for the re-ranking task.AcknowledgmentsThis research is partially supported by the Euro-pean project, PrestoSpace (FP6-IST-507336).ReferencesRoberto Basili and Alessandro Moschitti.
2005.
AutomaticText Categorization: from Information Retrieval to Sup-port Vector Learning.
Aracne Press, Rome, Italy.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduction tothe CoNLL-2005 shared task: Semantic role labeling.
InProceedings of CoNLL?05.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the NACL?00.Michael Collins and Nigel Duffy.
2002.
New ranking al-gorithms for parsing and tagging: Kernels over discretestructures, and the voted perceptron.
In ACL?02.Aron Culotta and Jeffrey Sorensen.
2004.
Dependency treekernels for relation extraction.
In Proceedings of ACL?04.Chad Cumby and Dan Roth.
2003.
Kernel methods for rela-tional learning.
In Proceedings of ICML?03.Daniel Gildea and Daniel Jurasfky.
2002.
Automaticlabeling of semantic roles.
Computational Linguistic,28(3):496?530.R.
Jackendoff.
1990.
Semantic Structures, Current Studies inLinguistics series.
Cambridge, Massachusetts: The MITPress.T.
Joachims.
1999.
Making large-scale SVM learning prac-tical.
In B. Scho?lkopf, C. Burges, and A. Smola, editors,Advances in Kernel Methods - Support Vector Learning.Christopher R. Johnson and Charles J. Fillmore.
2000.
Theframenet tagset for frame-semantic and syntactic codingof predicate-argument structure.
In In the ProceedingsANLP-NAACL.Paul Kingsbury and Martha Palmer.
2002.
From Treebank toPropBank.
In Proceedings of LREC?02.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of english: The PennTreebank.
Computational Linguistics, 19:313?330.Alessandro Moschitti.
2004.
A study on convolution kernelsfor shallow semantic parsing.
In Proceedings of ACL?04,Barcelona, Spain.Alessandro Moschitti, Bonaventura Coppola, Daniele Pighin,and Roberto Basili.
2005.
Engineering of syntactic fea-tures for shallow semantic parsing.
In of the ACL05 Work-shop on Feature Engineering for Machine Learning inNatural Language Processing, USA.Alessandro Moschitti.
2006.
Making tree kernels practicalfor natural language learning.
In Proceedings of EACL?06,Trento, Italy.Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, WayneWard, James H. Martin, and Daniel Jurafsky.
2005.
Sup-port vector learning for semantic argument classification.Machine Learning Journal.V.
Punyakanok, D. Roth, and W. Yih.
2005.
The necessity ofsyntactic parsing for semantic role labeling.
In Proceed-ings of IJCAI?05.Kristina Toutanova, Aria Haghighi, and Christopher Man-ning.
2005.
Joint learning improves semantic role label-ing.
In Proceedings of ACL?05.Nianwen Xue and Martha Palmer.
2004.
Calibrating featuresfor semantic role labeling.
In Proceedings of EMNLP2004.D.
Zelenko, C. Aone, and A. Richardella.
2003.
Ker-nel methods for relation extraction.
Journal of MachineLearning Research.56
