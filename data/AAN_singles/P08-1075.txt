Proceedings of ACL-08: HLT, pages 656?664,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsLearning Bigrams from UnigramsXiaojin Zhu?
and Andrew B. Goldberg?
and Michael Rabbat?
and Robert Nowak?
?Department of Computer Sciences, University of Wisconsin-Madison?Department of Electrical and Computer Engineering, McGill University?Department of Electrical and Computer Engineering, University of Wisconsin-Madison{jerryzhu, goldberg}@cs.wisc.edu, michael.rabbat@mcgill.ca, nowak@ece.wisc.eduAbstractTraditional wisdom holds that once docu-ments are turned into bag-of-words (unigramcount) vectors, word orders are completelylost.
We introduce an approach that, perhapssurprisingly, is able to learn a bigram lan-guage model from a set of bag-of-words docu-ments.
At its heart, our approach is an EM al-gorithm that seeks a model which maximizesthe regularized marginal likelihood of the bag-of-words documents.
In experiments on sevencorpora, we observed that our learned bigramlanguage models: i) achieve better test set per-plexity than unigram models trained on thesame bag-of-words documents, and are not farbehind ?oracle bigram models?
trained on thecorresponding ordered documents; ii) assignhigher probabilities to sensible bigram wordpairs; iii) improve the accuracy of ordered-document recovery from a bag-of-words.
Ourapproach opens the door to novel phenomena,for example, privacy leakage from index files.1 IntroductionA bag-of-words (BOW) is a basic document repre-sentation in natural language processing.
In this pa-per, we consider a BOW in its simplest form, i.e.,a unigram count vector or word histogram over thevocabulary.
When performing the counting, wordorder is ignored.
For example, the phrases ?reallyneat?
and ?neat really?
contribute equally to a BOW.Obviously, once a set of documents is turned intoa set of BOWs, the word order information withinthem is completely lost?or is it?In this paper, we show that one can in fact partlyrecover the order information.
Specifically, given aset of documents in unigram-count BOW representa-tion, one can recover a non-trivial bigram languagemodel (LM)1, which has part of the power of a bi-gram LM trained on ordered documents.
At firstglance this seems impossible: How can one learnbigram information from unigram counts?
However,we will demonstrate that multiple BOW documentsenable us to recover some higher-order information.Our results have implications in a wide range ofnatural language problems, in particular documentprivacy.
With the wide adoption of natural languageapplications like desktop search engines, softwareprograms are increasingly indexing computer users?personal files for fast processing.
Most index filesinclude some variant of the BOW.
As we demon-strate in this paper, if a malicious party gains accessto BOW index files, it can recover more than justunigram frequencies: (i) the malicious party can re-cover a higher-order LM; (ii) with the LM it may at-tempt to recover the original ordered document froma BOW by finding the most-likely word permuta-tion2.
Future research will quantify the extent towhich such a privacy breach is possible in theory,and will find solutions to prevent it.There is a vast literature on language modeling;see, e.g., (Rosenfeld, 2000; Chen and Goodman,1999; Brants et al, 2007; Roark et al, 2007).
How-1A trivial bigram LM is a unigram LM which ignores his-tory: P (v|u) = P (v).2It is possible to use a generic higher-order LM, e.g., a tri-gram LM trained on standard English corpora, for this purpose.However, incorporating a user-specific LM helps.656ever, to the best of our knowledge, none addressesthis reverse direction of learning higher-order LMsfrom lower-order data.
This work is inspired by re-cent advances in inferring network structure fromco-occurrence data, for example, for computer net-works and biological pathways (Rabbat et al, 2007).2 Problem Formulation and IdentifiabilityWe assume that a vocabulary of size W is given.For notational convenience, we include in the vo-cabulary a special ?begin-of-document?
symbol ?d?which appears only at the beginning of each docu-ment.
The training corpus consists of a collection ofn BOW documents {x1, .
.
.
,xn}.
Each BOW xi isa vector (xi1, .
.
.
, xiW ) where xiu is the number oftimes word u occurs in document i.
Our goal is tolearn a bigram LM ?, represented as aW?W transi-tion matrix with ?uv = P (v|u), from the BOW cor-pus.
Note P (v|?d?)
corresponds to the initial stateprobability for word v, and P (?d?|u) = 0,?u.It is worth noting that traditionally one needs or-dered documents to learn a bigram LM.
A naturalquestion that arises in our problem is whether or nota bigram LM can be recovered from the BOW cor-pus with any guarantee.
Let X denote the spaceof all possible BOWs.
As a toy example, considerW = 3 with the vocabulary {?d?, A, B}.
Assumingall documents have equal length |x| = 4 (including?d?
), then X = {(?d?
:1, A:3, B:0), (?d?
:1, A:2, B:1),(?d?
:1, A:1, B:2), (?d?
:1, A:0, B:3)}.
Our trainingBOW corpus, when sufficiently large, provides themarginal distribution p?
(x) for x ?
X .
Can we re-cover a bigram LM from p?
(x)?To answer this question, we first need to introducea generative model for the BOWs.
We assume thatthe BOW corpus is generated from a bigram LM ?in two steps: (i) An ordered document is generatedfrom the bigram LM ?
; (ii) The document?s unigramcounts are collected to produce the BOW x. There-fore, the probability of a BOW x being generatedby ?
can be computed by marginalizing over uniqueorderings z of x:P (x|?)
=?z??
(x)P (z|?)
=?z??
(x)|x|?j=2?zj?1,zj ,where ?
(x) is the set of unique orderings, and |x| isthe document length.
For example, if x =(?d?
:1,A:2, B:1) then ?
(x) = {z1, z2, z3} with z1 =??d?
A A B?, z2 = ??d?
A B A?, z3 = ??d?
B A A?.Bigram LM recovery then amounts to finding a ?that satisfies the system of marginal-matching equa-tionsP (x|?)
= p?
(x) , ?x ?
X .
(1)As a concrete example where one can exactly re-cover a bigram LM from BOWs, consider our toyexample again.
We know there are only three freevariables in our 3?3 bigram LM ?
: r = ?
?d?A, p =?AA, q = ?BB , since the rest are determined bynormalization.
Suppose the documents are gener-ated from a bigram LM with true parameters r =0.25, p = 0.9, q = 0.5.
If our BOW corpus is verylarge, we will observe that 20.25% of the BOWs are(?d?
:1, A:3, B:0), 37.25% are (?d?
:1, A:2, B:1), and18.75% are (?d?
:1, A:0, B:3).
These numbers arecomputed using the definition of P (x|?).
We solvethe reverse problem of finding r, p, q from the sys-tem of equations (1), now explicitly written as??????
?rp2 = 0.2025rp(1?
p) + r(1?
p)(1?
q)+(1?
r)(1?
q)p = 0.3725(1?
r)q2 = 0.1875.The above system has only one valid solution,which is the correct set of bigram LM parameters(r, p, q) = (0.25, 0.9, 0.5).However, if the true parameters were (r, p, q) =(0.1, 0.2, 0.3) with proportions of BOWs being0.4%, 19.8%, 8.1%, respectively, it is easy to ver-ify that the system would have multiple valid solu-tions: (0.1, 0.2, 0.3), (0.8819, 0.0673, 0.8283), and(0.1180, 0.1841, 0.3030).
In general, if p?
(x) isknown from the training BOW corpus, when canwe guarantee to uniquely recover the bigram LM??
This is the question of identifiability, whichmeans the transition matrix ?
satisfying (1) existsand is unique.
Identifiability is related to findingunique solutions of a system of polynomial equa-tions since (1) is such a system in the elements of ?.The details are beyond the scope of this paper, butapplying the technique in (Basu and Boston, 2000),it is possible to show that for W = 3 (including ?d?
)we need longer documents (|x| ?
5) to ensure iden-tifiability.
The identifiability of more general casesis still an open research question.6573 Bigram Recovery AlgorithmIn practice, the documents are not truly generatedfrom a bigram LM, and the BOW corpus may besmall.
We therefore seek a maximum likelihood es-timate of ?
or a regularized version of it.
Equiva-lently, we no longer require equality in (1), but in-stead find ?
that makes the distribution P (x|?)
asclose to p?
(x) as possible.
We formalize this notionbelow.3.1 The Objective FunctionGiven a BOW corpus {x1, .
.
.
,xn}, its nor-malized log likelihood under ?
is ?(?)
?1C?ni=1 logP (xi|?
), where C =?ni=1(|xi| ?
1)is the corpus length excluding ?d??s.
The idea is tofind ?
that maximizes ?(?).
This also brings P (x|?
)closest to p?
(x) in the KL-divergence sense.
How-ever, to prevent overfitting, we regularize the prob-lem so that ?
prefers to be close to a ?prior?
bi-gram LM ?.
The prior ?
is also estimated from theBOW corpus, and is discussed in Section 3.4.
Wedefine the regularizer to be an asymmetric dissimi-larity D(?,?)
between the prior ?
and the learnedmodel ?.
The dissimilarity is 0 if ?
= ?, andincreases as they diverge.
Specifically, the KL-divergence between two word distributions condi-tioned on the same history u is KL(?u???u?)
=?Wv=1 ?uv log ?uv?uv .
We define D(?,?)
to bethe average KL-divergence over all histories:D(?,?)
?
1W?Wu=1 KL(?u???u?
), which is con-vex in ?
(Cover and Thomas, 1991).
We will usethe following derivative later: ?D(?,?)/?
?uv =?
?uv/(W?uv).We are now ready to define the regularized op-timization problem for recovering a bigram LM ?from the BOW corpus:max??(?)?
?D(?,?
)subject to ?1 = 1, ?
?
0.
(2)The weight ?
controls the strength of the prior.
Theconstraints ensure that ?
is a valid bigram matrix,where 1 is an all-one vector, and the non-negativityconstraint is element-wise.
Equivalently, (2) can beviewed as themaximum a posteriori (MAP) estimateof ?, with independent Dirichlet priors for each rowof ?
: p(?u?)
= Dir(?u?|?u?)
and hyperparameters?uv = ?CW ?uv + 1.The summation over hidden ordered documentsz in P (x|?)
couples the variables and makes (2) anon-concave problem.
We optimize ?
using an EMalgorithm.3.2 The EM AlgorithmWe derive the EM algorithm for the optimizationproblem (2).
Let O(?)
?
?(?)
?
?D(?,?)
be theobjective function.
Let ?
(t?1) be the bigram LM atiteration t?
1.
We can lower-bound O as follows:O(?
)= 1Cn?i=1log?z??
(xi)P (z|?
(t?1),x) P (z|?
)P (z|?(t?1),x)??D(?,?)?
1Cn?i=1?z??
(xi)P (z|?
(t?1),x) log P (z|?
)P (z|?(t?1),x)??D(?,?)?
L(?,?
(t?1)).We used Jensen?s inequality above since log()is concave.
The lower bound L involvesP (z|?
(t?1),x), the probability of hidden orderingsof the BOW under the previous iteration?s model.In the E-step of EM we compute P (z|?
(t?1),x),which will be discussed in Section 3.3.
Onecan verify that L(?,?
(t?1)) is concave in ?, un-like the original objective O(?).
In addition, thelower bound ?touches?
the objective at ?
(t?1), i.e.,L(?(t?1),?
(t?1)) = O(?
(t?1)).The EM algorithm iteratively maximizes thelower bound, which is now a concave optimizationproblem: max?
L(?,?
(t?1)), subject to ?1 = 1.The non-negativity constraints turn out to be auto-matically satisfied.
Introducing Lagrange multipli-ers ?u for each history u = 1 .
.
.W , we form theLagrangian ?:?
?
L(?,?
(t?1))?W?u=1?u( W?v=1?uv ?
1).Taking the partial derivative with respect to ?uv andsetting it to zero: ??/?
?uv = 0, we arrive at thefollowing update:?uv ?n?i=1?z??
(xi)P (z|?
(t?1),x)cuv(z) +?CW ?uv.
(3)658Input: BOW documents {x1, .
.
.
,xn}, a prior bi-gram LM ?, weight ?.1.
t = 1.
Initialize ?
(0) = ?.2.
Repeat until the objective O(?)
converges:(a) (E-step) Compute P (z|?
(t?1),x) for z ??
(xi), i = 1, .
.
.
, n.(b) (M-step) Compute ?
(t) using (3).
Let t =t + 1.Output: The recovered bigram LM ?.Table 1: The EM algorithmThe normalization is over v = 1 .
.
.W .
We usecuv(z) to denote the number of times the bigram?uv?
appears in the ordered document z.
This is theM-step of EM.
Intuitively, the first term counts howoften the bigram ?uv?
occurs, weighing each order-ing by its probability under the previous model; thesecond term pulls the parameter towards the prior.If the weight of the prior ?
?
?, we would have?uv = ?uv.
The update is related to the MAP esti-mate for a multinomial distribution with a Dirichletprior, where we use the expected counts.We initialize the EM algorithm with ?
(0) = ?.The EM algorithm is summarized in Table 1.3.3 Approximate E-stepThe E-step needs to compute the expected bigramcounts of the form?z??
(x)P (z|?,x)cuv(z).
(4)However, this poses a computational problem.
Thesummation is over unique ordered documents.
Thenumber of unique ordered documents can be on theorder of |x|!, i.e., all permutations of the BOW.
For ashort document of length 15, this number is already1012.
Clearly, brute-force enumeration is only fea-sible for very short documents.
Approximation isnecessary to handle longer ones.A simple Monte Carlo approximation to (4)would involve sampling ordered documentsz1, z2, .
.
.
, zL according to zi ?
P (z|?,x), andreplacing (4) with ?Li=1 cuv(zi)/L.
This estimateis unbiased, and the variance decreases linearlywith the number of samples, L. However, samplingdirectly from P is difficult.Instead, we sample ordered documents zi ?R(zi|?,x) from a distribution R which is easyto generate, and construct an approximation us-ing importance sampling (see, e.g., (Liu, 2001)).With each sample, zi, we associate a weightwi ?
P (zi|?,x)/R(zi|?,x).
The importancesampling approximation to (4) is then given by(?Li=1 wicuv(zi))/(?Li=1 wi).
Re-weighting thesamples in this fashion accounts for the fact that weare using a sampling distribution R which is differ-ent the target distribution P , and guarantees that ourapproximation is asymptotically unbiased.The quality of an importance sampling approxi-mation is closely related to how closelyR resemblesP ; the more similar they are, the better the approxi-mation, in general.
Given a BOW x and our currentbigram model estimate, ?, we generate one sample(an ordered document zi) by sequentially drawingwords from the bag, with probabilities proportionalto ?, but properly normalized to form a distributionbased on which words remain in the bag.
For exam-ple, suppose x = (?d?
:1, A:2, B:1, C:1).
Then weset zi1 = ?d?, and sample zi2 = A with probabil-ity 2??d?A/(2?
?d?A + ?
?d?B + ??d?C).
Similarly,if zi(j?1) = u and if v is in the original BOW thathasn?t been sampled yet, then we set the next word inthe ordered document zij equal to v with probabilityproportional to cv?uv, where cv is the count of v inthe remaining BOW.
For this scheme, one can ver-ify (Rabbat et al, 2007) that the importance weightcorresponding to a sampled ordered document zi =(zi1, .
.
.
, zi|x|) is given by wi =?|x|t=2?|x|i=t ?zt?1zi .In our implementation, the number of importancesamples used for a document x is 10|x|2 if the lengthof the document |x| > 8; otherwise we enumerate?
(x) without importance sampling.3.4 Prior Bigram LM ?The quality of the EM solution ?
can depend on theprior bigram LM ?.
To assess bigram recoverabil-ity from a BOW corpus alone, we consider only pri-ors estimated from the corpus itself3.
Like ?, ?
is aW?W transition matrix with ?uv = P (v|u).
When3Priors based on general English text or domain-specificknowledge could be used in specific applications.659appropriate, we set the initial probability ?
?d?v pro-portional to the number of times word v appears inthe BOW corpus.
We consider three prior models:Prior 1: Unigram ?unigram.
The most na??ve?
is a unigram LM which ignores word history.The probability for word v is estimated from theBOW corpus frequency of v, with add-1 smoothing:?unigramuv ?
1 +?ni=1 xiv.
We should point outthat the unigram prior is an asymmetric bigram, i.e.,?unigramuv 6= ?unigramvu .Prior 2: Frequency of Document Co-occurrence (FDC) ?fdc.
Let ?
(u, v|x) = 1 ifwords u 6= v co-occur (regardless of their counts)in BOW x, and 0 otherwise.
In the case u = v,?
(u, u|x) = 1 only if u appears at least twice inx.
Let cfdcuv =?ni=1 ?
(u, v|xi) be the number ofBOWs in which u, v co-occur.
The FDC prior is?fdcuv ?
cfdcuv + 1.
The co-occurrence counts cfdcare symmetric, but ?fdc is asymmetric becauseof normalization.
FDC captures some notion ofpotential transitions from u to v. FDC is in spiritsimilar to Kneser-Ney smoothing (Kneser and Ney,1995) and other methods that accumulate indicatorsof document membership.Prior 3: Permutation-Based (Perm) ?perm.
Re-call that cuv(z) is the number of times the bigram?uv?
appears in an ordered document z.
We definecpermuv =?ni=1 Ez??
(xi)[cuv(z)], where the expecta-tion is with respect to all unique orderings of eachBOW.
We make the zero-knowledge assumption ofuniform probability over these orderings, rather thanP (z|?)
as in the EM algorithm described above.
EMwill refine these estimates, though, so this is a natu-ral starting point.
Space precludes a full discussion,but it can be proven that cpermuv =?ni=1 xiuxiv/|xi|if u 6= v, and cpermuu =?ni=1 xiu(xiu ?
1)/|xi|.
Fi-nally, ?permuv ?
cpermuv + 1.3.5 Decoding Ordered Documents from BOWsGiven a BOW x and a bigram LM ?, we for-mulate document recovery as the problem z?
=argmaxz??
(x)P (z|?).
In fact, we can generatethe top N candidate ordered documents in termsof P (z|?).
We use A?
search to construct suchan N-best list (Russell and Norvig, 2003).
Eachstate is an ordered, partial document.
Its succes-sor states append one more unused word in x tothe partial document.
The actual cost g from thestart (empty document) to a state is the log proba-bility of the partial document under bigram ?.
Wedesign a heuristic cost h from the state to the goal(complete document) that is admissible: the idea isto over-use the best bigram history for the remain-ing words in x.
Let the partial document end withword we.
Let the count vector for the remainingBOW be (c1, .
.
.
, cW ).
One admissible heuristicis h = log?Wu=1 P (u|bh(u);?
)cu , where the ?besthistory?
for word type u is bh(u) = argmaxv?vu,and v ranges over the word types with non-zerocounts in (c1, .
.
.
, cW ), plus we.
It is easy to see thath is an upper bound on the bigram log probabilitythat the remaining words in x can achieve.We use a memory-bounded A?
search similarto (Russell, 1992), because long BOWs would oth-erwise quickly exhaust memory.
When the priorityqueue grows larger than the bound, the worst states(in terms of g + h) in the queue are purged.
Thisnecessitates a double-ended priority queue that canpop either the maximum or minimum item.
We usean efficient implementation with Splay trees (Chongand Sahni, 2000).
We continue running A?
afterpopping the goal state from its priority queue.
Re-peating this N times gives the N-best list.4 ExperimentsWe show experimentally that the proposed algo-rithm is indeed able to recover reasonable bigramLMs from BOW corpora.
We observe:1.
Good test set perplexity: Using test (held-out) set perplexity (PP) as an objective measure ofLM quality, we demonstrate that our recovered bi-gram LMs are much better than na?
?ve unigram LMstrained on the same BOW corpus.
Furthermore, theyare not far behind the ?oracle?
bigram LMs trainedon ordered documents that correspond to the BOWs.2.
Sensible bigram pairs: We inspect the recov-ered bigram LMs and find that they assign higherprobabilities to sensible bigram pairs (e.g., ?i mean?,?oh boy?, ?that?s funny?
), and lower probabilities tononsense pairs (e.g., ?i yep?, ?you let?s?, ?right lot?).3.
Document recovery from BOW: With the bi-gram LMs, we show improved accuracy in recover-ing ordered documents from BOWs.We describe these experiments in detail below.660Corpus |V | # Docs # Tokens |x|SV10 10 6775 7792 1.2SV25 25 9778 13324 1.4SV50 50 12442 20914 1.7SV100 100 14602 28611 2.0SV250 250 18933 51950 2.7SV500 500 23669 89413 3.8SumTime 882 3341 68815 20.6Table 2: Corpora statistics: vocabulary size, documentcount, total token count, and mean document length.4.1 Corpora and ProtocolsWe note that although in principle our algorithmworks on large corpora, the current implementa-tion does not scale well (Table 3 last column).
Wetherefore experimented on seven corpora with rel-atively small vocabulary sizes, and with short doc-uments (mostly one sentence per document).
Ta-ble 2 lists statistics describing the corpora.
The firstsix contain text transcripts of conversational tele-phone speech from the small vocabulary ?SVitch-board 1?
data set.
King et al constructed each cor-pus from the full Switchboard corpus, with the re-striction that the sentences use only words in the cor-responding vocabulary (King et al, 2005).
We re-fer to these corpora as SV10, SV25, SV50, SV100,SV250, and SV500.
The seventh corpus comes fromthe SumTime-Meteo data set (Sripada et al, 2003),which contains real weather forecasts for offshoreoil rigs in the North Sea.
For the SumTime cor-pus, we performed sentence segmentation to pro-duce documents, removed punctuation, and replacednumeric digits with a special token.For each of the seven corpora, we perform 5-foldcross validation.
We use four folds other than thek-th fold as the training set to train (recover) bigramLMs, and the k-th fold as the test set for evaluation.This is repeated for k = 1 .
.
.
5, and we report theaverage cross validation results.
We distinguish theoriginal ordered documents (training set z1, .
.
.
zn,test set zn+1, .
.
.
, zm) and the corresponding BOWs(training set x1 .
.
.xn, test set xn+1 .
.
.xm).
In allexperiments, we simply set the weight ?
= 1 in (2).Given a training set and a test set, we perform thefollowing steps:1.
Build prior LMs ?X from the training BOWcorpus x1, .
.
.xn, for X = unigram, fdc, perm.2.
Recover the bigram LMs ?X with the EM al-gorithm in Table 1, from the training BOW corpusx1, .
.
.xn and using the prior from step 1.3.
Compute the MAP bigram LM from the or-dered training documents z1, .
.
.
zn.
We call this the?oracle?
bigram LM because it uses order informa-tion (not available to our algorithm), and we use itas a lower-bound on perplexity.4.
Test all LMs on zn+1, .
.
.
, zm by perplexity.4.2 Good Test Set PerplexityTable 3 reports the 5-fold cross validation mean-test-set-PP values for all corpora, and the run time perEM iteration.
Because of the long running time, weadopt the rule-of-thumb stopping criterion of ?twoEM iterations?.
First, we observe that all bigramLMs perform better than unigram LMs ?unigrameven though they are trained on the same BOW cor-pus.
Second, all recovered bigram LMs ?X im-proved upon their corresponding baselines ?X .
Thedifference across every row is statistically significantaccording to a two-tailed paired t-test with p < 0.05.The differences among PP(?X ) for the same corpusare also significant (except between ?unigram and?perm for SV500).
Finally, we observe that ?permtends to be best for the smaller vocabulary corpora,whereas ?fdc dominates as the vocabulary grows.To see how much better we could do if we had or-dered training documents z1, .
.
.
, zn, we present themean-test-set-PP of ?oracle?
bigram LMs in Table 4.We used three smoothing methods to obtain oracleLMs: absolute discounting using a constant of 0.5(we experimented with other values, but 0.5 workedbest), Good-Turing, and interpolated Witten-Bell asimplemented in the SRILM toolkit (Stolcke, 2002).We see that our recovered LMs (trained on un-ordered BOW documents), especially for small vo-cabulary corpora, are close to the oracles (trained onordered documents).
For the larger datasets, the re-covery task is more difficult, and the gap betweenthe oracle LMs and the ?
LMs widens.
Note that theoracle LMs do much better than the recovered LMson the SumTime corpus; we suspect the difference isdue to the larger vocabulary and significantly higheraverage sentence length (see Table 2).4.3 Sensible Bigram PairsThe next set of experiments compares the recov-ered bigram LMs to their corresponding prior LMs661Corpus X PP(?X ) PP(?X ) Time/IterSV10unigram 7.48 6.95 < 1sfdc 6.52 6.47 < 1sperm 6.50 6.45 < 1sSV25unigram 16.4 12.8 0.1sfdc 12.3 11.8 0.1sperm 12.2 11.7 0.1sSV50unigram 29.1 19.7 2sfdc 19.6 17.8 4sperm 19.5 17.7 5sSV100unigram 45.4 27.8 7sfdc 29.5 25.3 11sperm 30.0 25.6 11sSV250unigram 91.8 51.2 5mfdc 60.0 47.3 8mperm 65.4 49.7 8mSV500unigram 149.1 87.2 3hfdc 104.8 80.1 3hperm 123.9 87.4 3hSumTimeunigram 129.7 81.8 4hfdc 103.2 77.7 4hperm 187.9 85.4 3hTable 3: Mean test set perplexities of prior LMs and bi-gram LMs recovered after 2 EM iterations.in terms of how they assign probabilities to wordpairs.
One naturally expects probabilities for fre-quently occurring bigrams to increase, while rareor nonsensical bigrams?
probabilities should de-crease.
For a prior-bigram pair (?, ?
), we evaluatethe change in probabilities by computing the ratio?hw = P (w|h,?
)P (w|h,?)
=?hw?hw .
For a given history h, wesort words w by this ratio rather than by actual bi-gram probability because the bigrams with the high-est and lowest probabilities tend to stay the same,while the changes accounting for differences in PPscores are more noticeable by considering the ratio.Due to space limitation, we present one specificresult (FDC prior, fold 1) for the SV500 corpus inTable 5.
Other results are similar.
The table listsa few most frequent unigrams as history words h(left), and the words w with the smallest (center)and largest (right) ?hw ratio.
Overall we see that ourEM algorithm is forcing meaningless bigrams (e.g.,?i goodness?, ?oh thing?)
to have lower probabil-ities, while assigning higher probabilities to sensi-ble bigram pairs (e.g., ?really good?, ?that?s funny?
).Note that the reverse of some common expressions(e.g., ?right that?s?)
also rise in probability, suggest-ing the algorithm detects that the two words are of-Corpus AbsoluteDiscountGood-TuringWitten-Bell?
?SV10 6.27 6.28 6.27 6.45SV25 10.5 10.6 10.5 11.7SV50 14.8 14.9 14.8 17.7SV100 20.0 20.1 20.0 25.3SV250 33.7 33.7 33.8 47.3SV500 50.9 50.9 51.3 80.1SumTime 10.8 10.5 10.6 77.7Table 4: Mean test set perplexities for oracle bigram LMstrained on z1, .
.
.
, zn and tested on zn+1, .
.
.
, zm.
Forreference, the rightmost column lists the best result usinga recovered bigram LM (?perm for the first three corpora,?fdc for the latter four).ten adjacent, but lacks sufficient information to naildown the exact order.4.4 Document Recovery from BOWWe now play the role of the malicious party men-tioned in the introduction.
We show that, com-pared to their corresponding prior LMs, our recov-ered bigram LMs are better able to reconstruct or-dered documents out of test BOWs xn+1, .
.
.
,xm.We perform document recovery using 1-best A?
de-coding.
We use ?document accuracy?
and ?n-gramaccuracy?
(for n = 2, 3) as our evaluation criteria.We define document accuracy (Accdoc) as the frac-tion of documents4 for which the decoded documentmatches the true ordered document exactly.
Simi-larly, n-gram accuracy (Accn) measures the fractionof all n-grams in test documents (with n or morewords) that are recovered correctly.For this evaluation, we compare models built forthe SV500 corpus.
Table 6 presents 5-fold cross val-idation average test-set accuracies.
For each accu-racy measure, we compare the prior LM with therecovered bigram LM.
It is interesting to note thatthe FDC and Perm priors reconstruct documents sur-prisingly well, but we can always improve them byrunning our EM algorithm.
The accuracies obtainedby ?
are statistically significantly better (via two-tailed paired t-tests with p < 0.05) than their cor-responding priors ?
in all cases except Accdoc for?perm versus ?perm.
Furthermore, ?fdc and ?permare significantly better than all other models in termsof all three reconstruction accuracy measures.4We omit single-word documents from these computations.662h w (smallest ?hw) w (largest ?hw)i yep, bye-bye, ah, goodness, ahead mean, guess, think, bet, agreeyou let?s, us, fact, such, deal thank, bet, know, can, doright as, lot, going, years, were that?s, all, right, now, you?reoh thing, here, could, were, doing boy, really, absolutely, gosh, greatthat?s talking, home, haven?t, than, care funny, wonderful, true, interesting, amazingreally now, more, yep, work, you?re sad, neat, not, good, it?sTable 5: The recovered bigram LM ?fdc decreases nonsense bigram probabilities (center column) and increasessensible ones (right column) compared to the prior ?fdc on the SV500 corpus.
?perm reconstructions of test BOWs ?perm reconstructions of test BOWsjust it?s it?s it?s just going it?s just it?s just it?s goingit?s probably out there else something it?s probably something else out therethe the have but it doesn?t but it doesn?t have the theyou to talking nice was it yes yes it was nice talking to youthat?s well that?s what i?m saying well that?s that?s what i?m sayinga little more here home take a little more take home hereand they can very be nice too and they can be very nice tooi think well that?s great i?m well i think that?s great i?mbut was he because only always but only because he was alwaysthat?s think i don?t i no no i don?t i think that?sthat in and it it?s interesting and it it?s interesting that inthat?s right that?s right that?s difficult right that?s that?s right that?s difficultso just not quite a year so just not a quite yearwell it is a big dog well it is big a dogso do you have a car so you do have a carTable 7: Subset of SV500 documents that only ?perm or ?perm (but not both) reconstructs correctly.
The correctreconstructions are in bold.Accdoc Acc2 Acc3X ?X ?X ?X ?X ?X ?Xunigram 11.1 26.8 17.7 32.8 2.7 11.8fdc 30.2 31.0 33.0 35.1 11.4 13.3perm 30.9 31.5 32.7 34.8 11.5 13.1Table 6: Percentage of correctly reconstructed docu-ments, 2-grams and 3-grams from test BOWs in SV500,5-fold cross validation.
The same trends continue for 4-grams and 5-grams (not shown).We conclude our experiments with a closer lookat some BOWs for which ?
and ?
reconstruct dif-ferently.
As a representative example, we compare?perm to ?perm on one test set of the SV500 cor-pus.
There are 92 documents that are correctly re-constructed by ?perm but not by ?perm.
In con-trast, only 65 documents are accurately reordered by?perm but not by ?perm.
Table 7 presents a subsetof these documents with six or more words.
Over-all, we conclude that the recovered bigram LMs doa better job at reconstructing BOW documents.5 Conclusions and Future WorkWe presented an algorithm that learns bigram lan-guage models from BOWs.
We plan to: i) inves-tigate ways to speed up our algorithm; ii) extendit to trigram and higher-order models; iii) handlethe mixture of BOW documents and some ordereddocuments (or phrases) when available; iv) adapt ageneral English LM to a special domain using onlyBOWs from that domain; and v) explore novel ap-plications of our algorithm.AcknowledgmentsWe thank Ben Liblit for tips on doubled-endedpriority queues, and the anonymous reviewers forvaluable comments.
This work is supported inpart by the Wisconsin Alumni Research Founda-tion, NSF CCF-0353079 and CCF-0728767, and theNatural Sciences and Engineering Research Council(NSERC) of Canada.663ReferencesSamit Basu and Nigel Boston.
2000.
Identifiability ofpolynomial systems.
Technical report, University ofIllinois at Urbana-Champaign.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large language modelsin machine translation.
In Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL).Stanley F. Chen and Joshua T. Goodman.
1999.
Anempirical study of smoothing techniques for lan-guage modeling.
Computer Speech and Language,13(4):359?393.Kyun-Rak Chong and Sartaj Sahni.
2000.Correspondence-based data structures for double-ended priority queues.
The ACM Journal ofExperimental Algorithmics, 5(2).Thomas M. Cover and Joy A. Thomas.
1991.
Elementsof Information Theory.
John Wiley & Sons, Inc.Simon King, Chris Bartels, and Jeff Bilmes.
2005.SVitchboard 1: Small vocabulary tasks from Switch-board 1.
In Interspeech 2005, Lisbon, Portugal.Reinhard Kneser and Hermann Ney.
1995.
Im-proved backing-off forM-gram language modeling.
InICASSP.Jun S. Liu.
2001.
Monte Carlo Strategies in ScientificComputing.
Springer.Michael Rabbat, Ma?rio Figueiredo, and Robert Nowak.2007.
Inferring network structure from co-occurrences.
In Advances in Neural Information Pro-cessing Systems (NIPS) 20.Brian Roark, Murat Saraclar, and Michael Collins.
2007.Discriminative n-gram language modeling.
ComputerSpeech and Language, 21(2):373?392.Ronald Rosenfeld.
2000.
Two decades of statistical lan-guage modeling: Where do we go from here?
Pro-ceedings of the IEEE, 88(8).Stuart Russell and Peter Norvig.
2003.
Artificial Intel-ligence: A Modern Approach.
Prentice-Hall, Engle-wood Cliffs, NJ, second edition.Stuart Russell.
1992.
Efficient memory-bounded searchmethods.
In The 10th European Conference on Artifi-cial Intelligence.Somayajulu G. Sripada, Ehud Reiter, Jim Hunter, and JinYu.
2003.
Exploiting a parallel TEXT-DATA corpus.In Proceedings of Corpus Linguistics, pages 734?743,Lancaster, U.K.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of Interna-tional Conference on Spoken Language Processing,Denver, Colorado.664
