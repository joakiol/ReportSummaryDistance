Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1220?1229,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsCrowdsourcing Translation: Professional Quality from Non-ProfessionalsOmar F. Zaidan and Chris Callison-BurchDept.
of Computer Science, Johns Hopkins UniversityBaltimore, MD 21218, USA{ozaidan,ccb}@cs.jhu.eduAbstractNaively collecting translations by crowd-sourcing the task to non-professional trans-lators yields disfluent, low-quality results ifno quality control is exercised.
We demon-strate a variety of mechanisms that increasethe translation quality to near professional lev-els.
Specifically, we solicit redundant transla-tions and edits to them, and automatically se-lect the best output among them.
We propose aset of features that model both the translationsand the translators, such as country of resi-dence, LM perplexity of the translation, editrate from the other translations, and (option-ally) calibration against professional transla-tors.
Using these features to score the col-lected translations, we are able to discriminatebetween acceptable and unacceptable transla-tions.
We recreate the NIST 2009 Urdu-to-English evaluation set with Mechanical Turk,and quantitatively show that our models areable to select translations within the range ofquality that we expect from professional trans-lators.
The total cost is more than an order ofmagnitude lower than professional translation.1 IntroductionIn natural language processing research, translationsare most often used in statistical machine translation(SMT), where systems are trained using bilingualsentence-aligned parallel corpora.
SMT owes its ex-istence to data like the Canadian Hansards (which bylaw must be published in both French and English).SMT can be applied to any language pair for whichthere is sufficient data, and it has been shown to pro-duce state-of-the-art results for language pairs likeArabic?English, where there is ample data.
How-ever, large bilingual parallel corpora exist for rela-tively few languages pairs.There are various options for creating new train-ing resources for new language pairs.
These includeharvesting the web for translations or comparablecorpora (Resnik and Smith, 2003; Munteanu andMarcu, 2005; Smith et al, 2010; Uszkoreit et al,2010), improving SMT models so that they are bet-ter suited to the low resource setting (Al-Onaizanet al, 2002; Probst et al, 2002; Oard et al, 2003;Niessen and Ney, 2004), or designing models thatare capable of learning translations from monolin-gual corpora (Rapp, 1995; Fung and Yee, 1998;Schafer and Yarowsky, 2002; Haghighi et al, 2008).Relatively little consideration is given to the idea ofsimply hiring translators to create parallel data, be-cause it would seem to be prohibitively expensive.For example, Germann (2001) estimated the costof hiring professional translators to create a Tamil-English corpus at $0.36/word.
At that rate, translat-ing enough data to build even a small parallel corpuslike the LDC?s 1.5 million word Urdu?English cor-pus would exceed half a million dollars.In this paper we examine the idea of creating lowcost translations via crowdscouring.
We use Ama-zon?s Mechanical Turk to hire a large group of non-professional translators, and have them recreate anUrdu?English evaluation set at a fraction of the costof professional translators.
The original dataset already has professionally-produced reference trans-lations, which allows us to objectively and quantita-tively compare the quality of professional and non-professional translations.
Although many of the in-dividual non-expert translators produce low-quality,disfluent translations, we show that it is possible to1220Signs of human livings have been found in many cavesin   Attapure.
In 1994, the remains of pre-historic man,which are believed to be 800,000 years old werediscovered and they were named `Home Antecessor'meaning `The Founding Man'.
Prior to that 6 lac yearsold humans, named as   Homogenisens in scientificterms,were believed to be the   oldest dwellers of thisarea.
Archaeological experts say that evidence is foundthat proves that the inhabitants of this area usedmolded tools.
The ground where these digs took placehas been claimed to be the oldest known Europeandiscovery of civilization, as announced by the FrenchNews Agency.!
"#$"% &' ()*"+*, &-,./%, 0#1 234 5, 0#1 199467"89: ;2< &="> &*"1 &*,?
@ A"B C'D 8 E"FG8?H= )>?I"+*, &*"%?
&JK8 ?+#B &LJ8, )1)< 0#MJ> 0#NO &'P"#O "8: Q"* "'&+J-"B 0#MJ> I"+*, 2*,?
@ C'D 6 RG$ 2B 5,5, ;2< "="> "M' S+J#>?GTU#< )1)< 0#1 VW3X,P2Y= 2="> 2*"1 &Z-"<9 [8?= \8.$ 2' 2342+8, 0#M*, ]' 2< "JM' "' [8?<"1 2' ]^8.$ _9"`a2' 234 5, ]' 2< "/bc ]/@ 2B [> 0#< 2b1 .<,)dP2Y= 2=?'
A"^K/B, &Y% 9,ef, 2-)< 2#' &-Wgh i)TSigns of human life of ancient people have beendiscovered in several caves of Atapuerca.
In 1994,several homo antecessor fossils i.e.
pioneer humanwere uncovered in this region, which are supposed tobe 800,000 years old.
Previously, 600,000 years oldancestors, called homo hudlabar [sic] in scientificterm, were supposed to be the most ancientinhabitants of the region.Archeologists are of the viewthat they have gathered evidence that the people ofthis region had also been using fabricated tools.On the basis of the level at which this excavation wascarried out, the French news agency [AFP] has termedit the oldest European discovery.Urdu source Professional LDC Translation Non-Professional Mechanical Turk TranslationFigure 1: A comparison of professional translations provided by the LDC to non-professional translations created onMechanical Turk.get high quality translations in aggregate by solicit-ing multiple translations, redundantly editing them,and then selecting the best of the bunch.To select the best translation, we use a machine-learning-inspired approach that assigns a score toeach translation we collect.
The scores discrimi-nate acceptable translations from those that are not(and competent translators from those who are not).The scoring is based on a set of informative, intu-itive, and easy-to-compute features.
These includecountry of residence, number of years speaking En-glish, LM perplexity of the translation, edit rate fromthe other translations, and (optionally) calibrationagainst professional translators, with the weights setusing a small set of gold standard data from profes-sional translators.2 Crowdsourcing Translation toNon-ProfessionalsTo collect crowdsourced translations, we use Ama-zon?s Mechanical Turk (MTurk), an online market-place designed to pay people small sums of moneyto complete Human Intelligence Tasks (or HITs) ?tasks that are difficult for computers but easy forpeople.
Example HITs range from labeling imagesto moderating blog comments to providing feedbackon relevance of results for search queries.
Anyonewith an Amazon account can either submit HITs orwork on HITs that were submitted by others.
Work-ers are referred to as ?Turkers?, and designers ofHITs as ?Requesters.?
A Requester specifies the re-ward to be paid for each completed item, sometimesas low as $0.01.
Turkers are free to select whicheverHITs interest them, and to bypass HITs they find un-interesting or which they deem pay too little.The advantages of Mechanical Turk include:?
zero overhead for hiring workers?
a large, low-cost labor force?
easy micropayment system?
short turnaround time, as tasks get completedin parallel by many individuals?
access to foreign markets with native speakersof many rare languagesOne downside is that Amazon does not provideany personal information about Turkers.
(EachTurker is identifiable only through an anonymousID like A23KO2TP7I4KK2.)
In particular, no in-formation is available about a worker?s educationalbackground, skills, or even native language(s).
Thismakes it difficult to determine if a Turker is qualifiedto complete a translation task.Therefore, soliciting translations from anony-mous non-professionals carries a significant risk ofpoor translation quality.
Whereas hiring a profes-sional translator ensures a degree of quality andcare, it is not very difficult to find bad translationsprovided by Turkers.
One Urdu headline, profes-sionally translated as Barack Obama: America WillAdopt a New Iran Strategy, was rendered disfluentlyby a Turker as Barak Obam will do a new policywith Iran.
Another translated it with snarky sar-casm: Barak Obama and America weave new evilstrategies against Iran.
Figure 1 gives more typicaltranslation examples.
The translations often reflectnon-native English, but are generally done conscien-tiously (in spite of the relatively small payment).To improve the accuracy of noisy labels from non-experts, most existing quality control mechanisms1221employ some form of voting, assuming a discreteset of possible labels.
This is not the case for trans-lations, where the ?labels?
are full sentences.
Whendealing with such a structured output, the space ofpossible outputs is diverse and complex.
We there-fore need a different approach for quality control.That is precisely the focus of this work: to propose,and evaluate, such quality control mechanisms.In the next section, we discuss reproducing theUrdu-to-English 2009 NIST evaluation set.
We thendescribe a principled approach to discriminate goodtranslations from bad ones, given a set of redundanttranslations for the same source sentence.3 Datasets3.1 The Urdu-to-English 2009 NISTEvaluation SetWe translated the Urdu side of the Urdu?English testset of the 2009 NIST MT Evaluation Workshop.
Theset consists of 1,792 Urdu sentences from a vari-ety of news and online sources.
The set includesfour different reference translations for each sourcesentence, produced by professional translation agen-cies.
NIST contracted the LDC to oversee the trans-lation process and perform quality control.This particular dataset, with its multiple referencetranslations, is very useful because we can measurethe quality range for professional translators, whichgives us an idea of whether or not the crowdsourcedtranslations approach the quality of a professionaltranslator.3.2 Translation HIT designWe solicited English translations for the Urdu sen-tences in the NIST dataset.
Amazon has enabledpayments in rupees, which has attracted a large de-mographic of workers from India (Ipeirotis, 2010).Although it does not yet have s direct payment inPakistan?s local currency, we found that a large con-tingent of our workers are located in Pakistan.Our HIT involved showing the worker a sequenceof Urdu sentences, and asking them to provide anEnglish translation for each one.
The screen alsoincluded a brief set of instructions, and a short ques-tionnaire section.
The reward was set at $0.10 pertranslation, or roughly $0.005 per word.In our first collection effort, we solicited only onetranslation per Urdu sentence.
After confirming thatthe task is feasible due to the large pool of work-ers willing and able to provide translations, we car-ried out a second collection effort, this time solicit-ing three translations per Urdu sentence (from threedistinct translators).
The interface was also slightlymodified, in the following ways:?
Instead of asking Turkers to translate a full doc-ument (as in our first pass), we instead split thedata set into groups of 10 sentences per HIT.?
We converted the Urdu sentences into imagesso that Turkers could not cheat by copying-and-pasting the Urdu text into an MT system.?
We collected information about each worker?sgeographic location, using a JavaScript plugin.The translations from the first pass were of notice-ably low quality, most likely due to Turkers usingautomatic translation systems.
That is why we usedimages instead of text in our second pass, whichyielded significant improvements.
That said, we donot discard the translations from the first pass, andwe do include them in our experiments.3.3 Post-editing and Ranking HITsIn addition to collecting four translations per sourcesentence, we also collected post-edited versionsof the translations, as well as ranking judgmentsabout their quality.Figure 2 gives examples of the unedited transla-tions that we collected in the translation pass.
Thesetypically contain many simple mistakes like mis-spellings, typos, and awkward word choice.
Weposted another MTurk task where we asked workersto edit the translations into more fluent and gram-matical sentences.
We restrict the task to US-basedworkers to increase the likelihood that they would benative English speakers.We also asked US-based Turkers to rank the trans-lations.
We presented the translations in groups offour, and the annotator?s task was to rank the sen-tences by fluency, from best to worst (allowing ties).We collected redundant annotations in these twotasks as well.
Each translation is edited three times(by three distinct editors).
We solicited only one editper translation from our first pass translation effort.So, in total, we had 10 post-edited translations for1222Avoiding dieting to preventfrom fluabstention from dieting inorder to avoid FluAbstain from decrease eating inorder to escape from flueIn order to be safer from fluquit dietingThis research of Americanscientists came in front afterexperimenting on mice.This research from theAmerican Scientists havecome up after theexperiments on rats.This research of Americanscientists was shown aftermany experiments on mouses.According to the AmericanScientist this research has comeout after muchexperimentations on rats.Experiments proved that miceon a lower calorie diet hadcomparatively less ability tofight the flu virus.in has been proven fromexperiments that rats put ondiet with less calories had lessability to resist the Flu virus.It was proved by experimentsthe low calories eatersmouses had low defendingpower for flue in ratio.Experimentaions have provedthat those rats on less caloriesdiet have developed a tendencyof not overcoming the flu virus.research has proven this oldmyth wrong that its better tofast during fever.Research disproved the oldaxiom that " It is better tofast during fever"The research proved this oldtalk that decrease eating isuseful in fever.This Research has proved thevery old saying wrong that it isgood to starve while in fever.Figure 2: We redundantly translate each source sentence by soliciting multiple translations from different Turkers.These translations are put through a subsequent editing set, where multiple edited versions are produced.
We selectthe best translation from the set using features that predict the quality of each translation and each translator.each source sentence (plus the four original transla-tions).
In the ranking task, we collected judgmentsfrom five distinct workers for each translation group.3.4 Data Collection CostWe paid a reward of $0.10 to translate a sentence,$0.25 to edit a set of ten sentences, and $0.06 to ranka set of four translation groups.
Therefore, we hadthe following costs:?
Translation cost: $716.80?
Editing cost: $447.50?
Ranking cost: $134.40(If not done redundantly, those values would be$179.20, $44.75, and $26.88, respectively.
)Adding Amazon?s 10% fee, this brings the grandtotal to under $1,500, spent to collect 7,000+ transla-tions, 17,000+ edited translations, and 35,000+ ranklabels.1 We also use about 10% of the existing pro-fessional references in most of our experiments (see4.2 and 4.3).
If we estimate the cost at $0.30/word,that would roughly be an additional $1,000.3.5 MTurk Participation52 different Turkers took part in the translation task,each translating 138 sentences on average.
In theediting task, 320 Turkers participated, averaging 56sentences each.
In the ranking task, 245 Turkers par-ticipated, averaging 9.1 HITs each, or 146 rank la-bels (since each ranking HIT involved judging 16translations, in groups of four).1Data URL: www.cs.jhu.edu/?ozaidan/RCLMT.4 Quality Control ModelOur approach to building a translation set fromthe available data is to select, for each Urdu sen-tence, the one translation that our model believesto be the best out of the available translations.
Weevaluate various selection techniques by compar-ing the selected Turker translations against existingprofessionally-produced translations.
The more theselected translations resemble the professional trans-lations, the higher the quality.4.1 Features Used to Select Best TranslationsOur model selects one of the 14 English options gen-erated by Turkers.
For a source sentence si, ourmodel assigns a score to each sentence in the setof available translations {ti,1, ...ti,14}.
The chosentranslation is the highest scoring translation:tr(si) = tri,j?
s.t.
j?
= argmaxjscore(ti,j) (1)where score(.)
is the dot product:score(ti,j)def= ~w ?
~f(ti,j) (2)Here, ~w is the model?s weight vector (tuned asdescribed below in 4.2), and ~f is a translation?s cor-responding feature vector.
Each feature is a functioncomputed from the English sentence string, the Urdusentence string, the workers (translators, editors, andrankers), and/or the rank labels.
We use 21 features,categorized into the following three sets.1223Sentence-level (6 features).
Most of the Turk-ers performing our task were native Urdu speakerswhose second language was English, and they do notalways produce natural-sounding English sentences.Therefore, the first set of features attempt to discrim-inate good English sentences from bad ones.?
Language model features: each sentence isassigned a log probability and per-word per-plexity score, using a 5-gram language modeltrained on the English Gigaword corpus.?
Sentence length features: a good translationtends to be comparable in length to the sourcesentence, whereas an overly short or long trans-lation is probably bad.
We add two features thatare the ratios of the two lengths (one penalizesshort sentences and one penalizes long ones).?
Web n-gram match percentage: we assign ascore to each sentence based on the percentageof the n-grams (up to length 5) in the transla-tion that exist in the Google N-Gram Database.?
Web n-gram geometric average: we calculatethe average over the different n-gram matchpercentages (similar to the way BLEU is com-puted).
We add three features corresponding tomax n-gram lengths of 3, 4, and 5.?
Edit rate to other translations: a bad translationis likely not to be very similar to other transla-tions, since there are many more ways a trans-lation can be bad than for it to be good.
So, wecompute the average edit rate distance from theother translations (using the TER metric).Worker-level (12 features).
We add worker-levelfeatures that evaluate a translation based on who pro-vided it.?
Aggregate features: for each sentence-levelfeature above, we have a corresponding featurecomputed over all of that worker?s translations.?
Language abilities: we ask workers to provideinformation about their language abilities.
Wehave a binary feature indicating whether Urduis their native language, and a feature for howlong they have spoken it.
We add a pair ofequivalent features for English.?
Worker location: two binary features reflect aworker?s location, one to indicate if they are lo-cated in Pakistan, and one to indicate if they arelocated in India.Ranking (3 features).
The third set of features isbased on the ranking labels we collected (see 3.3).?
Average rank: the average of the five rank la-bels provided for this translation.?
Is-Best percentage: how often the translationwas top-ranked among the four translations.?
Is-Better percentage: how often the translationwas judged as the better translation, over allpairwise comparisons extracted from the ranks.Other features (not investigated here) could in-clude source-target information, such as translationmodel scores or the number of source words trans-lated correctly according to a bilingual dictionary.4.2 Parameter TuningOnce features are computed for the sentences, wemust set the model?s weight vector ~w.
Naturally, theweights should be chosen so that good translationsget high scores, and bad translations get low scores.We optimize translation quality against a small sub-set (10%) of reference (professional) translations.To tune the weight vector, we use the linear searchmethod of Och (2003), which is the basis of Min-imum Error Rate Training (MERT).
MERT is aniterative algorithm used to tune parameters of anMT system, which operates by iteratively generatingnew candidate translations and adjusting the weightsto give good translations a high score, then regener-ating new candidates based on the updated weights,etc.
In our work, the set of candidate translations isfixed (the 14 English sentences for each source sen-tence), and therefore iterating the procedure is notapplicable.
We use the Z-MERT software package(Zaidan, 2009) to perform the search.4.3 The Worker Calibration FeatureSince we use a small portion of the reference trans-lations to perform weight tuning, we can also usethat data to compute another worker-specific fea-ture.
Namely, we can evaluate the competency ofeach worker by scoring their translations against thereference translations.
We then use that feature forevery translation given by that worker.
The intuition1224is that workers known to produce good translationsare likely to continue to produce good translations,and the opposite is likely true as well.4.4 Evaluation StrategyTo measure the quality of the translations, we makeuse of the existing professional translations.
Sincewe have four professional translation sets, we cancalculate the BLEU score (Papineni et al, 2002) forone professional translator P1 using the other threeP2,3,4 as a reference set.
We repeat the process fourtimes, scoring each professional translator againstthe others, to calculate the expected range of profes-sional quality translation.
We can see how a trans-lation set T (chosen by our model) compares to thisrange by calculating T ?s BLEU scores against thesame four sets of three reference translations.
Wewill evaluate different strategies for selecting sucha set T , and see how much each improves on theBLEU score, compared to randomly picking fromamong the Turker translations.We also evaluate Turker translation quality by us-ing them as reference sets to score various submis-sions to the NIST MT evaluation.
Specifically, wemeasure the correlation (using Pearson?s r) betweenBLEU scores of MT systems measured against non-professional translations, and BLEU scores mea-sured against professional translations.
Since themain purpose of the NIST dataset was to compareMT systems against each other, this is a more di-rect fitness-for-task measure.
We chose the middle 6systems (in terms of performance) submitted to theNIST evaluation, out of 12, as those systems werefairly close to each other, with less than 2 BLEUpoints separating them.25 Experimental ResultsWe establish the performance of professional trans-lators, calculate oracle upper bounds on Turkertranslation quality, and carry out a set of experimentsthat demonstrate the effectiveness of our model andthat determine which features are most helpful.Each number reported in this section is an averageof four numbers, corresponding to the four possible2Using all 12 systems artificially inflates correlation, due tothe vast differences between the systems.
For instance, the topsystem outperforms the bottom system by 15 BLEU points!ways of choosing 3 of the 4 reference sets.
Further-more, each of those 4 numbers is itself based on afive-fold cross validation, where 80% of the data isused to compute feature values, and 20% used forevaluation.
The 80% portion is used to compute theaggregate worker-level features.
For the worker cal-ibration feature, we utilize the references for 10% ofthe data (which is within the 80% portion).5.1 Translation Quality: BLEU ScoresCompared to ProfessionalsWe first evaluated the reference sets against eachother, in order to quantify the concept of ?profes-sional quality?.
On average, evaluating one refer-ence set against the other three gives a BLEU scoreof 42.38 (Figure 3).
A Turker set of translationsscores 28.13 on average, which highlights the loss inquality when collecting translations from amateurs.To make the gap clearer, the output of a state-of-the-art machine translation system (the syntax-basedvariant of Joshua; Li et al (2010)) achieves a scoreof 26.91, a mere 1.22 worse than the Turkers.We perform two oracle experiments to determineif there exist high-quality Turker translations in thefirst place.
The first oracle operates on the segmentlevel: for each source segment, choose from the fourtranslations the one that scores highest against thereference sentence.
The second oracle operates onthe worker level: for each source segment, choosefrom the four translations the one provided by theworker whose translations (over all sentences) scorethe highest.
The two oracles achieve BLEU scoresof 43.75 and 40.64, respectively ?
well within therange of professional translators.We examined two voting-inspired methods, sincetaking a majority vote usually works well when deal-ing with MTurk data.
The first selects the translationwith the minimum average TER (Snover et al, 2006)against the other three translations, since that wouldbe a ?consensus?
translation.
The second method se-lects the translation that received the best averagerank, using the rank labels assigned by other Turkers(see 3.3).
These approaches achieve BLEU scores of34.41 and 36.64, respectively.The main set of experiments evaluated the fea-tures from 4.1 and 4.3.
We applied our approachusing each of the four feature types: sentence fea-tures, Turker features, rank features, and the cali-122526.91 28.13 43.75 40.64 34.41 36.6442.38 34.95 35.79 37.14 37.82 39.06202530354045Reference(ave.)Joshua(syntax)Turker(ave.)Oracle(segment)Oracle(Turker)LowestTERBestrankSentencefeaturesTurkerfeaturesRankfeaturesCalibrationfeatureAllfeaturesBLEUFigure 3: BLEU scores for different selection methods, measured against the reference sets.
Each score is an averageof four BLEU scores, each calculated against three LDC reference translations.
The five right-most bars are coloredin orange to indicate selection over a set that includes both original translations as well as edited versions of them.bration feature.
That yielded BLEU scores rangingfrom 34.95 to 37.82.
With all features combined, weachieve a higher score of 39.06, which is within therange of scores for the professional translators.5.2 Fitness for a Task: Correlation WithProfessionals When Ranking MT SystemsWe evaluated the selection methods by measuringcorrelation with the references, in terms of BLEUscores assigned to outputs of MT systems.
The re-sults, in Table 1, tell a fairly similar story as eval-uating with BLEU: references and oracles naturallyperform very well, and the loss in quality when se-lecting arbitrary Turker translations is largely elimi-nated using our selection strategy.Interestingly, when using the Joshua output asa reference set, the performance is quite abysmal.Even though its BLEU score is comparable to theTurker translations, it cannot be used to distinguishclosely matched MT systems from each other.36 AnalysisThe oracles indicate that there is usually an accept-able translation from the Turkers for any given sen-tence.
Since the oracles select from a small group ofonly 4 translations per source segment, they are notoverly optimistic, and rather reflect the true potentialof the collected translations.The results indicate that, although some featuresare more useful than others, much of the benefitfrom combining all the features can be obtainedfrom any one set of features, with the benefit of3It should be noted that the Joshua system was not one ofthe six MT systems we scored in the correlation experiments.34.71 35.45 37.14 37.22 37.96202530354045SentencefeaturesTurkerfeaturesRankfeaturesCalibrationfeatureAllfeaturesBLEUFigure 4: BLEU scores for the five right-most setups fromFigure 3, constrained over the original translations.adding more features being somewhat orthogonal.Finally, we performed a series of experiments ex-ploring the calibration feature, varying the amountof gold-standard references from 10% all the way upto 80%.
As expected, the performance improved asmore references were used to calibrate the transla-tors (Figure 5).
What?s particularly important aboutthis experiment is that it shows the added benefitof the other features: We would have to use 30%?40% of the references to get the same benefit ob-tained from combining the non-calibration featuresand only 10% for the calibration feature (dashed linein the Figure; BLEU = 39.06).6.1 Cost ReductionWhile the combined cost of our data collection ef-fort ($2,500; see 3.4) is quite low considering theamount of collected data, it would be more attractiveif the cost could be reduced further without losingmuch in translation quality.
To that end, we inves-tigated lowering cost along two dimensions: elimi-nating the need for professional translations, and de-creasing the amount of edited translations.1226Selection Method Pearson?s r2Reference (ave.) 0.81 ?
0.07Joshua (syntax) 0.08 ?
0.09Turker (ave.) 0.60 ?
0.17Oracle (segment) 0.81 ?
0.09Oracle (Turker) 0.79 ?
0.10Lowest TER 0.50 ?
0.26Best rank 0.74 ?
0.17Sentence features 0.56 ?
0.21Turker features 0.59 ?
0.19Rank features 0.75 ?
0.14Calibration feature 0.76 ?
0.13All features 0.77 ?
0.11Table 1: Correlation (?
std.
dev.)
for different selectionmethods, compared against the reference sets.The professional translations are used in our ap-proach for computing the worker calibration feature(subsection 4.3) and for tuning the weights of theother features.
We use a relatively small amountfor this purpose, but we investigate a different setupwhereby no professional translations are used at all.This eliminates the worker calibration feature, but,perhaps more critically, the feature weights must beset in a different fashion, since we cannot optimizeBLEU on reference data anymore.
Instead, we usethe rank labels (from 3.3) as a proxy for BLEU, andset the weights so that better ranked translations re-ceive higher scores.Note that the rank features will also be excludedin this setup, since they are perfect predictors of ranklabels.
On the one hand, this means no rank labelsneed to be collected, other than for a small set usedfor weight tuning, further reducing the cost of datacollection.
However, this leads to a significant dropin performance, yielding a BLEU score of 34.86.Another alternative for cost reduction would be toreduce the number of collected edited translations.To that end, we first investigate completely eliminat-ing the editing phase, and considering only uneditedtranslations.
In other words, the selection will beover a group of four English sentences rather than14 sentences.
Completely eliminating the editedtranslations has an adverse effect, as expected (Fig-ure 4).
Another option, rather than eliminating theediting phase altogether, would be to consider theedited translations of only the translation receiving37.037.538.038.539.039.540.040.50 20 40 60 80 100% References Used for CalibrationBLEU10%+other features(i.e.
"All features"from Figure 3)Figure 5: The effect of varying the amount of calibra-tion data (and using only the calibration feature).
The10% point (BLEU = 37.82) and the dashed line (BLEU =39.06) correspond to the two right-most bars of Figure 3.the best rank labels.
This would reflect a data col-lection process whereby the editing task is delayeduntil after the rank labels are collected, with the ranklabels used to determine which translations are mostpromising to post-edit (in addition to using the ranklabels for the ranking features).
Using this approachenables us to greatly reduce the number of editedtranslations collected, while maintaining good per-formance, obtaining a BLEU score of 38.67.It is therefore our recommendation that crowd-sourced translation efforts adhere to the follow-ing pipeline: collect multiple translations for eachsource sentence, collect rank labels for the transla-tions, and finally collect edited versions of the topranked translations.7 Related WorkDawid and Skene (1979) investigated filteringannotations using the EM algorithm, estimatingannotator-specific error rates in the context of patientmedical records.
Snow et al (2008) were among thefirst to use MTurk to obtain data for several NLPtasks, such as textual entailment and word sense dis-ambiguation.
Their approach, based on majorityvoting, had a component for annotator bias correc-tion.
They showed that for such tasks, a few non-expert labels usually suffice.Whitehill et al (2009) proposed a probabilisticmodel to filter labels from non-experts, in the con-text of an image labeling task.
Their system genera-tively models image difficulty, as well as noisy, even1227adversarial, annotators.
They apply their method tosimulated labels rather than real-life labels.Callison-Burch (2009) proposed several ways toevaluate MT output on MTurk.
One such methodwas to collect reference translations to score MToutput.
It was only a pilot study (50 sentences ineach of several languages), but it showed the pos-sibility of obtaining high-quality translations fromnon-professionals.
As a followup, Bloodgood andCallison-Burch (2010) solicited a single translationof the NIST Urdu-to-English dataset we used.
Theirevaluation was similar to our correlation experi-ments, examining how well the collected transla-tions agreed with the professional translations whenevaluating three MT systems.That paper appeared in a NAACL 2010 workshoporganized by Callison-Burch and Dredze (2010), fo-cusing on MTurk as a source of data for speech andlanguage tasks.
Two relevant papers from that work-shop were by Ambati and Vogel (2010), focusing onthe design of the translation HIT, and by Irvine andKlementiev (2010), who created translation lexiconsbetween English and 42 rare languages.Resnik et al (2010) explore a very interestingway of creating translations on MTurk, relying onlyon monolingual speakers.
Speakers of the targetlanguage iteratively identified problems in machinetranslation output, and speakers of the source lan-guage paraphrased the corresponding source por-tion.
The paraphrased source would then be re-translated to produce a different translation, hope-fully more coherent than the original.8 Conclusion and Future WorkWe have demonstrated that it is possible to ob-tain high-quality translations from non-professionaltranslators, and that the cost is an order of magni-tude cheaper than professional translation.
We be-lieve that crowdsourcing can play a pivotal role infuture efforts to create parallel translation datasets.Beyond the cost and scalability, crowdsourcing pro-vides access to languages that currently fall outsidethe scope of statistical machine translation research.We have begun an ongoing effort to collect transla-tions for several low resource languages, includingTamil, Yoruba, and dialectal Arabic.
We plan to:?
Investigate improvements from system combi-nation techniques to the redundant translations.?
Modify our editing step to collect an annotatedcorpus of English as a second language errors.?
Calibrate against good Turkers, instead of pro-fessionals, once they have been identified.?
Predict whether it is necessary to solicit anothertranslation instead of collecting a fixed number.?
Analyze how much quality matters if our goalis to train a statistical translation system.AcknowledgmentsThis research was supported by the Human Lan-guage Technology Center of Excellence, by giftsfrom Google and Microsoft, and by the DARPAGALE program under Contract No.
HR0011-06-2-0001.
The views and findings are the authors?
alone.We would like to thank Ben Bederson, PhilipResnik, and Alain De?silets for organizing work-shops focused on crowdsourcing translation (Bed-erson and Resnik, 2010; De?silets, 2010).
We aregrateful for the feedback of workshop participants,which helped shape this research.ReferencesYaser Al-Onaizan, Ulrich Germann, Ulf Hermjakob,Kevin Knight, Philipp Koehn, Daniel Marcu, andKenji Yamada.
2002.
Translation with scarce bilin-gual resources.
Machine Translation, 17(1), March.Vamshi Ambati and Stephan Vogel.
2010.
Can crowdsbuild parallel corpora for machine translation systems?In Proceedings of the NAACL HLT Workshop on Cre-ating Speech and Language Data With Amazon?s Me-chanical Turk, pages 62?65.Ben Bederson and Philip Resnik.
2010.
Workshop oncrowdsourcing and translation.
http://www.cs.umd.edu/hcil/monotrans/workshop/.Michael Bloodgood and Chris Callison-Burch.
2010.Using Mechanical Turk to build machine translationevaluation sets.
In Proceedings of the NAACL HLTWorkshop on Creating Speech and Language DataWith Amazon?s Mechanical Turk, pages 208?211.Chris Callison-Burch and Mark Dredze.
2010.
Creatingspeech and language data with Amazon?s MechanicalTurk.
In Proceedings of the NAACL HLT Workshop onCreating Speech and Language Data With Amazon?sMechanical Turk, pages 1?12.Chris Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using Amazon?s Me-1228chanical Turk.
In Proceedings of EMNLP, pages 286?295.A.
P. Dawid and A. M. Skene.
1979.
Maximum likeli-hood estimation of observer error-rates using the EMalgorithm.
Applied Statistics, 28(1):20?28.Alain De?silets.
2010.
AMTA 2010 workshop on collabo-rative translation: technology, crowdsourcing, and thetranslator perspective.
http://bit.ly/gPnqR2.Pascale Fung and Lo Yuen Yee.
1998.
An ir approach fortranslating new words from nonparallel, comparabletexts.
In Proceedings of ACL/CoLing.Ulrich Germann.
2001.
Building a statistical machinetranslation system from scratch: How much bang forthe buck can we expect?
In ACL 2001 Workshop onData-Driven Machine Translation, Toulouse, France.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexi-cons from monolingual corpora.
In Proceedings ofACL/HLT.Panos Ipeirotis.
2010.
New demographics of MechanicalTurk.
http://behind-the-enemy-lines.blogspot.com/2010/03/new-demographics-of-mechanical-turk.html.Ann Irvine and Alexandre Klementiev.
2010.
Using Me-chanical Turk to annotate lexicons for less commonlyused languages.
In Proceedings of the NAACL HLTWorkshop on Creating Speech and Language DataWith Amazon?s Mechanical Turk, pages 108?113.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Ann Irvine, Sanjeev Khudanpur, LaneSchwartz, Wren Thornton, Ziyuan Wang, JonathanWeese, and Omar Zaidan.
2010.
Joshua 2.0: Atoolkit for parsing-based machine translation with syn-tax, semirings, discriminative training and other good-ies.
In Proceedings of the Joint Fifth Workshop on Sta-tistical Machine Translation and MetricsMATR, pages133?137.Dragos Munteanu and Daniel Marcu.
2005.
Improvingmachine translation performance by exploiting compa-rable corpora.
Computational Linguistics, 31(4):477?504, December.Sonja Niessen and Hermann Ney.
2004.
Statisti-cal machine translation with scarce resources usingmorpho-syntatic analysis.
Computational Linguistics,30(2):181?204.Doug Oard, David Doermann, Bonnie Dorr, Daqing He,Phillip Resnik, William Byrne, Sanjeeve Khudanpur,David Yarowsky, Anton Leuski, Philipp Koehn, andKevin Knight.
2003.
Desperately seeking Cebuano.In Proceedings of HLT/NAACL.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL,pages 160?167.Kishore Papineni, Salim Poukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of ACL,pages 311?318.Katharina Probst, Lori Levin, Erik Peterson, Alon Lavie,and Jamie Carbonell.
2002.
MT for minority lan-guages using elicitation-based learning of syntactictransfer rules.
Machine Translation, 17(4).Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of ACL.Philip Resnik and Noah Smith.
2003.
The web as a par-allel corpus.
Computational Linguistics, 29(3):349?380, September.Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,Alex Quinn, and Benjamin Bederson.
2010.
Improv-ing translation via targeted paraphrasing.
In Proceed-ings of EMNLP, pages 127?137.Charles Schafer and David Yarowsky.
2002.
Induc-ing translation lexicons via diverse similarity measuresand bridge languages.
In Conference on Natural Lan-guage Learning-2002, pages 146?152.Jason R. Smith, Chris Quirk, and Kristina Toutanova.2010.
Extracting parallel sentences from comparablecorpora using document level alignment.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 403?411, Los An-geles, California, June.
Association for ComputationalLinguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Translationin the Americas (AMTA).Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast ?
but is itgood?
Evaluating non-expert annotations for natu-ral language tasks.
In Proceedings of EMNLP, pages254?263.Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, andMoshe Dubiner.
2010.
Large scale parallel documentmining for machine translation.
In Proc.
of the In-ternational Conference on Computational Linguistics(COLING).Jacob Whitehill, Paul Ruvolo, Tingfan Wu, JacobBergsma, and Javier Movellan.
2009.
Whose voteshould count more: Optimal integration of labels fromlabelers of unknown expertise.
In Proceedings ofNIPS, pages 2035?2043.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.1229
