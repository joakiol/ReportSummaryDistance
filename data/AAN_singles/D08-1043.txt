Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 410?418,Honolulu, October 2008. c?2008 Association for Computational LinguisticsBridging Lexical Gaps between Queries and Questions onLarge Online Q&A Collections with Compact Translation ModelsJung-Tae Lee?
and Sang-Bum Kim?
and Young-In Song?
and Hae-Chang Rim??Dept.
of Computer & Radio Communications Engineering, Korea University, Seoul, Korea?Search Business Team, SK Telecom, Seoul, Korea?Dept.
of Computer Science & Engineering, Korea University, Seoul, Korea{jtlee,sbkim,song,rim}@nlp.korea.ac.krAbstractLexical gaps between queries and questions(documents) have been a major issue in ques-tion retrieval on large online question andanswer (Q&A) collections.
Previous stud-ies address the issue by implicitly expandingqueries with the help of translation modelspre-constructed using statistical techniques.However, since it is possible for unimpor-tant words (e.g., non-topical words, commonwords) to be included in the translation mod-els, a lack of noise control on the models cancause degradation of retrieval performance.This paper investigates a number of empiricalmethods for eliminating unimportant words inorder to construct compact translation mod-els for retrieval purposes.
Experiments con-ducted on a real world Q&A collection showthat substantial improvements in retrieval per-formance can be achieved by using compacttranslation models.1 IntroductionCommunity-driven question answering services,such as Yahoo!
Answers1 and Live Search QnA2,have been rapidly gaining popularity among Webusers interested in sharing information online.
Byinducing users to collaboratively submit questionsand answer questions posed by other users, largeamounts of information have been collected in theform of question and answer (Q&A) pairs in recentyears.
This user-generated information is a valu-able resource for many information seekers, because1http://answers.yahoo.com/2http://qna.live.com/users can acquire information straightforwardly bysearching through answered questions that satisfytheir information need.Retrieval models for such Q&A collectionsshould manage to handle the lexical gaps or wordmismatches between user questions (queries) andanswered questions in the collection.
Consider thetwo following examples of questions that are seman-tically similar to each other:?
?Where can I get cheap airplane tickets???
?Any travel website for low airfares?
?Conventional word-based retrieval models wouldfail to capture the similarity between the two, be-cause they have no words in common.
To bridge thequery-question gap, prior work on Q&A retrieval byJeon et al (2005) implicitly expands queries with theuse of pre-constructed translation models, which letsyou generate query words not in a question by trans-lation to alternate words that are related.
In prac-tice, these translation models are often constructedusing statistical machine translation techniques thatprimarily rely on word co-occurrence statistics ob-tained from parallel strings (e.g., question-answerpairs).A critical issue of the translation-based ap-proaches is the quality of translation models con-structed in advance.
If no noise control is conductedduring the construction, it is possible for translationmodels to contain ?unnecessary?
translations (i.e.,translating a word into an unimportant word, such asa non-topical or common word).
In the query expan-sion viewpoint, an attempt to identify and decrease410the proportion of unnecessary translations in a trans-lation model may produce an effect of ?selective?implicit query expansion and result in improved re-trieval.
However, prior work on translation-basedQ&A retrieval does not recognize this issue and usesthe translation model as it is; essentially no attentionseems to have been paid to improving the perfor-mance of the translation-based approach by enhanc-ing the quality of translation models.In this paper, we explore a number of empiri-cal methods for selecting and eliminating unimpor-tant words from parallel strings to avoid unnecessarytranslations from being learned in translation modelsbuilt for retrieval purposes.
We use the term compacttranslation models to refer to the resulting models,since the total number of parameters for modelingtranslations would be minimized naturally.
We alsopresent experiments in which compact translationmodels are used in Q&A retrieval.
The main goal ofour study is to investigate if and how compact trans-lation models can improve the performance of Q&Aretrieval.The rest of this paper is organized as follows.The next section introduces a translation-based re-trieval model and accompanying techniques used toretrieve query-relevant questions.
Section 3 presentsa number of empirical ways to select and eliminateunimportant words from parallel strings for trainingcompact translation models.
Section 4 summarizesthe compact translation models we built for retrievalexperiments.
Section 5 presents and discusses theresults of retrieval experiments.
Section 6 presentsrelated works.
Finally, the last section concludes thepaper and discusses future directions.2 Translation-based Retrieval ModelThis section introduces the translation-based lan-guage modeling approach to retrieval that has beenused to bridge the lexical gap between queries andalready-answered questions in this paper.In the basic language modeling framework for re-trieval (Ponte and Croft, 1998), the similarity be-tween a query Q and a document D for ranking maybe modeled as the probability of the document lan-guage model MD built from D generating Q:sim(Q,D) ?
P (Q|MD) (1)Assuming that query words occur independentlygiven a particular document language model, thequery-likelihood P (Q|MD) is calculated as:P (Q|MD) =?q?QP (q|MD) (2)where q represents a query word.To avoid zero probabilities in document languagemodels, a mixture between a document-specificmultinomial distribution and a multinomial distribu-tion estimated from the entire document collectionis widely used in practice:P (Q|MD) =?q?Q[(1?
?)
?
P (q|MD)+?
?
P (q|MC)](3)where 0 < ?
< 1 and MC represents a languagemodel built from the entire collection.
The probabil-ities P (w|MD) and P (w|MC) are calculated usingmaximum likelihood estimation.The basic language modeling framework does notaddress the issue of lexical gaps between queriesand question.
Berger and Lafferty (1999) viewedinformation retrieval as statistical document-querytranslation and introduced translation models to mapquery words to document words.
Assuming thata translation model can be represented by a condi-tional probability distribution of translation T (?|?
)between words, we can model P (q|MD) in Equa-tion 3 as:P (q|MD) =?w?DT (q|w)P (w|MD) (4)where w represents a document word.3The translation probability T (q|w) virtually rep-resents the degree of relationship between queryword q and document word w captured in a differ-ent, machine translation setting.
Then, in the tra-ditional information retrieval viewpoint, the use oftranslation models produce an implicit query expan-sion effect, since query words not in a document aremapped to related words in the document.
This im-plies that translation-based retrieval models wouldmake positive contributions to retrieval performanceonly when the pre-constructed translation modelshave reliable translation probability distributions.3The formulation of our retrieval model is basically equiva-lent to the approach of Jeon et al (2005).4112.1 IBM Translation Model 1Obviously, we need to build a translation model inadvance.
Usually the IBM Model 1, developed inthe statistical machine translation field (Brown et al,1993), is used to construct translation models forretrieval purposes in practice.
Specifically, given anumber of parallel strings, the IBM Model 1 learnsthe translation probability from a source word s to atarget word t as:T (t|s) = ?
?1sN?ic(t|s;Ji) (5)where ?s is a normalization factor to make the sumof translation probabilities for the word s equal to 1,N is the number of parallel string pairs, and Ji is theith parallel string pair.
c(t|s; Ji) is calculated as:c(t|s; Ji) =( P (t|s)P (t|s1) + ?
?
?+ P (t|sn))?freqt,Ji ?
freqs,Ji (6)where {s1, .
.
.
, sn} are words in the source text inJ i. freqt,Ji and freqs,Ji are the number of timesthat t and s occur in Ji, respectively.Given the initial values of T (t|s), Equations (5)and (6) are used to update T (t|s) repeatedly untilthe probabilities converge, in an EM-based manner.Note that the IBM Model 1 solely relies onword co-occurrence statistics obtained from paral-lel strings in order to learn translation probabilities.This implies that if parallel strings have unimportantwords, a resulted translation model based on IBMModel 1 may contain unimportant words with non-zero translation probabilities.We alleviate this drawback by eliminating unim-portant words from parallel strings, avoiding themfrom being included in the conditional translationprobability distribution.
This naturally induces theconstruction of compact translation models.2.2 Gathering Parallel Strings from Q&ACollectionsThe construction of statistical translation modelspreviously discussed requires a corpus consisting ofparallel strings.
Since monolingual parallel texts aregenerally not available in real world, one must arti-ficially generate a ?synthetic?
parallel corpus.Question and answer as parallel pairs: Thesimplest approach is to directly employ questionsand their answers in the collections by setting ei-ther as source strings and the other as target strings,with the assumption that a question and its cor-responding answer are naturally parallel to eachother.
Formally, if we have a Q&A collection asC = {D1, D2, .
.
.
, Dn}, where Di refers to an ithQ&A data consisting of a question qi and its an-swer ai, we can construct a parallel corpus C ?
as{(q1, a1), .
.
.
, (qn, an)}?
{(a1, q1), .
.
.
, (an, qn)} =C ?
where each element (s, t) refers to a parallel pairconsisting of source string s and target string t. Thenumber of parallel string samples would eventuallybe twice the size of the collections.Similar questions as parallel pairs: Jeon etal.
(2005) proposed an alternative way of auto-matically collecting a relatively larger set of par-allel strings from Q&A collections.
Motivatedby the observation that many semantically identi-cal questions can be found in typical Q&A collec-tions, they used similarities between answers cal-culated by conventional word-based retrieval mod-els to automatically group questions in a Q&A col-lection as pairs.
Formally, two question strings qiand qj would be included in a parallel corpus C ?as {(qi, qj), (qj , qi)} ?
C ?
only if their answerstrings ai and aj have a similarity higher than apre-defined threshold value.
The similarity is cal-culated as the reverse of the harmonic mean of ranksas sim(ai, aj) = 12( 1rj + 1ri ), where rj and ri refer tothe rank of the aj and ai when ai and aj are given asqueries, respectively.
This approach may artificiallyproduce much more parallel string pairs for trainingthe IBM Model 1 than the former approach, depend-ing on the threshold value.4To our knowledge, there has not been any studycomparing the effectiveness of the two approachesyet.
In this paper, we try both approaches and com-pare the effectiveness in retrieval performance.3 Eliminating Unimportant WordsWe adopt a term weight ranking approach to iden-tify and eliminate unimportant words from parallelstrings, assuming that a word in a string is unim-4We have empirically set the threshold (0.05) for our exper-iments.412Figure 1: Term weighting results of tf-idf and TextRank (window=3).
Weighting is done on underlined words only.portant if it holds a relatively low significance in thedocument (Q&A pair) of which the string is origi-nally taken from.
Some issues may arise:?
How to assign a weight to each word in a doc-ument for term ranking??
How much to remove as unimportant wordsfrom the ranked list?The following subsections discuss strategies we useto handle each of the issues above.3.1 Assigning Term WeightsIn this section, the two different term weightingstrategies are introduced.tf-idf: The use of tf-idf weighting on evaluatinghow unimportant a word is to a document seems tobe a good idea to begin with.
We have used the fol-lowing formulas to calculate the weight of word win document D:tf -idfw,D = tfw,D ?
idfw (7)tfw,D = freqw,D|D| , idfw = log|C|dfwwhere freqw,D refers to the number of times w oc-curs in D, |D| refers to the size of D (in words), |C|refers to the size of the document collection, and dfwrefers to the number of documents where w appears.Eventually, words with low tf-idf weights may beconsidered as unimportant.TextRank: The task of term weighting, in fact,has been often applied to the keyword extractiontask in natural language processing studies.
Asan alternative term weighting approach, we haveused a variant of Mihalcea and Tarau (2004)?s Tex-tRank, a graph-based ranking model for keywordextraction which achieves state-of-the-art accuracywithout the need of deep linguistic knowledge ordomain-specific corpora.Specifically, the ranking algorithm proceeds asfollows.
First, words in a given document are addedas vertices in a graph G. Then, edges are added be-tween words (vertices) if the words co-occur in afixed-sized window.
The number of co-occurrencesbecomes the weight of an edge.
When the graph isconstructed, the score of each vertex is initializedas 1, and the PageRank-based ranking algorithm isrun on the graph iteratively until convergence.
TheTextRank score of a word w in document D at kthiteration is defined as follows:Rkw,D = (1?
d)+ d ???j:(i,j)?Gei,j?
?l:(j,l)?G ej,lRk?1w,D(8)where d is a damping factor usually set to 0.85, andei,j is an edge weight between i and j.The assumption behind the use of the variant ofTextRank is that a word is likely to be an importantword in a document if it co-occurs frequently withother important words in the document.
Eventually,words with low TextRank scores may be consideredas unimportant.
The main differences of TextRankcompared to tf-idf is that it utilizes the context infor-mation of words to assign term weights.Figure 1 demonstrates that term weighting resultsof TextRank and tf-idf are greatly different.
Noticethat TextRank assigns low scores to words that co-413Corpus: (Q?A) Vocabulary Size (%chg) Average Translations (%chg)tf-idf TextRank tf-idf TextRankInitial 90,441 7325%Removal 90,326 (?0.1%) 73,021 (?19.3%) 73 (?0.0%) 44 (?39.7%)50%Removal 90,230 (?0.2%) 72,225 (?20.1%) 72 (?1.4%) 43 (?41.1%)75%Removal 88,763 (?1.9%) 65,268 (?27.8%) 53 (?27.4%) 38 (?47.9%)Avg.Score 66,412 (?26.6%) 31,849 (?64.8%) 14 (?80.8%) 18 (?75.3%)Table 1: Impact of various word elimination strategies on translation model construction using (Q?A) corpus.Corpus: (Q?Q) Vocabulary Size (%chg) Average Translations (%chg)tf-idf TextRank tf-idf TextRankInitial 34,485 44225%Removal 34,374 (?0.3%) 26,900 (?22.0%) 437 (?1.1%) 282 (?36.2%)50%Removal 34,262 (?0.6%) 26,421 (?23.4%) 423 (?4.3%) 274 (?38.0%)75%Removal 32,813 (?4.8%) 23,354 (?32.3%) 288 (?34.8%) 213 (?51.8%)Avg.Score 28,613 (?17.0%) 16,492 (?52.2%) 163 (?63.1%) 164 (?62.9%)Table 2: Impact of various word elimination strategies on translation model construction using (Q?Q) corpus.occur only with stopwords.
This implies that Tex-tRank weighs terms more ?strictly?
than the tf-idfapproach, with use of contexts of words.3.2 Deciding the Quantity to be Removed fromRanked ListOnce a final score (either tf-idf or TextRank score)is obtained for each word, we create a list of wordsranked in decreasing order of their scores and elim-inate the ones at lower ranks as unimportant words.The question here is how to decide the proportion orquantity to be removed from the ranked list.Removing a fixed proportion: The first ap-proach we have used is to decide the number ofunimportant words based on the size of the originalstring.
For our experiments, we manually vary theproportion to be removed as 25%, 50%, and 75%.For instance, if the proportion is set to 50% and anoriginal string consists of ten words, at most fivewords would be remained as important words.Using average score as threshold: We also haveused an alternate approach to deciding the quantity.Instead of eliminating a fixed proportion, words areremoved if their score is lower than the average scoreof all words in a document.
This approach decidesthe proportion to be removed more flexibly than theformer approach.4 Building Compact Translation ModelsWe have initially built two parallel corpora froma Q&A collection5, denoted as (Q?A) corpus and(Q?Q) corpus henceforth, by varying the methodsin which parallel strings are gathered (described inSection 2.2).
The (Q?A) corpus consists of 85,938parallel string pairs, and the (Q?Q) corpus contains575,649 parallel string pairs.In order to build compact translation models, wehave preprocessed the parallel corpus using differ-ent word elimination strategies so that unimpor-tant words would be removed from parallel strings.We have also used a stoplist6 consisting of 429words to remove stopwords.
The out-of-the-boxGIZA++7 (Och and Ney, 2004) has been used tolearn translation models using the pre-processed par-allel corpus for our retrieval experiments.
We havealso trained initial translation models, using a par-allel corpus from which only the stopwords are re-moved, to compare with the compact translationmodels.Eventually, the number of parameters neededfor modeling translations would be minimized ifunimportant words are eliminated with different ap-5Details on this data will be introduced in the next section.6http://truereader.com/manuals/onix/stopwords1.html7http://www.fjoch.com/GIZA++.html414proaches.
Table 1 and 2 shows the impact of variousword elimination strategies on the construction ofcompact translation models using the (Q?A) corpusand the (Q?Q) corpus, respectively.
The two tablesreport the size of the vocabulary contained and theaverage number of translations per word in the re-sulting compact translation models, along with per-centage decreases with respect to the initial transla-tion models in which only stopwords are removed.We make these observations:?
The translation models learned from the (Q?Q)corpus have less vocabularies but more aver-age translations per word than the ones learnedfrom the (Q?A) corpus.
This result implies thata large amount of noise may have been cre-ated inevitably when a large number of parallelstrings (pairs of similar questions) were artifi-cially gathered from the Q&A collection.?
The TextRank strategy tends to eliminate largersets of words as unimportant words than thetf-idf strategy when a fixed proportion is re-moved, regardless of the corpus type.
Recallthat the TextRank approach assigns weights towords more strictly by using contexts of words.?
The approach to remove words according tothe average weight of a document (denoted asAvg.Score) tends to eliminate relatively largerportions of words as unimportant words thanany of the fixed-proportion strategies, regard-less of either the corpus type or the rankingstrategy.5 Retrieval ExperimentsExperiments have been conducted on a real worldQ&A collection to demonstrate the effectiveness ofcompact translation models on Q&A retrieval.5.1 Experimental SettingsIn this section, four experimental settings for theQ&A retrieval experiments are described in detail.Data: For the experiments, Q&A data have beencollected from the Science domain of Yahoo!
An-swers, one of the most popular community-basedquestion answering service on the Web.
We haveobtained a total of 43,001 questions with a best an-swer (selected either by the questioner or by votes ofother users) by recursively traversing subcategoriesof the Science domain, with up to 1,000 questionpages retrieved.8Among the obtained Q&A pairs, 32 Q&A pairshave been randomly selected as the test set, and theremaining 42,969 questions have been the referenceset to be retrieved.
Each Q&A pair has three textfields: question title, question content, and answer.9The fields of each Q&A pair in the test set are con-sidered as various test queries; the question title,the question content, and the answer are regardedas a short query, a long query, and a supplementaryquery, respectively.
We have used long queries andsupplementary queries only in the relevance judg-ment procedure.
All retrieval experiments have beenconducted using short queries only.Relevance judgments: To find relevant Q&Apairs given a short query, we have employed a pool-ing technique used in the TREC conference series.We have pooled the top 40 Q&A pairs from eachretrieval results generated by varying the retrievalalgorithms, the search field, and the query type.Popular word-based models, including the OkapiBM25, query-likelihood language model, and pre-vious translation-based models (Jeon et al, 2005),have been used.10Relevance judgments have been done by two stu-dent volunteers (both fluent in English).
Sincemany community-based question answering ser-vices present their search results in a hierarchicalfashion (i.e.
a list of relevant questions is shownfirst, and then the user chooses a specific questionfrom the list to see its answers), a Q&A pair has beenjudged as relevant if its question is semantically sim-ilar to the query; neither quality nor rightness of theanswer has not been considered.
When a disagree-ment has been made between two volunteers, one ofthe authors has made the final judgment.
As a result,177 relevant Q&A pairs have been found in total forthe 32 short queries.Baseline retrieval models: The proposed ap-8Yahoo!
Answers did not expose additional question pagesto external requests at the time of collecting the data.9When collecting parallel strings from the Q&A collection,we have put together the question title and the question contentas one question string.10The retrieval model using compact translation models hasnot been used in the pooling procedure.415proach to Q&A retrieval using compact translationmodels (denoted as CTLM henceforth) is comparedto three baselines:QLM: Query-likelihood language model for re-trieval (equivalent to Equation 3, without use oftranslation models).
This model represents word-based retrieval models widely used in practice.TLM(Q?Q): Translation-based language modelfor question retrieval (Jeon et al, 2005).
This modeluses IBM Model 1 learned from the (Q?Q) corpusof which stopwords are removed.TLM(Q?A): A variant of the translation-based ap-proach.
This model uses IBM model 1 learned fromthe (Q?A) corpus.Evaluation metrics: We have reported the re-trieval performance in terms of Mean Average Pre-cision (MAP) and Mean R-Precision (R-Prec).Average Precision can be computed based on theprecision at each relevant document in the ranking.Mean Average Precision is defined as the mean ofthe Average Precision values across the set of allqueries:MAP (Q) = 1|Q|?q?Q1mqmq?k=1Precision(Rk) (9)where Q is the set of test queries, mq is the numberof relevant documents for a query q, Rk is the set ofranked retrieval results from the top until rank posi-tion k, and Precision(Rk) is the fraction of relevantdocuments in Rk (Manning et al, 2008).R-Precision is defined as the precision afterR documents have been retrieved where R isthe number of relevant documents for the currentquery (Buckley and Voorhees, 2000).
Mean R-Precision is the mean of the R-Precisions across theset of all queries.We take MAP as our primary evaluation metric.5.2 Experimental ResultsPreliminary retrieval experiments have been con-ducted using the baseline QLM and different fieldsof Q&A data as retrieval unit.
Table 3 shows theeffectiveness of each field.The results imply that the question title field is themost important field in our Yahoo!
Answers collec-tion; this also supports the observation presented byRetrieval unit MAP R-PrecQuestion title 0.1031 0.2396Question content 0.0422 0.0999Answer 0.0566 0.1062Table 3: Preliminary retrieval results.Model MAP R-Prec(%chg) (%chg)QLM 0.1031 0.2396TLM(Q?Q)* 0.1121 0.2251(49%) (?6%)CTLM(Q?Q) 0.1415 0.2425(437%) (41%)TLM(Q?A) 0.1935 0.3135(488%) (431%)CTLM(Q?A) 0.2095 0.3585(4103%) (450%)Table 4: Comparisons with three baseline retrieval mod-els.
* indicates that it is equivalent to Jeon et al (2005)?sapproach.
MAP improvements of CTLMs have beentested to be statistically significant using paired t-test.Jeon et al (2005).
Based on the preliminary obser-vations, all retrieval models tested in this paper haveranked Q&A pairs according to the similarity scoresbetween queries and question titles.Table 4 presents the comparison results of threebaseline retrieval models and the proposed CTLMs.For each method, the best performance after empir-ical ?
parameter tuning according to MAP is pre-sented.Notice that both the TLMs and CTLMs have out-performed the word-based QLM.
This implies thatword-based models that do not address the issue oflexical gaps between queries and questions often failto retrieve relevant Q&A data that have little wordoverlap with queries, as noted by Jeon et al (2005).Moreover, notice that the proposed CTLMs haveachieved significantly better performances in allevaluation metrics than both QLM and TLMs, regard-less of the parallel corpus in which the incorporatedtranslation models are trained from.
This is a clearindication that the use of compact translation modelsbuilt with appropriate word elimination strategies iseffective in closing the query-question lexical gaps416(Q?Q) MAP (%chg)tf-idf TextRankInitial 0.112125%Rmv 0.1141 (41.8) 0.1308 (416.7)50%Rmv 0.1261 (412.5) 0.1334 (419.00)75%Rmv 0.1115 (?0.5) 0.1160 (43.5)Avg.Score 0.1056 (?5.8) 0.1415 (426.2)Table 5: Contributions of various word elimination strate-gies on MAP performance of CTLM(Q?Q).
(Q?A) MAP (%chg)tf-idf TextRankInitial 0.193525%Rmv 0.2095 (48.3) 0.1733 (?10.4)50%Rmv 0.2085 (47.8) 0.1623 (?16.1)75%Rmv 0.1449 (?25.1) 0.1515 (?21.7)Avg.Score 0.1168 (?39.6) 0.1124 (?41.9)Table 6: Contributions of various word elimination strate-gies on MAP performance of CTLM(Q?A).for improving the performance of question retrievalin the context of language modeling framework.Note that the retrieval performance varies by thetype of training corpus; CTLM(Q?A) has outper-formed CTLM(Q?Q) significantly.
This proves thestatement we made earlier that the (Q?Q) corpuswould contain much noise since the translation mod-els learned from the (Q?Q) corpus tend to havesmaller vocabulary sizes but significantly more aver-age translations per word than the ones learned fromthe (Q?A) corpus.Table 5 and 6 show the effect of various wordelimination strategies on the retrieval performanceof CTLMs in which the incorporated compact trans-lation models are trained from the (Q?Q) corpus andthe (Q?A) corpus, respectively.
It is interesting tonote that the importance of modifications in wordelimination strategies also varies by the type of train-ing corpus.The retrieval results indicate that when the trans-lation model is trained from the ?less noisy?
(Q?A)corpus, eliminating a relatively large proportions ofwords may hurt the retrieval performance of CTLM.In the case when the translation model is trainedfrom the ?noisy?
(Q?Q) corpus, a better retrievalperformance may be achieved if words are elimi-nated appropriately to a certain extent.In terms of weighting scheme, the TextRank ap-proach, which is more ?strict?
than tf-idf in elim-inating unimportant words, has led comparativelyhigher retrieval performances on all levels of re-moval quantity when the translation model has beentrained from the ?noisy?
(Q?Q) corpus.
On the con-trary, the ?less strict?
tf-idf approach has led betterperformances when the translation model has beentrained from the ?less noisy?
(Q?A) corpus.In summary, the results imply that the perfor-mance of translation-based retrieval models can besignificantly improved when strategies for buildingof compact translation models are chosen properly,regarding the expected noise level of the parallel cor-pus for training the translation models.
In a casewhere a noisy parallel corpus is given for trainingof translation models, it is better to get rid of noiseas much as possible by using ?strict?
term weight-ing algorithms; when a less noisy parallel corpus isgiven for building the translation models, a tolerantapproach would yield better retrieval performance.6 Related WorksOur work is most closely related to Jeon etal.
(2005)?s work, which addresses the issue ofword mismatch between queries and questions inlarge online Q&A collections by using translation-based methods.
Apart from their work, there havebeen some related works on applying translation-based methods for retrieving FAQ data.
Berger etal.
(2000) report some of the earliest work on FAQretrieval using statistical retrieval models, includ-ing translation-based approaches, with a small setof FAQ data.
Soricut and Brill (2004) present an an-swer passage retrieval system that is trained from 1million FAQs collected from the Web using trans-lation methods.
Riezler et al (2007) demonstratethe advantages of translation-based approach to an-swer retrieval by utilizing a more complex trans-lation model also trained from a large amount ofdata extracted from FAQs on the Web.
Although allof these translation-based approaches are based onthe statistical translation models, including the IBMModel 1, none of them focus on addressing the noiseissues in translation models.4177 Conclusion and Future WorkBridging the query-question gap has been a major is-sue in retrieval models for large online Q&A collec-tions.
In this paper, we have shown that the perfor-mance of translation-based retrieval on real onlineQ&A collections can be significantly improved byusing compact translation models of which the noise(unimportant word translations) is properly reduced.We have also observed that the performance en-hancement may be achieved by choosing the appro-priate strategies regarding the strictness of variousterm weighting algorithms and the expected noiselevel of the parallel data for learning such transla-tion models.Future work will focus on testing the effective-ness of the proposed method on a larger set of Q&Acollections with broader domains.
Since the pro-posed approach cannot handle many-to-one or one-to-many word transformations, we also plan to in-vestigate the effectiveness of phrase-based transla-tion models in closing gaps between queries andquestions for further enhancement of Q&A retrieval.AcknowledgmentsThis work was supported by Microsoft ResearchAsia.
Any opinions, findings, and conclusions orrecommendations expressed above are those of theauthors and do not necessarily reflect the views ofthe sponsor.ReferencesAdam Berger, Rich Caruana, David Cohn, Dayne Fre-itag, and Vibhu Mittal.
2000.
Bridging the LexicalChasm: Statistical Approaches to Answer-Finding.
InProceedings of the 23rd Annual International ACM SI-GIR Conference on Research and Development in In-formation Retrieval, pages 192?199.Adam Berger and John Lafferty.
1999.
Information Re-trieval as Statistical Translation.
In Proceedings of the22nd Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 222?229.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The Mathematicsof Statistical Machine Translation: Parameter Estima-tion.
Computational Linguistics, 19(2):263?311.Chris Buckley and Ellen M. Voorhees.
2000.
EvaluatingEvaluation Measure Stability.
In Proceedings of the23rd Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 33?40.Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding Similar Questions in Large Question and An-swer Archives.
In Proceedings of the 14th ACM Inter-national Conference on Information and KnowledgeManagement, pages 84?90.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press.Rada Mihalcea and Paul Tarau.
2004.
TextRank: Bring-ing Order into Text.
In Proceedings of the 2004 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 404?411.Franz J. Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51.Jay M. Ponte and W. Bruce Croft.
1998.
A LanguageModeling Approach to Information Retrieval.
In Pro-ceedings of the 21st Annual International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, pages 275?281.Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-taridis, Vibhu Mittal, and Yi Liu.
2007.
StatisticalMachine Translation for Query Expansion in AnswerRetrieval.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics,pages 464?471.Radu Soricut and Eric Brill.
2004.
Automatic QuestionAnswering: Beyond the Factoid.
In Proceedings ofthe 2004 Human Language Technology and Confer-ence of the North American Chapter of the Associationfor Computational Linguistics, pages 57?64.418
