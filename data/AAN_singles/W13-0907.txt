Proceedings of the First Workshop on Metaphor in NLP, pages 52?57,Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational LinguisticsIdentifying Metaphorical Word Use with Tree KernelsDirk Hovy1 Shashank Srivastava2 Sujay Kumar Jauhar2 Mrinmaya Sachan2Kartik Goyal2 Huiying Li2 Whitney Sanders2 Eduard Hovy2(1) ISI, University of Southern California, Marina del Rey(2) LTI, Carnegie Mellon University, Pittsburghdirkh@isi.edu, {shashans,sjauhar,mrinmays,kartikgo,huiyingl,wsanders,hovy}@cs.cmu.eduAbstractA metaphor is a figure of speech that refersto one concept in terms of another, as in ?Heis such a sweet person?.
Metaphors are ubiq-uitous and they present NLP with a rangeof challenges for WSD, IE, etc.
Identifyingmetaphors is thus an important step in lan-guage understanding.
However, since almostany word can serve as a metaphor, they areimpossible to list.
To identify metaphoricaluse, we assume that it results in unusual se-mantic patterns between the metaphor and itsdependencies.
To identify these cases, we useSVMs with tree-kernels on a balanced corpusof 3872 instances, created by bootstrappingfrom available metaphor lists.1 We outper-form two baselines, a sequential and a vector-based approach, and achieve an F1-score of0.75.1 IntroductionA metaphor is a figure of speech used to transferqualities of one concept to another, as in ?He issuch a sweet person?.
Here, the qualities of ?sweet?
(the source) are transferred to a person (the target).Traditionally, linguistics has modeled metaphors asa mapping from one domain to another (Lakoff andJohnson, 1980).Metaphors are ubiquitous in normal language andpresent NLP with a range of challenges.
First, due totheir very nature, they cannot be interpreted at facevalue, with consequences for WSD, IE, etc.
Second,metaphors are very productive constructions, andalmost any word can be used metaphorically (e.g.,1Available at http://www.edvisees.cs.cmu.edu/metaphordata.tar.gz?This is the Donald Trump of sandwiches.?).
Thisproperty makes them impossible to pre-define orlist.
Third, repeated use of a metaphor eventu-ally solidifies it into a fixed expression with themetaphorical meaning now accepted as just anothersense, no longer recognized as metaphorical at all.This gradient makes it hard to determine a boundarybetween literal and metaphorical use of some ex-pressions.
Identifying metaphors is thus a difficultbut important step in language understanding.2Since many words can be productively used asnew metaphors, approaches that try to identifythem based on lexical features alone are bound tobe unsuccessful.
Some approaches have thereforesuggested considering distributional propertiesand ?abstractness?
of the phrase (Turney et al2011).
This nicely captures the contextual natureof metaphors, but their ubiquity makes it impossibleto find truly ?clean?
data to learn the separatedistributions of metaphorical and literal use foreach word.
Other approaches have used pre-definedmappings from a source to a target domain, as in?X is like Y?, e.g., ?emotions are like temperature?
(Mason, 2004).
These approaches tend to do wellon the defined mappings, but they do not generalizeto new, creative metaphors.
It is doubtful that itis feasible to list all possible mappings, so theseapproaches remain brittle.In contrast, we do not assume any predefinedmappings.
We hypothesize instead that if we inter-preted every word literally, metaphors will manifestthemselves as unusual semantic compositions.Since these compositions most frequently occur2Shutova (2010) distinguishes between metaphor identifica-tion (which she calls recognition) and interpretation.
We aresolely concerned with the former.52in certain syntactic relations, they are usually con-sidered semantic preference violations; e.g., in themetaphorical ?You will have to eat your words?, thefood-related verb heads a noun of communication.In contrast, with the literal sense of ?eat?
in ?Youwill have to eat your peas?, it heads a food noun.This intuition is the basis of the approaches in(Iverson and Helmreich, 1991; Krishnakumaranand Zhu, 2007; Baumer et al 2010; Turney etal., 2011).3 We generalize this intuition beyondpreference selections of verbs and relational nouns.Given enough labeled examples of a word, weexpect to find distinctive differences in the compo-sitional behavior of its literal and metaphorical usesin certain preferred syntactic relationships.
If wecan learn to detect such differences/anomalies, wecan reliably identify metaphors.
Since we expectthese patterns in levels other than the lexical level,the approach expands well to creative metaphors.The observation that the anomaly tends to occurbetween syntactically related words makes depen-dency tree kernels a natural fit for the problem.
Treekernels have been successfully applied to a widerange of NLP tasks that involve (syntactic) relations(Culotta and Sorensen, 2004; Moschitti, 2006; Qianet al 2008; Giuliano et al 2009; Mirroshandel etal., 2011).Our contributions in this paper are:?
we annotate and release a corpus of 3872 in-stances for supervised metaphor classification?
we are the first to use tree kernels for metaphoridentification?
our approach achieves an F1-score of 0.75, thebest score of of all systems tested.2 Data2.1 AnnotationWe downloaded a list of 329 metaphor examplesfrom the web4.
For each expression, we extractedsentences from the Brown corpus that containedthe seed (see Figure 1 for an example).
To decide3A similar assumption can be used to detect the literal/non-literal uses of idioms (Fazly et al 2009).4http://www.metaphorlist.com and http://www.macmillandictionaryblog.comwhether a particular instance is used metaphorically,we set up an annotation task on Amazon MechanicalTurk (AMT).Annotators were asked to decide whether ahighlighted expression in a sentence was usedmetaphorically or not (see Figure 2 for a screen-shot).
They were prompted to think about whetherthe expression was used in its original meaning.5In some cases, it is not clear whether an expressionis used metaphorically or not (usually in shortsentences such as ?That?s sweet?
), so annotatorscould state that it was not possible to decide.
Wepaid $0.09 for each set of 10 instances.Each instance was annotated by 7 annotators.Instances where the annotators agreed that it wasimpossible to tell whether it is a metaphor or notwere discarded.
Inter-annotator agreement was0.57, indicating a difficult task.
In order to get thelabel for each instance, we weighted the annotator?sanswers using MACE (Hovy et al 2013), animplementation of an unsupervised item-responsemodel.
This weighted voting produces more reliableestimates than simple majority voting, since it iscapable of sorting out unreliable annotators.
Thefinal corpus consisted of 3872 instances, 1749 ofthem labeled as metaphors.Figure 2: Screenshot of the annotation interface on Ama-zon?s Mechanical TurkWe divided the data into training, dev, and testsets, using a 80-10-10 split.
All results reportedhere were obtained on the test set.
Tuning anddevelopment was only carried out on the dev set.2.2 Vector Representation of WordsThe same word may occur in a literal and ametaphorical usage.
Lexical information alone is5While this is somewhat imprecise and not always easy todecide, it proved to be a viable strategy for untrained annotators.53A bright idea.?
Peter is the bright , sympathetic guy when you ?re doing a deal , ?
says one agent .
yesBelow he could see the bright torches lighting the riverbank .
noHer bright eyes were twinkling .
yesWashed , they came out surprisingly clear and bright .
noFigure 1: Examples of a metaphor seed, the matching Brown sentences, and their annotationsthus probably not very helpful.
However, we wouldlike to capture semantic aspects of the word andrepresent it in an expressive way.
We use the exist-ing vector representation SENNA (Collobert et al2011) which is derived from contextual similarity.In it, semantically similar words are representedby similar vectors, without us having to definesimilarity or looking at the word itself.
In initialtests, these vectors performed better than binaryvectors straightforwardly derived from features ofthe word in context.2.3 Constructing Treesa) b) c)likeI peoplethe sweet inBostonNNSDT JJ INn.groupO adj.all ONNP n.locationVBPRPv.emotionOFigure 3: Graphic demonstration of our approach.
a) de-pendency tree over words, with node of interest labeled.b) as POS representation.
c) as supersense representationThe intuition behind our approach is thatmetaphorical use differs from literal use in certainsyntactic relations.
For example, the only differencebetween the two sentences ?I like the sweet peoplein Boston?
and ?I like the sweet pies in Boston?
isthe head of ?sweet?.
Our assumption is that?givenenough examples?certain patterns emerge (e.g.,that ?sweet?
in combination with food nouns isliteral, but is metaphorical if governed by a noundenoting people).We assume that these patterns occur on differentlevels, and mainly between syntactically relatedwords.
We thus need a data representation tocapture these patterns.
We borrow its structure fromdependency trees, and the different levels fromvarious annotations.
We parse the input sentencewith the FANSE parser (Tratz and Hovy, 2011)6.
Itprovides the dependency structure, POS tags, andother information.To construct the different tree representations,we replace each node in the tree with its word,lemma, POS tag, dependency label, or supersense(the WordNet lexicographer name of the word?sfirst sense (Fellbaum, 1998)), and mark the wordin question with a special node.
See Figure 3 fora graphical representation.
These trees are used inaddition to the vectors.This approach is similar to the ones described in(Moschitti et al 2006; Qian et al 2008; Hovy etal., 2012).2.4 Classification ModelsA tree kernel is simply a similarity matrix over treeinstances.
It computes the similarity between twotrees T1, T2 based on the number of shared subtrees.We want to make use of the information en-coded in the different tree representations duringclassification, i.e., a forest of tree kernels.
We thuscombine the contributions of the individual treerepresentation kernels via addition.
We use kernelsover the lemma, POS tag, and supersense treerepresentations, the combination which performedbest on the dev set in terms of accuracy.We use the SVMlight TK implementation byMoschitti (2006).7 We left most parameters setto default values, but tuned the weight of thecontribution of the trees and the cost factor on thedev set.
We set the multiplicative constant for thetrees to 2.0, and the cost factor for errors on positiveexamples to 1.7.6http://www.isi.edu/publications/licensed-sw/fanseparser/index.html7http://disi.unitn.it/moschitti/Tree-Kernel.htm54If we assume any word can be used metaphori-cally, we ultimately want to label every word in asentence, so we also evaluate a sequential model, inthis case a CRF.
We use CRFsuite (Okazaki, 2007)8to implement the CRF, and run it with averagedperceptron.
While the CRF produces labels forevery word, we only evaluate on the words thatwere annotated in our corpus (to make it maximallycomparable), and use the same representations(lemma, POS and SST) of the word and its parentas features as we did for the SVM.
Training methodand feature selection were again tuned on the devset to maximize accuracy.3 Experimentssystem acc P R F1BLall 0.49 0.49 1.0 0.66BLmost freq.
class 0.70 0.66 0.65 0.65CRF 0.69?
0.74?
0.50 0.59SVMvector?only 0.70?
0.63?
0.80 0.71SVM+tree 0.75?
0.70?
0.80 0.75?Table 1: Accuracy, precision, recall, and F1 for varioussystems on the held-out test set.
Values significantly bet-ter than baseline at p < .02 are marked ?
(two-tailed t-test).We compare the performance of two baselines,the CRF model, vanilla SVM, and SVM with treekernels and report accuracy, precision, recall, andF1 (Table 1).The first baseline (BLall) labels every instanceas metaphor.
Its accuracy and precision reflect themetaphor ratio in the data, and it naturally achievesperfect recall.
This is a rather indiscriminateapproach and not very viable in practice, so wealso apply a more realistic baseline, labeling eachword with the class it received most often in thetraining data (BLmost freq.
class ).
This is essentiallylike assuming that every word has a default class.Accuracy and precision for this baseline are muchbetter, although recall naturally suffers.The CRF improves in terms of accuracy andprecision, but lacks the high recall the baselinehas, resulting in a lower F1-score.
It does yield8http://www.chokkan.org/software/crfsuite/the highest precision of all models, though.
Sowhile not capturing every metaphor in the data, it isusually correct if it does label a word as metaphor.SVMlight allows us to evaluate the performanceof a classification using only the vector representa-tion (SVMvector?only).
This model achieves betteraccuracy and recall than the CRF, but is less precise.Accuracy is the same as for the most-frequent-class baseline, indicating that the vector-basedSVM learns to associate a class with each lexicalitem.
Once we add the tree kernels to the vector(SVM+tree), we see considerable gains in accuracyand precision.
This confirms our hypothesis thatmetaphors are not only a lexical phenomenon, butalso a product of the context a word is used in.
Thecontextual interplay with their dependencies createspatterns that can be exploited with tree kernels.We note that the SVM with tree kernels is the onlysystem whose F1 significantly improves over thebaseline (at p < .02).Testing with one tree representation at a time,we found the various representations differ in termsof informativeness.
Lemma, POS, and supersenseperformed better than lexemes or dependency labels(when evaluated on the dev set) and were thus usedin the reported system.
Combining more than onerepresentation in the same tree to form compoundleaves (e.g.
lemma+POS, such as ?man-NN?
)performed worse in all combinations tested.
Weomit further details here, since the combinatorics ofthese tests are large and yield only little insight.Overall, our results are similar to comparablemethods on balanced corpora, and we encouragethe evaluation of other methods on our data set.4 Related WorkThere is plenty of research into metaphors.
Whilemany are mainly interested in their general proper-ties (Shutova, 2010; Nayak, 2011), we focus on theones that evaluate their results empirically.Gedigian et al(2006) use a similar approachto identify metaphors, but focus on frames.
Theircorpus is with about 900 instances relatively small.They improve over the majority baseline, but onlyreport accuracy.
Both their result and the baselineare in the 90s, which might be due to the highnumber of metaphors (about 90%).
We use a larger,55more balanced data set.
Since accuracy can beuninformative in cases of unbalanced data sets, wealso report precision, recall, and F1.Krishnakumaran and Zhu (2007) also use se-mantic relations between syntactic dependenciesas basis for their classification.
They do not aim todistinguish literal and metaphorical use, but try todifferentiate various types of metaphors.
They use acorpus of about 1700 sentences containing differentmetaphors, and report a precision of 0.70, recall of0.61 (F1 = 0.65), and accuracy of 0.58.Birke and Sarkar (2006) and Birke and Sarkar(2007) present unsupervised and active learningapproaches to classifying metaphorical and literalexpressions, reporting F1 scores of 0.54 and 0.65,outperforming baseline approaches.
Unfortunately,as they note themselves, their data set is ?not largeenough to [...] support learning using a supervisedlearning method?
(Birke and Sarkar, 2007, 22),which prevents a direct comparison.Similarly to our corpus construction, (Shutova etal., 2010) use bootstrapping from a small seed set.They use an unsupervised clustering approach toidentify metaphors and report a precision of 0.79,beating the baseline system by a wide margin.
Dueto the focus on corpus construction, they cannotprovide recall or F1.
Their approach considers onlypairs of a single verbs and nouns, while we allowfor any syntactic combination.Tree kernels have been applied to a wide va-riety of NLP tasks (Culotta and Sorensen, 2004;Moschitti et al 2006; Qian et al 2008; Hovy etal., 2012).
They are specifically adept in capturinglong-range syntactic relationships.
In our case, weuse them to detect anomalies in syntactic relations.5 ConclusionUnder the hypothesis that the metaphorical use of aword creates unusual patterns with its dependencies,we presented the first tree-kernel based approachto metaphor identification.
Syntactic dependenciesallow us to capture those patterns at differentlevels of representations and identify metaphoricaluse more reliably than non-kernel methods.
Weoutperform two baselines, a sequential model, andpurely vector-based SVM approaches, and reach anF1 of 0.75.
Our corpus is available for downloadat http://www.edvisees.cs.cmu.edu/metaphordata.tar.gz and we encourage theresearch community to evaluate other methods on it.AcknowledgementsThe authors would like to thank the reviewers forhelping us clarify several points and giving con-structive input that helped to improve the quality ofthis paper.
This work was (in part) supported bythe Intelligence Advanced Research Projects Activ-ity (IARPA) via Department of Defense US ArmyResearch Laboratory contract number W911NF-12-C-0025.
The U.S. Government is authorized to re-produce and distribute reprints for Governmentalpurposes notwithstanding any copyright annotationthereon.
Disclaimer: The views and conclusionscontained herein are those of the authors and shouldnot be interpreted as necessarily representing the of-ficial policies or endorsements, either expressed orimplied, of IARPA, DoD/ARL, or the U.S. Govern-ment.ReferencesEric P.S.
Baumer, James P. White, and Bill Tomlinson.2010.
Comparing semantic role labeling with typeddependency parsing in computational metaphor identi-fication.
In Proceedings of the NAACL HLT 2010 Sec-ond Workshop on Computational Approaches to Lin-guistic Creativity, pages 14?22.
Association for Com-putational Linguistics.Julia Birke and Anoop Sarkar.
2006.
A clustering ap-proach for the nearly unsupervised recognition of non-literal language.
In Proceedings of EACL, volume 6,pages 329?336.Julia Birke and Anoop Sarkar.
2007.
Active learning forthe identification of nonliteral language.
In Proceed-ings of the Workshop on Computational Approachesto Figurative Language, pages 21?28.
Association forComputational Linguistics.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.Journal of Machine Learning Research, 12:2493?2537.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, page 423.
Association for Com-putational Linguistics.56Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.2009.
Unsupervised type and token identificationof idiomatic expressions.
Computational Linguistics,35(1):61?103.Christiane Fellbaum.
1998.
WordNet: an electronic lexi-cal database.
MIT Press USA.Matt Gedigian, John Bryant, Srini Narayanan, and Bran-imir Ciric.
2006.
Catching metaphors.
In Proceedingsof the 3rd Workshop on Scalable Natural LanguageUnderstanding, pages 41?48.Claudio Giuliano, Alfio Massimiliano Gliozzo, and CarloStrapparava.
2009.
Kernel methods for minimally su-pervised wsd.
Computational Linguistics, 35(4).Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Patward-han, and Christopher Welty.
2012.
When Did thatHappen?
?
Linking Events and Relations to Times-tamps.
In Proceedings of EACL.Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,and Eduard Hovy.
2013.
Learning Whom to trust withMACE.
In Proceedings of NAACL HLT.Eric Iverson and Stephen Helmreich.
1991.
Non-literalword sense identification through semantic networkpath schemata.
In Proceedings of the 29th annualmeeting on Association for Computational Linguistics,pages 343?344.
Association for Computational Lin-guistics.Saishuresh Krishnakumaran and Xiaojian Zhu.
2007.Hunting elusive metaphors using lexical resources.
InProceedings of the Workshop on Computational ap-proaches to Figurative Language, pages 13?20.
Asso-ciation for Computational Linguistics.George Lakoff and Mark Johnson.
1980.
Metaphors welive by, volume 111.
University of Chicago Press.Zachary J. Mason.
2004.
CorMet: a computational,corpus-based conventional metaphor extraction sys-tem.
Computational Linguistics, 30(1):23?44.Seyed A. Mirroshandel, Mahdy Khayyamian, and Gho-lamreza Ghassem-Sani.
2011.
Syntactic tree ker-nels for event-time temporal relation learning.
HumanLanguage Technology.
Challenges for Computer Sci-ence and Linguistics, pages 213?223.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2006.
Tree kernel engineering for propositionre-ranking.
MLG 2006, page 165.Alessandro Moschitti.
2006.
Making Tree Kernels Prac-tical for Natural Language Learning.
In In Proceed-ings of the 11th Conference of the European Chapterof the Association for Computational Linguistics.Sushobhan Nayak.
2011.
Towards a grounded modelfor ontological metaphors.
In Student Research Work-shop, pages 115?120.Naoaki Okazaki.
2007.
CRFsuite: a fast implementationof Conditional Random Fields (CRFs).Longhua Qian, Guodong Zhou, Fang Kong, QiaomingZhu, and Peide Qian.
2008.
Exploiting constituentdependencies for tree kernel-based semantic relationextraction.
In Proceedings of the 22nd InternationalConference on Computational Linguistics-Volume 1,pages 697?704.
Association for Computational Lin-guistics.Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010.Metaphor identification using verb and noun cluster-ing.
In Proceedings of the 23rd International Confer-ence on Computational Linguistics, pages 1002?1010.Association for Computational Linguistics.Ekaterina Shutova.
2010.
Models of metaphor in nlp.
InProceedings of the 48th Annual Meeting of the Associ-ation for Computational Linguistics, pages 688?697.Association for Computational Linguistics.Stephen Tratz and Eduard Hovy.
2011.
A fast, accurate,non-projective, semantically-enriched parser.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 1257?1268.
As-sociation for Computational Linguistics.Peter D. Turney, Yair Neuman, Dan Assaf, and YohaiCohen.
2011.
Literal and metaphorical sense iden-tification through concrete and abstract context.
InProceedings of the 2011 Conference on the EmpiricalMethods in Natural Language Processing, pages 680?690.57
