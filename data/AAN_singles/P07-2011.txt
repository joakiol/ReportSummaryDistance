Proceedings of the ACL 2007 Demo and Poster Sessions, pages 41?44,Prague, June 2007. c?2007 Association for Computational LinguisticsAn efficient algorithm for building a distributional thesaurus (and otherSketch Engine developments)Pavel Rychly?Masaryk UniversityBrno, Czech Republicpary@fi.muni.czAdam KilgarriffLexical Computing LtdBrighton, UKadam@lexmasterclass.comAbstractGorman and Curran (2006) argue that the-saurus generation for billion+-word corporais problematic as the full computation takesmany days.
We present an algorithm withwhich the computation takes under twohours.
We have created, and made pub-licly available, thesauruses based on largecorpora for (at time of writing) seven majorworld languages.
The development is imple-mented in the Sketch Engine (Kilgarriff etal., 2004).Another innovative development in the sametool is the presentation of the grammaticalbehaviour of a word against the backgroundof how all other words of the same wordclass behave.
Thus, the English noun con-straint occurs 75% in the plural.
Is thisa salient lexical fact?
To form a judge-ment, we need to know the distribution forall nouns.
We use histograms to present thedistribution in a way that is easy to grasp.1 Thesaurus creationOver the last ten years, interest has been growingin distributional thesauruses (hereafter simply ?the-sauruses?).
Following initial work by (Spa?rck Jones,1964) and (Grefenstette, 1994), an early, online dis-tributional thesaurus presented in (Lin, 1998) hasbeen widely used and cited, and numerous authorssince have explored thesaurus properties and param-eters: see survey component of (Weeds and Weir,2005).A thesaurus is created by?
taking a corpus?
identifying contexts for each word?
identifying which words share contexts.For each word, the words that share most contexts(according to some statistic which also takes accountof their frequency) are its nearest neighbours.Thesauruses generally improve in accuracy withcorpus size.
The larger the corpus, the more clearlythe signal (of similar words) will be distinguishedfrom the noise (of words that just happen to sharea few contexts).
Lin?s was based on around 300Mwords and (Curran, 2004) used 2B (billion).A direct approach to thesaurus computation looksat each word and compares it with each other word,checking all contexts to see if they are shared.
Thus,complexity is O(n2m) where n in the number oftypes and m is the size of the context vector.
Thenumber of types increases with the corpus size, and(Ravichandran et al, 2005) propose heuristics forthesaurus building without undertaking the completecalculation.
The line of reasoning is explored furtherby (Gorman and Curran, 2006), who argue that thecomplete calculation is not realistic given large cor-pora.
They estimate that, given a 2B corpus and its184,494-word vocabulary comprising all words oc-curring over five times, the full calculation will takenearly 300 days.
With the vocabulary limited to the75,800 words occuring over 100 times, the calcula-tion took 18 days.The naive algorithm has complexity O(n2m) butthis is not the complexity of the problem.
Most of41the n2 word pairs have nothing in common so thereis no reason to check them.
We proceed by workingonly with those word pairs that do have something incommon.
This allows us to create thesauruses from1B corpora in under 2 hours.1.1 AlgorithmWe prepare the corpus by lemmatizing and thenshallow parsing to identify grammatical relation in-stances with the form ?w1, r, w?
?, where r is agrammatical relation, w1 and w?
are words.
Wecount the frequency of each triple and sort all?w1, r, w?, score?
4-tuples by ?contexts?
where acontext is a ?r, w??
pair.
Only 4-tuples with positivescore are included.The algorithm then loops over each context(CONTEXTS is the set of all contexts):for ?r, w??
in CONTEXTS:WLIST = set of all w where ?w, r,w??
existsfor w1 in WLIST:for w2 in WLIST:sim(w1, w2)+ = f(frequencies)1The outer loop is linear in the number of contexts.The inner loop is quadratic in the number of wordsin WLIST, that is, the number of words sharing aparticular context ?r, w??.
This list is usually small(less than 1000), so the quadratic complexity is man-ageable.We use a heuristic at this point.
If WLIST hasmore than 10,000 members, the context is skipped.Any such general context is very unlikely to makea substantial difference to the similarity score, sincesimilarity scores are weighted according to how spe-cific they are.
The computational work avoided canbe substantial.The next issue is how to store the wholesim(w1, w2) matrix.
Most of the values are verysmall or zero.
These values are not stored in thefinal thesaurus but they are needed during the com-putation.
A strategy for this problem is to gener-ate, sort and sum in sequential scan.
That meansthat instead of incrementing the sim(w1, w2) scoreas we go along, we produce ?w1, w2, x?
triples ina very long list, running, for a billion-word corpus,1In this paper we do not discuss the nature of this functionas it is does not impact on the complexity.
It is explored exten-sively in (Curran, 2004; Weeds and Weir, 2005).into hundreds of GB.
For such huge data, a variantof TPMMS (Two Phase Multi-way Merge Sort) isused.
First we fill the whole available memory witha part of the data, sort in memory (summing wherewe have multiple instances of the same ?w1, w2?
aswe proceed) and output the sorted stream.
Then wemerge sorted streams, again summing as we pro-ceed.Another technique we use is partitioning.
Theouter loop of the algorithm is fast and can be runseveral times with a limit on which words to processand output.
For example, the first run processes onlyword pairs ?w1, w2?
where the ID of w1 is between0 and 99, the next, where it is between 100 and 199,etc.
In such limited runs there is a high probabilitythat most of the summing is done in memory.
We es-tablish a good partitioning with a dry run in which aplan is computed such that all runs produce approxi-mately the number of items which can be sorted andsummed in memory.1.2 ExperimentsWe experimented with the 100M-word BNC2, 1B-word Oxford English Corpus3 (OEC), and 1.9B-word Itwac (Baroni and Kilgarriff, 2006).All experiments were carried out on a machinewith AMD Opteron quad-processor.
The machinehas 32 GB of RAM but each process used only1GB (and changing this limit produced no signifi-cant speedup).
Data files were on a Promise diskarray running Disk RAID5.Parameters for the computation include:?
hits threshold MIN: only words entering into anumber of triples greater than MIN will havethesaurus entries, or will be candidates for be-ing in other words?
thesaurus entries.
(Notethat words not passing this threshold can stillbe in contexts, so may contribute to the simi-larity of two other words: cf Daelemans et al?stitle (1999).)?
the number of words (WDS) above the thresh-old2http://www.natcorp.ox.ac.uk3http://www.askoxford.com/oec/ We are grateful to OxfordUniversity Press for permission to use the OEC.42Corp MIN WDS TYP CTX TIMEBNC 1 152k 5.7m 608k 13m 9sBNC 20 68k 5.6m 588k 9m 30sOEC 2 269k 27.5m 994k 1hr 40mOEC 20 128k 27.3m 981k 1hr 27mOEC 200 48k 26.7m 965k 1hr 10mItwac 20 137k 24.8m 1.1m 1hr 16mTable 1: Thesaurus creation jobs and timings?
the number of triples (types) that these wordsoccur in (TYP)?
the number of contexts (types) that these wordsoccur in (CTX)We have made a number of runs with differentvalues of MIN for BNC, OEC and Itwac and presentdetails for some representative ones in Table 1.For the BNC, the number of partitions that the TP-MMS process was divided into was usually betweenten and twenty; for the OEC and ITwac it was around200.For the OEC, the heuristic came into play and, ina typical run, 25 high-frequency, low-salience con-texts did not play a role in the theasurus compu-tation.
They included: modifier?more; modifier?not; object-of?have; subject-of?have.
In Gormanand Curran, increases in speed were made at sub-stantial cost to accuracy.
Here, data from these high-frequency contexts makes negligible impact on the-saurus entries.1.3 Available thesaurusesThesauruses of the kind described are pub-licly available on the Sketch Engine server(http://www.sketchengine.co.uk) based on corporaof between 50M and 2B words for, at time of writ-ing, Chinese, English, French, Italian, Japanese,Portuguese, Slovene and Spanish.2 Histograms for presenting statisticalfacts about a word?s grammar75% of the occurrences of the English noun con-straint in the BNC are in the plural.
Many dictio-naries note that some nouns are usually plural: thequestion here is, how salient is the fact about con-Figure 1: Distribution of nouns with respect to pro-portion of instances in plural, from 0 to 1 in 10 steps,with the class that constraint is in, in white.straint?45To address it we need to know not only the propor-tion for constraint but also the proportion for nounsin general.
If the average, across nouns, is 50% thenit is probably not noteworthy.
But if the average is2%, it is.
If it is 30%, we may want to ask a morespecific question: for what proportion of nouns is thepercentage higher than 75%.
We need to view ?75%plural?
in the context of the whole distribution.All the information is available.
We can deter-mine, in a large corpus such as the BNC, for eachnoun lemma with more than (say) fifty occurrences,what percentage is plural.
We present the data in ahistogram: we count the nouns for which the propor-tion is between 0 and 0.1, 0.1 and 0.2, .
.
.
, 0.9 and1.
The histogram is shown in Fig 1, based on the14,576 nouns with fifty or more occurrences in theBNC.
(The first column corresponds to 6113 items.
)We mark the category containing the item of inter-est, in red (white in this paper).
We believe this isan intuitive and easy-to-interpret way of presentinga word?s relative frequency in a particular grammat-ical context, against the background of how otherwords of the same word class behave.We have implemented histograms like these in theSketch Engine for a range of word classes and gram-matical contexts.
The histograms are integrated into4Other 75% plural nouns which might have served as theexample include: activist bean convulsion ember feminist intri-cacy joist mechanic relative sandbag shutter siding teabag tes-ticle trinket tusk.
The list immediately suggests a typology ofusually-plural nouns, indicating how this kind of analysis pro-vokes new questions.5Of course plurals may be salient for one sense but not oth-ers.43the word sketch6 for each word.
(Up until now theinformation has been available but hard to interpret.
)In accordance with the word sketch principle of notwasting screen space, or user time, on uninterestingfacts, histograms are only presented where a word isin the top (or bottom) percentile for a grammaticalpattern or construction.Similar diagrams have been used for similar pur-poses by (Lieber and Baayen, 1997).
This is, webelieve, the first time that they have been offered aspart of a corpus query tool.3 Text type, subcorpora and keywordsWhere a corpus has components of different texttypes, users often ask: ?what words are distinctive ofa particular text type?, ?what are the keywords?
?.7Computations of this kind often give unhelpful re-sults because of the ?lumpiness?
of word distribu-tions: a word will often appear many times in anindividual text, so statistics designed to find wordswhich are distinctively different between text typeswill give high values for words which happen to bethe topic of just one particular text (Church, 2000).(Hlava?c?ova?
and Rychly?, 1999) address the prob-lem through defining ?average reduced frequency?
(ARF), a modified frequency count in which thecount is reduced according to the extent to whichoccurrences of a word are bunched together.The Sketch Engine now allows the user to preparekeyword lists for any subcorpus, either in relation tothe full corpus or in relation to another subcorpus,using a statistic of the user?s choosing and basingthe result either on raw frequency or on ARF.AcknowledgementsThis work has been partly supported by theAcademy of Sciences of Czech Republic under theproject T100300419, by the Ministry of Educationof Czech Republic within the Center of basic re-search LC536 and in the National Research Pro-gramme II project 2C06009.6A word sketch is a one-page corpus-derived account of aword?s grammatical and collocation behaviour.7The well-established WordSmith corpus tool(http://www.lexically.net/wordsmith) has a keywords functionwhich has been very widely used, see e.g., (Berber Sardinha,2000).ReferencesMarco Baroni and Adam Kilgarriff.
2006.
Largelinguistically-processed web corpora for multiple lan-guages.
In EACL.Tony Berber Sardinha.
2000.
Comparing corpora withwordsmith tools: how large must the reference corpusbe?
In Proceedings of the ACL Workshop on Compar-ing Corpora, pages 7?13.Kenneth Ward Church.
2000.
Empirical estimates ofadaptation: The chance of two noriegas is closer top/2 than p2.
In COLING, pages 180?186.James Curran.
2004.
From Distributional to SemanticSimilarity.
Ph.D. thesis, Edinburgh Univesity.Walter Daelemans, Antal van den Bosch, and Jakub Za-vrel.
1999.
Forgetting exceptions is harmful in lan-guage learning.
Machine Learning, 34(1-3).James Gorman and James R. Curran.
2006.
Scaling dis-tributional similarity to large corpora.
In ACL.Gregory Grefenstette.
1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer.Jaroslava Hlava?c?ova?
and Pavel Rychly?.
1999.
Dispersionof words in a language corpus.
In Proc.
TSD (TextSpeech Dialogue), pages 321?324.Adam Kilgarriff, Pavel Rychly?, Pavel Smrz?, and DavidTugwell.
2004.
The sketch engine.
In Proc.
EU-RALEX, pages 105?116.Rochelle Lieber and Harald Baayen.
1997.
Word fre-quency distributions and lexical semantics.
Computersin the Humanities, 30:281?291.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In COLING-ACL, pages 768?774.Deepak Ravichandran, Patrick Pantel, and Eduard H.Hovy.
2005.
Randomized algorithms and nlp: Usinglocality sensitive hash functions for high speed nounclustering.
In ACL.Karen Spa?rck Jones.
1964.
Synonymy and SemanticClassificiation.
Ph.D. thesis, Edinburgh University.Julie Weeds and David J. Weir.
2005.
Co-occurrence re-trieval: A flexible framework for lexical distributionalsimilarity.
Computational Linguistics, 31(4):439?475.44
