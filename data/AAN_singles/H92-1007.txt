SESSION 2: SPOKEN LANGUAGE SYSTEMS IIWayne Ward, ChairSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213Three papers in this session concern gathering spontaneousspeech data.
Two of the papers (AT&T and SRU) used aWizard of Oz paradigm to interact with users viatelephone.
In the WOZ paradigm, subjects are led tobelieve that they are interacting with a machine when anexperimenter is actually performing the machine's task.While this technique isused by many sites collecting spon-taneous peech data, most of the systems use visual dis-plays to present information to users.
These two papersgive some insight into the unique characteristics of theinteractions when the data is presented over a telephone.AT&T collected ata for the Air Travel Information Ser-vice task.
Since several other sites are collecting data forthis same task, direct comparisons can be made betweendata gathered using visual ouput and that using speech out-put over a telephone.
Since the ouput was verbal, particularattention had to be paid to the amount of information thatwas output.
Information was summarized and compressedbefore being given to users.
The AT&T system used theMIT Natural Language Understanding and databaseretrieval modules, so comparisons of data from these twosites is especially useful.
The percentage of utterances thatthe system could not understand was significantly higher inthe AT&T data.
User behavior after errors was similar inthe two systems, but more pronounced in the AT&T data.The rate of false starts and filled pauses was higher forAT&T than for other sites, but this may have been due tothe fact that the experimenter rather than the user con-trolled the recording.
When subjects explicitly press a but-ton before speaking, they may compose utterances morecarefully before they begin.
It is clear that audio inter-action paradigms present a unique set of problems for in-teractive Human-Machine communication.SRU also collected speech data from subjects over atelephone.
This study used a route planning task andfocused on the difference between Human-Human i ter-action and Human-Machine interaction.
One unique fea-tare of this experiment was that users really accessed thesystem in order to get information rather than participatingin an experiment using simulated scenarios.
There weretwo conditions in the experiment, Human-Human andHuman-Machine.
There was no machine used in the H-Mcondition, but callers were induced to believe that theywere talking to a machine by passing the experimenter'sresponse through a "voice disguise unit" to make it soundas if it were produced by a machine.
The only differencebetween the two conditions was the voice alteration.
Astandard opening phrase was spoken to each caller.
Notenough data has been gathered thus far to allow reliablestatistical analysis, but some interesting observations canbe made.
Many callers assumed that the machine knewwhen it was being addressed and made many asideremarks assuming that the machine would ignore them.Much of the dialog in the H-H condition concerned findingout about he capabilities of the service, while dialog in theH-M system was confined to getting route information.Significantly fewer words were spoken in each utterance inthe H-M condition.
SRU plans to conduct a larger versionof this experiment in the near future.
It should prove veryuseful to examine data from users actually performing atask (for real) and to contrast this with the behaviors eenin the simulated scenarios.The third paper which reported on spontaneous speech dataanalyzed the performance of users and their subjective x-periences with a Spoken Language System.
The paper ex-amines how speed/accuracy tradeoffs affect user percep-tions of a system.
Three versions of an ATIS system wereused which represented different speed/accuracy tradeoffs.In a debriefing questionnaire, subjects answered severalyes/no questions regarding system performance.
Answersto questions regarding speed and accuracy were what intui-tion would suggest, given the tradeoff.
One question,"Would you prefer this method to looking up the infor-mation in a book", seems to be more associated with over-all user satisfaction.
User responses to this question werenot significantly different across ystems.The effect of user experience on recognition performancewas also examined.
In general, it was found that userswho had a poor recognition rate on an initial scenarioshowed improved performance on a subsequent scenario.This improvement was correlated with a decrease in theperplexity of the utterances used.
To some degree thesesubjects were able to adapt o the language model of thesystem.
However, subjects with a relatively low initial er-ror rate showed no improvement onthe second scenario.A third experiment investigated the effect of speaking styleon recognition error.
When recognition errors are made,subjects often try to help the system by changing theirspeaking style.
As the authors point out, this degradessystem performance since the system was not trained onthis type of data.
Instructing subjects not to hyperarticulatedid not improve system performance.
The authors uggestthat training data should contain this type of speech so thatthere is a better match between the training and test con-ditions.The fourth paper presents an overview of the research be-ing done at LIMSI-CNRS.
LIMSI has a very broadprogram of research covering Voice Dictation, SpokenDialog, Natural Language Processing and Non Verbal and41Multimodal Communication.
They are pursuing avery am-bitious project using computer vision, natural anguage,knowledge r presentation, speech and gestures.42
