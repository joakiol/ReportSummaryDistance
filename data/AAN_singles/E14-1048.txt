Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 452?461,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsLearning Dictionaries for Named Entity Recognition using MinimalSupervisionArvind NeelakantanDepartment of Computer ScienceUniversity of Massachusetts, AmherstAmherst, MA, 01003arvind@cs.umass.eduMichael CollinsDepartment of Computer ScienceColumbia UniversityNew-York, NY 10027, USAmcollins@cs.columbia.eduAbstractThis paper describes an approach for au-tomatic construction of dictionaries forNamed Entity Recognition (NER) usinglarge amounts of unlabeled data and a fewseed examples.
We use Canonical Cor-relation Analysis (CCA) to obtain lowerdimensional embeddings (representations)for candidate phrases and classify thesephrases using a small number of labeledexamples.
Our method achieves 16.5%and 11.3% F-1 score improvement overco-training on disease and virus NER re-spectively.
We also show that by addingcandidate phrase embeddings as featuresin a sequence tagger gives better perfor-mance compared to using word embed-dings.1 IntroductionSeveral works (e.g., Ratinov and Roth, 2009; Co-hen and Sarawagi, 2004) have shown that inject-ing dictionary matches as features in a sequencetagger results in significant gains in NER perfor-mance.
However, building these dictionaries re-quires a huge amount of human effort and it is of-ten difficult to get good coverage for many namedentity types.
The problem is more severe when weconsider named entity types such as gene, virusand disease, because of the large (and growing)number of names in use, the fact that the names areheavily abbreviated and multiple names are usedto refer to the same entity (Leaman et al., 2010;Dogan and Lu, 2012).
Also, these dictionaries canonly be built by domain experts, making the pro-cess very expensive.This paper describes an approach for automaticconstruction of dictionaries for NER using largeamounts of unlabeled data and a small numberof seed examples.
Our approach consists of twosteps.
First, we collect a high recall, low preci-sion list of candidate phrases from the large unla-beled data collection for every named entity typeusing simple rules.
In the second step, we con-struct an accurate dictionary of named entities byremoving the noisy candidates from the list ob-tained in the first step.
This is done by learning aclassifier using the lower dimensional, real-valuedCCA (Hotelling, 1935) embeddings of the can-didate phrases as features and training it using asmall number of labeled examples.
The classifierwe use is a binary SVM which predicts whether acandidate phrase is a named entity or not.We compare our method to a widely used semi-supervised algorithm based on co-training (Blumand Mitchell, 1998).
The dictionaries are firstevaluated on virus (GENIA, 2003) and disease(Dogan and Lu, 2012) NER by using them directlyin dictionary based taggers.
We also give resultscomparing the dictionaries produced by the twosemi-supervised approaches with dictionaries thatare compiled manually.
The effectiveness of thedictionaries are also measured by injecting dictio-nary matches as features in a Conditional RandomField (CRF) based tagger.
The results indicatethat our approach with minimal supervision pro-duces dictionaries that are comparable to dictio-naries compiled manually.
Finally, we also com-pare the quality of the candidate phrase embed-dings with word embeddings (Dhillon et al., 2011)by adding them as features in a CRF based se-quence tagger.2 BackgroundWe first give background on Canonical Correla-tion Analysis (CCA), and then give background on452CRFs for the NER problem.2.1 Canonical Correlation Analysis (CCA)The input to CCA consists of n paired observa-tions (x1, z1), .
.
.
, (xn, zn) where xi?
Rd1, zi?Rd2(?i ?
{1, 2, .
.
.
, n}) are the feature represen-tations for the two views of a data point.
CCAsimultaneously learns projection matrices ?1?Rd1?k,?2?
Rd2?k(k is a small number) whichare used to obtain the lower dimensional represen-tations (x?1, z?1), .
.
.
, (x?n, z?n) where x?i= ?T1xi?Rk, z?i= ?T2zi?
Rk, ?i ?
{1, 2, .
.
.
, n}.
?1,?2are chosen to maximize the correlation between x?iand z?i, ?i ?
{1, 2, .
.
.
, n}.Consider the setting where we have a label forthe data point along with it?s two views and ei-ther view is sufficient to make accurate predic-tions.
Kakade and Foster (2007) and Sridharanand Kakade (2008) give strong theoretical guaran-tees when the lower dimensional embeddings fromCCA are used for predicting the label of the datapoint.
This setting is similar to the one consideredin co-training (Collins and Singer, 1999) but thereis no assumption of independence between the twoviews of the data point.
Also, it is an exact al-gorithm unlike the algorithm given in Collins andSinger (1999).
Since we are using lower dimen-sional embeddings of the data point for prediction,we can learn a predictor with fewer labeled exam-ples.2.2 CRFs for Named Entity RecognitionCRF based sequence taggers have been used fora number of NER tasks (e.g., McCallum and Li,2003) and in particular for biomedical NER (e.g.,McDonald and Pereira, 2005; Burr Settles, 2004)because they allow a great deal of flexibility in thefeatures which can be included.
The input to aCRF tagger is a sentence (w1, w2, .
.
.
, wn) wherewi, ?i ?
{1, 2, .
.
.
, n} are words in the sentence.The output is a sequence of tags y1, y2, .
.
.
, ynwhere yi?
{B, I, O}, ?i ?
{1, 2, .
.
.
, n}.
Bis the tag given to the first word in a named entity,I is the tag given to all words except the first wordin a named entity and O is the tag given to all otherwords.
We used the standard NER baseline fea-tures (e.g., Dhillon et al., 2011; Ratinov and Roth,2009) which include:?
Current Word wiand its lexical featureswhich include whether the word is capital-ized and whether all the characters are cap-italized.
Prefix and suffixes of the word wiwere also added.?
Word tokens in window of size twoaround the current word which includewi?2, wi?1, wi+1, wi+2and also the capital-ization pattern in the window.?
Previous two predictions yi?1and yi?2.The effectiveness of the dictionaries are evaluatedby adding dictionary matches as features alongwith the baseline features (Ratinov and Roth,2009; Cohen and Sarawagi, 2004) in the CRF tag-ger.
We also compared the quality of the candi-date phrase embeddings with the word-level em-beddings by adding them as features (Dhillon etal., 2011) along with the baseline features in theCRF tagger.3 MethodThis section describes the two steps in our ap-proach: obtaining candidate phrases and classify-ing them.3.1 Obtaining Candidate PhrasesWe used the full text of 110,369 biomedical pub-lications in the BioMed Central corpus1to get thehigh recall, low precision list of candidate phrases.The advantages of using this huge collection ofpublications are obvious: almost all (includingrare) named entities related to the biomedical do-main will be mentioned and contains more re-cent developments than a structured resource likeWikipedia.
The challenge however is that thesepublications are unstructured and hence it is a dif-ficult task to construct accurate dictionaries usingthem with minimal supervision.The list of virus candidate phrases were ob-tained by extracting phrases that occur between?the?
and ?virus?
in the simple pattern ?the ...virus?
during a single pass over the unlabeled doc-ument collection.
This noisy list had a lot of virusnames such as influenza, human immunodeficiencyand Epstein-Barr along with phrases that are notvirus names, like mutant, same, new, and so on.A similar rule like ?the ... disease?
did not givea good coverage of disease names since it is notthe common way of how diseases are mentionedin publications.
So we took a different approach1The corpus can be downloaded athttp://www.biomedcentral.com/about/datamining453to obtain the noisy list of disease names.
We col-lected every sentence in the unlabeled data col-lection that has the word ?disease?
in it and ex-tracted noun phrases2following the patterns ?dis-eases like ....?, ?diseases such as ....?
, ?diseases in-cluding ....?
, ?diagnosed with ....?, ?patients with....?
and ?suffering from ....?.3.2 Classification of Candidate PhrasesHaving found the list of candidate phrases, wenow describe how noisy words are filtered outfrom them.
We gather (spelling, context) pairs forevery instance of a candidate phrase in the unla-beled data collection.
spelling refers to the can-didate phrase itself while context includes threewords each to the left and the right of the candidatephrase in the sentence.
The spelling and the con-text of the candidate phrase provide a natural splitinto two views which multi-view algorithms likeco-training and CCA can exploit.
The only super-vision in our method is to provide a few spellingseed examples (10 in the case of virus, 18 in thecase of disease), for example, human immunodefi-ciency is a virus and mutant is not a virus.3.2.1 Approach using CCA embeddingsWe use CCA described in the previous sectionto obtain lower dimensional embeddings for thecandidate phrases using the (spelling, context)views.
Unlike previous works such as Dhillon etal.
(2011) and Dhillon et al.
(2012), we use CCA tolearn embeddings for candidate phrases instead ofall words in the vocabulary so that we don?t missnamed entities which have two or more words.Let the number of (spelling, context) pairs be n(sum of total number of instances of every can-didate phrase in the unlabeled data collection).First, we map the spelling and context to high-dimensional feature vectors.
For the spelling view,we define a feature for every candidate phrase andalso a boolean feature which indicates whether thephrase is capitalized or not.
For the context view,we use features similar to Dhillon et al.
(2011)where a feature for every word in the context inconjunction with its position is defined.
Eachof the n (spelling, context) pairs are mapped toa pair of high-dimensional feature vectors to getn paired observations (x1, z1), .
.
.
, (xn, zn) withxi?
Rd1, zi?
Rd2, ?i ?
{1, 2, .
.
.
, n} (d1, d2are the feature space dimensions of the spelling2Noun phrases were obtained usinghttp://www.umiacs.umd.edu/ hal/TagChunk/and context view respectively).
Using CCA3, welearn the projection matrices ?1?
Rd1?k,?2?Rd2?k(k << d1and k << d2) and obtainspelling view projections x?i= ?T1xi?
Rk, ?i ?
{1, 2, .
.
.
, n}.
The k-dimensional spelling viewprojection of any instance of a candidate phraseis used as it?s embedding4.The k-dimensional candidate phrase embed-dings are used as features to learn a binary SVMwith the seed spelling examples given in figure 1as training data.
The binary SVM predicts whethera candidate phrase is a named entity or not.
Sincethe value of k is small, a small number of labeledexamples are sufficient to train an accurate clas-sifier.
The learned SVM is used to filter out thenoisy phrases from the list of candidate phrasesobtained in the previous step.To summarize, our approach for classifyingcandidate phrases has the following steps:?
Input: n (spelling, context) pairs, spellingseed examples.?
Each of the n (spelling, context) pairs aremapped to a pair of high-dimensional fea-ture vectors to get n paired observations(x1, z1), .
.
.
, (xn, zn) with xi?
Rd1, zi?Rd2, ?i ?
{1, 2, .
.
.
, n}.?
Using CCA, we learn the projection matri-ces ?1?
Rd1?k,?2?
Rd2?kand ob-tain spelling view projections x?i= ?T1xi?Rk,?i ?
{1, 2, .
.
.
, n}.?
The embedding of a candidate phrase is givenby the k-dimensional spelling view projec-tion of any instance of the candidate phrase.?
We learn a binary SVM with the candi-date phrase embeddings as features and thespelling seed examples given in figure 1 astraining data.
Using this SVM, we predictwhether a candidate phrase is a named entityor not.3.2.2 Approach based on Co-trainingWe discuss here briefly the DL-CoTrain algorithm(Collins and Singer, 1999) which is based on co-training (Blum and Mitchell, 1998), to classify3Similar to Dhillon et al.
(2012) we used the method givenin Halko et al.
(2011) to perform the SVD computation inCCA for practical considerations.4Note that a candidate phrase gets the same spelling viewprojection across it?s different instances since the spellingfeatures of a candidate phrase are identical across it?s in-stances.454?
Virus seed spelling examples?
Virus Names: human immunodeficiency, hepatitis C, influenza, Epstein-Barr, hepatitis B?
Non-virus Names: mutant, same, wild type, parental, recombinant?
Disease seed spelling examples?
Disease Names: tumor, malaria, breast cancer, cancer, IDDM, DM, A-T, tumors, VHL?
Non-disease Names: cells, patients, study, data, expression, breast, BRCA1, protein, mutant1Figure 1: Seed spelling examplescandidate phrases.
We compare our approach us-ing CCA embeddings with this approach.
Here,two decision list of rules are learned simultane-ously one using the spelling view and the otherusing the context view.
The rules using thespelling view are of the form: full-string=humanimmunodeficiency?Virus, full-string=mutant?Not a virus and so on.
In the context view, weused bigram5rules where we considered all pos-sible bigrams using the context.
The rules are oftwo types: one which gives a positive label, forexample, full-string=human immunodeficiency?Virus and the other which gives a negative label,for example, full-string=mutant ?
Not a virus.The DL-CoTrain algorithm is as follows:?
Input: (spelling, context) pairs for every in-stance of a candidate phrase in the corpus, mspecifying the number of rules to be added inevery iteration, precision threshold , spellingseed examples.?
Algorithm:1.
Initialize the spelling decision list usingthe spelling seed examples given in fig-ure 1 and set i = 1.2.
Label the entire input collection using thelearned decision list of spelling rules.3.
Add i ?
m new context rules of eachtype to the decision list of context rulesusing the current labeled data.
Therules are added using the same criterionas given in Collins and Singer (1999),i.e., among the rules whose strength isgreater than the precision threshold ,the ones which are seen more often withthe corresponding label in the input datacollection are added.5We tried using unigram rules but they were very weakpredictors and the performance of the algorithm was poorwhen they were considered.4.
Label the entire input collection using thelearned decision list of context rules.5.
Add i ?
m new spelling rules of eachtype to the decision list of spelling rulesusing the current labeled data.
The rulesare added using the same criterion as instep 3.
Set i = i+1.
If rules were addedin the previous iteration, return to step 2.The algorithm is run until no new rules are left tobe added.
The spelling decision list along withits strength (Collins and Singer, 1999) is used toconstruct the dictionaries.
The phrases present inthe spelling rules which give a positive label andwhose strength is greater than the precision thresh-old, were added to the dictionary of named enti-ties.
We found the parameters m and  difficultto tune and they could significantly affect the per-formance of the algorithm.
We give more detailsregarding this in the experiments section.4 Related WorkPreviously, Collins and Singer (1999) introduceda multi-view, semi-supervised algorithm based onco-training (Blum and Mitchell, 1998) for collect-ing names of people, organizations and locations.This algorithm makes a strong independence as-sumption about the data and employs many heuris-tics to greedily optimize an objective function.This greedy approach also introduces new param-eters that are often difficult to tune.In other works such as Toral and Mu?noz (2006)and Kazama and Torisawa (2007) external struc-tured resources like Wikipedia have been used toconstruct dictionaries.
Even though these meth-ods are fairly successful they suffer from a num-ber of drawbacks especially in the biomedical do-main.
The main drawback of these approaches isthat it is very difficult to accurately disambiguateambiguous entities especially when the entities are455abbreviations (Kazama and Torisawa, 2007).
Forexample, DM is the abbreviation for the diseaseDiabetes Mellitus and the disambiguation page forDM in Wikipedia associates it to more than 50 cat-egories since DM can be expanded to Doctor ofManagement, Dichroic mirror, and so on, each ofit belonging to a different category.
Due to therapid growth of Wikipedia, the number of enti-ties that have disambiguation pages is growing fastand it is increasingly difficult to retrieve the articlewe want.
Also, it is tough to understand these ap-proaches from a theoretical standpoint.Dhillon et al.
(2011) used CCA to learn wordembeddings and added them as features in a se-quence tagger.
They show that CCA learns bet-ter word embeddings than CW embeddings (Col-lobert and Weston , 2008), Hierarchical log-linear(HLBL) embeddings (Mnih and Hinton, 2007)and embeddings learned from many other tech-niques for NER and chunking.
Unlike PCA, awidely used dimensionality reduction technique,CCA is invariant to linear transformations of thedata.
Our approach is motivated by the theoreti-cal result in Kakade and Foster (2007) which isdeveloped in the co-training setting.
We directlyuse the CCA embeddings to predict the label ofa data point instead of using them as features ina sequence tagger.
Also, we learn CCA embed-dings for candidate phrases instead of all words inthe vocabulary since named entities often containmore than one word.
Dhillon et al.
(2012) learna multi-class SVM using the CCA word embed-dings to predict the POS tag of a word type.
Weextend this technique to NER by learning a binarySVM using the CCA embeddings of a high recall,low precision list of candidate phrases to predictwhether a candidate phrase is a named entity ornot.5 ExperimentsIn this section, we give experimental results onvirus and disease NER.5.1 DataThe noisy lists of both virus and disease nameswere obtained from the BioMed Central corpus.This corpus was also used to get the collection of(spelling, context) pairs which are the input to theCCA procedure and the DL-CoTrain algorithm de-scribed in the previous section.
We obtained CCAembeddings for the 100, 000 most frequently oc-curring word types in this collection along withevery word type present in the training and de-velopment data of the virus and the disease NERdataset.
These word embeddings are similar to theones described in Dhillon et al.
(2011) and Dhillonet al.
(2012).We used the virus annotations in the GE-NIA corpus (GENIA, 2003) for our experiments.The dataset contains 18,546 annotated sentences.We randomly selected 8,546 sentences for train-ing and the remaining sentences were randomlysplit equally into development and testing sen-tences.
The training sentences are used only forexperiments with the sequence taggers.
Previ-ously, Zhang et al.
(2004) tested their HMM-basednamed entity recognizer on this data.
For diseaseNER, we used the recent disease corpus (Doganand Lu, 2012) and used the same training, devel-opment and test data split given by them.
We useda sentence segmenter6to get sentence segmenteddata and Stanford Tokenizer7to tokenize the data.Similar to Dogan and Lu (2012), all the differentdisease categories were flattened into one singlecategory of disease mentions.
The developmentdata was used to tune the hyperparameters and themethods were evaluated on the test data.5.2 Results using a dictionary-based taggerFirst, we compare the dictionaries compiled us-ing different methods by using them directly ina dictionary-based tagger.
This is a simple andinformative way to understand the quality of thedictionaries before using them in a CRF-tagger.Since these taggers can be trained using a hand-ful of training examples, we can use them to buildNER systems even when there are no labeled sen-tences to train.
The input to a dictionary tagger isa list of named entities and a sentence.
If there isan exact match between a phrase in the input listto the words in the given sentence then it is taggedas a named entity.
All other words are labeled asnon-entities.
We evaluated the performance of thefollowing methods for building dictionaries:?
Candidate List: This dictionary contains allthe candidate phrases that were obtained us-ing the method described in Section 3.1.
Thenoisy list of virus candidates and disease can-didates had 3,100 and 60,080 entries respec-tively.6https://pypi.python.org/pypi/text-sentence/0.137http://nlp.stanford.edu/software/tokenizer.shtml456MethodVirus NER Disease NERPrecision Recall F-1 Score Precision Recall F-1 ScoreCandidate List 2.20 69.58 4.27 4.86 60.32 8.99Manual 42.69 68.75 52.67 51.39 45.08 48.03Co-Training 48.33 66.46 55.96 58.87 23.17 33.26CCA 57.24 68.33 62.30 38.34 44.55 41.21Table 1: Precision, recall, F- 1 scores of dictionary-based taggers?
Manual: Manually constructed dictionaries,which requires a large amount of human ef-fort, are employed for the task.
We used thelist of virus names given in Wikipedia8.
Un-fortunately, abbreviations of virus names arenot present in this list and we could not findany other more complete list of virus names.Hence, we constructed abbreviations by con-catenating the first letters of all the strings ina virus name, for every virus name given inthe Wikipedia list.For diseases, we used the list of diseasenames given in the Unified Medical Lan-guage System (UMLS) Metathesaurus.
Thisdictionary has been widely used in diseaseNER (e.g., Dogan and Lu, 2012; Leaman etal., 2010)9.?
Co-Training: The dictionaries are con-structed using the DL-CoTrain algorithm de-scribed previously.
The parameters usedwere m = 5 and  = 0.95 as given in Collinsand Singer (1999).
The phrases present inthe spelling rules which give a positive labeland whose strength is greater than the preci-sion threshold, were added to the dictionaryof named entities.In our experiment to construct a dictionaryof virus names, the algorithm stopped afterjust 12 iterations and hence the dictionary hadonly 390 virus names.
This was because therewere no spelling rules with strength greaterthan 0.95 to be added.
We tried varyingboth the parameters but in all cases, the algo-rithm did not progress after a few iterations.We adopted a simple heuristic to increase thecoverage of virus names by using the strengthof the spelling rules obtained after the 12thit-eration.
All spelling rules that give a positive8http://en.wikipedia.org/wiki/List of viruses9The list of disease names from UMLS can be found athttps://sites.google.com/site/fmchowdhury2/bioenex .label and which has a strength greater than?
were added to the decision list of spellingrules.
The phrases present in these rules areadded to the dictionary.
We picked the ?
pa-rameter from the set [0.1, 0.2, 0.3, 0.4, 0.5,0.6, 0.7, 0.8, 0.9] using the development data.The co-training algorithm for constructingthe dictionary of disease names ran for closeto 50 iterations and hence we obtained bet-ter coverage for disease names.
We still usedthe same heuristic of adding more named en-tities using the strength of the rule since itperformed better.?
CCA: Using the CCA embeddings of thecandidate phrases10as features we learned abinary SVM11to predict whether a candidatephrase is a named entity or not.
We consid-ered using 10 to 30 dimensions of candidatephrase embeddings and the regularizer waspicked from the set [0.0001, 0.001, 0.01, 0.1,1, 10, 100].
Both the regularizer and the num-ber of dimensions to be used were tuned us-ing the development data.Table 1 gives the results of the dictionary basedtaggers using the different methods describedabove.
As expected, when the noisy list of candi-date phrases are used as dictionaries the recall ofthe system is quite high but the precision is verylow.
The low precision of the Wikipedia viruslists was due to the heuristic used to obtain ab-breviations which produced a few noisy abbrevia-tions but this heuristic was crucial to get a high re-call.
The list of disease names from UMLS givesa low recall because the list does not contain manydisease abbreviations and composite disease men-tions such as breast and ovarian cancer.
The pres-10The performance of the dictionaries learned from wordembeddings was very poor and we do not report it?s perfor-mance here.11we used LIBSVM (http://www.csie.ntu.edu.tw/ cjlin/libsvm/)in our SVM experiments4570 1000 2000 3000 4000 5000 6000 7000 8000 90000.50.550.60.650.70.750.80.85Number of Training SentencesF?1ScoreVirus NERbaselinemanualco?trainingcca0 1000 2000 3000 4000 5000 60000.450.50.550.60.650.70.750.8F?1ScoreNumber of Training SentencesDisease NERbaselinemanualco?trainingcca1Figure 2: Virus and Disease NER F-1 scores for varying training data size when dictionaries obtainedfrom different methods are injectedence of ambiguous abbreviations affected the ac-curacy of this dictionary.The virus dictionary constructed using the CCAembeddings was very accurate and the false pos-itives were mainly due to ambiguous phrases,for example, in the phrase HIV replication, HIVwhich usually refers to the name of a virus istagged as a RNA molecule.
The accuracy of thedisease dictionary produced using CCA embed-dings was mainly affected by noisy abbreviations.We can see that the dictionaries obtained us-ing CCA embeddings perform better than the dic-tionaries obtained from co-training on both dis-ease and virus NER even after improving the co-training algorithm?s coverage using the heuristicdescribed in this section.
It is important to notethat the dictionaries constructed using the CCAembeddings and a small number of labeled exam-ples performs competitively with dictionaries thatare entirely built by domain experts.
These re-sults show that by using the CCA based approachwe can build NER systems that give reasonableperformance even for difficult named entity typeswith almost no supervision.5.3 Results using a CRF taggerWe did two sets of experiments using a CRF tag-ger.
In the first experiment, we add dictionary fea-tures to the CRF tagger while in the second ex-periment we add the embeddings as features to theCRF tagger.
The same baseline model is used inboth the experiments whose features are describedin Section 2.2.
For both the CRF12experimentsthe regularizers from the set [0.0001, 0.001, 0.01,0.1, 1.0, 10.0] were considered and it was tunedon the development set.5.3.1 Dictionary FeaturesHere, we inject dictionary matches as features(e.g., Ratinov and Roth, 2009; Cohen andSarawagi, 2004) in the CRF tagger.
Given a dic-tionary of named entities, every word in the inputsentence has a dictionary feature associated withit.
When there is an exact match between a phrasein the dictionary with the words in the input sen-tence, the dictionary feature of the first word inthe named entity is set to B and the dictionary fea-ture of the remaining words in the named entityis set to I.
The dictionary feature of all the otherwords in the input sentence which are not part ofany named entity in the dictionary is set to O. Theeffectiveness of the dictionaries constructed fromvarious methods are compared by adding dictio-nary match features to the CRF tagger.
These dic-tionary match features were added along with thebaseline features.Figure 2 indicates that the dictionary features ingeneral are helpful to the CRF model.
We can seethat the dictionaries produced from our approachusing CCA are much more helpful than the dictio-naries produced from co-training especially whenthere are fewer labeled sentences to train.
Simi-lar to the dictionary tagger experiments discussed12We used CRFsuite (www.chokkan.org/software/crfsuite/)for our experiments with CRFs.4580 1000 2000 3000 4000 5000 6000 7000 8000 90000.50.550.60.650.70.750.80.85F?1ScoreNumber of Training SentencesVirus NERbaselinecca?wordcca?phrase0 1000 2000 3000 4000 5000 60000.450.50.550.60.650.70.750.8Number of Training SentencesF?1ScoreDisease NERbaselinecca?wordcca?phrase1Figure 3: Virus and Disease NER F-1 scores for varying training data size when embeddings obtainedfrom different methods are used as featurespreviously, the dictionaries produced from our ap-proach performs competitively with dictionariesthat are entirely built by domain experts.5.3.2 Embedding FeaturesThe quality of the candidate phrase embeddingsare compared with word embeddings by addingthe embeddings as features in the CRF tagger.Along with the baseline features, CCA-wordmodel adds word embeddings as features while theCCA-phrase model adds candidate phrase em-beddings as features.
CCA-word model is similarto the one used in Dhillon et al.
(2011).We considered adding 10, 20, 30, 40 and 50 di-mensional word embeddings as features for everytraining data size and the best performing modelon the development data was picked for the exper-iments on the test data.
For candidate phrase em-beddings we used the same number of dimensionsthat was used for training the SVMs to constructthe best dictionary.When candidate phrase embeddings are ob-tained using CCA, we do not have embeddingsfor words which are not in the list of candidatephrases.
Also, a candidate phrase having morethan one word has a joint representation, i.e., thephrase ?human immunodeficiency?
has a lowerdimensional representation while the words ?hu-man?
and ?immunodeficiency?
do not have theirown lower dimensional representations (assumingthey are not part of the candidate list).
To over-come this issue, we used a simple technique to dif-ferentiate between candidate phrases and the restof the words.
Let x be the highest real valued can-didate phrase embedding and the candidate phraseembedding be a d dimensional real valued vector.If a candidate phrase occurs in a sentence, the em-beddings of that candidate phrase are added as fea-tures to the first word of that candidate phrase.
Ifthe candidate phrase has more than one word, theother words in the candidate phrase are given anembedding of dimension d with each dimensionhaving the value 2 ?
x.
All the other words aregiven an embedding of dimension d with each di-mension having the value 4?
x.Figure 3 shows that almost always the candi-date phrase embeddings help the CRF model.
It isalso interesting to note that sometimes the word-level embeddings have an adverse affect on theperformance of the CRF model.
The CCA-phrasemodel performs significantly better than the othertwo models when there are fewer labeled sen-tences to train and the separation of the candidatephrases from the other words seems to have helpedthe CRF model.6 ConclusionWe described an approach for automatic construc-tion of dictionaries for NER using minimal super-vision.
Compared to the previous approaches, ourmethod is free from overly-stringent assumptionsabout the data, uses SVD that can be solved ex-actly and achieves better empirical performance.Our approach which uses a small number of seedexamples performs competitively with dictionar-ies that are compiled manually.459AcknowledgmentsWe are grateful to Alexander Rush, AlexandrePassos and the anonymous reviewers for theiruseful feedback.
This work was supported bythe Intelligence Advanced Research Projects Ac-tivity (IARPA) via Department of Interior Na-tional Business Center (DoI/NBC) contract num-ber D11PC20153.
The U.S. Government is autho-rized to reproduce and distribute reprints for Gov-ernmental purposes notwithstanding any copy-right annotation thereon.
The views and conclu-sions contained herein are those of the authors andshould not be interpreted as necessarily represent-ing the official policies or endorsements, either ex-pressed or implied, of IARPA, DoI/NBC, or theU.S.
Government.ReferencesAndrew McCallum and Wei Li.
Early Results forNamed Entity Recognition with Conditional Ran-dom Fields, Feature Induction and Web-EnhancedLexicons.
2003.
Conference on Natural LanguageLearning (CoNLL).Andriy Mnih and Geoffrey Hinton.
Three New Graph-ical Models for Statistical Language Modelling.2007.
International Conference on Machine learn-ing (ICML).Antonio Toral and Rafael Mu?noz.
A proposal to auto-matically build and maintain gazetteers for NamedEntity Recognition by using Wikipedia.
2006.Workshop On New Text Wikis And Blogs AndOther Dynamic Text Sources.Avrin Blum and Tom M. Mitchell.
Combining Labeledand Unlabeled Data with Co-Training.
1998.
Con-ference on Learning Theory (COLT).Burr Settles.
Biomedical Named Entity RecognitionUsing Conditional Random Fields and Rich FeatureSets.
2004. International Joint Workshop on NaturalLanguage Processing in Biomedicine and its Appli-cations (NLPBA).H.
Hotelling.
Canonical correlation analysis (cca)1935.
Journal of Educational Psychology.Jie Zhang, Dan Shen, Guodong Zhou, Jian Su andChew-Lim Tan.
Enhancing HMM-based Biomed-ical Named Entity Recognition by Studying SpecialPhenomena.
2004.
Journal of Biomedical Informat-ics.Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi andJun?ichi Tsujii.
GENIA corpus - a semantically an-notated corpus for bio-textmining.
2003.
ISMB.Junichi Kazama and Kentaro Torisawa.
ExploitingWikipedia as External Knowledge for Named EntityRecognition.
2007.
Association for ComputationalLinguistics (ACL).Karthik Sridharan and Sham M. Kakade.
An Informa-tion Theoretic Framework for Multi-view Learning.2008.
Conference on Learning Theory (COLT).Lev Ratinov and Dan Roth.
Design Challengesand Misconceptions in Named Entity Recognition.2009.
Conference on Natural Language Learning(CoNLL).Michael Collins and Yoram Singer.
UnsupervisedModels for Named Entity Classification.
1999.
InProceedings of the Joint SIGDAT Conference onEmpirical Methods in Natural Language Processingand Very Large Corpora.Nathan Halko, Per-Gunnar Martinsson, Joel A. Tropp.Finding structure with randomness: Probabilisticalgorithms for constructing approximate matrix de-compositions.
2011.
Society for Industrial and Ap-plied Mathematics.Paramveer S. Dhillon, Dean Foster and Lyle Ungar.Multi-View Learning of Word Embeddings via CCA.2011.
Advances in Neural Information ProcessingSystems (NIPS).Paramveer Dhillon, Jordan Rodu, Dean Foster and LyleUngar.
Two Step CCA: A new spectral method forestimating vector models of words.
2012.
Interna-tional Conference on Machine learning (ICML).Rezarta Islamaj Dogan and Zhiyong Lu.
An improvedcorpus of disease mentions in PubMed citations.2012.
Workshop on Biomedical Natural LanguageProcessing, Association for Computational Linguis-tics (ACL).Robert Leaman, Christopher Miller and Graciela Gon-zalez.
Enabling Recognition of Diseases in Biomed-ical Text with Machine Learning: Corpus andBenchmark.
2010.
Workshop on Biomedical Nat-ural Language Processing, Association for Compu-tational Linguistics (ACL).Ronan Collobert and Jason Weston.
A unified architec-ture for natural language processing: deep neuralnetworks with multitask learning.
2008.
Interna-tional Conference on Machine learning (ICML).Ryan McDonald and Fernando Pereira.
IdentifyingGene and Protein Mentions in Text Using Condi-tional Random Fields.
2005.
BMC Bioinformatics.Sham M. Kakade and Dean P. Foster.
Multi-view re-gression via canonical correlation analysis.
2007.Conference on Learning Theory (COLT).William W. Cohen and Sunita Sarawagi.
ExploitingDictionaries in Named Entity Extraction: Combin-ing Semi-Markov Extraction Processes and Data In-tegration Methods.
2004.
Semi-Markov Extraction460Processes and Data Integration Methods, Proceed-ings of KDD.461
