Phoneme-in-Context Modeling for Dragon'sContinuous Speech RecognizerPaul Bamberg and Laurence GillickDragon Systems, Inc.90 Bridge SL, Newton MA 02158AbstractFor large-vocabulary continuous speech recognition,the goal of training is to model phonemes with enoughprecision so that from the models one could reconstruct asequence of acoustic parameters that accurately represents hespectral characteristics of any naturally-occurring sentence,including all coarticuladon effects that arise either betweenphonemes in a word or across word boundaries.
The aim atDragon Systems is to collect and process enough trainingdata to accomplish this goal for all of natural spokenEnglish rather than for any one restricted task.The basic unit that must be trained is the "phoneme incontext" (PIC), a sequence of three phonemes accompaniedby a code for prepausal lengthening.
At present, syllableand word boundaries are ignored in defining PICs.More than 16,000 training tokens, half isolated wordsand half short phrases, were phonemically abeled by a semi-.
automatic procedure using hidden Markov models.
To modela phoneme in a specific context, a weighted average isconstructed from training data involving the desired contextand acoustically similar contexts.For use in HMM continuous-speech recognition, eachPIC is converted to a Markov model that is a concatenationof one to six node models.
No phoneme, in all its contexts,requires more than 64 distinct nodes, and the total number ofnode models ("phonemic segments") required to construct allPICs is only slightly more than 2000.
As a result, the entireset of PICs can be adapted to a new speaker on the basis of acouple of thousand isolated words or a few hundred sentencesof connected speech.The advantage of this approach to training is that it isnot task-specific.
From a single training database, DragonSystems has constructed models for use in a 30,000-wordisolated-word recognizer, for connected digits, and for twodifferent thousand-word continuous-speech tasks.1.
IntroductionThe nature of the training process for a speech-recognition system changes radically once the size of thevocabulary becomes larger than the number of words forwhich a user is willing to provide training tokens.
Belowthis threshold, it is reasonable to make an independent modelfor each word in the vocabulary.
Such a model, based ondata from that word and no others, can in principle captureall the acoustic-phonetic subtleties of the word, even thoughthe phonetic spelling of the word is not even used inconstructing the model.For continuous speech recognition, the quantity of datarequired for complete training rows much more rapidly thanvocabulary.
In the simple case of a recognizer for three-digitstrings, for example, each digit should at a minimum betrained in initial, medial, and final position, while foroptimum performance all digit triples should be included inthe training data.The approach to training at Dragon Systems has beento regard the recognition task as all of natural English,whether isolated words or connected speech.
We havedeveloped a training database from which we haveconstructed recognition models for a 30,000 word isolated-word recognizer and for two different 1000-word connectedspeech tasks.
All these recognition models are based on thesame set of "phonemes incontext."2.
Phonemes in ContextA speaker of English, given a phonemic spelling of anunfamiliar word from a dictionary, can pronounce the wordrecognizably or recognize the word when it is spoken.
Onthe other hand, it is impossible to put together an "alphabet"of recorded phonemes which, when concatenated, will soundlike natural English words.
Speakers of English apply a hostof duration and coarticulation rules when combiningphonemes into words and sentences, and they employ thesame rules in recognizing spoken language.
It comes as asurprise to most speakers, for example, to discover that thevowels in "will" and "kick", which are identical according todictionary pronunciations, are as different in their spectralcharacteristics a the vowels in "not" and "nut", or that thevowel in "size" has more than twice the duration of the samevowel in "seismograph".,,,in .
'!
;1:  ;.
;~ +; ,~t , t i~ ,~ i~. '
.
.
"P : ,~ l ;  " '"  :, " - ' " ' "  .-" .
':: " ' : ;  ? "
" ?
: ?
\[ -' .
.
:~ .
?
.
.
~ .
?
?
?
.
| :: ' ' .
, .  "
i ' .n  ?
: .
?
.- .?"
f .~t / I / l~ J~/~tu l : ;~  - h i .
.
.
: , :  .
: :?
~ 'w~i ' i i i i i i ' i i i i i ' i i | l l l ' l l l l l l l ' l l J J \ ]  ?
?sit-" ~L~:'p~'~ ~.. -.
: ~)~,.
'~..~.~...~.~ ..~.
.q L i .~ / i j .
.
:X: , . '
.
. '
"  ~, .
: .
.
.?
, * , ,  Z , ; .~v-n* \ ]~4.%~ :.
-~kk~lckk  t ~ ~L | t t I z'~ i i ,  d ,  kk~k\ ]~kkkk \ ]kk l~k\ ]4~kkkk \ ]d tX \ ] t -Figure 1.
"Will" Figure 2.
"Kick"163" ''"~:' """ " ' ' ' '  " ~ I~.~.
:~;.~:..~',,,"s,::,.
1" .
?
~v~ :, , :~,~ " "a  4f l ; .~.. ' , .
.
.~ .
, .
"~ ,  .
.
.
.
.
.
.
G .
.
.
.
? "
.
.S  .
, ' , : ' .91 . '
.
"~ ' .
.
~ 41 : .
"D .
-  .
-  .
.
\ [  .X  ~ ; ; .
.
.
.
.
,,.,ate.
~ "~. '
; ' " .
, ' , ,  ~ : - .
?
V I l ' q~, .
: .
?
?
?
, .
.
.
.
.~ .1~ .
.
.
.
.
.
~.
;~_ .
.~.
?~'~, .~.. .
.
.
.
.
~ ,, .
, .
.
.~ .
.
.
.
.
.
.
?
.
.
.
.
.
, .
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
L .
.
.
,  .
.
.
.
.?
". "
; , ' "  " : ~"  " ? "
" "  - " ' :  " : :  : "  " "  : ' " ' : : ' : : "  "F 'Wl !
?~,1  " : .
: , ~ ~ .
~ : ~ .
~  ~.
,.~.'.i.
L : ..
?, :~ :  ~,q |~" .
-  - .
-  .- :?
: " " .
.
.
.
~'"" "".'.
::, i .
: .
.
.
, :.- , , .
,  ~, ~ ` -  ~- -  r ; .
.~ .
- .
.
.
.
.
.
.
; .
: , .
..'.":.
"i I \[ I)4 ?
: .
"**~ ".414~llN)q~O?~. )
.
.
.
.
. "
. '
.
.
.
)~  .P .
.
.
*F igure  3.
"S i ze"  (p repausa i  lengthen ing  o f  " i " )s !~ - ~tt- St*bf ?
n U t, J tACE q~l?
~ .
t~.
?
.?'.
?
? "
,  ?
t .
.
.
.
.
.
?
: ?
?
?
?
?
?
~ ,~ .
.
.
.
.
?
: | *  ?
?
: : I t :  ?
.
, , "  ?
t " l~- ?
,~  ~"%i~t  : ?
i i ; ?
,  .
, '  , . '
?
,.
?
:.~ .
?
: ?
?
...-?~ -.
... __  .
.
.~  .
?
... "?
. "
: i~, ,~, , , , : : ,  .~.~:.
, , .~-:  ?
~.
{.~ I:, {~9~ll\[~t./:Ni~it~'~,?
:.".
:t'~ ,'r'r~.,-.'-~.'.
:: ~'~-~";'z':i: ,.
".. :~" d: ":-:~.. , t e ~ ; ; i .
i ; ~ !?
- - ' .
-g~r~ ~ ;4= ~, .k lL .X .
,1 , .
\ ]  ~t H ?
~?
'~ f r 1 1 l .~  ~ z  mMNr l lq~,u ,u~u.~.uu~2"~f~s ' :~ ' r  ~ '~z '~kPt t~4,4 ,~.
.~_~_~9.
i  ~a  a .~t~ae??~l~f??
, '~ '~ I'~ fF igure  4 .
"Se ismograph"  (no  prepausa i  lengthen ing  o f  " i " )In the Dragon Systems family of speech recognizers,the fundamental unit of speech to be trained is the "phonemein context" (PIC)\[3\].
Ultimately the defining property of aPIC is that by concatenating a sequence of PICs for anutterance one can construct an accurate simulated spectrumfor the utterance.
In the present implementation, a PIC istaken as completely specified by a phoneme accompanied bya preceding phoneme (or silence), asucceeding phoneme (orsilence), and a duration code that indicates the degree ofprepausal lengthening.
To restrict the proliferation of PICs,syllable boundaries, even word boundaries, are currentlyignoredThe set of phonemes i  taken from The RandomHouse?
Unabridged Dictionary.
The stress of each syllableis regarded as a property of the vowel or syllabic consonantin that syllable?
Excluding pronunciations which areexplicitly marked as foreign, there are 17 vowels, each withthree possible stress levels, plus 26 consonants and syllabicconsonants.A duration code of 3 indicates absence of prepausallengthening.
This will always be the case except in the lasttwo syllables of an utterance.A duration code of 6 indicates prepausal lengthening toapproximately twice the normal duration.
This occurs for thevowel in the final syllable of an utterance and for anyconsonant that follows that vowel, unless the vowel isfollowed by one of the unvoiced consonants k,p, t, th or ch.For example, in the word "harmed" every PIC except he onefor the initial 'h' will have a duration code of 6.A duration code of 4 indicates prepausal lengtheningby a factor of approximately 4/3.
This occurs in two cases:?
In the final syllable when the vowel is followed by k, p,t, ch, or th: for example, in both PICS of "at" and inthe last three PICS of "bench".?
For consonants that precede the vowel in the finalsyllable: for example, the 's' in "beside".PICs contain almost enough information to predict heacoustic realization of a phoneme.
For example, the PIC for't' is different in the word "mighty" (where the 't' is usuallyrealized as a flap) and in the phrase "my tea" (where the 't' isclearly aspirated).
This distinction is made, even thoughsyllable and word boundaries, are ignored, because the stressof the following vowel is part of the context?
Similarly,PICs capture the information that the final 't' in "create"(preceded by a stressed vowel) is more strongly released thatin "probate" (preceded by an unstressed vowel), that the 's'in "horseshoe" is realized as an "sh", that the 'n' in "SanFrancisco" or "NPR" is realized almost like an 'm', and thatthe 'n' in "month" or "in the" is the dental allophone of 'n'.3.
Selection of PICs for TrainingFor isolated-word ecognition, one could in principleenumerate all PICs by processing phonetic spellings for allthe words in an unabridged ictionary.
For the 25,000 wordsin the DragonDictate r cognizer, there are approximately30,000 PICs.
A subset of 8,000 words can be chosen thatincludes all but about 1,000 of these PICs, most of themoccurring in only a single word.
Increasing the vocabularysize to 64,000 words would increase the number of PICSonly slightly, to about 32,000.For connected speech the goal of including all possiblePICs is unachievable b cause of the wide variety of PICsthat can arise through coarticulation across word boundaries.For example, the sentence "Act proud when you're dubbedGareth" contains the PICs "ktp" and "bdg', neither of whichoccurs in any common English word.
A furthercomplication is that each PIC in a final syllable can occur ina sentence ither with or without prepausal lengthening.For the sort of connected-speech task which can becarried out in close to real time on today's microcomputers,the majority of PICs already arise only as a result ofcoarticulation across word boundaries.
The 1023pronunciations for the 842 words in the mammographyvocabulary that is used for research at Dragon Systemsinclude 2681 PICs.
A set of 3000 sentences using thisvocabulary includes only 1929 of these PICs, plus another4610 that are not present in the isolated words?
A differentset of 3000 sentences, reserved for testing, includes yetanother 1326 new PICs.
Among the 121 PICs, not present164in isolated words, that occur 100 or more times in thesentences are the vowel in the suffix "ation" withoutprepausal lengthening, the dental "n" of "in the" and "onthe", and the "zs" combination of"is seen".The Dragon Systems training set currently includesabout 8000 isolated words and about 8000 short phrases,each limited in duration to about 2.4 seconds.
Although thetotal number of words in the training set is no greater thanin the 6000 mammography sentences, the training setincludes 37,423 distinct PICs.
It is still far from complete.For example, a a set of 800 phrases drawn from aHemingway short story and a newspaper article on parallelprocessing includes lightly more than 1000 PICs that werenot in the training set (most, however, occurred only once).The problem of finding the smallest trainingvocabulary that includes a given set of PICs is probably NP-complete.
Still, it is easy to find a reasonably goodapproximation to the solution of the problem.
In 6000isolated words one can include about 22,000 different PICs.Beyond this point it becomes difficult to find words thatinclude more than one or two new PICs, but short phrasesof diverse text which contain three or more new PICs arestill easy to find.
By using such phrases to enlarge thetraining vocabulary, we hope to acquire training data for50,000 PICs within the next year.4.
Modeling PICs by PhonemicSegmentsA "vocabulary" of 50,000 independent PICs would beno more manageable than a vocabulary of 50,000independent isolated words, but PICs are not independent.Most of the PICs for a stop consonant, for example, involvean identical segment of silence, for example, while all PICsfor the sibilant "s" are characterized by the absence of low-frequency energy.
One can hope, therefore, to represent thethousand or so PICs that represent the same phoneme invarious contexts in terms of a much smaller number of"phonemic segments".
For phonemes that exhibit a greatdeal of allophonic variation, such as "t", "k", and schwa, asmany as 64 different segment models may be required, whilefor phonemes like "s" and "sh" that are little influenced bycontext, as few as ten may suffice.
For the complete set of77 phonemes used in English, slightly more than 2000segment models suffice.
In \[4\], an approach to modelingallphonic models using a small number of distributions wasdescribed.
Similarly, in \[5\], an alternate way of performingparameter tying across distinct riphones using a triphoneclustering procedure was described.A phonemic segment can be characterized in twoalternative ways.
At the simpler level, it can be regarded as afragment of the sort of acoustic data that would be generatedby the "front end" of a speech-recognition system.
In thecase of the current Dragon recognizer, this is nothing morethan a simulated spectrum based on an amplitude parameterand several spectral parameters.
At a more sophisticatedlevel, a phonemic segment includes enough information togenerate a probability distribution for use in hidden Markovmodeling.
For the current Dragon recognizer, this requirescalculation of the absolute deviation from the mean, as wellas the mean for each acoustic parameter.
The samedistinction between what will be called a "spectral model"and what will be called a "Markov model" applies also tocontinuous parameters that have no direct spectralinterpretation (cepstral parameters, for example), or todiscrete parameters.
In the following discussion, the term"spectrum" should be interpreted to mean any sequence ofparameters that results from processing a speech waveform,while "Markov model" should be interpreted as a randomprocess capable of generating such sequences.One may think of a PIC as a probabilistic model for aportion of a speech spectrogram corresponding to a singlephoneme.
The problem of representing this PIC as asequence of phonemic segments i solved by hidden Markovmodeling.
The sequence may be from one to six segments inlength, and the same segment may occur in more than oneposition in the sequence.
There is no constraint on the orderof segments within the sequence.Thus the model for aphoneme with n segments is represented by the diagrambelow.sta n dFigure 5.
A Markov Model for a Single PICThe arcs labeled 1, 2 .... n correspond to one or moreframes of acoustic data corresponding tothe single segment1, 2 .
.
.
.
n. The arcs labeled x permit a given phoneme tohave a sequence of fewer than six phonemes associated withit.
These null arcs are assigned slightly higher transitionprobabilities than the arcs associated with phonemicsegments.165Thus a PIC may be represented very compactly as asequence of one to six pairs, each pair consisting of aphonemic segment and a duration.This sequence may beregarded as the best piecewise-constant pproximation to thespectrogram.For speaker adaptation, the phonemic segment is thebasic unit.
It is assumed that the representation f a PIC interms of segments i valid for all speakers, o that adaptingthe small number of segments for a phoneme will have theeffect of adapting the much larger number of PICs.
Segmentdurations within a PIC can also be adapted, but only byacoustic data involving that particular PIC.5.
Labeling Training DataTo build a spectral model for a PIC, one must findone or more spectrograms that involve that PIC, then extractfrom these spectrograms the data for the phoneme in thedesired PIC.
Thus phonemically labeled training data arerequired.Given a complete set of hidden Markov modelsrepresenting PICs, the labeling problem could easily besolved by dynamic programming and traceback.
Thisapproach is the correct one to use for implementingadaptation, but it is inappropriate for training, since thelabeled training data would be required in order to produce thePIC models in the first place.
To do semiautomatic labelingwith an incomplete set of phonemic segments and with noPIC models, a simpler scheme must be used, one whichdeals gracefully with the situation where PIC models havenot yet been created and where some portions ofspectrograms cannot yet be labeled.The full Markov model for a word is a sequence ofmodels for the phonemes of the word, starting and endingwith silence.
Silence is modeled, like any other phoneme,by a set of segments.
Between the phoneme models are"transition odes" with fixed transition probabilities that arechosen to be slightly lower than the typical probability forthe best phoneme segment.
Thus the model for "at" mightbe represented as follows:I ---  silencetransition transition transition\/,... ' ~ / ':.
:.~~~i ~kk .............................../~~::::~..."il~ / ='": .................... .
....., v t ~ , : - :~B.
:~3 = = = =" ~"  -- ='lii::::i~:::~::i-::,:~i~::~ii~i~ir - - - r  - n ~ ~.
: : -~: : : : : .~ .
: : :~ : ' : |  v :::::::::::::::::::::::::::::::::::::::::::: v silence --OFigure 6.A Markov Model for "at".Each box represents a phoneme model of one to sixstates, as described above.Once the best path has been found by dynamicprogramming, traceback at the phoneme l vel assigns a starttime and end time to each phoneme.
If a complete set ofphonemic segments has been constructed, the start ime foreach phoneme coincides with the end time for its predecessorphoneme.
To the extent that there are acoustic segments hatare not yet well modeled by any phonemic segment, he datathat correspond to this segment will be assigned to aninterphoneme transition.The phoneme-level traceback is recorded within eachtraining token.
This makes it possible, without repeatingthe dynamic programming, to identify the portion of a giventraining token that correspond to a specified phoneme--animportant step in locating training data for a specific PIC.Traceback can also be performed at a lower level inorder to determine the sequence of phonemic segments hatcorresponds toan individual PIC.
The data thus assigned to asegment may then be used as ~aining data for that segmentto improve the estimates of the means and variances for theacoustic parameters of that segment.The net effect of dynamic programming followed bytraceback at the word level and at the phoneme level is toassign to each "frame" of acoustic data of the word aphoneme segment label, subject to the followingconstraints:?
Phonemes appear in the order specified by thepronunciation fthe word.?
For each phoneme, there are no more than fivetransitions from one segment to another.?
Transition frames with no segment assignment mayoccur only between phonemes.The process of labeling the training data is notcompletely automatic, but it becomes more and more nearlyso as the set of phonemic segments increases in size.
Inpractice, phonemic segments are initialized "by hand".
On aspectral display of a training token, a sequence of frames isselected.
The means and variances for the acoustic parametersof those frames provide the initial estimates for the segmentparameters.
Even in the absence of any previously labeledsegments, it is a straightforward matter to initialize a set ofsegments hat will provide a correct phonemic labeling of asingle token, and these segments in turn prove useful inlabeling other tokens.
As more and more tokens are labeledin this manner, a set of segments develops that suffices tolabel a greater and greater fraction of new tokens, untileventually any new token can be labeled without he need forinterphoneme transitions.As new segments are created during the labelingprocess, occasionally the limit of 64 segments for aphoneme is reached.
Whenever this occurs, the twosegments hat are most similar are automatically combinedinto a single segment.Once a thousand or so training tokens have beenlabeled, transition segments hat are more than about hirtymilliseconds long become difficult o find.
At this point the166best strategy is to label all the training tokensautomatically, then to search for the longest ransitionsegments and to use them to create new phonemic segments.This process can be iterated until no transition segmentsremain.To make use of duration constraints in labeling, analternative version of the dynamic programming is usedwhich closely resembles the one used by Dragon's mall-vocabulary recognition and training algorithm.
To eachphoneme in the word, an expected duration in millisecondsis assigned.
To the extent hat the actual duration of thespeech assigned to that phoneme is less than or greater thanthe expected uration, a duration penalty is added to thedynamic programming score.
The traceback is thendetermined both by acoustic match and by durationconstraints.
While a clear-cut phoneme boundary such as onebefore or after an 's' will be little affected by durationconstraints, a boundary that is associated with almost noacoustic feature (between two stops, for example) will beassigned primarily on the basis of durations.In order to estimate durations, the hypothesis i  madethat changing the left or right context of a phoneme haslittle effect on the duration of that phoneme xcept in thecase where the context is silence.
As stated above, theduration of the final T in "all" ought o be the same as theduration of the final T in "wheel", "bell", or other wordswhere there is a clear formant ransition into the 'T'.
Asanother example, the 'p' and 't' in "opted" should each havea duration close to that of a single intervocalic stop.For each PIC, an expected uration is determined byaveraging together four quantities:?
the duration of the phoneme inthe precise contextspecified by the PIC (which may occur only once in thetraining vocabulary).?
the duration of the phoneme with the specified leftcontext and an arbitrary right context.?
the duration of the phoneme with the specified rightcontext and an arbitrary left context.?
the duration of the phoneme with both left and rightcontext arbitrary.In no case, however, is a silence context substituted for anon-silence context or vice versa.The semiautomatic labeling process described abovehas been under development for more than a year, withresults that appear more and more satisfactory as the newphonemic segments are identified and duration estimates areimproved.
By using a set of about 2000 segments andimposing duration constraints on the dynamic programming,it is possible to achieve automatic phonemic labeling thatagrees with hand labeling in almost every case and that isprobably more consistent than hand labeling with regard tosuch difficult, arbitrary decisions as placing boundariesbetween adjacent front vowels or between glides and vowels.Most labels that a human labeler might question can belocated by looking just at the small fraction of words forwhich the actual and expected uration of a phoneme differsignificantly.By exploring situations in which the expecteddurations of phonemes in correctly labeled words aresystematically in error, it is possible to discover newduration rules which can be incorporated into more refinedcharacterization f PICs.
Each such rule, though, leads toan increase in the total number of PICs that must be trained.6.
Building Models for PICsGiven a sufficiently arge quantity of training data, onecan create an excellent model for a PIC by averagingtogether all examples of that PIC in the training vocabulary.For example, a model can be built for the phoneme "sh" inthe context "ation" by averaging together the data labeled as"sh" in words such as "nation", "creation", and "situation".Unfortunately, the assumption ofa large quantity of trainingdata for each PIC is unrealistic.
There are, for example,about 1500 contexts in the DragonDictate 25,000 wordvocabulary, and many contexts in connected speech, forwhich even the current training set of 16,000 items providesno examples.
For thousands of other PICs there is only asingle example in the training set.
Thus, in modeling aPIC,it is important to employ training data from closely relatedPICs.In most cases the left context of a phoneme influencesprimarily the first half of the phoneme, while the rightcontext influences primarily the second half.
Furthermore,there are groups of phonemes which give rise to almostidentical coarticulation effects: different stress levels of thesame vowel, for example.The general strategy for building a model for aphoneme in a given context is to compute a weightedaverage of all the data in the training vocabulary for thegiven phoneme in the desired context or any similar context.The weight assigned to a context depends upon how well itmatches the desired context.Weights are assigned separately for the left context andthe right context, and two models are constructed.
The firstof these, where a high weight implies that he left context isvery close to the desired left context (although the rightcontext may be wrong) is used for the first half of themodel.
The second model, where a high weight implies thatthe right context is correct, is used for the second half of themodel.Each phoneme is assigned both to a "left contextgroup" and to a "fight context group".
The phonemes in leftcontext group should all produce similar coarticulationeffects at the start of a phoneme, while those in the sameright context group should produce similar effects at the endof a phoneme.To build a model for a PIC, all examples of contextssimilar to the desired PIC are extracted from the trainingvocabulary.
Each context is assigned a "left weight" and a"right weight" according to the degree of match between thedesired context in the PIC and the actual context in thetraining item.From the data a weighted average of the durations inow computed.
Tokens for which the duration is close to theaverage are doubled in weight, while those that are far fromthe average duration are halved in weight.Finally all the examples of the desired phoneme areaveraged together using a linear alignment algorithm whichnormalizes all examples so that they have the same length,then averages together acoustic parameters atintervals of 10milliseconds.
This procedure iscarried out twice, once withleft weights, once with right weights.
The first half of the167"left model" and the second half of the "right model" areconcatenated to form the final spectral model for the PIC.Models for initial and final silence in each context arecreated by averaging the initial silence from training wordsthat begin with the desired phoneme and by averaging thefinal silence from words that end with the desired phoneme.Consider, for example, the comparatively unusual PIC"lak" (secondary stress on vowel, no prepausal lengthening).No word in the training set contains this PIC, although"Cadillacs" has the same PIC with prepausal lengthening.The "left" model, built from "implants", overlap shadows","eggplant", "Cadillacs", and "mainland gale", captures wellthe second formant transition between the 'T' and the vowel.The "fight" model captures the spectrum of the vowel before"k".
The concatenated model has both features wellmodeled.These spectral models for PICs are not yet hiddenMarkov models, since they include only the means ofacoustic parameters, but not the variances.
They also haveno direct connection with phonemic segments.
The finalstep in the training process is to convert them to adaptableMarkov models that are based on phonemic segments.Converting a spectral model for a PIC to a Markovmodel for that PIC employs the same algorithm that is usedfor labeling training data.
Dynamic programming is used todetermine the sequence of phonemic segments hat has thegreatest likelihood of generating the spectral model for thePIC.
These phonemic segments become the nodes of theMarkov model for the PIC.
Concatenating the parametermeans for the nodes, with each node given the durationdetermined by the dynamic programming, produces theoptimal piecewise-constant pproximation to the spectralmodel for the PIC.The variances in the parameters for each phonemicsegment correctly reflect he fact that each segment appearsin many different PICs.
Because training tokens are alreadyaverages of three utterances, the variances underestimate thevariation in parameters from one utterance to another.
Tocompensate for this, the variances in the phonemic segmentmodels that are used for recognition are made somewhatlarger than the estimates that arise from training.Because the large number of PIC models are allconstructed from about 2000 phonemic segments, they adaptquickly to a new speaker.
The strategy for adaptation issimply to treat each utterance as if it were new training data.By dynamic programming the utterance is segmented intoPICs, which are in turn subdivided in phonemic segments.The acoustic data assigned to each segment are used toreesfimate the means and variance for that segment.
For themammography task, a set of 500 sentences tobe used foradaptation has been developed that includes more than 90%of the PICs used by the recognizer.
Since most phonemicsegments occur in many different PICs, these 500 sentencesprovide diverse training data for almost all segments,sufficient to provide good estimates of their parameter meansand variances for a new speaker.
Estimates of segmentdurations for each PIC are also improved as a result ofadaptation, although for this purpose the 500 sentencesprovide much less data.To achieve real-time recognition of connected speech, arapid-match algofithm isused to reduce the number of wordsfor which full dynamic programming is carried out\[l\].
Thisalgorithm requires models which incorporate accurateduration information and which capture coarticulation effectsaveraged over all possible contexts for a word.
The trainingfor the rapid-match model for a word makes use of aconcatenation f spectral models for the PICs of the word,with a "generic speech" left context used for the firstphoneme and a "genetic speech" fight context used for thelast phoneme of the word.7.Recognition PerformanceThe training strategy described here is intended to yielda set of PICs that will serve for any isolated-word orconnected-speech recognition task in English.
Testing hasbeen carried out on four tasks, as follows.1.
The DragonDictate isolated-word ecognition systemuses 25,000 word models based on PICs and phonemicsegments, built from the same database of trainingutterances that is used for connected speech.
Recognitionperformance for two diverse texts, a short story byHemingway and a newspaper a ticle on parallel processing,was 83% correct on the first 500 words.
After adaptation on1500 words, performance rose to 89% correct for the speakerwho recorded the training database.
For two other speakers,performance without adaptation was dismal (45% for a malespeaker, 18% for a female speaker), but it rose afteradaptation on 2500 words to 87% for the male speaker and85% for the female.2.
For connected igit recognition, the error rate onfive-digit strings was less than half a percent for each ofthree different speakers after adaptation.
Less than 0.2% ofthe training database consists of digit strings.3.
For the mammography task used in testing the real-time implementation f continuous-speech recognition\[2\](842 words, 1023 distinct pronunciations), recognition wastested on a set of 1000 sentences which had not been usedeither in selecting training utterances or in determiningwhich PICs should be modeled.
Several hundred of the PICsin this test data did not occur in any of the "practice"sentences that had been for training; these PICs weremodeled only by genefic PICs in which an average wastaken over all left and fight contexts.
About 15% of thetraining database consists of short phrases extracted from the3000 practice sentences.
On this task, whose perplexity isabout 66, 96.6% of words were recognized correctly.Performance was slightly better on the "practice" sentencesthat had been used to construct he set of PICs to bemodeled, sentences for which no generic PICs were required.Preliminary results indicate that after several hundredsentences of adaptation, performance lose to this level canbe achieved for other speakers.4.
As a test of performance on a connected-speech taskwhich was not so heavily used in constructing the trainingdatabase, recognition was carried out on the 600 trainingsentences of the Resource Management task using the word-pair grammar.
This task has a perplexity of about 60,comparable to that of the mammography task.
PICs werebuilt from the same training database as descfibed above, inwhich about 5% of the tokens are phrases based on theresource management vocabulary.
Recognition performancewas 97.3% correct on a per-word basis.
For this task, as forthe mammography "practice" sentences, all PICs had beenmodeled, so that no genetic PICs were required.168References[1] L. Gillick and R. Roth, "A Rapid Match Algorithm forContinuous Speech Recognition", Proceedings of DARPASpeech and Natural Language Workshop, June 1990 HiddenValley, Pennsylvania.
[2] P. Bamberg et al, "The Dragon Continuous-SpeechRecognition System: A Real-Time Implementation",Proceedings of DARPA Speech and Natural LanguageWorkshop, June 1990 Hidden Valley, Pennsylvania.
[3] R. Schwartz et al, "Context-Dependent Modeling forAcoustic-Phonetic Recognition of Continuous Speech",IEEE International Conference on Acoustics, Speech, andSignal Processing, April 1985[4] Bahl et al, "Large Vocabulary Natural LanguageContinuous Speech Recognition", IEEE InternationalConference on Acoustics, Speech, and Signal Processing,May, 1989[5] K.F.Lee et al, "The Sphinx Speech RecognitionSystem", IEEE International Conference on Acoustics,Speech, and Signal Processing, May, 1989169
