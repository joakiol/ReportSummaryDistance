Semantic Role Labeling via Integer Linear Programming InferenceVasin Punyakanok Dan Roth Wen-tau Yih Dav ZimakDepartment of Computer ScienceUniversity of Illinois at Urbana-Champaign{punyakan,danr,yih,davzimak}@uiuc.eduAbstractWe present a system for the semantic role la-beling task.
The system combines a machinelearning technique with an inference procedurebased on integer linear programming that sup-ports the incorporation of linguistic and struc-tural constraints into the decision process.
Thesystem is tested on the data provided in CoNLL-2004 shared task on semantic role labeling andachieves very competitive results.1 IntroductionSemantic parsing of sentences is believed to be animportant task toward natural language understand-ing, and has immediate applications in tasks suchinformation extraction and question answering.
Westudy semantic role labeling(SRL).
For each verb ina sentence, the goal is to identify all constituentsthat fill a semantic role, and to determine their roles,such as Agent, Patient or Instrument, and their ad-juncts, such as Locative, Temporal or Manner.The PropBank project (Kingsbury and Palmer,2002) provides a large human-annotated corpusof semantic verb-argument relations.
Specifically,we use the data provided in the CoNLL-2004shared task of semantic-role labeling (Carreras andMa`rquez, 2003) which consists of a portion of thePropBank corpus, allowing us to compare the per-formance of our approach with other systems.Previous approaches to the SRL task have madeuse of a full syntactic parse of the sentence in or-der to define argument boundaries and to determinethe role labels (Gildea and Palmer, 2002; Chen andRambow, 2003; Gildea and Hockenmaier, 2003;Pradhan et al, 2003; Pradhan et al, 2004; Sur-deanu et al, 2003).
In this work, following theCoNLL-2004 shared task definition, we assume thatthe SRL system takes as input only partial syn-tactic information, and no external lexico-semanticknowledge bases.
Specifically, we assume as inputresources a part-of-speech tagger, a shallow parserthat can process the input to the level of basedchunks and clauses (Tjong Kim Sang and Buch-holz, 2000; Tjong Kim Sang and De?jean, 2001),and a named-entity recognizer (Tjong Kim Sangand De Meulder, 2003).
We do not assume a fullparse as input.SRL is a difficult task, and one cannot expecthigh levels of performance from either purely man-ual classifiers or purely learned classifiers.
Rather,supplemental linguistic information must be usedto support and correct a learning system.
So far,machine learning approaches to SRL have incorpo-rated linguistic information only implicitly, via theclassifiers?
features.
The key innovation in our ap-proach is the development of a principled method tocombine machine learning techniques with linguis-tic and structural constraints by explicitly incorpo-rating inference into the decision process.In the machine learning part, the system wepresent here is composed of two phases.
First, aset of argument candidates is produced using twolearned classifiers?one to discover beginning po-sitions and one to discover end positions of eachargument type.
Hopefully, this phase discovers asmall superset of all arguments in the sentence (foreach verb).
In a second learning phase, the candi-date arguments from the first phase are re-scoredusing a classifier designed to determine argumenttype, given a candidate argument.Unfortunately, it is difficult to utilize global prop-erties of the sentence into the learning phases.However, the inference level it is possible to in-corporate the fact that the set of possible role-labelings is restricted by both structural and lin-guistic constraints?for example, arguments cannotstructurally overlap, or, given a predicate, some ar-gument structures are illegal.
The overall decisionproblem must produce an outcome that consistentwith these constraints.
We encode the constraints aslinear inequalities, and use integer linear program-ming(ILP) as an inference procedure to make a fi-nal decision that is both consistent with the con-straints and most likely according to the learningsystem.
Although ILP is generally a computation-ally hard problem, there are efficient implementa-tions that can run on thousands of variables and con-straints.
In our experiments, we used the commer-cial ILP package (Xpress-MP, 2003), and were ableto process roughly twenty sentences per second.2 Task DescriptionThe goal of the semantic-role labeling task is to dis-cover the verb-argument structure for a given inputsentence.
For example, given a sentence ?
I left mypearls to my daughter-in-law in my will?, the goal isto identify different arguments of the verb left whichyields the output:[A0 I] [V left ] [A1 my pearls] [A2 to my daughter-in-law] [AM-LOC in my will].Here A0 represents the leaver, A1 represents thething left, A2 represents the benefactor, AM-LOCis an adjunct indicating the location of the action,and V determines the verb.Following the definition of the PropBank, andCoNLL-2004 shared task, there are six differenttypes of arguments labelled as A0-A5 and AA.These labels have different semantics for each verbas specified in the PropBank Frame files.
In addi-tion, there are also 13 types of adjuncts labelled asAM-XXX where XXX specifies the adjunct type.In some cases, an argument may span over differ-ent parts of a sentence, the label C-XXX is used tospecify the continuity of the arguments, as shown inthe example below.
[A1 The pearls] , [A0 I] [V said] , [C-A1 were leftto my daughter-in-law].Moreover in some cases, an argument might be arelative pronoun that in fact refers to the actual agentoutside the clause.
In this case, the actual agent is la-beled as the appropriate argument type, XXX, whilethe relative pronoun is instead labeled as R-XXX.For example,[A1 The pearls] [R-A1 which] [A0 I] [V left] , [A2to my daughter-in-law] are fake.See the details of the definition in Kingsbury andPalmer (2002) and Carreras and Ma`rquez (2003).3 System ArchitectureOur semantic role labeling system consists of twophases.
The first phase finds a subset of argumentsfrom all possible candidates.
The goal here is tofilter out as many as possible false argument candi-dates, while still maintaining high recall.
The sec-ond phase focuses on identifying the types of thoseargument candidates.
Since the number of candi-dates is much fewer, the second phase is able to useslightly complicated features to facilitate learninga better classifier.
This section first introduces thelearning system we use and then describes how welearn the classifiers in these two phases.3.1 SNoW Learning ArchitectureThe learning algorithm used is a variation of theWinnow update rule incorporated in SNoW (Roth,1998; Roth and Yih, 2002), a multi-class classifierthat is specifically tailored for large scale learningtasks.
SNoW learns a sparse network of linear func-tions, in which the targets (argument border predic-tions or argument type predictions, in this case) arerepresented as linear functions over a common fea-ture space.
It incorporates several improvementsover the basic Winnow multiplicative update rule.In particular, a regularization term is added, whichhas the effect of trying to separate the data with athick separator (Grove and Roth, 2001; Hang et al,2002).
In the work presented here we use this regu-larization with a fixed parameter.Experimental evidence has shown that SNoWactivations are monotonic with the confidence inthe prediction.
Therefore, it can provide a goodsource of probability estimation.
We use soft-max (Bishop, 1995) over the raw activation valuesas conditional probabilities, and also the score of thetarget.
Specifically, suppose the number of classesis n, and the raw activation values of class i is acti.The posterior estimation for class i is derived by thefollowing equation.score(i) = pi = eacti?1?j?n eactjThe score plays an important role in differentplaces.
For example, the first phase uses the scoresto decide which argument candidates should be fil-tered out.
Also, the scores output by the second-phase classifier are used in the inference procedureto reason for the best global labeling.3.2 First Phase: Find Argument CandidatesThe first phase is to predict the argument candidatesof a given sentence that correspond to the activeverb.
Unfortunately, it turns out that it is difficult topredict the exact arguments accurately.
Therefore,the goal here is to output a superset of the correctarguments by filtering out unlikely candidates.Specifically, we learn two classifiers, one to de-tect beginning argument locations and the otherto detect end argument locations.
Each multi-class classifier makes predictions over forty-threeclasses?thirty-two argument types, ten continuousargument types, and one class to detect not begin-ning/not end.
Features used for these classifiers are:?
Word feature includes the current word, twowords before and two words after.?
Part-of-speech tag (POS) feature includes thePOS tags of all words in a window of size two.?
Chunk feature includes the BIO tags forchunks of all words in a window of size two.?
Predicate lemma & POS tag show the lemmaform and POS tag of the active predicate.?
Voice feature is the voice (active/passive) ofthe current predicate.
This is extracted with asimple rule: a verb is identified as passive if itfollows a to-be verb in the same phrase chunkand its POS tag is VBN(past participle) or itimmediately follows a noun phrase.?
Position feature describes if the current wordis before or after the predicate.?
Chunk pattern encodes the sequence ofchunks from the current words to the predicate.?
Clause tag indicates the boundary of clauses.?
Clause path feature is a path formed from asemi-parsed tree containing only clauses andchunks.
Each clause is named with the chunkpreceding it.
The clause path is the path frompredicate to target word in the semi-parse tree.?
Clause position feature is the position of thetarget word relative to the predicate in thesemi-parse tree containing only clauses.
Thereare four configurations ?
target word and pred-icate share the same parent, target word parentis an ancestor of predicate, predicate parent isan ancestor of target word, or otherwise.Because each argument consists of a single be-ginning and a single ending, these classifiers can beused to construct a set of potential arguments (bycombining each predicted begin with each predictedend after it of the same type).Although this phase identifies typed arguments(i.e.
labeled with argument types), the second phasewill re-score each phrase using phrase-based classi-fiers ?
therefore, the goal of the first phase is sim-ply to identify non-typed phrase candidates.
In thistask, we achieves 98.96% and 88.65% recall (over-all, without verb) on the training and the develop-ment set, respectively.
Because these are the onlycandidates passed to the second phase, the final sys-tem performance is upper-bounded by 88.65%.3.3 Second Phase: Argument ClassificationThe second phase of our system assigns the final ar-gument classes to (a subset) of the argument can-didates supplied from the first phase.
Again, theSNoW learning architecture is used to train a multi-class classifier to label each argument to one of theargument types, plus a special class?no argument(null).
Training examples are created from the argu-ment candidates supplied from the first phase usingthe following features:?
Predicate lemma & POS tag, voice, position,clause Path, clause position, chunk patternSame features as those in the first phase.?
Word & POS tag from the argument, includ-ing the first,last,and head1 word and tag.?
Named entity feature tells if the target argu-ment is, embeds, overlaps, or is embedded in anamed entity with its type.?
Chunk tells if the target argument is, embeds,overlaps, or is embedded in a chunk with itstype.?
Lengths of the target argument, in the numbersof words and chunks separately.?
Verb class feature is the class of the activepredicate described in PropBank Frames.?
Phrase type uses simple heuristics to identifythe target argument as VP, PP, or NP.?
Sub-categorization describes the phrasestructure around the predicate.
We separatethe clause where the predicate is in into threeparts?the predicate chunk, segments beforeand after the predicate, and use the sequenceof phrase types of these three segments.?
Baseline features identified not in the mainverb chunk as AM-NEG and modal verb in themain verb chunk as AM-MOD.?
Clause coverage describes how much of thelocal clause (from the predicate) is covered bythe target argument.?
Chunk pattern length feature counts the num-ber of patterns in the argument.?
Conjunctions join every pair of the above fea-tures as new features.?
Boundary words & POS tag include twowords/tags before and after the target argu-ment.?
Bigrams are pairs of words/tags in the windowfrom two words before the target to the firstword of the target, and also from the last wordto two words after the argument.1We use simple rules to first decide if a candidate phrasetype is VP, NP, or PP.
The headword of an NP phrase is theright-most noun.
Similarly, the left-most verb/proposition of aVP/PP phrase is extracted as the headword?
Sparse collocation picks one word/tag fromthe two words before the argument, the firstword/tag, the last word/tag of the argument,and one word/tag from the two words after theargument to join as features.Although the predictions of the second-phaseclassifier can be used directly, the labels of argu-ments in a sentence often violate some constraints.Therefore, we rely on the inference procedure tomake the final predictions.4 Inference via ILPIdeally, if the learned classifiers are perfect, argu-ments can be labeled correctly according to the clas-sifiers?
predictions.
In reality, labels assigned to ar-guments in a sentence often contradict each other,and violate the constraints arising from the struc-tural and linguistic information.
In order to resolvethe conflicts, we design an inference procedure thattakes the confidence scores of each individual ar-gument given by the second-phase classifier as in-put, and outputs the best global assignment thatalso satisfies the constraints.
In this section we firstintroduce the constraints and the inference prob-lem in the semantic role labeling task.
Then, wedemonstrate how we apply integer linear program-ming(ILP) to reason for the global label assignment.4.1 Constraints over Argument LabelingFormally, the argument classifier attempts to assignlabels to a set of arguments, S1:M , indexed from 1to M .
Each argument Si can take any label from aset of argument labels, P , and the indexed set ofarguments can take a set of labels, c1:M ?
PM .If we assume that the classifier returns a score,score(Si = ci), corresponding to the likelihood ofseeing label ci for argument Si, then, given a sen-tence, the unaltered inference task is solved by max-imizing the overall score of the arguments,c?1:M = argmaxc1:M?PMscore(S1:M = c1:M )= argmaxc1:M?PMM?i=1score(Si = ci).
(1)In the presence of global constraints derived fromlinguistic information and structural considerations,our system seeks for a legitimate labeling that max-imizes the score.
Specifically, it can be viewed asthe solution space is limited through the use of a fil-ter function, F , that eliminates many argument la-belings from consideration.
It is interesting to con-trast this with previous work that filters individualphrases (see (Carreras and Ma`rquez, 2003)).
Here,we are concerned with global constraints as well asconstraints on the arguments.
Therefore, the finallabeling becomesc?1:M = argmaxc1:M?F(PM )M?i=1score(Si = ci) (2)The filter function used considers the following con-straints:1.
Arguments cannot cover the predicate exceptthose that contain only the verb or the verb andthe following word.2.
Arguments cannot overlap with the clauses(they can be embedded in one another).3.
If a predicate is outside a clause, its argumentscannot be embedded in that clause.4.
No overlapping or embedding arguments.5.
No duplicate argument classes for A0?A5,V.6.
Exactly one V argument per verb.7.
If there is C-V, then there should be a sequenceof consecutive V, A1, and C-V pattern.
For ex-ample, when split is the verb in ?split it up?,the A1 argument is ?it?
and C-V argument is?up?.8.
If there is an R-XXX argument, then there hasto be an XXX argument.
That is, if an ar-gument is a reference to some other argumentXXX, then this referenced argument must existin the sentence.9.
If there is a C-XXX argument, then there hasto be an XXX argument; in addition, the C-XXX argument must occur after XXX.
This isstricter than the previous rule because the orderof appearance also needs to be considered.10.
Given the predicate, some argument classesare illegal (e.g.
predicate ?stalk?
can take onlyA0 or A1).
This linguistic information can befound in PropBank Frames.We reformulate the constraints as linear(in)equalities by introducing indicator variables.The optimization problem (Eq.
2) is solved usingILP.4.2 Using Integer Linear ProgrammingAs discussed previously, a collection of potential ar-guments is not necessarily a valid semantic label-ing since it must satisfy all of the constraints.
Inthis context, inference is the process of finding thebest (according to Equation 1) valid semantic labelsthat satisfy all of the specified constraints.
We takea similar approach that has been previously usedfor entity/relation recognition (Roth and Yih, 2004),and model this inference procedure as solving anILP.An integer linear program(ILP) is basically thesame as a linear program.
The cost function and the(in)equality constraints are all linear in terms of thevariables.
The only difference in an ILP is the vari-ables can only take integers as their values.
In ourinference problem, the variables are in fact binary.A general binary integer programming problem canbe stated as follows.Given a cost vector p ?
<d, a set of variables,z = (z1, .
.
.
, zd) and cost matrices C1 ?
<t1 ?<d,C2 ?
<t2?<d , where t1 and t2 are the numbersof inequality and equality constraints and d is thenumber of binary variables.
The ILP solution z?
isthe vector that maximizes the cost function,z?
= argmaxz?
{0,1}dp ?
z,subject to C1z ?
b1, and C2z = b2,where b1,b2 ?
<d, and for all z ?
z, z ?
{0, 1}.To solve the problem of Equation 2 in this set-ting, we first reformulate the original cost function?Mi=1 score(Si = ci) as a linear function over sev-eral binary variables, and then represent the filterfunction F using linear inequalities and equalities.We set up a bijection from the semantic labelingto the variable set z.
This is done by setting z to a setof indicator variables.
Specifically, let zic = [Si =c] be the indicator variable that represents whetheror not the argument type c is assigned to Si, andlet pic = score(Si = c).
Equation 1 can then bewritten as an ILP cost function asargmaxz?
{0,1}dM?i=1|P|?c=1piczic,subject to|P|?c=1zic = 1 ?zic ?
z,which means that each argument can take only onetype.
Note that this new constraint comes from thevariable transformation, and is not one of the con-straints used in the filter function F .Constraints 1 through 3 can be evaluated on a per-argument basis ?
the sake of efficiency, argumentsthat violate these constraints are eliminated evenbefore given the second-phase classifier.
Next, weshow how to transform the constraints in the filterfunction into the form of linear (in)equalities overz, and use them in this ILP setting.Constraint 4: No overlapping or embedding Ifarguments Sj1 , .
.
.
, Sjk occupy the same word in asentence, then this constraint restricts only one ar-guments to be assigned to an argument type.
Inother words, k ?
1 arguments will be the specialclass null, which means the argument candidate isnot a legitimate argument.
If the special class nullis represented by the symbol ?, then for every set ofsuch arguments, the following linear equality repre-sents this constraint.k?i=1zji?
= k ?
1Constraint 5: No duplicate argument classesWithin the same sentence, several types of argu-ments cannot appear more than once.
For example,a predicate can only take one A0.
This constraintcan be represented using the following inequality.M?i=1ziA0 ?
1Constraint 6: Exactly one V argument For eachverb, there is one and has to be one V argument,which represents the active verb.
Similarly, this con-straint can be represented by the following equality.M?i=1ziV = 1Constraint 7: V?A1?C-V pattern This con-straint is only useful when there are three consec-utive candidate arguments in a sentence.
Supposearguments Sj1 , Sj2 , Sj3 are consecutive.
If Sj3 isC-V, then Sj1 and Sj2 have to be V and A1, respec-tively.
This if-then constraint can be represented bythe following two linear inequalities.zj3C-V ?
zj1V, and zj3C-V ?
zj2A1Constraint 8: R-XXX arguments Suppose thereferenced argument type is A0 and the referencetype is R-A0.
The linear inequalities that representthis constraint are:?m ?
{1, .
.
.
,M} :M?i=1ziA0 ?
zmR-A0If there are ?
reference argument pairs, then thetotal number of inequalities needed is ?M .Constraint 9: C-XXX arguments This con-straint is similar to the reference argument con-straints.
The difference is that the continued argu-ment XXX has to occur before C-XXX.
Assumethat the argument pair is A0 and C-A0, and argu-ment Sji appears before Sjk if i ?
k. The linearinequalities that represent this constraint are:?m ?
{2, .
.
.
,M} :j?1?i=1zjiA0 ?
zmR-A0Constraint 10: Illegal argument types Given aspecific verb, some argument types should never oc-cur.
For example, most verbs don?t have argumentsA5.
This constraint is represented by summing allthe corresponding indicator variables to be 0.M?i=1ziA5 = 0Using ILP to solve this inference problem en-joys several advantages.
Linear constraints arevery general, and are able to represent many typesof constraints.
Previous approaches usually relyon dynamic programming to resolve non over-lapping/embedding constraints (i.e., Constraint 4)when the data is sequential, but are unable to han-dle other constraints.
The ILP approach is flexibleenough to handle constraints regardless of the struc-ture of the data.
Although solving an ILP prob-lem is NP-hard, with the help of todays commer-cial numerical packages, this problem can usuallybe solved very fast in practice.
For instance, it onlytakes about 10 minutes to solve the inference prob-lem for 4305 sentences on a Pentium-III 800 MHzmachine in our experiments.
Note that ordinarysearch methods (e.g., beam search) are not neces-sarily faster than solving an ILP problem and do notguarantee the optimal solution.5 Experimental ResultsThe system is evaluated on the data provided inthe CoNLL-2004 semantic-role labeling shared taskwhich consists of a portion of PropBank corpus.The training set is extracted from TreeBank (Mar-cus et al, 1993) section 15?18, the development set,used in tuning parameters of the system, from sec-tion 20, and the test set from section 21.We first compare this system with the basic taggerthat we have, the CSCL shallow parser from (Pun-yakanok and Roth, 2001), which is equivalent to us-ing the scoring function from the first phase withonly the non-overlapping/embedding constraints.
InPrec.
Rec.
F?=11st-phase, non-overlap 70.54 61.50 65.711st-phase, All Const.
70.97 60.74 65.462nd-phase, non-overlap 69.69 64.75 67.132nd-phase, All Const.
71.96 64.93 68.26Table 1: Summary of experiments on the developmentset.
All results are for overall performance.Precision Recall F?=1Without Inference 86.95 87.24 87.10With Inference 88.03 88.23 88.13Table 2: Results of second phase phrase predictionand inference assuming perfect boundary detection inthe first phase.
Inference improves performance by re-stricting label sequences rather than restricting structuralproperties since the correct boundaries are given.
All re-sults are for overall performance on the development set.addition, we evaluate the effectiveness of using onlythis constraint versus all constraints, as in Sec.
4.Table 1 shows how additional constraints over thestandard non-overlapping constraints improve per-formance on the development set.
The argumentscoring is chosen from either the first phase or thesecond phase and each is evaluated by consideringsimply the non-overlapping/embedding constraintor the full set of linguistic constraints.
To makea fair comparison, parameters were set separatelyto optimize performance when using the first phaseresults.
In general, using all constraints increasesF?=1 by about 1% in this system, but slightly de-creases the performance when only the first phaseclassifier is used.
Also, using the two-phase archi-tecture improves both precision and recall, and theenhancement reflected in F?=1 is about 2.5%.It is interesting to find out how well the secondphase classifier can perform given perfectly seg-mented arguments.
This evaluates the quality of theargument classifier, and also provides a conceptualupper bound.
Table 2 first shows the results withoutusing inference (i.e.
F(PM ) = PM ).
The secondrow shows adding inference to the phrase classifica-tion can further improve F?=1 by 1%.Finally, the overall result on the official test setis given in Table 3.
Note that the result here is notcomparable with the best in this domain (Pradhan etal., 2004) where the full parse tree is assumed given.For a fair comparison, our system was among thebest at CoNLL-04, where the best system (Haciogluet al, 2004) achieve a 69.49 F1 score.6 ConclusionWe show that linguistic information is useful for se-mantic role labeling, both in extracting features andDist.
Prec.
Rec.
F?=1Overall 100.00 70.07 63.07 66.39A0 26.87 81.13 77.70 79.38A1 35.73 74.21 63.02 68.16A2 7.44 54.16 41.04 46.69A3 1.56 47.06 26.67 34.04A4 0.52 71.43 60.00 65.22AM-ADV 3.20 39.36 36.16 37.69AM-CAU 0.51 45.95 34.69 39.53AM-DIR 0.52 42.50 34.00 37.78AM-DIS 2.22 52.00 67.14 58.61AM-EXT 0.15 46.67 50.00 48.28AM-LOC 2.38 33.47 34.65 34.05AM-MNR 2.66 45.19 36.86 40.60AM-MOD 3.51 92.49 94.96 93.70AM-NEG 1.32 85.92 96.06 90.71AM-PNC 0.89 32.79 23.53 27.40AM-TMP 7.78 59.77 56.89 58.30R-A0 1.66 81.33 76.73 78.96R-A1 0.73 58.82 57.14 57.97R-A2 0.09 100.00 22.22 36.36R-AM-TMP 0.15 54.55 42.86 48.00Table 3: Results on the test set.deriving hard constraints on the output.
We alsodemonstrate that it is possible to use integer linearprogramming to perform inference that incorporatesa wide variety of hard constraints, which would bedifficult to incorporate using existing methods.
Inaddition, we provide further evidence supportingthe use of scoring arguments over scoring argumentboundaries for complex tasks.
In the future, we planto use the full PropBank corpus to see the improve-ment when more training data is provided.
In addi-tion, we would like to explore the possibility of in-teger linear programming approach using soft con-straints.
As more constraints are considered, we ex-pect the overall performance to improve.7 AcknowledgmentsWe thank Xavier Carreras and Llu?
?s Ma`rquez for thedata and scripts, Martha Palmer and the anonymousreferees for their useful comments, AMD for theirequipment donation, and Dash Optimization for thefree academic use of their Xpress-MP software.This research is supported by NSF grants ITR-IIS-0085836, ITR-IIS-0085980 and IIS-9984168, EIA-0224453 and an ONR MURI Award.ReferencesC.
Bishop, 1995.
Neural Networks for Pattern Recogni-tion, chapter 6.4: Modelling conditional distributions,page 215.
Oxford University Press.X.
Carreras and L. Ma`rquez.
2003.
Phrase recognitionby filtering and ranking with perceptrons.
In Proc.
ofRANLP-2003.J.
Chen and O. Rambow.
2003.
Use of deep linguisticfeatures for the recognition and labeling of semanticarguments.
In Proc.
of EMNLP-2003, Sapporo, Japan.D.
Gildea and J. Hockenmaier.
2003.
Identifying se-mantic roles using combinatory categorial grammar.In Proc.
of the EMNLP-2003, Sapporo, Japan.D.
Gildea and M. Palmer.
2002.
The necessity of parsingfor predicate argument recognition.
In Proc.
of ACL2002, pages 239?246, Philadelphia, PA.A.
Grove and D. Roth.
2001.
Linear concepts and hid-den variables.
Machine Learning, 42(1/2):123?141.K.
Hacioglu, S. Pradhan, W. Ward, J. H. Martin, andD.
Jurafsky.
2004.
Semantic role labeling by taggingsyntactic chunks.
In Proc.
of CoNLL-04.T.
Hang, F. Damerau, and D. Johnson.
2002.
Textchunking based on a generalization of winnow.
J. ofMachine Learning Research, 2:615?637.P.
Kingsbury and M. Palmer.
2002.
From Treebank toPropBank.
In Proc.
of LREC-2002, Spain.M.
P. Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330, June.S.
Pradhan, K. Hacioglu, W. ward, J. Martin, and D. Ju-rafsky.
2003.
Semantic role parsing adding semanticstructure to unstructured text.
In Proc.
of ICDM-2003,Melbourne, FL.S.
Pradhan, W. Ward, K. Hacioglu, J. H. Martin, andD.
Jurafsky.
2004.
Shallow semantic parsing usingsupport vector machines.
In Proc.
of NAACL-HLT2004.V.
Punyakanok and D. Roth.
2001.
The use of classi-fiers in sequential inference.
In NIPS-13; The 2000Conference on Advances in Neural Information Pro-cessing Systems, pages 995?1001.
MIT Press.D.
Roth and W. Yih.
2002.
Probabilistic reasoning forentity & relation recognition.
In Proc.
of COLING-2002, pages 835?841.D.
Roth and W. Yih.
2004.
A linear programmingformulation for global inference in natural languagetasks.
In Proc.
of CoNLL-2004.D.
Roth.
1998.
Learning to resolve natural language am-biguities: A unified approach.
In Proc.
of AAAI, pages806?813.M.
Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.2003.
Using predicate-argument structures for infor-mation extraction.
In Proc.
of ACL 2003.E.
F. Tjong Kim Sang and S. Buchholz.
2000.
Introduc-tion to the CoNLL-2000 shared task: Chunking.
InProc.
of the CoNLL-2000 and LLL-2000.E.
F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the conll-2003 shared task: Language-independent named entity recognition.
In Proc.
ofCoNLL-2003.E.
F. Tjong Kim Sang and H. De?jean.
2001.
Introductionto the CoNLL-2001 shared task: Clause identification.In Proc.
of the CoNLL-2001.Xpress-MP.
2003.
Dash Optimization.
Xpress-MP.http://www.dashoptimization.com/products.html.
