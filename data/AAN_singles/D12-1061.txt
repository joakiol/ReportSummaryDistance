Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 666?676, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsLearning Lexicon Models from Search Logs for Query ExpansionJianfeng GaoMicrosoft Research, RedmondWashington 98052, USAjfgao@microsoft.comShasha XieEducational Testing Service, PrincetonNew Jersey 08540, USAsxie@ets.orgXiaodong HeMicrosoft Research, RedmondWashington 98052, USAxiaohe@microsoft.comAlnur AliMicrosoft Bing, BellevueWashington 98004, USAalnurali@microsoft.comAbstractThis paper explores log-based query expan-sion (QE) models for Web search.
Threelexicon models are proposed to bridge thelexical gap between Web documents anduser queries.
These models are trained onpairs of user queries and titles of clickeddocuments.
Evaluations on a real world dataset show that the lexicon models, integratedinto a ranker-based QE system, not onlysignificantly improve the document retriev-al performance but also outperform twostate-of-the-art log-based QE methods.1 IntroductionTerm mismatch is a fundamental problem in Websearch, where queries and documents are com-posed using different vocabularies and languagestyles.
Query expansion (QE) is an effective strate-gy to address the problem.
It expands a query is-sued by a user with additional related terms, calledexpansion terms, so that more relevant documentscan be retrieved.In this paper we explore the use of clickthroughdata and translation models for QE.
We select ex-pansion terms for a query according to how likelyit is that the expansion terms occur in the title of adocument that is relevant to the query.
Assumingthat a query is parallel to the titles of documentsclicked for that query (Gao et alicon models are trained on query-title pairs ex-tracted from clickthrough data.
The first is a wordmodel that learns the translation probability be-tween single words.
The second model uses lexi-calized triplets to incorporate word dependenciesfor translation.
The third is a bilingual topic model,which represents a query as a distribution of hid-den topics and learns the translation between aquery and a title term at the semantic level.
Wewill show that the word model provides a rich setof expansion candidates while the triplet and topicmodels can effectively select good expansionterms, and that a ranker-based QE system whichincorporates all three of these models not only sig-nificantly improves Web search result but outper-forms other log-based QE methods that are state-of-the-art.There is growing interest in applying user logsto improve QE.
A recent survey is due to Baeze-Yates and Ribeiro-Neto (2011).
Below, we brieflydiscuss two log-based QE methods that are closestto ours and are re-implemented in this study forcomparison.
Both systems use the same type of logdata that we used to train the lexicon models.
Theterm correlation model of Cui et alto our knowledge the first to explore query-document relations for direct extraction of expan-sion terms for Web search.
The method outper-forms traditional QE methods that do not use logdata e.g.
the local analysis model of Xu and Croft(1996).
In addition, as pointed out by Cui et al(2003) there are three important advantages thatmake log-based QE a promising technology to im-prove the performance of commercial search en-gines.
First, unlike traditional QE methods that arebased on relevance feedback, log-based QE derivesexpansion terms from search logs, allowing termcorrelations to be pre-computed offline.
Comparedto methods that are based on thesauri either com-piled manually (Prager et alu-666tomatically from document collections (Jing andCroft 1994), the log-based method is superior inthat it explicitly captures the correlation betweenquery terms and document terms, and thus canbridge the lexical gap between them more effec-tively.
Second, since search logs retrain query-document pairs clicked by millions of users, theterm correlations reflect the preference of the ma-jority of users.
Third, the term correlations evolvealong with the accumulation of user logs, thus canreflect updated user interests at a specific time.However, as pointed out by Riezler et al(2008), Cui et ald method suf-fers low precision of QE partly because the corre-lation model does not explicitly capture contextinformation and is susceptible to noise.
Riezler etal.
developed a QE system by retraining a standardphrase-based statistical machine translation (SMT)system using query-snippet pairs extracted fromclickthrough data (Riezler et alLiu 2010).
The SMT-based system can producecleaner, more relevant expansion terms becauserich context information useful for filtering noisyexpansions is captured by combining languagemodel and phrase translation model in its decoder.Furthermore, in the SMT system all componentmodels are properly smoothed using sophisticatedtechniques to avoid sparse data problems while thecorrelation model relies on pure counts of termfrequencies.
However, the SMT system is used as ablack box in their experiments.
So the relative con-tribution of different SMT components is not veri-fied empirically.
In this study we break this blackbox in order to build a better, simpler QE system.We will show that the proposed lexicon modelsoutperform significantly the term correlation mod-el, and that a simpler QE system that incorporatesthe lexicon models can beat the sophisticated,black-box SMT system.2 Lexicon ModelsWe view search queries and Web documents astwo different languages, and cast QE as a means tobridge the language gap by translating queries todocuments, represented by their titles.
In this sec-tion, we will describe three translation models thatare based on terms, triplets, and topics, respective-ly, and the way these models are learned from que-ry-title pairs extracted from clickthrough data.2.1 Word ModelThe word model takes the form of IBM Model 1(Brown et alafferty 1999).
Letbe a query,   be an expansion termcandidate, the translation probability from   to   isdefined as|     ?
( |  ) (  | )(1)where    |   is the unsmoothed unigram proba-bility of word   in query  .
The word translationprobabilities    |   are estimated on the query-title pairs derived from the clickthrough data byassuming that the title terms are likely to be thedesired expansions of the paired query.
Our train-ing method follows the standard procedure oftraining statistical word alignment models pro-posed by Brown et al we opti-mize the model parameters   by maximizing theprobability of generating document titles from que-ries over the entire training corpus:?
|(2)where both the titles   and the paired queries   areviewed as bag of words.
The translation probability|      takes the form of IBM Model 1 as|??
(  |    )(3)where   is a constant,   is the length of  , and   isthe length of  .
To find the optimal word transla-tion probabilities of IBM Model 1, we used the EMalgorithm, where the number of iterations is deter-mined empirically on held-out data.2.2 Triplet ModelThe word model is context independent.
The tripletmodel, which is originally proposed for SMT (Ha-san et al to capture inter-termdependencies for selecting expansion terms.
Themodel is based on lexicalized triplets (       )which can be understood as two query terms trig-gering one expansion term.
The translation proba-bility of   given   for the triplet model is parame-terized as667|?
?
( |     )(4)where Z is a normalization factor based on the cor-responding query length, i.e.,, and(  |     ) is the probability of translating    intogiven another query word   .
Since    can beany word in   that is not necessary to be adjacentto   , the triple model is able to combine local (i.e.word and phrase level) and global (i.e.
query level)contextual information useful for word translation.Similar to the case of word model, we used theEM algorithm to estimate the translation probabili-ties    |      on the query-title pairs.
Since thenumber of all possible triplets (      ) is large andas a consequence the model training could sufferthe data sparseness problem, in our experimentscount-based cutoff is applied to prune the model toa manageable size.2.3 Bilingual Topic Model (BLTM)The BLTM was originally proposed for Web doc-ument ranking by Gao et aln-derlying the model is that a search query and itsrelevant Web documents share a common distribu-tion of (hidden) topics, but use different (probablyoverlapping) vocabularies to express these topics.Intuitively, BLTM-based QE works as follows.First, a query is represented as a vector of topics.Then, all the candidate expansion terms, which areselected from document, are ranked by how likelyit is that these document terms are selected to bestdescribe those topics.
In a sense, BLTM is similarto the word model and the triplet model since theyall map a query to a document word.
BLTM differsin that the mapping is performed at the topic level(via a language independent semantic representa-tion) rather than at the word level.
In our experi-ments BLTM is found to often select a different setof expansion terms and is complementary to theword model and the triplet model.Formally, BLTM-based QE assumes the follow-ing story of generating   from  :1.
First, for each topic  , a pair of differentword distributionsare selectedfrom a Dirichlet prior with concentration pa-rameter ?, whereis a topic-specific queryterm distribution, anda topic-specificdocument term distribution.
Assuming thereare   topics, we have two sets of distribu-tionsand.2.
Given  , a topic distribution    is drawnfrom a Dirichlet prior with concentration pa-rameter  .3.
Then a document term (i.e., expansion termcandidate)   is generated by first selecting atopic   according to the topic distribution   ,and then drawing a word from.By summing over all possible topics, we end upwith the following model form|   ?
||(5)The BLTM training follows the method describedin Gao et ale EM algorithm toestimate the parameters (         of BLTM bymaximizing the joint log-likelihood of the query-title pairs and the parameters.
In training, we alsoconstrain that the paired query and title have simi-lar fractions of tokens assigned to each topic.
Theconstraint is enforced on expectation using posteri-or regularization (Ganchev et al3 A Ranker-Based QE SystemThis section describes a ranker-based QE system inwhich the three lexicon models described aboveare incorporated.
The system expands an inputquery in two distinct stages, candidate generationand ranking, as illustrated by an example in Figure1.Original query jaguar locatorRanked expansion  jaguar findercandidates(altered words are incar locatorjaguar locationitalic) jaguar directory?jaguar listExpanded query OR(jaguar, car)(selected expansionterms are in italic)OR(locator, finder, location,directory)Figure 1.
An example of an original query, its expan-sion candidates and the expanded query generated bythe ranker-based QE system.668In candidate generation, an input query   isfirst tokenized into a sequence of terms.
For eachterm   that is not a stop word, we consult a wordmodel described in Section 2.1 to identify the bestaltered words according to their word transla-tion probabilities from  .
Then, we form a list ofexpansion candidates, each of which contains allthe original words in   except for the word that issubstituted by one of its altered words.
So, for aquery with   terms, there are at most    candi-dates.In the second stage, all the expansion candidatesare ranked using a ranker that is based on the Mar-kov Random Field (MRF) model in which thethree lexicon models are incorporated as features.Expansion terms of a query are taken from thoseterms in the  -best (     in our experiments)expansion candidates of the query that have notbeen seen in the original query string.In the remainder of this section we will describein turn the MRF-based ranker, the ranking features,and the way the ranker parameters are estimated.3.1 MRF-Based RankerThe ranker is based on the MRF model that modelsthe joint distribution of         over a set of ex-pansion term random variables             anda query random variable  .
It is constructed from agraph   consisting of a query node and nodes foreach expansion term.
Nodes in the graph representrandom variables and edges define the independ-ence semantics between the variables.
An MRFsatisfies the Markov property (Bishop 2006),which states that a node is independent of all of itsnon-neighboring nodes given observed values ofits neighbors, defined by the clique configurationsof  .
The joint distribution over the random varia-bles in   is defined as?
(6)where      is the set of cliques in  , and eachis a non-negative potential function de-fined over a clique configuration c that measuresthe compatibility of the configuration,   is a set ofparameters that are used within the potential func-tion, and    normalizes the distribution.
For rank-ing expansion candidates, we can drop the expen-sive computation of    since it is independent ofE, and simply rank each expansion candidate   byits unnormalized joint probability with   under theMRF.
It is common to define MRF potential func-tions of the exponential form as, where      is a real-valued featurefunction over clique values and    is the weight ofthe feature function.
Then, we can compute theposterior     |   as|(7)?
?
?which is essentially a weighted linear combinationof a set of features.Therefore, to instantiate the MRF model, oneneeds to define a graph structure and a set of po-tential functions.
In this paper, the graphical modelrepresentation we propose for QE is a fully con-nected graph shown in Figure 2, where all expan-sion terms and the original query are assumed de-pendent with each other.
In what follows, we willdefine six types of cliques that we are interested indefining features (i.e., potential functions) over.3.2 FeaturesThe cliques and features are inspired by the com-ponent models used in SMT systems.
The cliquesdefined in   for MRF can be grouped into two cat-egories.
The first includes three types of cliquesinvolving both the query node and one or moreexpansion terms.
The potential functions definedover these cliques attempt to abstract the idea be-hind the query to title translation models.
The otherthree types, belonging to the second category, in-volve only expansion terms.
Their potential func-Figure 2: The structure of the Markov random field forrepresenting the term dependency among the queryand the expansion terms        .669tions attempt to abstract the idea behind the targetlanguage models.The first type of cliques involves a single ex-pansion term and the query node.
The potentialsfunctions for these cliques are defined as(6)where the three feature functions of the formare defined as the log probabilities oftranslating   to   according to the word, triplet andtopic models defined in Equations (1), (4) and (5),respectively.|||The second type of cliques contains the querynode and two expansion terms,    and     , whichappear in consecutive order in the expansion.
Thepotential functions over these cliques are definedas(7)where the feature        is defined as the log prob-ability of generating an expansion bigram given|               |Unlike the language models used for documentranking (e.g., Zhai and Lafferty 2001), we cannotcompute the bigram probability by simply countingthe relative frequency of           in   becausethe query is usually very short and the bigram isunlikely to occur.
Thus, we approximate the bi-gram probability by assuming that the words inare independent with each other.
We thus have||   ?
|?where     |         is the translation probabilitycomputed using a variant of the triplet model de-scribed  in Section 2.2.
The model variation differsfrom the one of Equation (4) in two respects.
First,it models the translation in a different direction i.e.,from expansion to query.
Second, we add a con-straint to the triplets such that (       ) must be anordered, contiguous bigram.
The model variation isalso trained using EM on query-title pairs.and       |    are assigned respectively by theunigram and bigram language models, estimatedfrom the collection of document titles of the click-through data, and        is the unigram probabilityof the query term, estimated from the collection ofqueries of the clickthrough data.The third type of cliques contains the querynode and two expansion terms,    and   , whichoccur unordered within the expansion.
The poten-tial functions over these cliques are defined as(8)where the feature        is defined as the log prob-ability of generating a pair of expansion termsgiven|             |  .Unlike            |   defined in Equation (7), thisclass of features captures long-span term depend-ency in the expansion candidate.
Similar to thecomputation of          |   in Equation (7), weapproximate        |   as||   ?
|?where     |       is the translation probabilitycomputed using the triplet model described  in Sec-tion 2.2, but in the expansion-to-query direction.is assigned by a unigram language modelestimated from the collection of document titles ofthe clickthrough data.
|    is assigned by a co-occurrence model, estimated as|?where         is the number of times that the twoterms occur in the same title in clickthrough data.We now turn to the other three types of cliquesthat do not contain the query node.
The fourth typeof cliques contains only one expansion term.
Thepotential functions are defined as670(9)where       is the unigram probability computedusing a unigram language model trained on thecollection of document titles.The fifth type of cliques contains a pair of termsappearing in consecutive order in the expansion.The potential functions are defined as(10)|where       |    is the bigram probability com-puted using a bigram language model trained onthe collection of document titles.The sixth type of cliques contains a pair ofterms appearing unordered within the expansion.The potential functions are defined as(11)|where     |    is the assigned by a co-occurrencemodel trained on the collection of document titles.3.3 Parameter EstimationThe MRF model uses 8 classes of features definedon 6 types of cliques, as in Equations (6) to (11).Following previous work (e.g., Metzler and Croft2005; Bendersky et alhat allfeatures within the same feature class are weightedby the same tied parameter   .
Thus, the number offree parameters of the MRF model is significantlyreduced.
This not only makes the model trainingeasier but also improves the robustness of themodel.
After tying the parameters and using theexponential potential function form, the MRF-based ranker can be parameterized as|?
?(12)????
????
?where there are in total 8  ?s to be estimated.Although the MRF is by nature a generativemodel, it is not always appropriate to train the pa-rameters using conventional likelihood based ap-proaches due to the metric divergence problem(Morgan et alaximum likelihoodestimate is unlikely to be the one that optimizes theevaluation metric.
In this study the effectiveness ofa QE method is evaluated by first issuing a set ofqueries which are expanded using the method to asearch engine and then measuring the Web searchperformance.
Better QE methods are supposed tolead to better Web search results using the corre-spondingly expanded query set.For this reason, the parameters of the MRF-based ranker are optimized directly for Websearch.
In our experiments, the objective in train-ing is Normalized Discounted Cumulative Gain(NDCG, Jarvelin and Kekalainen 2000), which iswidely used as quality measure for Web search.Formally, we view parameter training as a multi-dimensional optimization problem, with each fea-ture class as one dimension.
Since NDCG is notdifferentiable, we tried in our experiments numeri-cal algorithms that do not require the computationof gradient.
Among the best performers was thePowell Search algorithm (Press et alconstructs a set of   virtual directions that are con-jugate (i.e., independent with each other), then ituses line search  times (    in our case), eachon one virtual direction, to find the optimum.
Linesearch is a one-dimensional optimization algo-rithm.
Our implementation follows the one de-scribed in Gao et alsed to opti-mize averaged precision.4 ExperimentsWe evaluate the performance of a QE method byfirst issuing a set of queries which are expandedusing the method to a search engine and then671measuring the Web search performance.
Better QEmethods are supposed to lead to better Web searchresults using the correspondingly expanded queryset.Due to the characteristics of our QE methods,we cannot conduct experiments on standard testcollections such as the TREC data because they donot contain related user logs we need.
Therefore,following previous studies of log-based QE (e.g.,Cui et alltheproprietary datasets that have been developed forbuilding a commercial search engine, and demon-strate the effectiveness of our methods by compar-ing them against previous state-of-the-art log-based QE methods.The relevance judgment set consists of 4,000multi-term English queries.
On average, each que-ry is associated with 197 Web documents (URLs).Each query-URL pair has a relevance label.
Thelabel is human generated and is on a 5-level rele-vance scale, 0 to 4, with 4 meaning document D isthe  most  relevant  to  query Q  and 0 meaning  Dis  not  relevant to Q.The relevance judgment set is constructed asfollows.
First, the queries are sampled from a yearof search engine logs.
Adult, spam, and bot queriesare all removed.
Queries are ?de-duped?
so thatonly unique queries remain.
To reflect a naturalquery distribution, we do not try to control thequality of these queries.
For example, in our querysets, there are roughly 20% misspelled queries, 20%navigational queries, and 10% transactional que-ries.
Second, for each query, we collect Web doc-uments to be judged by issuing the query to severalpopular search engines (e.g., Google, Bing) andfetching retrieval results from each.
Finally, thequery-document pairs are judged by a group ofwell-trained assessors.
In this study all the queriesare preprocessed as follows.
The text is white-space tokenized and lowercased, numbers are re-tained, and no stemming/inflection treatment isperformed.
We split the judgment set into two non-overlapping datasets, namely training and test sets,respectively.
Each dataset contains 2,000 queries.The query-title pairs used for model training areextracted from one year of query log files using aprocedure similar to Gao et alperiments we used a randomly sampled subset of20,692,219 pairs that do not overlap the queriesand documents in the test set.Our Web document collection consists of ap-proximately 2.5 billion Web pages.
In the retrievalexperiments we use the index based on the contentfields (i.e., body and title text) of each Web page.The Web search performance is evaluated bymean NDCG.
We report NDCG scores at trunca-tion levels of 1, 3, and 10.
We also perform a sig-nificance test using the paired t-test.
Differencesare considered statistically significant when p-value is less than 0.05.4.1 Comparing SystemsTable 1 shows the main document ranking resultsusing different QE systems, developed and evalu-ated using the datasets described above.NoQE (Row 1) is the baseline retrieval systemthat uses the raw input queries and the BM25 doc-ument ranking model.
Rows 2 to 4 are different QEsystems.
Their results are obtained by first expand-ing a query, then using BM25 to rank the docu-ments with respect to the expanded query.TC (Row 2) is our implementation of the corre-lation-based QE system (Cui et altakes the following steps to expand an input query:# QE methods NDCG@1 NDCG@3 NDCG@101 NoQE 34.70 36.50 41.542 TC 33.78 36.57 42.33?3 SMT 34.79?
36.98 ??
42.84 ?
?4 MRF 36.10 ???
38.06 ???
43.71 ??
?5 MRFum+bm+cm 33.31 36.12 42.26?6 MRFtc 34.50?
36.59 42.33 ?7 MRFwm 34.73?
36.62 42.73 ?
?8 MRFtm 35.13??
37.46 ???
42.82 ?
?9 MRFbltm 34.34?
36.19 41.98 ?10 MRFwm+tm 35.21???
37.46 ???
42.83 ?
?11 MRFwm+tm+bltm 35.84???
37.70 ???
43.14 ??
?Table 1: Ranking results using BM25 with differentquery expansion systems.
The superscripts      andindicate statistically significant improvementsover NoQE, TC, and SMT, respectively.Rows 5 to 11 are different versions of MRF in Row 5,They use the same candidate generator but use in theranker different feature classes, as specified by thesubscript.
tc specifies the feature class defined as thescoring function in Equation (13).
Refer to Equation(12) for the names of other feature classes.6721.
Extract all query terms   (eliminatingstopwords) from  .2.
Find all documents that have clicks on aquery that contains one or more of thesequery terms.3.
For each title term   in these documents,calculate its evidence of being selected asan expansion term according to the wholequery via a scoring function        |   .4.
Select n title terms with the highest score(where the value of n is optimized on train-ing data) and formulate the expanded que-ry by adding these terms into  .5.
Use the expanded query to rank documents.The scoring function is based on the term correla-tion model, and is defined as|     (?
|) (13)|   ?
|     |where    is the set of documents clicked for thequeries containing the term   and is collected fromsearch logs,    |   is a normalized tf-idf weightof the document term in  , and    |   is the rela-tive occurrence of   among all the documentsclicked for the queries containing  .
Table 1 showsthat TC leads to significant improvement overNoQE in NDCG@10, but not in NDCG@1 andNDCG@3 (Row 2 vs. Row 1).
The result is notentirely consistent with what reported in Cui et al(2003).
A possible reason is that Cui et alformed the evaluation using documents and searchlogs collected from the Encarta website, which ismuch cleaner and more homogenous than the datasets we used.
The result suggests that although QEimproves the recall of relevant documents, it isalso likely to introduce noise that hurts the preci-sion of document retrieval.SMT (Row 3) is a SMT-based QE system.
Fol-lowing Riezler et al is an im-plementation of a phrase-based SMT system with astandard set of features for translation model andlanguage model, combined under a log linear mod-el framework (Koehn et alromRiezler et al translation modelis trained on query-snippet pairs and the languagemodel on queries, in our implementation the trans-lation model is trained on query-title pairs and thelanguage model on titles.
To apply the system toQE, expansion terms of a query are taken fromthose terms in the 10-best translations of the querythat have not been seen in the original query string.We see that SMT significantly outperforms TC inNDCG at all levels.
The result confirms the con-clusion of Riezler et alt contextinformation is crucial for improving retrieval pre-cision by filtering noisy expansions.Both TC and SMT, considered as state-of-the-art QE methods, have been frequently used forcomparison in related studies.
Thus, we also usedthem as baselines in our experiments.MRF (Row 4) is the ranker-based QE systemdescribed in Section 3, which uses a MRF-basedranker to incorporate all 8 classes of features de-rived from a variety of lexicon translation modelsand language models as in Equation (12).
Resultsshow that the ranker-based QE system significantlyoutperforms both NoQE and the two state-of-the-art QE methods.
The fact that MRF beats SMTwith a statistically significant margin although theformer is a much simpler system indicates that texttranslation and QE are different tasks and someSMT components, designed for the task of regulartext translation, are not as effective in selectingexpansion terms.
We will explore this in more de-tail in the next section.4.2 Comparing ModelsThe experiments presented in this section investi-gate in detail the effectiveness of different models,e.g., the lexicon models and the language modelsdescribed in Sections 2 and 3, in ranking expansioncandidates for QE.
The results are summarized inRows 5 to 11 in Table 1, where a number of differ-ent versions of the ranker-based QE system arecompared.
These versions, labeled as MRFf, usethe same candidate generator, and differ in the fea-ture classes (which are specified by the subscript f)incorporated in the MRF-based ranker.
In whatfollows, we focus our discussion on the results ofthe three lexicon models.MRFwm (Row 7) uses the word translationmodel described in Section 2.1.
Both the wordmodel and term correlation model used in MRFtm(Row 6) are context independent.
They differmainly in the training methods.
For the sake ofcomparison, in our experiment the word model is673EM-trained with the correlation model as initialpoint.
Rezler et al that statisti-cal translation model is superior to correlationmodel because the EM training captures the hiddenalignment information when mapping documentterms to query terms, leading to a better smoothedprobability distribution.
Our result (Row 7 vs. Row6) verifies the hypothesis.
Notice that MRFtc out-performs TC in NDCG@1 (Row 6 vs. Row 2)mainly because in the former the expansion candi-dates are generated by a word translation modeland are less noisy.It is encouraging to observe that the rankers us-ing the triplet model features achieve the QE per-formance either in par with or better than that ofSMT (Rows 8, 10 and 11 vs. Row 3), although thelatter is a much more sophisticated system.
Theresult suggests that not all SMT components areuseful for QE.
For example, language models areindispensable for translation but are less effectivethan word models for QE (Row 5 vs.
Rows 6 and7).
We also observe that the triplet model not onlyoutperforms significantly the word model due tothe use of contextual information (Row 8 vs. Row7), but also seems to subsume the latter in thatcombining the features derived from both modelsin the ranker leads to little improvement over theranker that uses only the triplet model features(Row 10 vs. Row 8).The bilingual topic model underperforms theword model and the triplet model (Row 9 vs. Rows7 and 8).
However, we found that the bilingual top-ic model often selects a different set of expansionterms and is complementary to the other two lexi-con models.
As a result, unlike the case of combin-ing the word model and triplet model features, in-corporating the bilingual topic model features inthe ranker leads to some visible improvement inNDCG at all positions (Row 11 vs. Row 10).To better understand empirically how the MRF-based QE system achieves the improvement, weanalyzed the expansions generated by our systemin detail and obtained several interesting findings.First, as expected, in comparison with the wordmodel, the triplet translation model is more effec-tive in benefitting long queries, e.g., notably que-ries containing questions and queries containingsong lyrics.
Second, unlike the two lexicon models,the bilingual topic model tends to generate expan-sions that are more likely to relate to an entire que-ry rather than individual query terms.
Third, thefeatures involving the order of the expansion termsbenefitted queries containing named entities.5 Related WorkIn comparison with log-based methods studied inthis paper, the QE methods based on automaticrelevance feedback have been studied much moreextensively in the information retrieval (IR) com-munity, and have been proved useful for improvingIR performance on benchmark datasets such asTREC (e.g., Rocchio 1971; Xu and Croft 1996;Lavrenko 2001; Zhai and Lafferty 2001).
Howev-er, these methods cannot be applied directly to acommercial Web search engine because the rele-vant documents are not always available and gen-erating pseudo-relevant documents requires multi-phase retrieval, which is prohibitively expensive.Although automatic relevance feedback is not thefocus of this study, our method shares a lot of simi-larities with some of them.
For example, similar tothe way the parameters of our QE ranker are esti-mated, Cao et alethod of se-lecting expansion terms to directly optimize aver-age precision.
The MRF model has been previous-ly used for QE, in the form of relevance feedbackand pseudo-relevance feedback (Metzler et al2007; Lang et al MRF modelsuse the features derived from IR systems such asIndri, we use the SMT-inspired features.Using statistical translation models for IR is notnew (e.g., Berger and Lafferty 1999; Jin et alXue et alveness of the statisticaltranslation-based approach to Web search has beendemonstrated empirically in recent studies whereword-based and phrase-based translation modelsare trained on large amounts of clickthrough data(e.g., Gao et alwork extendsthese studies and constructs QE-oriented transla-tion models that capture more flexible dependen-cies.In addition to QE, search logs have also beenused for other Web search tasks, such as documentranking (Joachims 2002; Agichtein et alsearch query processing and spelling correction(Huang et alre-trieval (Craswell and Szummer 2007), and userquery clustering (Baeza-Yates and Tiberi 2007;Wen et al6 Conclusions674In this paper we extend the previous log-based QEmethods in two directions.
First, we formulate QEas the problem of translating a source language ofqueries into a target language of documents, repre-sented as titles.
This allows us to adapt the estab-lished techniques developed for SMT to QE.
Spe-cially, we propose three lexicon models based onterms, lexicalized triplets, and topics, respectively.These models are trained on pairs of user queriesand the titles of clicked documents using EM.
Se-cond, we present a ranker-based QE system, theheart of which is a MRF-based ranker in which thelexicon models are incorporated as features.
Weperform experiments on the Web search task usinga real world data set.
Results show that the pro-posed system outperforms significantly other state-of-the-art QE systems.This study is part of a bigger, ongoing project,aiming to develop a real-time QE system for Websearch, where simplicity is the key to the success.Thus, what we learned from this study is particu-larly encouraging.
We demonstrate that with largeamounts of clickthrough data for model training,simple lexicon models can achieve state-of-the-artQE performance, and that the MRF-based rankerprovides a simple and flexible framework to incor-porate a variety of features capturing differenttypes of term dependencies in such an effectiveway that the Web search performance can be di-rectly optimized.ReferencesAgichtein, E., Brill, E., and Dumais, S. 2006.
Im-proving web search ranking by incorporating us-er behavior information.
In SIGIR, pp.
19-26.Baeze-Yates, R., and Ribeiro-Neto, B.
2011.
Mod-ern Information Retrieval.
Addison-Wesley.Baeza-Yates, R. and Tiberi, A.
2007.
Extractingsemantic relations from query logs.
In SIGKDD,pp.
76-85.Bai, J., Song, D., Bruza, P., Nie, J-Y., and Cao, G.2005.
Query expansion using term relationshipsin language models for information retrieval.
InCIKM, pp.
688-695.Bendersky, M., Metzler, D., and Croft, B.
2010.Learning concept importance using a weighteddependence model.
In WSDM, pp.
31-40.Berger, A., and Lafferty, J.
1999.
Information re-trieval as statistical translation.
In SIGIR, pp.222-229.Bishop, C. M. 2006.
Patten recognition and ma-chine learning.
Springer.Blei, D. M., Ng, A. Y., and Jordan, M. J.
2003.Latent Dirichlet al MachineLearning Research, 3: 993-1022.Brown, P. F., Della Pietra, S. A., Della Pietra, V. J.,and Mercer, R. L. 1993.
The mathematics of sta-tistical machine translation: parameter estimation.Computational Linguistics, 19(2): 263-311.Cao, G., Nie, J-Y., Gao, J., and Robertson, S. 2008.Selecting good expansion terms for pseudo-relevance feedback.
In SIGIR, pp.
289-305.Craswell, N. and Szummer, M. 2007.
Randomwalk on the click graph.
In SIGIR.
pp.
239-246.Cui, H., Wen, J-R., Nie, J-Y.
and Ma, W-Y.
2002.Probabilistic query expansion using query logs.In WWW, pp.
325-332.Cui, H., Wen, J-R., Nie, J-Y.
and Ma, W-Y.
2003.Query expansion by mining user log.
IEEETrans on Knowledge and Data Engineering.
Vol.15, No.
4. pp.
1-11.Dempster, A., Laird, N., and Rubin, D. 1977.
Max-imum likelihood from incomplete data via theEM algorithm.
Journal of the Royal StatisticalSociety, 39: 1-38.Ganchev, K., Graca, J., Gillenwater, J., and Taskar,B.
2010.
Posterior regularization for structuredlatent variable models.
Journal of MachineLearning Research, 11 (2010): 2001-2049.Gao, J., Toutanova, K., Yih., W-T. 2011.
Click-through-based latent semantic models for websearch.
In SIGIR, pp.
675-684.Gao, J., He, X., and Nie, J-Y.
2010a.
Clickthrough-based translation models for web search: fromword models to phrase models.
In CIKM, pp.1139-1148.Gao, J., Li, X., Micol, D., Quirk, C., and Sun, X.2010b.
A large scale ranker-based system forquery spelling correction.
In COLING, pp.
358-366.675Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y.2009.
Smoothing clickthrough data for websearch ranking.
In SIGIR, pp.
355-362.Gao, J., Qi, H., Xia, X., and Nie, J-Y.
2005.
Lineardiscriminant model for information retrieval.
InSIGIR, pp.
290-297.Hasan, S., Ganitkevitch, J., Ney, H., and Andres-Fnerre, J.
2008.
Triplet lexicon models for statis-tical machine translation.
In EMNLP, pp.
372-381.Huang, J., Gao, J., Miao, J., Li, X., Wang, K., andBehr, F. 2010.
Exploring web scale languagemodels for search query processing.
In WWW, pp.451-460.Jarvelin, K. and Kekalainen, J.
2000.
IR evaluationmethods for retrieving highly relevant docu-ments.
In SIGIR, pp.
41-48Jin, R., Hauptmann, A. G., and Zhai, C. 2002.
Titlelanguage model for information retrieval.
InSIGIR, pp.
42-48.Jing, Y., and Croft., B.
1994.
An associationthesaurus for information retrieval.
In RIAO, pp.146-160.Joachims, T. 2002.
Optimizing search engines us-ing clickthrough data.
In SIGKDD, pp.
133-142.Koehn, P., Och, F., and Marcu, D. 2003.
Statisticalphrase-based translation.
In HLT/NAACL, pp.127-133.Lang, H., Metzler, D., Wang, B., and Li, J-T. 2010.Improving latent concept expansion usingmarkov random fields.
In CIKM, pp.
249-258.Lavrenko, V., and Croft, B.
2001.
Relevance-basedlanguage models.
In SIGIR, pp.
120-128.Lease, M. 2009.
An improved markov randomfield model for supporting verbose queries.
InSIGIR, pp.
476-483Metzler, D., and Croft, B.
2005.
A markov randomfield model for term dependencies.
In SIGIR, pp.472-479.Metzler, D., and Croft, B.
2007.
Latent conceptexpansion using markov random fields.
InSIGIR, pp.
311-318.Morgan, W., Greiff, W., and Henderson, J.
2004.Direct maximization of average precision byhill-climbing with a comparison to a maximumentropy approach.
Technical report.
MITRE.Och, F. 2002.
Statistical machine translation: fromsingle-word models to alignment templates.
PhDthesis, RWTH Aachen.Prager, J., Chu-Carroll, J., and Czuba, K. 2001.Use of Wordnet hypernyms for answering whatis questions.
In TREC 10.Press, W. H., Teukolsky, S. A., Vetterling, W. T.,and Flannery, B. P. 1992.
Numerical Recipes inC. Cambridge Univ.
Press.Rocchio, J.
1971.
Relevance feedback in infor-mation retrieval.
In The SMART retrieval system:experiments in automatic document processing,pp.
313-323, Prentice-Hall Inc.Riezler, S., Liu, Y. and Vasserman, A.
2008.Translating queries into snippets for improvingquery expansion.
In COLING 2008.
737-744.Riezler, S., and Liu, Y.
2010.
Query rewriting us-ing monolingual statistical machine translation.Computational Linguistics, 36(3): 569-582.Wen, J., Nie, J-Y., and Zhang, H. 2002.
Queryclustering using user logs.
ACM TOIS, 20(1): 59-81.Xu, J., and Croft, B.
1996.
Query expansion usinglocal and global document analysis.
In SIGIR.Xue, X., Jeon, J., Croft, W. B.
2008.
Retrievalmodels for Question and answer archives.
InSIGIR, pp.
475-482.Zhai, C., and Lafferty, J.
2001a.
Model-basedfeedback in the kl-divergence retrieval model.
InCIKM, pp.
403-410.Zhai, C., and Lafferty, J.
2001b.
A study ofsmoothing methods for language models appliedto ad hoc information retrieval.
In SIGIR, pp.334-342.676
