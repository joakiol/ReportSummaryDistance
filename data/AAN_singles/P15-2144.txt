Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 877?882,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsOn the Importance of Ezafe Construction in Persian ParsingAlireza Nourian?, Mohammad Sadegh Rasooli$, Mohsen Imany?, and Heshaam Faili2?Department of Computer Engineering, Iran University of Science and Technlogy, Tehran, Iran{nourian,m imany}@comp.iust.ac.ir$Department of Computer Science, Columbia University, New York, NY, USArasooli@cs.columbia.edu2School of Electrical and Computer Engineering, University of Tehran, Tehran, Iranhfaili@ut.ac.irAbstractEzafe construction is an idiosyncratic phe-nomenon in the Persian language.
It is agood indicator for phrase boundaries anddependency relations but mostly does notappear in the text.
In this paper, we showthat adding information about Ezafe con-struction can give 4.6% relative improve-ment in dependency parsing and 9% rela-tive improvement in shallow parsing.
Forevaluation purposes, Ezafe tags are man-ually annotated in the Persian dependencytreebank.
Furthermore, to be able to con-duct experiments on shallow parsing, wedevelop a dependency to shallow phrasestructure convertor based on the Persiandependencies.1 IntroductionThere have been many studies on improving syn-tactic parsing methods for natural languages.
Al-though most of the parsing methods are language-independent, we may still require some languagespecific knowledge for improving performance.Besides many studies on parsing morphologicallyrich languages (Seddah et al., 2013; Seeker andKuhn, 2013), syntactic parsing for the Persian lan-guage is not yet noticeably explored.
Concretelyspeaking, there are some recent work on depen-dency parsing for Persian (Seraji et al., 2012;Ghayoomi, 2012; Khallash et al., 2013) and veryfew studies on shallow parsing (Kian et al., 2009).The main focus of this paper is on the usefulnessof Ezafe construction in Persian syntactic process-ing.
Ezafe is an unstressed vowel -e that occurs atthe end of some words (-ye in some specific occa-sions) that links together elements belonging to asingle constituent (Ghomeshi, 1997).
It often ap-proximately corresponds in usage to the Englishpreposition ?of?
(Abrahams, 2004).
In the follow-ing example, the first word has an Ezafe vowel:{montazereAbwaitingEzafewaterwaiting for waterThis is an idiosyncratic construction that ap-pears in the Persian language with Perso-Arabicscript.
This construction is similar to Idafa con-struction in Arabic and construct state in Hebrew(Habash, 2010).
It is mostly used for showing apossessive marker, adjective of a noun or connect-ing parts of a name (i.e.
first and last name) or ti-tle.
As a general statement, Ezafe occurs betweenany two items that have some sort of connection(Ghomeshi, 1997).
Ezafe vowel is attached tothe head noun and to the modifiers that followit: attributive nouns, adjectival and prepositionalphrases (Samvelian, 2006).
As depicted in Figure1, this construction is very useful for disambiguat-ing syntactic structures.
The main issue here isthat Ezafe rarely appears in the written text.
Thisrelies on the fact that Persian is written in Perso-Arabic script and vowels are mostly not written.There are few studies (Noferesti and Shamsfard,2014; Asghari et al., 2014) on automatically find-ing Ezafe construction.
In this work, we modifythe part of speech tagset for the Persian words.This is done by adding an indicator of Ezafe toeach part of speech (POS) tag and then train a su-pervised tagger on the modified tags.
We showthat having this modified tagset can both improvedependency parsing and shallow parsing (chunk-ing).
We achieve 12.8% and 4.6% relative error re-duction in dependency parsing with gold and auto-877[.]
[I?
@] [ ?AJ?Q?]
[?
?P] [H.AJ?]
root.
is black table+Ezafe on+Ezafe bookpuncrootmossbjposdepnpostmod(a) First reading: The book is on the black table.[.]
[I?
@] [ ?AJ?]
[Q?
?
?P H.AJ?]
root.
is black table on+Ezafe book+Ezafepuncrootnpostmodsbjposdepmos(b) Second reading: The book on the table is black.Figure 1: This figure shows two different readings for the same sentence with different Ezafe construc-tions.
As shown in the trees, Ezafe affects both phrase boundaries and dependency relations.matic POS tags.
We also achieve 31% and 9% rel-ative error reduction in shallow parsing with goldand automatic POS tags.Our work is not only restricted to the effect ofEzafe in parsing, but as a byproduct, we create anopen-source rule-based dependency to chunk con-verter for the Persian language.
We have also man-ually tagged all words in the Persian dependencytreebank (Rasooli et al., 2013) with 99.6% anno-tator agreement.
This dataset is available for re-search purposes.1The main contributions of this paper are: 1)showing the usefulness of Ezafe construction ondependency parsing and chunking, 2) developinga statistical chunker for the Persian language, 3)enriching the Persian treebank with manual Ezafetags.
The remainder of this paper is organized asthe following: we describe our approach and datapreparation in ?2 and then conduct experiments in?3.
Error analysis and conclusion are made in ?4and ?5.2 Data PreparationWe define a simple procedure to include the in-formation about Ezafe construction in our data.Concretely, we attach the Ezafe indicator to thetags and train a POS tagger on the new tagset.This idea is very similar to that of (Asghari et al.,2014).
Thanks to the presence of Ezafe feature inthe Peykare corpus (Bijankhan et al., 2011), wecan easily train a POS tagger on the new tagset.We use the developed tagger to tag the dependencytreebank.
Peykare corpus has approximately tenmillion tokens and can give us a very accurate POStagger even with the finer-grained Ezafe tags.
Wetry this idea on two different tasks: dependencyparsing and shallow parsing.1http://dadegan.ir/catalog/ezafe2.1 Chunking Data PreparationUnfortunately there is no standard chunking datafor the Persian language.
To compensate for this,we define the following rules to convert a depen-dency tree (based on Dadegan treebank dependen-cies (Rasooli et al., 2013)) to a shallow phrasestructure:- We initialize every node (word) as a separatechunk; e.g.
verb creates a VP.- If a node has a head or dependent belongingto a chunk (without any gap), attach that nodeto the same chunk.- If a node is a preposition/postposition, attachit to its next/previous dependent and create aPP.- A node with a dependency relation ?non-verbal element?, ?verb particle?, or ?encliticnon-verbal element?
belongs to the same VPas its head.- If a node is a particle, subordinating clause,coordinating conjunction, or punctuation, weshould not create an independent chunk forit.- If a node is a ?noun post modifier?
or ?Ezafedependent?, attach it to its parent chunk.- If a node is a ?conjunction of a noun?
or?conjunction of an adjective?
and has a sib-ling with either ?Ezafe dependent?
or ?nounpost-modifier?
dependency relation, it shouldhave the same chunk as its parent.- If a node with pseudo-sentence POS has anadverbial dependency with its parent, it cre-ates an ADVP and otherwise a VP.878Implementation of the above rules is availablein the Hazm toolkit.2There are some minor excep-tions in the above rules that are handled manuallyin the toolkit.3 ExperimentsIn this section we describe our experiments onEzafe tagging, parsing and also adding manualEzafe tags to the Persian dependency treebank.3.1 Automatic Ezafe TaggingAs mentioned in ?2, we attach Ezafe feature in-dicator to the tags and train a POS tagger on thenew tagset.
We use Wapiti tagger (Lavergne et al.,2010) to train a standard trigram CRF sequencetagger model with standard transition features andthe following emission features: word form of thecurrent, previous and next word, combination ofthe current word and next word, combination ofthe current word and previous word, prefixes andsuffixes up to length 3, indicator of punctuationand number (digit) for the current, previous andnext word.
The tagger has an accuracy of 98.71%with the original tagset and 97.33% with the mod-ified tagset.3.2 Gold Standard Ezafe TagsThe Persian dependency treebank does not providegold Ezafe tags.
In order to evaluate the effect ofgold Ezafe tags, we try to manually annotate Ezafein the treebank.
This is done by six annotatorswhere all of them are native speakers and linguists.The inter-annotator agreement of a small portionof the data (one thousand sentences) is 99.6%.
Ourmanual investigation shows that almost half of thedisagreements was because of the mistakes andnot because of the complicated structure.
Table1 shows the statistics about the presence of Ezafetag for each specific POS.3.3 ChunkingWe use Wapiti tagger (Lavergne et al., 2010) totrain a standard CRF tagger with IOB tags forphrase chunking.
The features include third or-der transition features and emission features ofword form and POS for the current word, previ-ous word and the word before it, the next word andthe word after it.
As shown in Table 2 and 3, ourintuition holds for both gold and automatic tags.We observe that using Ezafe on gold tags, gives2https://github.com/sobhe/hazmTag Freq.
Relative Freq.
Ezafe %N 190048 39.24% 34.22%PREP 56376 11.64% 12.04%ADJ 35902 7.41% 17.45%PRENUM 6018 1.24% 1.21%IDEN 835 0.17% 5.03%POSNUM 560 0.12% 30.71%other 194572 40.18% 00.10%Table 1: Statistics about Ezafe for each POS tag inthe Persian dependency treebank.us better performance compared to using coarse-grained POS tags and also fine-grained POS tags(FPOS) provided by the dependency treebank an-notators.
The tagset in Peykare corpus is very dif-ferent from the treebank.
Because of this incon-sistency, we could not reproduce the results withautomatic FPOS tags trained on Peykare corpus.Our experiments on training solely on the tree-bank FPOS tags do not give us a reliable FPOStagger and this leads to very low parsing accuracy.Therefore we do not conduct experiments with au-tomatic FPOS tags.
Table 3 shows the results withautomatic tags.
As shown in the table, using thethe Ezafe tagset improves the chunking accuracy.Tagset Precision Recall F-MeasurePOS 91.98% 90.37% 91.17%FPOS 92.37% 90.92% 91.64%POSe 93.88% 93.97% 93.92%Table 2: Chunking results on the Persian de-pendency treebank test data with gold POS tags.FPOS refers to the fine-grained POS tags in thePersian dependency treebank and POSe is themodified Ezafe-enriched tagset.Tagset Tag Acc.
Precision Recall F-MeasurePOS 98.71% 89.44% 88.02% 88.72%POSe 97.33% 90.42% 89.13% 89.77%Table 3: Chunking results on the Persian depen-dency treebank test data with automatic POS tags.3.4 Dependency ParsingSimilar to the chunking experiments, we providetwo sets of experiments to validate our hypothesisabout the importance of Ezafe construction.
We879Tagset MaltParser YaraParser TurboParserLAS UAS LAS UAS LAS UASPOS 88.13% 90.69% 88.60% 91.17% 89.88% 92.25%FPOS 88.46% 91.01% 89.02% 91.56% 89.98% 92.30%POSe 89.12% 91.64% 89.91% 92.42% 90.85% 93.24%Table 4: Dependency Parsing results on the test data with different gold standard tagsets.
UAS is theunlabeled attachment score and LAS is the labeled attachment score.Tagset Tag acc.
MaltParser YaraParser TurboParserLAS UAS LAS UAS LAS UASPOS tagger 98.71% 85.34% 88.80% 85.90% 89.43% 87.28% 90.59%POSe tagger 97.33% 85.74% 89.24% 86.35% 89.86% 87.73% 91.02%Table 5: Dependency Parsing results on the test data with different automatic tagsets.use three different off-the-shelf parsers: 1) Maltparser v1.8 (Nivre et al., 2007), 2) Yara parserv0.2 (Rasooli and Tetreault, 2015), and 3) Turboparser v2.2 (Martins et al., 2013).
We train Maltwith Covington non-projective algorithm (Cov-ington, 1990) after optimizing it with Malt opti-mizer (Ballesteros and Nivre, 2012), Yara with thedefault settings (64 beam) and 10 training epochsand Turbo with its default settings.
The main rea-son for picking these three parsers is that we wantto see the effect of Ezafe construction on a greedyparser (Malt), beam parser (Yara), and a graph-based parser (Turbo).
As shown in Table 4 and5, the parsing accuracy is improved across all dif-ferent parsers by using the Ezafe tagset.4 Error AnalysisIn this section we provide some error analysis forshowing the effectiveness of our approach.Effect on the common POS tags Our investiga-tion on the development data shows that the depen-dency attachment accuracy is improved by 6.5%for adjectives and 6.2% for nouns.
This is consis-tent with our intuition because Ezafe constructionmostly occurs in nouns and adjectives.
It is worthnoting that for some tags such as determiners theEzafe construction does not help.Ezafe indicator as a feature We try to useEzafe as an independent feature in Malt parser.This is done by adding the indicator in the fea-ture column in CoNLL dependency format.
Wethen use Malt optimizer (Ballesteros and Nivre,2012) to find the optimized feature setting.
We seethat adding this feature gives us the same accuracyimprovement as having the modified tagset.
Thisshows that we do not really need to have a parserthat uses extra features to add Ezafe information.Manual data investigation We randomlypicked some sentences from the development dataand observed the same effect as we could expectfrom adding Ezafe to the tagset: the main gain ison those sentences where the presence/absence ofEzafe construction is crucial for making correctdecisions by the parser.
For example, in thefollowing sub-sentence, the word?gmeans?China?
but the dependency parser withoutknowing Ezafe tag, confused it with the othermeaning: ?ruffle?
and created a ?non-verbalelement?
(light verb) dependency with the verb,instead of making it an Ezafe dependent to theprevious word (?g@??)..
.
.
XP@X?g?g@?
?P@ ?A?X .
.
.
.
.
.has China beaches+Ezafe from denfensevclposdepnppposdepmoznveEffect on the training data size For investigat-ing the benefit of Ezafe construction, we train Maltparser on different data sizes starting from 50% ofthe original size.
This trend is depicted in Figure 2.The interesting fact is that we can leverage Ezafeconstruction and use only 70% of the training datawhile reaching the accuracy of the original part ofspeech tagset trained on the whole data.88050% 60% 70% 80% 90% 100%86%87%88%89%Training Data precentageParsingLASPOSPOSeFigure 2: Trend on the data size and accuracy.
Asshown by the horizontal dashed line, Ezafe tagscan improve over standard tags while having ap-proximately 70% of the data.5 ConclusionIn this paper we showed the effectiveness of Ezafeconstruction as a robust feature for syntactic pars-ing in Persian.
One interesting direction for fur-ther research would be to show the effect of thisfeature in other natural language processing tasks.AcknowledgementWe thank Computer Research Center of Is-lamic Sciences (CRCIS) for supporting us oncorpus annotation.
We thank Parinaz Dadras,Saeedeh Ghadrdoost-Nakhchi, ManouchehrKouhestani, Mostafa Mahdavi, Azadeh Mirzaei,Neda Poormorteza-Khameneh, Morteza Rezaei-Sharifabadi and Salimeh Zamani for helping uson annotation and the three anonymous reviewersfor their helpful comments.ReferencesSimin Abrahams.
2004.
Modern Persian: a course-book.
Routledge.Habibollah Asghari, Jalal Maleki, and Heshaam Faili.2014.
A probabilistic approach to persian ezaferecognition.
In Proceedings of the 14th Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 138?142, Gothenburg,Sweden, April.
Association for Computational Lin-guistics.Miguel Ballesteros and Joakim Nivre.
2012.
Mal-toptimizer: A system for maltparser optimization.In Proceedings of the Eighth International Confer-ence on Language Resources and Evaluation, pages2757?2763, Istanbul, Turkey, May.
European Lan-guage Resources Association (ELRA).Mahmood Bijankhan, Javad Sheykhzadegan, Moham-mad Bahrani, and Masood Ghayoomi.
2011.Lessons from building a Persian written corpus:Peykare.
Language Resources and Evaluation,45:143?164.Michael A Covington.
1990.
Parsing discontinu-ous constituents in dependency grammar.
Compu-tational Linguistics, 16(4):234?236.Masood Ghayoomi.
2012.
Word clustering for Per-sian statistical parsing.
In Advances in Natural Lan-guage Processing, pages 126?137.
Springer.Jila Ghomeshi.
1997.
Non-projecting nouns and theezafe: Construction in Persian.
Natural Language& Linguistic Theory, 15(4):729?788.Nizar Y Habash.
2010.
Introduction to Arabic naturallanguage processing.
Synthesis Lectures on HumanLanguage Technologies, 3(1):1?187.Mojtaba Khallash, Ali Hadian, and Behrouz Minaei-Bidgoli.
2013.
An empirical study on the effect ofmorphological and lexical features in Persian depen-dency parsing.
In Proceedings of the Fourth Work-shop on Statistical Parsing of Morphologically-Rich Languages, pages 97?107, Seattle, Washing-ton, USA, October.
Association for ComputationalLinguistics.Soheila Kian, Tara Akhavan, and Mehrnoush Shams-fard.
2009.
Developing a Persian chunker using ahybrid approach.
In International Multiconferenceon Computer Science and Information Technology,2009.
IMCSIT?09, pages 227?234.
IEEE.Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.2010.
Practical very large scale CRFs.
In Proceed-ings the 48th Annual Meeting of the Association forComputational Linguistics (ACL), pages 504?513.Association for Computational Linguistics, July.Andr?e F. T. Martins, Miguel Almeida, and Noah A.Smith.
2013.
Turning on the turbo: Fast third-ordernon-projective turbo parsers.
pages 617?622.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(02):95?135.Samira Noferesti and Mehrnoush Shamsfard.
2014.A hybrid algorithm for recognizing the position ofezafe constructions in Persian texts.
InternationalJournal of Interactive Multimedia and Artificial In-telligence, 2(6):17?25.881Mohammad Sadegh Rasooli and Joel Tetreault.
2015.Yara parser: A fast and accurate dependency parser.arXiv preprint arXiv:1503.06733.Mohammad Sadegh Rasooli, Manouchehr Kouhestani,and Amirsaeid Moloodi.
2013.
Development ofa Persian syntactic dependency treebank.
In Pro-ceedings of the 2013 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages306?314.Pollet Samvelian.
2006.
When morphology does bet-ter than syntax: the Ezafe construction in Persian.Ms., Universit?e de Paris, (1997):1?54.Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, MarieCandito, Jinho D. Choi, Richard Farkas, Jen-nifer Foster, Iakes Goenaga, Koldo Gojenola, YoavGoldberg, Spence Green, Nizar Habash, MarcoKuhlmann, Wolfgang Maier, Joakim Nivre, AdamPrzepiorkowski, Ryan Roth, Wolfgang Seeker, Yan-nick Versley, Veronika Vincze, Marcin Wolinski,Alina Wroblewska, and Eric Villemonte de la Clerg-erie.
2013.
Overview of the SPMRL 2013 sharedtask : Cross-framework evaluation of parsing mor-phologically rich languages.
Proceedings of theFourth Workshop on Statistical Parsing of Morpho-logically Rich Languages, (October):146?182.Wolfgang Seeker and Jonas Kuhn.
2013.
Morphologi-cal and syntactic case in statistical dependency pars-ing.
Computational Linguistics, 39(1):23?55.Mojgan Seraji, Be?ata Megyesi, and Joakim Nivre.2012.
Dependency parsers for Persian.
In Pro-ceedings of the 10th Workshop on Asian LanguageResources, pages 35?44, Mumbai, India, December.The COLING 2012 Organizing Committee.882
