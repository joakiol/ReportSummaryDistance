A HYBRID APPROACH TOADAPTIVE STATISTICAL LANGUAGE MODELINGRonald RosenfeldSchool of Computer ScienceCarnegie Mel lon UniversityPittsburgh, PA 12513ABSTRACTWe desert'be our  latest attempt at adaptive language modeling.
Atthe heart of our approach isa Maximum Entropy (ME) model whichinc.orlxnates many knowledge sources in a consistent manner.
Theother components are a selective unigram cache, aconditional bigramcache, and a conventionalstatic trigram.
We describe the knowledgesources used to build such a model with ARPA's official WSJ corpus,and report on perplexity and word error ate results obtained withit.
Then, three different adaptation paradigms are discussed, and anadditional experiment, based on AP wire data, is used to comparethem.1.
OVERVIEW OF ME FRAMEWORKUsing several different probability estimates to arrive at onecombined estimate is a general problem that arises in manytasks.
The Maximum Entropy (ME) principle has recentlybeen demonstrated as a powerful tool for combining statisticalestimates from diverse sources\[l, 2 3\].
The ME principle(\[4, 5\]) proposes the following:1.
Reformulate he different estimates as constraints on theexpectation of various functions, to be satisfied by thetarget (combined) estimate.2.
Among all probability distributions that satisfy these con-straints, choose the one that has the highest entropy.More specifically, for estimating a probability function P(x),each constraint i is associated with a constraintfunctionfi(x)and a desired expectation ci.
The constraint isthen written as:def E Eefi = P(x)fi(x) = ci.
(1)XGiven consistent constraints, a unique ME solutions is guar-anteed to exist, and to be of the form:P(x) = I I  mf'?
?, (2)iwhere the pi's are some unknown constants, to be found.Probability functions of the form (2) are called log-linear,and the family of functions defined by holding thefi's fixedand varying the pi's is called an exponential family.76TO search the family defined by (2) for the pi's that will makeP(x) satisfy all the constraints, an iterative algorithm, "Gen-eralized Iterative Scaling" (GIS), exists, which is guaranteedto converge to the solution (\[6\]), as long as the constraintsare mut~ally consistent.
GIS starts with arbitrary p~ values.At each iteration, it computes the expectations Epfi over thetraining data, compares them to the desired values c/s, andthen adjusts the tJz's by an amount proportional to the ratio ofthe two.Generalized Iterative Scaling can be used to find the MEestimate of a simple (non-conditional) probability distributionover some event space.
An ~0aptation of GIS to conditionalprobabilities was proposed by \[7\], as follows.
Let P(w\[h)be the desired probability estimate, and let lS(h,w) be theempirical distribution of the training data.
Letfi(h,w) beany constraint function, and let cl be its desired expectation.Equation 1 is now modified to:E P(h)" E P(w\[h) .fi(h, w) = ci (3)h wSee also \[1, 2\].2.
CAPTURING LONG-DISTANCELINGUISTIC PHENOMENAThe ME framework is very general, freeing the modeler toconcentrate on searching for significant information sourcesand choosing the phenomena to be modeled.
In statisticallanguage modeling, we are interested in information aboutthe identity of the next word, wi, given the history h, namelythe part of the document that was already processed by thesystem.
We have so far considered the following informationsources, all contained within the history:Conventional N-grams: the immediately preceding fewwords, say (wi-2, wi-l).Long distance N-grams\[8\]: N-grams preceding wi byjpo-sitions.triggers\[9\]: the appearance in the history of words relatedto wi.class triggers: trigger elations among word clusters.count-based cache: the number of times wi already oc-curred in the history.distance-based cache: the last time wi occurred in the his-tory.linguistically defined constraints: number agreement,tense agreement, etc.Any potential source can be considered separately, and theamount of information in it estimated.
For example, in esti-mating the potential of count-based caches, we might measuredependencies of the form depicted in figure 1, and calculatethe amount of information they may provide.
See also \[3\].P( DEFAULT )Similarly, the constraint function for the bigram wt, w2 is1 ffhendsinwl andw=w2f~,,n(h,w)= 0 otherwise (6)and its associated constraint is~P(h)  ~ P(wlh)f ~,)n(h,w) =~f ~,,a(h,w).h w(7)and similarly for higher-order N-grams.2.2.
Formulating long-distance N-grams asConstraintsThe constraint functions for long distance N-grams are verysimilar to those for conventional (distance 1) N-gram.
Forexample, the constrain function for the distance-2 trigram{wl, w2, w3} is:o l 2 3 4 5+ (2(DEFAUI~T)Figure 1: Count-basedcache information: Probabilityof'DE-FAULT' as a function of the number of times it already oc-curred in the document.
The horizontal line is the uncondi-tional probability.Perhaps the most important feature of the Maximum Entropyframework is its extreme generality.
For any conceivablelinguistic or statistical phenomena, ppropriate constraintfunctions can readily be written.
We will demonstrate hisprocess for several of the knowledge sources listed above.2.1.
Formulating N-grams as ConstraintsThe usual unigram, bigram and trigram Maximum Likelihoodestimates can be replaced by unigram, bigrarn and trigramconstraints conveying the same information.
Specifically, theconstraint function for the unigram wl is:1 ifw=wlf~  (h,w) = 0 otherwise (4)and its associated constraint is:P(h) ~ P(wlh~f~(h,w) = Ef w,(h,w).h w1f~,~,~ (h, w) = and w ffi w30 otherwiseff h ends in {wl, w2, w* } for some w*,and its associated constraint isl~(h) ~ P(wlh)f ~,~a,~(h, w)= l~f ~,,a,~(h, w).h w(s)(9)and similarly for other long distance N-grams.2.3.
Formulating Triggers as ConstraintsFor class triggers, let A, B be two related word clusters.
Definethe constraint functionfa.~ as:I ff3wjEA, wjEh, wEB (10)f A..~(h, w) = 0 otherwiseSet CA--~ tO E\[\]'~-~S\], the empirical expectation ffA--~ (i.e,its expectation i the training data).
NOW the constraint onP(h, w) is:Ee \[fA-~\] = i~tf~-~\] (11)(5)3.
SELECTIVE UNIGRAM CACHEIn a document-based unigram cache, all words that occurredin the history of the document are stored, and are used todynamically generate a unigram, which is in turn combinedwith other language model components.
N-gram caches werefirst reported by \[10\].The motivation behind a unigram cache is that, once a wordoccurs in a document, its probability of re-occurring is typ-ically greatly elevated.
But the extent of this phenomenon77depends on the prior frequency of the word, and is most pro-nounced for rare words.
The occurrence of a common wordlike "DIE" provides little new information.
Put another way,the occurrence of a rare word is more surprising, and henceprovides more information, whereas the occurrence of a morecommon word deviates less from the expectations of the staticmodel, and therefore requires a smaller modification to it.Bayesian analysis may be used to optimally combine the priorof a word with the new evidence provided by its occurrence.As a rough first approximation, we implemented a selectiveunigram cache, where only rare words are stored in the cache.A word is defined as rare relative to a threshold of staticunigram frequency.
The exact value of the threshold wasdetermined by optimizing perplexity on unseen data.
Thisscheme proved more useful for perplexity reduction than theconventional cache.4.
CONDIT IONAL B IGRAM ANDTR IGRAM CACHESIn a document-based bigram cache, all consecutive word pairsthat occurred in the history of the document are stored, andare used to dynamically generate a bigram, which is in turncombined with other language model components.
A trigramcache is similar but is based on all consecutive word triples.An alternative way of viewing a bigram cache is as a set ofunigram caches, one for each word in the history.
At mostone such unigram is consulted at any one time, dependingon the identity of the last word of the history.
Viewed thisway, it is clear that he bigram cache should contribute to thecombined model only if the last word of the history is a (non-selective) unigram "cache hit".
In all other cases, the uniformdistribution of the bigram cache would only serve to flatten,hence degrade, the combined estimate.We therefore chose to use a conditional bigram cache, whichhas a non-zero weight only during such a "hit".A similar argument can be applied to the trigram cache.
Sucha cache should only be consulted if the last two words ofthe history occurred before, i.e.
the trigram cache shouldcontribute only immediately following abigram cache hit.
Weexperimented with such a trigram cache, constructed similarlyto the conditional bigram cache.
However, we found thatit contributed little to perplexity reduction.
This is to beexpected: every bigram cache hit is also a unigram cache hit.Therefore, the trigram cache can only refine the distinctionsalready provided by the bigram cache.
A document's historyis typically small (225 words on average in the WSJ corpus).For such a modest cache, the refinement provided by thetrigram is small and statistically unreliable.Another way of viewing the selective bigram and trigramcaches is as regular (i.e.
non-selective) caches, which arelater interpolated using weights that depend on the count oftheir context.
Then, zero context-counts force respective zeroweights.5.
THE WSJ  SYSTEMAs a testbed for the above ideas, we used ARPA's CSR task.The training data was 38 million words of Wall Street Jour-nal OVSJ) text from 1987-1989.
The vocabulary used wasARPA's official "20o.nvp" (20,000 most common WSJ words,non-verbalized punctuation).To measure the impact of the amount of training d,t~ onlanguage model adaptation, we experimented with systemsbased on varying amounts of training d~t~= The largest modelwe built was based on the entire 38M words of WSJ trainingdata, and is described below.5.1 .
The Component  ModelsThe adaptive language model was based on four componentlanguage models:..A conventional "compact" backoff trigram model.
"Compact" here means that singleton trigrams (wordtriplets that occurred only once in the training d~ta) wereexcluded from the model.
It consisted of 3.2 million tri-grams and 3.5 million bigrams.
This model also servedas the baseline for comparisons, and was dubbed "thestatic model".A Maximum En~opy model trained on the same d a!8 asthe trigram, and consisting of the following knowledgesources :?
High cutoff, distance-1 (conventional) N-grams:- All trigrams that occurred 9 or more times inthe training data (428,000 in all).- All bigrams that occurred 9 or more times inthe training data (327,000).- all unigrams.The high cutoffs were necessary in order to reducethe heavy computational requirements ofthe train-ing procedure.?
High cutoff, distance-2 bigrams and trigrams:- All distance-2 trigrams that occurred 5or moretimes in the training data (795,000 in all).- All distance-2 bigrams that occurred 5or moretimes in the training data (651,000).The cutoffs used for the conventional N-gramswere higher than those applied to the distance-2N-grams.
This was done because we expected thatthe information lost from the former knowledge78source will be re-introduced, atleast partially, byinterpolation with the static model.?
Word Trigger Pairs: For every word in the vocabu-lary, the top 3 triggers were selected based on theirmutual information with that word as computedfrom the training data\[l, 2\].
This resulted in some43,000 word trigger pairs.3.
A selective unigram cache, as described earlier, using aunigram threshold of 0.001.4.
A conditional bigram cache, as described earlier.5.2.
Combining the LM ComponentsThe combined model was achieved by consulting an appropri-ate subset of the above four models.
At any one time, the fourcomponent LMs were combined linearly.
But the weightsused were not fixed, nor did they follow a linear pattern overtime.Since the Maximum Entropy model incorporated informationfrom trigger pairs, its relative weight should be increased withthe length of the history.
But since it also incorporated newinformation from distance-2 N-grams, it is useful even at thevery beginning of a document, and its weight should not startat zero.5.4.
Computational CostsThe computational bottleneck of the Generalized IterativeScaling algorithm is in constraints which, for typical histo-ties h, are non-zero for a large number of words w's.
Thismeans that bigram constraints are more expensive than trigramconstraints.
Implicit computation can be used for unigramconstraints.
Therefore, the time cost of bigram and triggerconstraints dominated the total time cost of the algorithm.The computational burden of training the Maximum Entropymodel for the large system (38MW) was quite severe.
For-tunately, the training procedure is highly paralleliTable (see\[1\]).
Training was run in parallel on 10-25 high performanceworkstations, with an average of perhaps 15 machines.
Evenso, it took 3 weeks to complete.In comparison, training the 5MW system took only a fewmachine-days, and training the 1MW system was trivial.5.5.
Perplexity ReductionWe used 325,000 words of unseen WSJ d~tg_ to measure per-plexities of the baseline trigram model, the Maximum En-tropy component, and the interpolated a0aptive model (thelatter consisting of the first two together with the unigram andbigram caches).
This was done for each of the three systems(38MW, 5MW and 1MW).
Results are summarized intable 1.We therefore started the Maximum Entropy model with aweight of ,,.,0.3, which was gradually increased over the first60 words of the document, to ~0.7.
The conventional trigramstarted with a weight of,,4).7, and was decreased concurrentlyto ~0.3.
The conditional bigram cache had a non-zero weightonly during a cache hit, which allowed for a relatively highweight of ,~,0.09.
The selective unigram cache had a weightproportional tothe size of the cache, saturating at -,,0.05.
Theweights were always normalized to sum to 1.While the general weighting scheme was chosen based on con-siderations discussed above, the specific values of the weightswere chosen by minimizing perplexity of unseen data.
It be-came clear later that his did not always correspond with mini-mizing error ate.
Subsequently, further weight modificationswere determined by direct trial-and-error measurements ofword error rate on development data.5.3.
Varying the Training DataAs mentioned before, we also experimented with systemsbased on less training data.
We built two such systems, onebased on 5 million words, and the other based on 1 millionwords.
Both systems were identical to the larger systemsdescribed above, except hat the Maximum Entropy modeldid not employ high cutoffs, but was instead based on thesame N-gram information as the conventional trigram model.amt.
of training data 1M 5M 38Mtrigram (baseline)perplexity 269 173 105Maximum Entropyperplexity 203 123 86PP reduction 24% 29% 18%interpolated modelperplexity 163 108 71PP reduction 39% 38% 32%Table 1: Perplexity (PP) improvement ofMaximum Entropyand interpolated a aptive models over a conventional trigrammodel, for varying amounts of training data.
The 38MW MEmodel used far fewer parameters than the baseline, since itemployed high N-gram cutoffs.
See texLAs can be observed, the Maximum Entropy model, even whenused alone, was significantly better than the static model.Its relative advantage seems greater with more training data.With the large (38MW) system, practical consideration re-quired imposing high cutoffs on the ME model, and yet itsperplexity isstill significantly better than that of the baseline.This is particularly notable because the ME model uses onlyone third the number of parameters u ed by the trigram model(2.26M vs. 6.72M).79When the Maximum Entropy model is supplemented with theother three components, perplexity is again reduced signifi-cantly.
Here the relationship with the amount of training datais reversed: the less training data, the greater the improve-ment.
This effect is due to the caches, and can be explained asfollows: The amount of information provided by the cachesis independent of the amount of training data, and is thereforefixed aCTOSS the three systems.
However, the 1MW systemhas higher perplexity, and therefore the relative improvementprovided by the caches is greater.
Put another way, mod-els based on more data are stronger, and therefore harder toimprove on.5.6.
Error  Rate Reduct ionTo evaluate error rate reduction, we used the Nov93 ARPAS1 evaluation set\[ll, 12, 13\].
It consisted of 424 utter-ances produced in the context of complete long documentsby two male and two female speakers.
We used the SPHINX-II recognizer(J14, 15, 16\]) with sex-dependent non-PD 10Ksenone acoustic models.
In addition to the 20K words inthe lexicon, 178 OOV words and their correct phonetic tran-scriptions were added in order to create closed vocabularyconditions.
We first ran the forward and backward passes ofSPHINX H to create word lattices, which were then used bythree independent A* passes.
The first such pass used the38MW static trigram language model.
The other two passesused the 38MW interpolated adaptive LM.
The first of thesetwo adaptive runs was for unsupervised word-by-word adap-tation, in which the decoder output was used to update thelanguage model.
The other un used supervised adaptation,in which the decoder output was used for within-sentenceadaptation, while the correct sentence transcription was usedfor across-sentence adaptation.
Results are summarized intable 2.language model word error ate % reductionstatic trigram (baseline) 19.9%unsupervised a aptation 17.8% 10%supervised adaptation 17.0% 14%Table 2: Word error ate reduction of adaptive language mod-els over a conventional trigram model.which the test data comes from a source to which the languagemodel has never been exposed.
The most salient aspect of thiscase is the large number of out-of-vocabulary words, as wellas the high proportion of new bigrams and trigrams.Cross-domain adaptation is most important in cases whereno data from the test domain is available for training thesystem.
But in practice this rarely happens.
More likely, alimited amount of LM training can be obtained.
Thus a hybridparadigm, limited-data domain, might be the most importantone for real-world applications.The main disadvantage ofthe Maximum Entropy frameworkis the computational requirements oftraining the ME model.But these are not severe for modest amounts of training d~t~(up to, say, 5M words, with current CPUs).
The approach isthus particularly attractive inlimited-data domains.7.
THE AP  WIRE  EXPERIMENTWe have already seen the effect of the amount of trainingdata on perplexity reduction in the WSJ system.
To testour adaptation mechanisms under both the cross-domain andlimited-data p radigms, we constructed another experiment,this time using AP wire data for testing.For measuring cross-domain aa_aptation, we used the 38MWWSJ models described above.
For measuring limited-dataadaptation, we used 5M words of AP wire to train a con-ventional compact backoff trigram, and a Maximum Entropymodel, similar to the ones used by the WSJ system, exceptthat he trigger pair list was copied from the WSJ system.All models were tested on 420,000 words of unseen AP a,t~:We chose the same "200" vocabulary used in the WSJ exper-iments, to facilitate cross comparisons.
As before, we mea-sured perplexities ofthebaseline trigram odel, the maximumEntropy component, and the interpolated a aptive model.
Re-suits are summarized in table 3.To test error rate reduction under the cross.domain adapta-tion paradigm, we used 206 sentences, recorded by 3 maleand 3 female speakers, under the same system configurationdescribed in section.
Results are reported in table 4.6.
THREE PARADIGMS OF  ADAPTAT IONThe adaptation we concentrated on so far was the kind we callwithin-domain adaptation.
In this paradigm, a heterogeneouslanguage source (such as WSJ) is treated as a complex productof multiple domains-of-discourse (" ublanguages").
The goalis then to produce acontinuously modified model that rackssublangnage mixtures, ublanguage shifts, style shifts, etc.In contrast, a cross-domain adaptation paradigm is one in8.
SUMMARYWe described our latest attempt at adaptive language model-ing.
At the heart of our approach isa Maximum Entropy (ME)model, which incorporates many knowledge sources in a con-sistent manner.
We have demonstrated that the ME modelsignificantly improves on the conventional static trigram, achallenge which has evaded many past attempts(\[17, 18\]).The approach is particularly applicable in domains with amodest amount of LM training data.80paradigm cross-domain limited-datatraining data 38MW (WSJ) 5M (AP)trigram (baseline)perplexity 206 170Maximum Entropyperplexity i70 135PP reduction 17 % 21%interpolated modelperplexity 130 114PP reduction 37% 33%Table 3: Perplexity improvement of Maximum Entropy andinterpolated ad_~ptive models, for both eross-domain andfimited-data adaptation, testing on 420KW of unseen AP wire9.
ACKNOWLEDGEMENTSI am grateful to the entire CMU speech group, and manyother individuals at CMU, for generously allowing me tomonopolize their machines for weeks on end.
I am particularlygrateful to Lin Chase and Ravishankar Mosur for much neededhelp in designing and implementing the interface to SPHINX-II, to Alex Rudnicky for conditioning tools for the AP wiredata, and to Raj Reddy for his support and encouragement.The ideas for this work were developed during my 1992 sum-mer visit with the Speech and Natural Language group atIBM Watson Research Center.
I am grateful to Peter Brown,Stephen Della Pietra, Vincent Della Pietra, Raymond Lau,Bob Mercer and Salim Roukos for their very significant par-t/cipat/on.This research was sponsored by the Department of the Navy,Naval Research Laboratory under Grant No.
N00014-93-1-2005.
The views and conclusions contained in this documentare those of the authors and should not be interpreted as rep-resenting the official policies, either expressed or implied, ofthe U.S. Government.training data 38MW (WSJ)test data 206 sentences (AP)language model word error ate 1% changeIrigram (baseline) 22.1%supervised adaptation 19.8% -10%Table 4: Word error rate reduction of the adaptive languagemodel over a conventional trigram model, under the cross-domain adaptation paradigm.References1.
Rosenfeld, R., "Adaptive Statistical Language Modeling: aMaximum Enlropy Approach."
Ph.D. Thesis, CarnegieMellonUniversity, April 1994.2.
Lan, R.o Rosenfeld, R., Roukos, S., "Trigger-Based LanguageModels: a Maximum Entropy Approach."
Proceedings ofICASSP-93, April 1993.3.
Lan, R., Rosenfeld, R., Roukos, S., "Adaptive Language Mod-eling Using the Maximum Entropy Principle", in Proc.
ARPAHuman Language Technology Workshop, March 1993.4.
Jaines, E. T., "Information Theo W and Statistical Mechanics."Phys.
Rev.
106, pp.
620-630, 1957.5.
Kullback.
S., Information Theory in Statistics.
W'fley, NewYork.
1959.6.
Darroch.
J N. and Ratcliff, D., "Generalized Iterative Sealingfor Log-Linear Models", The Annals of Mathematical Statis-tics, VoL 43, pp 1470-1480,1972.7.
Brown, P., Della Pielra, S., Della Pielra, V., Mercer, R., Nadu,A., and Roukos, S., "Maximum Enlropy Methods and TheirApplications to Maximum Likelihood Parameter Estimation ofConditional Exponential Models," A forthcoming IBM techni-col report.8.
Huang, X.D., Alleva, F., Hen, H.W., Hwang, M.Y., Lee, K.F.and Rosenfeld, R., "The SPHINX-II Speech Recognition Sys-tem: An Overview."
Computer, Speech andLan&ua&e, 1992.9.
Rosenfeld, R., and Huang, X. D., "Improvements in Stochas-tic Language Modeling."
Prec.
DARPA Speech and NaturalLanguage Workshop, February 1992.10.
Kuhn, R., "Speech Recognition and the Frequency of Re-cently Used Words: A Modified Marker Model for NaturalLanguage."
12th International Conference on ComputationalLinguistics \[COLlNG 88\], pages 348-350, Budapest, August1988.11.
Kubala, E et al, "The Hub and Spoke Paradigm for CSR Evalu-ation," in Proc.ARPA Human Language Technology Workshop,March 1994.12.
Pallett, D.S., Fiscus, J.G., Fisher, W.M., Garofolo, J.S., Lund,B., and IhTzbocki, M, "1993 Benchmark Tests for the ARPAspoken Language Program", in Prec.
ARPA Human LanguageTechnology Workshop, March 1994.13.
Rosenfeld, R., "Language Model Adaptation i  ARPA's CSREvaluation", ARPA Spoken Language Systems Workshop,March 1994.14.
Huang, X.D., Alleva, E, Hop., H.W., Hwang, M.Y., Lee, ICE,and Rosenfeld, R., "The SPHINX-II Speech Recognition Sys-tem: An Overview", Computer, Speech and Language, 1993.15.
Huang, X., Alieva, E, Hwang, M-Y, and Rosenfeld, R., "AnOverview of the SPHINX-II Speech Recognition System", inPrec.
ARPA Human Language Technology Workshop, March1993.16.
Hwang, M., Rosenfeld, R., Thayex; E., Mosur, R., Chase, L.,Weide, R., Huang, X., and Alleva, F., "Improving Speech-Recognition Performance Via Phone-Dependent VQ Code-books, Multiple Speaker Clusters And Adaptive LanguageModels", ARPA Spoken Language Systems Workshop, March1994.17.
Bahl, L., Brown, E, DeSouza, P., and Mercer, R., "A Tree-Based Statistical Language Model for natural Language SpeechRecognition", IEEE Transactions onAcustics.
Speech and Sig-nal Processing, 37, pp.
1001-1008, 1989.18.
Jelinek.
E, "Up From Ttigramsl" Eurospeech 1991.81
