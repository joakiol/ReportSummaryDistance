Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 78?85,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsOvercoming the Curse of Sentence Length for Neural MachineTranslation using Automatic SegmentationJean Pouget-Abadie?Ecole Polytechnique, FranceDzmitry Bahdanau?Jacobs University Bremen, GermanyBart van Merri?enboer Kyunghyun ChoUniversit?e de Montr?eal, CanadaYoshua BengioUniversit?e de Montr?eal, CanadaCIFAR Senior FellowAbstractThe authors of (Cho et al., 2014a) haveshown that the recently introduced neuralnetwork translation systems suffer froma significant drop in translation qualitywhen translating long sentences, unlikeexisting phrase-based translation systems.In this paper, we propose a way to ad-dress this issue by automatically segment-ing an input sentence into phrases that canbe easily translated by the neural networktranslation model.
Once each segment hasbeen independently translated by the neu-ral machine translation model, the trans-lated clauses are concatenated to form afinal translation.
Empirical results showa significant improvement in translationquality for long sentences.1 IntroductionUp to now, most research efforts in statistical ma-chine translation (SMT) research have relied onthe use of a phrase-based system as suggestedin (Koehn et al., 2003).
Recently, however, anentirely new, neural network based approach hasbeen proposed by several research groups (Kalch-brenner and Blunsom, 2013; Sutskever et al.,2014; Cho et al., 2014b), showing promising re-sults, both as a standalone system or as an addi-tional component in the existing phrase-based sys-tem.
In this neural network based approach, an en-coder ?encodes?
a variable-length input sentenceinto a fixed-length vector and a decoder ?decodes?a variable-length target sentence from the fixed-length encoded vector.It has been observed in (Sutskever et al., 2014),(Kalchbrenner and Blunsom, 2013) and (Cho etal., 2014a) that this neural network approach?Research done while these authors were visiting Uni-versit?e de Montr?ealworks well with short sentences (e.g., / 20words), but has difficulty with long sentences (e.g.,' 20 words), and particularly with sentences thatare longer than those used for training.
Trainingon long sentences is difficult because few availabletraining corpora include sufficiently many longsentences, and because the computational over-head of each update iteration in training is linearlycorrelated with the length of training sentences.Additionally, by the nature of encoding a variable-length sentence into a fixed-size vector representa-tion, the neural network may fail to encode all theimportant details.In this paper, hence, we propose to translate sen-tences piece-wise.
We segment an input sentenceinto a number of short clauses that can be confi-dently translated by the model.
We show empiri-cally that this approach improves translation qual-ity of long sentences, compared to using a neuralnetwork to translate a whole sentence without seg-mentation.2 Background: RNN Encoder?Decoderfor TranslationThe RNN Encoder?Decoder (RNNenc) model isa recent implementation of the encoder?decoderapproach, proposed independently in (Cho et al.,2014b) and in (Sutskever et al., 2014).
It consistsof two RNNs, acting respectively as encoder anddecoder.The encoder of the RNNenc reads each word ina source sentence one by one while maintaining ahidden state.
The hidden state computed at the endof the source sentence then summarizes the wholeinput sentence.
Formally, given an input sentencex = (x1, ?
?
?
, xTx), the encoder computesht= f (xt, ht?1) ,where f is a nonlinear function computing the nexthidden state given the previous one and the currentinput word.78x1 x2 xTyT' y2 y1cDecoderEncoderFigure 1: An illustration of the RNN Encoder?Decoder.
Reprinted from (Cho et al., 2014b).From the last hidden state of the encoder, wecompute a context vector c on which the decoderwill be conditioned:c = g(hTx),where g may simply be a linear affine transforma-tion of hTx.The decoder, on the other hand, generates eachtarget word at a time, until the end-of-sentencesymbol is generated.
It generates a word at a timegiven the context vector (from the encoder), a pre-vious hidden state (of the decoder) and the wordgenerated at the last step.
More formally, the de-coder computes at each time its hidden state byst= f (yt?1, st?1, c) .With the newly computed hidden state, the de-coder outputs the probability distribution over allpossible target words by:p(ft,j= 1 | ft?1, .
.
.
, f1, c) =exp(wjh?t?)?Kj?=1exp(wj?h?t?
), (1)where ft,jis the indicator variable for the j-thword in the target vocabulary at time t and onlya single indicator variable is on (= 1) each time.See Fig.
1 for the graphical illustration of theRNNenc.The RNNenc in (Cho et al., 2014b) uses a spe-cial hidden unit that adaptively forgets or remem-bers the previous hidden state such that the acti-vation of a hidden unit h?t?jat time t is computedbyh?t?j= zjh?t?1?j+ (1?
zj)?h?t?j,where?h?t?j=f([Wx]j+[U(r h?t?1?)]),zj=?([Wzx]j+[Uzh?t?1?]j),rj=?([Wrx]j+[Urh?t?1?
]j).zjand rjare respectively the update and resetgates.
is an element-wise multiplication.
Inthe remaining of this paper, we always assume thatthis hidden unit is used in the RNNenc.Although the model in (Cho et al., 2014b) wasoriginally trained on phrase pairs, it is straight-forward to train the same model with a bilin-gual, parallel corpus consisting of sentence pairsas has been done in (Sutskever et al., 2014).
Inthe remainder of this paper, we use the RNNenctrained on English?French sentence pairs (Cho etal., 2014a).3 Automatic Segmentation andTranslationOne hypothesis explaining the difficulty encoun-tered by the RNNenc model when translating longsentences is that a plain, fixed-length vector lacksthe capacity to encode a long sentence.
When en-coding a long input sentence, the encoder may losetrack of all the subtleties in the sentence.
Con-sequently, the decoder has difficulties recoveringthe correct translation from the encoded represen-tation.
One solution would be to build a largermodel with a larger representation vector to in-crease the capacity of the model at the price ofhigher computational cost.In this section, however, we propose to segmentan input sentence such that each segmented clausecan be easily translated by the RNN Encoder?Decoder.
In other words, we wish to find asegmentation that maximizes the total confidencescore which is a sum of the confidence scores ofthe phrases in the segmentation.
Once the confi-dence score is defined, the problem of finding thebest segmentation can be formulated as an integerprogramming problem.Let e = (e1, ?
?
?
, en) be a source sentence com-posed of words ek.
We denote a phrase, which is asubsequence of e, with eij= (ei, ?
?
?
, ej).79We use the RNN Encoder?Decoder to measurehow confidently we can translate a subsequenceeijby considering the log-probability log p(fk|eij) of a candidate translation fkgenerated by themodel.
In addition to the log-probability, we alsouse the log-probability log p(eij| fk) from a re-verse RNN Encoder?Decoder (translating from atarget language to source language).
With thesetwo probabilities, we define the confidence scoreof a phrase pair (eij, fk) as:c(eij, fk) =log p(fk| eij) + log q(eij| fk)2 |log(j ?
i+ 1)|,(2)where the denominator penalizes a short segmentwhose probability is known to be overestimated byan RNN (Graves, 2013).The confidence score of a source phrase only isthen defined ascij= maxkc(eij, fk).
(3)We use an approximate beam search to search forthe candidate translations fkof eij, that maximizelog-likelihood log p(fk|eij) (Graves et al., 2013;Boulanger-Lewandowski et al., 2013).Let xijbe an indicator variable equal to 1 if weinclude a phrase eijin the segmentation, and oth-erwise, 0.
We can rewrite the segmentation prob-lem as the optimization of the following objectivefunction:maxx?i?jcijxij= x ?
c (4)subject to ?k, nk= 1nk=?i,jxij1i?k?jis the number of sourcephrases chosen in the segmentation containingword ek.The constraint in Eq.
(4) states that for eachword ekin the sentence one and only one of thesource phrases contains this word, (eij)i?k?j, isincluded in the segmentation.
The constraint ma-trix is totally unimodular making this integer pro-gramming problem solvable in polynomial time.Let Skjbe the first index of the k-th segmentcounting from the last phrase of the optimal seg-mentation of subsequence e1j(Sj:= S1j), and sjbe the corresponding score of this segmentation(s0:= 0).
Then, the following relations hold:sj= max1?i?j(cij+ si?1), ?j ?
1 (5)Sj=argmax1?i?j(cij+ si?1), ?j ?
1 (6)With Eq.
(5) we can evaluate sjincrementally.With the evaluated sj?s, we can compute Sjaswell (Eq.
(6)).
By the definition of Skjwe find theoptimal segmentation by decomposing e1nintoeSkn,Sk?1n?1, ?
?
?
, eS2n,S1n?1, eS1n,n, where k is theindex of the first one in the sequence Skn.
Thisapproach described above requires quadratic timewith respect to sentence length.3.1 Issues and DiscussionThe proposed segmentation approach does notavoid the problem of reordering clauses.
Unlessthe source and target languages follow roughly thesame order, such as in English to French transla-tions, a simple concatenation of translated clauseswill not necessarily be grammatically correct.Despite the lack of long-distance reordering1inthe current approach, we find nonetheless signifi-cant gains in the translation performance of neuralmachine translation.
A mechanism to reorder theobtained clause translations is, however, an impor-tant future research question.Another issue at the heart of any purely neu-ral machine translation is the limited model vo-cabulary size for both source and target languages.As shown in (Cho et al., 2014a), translation qual-ity drops considerably with just a few unknownwords present in the input sentence.
Interestinglyenough, the proposed segmentation approach ap-pears to be more robust to the presence of un-known words (see Sec.
5).
One intuition is that thesegmentation leads to multiple short clauses withless unknown words, which leads to more stabletranslation of each clause by the neural translationmodel.Finally, the proposed approach is computation-ally expensive as it requires scoring all the sub-phrases of an input sentence.
However, the scoringprocess can be easily sped up by scoring phrasesin parallel, since each phrase can be scored inde-pendently.
Another way to speed up the segmen-tation, other than parallelization, would be to use1Note that, inside each clause, the words are reorderedautomatically when the clause is translated by the RNNEncoder?Decoder.800 10 20 30 40 50 60 70 80Sentence length05101520BLEUscoreSource textReference textBoth0 10 20 30 40 50 60 70 80Sentence length0510152025BLEUscoreSource textReference textBoth0 10 20 30 40 50 60 70 80Sentence length0510152025303540BLEUscoreSource textReference textBoth(a) RNNenc withoutsegmentation(b) RNNenc with segmentation(c) MosesFigure 2: The BLEU scores achieved by (a) the RNNenc without segmentation, (b) the RNNencwith the penalized reverse confidence score, and (c) the phrase-based translation system Moses on anewstest12-14.an existing parser to segment a sentence into a setof clauses.4 Experiment Settings4.1 DatasetWe evaluate the proposed approach on the taskof English-to-French translation.
We use a bilin-gual, parallel corpus of 348M words selectedby the method of (Axelrod et al., 2011) froma combination of Europarl (61M), news com-mentary (5.5M), UN (421M) and two crawledcorpora of 90M and 780M words respectively.2The performance of our models was testedon news-test2012, news-test2013, andnews-test2014.
When comparing with thephrase-based SMT system Moses (Koehn et al.,2007), the first two were used as a development setfor tuning Moses while news-test2014 wasused as our test set.To train the neural network models, we use onlythe sentence pairs in the parallel corpus, whereboth English and French sentences are at most 30words long.
Furthermore, we limit our vocabu-lary size to the 30,000 most frequent words forboth English and French.
All other words are con-sidered unknown and mapped to a special token([UNK]).In both neural network training and automaticsegmentation, we do not incorporate any domain-specific knowledge, except when tokenizing theoriginal text data.2The datasets and trained Moses models can be down-loaded from http://www-lium.univ-lemans.fr/?schwenk/cslm_joint_paper/ and the website ofACL 2014 Ninth Workshop on Statistical Machine Transla-tion (WMT 14).4.2 Models and ApproachesWe compare the proposed segmentation-basedtranslation scheme against the same neural net-work model translations without segmentation.The neural machine translation is done by an RNNEncoder?Decoder (RNNenc) (Cho et al., 2014b)trained to maximize the conditional probabilityof a French translation given an English sen-tence.
Once the RNNenc is trained, an approxi-mate beam-search is used to find possible transla-tions with high likelihood.3This RNNenc is used for the proposedsegmentation-based approach together with an-other RNNenc trained to translate from French toEnglish.
The two RNNenc?s are used in the pro-posed segmentation algorithm to compute the con-fidence score of each phrase (See Eqs.
(2)?
(3)).We also compare with the translations of a con-ventional phrase-based machine translation sys-tem, which we expect to be more robust whentranslating long sentences.5 Results and Analysis5.1 Validity of the Automatic SegmentationWe validate the proposed segmentation algorithmdescribed in Sec.
3 by comparing against twobaseline segmentation approaches.
The first onerandomly segments an input sentence such that thedistribution of the lengths of random segments hasits mean and variance identical to those of the seg-ments produced by our algorithm.
The second ap-proach follows the proposed algorithm, however,using a uniform random confidence score.From Table 1 we can clearly see that the pro-3In all experiments, the beam width is 10.81Model Test setNo segmentation 13.15Random segmentation 16.60Random confidence score 16.76Proposed segmentation 20.86Table 1: BLEU score computed onnews-test2014 for two control experi-ments.
Random segmentation refers to randomlysegmenting a sentence so that the mean andvariance of the segment lengths corresponded tothe ones our best segmentation method.
Randomconfidence score refers to segmenting a sentencewith randomly generated confidence score foreach segment.posed segmentation algorithm results in signifi-cantly better performance.
One interesting phe-nomenon is that any random segmentation wasbetter than the direct translation without any seg-mentation.
This indirectly agrees well with theprevious finding in (Cho et al., 2014a) that theneural machine translation suffers from long sen-tences.5.2 Importance of Using an Inverse Model0 2 4 6 8 10Max.
number of unknown words?9?8?7?6?5?4?3?2?10BLEUscoredecreaseWith segm.Without segm.Figure 3: BLEU score loss vs. maximum numberof unknown words in source and target sentencewhen translating with the RNNenc model with andwithout segmentation.The proposed confidence score averages thescores of a translation model p(f | e) and an in-verse translation model p(e | f) and penalizes forshort phrases.
However, it is possible to use alter-nate definitions of confidence score.
For instance,one may use only the ?direct?
translation model orvarying penalties for phrase lengths.In this section, we test three different confidencescore:p(f | e) Using a single translation modelp(f | e) + p(e | f) Using both direct and reversetranslation models without the short phrasepenaltyp(f | e) + p(e | f) (p) Using both direct and re-verse translation models together with theshort phrase penaltyThe results in Table 2 clearly show the impor-tance of using both translation and inverse trans-lation models.
Furthermore, we were able to getthe best performance by incorporating the shortphrase penalty (the denominator in Eq.
(2)).
Fromhere on, thus, we only use the original formula-tion of the confidence score which uses the bothmodels and the penalty.5.3 Quantitative and Qualitative AnalysisModel Dev TestAllRNNenc 13.15 13.92p(f | e) 12.49 13.57p(f | e) + p(e | f) 18.82 20.10p(f | e) + p(e | f) (p) 19.39 20.86Moses 30.64 33.30NoUNKRNNenc 21.01 23.45p(f | e) 20.94 22.62p(f | e) + p(e | f) 23.05 24.63p(f | e) + p(e | f) (p) 23.93 26.46Moses 32.77 35.63Table 2: BLEU scores computed on the develop-ment and test sets.
See the text for the descriptionof each approach.
Moses refers to the scores bythe conventional phrase-based translation system.The top five rows consider all sentences of eachdata set, whilst the bottom five rows includes onlysentences with no unknown wordsAs expected, translation with the proposed ap-proach helps significantly with translating longsentences (see Fig.
2).
We observe that trans-lation performance does not drop for sentencesof lengths greater than those used to train theRNNenc (?
30 words).Similarly, in Fig.
3 we observe that translationquality of the proposed approach is more robust82Source Between the early 1970s , when the Boeing 747 jumbo defined modern long-haul travel , andthe turn of the century , the weight of the average American 40- to 49-year-old male increasedby 10 per cent , according to U.S. Health Department Data .Segmentation [[ Between the early 1970s , when the Boeing 747 jumbo defined modern long-haul travel ,][ and the turn of the century , the weight of the average American 40- to 49-year-old male] [increased by 10 per cent , according to U.S. Health Department Data .
]]Reference Entre le d?ebut des ann?ees 1970 , lorsque le jumbo 747 de Boeing a d?efini le voyage long-courriermoderne , et le tournant du si`ecle , le poids de l?
Am?ericain moyen de 40 `a 49 ans a augment?ede 10 % , selon les donn?ees du d?epartement am?ericain de la Sant?e .WithsegmentationEntre les ann?ees 70 , lorsque le Boeing Boeing a d?efini le transport de voyageurs modernes ; etla fin du si`ecle , le poids de la moyenne am?ericaine moyenne `a l?
?egard des hommes a augment?ede 10 % , conform?ement aux donn?ees fournies par le U.S. Department of Health Affairs .WithoutsegmentationEntre les ann?ees 1970 , lorsque les avions de service Boeing ont d?epass?e le prix du travail , letaux moyen ?etait de 40 % .Source During his arrest Ditta picked up his wallet and tried to remove several credit cards but theywere all seized and a hair sample was taken fom him.Segmentation [[During his arrest Ditta] [picked up his wallet and tried to remove several credit cards but theywere all seized and] [a hair sample was taken from him.
]]Reference Au cours de son arrestation , Ditta a ramass?e son portefeuille et a tent?e de retirer plusieurs cartesde cr?edit , mais elles ont toutes ?et?e saisies et on lui a pr?elev?e un ?echantillon de cheveux .WithsegmentationPendant son arrestation J?
ai utilis?e son portefeuille et a essay?e de retirer plusieurs cartes decr?edit mais toutes les pi`eces ont ?et?e saisies et un ?echantillon de cheveux a ?et?e enlev?e.WithoutsegmentationLors de son arrestation il a tent?e de r?ecup?erer plusieurs cartes de cr?edit mais il a ?et?e saisi de tousles coups et des blessures.Source ?We can now move forwards and focus on the future and on the 90 % of assets that make up areally good bank, and on building a great bank for our clients and the United Kingdom,?
newdirector general, Ross McEwan, said to the press .Segmentation [[?We can now move forwards and focus on the future] [and] [on the 90 % of assets that makeup a really good bank, and on building] [a great bank for our clients and the United Kingdom,?
][new director general, Ross McEwan, said to the press.
]]Reference ?Nous pouvons maintenant aller de l?avant , nous pr?eoccuper de l?avenir et des 90 % des actifsqui constituent une banque vraiment bonne et construire une grande banque pour la client`ele etpour le Royaume Uni?, a dit le nouveau directeur g?en?eral Ross McEwan `a la presse .Withsegmentation?Nous pouvons maintenant passer `a l?avenir et se concentrer sur l avenir ou sur les 90 % d actifsqui constituent une bonne banque et sur la construction une grande banque de nos clients et duRoyaume-Uni?
Le nouveau directeur g?en?eral Ross Ross a dit que la presse.Withoutsegmentation?Nous pouvons maintenant passer et ?etudier les 90 % et mettre en place une banque importantepour la nouvelle banque et le directeur g?en?eral?
a soulign?e le journaliste .Source There are several beautiful flashes - the creation of images has always been one of Chouinard?sstrong points - like the hair that is ruffled or the black fabric that extends the lines.Segmentation [[There are several beautiful flashes - the creation of images has always been one of Chouinard?sstrong points -] [like the hair that is ruffled or the black fabric that extends the lines.
]]Reference Il y a quelques beaux flashs - la cr?eation d?images a toujours ?et?e une force chez Chouinard -comme ces ch eveux qui s?
?ebouriffent ou ces tissus noirs qui allongent les lignes .WithsegmentationIl existe plusieurs belles images - la cr?eation d images a toujours ?et?e l un de ses points forts .comme les cheveux comme le vernis ou le tissu noir qui ?etend les lignes.WithoutsegmentationIl existe plusieurs points forts : la cr?eation d images est toujours l un des points forts .Source Without specifying the illness she was suffering from, the star performer of ?Respect?
confirmedto the media on 16 October that the side effects of a treatment she was receiving were ?difficult?to deal with.Segmentation [[Without specifying the illness she was suffering from, the star performer of ?Respect?]
[con-firmed to the media on 16 October that the side effects of a treatment she was receiving were][?difficult?
to deal with.
]]Reference Sans pr?eciser la maladie dont elle souffrait , la c?el`ebre interpr`ete de Respect avait affirm?e auxm?edias le 16 octobre que les effets secondaires d?un traitement qu?elle recevait ?etaient ?diffi-ciles?.WithsegmentationSans pr?eciser la maladie qu?elle souffrait la star de l?
??uvre?
de ?respect?.
Il a ?et?e confirm?eaux m?edias le 16 octobre que les effets secondaires d?un traitement ont ?et?e rec?us.
?difficile?
detraiter .WithoutsegmentationSans la pr?ecision de la maladie elle a eu l?impression de ?marquer le 16 avril?
les effets d?un tel?traitement?.Table 3: Sample translations with the RNNenc model taken from the test set along with the sourcesentences and the reference translations.83Source He nevertheless praised the Government for responding to his request for urgent assis-tance which he first raised with the Prime Minister at the beginning of May .Segmentation [He nevertheless praised the Government for responding to his request for urgent assis-tance which he first raised ] [with the Prime Minister at the beginning of May .
]Reference Il a n?eanmoins f?elicit?e le gouvernement pour avoir r?epondu `a la demande d?
aide urgentequ?il a pr?esent?ee au Premier ministre d?ebut mai .WithsegmentationIl a n?eanmoins f?elicit?e le Gouvernement de r?epondre `a sa demande d?
aide urgente qu?ila soulev?ee .
avec le Premier ministre d?ebut mai .WithoutsegmentationIl a n?eanmoins f?elicit?e le gouvernement de r?epondre `a sa demande d?
aide urgente qu?ila adress?ee au Premier Ministre d?ebut mai .Table 4: An example where an incorrect segmentation negatively impacts fluency and punctuation.to the presence of unknown words.
We suspectthat the existence of many unknown words makeit harder for the RNNenc to extract the meaning ofthe sentence clearly, while this is avoided with theproposed segmentation approach as it effectivelyallows the RNNenc to deal with a less number ofunknown words.In Table 3, we show the translations of ran-domly selected long sentences (40 or more words).Segmentation improves overall translation quality,agreeing well with our quantitative result.
How-ever, we can also observe a decrease in transla-tion quality when an input sentence is not seg-mented into well-formed sentential clauses.
Addi-tionally, the concatenation of independently trans-lated segments sometimes negatively impacts flu-ency, punctuation, and capitalization by the RN-Nenc model.
Table 4 shows one such example.6 Discussion and ConclusionIn this paper we propose an automatic segmen-tation solution to the ?curse of sentence length?in neural machine translation.
By choosing anappropriate confidence score based on bidirec-tional translation models, we observed significantimprovement in translation quality for long sen-tences.Our investigation shows that the proposedsegmentation-based translation is more robust tothe presence of unknown words.
However, sinceeach segment is translated in isolation, a segmen-tation of an input sentence may negatively impacttranslation quality, especially the fluency of thetranslated sentence, the placement of punctuationmarks and the capitalization of words.An important research direction in the future isto investigate how to improve the quality of thetranslation obtained by concatenating translatedsegments.AcknowledgmentsThe authors would like to acknowledge the sup-port of the following agencies for research fundingand computing support: NSERC, Calcul Qu?ebec,Compute Canada, the Canada Research Chairsand CIFAR.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In Proceedings of the ACL Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 355?362.
Association for Compu-tational Linguistics.Nicolas Boulanger-Lewandowski, Yoshua Bengio, andPascal Vincent.
2013.
Audio chord recognition withrecurrent neural networks.
In ISMIR.Kyunghyun Cho, Bart van Merri?enboer, Dzmitry Bah-danau, and Yoshua Bengio.
2014a.
On theproperties of neural machine translation: Encoder?Decoder approaches.
In Eighth Workshop on Syn-tax, Semantics and Structure in Statistical Transla-tion, October.Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Fethi Bougares, Holger Schwenk, and YoshuaBengio.
2014b.
Learning phrase representa-tions using rnn encoder-decoder for statistical ma-chine translation.
In Proceedings of the EmpiricialMethods in Natural Language Processing (EMNLP2014), October.
to appear.A.
Graves, A. Mohamed, and G. Hinton.
2013.
Speechrecognition with deep recurrent neural networks.ICASSP.A.
Graves.
2013.
Generating sequences with recurrentneural networks.
arXiv:1308.0850 [cs.NE],August.Nal Kalchbrenner and Phil Blunsom.
2013.
Two re-current continuous translation models.
In Proceed-ings of the ACL Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages1700?1709.
Association for Computational Linguis-tics.84Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,USA.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Annual meet-ing of the association for computational linguistics(acl).
Prague, Czech Republic.
demonstration ses-sion.Ilya Sutskever, Oriol Vinyals, and Quoc Le.
2014.Anonymized.
In Anonymized.
under review.85
