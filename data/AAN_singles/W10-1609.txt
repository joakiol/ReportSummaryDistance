Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,pages 62?67, Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsA Machine Learning Approach forRecognizing Textual Entailment in SpanishJulio Javier CastilloNational University of C?rdobaCiudad Universitaria, 5000C?rdoba, Argentinajotacastillo@gmail.comAbstractThis paper presents a system that uses ma-chine learning algorithms for the task of re-cognizing textual entailment in Spanishlanguage.
The datasets used include SPARTECorpus and a translated version to Spanish ofRTE3, RTE4 and RTE5 datasets.
The featureschosen quantify lexical, syntactic and seman-tic level matching between text and hypothe-sis sentences.
We analyze how the differentsizes of datasets and classifiers could impacton the final overall performance of the RTEclassification of two-way task in Spanish.
TheRTE system yields 60.83% of accuracy and acompetitive result of 66.50% of accuracy isreported by train and test set taken fromSPARTE Corpus with 70% split.1 IntroductionThe objective of the Recognizing Textual Entail-ment Challenge is determining whether the mean-ing of the Hypothesis (H) can be inferred from atext (T) (Ido Dagan et al, 2006).
This challengehas been organized by NIST in recent years.Another related antecedent was Answer Valida-tion Exercise (AVE), part of Cross LanguageEvaluation Forum (CLEF), whose objective is todevelop systems which are able to decide whetherthe answer to a question is correct or not (Pe?as etal, 2006).
It was a three year-old track, from 2006to 2008.AVE challenge was an evaluation frameworkfor Question Answering (QA) systems to promotethe development and evaluation of subsystemsaimed at validating the correctness of the answersgiven by a QA system.
The Answer Validationtask must select the best answer for the final out-put.
There is a subtask for each language involvedin QA, the Spanish is one of these.
Thus, AVE taskis very similar to RTE (Recognition of TextualEntailments).In this paper, we address the RTE task problemof determining the entailment value between Textand Hypothesis pairs in Spanish, applying machinelearning techniques.In the past, RTEs Challenges machine learningalgorithms were widely used for the task of recog-nizing textual entailment (Marneffe et al, 2006;Zanzotto et al, 2007; Castillo, 2009) and they havereported goods results for English language.
Also,our system applies machine learning algorithms tothe Spanish.We built a set of datasets based on public avail-able datasets for English, together to SPARTE(Pe?as et al 2006), an available Corpus in Spanish.This corpus contains 2962 hypothesis with a doc-ument label and a True/False value indicatingwhether the document entails the hypothesis or not.Up to our knowledge, SPARTE corpus in the onlycorpus aimed at evaluating RTE systems in Span-ish.Finally, we generated a feature vector with thefollowing components for both Text and Hypothe-sis: Levenshtein distance, a lexical distance basedon Levenshtein, a semantic similarity measureWordnet based, and the LCS (longest common62substring) metric; in order to characterize the rela-tionships between the Text and the Hypothesis.The remainder of the paper is organized as fol-lows.
Section 2 shows the system description, whe-reas Section 3 describes the results of experimentalevaluation and discussion of them.
Section 4 dis-cusses opportunities of collaboration.
Finally, Sec-tion 5 summarizes the conclusions and lines forfuture work.2 System  DescriptionThis section provides an overview of our systemwhich is based on a machine learning approach forrecognizing textual entailment to the Spanish.
Thesystem produces feature vectors for the availabledevelopment data RTE3, RTE4, RTE5, andSPARTE(Pe?as et al 2006).
Weka (Witten andFrank, 2000) is used to train classifiers on thesefeature vectors.The SPARTE Corpus, was built from the Span-ish corpora used at Cross-Language EvaluationForum (CLEF) for evaluating QA systems duringthe years 2003, 2004 and 2005.
This corpus con-tains 2962 hypothesis with a True/False value indi-cating whether the document entails the hypothesisor not.Due to, all available dataset of PASCAL TextAnalysis Conference were in English, we trans-lated every dataset to Spanish by using an onlinetranslator engine1.
So, we had a Spanish dataset butwith some translation errors provided by the trans-lator.
It is important to note, that the ?quality?
ofthe translation is given by the Translator engine,and we suppose that the sense of the sentenceshould not be modified by the Translator.
Indeed, itis the situation for the majority of the cases that weanalyzed.
The new datasets were named RTE3-Sp(Spanish), RTE4-Sp, and RTE5-Sp.The following example is the pair number 799from RTE3-Sp with False as entailment value.Text:Otros dos marines, Tyler Jackson y Juan JodkaIII, ya han se declar?
culpables de asalto agra-vantes y conspiraci?n para obstruir la justicia yfueron condenados a 21 meses y 18 meses, respec-tivamente.1 http://www.microsofttranslator.com/Hypothesis:Tyler Jackson ha sido condenado a 18 meses.This example shows a little noisy (and a minimalsyntactic error) in the translation of the Text toSpanish (instead of ?ya han se declar??
should be?ya se han declarado?
); but the whole meaning wasnot changed.Also, we show a pair example (pair id=3) takenfrom Sparte Corpus with False as entailment value:Text: ?Cu?l es la capital de Croacia?Hypothesis :La capital de Croacia es ONU.In a similar way, all pairs from SPARTE belong toQA task and these are syntactically simpler thanRTE?s Corpus pairs.Additionally, we generate the following devel-opment sets: RTE3-Sp+RTE4-Sp, and SPARTE-Bal+RTE3-Sp+RTE4-Sp in order to train with dif-ferent corpus and different sizes.
In all cases,RTE5-Sp TAC 2009 gold standard dataset wasused as test-set.Also, we did additional experiments withSPARTE, using cross-validation technique andpercentage split method, in order to test the accu-racy of our system taking only this corpus as de-velopment and training set.2.1 FeaturesWe experimented with the following four ma-chine learning algorithms: Support Vector Ma-chine (SVM), Multilayer Perceptron(MLP),Decision Trees(DT) and AdaBoost(AB).The Decision Trees are interesting because wecan see what features were selected from the toplevels of the trees.
SVM and AdaBoost were se-lected because they are known for achieving highperformances, and MLP was used because it hasachieved high performance in others NLP tasks.We experimented with various settings for themachine learning algorithms, including only theresults for the best parameters.We generated a feature vector with the follow-ing components for every possible <T,H>: Le-venshtein distance, a lexical distance based onLevenshtein, a semantic similarity measure Word-63net based, and the LCS (longest common sub-string) metric.We chose only four features in order to learnthe development sets, having into account thatlarger feature sets do not necessarily lead to im-proving classification performance because itcould increase the risk of overfitting the trainingdata.Below the motivation for the input features:Levenshtein distance is motivated by the good re-sults obtained as a measure of similarity betweentwo strings.
Using stems, this measure improvesthe Levenshtein over words.
The lexical distancefeature based on Levenshtein distance is interestingbecause works to a sentence level.
Semantic simi-larity using WordNet is interesting because of thecapture of the semantic similarity between T and Hto sentence level.
Longest common substring isselected because it is easy to implement and pro-vides a good measure for word overlap.2.2 Lexical DistanceThe standard Levenshtein distance is a string me-tric for measuring the amount of difference be-tween two strings.
This distance quantifies thenumber of changes (character based) to generateone text string (T) from the other (H).
The algo-rithm works independently from the language thatwe are analyzing.We used a Spanish Stemmer that stems wordsin Spanish based on a modified version of theSnowball algorithm2.Additionally, by using Levenshtein distance wedefined a lexical distance and the procedure is thefollowing:?
Each string T and H are divided in a list oftokens.?
The similarity between each pair of tokensin T and H is performed using the Le-venshtein distance over stems.?
The string similarity between two lists oftokens is reduced to the problem of ?bipar-tite graph matching?, performed using theHungarian algorithm (Kuhn, 1955) overthis bipartite graph.
Then, we found the as-signment that maximizes the sum of rat-ings of each token.
Note that each graphnode is a token of the list.2 http://snowball.tartarus.org/The final score is calculated by:))(),(( HLengthTLengthMaxTotalSimfinalscore ?Where:TotalSim is the sum of the similarities withthe optimal assignment in the graph.Length (T) is the number of tokens in T.Length (H) is the number of tokens in H.2.3 Wordnet DistanceSince, all datasets are in Spanish, we need to con-vert <T, H> pair to English.
In the case of RTEs-Sp datasets, this action will backward to the Eng-lish language (source).Our ideal case would be to use EuroWordNet3to obtain the semantic information that we need,but we won?t be able to access to this resource.Thus, WordNet is used to calculate the seman-tic similarity between T and H. The following pro-cedure is applied:1.
Word sense disambiguation using the Leskalgorithm (Lesk, 1986), based on Wordnet defini-tions.2.
A semantic similarity matrix between wordsin T and H is defined.
Words are used only in syn-onym and hyperonym relationship.
The BreadthFirst Search algorithm is used over these tokens;similarity is calculated by using two factors: lengthof the path and orientation of the path.3.
To obtain the final score, we use matchingaverage.The semantic similarity between two words iscomputed as:)()()),((2),(tDepthsDepthtsLCSDepthtsSim??
?Where: s,t are source and target words that weare comparing (s is in H and t is in T).
Depth(s) isthe shortest distance from the root node to the cur-rent node.
LCS(s,t):is the least common subsumeof s and t.The matching average (step 3) between twosentences X and Y is calculated as follows:)()(),(2YLengthXLengthYXMatcherageMatchingAv??
?3 http://www.illc.uva.nl/EuroWordNet/642.4 Longest Common SubstringGiven two strings, T of length n and H of length m,the Longest Common Sub-string (LCS) problem(Dan, 1999) will find the longest string that is asubstring of both T and H. It is found by dynamicprogramming.3 Experimental Evaluation and Discus-sion of the ResultsWith the aim of exploring the differences amongtraining sets and machine learning algorithms, wedid many experiments looking for the best result toour system.First, we converted the RTE4 and RTE5 data-sets with Contradiction/Unknown/Entailment pairinformation to a binary True/False problem, namedtwo-way problem.Then, we used the following combination of da-tasets: RTE3-Sp, RTE4-Sp, RTE3-Sp+RTE4-Sp,SPARTE-Bal (balanced SPARTE Corpus with thesame number of true and false cases), andSPARTE-Bal+ RTE3-Sp+RTE4-Sp.
The trainingset SPARTE-Balanced was created by taking alltrue cases and randomly taking false cases, andthen we build a balanced training set containing1352 pairs, with 676 true and 676 false pairs.We used four classifiers to learn every devel-opment set: (1) Support Vector Machine, (2) AdaBoost, (3) Multilayer Perceptron (MLP) and (4)Decision Tree using the open source WEKA DataMining Software (Witten & Frank, 2005).
In all thetables results we show only the accuracy of thebest classifier.The results obtained to predict RTE5-Sp in atwo-way classification task are summarized in Ta-ble 1 below.
In addition, table 2 shows our resultsreported in RTE two-way classification task byusing with Cross Validation technique with 10folds.Dataset Classifier Accuracy%RTE3-Sp+RTE4-Sp SVM 60.83%RTE3-Sp SVM 60.50%RTE4-Sp MLP 60.50%SPARTE-Bal+RTE3-Sp+RTE4-SpMLP 60.17%SPARTE-Bal DT 50%Baseline - 50%Table 1.Results obtained in two-way classification task.Dataset Classifier Accuracy%SPARTE-Bal DT 68.19%RTE3-Sp SVM 66.50%RTE3-Sp+RTE4-Sp MLP 61.44%RTE4-Sp MLP 59.60%SPARTE-Bal+RTE3-Sp+RTE4-SpAdaBoost 56.83%Baseline - 50%Table 2.Results obtained with Cross Validation 10 foldsin two-way task.The performance in all cases was clearly abovethose baselines.
Only when using SPARTE-Bal weobtained a result equal to the baseline (50% truepairs and 50% false pairs).The SPARTE-Balanced dataset yields the worstresults, maybe because this dataset contains onlypairs with QA task, and an additional reason, couldbe that SPARTE is syntactically simpler thanPASCAL RTE.
In that sense, some authors havereported low performance when using syntacticallysimpler datasets; for instance, by using BPI4 data-set to predict RTEs datasets in English.
Therefore,SPARTE seems to be not enough good training setto predict RTEs test sets.The best performance of our system wasachieved with SVM classifier with RTE3-Sp+RTE4-Sp dataset; it was 60.83% of accuracy.In the majority of the cases, SVM or MLP classifi-ers appear as ?favorite?
in all classification tasks.Surprisingly, in the two-way task, a slight andnot statistical significant difference of 0.66% be-tween the best and worst combination (except forSPARTE-Bal) of datasets and classifiers is found.So, it suggests that the combination of dataset andclassifiers do not produce a strong impact predict-ing RTE5-Sp, at least, for these feature sets.4 http://www.cs.utexas.edu/users/pclark/bpi-test-suite/))(),(min()),((),(HLengthTLengthHTMaxComSubLengthHTlcs ?65Also, we observed that by including SPARTE-Balto RTE3-Sp+RTE4-Sp dataset, the performanceslightly decreases, although this difference was notstatistical significant.The results obtained in table 2(and table 4) withSPARTE-Bal and decision tree algorithm, are thebest for cross-validation experiments.
In fact, anaccuracy of 68.19% was obtained, which is18.19% bigger than the result obtained in table 1,and was statistical significant.Finally, we assessed our system only over theSPARTE Corpus.
First, we used cross validationtechnique with ten folds over SPARTE-Bal, testingover our four classifiers.
Then, we testedSPARTE-Bal by splitting the corpus in training set(70%), and test set (30%).The results are shown in the tables 4 and 5 below.Classifier Accuracy%DT 68.19%MLP 62.64%AdaBoost 61.31%SVM 60.35%Baseline 50%Table 4.Results obtained with Cross Validation 10 foldsin two-way task to predict SPARTE.Classifier Accuracy%DT 66.50%AdaBoost 62.31%SVM 59.60%MLP 52.70%Baseline 50%Table 5.Results obtained with SPARTE with split 70%.The results on cross-validation are better thanthose obtained on test set, which is most probablydue to overfitting of classifiers.Table 5 shows a good performance of 66.50%,predicting test set and using Decision trees.
Theseresults are opposed to the bad performance re-ported by SPARTE to predict RTEs datasets.
Here,in fact, the syntactic complexity and original taskdo not change between train and test set; and itseems to be the main problem with the low per-formance of SPARTE in Table 1.3.1 Related WorkUp to our knowledge, there are not available re-sults of other teams that used SPARTE to predictRTE, or used RTEs applied to Spanish.
However,some comparison with other results for Spanishcould be done in AVE Challenge (Alberto T?llez-Valero et al, 2008; Ferr?ndez et al, 2008; Castillo,2008), but we will need to modify our system totest AVE 2008 test set and computing differentmetric for the ranking of the result.On the other hand, comparing the results ob-tained with English in RTE5 TAC Challenge, weobtained a result not statistical significant with re-spect to the median score for English systems thatis 61.17% of accuracy.
Also, our system could becompared to independent-language RTE systems.To finish, we think that several improvementscould be done in order to improve the accuracy ofthe system, using syntactic features, more semanticinformation, and new external resources such asAcronyms database.4 Opportunities  for CollaborationOur work is oriented to create a Textual EntailmentSystem.
Such system could be used by another sys-tem or teams of others Universities, as an internalmodule.The entailment relations between texts orstrings are very useful for a variety of NaturalLanguage Processing applications, such as Ques-tion Answering, Information Extraction, Informa-tion Retrieval and Document Summarization.For example, a RTE module could be used in aQuestion Answering system, where the answer of aquestion must be entailed by the text that supportsthe correctness of the answer; or an AutomaticSummarization system could eliminate the passag-es whose meaning is already entailed by other pas-sages and, by this way, reduce the size of thepassages.In addition, a question answering system couldbe enhanced by a RTE module, and also, these re-sults are useful as Answer Validation System.Our system was designed having in mind theinteroperation among systems.
Thus, the systeminputs accept files in .xml format, and the output istext plain files and .xml files.On the other hand, one of the resources thatwould allow this work advance is the EuroWord-66net, because it could provide additional semanticinformation improving our semantic features, andso the performance of our system.
Due to being anexpensive and not freely available resource, we areavoiding using it, but we expect to be able to use itin the future.
In section 3, we used Wordnet in or-der to obtain the relationship between two differentconcepts.
Since Wordnet includes only synsets forEnglish and not for Spanish, we have translated the<t,h> pairs to English using the online MicrosoftBing translator5, in order to use Wordnet.
As a re-sult, a loss of performance was obtained.
We be-lieve that the use of EuroWordNet could benefitour semantic features.Currently, we are keeping improving our sys-tem, and we are looking forward to get opportuni-ties for collaboration with other teams of all theAmericas.5 Conclusion and Future workIn this paper we present an initial RTE Systembased for the Spanish language, based on machinelearning techniques that uses some of the availabletextual entailment corpus and yields 60.83% ofaccuracy.One issue found is that SPARTE Corpus seemsto be not useful to predict RTEs-Sp datasets, be-cause of the syntactic simplicity and the absence oftask information different to QA task.On the other hand, we found that a competitiveresult of 66.50%acc is reported by train and test settaken from SPARTE Corpus.Future work is oriented to experiment with ad-ditional lexical and semantic similarities featuresand to test the improvements they may yield.
Also,we must explore how to decrease the computation-al cost of the system.
Our plan is keeping applyingmachine learning algorithms, testing with new fea-tures, and adding new source of knowledge.ReferencesLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, DanilloGiampiccolo, and Bernardo Magnini.
2009.
The FifthPASCAL Recognizing Textual Entailment Challenge.In proceedings of Textual Analysis Conference(TAC).
NIST, Maryland USA.Adrian Iftene, Mihai-Alex Moruz.2009.
UAIC Partici-pation at RTE5, TAC 2009, Gaithersburg, Maryland,USA.5 http://www.microsofttranslator.com/S.
Mirkin, R. Bar-Haim, J. Berant, I. Dagan, E. Shnarch,A.
Stern, and I. Szpektor.2009.
Bar-Ilan University?ssubmission to RTE5, TAC 2009, Gaithersburg, Mary-land, USA.Castillo, Julio.
Sagan in TAC2009: Using Support Vec-tor Machines in Recognizing Textual Entailment andTE Search Pilot task.
TAC 2009, Gaithersburg, Mar-yland, USA.Marie-Catherine de Marneffe, Bill MacCartney, TrondGrenager, Daniel Cer, Anna Rafferty and ChristopherD.
Manning.
2006.
Learning to distinguish valid tex-tual entailments.
RTE2 Challenge, Italy.F.
Zanzotto, Marco Pennacchiotti and Alessandro Mo-schitti.2007.
Shallow Semantics in Fast Textual En-tailment Rule Learners, RTE3, Prague.Ian H. Witten and Eibe Frank.
2005.
?Data Mining:Practical machine learning tools and techniques",2nd Edition, Morgan Kaufmann, San Francisco,USA.Anselmo Pe?as, Alvaro Rodrigo, Felisa Verdejo.SPARTE, a Test Suite for Recognising Textual En-tailment in Spanish.
Cicling 2006, Mexico.Pe?as A., Rodrigo A., Sama V., and Verdejo F. Over-view of the Answer Validation Exercise 2006, In-Working notes for the Cross Language EvaluationForum Workshop (CLEF 2006), September 2006,Spain.Ido Dagan, Oren Glickman and Bernardo Magnini.
ThePASCAL Recognising Textual Entailment Challenge.In Qui?onero-Candela, J.; Dagan, I.; Magnini, B.;d'Alch?-Buc, F.
(Eds.)
Machine Learning Chal-lenges.
Lecture Notes in Computer Science , Vol.3944, pp.
177-190, Springer, 2006.M.
Lesk.
Automatic sense disambiguation using ma-chine readable dictionaries: How to tell a pine conefrom a ice cream cone.
In SIGDOC ?86, 1986.Harold W. Kuhn, The Hungarian Method for theassignment problem, Naval Research Logistics Quar-terly.
1955Alberto T?llez-Valero, Antonio Juarez-Gonzalez, Ma-nuel Montes-y-Gomez, Luis Villasenior-Pineda.INAOE at QA@CLEF 2008:Evaluating Answer Va-lidation in Spanish Question Answering.
CLEF 2008.Julio J. Castillo.
The Contribution of FaMAF atQA@CLEF 2008.Answer ValidationExercise.CLEF2008.Oscar Ferr?ndez, Rafael Mu?oz, and Manuel Palomar.A Lexical Semantic Approach to AVE. CLEF 2008.67
