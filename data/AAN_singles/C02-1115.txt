A Maximum Entropy-based Word Sense Disambiguation systemArmando Suarez Manuel PalomarDepartamento de Lenguajes y Sistemas InformaticosUniversidad de AlicanteApartado de correos, 99E-03080 Alicante, Spainfarmando, mpalomarg@dlsi.ua.eshttp://www.dlsi.ua.es/armando/publicaciones.htmlAbstractIn this paper, a supervised learning system ofword sense disambiguation is presented.
It isbased on conditional maximum entropy models.This system acquires the linguistic knowledgefrom an annotated corpus and this knowledgeis represented in the form of features.
Severaltypes of features have been analyzed using theSENSEVAL-2 data for the Spanish lexical sam-ple task.
Such analysis shows that instead oftraining with the same kind of information forall words, each one is more eectively learnedusing a dierent set of features.
This best-feature-selection is used to build some systemsbased on dierent maximum entropy classiers,and a voting system helped by a knowledge-based method.1 IntroductionWord sense disambiguation (WSD) is an openresearch eld in natural language processing(NLP).
The task of WSD consists in assign-ing the correct sense to words using an elec-tronic dictionary as the source of word deni-tions.
This is a hard problem that is receivinga great deal of attention from the research com-munity.Currently, there are two main methodologi-cal approaches in this research area: knowledge-based methods and corpus-based methods.
Theformer approach relies on previously acquiredlinguistic knowledge, and the latter uses tech-niques from statistics and machine learning toinduce models of language usage from largesamples of text (Pedersen, 2001).
Learning canbe supervised or unsupervised.
With supervisedThis paper has been partially supported by theSpanish Government (CICYT) under project numberTIC2000-0664-C02-02.learning, the actual status (here, sense label)for each piece of data in the training example isknown, whereas with unsupervised learning theclassication of the data in the training exampleis not known (Manning and Schutze, 1999).At SENSEVAL-2, researchers showed the lat-est contributions to WSD.
Some supervised sys-tems competed in the Spanish lexical sampletask.
The Johns Hopkins University system(Yarowsky et al, 2001) combined, by means ofa voting-based classier, several WSD subsys-tems based on dierent methods: decision lists(Yarowsky, 2000), cosine-based vector models,and Bayesian classiers.
The University ofMaryland system (UMD-SST) (Cabezas et al,2001) used support vector machines.Pedersen (2002) proposes a baseline method-ology for WSD based on decision tree learningand naive Bayesian classiers, using simple lex-ical features.
Several systems that combine dif-ferent classiers using distinct sets of featurescompeted at SENSEVAL-2, both in the Englishand Spanish lexical sample tasks.This paper presents a system that implementsa corpus-based method of WSD.
The methodused to perform the learning over a set of sense-disambiguated examples is that of maximum en-tropy (ME) probability models.
Linguistic in-formation is represented in the form of featurevectors, which identify the occurrence of certainattributes that appear in contexts containinglinguistic ambiguities.
The context is the textsurrounding an ambiguity that is relevant to thedisambiguation process.
The features used maybe of a distinct nature: word collocations, part-of-speech (POS) labels, keywords, topic anddomain information, grammatical relationships,and so on.
Instead of training with the samekind of information for all words, which under-estimates which information is more relevant toeach word, our research shows that each word ismore eectively learned using a dierent set offeatures.
Therefore, a more accurate feature se-lection can be done testing several combinationsof features by means of a n-fold cross-validationover the training data.At SENSEVAL-2, Stanford University pre-sented a metalearner (Ilhan et al, 2001) com-bining simple classiers (naive-Bayes, vectorspace, memory-based and others) that use vot-ing and conditional ME models.
Garca Vareaet al (2001) do machine translation tasks usingME to perform some kind of semantic classi-cation, but they also rely on another statisticaltraining procedure to dene word classes.In the following discussion, the ME frame-work will be described.
Then, feature imple-mentation and the complete set of feature de-nitions used in this work will be detailed.
Next,evaluation results using several combinations ofthese features will be shown.
Finally, some con-clusions will be presented, along with a briefdiscussion of work in progress and future workplanned.2 The Maximum EntropyFrameworkME modeling provides a framework for integrat-ing information for classication frommany het-erogeneous information sources (Manning andSchutze, 1999).
ME probability models havebeen successfully applied to some NLP tasks,such as POS tagging or sentence boundary de-tection (Ratnaparkhi, 1998).The WSD method used in this work is basedon conditional ME models.
It has been im-plemented using a supervised learning methodthat consists of building word-sense classiersusing a semantically tagged corpus.
A classi-er obtained by means of an ME technique con-sists of a set of parameters or coe?cients whichare estimated using an optimization procedure.Each coe?cient is associated with one featureobserved in the training data.
The main pur-pose is to obtain the probability distributionthat maximizes the entropy, that is, maximumignorance is assumed and nothing apart fromthe training data is considered.
Some advan-tages of using the ME framework are that evenknowledge-poor features may be applied accu-rately; the ME framework thus allows a virtu-ally unrestricted ability to represent problem-specic knowledge in the form of features (Rat-naparkhi, 1998).Let us assume a set of contexts X and a setof classes C. The function cl : X !
C choosesthe class c with the highest conditional proba-bility in the context x: cl(x) = argmaxcp(cjx).Each feature is calculated by a function that isassociated to a specic class c0, and it takes theform of equation (1), where cp(x) is some ob-servable characteristic in the context1.
The con-ditional probability p(cjx) is dened by equation(2), whereiis the parameter or weight of thefeature i, K is the number of features dened,and Z(x) is a value to ensure that the sum ofall conditional probabilities for this context isequal to 1.f(x; c) =1 if c0= c and cp(x) = true0 otherwise(1)p(cjx) =1Z(x)KYi=1fi(x;c)i(2)The implementation of this ME frameworkfor WSD was done in C++ and included thelearning module, the classication module, theevaluation module, and the corpus translationmodule.
The rst two modules comprise themain components.The learning module produces the classiersfor each word using a corpus that is syntacti-cally and semantically annotated.
This modulehas two subsystems.
The rst subsystem con-sists of two component actions: in a rst step,the module processes the learning corpus in or-der to dene the functions that will apprise thelinguistic features of each context; in a secondstep, the module then lls in the feature vectors.The second subsystem of the learning moduleperforms the estimation of the coe?cients andstores the classication functions.
For example,let us assume that we want to build a classierfor noun interest and that POS label of the pre-vious word is the type of feature to use and thetraining corpus has these three samples:1The ME approach is not limited to binary features,but the optimization procedure used for the estimationof the parameters, the Generalized Iterative Scaling pro-cedure, uses this kind of functions.... the widespread interest#1 in the ...... the best interest#5 of both ...... persons expressing interest#1 in the ...The learning module performs a sequen-tial processing of this corpus looking for pairs<POS-label, sense>.
Then, <adjective,#1>,<adjective,#5>, and <noun,#1> are used todene three functions (each context have a vec-tor of three features).
Next, each vector is lledin with the result of the evaluation of each func-tion.
Finally, the optimization procedure calcu-lates the coe?cients and the output is a classi-er for the word interest.The classication module carries out the dis-ambiguation of new contexts using the previ-ously stored classication functions.
When MEdoes not have enough information about a spe-cic context, several senses may achieve thesame maximum probability and thus the clas-sication cannot be done properly.
In thesecases, the most frequent sense in the corpus isassigned.
However, this heuristic is only neces-sary for minimum number of contexts or whenthe set of linguistic attributes processed is verysmall.3 Feature ImplementationAn important issue in the implementation ofthis ME framework is the form of the functionsthat calculate each feature.
These functions aredened in the training phase and depend uponthe data in the corpus.A usual denition of features would substi-tute cp(x) in equation (1) with an expressionlike info(x,i) = a, where info(x,i) informs abouta property that can be found at position i ina context x, and a is a predened value.
Forexample, if we consider that 0 is the position ofthe word to be learned and that i is related to 0,then POS(x,-1) = `adjective'.
Therefore, equa-tion (1) is used to generate a function for eachpossible value (sense; a) at position i. Hence-forth, we will refer to this type of features as\non-relaxed" features, against \relaxed" fea-tures described below.
In the example of theprevious section, three \non-relaxed" functionscould be dened.Other expressions, such as info(x,i) 2 W(c0;i),may be substituted for the term cp(x) as away to reduce the number of possible features.In the expression above, for example, W(c0;i)is the set of attributes present in the learningexamples at position i.
Again, if we assumethat POS(x; 1), then for each sense of theambiguous word, the system builds a set withthe POS tags occurring in their previous posi-tions.
So this kind of function reduces the num-ber of features to one per each sense at posi-tion i.
In the example of the previous section,two \relaxed" functions could be dened from<fadjective,noung,#1> and <adjective,#5>.Due to the nature of the disambiguation task,the number of times that a feature generated bythe rst type of function (\non-relaxed") is ac-tivated is very low, and feature vectors have alarge number of values equal to 0.
The newfunction drastically reduces the number of fea-tures, with a minimum of degradation in eval-uation results.
At the same time, new featurescan be incorporated into the learning processwith a minimum impact on e?ciency.4 Description of FeaturesThe set of features dened for the training of thesystem is described in gure 1, and is based onthe feature selection made by Ng and Lee (1996)and Escudero et al (2000).
Features are auto-matically dened as explained before and de-pend on the data in the training corpus.
Thesefeatures are based on words, collocations, andPOS tags in the local context.
Both \relaxed"and \non-relaxed" functions are used.Figure 1: List of types of features 0: ambiguous-word shape s : words at positions 1, 2, 3 p : POS-tags of words at positions 1, 2, 3 b : lemmas of collocations at positions ( 2; 1),( 1;+1), (+1;+2) c: collocations at positions ( 2; 1), ( 1;+1),(+1;+2) km: lemmas of nouns at any position in con-text, occurring at least m% times with a sense r : grammatical relation of the ambiguous word d : the word that the ambiguous word dependson L: lemmas of content-words at positions 1,2, 3 (\relaxed" denition) W : content-words at positions 1, 2, 3(\relaxed" denition) S, B, C, P, and D : \relaxed" versionsActually, each item in gure 1 groups severalsets of features.
The majority of them dependon nearest words (for example, s comprises allpossible features dened by the words occur-ring in each sample at positions w 3, w 2, w 1,w+1, w+2, w+3related to the ambiguous word).Types nominated with capital letters are basedon the \relaxed" function form, that is, thesefeatures consists of a simply recognition of anattribute as belonging to the training data.Keyword features (km) are vaguely inspiredby Ng and Lee (1996).
A nouns selection isdone using frequency information for nouns co-occurring with a particular sense.
For example,in a set of 100 examples of sense four of the nouninterest, if the noun bank is found ten times ormore (m = 10%), then a feature is dened foreach possible sense of interest.Moreover, new features have also been de-ned using other grammatical properties: rela-tionship features (r) that refer to the grammati-cal relationship of the ambiguous word (subject,object, complement, ...) and dependency fea-tures (d and D) extract the word related to theambiguous one through the dependency parsetree.5 EvaluationIn this section we present the results of ourevaluation over the training and test data ofthe SENSEVAL-2 Spanish lexical sample task.This corpus was parsed using Conexor Func-tional Dependency Grammar parser for Spanish(Tapanainen and Jarvinen, 1997).Table 1 shows the ve best results using sev-eral sets of features.
The classiers were builtTable 1: Evaluation on SENSEVAL-2 Spanish dataALL Nouns0.671 0LWSBCk5 0.683 LWSBCk50.666 LWSBCk5 0.682 0LWSBCk50.663 sbcpdk5 0.666 0LWSBCPDk50.662 0LWSBCPDk5 0.666 0LWsBCPDk50.662 0LWsBCPDk5 0.666 0LWSBCPDk5Verbs Adjectives0.595 sk5 0.783 LWsBCp0.584 sbcprdk3 0.778 0sprd0.583 sbcpdk5 0.777 0sbcprdk50.580 sbcpk5 0.777 0sbcprdk100.580 0sbcprdk3 0.772 0spdk5Table 2: 3-fold cross-validation results onSENSEVAL-2 Spanish training dataFeatures Functions Accur MFSautoridad,N sbcp 548 0.589 0.503bomba,N 0LWSBCk5 176 0.762 0.707canal,N sbcprdk3 1258 0.579 0.307circuito,N 0LWSBCk5 482 0.536 0.392corazon,N 0Sbcpk5 210 0.781 0.607corona,N sbcp 420 0.722 0.489gracia,N 0sk5 542 0.634 0.295grano,N 0LWSBCr 102 0.681 0.483hermano,N 0Sprd 152 0.731 0.602masa,N LWSBCk5 206 0.756 0.455naturaleza,N sbcprdk3 1213 0.527 0.424operacion,N 0LWSBCk5 399 0.543 0.377organo,N 0LWSBCPDk5 271 0.715 0.515partido,N 0LWSBCk5 111 0.839 0.524pasaje,N sk5 389 0.685 0.451programa,N 0LWSBCr 137 0.587 0.486tabla,N sk5 282 0.663 0.488actuar,V sk5 772 0.514 0.293apoyar,V 0sbcprdk3 1257 0.730 0.635apuntar,V 0LWsBCPDk5 729 0.661 0.478clavar,V sbcprdk3 1026 0.561 0.449conducir,V LWsBCPD 482 0.534 0.358copiar,V 0sbcprdk3 1231 0.457 0.338coronar,V sk5 739 0.698 0.327explotar,V 0LWSBCk5 643 0.593 0.318saltar,V LWsBC 518 0.403 0.132tocar,V 0sbcprdk3 1888 0.583 0.313tratar,V sbcpk5 1421 0.527 0.208usar,V 0Sprd 222 0.732 0.669vencer,V sbcprdk3 1063 0.696 0.618brillante,A sbcprdk3 1199 0.756 0.512ciego,A 0spdk5 478 0.812 0.565claro,A 0Sprd 177 0.919 0.854local,A 0LWSBCr 64 0.798 0.750natural,A sbcprdk10 949 0.471 0.267popular,A sbcprdk10 2624 0.865 0.632simple,A LWsBCPD 522 0.776 0.621verde,A LWSBCk5 556 0.601 0.317vital,A Sbcp 591 0.774 0.441from the training data and evaluated over thetest data.
These values mean the maximum ac-curacy that the system can achieve at this mo-ment with a xed set of features for all words.Nevertheless, there are clear dierences betweennouns, verbs and adjectives.Our main goal is to nd a method to auto-matically obtain the best feature selection fromthe training data.
Such method consists of an-fold cross-validation testing several combina-tions of features over the training data and theanalysis of the results obtained for each word.Table 2 shows the best results obtained us-ing a 3-fold cross-validation evaluation methodon training data.
Several feature combinationshave been tested in order to nd the best setfor each selected word.
The purpose was toachieve the most relevant information for eachword from the corpus rather than applying thesame combination of features to all of them.Therefore, column Features is the feature se-lection with the best result.
Strings in eachrow represent the whole set of features usedin the training of each classier.
For example,autoridad obtains its best result using nearestwords, collocations of two lemmas, collocationsof two words, and POS information; s, b, c andp features respectively (see gure 1).
Functionsis the number of functions generated from fea-tures, and Accur (for \accuracy") the number ofcorrectly classied contexts divided by the totalnumber of contexts.
Column MFS is the accu-racy obtained when the most frequent sense isselected.In order to perform the three tests on eachword, some preprocessing of the corpus wasdone.
For each word, all senses were uniformlydistributed in the three folds (each fold containsone third of examples of each sense).
Thosesenses that had fewer than three examples inthe original corpus le were rejected and notprocessed.The data summarized in Table 2 reveal thatutilization of \relaxed" features in the MEmethod is useful; both \relaxed" and \non-relaxed" functions are used, even for the sameword.
For example, adjective vital obtains thebest result with \Sbcp" (the \relaxed" ver-sion of words in a window ( 3:: + 3), colloca-tions of two lemmas and two words in a win-dow ( 2:: + 2), and POS labels, in a window( 3::+3) too); we can assume that single wordinformation is less important than collocationsin order to disambiguate vital correctly.Ambiguous word shape (0 features) is usefulfor nouns, verbs and adjectives, but many of thewords do not use it for its best feature selection.In general, these words have not a relevant re-lationship between shape and senses.
On theother hand, POS information (p and P features)is selected less often.
When comparing lemmafeatures against word features (e.g., L versusW , and B versus C), they are complementaryin the majority of cases.
Grammatical relation-ships (r features) and word-word dependencies(d and D features) seem very useful too if com-bined with other types of attributes.
Moreover,keywords (km features) are used very often, pos-sibly due to the source and size of contexts ofSENSEVAL-2 data.Table 3: Best feature selections per POSALL Nouns0.613 sbcprdk3 0.609 LWSBCk50.609 sbcprdk5 0.609 sbcprdk50.605 0sbcprdk3 0.609 sbcprdk30.604 sbcprdk10 0.608 sk50.602 sbcpdk5 0.602 0sbcprdk3Verbs Adjectives0.575 sbcprdk3 0.706 0spdk50.568 sbcpdk5 0.701 0sbcprdk100.567 sbcprdk5 0.699 sbcprdk100.567 sbcpk5 0.699 0sbcprdk50.560 sbcprdk10 0.696 LWsBCpTable 3 shows the rst ve best feature se-lections for all words, and for nouns, verbs, andadjectives.
Data in this table and in table 2were used to build four dierent sets of classi-ers in order to compare their accuracy: MExuses the overall best feature selection for allwords; MEbfs the best selection of features forTable 4: Evaluation of ME systemsALL Nouns0.677 MEbfs.pos 0.683 MEbfs.pos0.676 vME 0.678 vME0.667 MEbfs 0.661 MEbfs0.658 MEx 0.646 MExVerbs Adjectives0.583 vME 0.774 vME0.583 MEbfs.pos 0.772 MEbfs.pos0.583 MEx 0.771 MEbfs0.580 MEbfs 0.756 MExMEx: sbcprdk3 for all wordsMEbfs: each word with itsbest feature selectionMEbfs.pos: LWSBCk5 for nouns,sbcprdk3 for verbs,and 0spdk5 for adjectivesvME: majority voting between MEx,MEbfs.pos, and MEbfseach word; MEbfs.pos uses the best selectionof each POS for all words of that POS; nally,vME is a majority voting system that has asinput the answers of the three systems.Table 4 shows the comparison of these foursystems, the less e?cient is MEx that ap-plies the same set of types of features to allwords.
However, the best feature selection perword (MEbfs) is not the best, probably be-cause deeper analysis and more training exam-ples are necessary.
The best choice seems to se-lect a xed set of types of features for each POS(MEbfs.pos).
This last system obtains an ac-curacy slightly better than the best evaluationresult in table 1, that is, a best-feature-selectionstrategy from training data guarantees a suc-cessful disambiguation.In general, verbs are di?cult to learn andthe accuracy of the method for them too low;in our opinion, more information (knowledge-based perhaps) is needed to build their classi-ers, but the types of features used could be un-suitable too.
The voting system (vME), basedon the agreement between the other three sys-tems, does not improve the accuracy.Finally, the results of the ME method werecompared with the systems that competed atSENSEVAL-2 in the Spanish lexical sample task(table 5)23.
If the ME systems described previ-ously are ranked within this scoring table, nounsand adjectives obtain a excellent results; verbsobtain worse results.Table 5 also includes an enrichment of vME:vME+SM.
This new voting system adds an-other classier, specication marks (Montoyoand Palomar, 2001), a knowledge-based methodthat uses the semantic relationships betweenwords stored in WordNet and EuroWordNet(Vossen, 1998).
Because it works merely withnouns, vME+SM improves accuracy for themonly, but obtains the same score than JHU(R).Overall score reaches the second place.6 ConclusionsAWSD system based on maximum entropy con-ditional probability models has been presented.2Systems: JHU and JHU(R) by Johns Hopkins Uni-versity; CSS244 by Stanford University; UMD-SST byUniversity of Maryland; Duluth systems by Universityof Manitoba; UA by University of Alicante.3SENSECAL-2 data can be downloaded fromhttp://www.sle.sharp.co.uk/senseval2/It is a supervised learning method that needs acorpus previously annotated with sense labels.Using the training data of SENSEVAL-2 forthe Spanish lexical sample task, several com-binations of features were analyzed in order toidentify which were the best.
This informationis the basis of various sets of classiers, as wellas two majority voting systems.
The results ob-tained by these systems show that selecting bestfeature sets guarantees the success of the disam-biguation method.As we work to improve the ME method with adeeper analysis of the feature selection strategy,we are also working to develop a cooperativestrategy between several other methods as well,both knowledge-based and corpus-based.Future research will incorporate domain in-formation as an additional information sourcefor the system.
WordNet Domains (Magniniand Strapparava, 2000) is an enrichment ofWordNet with domain labels.
These attributeswill be incorporated into the learning of the sys-tem in the same way that features were incorpo-rated, as described above, except that domaindisambiguation will be evaluated as well; thatis, WordNet senses (synsets) will be substitutedfor domains labels, thereby reducing the num-ber of possible classes into which contexts canbe classied.ReferencesClara Cabezas, Philip Resnik, and JessicaStevens.
2001.
Supervised Sense Tagging us-ing Support Vector Machines.
In Preiss andYarowsky (Preiss and Yarowsky, 2001), pages59{62.Gerard Escudero, Lluis Marquez, and Ger-man Rigau.
2000.
Boosting applied toword sense disambiguation.
In Proceedingsof the 12th Conference on Machine LearningECML2000, Barcelona, Spain.Ismael Garca-Varea, Franz J. Och, HermannNey, and Francisco Casacuberta.
2001.
Re-ned lexicon models for statistical machinetranslation using a maximum entropy ap-proach.
In Proceedings of 39th Annual Meet-ing of the Association for Computational Lin-guistics, pages 204{211.H.
Tolga Ilhan, Sepandar D. Kamvar, DanKlein, Christopher D. Manning, and KristinaToutanova.
2001.
Combining HeterogeneusTable 5: Comparing with SENSEVAL-2 systemsALL Nouns Verbs Adjectives0.713 jhu(R) 0.702 jhu(R) 0.643 jhu(R) 0.802 jhu(R)0.684 vME+SM 0.702 vME+SM 0.609 jhu 0.774 vME0.682 jhu 0.683 MEbfs.pos 0.595 css244 0.772 MEbfs.pos0.677 MEbfs.pos 0.681 jhu 0.584 umd-sst 0.772 css2440.676 vME 0.678 vME 0.583 vME 0.771 MEbfs0.670 css244 0.661 MEbfs 0.583 MEbfs.pos 0.764 jhu0.667 MEbfs 0.652 css244 0.583 MEx 0.756 MEx0.658 MEx 0.646 MEx 0.580 MEbfs 0.725 duluth 80.627 umd-sst 0.621 duluth 8 0.515 duluth 10 0.712 duluth 100.617 duluth 8 0.612 duluth Z 0.513 duluth 8 0.706 duluth 70.610 duluth 10 0.611 duluth 10 0.511 ua 0.703 umd-sst0.595 duluth Z 0.603 umd-sst 0.498 duluth 7 0.689 duluth 60.595 duluth 7 0.592 duluth 6 0.490 duluth Z 0.689 duluth Z0.582 duluth 6 0.590 duluth 7 0.478 duluth X 0.687 ua0.578 duluth X 0.586 duluth X 0.477 duluth 9 0.678 duluth X0.560 duluth 9 0.557 duluth 9 0.474 duluth 6 0.655 duluth 90.548 ua 0.514 duluth Y 0.431 duluth Y 0.637 duluth Y0.524 duluth Y 0.464 uaClassiers for Word-Sense Disambigua-tion.
In Preiss and Yarowsky (Preiss andYarowsky, 2001), pages 87{90.Bernardo Magnini and C. Strapparava.
2000.Experiments in Word Domain Disambigua-tion for Parallel Texts.
In Proceedings of theACL Workshop on Word Senses and Multi-linguality, Hong Kong, China.Christopher D. Manning and Hinrich Schutze.1999.
Foundations of Statistical Natural Lan-guage Processing.
The MIT Press, Cam-bridge, Massachusetts.Andres Montoyo and Manuel Palomar.
2001.Specication Marks for Word Sense Disam-biguation: New Development.
In Alexan-der F. Gelbukh, editor, CICLing, volume2004 of Lecture Notes in Computer Science,pages 182{191.
Springer.Hwee Tou Ng and Hian Beng Lee.
1996.
Inte-grating multiple knowledge sources to disam-biguate word senses: An exemplar-based ap-proach.
In Arivind Joshi and Martha Palmer,editors, Proceedings of the 34th Annual Meet-ing of the ACL, San Francisco.
Morgan Kauf-mann Publishers.Ted Pedersen.
2001.
A decision tree of bigramsis an accurate predictor of word sense.
InProceedings of the 2nd Annual Meeting of theNorth American Chapter of the ACL, pages79{86, Pittsburgh, July.Ted Pedersen.
2002.
A baseline methodologyfor word sense disambiguation.
In Alexan-der F. Gelbukh, editor, CICLing, volume2276 of Lecture Notes in Computer Science,pages 126{135.
Springer.Judita Preiss and David Yarowsky, edi-tors.
2001.
Proceedings of SENSEVAL-2,Toulouse, France, July.
ACL-SIGLEX.Adwait Ratnaparkhi.
1998.
Maximum EntropyModels for Natural Language Ambiguity Res-olution.
Ph.D. thesis, University of Pennsyl-vania.Pasi Tapanainen and Timo Jarvinen.
1997.
Anon-projective dependency parser.
In Pro-ceedings of the Fifth Conference on AppliedNatural Language Processing, pages 64{71,April.Piek Vossen.
1998.
EuroWordNet: Buildinga Multilingual Database with WordNets forEuropean Languages.
The ELRA Newsletter,3(1).David Yarowsky, Silviu Cucerzan, Radu Flo-rian, Charles Schafer, and Richard Wi-centowski.
2001.
The Johns HopkinsSENSEVAL-2 System Description.
In Preissand Yarowsky (Preiss and Yarowsky, 2001),pages 163{166.David Yarowsky.
2000.
Hierarchical decisionlists for word sense disambiguation.
Comput-ers and the Humanities, 34(2):179{186.
