Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 468?476,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsShrinking Exponential Language ModelsStanley F. ChenIBM T.J. Watson Research CenterP.O.
Box 218, Yorktown Heights, NY 10598stanchen@watson.ibm.comAbstractIn (Chen, 2009), we show that for a vari-ety of language models belonging to the ex-ponential family, the test set cross-entropy ofa model can be accurately predicted from itstraining set cross-entropy and its parametervalues.
In this work, we show how this rela-tionship can be used to motivate two heuristicsfor ?shrinking?
the size of a language modelto improve its performance.
We use the firstheuristic to develop a novel class-based lan-guage model that outperforms a baseline wordtrigram model by 28% in perplexity and 1.9%absolute in speech recognition word-error rateon Wall Street Journal data.
We use the secondheuristic to motivate a regularized version ofminimum discrimination information modelsand show that this method outperforms othertechniques for domain adaptation.1 IntroductionAn exponential model p?
(y|x) is a model with a setof features {f1(x, y), .
.
.
, fF (x, y)} and equal num-ber of parameters ?
= {?1, .
.
.
, ?F } wherep?
(y|x) = exp(?Fi=1 ?ifi(x, y))Z?
(x) (1)and where Z?
(x) is a normalization factor.
In(Chen, 2009), we show that for many types of ex-ponential language models, if a training and test setare drawn from the same distribution, we haveHtest ?
Htrain + ?DF?i=1|?
?i| (2)where Htest denotes test set cross-entropy; Htrain de-notes training set cross-entropy; D is the number ofevents in the training data; the ?
?i are regularized pa-rameter estimates; and ?
is a constant independentof domain, training set size, and model type.1 Thisrelationship is strongest if the ??
= {?
?i} are esti-mated using `1+`22 regularization (Kazama and Tsu-jii, 2003).
In `1 + `22 regularization, parameters arechosen to optimizeO`1+`22(?)
= Htrain +?DF?i=1|?i|+ 12?2DF?i=1?2i (3)for some ?
and ?.
With (?
= 0.5, ?2 = 6) andtaking ?
= 0.938, test set cross-entropy can be pre-dicted with eq.
(2) for a wide range of models with amean error of a few hundredths of a nat, equivalentto a few percent in perplexity.2In this paper, we show how eq.
(2) can be appliedto improve language model performance.
First, weuse eq.
(2) to analyze backoff features in exponentialn-gram models.
We find that backoff features im-prove test set performance by reducing the ?size?
ofa model 1D?Fi=1 |?
?i| rather than by improving train-ing set performance.
This suggests the followingprinciple for improving exponential language mod-els: if a model can be ?shrunk?
without increasingits training set cross-entropy, test set cross-entropyshould improve.
We apply this idea to motivatetwo language models: a novel class-based languagemodel and regularized minimum discrimination in-formation (MDI) models.
We show how these mod-els outperform other models in both perplexity andword-error rate on Wall Street Journal (WSJ) data.The organization of this paper is as follows: InSection 2, we analyze the use of backoff features inn-gram models to motivate a heuristic for model de-sign.
In Sections 3 and 4, we introduce our novel1The cross-entropy of a model p?
(y|x) on some data D =(x1, y1), .
.
.
, (xD, yD) is defined as ?
1DPDj=1 log p?
(yj |xj).It is equivalent to the negative mean log-likelihood per event aswell as to log perplexity.2A nat is a ?natural?
bit and is equivalent to log2 e regularbits.
We use nats to be consistent with (Chen, 2009).468features Heval Hpred Htrain?
|?
?i|D3g 2.681 2.724 2.341 0.4082g+3g 2.528 2.513 2.248 0.2821g+2g+3g 2.514 2.474 2.241 0.249Table 1: Various statistics for letter trigram models builton a 1k-word training set.
Heval is the cross-entropy ofthe evaluation data; Hpred is the predicted test set cross-entropy according to eq.
(2); and Htrain is the trainingset cross-entropy.
The evaluation data is drawn from thesame distribution as the training; H values are in nats.-4-3-2-1012345?predicted letterFigure 1: Nonzero ?
?i values for bigram features in let-ter bigram model without unigram backoff features.
Ifwe denote bigrams as wj?1wj , each column contains the?
?i?s corresponding to all bigrams with a particular wj .The ???
marks represent the average |?
?i| in each column;this average includes history words for which no featureexists or for which ?
?i = 0.class-based model and discuss MDI domain adapta-tion, and compare these methods against other tech-niques on WSJ data.
Finally, in Sections 5 and 6 wediscuss related work and conclusions.32 N -Gram Models and Backoff FeaturesIn this section, we use eq.
(2) to explain why backofffeatures in exponential n-gram models improve per-formance, and use this analysis to motivate a generalheuristic for model design.
An exponential n-grammodel contains a binary feature f?
for each n?-gram?
occurring in the training data for n?
?
n, wheref?
(x, y) = 1 iff xy ends in ?.
We refer to featurescorresponding to n?-grams for n?
< n as backofffeatures; it is well known that backoff features help3A long version of this paper can be found at (Chen, 2008).-4-3-2-1012345?predicted letterFigure 2: Like Figure 1, but for model with unigrambackoff features.performance a great deal.
We present statistics inTable 1 for various letter trigram models built on thesame data set.
In these and all later experiments, allmodels are regularized with `1 + `22 regularizationwith (?
= 0.5, ?2 = 6).
The last row corresponds toa normal trigram model; the second row correspondsto a model lacking unigram features; and the firstrow corresponds to a model with no unigram or bi-gram features.
As backoff features are added, we seethat the training set cross-entropy improves, whichis not surprising since the number of features is in-creasing.
More surprising is that as we add features,the ?size?
of the model 1D?Fi=1 |?
?i| decreases.We can explain these results by examining a sim-ple example.
Consider an exponential model con-sisting of the features f1(x, y) and f2(x, y) with pa-rameter values ?
?1 = 3 and ?
?2 = 4.
From eq.
(1),this model has the formp??
(y|x) =exp(3f1(x, y) + 4f2(x, y))Z?
(x) (4)Now, consider creating a new feature f3(x, y) =f1(x, y)+f2(x, y) and setting our parameters as fol-lows: ?new1 = 0, ?new2 = 1, and ?new3 = 3.
Substitut-ing into eq.
(1), we see that p?new(y|x) = p??
(y|x)for all x, y.
As the distribution this model de-scribes does not change, neither will its training per-formance.
However, the (unscaled) size ?Fi=1 |?i|of the model has been reduced from 3+4=7 to0+1+3=4, and consequently by eq.
(2) we predictthat test performance will improve.44When sgn(?
?1) = sgn(?
?2),PFi=1 |?i| is reduced most by469In fact, since p?new = p?
?, test performance willremain the same.
The catch is that eq.
(2) appliesonly to the regularized parameter estimates for amodel, and in general, ?new will not be the regu-larized parameter estimates for the expanded featureset.
We can compute the actual regularized parame-ters ?
?new for which eq.
(2) will apply; this may im-prove predicted performance even more.Hence, by adding ?redundant?
features to a modelto shrink its total size?Fi=1 |?
?i|, we can improvepredicted performance (and perhaps also actual per-formance).
This analysis suggests the followingtechnique for improving model performance:Heuristic 1 Identify groups of features which willtend to have similar ?
?i values.
For each such fea-ture group, add a new feature to the model that isthe sum of the original features.The larger the original ?
?i?s, the larger the reductionin model size and the higher the predicted gain.Given this perspective, we can explain why back-off features improve n-gram model performance.For simplicity, consider a bigram model, one with-out unigram backoff features.
It seems intuitivethat probabilities of the form p(wj |wj?1) are sim-ilar across different wj?1, and thus so are the ?
?i forthe corresponding bigram features.
(If a word hasa high unigram probability, it will also tend to havehigh bigram probabilities.)
In Figure 1, we plot thenonzero ?
?i values for all (bigram) features in a bi-gram model without unigram features.
Each columncontains the ?
?i values for a different predicted wordwj , and the ???
mark in each column is the averagevalue of |?
?i| over all history words wj?1.
We seethat the average |?
?i| for each word wj is often quitefar from zero, which suggests creating featuresfwj (x, y) =?wj?1fwj?1wj (x, y) (5)to reduce the overall size of the model.In fact, these features are exactly unigram backofffeatures.
In Figure 2, we plot the nonzero ?
?i valuesfor all bigram features after adding unigram backofffeatures.
We see that the average |?
?i|?s are closerto zero, implying that the model size?Fi=1 |?
?i| hassetting ?new3 to the ?
?i with the smaller magnitude, and the sizeof the reduction is equal to |?new3 |.
If sgn(?
?1) 6= sgn(?
?2), noreduction is possible through this transformation.Heval Hpred Htrain?
|?
?i|Dword n-gram 4.649 4.672 3.354 1.405model M 4.536 4.544 3.296 1.330Table 2: Various statistics for word and class trigrammodels built on 100k sentences of WSJ training data.been significantly decreased.
We can extend thisidea to higher-order n-gram models as well; e.g., bi-gram parameters can shrink trigram parameters, andcan in turn be shrunk by unigram parameters.
Asshown in Table 1, both training set cross-entropy andmodel size can be reduced by this technique.3 Class-Based Language ModelsIn this section, we show how we can use Heuris-tic 1 to design a novel class-based model that outper-forms existing models in both perplexity and speechrecognition word-error rate.
We assume a word w isalways mapped to the same class c(w).
For a sen-tence w1 ?
?
?wl, we havep(w1 ?
?
?wl) =?l+1j=1 p(cj |c1 ?
?
?
cj?1, w1 ?
?
?wj?1)?
?lj=1 p(wj |c1 ?
?
?
cj , w1 ?
?
?wj?1) (6)where cj = c(wj) and cl+1 is the end-of-sentencetoken.
We use the notation png(y|?)
to denote an ex-ponential n-gram model, a model containing a fea-ture for each suffix of each ?y occurring in the train-ing set.
We use png(y|?1, ?2) to denote a model con-taining all features in png(y|?1) and png(y|?2).We can define a class-based n-gram model bychoosing parameterizations for the distributionsp(cj | ?
?
? )
and p(wj | ?
?
? )
in eq.
(6) above.
For exam-ple, the most widely-used class-based n-gram modelis the one introduced by Brown et al (1992); we re-fer to this model as the IBM class model:p(cj |c1 ?
?
?
cj?1, w1 ?
?
?wj?1)= png(cj |cj?2cj?1)p(wj |c1 ?
?
?
cj , w1 ?
?
?wj?1)= png(wj |cj) (7)(In the original work, non-exponential n-gram mod-els are used.)
Clearly, there is a large space of pos-sible class-based models.Now, we discuss how we can use Heuristic 1 todesign a novel class-based model by using class in-formation to ?shrink?
a word-based n-gram model.The basic idea is as follows: if we have an n-gram ?470and another n-gram ??
created by replacing a wordin ?
with a similar word, then the two correspond-ing features should have similar ??i?s.
For exam-ple, it seems intuitive that the n-grams on Mondaymorning and on Tuesday morning should have sim-ilar ??i?s.
Heuristic 1 tells us how to take advantageof this observation to improve model performance.Let?s begin with a word trigram modelpng(wj |wj?2wj?1).
First, we would like toconvert this model into a class-based model.Without loss of generality, we havep(wj |wj?2wj?1) =?cj p(wj , cj |wj?2wj?1)=?cj p(cj |wj?2wj?1)p(wj |wj?2wj?1cj) (8)Thus, it seems reasonable to use the distributionspng(cj |wj?2wj?1) and png(wj |wj?2wj?1cj) as thestarting point for our class model.
This model canexpress the same set of word distributions as ouroriginal model, and hence may have a similar train-ing cross-entropy.
In addition, this transformationcan be viewed as shrinking together word n-gramsthat differ only in wj .
That is, we expect that pairsof n-grams wj?2wj?1wj that differ only in wj (be-longing to the same class) should have similar ?
?i.From Heuristic 1, we can make new featuresfwj?2wj?1cj (x, y) =?wj?cjfwj?2wj?1wj (x, y) (9)These are exactly the features in png(cj |wj?2wj?1).When applying Heuristic 1, all features typically be-long to the same model, but even when they don?tone can achieve the same net effect.Then, we can use Heuristic 1 to also shrink to-gether n-gram features for n-grams that differ onlyin their histories.
For example, we can create newfeatures of the formfcj?2cj?1cj (x, y) =?wj?2?cj?2,wj?1?cj?1fwj?2wj?1cj (x, y) (10)This corresponds to replacing png(cj |wj?2wj?1)with the distribution png(cj |cj?2cj?1, wj?2wj?1).We refer to the resulting model as model M:p(cj |c1???cj?1,w1??
?wj?1)=png(cj |cj?2cj?1,wj?2wj?1)p(wj |c1??
?cj ,w1??
?wj?1)=png(wj |wj?2wj?1cj) (11)By design, it is meant to have similar training setcross-entropy as a word n-gram model while beingsignificantly smaller.To give an idea of whether this model behaves asexpected, in Table 2 we provide statistics for thismodel (as well as for an exponential word n-grammodel) built on 100k WSJ training sentences with 50classes using the same regularization as before.
Wesee that model M is both smaller than the baselineand has a lower training set cross-entropy, similar tothe behavior found when adding backoff features toword n-gram models in Section 2.
As long as eq.
(2)holds, model M should have good test performance;in (Chen, 2009), we show that eq.
(2) does indeedhold for models of this type.3.1 Class-Based Model ComparisonIn this section, we compare model M against otherclass-based models in perplexity and word-errorrate.
The training data is 1993 WSJ text with verbal-ized punctuation from the CSR-III Text corpus, andthe vocabulary is the union of the training vocabu-lary and 20k-word ?closed?
test vocabulary from thefirst WSJ CSR corpus (Paul and Baker, 1992).
Weevaluate training set sizes of 1k, 10k, 100k, and 900ksentences.
We create three different word classingscontaining 50, 150, and 500 classes using the algo-rithm of Brown et al (1992) on the largest trainingset.5 For each training set and number of classes, webuild 3-gram and 4-gram versions of each model.From the verbalized punctuation data from thetraining and test portions of the WSJ CSR corpus,we randomly select 2439 unique utterances (46888words) as our evaluation set.
From the remainingverbalized punctuation data, we select 977 utter-ances (18279 words) as our development set.We compare the following model types: con-ventional (i.e., non-exponential) word n-gram mod-els; conventional IBM class n-gram models in-terpolated with conventional word n-gram models(Brown et al, 1992); and model M. All conven-tional n-gram models are smoothed with modifiedKneser-Ney smoothing (Chen and Goodman, 1998),except we also evaluate word n-gram models withKatz smoothing (Katz, 1987).
Note: Because word5One can imagine choosing word classes to optimize modelshrinkage; however, this is not an avenue we pursued.471training set (sents.
)1k 10k 100k 900kconventional word n-gram, Katz3g 579.3 317.1 196.7 137.54g 592.6 325.6 202.4 136.7interpolated IBM class model3g, 50c 358.4 224.5 156.8 117.83g, 150c 346.5 210.5 149.0 114.73g, 500c 372.6 210.9 145.8 112.34g, 50c 362.1 220.4 149.6 109.14g, 150c 346.3 207.8 142.5 105.24g, 500c 371.5 207.9 140.5 103.6training set (sents.
)1k 10k 100k 900kconventional word n-gram, modified KN3g 488.4 270.6 168.2 121.54g 486.8 267.4 163.6 114.4model M3g, 50c 341.5 210.0 144.5 110.93g, 150c 342.6 203.7 140.0 108.03g, 500c 387.5 212.7 142.2 108.14g, 50c 345.8 209.0 139.1 101.64g, 150c 344.1 202.8 135.7 99.14g, 500c 390.7 211.1 138.5 100.6Table 3: WSJ perplexity results.
The best performance for each training set for each model type is highlighted in bold.training set (sents.
)1k 10k 100k 900kconventional word n-gram, Katz3g 35.5% 30.7% 26.2% 22.7%4g 35.6% 30.9% 26.3% 22.7%interpolated IBM class model3g, 50c 32.2% 28.7% 25.2% 22.5%3g, 150c 31.8% 28.1% 25.0% 22.3%3g, 500c 32.5% 28.5% 24.5% 22.1%4g, 50c 32.2% 28.6% 25.0% 22.0%4g, 150c 31.8% 28.0% 24.6% 21.8%4g, 500c 32.7% 28.3% 24.5% 21.6%training set (sents.
)1k 10k 100k 900kconventional word n-gram, modified KN3g 34.5% 30.5% 26.1% 22.6%4g 34.5% 30.4% 25.7% 22.3%model M3g, 50c 30.8% 27.4% 24.0% 21.7%3g, 150c 31.0% 27.1% 23.8% 21.5%3g, 500c 32.3% 27.8% 23.9% 21.4%4g, 50c 30.8% 27.5% 23.9% 21.2%4g, 150c 31.0% 27.1% 23.5% 20.8%4g, 500c 32.4% 27.9% 24.1% 21.1%Table 4: WSJ lattice rescoring results; all values are word-error rates.
The best performance for each training set sizefor each model type is highlighted in bold.
Each 0.1% in error rate corresponds to about 47 errors.classes are derived from the largest training set, re-sults for word models and class models are compa-rable only for this data set.
The interpolated model isthe most popular state-of-the-art class-based modelin the literature, and is the only model here using thedevelopment set to tune interpolation weights.We display the perplexities of these models on theevaluation set in Table 3.
Model M performs best ofall (even without interpolating with a word n-grammodel), outperforming the interpolated model withevery training set and achieving its largest reductionin perplexity (4%) on the largest training set.
Whilethese perplexity reductions are quite modest, whatmatters more is speech recognition performance.For the speech recognition experiments, we usea cross-word quinphone system built from 50 hoursof Broadcast News data.
The system contains 2176context-dependent states and a total of 50336 Gaus-sians.
To evaluate our language models, we use lat-tice rescoring.
We generate lattices on both our de-velopment and evaluation data sets using the Latt-AIX decoder (Saon et al, 2005) in the Attila speechrecognition system (Soltau et al, 2005).
The lan-guage model for lattice generation is created bybuilding a modified Kneser-Ney-smoothed word tri-gram model on our largest training set; this model ispruned to contain a total of 350k n-grams using thealgorithm of Stolcke (1998).
We choose the acousticweight for each model to optimize word-error rateon the development set.In Table 4, we display the word-error rates foreach model.
If we compare the best performanceof model M for each training set with that of thestate-of-the-art interpolated class model, we find thatmodel M is superior by 0.8?1.0% absolute.
Thesegains are much larger than are suggested by theperplexity gains of model M over the interpolatedmodel; as has been observed earlier, perplexity is472Heval Hpred Htrain?
|?
?i|Dbaseline n-gram model1k 5.915 5.875 2.808 3.26910k 5.212 5.231 3.106 2.265100k 4.649 4.672 3.354 1.405MDI n-gram model1k 5.444 5.285 2.678 2.78010k 5.031 4.973 3.053 2.046100k 4.611 4.595 3.339 1.339Table 5: Various statistics for WSJ trigram models, withand without a Broadcast News prior model.
The first col-umn is the size of the in-domain training set in sentences.not a reliable predictor of speech recognition perfor-mance.
While we can only compare class modelswith word models on the largest training set, for thistraining set model M outperforms the baseline Katz-smoothed word trigram model by 1.9% absolute.64 Domain AdaptationIn this section, we introduce another heuristic forimproving exponential models and show how thisheuristic can be used to motivate a regularized ver-sion of minimum discrimination information (MDI)models (Della Pietra et al, 1992).
Let?s say we havea model p??
estimated from one training set and a?similar?
model q estimated from an independenttraining set.
Imagine we use q as a prior model forp?
; i.e., we make a new model pq?new as follows:pq?new(y|x) = q(y|x)exp(?Fi=1 ?newi fi(x, y))Z?new(x) (12)Then, choose ?new such that pq?new(y|x) = p??
(y|x)for all x, y (assuming this is possible).
If q is ?simi-lar?
to p?
?, then we expect the size 1D?Fi=1 |?newi | ofpq?new to be less than that of p??.
Since they describethe same distribution, their training set cross-entropywill be the same.
By eq.
(2), we expect pq?new tohave better test set performance than p??
after reesti-mation.7 In (Chen, 2009), we show that eq.
(2) doesindeed hold for models with priors; q need not beaccounted for in computing model size as long as itis estimated on a separate training set.6Results for several other baseline language models and witha different acoustic model are given in (Chen, 2008).7That is, we expect the regularized parameters ?
?new to yieldimproved performance.This analysis suggests the following method forimproving model performance:Heuristic 2 Find a ?similar?
distribution estimatedfrom an independent training set, and use this distri-bution as a prior.It is straightforward to apply this heuristic to the taskof domain adaptation for language modeling.
In theusual formulation of this task, we have a test set anda small training set from the same domain, and alarge training set from a different domain.
The goalis to use the data from the outside domain to max-imally improve language modeling performance onthe target domain.
By Heuristic 2, we can build alanguage model on the outside domain, and use thismodel as the prior model for a language model builton the in-domain data.
This method is identical tothe MDI method for domain adaptation, except thatwe also apply regularization.In our domain adaptation experiments, our out-of-domain data is a 100k-sentence Broadcast Newstraining set.
For our in-domain WSJ data, we usetraining set sizes of 1k, 10k, and 100k sentences.
Webuild an exponential n-gram model on the Broad-cast News data and use this model as the prior modelq(y|x) in eq.
(12) when building an exponential n-gram model on the in-domain data.
In Table 5, wedisplay various statistics for trigram models built onvarying amounts of in-domain data when using aBroadcast News prior and not.
Across training sets,the MDI models are both smaller in 1D?Fi=1 |?
?i| andhave better training set cross-entropy than the un-adapted models built on the same data.
By eq.
(2),the adapted models should have better test perfor-mance and we verify this in the next section.4.1 Domain Adaptation Method ComparisonIn this section, we examine how MDI adapta-tion compares to other state-of-the-art methods fordomain adaptation in both perplexity and speechrecognition word-error rate.
For these experiments,we use the same development and evaluation setsand lattice rescoring setup from Section 3.1.The most widely-used techniques for domainadaptation are linear interpolation and count merg-ing.
In linear interpolation, separate n-gram modelsare built on the in-domain and out-of-domain dataand are interpolated together.
In count merging, the473in-domain data (sents.)
in-domain data (sents.
)1k 10k 100k 1k 10k 100kin-domain data only3g 488.4 270.6 168.2 34.5% 30.5% 26.1%4g 486.8 267.4 163.6 34.5% 30.4% 25.7%count merging3g 503.1 290.9 170.7 30.4% 28.3% 25.2%4g 497.1 284.9 165.3 30.0% 28.0% 25.3%linear interpolation3g 328.3 234.8 162.6 30.3% 28.5% 25.8%4g 325.3 230.8 157.6 30.3% 28.4% 25.2%MDI model3g 296.3 218.7 157.0 30.0% 28.0% 24.9%4g 293.7 215.8 152.5 29.6% 27.9% 24.9%Table 6: WSJ perplexity and lattice rescoring results fordomain adaptation models.
Values on the left are perplex-ities and values on the right are word-error rates.in-domain and out-of-domain data are concatenatedinto a single training set, and a single n-gram modelis built on the combined data set.
The in-domaindata set may be replicated several times to moreheavily weight this data.
We also consider the base-line of not using the out-of-domain data.In Table 6, we display perplexity and word-errorrates for each method, for both trigram and 4-grammodels and with varying amounts of in-domaintraining data.
The last method corresponds to theexponential MDI model; all other methods employconventional (non-exponential) n-gram models withmodified Kneser-Ney smoothing.
In count merging,only one copy of the in-domain data is included inthe training set; including more copies does not im-prove evaluation set word-error rate.Looking first at perplexity, MDI models outper-form the next best method, linear interpolation, byabout 10% in perplexity on the smallest data set and3% in perplexity on the largest.
In terms of word-error rate, MDI models again perform best of all,outperforming interpolation by 0.3?0.7% absoluteand count merging by 0.1?0.4% absolute.5 Related Work5.1 Class-Based Language ModelsIn past work, the most common baseline models areKatz-smoothed word trigram models.
Compared tothis baseline, model M achieves a perplexity reduc-tion of 28% and word-error rate reduction of 1.9%absolute with a 900k-sentence training set.
The mostclosely-related existing model to model M is themodel fullibmpredict proposed by Goodman (2001):p(cj |cj?2cj?1,wj?2wj?1)=?
p(cj |wj?2wj?1)+(1??)
p(cj |cj?2cj?1)p(wj |cj?2cj?1cj ,wj?2wj?1)=?
p(wj |wj?2wj?1cj)+(1??)
p(wj |cj?2cj?1cj) (13)This is similar to model M except that linear in-terpolation is used to combine word and class his-tory information, and there is no analog to the fi-nal term in eq.
(13) in model M. Using the NorthAmerican Business news corpus, the largest perplex-ity reduction achieved over a Katz-smoothed trigrammodel baseline by fullibmpredict is about 25%, witha training set of 1M words.
In N -best list rescor-ing with a 284M-word training set, the best resultachieved for an individual class-based model is an0.5% absolute reduction in word-error rate.To situate the quality of our results, we also re-view the best perplexity and word-error rate resultsreported for class-based language models relativeto conventional word n-gram model baselines.
Interms of absolute word-error rate, the best gains wefound in the literature are from multi-class com-posite n-gram models, a variant of the IBM classmodel (Yamamoto and Sagisaka, 1999; Yamamotoet al, 2003).
These are called composite modelsbecause frequent word sequences can be concate-nated into single units within the model; the termmulti-class refers to choosing different word clus-terings depending on word position.
In experimentson the ATR spoken language database, Yamamoto etal.
(2003) report a reduction in perplexity of 9% andan increase in word accuracy of 2.2% absolute overa Katz-smoothed trigram model.In terms of perplexity, the best gains we foundare from SuperARV language models (Wang andHarper, 2002; Wang et al, 2002; Wang et al, 2004).In these models, classes are based on abstract rolevalues as given by a Constraint Dependency Gram-mar.
The class and word prediction distributions aren-gram models that back off to a variety of mixedword/class histories in a specific order.
With a WSJtraining set of 37M words and a Katz-smoothed tri-gram model baseline, a perplexity reduction of up to47453% is achieved as well as a decrease in word-errorrate of up to 1.0% absolute.All other perplexity and absolute word-error rategains we found in the literature are considerablysmaller than those listed here.
While different datasets are used in previous work so results are not di-rectly comparable, our results appear very competi-tive with the body of existing results in the literature.5.2 Domain AdaptationHere, we discuss methods for supervised domainadaptation that involve only the simple static combi-nation of in-domain and out-of-domain data or mod-els.
For a survey of techniques using word classes,topic, syntax, etc., refer to (Bellegarda, 2004).Linear interpolation is the most widely-usedmethod for domain adaptation.
Jelinek et al (1991)describe its use for combining a cache languagemodel and static language model.
Another popularmethod is count merging; this has been motivatedas an instance of MAP adaptation (Federico, 1996;Masataki et al, 1997).
In terms of word-error rate,Iyer et al (1997) found linear interpolation to givebetter speech recognition performance while Bac-chiani et al (2006) found count merging to be su-perior.
Klakow (1998) proposes log-linear interpo-lation for domain adaptation.
As compared to reg-ular linear interpolation for bigram models, an im-provement of 4% in perplexity and 0.2% absolute inword-error rate is found.Della Pietra et al (1992) introduce the idea ofminimum discrimination information distributions.Given a prior model q(y|x), the goal is to findthe nearest model in Kullback-Liebler divergencethat satisfies a set of linear constraints derived fromadaptation data.
The model satisfying these condi-tions is an exponential model containing one fea-ture per constraint with q(y|x) as its prior as ineq.
(12).
While MDI models have been used manytimes for language model adaptation, e.g., (Kneser etal., 1997; Federico, 1999), they have not performedas well as linear interpolation in perplexity or word-error rate (Rao et al, 1995; Rao et al, 1997).One important issue with MDI models is how toselect the feature set specifying the model.
With asmall amount of adaptation data, one should intu-itively use a small feature set, e.g., containing justunigram features.
However, the use of regulariza-tion can obviate the need for intelligent feature se-lection.
In this work, we include all n-gram fea-tures present in the adaptation data for n ?
{3, 4}.Chueh and Chien (2008) propose the use of inequal-ity constraints for regularization (Kazama and Tsu-jii, 2003); here, we use `1+`22 regularization instead.We hypothesize that the use of state-of-the-art regu-larization is the primary reason why we achieve bet-ter performance relative to interpolation and countmerging as compared to earlier work.6 DiscussionFor exponential language models, eq.
(2) tells usthat with respect to test set performance, the num-ber of model parameters seems to matter not at all;all that matters are the magnitudes of the parame-ter values.
Consequently, one can improve exponen-tial language models by adding features (or a priormodel) that shrink parameter values while maintain-ing training performance, and from this observa-tion we develop Heuristics 1 and 2.
We use theseideas to motivate a novel and simple class-basedlanguage model that achieves perplexity and word-error rate improvements competitive with the bestreported results for class-based models in the litera-ture.
In addition, we show that with regularization,MDI models can outperform both linear interpola-tion and count merging in language model combina-tion.
Still, Heuristics 1 and 2 are quite vague, andit remains to be seen how to determine when theseheuristics will be effective.In summary, we have demonstrated how the trade-off between training set performance and model sizeimpacts aspects of language modeling as diverse asbackoff n-gram features, class-based models, anddomain adaptation.
In particular, we can frameperformance improvements in all of these areas asmethods that shrink models without degrading train-ing set performance.
All in all, eq.
(2) is an impor-tant tool for both understanding and improving lan-guage model performance.AcknowledgementsWe thank Bhuvana Ramabhadran and the anony-mous reviewers for their comments on this and ear-lier versions of the paper.475ReferencesMichiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat.
2006.
MAP adaptation of stochas-tic grammars.
Computer Speech and Language,20(1):41?68.Jerome R. Bellegarda.
2004.
Statistical language modeladaptation: review and perspectives.
Speech Commu-nication, 42(1):93?108.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,Jennifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479, December.Stanley F. Chen and Joshua Goodman.
1998.
An empiri-cal study of smoothing techniques for language model-ing.
Technical Report TR-10-98, Harvard University.Stanley F. Chen.
2008.
Performance prediction for expo-nential language models.
Technical Report RC 24671,IBM Research Division, October.Stanley F. Chen.
2009.
Performance prediction for expo-nential language models.
In Proc.
of HLT-NAACL.Chuang-Hua Chueh and Jen-Tzung Chien.
2008.
Reli-able feature selection for language model adaptation.In Proc.
of ICASSP, pp.
5089?5092.Stephen Della Pietra, Vincent Della Pietra, Robert L.Mercer, and Salim Roukos.
1992.
Adaptive languagemodeling using minimum discriminant estimation.
InProc.
of the Speech and Natural Language DARPAWorkshop, February.Marcello Federico.
1996.
Bayesian estimation methodsfor n-gram language model adaptation.
In Proc.
of IC-SLP, pp.
240?243.Marcello Federico.
1999.
Efficient language modeladaptation through MDI estimation.
In Proc.
of Eu-rospeech, pp.
1583?1586.Joshua T. Goodman.
2001.
A bit of progress in languagemodeling.
Technical Report MSR-TR-2001-72, Mi-crosoft Research.Rukmini Iyer, Mari Ostendorf, and Herbert Gish.
1997.Using out-of-domain data to improve in-domain lan-guage models.
IEEE Signal Processing Letters,4(8):221?223, August.Frederick Jelinek, Bernard Merialdo, Salim Roukos, andMartin Strauss.
1991.
A dynamic language model forspeech recognition.
In Proc.
of the DARPA Workshopon Speech and Natural Language, pp.
293?295, Mor-ristown, NJ, USA.Slava M. Katz.
1987.
Estimation of probabilities fromsparse data for the language model component of aspeech recognizer.
IEEE Transactions on Acoustics,Speech and Signal Processing, 35(3):400?401, March.Jun?ichi Kazama and Jun?ichi Tsujii.
2003.
Evaluationand extension of maximum entropy models with in-equality constraints.
In Proc.
of EMNLP, pp.
137?144.Dietrich Klakow.
1998.
Log-linear interpolation of lan-guage models.
In Proc.
of ICSLP.Reinhard Kneser, Jochen Peters, and Dietrich Klakow.1997.
Language model adaptation using dynamicmarginals.
In Proc.
of Eurospeech.Hirokazu Masataki, Yoshinori Sagisaka, Kazuya Hisaki,and Tatsuya Kawahara.
1997.
Task adaptation us-ing MAP estimation in n-gram language modeling.
InProc.
of ICASSP, volume 2, pp.
783?786, Washington,DC, USA.
IEEE Computer Society.Douglas B. Paul and Janet M. Baker.
1992.
The de-sign for the Wall Street Journal-based CSR corpus.In Proc.
of the DARPA Speech and Natural LanguageWorkshop, pp.
357?362, February.P.
Srinivasa Rao, Michael D. Monkowski, and SalimRoukos.
1995.
Language model adaptation via mini-mum discrimination information.
In Proc.
of ICASSP,volume 1, pp.
161?164.P.
Srinivasa Rao, Satya Dharanipragada, and SalimRoukos.
1997.
MDI adaptation of language modelsacross corpora.
In Proc.
of Eurospeech, pp.
1979?1982.George Saon, Daniel Povey, and Geoffrey Zweig.
2005.Anatomy of an extremely fast LVCSR decoder.
InProc.
of Interspeech, pp.
549?552.Hagen Soltau, Brian Kingsbury, Lidia Mangu, DanielPovey, George Saon, and Geoffrey Zweig.
2005.
TheIBM 2004 conversational telephony system for richtranscription.
In Proc.
of ICASSP, pp.
205?208.Andreas Stolcke.
1998.
Entropy-based pruning of back-off language models.
In Proc.
of the DARPA Broad-cast News Transcription and Understanding Work-shop, pp.
270?274, Lansdowne, VA, February.Wen Wang and Mary P. Harper.
2002.
The Super-ARV language model: Investigating the effectivenessof tightly integrating multiple knowledge sources.
InProc.
of EMNLP, pp.
238?247.Wen Wang, Yang Liu, and Mary P. Harper.
2002.Rescoring effectiveness of language models using dif-ferent levels of knowledge and their integration.
InProc.
of ICASSP, pp.
785?788.Wen Wang, Andreas Stolcke, and Mary P. Harper.
2004.The use of a linguistically motivated language modelin conversational speech recognition.
In Proc.
ofICASSP, pp.
261?264.Hirofumi Yamamoto and Yoshinori Sagisaka.
1999.Multi-class composite n-gram based on connection di-rection.
In Proc.
of ICASSP, pp.
533?536.Hirofumi Yamamoto, Shuntaro Isogai, and YoshinoriSagisaka.
2003.
Multi-class composite n-gram lan-guage model.
Speech Communication, 41(2-3):369?379.476
