Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524?530,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsCombining Word Patterns and Discourse Markersfor Paradigmatic Relation ClassificationMichael RothILCC, School of InformaticsUniversity of Edinburghmroth@inf.ed.ac.ukSabine Schulte im WaldeInstitut f?ur Maschinelle SprachverarbeitungUniversit?at Stuttgartschulte@ims.uni-stuttgart.deAbstractDistinguishing between paradigmatic rela-tions such as synonymy, antonymy and hy-pernymy is an important prerequisite in arange of NLP applications.
In this paper,we explore discourse relations as an alter-native set of features to lexico-syntacticpatterns.
We demonstrate that statisticsover discourse relations, collected via ex-plicit discourse markers as proxies, can beutilized as salient indicators for paradig-matic relations in multiple languages, out-performing patterns in terms of recall andF1-score.
In addition, we observe thatmarkers and patterns provide complemen-tary information, leading to significantclassification improvements when appliedin combination.1 IntroductionParadigmatic relations (such as synonymy,antonymy and hypernymy; cf.
Murphy, 2003) arenotoriously difficult to distinguish automatically,as first-order co-occurrences of the related wordstend to be very similar across the relations.
Forexample, in The boy/girl/person loves/hates thecat, the nominal co-hyponyms boy, girl and theirhypernym person as well as the verbal antonymslove and hate occur in identical contexts, respec-tively.
Vector space models, which representwords by frequencies of co-occurring words toenable comparisons in terms of distributionalsimilarity (Sch?utze, 1992; Turney and Pantel,2010), hence perform below their potential wheninferring the type of relation that holds betweentwo words.
This distinction is crucial, however,in a range of tasks: in sentiment analysis, forexample, words of the same and opposing polarityneed to be distinguished; in textual entailment,systems further need to identify hypernymybecause of directional inference requirements.Accordingly, while there is a rich tradition onidentifying word pairs of a single paradigmatic re-lation, there is little work that has addressed thedistinction between two or more paradigmatic re-lations (cf.
Section 2 for details).
In more gen-eral terms, previous approaches to distinguish-ing between several semantic relations have pre-dominantly relied on manually created knowledgesources, or lexico-syntactic patterns that can beautomatically extracted from text.
Each optioncomes with its own shortcomings: knowledgebases, on the one hand, are typically developed fora single language or domain, meaning that theymight not generalize well; word patterns, on theother hand, are noisy and can be sparse for infre-quent word pairs.In this paper, we propose to strike a balancebetween availability and restrictedness by mak-ing use of discourse markers.
This approach hasseveral advantages: markers are frequently foundacross genres (Webber, 2009), they exist in manylanguages (Jucker and Yiv, 1998), and capturevarious semantic properties (Hutchinson, 2004).We implement discourse markers within a vectorspace model that aims to distinguish between thethree paradigmatic relations synonymy, antonymyand hypernymy in German and in English, acrossthe three word classes of nouns, verbs, adjectives.We examine the performance of discourse markersas vector space dimensions in isolation and alsoexplore their contribution in combination with lex-ical patterns.2 Related WorkAs mentioned above, there is a rich tradition ofresearch on identifying a single paradigmatic rela-tions.
Work on synonyms includes Edmonds andHirst (2002), who employed a co-occurrence net-work and second-order co-occurrence, and Cur-ran (2003), who explored word-based and syntax-based co-occurrence for thesaurus construction.524Van der Plas and Tiedemann (2006) compareda standard distributional approach against cross-lingual alignment; Erk and Pad?o (2008) defineda vector space model to identify synonyms andthe substitutability of verbs.
Most computationalwork on hypernyms was performed for nouns, cf.the lexico-syntactic patterns by Hearst (1992) andan extension of the patterns by dependency paths(Snow et al, 2004).
Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identi-fied hypernyms in distributional spaces.
Computa-tional work on antonyms includes approaches thattested the co-occurrence hypothesis (Charles andMiller, 1989; Fellbaum, 1995), and approachesdriven by text understanding efforts and contradic-tion frameworks (Harabagiu et al, 2006; Moham-mad et al, 2008; de Marneffe et al, 2008).Among the few approaches that distinguishedbetween paradigmatic semantic relations, Lin et al(2003) used patterns and bilingual dictionaries toretrieve distributionally similar words, and reliedon clear antonym patterns such as ?either X or Y?in a post-processing step to distinguish synonymsfrom antonyms.
The study by Mohammad et al(2013) on the identification and ranking of oppo-sites also included synonym/antonym distinction.Yih et al (2012) developed an LSA approach in-corporating a thesaurus, to distinguish the sametwo relations.
Chang et al (2013) extended thisapproach to induce vector representations that cancapture multiple relations.
Whereas the abovementioned approaches rely on additional knowl-edge sources, Turney (2006) developed a corpus-based approach to model relational similarity, ad-dressing (among other tasks) the distinction be-tween synonyms and antonyms.
More recently,Schulte im Walde and K?oper (2013) proposed todistinguish between the three relations antonymy,synonymy and hyponymy based on automaticallyacquired word patterns.Regarding pattern-based approaches to iden-tify and distinguish lexical semantic relations inmore general terms, Hearst (1992) was the firstto propose lexico-syntactic patterns as empiricalpointers towards relation instances, focusing onhyponymy.
Girju et al (2003) applied a sin-gle pattern to distinguish pairs of nouns that arein a causal relationship from those that are not,and Girju et al (2006) extended the work to-wards part?whole relations, applying a super-vised, knowledge-intensive approach.
Chklovskiand Pantel (2004) were the first to apply pattern-based relation extraction to verbs, distinguish-ing five non-disjoint relations (similarity, strength,antonymy, enablement, happens-before).
Panteland Pennacchiotti (2006) developed Espresso, aweakly-supervised system that exploits patterns inlarge-scale web data to distinguish between fivenoun-noun relations (hypernymy, meronymy, suc-cession, reaction, production).
Similarly to Girjuet al (2006), they used generic patterns, but reliedon a bootstrapping cycle combined with reliabilitymeasures, rather than manual resources.
Whereaseach of the aforementioned approaches considersonly one word class and clearly disjoint categories,we distinguish between paradigmatic relations thatcan be distributionally very similar and propose aunified framework for nouns, verbs and adjectives.3 Baseline Model and Data SetThe task addressed in this work is to distin-guish between synonymy, antonymy and hyper-nymy.
As a starting point, we build on the ap-proach and data set used by Schulte im Waldeand K?oper (2013, henceforth just S&K).
In theirwork, frequency statistics over automatically ac-quired co-occurrence patterns were found to begood indicators for the paradigmatic relation thatholds between two given words of the same wordclass.
They further experimented with refinementsof the vector space model, for example, by onlyconsidering patterns of a specific length, weight-ing by pointwise mutual information and applyingthresholds based on frequency and reliability.Baseline Model.
We re-implemented the bestmodel from S&K with the same setup: word pairsare represented by vectors, with each entry corre-sponding to one out of almost 100,000 patterns oflemmatized word forms (e.g., X affect howyou Y ).
Each value is calculated as the log fre-quency of the corresponding pattern occurring be-tween the word pairs in a corpus, based on exactmatch.
For English, we use the ukWaC corpus(Baroni et al, 2009); for German, we rely on theCOW corpus instead of deWaC, as it is larger andbetter balanced (Sch?afer and Bildhauer, 2012).Data Set.
The evaluation data set by S&K is acollection of target and response words in Ger-man that has been collected via Amazon Mechan-ical Turk.
The data contains a balanced amountof instances across word categories and relations,also taking into account corpus frequency, degreeof ambiguity and semantic classes.
In total, the525S&K ReimplementedP R F1P R F1NounsSYN?ANT 77.4 65.0 70.7 76.7 62.2 68.7SYN?HYP 75.0 57.0 64.8 73.3 59.5 65.7VerbsSYN?ANT 70.6 40.0 51.1 84.6 36.7 51.2SYN?HYP 42.0 26.7 32.6 52.6 33.3 40.8AdjectivesSYN?ANT 88.9 66.7 76.2 94.1 66.7 78.0SYN?HYP 68.4 54.2 60.5 65.0 54.2 59.1Table 1: 2-way classification results by Schulteim Walde and K?oper (2013) and our re-implementation.
All numbers in percent.data set consists of 692 pairs of instances, dis-tributed over three word classes (nouns, verbs,adjectives) and three paradigmatic relations (syn-onymy, antonymy, hypernymy).Intermediate Evaluation.
We compare our re-implementation to the model by S&K using their80% training and 20% test split, focusing on 2-way classifications involving synonymy.
The re-sults, summarized in Table 1, confirm that our re-implementation achieves similar results.
Observeddifferences are probably an effect of the distinctcorpora applied to induce patterns and counts.We notice that the performance of both modelsstrongly depends on the affected pair of relationsand word category.
For example, precision variesin the 2-way classification between synonymy andantonymy from 70.6% to 94.1%.
Given the smallamount of test data, some of the 80/20 splits mightbe better suited for the model than others.
To avoidresulting bias effects, we perform our final evalua-tion using 5-fold cross-validation on a merged setof all training and test instances.
To illustrate theperformance of models in multiple languages, wefurther conduct experiments on a data set for En-glish relation pairs that has been collected by Giu-lia Benotto and Alessandro Lenci, following thesame methodology as the German collection.
TheEnglish data set consists of 648 pairs of instances,also distributed over nouns, verbs, adjectives, andcovering synonymy, antonymy, hypernymy.4 Markers for Relation ClassificationThe aim of this work is to establish corpus statis-tics over discourse relations as a salient source ofCONTRAST but, altough, rather .
.
.RESTATEMENT indeed, specifically, .
.
.INSTANTIATION (for) example, instance, .
.
.Table 2: Examples of discourse relations/markers.information to distinguish between paradigmaticrelations.
Our approach is motivated by linguis-tic studies that indicated a connection between dis-course relations and lexical relations of words oc-curring in the respective discourse segments: Mur-phy et al (2009) have shown, for example, thatantonyms frequently serve as indicators for con-trast relations in English and Swedish.
More gen-erally, pairs of word tokens have been identified asstrong features for classifying discourse relationswhen no explicit discourse markers are available(Pitler et al, 2009; Biran and McKeown, 2013).Whereas word pairs have frequently been usedas features for disambiguating discourse relations,to the best of our knowledge, our approach is novelin that we are the first to apply discourse relationsas features for classifying lexical relations.
Onereason for this might be that discourse relations ingeneral are only available in manually annotatedcorpora.
Previous work has shown, however, thatsuch relations can be classified reliably given thepresence of explicit discourse markers.1We hencerely on such markers as proxies for discourse rela-tions (for examples, cf.
Table 2).4.1 Model and HypothesisWe propose a vector space model that representspairs of words using as features the discoursemarkers that occur between them.
The under-lying hypothesis of this model is as follows: iftwo phrases frequently co-occur with a specificdiscourse marker, then the discourse relation ex-pressed by the corresponding marker should alsoindicate the relation between the words in the af-fected phrases.
Following this hypothesis, contrastrelations might indicate antonymy, whereas elab-orations may indicate synonymy or hyponymy.Although such relations will not hold betweenevery pair of words in two connected discoursesegments, we hypothesize that correct instances(of all considered word classes) can be identifiedbased on high relative frequency.In our model, frequency statistics are com-puted over sentence-internal co-occurrences of1Pitler et al (2008) report an accuracy of up to 93%.526word pairs and discourse markers.
Since discourserelations are typically directed, we take into con-sideration whether a word occurs to the left orto the right of the respective marker.
Accord-ingly, the features of our model are special cases ofsingle-word patterns with an arbitrary number ofwild card tokens (e.g., the marker feature ?though?corresponds to the pattern ?X ?
though ?
Y ?
).Yet, our specific choice of features has several ad-vantages: Whereas strict and potentially long pat-terns can be rare in text, discourse markers such as?however?, ?for example?
and ?additionally?
arefrequently found across genres (Webber, 2009).Although combinations of tokens could also be re-placed by wild cards in any automatically acquiredpattern, this would generally lead to an exponen-tially growing feature space.
In contrast, the setof discourse markers in our work is fixed: for En-glish, we use 61 markers annotated in the PennDiscourse TreeBank 2.0 (Prasad et al, 2008); forGerman, we use 155 one-word translations of theEnglish markers, as obtained from an online dic-tionary.23Taking directionality into account, ourvector space model consists of 2x61 and 2x155features, respectively.4.2 Development Set and HyperparametersWe select the hyperparameters of our model usingan independent development set, which we extractfrom the lexical resource GermaNet (Hamp andFeldweg, 1997).
For each considered word cate-gory, we extract instances of synonymy, antonymyand hypernymy.
In total, 1502 instances are iden-tified, with 64 of them overlapping with the evalu-ation data set described in Section 3.
Note thoughthat the development set is not used for evaluationbut only to select the following hyperparameters.We experimented with different vector values(absolute frequency, log frequency, pointwise mu-tual information (PMI)), distance measures (co-sine, euclidean) and normalization schemes.
Incontrast to S&K, who did not observe any im-provements using PMI, we found it to performbest, combined with euclidean distance and noadditional normalization.
This finding might bean immediate effect of discourse markers being2http://dict.leo.org3We also experimented with larger sets of markers, in-cluding conjunctions and adverbials in sentence-initial posi-tions, but did not notice any considerable effect.
Future workcould use manual sets of markers, e.g.
those by Pasch et al(2003), though such sets are only available in few languages.generally more frequent than strict word patterns,which also leads to more reliable PMI values.5 EvaluationIn our evaluation, we assess the performance of themarker-based model and demonstrate the benefitsof incorporating discourse markers into a pattern-based model, which we apply as a baseline.
Weevaluate on several data sets: the collection oftarget-response pairs in German from previouswork, and a similar data set that was collected forEnglish target words (cf.
Section 3); for compari-son reasons, we also apply our models to the bal-anced data set of related and unrelated noun pairsby Yap and Baldwin (2009).4We perform 3-wayand 2-way relation classification experiments, us-ing 5-fold cross-validation and a nearest centroidclassifier (as applied by S&K).Results.
The 3-way classification results of thebaseline and our marker-based model are summa-rized in Table 3, with best results for each set-ting marked in bold.
On the German data set,our model always outperforms a random baseline(33% F1-score).
The results on the English dataset are overall a bit lower, possibly due to corpussize.
In almost all classification tasks, our marker-based model achieves a higher recall and F1-scorethan the pattern-based approach.
The precisionresults of the marker-based model are overall be-low the pattern-based model.
This drop in perfor-mance does not come as a surprise though, con-sidering that the model only makes use of 122 and310 features, in comparison to tens of thousandsof features in the pattern approach.A randomized significance test over classifiedinstances (cf.
Yeh, 2000) revealed that only twodifferences in results are significant.
We hypoth-esize that one reason for this outcome might bethat both models cover complementary sets of in-stances.
To verify this hypothesis, we apply acombined model, which is based on a weightedlinear combination of distances computed by thetwo individual models.5As displayed in Table 3,this combined model yields further improvements4Note that we could, in principle, also apply our models tounbalanced data.
Our main focus lies however on examiningthe direct impact of different feature sets.
We hence decidedto keep the evaluation setup simple and used a classifier thatdoes not take into account class frequency.5We determined the best weights on the development setand found these to be 0.9 and 0.1 for the output of the pattern-based and marker-based model, respectively.527Nouns Verbs AdjectivesP R F1P R F1P R F1GermanPatterns 55.6 40.8 47.0 55.6 35.6 43.4 53.5 41.1 46.5Markers 42.6 38.7 40.5 48.4 46.2** 47.3 51.1 48.6 49.9Combined 50.4 45.7* 48.0 52.6 50.2** 51.4** 53.4 50.8** 52.1English Patterns 46.4 28.0 34.9 44.7 28.5 34.8 56.6 32.1 41.0Markers 39.0 34.3 36.5 38.3 36.3 37.2 50.0 41.2** 45.2Combined 43.0 37.8** 40.3* 41.8 39.6** 40.7* 53.5 44.4** 48.5**Table 3: 3-way classification results using 5-fold cross-validation.
All numbers in percent.
Asterisksindicate significant differences to the pattern-based baseline model (* p<0.10, ** p<0.05).CombinedmodelGerman EnglishP R F1P R F1NounsSYN?ANT 61.7 55.7 58.5 52.9 44.2 48.2SYN?HYP 66.5 60.4 63.3 62.2 58.6 60.4ANT?HYP 70.9 64.6 67.6 59.1 50.6 54.5VerbsSYN?ANT 58.9 55.0 56.8 49.6 45.8 47.6SYN?HYP 67.6 64.0 65.8 66.4 63.0 64.7ANT?HYP 67.3 66.4 66.9 62.9 60.7 61.8AdjectivesSYN?ANT 74.8 69.4 72.0 67.0 56.6 61.3SYN?HYP 58.0 56.1 57.0 56.4 46.0 50.7ANT?HYP 73.7 70.7 72.2 69.8 57.8 63.2Table 4: 2-way results of the combined model.Bold numbers indicate improvements over bothindividual models.
All numbers in percent.in recall and F1-score, leading to the best 3-wayclassification results.
All gains in recall are sig-nificant, confirming that the single models in-deed contribute complementary information.
Forexample, only the pattern-based model classifies?intentional???accidental?
as antonyms, and onlythe marker-based model predicts the correct rela-tion for ?double???multiple?
(hypernymy).
Thecombined model classifies both pairs correctly.Table 4 further assesses the strength of the com-bined model on the 2-way classifications.
Thetable highlights results indicating improvementsover both individual models.
We observe that thecombined model achieves the best recall and F1-score in 15 out of 18 cases.Relation SYN ANT HYPPatterns 0.97 0.97 0.94Markers 0.77* 0.82* 0.91*Combined 0.93* 0.98 0.96*Table 5: Results in F1-score on the balanced dataset by Yap and Baldwin (* p<0.05).A final experiment is performed on the data setby Yap and Baldwin (2009) to see whether ourmodels can also distinguish word pairs of individ-ual relations from unrelated pairs of words.
Theresults, listed in Table 5, show that the marker-based model cannot perform this task as well asthe pattern-based model.
The combined model,however, outperforms both individual models in 2out of 3 cases.
Despite their simplicity, our modelsachieve results close to the F1-scores reported byYap and Baldwin (0.98?0.99), who employed syn-tactic pre-processing and an SVM-based classifier,and experimented with different corpora.6 ConclusionsIn this paper, we proposed to use discourse mark-ers as indicators for paradigmatic relations be-tween words and demonstrated that a small setof such markers can achieve higher recall than apattern-based model with tens of thousands of fea-tures.
Combining patterns and markers can furtherimprove results, leading to significant gains in re-call and F1.
As our new model only relies on a rawcorpus and a fixed list of discourse markers, it caneasily be extended to other languages.AcknowledgmentsThe research presented in this paper was fundedby the DFG grant SCHU-2580/2-1 and the DFGHeiselberg Fellowship SCHU-2580/1-1.528ReferencesMarco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The wacky wideweb: a collection of very large linguistically pro-cessed web-crawled corpora.
Language resourcesand evaluation, 43(3):209?226.Or Biran and Kathleen McKeown.
2013.
Aggre-gated word pair features for implicit discourse rela-tion disambiguation.
In Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pages 69?73,Sofia, Bulgaria, August.Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.2013.
Multi-relational latent semantic analysis.
InProceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing, pages1602?1612, Seattle, Washington, USA, October.Walter G. Charles and George A. Miller.
1989.
Con-texts of antonymous adjectives.
Applied Psycholin-guistics, 10(3):357?375.Tim Chklovski and Patrick Pantel.
2004.
VerbOcean:Mining the Web for fine-grained semantic verb re-lations.
In Proceedings of the 2004 Conference onEmpirical Methods in Natural Language Process-ing, Barcelona, Spain, 25?26 July 2004, pages 33?40.James Curran.
2003.
From Distributional to SemanticSimilarity.
Ph.D. thesis, Institute for Communica-tion and Collaborative Systems, School of Informat-ics, University of Edinburgh.Marie-Catherine de Marneffe, Anna N. Rafferty, andChristopher D. Manning.
2008.
Finding contra-dictions in text.
In Proceedings of the 46th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages1039?1047, Columbus, Ohio, USA.Philip Edmonds and Graeme Hirst.
2002.
Near-synonymy and lexical choice.
Computational Lin-guistics, 28(2):105?144.Katrin Erk and Sebastian Pad?o.
2008.
A structuredvector space model for word meaning in context.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, Waikiki,Honolulu, Hawaii, 25-27 October 2008.Christiane Fellbaum.
1995.
Co-occurrence andantonymy.
International Journal of Lexicography,8(4):281?303.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2003.
Learning semantic constraints for the auto-matic discovery of part-whole relations.
In Proceed-ings of the Human Language Technology Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics, Edmonton, Al-berta, Canada, 27 May ?1 June 2003, pages 80?87.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2006.
Automatic discovery of part-whole relations.Computational Linguistics, 32(1):83?135.Birgit Hamp and Helmut Feldweg.
1997.
GermaNet -a lexical-semantic net for German.
In Proceedingsof the Workshop on Automatic Information Extrac-tion and Building of Lexical Semantic Resources forNLP Applications at ACL/EACL-97, Madrid, Spain,12 July 1997, pages 9?15.Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.2006.
Negation, contrast and contradiction in textprocessing.
In In Proceedings of the 21st NationalConference on Artificial Intelligence, pages 755?762.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedingsof the 15th International Conference on Computa-tional Linguistics, Nantes, France, 23-28 August1992, pages 539?545.Ben Hutchinson.
2004.
Acquiring the meaning of dis-course markers.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Lin-guistics, Barcelona, Spain, 21?26 July 2004, pages685?692.Andreas H. Jucker and Zael Yiv, editors.
1998.
Dis-course Markers: Descriptions and Theory, vol-ume 57 of Discourse & Beyond New Series.
JohnBenjamin Publishing Company.Alessandro Lenci and Giulia Benotto.
2012.
Identify-ing hypernyms in distributional semantic spaces.
InProceedings of the First Joint Conference on Lexicaland Computational Semantic, pages 75?79.Dekang Lin, Shaojun Zhao, Lijuan Qin, and MingZhou.
2003.
Identifying synonyms among distri-butionally similar words.
In Proceedings of the 18thInternational Joint Conference on Artificial Intelli-gence, pages 1492?1493.
Morgan Kaufmann Pub-lishers Inc.Saif M. Mohammad, Bonnie Dorr, and Graeme Hirst.2008.
Computing word-pair antonymy.
In Proceed-ings of the 2008 Conference on Empirical Methodsin Natural Language Processing, pages 982?991,Honolulu, Hawaii, USA.Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, andPeter D. Turney.
2013.
Computing lexical contrast.Computational Linguistics, 39(3):555?590.M.
Lynne Murphy, Carita Paradis, Caroline Will-ners, and Steven Jones.
2009.
Discourse func-tions of antonymy: A cross-linguistic investigationof Swedish and English.
Journal of Pragmatics,41(11):2159?2184.M.
Lynne Murphy.
2003.
Semantic relations and thelexicon.
Cambridge University Press.529Patrick Pantel and Marco Pennacchiotti.
2006.Espresso: Leveraging generic patterns for automati-cally harvesting semantic relations.
In Proceedingsof the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, Sydney,Australia, 17?21 July 2006, pages 113?120.Renate Pasch, Ursula Brausse, Eva Breindl, and UlrichWassner.
2003.
Handbuch der deutschen Konnek-toren.
Walter de Gruyter, Berlin.Emily Pitler and Ani Nenkova.
2008.
Revisitingreadability: A unified framework for predicting textquality.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 186?195, Honolulu, Hawaii, October.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.Automatic sense prediction for implicit discourse re-lations in text.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 683?691,Suntec, Singapore, August.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind K Joshi, and Bon-nie L Webber.
2008.
The Penn Discourse Tree-Bank 2.0.
In Proceedings of the Sixth InternationalConference on Language Resources and Evaluation(LREC-2008), Marrakesh, Marocco, May.Enrico Santus, Alessandro Lenci, Qin Lu, and SabineSchulte im Walde.
2014.
Chasing hypernyms invector spaces with entropy.
In Proceedings of the14th Conference of the European Chapter of the As-sociation for Computational Linguistics, volume 2:Short Papers, pages 38?42, Gothenburg, Sweden.Roland Sch?afer and Felix Bildhauer.
2012.
Build-ing large corpora from the web using a new effi-cient tool chain.
In Proceedings of the Eighth In-ternational Conference on Language Resources andEvaluation (LREC-2012), pages 486?493, Istanbul,Turkey, May.Sabine Schulte im Walde and Maximilian K?oper.
2013.Pattern-based distinction of paradigmatic relationsfor German nouns, verbs, adjectives.
In LanguageProcessing and Knowledge in the Web, pages 184?198.
Springer.Hinrich Sch?utze.
1992.
Dimensions of meaning.
In InProceedings of Supercomputing, pages 787?796.Rion Snow, Daniel Jurafsky, and Andrew Y Ng.
2004.Learning syntactic patterns for automatic hypernymdiscovery.
In Advances in Neural Information Pro-cessing Systems, volume 17, pages 1297?1304.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37(1):141?188.Peter D. Turney.
2006.
Similarity of semantic rela-tions.
Computational Linguistics, 32(3):379?416.Lonneke Van der Plas and J?org Tiedemann.
2006.Finding synonyms using automatic word alignmentand measures of distributional similarity.
In Pro-ceedings of the COLING/ACL on Main conferenceposter sessions, pages 866?873.Bonnie Webber.
2009.
Genre distinctions for dis-course in the Penn TreeBank.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages674?682, Suntec, Singapore, August.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributionalsimilarity.
In Proceedings of the 20th InternationalConference on Computational Linguistics, pages1015?1021.Willy Yap and Timothy Baldwin.
2009.
Experimentson pattern-based relation learning.
In Proceedingsof the 18th ACM Conference on Information andKnowledge Management, pages 1657?1660.Alexander Yeh.
2000.
More accurate tests forthe statistical significance of result differences.In Proceedings of the 18th International Confer-ence on Computational Linguistics, pages 947?953,Saarbr?ucken, Germany, August.Wen-tau Yih, Geoffrey Zweig, and John Platt.
2012.Polarity inducing latent semantic analysis.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1212?1222, Jeju Island, Korea, July.530
