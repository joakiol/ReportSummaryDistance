Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 333?336,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsGenerating Expository Dialogue from Monologue:Motivation, Corpus and Preliminary RulesPaul PiwekCentre for Research in ComputingThe Open UniversityWalton Hall, Milton Keynes, UKp.piwek@open.ac.ukSvetlana StoyanchevCentre for Research in ComputingThe Open UniversityWalton Hall, Milton Keynes, UKs.stoyanchev@open.ac.ukAbstractGenerating expository dialogue from mono-logue is a task that poses an interesting and re-warding challenge for Natural Language Pro-cessing.
This short paper has three aims:firstly, to motivate the importance of thistask, both in terms of the benefits of ex-pository dialogue as a way to present in-formation and in terms of potential applica-tions; secondly, to introduce a parallel cor-pus of monologues and dialogues which en-ables a data-driven approach to this challenge;and, finally, to describe work-in-progress onsemi-automatic construction of Monologue-to-Dialogue (M2D) generation rules.1 IntroductionThe tasks of text generation ?
e.g., Reiter et al(2005) and Demir et al (2008) ?
and generationin dialogue ?
e.g., Stent (2002) and DeVault et al(2008) ?
are central topics in Natural Language Gen-eration (NLG).
What sets the two tasks apart is theinteractive nature of dialogue, where participantsneed to adapt their contributions to each other.This paper introduces an NLG task, the genera-tion of expository dialogue, to the ComputationalLinguistics community which occupies the middleground between these two tasks.
An expository di-alogue is an authored conversation between two fic-tive characters.
It can be presented as text, audio orfilm.
Although there is no real-time interactivity, inexpository dialogue the contributions of the charac-ters do need to mesh with each other.
The main pur-pose of expository dialogue is to present informa-tion (a description, explanation or definition) to thereader, hearer or viewer, in contrast with dramaticdialogue, which tells a story.The use of expository dialogue goes back as far asPlato (c. 470-399 BC), who expressed his ideas asdialogues between Socrates and his contemporaries.Recently, a number of empirical studies show thatfor some purposes expository dialogue has advan-tages over monologue: for learners, dialogue can bemore memorable, stimulate them to formulate theirown questions (Craig et al, 2000), and get them totalk with each other (Lee et al, 1998).
Expositorydialogue has also been found to be more effectivefor persuasion (Suzuki and Yamada, 2004).Additionally, dialogue lends itself very wellfor multimedia presentations by computer-animatedagents (Andre?
et al, 2000; van Deemter et al,2008).
Potential application domains include ed-ucation, (serious) games and E-Health.
In educa-tion, information from textbooks could be presentedin dialogue form, possibly using virtual reality plat-forms such as Second Life.
Automatically gener-ating dialogue from text for non-player characterscould have a tremendous impact on the gaming in-dustry; e.g., (IGDA Game Writers SIG, 2003) statethat the amount of dialogue script for a character-driven computer game is usually many times thatfor the average film.
In connection with E-health,consider patient information leaflets, which are of-ten left unread; presenting them as movies betweena virtual pharmacist and client may help address this.Thus instead of being presented with(1) a.
You can take aspirin,b.
if you have a headache.333c.
Though aspirin does have side effects:d. it can harm circulation.the patient could watch a movie on their mobile de-vice of an exchange between a virtual client (lay-man, L) and pharmacist (expert, E):(2) L: What if I have a headache?E: You can take aspirinL: But does it have side effects?E: Yes, it can harm circulation.So far, research on generating expository dialoguehas been firmly rooted in classical AI approaches.Work in this area starts from knowledge represen-tations or databases (Andre?
et al, 2000), and evenresearch that does take text as input ?
e.g., Piweket al (2007) describe a system for generating di-alogues such as Example 2 ?
relies on handcraftedrules.
Two challenges present themselves for NLPresearch: 1) generation of expository dialogue fromtext, and 2) use of data-driven, rather than manuallyauthored, generation rules.Apart from the cost of manually authoring gener-ation rules, previous research has found that human-authored rules can result in ?too much information[being] given too quickly?
(Williams et al, 2007),which can be addressed by conversational padding.We argue that rather than trying to invent paddingrules, the best strategy is to learn rules automaticallyfrom professionally authored dialogues.2 The CODA CorpusTo make inroads into data-driven dialogue genera-tion, we first need to have the necessary resources.We propose to view Monologue-to-Dialogue (M2D)generation as analogous to machine translation; con-sequently we need a parallel corpus for learningmappings from the source (monologue) to the tar-get (dialogue) texts.
In the ongoing CODA1 projectwe have created such a corpus.
It consists of profes-sionally authored dialogues2 that have been alignedwith monologues (written by ourselves) expressingthe same information.
Since our ultimate aim is togenerate dialogues that resemble those written by1COherent Dialogue Automatically generated from text2Most dialogues are from the Gutenberg library to facilitateour planned release of the corpus to the research community.Sp Dialog act Dialogue Turn MonologueE: ComplexQuestionWhen you havea pain in yourfoot, how doyou know it?When youhave a pain inyour foot (i)you know itbecause youL: Explain I feel it.
can feel it.
(ii)E: Explain-ContradictBut you do notfeel it until anerve reportsthe hurt to thebrain.But you do notfeel it until anerve reportsthe hurt to thebrain.
(iii)E: YN-QuestionYet the brain isthe seat of themind , is it not?Yet the brain isthe seat of themind.
(iv)Table 1: Parallel Monologue and Dialogue Example fromMark Twain?s ?What is Man?
?acclaimed authors, we started with professionallyauthored dialogues and created the correspondingmonologues.
From a practical point of view, it wasmore feasible to use existing dialogue by acclaimedauthors than to hire professional authors to write di-alogue based on monologues.We have annotated both dialogues and mono-logues: dialogue with dialogue acts and monologuewith discourse relations.3 We achieved 91% agree-ment on segmentation and kappa=.82 for dialogueact annotation on 11 dialogue act tags.
We devel-oped a D2MTranslation tool for monologue author-ing, segmentation and dialogue annotation.In January 2010, the corpus included 500 turnsfrom ?What is man?
?, a dialogue by Mark Twain,and 88 turns from ?Evolving Algebras?, an aca-demic paper in the form of dialogue by Yuri Gure-vich.4 Both of these expository dialogues presentconversation between an expert (Old Man in Twainand Author in Gurevich) and a layman (Young Manin Twain and Quisani in Gurevich).
Table 1 showsan example of a dialogue fragment, aligned mono-logue and dialogue act annotations.
The discoursestructure of the monologue is depicted in Figure 1.Table 2 shows the distribution of the dialogue actsbetween expert and layman.
In both dialogues, the3See (Stoyanchev and Piwek, 2010) for details.4In addition to these dialogues we are working on a dialogueby Berkeley (Three Dialogues between Hylas and Philonous)and a selection of shorter fragments (for copyrights reasons) byauthors such as Douglas Hofstadter and Paul Feyerabend.334Figure 1: Discourse structure of the monologue in Table 1most frequent dialogue act is Explain, where a char-acter presents information (as a new idea or as a re-sponse to another utterance).
Also, in both dialoguesthe layman asks more often for clarification thanthe expert.
The distribution over information re-quests (yes/no, factoid, and complex questions) andresponses (yes, no, factoid) differs between the twodialogues: in Twain?s dialogue, the expert mostlyrequests information and the layman responds to re-quests, whereas in Gurevich?s dialogue it is the otherway around.The differences in style suggests that the M2Dmapping rules will be author or style-specific.
Byapplying M2D rules obtained from two different au-thors (e.g., Twain and Gurevich) to the same text(e.g., the aspirin example) we can generate two dif-ferent dialogues.
This will enable us to vary the pre-sentation style of automatically generated dialogues.Twain GurevichTag Expert Layman Expert LaymanExplain 69 55 49 24Clarify 1 15 0 6Request 60 26 2 29Response 14 43 9 0Table 2: Dialogue act tag frequencies for expert and lay-man in a sample of 250 turns from Twain and 88 turnsfrom Gurevich dialogues.3 RulesWe automatically derive M2D rules from the aligneddiscourse relations and dialogue acts in our parallelcorpus of monologues and dialogues.
Table 3 showsthree rules generated from the parallel dialogue?monologue fragment in Table 1.
The first rule, R1,is based on the complete discourse structure of themonologue (i?iv), whereas R2 and R3 are based ononly a part of it: R2 is based on i?iii, whereas R3 isbased on i and ii.
By generating rules from subtreesof a discourse structure, we obtain several rules froma single dialogue fragment in the corpus.Condition Elaborationb a dcContrastCondition Elaborationb a dcConditionb aContrastc ?
d(1)(2)ElaborationdcContrasta ?
b(4)(3)Figure 2: Discourse structures of the monologue in Ex-ample 1. a-b and c-d indicate a concatenation of twoclauses.Let us illustrate the use of such rules by applyingthem to Example 1 about aspirin.
The relations be-tween the clauses of the example are depicted in Fig-ure 2 (1).
To generate a dialogue, we apply a match-ing M2D rule.
Alternatively, we can first simplifythe discourse structure of the monologue by remov-ing relation nodes as illustrated in Figure 2 (2?4).The simplified structure in Figure 2 (2) matchesrule R2 from Table 3.
By applying R2 we gener-ate the dialogue in Table 4: the expert asks a com-plex question composed of clauses a and b, whichthe layman answers with an explanation generatedfrom the same set of clauses.
Then the expert offersa contradicting explanation generated from c and d.To generate dialogue sentences for a correspondingdiscourse structure we are adapting the approach toparaphrasing of Barzilay and McKeown (2001).4 ConclusionThis short paper presented three angles on theMonologue-to-Dialogue (M2D) task.
First, as anopinion piece, it motivates the task of generating ex-pository dialogue from monologue.
We describedempirical research that provides evidence for theeffectiveness of expository dialogue and discussedapplications from education, gaming and E-health.Second, we introduced the CODA corpus for ad-dressing the task.
Finally, we reported on work-in-progress on semi-automatic construction of M2Drules.
Our implemented algorithm extracts severalM2D rules from the corpus that are applicable evento a relatively simple input.
Additionally, frequencyanalysis of dialogue tags suggests that there is scopefor generating different dialogue styles.The timeliness of this research is evidenced by theemergence of a Question Generation (QG) commu-335ID Dialogue Structure Monologue StructureR1 E: Complex Question (i-ii) Contrast (Contrast (Condition(i,ii), iii, iv))L: Explain (i-ii)E: Explain-Contradict (iii)E: YNQuestion (iv)R2 E: Complex Question (i-ii) Contrast (Condition(i,ii), iii)L: Explain(i-ii)E: Explain-Contradict (iii)R3 E: Complex Question (i-ii) Condition (i,ii)L: Explain (i-ii)Table 3: Monologue-to-Dialogue rules extracted from the parallel example in Table 1Sp Dialogue act Dialogue TurnE: Complex Ques-tion a-bIf you have a headache, whatdo you do?L: Explain a-b Take aspirin.E: Explain-Contradictc-dBut aspirin does have sideeffects: it can harm circula-tionTable 4: A dialogue generated from the monologue aboutaspirin by applying the rule R2 (see Table 3)nity.
QG is a subtask of M2D.
The first QG work-shop was held at the end of 2008, resulting in pro-posals for a Shared Task and Evaluation Campaign(Rus and Graesser, 2009) for 2010.
The CODA cor-pus should prove to be a useful resource not only forM2D researchers, but also for the QG community.AcknowledgmentsThe research reported in this paper was funded bythe UK Engineering and Physical Sciences ResearchCouncil under grant EP/G/020981/1.ReferencesE.
Andre?, T. Rist, S. van Mulken, M. Klesen, andS.
Baldes.
2000.
The automated design of believabledialogues for animated presentation teams.
In Em-bodied Conversational Agents, pages 220?255.
MITPress, Cambridge, Mass.R.
Barzilay and K. McKeown.
2001.
Extracting para-phrases from a parallel corpus.
In Proc.
of ACL/EACL,Toulouse.S.
Craig, B. Gholson, M. Ventura, A. Graesser, and theTutoring Research Group.
2000.
Overhearing dia-logues and monologues in virtual tutoring sessions.International Journal of Artificial Intelligence in Ed-ucation, 11:242?253.S.
Demir, S. Carberry, and K. McCoy.
2008.
GeneratingTextual Summaries of Bar Charts .
In Procs of INLG2008, Ohio, June.D.
DeVault, D. Traum, and R. Artstein.
2008.
MakingGrammar-Based Generation Easier to Deploy in Dia-logue Systems.
In Procs SIGdial 2008, Ohio, June.J.
Lee, F. Dinneen, and J. McKendree.
1998.
Supportingstudent discussions: it isn?t just talk.
Education andInformation Technologies, 3:217?229.P.
Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.2007.
T2D: Generating Dialogues between VirtualAgents Automatically from Text.
In Intelligent VirtualAgents, LNAI 4722, pages 161?174.
Springer Verlag.E.
Reiter, S. Sripada, J.
Hunter, J. Yu, and I. Davy.
2005.Choosing Words in Computer-Generated WeatherForecasts.
Artificial Intelligence, 167:137?169.V.
Rus and A. Graesser, editors.
2009.
The Ques-tion Generation Shared Task and Evaluation Chal-lenge.
The University of Memphis.
Available at:http://www.questiongeneration.org/.IGDA Game Writers SIG.
2003. International gamedevelopers association?s (IGDA) guide to writing forgames.
IGDA White Paper.A.
Stent.
2002.
A conversation acts model for generatingspoken dialogue contributions.
Computer Speech andLanguage, 16(3-4):313?352.S.
Stoyanchev and P. Piwek.
2010.
Constructing theCODA corpus.
In Procs of LREC 2010, Malta, May.S.
V. Suzuki and S. Yamada.
2004.
Persuasion throughoverheard communication by life-like agents.
In Procsof the 2004 IEEE/WIC/ACM International Conferenceon Intelligent Agent Technology, Beijing, September.K.
van Deemter, B. Krenn, P. Piwek, M. Klesen,M.
Schro?der, and S. Baumann.
2008.
Fully gener-ated scripted dialogue for embodied agents.
ArtificialIntelligence Journal, 172(10):1219?1244.S.
Williams, P. Piwek, and R. Power.
2007.
Generat-ing Monologue and Dialogue to Present PersonalisedMedical Information to Patients.
In Procs ENLG2007, pages 167?170, Schloss Dagstuhl, Germany.336
