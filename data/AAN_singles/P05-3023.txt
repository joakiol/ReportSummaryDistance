Proceedings of the ACL Interactive Poster and Demonstration Sessions,pages 89?92, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsTransonics: A Practical Speech-to-Speech Translator for English-FarsiMedical DialoguesEmil Ettelaie, Sudeep Gandhe, Panayiotis Georgiou,Kevin Knight, Daniel Marcu, Shrikanth Narayanan ,David TraumUniversity of Southern CaliforniaLos Angeles, CA 90089ettelaie@isi.edu, gandhe@ict.usc.edu,georgiou@sipi.usc.edu, knight@isi.edu,marcu@isi.edu, shri@sipi.usc.edu,traum@ict.use.eduRobert BelvinHRL Laboratories, LLC3011 Malibu Canyon Rd.Malibu, CA 90265rsbelvin@hrl.comAbstractWe briefly describe a two-way speech-to-speech English-Farsi translation systemprototype developed for use in doctor-patient interactions.
The overarchingphilosophy of the developers has been tocreate a system that enables effectivecommunication, rather than focusing onmaximizing component-level perform-ance.
The discussion focuses on the gen-eral approach and evaluation of thesystem by an independent governmentevaluation team.1 IntroductionIn this paper we give a brief description of atwo-way speech-to-speech translation system,which was created under a collaborative effortbetween three organizations within USC (theSpeech Analysis and Interpretation Lab of theElectrical Engineering department, the InformationSciences Institute, and the Institute for CreativeTechnologies) and the Information Sciences Lab ofHRL Laboratories.
The system is intended to pro-vide a means of enabling communication betweenmonolingual English speakers and monolingualFarsi (Persian) speakers.
The system is targeted ata domain which may be roughly characterized as"urgent care" medical interactions, where the Eng-lish speaker is a medical professional and the Farsispeaker is the patient.
In addition to providing abrief description of the system (and pointers to pa-pers which contain more detailed information), wegive an overview of the major system evaluationactivities.2 General Design of the systemOur system is comprised of seven speech andlanguage processing components, as shown in Fig.1.
Modules communicate using a centralized mes-sage-passing system.
The individual subsystemsare the Automatic Speech Recognition (ASR) sub-system, which uses n-gram Language Models(LM) and produces n-best lists/lattices along withthe decoding confidence scores.
The output of theASR is sent to the Dialog Manager (DM), whichdisplays the n-best and passes one hypothesis on tothe translation modules, according to a user-configurable state.
The DM sends translation re-quests to the Machine Translation (MT) unit.
TheMT unit works in two modes: Classifier based MTand a fully Stochastic MT.
Depending on the dia-logue manager mode, translations can be sent tothe unit selection based Text-To-Speech synthe-sizer (TTS), to provide the spoken output.
Thesame basic pipeline works in both directions: Eng-lish ASR, English-Persian MT, Persian TTS, orPersian ASR, Persian-English MT, English TTS.There is, however, an asymmetry in the dia-logue management and control, given the desire forthe English-speaking doctor to be in control of thedevice and the primary "director" of the dialog.The English ASR used the University of Colo-rado Sonic recognizer, augmented primarily withLM data collected from multiple sources, including89our own large-scale simulated doctor-patient dia-logue corpus based on recordings of medical stu-dents examining standardized patients (details inBelvin et al 2004).1The Farsi acoustic models r e-quired an eclectic approach due to the lack of ex-isting labeled speech corpora.
The approachincluded borrowing acoustic data from English bymeans of developing a sub-phonetic mapping be-tween the two languages, as detailed in (Srini-vasamurthy & Narayanan 2003), as well as use ofa small existing Farsi speech corpus (FARSDAT),and our own team-internally generated acousticdata.
Language modeling data was also obtainedfrom multiple sources.
The Defense LanguageInstitute translated approximately 600,000 wordsof English medical dialogue data (including ourstandardized patient data mentioned above), and inaddition, we were able to obtain usable Farsi textfrom mining the web for electronic news sources.Other  smaller  amounts of  training  data  were obtained from various sources, as detailed in  (Nara-yanan et al 2003, 2004).
Additional detail on de-velopment methods for all of these components,system integration and evaluation can also befound in the papers just cited.The MT components, as noted, consist of both aClassifier and a stochastic translation engine,  both1Standardized Patients are typically actors who have beentrained by doctors or nurses to portray symptoms of particularillnesses or injuries.
They are used extensively in medicaleducation so that doctors in training don't have to "practice"on real patients.developed by USC-ISI team members.
The Eng-lish Classifier uses approximately 1400 classesconsisting mostly of standard questions used bymedical care providers in medical interviews.Each class has a large number of paraphrases asso-ciated with it, such that if the care provider speaksone of those phrases, the system will identify itwith the class and translate it to Farsi via table-lookup.
If the Classifier cannot succeed in findinga match exceeding a confidence threshold, the sto-chastic MT engine will be employed.
The sto-chastic MT engine relies on n-gramcorrespondences between the source and targetlanguages.
As with ASR, the performance of thecomponent is highly dependent on very largeamounts of training data.
Again, there were multi-ple sources of training data used, the most signifi-cant being the data generated by our own team'sEnglish collection effort, supported by translationinto Farsi by DLI.
Further details of the MT com-ponents can be found in Narayanan et al, op.cit.3 Enabling Effective CommunicationThe approach taken in the development of Tran-sonics was what can be referred to as the totalcommunication pathway.
We are not so concernedwith trying to maximize the performance of agiven component of the system, but rather with theeffectiveness of the system as a whole in facilitat-ing actual communication.
To this end, our designand development included the following:MTEnglish to FarsiFarsi to EnglishASREnglishPrompts or TTSFarsiPrompts or TTSEnglishASRFarsiGUI:prompts,confirmations,ASR switchDialogManagerSMTEnglish to FarsiFarsi to EnglishFigure 1: Architecture of the Transonics system.
The Dialogue Manager acts as the hub through which theindividual components interact.90i.
an "educated guess" capability (systemguessing at the meaning of an utterance) from theClassifier translation mechanism?this proved veryuseful for noisy ASR output, especially for the re-stricted domain of medical interviews.ii.
a flexible and robust SMT good for filling inwhere the more accurate Classifier misses.iii.
exploitation of a partial n-best list as part ofthe GUI used by the doctor/medic for the EnglishASR component and the Farsi-to-English transla-tion component.iv.
a dialog manager which in essence occa-sionally makes  "suggestions" (for next questionsfor the doctor to ask) based on query sets which aretopically related to the query the system believes itrecognized the doctor to have spoken.Overall, the system achieves a respectable level ofperformance in terms of allowing users to follow aconversational thread in a fairly coherent way, de-spite the presence of frequent ungrammatical orawkward translations (i.e.
despite what we mightcall non-catastrophic errors).4 Testing and EvaluationIn addition to our own laboratory tests, the sys-tem was evaluated by MITRE as part of theDARPA program.
There were two parts to theMITRE evaluations, a "live" part, designed pri-marily to evaluate the overall task-oriented effec-tiveness of the systems, and a "canned" part,designed primarily to evaluate individual compo-nents of the systems.The live evaluation consisted of six medicalprofessionals (doctors, corpsmen and physician?sassistants from the Naval Medical Center at Quan-tico, and a nurse from a civilian institution) con-ducting unrehearsed "focused history and physicalexam" style interactions with Farsi speakers play-ing the role of patients, where the English-speakingdoctor and the Farsi-speaking patient communi-cated by means of the Transonics system.
Sincethe cases were common enough to be within therealm of general internal medicine, there was noattempt to align ailments with medical specializa-tions among the medical professionals.MITRE endeavored to find primarily monolin-gual Farsi speakers to play the role of patient, so asto provide a true test of the system to enable com-munication between people who would otherwisehave no way to communicate.
This goal was onlypartially realized, since one of the two Farsi patientrole-players was partially competent in English.2The Farsi-speaking role-players were trained by amedical education specialist in how to simulatesymptoms of someone with particular injuries orillnesses.
Each Farsi-speaking patient role-playerreceived approximately 30 minutes of training forany given illness or injury.
The approach wassimilar to that used in training standardized pa-tients, mentioned above (footnote 1) in connectionwith generation of the dialogue corpus.MITRE established a number of their own met-rics for measuring the success of the systems, aswell as using previously established metrics.
Afull discussion of these metrics and the results ob-tained for the Transonics system is beyond thescope of this paper, though we will note that one ofthe most important of these was task-completion.There were 5 significant facts (5 distinct facts foreach of 12 different scenarios) that the medicalprofessional should have discovered in the processof interviewing/examining each Farsi patient.
TheUSC/HRL system averaged 3 out of the 5 facts,which was a slightly above-average score amongthe 4 systems evaluated.
A "significant fact" con-sisted of determining a fact which was critical fordiagnosis, such as the fact that the patient had beeninjured in a fall down a stairway, the fact that thepatient was experiencing blurred vision, and so on.Significant facts did not include items such as apatient's age or marital status.3We report on thismeasure in that it is perhaps the single most im-portant component in the assessment, in our opin-ion, in that it is an indication of many aspects ofthe system, including both directions of the trans-lation system.
That is, the doctor will very likelyconclude correct findings only if his/her question istranslated correctly to the patient, and also if thepatient's answer is translated correctly for the doc-tor.
In a true medical exam, the doctor may have2There were additional difficulties encountered as well, hav-ing to do with one of the role-players not adequately graspingthe goal of role-playing.
This experience highlighted themany challenges inherent in simulating domain-specificspontaneous dialogue.3Unfortunately, there was no baseline evaluation this could becompared to,  such as assessing whether any of the criticalfacts could be determined without the use of the system at all.91other means of determining some critical factseven in the absence of verbal communication, butin the role-playing scenario described, this is veryunlikely.
Although this measure is admittedlycoarse-grained, it simultaneously shows, in a crudesense, that the USC/HRL system compared fa-vorably against the other 3 systems in the evalua-tion, and also that there is still significant room forimprovement in the state of the art.As noted, MITRE devised a component evalua-tion process also consisting of running 5 scripteddialogs through the systems and then measuringASR and MT performance.
The two primarycomponent measures were a version of BLEU forthe MT component (modified slightly to handle themuch shorter sentences typical of this kind of dia-log) and a standard Word-Error Rate for the ASRoutput.
These scores are shown below.Table 1:  Farsi BLEU ScoresIBM BLEUASRIBM BLEUTEXTEnglish to Farsi0.2664 0.3059Farsi  to English 0.2402 0.2935The reason for the two different BLEU scores isthat one was calculated based on the ASR compo-nent output being translated to the other language,while the other was calculated from human tran-scribed text being translated to the other language.Table 2:  HRL/USC WER for Farsi and EnglishEnglish FarsiWER 11.5% 13.4%5 ConclusionIn this paper we have given an overview of thedesign, implementation and evaluation of the Tran-sonics speech-to-speech translation system for nar-row domain two-way translation.
Although thereare still many significant hurdles to be overcomebefore this kind of technology can be called trulyrobust, with appropriate training and two coopera-tive interlocutors, we can now see some degree ofgenuine communication being enabled.
And this isvery encouraging indeed.6 AcknowledgementsThis work was supported primarily by the DARPACAST/Babylon program, contract N66001-02-C-6023.ReferencesR.
Belvin, W. May, S. Narayanan, P. Georgiou, S. Gan-javi.
2004.
Creation of a Doctor-Patient DialogueCorpus Using Standardized Patients.
In Proceedings ofthe Language Resources and Evaluation Conference(LREC), Lisbon, Portugal.S.
Ganjavi, P. G. Georgiou, and S. Narayanan.
2003.Ascii based transcription schemes for languages withthe Arabic script: The case of Persian.
In Proc.
IEEEASRU,  St. Thomas, U.S. Virgin Islands.S.
Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-laie, S. Ganjavi, P. Georgiou, C. Hein, S. Kadambe,K.
Knight, D. Marcu, H. Neely, N. Srinivasamurthy,D.
Traum and D. Wang.
2003.
Transonics: A speechto speech system for English-Persian Interactions,Proc.
IEEE ASRU,  St. Thomas, U.S. Virgin Islands.S.
Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-laie, S. Gandhe, S. Ganjavi, P. G. Georgiou, C. M.Hein, S. Kadambe, K. Knight, D. Marcu, H. E.Neely, N. Srinivasamurthy, D. Traum, and D. Wang.2004.
The Transonics Spoken Dialogue Translator:An aid for English-Persian Doctor-Patient interviews,in Working Notes of the AAAI Fall symposium onDialogue Systems for Health Communication, pp 97--103.N.
Srinivasamurthy, and S. Narayanan.
2003.
Languageadaptive Persian speech recognition.
In proceedingsof Eurospeech 2003.92
