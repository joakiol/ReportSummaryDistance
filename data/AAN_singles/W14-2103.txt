Proceedings of the First Workshop on Argumentation Mining, pages 19?23,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsAn automated method to build a corpus of rhetorically-classifiedsentences in biomedical textsHospice HoungboDepartment of Computer ScienceThe University of Western Ontariohhoungbo@uwo.caRobert E. MercerDepartment of Computer ScienceThe University of Western Ontariomercer@csd.uwo.caAbstractThe rhetorical classification of sentencesin biomedical texts is an important taskin the recognition of the components ofa scientific argument.
Generating super-vised machine learned models to do thisrecognition requires corpora annotated forthe rhetorical categories Introduction (orBackground), Method, Result, Discus-sion (or Conclusion).
Currently, a few,small annotated corpora exist.
We usea straightforward feature of co-referringtext using the word ?this?
to build a self-annotating corpus extracted from a largebiomedical research paper dataset.
Thecorpus is annotated for all of the rhetori-cal categories except Introduction with-out involving domain experts.
In a 10-foldcross-validation, we report an overall F-score of 97% with Na?
?ve Bayes and 98.7%with SVM, far above those previously re-ported.1 IntroductionSentence classification is an important pre-processing task in the recognition of the compo-nents of an argument in scientific text.
For in-stance, sentences that are deemed as conclusionsof a research paper can be used to validate or re-fute an hypothesis presented in background or in-troduction sentences in that paper.
Therefore, inorder to understand the argumentation flow in sci-entific publications, we need to understand howdifferent sentences fit into the complete rhetoricalstructure of scientific writing.To perform sentence classification using su-pervised machine learning techniques requires alarge training corpus annotated with the appropri-ate classification tags.
In the biomedical domain,some corpora already exist, but many of these cor-pora are still limited and cannot be generalized toevery context.
The task of sentence classificationin various rhetorical categories is often performedon ad hoc corpora derived from a limited num-ber of papers that don?t necessarily represent allof the text in the biomedical domain.
For instance,the corpus used by Agarwal and Yu (2009) for thetask of sentence classification into the IMRaD cat-egories, is composed of only 1131 sentences.In this study, we hypothesize that using a simplelinguistically-based heuristic, we can build a sig-nificantly larger corpus comprising sentences thatbelong to specific categories of the IMRaD rhetor-ical structure of the biomedical research text, thatwill not need domain experts to annotate them,and will represent a wider range of publications inthe biomedical literature.
We have collected pairsof sequential sentences where the second sentencebegins with ?This method.
.
.
?, ?This result.
.
.
?,?This conclusion.
.
.
?.
Our hypothesis is that thefirst sentence in each pair is a sentence that can becategorized respectively as Method, Result andConclusion sentences.We have a number of motivations for this work.First, sentences are the basis for most text min-ing and extraction systems.
The second motiva-tion is that biomedical texts are the reports of sci-entific investigations and their discourse structuresshould represent the scientific method that drivesthese investigations.
The third and last motivationis that categorizing sentences into the IMRaD cat-egories can help in the task of extracting knowl-edge discovery elements from scientific papers.The contribution of our work is twofold.
First,we have used a simple linguistic filter to automati-cally select thousands of sentences that have a highprobability of being correctly categorized in theIMRAD scheme, and second, we have used ma-chine learning techniques to classify sentences inorder to validate our hypothesis that this linguis-tic filter works.
The rest of this paper is organizedas follows.
The next section reviews some related19work.
In Section 3, a detailed methodology of cor-pus construction and sentence classification tech-niques is presented.
In Section 4, the results aredescribed.2 Related WorkThe classification of sentences from scientific re-search papers into different categories has been in-vestigated in previous works.
Many schemes havebeen used and currently no standard classificationscheme has been agreed upon.
Teufel et al.
(1999)use a classification scheme termed ArgumentativeZoning (AZ) to model the rhetorical and argumen-tative aspects of scientific writing in order to easilydetect the different claims that are mentioned in ascientific research paper.
AZ has been modifiedfor the annotation of biology articles (Yoko et al.,2006) and chemistry articles (Teufel et al., 2009).Scientific discourse has also been studied interms of speculation and modality by Kilicogluand Bergler (2008) and Medlock and Briscoe(2007).
Also, Shatkay et al.
(2008) and Wilburet al.
(2006) have proposed an annotation schemethat categorizes sentences according to various di-mensions such as focus, polarity and certainty.Many annotation units have also be proposed inprevious studies.
Sentence level annotation is usedin Teufel et al.
(1999) whereas de Waard et al.
(2009) used a multi-dimensional scheme for theannotation of biomedical events (bio-events) intexts.Liakata et al.
(2012) attempt to classify sen-tences into the Core Scientific Concept (CoreSC)scheme.
This classification scheme consists of anumber of categories distributed into hierarchicallayers.
The first layer consists of 11 categories,which describe the main components of a sci-entific investigation, the second layer consists ofproperties of those categories (e.g.
Novelty, Ad-vantage), and the third layer provides identifiersthat link together instances of the same concept.Some other recent works have focussed on theclassification of sentences from biomedical arti-cles into the IMRaD (Introduction, Methods, Re-search, and, Discussion) categories.
Agarwal andYu (2009) use a corpus of 1131 sentences to clas-sify sentences from biomedical research papersinto these categories.
In this study, sentence levelannotation is used and multinomial Na?
?ve Bayesmachine learning has proved to perform betterthan simple Na?
?ve Bayes.
The authors report anoverall F-measure score of 91.55% with a mu-tual information feature selection technique.
Thepresent study provides an alternative way to builda larger IMRaD annotated corpus, which com-bined with existing corpora achieves a better per-formance.Methods for training supervised machine-learning systems on non-annotated data, were pre-sented in (Yu and Hatzivassiloglou, 2003), whichassumed that in a full-text, IMRaD-structured ar-ticle, the majority of sentences in each sectionwill be classified into their respective IMRaD cate-gory.
Also, Agarwal and Yu (2009) used the samemethod to build a baseline classifier that achievedabout 77.81% accuracy on their corpus.3 Methodology3.1 Constructing a self-annotating corpusfrom a biomedical datasetThe goal of this study is to show that the classi-fication of sentences from scientific research pa-pers to match the IMRaD rhetorical structure withsupervised machine learning can be enhanced us-ing a self-annotating corpus.
The first task con-sists of the curation of a corpus that contains sen-tences representative of the defined categorizationscheme.
We have chosen to build the corpus by ex-tracting sentences from a large repository of full-text scientific research papers, a publicly availablefull-text subset of the PubMed repository.Since most demonstrative pronouns are co-referential, a sentence that begins with the demon-strative noun phrase ?This method.
.
.
?
or ?This re-sult.
.
.
?
or ?This conclusion.
.
.
?
is co-referentialand its antecedents are likely to be found in previ-ous sentences.
Torii and Vijay-Shanker (2005) re-ported that nearly all antecedents of such demon-strative phrases can be found within two sen-tences.
As well, Hunston (2008) reported thatinterpreting recurring phrases in a large corpusenables us to capture the consistency in mean-ing as well as the role of specific words in suchphrases.
So, the recurring semantic sequences?This method.
.
.
?
or ?This result.
.
.
?
or ?Thisconclusion.
.
.
?
in the Pubmed corpus can helpus to capture valuable information in the contextof their usage.
A similar technique was used in(Houngbo and Mercer, 2012), to build a corpusfor method mention extraction from biomedicalresearch papers.Our assumption is that a sentence that appears20in the co-referential context of the co-referencingphrase ?This method.
.
.
?, will likely talk abouta methodology used in a research experiment oranalysis.
Similarly, a sentence that starts with theexpression ?This result.
.
.
?
is likely to refer toa result.
And, similarly, for sentences that be-gin with ?This conclusion.
.
.
?.
The Introduction(Background) rhetorical category does not have asimilar co-referential structure.
We have chosen toonly consider the immediately preceding sentenceto the ?This?
referencing sentence.
Some exam-ples are shown below.Category # of Sentences ProportionMethod 3163 31.9%Result 6288 62.7%Conclusion 534 5.4%Total 9985 100%Table 1: Initial Self-annotated Corpus Statistics1.
We have developed a DNA microarray-basedmethod for measuring transcript length .
.
.This method, called the Virtual Northern, isa complementary approach .
.
.2.
Interestingly, Drice the downstream caspaseactivated .
.
.
was not affected by inhibition ofDronc and Dredd.This result, .
.
.
suggests that some othermechanism activates Drice.3.
We obtained a long-range PCR product fromthe latter interval, that appeared to encom-pass the breakpoint on chromosome 2 .
.
.This conclusion, however , was regardedwith caution , since .
.
.Table 1 shows the number of sentences per cate-gory in this initial self-annotated corpus.3.1.1 Feature ExtractionWe have used the set of features extracted fromthe Agarwal and Yu (2009) IMRaD corpus.
Thereason for this choice is to be able to validate ourclaim against this previous work.
Agarwal andYu (2009) experimented with mutual informationand chi-squared for feature selection and obtainedtheir best performance using the top 2500 featurescomprised of a combination of individual wordsas well as bigrams and trigrams.
A feature thatindicates the presence of a citation in a sentenceis also used as it can be an important feature for(a) Classification with Multinomial Na?
?ve Bayes.Class Precision Recall F-MeasureMethod 0.923 0.661 0.77Result 0.627 0.813 0.708Conclusion 0.68 0.821 0.744Average 0.779 0.74 0.744(b) Classification with Support Vector MachineClass Precision Recall F-MeasureMethod 0.818 0.521 0.636Result 0.511 0.908 0.654Conclusion 0.923 0.226 0.364Average 0.72 0.621 0.604Table 2: Precision, Recall, F-measure : Classifier trainedwith the initial self-annotated corpus and tested on a reducedAgarwal and Yu (2009) corpus (Method, Result, Conclusion)distinguishing some categories; for example, ci-tations are more frequently used in Introductionthan in Results.
All numbers were replaced by aunique symbol #NuMBeR.
Stop words were notremoved since certain stop words are also morelikely to be associated with certain IMRaD cate-gories.
Words that refer to a figure or table are notremoved, since such references are more likely tooccur in sentences indicating the outcome of thestudy.
We also used verb tense features as somecategories may be associated with the presence ofthe present tense or the past tense in the sentence.We used the Stanford parser (Klein and Manning,2003) to identify these tenses.3.1.2 Self-annotationIn our first experiment we trained a model on theinitial self-annotated corpus discussed above andtested the model on the Agarwal and Yu (2009)corpus.
Table 2 shows F-measures that are belowthe baseline classifier levels.
We suggest that thereare two causes: many of the important n-gramsin the larger corpus are not present in the 2500n-gram feature set; and there is noise in the ini-tial self-annotated corpus.
To reduce the noise inthe initial self-annotated corpus and to maintainthe 2500 n-gram feature set we pruned our ini-tial self-annotated corpus using a semi-supervisedlearning step using an initial model based on theAgarwal and Yu feature set and learned from theAgarwal and Yu corpus.
We describe below thesemi-supervised method to do this pruning of theinitial self-annotated corpus.21Our method for categorizing sentences into theIMRaD categories does not work for the Intro-duction category, so from the Agarwal and Yu(2009) IMRaD corpus, we have extracted in-stances belonging to the Method, Result andConclusion categories and have used this corpusto build a model with a supervised multinomialNa?
?ve Bayes method.
This model is then usedto classify sentences in the initial self-annotatedcorpus.
When the model matches the initial self-annotated corpus category with a confidence levelgreater than 98%, this instance is added to what wewill now call the model-validated self-annotatedcorpus.
The composition of this model-validatedcorpus is presented in Table 3.Category # of Sentences ProportionMethod 878 23.6%Result 2399 64.5%Conclusion 443 11.9%Total 3719 100%Table 3: Model-validated Self-annotated Corpus Statistics3.2 Automatic text classificationFor all supervised learning, we have used twopopular supervised machine-learning algorithms,multinomial Na?
?ve Bayes (NB) and Support Vec-tor Machine (SVM), provided by the open-sourceJava-based machine-learning library Weka 3.7(Witten and Frank, 2005).4 Results and DiscussionIn the first classification task a classifier is trainedwith the model-validated self-annotated corpus us-ing 10-fold cross-validation.
The model achievesan F-measure score of 97% with NB and 98.7%with SVM.
See Table 4.
The average F-measurethat Agarwal and Yu (2009) report for their 10-foldcross-validation (which includes Introduction) is91.55.
The category F-measures that Agarwal andYu (2009) report for their 10-fold cross-validationwith the features that we use are: Method: 91.4(95.04) (their best scores, in parentheses, requireinclusion of the IMRaD section as a feature), Re-sult: 88.3 (92.24), and Conclusion: 69.03 (73.77).In the last classification task, a classifier istrained with the model-validated self-annotatedcorpus and tested on the Agarwal and Yu (2009)corpus.
The F-measures in Table 5 are a substan-tial improvement over those in Table 2.
(a) Classification with Multinomial Na?
?ve Bayes.Class Precision Recall F-MeasureMethod 0.981 0.957 0.969Result 0.966 0.992 0.979Conclusion 0.98 0.885 0.93Average 0.971 0.971 0.971(b) Classification with Support Vector MachineClass Precision Recall F-MeasureMethod 0.986 0.984 0.985Result 0.988 0.995 0.992Conclusion 0.986 0.95 0.968Average 0.987 0.987 0.987Table 4: Precision, Recall, F-measure : Classifier trainedwith the model-validated self-annotated corpus (Method, Re-sult, Conclusion) using 10-fold cross-validation(a) Classification with Multinomial Na?
?ve Bayes.Class Precision Recall F-MeasureMethod 0.937 0.806 0.866Result 0.763 0.873 0.814Conclusion 0.836 0.911 0.872Average 0.858 0.847 0.848(b) Classification with Support Vector MachineClass Precision Recall F-MeasureMethod 0.893 0.824 0.857Result 0.763 0.85 0.804Conclusion 0.835 0.811 0.823Average 0.837 0.832 0.833Table 5: Precision, Recall, F-measure : Classifier trainedwith the model-validated self-annotated corpus and tested ona reduced Agarwal and Yu (2009) corpus (Method, Result,Conclusion)Sentence classification is important in determin-ing the different components of argumentation.We have suggested a method to annotate sentencesfrom scientific research papers into their IMRaDcategories, excluding Introduction.
Our resultsshow that it is possible to extract a large self-annotated corpus automatically from a large repos-itory of scientific research papers that generatesvery good supervised machine learned models.AcknowledgmentsThis work was partially funded through a Natu-ral Sciences and Engineering Research Council ofCanada (NSERC) Discovery Grant to R. Mercer.22ReferencesShashank Agarwal and Hong Yu.
2009.
Automaticallyclassifying sentences in full-text biomedical articlesinto introduction, methods, results and discussion.Bioinformatics, 25(23):3174?3180.Anita de Waard, Paul Buitelaar, and Thomas Eigner.2009.
Identifying the epistemic value of discoursesegments in biology texts.
In Proceedings of theEighth International Conference on ComputationalSemantics, IWCS-8 ?09, pages 351?354.
Associa-tion for Computational Linguistics.Hospice Houngbo and Robert E. Mercer.
2012.Method mention extraction from scientific researchpapers.
In Proceedings of COLING 2012, pages1211?1222, Mumbai, India.Susan Hunston.
2008.
Starting with the small words.Patterns, lexis and semantic sequences.
Interna-tional Journal of Corpus Linguistics, 13:271?295.Halil Kilicoglu and Sabine Bergler.
2008.
Recogniz-ing speculative language in biomedical research ar-ticles: A linguistically motivated perspective.
BMCBioinformatics, 9(S-11):S10.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics, ACL ?03, pages 423?430.
Asso-ciation for Computational Linguistics.Maria Liakata, Shyamasree Saha, Simon Dob-nik, Colin R. Batchelor, and Dietrich Rebholz-Schuhmann.
2012.
Automatic recognition of con-ceptualization zones in scientific articles and two lifescience applications.
Bioinformatics, 28(7):991?1000.Ben Medlock and Ted Briscoe.
2007.
Weakly super-vised learning for hedge classification in scientificliterature.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,ACL ?07, pages 992?999.
Association for Computa-tional Linguistics.Hagit Shatkay, Fengxia Pan, Andrey Rzhetsky, andW.
John Wilbur.
2008.
Multi-dimensional classifi-cation of biomedical text: Toward automated, prac-tical provision of high-utility text to diverse users.Bioinformatics, 24(18):2086?2093.Simone Teufel, Jean Carletta, and Marc Moens.
1999.An annotation scheme for discourse-level argumen-tation in research articles.
In Proceedings of theNinth Conference of the European Chapter of theAssociation for Computational Linguistics, pages110?117.
Association for Computational Linguis-tics.Simone Teufel, Advaith Siddharthan, and Colin Batch-elor.
2009.
Towards discipline-independent ar-gumentative zoning: Evidence from chemistry andcomputational linguistics.
In Proceedings of the2009 Conference on Empirical Methods in Natu-ral Language Processing: Volume 3, EMNLP ?09,pages 1493?1502.
Association for ComputationalLinguistics.Manabu Torii and K. Vijay-Shanker.
2005.
Anaphoraresolution of demonstrative noun phrases in Medlineabstracts.
In Proceedings of PACLING 2005, pages332?339.W.
John Wilbur, Andrey Rzhetsky, and Hagit Shatkay.2006.
New directions in biomedical text annota-tion: definitions, guidelines and corpus construction.BMC Bioinformatics, 7:356.Ian H. Witten and Eibe Frank.
2005.
Data Mining:Practical Machine Learning Tools and Techniques.Morgan Kaufmann Series in Data Management Sys-tems.
Morgan Kaufmann Publishers Inc., San Fran-cisco, CA, USA, 2nd edition.Mizuta Yoko, Anna Korhonen, Tony Mullen, and NigelCollier.
2006.
Zone analysis in biology articles as abasis for information extraction.
International Jour-nal of Medical Informatics, 75:468?487.Hong Yu and Vasileios Hatzivassiloglou.
2003.
To-wards answering opinion questions: Separating factsfrom opinions and identifying the polarity of opin-ion sentences.
In Proceedings of the 2003 Confer-ence on Empirical Methods in Natural LanguageProcessing, EMNLP ?03, pages 129?136.
Associa-tion for Computational Linguistics.23
