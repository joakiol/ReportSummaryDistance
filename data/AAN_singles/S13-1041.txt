Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 286?295, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsUsing the text to evaluate short answers for reading comprehension exercisesAndrea Horbach, Alexis Palmer and Manfred PinkalDepartment of Computational Linguistics, Saarland University, Saarbru?cken, Germany(andrea|apalmer|pinkal)@coli.uni-saarland.deAbstractShort answer questions for reading compre-hension are a common task in foreign lan-guage learning.
Automatic short answer scor-ing is the task of automatically assessing thesemantic content of a student?s answer, mark-ing it e.g.
as correct or incorrect.
While pre-vious approaches mainly focused on compar-ing a learner answer to some reference an-swer provided by the teacher, we explore theuse of the underlying reading texts as addi-tional evidence for the classification.
First, weconduct a corpus study targeting the links be-tween sentences in reading texts for learners ofGerman and answers to reading comprehen-sion questions based on those texts.
Second,we use the reading text directly for classifi-cation, considering three different models: ananswer-based classifier extended with textualfeatures, a simple text-based classifier, and amodel that combines the two according to con-fidence of the text-based classification.
Themost promising approach is the first one, re-sults for which show that textual features im-prove classification accuracy.
While the othertwo models do not improve classification ac-curacy, they do investigate the role of the textand suggest possibilities for developing auto-matic answer scoring systems with less super-vision needed from instructors.1 IntroductionReading comprehension exercises are a commonmeans of assessment for language teaching: studentsread a text in the language they are learning and arethen asked to answer questions about the text.
Thetypes of questions asked of the learner may vary intheir scope and in the type of answers they are de-signed to elicit; in this work we focus on ?short an-swer?
responses, which are generally in the range of1?3 sentences.The nature of the reading comprehension task isthat the student is asked to show that he or she hasunderstood the text at hand.
Questions focus on oneor more pieces of information from the text, and cor-rect responses should contain the relevant semanticcontent.
In the language learning context, responsesclassified as correct might still contain grammaticalor spelling errors; the focus lies on the content ratherthan the form of the learner answer.Automatic scoring of short answer responses toreading comprehension questions is in essence a tex-tual entailment task, with the additional complica-tion that, in order to answer a question correctly, thelearner must have identified the right portion of thetext.
It isn?t enough that a student answer is en-tailed by some part of the reading text; it must beentailed by the part of the text which is responsiveto the question under discussion.Previous approaches to automatic short answerscoring have seldom considered the reading text it-self, instead comparing student answers to target an-swers supplied by instructors; we will refer to theseas answer-based models.
In this paper we explorethe role of the text for short answer scoring, evalu-ating several models for considering the text in au-tomatic scoring, and presenting results of an anno-tation study regarding the semantic links betweenreading texts and answers to reading comprehensionquestions.286TEXT: SCHLOSS PILLNITZThis palace, which lies in the east of Dresden, is to me the most beautiful palace in the Dresden area.
(.
.
. )
One specialattraction in the park is the camellia tree.
In 1992, the camellia, which is more than 230 years old and 8.90 meters tall, got anew, moveable home, in which temperature, ventilation, humidity, and shade are controlled by a climate regulation computer.In the warm seasons, the house is rolled away from the tree.
During the Blossom Time, from the middle of February until April,the camellia has tens of thousands of crimson red blossoms.
Every year, a limited number of shoots from the Pillnitz camelliaare sold during the Blossom Time, making it an especially worthwhile time to visit.QUESTION:A friend of yours would like to see the historic camellia tree.
When should he go to Pillnitz, and why exactly at this time?TARGET ANSWERS:?
From the middle of February until April is the Blossom Time.?
In spring the camellia has tens of thousands of crimson red blossoms.LEARNER ANSWERS:?
[correct] He should go from the middle of February until April, because then the historic camellia has tens of thousandsof crimson red blossoms.?
[incorrect] Every year, a limited number of Pillnitz camellia are sold during the Blossom Time.?
[incorrect] All year round against temperature and humidity are controlled by a climate regulation computer.Figure 1: Example of reading text with question and answers (translation by authors)These investigations are done for German lan-guage texts, questions, and answers.
Figure 1 showsa (translated) sample reading text, question, set oftarget answers, and set of learner answers.We show that the use of text-based featuresimproves classification performance over purelyanswer-based models.
We also show that a very sim-ple text-based classifier, while it does not achievethe same performance as the answer-based classifier,does reach an accuracy of 76% for binary classifica-tion (correct/incorrect) of student answers.
The im-plication of this for automatic scoring is that reason-able results may be achievable with much less efforton the part of instructors; namely, a classifier trainedon the supervision provided by marking the regionof a text relevant to a given question performs rea-sonably well, though not as well as one trained onfull target answers.The paper proceeds as follows: in Section 2 wediscuss the task and related approaches.
In Sec-tion 3, we describe our baseline model and the dataset we use.
In Section 4 and Section 5 we discussour text-based models and present experiments andresults.2 Approaches to short answer scoringIn short answer scoring (SAS) the task is to auto-matically assign labels to individual learner answers.Those labels can either be binary, a value on somescale of points or grades, or a more fine-grained di-agnosis.
For example, one fine-grained set of labels(Bailey, 2008) classifies answers as (among others)correct, as missing a necessary concept or concepts,containing extra content, or as failing to answer thequestion.
Our present study is restricted to binaryclassification.Previous work on SAS, including early systemslike (Leacock and Chodorow, 2003; Pulman andSukkarieh, 2005; Sukkarieh and Pulman, 2005) isof course not only in the domain of foreign lan-guage learning.
For example, Mohler et al(2011)and Mohler and Mihalcea (2009) use semantic graphalignments and semantic similarity measures to as-sess student answers to computer science questions,comparing them to sample solutions provided by ateacher.
Accordingly, not all SAS settings includereading or other reference texts; many involve onlyquestions, target answers, and learner answers.
Ourapproach is relevant for scenarios in which some sort287of reference text is available.The work we present here is strongly based onapproaches towards SAS by Meurers and colleagues(Bailey and Meurers, 2008; Meurers et al 2011a;Meurers et al 2011b; Ziai et al 2012).
Specifically,the sentence alignment model described in Section 3(and again discussed in Section 4) is modeled afterthe one used by Meurers et alto align target answersand student answers.Rather than using answers provided by instruc-tors, Nielsen et al(2008) represent target answers toscience questions as a set of hand-annotated facets,i.e.
important aspects of the answer, typically repre-sented by a pair of words and the relation that con-nects them.
Student answers, and consequently stu-dents?
understanding of target science concepts, arethen assessed by determining whether the relevantfacets are addressed by the learner answers.Evaluating short answers on the basis of associ-ated reading texts, as we do here, is a task relatedto textual entailment.
In the context of tutoring sys-tems, Bethard et al(2012) identify students?
mis-conceptions of science concepts in essay writing us-ing textual entailment techniques.
They align stu-dents?
writings to extracted science concepts in or-der to identify misconceptions, using a similar ap-proach to identify the correct underlying concept.An excellent and more detailed overview of re-lated work can be found in Ziai et al(2012).To our knowledge, there is no previous work thatuses reading texts as evidence for short answer scor-ing in the context of foreign language learning.3 Answer-based modelsIn order to compare to previous work, we first im-plement an alignment-based model following thatproposed in (Meurers et al 2011b).
We refer tothis class of models as answer-based because theyfunction by aligning learner answers to instructor-supplied target answers along several different di-mensions, discussed below.
Answers are then clas-sified as correct or incorrect on the basis of featuresderived from these alignments.Wherever possible/practical, we directly re-implement the Meurers model for German data.In this section we describe relevant aspects of theMeurers model, along with modifications and exten-sions in our implementation of that model.1PreprocessingWe preprocess all material (learner answers, targetanswers, questions and reading texts) using stan-dard NLP tools for sentence splitting and tokeniza-tion (both OpenNLP2), POS tagging and stemming(both Treetagger (Schmid, 1994)), NP chunking(OpenNLP), and dependency parsing (Zurich Parser(Sennrich et al 2009)).
We use an NE Tagger(Faruqui and Pado?, 2010) to annotate named enti-ties.
Synonyms and semantic types are extractedfrom GermaNet (Hamp and Feldweg, 1997).For keywords, which serve to give more emphasisto content words in the target answer, we extract allnouns from the target answer.Given that we are dealing with learner language,but do not want to penalize answers for typicallearner errors, spellchecking (and subsequent cor-rection of spelling errors) is especially important forthis task.
Our approach is as follows: we first iden-tify all words from the learner answers that are notaccepted by a German spellchecker (aspell3).
Wethen check for each word whether the word never-theless occurs in the target answer, question or read-ing text.
If so, we accept it as correct.
Otherwise, wetry to identify (using Levenshtein distance) whichword from the target answer, question, or readingtext is most likely to be the form intended by thestudent.Prior to alignment, we remove from the answerall punctuation, stopwords (restricted to determinersand auxiliaries), and material present in the question.AlignmentThe alignment process in short answer scoring ap-proximates determination of semantic equivalencebetween target answer and learner answer.
Dur-ing alignment, we identify matches between an-swer pairs on a number of linguistic levels: tokens,chunks, and dependency triples.On the token level, we consider a number of dif-ferent metrics for identity between tokens, with each1Some extensions were made in order to bring performanceof our re-implementation closer to the figures reported in previ-ous work.2http://opennlp.apache.org/index.html3http://aspell.net/288metric associated with a certain alignment weight.After weights have been determined for all possibletoken pairs, the best applicable weight is used as in-put for a traditional marriage alignment algorithm(Gale and Shapley, 1962).We use the following types of identity (id),weighted in the following order:token id > lemma id >spelling id > synonym & NE id >similarity id>NE type, semantic type & POS idFor synonym identity, we take a broad notion ofsynonymy, extracting (from GermaNet) as potentialsynonyms all words which are at most two levels (ineither direction) away from the target word.
Simi-larity identity is defined as two words having a Ger-maNet path relatedness above some threshhold.
Inorder to have semantic type identity, two words musthave a common GermaNet hypernym (from a pre-determined set of relevant hypernyms).
Only someclosed-class words are eligible for POS identity.
Wetreat e.g.
all types of determiners as POS identical.Unlike, for example, alignment in machine trans-lation, in which every token pair is considered a can-didate for alignment, under the Meurers model onlycandidates with at least one type of token identity areavailable for alignment.
This aims to prevent com-pletely unrelated word pairs from being consideredfor alignment.In order to favor alignment of content words overalignment of function words, and in departure fromthe Meurers model, we use a content word multiplierfor alignment weights.Chunks can only be aligned if at least one pairof tokens within the respective chunks has beenaligned, and the percentage of aligned tokens be-tween learner and target answer chunks is used asinput for the alignment process.
Dependency triplepairs are aligned when they share dependency rela-tion, head lemma, and dependent lemma.Features and classifierAfter answers have been aligned, the following fea-tures are extracted as input for the classifier: key-word overlap (percentage of aligned keywords), tar-get token overlap (percentage of aligned target to-kens), learner token overlap (percentage of alignedlearner tokens), token match (percentage of tokenalignments that are token identical), lemma match,synonym match, type match, target triple overlap,learner triple overlap, target chunk overlap, learnerchunk overlap, target bigram overlap, learner bigramoverlap, target trigram overlap, learner trigram over-lap, and variety of alignment (number of differenttoken alignment types).The n-gram features are the only new features inour re-implementation of the Meurers model, hopingto capture the influence of linear ordering of alignedtokens.
These features did not in the end improvethe model?s performance.For classification, we use the timbl toolkit (Daele-mans et al 2009) for k-nearest neighbors classi-fication.
We treat all features as numeric valuesand evaluate performance via leave-one-out cross-validation.
Further details appear in Section 5.DataFor all work reported in this paper, we use the Ger-man CREG corpus (Ott et al 2012) of short answersto questions for reading comprehension tasks.
Morespecifically, we use a balanced subset of the CREGcorpus containing a total of 1032 learner answers.This corpus consists of 30 reading texts with an av-erage of 5.9 questions per text.
Each question is as-sociated with one or more target answers, specifiedby a teacher.
For each question in turn there are anaverage of 5.8 learner answers, each manually anno-tated according to both binary and fine-grained la-beling schemes.
When there are several target an-swers for a question, the best target answer for eachlearner answer is indicated.4 Text-based approachPrevious approaches to this task take the instructor-supplied target answer(s) as a sort of supervision;the target answer is meant to indicate the seman-tic content necessary for a correct student answer.Alignment between student answer and target an-swer is then taken as a way of approximating se-mantic equivalence.
The key innovation of the cur-rent study is to incorporate the reading text into theevaluation of student answers.
In this section we de-scribe and evaluate three approaches to incorporat-ing the text.
The aim is to consider the semantic289relationships between target answer, learner answer,and the text itself.4A target answer is in fact just one way of ex-pressing the requisite semantic content.
Teacherswho create such exercises are obviously looking atthe text while creating target answers, and target an-swers are often paraphrases of one or more sentencesof the reading text.
Some learner answers which arescored as incorrect by the answer-based system mayin fact be variant expressions of the same semanticcontent as the target answer.
Due to the nature of thereading comprehension task, in which students areable to view the text while answering questions, wemight expect students to express things in a man-ner similar to the text.
This is especially true forlanguage learners, as they are likely to have a lim-ited range of options both for lexical expression andgrammatical constructions.Along similar lines, one potential source of incor-rect answers is an inability on the part of the stu-dent to correctly identify the portion of the text thatis relevant to the question at hand.
Our hypothesistherefore is that a learner answer which links to thesame portion of the reading text as the target answeris likely to be a correct answer.
Similarly, a learneranswer which closely matches some part of the textthat is not related to the target answer is likely to beincorrect.Our text-based models investigate this hypothesisin several different ways, described in Section 4.2.4.1 Annotation studyThe CREG data includes questions, learner answers,target answers, and reading texts; associations be-tween text and answers are not part of the anno-tations.
We undertook an annotation project in or-der to have gold-standard source sentences for bothlearner and target answers.
This gold-standard isthen used to inform the text-based models describedbelow.After removing a handful of problematic ques-tions and their associated answers, we acquired hu-man annotations for 889 of the 1032 learner an-swers from the balanced subset of the CREG cor-pus, in addition to 294 target answers.
Each answer4In future work we will also consider semantic relationshipsbetween the question and the text.was labeled separately by two (of three) annotators,who were given the reading text and the questionand asked to identify the single best source sentencefrom the text.
Annotators were not told whether anygiven instance was a target or learner answer, norwhether learner answers were correct or incorrect.Although we expected most answers to corre-spond directly to a single text passage (Meurers etal., 2011b), annotators were asked to look for (andannotate appropriately) two different conditions inwhich more than one source sentence may be rele-vant.
We refer to these as the repeated content con-dition and the distributed content condition.In the repeated content condition, the same se-mantic content may be fully represented in morethan one sentence from the original text.
In suchcases, we would expect the text to contain sen-tences that are paraphrases or near-paraphrases ofone another.
The distributed content condition oc-curs when the relevant semantic content spans multi-ple sentences, and some degree of synthesis or eveninference may be required to arrive at the answer.Annotators were instructed to assume that pronounshad been resolved; in other words, a sentence shouldnot be considered necessary semantic content sim-ply because it contains the NP to which a pronoun inanother sentence refers.
For both of these multiple-sentence conditions, annotators were asked to selectone single-best source sentence from among the setand also to mark the alternative source sentences.For 31.2% of the answers annotated, one or moreannotator provided more than one possible sourcesentence.
Upon closer inspection, though, the an-notations for these conditions are not very consis-tent.
In the repeated content condition, there is verylittle agreement between annotators regarding whenthe text contains more than one full-sentence sourcefor the answer.
In the distributed content condition,sometimes annotators disagree on the primary sen-tence, and in many instances, one annotator identi-fied multiple sentences and the other only one.
Dueto these inconsistencies, for the purpose of this studywe decided to treat the multiple-sentence conditionsin an underspecified fashion.
When an annotator hasidentified either of these conditions, we convert theannotations to a single-best sentence and a set of al-ternatives.The annotations were processed to automatically290Answer type agree altagree disagree nolinkLearner answers (all) 70.3% 9.4% 16.9% 3.4%Learner answers (correct) 75.1% 11.7% 12.7% 0.5%Learner answers (incorrect) 65.9% 7.3% 20.7% 6.4%Target 73.1% 8.1% 17.3% 1.4%Table 1: Inter-annotator agreement for linking answers to source sentences in textproduce a gold-standard set of source sentence IDs,indicating the single sentence in the reading text towhich each answer is most closely linked.
We iden-tify four distinct categories with respect to agree-ment between annotators.
Agreement figures appearin Table 1.
** agree: In this case, both annotators linked theanswer to the same source sentence, and that sen-tence is identified as the gold-standard link to theanswer.
** altagree: This category covers two differentsituations in which the two annotators fail to agreeon the single-best sentence.
First, there are cases inwhich the best sentence selected by one annotatoris a member of the set of alternatives indicated bythe other.
Second, in a small number of cases, bothannotators agree on one member of the set of alter-natives.
In other words, the single sentence in theintersection of the sets of sentences identified by thetwo annotators is taken as the gold-standard annota-tion.
There was no (non-agree) case in which thatintersection contained more than one sentence.
** disagree: This category also includes two dif-ferent types of cases.
In the first, one of the twoannotators failed to identify a source sentence tolink with the answer.
In that case, we considerthe annotators to be in disagreement, and for thegold-standard we use the sentence ID provided bythe one responding annotator.
In the second case,the annotators disagree on the single-best sentenceand there is no overlap between indicated alterna-tive sentences.
In those cases, for the gold standardwe choose from the two source sentences that whichappears first in the reading text.5** nolink: For a small number of answers (n=34),5This is a relatively arbitrary decision motivated by the de-sire to have a source sentence associated with as many answersas possible.
Future work may include adjudication of annota-tions to reduce the noise introduced to the gold standard by thiscategory of responses.both annotators found no link to the text.
One ex-ample of such a case is an answer given entirely inEnglish.
For these cases, the gold standard providesno best source sentence.If we consider both altagree and nolink to beforms of agreement, interannotator agreement isabout 74% for both learner and target answers.4.2 Text-based modelsIn this paper we consider two different models forincorporating the reading text into automatic shortanswer scoring.
In the first approach, we employa purely text-based model.
The second combineseither text-based features or the text-based modelwith the answer-based model described in Section 3.Evaluation of all three approaches appears in Sec-tion 5.4.2.1 Simple text-based modelThis model classifies student answers by compar-ing the source sentence most closely associated withthe student answer to that associated with the tar-get answer.
If the two sentences are identical, theanswer is classified as correct, and otherwise as in-correct.We consider both the annotated best sentences(goldlink) and automatically-identified answer-sentence pairs (autolink).
For automatic identifica-tion, we use the alignment model described in Sec-tion 3 to identify the best matching source sentencein the text for both learner and target answers.
Weuse the token alignment process to align a given an-swer with each sentence from its respective readingtext; the best-matching source sentence is that withthe highest alignment weight.
Chunk alignments areused only for correction of token alignments, anddependency alignments are not considered.This model takes an extremely simple approach toanswer classification, and could certainly be refinedand improved.
At the same time, its relatively strong291performance (see Table 3) suggests that the mini-mal level of supervision offered by teachers simplymarking the sentence of a text most relevant to agiven reading comprehension question may be ben-eficial for automatic answer scoring.4.2.2 Combining text-based and answer-basedmodelsIn addition to the purely text-based model, we ex-plore two ways of combining text- and answer-basedmodels.Textual features in the answer-based model Inthe first, we extract four features from the align-ments between answers and source sentences and in-corporate these as additional features in the answer-based model.Features 1, 3, and 4 are each computed in twoversions, using source sentences from either the an-notated gold standard (goldlink), or the alignmentmodel (autolink).1.
SourceAgree This boolean feature is true ifboth learner and target answer link to the samesource sentence, and false otherwise (also if nosource sentence was annotated or automaticallyfound).2.
SourceEntropy For this feature we look at thetwo most-likely source sentences for the learneranswer, as determined by automatic alignmentscores.
We treat the alignment weights asprobabilities, normalizing so that they sum upto one.
We then take the entropy betweenthese two alignment weights as indicative of theconfidence of the automatic alignment for thelearner answer.3.
AgreeEntropy Here we weight the first featureaccording to the second, taking the entropy as aconfidence score for the binary feature.
Specif-ically, we value SourceAgree at 0.5 when thefeature is true, ?0.5 when false, and multiplythis with (1?
entropy).4.
TextAdjacency This feature captures the dis-tance (in number of sentences) between thesource sentence linked to the learner answerand that linked to the target answer.
With thisfeature we aim to capture the tendency of adja-cent passages in a text to exhibit topical coher-ence (Mirkin et al 2010).Classifier combination In the second approach,we combine the output of the answer-based andtext-based classifiers to arrive at a final classifica-tion system, allowing the text-based classifier to pre-dominate in those cases for which it is most con-fident and falling back to the answer-based classi-fier for other cases.
Confidence of the text-basedclassifier is determined based on entropy of the twohighest-scoring alignments between learner answerand source sentence.
The entropy threshhold wasdetermined empirically to 0.5.5 Experiments and resultsThis section discusses experiments on short an-swer scoring (binary classification) for German, inthe context of reading comprehension for languagelearning.
Specifically, we investigate the text-basedmodels described in Section 4.2.
In all cases, fea-tures and parameter settings were tuned on a de-velopment set which was extracted from the largerCREG corpus.
In other words, there is no over-lap between test and development data.
For test-ing, we perform leave-one-out cross-validation onthe slightly-smaller subset of the corpus which wasused for annotation.5.1 Answer-based baselineAs a baseline for our text-based models we takeour implementation of the answer-based model from(Meurers et al 2011b).
As previously mentioned,our implementation diverges from theirs at somepoints, and we do not quite reach the performancereported for their model (accuracy of 84.6% on thebalanced CREG corpus) and are far from reachingthe current state of the art accuracy of 86.3%, as re-ported in Hahn and Meurers (2012).Our answer-based model appears as baseline inTable 2.
During development, the one extension tothe baseline which helped most was the use of ex-tended synonyms.
This variant of the model appearsin the results table with the annotation +syn.292model k=5 k=15 k=30baseline 0.817 0.820 0.822baseline+syn 0.822 0.826 0.825text: goldlink 0.827 0.827 0.829text+syn:goldlink 0.830 0.835* 0.837*text:autolink 0.837* 0.836* 0.825text+syn:autolink 0.844* 0.836* 0.832combined 0.810 0.819 0.816combined+syn 0.817 0.822 0.818Table 2: Classification accuracy for answer-based base-line (baseline), answer-based plus textual features (text),and classifier combination (combined).
+syn indicatesexpanded synonymy features, goldlink indicates identi-fying the source sentences via annotated links, autolinkindicates determining source sentences using the align-ment model, k=number of neighbors.
Results markedwith * are significant compared to the best baselinemodel.
See Section 5.2.1 for details.5.2 Text-based modelsAs described in Section 4.2, we consider three dif-ferent approaches for incorporating the reading textinto answer classification: use of textual featuresin the answer-based model, combination of separateanswer-based and text-based models, and a simpletext-based classifier.5.2.1 Combining text-based and answer-basedmodelsWe explore two ways of combining text- andanswer-based models.Adding textual features to the answer-basedmodelWe evaluate the contribution of the four new text-based features, computed in two variations: withsource sentences as they are identified in the goldstandard (goldlink) and as they are computed usingthe alignment model (autolink).
We add those ad-ditional features to the two answer-based systems:the baseline (text) and the baseline with extendedsynonym set (text+syn).
Results are presented inTable 2.We present results for using the 5, 15, and 30 near-est neighbors for classification, as the influence ofvarious features changes with the number of neigh-bors.
We calculate the significance for the differenceautolink goldlink alt-setAccuracy 0.762 0.722 0.747P correct 0.805 0.781 0.753R correct 0.667 0.585 0.702F correct 0.729 0.668 0.727P incorrect 0.735 0.689 0.742R incorrect 0.851 0.849 0.788F incorrect 0.789 0.761 0.764Table 3: Classification accuracy, precision, recall, and f-score for simple text-based classifier, under three differ-ent conditions.
See Section 5.2.2 for details.between the best baseline model (0.826) and eachmodel which uses textual features, using a resam-pling test (Edgington, 1986).
The results markedwith a * in the Table 2 are significant at p ?
0.01.Although the impact of the textual features isclearly not as big with a stronger baseline model,we still see a clear pattern of improved accuracy.We may expect this difference to increase with moredata and with additional and/or improved text-basedfeatures.Classifier combinationCombining the two classifiers (answer-based andtext-based) according to confidence levels results indecreased performance compared to the baseline.These results appear in Table 2 as combined.5.2.2 Simple text-based classificationWe have seen that textual features improve clas-sification accuracy over the answer-driven model,yet this approach still requires the supervision pro-vided by teacher-supplied target answers.
In ourthird model, we investigate how the system performswithout this degree of supervision, considering howfar we can get by using only the text.The simple text-based classifier, rather than tak-ing a feature-based approach to classification, basesits decision solely on whether or not the learner andtarget answers link to the same source sentence.
Wecompare three different methods for obtaining theselinks.
The first approach (autolink) automaticallylinks each answer to a source sentence from thetext, based on alignments as described in Section 3.The second (goldlink) uses links as provided by thegold standard; in this case, learner answers without293a linked sentence (e.g.
nolink cases) are immedi-ately classified as incorrect.
The third approach (alt-set) exploits that fact that in many cases annotatorsprovided alternate source sentences.
Under this ap-proach, an answer is classified as correct providedthat there is a non-empty intersection between theset of possible source sentences for the learner an-swer and that for the target answer.
For the secondand third approaches, we classify as incorrect thoselearner answers lacking a gold-standard annotationfor the corresponding target answer.In Table 3 we present classification accuracy, pre-cision, recall, and f-score for the three different con-ditions.
Precision, recall, and f-score are reportedseparately for correct and incorrect learner answers.The 76% accuracy reached using the simple text-based classifier suggests that a system which hasteachers supply source sentences instead of targetanswers and then automatically aligns learner an-swers to the text, while nowhere near comparable tothe state-of-the-art supervised system, still achievesa reasonably accurate classification.6 ConclusionIn this paper we have presented the first use ofreading texts for automatic short answer scoring inthe context of foreign language learning.
We showthat, for German, the use of simple text-based fea-tures improves classification accuracy over purelyanswer-based models.
We plan in the future to inves-tigate a wider range of text-based features.
We havealso shown that a simple classification model basedonly on linking answers to source sentences in thetext achieves a reasonable classification accuracy.This finding has the potential to reduce the amountof teacher supervision necessary for authoring shortanswer exercises within automatic answer scoringsystems.
In addition to these findings, we have pre-sented the results of an annotation study linking bothtarget and learner answers to source sentences.In the near-term future we plan to further inves-tigate the role of the reading text for short answerscoring along three lines.
First, we will address thequestion of the best size of text unit for alignment.In many cases, the best answers are linked not toentire sentences but to regions of sentences; in oth-ers, answers may correspond to more than one sen-tence.
Our current approach ignores this issue.
Sec-ond, we are interested in the variety of semantic re-lationships holding between questions, answers andtexts.
Along these lines, we will further investigatethe sets of alternatives provided by annotators, aswell as bringing in notions from work on paraphras-ing and recognizing textual entailment.
Finally, weare interested in moving from simple binary classi-fication to the fine-grained level of diagnosis.AcknowledgmentsWe would like to thank Erik Hahn, David AlejandroPrzybilla and Jonas Sunde for carrying out the an-notations.
We thank the three anonymous reviewersfor their helpful comments.
This work was fundedby the Cluster of Excellence ?Multimodal Comput-ing and Interaction?
of the German Excellence Ini-tiative and partially funded through the INTERREGIV A programme project ALLEGRO (Project No.
:67 SMLW 11137).ReferencesStacey Bailey and Detmar Meurers.
2008.
Diagnosingmeaning errors in short answers to reading comprehen-sion questions.
In Proceedings of the Third Workshopon Innovative Use of NLP for Building EducationalApplications, pages 107?115, Columbus, Ohio, June.Stacey Bailey.
2008.
Content Assessment in IntelligentComputer-Aided Language Learning: Meaning ErrorDiagnosis for English as a Second Language.
Ph.D.thesis, The Ohio State University.Steven Bethard, Haojie Hang, Ifeyinwa Okoye, James H.Martin, Md.
Arafat Sultan, and Tamara Sumner.
2012.Identifying science concepts and student misconcep-tions in an interactive essay writing tutor.
In Proceed-ings of the Seventh Workshop on Building EducationalApplications Using NLP, pages 12?21.Walter Daelemans, Jakub Zavrel, Ko Sloot, and AntalVan Den Bosch.
2009.
TiMBL: Tilburg Memory-Based Learner, version 6.2, Reference Guide.
ILKTechnical Report 09-01.Eugene S Edgington.
1986.
Randomization tests.
Mar-cel Dekker, Inc., New York, NY, USA.Manaal Faruqui and Sebastian Pado?.
2010.
Training andevaluating a German named entity recognizer with se-mantic generalization.
In Proceedings of KONVENS2010, Saarbru?cken, Germany.David Gale and Lloyd S. Shapley.
1962.
College ad-missions and the stability of marriage.
The AmericanMathematical Monthly, 69(1):9?15.294Michael Hahn and Detmar Meurers.
2012.
Evaluat-ing the meaning of answers to reading comprehensionquestions: A semantics-based approach.
In Proceed-ings of the 7th Workshop on Innovative Use of NLPfor Building Educational Applications (BEA7), pages326?336, Montreal, Canada.
Association for Compu-tational Linguistics.Birgit Hamp and Helmut Feldweg.
1997.
Germanet - alexical-semantic net for German.
In In Proceedings ofACL workshop Automatic Information Extraction andBuilding of Lexical Semantic Resources for NLP Ap-plications, pages 9?15.Claudia Leacock and Martin Chodorow.
2003.
C-rater:Automated scoring of short-answer questions.
Com-puters and the Humanities, 37(4):389?405.Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey Bai-ley.
2011a.
Integrating parallel analysis modules toevaluate the meaning of answers to reading compre-hension questions.
Special Issue on Free-text Auto-matic Evaluation.
International Journal of Continu-ing Engineering Education and Life-Long Learning(IJCEELL), 21(4):355?369.Detmar Meurers, Ramon Ziai, Niels Ott, and JaninaKopp.
2011b.
Evaluating answers to reading com-prehension questions in context: Results for Germanand the role of information structure.
In Proceedingsof the TextInfer 2011 Workshop on Textual Entailment,pages 1?9, Edinburgh, Scottland, UK.Shachar Mirkin, Ido Dagan, and Sebastian Pado?.
2010.Assessing the role of discourse references in entail-ment inference.
In ACL.Michael Mohler and Rada Mihalcea.
2009.
Text-to-textsemantic similarity for automatic short answer grad-ing.
In Alex Lascarides, Claire Gardent, and JoakimNivre, editors, EACL, pages 567?575.Michael Mohler, Razvan C. Bunescu, and Rada Mihal-cea.
2011.
Learning to grade short answer questionsusing semantic similarity measures and dependencygraph alignments.
In Dekang Lin, Yuji Matsumoto,and Rada Mihalcea, editors, ACL, pages 752?762.Rodney D. Nielsen, Wayne Ward, and James H. Martin.2008.
Learning to assess low-level conceptual under-standing.
In David Wilson and H. Chad Lane, editors,FLAIRS Conference, pages 427?432.Niels Ott, Ramon Ziai, and Detmar Meurers.
2012.
Cre-ation and analysis of a reading comprehension exer-cise corpus: Towards evaluating meaning in context.In Thomas Schmidt and Kai Wo?rner, editors, Mul-tilingual Corpora and Multilingual Corpus Analysis,Hamburg Studies in Multilingualism (HSM), pages47?69.
Benjamins, Amsterdam.Stephen G. Pulman and Jana Z. Sukkarieh.
2005.
Au-tomatic short answer marking.
In Proceedings of thesecond workshop on Building Educational Applica-tions Using NLP, EdAppsNLP 05, pages 9?16.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of the In-ternational Conference on New Methods in LanguageProcessing, Manchester, United Kingdom.Rico Sennrich, Gerold Schneider, Martin Volk, and Mar-tin Warin.
2009.
A new hybrid dependency parserfor German.
In Christian Chiarcos, Richard Eckartde Castilho, and Manfred Stede, editors, Von der Formzur Bedeutung: Texte automatisch verarbeiten ?
FromForm to Meaning: Processing Texts Automatically.Proceedings of the Biennial GSCL Conference 2009,pages 115?124.
Narr, Tu?bingen.Jana Z. Sukkarieh and Stephen G. Pulman.
2005.
In-formation extraction and machine learning: Auto-marking short free text responses to science questions.In Chee-Kit Looi, Gordon I. McCalla, Bert Bredeweg,and Joost Breuker, editors, AIED, volume 125 of Fron-tiers in Artificial Intelligence and Applications, pages629?637.Ramon Ziai, Niels Ott, and Detmar Meurers.
2012.Short answer assessment: Establishing links betweenresearch strands.
In Proceedings of the 7th Workshopon Innovative Use of NLP for Building EducationalApplications (BEA7), Montreal, Canada.295
