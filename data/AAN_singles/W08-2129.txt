CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 213?217Manchester, August 2008A Puristic Approach for Joint Dependency Parsing and SemanticRole LabelingAlexander VolokhLT-lab, DFKI66123 Saarbr?cken, GermanyAlexander.Volokh@dfki.deG?nter NeumannLT-lab, DFKI66123 Saarbr?cken, Germanyneumann@dfki.deAbstractWe present a puristic approach for com-bining dependency parsing and semanticrole labeling.
In a first step, a data-drivenstrict incremental deterministic parser isused to compute a single syntactic de-pendency structure using a MEM trainedon the syntactic part of the CoNLL 2008training corpus.
In a second step, a cas-cade of MEMs is used to identify predi-cates, and, for each found predicate, toidentify its arguments and their types.
Allthe MEMs used here are trained onlywith labeled data from the CoNLL 2008corpus.
We participated in the closedchallenge, and obtained a labeled macroF1 for WSJ+Brown of 19.93 (20.13 onWSJ only, 18.14 on Brown).
For the syn-tactic dependencies we got similar badresults (WSJ+Brown=16.25, WSJ= 16.22,Brown=16.47), as well as for the seman-tic dependencies (WSJ+Brown=22.36,WSJ=22.86, Brown=17.94).
The currentresults of the experiments suggest thatour risky puristic approach of following astrict incremental parsing approach to-gether with the closed data-driven per-spective of a joined syntactic and seman-tic labeling was actually too optimisticand eventually too puristic.The CoNLL 2008 shared task on joint parsing ofsyntactic and semantic dependencies (cf.
Sur-deanu, 2008) offered to us an opportunity to ini-tiate, implement and test new ideas on large-scale data-driven incremental dependency pars-ing.
The topic and papers of the ACL-2004workshop ?Incremental Parsing: Bringing Engi-?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.neering and Cognition Together?
(accessible athttp://aclweb.org/anthology-new/W/W04/#0300)present a good recent overview into the field ofincremental processing from both an engineeringand cognitive point of view.Our particular interest is the exploration anddevelopment of strict incremental deterministicstrategies as a means for fast data-driven depend-ency parsing of large-scale online natural lan-guage processing.
By strict incremental process-ing we mean, that the parser receives a stream ofwords w1 to wn word by word in left to right or-der, and that the parser only has informationabout the current word wi, and the previouswords w1 to wi-1.1 By deterministic processing wemean that the parser has to decide immediatelyand uniquely whether and how to integrate thenewly observed word wi with the already con-structed (partial) dependency structure withoutthe possibility of revising its decision at laterstages.
The strategy is data-driven in the sensethat the parsing decisions are made on basis of astatistical language model, which is trained onthe syntactic part of the CoNLL 2008 trainingcorpus.
The whole parsing strategy is based onNivre (2007), but modifies it in several ways, seesec.
2 for details.Note that there are other approaches of incre-mental deterministic dependency parsing thatassume that the complete input string of a sen-tence is already given before parsing starts andthat this additional right contextual informationis also used as a feature source for languagemodeling, e.g., Nivre (2007).In light of the CoNLL 2008 shared task, thisactually means that, e.g., part-of-speech taggingand lemmatization has already been performed1Note that in a truly strict incremental processingregime the input to the NLP system is actually astream of signals where even the sentence segmenta-tion is not known in advance.
Since in our currentsystem, the parser receives a sentence as given input,we are less strict as we could be.213for the complete sentence before incrementalparsing starts, so that this richer source of infor-mation is available for defining the feature space.Since, important word-based information espe-cially for a dependency analysis is alreadyknown for the whole sentence before parsingstarts, and actually heavily used during parsing,one might wonder, what the benefit of such aweak incremental parsing approach is comparedto a non-incremental approach.
Since, wethought that such an incremental processing per-spective is a bit too wide (especially when con-sidering the rich input of the CoNLL 2008 sharedtask), we wanted to explore a strict incrementalstrategy.Semantic role labeling is considered as a post-process that is applied on the output of the syn-tactic parser.
Following Hacioglu (2004), weconsider the labeling of semantic roles as a clas-sification problem of dependency relations intoone of several semantic roles.
However, insteadof post-processing a dependency tree firstly intoa sequence of relations, as done by Hacioglu(2004), we apply a cascade of statistical modelson the unmodified dependency tree in order toidentify predicates, and, for each found predicate,to identify its arguments and their types.
All thelanguage models used here are trained only withlabeled data from the CoNLL 2008 corpus; cf.sec.
3 for more details.Both, the syntactic parser and the semanticclassifier are language independent in the sensethat only information contained in the giventraining corpus is used (e.g., PoS tags, depend-ency labels, information about direction etc.
), butno language specific features, e.g., no PropBankframes nor any other external language andknowledge specific sources.The complete system has been designed andimplemented from scratch after the announce-ment of the CoNLL 2008 shared task.
The maingoal of our participation was therefore actuallyon being able to create some initial software im-plementation and baseline experimentations as astarting point for further research in the area ofdata-driven incremental deterministic parsing.In the rest of this brief report, we will describesome more details of the syntactic and semanticcomponent in the next two sections, followed bya description and discussion of the achieved re-sults.1 Syntactic ParsingOur syntactic dependency parser is a variant ofthe incremental non-projective dependencyparser described in Nivre (2007).
Nivres?
parseris incremental in the sense, that although thecomplete list of words of a sentence is known,construction of the dependency tree is performedstrictly from left to right.
It uses Treebank-induced classifiers to deterministically predictthe actions of the parser.
The classifiers aretrained using support vector machines (SVM).
Afurther interesting property of the parser is itscapability to derive (a subset of) non-projectivestructures directly.
The core idea here is to ex-ploit a function permissible(i, j, d) that returnstrue if and only if the dependency links i ?
j andj ?
i have a degree less than or equal to d giventhe dependency graph built so far.
A degree d=0gives strictly projective parsing, while settingd=?
gives unrestricted non-projective parsing; cf.Nivre (2007) for more details.
The goal of thisfunction is to restrict the call of a function link(i,j) which is a nondeterministic operation that addsthe arc i ?
j, the arc j ?
i, or does nothing at all.Thus the smaller the value of d is the fewer linkscan be drawn.The function link(i, j) is directed by a trainedSVM classifier that takes as input the feature rep-resentation of the dependency tree built so farand the (complete) input x = w1, ?, wn and out-puts a decision for choosing exactly one of thethree possible operations.We have modified Nivres algorithm as follows:1.
Instead of using classifiers learned bySVM, we are using classifiers based onMaximum Entropy Models (MEMs), cf.
(Manning and Sch?tze, 1999).22.
Instead of using the complete input x, weonly use the prefix from w1 up to the cur-rent word wi.
In this way, we are able tomodel a stricter incremental processingregime.3.
We are using a subset of feature set de-scribed in Nivre (2007).
3  In particular,we had to discard all features fromNivre?s set that refer to a word right tothe current word in order to retain our2We are using the opennlp.maxent package availablevia http://maxent.sourceforge.net/.3We mean here all features that are explicitly de-scribed in Nivre (2007).
He also mentions the use ofsome additional language specific features, but theyare not further described, and, hence not known to us.214strict incremental behavior.
Additionally,we added the following features:a.
Has j more children in the currentdependency graph compared withthe average number of children ofelement of same POS.b.
Analogously for node ic.
Distance between i and jAlthough some results ?
for example Wang etal.
(2006) ?
suggest that SVMs are actually moresuitable for deterministic parsing strategies thanMEMs, we used MEMs instead of SVM basi-cally for practical reasons: 1) we already hadhands-on experience with MEMs, 2) trainingtime was much faster than SVM, and 3) the theo-retical basis of MEMs should give us enoughflexibility for testing with different sets of fea-tures.Initial experiments applied on the same cor-pora as used by Nivre (2007), soon showed thatour initial prototype is certainly not competitivein its current form.
For example, our best resulton the TIGER Treebank of German (Brants et al,2002) is 53.6% (labeled accuracy), where Nivrereports 85.90%; cf.
Volokh (2008) and sec.
4 formore detailsAnyway, we decided to use it as a basis for theCoNLL 2008 shared task and to combine it witha component for semantic role labeling at least toget some indication of ?what went wrong?.2 Semantic Role LabelingOn the one hand, it is clear that we should expectthat our current version of the strict incrementaldeterministic parsing regime still returns too er-roneous dependency analysis.
On the other hand,we decided to apply semantic role labeling on theparser?s output.
Hence, the focus was set towardsa robust strictly data-driven approach.Semantic role labeling is modeled as a se-quence of classifiers that follow the structure ofpredicates, i.e., firstly candidate predicates areidentified and then the arguments are looked up.Predicate and argument identification bothproceed in two steps: first determine whether aword can be a predicate or argument (or not), andthen, each found predicate (argument) is typed.More precisely, semantic role labeling receivesthe output of the syntactic parser and performsthe following steps in that order:1.
Classify each word as being a predicateor not using a MEM-based classifier.2.
Assign to each predicate its reading.
Cur-rently, this is done on basis of the fre-quency readings as determined from thecorpus (for unknown words, we simplyassign the reading .01 to the lemma if thewhole word was classified as a predicate).3.
For each predicate identified in a sen-tence, classify each word as argument forthis predicate or not using a MEM-basedclassifier.4.
For each argument identified for eachpredicate, assign its semantic role using aMEM-based classifier.For step 1 the following features are used forword wi: 1) word form, 2) word lemma, 3) POS,4) dependency type, 5) number of dependentelements in subtree of wi, 6) POS of parent, 7)dependency type of parent, 8) children or parentof word belong to prepositions, and 9) parent ispredicate.For step 3 the same features are used as in step1, but 5) (for arguments the number of children isnot important) and two additional features areused: 10) left/right of predicate (arguments areoften to the right of its predicate), and 11) dis-tance to predicate (arguments are not far from thepredicate).
Finally, for step 4 the same featuresare used as in step 1, but 5) and 9).3 ExperimentsAs mentioned above, we started the develop-ment of the system from scratch with a verysmall team (actually only one programmer).Therefore we wanted to focus on certain aspects,totally abandoning our claims for achieving de-cent results for the others.
One of our majorgoals was the construction of correct syntactictrees and the recognition of the predicate-argument structure - a subtask which mainly cor-responds to the unlabeled accuracy.
For that rea-son we reduced the scale of our experimentsconcerning such steps as dependency relationlabeling, determining the correct reading for thepredicates or the proper type of the arguments.Unfortunately only the labeled accuracy wasevaluated at this year?s task, which was veryfrustrating in the end.3.1 Syntactic DependenciesFor testing the strict incremental dependencyparser we used the CoNLL 2008 shared tasktraining and development set.
Our best syntacticscore that we could achieve on the developmentdata was merely unlabeled attachment score(UAL) of 45.31%.
However, as mentioned in sec.2, we used a set of features proposed by Nivre,215which contains 5 features relying on the depend-ency types.
Since we couldn?t develop a goodworking module for this part of the task due tothe lack of time, we couldn?t exploit these fea-tures.Note that for this experiment and all others re-ported below, we used the default settings of theopennlp MEM trainer.
In particular this meansthat 100 iterations were used in all training runsand that for all experiments no tuning of parame-ters and smoothing was done, basically becausewe had no time left to exploit it in a sensible way.These parts will surely be revised and improvedin the future.3.2 Semantic DependenciesAs we describe in the sec.
3 our semantic moduleconsists of 4 steps.
For the first step we achievethe F-score of 76.9%.
Whereas the verb predi-cates are recognized very well (average score forevery verb category is almost 90%), we do badlywith the noun categories.
Since our semanticmodule depends on the input produced by thesyntactic parser, and is influenced by its errors,we also did a test assuming a 100% correct parse.In this scenario we could achieve the F-score of79.4%.We have completely neglected the second stepof the semantic task.
We didn?t even try to do thefeature engineering and to train a model for thisassignment, basically because of time con-straints.
Neither did we try to include some in-formation about the predicate-argument structurein order to do better on this part of the task.
Thesimple assignment of the statistically most fre-quent reading for each predicate reduced the ac-curacy from 76.9% down to 69.3%.
In case ofperfect syntactic parse the result went down from79.4% to 71.5%.Unfortunately the evaluation software doesn?tprovide the differentiation between the unlabeledand labeled argument recognition, which corre-sponds to our third and fourth steps respectively.Whereas we put some effort on identifying thearguments, we didn?t focus on their classifica-tion.
Therefore the overall best labeled attach-ment score for our system is 29.38%, whereasthe unlabeled score is 50.74%.
Assuming theperfect parse the labeled score is 32.67% and theunlabeled score is 66.73%.
In our further workwe will try to reduce this great deviation betweenboth results.3.3 Runtime performanceOne of the main strong sides of the strict incre-mental approach is its runtime performance.Since we are restricted in our feature selectionto the already seen space to the left of the currentword, both the training and the application of ourstrategy are done fast.The training of our MEMs for the syntacticpart requires 62 minutes.
The training of themodels for our semantic components needs 31minutes.
The test run of our system for the testdata from the Brown corpus (425 sentences with7207 tokens) lasted 1 minute and 18 seconds.The application on the WSJ test data (2399 sen-tences with 57676 tokens) took 20 minutes and42 seconds.
The experiments have been per-formed on a computer with one Intel Pentium1,86 Ghz processor and 1GB memory.4 Results and DiscussionThe results of running our current version on theCoNLL 2008 shared task test data were actuallya knockdown blow.
We participated in the closedchallenge, and obtained for the complete problema labeled macro F1 for WSJ+Brown of 19.93(20.13 on WSJ only, 18.14 on Brown).
For thesyntactic dependencies we got similar bad results(WSJ+Brown = 16.25, WSJ = 16.22, Brown =16.47), as well as for the semantic dependencies(WSJ+Brown = 22.36, WSJ = 22.86, Brown =17.94).We see at least the following two reasons forthis disastrous result: On the one hand we fo-cused on the construction of correct syntactictrees and the recognition of the predicate-argument structure which were only parts of thetask.
On the other hand we stuck to our strict in-cremental approach, which greatly restricted thescope of development of our system.Whereas the labeling part, which was so farconsiderably neglected, will surely be improvedin the future, the strict incremental strategy in itscurrent form will probably have to be revised.4.1 Post-evaluation experimentsWe have already started beginning the im-provement of our parsing system, and we brieflydiscuss our current findings.
On the technicallevel we already found a software bug that atleast partially might explain the unexpected highdifference in performance between the resultsobtained for the development set and the test set.Correcting this error now yields an UAL of53.45% and an LAL of 26.95% on the syntactic216part of the Brown test data which is a LAL-improvement of about 10%.On the methodological level we are studyingthe effects of relaxing some of the assumptionsof our strict incremental parsing strategy.
In or-der to do so, we developed a separate model forpredicting the unlabeled edges and a separatemodel for labeling them.
In both cases we usedthe same features as described in sec.
2, butadded features that used a right-context in orderto take into account the PoS-tag of the N-nextwords viz.
N=5 for the syntactic parser and N=3for the labeling case.
Using both models duringparsing interleaved, we obtained UAL=65.17%and LAL=28.47% on the development set.We assumed that the low LAL might havebeen caused by a too narrow syntactic context.
Inorder to test this assumption, we decoupled theprediction of the unlabeled edges and their label-ing, such that the determination of the edge la-bels is performed after the complete unlabeleddependency tree is computed.
Labeling of thedependency edges is then simply performed byrunning through the constructed parse trees as-signing each edge the most probable dependencytype.
This two-phase strategy achieved an LALof 60.44% on the development set, which meansan improvement of about 43%.
Applying thetwo-phase parser on the WSJ test data resulted inUAL=65.22% and LAL=62.83%; applying it onthe Brown test data resulted in UAL=66.50% andLAL=61.11%, respectively.Of course, these results are far from being op-timal.
Thus, beside testing and improving ourparser on the technical level, we will run furtherexperiments for different context sizes, exploit-ing different settings of parameters of the classi-fier and feature values, and eventually testingother ML approaches.
The focus here will be onthe development of unlabeled edge models, be-cause it seems that an improvement here is sub-stantial for an overall improvement.
For exam-ple, applying the decoupled edge labeling modeldirectly on the given unlabeled dependency treesof the development set (i.e.
we assume an UALof 100%) gave as an LAL of 92.88%.Beside this, we will also re-investigate inter-leaved strategies of unlabeled edge and edge la-beling prediction as a basis for (mildly-) strictincremental parsing.
Here, it might be useful torelax the strict linear control regime by exploringbeam search strategies, e.g.
along the lines ofCollins and Roark (2004).5 ConclusionWe have presented a puristic approach forjoint dependency parsing and semantic role la-beling.
Since, the development of our approachhas been started from scratch, we didn?t manageto deal with all problems.
Our focus was on set-ting up a workable backbone, and then on tryingto do as much feature engineering as possible.Our bad results on the CoNLL 2008 suggest thatour current strategy was a bit too optimistic andrisky, and that the strict incremental deterministicparsing regime seemed to have failed in its cur-rent form.
We are now in the process of analysisof ?what went wrong?, and have already indi-cated some issues in the paper.AcknowledgementThe work presented here was partially supportedby a research grant from the German FederalMinistry of Education, Science, Research andTechnology (BMBF) to the DFKI project HyLaP,(FKZ: 01 IW F02).
We thank the developers ofthe Opennlp.maxent software package.ReferencesBrants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERTreebank in Proceedings of the Workshop onTreebanks and Linguistic Theories Sozopol.Collins, Michael, and Brian Roark.
(2004).
Incre-mental Parsing with the Perceptron Algorithm.ACL 2004.Hacioglu, Kadri.
2004.
Semantic Role Labeling UsingDependency Trees.
Coling 2004.Nivre, Joakim.
2007.
Incremental Non-Projective De-pendency Parsing.
NAACL-HLT 200).Manning, Christopher, and Hinrich Schutze.
1999.Foundations of statistical natural language process-ing.
Cambridge, Mass.
: MIT Press.Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?s M?rquez, and Joakim Nivre.
2008.
TheCoNLL-2008 Shared Task on Joint Parsing of Syn-tactic and Semantic Dependencies.
In Proceedingsof the 12th Conference on Computational NaturalLanguage Learning (CoNLL-2008).Volokh, Alexander.
2008.
Datenbasiertes De-pendenzparsing.
Bachelor Thesis, Saarland Uni-versity.Wang, Mengqui, Kenji Sagae, and Teruko Mitamura.2006.
A Fast, Accurate Deterministic Parser forChinese.
ACL 2006.217
