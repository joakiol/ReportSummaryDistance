Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 648?657,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPImproving Web Search Relevance with Semantic FeaturesYumao Lu Fuchun Peng Gilad Mishne Xing Wei Benoit DumoulinYahoo!
Inc.701 First AvenueSunnyvale, CA, 94089yumaol,fuchun,gilad,xwei,benoitd@yahoo-inc.comAbstractMost existing information retrieval (IR)systems do not take much advantage ofnatural language processing (NLP) tech-niques due to the complexity and limitedobserved effectiveness of applying NLPto IR.
In this paper, we demonstrate thatsubstantial gains can be obtained over astrong baseline using NLP techniques, ifproperly handled.
We propose a frame-work for deriving semantic text matchingfeatures from named entities identified inWeb queries; we then utilize these featuresin a supervised machine-learned rankingapproach, applying a set of emerging ma-chine learning techniques.
Our approachis especially useful for queries that containmultiple types of concepts.
Comparing toa major commercial Web search engine,we observe a substantial 4% DCG5 gainover the affected queries.1 IntroductionMost existing IR models score documents pri-marily based on various term statistics.
In tra-ditional models?from classic probabilistic mod-els (Croft and Harper, 1979; Fuhr, 1992), throughvector space models (Salton et al, 1975; Naritaand Ogawa, 2000), to well studied statistical lan-guage models (Ponte and Croft, 2000; Laffertyand Zhai, 2001)?these term statistics have beencaptured directly in the ranking formula.
More re-cently, learning to rank approaches to IR (Fried-man, 2002) have become prominent; in theseframeworks, that aim at learning a ranking func-tion from data, term statistics are often modeledas term matching features in a machine learningprocess.Traditional text matching features are mainlybased on frequencies of n-grams of the user?squery in a variety of document sections, such asthe document title, body text, anchor text, and soon.
Global information such as frequency of termor term group in the corpus may also be used, aswell as its combination with local statistics ?
pro-ducing relative scores such as tf ?
idf or BM25scores (Robertson et al, 1995).
Matching maybe restricted to certain window sizes to enforceproximity, or may be more lenient, allowing un-ordered sequences and nonconsecutive sequencesfor a higher recall.Even before machine learning was applied toIR, NLP techniques such as Named Entity Recog-nition (NER), Part-of-Speech (POS) tagging, andparsing have been applied to both query model-ing and document indexing (Smeaton and van Ri-jsbergen, 1988; Narita and Ogawa, 2000; Sparck-Jones, 1999).
For example, statistical conceptlanguage models generalize classic n-gram mod-els to concept n-gram model by enforcing queryterm proximity within each concept (Srikanth andSrihari, 2003).
However, researchers have of-ten reported limited gains or even decreased per-formance when applying NLP to IR (Voorhees,1999).Typically, concepts detected through NLP tech-niques either in the query or in documents areused as proximity constraints for text match-ing (Sparck-Jones, 1999), ignoring the actual con-cept type.
The machine learned approach to docu-ment ranking provides us with an opportunity torevisit the manner in which NLP information isused for ranking.
Using knowledge gained fromNLP application as features rather than heuris-tically allows us much greater flexibility in theamount and variability of information used ?
e.g.,incorporating knowledge about the actual entitytypes.
This has several benefits: first, entity typesappearing in queries are an indicator of the user?sintent.
A query consisting of a business categoryand a location (e.g., hotels Palo Alto) appears to be648informational, and perhaps is best answered witha page containing a list of hotels in Palo Alto.Queries containing a business name and a location(e.g., Fuki Sushi Palo Alto) are more navigationalin nature ?
for many users, the intent is finding thehome page of a specific business.
Similarly, entitytypes appearing in documents are an indicator ofthe document type.
For example, if ?Palo Alto?appears ten times in document?s body text, it ismore likely to be a local listing page than a homepage.
For the query hotels Palo Alto, a local listingpage may be a good page, while for the query FukiSushi Palo Alto a listing page is not a good page.In addition, knowledge of the particular entitiesin queries allows us to incorporate external knowl-edge about these entities, such as entity-specificstopwords (?inc.?
as in Yahoo Inc. or ?services?as in kaiser medical service), and so on.Finally, even when using named entities onlyfor deriving proximity-related features, we canbenefit from applying different levels of proxim-ity for different entities.
For example, for enti-ties like cities (e.g., ?River Side?
), the proximityrequirement is fairly strict: we should not allowextra words between the original terms, and pre-serve their order.
For other entities the proximityconstraint can be relaxed?for example, for per-son names, due to the middle name convention:Hillary Clinton vs. Hillary R. Clinton.In this paper, we propose a systematic approachto modeling semantic features, incorporating con-cept types extracted from query analysis.
Ver-tical attributes, such as city-state relationships,metropolitan definition, or idf scores from a do-main specific corpus, are extracted for each con-cept type from vertical database.
The vertical at-tributes, together with the concept attributes, areused to compose a set of semantic features for ma-chine learning based IR models.
A few machinelearning techniques are discussed to further im-prove relevance for subclass of difficult queriessuch as queries containing multiple types of con-cepts.
Figure 1 shows an overview of our ap-proach; after discussing related work in Section 2,we spend Sections 3 to 5 of the paper describingthe components of our system.
We then evaluatethe effectiveness of our approach both using gen-eral queries and with a set of ?difficult?
queries;our results show that the techniques are robust, andparticularly effective for this type of queries.
Weconclude in Section 7.Tagger1 Tagger2 Tagger nResolution ModuleQuerywithAnnotationsQueryQueryLinguisticAnalysisVertical AttributeLocationDBVertical AttributeBusinessDBVertical AttributeSemantic Text MatchingDocumentIndexSemantic FeaturesSpecializedRankingModuleSpecializedRankingModuleSpecializedRankingModule...DBName.........Figure 1: Ranking with Semantic Features2 Related WorkThere is substantial body of work involving us-age of NLP techniques to improve information re-trieval (Brants, 2003; Strzalkowski et al, 1996).Allan and Ragahavan (Allan and Raghavan, 2002)use Part-of-Speech tagging to reduce ambiguityof difficult queries by converting short queriesto questions.
In other POS-tags work, Aram-patzis et al (Arampatzis et al, 1990) observedan improvement when using nouns only for re-trieval.
Croft et al (Croft et al, 1991) and Tonget al (Buckley et al, 1993; Tong et al, 1996) ex-plored phrases and structured queries and foundphrases are effective in improving retrieval per-formance.
Voorhees (Voohees, 1993) uses wordsense disambiguation to improve retrieval perfor-mance.
One IR domain that consistently benefitsfrom usage of various NLP techniques is questionanswering, where queries are formed in naturallanguage format; e.g., (Peng et al, 2005).In general, however, researchers often observelimited gains or even degraded performance whenapplying NLP to IR (Voorhees, 1999).
Havingsaid this, most past studies use small datasets anda modest baseline; it is unclear whether a similarconclusion would be reached when using a state-of-art system such as a commercial web searchengine as a baseline, and a full-web corpus ?
aswe do in this paper.
This leads to another differ-ence between this work and existing work involv-ing named entity recognition for retrieval.
Most649previous research on usage of named entities inIR combines entity detection in documents andqueries (Prager et al, 2000).
Entity detection indocument has a high indexing cost that is oftenoverlooked, but cannot be ignored in the case ofcommercial search engines.
For this reason, werestrict NLP processing to queries only ?
althoughwe believe that document-side NLP processingwill provide additional useful information.3 Query AnalysisWe begin by briefly describing our approach tonamed entity recognition in web queries, whichserves as the basis for deriving the semantic textmatching features.Named entity recognition (NER) is the task ofidentifying and classifying entities, such as per-son names or locations, in text.
The majority ofstate-of-the-art NER methods utilize a statisticalapproach, attempting to learn a mapping betweena sequence of observations (words) and a sequenceof tags (entity types).
In these methods, the se-quential nature of the data is often central to themodel, as named entities tend to appear in particu-lar context in text.
For example, for most types oftext, in the two sequences met with X and buy theY, the likelihood of X being a person name is sub-stantially higher than the corresponding likelihoodof Y .
Indeed, many named entity taggers performwell when applied to grammatical text with suf-ficient contexts, such as newswire text (Sang andMeulder, 2003).Web queries, however, tend to be short, withmost queries consisting of 1?3 words, and lackcontext ?
posing a particular challenge for iden-tifying named entities in them.
Existing work onNER in web queries focuses on tailoring a solu-tion for a particular entity type and its usage inweb search (Wang et al, 2005; Shen et al, 2008);in contrast, we aim at identifying a large rangeof possible entities in web queries, and using ageneric solution for all of them.In web queries, different entity types may bene-fit from different detection techniques.
For exam-ple, an entity type with a large variability amonginstances as well as existence of external resourceslike product name calls for an approach that canmake use of many features, such as a conditionalrandom field; for entity types that are more struc-tured like person names, a grammar-based ap-proach can be more effective (Shen et al, 2008).To this end, we utilize multiple approaches for en-tity detection and combine them into a single, co-herent ?interpretation?
of the query.Given a query, we use several entity recogniz-ers in parallel, one for each of the common en-tity types found in web queries.
The modelingtypes may differ between the recognizers: someare Markovian models, while others are just dic-tionary lookups; the accuracy of each recognizeris also different.
We then have a machine-learneddisambiguation module that combines output fromdifferent taggers, ranking the tagging sequences.The details of scoring is out of the scope of thispaper, and we omit it for simplicity.4 Semantic Text Matching FeaturesOur proposed semantic features operate at thesemantic type level rather than at the term level:instead of matching a term (or set of terms) in doc-uments, we match their semantic type.
Given thequery San Francisco colleges and the annotation[San Francisco]CityName[colleges]BusinessCategory,the semantic text matching features would de-scribe how relevant a document section is for a en-tity of type CityName, for BusinessCategory,and for their combination.Concretely, we exploit a set of features thatattempts to capture proximity, general relevance,and vertical relevance for each type of semantictag and for each section of the document.
We nowreview these feature by their broad types.4.1 Semantic Proximity FeaturesProximity features?features that capture the de-gree to which search terms appear close to eachother in a document?are among the most impor-tant feature sets in ranking functions.
Traditionalproximity features are typically designed for allquery terms (Metzler and Croft, 2005) and maysuffer from wrong segmentations of the query.
Forexample, for the query New York city bus char-ter, a traditional proximity feature may treat ?citybus?
similarly to ?York city.?
But given detailedinformation about the entities in the query in theirtypes, we can enforce proximity for ?New Yorkcity?
and ?bus charter?
more accurately.
Differenttypes of entities usually have different proximitycharacteristics in relevant documents.
Strongly-bound entities such as city names typically havevery high proximity in relevant documents, whileentities such as business names may have much650lower proximity: a search for Kaiser medical of-fice, for example, may be well-served with docu-ments referring to Kaiser Permanente medical of-fice, and as we mentioned before, person namesmatches may also benefit from lenient proximityenforcement.
This is naturally addressed by treat-ing each entity type differently.We propose a set of semantic proximity fea-tures that associate each semantic tag type withgeneric proximity measures.
We also consider tag-ging confidence together with term group proxim-ity; we discuss these two approaches next.4.1.1 Semantic Minimum Coverage (SMC)Minimum Coverage (MC) is a popular span basedproximity distance measure, which is defined asthe length of the shortest document segment thatcover the query term at least once in a docu-ment (Tao and Zhai, 2007).
We extend this mea-sure to Semantic Minimum Coverage (SMC) foreach semantic type t in document section s anddefine it asSMCt,s=1|{k|Tk= t}|?i?
{k|Tk=t}wiMCi,s,where wiis a weight for tagged term group i,MCi,sis the the minimum coverage of term groupi in document section s, {k|Tk= t} denotes theset of all concepts having type t, and |{k|Tk= t}|is the size of the set.
The definition of the weightw is flexible.
We list a few candidate weight-ing schemes in this paper: uniform weights (wu),weights based on idf scores (widf) and ?strength?-based weight (ws), which we define as follows:wu= 1;widf=cfqwhere c is a constant and fqis the frequency of theterm group in a large query log;ws= minlMIlwhere MIlis the point-wise mutual information ofthe l-th consecutive pair within the semantic tag.We can also combine strength and idf scores suchthat the weight reflects both relative importanceand constraints in proximity.
In this paper, we usewsi= wswidf.In Section 6, we use all four weighting schemesmentioned above in the semantic feature set.4.1.2 Semantic Moving Average BM25(SMABM25)BM25, a commonly-used bag-of-words relevanceestimation method (Robertson et al, 1995), is de-fined (when applied to document sections) asBM25 =?jidfjfj,s(c1+ 1)fi,s+ c1(1?
c2+ c2ls?ls)where fj,sis the frequency of term j in section s,lsis the length of section s, ?lsis the average lengthof document section s, c1, c2, c3are constants andthe idf score of term j is defined asidfj= logc4?
dj+ c5dj+ c5,where djis the number of sections in all collec-tions that contains term j and c4, c5are constants.To characterize proximity, we could use a fixedlength sliding window and calculate the averageBM25.
We further associate each sliding averageBM25 with each type of semantic term groups.This results in a Semantic Moving Average BM25(SMABM25) of type t, which we define as fol-lows:1|{k|Tk= t}|?i?
{k|Tk=t}(1/M)?mBM25mwhere m is a fixed length sliding window m andM is the total number of sliding windows (that de-pends on the length of the section window size).4.2 Semantic Vertical Relevance FeaturesVertical databases contain a large amount of struc-tured domain knowledge typically discarded bytraditional web relevance features.
Having accessto the semantic types in queries, we can tap intothat knowledge to improve accuracy.
For exam-ple, term frequencies in different corpora can as-sist in determining relevance given an entity type.As we mentioned in Section 1, we observe thatterm frequency in a database of business namesprovides an indication of the business brand, thekey part of the business name phrase.
While both?yahoo?
and ?inc?
are very common terms on theweb, in a database of businesses only ?inc?
is com-mon enough to be considered a stopword in thecontext of business names.We propose a Vertical Moving Average BM25(VMABM25) as a feature aiming at quantifyingthe vertical knowledge for web search.
The ba-sic idea here is to replace the idf score idfjof651SMABM25 with an idf score calculated from avertical database for type t, namely idftj:1|{k|Tk= t}|?i?
{k|Tk=t}(1/M)?mBM25m,twhereBM25m,t=?jidftjfj,s(c1 + 1)fi,s+ c1(1?
c2+ c2ls?ls)where the idftjis associated with the semantic typet and calculated from the corpus associated withthat type.VMABM25 links vertical knowledge, proxim-ity, and page relevance together; we show later thatit is one of most salient features among all seman-tic features.4.3 Generalized Semantic FeaturesFinally, we develop a generalized feature based onthe previous features by removing tags.
Semanticfeatures are often sparse, as many queries containone entity or no entities at all; generalized featuresincrease their coverage by combining the basic se-mantic features.
An entity without tag is essen-tially a segment.A segment feature xifor query i does not haveentity type and can be expressed asxi=1KiKi?k=1xT (k)where Kiis the number of segments in the queryand T (k) is the semantic type associated with kthconcept.Although these features are less informativethan type-specific features, one advantage of usingthem is that they have substantially higher cover-age.
In our experiments, more than 40% of thequeries have some identified entity.
Another rel-atively subtle advantage is that segment featureshave no type related errors: the only possible erroris a mistake in entity boundaries.5 Ranking Function OptimizationThe ultimate goal of the machine learning ap-proach to web search is to learn a ranking func-tion h(xi), where xiis a feature vector of a query-document pair i, such that the errorL(h) ?N?i=1(yi?
h(xi))2 (1)is minimized.
Here, yiis the actual relevance scorefor the query-document pair i (typically assignedby a human) and N is the number of training sam-ples.As mentioned in the previous Section, an inher-ent issue with semantic features is their sparse-ness.
User queries are usually short, with an av-erage length of less than 3 words.
Text matchingfeatures that are associated with the semantic typeof query term or term groups are clearly sparsecomparing with traditional, non-entity text match-ing features ?
that can be derived for any query.When a feature is very sparse, it is unlikely thatit would play a very meaningful role in a machinelearned ranking function, since the error L wouldlargely depend on other samples that do not con-tain the specific semantic features at all.
To over-come the spareness issue and take advantage ofsemantic features, we suggested generalizing ourfeatures; but we also exploit a few ranking func-tion modeling techniques.First, we use a ?divide-and-conquer?
approach.Long queries usually contain multiple conceptsand could be difficult to retrieve relevant docu-ments.
Semantic features, however, are rich inthis set of queries.
We may train special modelsto further optimize our ranking function for thosequeries.
The loss function over ranking function hbecomesLC(h) ??i?C(yi?
h(xi))2 (2)where C is the training set that falls into a pre-defined subclass.
For example, queries containingboth location and business name, queries containsboth location and business category, etc, are goodcandidates to apply semantic features.To this end, we first classify queries into severalclasses, each of which has multiple types of enti-ties.
The semantic features of those types wouldbe dense for this subclass of queries.
We thentrain models that may rank the specific class ofqueries well.
This approach, however, may suf-fer from significantly less training samples due totraining data partition resulted from the query clas-sification.
Increasing the modeling accuracy, then,comes at a cost of reduced data available for train-ing.
We apply two techniques to address this is-sue.
The first approach is to over-weight subclasstraining samples such that the subclass of queriesplays a more important role in modeling while still652keeping a large pool of the overall training sam-ples.
The second approach is model adaptation:a generalized incremental learning method.
Here,instead of being over-weighted in a joint optimiza-tion, the subclass of training data is used to mod-ify an existing model such that the new model is?adapted?
to the subclass problem.
We elaborateon our approaches as follows.5.1 Weighted Training SamplesTo take advantage of both large a pool of trainingsamples and sparse related semantic features fora subclass of queries, we could modify the lossfunction as followsLwC(h) ?
w?i?C(yi?
h(xi))2+?i??C(yi?
h(xi))2,(3)where ?C is the complement of set C. Here, theweight w is a compromise between loss function(1) and (2).
When w = 1, we haveL1C(h) ?
L(h);when w?
> ?L?C(h) ?
LC(h).A large weight may help optimize the training fora special subclass of queries, and a small weightmay help to preserve good generality of the ranker.We could use cross-validation to select the weightw to optimize a the ranking function for a sub-class of queries.
In practice, a small w is desiredto avoid overfitting.5.2 Model AdaptationModel adaptation is an emerging machine learn-ing technique that is used for information retrievalapplications with limited amount of training data.In this paper, we apply Trada, proposed by Chenet al (Chen et al, 2008), as our adaptation algo-rithm.The Trada algorithm aims at adapting tree-based models.
A popular tree based regression ap-proach is Gradient Boosting Trees (GBT) , whichis an additive model h(x) =?Kk=1?khk(x),where each regression tree hkis sequentially op-timized with a hill-climbing procedure.
As withother decision trees, a binary regression tree hk(x)consists of a set of decision nodes; each node isassociated with a feature variable and a splittingvalue that partition the data into two parts, with thecorresponding predicted value defined in the leavenode.
The basic idea of Trada is to apply piece-wise linear transformation to the base model basedon the new training data.
A set of linear transfor-mations are applied to each decision node, eitherpredict or split point or both, such that the new pre-dict or the split point of a node in a decision treesatisfiesv = (1?
pC)v?
+ pCvCwhere v?
denotes predict or split point of that nodein the base mode and vCdenotes predict or splitpoint of that node using new data set C, andthe weight pCdepends on the number of origi-nal training data and new training data that fallthrough the node.
For each node, the split or pre-dict can be estimated bypC=?nCn+ ?nC,where n is the number of training sample of thebase model that fall through the node, nCis thenumber of new training sample that fall throughthe node, and ?
is a parameter that can be deter-mined using cross validation.
The parameter ?is used to over-weight new training data, an ap-proach that is very effective in practice.
For newfeatures that are not included in the base model,more trees are allowed to be added to incorporatethem.6 ExperimentsWe now measure the effectiveness of our proposal,and answer related questions, through extensiveexperimental evaluation.
We begin by examiningthe effectiveness of features as well as the model-ing approaches introduced in Section 5 on a par-ticular class of queries?those with a local intent.We proceed by evaluating whether if the type asso-ciated with each entity really matters by compar-ing results with type dependent semantic featuresand segment features.
Finally, we examine the ro-bustness of our features by measuring the changein the accuracy of our resulting ranking functionwhen the query analysis is wrong; we do this byintroducing simulated noise into the query analy-sis results.6.1 DatasetOur training, validation and test sets are human-labeled query-document pairs.
Each item in the653sets consists of a feature vector xirepresent-ing the query and the document, and a judg-ment score yiassigned by a human.
There arearound 600 features in each vector, including boththe newly introduced semantic features and exist-ing features; features are either query-dependentones, document-dependent ones, or query-and-document-dependent features.The training set is based on uniformly sampledWeb queries from our query log, and top rankeddocuments returned by commercial search enginesfor these queries; this set consists of 1.24M query-document pairs.We use two additional sets for validation andtesting.
One set is based on uniformly sampledWeb queries, and contains 42790 validation sam-ples and 70320 test samples.
The second set isbased on uniformly sampled local queries.
By lo-cal queries, we mean queries that contain at leasttwo types of semantic tags: a location tag (suchas street, city or state name) and a business tag (abusiness name or business category).
We refer tothis class of queries ?local queries,?
as users oftentype this kind of queries in local vertical search.The local query set consists of 11040 validationsamples and 39169 test samples.
In the trainingset we described above, there are 56299 trainingsamples out of the 1.24M total number of trainingsamples that satisfy the definition of local queries.We call this set local training subset.6.2 Evaluation MetricsTo evaluate the effectiveness of our semanticfeatures we use Discounted Cumulative Gain(DCG) (Jarvelin and Kekalainen, 2000), a widely-used metric for measuring Web search relevance.
(Jarvelin and Kekalainen, 2000).
Given a queryand a ranked list of K documents (K = 5 in ourexperiments), the DCG for this query is defined asDCG(K) =K?i=1yilog2(1 + i).
(4)where yi?
[0, 10] is a relevance score for thedocument at position i, typically assigned by a hu-man, where 10 is assigned to the most relevantdocuments and 0 to the least relevant ones.To measure statistical significance, we use theWilcoxon test (Wilcoxon, 1945); when the p-valueis below 0.01 we consider a difference to be statis-tically significant and mark it with a bold font inthe result table.6.3 Experimental ResultsWe use Stochastic Gradient Boosting Trees(SGBT) (Friedman, 2002), a robust none linearregression algorithm, for training ranking func-tions and, as mentioned earlier, Trada (Chen et al,2008) for model adaptation.Training parameters are selected to optimize therelevance on a separated validation set.
The bestresulting is evaluated against the test set; all resultspresented here use the test set for evaluation.6.3.1 Feature Effectiveness with RankingFunction ModelingWe apply the modeling approaches introduced inSection 5 to improve feature effectiveness on ?dif-ficult?
queries?those more than one entity type;we evaluate these approaches with the semantic-feature-rich set, the local query test set.
We splittraining sets into two parts: one set belongs to thelocal queries, the other is the rest.
We first weightthe local queries and use the combined dataset astraining data to learn the ranking functions; wetrain functions with and without the semantic fea-tures.
We evaluate these functions against the lo-cal query test set.
The results are summarized inTable 1, where w denotes the weight assigned tothe local training set, bolded numbers are statis-tically significant result compared to the baseline,uniformly weighted training data without seman-tic features (with superscript b).
It is interestingto observe that without semantic features, over-weighted local training data does not have statis-tically significant impact on the test performance;with semantic features, a proper weight over train-ing samples does improve test performance sub-stantially.Table 1: Evaluation of Ranking Models TrainedAgainst Over-weighted Local Queries with Se-mantic Features on the Local Query Test Setw/o semantic features w/ semantic featuresWeight DCG(5) Impr.
DCG(5) Impr.w = 0 8.09b- 8.25 2.0%w = 2 8.09 0.02% 8.26 2.1%w = 4 8.13 0.49% 8.34 3.1%w = 8 8.13 0.49% 8.42 4.1%w = 16 8.13 0.49% 8.30 2.6%w = 32 8.04 ?0.60% 8.27 2.2%Next, we use the local query training set as ?newdata?
in the tree adaptation approach.
In tree adap-tations, all parameters are set to optimize the per-formance over the local validation set.
We com-654pare two major adaptation approaches proposedin (Chen et al, 2008): adapting predict only andadapting both predict and split.
We use the modeltrained with the combined training and uniformweights as the baseline; results are summarized inTable 2.Table 2: Trada Algorithms with Semantic Featureson Local Query Test Setw/o semantic feat.
w/ semantic feat.Ada.
Appr.
DCG(5) Impr.
DCG(5) Impr.Combined data 8.09b - 8.25 2.0%Ada.
predict 8.02 ?0.1% 8.14 0.6%Ada.
predict 8.00 ?0.1% 8.17 1.0%& splitComparing Tables 1 and 2, note that using thecombined training data with local query trainingsamples over-weighted achieves better results thantree adaption.
The latter approach, however, hasthe advantage of far less training time, since theadaptation is over a much smaller local querytraining set.
With the same hardware, it takesjust a few minutes to train an adaptation model,while it takes days to train a model over the entire,combined training data.
Considering that massivemodel validation tasks are required to select goodtraining parameters, training many different mod-els with over a million training samples becomesprohibitly costly.
Applying tree adaptation tech-niques makes research and prototyping of thesemodels feasible.6.3.2 Type Dependent Semantic Features vs.Segment FeaturesOur next experiment compares type-dependentfeatures and segment features, evaluating modelstrained with these features against the local querytest set.
No special modeling approach is appliedhere; results are summarized in Table 3.
We ob-serve that by using type-dependent semantic fea-tures only, we can achieve as much as by usingall semantic features.
Since segment features onlyconvey proximity information while the base fea-ture set aleady contain a systematic set of prox-imity measures, the improvement through segmentfeatures is not as significant as the the type depen-dent ones.6.3.3 Robustness of Semantic FeaturesOur final set of experiments aims at evaluating therobustness of our semantic features by introducingTable 3: Type-dependent Semantic Features vs.Segment FeaturesFeature set DCG(5)base + type dependent semantic features 8.23base + segment features 8.19base + all semantic features 8.25simulated errors to the output of our query analy-sis.
Concretely, we manipulate the precision andthe recall of a specific type of entity tagger, t, onthe training and test set.
To decrease the recall oftype t, we uniformly remove a set of a% tags oftype t ?
preserving precision.
To decrease preci-sion, we uniformly select a set of query segments(viewing the entity detection as simple segmenta-tion, as detailed earlier) and assign the semantictype t to those segments.
Since the newly addedterm group are selected from query segmentationresults, the introduced errors are rather semantictype error than boundary error or proximity error.The total number of newly assigned type t tags areb% of the original number of type t tags in thetraining set.
By doing this, we decrease the preci-sion of type t while keeping the recall of it at thesame level.Suppose the original tagger achieves precisionp and recall r. By removing a% of tags, we haveestimated precision p?
and recall r?
defined as fol-lows:r?
=100r ?
ar100,p?
= p.By adding b% more term group to this type, wehave estimated precision and recall asp?
=100p100 + bp,r?
= r.In the experiment reported here we use BUSI-NESS NAME as the target semantic type for this ro-bustness experiment.
An editorial test shows thatour tagger achieves 74% precision and 66% recallbased on a random set of human labeled queriesfor this entity type.
We train ranking models withvarious values of a and b.
When we reduce theestimated recall, we evaluate these models againstthe local test set since other data are not affected.The results are summarized in Table 4.When we reduce the precision, we evaluate theresulting models against the general test set as655Table 4: Relevance with simulated error on localquery test seta b p?
r?
DCG(5) Impr.0 0 0.74 0.66 8.25 ?10 0 0.74 0.594 8.21 0.48%20 0 0.74 0.528 8.19 0.72%40 0 0.74 0.396 8.18 0.85%Table 5: Search relevance with simulated error forsemantic features on general test seta b p?
r?
DCG(5) Impr.0 0 0.74 0.66 10.11 -0 10 0.689 0.66 10.11 0.00%0 20 0.645 0.66 10.12 0.10%0 40 0.571 0.66 10.12 0.10%0 60 0.513 0.66 10.12 0.10%0 80 0.465 0.66 10.11 0.00%0 100 0.425 0.66 10.10 ?0.10%simulated errors would virtually affect any sam-ples with certain probability.
Results appear inTable 5.
The results are quite interesting: whenthe recall of business name entity decreases, weobserve statistically significant relevance degrada-tion: if less entities are discovered, search rele-vance is hit.
The experiments with simulated pre-cision error, however, are less conclusive.
Onemay note the experiments are conducted over thegeneral test set.
Therefore, it is not clear if the pre-cision of the NER system really has insignificantimpact on the IR relevance or just the impact isdiluted in a larger test set.6.4 Case AnalysisIn this section, we take a close look at a fewcases where our new semantic features helpmost and where they fail.
For the query sil-verado ranch in irving texas, with no semanticfeatures, the ranking function ranks a locallisting page for this business, http://local.yahoo.com/info-28646193, as the topdocument.
With semantic features, the rankingfunction ranks the business home page: http://www.silveradoranchparties.com/as top URL.
Examining the two documents, thelocal listing page actually contains much more rel-evant anchor text, which are the among the mostsalient features in traditional ranking models.
Thehome page, however, contains almost no relevantanchor text: for a small business home page, thisis not a rare situation.
Looking at the semanticfeatures of these two pages, the highest resolutionof location, the city name ?Irving,?
appears in thedocument body text 19 times in the local listingpage body text, and only 2 times in the home pagebody text.
The training process learns, then, thatfor a query for a local business name (rather thana business category), home pages?even withfewer location terms in them?are likely to bemore relevant than a local listing page that usuallycontain high frequency location terms.In some cases, however, our new features dohurt performance.
For the query pa treasur-ers office, the ranking function with no seman-tic features ranks the document http://www.patreasury.org highest, while the one withsemantic features ranks the page http://www.pikepa.org/treasurer.htm higher.
Thelatter page is somewhat relevant: it is a treasurer?soffice in Pennsylvania.
However, it belongs to aspecific county, which makes it less relevant thanthe former page.
This is a classic error that we ob-serve: a mismatch of the intended location area.While users are looking for state level business,we provide results of county level.
To resolvethis type of error, query analysis and semantic textmatching are no longer enough: here, the rank-ing function needs to know that Pike County is acounty in Pennsylvania, Milford is a city in PikeCounty, and neither are referred to by the user.Document-side entity recognition, however, mayprovide this type of information, helping to ad-dress this type of errors.7 Conclusion and Future ResearchIn this paper, we investigate how semantic featurescan improve search relevance in a large-scale in-formation retrieval setting; to our knowledge, it isthe first study of this approach on a web scale.
Wepresent a set of features that incorporate semanticand vertical knowledge into the retrieval process,propose techniques to handle the sparseness prob-lem for these features, and describe how they fitin the learning process.
We demonstrate that thesecarefully designed features significantly improverelevance, particularly for difficult queries ?
longqueries with multiple entities.The work reported here focuses on query-sideprocessing, avoiding the indexing cost of docu-ment processing.
We are currently investigatingdocument-side analysis to complement the query-side work, and believe that this will further boostthe retrieval accuracy; we hope to report on this ina follow-up study.656ReferencesJ.
Allan and H. Raghavan.
2002.
Using Part-of-SpeechPatterns to Reduce Query Ambiguity.
In Proceed-ings of SIGIR.A.
T. Arampatzis, Th.
P. Weide, C. H. A. Koster,and P. Bommel.
1990.
Text Filtering usingLinguistically-motivated Indexing Terms.
Techni-cal Report CSI-R9901, Computing Science Institute,University of Nijmegen, Nijmegen,The Netherlands.Thorsten Brants.
2003.
Natural Language Processingin Information Retrieval.
In Proceedings of CLIN.C.
Buckley, J. Allan, and G. Salton.
1993.
Auto-matic Routing and Ad-hoc Retrieval Using SMART:TREC 2.
In Proceedings of TREC-2.Keke Chen, Rongqing Lu, C.K.
Wong, Gordon Sun,Larry Heck, and Belle Tseng.
2008.
Trada: treebased ranking function adaptation.
In Proceedingsof CIKM.W.B.
Croft and D.J.
Harper.
1979.
Using ProbabilisticModels of Document Retrieval without RelevanceInformation.
Journal of Documentation, 37:285?295.W.
Bruce Croft, Howard R. Turtle, and David D. Lewis.1991.
The Use of Phrases and Structured queries inInformation Retrieval.
In Proceedings of SIGIR.J.
H. Friedman.
2002.
Stochastic Gradient Boost-ing.
Computational Statistics and Data Analysis,38(4):367?378.N.
Fuhr.
1992.
Probabilistic Models in InformationRetrieval.
The Computer Journal, 35:243?255.K.
Jarvelin and J. Kekalainen.
2000.
IR Evalua-tion Methods for Retrieving Highly Relevant Doc-uments.
In Proceedings of SIGIR.J.
Lafferty and C. Zhai.
2001.
Document LanguageModels, Query Models and Risk Minimization forInformation Retrieval.
In Proceedings of SIGIR.Donald Metzler and W. Bruce Croft.
2005.
A markovrandom field model for term dependencies.
In Pro-ceedings of SIGIR.M.
Narita and Y. Ogawa.
2000.
The Use of Phrasesfrom Query Texts in Information Retrieval.
In Pro-ceedings of SIGIR.Fuchun Peng, Ralph Weischedel, Ana Licuanan, andJinxi Xu.
2005.
Combining Deep LinguisticsAnalysis and Surface Pattern Learning: A HybridApproach to Chinese Definitional Question Answer-ing.
In In Proceedings of the HLT-EMNLP.J.
Ponte and W.B.
Croft.
2000.
A Language ModelingApproach to Informaiton Retrieval.
In Proceedingsof SIGIR.John Prager, Eric Brown, Anni Coden, and DragomirRadev.
2000.
Question-answering by PredictiveAnnotation.
In Proceedings of SIGIR.Stephen Robertson, Steve Walker, Susan Jones, Miche-line Hancock-Beaulieu, and Mike Gatford.
1995.Okapi at TREC-3.
In Proceedings of TREC-3.G.
Salton, C. S. Yang, and C. T. Yu.
1975.
A Theory ofTerm Importance in Automatic Text Analysis.
Jour-nal of the Ameican Society of Information Science,26:33?44.Erik Tjong Kim Sang and Fien De Meulder.
2003.Introduction to the CoNLL-2003 Shared Task:Language-Independent Named Entity Recognition.In Proceedings of CoNLL.Dou Shen, Toby Walkery, Zijian Zheng, Qiang Yang,and Ying Li.
2008.
Personal Name Classification inWeb Queries.
In Proceedings of WSDM.A.F.
Smeaton and C.J.
van Rijsbergen.
1988.
Experi-ments on Incorporating Syntactic Processing of UserQueries into a Document Retrieval Stragegy.
In Pro-ceedings of SIGIR.K.
Sparck-Jones, 1999.
What is the Role of NLP in TextRetrieval, pages 1?25.
Kluwer.M.
Srikanth and R.K. Srihari.
2003.
IncorporatingQuery Term Dependencies in Language Models forDocument Retrieval.
In Proceedings of SIGIR.Tomek Strzalkowski, Jose Perez-Carballo, Jussi Karl-gren, Anette Hulth, Pasi Tapanainen, and TimoLahtinen.
1996.
Natural Language Information Re-trieval: TREC-8 Report.
In Proceedings of TREC-8.Tao Tao and ChengXiang Zhai.
2007.
An explorationof proximity measures in information retrieval.
InProceedings of SIGIR.X.
Tong, C. Zhai, N. Millic-Frayling, and D Evans.1996.
Evaluation of Syntactic Phrase Indexing ?CLARIT NLP Track Report.
In Proceedings ofTREC-5.E.
Voohees.
1993.
Using WordNet to DisambiguateWord Senses for Text Retrieval.
In Proceedings ofSIGIR.Ellen Voorhees.
1999.
Natural Language Processingand Information Retrieval.
Lecture Notes in Com-puter Science, 1714:32?48.Lee Wang, Chuang Wang, Xing Xie, Josh Forman,Yansheng Lu, Wei-Ying Ma, and Ying Li.
2005.Detecting dominant locations from search queries.In Proceedings of SIGIR.F.
Wilcoxon.
1945.
Individual Comparisons by Rank-ing Methods.
Biometrics, 1:80?83.657
