Coling 2010: Poster Volume, pages 946?954,Beijing, August 2010MT Error Detection for Cross-Lingual Question AnsweringKristen PartonColumbia UniversityNew York, NY, USAkristen@cs.columbia.eduKathleen McKeownColumbia UniversityNew York, NY, USAkathy@cs.columbia.eduAbstractWe present a novel algorithm for de-tecting errors in MT, specifically focus-ing on content words that are deletedduring MT.
We evaluate it in the con-text of cross-lingual question answering(CLQA), where we try to correct thedetected errors by using a better (butslower) MT system to retranslate a lim-ited number of sentences at query time.Using a query-dependent ranking heuris-tic enabled the system to direct scarceMT resources towards retranslating thesentences that were most likely to ben-efit CLQA.
The error detection algo-rithm identified spuriously deleted con-tent words with high precision.
How-ever, retranslation was not an effectiveapproach for correcting them, which in-dicates the need for a more targeted ap-proach to error correction in the future.1 IntroductionCross-lingual systems allow users to find infor-mation in languages they do not know, an in-creasingly important need in the modern globaleconomy.
In this paper, we focus on the spe-cial case of cross-lingual tasks with result trans-lation, where system output must be translatedback into the user?s language.
We refer to taskssuch as these as task-embedded machine trans-lation, since the performance of the system as awhole depends on both task performance and thequality of the machine translation (MT).Consider the case of cross-lingual questionanswering (CLQA) with result translation: a userenters an English question, the corpus is Ara-bic, and the system must return answers in En-glish.
If the corpus is translated into English be-fore answer extraction, an MT error may causethe system to miss a relevant sentence, leadingto decreased recall.
Boschee et al (2010) de-scribe six queries from a formal CLQA evalu-ation where none of the competing systems re-turned correct responses, due to poor translation.In one example, the answer extractor missed arelevant sentence because the name ?Abu Hamzaal-Muhajir?
was translated as ?Zarqawi?s succes-sor Issa.?
However, even if answer extraction isdone in Arabic, errorful translations of the cor-rect answer can affect precision: if the user can-not understand the translated English sentence,the result will be perceived irrelevant.
For in-stance, the user may not realize that the mistrans-lation ?Alry$Awy?
refers to Al-Rishawi.Our goal was not to improve a specific CLQAsystem, but rather to find MT errors that arelikely to impact CLQA and correct them.
We in-troduce an error detection algorithm that focuseson several common types of MT errors that arelikely to impact translation adequacy:?
content word deletion?
out-of-vocabulary (OOV) words?
named entity missed translationsThe algorithm is language-independent and MT-system-independent, and generalizes prior workby detecting errors at the word level and detect-ing errors across multiple parts of speech.We demonstrate the utility of our algorithm byapplying it to CLQA at query time, and investi-gate using a higher-quality MT system to correctthe errors.
The CLQA system translates the fullcorpus, containing 119,879 text documents and150 hours of speech, offline using a productionMT system, which is able to translate quickly(5,000 words per minute) at the cost of lowerquality translations.
A research MT system hashigher quality but is too slow to be practical fora large amount of data (at 2 words per minute,946it would take 170 days on 50 machines to trans-late the corpus).
At query-time, we can call theresearch MT system to retranslate sentences, butdue to time constraints, we can only retranslate ksentences (we set k=25).
In order to choose thesentences to best improve CLQA performance,we rank potential sentences using a relevancemodel and a model of error importance.Our results touch on three areas:?
Evaluation of our algorithm for detectingcontent word deletion shows that it is ef-fective, accurately pinpointing errors 89%of the time (excluding annotator disagree-ments).?
Evaluation of the impact of re-rankingshows that it is crucial for directing scarceMT resources wisely as the higher-rankedsentences were more relevant.?
Although the research MT system was per-ceived to be significantly better than theproduction system, evaluation shows thatit corrected the detected errors only 39%of the time.
Furthermore, retranslationseems to have a negligible effect on rele-vance.
These unexpected results indicatethat, while we can identify errors, retrans-lation is not a good approach for correctingthem.
We discuss this finding and its impli-cations in our conclusion.2 Task-Embedded MTA variety of cross-lingual applications use MTto enable users to find information in other lan-guages: e.g., CLQA, cross-lingual informationretrieval (CLIR), and cross-lingual image re-trieval.
However, cross-lingual applications suchas these typically do not do result translation?
for instance, an English-French CLIR systemwould take an English query and return Frenchdocuments, assuming that result translation is aseparate MT problem.
Part of the reason forthe separation between cross-lingual tasks andMT is that evaluating task performance on MTis often difficult.
For example, for a multilin-gual summarization task combining English andmachine translated English, Daume?
and Marcu(2006) found that doing a pyramid annotation onMT was difficult due to the poor MT quality.Assessing cross-lingual task performancewithout result translation is problematic, becausein a real-world application, result translationwould affect task performance.
For instance, inEnglish-Arabic CLIR, a poorly translated rele-vant Arabic document may appear to be irrel-evant to an English speaker.
Decoupling thecross-lingual application from the MT systemalso limits the opportunity for feedback betweenthe application and the MT system.
Ji and Grish-man (2007) exploited a feedback loop betweenChinese and English named entity (NE) taggingand Chinese-English NE translation to improveboth NE extraction and NE translation.In this paper, error detection is done at querytime so that query context can be taken into ac-count when determining which sentences to re-translate.
We also use the task context to detecterrors in translating NEs present in the query.3 Related WorkThere is extensive prior work in describing MTerrors, but they usually involve post-hoc erroranalysis of specific MT systems (e.g., (Kirch-hoff et al, 2007), (Vilar et al, 2006)) rather thanonline error detection.
One exception is Herm-jakob et al (2008), who studied NE translationerrors, and integrated an improved on-the-fly NEtransliterator into an SMT system.Content word deletion in MT has been stud-ied from different perspectives.
Li et al (2008)and Menezes and Quirk (2008) explored ways ofmodeling (intentional) source-word deletion inMT and showed that it can improve BLEU score.Zhang et al (2009) described how errors madeduring the word-alignment and phrase-extractionphases in training phrase-based SMT often leadto spurious insertions and deletions during trans-lation decoding.
This is a common error ?
Vilaret al (2006) found that 22% of errors producedby their Chinese-English MT system were due tomissing content words.
Parton et al (2009) dida post-hoc analysis on the cross-lingual 5W taskand found that content word deletion accountedfor 17-22% of the errors on that task.Some work has been done in addressing MTerrors for different cross-lingual tasks.
Ji and9471) Source kmA tHdv wzyr AldfAE AlAsrA}yly Ayhwd bArAk Al*y zAr mwqE Altfjyr AlAntHAry fy dymwnp fywqt sAbq En Altfjyr AlAntHAry .
.
.ProdM?T There also the Israeli Defense Minister Ehud Barak, who visited the site of the suicide bombing in Dimonaearlier, the suicide bombing .
.
.Ref.
Moreover, Israeli Defense Minister Ehud Barak, who visited the scene of the suicide bombing in Dimonaearlier, spoke about the suicide bombing .
.
.2) Source .
.
.
Akd Ely rgbp hrAry AlAstfAdp mn AltjArb AlAyrAnyp fy mwAjhp Alqwy AlmEtdyp.ProdM?T .
.
.
stressed the desire to test the Iranian Harare in the face of the invading forces.Ref.
.
.
.
stressed Harare?s desire to benefit from the Iranian experience in the face of the forces of aggressors.Table 1: Two examples of content word deletion during MT.Grishman (2007) detected NE translation errorsin the context of cross-lingual entity extraction,and used the task context to improve NE transla-tion.
Ma and McKeown (2009) investigated verbdeletion in Chinese-English MT in the contextof CLQA.
They tested two SMT systems, andfound deleted verbs in 4-7% of the translations.After using post-editing to correct the verb dele-tion, QA relevance increased for 7% of the sen-tences, showing that an error that may have littleimpact on translation metrics such as BLEU (Pa-pineni et al, 2002) can have a significant impacton cross-lingual applications.Our work generalizes Ma and McKeown(2009) by detecting content-word deletions andother MT errors rather than just verb deletions.We also relax the assumption that translation pre-serves part of speech (i.e., that verbs must trans-late into verbs), assuming only that a phrase con-taining a content word should be translated intoa phrase containing a content word.
Instead ofpost-editing, we use an improved MT system toretranslate sentences with detected errors.Using retranslation to correct errors exploitsthe fact that some sentences are harder to trans-late than others.
In a resource-constrained set-ting, it makes sense to apply a better MT systemonly to sentences for which the fast MT systemhas lower confidence.
We do not know of othersystems that do multi-pass translation, but it isan interesting area for further work.4 MT Error DetectionMost MT systems try to balance translation flu-ency with adequacy, which refers to the amountof meaning expressed in the original that is alsoexpressed in the translation.
For task-embeddedMT, errors in adequacy are more likely to havean impact on performance than errors in fluency.Many MT metrics (such as BLEU) treat all to-kens equally, so deleting a verb is penalized thesame as deleting a comma.
In contrast, we focuson errors in translating content words, which arewords with open-class parts of speech (POS), asthey are more likely to impact adequacy.
Firstwe describe how MT deletion errors arise andhow we can detect them, and finally we describedetection of other types of errors.4.1 Deletion in MTThe simplest case of content word deletion isa complete deletion by the translation model?
in other words, a token was not translated.We assume the MT system produces word orphrase alignments, so this case can be detectedby checking for a null alignment.
However, itis necessary to distinguish correct deletion fromspurious deletion.
Some content words do notneed to be translated ?
for example the Arabiccopular verb ?kAn?
(?to be?)
is often correctlydeleted when translating into English.A more subtle form of content word deletionoccurs when a content word is translated as anon-content word.
This can be detected by com-paring the parts of speech of aligned words.
Con-sider the production MT System (Prod.
MT) ex-ample in Table 1: the verb ?tHdv?1 (?spoke?)
hasbeen translated as the expletive ?there.
?Finally, another case of content word deletionoccurs when a content word is translated as partof a larger MT phrase, but the content word isnot translated.
In the second example in Table 1,an Arabic phrase consisting of a noun and prepo-sition is translated as just the preposition ?to.
?1Arabic examples in this paper are shown in Buckwaltertransliteration (Buckwalter, 2002).948The latter two kinds of content word deletionare considered mistranslations rather than dele-tions by the translation model, since the deletedsource-language token does produce one or moretarget-language tokens.
However, from the per-spective of a cross-lingual application, there wasa deletion, since some content that was presentin the original is not present in the translation.4.2 Detecting Deleted Content WordsThe deletion detection algorithm is motivatedby the assumption that a source-languagephrase containing one or more meaning-bearingwords should produce a phrase with one ormore meaning-bearing words in the translation.
(Phrase refers to an n-gram rather than a syntac-tic phrase.)
Note that this does not assume a one-to-one correspondence between content words?
for example, translating the phrase ?spokeloudly?
as the single word ?yelled?
satisfies theassumption.
This hypothesis favors precisionover recall, since it may miss cases where twocontent words are incorrectly translated as a sin-gle content word (for instance, if ?coffee table?is translated as ?coffee?
).The algorithm takes as input POS tags in bothlanguages and word alignments produced by theMT system during translation.
The exact defi-nition of ?content word?
will depend upon thelanguage and POS tagset.
The system iteratesover all content words in the source sentence,and, for each word, checks whether it is alignedto one or more content words in the target sen-tence.
If it has no alignment, or is aligned toonly function words, the system reports an error.This rule-based approach has poor precision be-cause of content words that are correctly deleted.For example, in the sentence ?I am going towatch TV,?
?am?
and ?going?
are tagged asverbs, but may be translated as function words.To address this, frequent content words wereheuristically filtered using source-language IDF(inverse-document frequency) over the QA cor-pus.
The cut-off was tuned on a development set.This algorithm is a lightweight, language-independent and MT-system-independent wayto find errors in MT.
The only requirement isthat the MT system produce word or phrasealignments.
This algorithm generalizes Ma andMcKeown (2009) in several ways.
First, itdetects any deleted content words, rather thanjust verbs.
The previous work only addressescomplete deletions, where the deleted token hasa null alignment, whereas this approach findscases where content words are mistranslated asnon-content words.
Finally, this error detectionalgorithm is more fine-grained, since it is at theword level rather than the phrase level.4.3 Additional Error Detection HeuristicsFor the CLQA task, we extended our MT er-ror detection algorithm to handle two additionaltypes of MT errors, OOV words and NE mis-translations, and to rank the errors.
The pro-duction MT system was explicitly set to notdelete OOV words, so they were easy to detectas source-language words left in the target lan-guage.
The CLIR system was used to find occur-rences of query NEs in the corpus, and then wordalignments were used to extract the correspond-ing translations.
If the translations were not afuzzy match to the query, then it was flagged asa possible NE translation error.
For instance,in a query about al-Rishawi, the CLIR wouldreturn Arabic-language matches to the Arabicword Alry$Awy.
If the aligned English trans-lation was al-Ryshoui instead of al-Rishawi, itwould be flagged as an error.Even if the retranslation corrects the errorsin MT, if the sentences are not relevant, theywill have no impact on CLQA.
To account forrelevance, we implemented a bilingual bag-of-words matching model, and ranked sentenceswith more keyword matches to the query higher.Sentences with the same estimated relevancewere further sorted by potential impact of theMT error on the task.
Errors affecting NEs (ei-ther via source-language POS tagging or source-language NE recognition) were ranked highest,since our particular CLQA task is focused onNEs.
The final output of the algorithm is a list ofsentences with MT errors, ranked by relevanceto the query and importance of the error.9495 Experimental SetupWe begin by describing the MT systems, whichmotivate the need for time-constrained MT.
Thenwe describe the CLQA task and the baselineCLQA system, and finally how the error detec-tion algorithm is used by the CLQA system.5.1 MT SystemsBoth the research and production MT systemsused in our evaluation were based on DirectTranslation Model 2 (Ittycheriah and Roukos,2007), which uses a maximum entropy approachto extract minimal translation blocks (one-to-M phrases with optional variable slots) andtrain system parameters over a large number ofsource- and target-language features.
The re-search system incorporates many additional syn-tactic features and does a deeper (and slower)beam search, both of which cause it to be muchslower than the production system.
In addition,the research MT system filters the training datato match the test data, as is customary in MTevaluations, whereas the production system mustbe able to handle a wide range of input data.
Partof the reason for the slower running time is thatthe research system has to retrain; the advan-tage is that more test-specific training data canbe used to tailor the MT system to the input.Overall, the research MT system performs 4BLEU points better than the production MT sys-tem on a standard MT evaluation test corpus, butat a great cost: the production MT handles 5,000words per minute, while the research MT systemhandles 2 words per minute.
Using 50 machines,the production MT system could translate thecorpus in under 2 hours, whereas the researchMT system would take 170 days.
This vast dif-ference succinctly captures the motivation be-hind the time-constrained retranslation step.5.2 CLQA TaskThe CLQA task was designed for the DARPAGALE (Global Autonomous Language Exploita-tion) project.
The questions found are open-ended, non-factoid information needs.
There are22 question types, and each type has its ownrelevance guidelines.
For instance, one type is?Describe the election campaign of [PERSON],?and a question could be about Barack Obama.Queries are in English, the corpus is in Arabic,and the system must output comprehensible En-glish sentences that are relevant to the question.The Arabic corpus was created for the eval-uation and consists of four genres: formal text(72,677 documents), informal text (47,202 doc-uments), formal speech (50 hours), and informalspeech (80 hours).
The speech data was storysegmented and run through a speech recogni-tion system before translation.
We used 31 textqueries developed by the Linguistic Data Con-sortium (LDC), and 39 speech queries developedby other researchers working on the CLQA task.5.3 CLQA SystemThe baseline CLQA system translates the fullcorpus offline before running further processingon the translated sentences (parsing, NE recog-nition, information extraction, etc.)
and index-ing the corpus.
At query-time, CLIR (imple-mented with Apache Lucene) returns documentsrelevant to the query, and the CLQA answer ex-traction system is run over the translated doc-uments.
The answer extraction system relieson target-language annotations, but any MT er-rors will propagate to target-language process-ing, and therefore affect answer extraction.5.4 CLQA System with MT ErrorDetectionThe error detection and retranslation module wasadded to the baseline system after CLIR, but be-fore answer extraction.
The inputs to the de-tection algorithm are the query and a list ofranked documents returned by CLIR.
The detec-tion algorithm has access to the indexed (bilin-gual) corpus, source- and target-language anno-tations (POS tagging and NE recognition), andMT word alignments.
The error detection algo-rithm has two stages: first it runs over sentencesin documents related to the query, and after itfinds 2k sentences with errors (or exhausts thedocument list), it reranks the errors as describedin section 4.3 and retranslates the top k=25 sen-tences.
Then the merged set of original and re-translated relevant sentences are passed to the950answer extraction module.By doing retranslation before answer extrac-tion, the algorithm has the potential to improveprecision and recall.
An improved translation ofa relevant Arabic sentence is more likely to beselected by the answer extraction system and in-crease recall, as in Boschee et al (2010), whereanswers were missed due to mistranslation.
Abetter translation of a relevant sentence is alsomore likely to be perceived as relevant, as shownby Ma and McKeown (2009).6 EvaluationAmazon Mechanical Turk (AMT) was used toconduct a large-scale evaluation of the impactof error detection and retranslation on relevance.An intrinsic evaluation of the error detection wasrun on a subset of the sentences, since it requiredbilingual annotators.6.1 Task-Based EvaluationEach sentence was annotated in the productionMT version and the research MT version.
Theannotators were first presented with templaterelevance guidelines and an example question,along with 3 ?
4 example sentences and expectedjudgments.
Then the actual question was pre-sented to the annotator, along with 5 sentences(all from a single MT system).
For each sen-tence, the annotators were first asked to judgeperceived adequacy and then relevance.The perceived adequacy rating was looselybased upon MT adequacy evaluations ?
in otherwords, annotators were told to ignore grammati-cal errors and focus on perceived meaning.
How-ever, since there were no reference translations,annotators were asked to rate how much of thesentence they believed they understood by se-lecting one of (All, More than half, About half,# detected errors # detected errorsGenre per sentence per 1,000 tokensNewswire 0.16 56Broadcast 0.23 105newsBroadcast 0.14 84conversationTable 2: Number of errors detected across differ-ent genres.Less than half, and None).The relevance rating was based on the tem-plate relevance guidelines, and annotators couldselect one of (Relevant, Maybe relevant, Not rel-evant, Can?t tell due to bad translation and Can?ttell due to other reason).6.2 Amazon Mechanical Turk (AMT)The evaluation was run on AMT, which has beenextensively used in NLP and has been shown tohave high correlation with expert annotators onmany NLP tasks at a lower cost (Snow et al,2008).
It has also been used in MT evaluation(Callison-Burch, 2009), though that evaluationused reference translations.For 70 queries, the top 25 ranked sentencesin both the production and research MT versionswere evaluated.
Each sentence was judged forboth relevance and perceived adequacy by 5 an-notators, for a total of 35,000 individual judg-ments.
As is standard, some of the judgmentswere filtered due to noise by using the percentof time that an annotator disagreed with all otherannotators, and the relative time spent on a givenannotation.
The percent of sentences with ma-jority agreement was 91% for relevance and 72%for perceived adequacy.6.3 Intrinsic EvaluationAnnotators were presented with an Arabic sen-tence with a single token highlighted, and askedwhether the token was a ?content word?
or not.Then annotators were asked to decide which oftwo translations (in random order) translated thehighlighted Arabic word best, or whether theywere equal.
In total, 150 sentences were judgedby annotators with knowledge of Arabic.
Forboth questions, kappa agreement was moderate.7 ResultsTable 2 shows how many errors were foundby the error detection algorithm for each genre.Not surprisingly, more errors are detected in thespeech genres (84 and 105 errors per 1,000 to-kens) than in formal text (56 errors per 1,000tokens).
We attribute the large difference be-tween broadcast news and broadcast conversa-951Perceived Adequacy Res.MT Prod.
MT12345678910111213141516171819202122232425RankRelevanceProd.
MTRankFigure 1: Average normalized cumulative sen-tence perceived adequacy and relevance versusrank of the sentence, by the ranking heuristic.tion to the large number of short sentences with-out content words in informal speech (such as?hello?, ?thank you?, etc.
).7.1 Perceived MT AdequacyThe research MT significantly outperformed theproduction MT in perceived adequacy (accord-ing to ANOVA with p=0.001).
Of the productionMT translations, 58% were considered ?morethan half?
or ?all?
understandable, whereas 69%of the research MT were.
Overall, retranslationincreased perceived adequacy in 17% of the sen-tences, and decreased it in only 5% of sentences.7.2 Ranking AlgorithmFigure 1 show the average cumulative sentencerelevance and perceived adequacy, as ranked bythe error detection algorithm.
In other words, ateach rank i, the average relevance (or perceivedadequacy) of sentences (1 ?
i) was calculated.On the perceived adequacy chart, the researchMT system consistently outperforms the produc-tion MT system by a statistically significant mar-gin.
For relevance, the research MT curve is onlymarginally higher than the production MT curve.The shape of the relevance curves shows thatranking sentences by a simple bilingual bag-of-words model did affect sentence relevance, sincesentences that are higher ranked have higher cu-mulative average relevance.
By ranking sen-tences with a basic relevance model, we wereable to focus the scarce MT resources on sen-Relevance?
Same ?
No maj./Don?t knowMT ?
20 201 9 56 17%MT same 93 919 72 212 78%MT ?
2 56 4 28 5%7% 70% 5% 18%Table 3: The relationship between changes inperceived adequacy and changes in relevance.tences that are most likely to help the CLQAtask.
This underscores the importance of usingthe task context to guide MT error detection, es-pecially in the case of time-constrained MT.7.3 CLQA RelevanceAnnotators judged 14.5% of the production MTsentences relevant.
After retranslation, the over-all number of sentences considered relevant in-creased to 14.7%.
Although the overall numbersare similar, the relevance of many individual sen-tences did change.
Table 3 shows the results ofcomparing annotations on the original MT withannotations on the retranslated MT.
Relevancewas classified as ?
or ?
by comparing the ma-jority judgment of the production MT to the re-search MT.
Changes in MT were based on com-paring the average rating of both versions, witha tolerance of 1.0.Of the sentences with better perceived MT,7% increased in relevance, and 3% decreased inrelevance.
When the retranslated sentence wasconsidered worse, there was a 2% increased inrelevance and a 4% decrease.
In other words,when retranslation had a positive effect, it moreoften led to increased relevance.
However, theimpact of retranslation was mixed, and none ofthe changes was statistically significant.7.4 Intrinsic EvaluationWhile the extrinsic evaluation focused on the im-pact on CLQA relevance, the goal of the intrinsicevaluation was to measure the precision of theerror detection algorithm, and whether retransla-tion addressed the detected errors.Of the 82% of sentences where both judgesagreed, 89% of the detected errors were con-sidered content words.
All of the OOV tokenswere content words (except for one disagree-952ment).
Surprisingly, for the errors involving con-tent words, 60% of the time both systems werejudged the same with regard to the highlightederror.
The research system was better 39% of thetime, and the original was better only 1% of thetime (excluding 26% disagreements).8 DiscussionThe CLQA evaluation was based on three hy-potheses:?
That we could detect errors in MT with highprecision.?
That retranslating errorful sentences with amuch better MT system would correct theerrors we detected.?
That correcting errors would cause somesentences to become relevant which werenot previously relevant, as in (Ma and McK-eown, 2009).The intrinsic evaluation confirmed that we canidentify content word deletions in MT with highprecision, thus validating the first hypothesis.However, detecting the errors and retranslat-ing them did not lead to large improvements inCLQA relevance ?
the impact of increased per-ceived adequacy on relevance was mixed and notsignificant.
The intrinsic evaluation explains thisnegative result: even though the retranslated sen-tences were judged significantly better, the re-translation only corrected the detected error 39%of the time.
In other words, the better researchMT system was making many of the same mis-takes as the production MT system, despite us-ing syntactic features and a much deeper searchspace during decoding.
Since the second hypoth-esis did not hold, we need to improve our errorcorrection algorithm before we can tell whetherthe third hypothesis holds.This result directly motivates the need for tar-geted error correction of MT.
Automatic MTpost-editing has been successfully used for se-lecting determiners (Knight and Chander, 1994),reinserting deleted verbs (Ma and McKeown,2009), correcting NE translations (Parton et al,2008), and lexical substitutions (Elming, 2006).Since Arabic and English word order differsignificantly, straightforward re-insertion of thedeleted words is not sufficient for error correc-tion, so we are currently working on more so-phisticated post-editing techniques.9 ConclusionsWe presented a novel online algorithm for de-tecting MT errors in the context of a question,and a heuristic for ranking MT errors by theirpotential impact on the CLQA task.
The er-ror detection algorithm focused on content worddeletion, which has previously been shown to bea significant problem in SMT.
The algorithm isgenerally applicable to any MT system that pro-duces word or phrase alignments for its outputand any language pair that can be POS-tagged,and it is more fine-grained and covers more typesof errors than previous work.
It was able to de-tect errors in Arabic-English MT across multipletext and speech genres, and the intrinsic evalu-ation showed that the large majority of tokensflagged as errors were indeed content words.The large-scale CLQA evaluation confirmedthat the slower research MT system was signif-icantly better than the production MT system.Relevance judgments showed that the rankingcomponent was crucial for directing scarce MTresources wisely, as the higher-ranked sentenceswere most likely to be relevant to the query, andtherefore most likely to benefit the CLQA sys-tem by being retranslated.Although we correctly identified MT errors,retranslating the sentences with the errors had anegligible effect on CLQA relevance.
This un-expected result may be explained by the fact thatonly 39% of the errors were actually correctedby the research MT system, so re-translation wasnot a good approach for error correction.
Weare currently working on correcting content worddeletion in MT via post-editing.Acknowledgments The authors are grateful toRadu Florian, Salim Roukos, Vittorio Castelli,Dan Bikel and the whole GALE IBM team forproviding the experimental testbed, including theCLQA and MT systems.
This research was par-tially supported by DARPA grant HR0011-08-C-0110.953ReferencesBoschee, Elizabeth, Marjorie Freedman, RogerBock, John Graettinger, and Ralph Weischedel.2010.
Error analysis and future directions for dis-tillation.
In GALE book (in preparation).Buckwalter, Tim.
2002.
Buckwalter arabic mor-phological analyzer.
Linguistic Data Consortium.
(LDC2002L49).Callison-Burch, Chris.
2009.
Fast, cheap, and cre-ative: evaluating translation quality using ama-zon?s mechanical turk.
In EMNLP ?09, pages 286?295, Morristown, NJ, USA.
Association for Com-putational Linguistics.Daume?, III, Hal and Daniel Marcu.
2006.
Bayesianquery-focused summarization.
In ACL, pages305?312, Morristown, NJ, USA.
Association forComputational Linguistics.Elming, Jakob.
2006.
Transformation-based correc-tions of rule-based mt.
In EAMT-2006: 11th An-nual Conference of the European Association forMachine Translation, pages 219?226.Hermjakob, Ulf, Kevin Knight, and Hal Daume?
III.2008.
Name translation in statistical machinetranslation - learning when to transliterate.
InProceedings of ACL-08: HLT, pages 389?397,Columbus, Ohio, June.
Association for Computa-tional Linguistics.Ittycheriah, Abraham and Salim Roukos.
2007.
Di-rect translation model 2.
In Sidner, Candace L.,Tanja Schultz, Matthew Stone, and ChengXiangZhai, editors, HLT-NAACL, pages 57?64.
The As-sociation for Computational Linguistics.Ji, Heng and Ralph Grishman.
2007.
Collaborativeentity extraction and translation.
In InternationalConference on Recent Advances in Natural Lan-guage Processing.Kirchhoff, Katrin, Owen Rambow, Nizar Habash,and Mona.
Diab.
2007.
Semi-automatic erroranalysis for large-scale statistical machine trans-lation systems.
In Proceedings of the MachineTranslation Summit IX (MT-Summit IX).Knight, Kevin and Ishwar Chander.
1994.
Auto-mated postediting of documents.
In AAAI, pages779?784.Li, Chi-Ho, Dongdong Zhang, Mu Li, Ming Zhou,and Hailei Zhang.
2008.
An empirical studyin source word deletion for phrase-based statisti-cal machine translation.
In StatMT ?08: Proceed-ings of the Third Workshop on Statistical MachineTranslation, pages 1?8, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Ma, Wei-Yun and Kathleen McKeown.
2009.Where?s the verb?
: correcting machine transla-tion during question answering.
In ACL-IJCNLP?09: Proceedings of the ACL-IJCNLP 2009 Con-ference Short Papers, pages 333?336, Morristown,NJ, USA.
Association for Computational Linguis-tics.Menezes, Arul and Chris Quirk.
2008.
Syntacticmodels for structural word insertion and deletion.In EMNLP ?08, pages 735?744, Morristown, NJ,USA.
Association for Computational Linguistics.Papineni, Kishore, Salim Roukos, Todd Ward, andWei jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In ACL, pages311?318.Parton, Kristen, Kathleen R. McKeown, James Al-lan, and Enrique Henestroza.
2008.
Simultane-ous multilingual search for translingual informa-tion retrieval.
In CIKM 08, pages 719?728, NewYork, NY, USA.
ACM.Parton, Kristen, Kathleen R. McKeown, Bob Coyne,Mona T. Diab, Ralph Grishman, Dilek Hakkani-Tu?r, Mary Harper, Heng Ji, Wei Yun Ma, AdamMeyers, Sara Stolbach, Ang Sun, Gokhan Tur, WeiXu, and Sibel Yaman.
2009. Who, what, when,where, why?
: comparing multiple approaches tothe cross-lingual 5w task.
In ACL-IJCNLP ?09,pages 423?431, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Snow, Rion, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but is itgood?
: evaluating non-expert annotations for nat-ural language tasks.
In EMNLP ?08, pages 254?263, Morristown, NJ, USA.
Association for Com-putational Linguistics.Vilar, David, Jia Xu, Luis Fernando D?Haro, and Her-mann Ney.
2006.
Error analysis of machine trans-lation output.
In International Conference on Lan-guage Resources and Evaluation, pages 697?702,Genoa, Italy, May.Zhang, Yuqi, Evgeny Matusov, and Hermann Ney.2009.
Are unaligned words important for machinetranslation ?
In Conference of the European Asso-ciation for Machine Translation, pages 226?233,Barcelona, March.954
