Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 56?64,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsSpectral Learning of Refinement HMMsKarl Stratos1 Alexander M. Rush2 Shay B. Cohen1 Michael Collins11Department of Computer Science, Columbia University, New-York, NY 10027, USA2MIT CSAIL, Cambridge, MA, 02139, USA{stratos,scohen,mcollins}@cs.columbia.edu, srush@csail.mit.eduAbstractWe derive a spectral algorithm for learn-ing the parameters of a refinement HMM.This method is simple, efficient, and canbe applied to a wide range of supervisedsequence labeling tasks.
Like other spec-tral methods, it avoids the problem of lo-cal optima and provides a consistent esti-mate of the parameters.
Our experimentson a phoneme recognition task show thatwhen equipped with informative featurefunctions, it performs significantly betterthan a supervised HMM and competitivelywith EM.1 IntroductionConsider the task of supervised sequence label-ing.
We are given a training set where the j?thtraining example consists of a sequence of ob-servations x(j)1 ...x(j)N paired with a sequence oflabels a(j)1 ...a(j)N and asked to predict the cor-rect labels on a test set of observations.
Acommon approach is to learn a joint distribu-tion over sequences p(a1 .
.
.
aN , x1 .
.
.
xN ) as ahidden Markov model (HMM).
The downside ofHMMs is that they assume each label ai is inde-pendent of labels before the previous label ai?1.This independence assumption can be limiting,particularly when the label space is small.
To re-lax this assumption we can refine each label aiwith a hidden state hi, which is not observed inthe training data, and model the joint distribu-tion p(a1 .
.
.
aN , x1 .
.
.
xN , h1 .
.
.
hN ).
This re-finement HMM (R-HMM), illustrated in figure 1,is able to propagate information forward throughthe hidden state as well as the label.Unfortunately, estimating the parameters of anR-HMM is complicated by the unobserved hid-den variables.
A standard approach is to use theexpectation-maximization (EM) algorithm whicha1, h1 a2, h2 aN , hNx1 x2 xN(a)a1 a2 aNh1 h2 hNx1 x2 xN(b)Figure 1: (a) An R-HMM chain.
(b) An equivalentrepresentation where labels and hidden states areintertwined.has no guarantee of finding the global optimum ofits objective function.
The problem of local op-tima prevents EM from yielding statistically con-sistent parameter estimates: even with very largeamounts of data, EM is not guaranteed to estimateparameters which are close to the ?correct?
modelparameters.In this paper, we derive a spectral algorithm forlearning the parameters of R-HMMs.
Unlike EM,this technique is guaranteed to find the true param-eters of the underlying model under mild condi-tions on the singular values of the model.
The al-gorithm we derive is simple and efficient, relyingon singular value decomposition followed by stan-dard matrix operations.We also describe the connection of R-HMMsto L-PCFGs.
Cohen et al(2012) present a spec-tral algorithm for L-PCFG estimation, but thena?
?ve transformation of the L-PCFG model andits spectral algorithm to R-HMMs is awkward andopaque.
We therefore work through the non-trivialderivation the spectral algorithm for R-HMMs.We note that much of the prior work on spec-tral algorithms for discrete structures in NLP hasshown limited experimental success for this fam-ily of algorithms (see, for example, Luque et al2012).
Our experiments demonstrate empirical56success for the R-HMM spectral algorithm.
Thespectral algorithm performs competitively withEM on a phoneme recognition task, and is morestable with respect to the number of hidden states.Cohen et al(2013) present experiments with aparsing algorithm and also demonstrate it is com-petitive with EM.
Our set of experiments comes asan additional piece of evidence that spectral algo-rithms can function as a viable, efficient and moreprincipled alternative to the EM algorithm.2 Related WorkRecently, there has been a surge of interest in spec-tral methods for learning HMMs (Hsu et al 2012;Foster et al 2012; Jaeger, 2000; Siddiqi et al2010; Song et al 2010).
Like these previousworks, our method produces consistent parameterestimates; however, we estimate parameters for asupervised learning task.
Balle et al(2011) alsoconsider a supervised problem, but our model isquite different since we estimate a joint distribu-tion p(a1 .
.
.
aN , x1 .
.
.
xN , h1 .
.
.
hN ) as opposedto a conditional distribution and use feature func-tions over both the labels and observations of thetraining data.
These feature functions also go be-yond those previously employed in other spectralwork (Siddiqi et al 2010; Song et al 2010).
Ex-periments show that features of this type are cru-cial for performance.Spectral learning has been applied to relatedmodels beyond HMMs including: head automatafor dependency parsing (Luque et al 2012),tree-structured directed Bayes nets (Parikh et al2011), finite-state transducers (Balle et al 2011),and mixture models (Anandkumar et al 2012a;Anandkumar et al 2012b).Of special interest is Cohen et al(2012), whodescribe a derivation for a spectral algorithm forL-PCFGs.
This derivation is the main drivingforce behind the derivation of our R-HMM spec-tral algorithm.
For work on L-PCFGs estimatedwith EM, see Petrov et al(2006), Matsuzaki et al(2005), and Pereira and Schabes (1992).
Petrovet al(2007) proposes a split-merge EM procedurefor phoneme recognition analogous to that used inlatent-variable parsing.3 The R-HMM ModelWe decribe in this section the notation usedthroughout the paper and the formal details of R-HMMs.3.1 NotationWe distinguish row vectors from column vectorswhen such distinction is necessary.
We use asuperscript > to denote the transpose operation.We write [n] to denote the set {1, 2, .
.
.
, n} forany integer n ?
1.
For any vector v ?
Rm,diag(v) ?
Rm?m is a diagonal matrix with en-tries v1 .
.
.
vm.
For any statement S , we use [[S]]to refer to the indicator function that returns 1 if Sis true and 0 otherwise.
For a random variable X ,we use E[X] to denote its expected value.A tensor C ?
Rm?m?m is a set of m3 val-ues Ci,j,k for i, j, k ?
[m].
Given a vector v ?Rm, we define C(v) to be the m ?
m matrixwith [C(v)]i,j = ?k?[m]Ci,j,kvk.
Given vectorsx, y, z ?
Rm, C = xy>z> is anm?m?m tensorwith [C]i,j,k = xiyjzk.3.2 Definition of an R-HMMAn R-HMM is a 7-tuple ?l,m, n, pi, o, t, f?
for in-tegers l,m, n ?
1 and functions pi, o, t, f where?
[l] is a set of labels.?
[m] is a set of hidden states.?
[n] is a set of observations.?
pi(a, h) is the probability of generating a ?
[l] and h ?
[m] in the first position in thelabeled sequence.?
o(x|a, h) is the probability of generating x ?
[n], given a ?
[l] and h ?
[m].?
t(b, h?|a, h) is the probability of generatingb ?
[l] and h?
?
[m], given a ?
[l] andh ?
[m].?
f(?|a, h) is the probability of generating thestop symbol ?, given a ?
[l] and h ?
[m].See figure 1(b) for an illustration.
At any time stepof a sequence, a label a is associated with a hiddenstate h. By convention, the end of an R-HMMsequence is signaled by the symbol ?.For the subsequent illustration, let N be thelength of the sequence we consider.
A full se-quence consists of labels a1 .
.
.
aN , observationsx1 .
.
.
xN , and hidden states h1 .
.
.
hN .
The modelassumesp(a1 .
.
.
aN , x1 .
.
.
xN , h1 .
.
.
hN ) = pi(a1, h1)?N?i=1o(xi|ai, hi)?N?1?i=1t(ai+1, hi+1|ai, hi)?
f(?|aN , hN )57Input: a sequence of observations x1 .
.
.
xN ; operators?Cb|a, C?|a, c1a, cax?Output: ?
(a, i) for all a ?
[l] and i ?
[N ][Forward case]?
?1a ?
c1a for all a ?
[l].?
For i = 1 .
.
.
N ?
1?i+1b ??a?[l]Cb|a(caxi)?
?ia for all b ?
[l][Backward case]?
?N+1a ?
C?|a(caxN ) for all a ?
[l]?
For i = N .
.
.
1?ia ??b?
[l]?i+1b ?
Cb|a(caxi) for all a ?
[l][Marginals]?
?
(a, i)?
?ia ?
?ia for all a ?
[l], i ?
[N ]Figure 2: The forward-backward algorithmA skeletal sequence consists of labels a1 .
.
.
aNand observations x1 .
.
.
xN without hidden states.Under the model, it has probabilityp(a1 .
.
.
aN , x1 .
.
.
xN )=?h1...hNp(a1 .
.
.
aN , x1 .
.
.
xN , h1 .
.
.
hN )An equivalent definition of an R-HMM isgiven by organizing the parameters in matrixform.
Specifically, an R-HMM has parameters?pia, oax, T b|a, fa?
where pia ?
Rm is a columnvector, oax is a row vector, T b|a ?
Rm?m is a ma-trix, and fa ?
Rm is a row vector, defined for alla, b ?
[l] and x ?
[n].
Their entries are set to?
[pia]h = pi(a, h) for h ?
[m]?
[oax]h = o(x|a, h) for h ?
[m]?
[T b|a]h?,h = t(b, h?|a, h) for h, h?
?
[m]?
[fa]h = f(?|a, h) for h ?
[m]4 The Forward-Backward AlgorithmGiven an observation sequence x1 .
.
.
xN , we wantto infer the associated sequence of labels underan R-HMM.
This can be done by computing themarginals of x1 .
.
.
xN?
(a, i) =?a1...aN : ai=ap(a1 .
.
.
aN , x1 .
.
.
xN )for all labels a ?
[l] and positions i ?
[N ].
Thenthe most likely label at each position i is given bya?i = arg maxa?
[l] ?
(a, i)The marginals can be computed using a tensorvariant of the forward-backward algorithm, shownin figure 2.
The algorithm takes additional quanti-ties ?Cb|a, C?|a, c1a, cax?
called the operators:?
Tensors Cb|a ?
Rm?m?m for a, b ?
[l]?
Tensors C?|a ?
R1?m?m for a ?
[l]?
Column vectors c1a ?
Rm for a ?
[l]?
Row vectors cax ?
Rm for a ?
[l] and x ?
[n]The following proposition states that these opera-tors can be defined in terms of the R-HMM param-eters to guarantee the correctness of the algorithm.Proposition 4.1.
Given an R-HMM with param-eters ?pia, oax, T b|a, fa?, for any vector v ?
Rmdefine the operators:Cb|a(v) = T b|adiag(v) c1a = piaC?|a(v) = fadiag(v) cax = oaxThen the algorithm in figure 2 correctly computesmarginals ?
(a, i) under the R-HMM.The proof is an algebraic verification and deferredto the appendix.
Note that the running time of thealgorithm as written is O(l2m3N).1Proposition 4.1 can be generalized to the fol-lowing theorem.
This theorem implies that the op-erators can be linearly transformed by some invert-ible matrices as long as the transformation leavesthe embedded R-HMM parameters intact.
Thisobservation is central to the derivation of the spec-tral algorithm which estimates the linearly trans-formed operators but not the actual R-HMM pa-rameters.Theorem 4.1.
Given an R-HMM with parameters?pia, oax, T b|a, fa?, assume that for each a ?
[l] wehave invertible m ?m matrices Ga and Ha.
Forany vector v ?
Rm define the operators:Cb|a(v) = GbT b|adiag(vHa)(Ga)?1 c1a = GapiaC?|a(v) = fadiag(vHa)(Ga)?1 cax = oax(Ha)?1Then the algorithm in figure 2 correctly computesmarginals ?
(a, i) under the R-HMM.The proof is similar to that of Cohen et al(2012).1We can reduce the complexity to O(l2m2N) by pre-computing the matricesCb|a(cax) for all a, b ?
[l] and x ?
[n]after parameter estimation.585 Spectral Estimation of R-HMMsIn this section, we derive a consistent estimator forthe operators ?Cb|a, C?|a, c1a, cax?
in theorem 4.1through the use of singular-value decomposition(SVD) followed by the method of moments.Section 5.1 describes the decomposition of theR-HMM model into random variables which areused in the final algorithm.
Section 5.2 can beskimmed through on the first reading, especiallyif the reader is familiar with other spectral algo-rithms.
It includes a detailed account of the deriva-tion of the R-HMM algorithm.For a first reading, note that an R-HMM se-quence can be seen as a right-branching L-PCFGtree.
Thus, in principle, one can convert a se-quence into a tree and run the inside-outside algo-rithm of Cohen et al(2012) to learn the parame-ters of an R-HMM.
However, projecting this trans-formation into the spectral algorithm for L-PCFGsis cumbersome and unintuitive.
This is analo-gous to the case of the Baum-Welch algorithm forHMMs (Rabiner, 1989), which is a special case ofthe inside-outside algorithm for PCFGs (Lari andYoung, 1990).5.1 Random VariablesWe first introduce the random variables un-derlying the approach then describe the opera-tors based on these random variables.
Fromp(a1 .
.
.
aN , x1 .
.
.
xN , h1 .
.
.
hN ), we draw an R-HMM sequence (a1 .
.
.
aN , x1 .
.
.
xN , h1 .
.
.
hN )and choose a time step i uniformly at random from[N ].
The random variables are then defined asX = xiA1 = ai and A2 = ai+1 (if i = N , A2 = ?
)H1 = hi and H2 = hi+1F1 = (ai .
.
.
aN , xi .
.
.
xN ) (future)F2 = (ai+1 .
.
.
aN , xi+1 .
.
.
xN ) (skip-future)P = (a1 .
.
.
ai, x1 .
.
.
xi?1) (past)R = (ai, xi) (present)D = (a1 .
.
.
aN , x1 .
.
.
xi?1, xi+1 .
.
.
xN ) (destiny)B = [[i = 1]]Figure 3 shows the relationship between the ran-dom variables.
They are defined in such a waythat the future is independent of the past and thepresent is independent of the destiny conditioningon the current node?s label and hidden state.Next, we require a set of feature functions overthe random variables.?
?
maps F1, F2 to ?
(F1), ?
(F2) ?
Rd1 .a1 ai?1 ai ai+1 aNx1 xi?1 xi xi+1 xNPF1F2(a)a1 ai?1 ai ai+1 aNx1 xi?1 xi xi+1 xND R(b)Figure 3: Given an R-HMM sequence, we definerandom variables over observed quantities so thatconditioning on the current node, (a) the future F1is independent of the past P and (b) the present Ris independent of the density D.?
?
maps P to ?
(P ) ?
Rd2 .?
?
maps R to ?
(R) ?
Rd3 .?
?
maps D to ?
(D) ?
Rd4 .We will see that the feature functions should bechosen to capture the influence of the hiddenstates.
For instance, they might track the next la-bel, the previous observation, or important combi-nations of labels and observations.Finally, we require projection matrices?a ?
Rm?d1 ?a ?
Rm?d2?a ?
Rm?d3 ?a ?
Rm?d4defined for all labels a ?
[l].
These matriceswill project the feature vectors of ?, ?, ?, and ?from (d1, d2, d3, d4)-dimensional spaces to an m-dimensional space.
We refer to this reduced di-mensional representation by the following randomvariables:F 1 = ?A1?
(F1) (projected future)F 2 = ?A2?
(F2) (projected skip-future: if i = N , F 2 = 1)P = ?A1?
(P ) (projected past)R = ?A1?
(R) (projected present)D = ?A1?
(D) (projected destiny)Note that they are all vectors in Rm.595.2 Estimation of the OperatorsSince F 1, F 2, P , R, and D do not involve hid-den variables, the following quantities can be di-rectly estimated from the training data of skeletalsequences.
For this reason, they are called observ-able blocks:?a = E[F 1P>|A1 = a] ?a ?
[l]?a = E[R D>|A1 = a] ?a ?
[l]Db|a = E[[[A2 = b]]F 2P>R>|A1 = a] ?a, b ?
[l]dax = E[[[X = x]]D>|A1 = a] ?a ?
[l], x ?
[n]The main result of this paper is that under cer-tain conditions, matrices ?a and ?a are invert-ible and the operators ?Cb|a, C?|a, c1a, cax?
in the-orem 4.1 can be expressed in terms of these ob-servable blocks.Cb|a(v) = Db|a(v)(?a)?1 (1)C?|a(v) = D?|a(v)(?a)?1 (2)cax = dax(?a)?1 (3)c1a = E[[[A1 = a]]F 1|B = 1] (4)To derive this result, we use the following defini-tion to help specify the conditions on the expecta-tions of the feature functions.Definition.
For each a ?
[l], define matricesIa ?
Rd1?m, Ja ?
Rd2?m, Ka ?
Rd3?m,W a ?Rd4?m by[Ia]k,h = E[[?
(F1)]k|A1 = a,H1 = h][Ja]k,h = E[[?
(P )]k|A1 = a,H1 = h][Ka]k,h = E[[?
(R)]k|A1 = a,H1 = h][W a]k,h = E[[?
(D)]k|A1 = a,H1 = h]In addition, let ?a ?
Rm?m be a diagonal matrixwith [?a]h,h = P (H1 = h|A1 = a).We now state the conditions for the correctness ofEq.
(1-4).
For each label a ?
[l], we require thatCondition 6.1 Ia, Ja,Ka,W a have rank m.Condition 6.2 [?a]h,h > 0 for all h ?
[m].The conditions lead to the following proposition.Proposition 5.1.
Assume Condition 6.1 and 6.2hold.
For all a ?
[l], define matrices?a1 = E[?(F1)?
(P )>|A1 = a] ?
Rd1?d2?a2 = E[?(R)?
(D)>|A1 = a] ?
Rd3?d4Let ua1 .
.
.
uam ?
Rd1 and va1 .
.
.
vam ?
Rd2 be thetop m left and right singular vectors of ?a.
Sim-ilarly, let la1 .
.
.
lam ?
Rd3 and ra1 .
.
.
ram ?
Rd4 bethe top m left and right singular vectors of ?a.Define projection matrices?a = [ua1 .
.
.
uam]> ?a = [va1 .
.
.
vam]>?a = [la1 .
.
.
lam]> ?a = [ra1 .
.
.
ram]>Then the following m?m matricesGa = ?aIa Ga = ?aJaHa = ?aKa Ha = ?aW aare invertible.The proof resembles that of lemma 2 of Hsu et al(2012).
Finally, we state the main result that shows?Cb|a, C?|a, c1a, cax?
in Eq.
(1-4) using the projec-tions from proposition 5.1 satisfy theorem 4.1.
Asketch of the proof is deferred to the appendix.Theorem 5.1.
Assume conditions 6.1 and 6.2hold.
Let ??a,?a,?a,?a?
be the projection ma-trices from proposition 5.1.
Then the operators inEq.
(1-4) satisfy theorem 4.1.In summary, these results show that with theproper selection of feature functions, we can con-struct projection matrices ??a,?a,?a,?a?
to ob-tain operators ?Cb|a, C?|a, c1a, cax?
which satisfythe conditions of theorem 4.1.6 The Spectral Estimation AlgorithmIn this section, we give an algorithm to estimatethe operators ?Cb|a, C?|a, c1a, cax?
from samples ofskeletal sequences.
Suppose the training set con-sists of M skeletal sequences (a(j), x(j)) for j ?
[M ].
ThenM samples of the random variables canbe derived from this training set as follows?
At each j ?
[M ], choose a positionij uniformly at random from the positionsin (a(j), x(j)).
Sample the random vari-ables (X,A1, A2, F1, F2, P,R,D,B) usingthe procedure defined in section 5.1.This process yields M samples(x(j), a(j)1 , a(j)2 , f(j)1 , f(j)2 , p(j), r(j), d(j), b(j)) for j ?
[M ]Assuming (a(j), x(j)) are i.i.d.
draws fromthe PMF p(a1 .
.
.
aN , x1 .
.
.
xN ) over skeletal se-quences under an R-HMM, the tuples obtainedthrough this process are i.i.d.
draws from the jointPMF over (X,A1, A2, F1, F2, P,R,D,B).60Input: samples of (X,A1, A2, F1, F2, P,R,D,B); featurefunctions ?, ?, ?, and ?
; number of hidden states mOutput: estimates?C?b|a, C?
?|a, c?1a, c?ax?of the operatorsused in algorithm 2[Singular Value Decomposition]?
For each label a ?
[l], compute empirical estimates of?a1 = E[?(F1)?
(P )>|A1 = a]?a2 = E[?(R)?
(D)>|A1 = a]and obtain their singular vectors via an SVD.
Usethe top m singular vectors to construct projections??
?a, ?
?a, ?
?a, ??a?.
[Sample Projection]?
Project (d1, d2, d3, d4)-dimensional samples of(?
(F1), ?
(F2), ?
(P ), ?
(R), ?
(D))with matrices??
?a, ?
?a, ?
?a, ?
?a?to obtain m-dimensional samples of(F 1, F 2, P ,R,D)[Method of Moments]?
For each a, b ?
[l] and x ?
[n], compute empiricalestimates??
?a, ?
?a, D?b|a, d?ax?of the observable blocks?a = E[F 1P>|A1 = a]?a = E[R D>|A1 = a]Db|a = E[[[A2 = b]]F 2P>R>|A1 = a]dax = E[[[X = x]]D>|A1 = a]and also c?1a = E[[[A1 = a]]F 1|B = 1].
Finally, setC?b|a(v)?
D?b|a(v)(??a)?1C??|a(v)?
D??|a(v)(?
?a)?1c?ax ?
d?ax(?
?a)?1Figure 4: The spectral estimation algorithmThe algorithm in figure 4 shows how to deriveestimates of the observable representations fromthese samples.
It first computes the projectionmatrices ??a,?a,?a,?a?
for each label a ?
[l]by computing empirical estimates of ?a1 and ?a2in proposition 5.1, calculating their singular vec-tors via an SVD, and setting the projections interms of these singular vectors.
These projectionmatrices are then used to project (d1, d2, d3, d4)-0 5 10 15 20 25 30hidden states (m)54.054.555.055.556.056.557.057.5accuracySpectralEMFigure 5: Accuracy of the spectral algorithm andEM on TIMIT development data for varying num-bers of hidden states m. For EM, the highest scor-ing iteration is shown.dimensional feature vectors(?
(f (j)1 ), ?
(f(j)2 ), ?
(p(j)), ?
(r(j)), ?
(d(j)))down to m-dimensional vectors(f (j)1 , f(j)2 , p(j), r(j), d(j))for all j ?
[M ].
It then computes correlationbetween these vectors in this lower dimensionalspace to estimate the observable blocks which areused to obtain the operators as in Eq.
(1-4).
Theseoperators can be used in algorithm 2 to computemarginals.As in other spectral methods, this estimation al-gorithm is consistent, i.e., the marginals ??
(a, i)computed with the estimated operators approachthe true marginal values given more data.
Fordetails, see Cohen et al(2012) and Foster et al(2012).7 ExperimentsWe apply the spectral algorithm for learningR-HMMs to the task of phoneme recognition.The goal is to predict the correct sequence ofphonemes a1 .
.
.
aN for a given a set of speechframes x1 .
.
.
xN .
Phoneme recognition is oftenmodeled with a fixed-structure HMM trained withEM, which makes it a natural application for spec-tral training.We train and test on the TIMIT corpus of spokenlanguage utterances (Garofolo and others, 1988).The label set consists of l = 39 English phonemesfollowing a standard phoneme set (Lee and Hon,1989).
For training, we use the sx and si utter-ances of the TIMIT training section made up of61?
(F1) ai+1 ?
xi, ai+1, xi, np(ai .
.
.
aN )?
(P ) (ai?1, xi?1), ai?1, xi?1, pp(a1 .
.
.
ai)?
(R) xi?
(D) ai?1 ?
xi?1, ai?1, xi?1, pp(a1 .
.
.
ai),pos(a1 .
.
.
aN )iy r r r r r r ow .
.
.. .
.pp b m e npFigure 6: The feature templates for phonemerecognition.
The simplest features look only at thecurrent label and observation.
Other features in-dicate the previous phoneme type used before ai(pp), the next phoneme type used after ai (np),and the relative position (beginning, middle, orend) of ai within the current phoneme (pos).
Thefigure gives a typical segment of the phoneme se-quence a1 .
.
.
aNM = 3696 utterances.
The parameter estimate issmoothed using the method of Cohen et al(2013).Each utterance consists of a speech signalaligned with phoneme labels.
As preprocessing,we divide the signal into a sequence of N over-lapping frames, 25ms in length with a 10ms stepsize.
Each frame is converted to a feature repre-sentation using MFCC with its first and secondderivatives for a total of 39 continuous features.To discretize the problem, we apply vector quanti-zation using euclidean k-means to map each frameinto n = 10000 observation classes.
After pre-processing, we have 3696 skeletal sequence witha1 .
.
.
aN as the frame-aligned phoneme labels andx1 .
.
.
xN as the observation classes.For testing, we use the core test portion ofTIMIT, consisting of 192 utterances, and for de-velopment we use 200 additional utterances.
Ac-curacy is measured by the percentage of frameslabeled with the correct phoneme.
During infer-ence, we calculate marginals ?
for each label ateach position i and choose the one with the highestmarginal probability, a?i = arg maxa?
[l] ?
(a, i).The spectral method requires defining featurefunctions ?, ?, ?, and ?.
We use binary-valuedfeature vectors which we specify through featurestemplates, for instance the template ai ?
xi corre-sponds to binary values for each possible label andoutput pair (ln binary dimensions).Figure 6 gives the full set of templates.
Thesefeature functions are specially for the phonemelabeling task.
We note that the HTK baselineexplicitly models the position within the currentMethod AccuracyEM(4) 56.80EM(24) 56.23SPECTRAL(24), no np, pp, pos 55.45SPECTRAL(24), no pos 56.56SPECTRAL(24) 56.94Figure 7: Feature ablation experiments on TIMITdevelopment data for the best spectral model (m =24) with comparisons to the best EM model (m =4) and EM with m = 24.Method AccuracyUNIGRAM 48.04HMM 54.08EM(4) 55.49SPECTRAL(24) 55.82HTK 55.70Figure 8: Performance of baselines and spectralR-HMM on TIMIT test data.
Number of hiddenstates m optimized on development data (see fig-ure 5).
The improvement of the spectral methodover the EM baseline is significant at the p ?
0.05level (and very close to significant at p ?
0.01,with a precise value of p ?
0.0104).phoneme as part of the HMM structure.
The spec-tral method is able to encode similar informationnaturally through the feature functions.We implement several baseline for phonemerecognition: UNIGRAM chooses the most likelylabel, arg maxa?
[l] p(a|xi), at each position;HMM is a standard HMM trained with maximum-likelihood estimation; EM(m) is an R-HMMwith m hidden states estimated using EM; andSPECTRAL(m) is an R-HMM with m hiddenstates estimated with the spectral method de-scribed in this paper.
We also compare to HTK,a fixed-structure HMM with three segments perphoneme estimated using EM with the HTKspeech toolkit.
See Young et al(2006) for moredetails on this method.An important consideration for both EM and thespectral method is the number of hidden states min the R-HMM.
More states allow for greater labelrefinement, with the downside of possible overfit-ting and, in the case of EM, more local optima.To determine the best number of hidden states, weoptimize both methods on the development set fora range of m values between 1 to 32.
For EM,62we run 200 training iterations on each value of mand choose the iteration that scores best on the de-velopment set.
As the spectral algorithm is non-iterative, we only need to evaluate the develop-ment set once per m value.
Figure 5 shows thedevelopment accuracy of the two method as weadjust the value of m. EM accuracy peaks at 4hidden states and then starts degrading, whereasthe spectral method continues to improve until 24hidden states.Another important consideration for the spectralmethod is the feature functions.
The analysis sug-gests that the best feature functions are highly in-formative of the underlying hidden states.
To testthis empirically we run spectral estimation with areduced set of features by ablating the templatesindicating adjacent phonemes and relative posi-tion.
Figure 7 shows that removing these featuresdoes have a significant effect on development ac-curacy.
Without either type of feature, develop-ment accuracy drops by 1.5%.We can interpret the effect of the features ina more principled manner.
Informative featuresyield greater singular values for the matrices ?a1and ?a2, and these singular values directly affectthe sample complexity of the algorithm; see Cohenet al(2012) for details.
In sum, good feature func-tions lead to well-conditioned ?a1 and ?a2, which inturn require fewer samples for convergence.Figure 8 gives the final performance for thebaselines and the spectral method on the TIMITtest set.
For EM and the spectral method, weuse best performing model from the develop-ment data, 4 hidden states for EM and 24 forthe spectral method.
The experiments show thatR-HMM models score significantly better than astandard HMM and comparatively to the fixed-structure HMM.
In training the R-HMM models,the spectral method performs competitively withEM while avoiding the problems of local optima.8 ConclusionThis paper derives a spectral algorithm for thetask of supervised sequence labeling using an R-HMM.
Unlike EM, the spectral method is guar-anteed to provide a consistent estimate of the pa-rameters of the model.
In addition, the algorithmis simple to implement, requiring only an SVDof the observed counts and other standard ma-trix operations.
We show empirically that whenequipped with informative feature functions, thespectral method performs competitively with EMon the task of phoneme recognition.AppendixProof of proposition 4.1.
At any time step i ?
[N ] in the al-gorithm in figure 2, for all label a ?
[l] we have a columnvector ?ia ?
Rm and a row vector ?ia ?
Rm.
The value ofthese vectors at each index h ?
[m] can be verified as[?ia]h =?a1...ai,h1...hi:ai=a,hi=hp(a1 .
.
.
ai, x1 .
.
.
xi?1, h1 .
.
.
hi)[?ia]h =?ai...aN ,hi...hN :ai=a,hi=hp(ai+1 .
.
.
aN , xi .
.
.
xN , hi+1 .
.
.
hN |ai, hi)Thus ?ia?ia is a scalar equal to?a1...aN ,h1...hN :ai=ap(a1 .
.
.
aN , x1 .
.
.
xN , h1 .
.
.
hN )which is the value of the marginal ?
(a, i).Proof of theorem 5.1.
It can be verified that c1a = Gapia.
Forthe others, under the conditional independence illustrated infigure 3 we can decompose the observable blocks in terms ofthe R-HMM parameters and invertible matrices?a = Ga?a(Ga)> ?a = Ha?a(Ha)>Db|a(v) = GbT b|adiag(vHa)?a(Ga)>D?|a(v) = fadiag(vHa)?a(Ga)> dax = oax?a(Ha)>using techniques similar to those sketched in Cohen et al(2012).
By proposition 5.1, ?a and ?a are invertible, andthese observable blocks yield the operators that satisfy theo-rem 4.1 when placed in Eq.
(1-3).ReferencesA.
Anandkumar, D. P. Foster, D. Hsu, S.M.
Kakade, and Y.K.Liu.
2012a.
Two svds suffice: Spectral decompositionsfor probabilistic topic modeling and latent dirichlet al-cation.
Arxiv preprint arXiv:1204.6703.A.
Anandkumar, D. Hsu, and S.M.
Kakade.
2012b.
Amethod of moments for mixture models and hiddenmarkov models.
Arxiv preprint arXiv:1203.0683.B.
Balle, A. Quattoni, and X. Carreras.
2011.
A spectrallearning algorithm for finite state transducers.
MachineLearning and Knowledge Discovery in Databases, pages156?171.S.
B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Un-gar.
2012.
Spectral learning of latent-variable PCFGs.
InProceedings of the 50th Annual Meeting of the Associationfor Computational Linguistics.
Association for Computa-tional Linguistics.S.
B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Un-gar.
2013.
Experiments with spectral learning of latent-variable pcfgs.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association for Com-putational Linguistics: Human Language Technologies.63D.
P. Foster, J. Rodu, and L.H.
Ungar.
2012.
Spec-tral dimensionality reduction for hmms.
Arxiv preprintarXiv:1203.6130.J.
S. Garofolo et al1988.
Getting started with the darpatimit cd-rom: An acoustic phonetic continuous speechdatabase.
National Institute of Standards and Technology(NIST), Gaithersburgh, MD, 107.D.
Hsu, S.M.
Kakade, and T. Zhang.
2012.
A spectral al-gorithm for learning hidden markov models.
Journal ofComputer and System Sciences.H.
Jaeger.
2000.
Observable operator models for discretestochastic time series.
Neural Computation, 12(6):1371?1398.K.
Lari and S. J.
Young.
1990.
The estimation of stochasticcontext-free grammars using the inside-outside algorithm.Computer speech & language, 4(1):35?56.K.F.
Lee and H.W.
Hon.
1989.
Speaker-independent phonerecognition using hidden markov models.
Acoustics,Speech and Signal Processing, IEEE Transactions on,37(11):1641?1648.F.
M. Luque, A. Quattoni, B. Balle, and X. Carreras.
2012.Spectral learning for non-deterministic dependency pars-ing.
In EACL, pages 409?419.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilistic cfgwith latent annotations.
In Proceedings of the 43rd An-nual Meeting on Association for Computational Linguis-tics, pages 75?82.
Association for Computational Linguis-tics.A.
Parikh, L. Song, and E.P.
Xing.
2011.
A spectral algo-rithm for latent tree graphical models.
In Proceedings ofthe 28th International Conference on Machine Learning.F.
Pereira and Y. Schabes.
1992.
Inside-outside reestima-tion from partially bracketed corpora.
In Proceedingsof the 30th annual meeting on Association for Computa-tional Linguistics, pages 128?135.
Association for Com-putational Linguistics.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.
Learn-ing accurate, compact, and interpretable tree annotation.In Proceedings of the 21st International Conference onComputational Linguistics and the 44th annual meetingof the Association for Computational Linguistics, pages433?440.
Association for Computational Linguistics.Slav Petrov, Adam Pauls, and Dan Klein.
2007.
Learn-ing structured models for phone recognition.
In Proc.
ofEMNLP-CoNLL.L.
R. Rabiner.
1989.
A tutorial on hidden markov modelsand selected applications in speech recognition.
Proceed-ings of the IEEE, 77(2):257?286.S.
Siddiqi, B.
Boots, and G. J. Gordon.
2010.
Reduced-rank hidden Markov models.
In Proceedings of the Thir-teenth International Conference on Artificial Intelligenceand Statistics (AISTATS-2010).L.
Song, B.
Boots, S. Siddiqi, G. Gordon, and A. Smola.2010.
Hilbert space embeddings of hidden markov mod-els.
In Proceedings of the 27th International Conferenceon Machine Learning.
Citeseer.S.
Young, G. Evermann, M. Gales, T. Hain, D. Kershaw,XA Liu, G. Moore, J. Odell, D. Ollason, D. Povey, et al2006.
The htk book (for htk version 3.4).64
