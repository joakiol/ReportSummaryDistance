Lexicalized Grammar 101Matthew StoneDepartment of Computer Science and Center for Cognitive ScienceRutgers, the State University of New JerseyPiscataway NJ 08854-8019 USAhttp://www.cs.rutgers.edu/?mdstonemdstone@cs.rutgers.eduAbstractThis paper presents a simple and ver-satile tree-rewriting lexicalized grammarformalism, TAGLET, that provides an ef-fective scaffold for introducing advancedtopics in a survey course on natural lan-guage processing (NLP).
Students whoimplement a strong competence TAGLETparser and generator simultaneously getexperience with central computer scienceideas and develop an effective startingpoint for their own subsequent projects indata-intensive and interactive NLP.1 IntroductionThis paper is particularly addressed to readers at in-stitutions whose resources and organization rule outextensive formal course-work in natural languageprocessing (NLP).
This is typical at universities inNorth America.
In such places, NLP teaching mustbe ambitious but focused; courses must quickly ac-quaint a broad range of students to the essentialconcepts of the field and sell them on its currentresearch opportunities and challenges.
This paperpresents one resource that may help.
Specifically,I outline a simple and versatile lexicalized formal-ism for natural language syntax, semantics and prag-matics, called TAGLET, and draw on my experi-ence with CS 533 (NLP) at Rutgers to motivate thepotential role for TAGLET in a broad NLP classwhose emphasis is to introduce topics of current re-search.
Notes, assignments and implementations forTAGLET are available on the web.I begin in Section 2 by describing CS 533?situating the course within the university and outlin-ing its topics, audience and goals.
I then describe thespecific goals for teaching and implementing gram-mar formalisms within such a course, in Section 3.Section 4 gives an informal overview of TAGLET,and the algorithms, specifications and assignmentsthat fit TAGLET into a broad general NLP class.In brief, TAGLET is a context-free tree-rewritingformalism, defined by the usual complementationoperation and the simplest imaginable modifica-tion operation.
By implementing a strong compe-tence TAGLET parser and generator students simul-taneously get experience with central computer sci-ence ideas?data structures, unification, recursionand abstraction?and develop an effective startingpoint for their own subsequent projects.
Two note-worthy directions are the construction of interac-tive applications, where TAGLET?s relatively scal-able and reversible processing lets students easilyexplore cutting-edge issues in dialogue semanticsand pragmatics, and the development of linguisticspecifications, where TAGLET?s ability to lexical-ize tree-bank parses introduces a modern perspec-tive of linguistic intuitions and annotations as pro-grams.
Section 5 briefly summarizes the advantagesof TAGLET over the many alternative formalismsthat are available; an appendix to the paper providesmore extensive technical details.2 CS 533NLP at Rutgers is taught as part of the graduate ar-tificial intelligence (AI) sequence in the computerscience department.
As a prerequisite, computer sci-ence students are expected to be familiar with prob-July 2002, pp.
77-84.
Association for Computational Linguistics.Natural Language Processing and Computational Linguistics, Philadelphia,Proceedings of the Workshop on Effective Tools and Methodologies for Teachingabilistic and decision-theoretic modeling (includingstatistical classification, hidden Markov models andMarkov decision processes) from the graduate-levelAI foundations class.
They might take NLP as a pre-liminary to research in dialogue systems or in learn-ing for language and information?or simply to ful-fill the breadth requirement of MS and PhD degrees.Students from a number of other departments fre-quently get involved in natural language research,however, and are also welcome in 533; on average,only about half the students in 533 come from com-puter science.
Students from the linguistics depart-ment frequently undertake computational work asa way of exploring practical learnability as a con-straint on universal grammar, or practical reasoningas a constraint on formal semantics and pragmatics.The course also attracts students from Rutgers?s li-brary and information science department, its pri-mary locus for research in information retrieval andhuman-computer interaction.
Ambitious undergrad-uates can also take 533 their senior year; most par-ticipate in the interdisciplinary cognitive science un-dergraduate major.
533 is the only computationalcourse in natural language at Rutgers.Overall, the course is structured into three mod-ules, each of which represents about fifteen hours ofin-class lecture time.The first module gives a general overview of lan-guage use and dialogue applications.
Lectures fol-low (Clark, 1996), but instill the practical method-ology for specifying and constructing knowledge-based systems, in the style of (Brachman et al,1990), into the treatment of communication.
Con-currently, students explore precise descriptions oftheir intuitions about language and communicationthrough a series of short homework exercises.The second module focuses on general techniquesfor linguistic representation and implementation, us-ing TAGLET.
With an extended TAGLET project,conveniently implemented in stages, we use basictree operations to introduce Prolog programming,including data structures, recursion and abstractionmuch as outlined in (Sterling and Shapiro, 1994);then we write a simple chart parser with incrementalinterpretation, and a simple communicative-intentgenerator scaled down after (Stone et al, 2001).The third module explores the distinctive prob-lems of specific applications in NLP, including spo-ken dialogue systems, information retrieval and textclassification, spelling correction and shallow tag-ging applications, and machine translation.
Jurafskyand Martin (2000) is our source-book.
Concurrently,students pursue a final project, singly or in cross-disciplinary teams, involving a more substantial andpotentially innovative implementation.In its overall structure, the course seems quitesuccessful.
The initial emphasis on clarifying in-tuitions about communication puts students on aneven footing, as it highlights important ideas aboutlanguage use without too much dependence on spe-cialized training in language or computation.
By theend of the class, students are able to build on themore specifically computational material to come upwith substantial and interesting final projects.
InSpring 2002 (the first time this version of 533 wastaught), some students looked at utterance interpre-tation, response generation and graphics generationin dialogue interaction; explored statistical methodsfor word-sense disambiguation, summarization andgeneration; and quantified the potential impact ofNLP techniques on information tasks.
Many of theseresults represented fruitful collaborations betweenstudents from different departments.Naturally, there is always room for improvement,and the course is evolving.
My presentation ofTAGLET here, for example, represents as much aproject for the next run of 533 as a report of thisyear?s materials; in many respects, TAGLET actu-ally emerged during the semester as a dynamic reac-tion to the requirements and opportunities of a six-week module on general techniques for linguisticrepresentation and implementation.3 Language and Computation in NLPIn a survey course for a broad, research-oriented au-dience, like CS 533 at Rutgers, a module on linguis-tic representation must orient itself to central ideasabout computation.
533 may be the first and lastplace linguistics or information science students en-counter concepts of specification, abstraction, com-plexity and search in class-work.
The students whoattack interdisciplinary research with success will bethe ones who internalize and draw on these concepts,not those who merely hack proficiently.
At the sametime, computer scientists also can benefit from anemphasis on computational fundamentals; it meansthat they are building on and reinforcing their ex-pertise in computation in exploring its application tolanguage.
Nevertheless, NLP is not compiler con-struction.
Programming assignments should alwaysunderline a worthwhile linguistic lesson, not indulgein implementation for its own sake.This perspective suggests a number of desideratafor the grammar formalism for a survey course inNLP.Tree rewriting.
Students need to master recur-sive data-structures and programming.
NLP directsour attention to the recursive structures of linguisticsyntax.
In fact, by adopting a grammar formalismwhose primitives operate on these structures as first-class objects, we can introduce a rich set of relativelystraightforward operations to implement, and moti-vate them by their role in subsequent programs.Lexicalization.
Students need to distinguish be-tween specification and implementation, and to un-derstand the barriers of abstraction that underliethe distinction.
Lexicalized grammars come with aready notion of abstraction.
From the outside, ab-stractly, a lexicalized grammar analyzes each sen-tence as a simple combination of atomic elementsfrom a lexicon of options.
Simultaneously, a con-crete implementation can assign complex structuresto the atomic elements (elementary trees) and imple-ment complex combinatory operations.Strong competence implementation.
Studentsneed to understand how natural language must anddoes respond to the practical logic of physical re-alization, like all AI (Agre, 1997).
Mechanisms thatuse grammars face inherent computational problemsand natural grammars in particular must respond tothese problems: students should undertake imple-mentations which directly realize the operations ofthe grammar in parsing and generation.
But thesemust be effective programs that students can buildon?our time and interest is too scarce for extensivereimplementations.Simplicity.
Where possible, linguistic proposalsshould translate readily to the formalism.
At thesame time, students should be able to adapt aspectsof the formalism to explore their own judgmentsand ideas.
Where possible, students should get in-tuitive and satisfying results from straightforwardalgorithms implemented with minimal bookkeepingand case analysis.
At the same time, there is no rea-son why the formalism should not offer opportuni-ties for meaningful optimization.We cannot expect any formalism to fare perfectlyby all these criteria?if any does, it is a deep factabout natural language!
Still, it is worth remark-ing just how badly these criteria judge traditionalunification-based context-free grammars (CFGs), aspresented in say (Pereira and Shieber, 1987).
Data-structures are an afterthought in CFGs; CFGs can-not in principle be lexicalized; and, whatever theirmerits in parsing or recognition, CFGs set up a pos-itively abysmal search space for meaningful genera-tion tasks.4 TAGLETTAGLET1 is my response to the objectives mo-tivated in Section 2 and outlined in Section 3.TAGLET represents my way of distilling the essen-tial linguistic and computational insights of lexical-ized tree-adjoining grammar?LTAG (Joshi et al,1975; Schabes, 1990)?into a form that students caneasily realize in end-to-end implementations.4.1 OverviewLike LTAG, TAGLET analyzes sentences as a com-plex of atomic elements combined by two kinds ofoperations, complementation and modification.
Ab-stractly, complementation combines a head with anargument which is syntactically obligatory and se-mantically dependent on the head.
Abstractly, mod-ification combines a head with an adjunct which issyntactically optional and need not involve any spe-cial semantic dependence.
Crucially for generation,in a derivation, modification and complementationoperations can apply to a head in any order, oftenyielding identical structures in surface syntax.
Thismeans the generator can provide required materialfirst, then elaborate it, enabling use of grammar inhigh-level tasks such as the planning of referring ex-pressions or the ?aggregation?
of related semanticmaterial into a single complex sentence.Concretely, TAGLET operations are implementedby operations that rewrite trees.
Each lexical el-ement is associated with a fragmentary phrase-1If the acronym must stand for something, ?Tree AssemblyGrammar for LExicalized Teaching?
will do.CT +T?C)T?CTFigure 1: Substitution (complementation).CT+T?C?C*)T?C?CTFigure 2: Forward sister-adjunction (modification.
)structure tree containing a distinguished word calledthe anchor.
For complementation, TAGLET adoptsTAG?s substitution operation; substitution replacesa leaf node in the head tree with the phrase struc-ture tree associated with the complement.
See Fig-ure 1.
For modification, TAGLET adopts the thesister-adjunction operation defined in (Rambow etal., 1995); sister-adjunction just adds the modifiersubtree as a child of an existing node in the headtree?either on the left of the head (forward sister-adjunction) as in Figure 2, or on the right of the head(backward sister-adjunction).
I describe TAGLETformally in Appendix A.TAGLET is equivalent in weak generative powerto context-free grammar.
That is, any language de-fined by a TAGLET also has a CFG, and any lan-guage defined by a CFG also has a TAGLET.
On theother hand context-free languages can have deriva-tions in which all lexical items are arbitrarily farfrom the root; TAGLET derived structures alwayshave an anchor whose path to the root of the sen-tence has a fixed length given by a grammatical ele-ment.
See Appendix B.
The restriction seems of lit-tle linguistic significance, since any tree-bank parseinduces a unique TAGLET grammar once you la-bel which child of each node is the head, which arecomplements and which are modifiers.
Indeed, sinceTAGLET thus induces bigram dependency struc-tures from trees, this invites the estimation of proba-bility distributions on TAGLET derivations based onNPChrisSHHNP VPHHVlovesNPNPSandyVP*nADVPmadlyFigure 3: Parallel analysis in TAGLET and TAG.observed bigram dependencies; see (Chiang, 2000).To implement an effective TAGLET generator,you can perform a greedy head-first search of deriva-tions guided by heuristic progress toward achievingcommunicative goals (Stone et al, 2001).
Mean-while, because TAGLET is context-free, you caneasily write a CKY-style dynamic programmingparser that stores structures recognized for spans oftext in a chart, and iteratively combines structuresin adjacent spans until the analyses span the entiresentence.
(More complexity would be required formultiply-anchored trees, as they induce discontinu-ous constituents.)
The simple requirement that op-erations never apply inside complements or modi-fiers, and apply left-to-right within a head, sufficesto avoid spurious ambiguity.
See Appendix C.4.2 ExamplesWith TAGLET, two kinds of examples are instruc-tive: those where TAGLET can mirror TAG, andthose where it cannot.
For the first case, consideran analysis of Chris loves Sandy madly by the treesof Figure 3.
The final structure is:SHHHHNPChrisVPHHHHVlovesNPSandyADVPmadlyFor the second case, consider the embedded ques-tion who Chris thinks Sandy likes.
The usual TAGanalysis uses the full power of adjunction.
TAGLETrequires the use of one of the familiar context-freefiller-gap analyses, as perhaps that suggested by thetrees in Figure 4, and their composition:QHHNPwhoS/NPNPChrisS/NPHHHNP VP/NPHHVthinksS/NPNPSandyS/NPHHNP VP/NPVlikesFigure 4: TAGLET requires a gap-threading analy-sis of extraction (or another context-free analysis).QHHHHNPwhoS/NPHHHHNPChrisVP/NPHHHVthinksS/NPHHNPSandyVP/NPVlikesThe use of syntactic features amounts to an in-termediate case.
In TAGLET derivations (unlike inTAG) nodes accrete children during the course of aderivation but are never rewritten or split.
Thus, wecan decorate any TAGLET node with a single setof syntactic features that is preserved throughout thederivation.
Consider the trees for he knows below:NP[NM SGCS X]/he/SHHHHNP[NM YCS N]VPV[NM Y]/know/When these trees combine, we can immediatelyunify the number Y of the verb with the pronoun?ssingular; we can immediately unify the case X of thepronoun with the nominative assigned by the verb:SHHHHNP[NM SGCS N]/he/VPV[NM SG]/know/The feature values will be preserved by further stepsof derivation.4.3 Building on TAGLETSemantics and pragmatics are crucial to NLP.TAGLET lets students explore meaty issues in se-mantics and pragmatics, using the unification-basedsemantics proposed in (Stone and Doran, 1997).
Weview constituents as referential, or better, indexical;we link elementary trees with constraints on theseindices and conjoin the constraints in the meaningof a compound structure.
This example shows howthe strategy depends on a rich ontology:S:eHHHHNP:cChrisVP:eHHHHHV:elovesNP:sSandyADVP:emadlychris(c)^ sandy(s)^ love(e,c,s)^mad(e)The example also shows how the strategy lets usquickly implement, say, the constraint-satisfactionapproaches to reference resolution or the plan-recognition approaches to discourse integration de-scribed in (Stone and Webber, 1998).4.4 Lectures and AssignmentsHere is a plan for a six-week TAGLET module.
Thefirst two weeks introduce data structures and recur-sive programming in Prolog, with examples drawnfrom phrase structure trees and syntactic combi-nation; and discuss dynamic-programming parsers,with an aside on convenient implementation usingProlog assertion.
As homework, students implementsimple tree operations, and build up to definitions ofsubstitution and modification for parsing and gener-ation; they use these combinatory operations to writea CKY TAGLET parser.The next two weeks begin with lectures on thelexicon, emphasizing abstraction on the computa-tional side and the idiosyncrasy of lexical syntax andthe indexicality of lexical semantics on the linguis-tic side; and continue with lectures on semantics andinterpretation.
Meanwhile, students add referenceresolution to the parser, and implement routines toconstruct grammars from tree-bank parses.The final two weeks cover generation as problem-solving, and search through the grammar.
Studentsreuse the grammar and interpretation model they al-ready have to construct a generator.5 ConclusionImportant as they are, lexicalized grammars canbe forbidding.
Versions of TAG and combinatorycategorial grammars (CCG) (Steedman, 2000), aspresented in the literature, require complex book-keeping for effective computation.
When I wrotea CCG parser as an undergraduate, it took me awhole semester to get an implemented handle onthe metatheory that governs the interaction of (cross-ing) composition or type-raising with spurious am-biguity; I still have never written a TAG parser or aCCG generator.
Variants of TAG like TIG (Schabesand Waters, 1995) or D-Tree grammars (Rambowet al, 1995) are motivated by linguistic or formalconsiderations rather than pedagogical or computa-tional ones.
Other formalisms come with linguisticassumptions that are hard to manage.
Link gram-mar (Sleator and Temperley, 1993) and other puredependency formalisms can make it difficult to ex-plore rich hierarchical syntax and the flexibility ofmodification; HPSG (Pollard and Sag, 1994) comeswith a commitment to its complex, rather bewilder-ing regime for formalizing linguistic information asfeature structures.
Of course, you probably couldrefine any of these theories to a simple core?andwould get something very like TAGLET.I strongly believe that this distillation is worththe trouble, because lexicalization ties grammar for-malisms so closely to the motivations for studyinglanguage in the first place.
For linguistics, this phi-losophy invites a fine-grained description of sen-tence syntax, in which researchers document the di-versity of linguistic constructions within and acrosslanguages, and at the same time uncover impor-tant generalizations among them.
For computation,this philosophy suggests a particularly concrete ap-proach to language processing, in which the infor-mation a system maintains and the decisions it takesultimately always just concern words.
In takingTAGLET as a starting point for teaching implemen-tation in NLP, I aim to expose a broad range of stu-dents to a lexicalized approach to the cognitive sci-ence of human language that respects and integratesboth linguistic and computational advantages.AcknowledgmentsThanks to the students of CS 533 and four anony-mous reviewers for helping to disabuse me of nu-merous preconceptions.ReferencesPhilip E. Agre.
1997.
Computation and Human Experi-ence.
Cambridge.Ronald Brachman, Deborah McGuinness, Peter Pa-tel Schneider, Lori Alperin Resnick, and AlexanderBorgida.
1990.
Living with CLASSIC: when and howto use a KL-ONE-like language.
In J. Sowa, editor,Principles of Semantic Networks.
Morgan Kaufmann.David Chiang.
2000.
Statistical parsing with anautomatically-extracted tree adjoining grammar.
InACL, pages 456?463.Herbert H. Clark.
1996.
Using Language.
CambridgeUniversity Press, Cambridge, UK.John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ull-man.
2000.
Introduction to automata theory, lan-guages and computation.
Addison-Wesley, secondedition.Aravind K. Joshi, L. Levy, and M. Takahashi.
1975.
Treeadjunct grammars.
Journal of the Computer and Sys-tem Sciences, 10:136?163.Daniel Jurafsky and James H. Martin.
2000.
Speechand Language Processing: An introduction to nat-ural language processing, computational linguisticsand speech recognition.
Prentice-Hall.Fernando C. N. Pereira and Stuart M. Shieber.
1987.Prolog and Natural Language Analysis.
CSLI, Stan-ford CA.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press,Chicago.Owen Rambow, K. Vijay-Shanker, and David Weir.1995.
D-Tree grammars.
In ACL, pages 151?158.Yves Schabes and Richard C. Waters.
1995.
Tree-insertion grammar: A cubic-time parsable formalismthat lexicalizes context-free grammar without chang-ing the trees produced.
Computational Linguistics,21:479?513.Yves Schabes.
1990.
Mathematical and ComputationalAspects of Lexicalized Grammars.
Ph.D. thesis, Com-puter Science Department, University of Pennsylva-nia.Daniel Sleator and Davy Temperley.
1993.
ParsingEnglish with a link grammar.
In Third InternationalWorkshop on Parsing Technologies.Mark Steedman.
2000.
The Syntactic Process.
MIT.Leon Sterling and Ehud Shapiro.
1994.
The Art of Pro-log.
MIT, second edition.Matthew Stone and Christine Doran.
1997.
Sentenceplanning as description using tree-adjoining grammar.In Proceedings of ACL, pages 198?205.Matthew Stone and Bonnie Webber.
1998.
Textualeconomy through close coupling of syntax and seman-tics.
In Proceedings of International Natural Lan-guage Generation Workshop, pages 178?187.Matthew Stone, Christine Doran, Bonnie Webber, ToniaBleam, and Martha Palmer.
2001.
Microplanningwith communicative intentions: The SPUD system.Under review.A DefinitionsI define TAGLET in terms of primitive trees.
Thedefinitions require a set VT of terminal categories,corresponding to our lexical items, and a disjoint setVN of nonterminal categories, corresponding to con-stituent categories.
TAGLET uses trees labeled bythese categories both as representations of the syn-tactic structure of sentences and as representationsof the grammatical properties of words: A syntactic tree is a tree whose nodes are eachassigned a unique label in VN [VT , such thatonly leaf nodes are assigned a label in VT . A lexical tree is a syntactic tree in which ex-actly one node, called the anchor, is assigned alabel in VT .
The path through such a tree fromthe root to the anchor is called the spine.A primitive tree is lexical tree in which every leaf isthe child of a node on the spine.
See Figures 3 and 4.A TAGLET element is a pair hT,Oi consisting ofprimitive tree together with the specification of theoperation for the tree; the allowable operations arecomplementation, indicated by ?
; premodificationat a specified category C 2 VN , indicated by ?!
(C)and postmodification at a specified category C 2VN ,indicated by ?
(C).Formally, then, a TAGLET grammar is a tupleG = hVT ,VN ,?i where VT gives the set of termi-nal categories, VN gives the set of nonterminal cat-egories, and ?
gives a set of TAGLET elements forVT and VN .
Given a TAGLET grammar G, the setof derived trees for G is defined as the smallest setclosed under the following operations: (Initial) Suppose hT,Oi 2 ?.
Then hT,Oi is aderived tree for G. (Substitution) Suppose hT,Oi is a derived treefor G where T contains leaf node n with labelC 2VN ; and suppose hT 0,?i is a derived tree forG where the root of T 0 also has label C. ThenhT 00,Oi is a derived tree for G where T 00 is ob-tained from T by identifying node n with theroot of T 0. (Premodification) Suppose hT,Oi is a derivedtree for G where T contains node n with labelC 2 VN , and suppose hT 0,?!
(C)i is a derivedtree for G. Then hT 00,Oi is a derived tree for Gwhere T 00 is obtained from T by adding T 0 asthe first child of node n. (Postmodification) Suppose hT,Oi is a derivedtree for G where T contains node n with labelC 2 VN , and suppose hT 0,?
(C)i is a derivedtree for G. Then hT 00,Oi is a derived tree for Gwhere T 00 is obtained from T by adding T 0 asthe last child of node n.A derivation for G is a derived tree hT,?i for G, inwhich all the leaves of T are elements of VT .
Theyield of a derivation hT,?i is the string consisting ofthe leaves of T in order.
A string ?
is in the languagegenerated by G just in case ?
is the yield of somederivation for G.B PropertiesEach node in a TAGLET derived tree T is first con-tributed by a specific TAGLET element, and so in-directly by a particular anchor.
Accordingly, we canconstruct a lexicalized derivation tree correspondingto T .
Nodes in the derivation tree are labeled by theelements used in deriving T .
An edge leads fromparent E to child E 0 if T includes a step of deriva-tion in which E 0 is substituted or sister-adjoined at anode first contributed by E .
To make the derivationunambiguous, we record the address of the node inE at which the operation applies, and we order theedges in the derivation tree in the same order thatthe corresponding operations are applied in T .
ForFigure 3, we have:?2:lovesHHHHHHHH?1:Chris (0) ?3:Sandy (1.1) ?
4 :madly (1.1)Let L be a CFL.
Then there is a grammar G forL in Greibach normal form (Hopcroft et al, 2000),where each production has the formA!
xB1 .
.
.Bnwhere x2VT and Bi 2VN .
For each such production,create the TAGLET element which allows comple-mentation with a tree as below:AHHx B1 BnAn easy induction transforms any derivation in Gto a derivation in this TAGLET grammar, and viceversa.
So both generate the same language L.Conversely, we can build a CFG for a TAGLET bycreating nonterminals and productions for each nodein a TAGLET elementary structure, taking into ac-count the possibilities for optional premodificationand postmodification as well as complementation.C ParsingSuppose we make a bottom-up traversal of aTAGLET derivation tree to construct the derivedtree.
After we finish with each node (and all its chil-dren), we obtain a subtree of the final derived tree.This subtree represents a complete constituent thatmust appear as a subsequence of the final sentence.A CKY TAGLET parser just reproduces this hier-archical discovery of constituents, by adding com-pleted constituents for complements and modifiersinto an open constituent for a head.The only trick is to preserve linear order; thismeans adding each new complement and modifier ata possible ?next place?, without skipping past miss-ing complements or slipping under existing modi-fiers.
To do that, we only apply operations that addcompleted constituents T2 along what is known asthe frontier of the head tree T1, further away fromthe head than previously incorporated material.
Thisconcept, though complex, is essential in any accountof incremental structure-building.
To avoid spuri-ous ambiguities, we also require that operations tothe left frontier must precede operations to the rightfrontier.
This gives a relation COMBINE(T1,T2,T3).The parser analyses a string of length N using adynamic-programming procedure to enumerate allthe analyses that span contiguous substrings, short-est substrings first.
We write T 2 (i, j) to indicatethat object T spans position i to j.
The start of thestring is position 0; the end is position N. So wehave:for word w 2 (i, i + 1), T with anchor wadd T 2 (i, i + 1)for k 2 up to Nfor i k?2 down to 0for j i + 1 up to k?1for T1 2 (i, j) and T2 2 ( j,k)for T3 with COMBINE(T1,T2,T3)add T3 2 (i,k)Now, any parser that delivers possible analyses ex-haustively will be prohibitively expensive in theworst-case; analyses of ambiguities multiply expo-nentially.
At the cost of a strong-competence imple-mentation, one can imagine avoiding the complexityby maintaining TAGLET derivation forests.
This en-ables O(N3) recognition, since TAGLET parsing op-erations apply within spans of the spine of single ele-mentary trees and therefore the number of COMBINEresults for T1 and T2 is independent of N.
