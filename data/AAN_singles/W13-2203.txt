Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 52?61,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsThe Feasibility of HMEANT as a Human MT Evaluation MetricAlexandra Birch Barry Haddow Ulrich Germanna.birch@ed.ac.uk bhaddow@inf.ed.ac.uk ugermann@inf.ed.ac.ukMaria Nadejde Christian Buck Philipp Koehnmaria.nadejde@gmail.com cbuck@lantis.de pkoehn@inf.ed.ac.ukUniversity of Edinburgh10 Crichton StreetEdinburgh, EH8 9AB, UKAbstractThere has been a recent surge of interest insemantic machine translation, which stan-dard automatic metrics struggle to evalu-ate.
A family of measures called MEANThas been proposed which uses semanticrole labels (SRL) to overcome this prob-lem.
The human variant, HMEANT, haslargely been evaluated using correlationwith human contrastive evaluations, thestandard human evaluation metric for theWMT shared tasks.
In this paper we claimthat for a human metric to be useful, itneeds to be evaluated on intrinsic proper-ties.
It needs to be reliable; it needs towork across different language pairs; andit needs to be lightweight.
Most impor-tantly, however, a human metric must bediscerning.
We conclude that HMEANTis a step in the right direction, but hassome serious flaws.
The reliance on verbsas heads of frames, and the assumptionthat annotators need minimal guidelinesare particularly problematic.1 IntroductionHuman evaluation is essential in machine transla-tion (MT) research because it is the ultimate wayto judge system quality.
Furthermore, human eval-uation is used to evaluate automatic metrics whichare necessary for tuning system parameters.
Un-fortunately, there is no clear consensus on whichevaluation strategy is best.
Humans have beenasked to judge if translations are correct, to gradethem and to rank them.
But it is often very difficultto decide how good a translation is, when there areso many possible ways of translating a sentence.Another problem is that different types of evalua-tion might be useful for different purposes.
If theMT is going to be the basis of a human transla-tor?s work-flow, then post-editing effort seems likea natural fit.
However, for people using MT forgisting, what we really want is some measure ofhow much meaning has been retained.We clearly need a metric which tries to answerthe question, how much of the meaning does thetranslation capture.
In this paper, we explore theuse of human evaluation metrics which attemptto capture the extent of this meaning retention.In particular, we consider HMEANT (Lo and Wu,2011a), a metric that uses semantic role labelsto measure how much of the ?who, why, when,where?
has been preserved.
For HMEANT evalua-tion, annotators are instructed to identify verbs asheads of semantic frames.
Then they attach rolefillers to the heads and finally they align headsand role fillers in the candidate translation withthose in a reference translation.
In a series of pa-pers, Lo and Wu (2010, 2011b,a, 2012) explored anumber of questions, evaluating HMEANT by us-ing correlation statistics to compare it to judge-ments of human adequacy and contrastive evalu-ations.
Given the drawbacks of those evaluationmeasures, which we discuss in Sec.
2, they couldjust as well have been evaluating the human ade-quacy and contrastive judgements using HMEANT.Human evaluation metrics need to be judged onother intrinsic qualities, which we describe below.The aim of this paper is to evaluate the effec-tiveness of HMEANT, with the goal of using it tojudge the relative merits of different MT systems,for example in the shared task of the Workshop onMachine Translation.In order to be useful, an MT evaluation metricmust be reliable, be language independent, havediscriminatory power, and be efficient.
We addresseach of these criteria as follows:52Reliability We produce extensive IAA (Inter-annotator agreement) for HMEANT, breaking itdown into the different stages of annotation.
Ourexperimental results show that whilst the IAA forHMEANT is acceptable at the individual stages ofthe annotation, the compounding effect of dis-agreement at each stage of the pipeline greatly re-duces the effective overall IAA ?
to 0.44 on rolealignment for German, and, only slightly better,0.59 for English.
This raises doubts about the reli-ability of HMEANT in its current form.Discriminatory Power We consider output ofthree types of MT system (Phrase-based, Syntax-based and Rule-based) to attempt to gain insightinto the different types of semantic informationpreserved by the different systems.
The Syntax-based system seems to have a slight edge overall,but since IAA is so low, this result has to be takenwith a grain of salt.Language Independence We apply HMEANTto both English and German translation outputs,showing that the guidelines can be adapted to thenew language.Efficiency Whilst HMEANT evaluation willnever be as fast as, for example, the contrastivejudgements used for the WMT shared task,it is still reasonably efficient considering thefine-grained nature of the evaluation.
On average,annotators evaluated about 10 sentences per hour.2 Related WorkEven though the idea that machine translation re-quires a semantic representation of the translatedcontent is as old as the idea of computer-basedtranslation itself (Weaver, 1955), it has not beenuntil recently that people have begun to combinestatistical models with semantic representations.Jones et al(2012), for example, represent mean-ing as directed acyclic graphs and map these toPropBank (Palmer et al 2005) style dependen-cies.
To evaluate such approaches properly, weneed evaluation metrics that capture the accuracyof the translation.Current automatic metrics of machine trans-lation, such as BLEU (Papineni et al 2002),METEOR (Lavie and Denkowski, 2009) andTER (Snover et al 2009b), which have greatlyaccelerated progress in MT research, rely on shal-low surface properties of the translations, andonly indirectly capture whether or not the trans-lation preserves the meaning.
This has meant thatpotentially more sophisticated translation modelsare pitted against the flatter phrase-based mod-els, based on metrics which cannot reflect theirstrengths.
Callison-Burch et al(2011) provide ev-idence that automatic metrics are inconsistent withhuman judgements when comparing rule-basedagainst statistical machine translation systems.Automatic evaluation metrics are evaluated andcalibrated based on their correlation with humanjudgements.
However, after more than 60 yearsof research into machine translation, there is stillno consensus on how to evaluate machine transla-tion based on human judgements.
(Hutchins andSomers, 1992; Przybocki et al 2009).One obvious approach is to ask annotators torate translation candidates on a numerical scale.Under the DARPA TIDES program, the LinguisticData Consortium (2002) developed an evaluationscheme that relies on two five-point scales repre-senting fluency and adequacy.
This was also thehuman evaluation scheme used in the annual MTcompetitions sponsored by NIST (2005).In an analysis of human evaluation results forthe WMT ?07 workshop, however, Callison-Burchet al(2007) found high correlation between flu-ency and adequacy scores assigned by individualannotators, suggesting that human annotators arenot able to separate these two evaluation dimen-sions easily.
Furthermore these absolute scoresshow low inter-annotator agreement.
Instead ofgiving absolute quality assessments, annotatorsappeared to be using their ratings to rank trans-lation candidates according to their overall prefer-ence for one over the other.In line with these findings, Callison-Burch et al(2007) proposed to let annotators rank translationcandidates directly, without asking them to assignan absolute quality assessment to each candidate.This type of human evaluation has been performedin the last six Workshops on Statistical MachineTranslation.Although it is useful to have a score or a rankfor a particular sentence, especially for evaluat-ing automatic metrics, these ratings are necessar-ily a simplification of the real differences betweentranslations.
Translations can contain a large num-ber of different types of errors of varying severity.Even if we put aside difficulties with selecting onepreferred sentence, ranking judgements are diffi-cult to generalise.
Humans are shown five transla-tions at a time, and there is a high cognitive cost toranking these at once.
Furthermore, these repre-53sent a subset of the competing systems, and theserankings must be combined with other annotatorsjudgements on five other system outputs to com-pute an overall ranking.
The methodology for in-terpreting the contrastive evaluations has been thesubject of much recent debate in the community(Bojar et al 2011; Lopez, 2012).There has been some effort to overcome theseproblems.
HTER (Snover et al 2009a) is a met-ric which counts the number of edits needed by ahuman to convert the machine translation so as toconvey the same meaning as the reference.
Thistype of evaluation is of some use when one is us-ing MT to aid human translation (although the re-lationship between number of edits and actual ef-fort is not straightforward (Koponen, 2012)), butit is not so helpful when one?s task is gisting.
Thenumber of edits need not correlate with the sever-ity of the semantic differences between the twosentences.
The loss of a negative, for instance, isonly one edit away from the original, but the se-mantics change completely.Alternatively, HyTER (Dreyer and Marcu,2012) is an annotation tool which allows a userto create an exponential number of correct trans-lations for a given sentence.
These references arethen efficiently exploited to compare with machinetranslation output.
The authors argue that the cur-rent metrics fail simply because they have accessto sets of reference translations which are simplytoo small.
However, the fact is that even if onedoes have access to large numbers of translations,it is very difficult to determine whether the refer-ence correctly captures the essential semantic con-tent of the references.The idea of using semantic role labels to evalu-ate machine translation is not new.
Gime?nez andMa`rquez (2007) proposed using automatically as-signed semantic role labels as a feature in a com-bined MT metric.
The main difference betweenthis application of semantic roles and MEANT isthat arguments for specific verbs are taken into ac-count, instead of just applying the subset agent,patient and benefactor.
This idea would probablyhelp human annotators to handle sentences withpassives, copulas and other constructions whichdo not easily match the most basic arguments.
Onthe other hand, verb specific arguments are lan-guage dependent.Bojar and Wu (2012), applying HMEANT toEnglish-to-Czech MT output, identified a numberof problems with HMEANT, and suggested a vari-ety of improvements.
In some respects, this workis very similar, except that our goal is to evaluateHMEANT along a range of intrinsic properties, todetermine how useful the metric really is to evalu-ation campaigns such as the workshop on machinetranslation.3 Evaluation with HMEANT3.1 Annotation ProcedureThe goal of the HMEANT metric is to capture es-sential semantic content, but still be simple andfast.
There are two stages to the annotation, thefirst of which is semantic role labelling (SRL).Here the annotator is directed to select the actions,or frame heads, by marking all the verbs in the sen-tence except for auxilliaries and modals.
The roles(or slot fillers) within the frame are then markedand each is linked with a unique action.
Each roleis given a type from an inventory of 11 (Table 1),and an action with its collection of correspondingroles is known as a frame.
In the role annotationthe idea is to get the annotator to recognise whodid what to who, when, where and why in both thereferences and the MT outputs.who what whom when whereagent patient benefactive temporal locativewhy howpurpose degree, manner, modal, negation, otherTable 1: Semantic rolesThe second stage in the annotation is alignment,where the annotators match elements of the SRLannotation in the reference with that in the MToutput.
The annotators link both actions and roles,and these alignments can be matched as ?Correct?or ?Partial?
matches, depending on how well theaction or role is translated.
The guidelines for theannotators are deliberately minimalistic, with theargument being that non-experts can get startedquickly.
Lo and Wu (2011a) claim that unskilledannotators can be trained within 15 minutes.In all such human evaluation, there is a trade-off between simplicity and accuracy.
Clearly whenevaluating bad machine translation output, we donot want to label too much.
However, sometimeshaving so little choice of semantic roles can leadto confusion and slow down the annotator whenmore complicated examples do not fit the scheme.Therefore, common exceptions need to be handledeither in the roles provided, or in the annotatorguidelines.543.2 Calculation of ScoreThe overall HMEANT score for MT evaluationis computed as the f-score from the counts ofmatches of frames and their role fillers betweenthe reference and the MT output.
Unmatchedframes are excluded from the calculation togetherwith all their corresponding roles.In recognition that preservation of some typesof semantic relations may be more important thanothers for a human to understand a sentence, onemay want to weight them differently in the com-putation of the HMEANT score.
Lo and Wu (2012)train weights for each role filler type to optimisecorrelation with human adequacy judgements.
Asan unsupervised alternative, they suggest weight-ing roles according to their frequency as approxi-mation to their importance.Since the main focus of the current paper is theannotation of the actions, roles and alignments thatHMEANT depends on, we do not explore such dif-ferent weight-setting schemes, but set the weightsuniformly, with the exception of a partial align-ment, which is given a weight of 0.5.
HMEANT isthus defined as follows:Fi = # correct or partially correct fillersfor PRED i in MTMTi = total # fillers for PRED i in MTREFi = total # fillers for PRED i in REFP =?matched iFiMTiR =?matched iFiREFiPtotal =Pcorrect + 0.5Ppartialtotal # predicates in MTRtotal =Pcorrect + 0.5Ppartialtotal # predicates in REFHMEANT = 2 ?
Ptotal ?RtotalPtotal +Rtotal3.3 Automating HMEANTOne of the main directions taken by the authors ofHMEANT is in creating a fully automated versionof the metric (MEANT) in (Lo et al 2012).
Themetric combines shallow semantic parsing with asimple maximum weighted bipartite matching al-gorithm for aligning semantic frames.
They useapproximate matching schemes (Cosine and Jac-card similarity) for matching roles, with the lat-ter producing better alignments (Tumuluru et al2012).
They demonstrate that MEANT corre-lates with human adequacy judgements better thanother commonly used automatic metrics.
In thispaper we focus on human evaluation, as it is es-sential for building better automatic metrics, andtherefore a more fundamental problem.4 Experimental Setup4.1 Systems and Data SetsWe performed HMEANT evaluation on threesystems selected from 2013 WMT evaluation1.The systems we selected were uedin-wmt13,uedin-syntax and rbmt-3, which were cho-sen to provide us with a high performing phrase-based system, a high performing syntax-basedsystem and the top performing rule-based system,respectively.
The cased BLEU scores of the threesystems are shown in Table 2.System Type de-en en-deuedin-wmt13 Phrase 26.6 20.1uedin-syntax Syntax 26.3 19.4rbmt-3 Rule 18.8 16.5Table 2: Cased BLEU on the full newstest2013test set for the systems used in this studyWe randomly selected sentences from the en-deand de-en newstest2013 tasks, and extractedthe corresponding references and system outputsfor these sentences.
For the en-de task, 75% of ourselected sentences were selected from the sectionof newstest2013 that was originally in Ger-man, with the other 25% from the section that wasoriginally in English.
The sentence selection forthe de-en task was performed in a similar man-ner.
For presentation to the annotators, the sen-tences were split into segments of 12.
We foundthat with practice, annotators could complete oneof these segments in around 100-120 minutes.
Intotal, with close to 70 hours of annotator effort,we evaluated 142 sentences of German, and 72sentences of English.
The annotation for eachsentence includes 1 reference, 3 system outputs,and their corresponding alignments.
Apart from 5singly-annotated German sentences, and 1 singly-annotated English sentence, all sentences were an-notated by exactly 2 annotators.1www.statmt.org/wmt13554.2 AnnotationThe annotation for English was performed by 3different annotators (E1, E2 and E3), and the Ger-man annotation by 2 annotators (D1 and D2).All the English annotators were machine transla-tion researchers, with E1 and E2 both native En-glish speakers whereas E3 is not a native speaker,but lives and works in an English-speaking coun-try.
The two German annotators were both nativespeakers of German, with no background in com-putational linguistics, although D2 is a teacher ofGerman as a second language and has had linguis-tic training.The HMEANT evaluation task was carried outfollowing the framework described in Lo and Wu(2011a) and Bojar and Wu (2012).
For each sen-tence in the evaluation set, the annotators were firstasked to mark the semantic frames and roles (i.e.,slot fillers within the frame) in a human referencetranslation of the respective sentence.
They werethen presented with the output of several machinetranslation systems for the same source sentence,one system at a time, with the reference transla-tion and its annotations visible in the left half ofthe screen (cf.
Fig.
1).
For each system, the an-notators were asked to annotate semantic framesand slot fillers in the translation first, and thenalign them with frame heads and slot fillers inthe human reference translation.
Annotations andalignment were performed with Edi-HMEANT2,a web-based annotation tool for HMEANT thatwe developed on the basis of Yawat (Germann,2008).
The tool allows the alignment of slots fromdifferent semantic frames, and the alignment ofslots of different types; however, such alignmentsare not considered in the computation of the finalHMEANT score.The annotation guidelines were essentiallythose used in Bojar and Wu (2012), with some ad-ditional English examples, and a complete set ofGerman examples.
For ease of comparison withprior work, we used the same set of semantic rolelabels as Bojar and Wu (2012), shown in Table 1.Given the restriction that the head of a frame canconsist of only one word, a convention was madethat all other verbs attached to the main verb suchas modals, auxiliaries or separable particles forGerman verbs, would be labelled as modal.
Thiswas the only change we made to the HMEANT2Edi-HMEANT is part of the EdinburghMulti-text Annotation and Alignment Tool Suite(http://www.statmt.org/edimtaats).scheme.5 Results and Discussion5.1 Inter-Annotator AgreementWe first measured IAA on role identification, asin Lo and Wu (2011a), except that we use exactmatch on word spans as opposed to the approx-imate match employed in that reference.
Whilstexact match is a harsher measure, penalising dis-agreements related to punctuation and articles, us-ing any sort of approximate match would meanhaving to deal with N:M matches.
IAA is definedas follows:IAA = 2 ?
P ?RP +RWhere P is defined as the number of labels (ei-ther heads, roles, or alignments) that match be-tween annotators, divided by the total number oflabels given by annotator 1.
And R is defined thesame way for annotator 2.
This is similar to anF-measure (f1), where we consider one of the an-notators as the gold standard.
The IAA for roleidentification is shown in Table 3.Reference HypothesisLang.
matches f1 matches f1de 865 0.846 2091 0.737en 461 0.759 1199 0.749Table 3: IAA for role identification.
This is calcu-lated by considering exact endpoint matches on allspans (predicates and arguments).The agreements in Table 3 are not too differ-ent from those reported in earlier work.
We notethat the IAA for the German annotators drops forthe MT system outputs, but this may be becausethe English annotators (as MT researchers) are lessbothered by bad MT output than their counterpartsworking on the German texts.Next we looked at the IAA on role classifica-tion, the other IAA figure provided by Lo and Wu(2011a).
We only considered roles where both an-notators had marked the same span in the sameframe, with the frame being identified by its ac-tion.
The IAA for role classification is shown inTable 4.Again, we show similar levels of IAA to thosereported in (Lo and Wu, 2011a).
Examining thedisagreements in more detail, we produced countsof the most common role type disagreements, by56Figure 1: Example of a sentence pair annotated with Edi-HMEANT.
The reference translation is onthe left, the machine translation output on the right.
Head and slot fillers for each semantic frame aremarked by selecting spans in the text and automatically listed in tables below the respective sentences.Frames and slot fillers are aligned by clicking on table cells.
The alignments of the semantic frames arehighlighted: green (grey in black and white version) for exact match and grey (light grey) for partialmatch.Reference HypothesisLang.
matches f1 matches f1de 425 0.717 1050 0.769en 245 0.825 634 0.826Table 4: IAA for role classification.
We only con-sider cases where annotators had marked the samespan in the same frame.Role 1 Role 2 CountAgent Experiencer-Patient 110Degree-Extent Modal 92Beneficiary Experiencer-Patient 45Experiencer-Patient Manner 26Manner Other 25Table 5: Most common role type disagreements,for Germanlanguage.
We show the top 5 disagreements in Ta-bles 5 and 6.
Essentially these show that the mostcommon role types provide the most confusions.In order to shed more light on the role type dis-agreements, we examined a random sample of 10of the English annotations where the annotatorshad disagreed about ?Agent?
versus ?Experiencer-Patient?.
In 7 of these cases, there was a definitecorrect answer, according to the annotation guide-lines.
Of the other 3, there were 2 cases of poorMT output making the semantic interpretation dif-ficult, and one case of existential ?there?.
Of the 7cases where one annotator appears in error, 3 werepassive, 1 was a copula, and 1 involved the verbRole 1 Role 2 CountAgent Experiencer-Patient 44Manner Other 22Degree-Extent Temporal 12Degree-Extent Other 12Beneficiary Experiencer-Patient 11Table 6: Most common role type disagreements,for English?receive?.
For the other 2 there was no clear rea-son for the error.
From this small sample, we sug-gest that passive constructions are still difficult toannotate semantically.The last of elements of the semantic frames tobe considered for IAA are the actions, i.e.
theframe heads or predicates.
In this case identifyinga match was straightforward as actions are identi-fied by a single token.
The IAA for action identi-fication is shown in Table 7.Reference HypothesisLang.
matches f1 matches f1de 238 0.937 592 0.826en 126 0.818 362 0.868Table 7: IAA for action identification.We see fairly high IAA for actions, which seemsencouraging, but given the importance of actionsin HMEANT, we probably need the scores to behigher.
Most of the problems with the identifica-tion of actions centre around multiple-verb con-structions and participles.We now turn our attention to the second stageof the annotation process where the annotatorsmarked alignments between slots and roles.
Theseprovide the relevant statistics for the calculation ofthe HMEANT score so it is important that they areannotated reliably.Firstly, we consider the alignment of actions.
Inthis case, we use pipelined statistics, in that if oneannotator marks actions in the reference and hy-pothesis, then aligns them, whilst the other anno-tator does not mark the corresponding actions, westill count this as an action alignment mismatch.This creates a harsher measure on action align-ment, but gives a better idea of the overall relia-bility of the annotation task.
In Table 8 we showthe IAA (as F1) on action alignments.
ComparingTables 8 and 7 we see that, for English at least, the57Lang.
matches f1de 300 0.655en 275 0.769Table 8: IAA for action alignment, collapsing par-tial and full alignmentagreement on action alignment is not much lowerthan that on action identification, indicating that ifannotators agree on the actions then they generallyagree on how they align.
For German, however,the IAA on action alignment is a bit lower, ap-parently because one of the annotators was muchstricter about which actions they aligned.In order to calculate the IAA on role align-ments, we only consider those alignments thatconnect two roles in aligned frames, of the sametype, since these are the only role alignments thatcount for computing the HMEANT score.
Thismeans that if one of the annotators does not alignthe frames, then all the contained role alignmentsare counted as mismatches.
We do not considerthe spans when calculating the agreement on rolealignments, meaning that if one annotator has analignment between roles of type T in frame F ,and the other annotator also aligns the same typesof roles in the same frame, then they are consid-ered as a match.
This is done because it is only thecounts of alignments that are relevant for HMEANTscoring.
The IAA on the role alignments is quiteLang.
matches f1de 448 0.442en 506 0.596Table 9: IAA for role alignment.low, dipping below 0.5 for German.
This is mainlybecause of the pipelining effect, where annota-tion disagreements at each stage are compounded.Since the final HMEANT score is computed essen-tially by counting role alignments, this level ofIAA causes problems for this score calculation.We computed HMEANT and BLEU scores for thehypotheses annotated by each annotator pair.
TheHMEANT scores were calculated as described inSection 3.2.
The two metrics are calculated foreach sentence (we apply +1 smoothing for BLEU),then averaged across all sentences.
Table 10 showsthe scores organised by annotator pair and sys-tem type.
The agreement in the overall scores isnot good, but really just reflects the compoundedAnnotator System BLEU HMEANT HMEANTPair (Annot.
1) (Annot.
2)Phrase 0.310 0.626 (2) 0.672 (3)E1, E2 Syntax 0.291 0.635 (1) 0.730 (1)Rule 0.252 0.578 (3) 0.673 (2)Phrase 0.378 0.569 (1) 0.602 (3)E1, E3 Syntax 0.376 0.553 (2) 0.627 (2)Rule 0.320 0.546 (3) 0.646 (1)Phrase 0.360 0.669 (2) 0.696 (3)E2, E3 Syntax 0.362 0.751 (1) 0.739 (1)Rule 0.308 0.624 (3) 0.716 (2)Phrase 0.296 0.327 (1) 0.631 (3)D1, D2 Syntax 0.321 0.312 (2) 0.707 (1)Rule 0.242 0.274 (3) 0.648 (2)Table 10: Scores assigned by each annotator pair.The numbers in brackets after the HMEANT scoresshow the relative ranking assigned by each anno-tator.agreement problems in the role alignments (Table9).
In no case do the annotators choose a consis-tent ranking of the 3 systems, and in 2 of the 4 an-notator pairs, the annotators disagree about whichis the top performing system.5.2 Overall ScoresIn this section we report the overall HMEANTscores of the three systems whose output we an-notated.
Our main focus on this paper was on theannotation task, so we do not wish to emphasisethe scoring, but it is nevertheless an important end-product of the HMEANT annotation process.
Theoverall scores (HMEANT and +1 smoothed sen-tence BLEU, averaged across sentences and anno-tators) are given in Table 11.Language System BLEU HMEANTPhrase 0.351 0.634en Syntax 0.344 0.667Rule 0.295 0.625Phrase 0.294 0.482de Syntax 0.302 0.517Rule 0.242 0.464Table 11: Comparison of mean HMEANT and(smoothed sentence) BLEU for the three systems.From the table we can observe that, whilstBLEU shows similar scores for the phrase-basedand syntax-based systems, with lower scores forthe rule-based system, HMEANT shows the syntax-based system as being ahead, with the other twoshowing similar performance.
We would cautionagainst reading too much into this, considering therelatively small number of sentences annotated,58and the issues with IAA exposed in the previoussection, but it is an encouraging results for syntax-based MT.5.3 DiscussionMachine translation research needs a reliablemethod for evaluating and comparing differentmachine translation systems.
The performance ofHMEANT as shown in the previous section is dis-appointing.
The fact that the final role IAA, in Ta-ble 9, is 0.442 for German and 0.596 for English,demonstrates that there are fundamental problemswith the scheme.
One of the areas of greatest con-fusion is between what seems like one of the eas-iest role types to distinguish: agent and patient.Here is an example of a passive where one anno-tator has marked ?tea?
wrongly as agent, and theother annotator correctly labelled it as patient:Reference: In the kitchen, tea is prepared forthe guestsACTION preparedLOCATIVE In the kitchenAGENT / PATIENT teaMODAL isBENEFICIARY for the guestsWe would argue that the most important changeto HMEANT must be in creating more comprehen-sive annotation guidelines, with examples of diffi-cult cases.
Bojar and Wu (2012) listed a number ofproblems and improvements to HMEANT, whichwe largely agree with.
We list the most importantlimitations of HMEANT that we have encountered:?
Single Word Heads Verbal predicates oftenconsist of multiple words, which can be split.For example: ?Take him up on his offer?.?
Heads being limited to verbs The semanticsof verbs can often be carried by an equivalentnoun and should be allowed by HMEANT.
Forexample ?My father broke down and cried .
?,the verb ?cried?
is correctly paraphrased in?My father collapsed in tears .??
Copular Verbs These do not fit in to the lim-ited list of role types.
For example forcingthis sentence ?The story is plausible?, to haveand agent and patient is confusing.?
Prepositional Phrases attaching to a nounThese can greatly affect the semantics of asentence, but HMEANT has no way of captur-ing this.?
Semantics not on head This frequently oc-curs with light verbs, for example ?Bousondid the review of the paper?
is equivalent to?Bouson reviewed the paper?.?
Hierarchy of frames There are often frameswhich are embedded in other frames, for ex-ample in reported speech.
It is not clearwhether errors at the lowest level should bemarked wrong just at that point, or whetherthey should be marked wrong all the way upthe semantic tree.
For example: ?Arafat said?Isreal suffocates such a hope in the germ?
?.The frame headed by ?said?
is largely cor-rect, but the reported speech is not.
The pa-tient role of the verb ?said?
could be alignedas correct, as the error is already captured inrelation to the verb ?suffocates?.?
No discourse markers These are impor-tant for capturing the relationships betweenframes and should be labelled.6 ConclusionHMEANT represents an attempt to create a humanevaluation for machine translation which directlymeasures the semantic content preserved by theMT.
It partly succeeds.
However we have castdoubt on the claim that HMEANT can be reliablyannotated with minimal annotator training andguidelines.
In the most extensive study of inter-annotator agreement yet performed for HMEANT,across two language pairs, we have shown that thedisagreements between annotators make it diffi-cult to reliably compare different MT systems withHMEANT scores.Furthermore, the fact that HMEANT is restrictedto annotating purely verbal predicates results insome important disadvantages.
Ideally we need amore general definition of a frame, not restrictedto purely verbal predicates, and we would liketo be able to link frames.
We should explorethe feasibility of a semantic framework which at-tempts to overcome reliance on syntactic proper-ties such as Universal Conceptual Cognitive An-notation (Abend and Rappoport, 2013).7 AcknowledgementsThe research leading to these results has receivedfunding from the European Union Seventh Frame-work Programme (FP7/2007-2013) under grantagreement 287658 (EU BRIDGE).59ReferencesAbend, Omri and Ari Rappoport.
2013.
?Univer-sal Conceptual Cognitive Annotation (UCCA).
?Proceedings of ACL.Bojar, Ondrej, Milos?
Ercegovc?evic?, Martin Popel,and Omar Zaidan.
2011.
?A Grain of Salt for theWMT Manual Evaluation.?
Proceedings of theSixth Workshop on Statistical Machine Transla-tion, 1?11.
Edinburgh, Scotland.Bojar, Ondrej and Dekai Wu.
2012.
?Towards aPredicate-Argument Evaluation for MT.?
Pro-ceedings of SSST, 30?38.Callison-Burch, Chris, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.2007.
?
(Meta-) evaluation of machine trans-lation.?
Proceedings of the Second Workshopon Statistical Machine Translation, 136?158.Prague, Czech Republic.Callison-Burch, Chris, Philipp Koehn, ChristofMonz, and Omar F Zaidan.
2011.
?Findings ofthe 2011 workshop on statistical machine trans-lation.?
Proceedings of the Sixth Workshop onStatistical Machine Translation, 22?64.Dreyer, Markus and Daniel Marcu.
2012.
?Hyter:Meaning-equivalent semantics for translationevaluation.?
Proceedings of the 2012 Con-ference of the North American Chapter ofthe Association for Computational Linguis-tics: Human Language Technologies, 162?171.Montre?al, Canada.Germann, Ulrich.
2008.
?Yawat: Yet AnotherWord Alignment Tool.?
Proceedings of theACL-08: HLT Demo Session, 20?23.
Colum-bus, Ohio.Gime?nez, Jesu?s and Llu?
?s Ma`rquez.
2007.
?Lin-guistic features for automatic evaluation of het-erogenous mt systems.?
Proceedings of the Sec-ond Workshop on Statistical Machine Transla-tion, StatMT ?07, 256?264.
Stroudsburg, PA,USA.Hutchins, W. J. and H. L. Somers.
1992.
An intro-duction to machine translation.
Academic PressNew York.Jones, Bevan, Jacob Andreas, Daniel Bauer,Karl Moritz Hermann, and Kevin Knight.
2012.?Semantics-based machine translation with hy-peredge replacement grammars.?
Proceedingsof COLING.Koponen, Maarit.
2012.
?Comparing human per-ceptions of post-editing effort with post-editingoperations.?
Proceedings of the Seventh Work-shop on Statistical Machine Translation, 181?190.
Montre?al, Canada.Lavie, Alon and Michael Denkowski.
2009.
?TheMETEOR metric for automatic evaluation ofmachine translation.?
Machine Translation.Linguistic Data Consortium.
2002.
?Lin-guistic data annotation specification: As-sessment of fluency and adequacy inChinese-English translation.?
http://projects.ldc.upenn.edu/TIDES/Translation/TranAssessSpec.pdf.Lo, Chi-kiu, Anand Karthik Tumuluru, and DekaiWu.
2012.
?Fully automatic semantic MT eval-uation.?
Proceedings of WMT, 243?252.Lo, Chi-kiu and Dekai Wu.
2010.
?Evaluatingmachine translation utility via semantic role la-bels.?
Proceedings of LREC, 2873?2877.Lo, Chi-kiu and Dekai Wu.
2011a.
?MEANT : Aninexpensive , high-accuracy , semi-automaticmetric for evaluating translation utility via se-mantic frames.?
Proceedings of ACL, 220?229.Lo, Chi-kiu and Dekai Wu.
2011b.
?Structured vs.flat semantic role representations for machinetranslation evaluation.?
Proceedings of SSST,10?20.Lo, Chi-kiu and Dekai Wu.
2012.
?Unsupervisedvs.
supervised weight estimation for semanticMT evaluation metrics.?
Proceedings of SSST,49?56.Lopez, Adam.
2012.
?Putting human assessmentsof machine translation systems in order.?
Pro-ceedings of WMT, 1?9.NIST.
2005.
?The 2005 NIST machinetranslation evaluation plan (MT-05).
?http://www.itl.nist.gov/iad/mig/tests/mt/2005/doc/mt05_evalplan.v1.1.pdf.Palmer, Martha, Daniel Gildea, and Paul Kings-bury.
2005.
?The proposition bank: An anno-tated corpus of semantic roles.?
ComputationalLinguistics, 31(1):71?106.Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
?BLEU: a method for au-tomatic evaluation of machine translation.?
Pro-ceedings of the Association for ComputationalLinguistics, 311?318.
Philadelphia, USA.Przybocki, Mark, Kay Peterson, Se?bastien Bron-sart, and Gregory Sanders.
2009.
?The NIST602008 metrics for machine translation challen-geoverview, methodology, metrics, and results.
?Machine Translation, 23(2):71?103.Snover, Matthew, Nitin Madnani, Bonnie Dorr,and Richard Schwartz.
2009a.
?Fluency, ad-equacy, or HTER?
exploring different humanjudgments with a tunable MT metric.?
Proceed-ings of the Workshop on Statistical MachineTranslation at the Meeting of the EuropeanChapter of the Association for ComputationalLinguistics (EACL-2009).
Athens, Greece.Snover, Matthew, Nitin Madnani, Bonnie Dorr,and Richard Schwartz.
2009b.
?TER-plus:paraphrase, semantic, and alignment enhance-ments to translation edit rate.?
Machine Trans-lation.Tumuluru, Anand Karthik, Chi-kiu Lo, and DekaiWu.
2012.
?Accuracy and robustness in measur-ing the lexical similarity of semantic role fillersfor automatic semantic MT evaluation.?
Pro-ceedings of PACLIC, 574?581.Weaver, Warren.
1955.
?Translation.?
William N.Locke and Andrew D. Booth (eds.
), MachineTranslation of Languages; Fourteen Essays,15?23.
Cambridge, MA: MIT Press.
Reprint ofa memorandum written in 1949.61
