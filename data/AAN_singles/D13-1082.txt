Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 845?850,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsConverting Continuous-Space Language Models intoN-gram Language Models for Statistical Machine TranslationRui Wang1,2,3, Masao Utiyama2, Isao Goto2, Eiichro Sumita2, Hai Zhao1,3 and Bao-Liang Lu1,31 Center for Brain-Like Computing and Machine Intelligence,Department of Computer Science and Engineering,Shanghai Jiao Tong Unviersity, Shanghai, 200240, China2 Multilingual Translation Laboratory, MASTAR Project,National Institute of Information and Communications Technology3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan3 MOE-Microsoft Key Lab.
for Intelligent Computing and Intelligent SystemsShanghai Jiao Tong Unviersity, Shanghai 200240 Chinawangrui.nlp@gmail.com, mutiyama/igoto/eiichiro.sumita@nict.go.jp, zhaohai@cs.sjtu.edu.cn, bllu@sjtu.edu.cnAbstractNeural network language models, orcontinuous-space language models (CSLMs),have been shown to improve the performanceof statistical machine translation (SMT)when they are used for reranking n-besttranslations.
However, CSLMs have notbeen used in the first pass decoding of SMT,because using CSLMs in decoding takes a lotof time.
In contrast, we propose a methodfor converting CSLMs into back-off n-gramlanguage models (BNLMs) so that we canuse converted CSLMs in decoding.
We showthat they outperform the original BNLMs andare comparable with the traditional use ofCSLMs in reranking.1 IntroductionLanguage models are important in natural languageprocessing tasks such as speech recognition andstatistical machine translation.
Traditionally, back-off n-gram language models (BNLMs) (Chen andGoodman, 1996; Chen and Goodman, 1998;Stolcke, 2002) are being widely used for these tasks.Recently, neural network language models,or continuous-space language models (CSLMs)(Bengio et al 2003; Schwenk, 2007; Le et al 2011)are being used in statistical machine translation(SMT) (Schwenk et al 2006; Son et al 2010;Schwenk et al 2012; Son et al 2012; Niehuesand Waibel, 2012).
These works have shown thatCSLMs can improve the BLEU (Papineni et al2002) scores of SMT when compared with BNLMs,on the condition that the training data for languagemodeling are the same size.
However, in practice,CSLMs have not been widely used in SMT.One reason is that the computational costs oftraining and using CSLMs are very high.
Variousmethods have been proposed to tackle the trainingcost issues (Son et al 2010; Schwenk et al 2012;Mikolov et al 2011).
However, there has been littlework on reducing using costs.
Since the using costsof CSLMs are very high, it is difficult to use CSLMsin decoding directly.A common approach in SMT using CSLMs isthe two pass approach, or n-best reranking.
In thisapproach, the first pass uses a BNLM in decodingto produce an n-best list.
Then, a CSLM is used torerank those n-best translations in the second pass.
(Schwenk et al 2006; Son et al 2010; Schwenk etal., 2012; Son et al 2012)Another approach is using restricted Boltzmannmachines (RBMs) (Niehues and Waibel, 2012)instead of using multi-layer neural networks(Bengio et al 2003; Schwenk, 2007; Le et al2011).
Since probability in a RBM can be calculatedvery efficiently (Niehues and Waibel, 2012), theycan use the RBM language model in SMT decoding.However, the RBM was just used in an adaptation ofSMT, not in a large SMT task, because the trainingcosts of RBMs are very high.The last approach is using a BNLM to simulatea CSLM (Deoras et al 2011; Arsoy et al 2013).
(Deoras et al 2011) used a recurrent neural networklanguage model (RNNLM) to generate a largeamount of text, which was generated by samplingwords from the probability distributions calculatedby the RNNLM.
Then, they trained the BNLM845from the text using the interpolated Kneser-Neysmoothing method.
(Arsoy et al 2013) convertedneural network language models of increasing orderto pruned back-off language models, using lower-order models to constrain the n-grams allowed inhigher-order models.Both of these methods were used in decoding forspeech recognition.
These methods were appliedto not-so-large scale experiments (55 million (M)words for training their BNLMs) (Arsoy et al2013).
In contrast, our method is applied to SMTand can be used to improve a BNLM created from746 M words by using a CSLM trained from 42 Mwords.Because BNLMs can be trained from much largercorpora than those that can be used for trainingCSLMs, improving a BNLM by using a CSLMtrained from a smaller corpus is very important.Actually, a CSLM trained from a smaller corpuscan improve the BLEU scores of SMT if it is usedin the n-best reranking (Schwenk, 2010; Huang etal., 2013).
In contrast, we will demonstrate that aBNLM simulating a CSLM can improve the BLEUscores of SMT in the first pass decoding.Our approach is as follows: (1) First, we train aCSLM (Schwenk, 2007) from a corpus.
(2) Second,we also train a BNLM from the same corpus orlarger corpus.
(3) Finally, we rewrite the probabilityof each n-gram of the BNLM with that probabilitycalculated from the CSLM.We also re-normalize theprobabilities of the BNLM, then use the re-writtenBNLM in SMT decoding.In Section 2, we describe the BNLM and CSLM(Schwenk, 2010) used for re-writing BNLMs.
InSection 3, we describe the method of convertinga CSLM into a BNLM.
In Sections 4 and 5, weevaluate our method and conclude.2 Language ModelsIn this section, we will introduce the standardBNLM and CSLM structure and probabilitycalculation.2.1 Standard back-off ngram language modelA BNLM predicts the probability of a wordwi givenits preceding n ?
1 words hi = wi?1i?n+1.
Butit will suffer from data sparseness if the context,hi, does not appear in the training data.
So anestimation by ?backing-off?
to models with smallerhistories is necessary.
In the case of the modifiedKneser-Ney smoothing (Chen and Goodman, 1998),the probability of wi given hi under a BNLM,Pb(wi|hi), is:Pb(wi|hi) = P?b(wi|hi) + ?
(hi)Pb(wi|wi?1i?n+2) (1)where P?b(wi|hi) is a discounted probability and?
(hi) is the back-off weight.
A BNLM is used witha CSLM as shown below.2.2 CSLM structure and probabilitycalculationThe main structure of a CSLM using a multi-layer neural network contains four layers: the inputlayer projects all words in the context hi ontothe projection layer (the first hidden layer); thesecond hidden layer and the output layer achieve thenon-liner probability estimation and calculate thelanguage model probability P (wi|hi) for the givencontext.
(Schwenk, 2007).The CSLM calculates the probabilities of allwords in the vocabulary of the corpus giventhe context at once.
However, because thecomputational complexity of calculating theprobabilities of all words is quite high, the CSLM isonly used to calculate the probabilities of a subsetof the whole vocabulary.
This subset is calleda short-list, which consists of the most frequentwords in the vocabulary.
The CSLM also calculatesthe sum of the probabilities of all words not in theshort-list by assigning a neuron for that purpose.The probabilities of other words not in the short-listare obtained from a BNLM (Schwenk, 2007;Schwenk, 2010).Let wi, hi be the current word and history.
TheCSLM with a BNLM calculates the probability ofwi given hi, P (wi|hi), as follows:P (wi|hi) ={Pc(wi|hi)1?Pc(o|hi)Ps(hi) if wi ?
short-listPb(wi|hi) otherwise(2)where Pc(?)
is the probability calculated by theCSLM, Pc(o|hi) is the probability of the neuronfor the words not in the short-list, Pb(?)
is theprobability calculated by the BNLM as in Eq.
1,andPs(hi) =?v?short-listPb(v|hi).
(3)846It can be considered that the CSLM redistributesthe probability mass of all words in the short-list.This probability mass is calculated by using theBNLM.3 Conversion of CSLM into BNLMAs described in the introduction, we first train aCSLM from a corpus.
We also train a BNLM fromthe same corpus or a larger corpus.
Then, we rewritethe probability of each ngram in the BNLM with theprobability calculated from the CSLM.First, we use the probabilities of 1-grams inthe BNLM as they are.
Next, we rewrite theprobabilities of n-grams (n=2,3,4,5) in the BNLMwith the probabilities calculated by using the n-gramCSLM, respectively.
Note that the n-gram CSLMmeans that the length of its history is n ?
1.
Notealso that we only need to rewrite the probabilitiesof n-grams ending with a word in the short-list.Finally, we re-normalize the probabilities of theBNLM using the SRILM?s ?-renorm?
option.When we rewrite a BNLM trained from a largercorpus, the ngrams in the BNLM often containunknown words for the CSLM.
In that case, we usethe probabilities in the BNLM as they are.4 Experiments4.1 Common settingsWe used the patent data for the Chinese to Englishpatent translation subtask from the NTCIR-9 patenttranslation task (Goto et al 2011).
The paralleltraining, development, and test data consisted of 1M, 2,000, and 2,000 sentences, respectively.We followed the settings of the NTCIR-9 Chineseto English translation baseline system (Goto et al2011) except that we used various language modelsto compare them.
We used the MOSES phrase-based SMT system (Koehn et al 2003), togetherwith Giza++ (Och and Ney, 2003) for alignment andMERT (Och, 2003) for tuning on the developmentdata.
The translation performance was measured bythe case-insensitive BLEU scores on the tokenizedtest data.
We used mteval-v13a.pl forcalculating BLEU scores.11It is available at http://www.itl.nist.gov/iad/mig/tests/mt/2009/We used the 14 standard SMT features: fivetranslation model scores, one word penalty score,seven distortion scores and one language modelscore.
Each of the different language models wasused to calculate the language model score.As the baseline BNLM, we trained a 5-gramBNLM with modified Kneser-Ney smoothing usingthe English side of the 1 M sentences training data,which consisted of 42 M words.
We did not discardany n-grams in training this model.
That is, wedid not use count cutoffs.
We call this BNLM asBNLM42.A 5-gram CSLM was trained on the same1 M training sentences using the CSLM toolkit(Schwenk, 2010).
The settings for the CSLMwere: projection layer of dimension 256 for eachword, hidden layer of dimension 384 and outputlayer (short-list) of dimension 8192, which wererecommended in the CSLM toolkit.
We call thisCSLM CSLM42.
CSLM42 used BNLM42 as thebackground BNLM.We also trained a larger 5-gram BNLM withmodified Kneser-Ney smoothing by addingsentences from the 2005 US patent data distributedin the NTCIR-8 patent translation task (Fujii et al2010) to the 42 M words.
The data consisted of746 M words.
We call this BNLM BNLM746.
Wediscarded 3,4,5-grams that occurred only once whenwe created BNLM746.Next, we re-wrote BNLM42 with CSLM42 byusing the method described in Section 3.
Thisre-written BNLM was interpolated with BNLM42.The interpolation weight was determined by the gridsearch.
That is, we changed the interpolation weightto 0.1, 0.3, 0.5, 0.7, 0.9 to create an interpolatedBNLM.
Then we used that BNLM in the SMTsystem to tune the weight parameters on the firsthalf of the development data.
Next, we selectedthe interpolation weight that obtained the highestBLEU score on the second half of the developmentdata.
After we selected the interpolation weight,we applied MERT again to the 2,000 sentencedevelopment data to tune the weight parameters.2We call this BNLM CONV42.
We also obtainedCONV746 by re-writing BNLM746 with CSLM422We aware that the interpolation weight might bedetermined by minimizing the perplexity on the developmentdata.
However, we opted to directly maximize the BLEU score.847in the same way.The vocabulary of these language models was thesame, which was extracted from the 1 M trainingsentences.4.2 Experimental resultsTable 1 shows the percent BLEU scores on the testdata.
The figures in the ?1st pass?
column showthe BLEU scores in the first pass decoding whenwe changed the language model.
The figures in the?reranking?
column show the BLEU scores whenwe applied CSLM42 to rerank the 100-best lists forthe different language models.
When we appliedCSLM42 for reranking, we added the CSLM42score as the additional 15th feature.
The weightparameters were tuned by using Z-MERT (Zaidan,2009).LMs 1st pass rerankBNLM42 31.60 32.44CONV42 32.58 32.98BNLM746 32.83 33.36CONV746 33.22 33.54Table 1: Comparison of BLEU scoresWe also performed the paired bootstrap re-sampling test (Koehn, 2004).3 We sampled 2000samples for each significance test.Table 2 shows the results of a statisticalsignificance test, in which the ?1st?
is short forthe ?1st pass?.
The marks indicate whether theLM to the left of a mark is significantly betterthan that above the mark at a certain level.
(???
:significantly better at ?
= 0.01, ?>?
: ?
= 0.05,???
: not significantly better at ?
= 0.05)First, as shown in the tables, the rerankingby applying CSLM42 increased the BLEU scoresfor all language models.
This observation is inaccordance with those of previous work (Schwenk,2010; Huang et al 2013).Second, the reranking results of BNLM42 (32.44)were not better than those of the first pass ofBNLM746 (32.83).
This indicates that if theunderlying BNLM is made from a small corpus, thereranking using CSLM can not compensate for it.3We used the code available at http://www.ark.cs.cmu.edu/MT/.BNLM746(rerank)CONV746(1st)CONV42(rerank)BNLM746(1st)CONV42(1st)BNLM42(rerank)BNLM42(1st)CONV746 (rerank) ?
?
?
?
?
?
?BNLM746 (rerank) ?
?
> ?
?
?CONV746 (1st) ?
?
?
?
?CONV42 (rerank) ?
?
?
?BNLM746 (1st) ?
?
?CONV42 (1st) ?
?BNLM42 (rerank) ?Table 2: Significance tests for systems with different LMsThird, CONV42 was better than BNLM42 forboth first-pass and reranking.
This also holds in thecase of CONV746 and BNLM746.
This indicatedthat our conversion method improved the BNLMs,even if the underlying BNLMwas trained on a largercorpus than that used for training the CSLM.
Asdescribed in the introduction, this is very importantbecause BNLMs can be trained from much largercorpora than those that can be used for trainingCSLMs.
This observation has not been found in theprevious work.In addition, the first-pass of CONV42 andCONV746 (32.58 and 33.22) were comparable withthose of the reranking results of BNLM42 andBNLM746 (32.44 and 33.36), respectively.
That is,there were no significant differences between theseresults.
This indicates that our conversion methodpreserves the performance of the reranking usingCSLM.5 ConclusionWe have proposed a method for converting CSLMsinto BNLMs.
The method can be used to improvea BNLM by using a CSLM trained from a smallercorpus than that used for training the BNLM.
Wehave also shown that BNLMs created by our methodperforms as good as the reranking using CSLMs.Our future work is to compare our conversionmethod with that of (Arsoy et al 2013).44We aware that (Arsoy et al 2013) compared their methodwith the one that is identical with our method.
However, theexperiments were conducted on a speech recognition task andthe scale of the experiment was not so large.
Since we noticedtheir work just before the submission of our paper, we did nothave time to compare their method with our method in SMT.848AcknowledgmentsWe appreciate the helpful discussion with AndrewFinch and Paul Dixon, and three anonymousreviewers for many invaluable comments andsuggestions to improve our paper.
This workis supported by the National Natural ScienceFoundation of China (Grant No.
60903119, No.61170114 and No.
61272248), the NationalBasic Research Program of China (Grant No.2013CB329401) and the Science and TechnologyCommission of Shanghai Municipality (Grant No.13511500200).ReferencesEbru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,and Abhinav Sethy.
2013.
Converting neural networklanguage models into back-off language models forefficient decoding in automatic speech recognition.In Proc.
of IEEE Int.
Conf.
on Acoustics, Speechand Signal Processing (ICASSP 2013), Vancouver,Canada, May.
IEEE.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilisticlanguage model.
Journal of Machine LearningResearch (JMLR), 3:1137?1155, March.Stanley F. Chen and Joshua Goodman.
1996.
Anempirical study of smoothing techniques for languagemodeling.
In Proceedings of the 34th annual meetingon Association for Computational Linguistics, ACL?96, pages 310?318, Santa Cruz, California, June.Association for Computational Linguistics.Stanley F. Chen and Joshua Goodman.
1998.
Anempirical study of smoothing techniques for languagemodeling.
Technical report, Computer Science Group,Harvard Univ.A.
Deoras, T. Mikolov, S. Kombrink, M. Karafiat,and Sanjeev Khudanpur.
2011.
Variationalapproximation of long-span language models for lvcsr.In Acoustics, Speech and Signal Processing (ICASSP),2011 IEEE International Conference on, pages 5532?5535, Prague, Czech Republic, May.
IEEE.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, andTakehito Utsuro.
2010.
Overview of the patenttranslation task at the ntcir-8 workshop.
In InProceedings of the 8th NTCIR Workshop Meetingon Evaluation of Information Access Technologies:Information Retrieval, Question Answering and Cross-lingual Information Access, pages 293?302, Tokyo,Japan, June.Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, andBenjamin K. Tsou.
2011.
Overview of the patentmachine translation task at the NTCIR-9 workshop.In Proceedings of NTCIR-9 Workshop Meeting, pages559?578, Tokyo, Japan, December.Zhongqiang Huang, Jacob Devlin, and SpyrosMatsoukas.
2013.
Bbn?s systems for the chinese-english sub-task of the ntcir-10 patentmt evaluation.In NTCIR-10, Tokyo, Japan, June.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InProceedings of the 2003 Conference of theNorth American Chapter of the Association forComputational Linguistics on Human LanguageTechnology - Volume 1, NAACL ?03, pages 48?54,Edmonton, Canada.
Association for ComputationalLinguistics.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Hai-Son Le, I. Oparin, A. Allauzen, J. Gauvain, andF.
Yvon.
2011.
Structured output layer neuralnetwork language model.
In Acoustics, Speech andSignal Processing (ICASSP), 2011 IEEE InternationalConference on, pages 5524?5527, Prague, CzechRepublic, May.
IEEE.Tomas Mikolov, Anoop Deoras, Daniel Povey, LukasBurget, and Jan Cernock.
2011.
Strategies fortraining large scale neural network language models.In Acoustics, Speech and Signal Processing (ICASSP),2011 IEEE International Conference on, pages 196?201, Prague, Czech Republic, May.
IEEE.Jan Niehues and Alex Waibel.
2012.
Continuous spacelanguage models using restricted boltzmann machines.In Proceedings of the International Workshop forSpoken Language Translation, IWSLT 2012, pages311?318, Hong Kong.Franz Josef Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51, March.Franz Josef Och.
2003.
Minimum error ratetraining in statistical machine translation.
InProceedings of the 41st Annual Meeting of theAssociation for Computational Linguistics, pages160?167, Sapporo, Japan, July.
Association forComputational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting on Association forComputational Linguistics, ACL ?02, pages 311?849318, Philadelphia, Pennsylvania, June.
Association forComputational Linguistics.Holger Schwenk, Daniel Dchelotte, and Jean-LucGauvain.
2006.
Continuous space language modelsfor statistical machine translation.
In Proceedingsof the COLING/ACL on Main conference postersessions, COLING-ACL ?06, pages 723?730, Sydney,Australia, July.
Association for ComputationalLinguistics.Holger Schwenk, Anthony Rousseau, and MohammedAttik.
2012.
Large, pruned or continuous spacelanguage models on a gpu for statistical machinetranslation.
In Proceedings of the NAACL-HLT 2012Workshop: Will We Ever Really Replace the N-gramModel?
On the Future of LanguageModeling for HLT,WLM ?12, pages 11?19, Montreal, Canada, June.Association for Computational Linguistics.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech and Language, 21(3):492?518.Holger Schwenk.
2010.
Continuous-space languagemodels for statistical machine translation.
The PragueBulletin of Mathematical Linguistics, pages 137?146.Le Hai Son, Alexandre Allauzen, Guillaume Wisniewski,and Franc?ois Yvon.
2010.
Training continuousspace language models: some practical issues.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 778?788, Cambridge, Massachusetts,October.
Association for Computational Linguistics.Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous space translation models withneural networks.
In Proceedings of the 2012Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, NAACL HLT ?12, pages39?48, Montreal, Canada, June.
Association forComputational Linguistics.Andreas Stolcke.
2002.
Srilm-an extensible languagemodeling toolkit.
In Proceedings InternationalConference on Spoken Language Processing, pages257?286, November.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.850
