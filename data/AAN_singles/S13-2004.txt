Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 20?24, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsATT1: Temporal Annotation Using Big Windows and Rich Syntactic andSemantic FeaturesHyuckchul Jung and Amanda StentAT&T Labs - Research180 Park AveFlorham Park, NJ 07932, USAhjung, stent@research.att.comAbstractIn this paper we present the results of exper-iments comparing (a) rich syntactic and se-mantic feature sets and (b) big context win-dows, for the TempEval time expression andevent segmentation and classification tasks.We show that it is possible for models usingonly lexical features to approach the perfor-mance of models using rich syntactic and se-mantic feature sets.1 IntroductionTempEval-3 Temporal Annotation Task (UzZamanet al 2012) has three subtasks:A Time expression extraction and classification -extract time expressions from input text, and de-termine the type and normalised value for eachextracted time expression.B Event extraction and classification - extract eventmentions from input text, and determine the class,tense and aspect features for each extracted event.C Temporal link identification - identify and cate-gorise temporal links between events in the sameor consecutive sentences, events and time expres-sions in the same sentence, and events and thedocument creation time of the input text.Here we report results for the first two tasks.Previous TempEval competitions have shown thatrich syntactic and semantic feature sets can lead togood performance on event and time expression ex-traction and classification tasks (e.g.
(Llorens et alType Files EVENT TIMEXAQUAINT gold 73 4431 579TimeBank gold 183 6698 1243TE3-Silver silver 2452 81329 12739Table 1: Frequency of event and time expressions in thetext portions of the TempEval-3 data sets2010; UzZaman and Allen, 2010)).
In this work, weshow that with large windows of context, it is pos-sible for models using only lexical features to ap-proach the performance of models using rich syn-tactic and semantic feature sets.2 DataUsing the gold and silver data distributed by theTempEval-3 task organizers (see Table 1), we pro-cessed each input file with the Stanford CoreNLP(Stanford Natural Language Processing Group,2012) and SENNA (Collobert et al 2011) open-source NLP tools.
From the Stanford CoreNLPtools we obtained a tokenization of the input text,the lemma and part of speech (POS) tag for eachtoken, and dependency and constituency parses foreach sentence.
From SENNA, we obtained a seman-tic role labelling for each sentence.3 ApproachWe were curious to explore the tradeoff between ad-ditional context on the one hand, and additional lay-ers of representation on the other, for the event andtime expression extraction tasks.
Researchers haveinvestigated the impacts of different sets of features(Adafre and de Rijke, 2005; Angeli et al 2012;20Feature type Features Used inLexical 1 token ATT1,ATT2, ATT3Lexical 2 lemma ATT1, ATT2Part of speech POS tag ATT1, ATT2Dependency governing verb, governing verb POS, governing preposition,phrase tag, path to root of parse tree, head word, head word lemma,head word POSATT1, ATT2Constituencyparsegoverning verb, governing verb POS, governing preposition,phrase tag, path to root of parse treeATT1, ATT2Semantic role semantic role label, semantic role labels along path to root of parsetreeATT1Table 2: Features used in our modelsTag type Tagstime expression extraction tags B DATE, B DURATION, B SET, B TIME, I DATE,I DURATION, I SET, I TIME, OEvent expression extraction tags B ACTION, B ASPECTUAL, B ACTION, B OCCURRENCE,B PERCEPTION, B REPORTING, B STATE, OEvent tense FUTURE, INFINITIVE, PAST, PASTPART, PRESENT, PRES-PART, NONE, OEvent aspect PROGRESSIVE, PREFECTIVE PROGRESSIVE, PERFEC-TIVE, NONE, OEvent polarity NEG, POSEvent modality ?D, CAN, CLOSE, COULD, DELETE, HAVE TO, HAVE TO,LIKELIHOOD, MAY, MIGHT, MUST, NONE, O, POSSIBLE,POTENTIAL, SHOULD, SHOULD HAVE TO, TO, UNLIKELY,UNTIL, WOULD, WOULD HAVE TOTable 3: Tags assigned by our classifiers for TempEval-3 tasks A and BRigo and Lavelli, 2011).
In particular, (Rigo andLavelli, 2011) also examined performance based ondifferent sizes of n-grams in a small scale (n=1,3).In this work, we intended to systematically inves-tigate the performance of various models with differ-ent layers of representation (based on much largersets of rich syntactic/semantic features) as well asadditional context.
For each time expression/eventsegmentation/classification task, we trained twelvemodels exploring these two dimensions, three ofwhich we submitted for TempEval-3.Additional layers of representation Wetrained three types of model: (ATT1) STAN-FORD+SENNA, (ATT2) STANFORD and (ATT3)WORDS ONLY.
The basic features used in eachtype of model are given in Table 2: ATT1 modelsinclude lexical, syntactic and semantic features,ATT2 models include only lexical and syntacticfeatures, and ATT3 models include only lexicalfeatures.
For the ATT1 models we had 18 basicfeatures per token, for the ATT2 models we had 16basic features per token, and for the ATT3 modelswe had one basic feature per token.Additional context We experimented with contextwindows of 0, 1, 3, and 7 words preceding and fol-lowing the token to be labeled (i.e.
window sizes of1, 3, 7, and 15).
For each window size, we trainedATT1, ATT2 and ATT3 models.
The ATT1 mod-els had 18 basic features per token in the contextwindow, for up to 15 tokens, so up to 270 basic fea-tures for each token to be labeled.
The ATT2 mod-els had 16 basic features per token in the context21window, so up to 240 basic features for each tokento be labeled.
The ATT3 models had 1 basic featureper token in the context window, so up to 15 basicfeatures for each token to be labeled.Model training For event extraction and classifica-tion, time expression extraction and classification,and event feature classification, we used the machinelearning toolkit LLAMA (Haffner, 2006).
LLAMAencodes multiclass classification problems using bi-nary MaxEnt classifiers to increase the speed oftraining and to scale the method to large data sets.We also used a front-end to LLAMA that builds un-igram, bigram and trigram extended features frombasic features; for example, from the basic feature?go there today?, it would build the features ?go?,?there?, ?today?, ?go there?, ?there today?, and ?gothere today?.
We grouped our basic features (see Ta-ble 2) by type rather than by token, and the LLAMAfront-end then produced ngram features.
We choseLLAMA primarily because of the proven powerof the ngram feature-extraction front-end for NLPtasks.4 Event and Time Expression ExtractionFor event and time expression extraction, we trainedBIO classifiers.
A BIO classifier tags each input to-ken as either Beginning, In, or Out of an event/timeexpression.
Our classifier for events simultaneouslyassigns a B, I or O to each token, and classifies theclass of the event for tokens that Begin or are In anevent.
Our time expression classifier simultaneouslyassigns a B, I, or O to each token, and classifies thetype of the time expression for tokens that Begin orare In a time expression (see Table 3).A BIO model may sometimes be inconsistent; forexample, a token may be labeled as Inside a segmentof a particular type, while the previous token maybe labeled as Out of any segment.
We consideredthe two most likely labels for each token (as long aseach had likelihood at least 0.9), choosing the onemost consistent with the context.5 Event Feature ClassificationWe determined the event features for each extractedevent using four additional classifiers, one each fortense, aspect, polarity and modality.
These classi-fiers were trained only on tokens identified as part ofevent expressions.
Since the event expressions weresingle words for all but a few (erroneous) cases in thesilver data, for determining the event features, weused the same features as before, with the single ad-dition of the event class (during testing, we used thedynamically assigned event class from the event seg-mentation classifier).
As before, we experimentedwith ATT1, ATT2, and ATT3 models.
TempEval-3 only includes evaluation of tense and aspect fea-tures, so we only report for those.
The tags assignedby each classifier are listed in Table 3.6 Time NormalizationTo compute TIMEX3 standard based values forextracted time expressions, we used the TIMEN(Llorens et al 2012) and TRIOS (UzZaman andAllen, 2010) time normalizers.
Values from thenormalizers were validated in post-processing (e.g.?T2445?
is invalid) and, when the normalizers re-turned different non-nil values, TIMEN?s valueswere selected without further reasoning.
Time nor-malization was out of scope in our research for thisevaluation, but it remains as part of our future work.7 Results and DiscussionOur results for event segmentation/classification onthe TempEval-3 test data are provided in Table 4.The absence of semantic features causes only smallchanges in F1.
The absence of syntactic featurescauses F1 to drop slightly (less than 2.5% for allbut the smallest window size), with recall decreasingwhile precision improves somewhat.
Attribute F1 isalso impacted minimally by the absence of semanticfeatures, and about 2-5% by the absence of syntacticfeatures for all but the smallest window size.1Our results for time expression extraction andclassification on the TempEval-3 test data are pro-vided in Table 5.
Here, the performance drops morein the absence of semantic and syntactic features;however, there is an interaction between length oftime expression and performance drop which wemay be able to ameliorate in future work by han-dling consistency issues in the BIO time expressionextraction model better.1In Tables 4 and 5, we present results that are slightly dif-ferent from our submission due to a minor fix in our models byremoving some redundant feature values used twice.22Features Window size F1 P R Class Tense AspectSTANFORD+SENNA 15 (ATT1) 81.16 81.49 80.83 71.60 59.62 73.767 81.08 81.74 80.43 71.49 59.05 73.783 80.35 81.23 79.49 71.41 58.67 73.171 80.94 80.77 81.10 72.37 58.06 73.71STANFORD 15 (ATT2) 80.86 81.02 80.70 71.05 59.10 73.347 81.30 81.90 80.70 71.57 59.01 74.143 80.87 81.58 80.16 71.94 58.96 73.701 80.78 80.72 80.83 71.80 57.47 73.41WORDS ONLY 15 (ATT3) 78.58 81.95 75.47 69.5 55.27 70.767 78.40 82.21 74.93 69.14 55.54 70.273 78.14 82.44 74.26 69.39 52.75 70.381 73.55 79.78 68.23 66.33 44.94 63.15Table 4: Event extraction results (F1, P and R, strict match); feature classification results (attribute F1)Features Window size F1 P R Type ValueSTANFORD+SENNA 15 (ATT1) 80.17 (85.95) 93.27 (100) 70.29 (75.36) 77.69 65.297 76.99 (83.68) 91.09 (99.01) 66.67 (72.46) 75.31 64.443 75.52 (83.82) 88.35 (98.06) 65.94 (73.19) 75.52 63.071 66.12 (83.27) 75.70 (95.33) 58.70 (73.91) 72.65 59.59STANFORD 15 (ATT2) 78.69 (85.25) 90.57 (98.11) 69.57 (75.36) 76.23 65.577 78.51 (84.30) 91.35 (98.08) 68.84 (73.91) 76.03 63.643 78.19 (84.77) 90.48 (98.10) 68.84 (74.64) 75.72 64.201 67.48 (83.74) 76.85 (95.37) 60.14 (74.64) 73.17 59.35WORDS ONLY 15 (ATT3) 72.34 (80.85) 87.63 (97.94) 61.59 (68.84) 74.04 60.437 72.34 (80.85) 87.63 (97.94) 61.59 (67.84) 74.04 59.573 74.48 (82.85) 88.12 (98.02) 64.49 (71.74) 75.31 61.091 44.62 (82.87) 49.56 (92.04) 40.58 (75.36) 70.92 39.84Table 5: Time expression extraction results (F1, P and R, strict match with relaxed match in parentheses); attribute F1for type and value featuresA somewhat surprising finding is that both eventand time expression extraction are subject to rela-tively tight constraints from the lexical context.
Wewere surprised by how well the ATT3 (WORDSONLY) models performed, especially in terms ofprecision.
We were also surprised that the wordsonly models with window sizes of 3 and 7 performedas well as the models with a window size of 15.
Wethink these results are promising for ?big data?
textanalytics, where there may not be time to do heavypreprocessing of input text or to train large models.8 Future WorkFor us, participation in TempEval-3 is a first stepin developing a temporal understanding componentfor text analytics and virtual agents.
We now in-tend to appy our best performing models to this task.In future work, we plan to evaluate our initial re-sults with larger data sets (e.g., cross validation onthe tempeval training data) and experiment with hy-brid/ensemble methods for performing time expres-sion and temporal link extraction.AcknowledgmentsWe thank Srinivas Bangalore, Patrick Haffner, andSumit Chopra for helpful discussions and for sup-plying LLAMA and its front-end for our use.23ReferencesS.
F. Adafre and M. de Rijke.
2005.
Feature engineeringand post-processing for temporal expression recogni-tion using conditional random fields.
In Proceedingsof the ACL Workshop on Feature Engineering for Ma-chine Learning in Natural Language Processing.G.
Angeli, C. D. Manning, and D. Jurafsky.
2012.
Pars-ing time: Learning to interpret time expressions.
InProceedings of the Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies (HLT-NAACL).R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12.P.
Haffner.
2006.
Scaling large margin classifiers for spo-ken language understanding.
Speech Communication,48(3?4).H.
Llorens, E. Saquete, and B. Navarro.
2010.
TIPSem(English and Spanish): Evaluating CRFs and semanticroles in TempEval-2.
In Proceedings of the Interna-tional Workshop on Semantic Evaluation (SemEval).H.
Llorens, L. Derczynski, R. Gaizauskas, and E. Sa-quete.
2012.
Timen: An open temporal expressionnormalisation resource.
In Proceedings of the Interna-tional Conference on Language Resources and Evalu-ation (LREC).S.
Rigo and A. Lavelli.
2011.
Multisex - a multi-language timex sequential extractor.
In Proceedingsof Temporal Representation and Reasoning (TIME).Stanford Natural Language Processing Group.
2012.Stanford CoreNLP.
http://nlp.stanford.edu/software/corenlp.shtml.N.
UzZaman and J. F. Allen.
2010.
TRIPS and TRIOSsystem for TempEval-2: Extracting temporal informa-tion from text.
In Proceedings of the InternationalWorkshop on Semantic Evaluation (SemEval).N.
UzZaman, H. Llorens, J. Allen, L. Derczynski,M.
Verhagen, and J. Pustejovsky.
2012.
Tempeval-3: Evaluating events, time expressions, and tempo-ral relations.
http://arxiv.org/abs/1206.5333v1.24
