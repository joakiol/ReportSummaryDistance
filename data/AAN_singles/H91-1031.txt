Signal Representation  Attr ibute Extract ion and  the UseDist inct ive Features for Phonet ic  Classif ication 1Helen M. Meng, Victor W. Zue, and Hong C. LeungofSpoken Language Systems GroupLaboratory  for  Computer  Sc ienceMassachuset ts  Ins t i tu te  o f  Techno logyCambr idge ,  Massachuset ts  02139ABSTRACTThe study reported in this paper addresses three issues relatedto phonetic lassification: 1) whether it is important to choose anappropriate signal representation, 2) whether there are any ad-vantages in extracting acoustic attributes over directly using thespectral information, and 3) whether it is advantageous to intro-duce an intermediate set of linguistic units, i.e.
distinctive fea-tures.
To restrict he scope of our study, we focused on 16 vowelsin American English, and investigated classification performanceusing an artificial neural network with nearly 22,000 vowels tokensfrom 550 speakers excised from the TIMIT corpus.
Our resultsindicate that 1) the combined outputs of Seneff's auditory modeloutperforms five other representations with both undegraded andnoisy speech, 2) acoustic attributes give similar performance toraw spectral information, but at potentially considerable com-putational savings, and 3) the distinctive feature representationgives similar performance to direct vowel classification, but po-tentially offers a more flexible mechanism for describing contextdependency.INTRODUCTIONThe overall goal of our study is to explore the use of dis-tinctive features for automatic speech recognition.
Distinc-tive features are a set of properties that linguists use to clas-sify phonemes \[1,13\].
More precisely, a feature is a minimalunit which distinguishes two maximally-close phonemes; forexample /b /and/p /are  distinguished by the feature \[voicE\].Sounds are more often confused in relation to the number offeatures they share, and it is believed that around 15 to 20distinctive features are sufficient o account for phonemes inall languages of the world.
Moreover~ the values of these fea-tures, such as \[+HIGH\] or \[-aOUND\], correspond irectly tocontextual variability and coarticulatory phenomena, nd of-ten manifest hemselves as well-defined acoustic orrelates inthe speech signal \[3\].
The compactness and descriptive powerof distinctive features may enable us to describe contextualinfluence more  parsimoniously, and thus to make more effec-tive use of available training data.1This research was supported by DARPA under Contract N00014-82-K-0727, monitored through the Office of Naval Research.In order to fully assess the utility of this linguisticallywell-motivated set of units, several important issues must beaddressed.
First, is there a particular spectral representa-tion that is preferred over others?
Second, should we use thespectral representation directly for phoneme/feature classi-fication, or should we instead extract and use acoustic cor-relates?
Finally, does the introduction of an intermediatefeature-based representation between the signal and the lexi-con offer performance advantages?
We have chosen to answerthese questions by performing a set of phoneme classificationexperiments in which conditional variables are systematicallyvaried.
The usefulness of one condition over another is in-ferred fl'om the performance of the classifier.In this paper, we will report our study on the three ques-tions that we posed earlier.
First, we will report our compar-ative study on signal representations.
Based on these results,we will then describe our experiments and results on acous-tic attribute extraction, and the use of distinctive features.Finally, we will discuss the implications and make some ten-tative conclusions.TASK AND CORPUSThe task chosen for our experiments i the classification 2of vowels in American English.
The corpus consists of 13monothongs /i, I, c, e, a~, a, o, A, o, u, u, ii and 3"/ and 3diphthongs /aY, o y, aw/.
The vowels are excised from theacoustic-phonetically compact portion of the TIMIT corpus\[6\], with no restrictions imposed on the phonetic ontexts ofthe vowels.
For the signal representation study, experimentsare based on the task of classifying all 16 vowels.
However,the dynamic nature of the diphthongs may render distinctivefeature specification ambiguous.
As a result, we excluded thediphthongs in our investigation i volving distinctive features,and the size of the training and test sets were reduced cor-respondingly.
The size and contents of the two corpora aresummarized in Table 1.sit is a classification task in that the left and right boundaries ofthe vowel token are known through a hand-labelling procedure, and theclassifier is only asked to determine the most likely label.176ITraining TestingSpeakers (M/F) Speakers (M/F)(357/143) 50 (33/17)500 (357/143) 50 (33/17)Training TestingTokens Tokens20,000 2,00019,000 1,700Table 1: Corpus I consists of 16 monothong and diphthong vow-els.
It is used for investigation of signal representation.
CorpusII is a subset of Corpus I.
It consists of the monothongs only, andis used for investigation ofdistinctive features.For the experiments dealing with distinctive features, wecharacterized the 13 vowels in terms of 6 distinctive features,following the conventions set forth by others \[13\].
The featurevalues for these vowels are summarized in Table 2.The classifier for our experiments was selected with thefollowing considerations.
First, to facilitate comparisons ofdifferent results, we restrict ourselves to use the same classi-fier for all experiments.
Second, the classifier must be flexiblein that it does not make assumptions about specific statis-tical distributions or distance metrics, since different signalrepresentations may have different characteristics.
Based onthese two constraints, we have chosen to use the multi-layerperceptron (MLP) \[7\].
In our signal representation exper-iments, the network contains 16 output units representingeach of the 16 vowels.
The input layer contains 120 units,40 units each representing the initial, middle, and final thirdof the vowel segment.
For the experiments involving acous-tic attributes and distinctive features, the input layer maybe the spectral vectors, a set of acoustic attributes, or thedistinctive features, and the output layer may be the vowellabels or the distinctive features, as will be described later.All networks have a single hidden layer with 32 hiddenunits.
This and other parameters had previously been adaptedfor better learning capabilities.
In addition, input normaliza-tion and center initialization have been used \[8\].S IGNAL REPRESENTATIONRev iew of  Past  WorkSeveral experiments on comparing signal representationshave been reported in the past.
Mermelstein and Davis \[10\]compared the reel-frequency epstral coefficients (MFCC) withfour other more conventional representations.
They foundthat a set of 10 MFCC resulted in the best performance,suggesting that the reel-frequency epstra possess ignificantadvantages over the other representations.
Hunt and Lefeb-vre \[4\] compared the performance oftheir psychoacoustically-motivated auditory model with that of a 20-channel mel-cepstrum.
They found that the auditory model gave thehighest performance under all conditions, and is least affectedby changes in loudness, interfering noise and spectral shap-ing distortions.
Later, they \[5\] conducted another comparisonwith the auditory model output, the reel-scale cepstrum withvarious weighing schemes, cepstrum coefficients augmentedby the 5-cepstrum coefficients, and the IMELDA represen-tation which combined between-class covariance informationwith within-class covariance information of the reel-scale fil-ter bank outputs to generate a set of linear discriminant func-tions.
The IMELDA outperformed all other representations.HIGHTENSELOWBACKI~OUNDRETIrt0FLEXi I e, ~ ~ a o o A U ~ 0 fi++ + - + +.
.
.
.
.
+ + + + +i+ '+ -.
.
.
.
.
.
+ + - +~+ +!+.
.
.
.
.
.
_ ~ _ _ + - _Table 2: The set of distinctive features used to characterize 13vowelsThese studies generally show that the choice of paramet-ric representations is very important o recognition perfor-mance, and auditory-based representations generally yieldbetter performance than more conventional representations.In the comparison of the psychoacoustically-motivated udi-tory model with MFCC, however, different methods of analy-sis led to different results.
Therefore, it will be interesting tocompare outputs of an auditory model with the computation-ally simpler eel-based representation when the experimentalconditions are more carefully controlled.Exper imenta l  P rocedureOur study compares six acoustic representations \[9\], usingthe MLP classifier.
Three of the representations are obtainedfrom the auditory model proposed by Seneff \[12\].
Two repre-sentations are based on reel-frequency, which has gained pop-ularity in the speech recognition community.
The remainingone is based on conventional Fourier transform.
Attentionis focused upon the relative classification performance of therepresentations, the effects of varying the amount of train-ing data, and the tolerance of the different representations toadditive white noise.For each representation, the speech signal is sampled at 16kHz and a 40-dimensional spectral vector is computed onceevery 5 ms, covering a frequency range of slightly over 6 kHz.To capture the dynamic haracteristics of vowel articulation,three feature vectors, representing the average spectra for theinitial, middle, and final third of every vowel token, are de-termined for each representation.
A 120-dimensional featurevector for the MLP is then obtained by appending the threeaverage vectors.Seneff's auditory model (SAM) produces two outputs: themean-rate response (MR) which corresponds to the meanprobability of firing on the auditory nerve, and the synchronyresponse (SR) which measures the extent of dominance at177the critical band filters' characteristic frequencies.
Each ofthese responses i a 40-dimensional spectral vector.
Since themean-rate and synchrony responses were intended to encodecomplementary acoustic information in the signal, a repre-sentation combining the two is also included by appendingthe first 20 principal components of the MR and SiR to formanother 40-dimensional vector (SAM-PC).To obtain the mel-frequency spectral and cepstral coeffi-cients (MFSC and MFCC, respectively), the signal is pre-emphasized via first differencing and windowed by a 25.6ms Hamming window.
A 256-point discrete Fourier trans-form (DFT) is then computed from the windowed waveform.Following Mermelstein et al\[10\], these Fourier transformcoefficients are later squared, and the resultant magnitudesquared spectrum is passed through the reel-frequency tri-angular filter-banks described below.
The log energy output(in decibels) of each filter, Xk,  k = 1,2,..,40, collectivelyform the 40-dimensional MFSC vector.
Carrying out a co-sine transform \[10\] on the MFSC according to the followingequation yields the MFCC's, Yi, i = 1,2, .., 40.4Ok=lThe lowest cepstrum coefficient, Y0, is excluded to reducesensitivity to overall oudness.The mel-frequency triangular filter banks are designed toresemble the critical band filter bank of SAM.
The filter bankconsists of 40 overlapping triangular filters spanning the fre-quency region from 130 to 6400 Hz.
Thirteen triangles areevenly spread on a linear frequency scale from 130 Hz to 1kHz, and the remaining 27 triangles are evenly distributed ona logarithmic frequency scale from 1 kHz to 6.4 kHz, whereeach subsequent filter is centered at 1.07 times the previousfilter's center frequency.
The area of each triangle is normal-ized to unit magnitude.The Fourier transform representation is obtained by com-puting a 256-point DFT from a smoothed cepstrum, and thendownsampling to 40 points.One of the experiments investigates the relative immunityof each representation to additive white noise.
The noisy testtokens are constructed by adding white noise to the signal toachieve a peak signal-to-noise ratio (SNR) of 20dB, whichcorresponds to a SNR (computed with average nergies) ofslightly below 10dB.ResultsFor each acoustic representation, four separate xperi-ments were conducted using 2,000, 4,000, 8,000, and finally20,000 training tokens.
In general, performance improves asmore training tokens are utilized.
This is illustrated in Fig-ure 1, in which accuracies on training and testing data as afunction of the amount of training tokens for SAM-PC andMFCC.
As the size of the training set increases, so does theclassification accuracy on testing data.
This is accompaniedby a corresponding decrease in performance on training data.At 20,000 training tokens, the difference between training andtesting set performance is about 5% for both representations.SAM PC (train)----O--- MFCC(wain)SAM PCMFCC7O66608c50 1000 10000 100000Number of Training TokensF igure  1: Effect of increasing training data on testing accuraciesTo investigate the relative immunity of the various acous-tic representations to noise degradation, we determine theclassification accuracy of the noise-corrupted test set on thenetworks after they have been fully trained on clean tokens.The results with noisy test speech are shown in Figure 2,together with the corresponding results on the clean test set.The decrease in accuracy ranges from about 12% (for thecombined auditory model) to almost 25% (for the DFT).
"iOBSAM PC Mean Rat?
Synchro~y MFSC MFCC DFFAcoustic RepresentationF igure 2: Performance on noisy and clean speechACOUSTIC  ATTRIBUTES ANDDIST INCT IVE  FEATURESOur experiments were again conducted using an MLPclassifier for speaker independent vow(.\] classification.
Threeexperimental parameters were systematically varied, result-ing in six different conditions, as depicted in Figure 3.
These178three parameters specify whether the acoustic attributes areextracted, whether an intermediate distinctive feature repre-sentation is used, and how the feature values are combinedfor vowel classification.
In some conditions (cf.
conditionsA, E, and F), the spectral vectors from the mean-rate re-sponse were used directly, whereas in others (cf.
conditionsB, C, and D), each vowel token was represented by a setof automatically-extracted acoustic attributes.
In still otherconditions (cf.
conditions c, D, E, and F), an intermediaterepresentation based on distinctive features was introduced.The feature values were either used directly for vowel identi-fication through one bit quantization (i.e.
transforming theminto a binary representation) and table look-up (cf.
condi-tions c and E), or were fed to another MLP for further clas-sification (cf.
conditions D and F).
Taken as a whole, theseexperiments will enable us to answer the questions that weposed earlier.
Thus, for example, we can assess the usefulnessof extracting acoustic attributes by comparing the classifica-tion performance of conditions A versus S and D versus F.(A J ' BVowelsC Features ) C Features )Figure 3: Experimental paradigm comparing direct phoneticclassification with attribute xtraction, and the use of linguisticfeatures.
The mean rate response is chosen to be the signal.Acoustic RepresentationEach vowel token is characterized either directly by a setof spectral coefficients, or indirectly by a set of automaticallyderived acoustic attributes.
In either case, three average vec-tors are used to characterize the left, middle, and right thirdsof the token, in order to implicitly capture the context de-pendency of vowel articulation.Spectral Representation Comparative xperiments des-cribed in the previous section indicate that representationsfrom Seneff's auditory model result in performance superiorto others.
While the combined mean rate and synchronyrepresentation (SAM-PC) gave the best performance, it maynot be an appropriate choice for our present work, since theheterogeneous nature of the representation poses difficultiesin acoustic attribute xtraction.
As a result, we have selectedthe next best representation - the mean rate response (MR).Acoustic Attributes The attributes that we extract areintended to correspond to the acoustic orrelates of distinc-tive features.
However, we do not as yet possess a full under-standing of how these correlates can be extracted robustly.Besides, we must somehow capture the variabilities of thesefeatures across speakers and phonetic environments.
Forthese reasons, we have adopted a more statistical and data-driven approach.
In this approach, a general property de-tector is proposed, and the specific numerical values of thefree parameters are determined from training data using anoptimization criterion \[14\].
In our case, the general propertydetectors chosen are the spectral center of gravity and itsamplitude.
This class of detectors may carry formant infor-mation, and can be easily computed from a given spectralrepresentation.
Specifically, we used the mean rate response,under the assumption that the optimal signal representationfor phonetic classification should also be the most suitablefor defining and quantifying acoustic attributes, from whichdistinctive features can eventually be extracted.The process of attribute xtraction is as follows.
First,the spectrum is shifted down linearly on the bark scale bythe median pitch for speaker normalization.
For each distinc-tive feature, the training tokens are divided into two classes- \[+feature\] and \[-feature\].
The lower and upper frequencyedges (or "free parameters")of the spectral center of grav-ity are chosen so that the resultant measurement can maxi-mize the Fisher's Discriminant Criterion (FDC) between theclasses \[+feature\] and \[-feature\] \[2\].For the features \[BACK\], \[TENSE\], \[ROUND\], and \[RETRO-FLEX\] only one attribute per feature is used.
For \[HIGH\] and\[LOW\], we found it necessary to include two attributes perfeature, using the two sets of optimized free parameters givingthe highest and the second highest FDC.
These 8 frequencyvalues, together with their corresponding amplitudes, makeup 16 attributes for each third of a vowel token.
Therefore,the overall effect of performing acoustic attribute xtractionis to reduce the input dimensions from 120 to 48.ResultsThe results of our experiments are summarized in Fig-ure 4, plotted as classification accuracy for each of the condi-tions shown in Figure 3.
The values in this figure representthe average of six iterations; performance variation amongiterations of the same experiment amounts to about 1%.By comparing the results for conditions A and B, we seethat there is no statistically significant difference in perfor-mance as one replaces the spectral representation by the179ABC6DF40|45 50 55 60 65 70 75 80Per~rmance(%)F igure  4: Performance of the six classification pathways in ourexperimental paradigmacoustic attributes.
This result is further corroborated bythe comparison between conditions c and E, and D and F.Figure 4 shows a significant deterioration i performancewhen one simply maps the feature values to a binary repre-sentation for table look-up (i.e., comparing conditions A to Eand B to C).
We can also examine the accuracies of binaryfeature assignment for each feature, and the results are shownin Figure 5.
The accuracy for individual features ranges from87% to 98%, and there is again little difference between theresults using the mean rate response and using acoustic at-tributes.
It is perhaps not surprising that table look-up us-ing binary feature values result in lower performance, since itwould require that all of the features be identified correctly.12(\]tlG g"~ 80~ 7o50Mean RamAcoustic Attributesm ~HIGH LOW BACK TENSE ROUND RETROFLEX AU.,Distinctive FeatureFigure 5: Distinctive features mapping accuracies for the meanrate response and acoustic attributesHowever, when we use a second MLP to classify the fea-tures into vowels, a considerable improvement (> 4%) is ob-tained to the extent hat the resulting accuracy is again com-parable to other conditions (cf.
conditions A and F, and con-ditions B and D).DISCUSSIONOur results indicate that, on a fully trained network, rep-resentations based on auditory modelling consistently out-perform other representations.
The best among the threeauditory-based representations, SAM PC, achieved a top-choice accuracy of 66%.The MFSC and MFCC representations performed worsethan the auditory-based representations and slightly betterthat the DFT.
At first glance, it may appear that the dis-crepancies are small, since the error rate is only increasedslightly (from 33% to 38%).
However, previous research onhuman and machine identification of vowels, independent ofcontext, have shown that the best performance attained isaround 65% \[11\].
Looking in this light, the difference in per-formance becomes much more significant.
One legitimateconcern may be that principal component analysis has beenapplied to SAM PC, but not to MFCC.
However, the cosinetransform used in obtaining the MFCC performs a similarfunction to principal component analysis.
Experiments havebeen conducted using the first 40 principal components ofMFCC, and the classification accuracy (61.3%) shows thatprincipal component analysis has no statistically significanteffects on performance.
It may also be argued that too manyMFCC coefficients have been used, and this may degrade itsperformance.
But further experiments have shown that clas-sification accuracy increases with the number of MFCC used,and using 40 MFCC yielded the highest performance.
There-fore, we may tentatively conclude that auditory-based signalrepresentations are preferred, at least within the bounds ofour experimental conditions.Performance on noisy speech for the various representa-tions follows the trend of that on clean speech, with the ex-ception that the range of accuracy increased substantially.The degradation of the SAM representations was least se-vere - about 12%, whereas the reel-representations showed adrop of 17%.
The DFT is most affected by noise, and itsperformance degraded by over 24%.
We believe that train-ing with clean speech and testing with noisy speech is a fairexperimental paradigm since the noise level of test speech isoften unknown in practice, but the environment for recordingtraining speech can always be controlled.Our investigation on the use of acoustic attributes i partlymotivated by the belief that these attributes can enhancephonetic ontrasts by focusing upon relevant information inthe signal, thereby leading to improved phonetic classifica-tion performance when only a finite amount of training datais available.
The acoustic attributes that we have chosen areintuitively reasonable and easy to measure.
But they are byno means optimum, since we did not set out to design thebest set of attributes for enhancing vowel contrasts.
Never-theless, their use has led to performance comparable to thedirect use of spectral information.
With an improved under-standing of the relationship between distinctive features and180I Pathway \] A I B I C 24~0 40E64 47F04 Connections 4288 1984 1760Table 3: Sizes of the networks in our experimental paradigm.their acoustic correlates, and a little more care in the de-sign and extraction of these attributes, it is conceivable thatbetter classification accuracy can be obtained.Another advantage ofusing acoustic attributes i a savingon run-time computations through reduction of input dimen-sions.
Table 3 compares the total number of connections inthe one or more MLP within each condition in our experi-mental paradigm.
With a small amount of preprocessing, theuse of acoustic attributes can save about half of the compu-tations required by the direct use of spectral representation.One potential source of discrepancy in our experimentshas to do with pitch normalization, which was not performedon the mean-rate response.
However, a pitch-normalizedspectral center of gravity measure was used to extract acous-tic attributes, ince it can eliminate singularities that compli-cate the search for a maximum FDC value in the optimizationprocess.
However, this advantage is obtained sometimes atthe expense of getting a lower FDC value, thus leading topoorer performance.
While we do not feel that pitch nor-malization has any significant effect on the outcome of ourexperiments, further experiments are clearly necessary.To introduce a set of linguistically motivated istinctivefeatures as an intermediate r presentation forphoneti c classi-fication, we first transform the acoustic representations intoa set of features, and then map the features into vowel la-bels.
While one may argue that such a two-step process isinherently sub-optimal, we nevertheless were able to obtaincomparable performance, corroborating the findings of Leung\[7\].
Such an intermediate r presentation can offer us a greatdeal of flexibility in describing contextual variations.
For ex-ample, all vowels sharing the feature \[+ROUND\] will affect heacoustic properties of neighboring consonants in predictableways, which can be described more parsimoniously.
By de-scribing context dependencies this way, we can also make useof training data more effectively by collapsing all availabledata along a given feature dimension.Figure 5 shows that performance on some features i  worsethan others, presumably due to inadequacies in the attributesthat we use.
For example, performance on the feature \[TENSE\]should be improved by incorporating segment duration as anadditional attribute.
When a second classifier is used to mapthe feature values into vowel abels, a 4-5% accuracy increaseis realized such that the performance is again comparable tocases without this intermediate f ature representation.
Thisresult suggests that the acoustic-phonetic information is pre-served in the aggregate of the features, and that the sub-sequent performance r covery may be a consequence of theredundant nature of distinctive features, as well as the abilityby the second classifier to capture various contextual effects.Based on the results of our experiments, we may tenta-tively conclude that the auditory-based representations arepreferred.
Furthermore, the use of acoustic attributes cansignificantly reduce run-time computation for vowel classifi-cation with no cost to accuracy.
Finally, the introduction ofan intermediate r presentation based on distinctive featurescan potentially provide us with a flexible framework to de-scribe contextual variations and make more effective use oftraining data, again at no cost to classification performance.REFERENCES\[1\] Chomsky N. and M. Halle, Sound Pattern of English, HarperRow, 1968.\[2\] Duda, 1~.O.
and P.E.
Hart, "Pattern Classification and SceneAnalysis", a Wiley-Interscience publication, 1973.\[3\] Fant, G., "Manual of Phonetics, ch.
8, edited by Bertil Maim-berg", North-Holland Publishing Company, 1970.\[4\] Hunt, M. and C. Lefebvre, "Speaker Dependent and Inde-pendent Speech l~ecoguition Experiments with an AuditoryModel", Proc.
ICASSP-88, New York, 1988.\[5\] Hunt, M. and C. Lefebvre, "A Comparison of Several Acous-tic Representation for Speech Recognition with Degradedand Undegraded Speech", Proc.
ICASSP-89, 1989.\[6\] Lamel, L.F., R.H. Kassel, and S. Seneff, "Speech DatabaseDevelopment: Design and Analysis of the Acoustic-PhoneticCorpus", Proc.
DARPA Speech Recognition Workshop, Re-port No.
SAIC-86/1546, February 1986.\[7\] Leung, H.C. "The Use of Artificial Neural Networks for Pho-netic Recognition," Ph.D. Thesis, MIT Depart.
of Elect.
En-gin.
and Comp.
Sci., Cambridge, MA, 1989.\[8\] Leung, H.C. and V.W.
Zue, "Phonetic Classification UsingMulti-Layer Perceptrons", Proc.
ICASSP-90, 1990.\[9\] Meng, H.M. and V.W.
Zue, " A Comparative Study ofAcoustic Representations of Speech for Vowel Classifica-tion using Multi-Layer Perceptrons", Proc.
ICSLP-90, Kobe,1990.\[10\] Mermelstein, P. and S. Davis, "Comparison of ParametricRepresentations forMonosyllabic Word l~ecoguition i Con-tinuously Spoken Sentences", IEEE Transactions on Acous-tics, Speech and Signal Processing, August 1980.\[11\] Phillips, M.S., "Speaker Independent Classification of Vowelsand Diphthongs in Continuous Speech, Proc.
of the 11th In-ternational Congress of Phonetic Sciences, Estonia, USSP~,1987.\[12\] Seneff, S., "A Joint Synchrony/Mean Rate Model of Audi-tory Speech Processing", J. of Phonetics, January 1988.\[13\] Stevens, K.N., Unpublished course notes for Speech Commu-nications, Department of Electrical Engineering and Com-puter Science, MIT, Spring term, 1989.\[14\] Zue, V.W., J.l~.
Glass, M.S.
Phillips, and S. Seneff, "Acous-tic Segmentation a d Phonetic Classification in the SUMMITSystem", Proc.
ICASSP-89, Scotland, 1989.181
