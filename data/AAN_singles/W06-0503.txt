Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 18?25,Sydney, July 2006. c?2006 Association for Computational LinguisticsLEILA: Learning to Extract Information by Linguistic AnalysisFabian M. SuchanekMax-Planck-Institutefor Computer ScienceSaarbru?cken/Germanysuchanek@mpii.mpg.deGeorgiana IfrimMax-Planck-Institutefor Computer ScienceSaarbru?cken/Germanyifrim@mpii.mpg.deGerhard WeikumMax-Planck-Institutefor Computer ScienceSaarbru?cken/Germanyweikum@mpii.mpg.deAbstractOne of the challenging tasks in the con-text of the Semantic Web is to automati-cally extract instances of binary relationsfrom Web documents ?
for example allpairs of a person and the correspondingbirthdate.
In this paper, we present LEILA,a system that can extract instances of ar-bitrary given binary relations from natu-ral language Web documents ?
withouthuman interaction.
Different from previ-ous approaches, LEILA uses a deep syn-tactic analysis.
This results in consistentimprovements over comparable systems(such as e.g.
Snowball or TextToOnto).1 Introduction1.1 MotivationSearch engines, question answering systems andclassification systems alike can greatly profit fromformalized world knowledge.
Unfortunately, man-ually compiled collections of world knowledge(such as e.g.
WordNet (Fellbaum, 1998)) oftensuffer from low coverage, high assembling costsand fast aging.
In contrast, the World Wide Webprovides an enormous source of knowledge, as-sembled by millions of people, updated constantlyand available for free.
Since the Web data con-sists mostly of natural language documents, a firststep toward exploiting this data would be to ex-tract instances of given target relations.
For exam-ple, one might be interested in extracting all pairsof a person and her birthdate (the birthdate-relation), pairs of a company and the city of itsheadquarters (the headquarters-relation) orpairs of an entity and the concept it belongs to (theinstanceOf-relation).
The task is, given a setof Web documents and given a target relation, ex-tracting pairs of entities that are in the target rela-tion.
In this paper, we propose a novel method forthis task, which works on natural language Webdocuments and does not require human interac-tion.
Different from previous approaches, our ap-proach involves a deep linguistic analysis, whichhelps it to achieve a superior performance.1.2 Related WorkThere are numerous Information Extraction (IE)approaches, which differ in various features:?
Arity of the target relation: Some systems aredesigned to extract unary relations, i.e.
sets ofentities (Finn and Kushmerick, 2004; Califf andMooney, 1997).
In this paper we focus on themore general binary relations.?
Type of the target relation: Some systemsare restricted to learning a single relation,mostly the instanceOf-relation (Cimianoand Vo?lker, 2005b; Buitelaar et al, 2004).In this paper, we are interested in extractingarbitrary relations (including instanceOf).Other systems are designed to discover newbinary relations (Maedche and Staab, 2000).However, in our scenario, the target relation isgiven in advance.?
Human interaction: There are systems that re-quire human intervention during the IE process(Riloff, 1996).
Our work aims at a completelyautomated system.?
Type of corpora: There exist systems that canextract information efficiently from formatteddata, such as HTML-tables or structured text(Graupmann, 2004; Freitag and Kushmerick,2000).
However, since a large part of the Webconsists of natural language text, we consider inthis paper only systems that accept also unstruc-tured corpora.?
Initialization: As initial input, some systemsrequire a hand-tagged corpus (J. Iria, 2005;Soderland et al, 1995), other systems requiretext patterns (Yangarber et al, 2000) or tem-plates (Xu and Krieger, 2003) and again oth-ers require seed tuples (Agichtein and Gravano,2000; Ruiz-Casado et al, 2005; Mann andYarowsky, 2005) or tables of target concepts(Cimiano and Vo?lker, 2005a).
Since hand-18labeled data and manual text patterns requirehuge human effort, we consider only systemsthat use seed pairs or tables of concepts.Furthermore, there exist systems that use thewhole Web as a corpus (Etzioni et al, 2004) or thatvalidate their output by the Web (Cimiano et al,2005).
In order to study different extraction tech-niques in a controlled environment, however, werestrict ourselves to systems that work on a closedcorpus for this paper.One school of extraction techniques concen-trates on detecting the boundary of interesting en-tities in the text, (Califf and Mooney, 1997; Finnand Kushmerick, 2004; Yangarber et al, 2002).This usually goes along with the restriction tounary target relations.
Other approaches makeuse of the context in which an entity appears(Cimiano and Vo?lker, 2005a; Buitelaar and Ra-maka, 2005).
This school is mostly restricted tothe instanceOf-relation.
The only group thatcan learn arbitrary binary relations is the groupof pattern matching systems (Etzioni et al, 2004;Agichtein and Gravano, 2000; Ravichandran andHovy, 2002; Brin, 1999; Soderland, 1999; Xu etal., 2002; Ruiz-Casado et al, 2005; Mann andYarowsky, 2005).
Surprisingly, none of these sys-tems uses a deep linguistic analysis of the cor-pus.
Consequently, most of them are extremelyvolatile to small variations in the patterns.
For ex-ample, the simple subordinate clause in the fol-lowing example (taken from (Ravichandran andHovy, 2002)) can already prevent a surface pat-tern matcher from discovering a relation between?London?
and the ?river Thames?
: ?London, which hasone of the busiest airports in the world, lies on the banksof the river Thames.
?1.3 ContributionThis paper presents LEILA (Learning to ExtractInformation by Linguistic Analysis), a system thatcan extract instances of an arbitrary given binaryrelation from natural language Web documentswithout human intervention.
LEILA uses a deepanalysis for natural-language sentences as well asother advanced NLP methods like anaphora reso-lution, and combines them with machine learningtechniques for robust and high-yield informationextraction.
Our experimental studies on a varietyof corpora demonstrate that LEILA achieves verygood results in terms of precision and recall andoutperforms the prior state-of-the-art methods.1.4 Link GrammarsThere exist different approaches for parsing nat-ural language sentences.
They range from sim-ple part-of-speech tagging to context-free gram-mars and more advanced techniques such as Lex-ical Functional Grammars, Head-Driven PhraseStructure Grammars or stochastic approaches.
Forour implementation, we chose the Link GrammarParser (Sleator and Temperley, 1993).
It is basedon a context-free grammar and hence it is simplerto handle than the advanced parsing techniques.At the same time, it provides a much deeper se-mantic structure than the standard context-freeparsers.
Figure 1 shows a simplified example ofa linguistic structure produced by the link parser(a linkage).A linkage is a connected planar undirectedgraph, the nodes of which are the words of the sen-tence.
The edges are called links.
They are labeledwith connectors.
For example, the connector subjin Figure 1 marks the link between the subject andthe verb of the sentence.
The linkage must ful-fill certain linguistic constraints, which are givenby a link grammar.
The link grammar specifieswhich word may be linked by which connector topreceding and following words.
Furthermore, theparser assigns part-of-speech tags, i.e.
symbolsidentifying the grammatical function of a word ina sentence.
In the example in Figure 1, the let-ter ?n?
following the word ?composers?
indentifies?composers?
as a noun.Chopin was.v     great  among the composers.n of   his  time.nsubj compl modprepObjmodprepObjdetdetFigure 1: A simple linkageFigure 2 shows how the Link Parser copes with amore complex example.
The relationship betweenthe subject ?London?
and the verb ?lies?
is not dis-rupted by the subordinate clause:London, which has one of the busiest airports, lies on the banks of the river Thames.subjmod subj obj prepprepObjdetsup modprepObjdet modprepObjdet grpFigure 2: A complex linkageWe say that a linkage expresses a relation r, ifthe underlying sentence implies that a pair of enti-ties is in r. Note that the deep grammatical anal-ysis of the sentence would allow us to define themeaning of the sentence in a theoretically well-founded way (Montague, 1974).
For this paper,however, we limit ourselves to an intuitive under-standing of the notion of meaning.We define a pattern as a linkage in which two19words have been replaced by placeholders.
Figure3 shows a pattern derived from the linkage in Fig-ure 1 by replacing ?Chopin?
and ?composers?
by theplaceholders ?X?
and ?Y?.X       was.v       great  among the       Y        of  his    time.nsubj compl modprepObjmodprepObjdetdetFigure 3: A patternWe call the (unique) shortest path from oneplaceholder to the other the bridge, marked in boldin the figure.
The bridge does not include theplaceholders.
Two bridges are regarded as equiva-lent, if they have the same sequence of nodes andedges, although nouns and adjectives are allowedto differ.
For example, the bridge in Figure 3 andthe bridge in Figure 4 (in bold) are regarded asequivalent, because they are identical except fora substitution of ?great?
by ?mediocre?.
A patternmatches a linkage, if an equivalent bridge occursin the linkage.
For example, the pattern in Figure3 matches the linkage in Figure 4.Mozart was.v clearly mediocre  among the composers.n.subjcomplmodprepObjdetmodFigure 4: A matching linkageIf a pattern matches a linkage, we say that thepattern produces the pair of words that the link-age contains in the position of the placeholders.In Figure 4, the pair ?Mozart?
/ ?composers?
is pro-duced by the pattern in Figure 3.2 System Description2.1 Document Pre-ProcessingLEILA accepts HTML documents as input.
Toallow the system to handle date and number ex-pressions, we normalize these constructions byregular expression matching in combination witha set of functions.
For example, the expression?November 23rd to 24th 1998?
becomes ?1998-11-23to 1998-11-24?
and the expression ?0.8107 acre-feet?becomes ?1000 cubic-meters?.
Then, we split theoriginal HTML-document into two files: The firstfile contains the proper sentences with the HTML-tags removed.
The second file contains the non-grammatical parts, such as lists, expressions us-ing parentheses and other constructions that can-not be handled by the Link Parser.
For example,the character sequence ?Chopin (born 1810) was agreat composer?
is split into the sentence ?Chopinwas a great composer?
and the non-grammatical in-formation ?Chopin (born 1810)?.
The grammaticalfile is parsed by the Link Parser.The parsing allows for a restricted named entityrecognition, because the parser links noun groupslike ?United States of America?
by designated con-nectors.
Furthermore, the parsing allows us to doanaphora resolution.
We use a conservative ap-proach, which simply replaces a third person pro-noun by the subject of the preceding sentence.For our goal, it is essential to normalize nounsto their singular form.
This task is non-trivial,because there are numerous words with irregularplural forms and there exist even word forms thatcan be either the singular form of one word or theplural form of another.
By collecting these excep-tions systematically from WordNet, we were ableto stem most of them correctly with our Plural-to-Singular Stemmer (PlingStemmer1).
For the non-grammatical files, we provide a pseudo-parsing,which links each two adjacent items by an artifi-cial connector.
As a result, the uniform output ofthe preprocessing is a sequence of linkages, whichconstitutes the input for the core algorithm.2.2 Core AlgorithmAs a definition of the target relation, our algorithmrequires a function (given by a Java method) thatdecides into which of the following categories apair of words falls:?
The pair can be an example for the target re-lation.
For instance, for the birthdate-relation, the examples can be given by a list ofpersons with their birth dates.?
The pair can be a counterexample.
For thebirthdate-relation, the counterexamples canbe deduced from the examples (e.g.
if ?Chopin?/ ?1810?
is an example, then ?Chopin?
/ ?2000?must be a counterexample).?
The pair can be a candidate.
For birthdate,the candidates would be all pairs of a propername and a date that are not an example or acounterexample.?
The pair can be none of the above.The core algorithm proceeds in three phases:1.
In the Discovery Phase, it seeks linkages inwhich an example pair appears.
It replaces thetwo words by placeholders, thus producing apattern.
These patterns are collected as positivepatterns.
Then, the algorithm runs through thesentences again and finds all linkages that match1available at http://www.mpii.mpg.de/ ?suchanek20a positive pattern, but produce a counterexam-ple.
The corresponding patterns are collected asnegative patterns2.2.
In the Training Phase, statistical learning is ap-plied to learn the concept of positive patterns.The result of this process is a classifier for pat-terns.3.
In the Testing Phase, the algorithm considersagain all sentences in the corpus.
For each link-age, it generates all possible patterns by replac-ing two words by placeholders.
If the two wordsform a candidate and the pattern is classified aspositive, the produced pair is proposed as a newelement of the target relation (an output pair).In principle, the core algorithm does not depend ona specific grammar or a specific parser.
It can workon any type of grammatical structures, as long assome kind of pattern can be defined on them.
It isalso possible to run the Discovery Phase and theTesting Phase on different corpora.2.3 Learning ModelThe central task of the Discovery Phase is deter-mining patterns that express the target relation.These patterns are generalized in the TrainingPhase.
In the Testing Phase, the patterns are usedto produce the output pairs.
Since the linguisticmeaning of the patterns is not apparent to the sys-tem, the Discovery Phase relies on the followinghypothesis: Whenever an example pair appearsin a sentence, the linkage and the correspondingpattern express the target relation.
This hypoth-esis may fail if a sentence contains an examplepair merely by chance, i.e.
without expressing thetarget relation.
Analogously, a pattern that doesexpress the target relation may occasionally pro-duce counterexamples.
We call these patterns falsesamples.
Virtually any learning algorithm can dealwith a limited number of false samples.To show that our approach does not dependon a specific learning algorithm, we implementedtwo classifiers for LEILA: One is an adaptive k-Nearest-Neighbor-classifier (kNN) and the otherone uses a Support Vector Machine (SVM).
Theseclassifiers, the feature selection and the statisticalmodel are explained in detail in (Suchanek et al,2006).
Here, we just note that the classifiers yielda real valued label for a test pattern.
This valuecan be interpreted as the confidence of the classifi-cation.
Thus, it is possible to rank the output pairsof LEILA by their confidence.2Note that different patterns can match the same linkage.3 Experiments3.1 SetupWe ran LEILA on different corpora with increasingheterogeneity:?
Wikicomposers: The set of all Wikipedia arti-cles about composers (872 HTML documents).We use it to see how LEILA performs on a docu-ment collection with a strong structural and the-matic homogeneity.?
Wikigeography: The set of all Wikipediapages about the geography of countries (313HTML documents).?
Wikigeneral: A set of random Wikipedia arti-cles (78141 HTML documents).
We chose it toassess LEILA?s performance on structurally ho-mogenous, but thematically random documents.?
Googlecomposers: This set contains one doc-ument for each baroque, classical, and roman-tic composer in Wikipedia?s list of composers,as delivered by a Google ?I?m feeling lucky?search for the composer?s name (492 HTMLdocuments).
We use it to see how LEILA per-forms on a corpus with a high structural hetero-geneity.
Since the querying was done automat-ically, the downloaded pages include spuriousadvertisements as well as pages with no propersentences at all.We tested LEILA on different target relations withincreasing complexity:?
birthdate: This relation holds between a personand his birth date (e.g.
?Chopin?
/ ?1810?).
It iseasy to learn, because it is bound to strong sur-face clues (the first element is always a name,the second is always a date).?
synonymy: This relation holds between twonames that refer to the same entity (e.g.
?UN?/?United Nations?).
The relation is more so-phisticated, since there are no surface clues.?
instanceOf: This relation is even more sophis-ticated, because the sentences often express itonly implicitly.We compared LEILA to different competitors.
Weonly considered competitors that, like LEILA, ex-tract the information from a corpus without usingother Internet sources.
We wanted to avoid run-ning the competitors on our own corpora or on ourown target relations, because we could not be sureto achieve a fair tuning of the competitors.
Hencewe ran LEILA on the corpora and the target rela-tions that our competitors have been tested on bytheir authors.
We compare the results of LEILAwith the results reported by the authors.
Our com-petitors, together with their respective corpora andrelations, are:21?
TextToOnto3: A state-of-the-art representativefor non-deep pattern matching.
The system pro-vides a component for the instanceOf rela-tion and takes arbitrary HTML documents as in-put.
For completeness, we also consider its suc-cessor Text2Onto (Cimiano and Vo?lker, 2005a),although it contains only default methods in itscurrent state of development.?
Snowball (Agichtein and Gravano, 2000):A recent representative of the slot-extractionparadigm.
In the original paper, Snowball hasbeen tested on the headquarters relation.This relation holds between a company and thecity of its headquarters.
Snowball was trainedon a collection of some thousand documentsand then applied to a test collection.
For copy-right reasons, we only had access to the test col-lection (150 text documents).?
(Cimiano and Vo?lker, 2005b) present a new sys-tem that uses context to assign a concept toan entity.
We will refer to this system as theCV-system.
The approach is restricted to theinstanceOf-relation, but it can classify in-stances even if the corpus does not contain ex-plicit definitions.
In the original paper, the sys-tem was tested on a collection of 1880 files fromthe Lonely Planet Internet site4.For the evaluation, the output pairs of the sys-tem have to be compared to a table of ideal pairs.One option would be to take the ideal pairs from apre-compiled data base.
The problem is that theseideal pairs may differ from the facts expressed inthe documents.
Furthermore, these ideal pairs donot allow to measure how much of the documentcontent the system actually extracted.
This is whywe chose to extract the ideal pairs manually fromthe documents.
In our methodology, the ideal pairscomprise all pairs that a human would understandto be elements of the target relation.
This involvesfull anaphora resolution, the solving of referenceambiguities, and the choice of truly defining con-cepts.
For example, we accept Chopin as instanceof composer but not as instance of member,even if the text says that he was a member of someclub.
Of course, we expect neither the competi-tors nor LEILA to achieve the results in the idealtable.
However, this methodology is the only fairway of manual extraction, as it is guaranteed tobe system-independent.
If O denotes the multi-set of the output pairs and I denotes the multi-setof the ideal pairs, then precision, recall, and their3http://www.sourceforge.net/projects/texttoonto4http://www.lonelyplanet.com/harmonic mean F1 can be computed asrecall = |O ?
I||I| precision =|O ?
I||O|F1 = 2 ?
recall ?
precisionrecall + precision .To ensure a fair comparison of LEILA to Snow-ball, we use the same evaluation as employed inthe original Snowball paper (Agichtein and Gra-vano, 2000), the Ideal Metric.
The Ideal Metricassumes the target relation to be right-unique (i.e.a many-to-one relation).
Hence the set of idealpairs is right-unique.
The set of output pairs canbe made right-unique by selecting the pair with thehighest confidence for each first component.
Du-plicates are removed from the ideal pairs and alsofrom the output pairs.
All output pairs that havea first component that is not in the ideal set areremoved.There is one special case for the CV-system,which uses the Ideal Metric for the non-right-unique instanceOf relation.
To allow for a faircomparison, we used the Relaxed Ideal Metric,which does not make the ideal pairs right-unique.The calculation of recall is relaxed as follows:recall = |O ?
I||{x|?y : (x, y) ?
I}|Due to the effort, we could extract the ideal pairsonly for a sub-corpus.
To ensure significance inspite of this, we compute confidence intervals forour estimates: We interpret the sequence of out-put pairs as a repetition of a Bernoulli-experiment,where the output pair can be either correct (i.e.contained in the ideal pairs) or not.
The parameterof this Bernoulli-distribution is the precision.
Weestimate the precision by drawing a sample (i.e.by extracting all ideal pairs in the sub-corpus).
Byassuming that the output pairs are identically in-dependently distributed, we can calculate a confi-dence interval for our estimation.
We report confi-dence intervals for precision and recall for a con-fidence level of ?
= 95%.
We measure precisionat different levels of recall and report the valuesfor the best F1 value.
We used approximate stringmatching techniques to account for different writ-ings of the same entity.
For example, we countthe output pair ?Chopin?
/ ?composer?
as correct,even if the ideal pairs contain ?Frederic Chopin?
/?composer?.
To ensure that LEILA does not justreproduce the example pairs, we list the percent-age of examples among the output pairs.
Duringour evaluation, we found that the Link Grammarparser does not finish parsing on roughly 1% ofthe files for unknown reasons.22Table 1: Results with different relationsCorpus Relation System #D #O #C #I Precision Recall F1 %EWikicomposers birthdate LEILA(SVM) 87 95 70 101 73.68%?
8.86% 69.31%?
9.00% 71.43% 4.29%Wikicomposers birthdate LEILA(kNN) 87 90 70 101 78.89%?
8.43% 70.30%?
8.91% 74.35% 4.23%Wikigeography synonymy LEILA(SVM) 81 92 74 164 80.43%?
8.11% 45.12%?
7.62% 57.81% 5.41%Wikigeography synonymy LEILA(kNN) 81 143 105 164 73.43%?
7.24% 64.02%?
7.35% 68.40% 4.76%Wikicomposers instanceOf LEILA(SVM) 87 685 408 1127 59.56%?
3.68% 36.20%?
2.81% 45.03% 6.62%Wikicomposers instanceOf LEILA(kNN) 87 790 463 1127 58.61%?
3.43% 41.08%?
2.87% 48.30% 7.34%Wikigeneral instanceOf LEILA(SVM) 287 921 304 912 33.01%?
3.04% 33.33%?
3.06% 33.17% 3.62%Googlecomposers instanceOf LEILA(SVM) 100 787 210 1334 26.68%?
3.09% 15.74%?
1.95% 19.80% 4.76%Googlecomposers instanceOf LEILA(kNN) 100 840 237 1334 28.21%?
3.04% 17.77%?
2.05% 21.80% 8.44%Googlec.+Wikic.
instanceOf LEILA(SVM) 100 563 203 1334 36.06%?
3.97% 15.22%?
1.93% 21.40% 5.42%Googlec.+Wikic.
instanceOf LEILA(kNN) 100 826 246 1334 29.78%?
3.12% 18.44%?
2.08% 22.78% 7.72%#O ?
number of output pairs #D ?
number of documents in the hand-processed sub-corpus#C ?
number of correct output pairs %E ?
proportion of example pairs among the correct output pairs#I ?
number of ideal pairs Recall and Precision with confidence interval at ?
= 95%3.2 Results3.2.1 Results on different relationsTable 1 summarizes our experimental resultswith LEILA on different relations.
For the birth-date relation, we used Edward Morykwas?
list offamous birthdays5 as examples.
As counterexam-ples, we chose all pairs of a person that was in theexamples and an incorrect birthdate.
All pairs ofa proper name and a date are candidates.
We ranLEILA on the Wikicomposer corpus.
LEILA per-formed quite well on this task.
The patterns foundwere of the form ?X was born in Y ?
and ?X (Y )?.For the synonymy relation we used all pairsof proper names that share the same synset inWordNet as examples (e.g.
?UN?/?United Na-tions?).
As counterexamples, we chose all pairs ofnouns that are not synonymous in WordNet (e.g.?rabbit?/?composer?).
All pairs of proper names arecandidates.
We ran LEILA on the Wikigeographycorpus, because this set is particularly rich in syn-onyms.
LEILA performed reasonably well.
Thepatterns found include ?X was known as Y ?
as wellas several non-grammatical constructions such as?X (formerly Y )?.For the instanceOf relation, it is difficult to se-lect example pairs, because if an entity belongsto a concept, it also belongs to all super-concepts.However, admitting each pair of an entity and oneof its super-concepts as an example would result infar too many false positives.
The problem is to de-termine for each entity the (super-)concept that ismost likely to be used in a natural language defini-tion of that entity.
Psychological evidence (Roschet al, 1976) suggests that humans prefer a certainlayer of concepts in the taxonomy to classify en-tities.
The set of these concepts is called the Ba-sic Level.
Heuristically, we found that the low-est super-concept in WordNet that is not a com-pound word is a good approximation of the ba-5http://www.famousbirthdates.comsic level concept for a given entity.
We used allpairs of a proper name and the corresponding ba-sic level concept of WordNet as examples.
Wecould not use pairs of proper names and incorrectsuper-concepts as counterexamples, because ourcorpus Wikipedia knows more meanings of propernames than WordNet.
Therefore, we used all pairsof a common noun and an incorrect super-conceptfrom WordNet as counterexamples.
All pairs ofa proper name and a WordNet concept are candi-dates.We ran LEILA on the Wikicomposers corpus.The performance on this task was acceptable, butnot impressive.
However, the chances to obtain ahigh recall and a high precision were significantlydecreased by our tough evaluation policy: Theideal pairs include tuples deduced by resolvingsyntactic and semantic ambiguities and anaphoras.Furthermore, our evaluation policy demands thatnon-defining concepts like member not be cho-sen as instance concepts.
In fact, a high propor-tion of the incorrect assignments were friend,member, successor and predecessor, de-creasing the precision of LEILA.
Thus, comparedto the gold standard of humans, the performanceof LEILA can be considered reasonably good.
Thepatterns found include the Hearst patterns (Hearst,1992) ?Y such as X?, but also more complex pat-terns like ?X was known as a Y ?, ?X [.
.
. ]
as Y ?, ?X[.
.
. ]
can be regarded as Y ?
and ?X is unusual amongY ?.
Some of these patterns could not have beenfound by primitive regular expression matching.To test whether thematic heterogeneity influ-ences LEILA, we ran it on the Wikigeneral corpus.Finally, to try the limits of our system, we ran it onthe Googlecomposers corpus.
As shown in Table1, the performance of LEILA dropped in these in-creasingly challenging tasks, but LEILA could stillproduce useful results.
We can improve the resultson the Googlecomposers corpus by adding the Wi-kicomposers corpus for training.23The different learning methods (kNN and SVM)performed similarly for all relations.
Of course, ineach of the cases, it is possible to achieve a higherprecision at the price of a lower recall.
The run-time of the system splits into parsing (?
40s foreach document, e.g.
3:45h for Wikigeography)and the core algorithm (2-15min for each corpus,5h for the huge Wikigeneral).3.2.2 Results with different competitorsTable 2 shows the results for comparing LEILAagainst various competitors (with LEILA in bold-face).
We compared LEILA to TextToOnto andText2Onto for the instanceOf relation on theWikicomposers corpus.
TextToOnto requires anontology as source of possible concepts.
We gaveit the WordNet ontology, so that it had the samepreconditions as LEILA.
Text2Onto does not re-quire any input.
Text2Onto seems to have a preci-sion comparable to ours, although the small num-ber of found pairs does not allow a significant con-clusion.
Both systems have drastically lower recallthan LEILA.For Snowball, we only had access to the testcorpus.
Hence we trained LEILA on a small por-tion (3%) of the test documents and tested onthe remaining ones.
Since the original 5 seedpairs that Snowball used did not appear in the col-lection at our disposal, we chose 5 other pairsas examples.
We used no counterexamples andhence omitted the Training Phase of our algorithm.LEILA quickly finds the pattern ?Y -based X?.
Thisled to very high precision and good recall, com-pared to Snowball ?
even though Snowball wastrained on a much larger training collection.The CV-system differs from LEILA, because itsideal pairs are a table, in which each entity is as-signed to its most likely concept according to a hu-man understanding of the text ?
independently ofwhether there are explicit definitions for the entityin the text or not.
We conducted two experiments:First, we used the document set used in Cimianoand Vo?lker?s original paper (Cimiano and Vo?lker,2005a), the Lonely Planet corpus.
To ensure afair comparison, we trained LEILA separately onthe Wikicomposers corpus, so that LEILA cannothave example pairs in its output.
For the evalu-ation, we calculated precision and recall with re-spect to an ideal table provided by the authors.Since the CV-system uses a different ontology, weallowed a distance of 4 edges in the WordNet hi-erarchy to count as a match (for both systems).Since the explicit definitions that our system relieson were sparse in the corpus, LEILA performedworse than the competitor.
In a second experi-ment, we had the CV-system run on the Wikicom-posers corpus.
As the CV-system requires a setof target concepts, we gave it the set of all con-cepts in our ideal pairs.
Furthermore, the sys-tem requires an ontology on these concepts.
Wegave it the WordNet ontology, pruned to the tar-get concepts with their super-concepts.
We evalu-ated by the Relaxed Ideal Metric, again allowinga distance of 4 edges in the WordNet hierarchy tocount as a match (for both systems).
This time,our competitor performed worse.
This is becauseour ideal table is constructed from the definitionsin the text, which our competitor is not designedto follow.
These experiments only serve to showthe different philosophies in the definition of theideal pairs for the CV-system and LEILA.
The CV-system does not depend on explicit definitions, butit is restricted to the instanceOf-relation.4 Conclusion and OutlookWe addressed the problem of automatically ex-tracting instances of arbitrary binary relationsfrom natural language text.
The key novelty of ourapproach is to apply a deep syntactic analysis tothis problem.
We have implemented our approachand showed that our system LEILA outperformsexisting competitors.Our current implementation leaves room for fu-ture work.
For example, the linkages allow formore sophisticated ways of resolving anaphorasor matching patterns.
LEILA could learn nu-merous interesting relations (e.g.
country /president or isAuthorOf) and build up anontology from the results with high confidence.LEILA could acquire and exploit new corpora onits own (e.g., it could read newspapers) and itcould use its knowledge to acquire and structureits new knowledge more efficiently.
We plan toexploit these possibilities in our future work.4.1 AcknowledgementsWe would like to thank Eugene Agichtein for hiscaring support with Snowball.
Furthermore, Jo-hanna Vo?lker and Philipp Cimiano deserve oursincere thanks for their unreserved assistance withtheir system.References[Agichtein and Gravano2000] E. Agichtein and L. Gravano.2000.
Snowball: extracting relations from large plain-textcollections.
In ACM 2000, pages 85?94, Texas, USA.
[Brin1999] Sergey Brin.
1999.
Extracting patterns and rela-tions from the world wide web.
In Selected papers fromthe Int.
Workshop on the WWW and Databases, pages172?183, London, UK.
Springer-Verlag.
[Buitelaar and Ramaka2005] P. Buitelaar and S. Ramaka.2005.
Unsupervised ontology-based semantic tagging24Table 2: Results with different competitorsCorpus M Relation System #D #O #C #I Prec Rec F1Snowball corp. S headquarters LEILA(SVM) 54 92 82 165 89.13%?
6.36% 49.70%?
7.63% 63.81%Snowball corp. S headquarters LEILA(kNN) 54 91 82 165 90.11%?
6.13% 49.70%?
7.63% 64.06%Snowball corp. S headquarters Snowball 54 144 49 165 34.03%?
7.74% 29.70%?
6.97% 31.72%Snowball corp.
I headquarters LEILA(SVM) 54 50 48 126 96.00%?
5.43% 38.10%?
8.48% 54.55%Snowball corp.
I headquarters LEILA(kNN) 54 49 48 126 97.96%?
3.96% 38.10%?
8.48% 54.86%Snowball corp.
I headquarters Snowball 54 64 31 126 48.44%?12.24% 24.60%?
7.52% 32.63%Wikicomposers S instanceOf LEILA(SVM) 87 685 408 1127 59.56%?
3.68% 36.20%?
2.81% 45.03%Wikicomposers S instanceOf LEILA(kNN) 87 790 463 1127 58.61%?
3.43% 41.08%?
2.87% 48.30%Wikicomposers S instanceOf Text2Onto 87 36 18 1127 50.00% 1.60%?
0.73% 3.10%Wikicomposers S instanceOf TextToOnto 87 121 47 1127 38.84%?
8.68% 4.17%?
1.17% 7.53%Wikicomposers R instanceOf LEILA(SVM) 87 336 257 744 76.49%?
4.53% 34.54%?
3.42% 47.59%Wikicomposers R instanceOf LEILA(kNN) 87 367 276 744 75.20%?
4.42% 37.10%?
3.47% 49.68%Wikicomposers R instanceOf CV-system 87 134 30 744 22.39% 4.03%?
1.41% 6.83%Lonely Planet R instanceOf LEILA(SVM) ?
159 42 289 26.42%?
6.85% 14.53%?
4.06% 18.75%Lonely Planet R instanceOf LEILA(kNN) ?
168 44 289 26.19%?
6.65% 15.22%?
4.14% 19.26%Lonely Planet R instanceOf CV-system ?
289 92 289 31.83%?
5.37% 31.83%?
5.37% 31.83%M ?
Metric (S: Standard, I: Ideal Metric, R: Relaxed Ideal Metric).
Other abbreviations as in Table 1for knowledge markup.
In W. Buntine, A. Hotho, andStephan Bloehdorn, editors, Workshop on Learning in WebSearch at the ICML 2005.
[Buitelaar et al2004] P. Buitelaar, D. Olejnik, and M. Sin-tek.
2004.
A protege plug-in for ontology extraction fromtext based on linguistic analysis.
In ESWS 2004, Herak-lion, Greece.
[Califf and Mooney1997] M. Califf and R. Mooney.
1997.Relational learning of pattern-match rules for informa-tion extraction.
ACL-97 Workshop in Natural LanguageLearning, pages 9?15.
[Cimiano and Vo?lker2005a] P. Cimiano and J. Vo?lker.2005a.
Text2onto - a framework for ontology learn-ing and data-driven change discovery.
In A. Montoyo,R.
Munozand, and E. Metais, editors, Proc.
of the 10th Int.Conf.
on Applications of Natural Language to InformationSystems, pages 227?238, Alicante, Spain.
[Cimiano and Vo?lker2005b] P. Cimiano and J. Vo?lker.2005b.
Towards large-scale, open-domain and ontology-based named entity classification.
In Int.
Conf.
on RecentAdvances in NLP 2005, pages 166?172.
[Cimiano et al2005] P. Cimiano, G. Ladwig, and S. Staab.2005.
Gimme the context: Contextdriven automatic se-mantic annotation with cpankow.
In Allan Ellis and Tat-suya Hagino, editors, WWW 2005, Chiba, Japan.
[Etzioni et al2004] O. Etzioni, M. Cafarella, D. Downey,S.
Kok, A. Popescu, T. Shaked, S. Soderland, D. S. Weld,and A. Yates.
2004.
Web-scale information extractionin knowitall (preliminary results).
In WWW 2004, pages100?110.
[Fellbaum1998] C. Fellbaum.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.
[Finn and Kushmerick2004] A. Finn and N. Kushmerick.2004.
Multi-level boundary classification for informationextraction.
In ECML 2004, pages 111?122.
[Freitag and Kushmerick2000] D. Freitag and N. Kushmer-ick.
2000.
Boosted wrapper induction.
In American Nat.Conf.
on AI 2000.
[Graupmann2004] Jens Graupmann.
2004.
Concept-basedsearch on semi-structured data exploiting mined semanticrelations.
In EDBT Workshops, pages 34?43.
[Hearst1992] A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In ICCL 1992, Nantes,France.[J.
Iria2005] F. Ciravegna J. Iria.
2005.
Relation extractionfor mining the semantic web.
[Maedche and Staab2000] A. Maedche and S. Staab.
2000.Discovering conceptual relations from text.
In W. Horn,editor, ECAI 2000, pages 85?94, Berlin, Germany.
[Mann and Yarowsky2005] Gideon Mann and DavidYarowsky.
2005.
Multi-field information extraction andcross-document fusion.
In ACL 2005.
[Montague1974] R. Montague.
1974.
Universal grammar.In Formal Philosophy.
Selected Papers of Richard Mon-tague.
Yale University Press.
[Ravichandran and Hovy2002] D. Ravichandran andE.
Hovy.
2002.
Learning surface text patterns for aquestion answering system.
In ACL 2002, Philadelphia,USA.
[Riloff1996] E. Riloff.
1996.
Automatically generating ex-traction patterns from untagged text.
Annual Conf.
on AI1996, pages 1044?1049.
[Rosch et al1976] E. Rosch, C.B.
Mervis, W.D.
Gray, D.M.Johnson, and P. Boyes-Bream.
1976.
Basic objects innatural categories.
Cognitive Psychology, pages 382?439.
[Ruiz-Casado et al2005] Maria Ruiz-Casado, Enrique Al-fonseca, and Pablo Castells.
2005.
Automatic extractionof semantic relationships for wordnet by means of patternlearning from wikipedia.
In NLDB 2006, pages 67?79.
[Sleator and Temperley1993] D. Sleator and D. Temperley.1993.
Parsing english with a link grammar.
3rd Int.
Work-shop on Parsing Technologies.
[Soderland et al1995] S. Soderland, D. Fisher, J. Aseltine,and W. Lehnert.
1995.
Crystal: Inducing a conceptualdictionary.
IJCAI 1995, pages 1314?1319.
[Soderland1999] S. Soderland.
1999.
Learning informationextraction rules for semi-structured and free text.
MachineLearning, pages 233?272.
[Suchanek et al2006] Fabian M. Suchanek, GeorgianaIfrim, and Gerhard Weikum.
2006.
Combining Linguisticand Statistical Analysis to Extract Relations from WebDocuments.
In SIGKDD 2006.
[Xu and Krieger2003] F. Xu and H. U. Krieger.
2003.
In-tegrating shallow and deep nlp for information extraction.In RANLP 2003, Borovets, Bulgaria.
[Xu et al2002] F. Xu, D. Kurz, J. Piskorski, andS.
Schmeier.
2002.
Term extraction and miningterm relations from free-text documents in the financialdomain.
In Int.
Conf.
on Business Information Systems2002, Poznan, Poland.
[Yangarber et al2000] R. Yangarber, R. Grishman,P.
Tapanainen, and S. Huttunen.
2000.
Automaticacquisition of domain knowledge for information extrac-tion.
In ICCL 2000, pages 940?946, Morristown, NJ,USA.
Association for Computational Linguistics.
[Yangarber et al2002] R. Yangarber, W. Lin, and R. Grish-man.
2002.
Unsupervised learning of generalized names.In ICCL 2002, pages 1?7, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.25
