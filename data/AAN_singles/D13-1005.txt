Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 42?54,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsA Joint Learning Model of Word Segmentation, Lexical Acquisition,and Phonetic VariabilityMicha Elsnermelsner0@gmail.comDept.
of LinguisticsThe Ohio State UniversitySharon Goldwatersgwater@inf.ed.ac.ukILCC, School of InformaticsUniversity of EdinburghNaomi H. Feldmannhf@umd.eduDept.
of LinguisticsUniversity of MarylandFrank Woodfwood@robots.ox.ac.ukDept.
of EngineeringUniversity of OxfordAbstractWe present a cognitive model of early lexi-cal acquisition which jointly performs wordsegmentation and learns an explicit model ofphonetic variation.
We define the model as aBayesian noisy channel; we sample segmen-tations and word forms simultaneously fromthe posterior, using beam sampling to controlthe size of the search space.
Compared to apipelined approach in which segmentation isperformed first, our model is qualitatively moresimilar to human learners.
On data with vari-able pronunciations, the pipelined approachlearns to treat syllables or morphemes as words.In contrast, our joint model, like infant learners,tends to learn multiword collocations.
We alsoconduct analyses of the phonetic variations thatthe model learns to accept and its patterns ofword recognition errors, and relate these to de-velopmental evidence.1 IntroductionBy the end of their first year, infants have acquiredmany of the basic elements of their native language.Their sensitivity to phonetic contrasts has becomelanguage-specific (Werker and Tees, 1984), and theyhave begun detecting words in fluent speech (Jusczykand Aslin, 1995; Jusczyk et al 1999) and learn-ing word meanings (Bergelson and Swingley, 2012).These developmental cooccurrences lead some re-searchers to propose that phonetic and word learningoccur jointly, each one informing the other (Swingley,2009; Feldman et al 2013).
Previous computationalmodels capture some aspects of this joint learningproblem, but typically simplify the problem consid-erably, either by assuming an unrealistic degree ofphonetic regularity for word segmentation (Goldwa-ter et al 2009) or assuming pre-segmented inputfor phonetic and lexical acquisition (Feldman et al2009; Feldman et al in press; Elsner et al 2012).This paper presents, to our knowledge, the first broad-coverage model that learns to segment phoneticallyvariable input into words, while simultaneously learn-ing an explicit model of phonetic variation that allowsit to cluster together segmented tokens with differentphonetic realizations (e.g., [ju] and [jI]) into lexicalitems (/ju/).We base our model on the Bayesian word segmen-tation model of Goldwater et al(2009) (henceforthGGJ), using a noisy-channel setup where phoneticvariation is introduced by a finite-state transducer(Neubig et al 2010; Elsner et al 2012).
This in-tegrated model allows us to examine how solvingthe word segmentation problem should affect infants?strategies for learning about phonetic variability andhow phonetic learning can allow word segmentationto proceed in ways that mimic the idealized inputused in previous models.In particular, although the GGJ model achieveshigh segmentation accuracy on phonemic (non-variable) input and makes errors that are qualitativelysimilar to human learners (tending to undersegmentthe input), its accuracy drops considerably on phonet-ically noisy data and it tends to oversegment ratherthan undersegment.
Here, we demonstrate that whenthe model is augmented to account for phonetic vari-ability, it is able to learn common phonetic changes42and by doing so, its accuracy improves and its errorsreturn to the more human-like undersegmentationpattern.
In addition, we find small improvementsin lexicon accuracy over a pipeline model that seg-ments first and then performs lexical-phonetic learn-ing (Elsner et al 2012).
We analyze the model?sphonetic and lexical representations in detail, draw-ing comparisons to experimental results on adult andinfant speech processing.
Taken together, our resultssupport the idea that a Bayesian model that jointlyperforms word segmentation and phonetic learningprovides a plausible explanation for many aspects ofearly phonetic and word learning in infants.2 Related WorkNearly all computational models used to explore theproblems addressed here have treated the learningtasks in isolation.
Examples include models of wordsegmentation from phonemic input (Christiansen etal., 1998; Brent, 1999; Venkataraman, 2001; Swing-ley, 2005) or phonetic input (Fleck, 2008; Rytting,2007; Daland and Pierrehumbert, 2011; Boruta etal., 2011), models of phonetic clustering (Vallabhaet al 2007; Varadarajan et al 2008; Dupoux et al2011) and phonological rule learning (Peperkamp etal., 2006; Martin et al 2013).Elsner et al(2012) present a model that is similarto ours, using a noisy channel model implementedwith a finite-state transducer to learn about phoneticvariability while clustering distinct tokens into lexi-cal items.
However (like the earlier lexical-phoneticlearning model of Feldman et al(2009; in press))their model assumes known word boundaries, soto perform both segmentation and lexical-phoneticlearning, they use a pipeline that first segments usingGGJ and then applies their model to the results.Neubig et al(2010) also present a transducer-based noisy channel model that performs joint in-ference on two out of the three tasks we considerhere; their model assumes fixed probabilities for pho-netic changes (the noise model) and jointly infersthe word segmentation and lexical items, as in our?oracle?
model below (though unlike our system theirmodel learns from phone lattices rather than a singletranscription).
They evaluate only on phone recogni-tion, not scoring the inferred lexical items.Recently, Bo?rschinger et al(2013) did present a?
Geom a, b, ..., ju, ... want, ... juwant, ...Generator for possible wordsProbabilities for each word(sparse)p(?i) = .1, p(a) = .05, p(want) = .01...?
contextsConditional probabilitiesfor each word after each wordp(?i | want) = .3, p(a | want) = .1, p(want | want) = .0001...GGx?1n utterancesx1 x2 ... Intended formsju want ?
kukiju want ?t...s1 s2 ...00Surface formsj?
wan ?
kukiju wand ?t...TGGJ09Figure 1: The graphical model for our system (Eq.
1-4).
Note that the si are not distinct observations; theyare concatenated together into a continuous sequence ofcharacters which constitute the observations.joint learner for segmentation, phonetic learning, andlexical clustering, but the model and inference aretailored to investigate word-final /t/-deletion, ratherthan aiming for a broad coverage system as we do.3 ModelWe follow several previous models of lexical acquisi-tion in adopting a Bayesian noisy channel framework(Eq.
1-4; Fig.
1).
The model has two components:a source distribution P (X) over utterances withoutphonetic variability X , i.e., intended forms (Elsner etal., 2012) and a channel or noise distribution T (S|X)that translates them into the observed surface formsS.
The boundaries between surface forms are thendeterministically removed so that the actual observa-tions are just the unsegmented string of characters inthe surface forms.G0|?0, pstop ?
DP (?0, Geom(pstop)) (1)Gx|G0, ?1 ?
DP (?1, G0) (2)Xi|Xi?1 ?
GXi?1 (3)S|X; ?
?
T (S|X; ?)
(4)The source model is an exact copy of GGJ1: togenerate the intended-form word sequences X , we1We use their best reported parameter values: ?0 =3000, ?1 = 100, pstop = .2 and for unigrams, ?0 = 20.43sample a random language model from a hierarchi-cal Dirichlet process (Teh et al 2006) with char-acter strings as atoms.
To do so, we first draw aunigram distribution G0 from a Dirichlet processprior whose base distribution generates intended formword strings by drawing each phone in turn until thestop character is drawn (with probability pstop).
Then,for each possible context word x, we draw a condi-tional distribution on words following that contextGx = P (Xi = ?|Xi?1 = x) using G0 as a prior.Finally, we sample word sequences x1 .
.
.
xn fromthe bigram model.The channel model is a finite transducer with pa-rameters ?
which independently rewrites single char-acters from the intended string into characters of thesurface string.
We use MAP point estimates of theseparameters; single characters (without n-gram con-text) are used for computational efficiency.
Also forefficiency, the transducer can insert characters intothe surface string, but cannot delete characters fromthe intended string.
As in several previous phonolog-ical models (Dreyer et al 2008; Hayes and Wilson,2008), the probabilities are learned using a feature-based log-linear model.
For features, we use all theunigram features from Elsner et al(2012), whichcheck faithfulness to voicing, place and manner ofarticulation (for example, for k ?
g, active featuresare faith-manner, faith-place, output-g and voiceless-to-voiced).Below, we present two methods for learning thetransducer parameters ?.
The oracle transducer is es-timated using the gold-standard word segmentationsand intended forms for the dataset; it represents thebest possible approximation under our model of theactual phonetics of the dataset.
We can also estimatethe transducer using the EM algorithm.
We first ini-tialize a simple transducer by putting small weightson the faithfulness features to encourage phonologi-cally plausible changes.
With this initial model, webegin running the sampler used to learn word segmen-tations.
After several hundred sampler iterations, westart re-estimating the transducer by maximum likeli-hood after each iteration.
We regularize our estimatesby adding 200 pseudocounts for the rewrite x ?
xduring training (rather than regularizing the weightsfor particular features).
We also show segment onlyresults for a model without the transducer component(i.e., S = X); this recovers the GGJ baseline.4 InferenceInference for this model is complicated for two rea-sons.
First, the hypothesis space is extremely large.Since we allow the input string to be probabilisticallylengthened, we cannot be sure how long it is, norwhich characters it contains.
Second, our hypothe-ses about nearby characters are highly correlated dueto lexical effects.
When deciding how to interpret[w@nt], if we posit that the intended vowel is /2/, theword is likely to be /w2n/ ?one?
and the next wordbegins with /t/ ; if instead we posit that the vowelis /O/, the word is probably /wOnt/ ?want?.
Thus,inference methods that change only one character ata time are unlikely to mix well.
Since they cannotsimultaneously change the vowel and resegment the/t/, they must pass through a low-probability inter-mediate state to get from one state to the other, sowill tend to get stuck in a bad local minimum.
AGibbs sampler which inserts or deletes a single seg-ment boundary in each step (Goldwater et al 2009)suffers from this problem.Mochihashi et al(2009) describe an inferencemethod with higher mobility: a block sampler forthe GGJ model that samples from the posterior overanalyses of a whole utterance at once.
This methodencodes the model as a large HMM, using dynamicprogramming to select an analysis.
We encode ourown model in the same way, constructing the HMMand composing it with the transducer (Mohri, 2004)to form a larger finite-state machine which is stillamenable to forward-backward sampling.4.1 Finite-state encodingFollowing Mochihashi et al(2009) and Neubig etal.
(2010), we can write the original GGJ modelas a Hidden Semi-Markov model.
States in theHMM, written ST:[w][C], are labeled with theprevious word w and the sequence of characters Cwhich have so far been incorporated into the currentword.
To produce a word boundary, we transitionfrom ST:[w][C] to ST:[C][] with probabilityP (xi = C|xi?1 = w).
We can also add the nextcharacter s to the current word, transitioning fromST:[w][C] to ST:[w][C : s], at no cost (sincethe full cost of the word is paid at its boundary, there44?
word j?p(j?|[s])dj u word ju[s]word j u word up(j|[s]) p(u|j)p(ju|[s])j/jd/j ?/uu/uu/uFigure 2: A fragment of the composed finite-state machinefor word segmentation and character replacement for thesurface string ju.
The start state [s] is followed by a wordboundary (filled circle); the next intended character isprobably j but can be d or others with lower probability.After j can be a word boundary (forming the intendedword j), or another character such as u, @ or other (notshown) alternatives.is no cost for the individual characters)2.In addition to analyses using known words, wecan also encode the uniform-geometric prior overunknown words using a finite-state machine.
Wecan choose to select a word from the prior by tran-sitioning to a state ST:[Geom][] with probabilityP (new word|xi?1 = w) immediately after a wordboundary.
While inGeom, we can transition to a newGeom state and produce any character with uniformprobability P (c) = (1?Pstop) 1|C| ; otherwise, we canend the word, transitioning to ST:[unk .word][],with probability Pstop.This construction is also approximate; it ignoresthe possibility that the prior will generate a knownword w, in which case our final transition ought tobe to ST:[w][] instead of ST:[unk .word][].
Thisapproximation means we do not need to add contextto the Geom state to remember the sequence of char-acters it produced, which allows us to keep only asingle Geom state on the chart at each timestep.When we compose this model with the channelmodel, the number of states expands.
Each state mustnow keep track of the previous word, what intendedcharactersC have been posited and what surface char-acters S have been recognized, ST:[w][C][S].2Though not mentioned by Mochihashi et al(2009) or Neu-big et al(2010), this construction is not exact, since transitionsin a Bayesian HMM are exchangeable but not independent (Bealet al 2001): if a word occurs twice in an utterance, its probabil-ity is slightly higher the second time.
For single utterances, thisbias is small and easy to correct for using a Metropolis-Hastingsacceptance check (Bo?rschinger and Johnson, 2012) using thepath probability from the HMM as the proposal.To recognize the current word, we transition toST:[C][][] with probability P (xi = C|xi?1 =w).
To parse a new surface character s by positingintended character x (note that x might be ), wetransition to ST:[w][C : x][S : s] with probabil-ity T (s|x).
(As above, we pay no cost for our choiceof x, which is paid for when we recognize the word;however, we must pay for s.) For efficiency, we donot allow the G0 states to hypothesize different sur-face and intended characters, so when we initiallypropose an unknown word, it must surface as itself.34.2 Beam samplerThis machine has too many states to fully fill the chartbefore backward sampling, so we restrict the set oftrajectories under consideration using beam sampling(Van Gael et al 2008) and simulated annealing.The beam sampler is closely related to the standardbeam search technique, which uses a probability cut-off to discard parts of the FST which are unlikely tofigure in the eventual solution.
Unlike conventionalbeam search, the sampler explores using stochasticcutoffs, so that all trajectories are explored, but mostof the bad ones are explored infrequently, leading tohigher efficiency.We design our beam sampler to restrict the setof potential intended characters at each timestep.In particular, given a stream of input charactersS = s1 .
.
.
sn, we introduce a set of auxiliary cutoffvariables U = u1 .
.
.
un.
The ui variables representlimits on the probability of the emission of surfacecharacter si; we exclude any hypothesized xi whoseprobability of generating si, T (si|xi), is less thanui.
To create a beam sampling scheme, we must de-vise a distribution for U given a state sequence Q (asdiscussed above, the sequence of states encodes theintended character sequence and the segmentationof the surface string), Pu(U |Q) and then incorporatethe probability of U into the forward messages.If qi is the state in Q at which si is generated, andxi the corresponding intended character, we requirethat Pu < T (si|xi); that is, the cutoffs must notexclude any states in the sequence Q.
We define Pu3Again, this approximation is corrected for by the Metropolis-Hastings step.45as a ?-mixture of two distributions:Pu(u|si, xi) = ?U [0,min(.05, T (si|xi))]+(1?
?
)T (si|xi)Beta(5, 1e?
5)The former distribution is quite unrestrictive, whilethe latter prefers to prune away nearly all the states.Thus, for most characters in the string, we do notpermit radical changes, while for a fraction, we do.We follow Huggins and Wood (2013), who ex-tended Van Gael et al(2008) to the case of a non-uniform Pu, to define our forward message ?
as:?
(qi, i) ?
P (qi, S0..i, U0..i) (5)=?qi?1Pu(ui|si, xi)T (si|xi)?
(qi?1, i?
1)This is the standard HMM forward message, aug-mented with the probability of u.
Since Pu(?|si, xi)is required to be less than T (si|xi), it will be 0 when-ever T (si|xi) < u; this is how the u variables func-tion as cutoffs.
In practice, we use the u variables tofilter the lexical items that begin at each position iin advance, using a simple 0/1 edit distance Markovmodel which runs faster than our full model.
(For ex-ample, we can quickly check if the current U allowswant as the intended form for wOlk at i; if not, we canavoid constructing the prefix ST:[xi?1][wa][wO]since the continuation will fail.
)The algorithm?s speed depends on the size anduncertainty of the inferred LM: large numbers ofplausible words mean more states to explore.
Wheninference starts, and the system is highly uncertainabout word boundaries, it is therefore reasonable tolimit the exploration of the character sequence.
Wedo so by annealing in two ways: as in Goldwateret al(2009), we raise P (X) (Eq.
3) to a power twhich increases linearly from .3.
To sample fromthe posterior, we would want to end with t = 1, butas in previous noisy-channel models (Elsner et al2012; Bahl et al 1980) we get better results when weemphasize the LM at the expense of the channel andso end at t = 2.
Meanwhile, as t rises and we explorefewer implausible lexical sequences, we can explorethe character sequence more.
We begin by settingthe ?
interpolation parameter of Pu to 0 to minimizeexploration and increase it linearly to .3 (allowingthe system to change about a third of the characterson each sweep).
This is similar to the scheme foraltering Pu in Huggins and Wood (2013).4.3 Dataset and metricsWe use the corpus released by Elsner et al(2012),which contains 9790 child-directed English utter-ances originally from the Bernstein-Ratner corpus(Bernstein-Ratner, 1987) and later transcribed phone-mically (Brent, 1999).
This standard word segmenta-tion dataset was modified by Elsner et al(2012) toinclude phonetic variation by assigning each token apronunciation independently selected from the empir-ical distribution of pronunciations of that word typein the closely-transcribed Buckeye Speech Corpus(Pitt et al 2007).
Following previous work, we holdout the last 1790 utterances as unseen test data duringdevelopment.
In the results presented here, we runthe model on all 9790 utterances but score only these1790.
We average results over 5 runs of the modelwith different random seeds.We use standard metrics for segmentation and lex-icon recovery.
For segmentation, we report precision,recall and F-score for word boundaries (bds), and forthe positions of word tokens in the surface string (srf ;both boundaries must be correct).For normalization of the pronunciation variation,we follow Elsner et al(2012) in measuring how wellthe system clusters together variant pronunciationsof the same lexical item, without insisting that theintended form the system proposes for them matchthe one in our corpus.
For example, if the systemcorrectly clusters [ju] and [jI] together but assignsthem the incorrect intended form /jI/, we can stillgive credit to this cluster if it is the one that overlapsbest with the gold-standard /ju/ cluster.
To computethese scores, we find the optimal one-to-one map-ping between our clusters of pronunciations and thetrue lexical entries, then report scores for mapped to-kens (mtk; boundaries and mapping to gold standardcluster must be correct) and mapped types4 (mlx).4Elsner et al(2012) calls the mlx metric lexicon F, whichis possibly confusing.
We map the clusters to a gold-standardlexicon (plus potentially some words that don?t correspond toanything in the gold standard) and compute a type-level F-scoreon this lexicon.46Prec Rec F-scorePipeline (segment, then cluster): (Elsner et al 2012)Bds 70.4 93.5 80.3Srf 56.5 69.7 62.4Mtk 44.2 54.5 48.8Mlx 48.6 43.1 45.7Bigram model, segment onlyBds 73.9 (-0.6:0.7) 91.0 (-0.6:0.4) 81.6 (-0.5:0.6)Srf 60.8 (-0.7:1.1) 70.8 (-0.8:0.9) 65.4 (-0.6:1.0)Mtk 41.6 (-0.6:1.2) 48.4 (-0.5:1.2) 44.8 (-0.6:1.2)Mlx 36.6 (-0.7:0.8) 49.8 (-1.0:0.8) 42.2 (-0.9:0.8)Unigram model, oracle transducerBds 81.4 (-0.8:0.4) 72.1 (-0.9:0.8) 76.4 (-0.5:0.7)Srf 63.6 (-1.0:1.1) 58.5 (-1.2:1.2) 60.9 (-0.9:1.2)Mtk 46.8 (-1.0:1.1) 43.0 (-1.1:1.2) 44.8 (-1.0:1.2)Mlx 56.7 (-1.1:1.0) 47.6 (-1.4:0.8) 51.7 (-1.2:0.8)Bigram model, oracle transducerBds 76.1 (-0.6:0.6) 83.8 (-0.9:1.0) 79.8 (-0.8:0.4)Srf 62.2 (-0.9:1.0) 66.7 (-1.2:1.1) 64.4 (-1.1:0.8)Mtk 47.2 (-0.7:0.9) 50.6 (-1.0:0.8) 48.8 (-0.8:0.7)Mlx 40.1 (-1.0:1.2) 43.7 (-0.6:0.7) 41.8 (-0.8:0.6)Bigram model, EM transducerBds 80.1 (-0.5:0.8) 83.0 (-1.4:1.3) 81.5 (-0.5:0.7)Srf 66.1 (-0.8:1.4) 67.8 (-1.4:1.7) 66.9 (-0.9:1.4)Mtk 49.0 (-0.9:0.7) 50.3 (-1.1:1.4) 49.6 (-1.0:1.0)Mlx 43.0 (-1.0:1.4) 49.5 (-1.5:1.1) 46.0 (-1.0:1.3)Table 1: Mean segmentation (bds, srf ) and normalization(mtk, mlx) scores on the test set over 5 runs.
Parenthesesshow min and max scores as differences from the mean.5 Results and discussionIn the following sections, we analyze how our modelwith variability compares to GGJ on noisy data.
Wegive quantitative scores and also show that qualitativepatterns of errors are often similar to those of humanlearners and listeners.5.1 Clean versus variable inputWe begin by evaluating our model as a word seg-mentation system.
(Table 1 gives segmentation andnormalization scores for various models and base-lines on the 1790 test utterances.)
We first confirmthat our inference method is reasonable.
The bigrammodel without variability (?segment only?)
shouldhave the same segmentation performance as the stan-dard dpseg implementation of GGJ.
This is the case:dpseg has boundary F of 80.3 and token F of 62.4;we get 81.6 and 65.4.
Thus, our sampler is findinggood solutions, at least for the no-variability model.We compare segmentation scores between the?segment only?
system and the two bigram modelswith transducers (?oracle?
and ?EM?).
While thesesystems all achieve similar segmentation scores, theydo so in different ways.
?Segment only?
finds a so-lution with boundary precision 73.9% and boundaryrecall 91.0% for a total F of 81.6%.
The low pre-cision and high recall here indicate a tendency tooversegment; when the analysis of a given subse-quence is unclear, the system prefers to chop it intosmall chunks.
The bigram models which incorporatetransducers score P : 76.1, R: 83.8 (oracle) and P :80.1,R: 83.0 (EM), indicating that they prefer to findlonger sequences (undersegment) more.In previous experiments on datasets without varia-tion, GGJ also has a strong tendency to undersegmentthe data (boundary P : 90.1, R: 80.3), which Gold-water et alargue is rational behavior for an ideallearner seeking a parsimonious explanation for thedata.
Undersegmentation occurs especially when ig-noring lexical context (a unigram model), but to someextent even in bigram models.
Human learners alsotend to learn collocations as single words (Peters,1983; Tomasello, 2000), and the GGJ model has beenshown to capture several other effects seen in labora-tory segmentation tasks (Frank et al 2010).
Together,these findings support the idea that human learnersmay behave in important respects like the Bayesianideal learners that Goldwater et alpresented.However, experiments on data with variation havecalled these conclusions into question.
In particu-lar, GGJ has previously been shown to oversegmentrather than undersegment as the input grows noisier(Fleck, 2008), and our results replicate this finding(oversegmentation for the ?segment only?
model).In addition, the GGJ bigram model, which achievesmuch higher segmentation accuracy than the unigrammodel on clean data, actually performs worse on verynoisy data (Jansen et al 2013).
Infants are known totrack statistical dependencies across words (Go?mezand Maye, 2005), so it is worrisome that these de-pendencies hurt GGJ?s segmentation accuracy whenlearning from noisy data.Our results show that modeling phonetic variabil-ity reverses the problematic trends described above.Although the models with phonetic variability showsimilar overall segmentation accuracy on noisy datato the original GGJ model, the pattern of errorschanges, with less oversegmentation and more un-47dersegmentation.
Thus, their qualitative performanceon variable data resembles GGJ?s on clean data, andtherefore the behavior of human learners.5.2 Phonetic variabilityWe next analyze the model?s ability to normalize vari-ations in the pronunciation of tokens, by inspectingthe mtk score.
The ?segment only?
baseline is pre-dictably poor, F : 44.8.
The pipeline model scores48.8, and our oracle transducer model matches thisexactly.
The EM transducer scores better, F : 49.6.Although the confidence intervals overlap slightly,the EM system also outperforms the pipeline on theother F -measures; altogether, these results suggestat least a weak learning synergy (Johnson, 2008) be-tween segmentation and phonetic learning.It is interesting that EM can perform better thanthe oracle.
However, EM is more conservative aboutwhich sound changes it will allow, and thus tends toavoid mistakes caused by the simplicity of the trans-ducer model.
Since the transducer works segment-by-segment, it can apply rare contextual variationsout of context.
EM benefits from not learning thesevariations to begin with.We can also compare the bigram and unigram ver-sions of the model.
The unigram model is a rea-sonable segmenter, though not quite as good as thebigram model, with boundary F of 76.4 and tokenF of 60.9 (compared to 79.8 and 64.4 using the bi-gram model).
However, it is not good at normalizingvariation; its mtk score is comparable to the baselineat 44.8%5.
Although bigram context is only moder-ately effective for telling where words are, the modelseems heavily reliant on lexical context to decidewhat words it is hearing.5.3 Error analysisTo gain more insight into the differing behavior ofour model versus a pipelined system, we inspect theintended word strings X proposed by each one indetail.
Below, we categorize the kinds of intendedword strings that the model might propose to span agiven gold-standard word token:Correct Correctly segmented, mapped to the correctlexical item (e.g., gold intended /ju/, surface5Elsner et al(2012) show a similar result for a unigramversion of their pipelined system.EM-learned Segment onlyCorrect 49.88 47.61Wrong form 17.96 23.73Collocation 14.25 7.59Split 8.26 15.18One bound 7.11 15.18Corr.
colloc.
1.35 < 0.01Other 0.75 0.22Corr.
split 0.43 0.66Table 2: Distribution (%) of error types (see text) in asingle run on the full dataset.segmentation [ju], intended /ju/)Wrong form Correctly segmented, mapped to thewrong lexical item (/ju/, surf.
[ju], int.
/jEs/)Colloc Missegmented as part of a sequence whoseboundaries correspond to real word boundaries(/ju?want/, surf.
[juwant], int.
/juwant/)Corr.
colloc As above, but proposed lexical itemmaps to this word (/ar?ju/, surf.
[arj@] int./ju/)Split Missegmented with a word-internal boundary(/dOgiz/, surf.
[dO?giz], int.
/dO?giz/)Corr.
split As above, but one proposed word mapscorrectly (/dOgi/, surf.
[dOg?i], int.
/dOgi?
@/)One boundary One boundary correct, the otherwrong (/ju?wa.
.
./, surf.
[juw], int.
/juw/)Other Not a collocation, both boundaries are wrong(/du?ju?wa.
.
./, surf.
[ujuw], int.
/ujuw/)Table 2 shows the distribution over intended wordstrings proposed by the ?segment only?
baseline andthe EM-learned transducer.
Both systems proposea large number of correct forms, and the most com-mon error category is ?wrong form?
(lexical errorwithout segmentation error), an error which couldpotentially be repaired in a pipeline system.
How-ever, the remaining errors represent segmentationmistakes which a pipeline could not repair.
Herethe two systems behave quite differently.
The EM-learned transducer analyses 14% of real tokens asparts of multiword collocations like ?doyou?
; in an-other 1.35%, the underlying content word is evencorrectly detected.
The non-variable system, on theother hand, analyses 15% of real tokens by splittingthem into pieces.
Since infant learners tend to learncollocations, this supports our analysis that the modelwith variation better models human behavior.48EM ju: 805, duju: 239, juwan: 88, jI: 58, e~ju: 54, judu:47, j?
: 39, jul2k: 39, Su: 30, u: 23, Zu: 18, j: 17,je~: 16, tSu: 15, aj:15, Derjugo: 12, dZu: 12GGJ ju: 498, jI: 280, j@: 165, ji: 119, duju: 106, dujI: 44,kInju: 39, i: 32, u: 29, kInjI: 29, jul2k: 24, juwan:23, j: 22, Su: 19, jU: 18, e~ju: 18, I:16, Zu: 15, dZ?u:13, jE: 12, SI: 11, T?Nkju: 11Table 3: Forms proposed with frequency > 10 forgold-standard tokens of ?you?
in one sample from EM-transducer and segment-only (GGJ) system.To illustrate this behavior anecdotally, we presentthe distribution of intended word strings spanningtokens whose gold intended form is /ju/ ?you?
(Table3).
The EM-learned solution proposes 805 tokensof /ju/, which is the correct analysis6; the ?segmentonly?
system instead finds varying forms like /jI/,/j?/ etc.
This is unsurprising and could be repairedby a suitable pipelined system.
However, the EMsystem also proposes 239 instances of ?doyou?, 88instances of ?youwant?, 54 instances of ?areyou?
andseveral other collocations.
The ?segment only?
sys-tem finds some of these collocations, split into dif-ferent versions: for instance 106 instances of /duju/and 44 of /dujI/.
In a pipelined system, we couldcombine these variants to find 150 instances?
butthis is still 89 instances short of the 239 found whenallowing for variability.
The same pattern holds for?youlike?
and ?youwant?.
Because the non-variablesystem must learn each variant separately, it learnsonly the most common instances of these long collo-cations, and analyzes infrequent variants differently.We also perform this analysis specifically forwords beginning with vowels.
Infants show a delayin their ability to segment these words from continu-ous speech (Mattys and Jusczyk, 2001; Nazzi et al2005; Seidl and Johnson, 2008), and Seidl and John-son (2008) suggest a perceptual explanation?
initialvowels can be hard to hear and often exhibit variationdue to coarticulation or resyllabification.
Althoughour dataset does not contain coarticulation as such, itshould show this pattern of greater variation, whichwe hypothesize might lead to difficulty in segmentingand recognizing vowel-initial words.The model?s behavior is consistent with this hy-pothesis (Table 4).
Both the ?segment only?
andEM transducer models find approximately the same6Not all the variants are merged, however.
jI, j?, Su etc.
arestill occasionally analyzed as separate lexical items.Segment only Vow.
init Cons.
initCorrect 47.5 51.7Wrong form 18.6 15.7Collocation 14.6 12.2Split 6.2 10.8Right bd.
corr.
5.8 3.6Left bd.
corr.
4.6 3.8EM transducer Vow.
init Cons.
initCorrect 41.5 52.1Wrong form 20.4 17.3Collocation 19.2 12.5Split 5.2 9.1Right bd.
corr.
6.2 2.7Left bd.
corr.
2.7 3.1Table 4: Most common error types (%; see text) for in-tended forms beginning with vowels or consonants.
Rareerror types are not shown.
?One bound?
errors are split upby which boundary is correct.proportion of vowel-initial tokens, and both systemsdo somewhat better on consonant-initial words thanvowel-initial words.
The advantage is stronger forthe transducer model, which gets only 41.5% ofvowel-initial tokens correct as opposed to 52.1% ofconsonant-initial words.
It proposes more colloca-tions for vowel-initial words (19.2%) than for conso-nants (12.5%).
In cases where they do not propose acollocation, both systems are somewhat more likelyto find the right boundary of a vowel-initial tokenthan the left boundary (although again this differenceis larger for the EM system); this suggests that theproblem is indeed caused by the initial segment.5.4 Phonetic LearningWe next compare phonetic variations learned by themodel to characteristics of infant speech perception.Infants show an asymmetry between consonants andvowels, losing sensitivity to non-native vowel con-trasts by eight months (Kuhl et al 1992; Boschand Sebastia?n-Galle?s, 2003) but to non-native con-sonant contrasts only by 10-12 months (Werker andTees, 1984).
The observed ordering is somewhatpuzzling when one considers the availability for dis-tributional information (Maye et al 2002), which ismuch stronger for stop consonants than for vowels(Lisker and Abramson, 1964; Peterson and Barney,1952).
Infants are also conservative in generalizingacross phonetic variability, showing a delayed abil-49ity to generalize across talkers, affects, and dialects.They have difficulty recognizing word tokens that arespoken by a different talker or in a different tone ofvoice until 11 months (Houston and Jusczyk, 2000;Singh et al 2004), and the ability to adapt to unfa-miliar dialects appears to develop even later, between15 and 19 months (Best et al 2009; Heugten andJohnson, in press; White and Aslin, 2011).Similar to infants, our model shows both a vowel-consonant asymmetry and a reluctance to accept thefull range of adult phonetic variability.
Table 5 showssome segment-to-segment alternations learned in var-ious transducers.
The oracle learns a large amountof variation (u surfaces as itself only 68% of thetime) involving many different segments, whereasEM is similar to infant learners in learning a moreconservative solution with fewer alternations over-all.
Moreover, EM appears to identify patterns ofvariability in vowels before consonants.
It learns asimilar range of alternations for u as in the oracle,although it treats the sound as less variable than itactually is.
It learns much less variability for con-sonants; it picks up the alternation of D with s andz, but predicts that D will surface as itself 91% ofthe time when the true figure is only 69%.
And itfails to learn any meaningful alternations involvingk.
These results suggest that patterns of variability invowels are more evident than patterns of variabilityin consonants when infants are beginning to solve theword segmentation problem.To investigate the effect of data size on this con-servativism, we ran the system on 1000 utterancesinstead of 9790.
This leads to an even more conser-vative solution, with variations for u but none of theothers (although i and D still vary more than k).5.5 Segmentation and recognition errorsA particularly interesting set of errors are those thatinvolve both a missegmentation and a simultaneousmisrecognition, since the joint model is prone tosuch errors while the pipelined model is not.
Rel-atively little is known about infants?
misrecognitionsof words in fluent speech, although it is clear that theyfind words in medial position harder (Plunkett, 2005;Seidl and Johnson, 2006).
However, adults makemissegmentation/misrecognition errors fairly often,especially when listening to noisy audio (Butterfieldand Cutler, 1988).
Such errors are more commonSystem x top 4 outputs sOracleu u .68 @ .05 a .04 U .04i i .85 I .03 @ .03 E .02D D .69 s .07 [?]
.07 z .04k k .93 d .02 g .02[?]
r .21 h .11 d .01 @ .07EM(full)u u .75 @ .08 I .04 U .03i i .90 I .04 E .02D D .91 s .03 z 0.1k k .98[?]
@ .32 I .14 n .13 t .13EM(only1000utts)u u .82 I .04 @ .04 a .02i i .97D D .95k k .99[?]
@ .21 I .18 t .12 s .12Table 5: Learned phonetic alternations: top 4 outputs swith p > .001 for inputs x = uw (/u/ ), iy (/i/ ), dh (/D/ ),k (/k/) and [?
], the null character.
Outputs from [?]
areinsertions.
The oracle allows [?]
as an output (deletion)but for computational reasons, the model does not.when the misrecognized word belongs to a prosod-ically rare class and when the incorrectly hypothe-sized string contains frequent words (Cutler, 1990);phonetically ambiguous words are also more com-monly recognized as the more frequent of two op-tions (Connine et al 1993).
For the indefinite article?a?
(often reduced to [@]), lexical context is the mainfactor in deciding between ambiguous interpretations(Kim et al 2012).
In rapid speech, listeners have fewphonetic cues to indicate whether it is present at all(Dilley and Pitt, 2010).
Below, we analyze variousmisrecognitions made by our system (using the EMtransducer), and find some similar effects.The easiest cases to analyze are those with no mis-segmentation: the proposed boundaries are correct,and the proposed lexical entry corresponds to a realword7, but not the correct one.
Most of them corre-spond to homophones (Table 6).Common cases with a missegmentation include itand is, a and is, it?s and is, who, who?s and whose,that?s and what?s, and there and there?s.
In general,these errors involve words which sometimes appear7The one-to-one mapping can be misleading, as it may mapa large cluster to a real word on the basis of one or two tokens ifall other tokens correspond to a different word already used foranother cluster.
We manually filter out a few cases like this.50Actual proposed count/tu/ ?two?
/t@/ ?to?
95/kin/ ?can?
/k?nt/ ?can?t?
67/En/ ?and?
/?n/ ?an?
61/hIz/ ?his?
/Iz/ ?is?
57/D@/ ?the?
/@/ ?ah?
51/w@ts/ ?what?s?
/wants/ ?wants?
40/wan/ ?want?
/won/ ?won?t?
39/yu/ ?you?
/y?/ ?yeah?
39/f@~/ ?for?
/fOr/ ?four?
30/hir/ ?here?
/hil/ ?he?ll?
28Table 6: Top ten errors involving confusion between real,correctly segmented words: the most common pronunci-ation of the actual token and its orthographic form, thesame for the proposed token, and the frequency.with a morpheme or clitic (which can easily be mis-segmented as part of something else), words whichdiffer by one segment, and frequent function wordswhich often appear in similar contexts.
These tenden-cies match those shown by adult human listeners.A particularly distinctive set of joint recognitionand segmentation errors are those where an entirereal token is treated as phonetic ?noise??
that is, itis segmented along with an adjacent word, and thesystem clusters the whole sequence as a token ofthat word.
The most common examples are ?that?s a?identified as ?that?s?, ?have a?
identified as ?have?,?sees a?
identified as ?sees?
and other examples in-volving ?a?, a word which also frequently confuseshumans (Kim et al 2012; Dilley and Pitt, 2010).However, there are also instances of ?who?s in?
as?who?s?, ?does it?
as ?does?, and ?can you?
as ?can?.6 ConclusionWe have presented a model that jointly infers wordsegmentation, lexical items, and a model of phoneticvariability; we believe this is the first model to doso on a broad-coverage naturalistic corpus8.
Our re-sults show a small improvement in both segmentationand normalization over a pipeline model, providingevidence for a synergistic interaction between theselearning tasks and supporting claims of interactivelearning from the developmental literature on infants.We also reproduced several experimental findings;our results suggest that two vowel-consonant asym-8Software is available from the ACL archive; updatedversions may be posted at https://bitbucket.org/melsner/beamseg.metries, one from the word segmentation literatureand another from the phonetic learning literature, arelinked to the large variability in vowels found in nat-ural corpora.
The model?s correspondence with hu-man behavioral results is by no means exact, but webelieve these kinds of predictions might help guidefuture research on infant phonetic and word learning.AcknowledgementsThanks to Mary Beckman for comments.
This workwas supported by EPSRC grant EP/H050442/1 to thesecond author.ReferencesLalit Bahl, Raimo Bakis, Frederick Jelinek, and RobertMercer.
1980.
Language-model/acoustic-channel-model balance mechanism.
Technical disclosure bul-letin Vol.
23, No.
7b, IBM, December.Matthew J. Beal, Zoubin Ghahramani, and Carl EdwardRasmussen.
2001.
The infinite Hidden Markov Model.In NIPS, pages 577?584.Elika Bergelson and Daniel Swingley.
2012.
At 6-9months, human infants know the meanings of manycommon nouns.
Proceedings of the National Academyof Sciences, 109:3253?3258.Nan Bernstein-Ratner.
1987.
The phonology of parent-child speech.
In K. Nelson and A. van Kleeck, editors,Children?s Language, volume 6.
Erlbaum, Hillsdale,NJ.Catherine T. Best, Michael D. Tyler, Tiffany N. Good-ing, Corey B. Orlando, and Chelsea A. Quann.
2009.Development of phonological constancy: Toddlers?
per-ception of native- and jamaican-accented words.
Psy-chological Science, 20(5):539?542.Benjamin Bo?rschinger and Mark Johnson.
2012.
Usingrejuvenation to improve particle filtering for Bayesianword segmentation.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Linguis-tics (Volume 2: Short Papers), pages 85?89, Jeju Island,Korea, July.
Association for Computational Linguistics.Benjamin Bo?rschinger, Mark Johnson, and Katherine De-muth.
2013.
A joint model of word segmentationand phonological variation for English word-final /t/-deletion.
In Proceedings of the 51st Annual Meeting ofthe Association for Computational Linguistics, Sofia,Bulgaria, August.
Association for Computational Lin-guistics.Luc Boruta, Sharon Peperkamp, Beno?
?t Crabbe?, and Em-manuel Dupoux.
2011.
Testing the robustness of onlineword segmentation: Effects of linguistic diversity and51phonetic variation.
In Proceedings of the 2nd Workshopon Cognitive Modeling and Computational Linguistics,pages 1?9.Laura Bosch and Nu?ria Sebastia?n-Galle?s.
2003.
Simulta-neous bilingualism and the perception of a language-specific vowel contrast in the first year of life.
Lan-guage and Speech, 46(2-3):217?243.Michael R. Brent.
1999.
An efficient, probabilisticallysound algorithm for segmentation and word discovery.Machine Learning, 34:71?105, February.Sally Butterfield and Anne Cutler.
1988.
Segmentationerrors by human listeners: Evidence for a prosodicsegmentation strategy.
In Proceedings of SPEECH?88: Seventh Symposium of the Federation of AcousticSocieties of Europe, vol.
3, pages 827?833, Edinburgh.Morten H. Christiansen, Joseph Allen, and Mark S. Sei-denberg.
1998.
Learning to Segment Speech UsingMultiple Cues: A Connectionist Model.
Language andCognitive Processes, 13(2/3):221?269.C.
M. Connine, D. Titone, and J. Wang.
1993.
Audi-tory word recognition: Extrinsic and intrinsic effects ofword frequency.
Journal of Experimental Psychology:Learning, Memory and Cognition, 19:81?94.Anne Cutler.
1990.
Exploiting prosodic probabilities inspeech segmentation.
In G. A. Altmann, editor, Cog-nitive models of speech processing: Psycholinguisticand computational perspectives, pages 105?121.
MITPress, Cambridge, MA.Robert Daland and Janet B. Pierrehumbert.
2011.
Learn-ing diphone-based segmentation.
Cognitive Science,35(1):119?155.Laura C. Dilley and Mark Pitt.
2010.
Altering contextspeech rate can cause words to appear or disappear.Psychological Science, 21(11):1664?1670.Markus Dreyer, Jason R. Smith, and Jason Eisner.
2008.Latent-variable modeling of string transductions withfinite-state methods.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Processing,EMNLP ?08, pages 1080?1089, Stroudsburg, PA, USA.Association for Computational Linguistics.Emmanuel Dupoux, Guillaume Beraud-Sudreau, andShigeki Sagayama.
2011.
Templatic features for mod-eling phoneme acquisition.
In Proceedings of the 33rdAnnual Cognitive Science Society.Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.2012.
Bootstrapping a unified model of lexical and pho-netic acquisition.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 184?193, JejuIsland, Korea, July.
Association for Computational Lin-guistics.Naomi Feldman, Thomas Griffiths, and James Morgan.2009.
Learning phonetic categories by learning a lexi-con.
In Proceedings of the 31st Annual Conference ofthe Cognitive Science Society.Naomi H. Feldman, Emily B. Myers, Katherine S. White,Thomas L. Griffiths, and James L. Morgan.
2013.Word-level information influences phonetic learningin adults and infants.
Cognition, 127(3):427?438.Naomi H. Feldman, Thomas L. Griffiths, Sharon Gold-water, and James L. Morgan.
in press.
A role for thedeveloping lexicon in phonetic category acquisition.Psychological Review.Margaret M. Fleck.
2008.
Lexicalized phonotactic wordsegmentation.
In Proceedings of ACL-08: HLT, pages130?138, Columbus, Ohio, June.
Association for Com-putational Linguistics.Michael C. Frank, Sharon Goldwater, Thomas L. Griffiths,and Joshua B. Tenenbaum.
2010.
Modeling human per-formance in statistical word segmentation.
Cognition,117(2):107?125.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2009.
A Bayesian framework for word segmen-tation: Exploring the effects of context.
Cognition,112(1):21?54.Rebecca Go?mez and Jessica Maye.
2005.
The develop-mental trajectory of nonadjacent dependency learning.Infancy, 7:183?206.Bruce Hayes and Colin Wilson.
2008.
A maximum en-tropy model of phonotactics and phonotactic learning.Linguistic Inquiry, 39(3):379?440.Marieke van Heugten and Elizabeth K. Johnson.
in press.Learning to contend with accents in infancy: Benefitsof brief speaker exposure.
Journal of ExperimentalPsychology: General.Derek M. Houston and Peter W. Jusczyk.
2000.
The roleof talker-specific information in word segmentation byinfants.
Journal of Experimental Psychology: HumanPerception and Performance, 26:1570?1582.Jonathan Huggins and Frank Wood.
2013.
Infinite struc-tured hidden semi-Markov models.
Transactions onPattern Analysis and Machine Intelligence (TPAMI), toappear, September.Aren Jansen, Emmanuel Dupoux, Sharon Goldwater,Mark Johnson, Sanjeev Khudanpur, Kenneth Church,Naomi Feldman, Hynek Hermansky, Florian Metze,Richard Rose, Mike Seltzer, Pascal Clark, Ian McGraw,Balakrishnan Varadarajan, Erin Bennett, BenjaminBorschinger, Justin Chiu, Ewan Dunbar, Abdellah Four-tassi, David Harwath, Chia-ying Lee, Keith Levin,Atta Norouzian, Vijay Peddinti, Rachael Richardson,Thomas Schatz, and Samuel Thomas.
2013.
A sum-mary of the 2012 JHU CLSP workshop on zero re-source speech technologies and early language acqui-sition.
Proceedings of the IEEE International Confer-ence on Acoustics, Speech, and Signal Processing.52Mark Johnson.
2008.
Using adaptor grammars to identifysynergies in the unsupervised acquisition of linguis-tic structure.
In Proceedings of ACL-08: HLT, pages398?406, Columbus, Ohio, June.
Association for Com-putational Linguistics.Peter W. Jusczyk and Richard N. Aslin.
1995.
Infants?
de-tection of the sound patterns of words in fluent speech.Cognitive Psychology, 29:1?23.Peter W. Jusczyk, Derek M. Houston, and Mary Newsome.1999.
The beginnings of word segmentation in English-learning infants.
Cognitive Psychology, 39:159?207.Dahee Kim, Joseph D.W. Stephens, and Mark A. Pitt.2012.
How does context play a part in splitting wordsapart?
Production and perception of word boundariesin casual speech.
Journal of Memory and Language,66(4):509 ?
529.Patricia K. Kuhl, Karen A. Williams, Francisco Lacerda,Kenneth N. Stevens, and Bjorn Lindblom.
1992.
Lin-guistic experience alters phonetic perception in infantsby 6 months of age.
Science, 255(5044):606?608.Leigh Lisker and Arthur S. Abramson.
1964.
A cross-language study of voicing in initial stops: Acousticalmeasurements.
Word, 20:384?422.Andrew Martin, Sharon Peperkamp, and EmmanuelDupoux.
2013.
Learning phonemes with a proto-lexicon.
Cognitive Science, 37:103?124.Sven L. Mattys and Peter W. Jusczyk.
2001.
Do infantssegment words or recurring contiguous patterns?
Jour-nal of Experimental Psychology: Human Perceptionand Performance, 27(3):644?655+.Jessica Maye, Janet F. Werker, and LouAnn Gerken.
2002.Infant sensitivity to distributional information can affectphonetic discrimination.
Cognition, 82(3):B101?11.Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.2009.
Bayesian unsupervised word segmentation withnested pitman-yor language modeling.
In Proceedingsof the Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP, pages100?108, Suntec, Singapore, August.
Association forComputational Linguistics.Mehryar Mohri, 2004.
Weighted Finite-State TransducerAlgorithms: An Overview, chapter 29, pages 551?564.Physica-Verlag.Thierry Nazzi, Laura C. Dilley, Ann Marie Jusczyk, Ste-fanie Shattuck-Hufnagel, and Peter W. Jusczyk.
2005.English-learning infants?
segmentation of verbs fromfluent speech.
Language and Speech, 48(3):279?298+.Graham Neubig, Masato Mimura, Shinsuke Mori, andTatsuya Kawahara.
2010.
Learning a language modelfrom continuous speech.
In 11th Annual Conferenceof the International Speech Communication Associa-tion (InterSpeech 2010), pages 1053?1056, Makuhari,Japan, 9.Sharon Peperkamp, Rozenn Le Calvez, Jean-Pierre Nadal,and Emmanuel Dupoux.
2006.
The acquisition ofallophonic rules: Statistical learning with linguisticconstraints.
Cognition, 101(3):B31?B41.Ann M. Peters.
1983.
The Units of Language Acquisi-tion.
Cambridge Monographs and Texts in AppliedPsycholinguistics.
Cambridge University Press.Gordon E. Peterson and Harold L. Barney.
1952.
Controlmethods used in a study of the vowels.
Journal of theAcoustical Society of America, 24(2):175?184.Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-ling, William Raymond, Elizabeth Hume, and EricFosler-Lussier.
2007.
Buckeye corpus of conversa-tional speech (2nd release).Kim Plunkett.
2005.
Learning how to be flexible withwords.
Attention and Performance, XXI:233?248.Anton Rytting.
2007.
Preserving Subsegmental Varia-tion in Modeling Word Segmentation (Or, the Raisingof Baby Mondegreen).
Ph.D. thesis, The Ohio StateUniversity.Amanda Seidl and Elizabeth Johnson.
2006.
Infant wordsegmentation revisited: Edge alignment facilitates tar-get extraction.
Developmental Science, 9:565?573.Amanda Seidl and Elizabeth Johnson.
2008.
Perceptualfactors influence infants?
extraction of onsetless wordsfrom continuous speech.
Journal of Child Language,34.Leher Singh, James Morgan, and Katherine White.
2004.Preference and processing: The role of speech affectin early spoken word recognition.
Journal of Memoryand Language, 51:173?189.Daniel Swingley.
2005.
Statistical clustering and the con-tents of the infant vocabulary.
Cognitive Psychology,50:86?132.Daniel Swingley.
2009.
Contributions of infant wordlearning to language development.
PhilosophicalTransactions of the Royal Society B: Biological Sci-ences, 364(1536):3617?3632, December.Y.
W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006.Hierarchical Dirichlet processes.
Journal of the Ameri-can Statistical Association, 101(476):1566?1581.Michael Tomasello.
2000.
The item-based nature of chil-dren?s early syntactic development.
Trends in CognitiveSciences, 4(4):156 ?
163.Gautam K. Vallabha, James L. McClelland, Ferran Pons,Janet F. Werker, and Shigeaki Amano.
2007.
Unsuper-vised learning of vowel categories from infant-directedspeech.
Proceedings of the National Academy of Sci-ences, 104(33):13273?13278.Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, andZoubin Ghahramani.
2008.
Beam sampling for theinfinite Hidden Markov model.
In Proceedings of the25th International Conference on Machine learning,53ICML ?08, pages 1088?1095, New York, NY, USA.ACM.Balakrishnan Varadarajan, Sanjeev Khudanpur, and Em-manuel Dupoux.
2008.
Unsupervised learning ofacoustic sub-word units.
In Proceedings of the As-sociation for Computational Linguistics: Short Papers,pages 165?168.Anand Venkataraman.
2001.
A statistical model for worddiscovery in transcribed speech.
Computational Lin-guistics, 27(3):351?372.Janet F. Werker and Richard C. Tees.
1984.
Cross-language speech perception: Evidence for perceptualreorganization during the first year of life.
Infant Be-havior and Development, 7(1):49 ?
63.Katherine S. White and Richard N. Aslin.
2011.
Adap-tation to novel accents by toddlers.
DevelopmentalScience, 14(2):372?384.54
