Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 748?753,Dublin, Ireland, August 23-24, 2014.UNIBA: Combining Distributional Semantic Models and Word SenseDisambiguation for Textual SimilarityPierpaolo Basile and Annalina Caputo and Giovanni SemeraroDepartment of Computer ScienceUniversity of Bari Aldo MoroVia, E. Orabona, 4 - 70125 Bari (Italy){firstname.surname}@uniba.itAbstractThis paper describes the UNIBA teamparticipation in the Cross-Level SemanticSimilarity task at SemEval 2014.
We pro-pose to combine the output of different se-mantic similarity measures which exploitWord Sense Disambiguation and Distribu-tional Semantic Models, among other lex-ical features.
The integration of similar-ity measures is performed by means oftwo supervised methods based on Gaus-sian Process and Support Vector Machine.Our systems obtained very encouragingresults, with the best one ranked 6thoutof 38 submitted systems.1 IntroductionCross-Level Semantic Similarity (CLSS) is thetask of computing the similarity between two textfragments of different sizes.
The task focuses onthe comparison between texts at different lexicallevels, i.e.
between a larger and a smaller text.The task comprises four different levels: 1) para-graph to sentence; 2) sentence to phrase; 3) phraseto word; 4) word to sense.
The task objective isto provide a framework for evaluating general vs.level-specialized methods.Our general approach consists in combiningscores coming from different semantic similarityalgorithms.
The combination is performed by asupervised method using the training data pro-vided by the task organizers.
The data set com-prises pairs of text fragments that can be rated witha score between 0 and 4, where 4 indicates themaximum level of similarity.We select algorithms which provide similaritiesat different levels of semantics: surface (or string-This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/based), lexical (word sense disambiguation), anddistributional level.
The idea is to combine in aunique system the semantic aspects that pertaintext fragments.The following section gives more details aboutthe similarity measures and their combination in aunique score through supervised methods (Section2).
Section 3 describes the system set up for theevaluation and comments on the reported results,while Section 4 concludes the paper.2 System DescriptionThe idea behind our system is to combine theoutput of several similarity measures/features bymeans of a supervised algorithm.
Those featureswere grouped in three main categories.
The fol-lowing three sub-sections describe in detail eachfeature exploited by the system.2.1 Distributional Semantics LevelDistributional Semantic Models (DSM) are aneasy way for building geometrical spaces of con-cepts, also known as Semantic (or Word) Spaces,by skimming through huge corpora of text in or-der to learn the context of word usage.
In the re-sulting space, semantic relatedness/similarity be-tween two words is expressed by the opposite ofthe distance between points that represent thosewords.
Thus, the semantic similarity can be com-puted as the cosine of the angle between the twovectors that represent the words.
This conceptof similarity can be extended to whole sentencesby combining words through vector addition (+),which corresponds to the point-wise sum of thevector components.
Our DSM measure (DSM)is based on a SemanticSpace, represented by aco-occurrences matrix M , built by analysing thedistribution of words in the British National Cor-pus (BNC).
Then, M is reduced using the LatentSemantic Analysis (LSA) (Landauer and Dumais,1997).
Vector addition and cosine similarity are748then used for building the vector representation ofeach text fragment and computing their pairwisesimilarity, respectively.2.2 Lexical Semantics LevelWord Sense Disambiguation.
Most of ourmeasures rely on the output of a Word Sense Dis-ambiguation (WSD) algorithm.
Our newest ap-proach to WSD, recently presented in Basile etal.
(2014), is based on the simplified Lesk algo-rithm (Vasilescu et al., 2004).
Each word wiina sequence w1w2...wnis disambiguated individ-ually by choosing the sense that maximizes thesimilarity between the gloss and the context of wi(i.e.
the whole text where wioccurs).
To boostthe overlap between the context and the gloss,this last is expanded with glosses of related mean-ings, following the approach described in Baner-jee and Pedersen (2002).
As sense inventory wechoose BabelNet 1.1, a huge multilingual seman-tic network which comprises both WordNet andWikipedia (Navigli and Ponzetto, 2012).
The al-gorithm consists of the following steps:1.
Building the glosses.
We retrieve all possibleword meanings for the target word withat arelisted in BabelNet.
BabelNet mixes sensesin WordNet and Wikipedia.
First, sensesin WordNet are searched for; if no sense isfound (as often happens with named enti-ties), senses for the target word are sought inWikipedia.
We preferred that strategy ratherthan retrieving senses from both sources atonce because this last approach producedworse results when tuning the system.
Oncethe set of senses Si= {si1, si2, ..., sik} as-sociated to the target word wihas been re-trieved, gloss expansion occurs.
For eachsense sijof wi, the algorithm builds the senseextended gloss g?ijby appending the glossesof meanings related to sijto its original glossgij.
The related meanings, with the exceptionof ?antonym?
senses, are the output of theBabelNet function ?getRelatedMap?.
More-over, each word in g?ijis weighted by a func-tion inversely proportional to the distance be-tween sijand its related meaning.
The dis-tance d is computed as the number of edgeslinking two senses in the graph.
The func-tion takes also into account the frequenciesof the words in all the glosses giving moreemphasis to the most discriminative words;this can be considered as a variation of the in-verse document frequency (idf ) for retrievalthat we named inverse gloss frequency (igf ).The igf for a word wkoccurring gf?ktimes inthe set of extended glosses for all the sensesin Si, the sense inventory of wi, is computedas IGFk= 1 + log2|Si|gf?k.
The final weightfor the word wkappearing h times in the ex-tended gloss g?ijis given by:weight(wk, g?ij) = h?
IGFk?11 + d(1)2.
Building the context.
The context C for theword wiis represented by all the words thatoccur in the text.3.
Building the vector representations.
The con-text C and each extended gloss g?ijare repre-sented as vectors in the SemanticSpace builtthrough the DSM described in Subsection2.1.4.
Sense ranking.
The algorithm computes thecosine similarity between the vector repre-sentation of each extended gloss g?ijand thatof the context C. Then, the cosine similar-ity is linearly combined with the probabilityp(sij|wi), which takes into account the sensedistribution of sijgiven the word wi.
Thesense distribution is computed as the num-ber of times the word wiwas tagged withthe sense sijin SemCor, a collection of 352documents manually annotated with Word-Net synsets.
T he additive (Laplace) smooth-ing prevents zero probabilities, which can oc-cur when some synsets do not appear in Sem-Cor.
The probability is computed as follows:p(sij|wi) =t(wi, sij) + 1#wi+ |Si|(2)The output of this step is a ranked list ofsynsets.The WSD measure (WSD) is computed on the topof the output of the last step.
For each text frag-ment, we build a Bag-of-Synset (BoS) as the sum,over the whole text, of the weighted synsets as-sociated with each word.
Then, we compute theWSD similarity as the cosine similarity betweenthe two BoS.749Graph.
A sub-graph of BabelNet is built foreach text fragment starting from the synsets pro-vided by the WSD algorithm.
For each word thesynset with the highest score is selected, then thisinitial set is expanded with the related synsets inBabelNet.
We apply the Personalized Page Rank(Haveliwala, 2002) to each sub-graph where thesynset scores computed by the WSD algorithm areexploited as prior probabilities.
The weighted rankof synsets provided by Page Rank is used to buildthe BoS of the two text fragments, then the Person-alized Page Rank (PPR) is computed as the cosinesimilarity between them.Synset Distributional Space.
Generally, sim-ilarity measures between synsets rely on thesynsets hierarchy in a semantic network (e.g.WordNet).
We define a new approach that is com-pletely different, and represents synsets as pointsin a geometric space that we call SDS (Synset Dis-tributional Space).
SDS is generated taking intoaccount the synset relationships, and similarity isdefined as the synsets closeness in the space.
Webuild a symmetric matrix S which contains synsetson both rows and columns.
Each cell in the matrixis set to one if a semantic relation exists betweenthe corresponding synsets.
The relationships areextracted from BabelNet limiting synsets to thoseoccurring also in WordNet, while synsets comingfrom Wikipedia are removed to reduce the sizeof S. The method for building the matrix S re-lies on Reflective Random Indexing (RRI) (Co-hen et al., 2010), a variation of the Random In-dexing technique for matrix reduction (Kanerva,1988).
RRI retains the advantages of RI whichincrementally builds a reduced space where dis-tance between points is nearly preserved.
More-over, cyclical training, i.e.
the retraining of a newspace exploiting the RI output as basis vectors,makes indirect inference to emerge.
Two differ-ent similarity measures can be defined by exploit-ing this space for representing synsets: WSD-SDSand PPR-SDS, based on WSD and PPR respec-tively.
Each BoS is represented as the sum of thesynset vectors in the SDS space.
Then, the simi-larity is computed as the cosine similarity betweenthe two vector representations.2.3 Surface LevelAt the surface level, we compute the followingfeatures:EDIT The edit, or Levenshtein, distance betweenthe two texts;MCS The most common subsequence betweenthe two texts;2-gram, 3-gram For each text fragment, webuild the Bag-of-n-gram (with n varying in{2, 3}); then we compute the cosine similar-ity between the two Bag-of-n-gram repre-sentations.BOW For each tokenized text fragment, we buildits Bag-of-Word, and then compute the co-sine similarity between the two BoW.L1 The length in characters of the first text frag-ment;L2 The length in caracters of the second text frag-ment;DIFF The difference between L1 and L2.2.4 Word to SenseThe word to sense level is different from the otherones: in this case the similarity is computed be-tween a word and a particular word meaning.Since a word meaning is not a text fragment, thislevel poses a new challenge with respect to theclassical text similarity task.
In this case we de-cide to consider the word on its own as the firsttext fragment, while for the second text fragmentwe build a dummy text using the BabelNet glossassigned to the word sense.
In that way, the distri-butional and the lexical measures (WSD, Graph,and DSM) can be applied to both fragments.
Ta-ble 1 recaps the features used for each task.3 EvaluationDataset Description.
The SemEval-2014 Task 3Cross-Level Semantic Similarity is designed forevaluating systems on their ability to capture thesemantic similarity between lexical items of dif-ferent length (Jurgens et al., 2014).
To this ex-tent, the organizers provide four different levelsof comparison which correspond to four differentdatasets: 1) Paragraph to Sentence (Par2Sent); 2)Sentence to Phrase (Sent2Ph); 3) Phrase to Word(Ph2W); and 4) Word to Sense (W2Sense).For each dataset, the organizer released trial,training and test data.
While the trial includes afew examples (approximately 40), both trainingand test data comprise 500 pairs of text fragments.750Run Par2Sent Sent2Ph Ph2W W2SenseOfficialRankSpearmanCorrelationbestTrain .861 .793 .555 .420 - -LCS .527 .562 .165 .109 - -run1 .769 .729 .229 .165 7 10run2 .784 .734 .255 .180 6 8run3 .769 .729 .229 .165 8 11Table 2: Task results.FeaturePar2SentSent2PhPh2WW2SenseDSM?
?WSD?
?PPR?
?WSD-SDS?
?PPR-SDS?
?EDIT?-MCS?-2-gram?-3-gram?-BOW?-L1?-L2?-DIFF?-Table 1: Features per task.Each pair is associated with a human-assignedsimilarity score, which varies from 4 (very similar)to 0 (unrelated).
Organizers provide the normal-ized Longest Common Substring (LCS) as base-line.
The evaluation is performed through thePearson (official rank) and the Spearman?s rankcorrelation.System setup.
We develop our system in JAVArelying on the following resources:?
Stanford CoreNLP to pre-process the text:tokenization, lemmatization and PoS-taggingare applied to the two text fragments;?
BabelNet 1.1 as knowledge-base in the WSDalgorithm;?
JAVA JUNG library for Personalized PageRank;?
British National Corpus (tokenized text withstop word removal) and SVDLIB to build theSemanticSpace described in Subsection 2.1;?
A proprietary implementation of ReflectiveRandom Indexing to build the distributionalspace based on synsets (SDS) extracted fromBabelNet (we used two cycles of retraining);?
Weka for the supervised approach.After a tuning step using both training and trialdata provided by organizers, we selected three dif-ferent supervised systems: Gaussian Process withPuk kernel (run1), Gaussian Process with RBFkernel (run2), and Support Vector Machine Re-gression with Puk kernel (run3).
All the sys-tems are implemented with the default parame-ters set by Weka.
We trained a different model oneach dataset.
The DSM is built using the 100, 000most frequent terms in the BNC, while the co-occurrences are computed on a window size of 5words.
The vector dimension is set to 400, thesame value is adopted for building the SDS, wherethe number of seeds (no zero components) gener-ated in the random vectors is set to 10 with onestep of retraining.
The total number of synset vec-tors in the SDS is 576, 736.
In the WSD algorithm,we exploited the whole sentence as context.
Thelinear combination between the cosine similarityand the probability p(sij|wi) is performed with afactor of 0.5.
The distance for expanding a synsetwith its related meaning is set to one.
The samedepth is used for building the graph in the PPRmethod, where we fixed the maximum number ofiterations up to 50 and the dumpling factor to 0.85.Results.
Results of our three systems foreach similarity level are reported in Table 2 withthe baseline provided by the organizer (LCS).Our three systems always outperform the LCSbaseline.
Table 2 also shows the best results(bestTrain) obtained on the training data by aGaussian Process with Puk kernel and a 10-foldcross-validation.
Support Vector Machine andGaussian Process with Puk kernel, run1 and run3respectively, produce the same results.
Comparing751TaskDSMWSDPPRWSD-SDSPPR-SDSEDITMCS2-gram3-gramBOWL1L2DIFFP ar2Sent .612 .697 -.580 .129 .129 .461 .44 .630 .478 .585 .002 .231 .116Sent2Ph .540 .649 -.641 .110 .110 .526 .474 .376 .236 .584 .069 .357 .218Ph2W .228 .095 -.094 .087 .087 .136 .120 - - .095 .079 .013 .071W2Sense .147 .085 -.062 .084 .062Table 3: Individual measures for each task.these figures with those obtained on training data(run1 and run3 vs. bestTrain), we can observethat the Puk kernel tends to over-fit on trainingdata, while RBF kernel seems to be less sensitiveto this problem.We analysed also the performance of each mea-sure on its own; results in Table 3 are obtained bytraining the best supervised system (run2) withdefault parameters on each feature individually.WSD obtains the best results in the first two lev-els, while DSM is the best method in the last twoones.
This behaviour can be ascribed to the sizeof the text fragments.
In large text fragments theWSD algorithm can rely on wider contexts to ob-tain good performance; while in short texts infor-mation about context is poor.
At the W2Senselevel, the measure based on the Personalized PageRank obtains the worst results; however, we no-ticed that the ablation of that feature causes a dropin performance of the supervised systems.After the submission deadline, we noticed thatsometimes PoS-tagging produced wrong resultson small texts.
This incorrect behaviour influencednegatively the correct retrieval of synsets from Ba-belNet.
Thus, we decided to exclude PoS-taggingfor text fragments with less than three words.
Insuch a case, all the synsets for a given word areretrieved.
Making this adjustment, we were ableto obtain the improvements (?%) with respect tothe submitted runs reported on Table 4.Run Ph2W ?% W2Sense ?%run1 .263 +14.85 .242 +46.67run2 .257 +00.78 .237 +31.67run3 .263 +14.85 .242 +46.66Table 4: Results after PoS-tagging removal forshort text (< 3 words).4 ConclusionsWe have reported the results of our participa-tion in the cross-level semantic similarity taskof SemEval-2014.
Our systems combine differ-ent similarity measures based on string-matching,word sense disambiguation and distributional se-mantic models.
Our best system ranks 6th outof the 38 participants in the task with respect tothe Pearson correlation, while it ranks 8th whenSpearman was used.
These results suggest that ourmethods are robust with respect to the evaluationmeasures.AcknowledgmentsThis work fulfils the research objectives of thePON 02 00563 3470993 project ?VINCENTE -A Virtual collective INtelligenCe ENvironment todevelop sustainable Technology Entrepreneurshipecosystems?
funded by the Italian Ministry of Uni-versity and Research (MIUR).ReferencesSatanjeev Banerjee and Ted Pedersen.
2002.
Anadapted lesk algorithm for word sense disambigua-tion using wordnet.
In Computational linguis-tics and intelligent text processing, pages 136?145.Springer.Pierpaolo Basile, Annalina Caputo, and Giovanni Se-meraro.
2014.
An Enhanced Lesk Word SenseDisambiguation algorithm through a DistributionalSemantic Model.
In Proceedings of COLING2014, Dublin, Ireland, August.
(http://www.coling-2014.org/accepted-papers.php, in press).Trevor Cohen, Roger Schvaneveldt, and Dominic Wid-dows.
2010.
Reflective random indexing and indi-rect inference: A scalable method for discovery ofimplicit connections.
Journal of Biomedical Infor-matics, 43(2):240 ?
256.Taher H. Haveliwala.
2002.
Topic-sensitive pagerank.In Proceedings of the 11th International Conference752on World Wide Web, WWW ?02, pages 517?526,New York, NY, USA.
ACM.David Jurgens, Mohammad Taher Pilehvar, andRoberto Navigli.
2014.
Semeval-2014 task 3:Cross-level semantic similarity.
In Proceedings ofthe 8th International Workshop on Semantic Evalu-ation (SemEval-2014), Dublin, Ireland, August 23?24.Pentti Kanerva.
1988.
Sparse Distributed Memory.MIT Press.Thomas K. Landauer and Susan T. Dumais.
1997.
ASolution to Plato?s Problem: The Latent SemanticAnalysis Theory of Acquisition, Induction, and Rep-resentation of Knowledge.
Psychological Review,104(2):211?240.Roberto Navigli and Simone Paolo Ponzetto.
2012.BabelNet: The automatic construction, evaluationand application of a wide-coverage multilingual se-mantic network.
Artificial Intelligence, 193:217?250.Florentina Vasilescu, Philippe Langlais, and Guy La-palme.
2004.
Evaluating variants of the lesk ap-proach for disambiguating words.
In Proceedings ofthe Conference on Language Resources and Evalu-ation (LREC), pages 633?636.753
