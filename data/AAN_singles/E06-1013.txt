Generalized Hebbian Algorithm for Incremental Singular ValueDecomposition in Natural Language ProcessingGenevieve GorrellDepartment of Computer and Information ScienceLinko?ping University581 83 LINKO?PINGSwedengengo@ida.liu.seAbstractAn algorithm based on the GeneralizedHebbian Algorithm is described thatallows the singular value decompositionof a dataset to be learned based onsingle observation pairs presented seri-ally.
The algorithm has minimal mem-ory requirements, and is therefore in-teresting in the natural language do-main, where very large datasets are of-ten used, and datasets quickly becomeintractable.
The technique is demon-strated on the task of learning wordand letter bigram pairs from text.1 IntroductionDimensionality reduction techniques are ofgreat relevance within the field of natural lan-guage processing.
A persistent problem withinlanguage processing is the over-specificity oflanguage, and the sparsity of data.
Corpus-based techniques depend on a sufficiency ofexamples in order to model human languageuse, but the Zipfian nature of frequency be-haviour in language means that this approachhas diminishing returns with corpus size.
Inshort, there are a large number of ways to saythe same thing, and no matter how large yourcorpus is, you will never cover all the thingsthat might reasonably be said.
Language isoften too rich for the task being performed;for example it can be difficult to establish thattwo documents are discussing the same topic.Likewise no matter how much data your sys-tem has seen during training, it will invari-ably see something new at run-time in a do-main of any complexity.
Any approach to au-tomatic natural language processing will en-counter this problem on several levels, creat-ing a need for techniques which compensatefor this.Imagine we have a set of data stored as amatrix.
Techniques based on eigen decomposi-tion allow such a matrix to be transformed intoa set of orthogonal vectors, each with an asso-ciated ?strength?, or eigenvalue.
This trans-formation allows the data contained in the ma-trix to be compressed; by discarding the lesssignificant vectors (dimensions) the matrix canbe approximated with fewer numbers.
Thisis what is meant by dimensionality reduction.The technique is guaranteed to return the clos-est (least squared error) approximation possi-ble for a given number of numbers (Golub andReinsch, 1970).
In certain domains, however,the technique has even greater significance.
Itis effectively forcing the data through a bot-tleneck; requiring it to describe itself usingan impoverished construct set.
This can al-low the critical underlying features to revealthemselves.
In language, for example, thesefeatures might be semantic constructs.
It canalso improve the data, in the case that the de-tail is noise, or richness not relevant to thetask.Singular value decomposition (SVD) is anear relative of eigen decomposition, appro-priate to domains where input is asymmetri-cal.
The best known application of singularvalue decomposition within natural languageprocessing is Latent Semantic Analysis (Deer-wester et al, 1990).
Latent Semantic Analysis(LSA) allows passages of text to be comparedto each other in a reduced-dimensionality se-mantic space, based on the words they contain.97The technique has been successfully applied toinformation retrieval, where the overspecificityof language is particularly problematic; textsearches often miss relevant documents wheredifferent vocabulary has been chosen in thesearch terms to that used in the document (forexample, the user searches on ?eigen decom-position?
and fails to retrieve documents onfactor analysis).
LSA has also been applied inlanguage modelling (Bellegarda, 2000), whereit has been used to incorporate long-span se-mantic dependencies.Much research has been done on optimis-ing eigen decomposition algorithms, and theextent to which they can be optimised de-pends on the area of application.
Most natu-ral language problems involve sparse matrices,since there are many words in a natural lan-guage and the great majority do not appear in,for example, any one document.
Domains inwhich matrices are less sparse lend themselvesto such techniques as Golub-Kahan-Reinsch(Golub and Reinsch, 1970) and Jacobi-like ap-proaches.
Techniques such as those describedin (Berry, 1992) are more appropriate in thenatural language domain.Optimisation is an important way to in-crease the applicability of eigen and singu-lar value decomposition.
Designing algorithmsthat accommodate different requirements isanother.
For example, another drawback toJacobi-like approaches is that they calculateall the singular triplets (singular vector pairswith associated values) simultaneously, whichmay not be practical in a situation where onlythe top few are required.
Consider also thatthe methods mentioned so far assume that theentire matrix is available from the start.
Thereare many situations in which data may con-tinue to become available.
(Berry et al, 1995) describe a number oftechniques for including new data in an ex-isting decomposition.
Their techniques applyto a situation in which SVD has been per-formed on a collection of data, then new databecomes available.
However, these techniquesare either expensive, or else they are approxi-mations which degrade in quality over time.They are useful in the context of updatingan existing batch decomposition with a sec-ond batch of data, but are less applicable inthe case where data are presented serially, forexample, in the context of a learning system.Furthermore, there are limits to the size of ma-trix that can feasibly be processed using batchdecomposition techniques.
This is especiallyrelevant within natural language processing,where very large corpora are common.
Ran-dom Indexing (Kanerva et al, 2000) providesa less principled, though very simple and ef-ficient, alternative to SVD for dimensionalityreduction over large corpora.This paper describes an approach to singu-lar value decomposition based on the General-ized Hebbian Algorithm (Sanger, 1989).
GHAcalculates the eigen decomposition of a ma-trix based on single observations presented se-rially.
The algorithm presented here differs inthat where GHA produces the eigen decom-position of symmetrical data, our algorithmproduces the singular value decomposition ofasymmetrical data.
It allows singular vectorsto be learned from paired inputs presented se-rially using no more memory than is requiredto store the singular vector pairs themselves.It is therefore relevant in situations where thesize of the dataset makes conventional batchapproaches infeasible.
It is also of interest inthe context of adaptivity, since it has the po-tential to adapt to changing input.
The learn-ing update operation is very cheap computa-tionally.
Assuming a stable vector length, eachupdate operation takes exactly as long as eachprevious one; there is no increase with corpussize to the speed of the update.
Matrix di-mensions may increase during processing.
Thealgorithm produces singular vector pairs oneat a time, starting with the most significant,which means that useful data becomes avail-able quickly; many standard techniques pro-duce the entire decomposition simultaneously.Since it is a learning technique, however, it dif-fers from what would normally be consideredan incremental technique, in that the algo-rithm converges on the singular value decom-position of the dataset, rather than at any onepoint having the best solution possible for thedata it has seen so far.
The method is poten-tially most appropriate in situations where thedataset is very large or unbounded: smaller,bounded datasets may be more efficiently pro-cessed by other methods.
Furthermore, our98approach is limited to cases where the finalmatrix is expressible as the linear sum of outerproducts of the data vectors.
Note in particu-lar that Latent Semantic Analysis, as usuallyimplemented, is not an example of this, be-cause LSA takes the log of the final sums ineach cell (Dumais, 1990).
LSA, however, doesnot depend on singular value decomposition;Gorrell and Webb (Gorrell and Webb, 2005)discuss using eigen decomposition to performLSA, and demonstrate LSA using the Gen-eralized Hebbian Algorithm in its unmodifiedform.
Sanger (Sanger, 1993) presents similarwork, and future work will involve more de-tailed comparison of this approach to his.The next section describes the algorithm.Section 3 describes implementation in practi-cal terms.
Section 4 illustrates, using wordn-gram and letter n-gram tasks as examplesand section 5 concludes.2 The AlgorithmThis section introduces the Generalized Heb-bian Algorithm, and shows how the techniquecan be adapted to the rectangular matrix formof singular value decomposition.
Eigen decom-position requires as input a square diagonally-symmetrical matrix, that is to say, one inwhich the cell value at row x, column y isthe same as that at row y, column x. Thekind of data described by such a matrix isthe correlation between data in a particularspace with other data in the same space.
Forexample, we might wish to describe how of-ten a particular word appears with a particu-lar other word.
The data therefore are sym-metrical relations between items in the samespace; word a appears with word b exactly asoften as word b appears with word a.
In sin-gular value decomposition, rectangular inputmatrices are handled.
Ordered word bigramsare an example of this; imagine a matrix inwhich rows correspond to the first word in abigram, and columns to the second.
The num-ber of times that word b appears after worda is by no means the same as the numberof times that word a appears after word b.Rows and columns are different spaces; rowsare the space of first words in the bigrams,and columns are the space of second words.The singular value decomposition of a rect-angular data matrix, A, can be presented as;A = U?V T (1)where U and V are matrices of orthogonal leftand right singular vectors (columns) respec-tively, and ?
is a diagonal matrix of the cor-responding singular values.
The U and V ma-trices can be seen as a matched set of orthogo-nal basis vectors in their corresponding spaces,while the singular values specify the effectivemagnitude of each vector pair.
By convention,these matrices are sorted such that the diag-onal of ?
is monotonically decreasing, and itis a property of SVD that preserving only thefirst (largest) N of these (and hence also onlythe first N columns of U and V) provides aleast-squared error, rank-N approximation tothe original matrix A.Singular Value Decomposition is intimatelyrelated to eigenvalue decomposition in that thesingular vectors, U and V , of the data matrix,A, are simply the eigenvectors of A ?
AT andAT ?
A, respectively, and the singular values,?, are the square-roots of the correspondingeigenvalues.2.1 Generalised Hebbian AlgorithmOja and Karhunen (Oja and Karhunen, 1985)demonstrated an incremental solution to find-ing the first eigenvector from data arriving inthe form of serial data items presented as vec-tors, and Sanger (Sanger, 1989) later gener-alized this to finding the first N eigenvectorswith the Generalized Hebbian Algorithm.
Thealgorithm converges on the exact eigen decom-position of the data with a probability of one.The essence of these algorithms is a simpleHebbian learning rule:Un(t + 1) = Un(t) + ?
?
(UTn ?Aj) ?Aj (2)Un is the n?th column of U (i.e., the n?th eigen-vector, see equation 1), ?
is the learning rateand Aj is the j?th column of training matrixA.
t is the timestep.
The only modification tothis required in order to extend it to multipleeigenvectors is that each Un needs to shadowany lower-ranked Um(m > n) by removing itsprojection from the input Aj in order to assureboth orthogonality and an ordered ranking of99the resulting eigenvectors.
Sanger?s final for-mulation (Sanger, 1989) is:cij(t+ 1) = cij(t) + ?
(t)(yi(t)xj(t) (3)?yi(t)?k?ickj(t)yk(t))In the above, cij is an individual element inthe current eigenvector, xj is the input vectorand yi is the activation (that is to say, ci.xj ,the dot product of the input vector with theith eigenvector).
?
is the learning rate.To summarise, the formula updates the cur-rent eigenvector by adding to it the input vec-tor multiplied by the activation minus the pro-jection of the input vector on all the eigenvec-tors so far including the current eigenvector,multiplied by the activation.
Including thecurrent eigenvector in the projection subtrac-tion step has the effect of keeping the eigen-vectors normalised.
Note that Sanger includesan explicit learning rate, ?.
The formula canbe varied slightly by not including the currenteigenvector in the projection subtraction step.In the absence of the autonormalisation influ-ence, the vector is allowed to grow long.
Thishas the effect of introducing an implicit learn-ing rate, since the vector only begins to growlong when it settles in the right direction, andsince further learning has less impact once thevector has become long.
Weng et al (Wenget al, 2003) demonstrate the efficacy of thisapproach.
So, in vector form, assuming C tobe the eigenvector currently being trained, ex-panding y out and using the implicit learningrate;ci = ci.x(x?
?j<i(x.cj)cj) (4)Delta notation is used to describe the updatehere, for further readability.
The subtractedelement is responsible for removing from thetraining update any projection on previoussingular vectors, thereby ensuring orthgonal-ity.
Let us assume for the moment that weare calculating only the first eigenvector.
Thetraining update, that is, the vector to be addedto the eigenvector, can then be more simplydescribed as follows, making the next stepsmore readable;c = c.x(x) (5)2.2 Extension to Paired DataLet us begin with a simplification of 5:c = 1ncX(X) (6)Here, the upper case X is the entire data ma-trix.
n is the number of training items.
Thesimplification is valid in the case that c is sta-bilised; a simplification that in our case willbecome more valid with time.
Extension topaired data initially appears to present a prob-lem.
As mentioned earlier, the singular vectorsof a rectangular matrix are the eigenvectorsof the matrix multiplied by its transpose, andthe eigenvectors of the transpose of the matrixmultiplied by itself.
Running GHA on a non-square non-symmetrical matrix M, ie.
paireddata, would therefore be achievable using stan-dard GHA as follows:ca = 1ncaMMT (MMT ) (7)cb = 1ncbMTM(MTM) (8)In the above, ca and cb are left and right sin-gular vectors.
However, to be able to feed thealgorithm with rows of the matrices MMTand MTM , we would need to have the en-tire training corpus available simultaneously,and square it, which we hoped to avoid.
Thismakes it impossible to use GHA for singu-lar value decomposition of serially-presentedpaired input in this way without some furthertransformation.
Equation 1, however, gives:?ca = cbMT =?x(cb.bx)ax (9)?cb = caM =?x(ca.ax)bx (10)Here, ?
is the singular value and a and b areleft and right data vectors.
The above is validin the case that left and right singular vectorsca and cb have settled (which will become moreaccurate over time) and that data vectors aand b outer-product and sum to M.100Inserting 9 and 10 into 7 and 8 allows themto be reduced as follows:ca = ?ncbMTMMT (11)cb = ?ncaMMTM (12)ca = ?2n caMMT (13)cb = ?2n cbMTM (14)ca = ?3n cbMT (15)cb = ?3n caM (16)ca = ?3(cb.b)a (17)cb = ?3(ca.a)b (18)This element can then be reinserted into GHA.To summarise, where GHA dotted the inputwith the eigenvector and multiplied the resultby the input vector to form the training up-date (thereby adding the input vector to theeigenvector with a length proportional to theextent to which it reflects the current direc-tion of the eigenvector) our formulation dotsthe right input vector with the right singularvector and multiplies the left input vector bythis quantity before adding it to the left singu-lar vector, and vice versa.
In this way, the twosides cross-train each other.
Below is the finalmodification of GHA extended to cover mul-tiple vector pairs.
The original GHA is givenbeneath it for comparison.cai = cbi .b(a?
?j<i(a.caj )caj ) (19)cbi = cai .a(b?
?j<i(b.cbj)cbj) (20)ci = ci.x(x?
?j<i(x.cj)cj) (21)In equations 6 and 9/10 we introduced approx-imations that become accurate as the directionof the singular vectors settles.
These approx-imations will therefore not interfere with theaccuracy of the final result, though they mightinterfere with the rate of convergence.
Theconstant ?3 has been dropped in 19 and 20.Its relevance is purely with respect to the cal-culation of the singular value.
Recall that in(Weng et al, 2003) the eigenvalue is calcula-ble as the average magnitude of the trainingupdate c.
In our formulation, according to17 and 18, the singular value would be c di-vided by ?3.
Dropping the ?3 in 19 and 20achieves that implicitly; the singular value isonce more the average length of the trainingupdate.The next section discusses practical aspectsof implementation.
The following section illus-trates usage, with English language word andletter bigram data as test domains.3 ImplementationWithin the framework of the algorithm out-lined above, there is still room for some im-plementation decisions to be made.
The naiveimplementation can be summarised as follows:the first datum is used to train the first singu-lar vector pair; the projection of the first singu-lar vector pair onto this datum is subtractedfrom the datum; the datum is then used totrain the second singular vector pair and so onfor all the vector pairs; ensuing data items areprocessed similarly.
The main problem withthis approach is as follows.
At the beginningof the training process, the singular vectors areclose to the values they were initialised with,and far away from the values they will settleon.
The second singular vector pair is trainedon the datum minus its projection onto thefirst singular vector pair in order to preventthe second singular vector pair from becom-ing the same as the first.
But if the first pairis far away from its eventual direction, thenthe second has a chance to move in the direc-tion that the first will eventually take on.
Infact, all the vectors, such as they can whilst re-maining orthogonal to each other, will move inthe strongest direction.
Then, when the firstpair eventually takes on the right direction,the others have difficulty recovering, since theystart to receive data that they have very lit-tle projection on, meaning that they learn very101slowly.
The problem can be addressed by wait-ing until each singular vector pair is relativelystable before beginning to train the next.
By?stable?, we mean that the vector is changinglittle in its direction, such as to suggest it isvery close to its target.
Measures of stabilitymight include the average variation in posi-tion of the endpoint of the (normalised) vectorover a number of training iterations, or simplylength of the (unnormalised) vector, since along vector is one that is being reinforced bythe training data, such as it would be if it wassettled on the dominant feature.
Terminationcriteria might include that a target numberof singular vector pairs have been reached, orthat the last vector is increasing in length onlyvery slowly.4 ApplicationThe task of relating linguistic bigrams to eachother, as mentioned earlier, is an example ofa task appropriate to singular value decom-position, in that the data is paired data, inwhich each item is in a different space to theother.
Consider word bigrams, for example.First word space is in a non-symmetrical re-lationship to second word space; indeed, thespaces are not even necessarily of the same di-mensionality, since there could conceivably bewords in the corpus that never appear in thefirst word slot (they might never appear at thestart of a sentence) or in the second word slot(they might never appear at the end.)
So amatrix containing word counts, in which eachunique first word forms a row and each uniquesecond word forms a column, will not be asquare symmetrical matrix; the value at rowa, column b, will not be the same as the valueat row b column a, except by coincidence.The significance of performing dimension-ality reduction on word bigrams could bethought of as follows.
Language clearly ad-heres to some extent to a rule system lessrich than the individual instances that formits surface manifestation.
Those rules governwhich words might follow which other words;although the rule system is more complex andof a longer range that word bigrams can hopeto illustrate, nonetheless the rule system gov-erns the surface form of word bigrams, and wemight hope that it would be possible to discernfrom word bigrams something of the nature ofthe rules.
In performing dimensionality reduc-tion on word bigram data, we force the rules todescribe themselves through a more impover-ished form than via the collection of instancesthat form the training corpus.
The hope isthat the resulting simplified description willbe a generalisable system that applies even toinstances not encountered at training time.On a practical level, the outcome has ap-plications in automatic language acquisition.For example, the result might be applicable inlanguage modelling.
Use of the learning algo-rithm presented in this paper is appropriategiven the very large dimensions of any real-istic corpus of language; The corpus chosenfor this demonstration is Margaret Mitchell?s?Gone with the Wind?, which contains 19,296unique words (421,373 in total), which fully re-alized as a correlation matrix with, for exam-ple, 4-byte floats would consume 1.5 gigabytes,and which in any case, within natural languageprocessing, would not be considered a particu-larly large corpus.
Results on the word bigramtask are presented in the next section.Letter bigrams provide a useful contrast-ing illustration in this context; an input di-mensionality of 26 allows the result to bemore easily visualised.
Practical applicationsmight include automatic handwriting recogni-tion, where an estimate of the likelihood ofa particular letter following another would beuseful information.
The fact that there areonly twenty-something letters in most westernalphabets though makes the usefulness of theincremental approach, and indeed, dimension-ality reduction techniques in general, less ob-vious in this domain.
However, extending thespace to letter trigrams and even four-gramswould change the requirements.
Section 4.2discusses results on a letter bigram task.4.1 Word Bigram Task?Gone with the Wind?
was presented to thealgorithm as word bigrams.
Each word wasmapped to a vector containing all zeros but fora one in the slot corresponding to the uniqueword index assigned to that word.
This hadthe effect of making input to the algorithm anormalised vector, and of making word vec-tors orthogonal to each other.
The singularvector pair?s reaching a combined Euclidean102magnitude of 2000 was given as the criterionfor beginning to train the next vector pair, thereasoning being that since the singular vectorsonly start to grow long when they settle inthe approximate right direction and the datastarts to reinforce them, length forms a reason-able heuristic for deciding if they are settledenough to begin training the next vector pair.2000 was chosen ad hoc based on observationof the behaviour of the algorithm during train-ing.The data presented are the words most rep-resentative of the top two singular vectors,that is to say, the directions these singularvectors mostly point in.
Table 1 shows thewords with highest scores in the top two vec-tor pairs.
It says that in this vector pair, thenormalised left hand vector projected by 0.513onto the vector for the word ?of?
(or in otherwords, these vectors have a dot product of0.513.)
The normalised right hand vector hasa projection of 0.876 onto the word ?the?
etc.This first table shows a left side dominatedby prepositions, with a right side in which?the?
is by far the most important word, butwhich also contains many pronouns.
The factthat the first singular vector pair is effectivelyabout ?the?
(the right hand side points farmore in the direction of ?the?
than any otherword) reflects its status as the most commonword in the English language.
What this resultis saying is that were we to be allowed only onefeature with which to describe word Englishbigrams, a feature describing words appear-ing before ?the?
and words behaving similarlyto ?the?
would be the best we could choose.Other very common words in English are alsoprominent in this feature.Table 1: Top words in 1st singular vector pairVector 1, Eigenvalue 0.00938of 0.5125468 the 0.8755944in 0.49723375 her 0.28781646and 0.39370865 a 0.23318098to 0.2748983 his 0.14336193on 0.21759394 she 0.1128443at 0.17932475 it 0.06529821for 0.16905183 he 0.063333265with 0.16042696 you 0.058997907from 0.13463423 their 0.05517004Table 2 puts ?she?, ?he?
and ?it?
at thetop on the left, and four common verbs on theright, indicating a pronoun-verb pattern as thesecond most dominant feature in the corpus.Table 2: Top words in 2nd singular vector pairVector 2, Eigenvalue 0.00427she 0.6633538 was 0.58067155he 0.38005337 had 0.50169927it 0.30800354 could 0.2315106and 0.18958427 would 0.175892794.2 Letter Bigram TaskRunning the algorithm on letter bigrams illus-trates different properties.
Because there areonly 26 letters in the English alphabet, it ismeaningful to examine the entire singular vec-tor pair.
Figure 1 shows the third singular vec-tor pair derived by running the algorithm onletter bigrams.
The y axis gives the projectionof the vector for the given letter onto the sin-gular vector.
The left singular vector is givenon the left, and the right on the right, that isto say, the first letter in the bigram is on theleft and the second on the right.
The first twosingular vector pairs are dominated by letterfrequency effects, but the third is interestingbecause it clearly shows that the method hasidentified vowels.
It means that the third mostuseful feature for determining the likelihood ofletter b following letter a is whether letter ais a vowel.
If letter b is a vowel, letter a isless likely to be (vowels dominate the nega-tive end of the right singular vector).
(Laterfeatures could introduce subcases where a par-ticular vowel is likely to follow another partic-ular vowel, but this result suggests that themost dominant case is that this does not hap-pen.)
Interestingly, the letter ?h?
also appearsat the negative end of the right singular vec-tor, suggesting that ?h?
for the most part doesnot follow a vowel in English.
Items near zero(?k?, ?z?
etc.)
are not strongly represented inthis singular vector pair; it tells us little aboutthem.5 ConclusionAn incremental approach to approximatingthe singular value decomposition of a cor-relation matrix has been presented.
Use103Figure 1: Third Singular Vector Pair on Letter Bigram Task-0.6-0.4-0.200.20.40.6i aoeu_ q x j z n y f k p g b s d wv l c mrt hnr ts ml c d f v wp g b u k x z j q_ ya i oheof the incremental approach means thatsingular value decomposition is an optionin situations where data takes the form ofsingle serially-presented observations from anunknown matrix.
The method is particularlyappropriate in natural language contexts,where datasets are often too large to be pro-cessed by traditional methods, and situationswhere the dataset is unbounded, for examplein systems that learn through use.
Theapproach produces preliminary estimationsof the top vectors, meaning that informa-tion becomes available early in the trainingprocess.
By avoiding matrix multiplication,data of high dimensionality can be processed.Results of preliminary experiments have beendiscussed here on the task of modelling wordand letter bigrams.
Future work will includean evaluation on much larger corpora.Acknowledgements: The author would liketo thank Brandyn Webb for his contribution,and the Graduate School of Language Technol-ogy and Vinnova for their financial support.ReferencesJ.
Bellegarda.
2000.
Exploiting latent semantic in-formation in statistical language modeling.
Pro-ceedings of the IEEE, 88:8.Michael W. Berry, Susan T. Dumais, and Gavin W.O?Brien.
1995.
Using linear algebra for in-telligent information retrieval.
SIAM Review,34(4):573?595.R.
W. Berry.
1992.
Large-scale sparse singularvalue computations.
The International Journalof Supercomputer Applications, 6(1):13?49.Scott C. Deerwester, Susan T. Dumais, Thomas K.Landauer, George W. Furnas, and Richard A.Harshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society of In-formation Science, 41(6):391?407.S.
Dumais.
1990.
Enhancing performance in latentsemantic indexing.
TM-ARH-017527 TechnicalReport, Bellcore, 1990.G.
H. Golub and C. Reinsch.
1970.
Handbook se-ries linear algebra.
singular value decompositionand least squares solutions.
Numerical Mathe-matics, 14:403?420.G.
Gorrell and B. Webb.
2005.
Generalized heb-bian algorithm for latent semantic analysis.
InProceedings of Interspeech 2005.P.
Kanerva, J. Kristoferson, and A. Holst.
2000.Random indexing of text samples for latent se-mantic analysis.
In Proceedings of 22nd AnnualConference of the Cognitive Science Society.E.
Oja and J. Karhunen.
1985.
On stochastic ap-proximation of the eigenvectors and eigenvaluesof the expectation of a random matrix.
J. Math.Analysis and Application, 106:69?84.Terence D. Sanger.
1989.
Optimal unsupervisedlearning in a single-layer linear feedforward neu-ral network.
Neural Networks, 2:459?473.Terence D. Sanger.
1993.
Two iterative algorithmsfor computing the singular value decompositionfrom input/output samples.
NIPS, 6:144?151.Juyang Weng, Yilu Zhang, and Wey-ShiuanHwang.
2003.
Candid covariance-free incremen-tal principal component analysis.
IEEE Trans-actions on Pattern Analysis and Machine Intel-ligence, 25:8:1034?1040.104
