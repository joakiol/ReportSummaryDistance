Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 102?110,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsUsing the argumentative structure of scientific literature to improveinformation accessAntonio Jimeno YepesNational ICT AustraliaVictoria Research LaboratoryMelbourne, Australiaantonio.jimeno@gmail.comJames G. MorkNational Library of Medicine8600 Rockville PikeBethesda, 20894, MD, USAmork@nlm.nih.govAlan R. AronsonNational Library of Medicine8600 Rockville PikeBethesda, 20894, MD, USAalan@nlm.nih.govAbstractMEDLINE/PubMed contains structuredabstracts that can provide argumentativelabels.
Selection of abstract sentencesbased on the argumentative label hasshown to improve the performance of in-formation retrieval tasks.
These abstractsmake up less than one quarter of all theabstracts in MEDLINE/PubMed, so it isworthwhile to learn how to automaticallylabel the non-structured ones.We have compared several machine learn-ing algorithms trained on structured ab-stracts to identify argumentative labels.We have performed an intrinsic evalua-tion on predicting argumentative labels fornon-structured abstracts and an extrinsicevaluation to predict argumentative labelson abstracts relevant to Gene ReferenceInto Function (GeneRIF) indexing.Intrinsic evaluation shows that argumen-tative labels can be assigned effectivelyto structured abstracts.
Algorithms thatmodel the argumentative structure seemto perform better than other algorithms.Extrinsic results show that assigning ar-gumentative labels to non-structured ab-stracts improves the performance onGeneRIF indexing.
On the other hand, thealgorithms that model the argumentativestructure of the abstracts obtain lower per-formance in the extrinsic evaluation.1 IntroductionMEDLINE R?/PubMed R?
is the largest repositoryof biomedical abstracts.
The large quantity ofunstructured information available from MED-LINE/PubMed prevents finding information effi-ciently.
Reducing the information that users needto process could improve information access andsupport database curation.
It has been suggestedthat identifying the argumentative label of the ab-stract sentences could provide better informationthrough information retrieval (Ruch et al 2003;Jonnalagadda et al 2012) and/or information ex-traction (Mizuta et al 2006).Some journals indexed in MEDLINE/PubMedalready provide the abstracts in a structured for-mat (Ripple et al 2012).
A structured abstract1 isan abstract with distinct labeled sections (e.g., In-troduction, Background, or Results).
In the MED-LINE/PubMed data, these labels usually appear inall uppercase letters and are followed by a colon(e.g., MATERIALS AND METHODS:).
Structuredabstracts are becoming an increasingly larger seg-ment of the MEDLINE/PubMed database with al-most a quarter of all abstracts added to the MED-LINE/PubMed database each year being struc-tured abstracts.
A recent PubMed query (April 22,2013) shows 1,050,748 citations from 2012, and249,196 (23.72%)2 of these are considered struc-tured abstracts.On August 16, 2010, PubMed began display-ing structured abstracts formatted to highlight thevarious sections within the structured abstracts tohelp readers identify areas of interest3.
The XMLformatted abstract from MEDLINE/PubMed sep-arates each label in the structured abstract and in-cludes a mapping to one of five U.S. National Li-brary of Medicine (NLM) assigned categories asshown in the example below:<AbstractText Label=?MATERIALS ANDMETHODS?
NlmCategory=?METHODS?>The five NLM categories that all labelsare mapped to are OBJECTIVE, CONCLU-SIONS, RESULTS, METHODS, and BACK-GROUND (Ripple et al 2011).
If a label is new1http://www.nlm.nih.gov/bsd/policy/structured abstracts.html2hasstructuredabstract AND 2012[pdat]3http://www.nlm.nih.gov/pubs/techbull/ja10/ja10 structured abstracts.html102or not in the list of reviewed structured abstract la-bels, it will receive a category of UNASSIGNED.There are multiple criteria for deciding what ab-stracts are considered structured abstracts or not.One simple definition would be that an abstractcontains one or more author defined labels.
Amore rigid criterion which is followed by NLM4is that an abstract must contain three or moreunique valid labels (previously identified and cat-egorized), and one of the labels must be an endingtype label (e.g., CONCLUSIONS).
The five NLMcategories are normally manually reviewed and as-signed once a year to as many new labels as pos-sible.
Currently, NLM has identified 1,949 (Au-gust 31, 2012) unique labels and categorized theminto one of the five categories.
These 1,949 labelsmake up approximately 98% of all labels and la-bel variations found in the structured abstracts inMEDLINE/PubMed3.
An example of structuredabstract is presented in Table 1.Several studies have shown that the labels of thestructured abstracts can be reassigned effectivelybased on a Conditional Random Field (CRF) mod-els (Hirohata et al 2008).
On the other hand, itis unclear if these models are as effective on non-structured abstracts (Agarwal and Yu, 2009).In this paper, we compare several learning al-gorithms trained on structured abstract data to as-sign argumentative labels to non-structured ab-stracts.
We performed comparison tests of thetrained models both intrinsically on a held out setof the structured abstracts and extrinsically on aset of non-structured abstracts.The intrinsic evaluation is performed on a dataset of held out structured abstracts that have hadtheir label identification removed to model non-structured abstracts.
Argumentative labels are as-signed to the sentences based on the trained mod-els and used to identify label categorization.The extrinsic evaluation is performed on a dataset of non-structured abstracts on the task of iden-tifying GeneRIF (Gene Into Function) sentences.Argumentative labels are assigned to the sentencesbased on the trained models and used to performthe selection of relevant GeneRIF sentences.Intrinsic evaluation shows that argumentativelabels can be assigned effectively to structured ab-stracts.
Algorithms that model the argumentativestructure, like Conditional Random Field (CRF),seem to perform better than other algorithms.
Re-4http://structuredabstracts.nlm.nih.gov/Implementation.shtmlsults show that using the argumentative labels as-signed by the learning algorithms improves theperformance in GeneRIF sentence selection.
Onthe other hand, models like CRF, which bettermodel the argumentative structure of the struc-tured abstracts, tend to perform below other learn-ing algorithms on the extrinsic evaluation.
Thisshows that non-structured abstracts do not have thesame layout compared to structured ones.2 Related workAs presented in the introduction, one of the ob-jectives of our work is to assign structured ab-stract labels to abstracts without these labels.
Theidea is to help in the curation process of exist-ing databases and to improve the efficiency ofinformation access.
Previous work on MED-LINE/PubMed abstracts has focused on learningto identify these labels mainly in the RandomizedControl Trials (RCT) domain.
(McKnight andSrinivasan, 2003) used a Support Vector Machine(SVM) and a linear classifier and tried to pre-dict the labels of MEDLINE structured abstracts.Their work finds that it is possible to learn a modelto label the abstract with modest results.
Furtherstudies have been conducted by (Ruch et al 2003;Tbahriti et al 2005; Ruch et al 2007) to usethe argumentative model of the abstracts.
Theyhave used this to improve retrieval and indexing ofMEDLINE citations, respectively.
In their work,they have used a multi-class Na?
?ve Bayes classi-fier.
(Hirohata et al 2008) have shown that the la-bels in structured abstracts follow a certain argu-mentative structure.
Using the current set of labelsused at the NLM, a typical argumentative struc-ture consists of OBJECTIVE, METHODS, RE-SULTS and CONCLUSION.
This notion is some-what already explored by (McKnight and Srini-vasan, 2003) by using the position of the sentence.More advanced approaches have been used thattrain a model that considers the sequence of labelsin the structured abstracts.
(Lin et al 2006) used agenerative model, comparing them to discrimina-tive ones.
More recent work has been dealing withConditional Random Fields (Hirohata et al 2008)with good performance.
(Agarwal and Yu, 2009) used similar ap-proaches and evaluated the labeling of full textarticles with the trained model on structured ab-stracts.
Their evaluation included as well a set of103<Abstract><AbstractText Label=?PURPOSE?
NlmCategory=?OBJECTIVE?>To explore the effects of cervical loopelectrosurgical excision procedure (LEEP) or cold knife conization (CKC) on pregnancy outcomes.</AbstractText><AbstractText Label=?MATERIALS AND METHODS?
NlmCategory=?METHODS?>Patients with cervical intraep-ithelial neoplasia (CIN) who wanted to become pregnant and received LEEP or CKC were considered as the treat-ment groups.
Women who wanted to become pregnant and only underwent colposcopic biopsy without any treat-ments were considered as the control group.
The pregnancy outcomes were observed and compared in the threegroups.</AbstractText><AbstractText Label=?RESULTS?
NlmCategory=?RESULTS?>Premature delivery rate was higher (p = 0.048) in theCKC group (14/36, 38.88%) than in control group (14/68, 20.5%) with a odds ratio (OR) of 2.455 (1.007 - 5.985);and premature delivery was related to cone depth, OR was significantly increased when the cone depth was more than15 mm.
There was no significant difference in premature delivery between LEEP (10 / 48, 20.83%) and the controlgroups.
The average gestational weeks were shorter (p = 0.049) in the CKC group (36.9 +/- 2.4) than in the controlgroup (37.8 +/- 2.6), but similar in LEEP (38.1 +/- 2.4) and control groups.
There were no significant differencesin cesarean sections between the three groups.
The ratio of neonatal birth weight less than 2,500 g was significantlyhigher (p = 0.005) in the CKC group (15/36) than in the control group (10/68), but similar in the LEEP and controlgroups.</AbstractText><AbstractText Label=?CONCLUSION?
NlmCategory=?CONCLUSIONS?>Compared with CKC, LEEP is relativelysafe.
LEEP should be a priority in the treatment of patients with CIN who want to become pregnant.</AbstractText></Abstract>Table 1: XML example for PMID 23590007abstracts manually annotated.
They found that theperformance on full-text was below what was ex-pected.
A similar result was found in the manu-ally annotated set.
They found, as well, that theabstract sentences are noisy and sometimes thesentences from structured abstracts did not belongwith the label they were assigned to.A large number of abstracts in MEDLINE arenot structured; thus intrinsic evaluation of the al-gorithms trained to predict the argumentative la-bels on structured abstracts is not completely real-istic.
Extrinsic evaluation has been previously per-formed by (Ruch et al 2003; Tbahriti et al 2005;Ruch et al 2007) in information retrieval resultsevaluating a Na?
?ve Bayes classifier.
We have ex-tended this work by evaluating a larger set of al-gorithms and heuristics on a data set developedto tune and evaluate a system for GeneRIF index-ing on a data set containing mostly non-structuredabstracts.
The idea is that GeneRIF relevant sen-tences will be assigned distinctive argumentativelabels.A Gene Reference Into Function (GeneRIF) de-scribes novel functionality of genes.
The cre-ation of GeneRIF entries involves the identifica-tion of the genes mentioned in MEDLINE cita-tions and the citation sentences describing a novelfunction.
GeneRIFs are available from the NCBI(National Center for Biotechnology Information)Gene database5.
An example sentence is shownbelow linked to the BRCA1 gene with gene id672 from the citation with PubMed R?
identifier(PMID) 22093627:5http://www.ncbi.nlm.nih.gov/sites/entrez?db=geneFISH-positive EGFR expression is associatedwith gender and smoking status, but notcorrelated with the expression of ERCC1 andBRCA1 proteins in non-small cell lung cancer.There is limited previous work related toGeneRIF span extraction.
Most of the availablepublications are related to the TREC GenomicsTrack in 2003 (Hersh and Bhupatiraju, 2003).There were two main tasks in this track, the firstone consisted of identifying relevant citations tobe considered for GeneRIF annotation.In the second task, the participants had to pro-vide spans of text that would correspond to rel-evant GeneRIF annotations for a set of citations.Considering this second task, the participants werenot provided with a training data set.
The Dicecoefficient was used to measure the similarity be-tween the submitted span of text from the title andabstract of the citation and the official GeneRIFtext in the test set.Surprisingly, one of the main conclusions wasthat a very competitive system could be obtainedby simply delivering the title of the citation as thebest GeneRIF span of text.
Few teams (EMC (Je-lier et al 2003) and Berkley (Bhalotia et al 2003)being exceptions), achieved results better than thatsimple strategy.
Another conclusion of the Ge-nomics Track was that the sentence position in thecitation is a good indicator for GeneRIF sentenceidentification: either the title or sentences close tothe end of the citation were found to be the bestcandidates.Subsequent to the 2003 Genomics Track, therehas been some further work related to GeneRIF104sentence selection.
(Lu et al 2006; Lu et al2007) sought to reproduce the results alreadyavailable from Entrez Gene (former name for theNCBI Gene database).
In their approach, a setof features is identified from the sentences andused in the algorithm: Gene Ontology (GO) to-ken matches, cue words and sentence position inthe abstract.
(Gobeill et al 2008) combined argu-mentative features using discourse-analysis mod-els (LASt) and an automatic text categorizer toestimate the density of Gene Ontology categories(GOEx).
The combination of these two featuresets produced results comparable to the best 2003Genomics Track system.3 MethodsAs in previous work, we approach the problemof learning to label sentences in abstracts us-ing machine learning methods on structured ab-stracts.
We have compared a large range of ma-chine learning algorithms, including ConditionalRandom Field.
The evaluation is performed in-trinsically on a held out set of structured abstractsand then evaluated extrinsically on a dataset devel-oped for the evaluation of algorithms for GeneRIFindexing.3.1 Structured abstracts data setThis data set is used to train the machine learningalgorithms and to peform the intrinsic evaluationof structured abstracts.
The abstracts have beencollected from PubMed using the query hasstruc-turedabstract, selecting the top 100k citations sat-ifying the query.The abstract defined within the Abstract at-tribute is split into several AbstractText tags.
EachAbstractText tag has the label Label that showsthe original label as provided by the journal whilethe NlmCategory represents the category as addedby the NLM.From this set, 2/3 of the citations (66,666) areconsidered for training the machine learning algo-rithms while 1/3 of the citations (33,334) are re-served for testing.
The abstract paragraphs havebeen split into sentences and the structured ab-stract label has been transferred to them.
For in-stance, all the sentences in the INTRODUCTIONsection are labeled as INTRODUCTION.An analysis of the abstracts has shown that thereare cases in which the article keywords were in-cluded as part of the abstract in a BACKGROUNDsection.
These were easily recognized by the orig-inal label KEYWORD.
We have removed theseparagraphs since they are not typical sentencesin MEDLINE but a list of keywords.
We findthat there are sections like OBJECTIVE where thenumber of sentences is very low, with less than 2sentences on average, while RESULTS is the sec-tion with the largest number of sentences on aver-age with over 4.5 sentences.There are five candidate labels identified fromthe structured abstracts, presented in Table 2.
Thedistribution of labels shows that some labels likeCONCLUSIONS, METHODS and RESULTS arevery frequent.
CONCLUSIONS and METHODSare assigned to more than one paragraph since thenumber is bigger compared to the number of cita-tions in each set.
This seems to happen when morethan one journal label in the same citation mapto METHODS or CONCLUSION, e.g.
PMID:23538919.Label Paragraphs SentencesBACKGROUND 53,348 132,890CONCLUSIONS 101,830 205,394METHODS 107,227 304,487OBJECTIVE 60,846 95,547RESULTS 95,824 436,653Table 2: Structured abstracts data set statisticsWe have compared the performance of sev-eral learning algorithms.
Among other classi-fiers, we use Na?
?ve Bayes and Linear Regression,which might be seen as a generative learner ver-sus discriminative (Jordan, 2002) learner.
We haveused the implementation available from the Malletpackage (McCallum, 2002).In addition to these two classifiers, we haveused AdaBoostM1 and SVM.
SVM has beentrained using stochastic gradient descent (Zhang,2004), which is very efficient for linear ker-nels.
Table 2 shows a large imbalance betweenthe labels, so we have used the modified HuberLoss (Zhang, 2004), which has already been usedin the context of MeSH indexing (Yeganova et al2011).
Both algorithms were trained based on theone-versus-all approach.
We have turned the algo-rithms into multi-class classifiers by selecting theprediction with the highest confidence by the clas-sifiers (Tsoumakas and Katakis, 2007).
We haveused the implementation of these algorithms avail-105able from the MTI ML package6, previously usedin the task of MeSH indexing (Jimeno-Yepes et al2012).The learning algorithms have been trained onthe text of the paragraph or sentences from thedata set presented above.
The text is lowercasedand tokenized.
In addition to the textual features,the position of the sentence or paragraph from thebeginning of the abstract is used as well.As we have seen, argumentative structure of theabstract labels has been previously modeled usinga linear chain CRF (Lafferty et al 2001).
CRFis trained using the text features from sentences orparagraphs in conjunction of the abstract labels toperform the label assignment.
In our experiments,we have used the implementation available fromthe Mallet package, using only an order 1 model.3.2 GeneRIF data setWe have developed a data set to compare andevaluate GeneRIF indexing approaches (Jimeno-Yepes et al 2013) as part of the Gene IndexingAssistant project at the NLM7.
The current scopeof our work is limited to the human species.
Thedevelopment is performed in two steps describedbelow.
The first step consists of selecting cita-tions from journals typically associated with hu-man species.
During the second step, we applyIndex Section rules for citation filtering plus ad-ditional rules to further focus the set of selectedcitations.
Since there was no GeneRIF indexingbefore 2002, only articles from 2002 through 2011from the 2011 MEDLINE Baseline 8 (11/19/2010)were used to build the data set.A subset of the filtered citations was collectedfor annotation.
The annotations were performedby two annotators.
Guidelines were prepared andtested on a small set by the two annotators and re-fined before annotating the entire set.The data set has been annotated with GeneRIFcategories of the sentences.
The categories are:Expression, Function, Isolation, Non-GeneRIF,Other, Reference, and Structure.
We assigned theGeneRIF category to all the categories that didnot belong to Non-GeneRIF.
The indexing task isthen to categorize the sentences into GeneRIF sen-tences and Non-GeneRIF ones.
Based on their an-notation work on the data set, the F-measure for6http://ii.nlm.nih.gov/MTI ML/index.shtml7http://www.lhncbc.nlm.nih.gov/project/automated-indexing-research8http://mbr.nlm.nih.govthe annotators is 0.81.
We have used this annota-tion for the extrinsic evaluation of GeneRIF index-ing.This data set has been further split into trainingand testing subsets.
Table 3 shows the distributionbetween GeneRIF and Non-GeneRIF sentences.Set Total GeneRIF Non-GeneRIFTraining 1987 829 (42%) 1158 (58%)Testing 999 433 (43%) 566 (57%)Table 3: GeneRIF sentence distributionIn previous work, the indexing of GeneRIF sen-tences, on our data set, was performed based ona trained classifier on a set of features that per-formed well on the GeneRIF testing set (Jimeno-Yepes et al 2013).
Na?
?ve Bayes was the learningalgorithm that performed the best compared to theother methods and has been selected in this workas the method to be used to combine the featuresof the argumentative labeling algorithms.The set of features in the baseline experimentsinclude the position of the sentence from the be-ginning of the abstract, the position of the sentencecounting from the end of the abstract, the sen-tence text, the annotation of disease terms, basedon MetaMap (Aronson and Lang, 2010), and geneterms, based on a dictionary approach, and theGene Ontology term density (Gobeill et al 2008).4 ResultsAs mentioned before, we have performed the eval-uation of the algorithms intrinsically, given a setof structured abstracts, and extrinsically based ontheir performance on GeneRIF sentence indexing.4.1 Intrinsic evaluation (structuredabstracts)Tables 4 and 5 show the results of the intrinsicevaluation for paragraph and sentence experimentsrespectively.
The algorithms are trained to labelthe paragraphs or sentences from the structuredabstracts.
The precision (P), recall (R) and F1(F) values are presented for each argumentative la-bel.
The methods evaluated include Na?
?ve Bayes(NB), Logistic Regression (LR), SVM based onmodified Huber Loss (Huber) and AdaBoostM1(ADA).
These methods have been trained on thetext of either the sentence or the paragraph, andmight include their position feature, indicated withthe letter P (e.g.
NB P for Na?
?ve Bayes trained106Label NB NB P LR LR P ADA ADA P Huber HuberP CRFBACKGROUND P 0.6047 0.6853 0.6374 0.7369 0.6098 0.7308 0.5862 0.7166 0.7357R 0.5672 0.7190 0.5868 0.7207 0.3676 0.7337 0.4984 0.6694 0.7093F 0.5854 0.7017 0.6110 0.7287 0.4587 0.7323 0.5387 0.6922 0.7223CONCLUSIONS P 0.7532 0.8626 0.8365 0.9413 0.6975 0.8862 0.7578 0.9051 0.9769R 0.8606 0.9366 0.8675 0.9552 0.8246 0.9404 0.7987 0.9340 0.9784F 0.8033 0.8981 0.8517 0.9482 0.7557 0.9125 0.7777 0.9193 0.9776METHODS P 0.9002 0.9278 0.9113 0.9396 0.8256 0.9041 0.8668 0.9116 0.9684R 0.9040 0.9126 0.9294 0.9493 0.8955 0.9250 0.9012 0.9237 0.9675F 0.9021 0.9201 0.9203 0.9444 0.8591 0.9144 0.8837 0.9176 0.9680OBJECTIVE P 0.7294 0.7650 0.7167 0.7531 0.6763 0.7565 0.6788 0.7160 0.7608R 0.6453 0.7190 0.7255 0.7549 0.6937 0.7228 0.6733 0.7365 0.7759F 0.6848 0.7413 0.7210 0.7540 0.6849 0.7393 0.6761 0.7261 0.7683RESULTS P 0.8841 0.9106 0.9086 0.9372 0.8554 0.9157 0.8560 0.9122 0.9692R 0.8414 0.8542 0.8857 0.9216 0.7842 0.8564 0.8447 0.8846 0.9758F 0.8622 0.8815 0.8970 0.9294 0.8182 0.8851 0.8503 0.8981 0.9725Average P 0.7743 0.8303 0.8021 0.8616 0.7329 0.8387 0.7491 0.8323 0.8822R 0.7637 0.8283 0.7990 0.8604 0.7131 0.8357 0.7433 0.8296 0.8814F 0.7690 0.8293 0.8005 0.8610 0.7229 0.8372 0.7462 0.8310 0.8818Table 4: Intrinsic evaluation of paragraph based labelingLabel NB NB P LR LR P ADA ADA P Huber HuberP CRFBACKGROUND P 0.4983 0.6313 0.5558 0.6862 0.4779 0.6417 0.5153 0.6495 0.6738R 0.4980 0.6921 0.5084 0.7139 0.3207 0.6993 0.3372 0.6554 0.7104F 0.4981 0.6603 0.5311 0.6998 0.3838 0.6693 0.4076 0.6524 0.6916CONCLUSIONS P 0.5876 0.7270 0.6794 0.8431 0.5672 0.7651 0.6153 0.7767 0.8977R 0.7103 0.8388 0.6788 0.8187 0.4998 0.6816 0.5163 0.7213 0.8671F 0.6431 0.7789 0.6791 0.8307 0.5314 0.7209 0.5615 0.7480 0.8821METHODS P 0.7857 0.8206 0.8193 0.8549 0.7224 0.7793 0.7343 0.7894 0.8931R 0.8084 0.8366 0.8427 0.8696 0.7789 0.8152 0.7828 0.8250 0.8988F 0.7969 0.8285 0.8308 0.8622 0.7496 0.7968 0.7578 0.8068 0.8960OBJECTIVE P 0.5522 0.6237 0.6032 0.6696 0.5497 0.6671 0.5525 0.6259 0.6258R 0.4894 0.5530 0.4995 0.5534 0.4082 0.4518 0.4479 0.5036 0.5779F 0.5189 0.5862 0.5465 0.6060 0.4685 0.5388 0.4947 0.5581 0.6009RESULTS P 0.8294 0.8517 0.8071 0.8449 0.6903 0.7665 0.6957 0.7877 0.8892R 0.7517 0.7743 0.8429 0.8679 0.7998 0.8143 0.6957 0.8208 0.8995F 0.7886 0.8112 0.8246 0.8563 0.7410 0.7897 0.6957 0.8039 0.8943Average P 0.6506 0.7309 0.6930 0.7797 0.6015 0.7239 0.6226 0.7258 0.7959R 0.6516 0.7390 0.6745 0.7647 0.5615 0.6924 0.5560 0.7052 0.7907F 0.6511 0.7349 0.6836 0.7721 0.5808 0.7078 0.5874 0.7154 0.7933Table 5: Intrinsic evaluation of sentence based labelingwith the features from text and the position).
Theresults include those based on CRF trained on thetext of either the sentence or the paragraph takinginto account the labeling sequence.CRF has the best performance in both tables,with the differences being more dramatic on theparagraph results.
These results are comparableto (Hirohata et al 2008), even though we areworking with a different set of labels.
Compar-ing the remaining learning algorithms, LR per-forms better than the other classifiers.
Both Ad-aBoostM1 and SVM perform not as well as NBand LR; this could be due to the noise referredto by (Agarwal and Yu, 2009) that appears in thestructured abstract sentences.
Considering eitherthe paragraph or the sentence text, the position in-formation helps improve their performance.CONCLUSIONS, METHODS and RESULTSlabels have the best performance, which matchesthe most frequent labels in the dataset (see Ta-ble 2).
BACKGROUND and OBJECTIVE haveworse performance compared to the other labels.These two labels have the largest imbalance com-pared to the other labels, which seems to nega-tively impact the classifiers performance.The results based on the paragraphs outperformthe ones based on the sentences.
Argumentativestructure of the paragraphs seems to be easier,probably due to the fact that individual sentenceshave been shown to be noisy (Agarwal and Yu,2009), and this could explain this behaviour.1074.2 Extrinsic evaluation (GeneRIFs)Extrinsic evaluation is performed on the GeneRIFdata set presented in the Methods section.
Theidea of the evaluation is to assign one of the ar-gumentative labels to the sentences, based on themodels trained on structured abstracts, and eval-uate the impact of this assignment in the selec-tion of GeneRIF sentences.
From the set of ma-chine learning algorithms intrinsically evaluated,we have selected the LR models trained with andwithout position information (Pos) and the CRFmodel.
The LR and CRF models are used to la-bel the GeneRIF training and testing data with theargumentative labels.Table 6 shows the results of the extrinsic evalu-ation.
Results obtained with the argumentative la-bel feature and with or without the set of featuresused in the baseline are compared to the baselinemodel, i.e.
NB and the set of features presentedin the Methods section.
In all the cases, precision(P), recall (R) and F1 using the argumentative fea-tures improve over the baseline.The intrinsic evaluation was performed eitheron sentences or paragraphs.
The sentence mod-els perform better than the paragraph based mod-els.
We find as well that LR with sentence positionperforms slightly better than when combined withthe baseline features, with higher recall but lowerprecision.
Contrary to the intrinsic results, LR per-forms better than CRF, even though both outper-form the baseline.
This means that non-structuredsentences do not necessarily follow the same argu-mentative structure as the structured abstracts.Label P R FBaseline 0.6210 0.6605 0.6405LR Par 0.7235 0.6767 0.6993LR Par + Base 0.7184 0.8014 0.7576LR Par Pos 0.5978 0.8891 0.7149LR Par Pos + Base 0.6883 0.8060 0.7426LR Sen 0.7039 0.7852 0.7424LR Sen + Base 0.7325 0.7968 0.7633LR Sen Pos 0.7014 0.9007 0.7887LR Sen Pos + Base 0.7222 0.8406 0.7769CRF Par 0.6682 0.6744 0.6713CRF Par + Base 0.7036 0.8060 0.7513CRF Sen 0.6536 0.8499 0.7390CRF Sen + Base 0.7134 0.7875 0.7486Table 6: GeneRIF extrinsic evaluation5 DiscussionResults show that it is possible to automaticallypredict the argumentative label of the structuredabstracts and to improve the performance forGeneRIF annotation.
Intrinsic evaluation showsthat paragraph labeling is easier compared to sen-tence labeling, which might be partly due to thenoise in the sentences as identified by (Agarwaland Yu, 2009).
The excellent performance forparagraph labeling was already shown by previouswork (Hirohata et al 2008) while sentence label-ing issues for structured abstracts was previouslyintroduced by (Agarwal and Yu, 2009).
In both in-trinsic tasks, adding the position of the paragraphor sentence improves the performance of the learn-ing algorithms.Extrinsic evaluation shows that, compared tothe baseline features for GeneRIF annotation,adding argumentative labeling using the trainedmodels improves its performance, which is closeto the human performance reported in the Meth-ods section.
On the other hand, we find that theCRF models show lower performance comparedto the LR models.
From the LR models, the po-sition of the sentence or paragraph seems to havebetter performance.In addition, the LR model trained on the sen-tences performs better compared to the modeltrained on the paragraphs.
This might be partlydue to the fact that sentence based models seemto be better suited than the paragraph based onesas might have been expected.
The fact that theCRF models performance is below the LR mod-els denotes that the structured abstracts seem tofollow a pattern that is different in the case ofnon-structured abstracts.
Looking closer at theassigned labels, the LR models tend to assignmore CONCLUSIONS and RESULTS labels tothe GeneRIF sentences compared to the CRF ones.6 Conclusions and Future WorkWe have presented an evaluation of several learn-ing algorithms to label abstract text in MED-LINE/PubMed with argumentative labels, basedon MEDLINE/PubMed structured abstracts.
Theresults show that this task can be achieved withhigh performance in the case of labeling the para-graphs but this is not the same in the case of sen-tences.
This intrinsic evaluation was performed onstructured abstracts, and in this set the CRF mod-els seem to perform much better compared to the108other models that do not use the labeling sequence.On the other hand, when applying the trainedmodels to MEDLINE/PubMed non-structured ab-stracts, we find that the extrinsic evaluation ofthese labeling on the GeneRIF task shows lowerperformance for the CRF models.
This indicatesthat the structured abstracts follow a pattern thatnon-structured ones do not follow.
The extrin-sic evaluation shows that labeling the sentenceswith argumentative labels improves the indexingof GeneRIF sentences.
The argumentative labelshelp identifying target sentences for the GeneRIFindexing, but more refined labels learned fromnon-structured abstracts could provide better per-formance.
An idea to extend this research wouldbe evaluating the latent discovery of section labelsand to apply this labeling to the proposed GeneRIFtask and to other tasks, e.g.
MeSH indexing.
La-tent labels might accommodate better the argu-mentative structure of non-structured abstracts.As shown in this work, the argumentative lay-out of non-structured abstracts and structured ab-stracts is not the same.
There is still the open ques-tion if there is any layout regularity in the non-structured abstracts that could be exploited to im-prove information access.7 AcknowledgementsNICTA is funded by the Australian Governmentas represented by the Department of Broadband,Communications and the Digital Economy andthe Australian Research Council through the ICTCentre of Excellence program.This work was also supported in part by the In-tramural Research Program of the NIH, NationalLibrary of Medicine.ReferencesS Agarwal and H Yu.
2009.
Automatically classifyingsentences in full-text biomedical articles into Intro-duction, Methods, Results and Discussion.
Bioin-formatics, 25(23):3174?3180.A R Aronson and F M Lang.
2010.
An overviewof MetaMap: historical perspective and recent ad-vances.
Journal of the American Medical Informat-ics Association, 17(3):229?236.G.
Bhalotia, PI Nakov, A S Schwartz, and M A Hearst.2003.
BioText team report for the TREC 2003 ge-nomics track.
In Proceedings of TREC.
Citeseer.J Gobeill, I Tbahriti, F Ehrler, A Mottaz, A Veuthey,and P Ruch.
2008.
Gene Ontology density estima-tion and discourse analysis for automatic GeneRiFextraction.
BMC Bioinformatics, 9(Suppl 3):S9.W Hersh and R T Bhupatiraju.
2003.
TREC genomicstrack overview.
In TREC 2003, pages 14?23.K Hirohata, Naoaki Okazaki, Sophia Ananiadou, Mit-suru Ishizuka, and Manchester InterdisciplinaryBiocentre.
2008.
Identifying sections in scientificabstracts using conditional random fields.
In Proc.of 3rd International Joint Conference on NaturalLanguage Processing, pages 381?388.R Jelier, M Schuemie, C Eijk, M Weeber, E Mulligen,B Schijvenaars, B Mons, and J Kors.
2003.
Search-ing for GeneRIFs: concept-based query expansionand Bayes classification.
In Proceedings of TREC,pages 167?174.A Jimeno-Yepes, J G Mork, D Demner-Fushman, andA R Aronson.
2012.
A One-Size-Fits-All IndexingMethod Does Not Exist: Automatic Selection Basedon Meta-Learning.
Journal of Computing Scienceand Engineering, 6(2):151?160.A Jimeno-Yepes, J C Sticco, J G Mork, and A R Aron-son.
2013.
GeneRIF indexing: sentence selectionbased on machine learning.
BMC Bioinformatics,14(1):147.S Jonnalagadda, G D Fiol, R Medlin, C Weir, M Fisz-man, J Mostafa, and H Liu.
2012.
Automaticallyextracting sentences from medline citations to sup-port clinicians?
information needs.
In HealthcareInformatics, Imaging and Systems Biology (HISB),2012 IEEE Second International Conference on,pages 72?72.
IEEE.A Jordan.
2002.
On discriminative vs. generativeclassifiers: A comparison of logistic regression andnaive bayes.
Advances in neural information pro-cessing systems, 14:841.J D Lafferty, A McCallum, and F Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceed-ings of the Eighteenth International Conference onMachine Learning, ICML ?01, pages 282?289, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.J Lin, D Karakos, D Demner-Fushman, and S Khudan-pur.
2006.
Generative content models for structuralanalysis of medical abstracts.
In Proceedings of theHLT-NAACL BioNLP Workshop on Linking NaturalLanguage and Biology, pages 65?72.
Associationfor Computational Linguistics.Z Lu, K B Cohen, and L Hunter.
2006.
FindingGeneRIFs via gene ontology annotations.
In PacificSymposium on Biocomputing.
Pacific Symposium onBiocomputing, page 52.
NIH Public Access.Z Lu, K B Cohen, and L Hunter.
2007.
GeneRIF qual-ity assurance as summary revision.
In Pacific Sym-posium on Biocomputing, page 269.
NIH Public Ac-cess.109A McCallum.
2002.
Mallet: A machine learning forlanguage toolkit.
URL http://mallet.cs.umass.edu.L McKnight and P Srinivasan.
2003.
Categorizationof sentence types in medical abstracts.
In AMIA An-nual Symposium Proceedings, volume 2003, page440.
American Medical Informatics Association.Y Mizuta, A Korhonen, T Mullen, and N Collier.
2006.Zone analysis in biology articles as a basis for infor-mation extraction.
International journal of medicalinformatics, 75(6):468?487.A M Ripple, J G Mork, L S Knecht, and B LHumphreys.
2011.
A retrospective cohort studyof structured abstracts in MEDLINE, 1992?2006.Journal of the Medical Library Association: JMLA,99(2):160.A M Ripple, J G Mork, J M Rozier, and L S Knecht.2012.
Structured Abstracts in MEDLINE: Twenty-Five Years Later.P Ruch, C Chichester, G Cohen, G Coray, F Ehrler,H Ghorbel, and V Mu?ller, Hand Pallotta.
2003.
Re-port on the TREC 2003 experiment: Genomic track.TREC-03.P Ruch, A Geissbuhler, J Gobeill, F Lisacek, I Tbahriti,A Veuthey, and A R Aronson.
2007.
Using dis-course analysis to improve text categorization inMEDLINE.
Studies in health technology and infor-matics, 129(1):710.I Tbahriti, C Chichester, F Lisacek, and P Ruch.
2005.Using argumentation to retrieve articles with similarcitations: An inquiry into improving related articlessearch in the MEDLINE digital library.
In Interna-tional Journal of Medical Informatics.
Citeseer.G Tsoumakas and I Katakis.
2007.
Multi-label clas-sification: An overview.
International Journal ofData Warehousing and Mining (IJDWM), 3(3):1?13.L Yeganova, Donald C Comeau, W Kim, and J Wilbur.2011.
Text mining techniques for leveraging posi-tively labeled data.
In Proceedings of BioNLP 2011Workshop, pages 155?163.
Association for Compu-tational Linguistics.T Zhang.
2004.
Solving large scale linear predic-tion problems using stochastic gradient descent al-gorithms.
In Proceedings of the twenty-first inter-national conference on Machine learning, page 116.ACM.110
