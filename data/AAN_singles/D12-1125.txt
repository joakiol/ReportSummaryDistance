Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1368?1378, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsLearning to Map into a Universal POS TagsetYuan Zhang, Roi Reichart, Regina BarzilayMassachusetts Institute of Technology{yuanzh, roiri, regina}@csail.mit.eduAmir GlobersonThe Hebrew Universitygamir@cs.huji.ac.ilAbstractWe present an automatic method for mappinglanguage-specific part-of-speech tags to a setof universal tags.
This unified representationplays a crucial role in cross-lingual syntactictransfer of multilingual dependency parsers.Until now, however, such conversion schemeshave been created manually.
Our central hy-pothesis is that a valid mapping yields POSannotations with coherent linguistic proper-ties which are consistent across source andtarget languages.
We encode this intuitionin an objective function that captures a rangeof distributional and typological characteris-tics of the derived mapping.
Given the ex-ponential size of the mapping space, we pro-pose a novel method for optimizing over softmappings, and use entropy regularization todrive those towards hard mappings.
Our re-sults demonstrate that automatically inducedmappings rival the quality of their manuallydesigned counterparts when evaluated in thecontext of multilingual parsing.11 IntroductionIn this paper, we explore an automatic method formapping language-specific part-of-speech tags to auniversal tagset.
In multilingual parsing, this uni-fied input representation is required for cross-lingualsyntactic transfer.
Specifically, the universal tagsetannotations enable an unlexicalized parser to capi-talize on annotations from one language when learn-ing a model for another.1The source code and data for the work presented in thispaper is available at http://groups.csail.mit.edu/rbg/code/unitag/emnlp2012While the notion of a universal POS tagset iswidely accepted, in practice it is hardly ever usedfor annotation of monolingual resources.
In fact,available POS annotations are designed to capturelanguage-specific idiosyncrasies and therefore aresubstantially more detailed than a coarse universaltagset.
To reconcile these cross-lingual annotationdifferences, a number of mapping schemes havebeen proposed in the parsing community (Zemanand Resnik, 2008; Petrov et al 2011; Naseem etal., 2010).
In all of these cases, the conversion isperformed manually and has to be repeated for eachlanguage and annotation scheme anew.Despite the apparent simplicity, deriving a map-ping is by no means easy, even for humans.
In fact,the universal tagsets manually induced by Petrovet al(2011) and by Naseem et al(2010) disagreeon 10% of the tags.
An example of such discrep-ancy is the mapping of the Japanese tag ?PVfin?
tothe universal tag ?particle?
according to one scheme,and to ?verb?
according to another.
Moreover, thequality of this conversion has a direct implication onthe parsing performance.
In the Japanese exampleabove, this difference in mapping yields a 6.7% dif-ference in parsing accuracy.The goal of our work is to induce the mappingfor a new language, utilizing existing manually-constructed mappings as training data.
The exist-ing mappings developed in the parsing communityrely on gold POS tags for the target language.
Amore realistic scenario is to employ the mappingtechnique to resource-poor languages where goldPOS annotations are lacking.
In such cases, a map-ping algorithm has to operate over automatically in-1368duced clusters on the target language (e.g., usingthe Brown algorithm) and convert them to universaltags.
We are interested in a mapping approach thatcan effectively handle both gold tags and inducedclusters.Our central hypothesis is that a valid mappingyields POS annotations with coherent linguisticproperties which are consistent across languages.Since universal tags play the same linguistic rolein source and target languages, we expect similar-ity in their global distributional statistics.
Figure 1ashows statistics for two close languages, English andGerman.
We can see that their unigram frequencieson the five most common tags are very close.
Otherproperties concern POS tag per sentence statistics ?e.g., every sentence has to have at least one verb.
Fi-nally, the mappings can be further constrained by ty-pological properties of the target language that spec-ify likely tag sequences.
This information is readilyavailable even for resource poor language (Haspel-math et al 2005).
For instance, since English andGerman are prepositional languages, we expect toobserve adposition-noun sequences but not the re-verse (see Figure 1b for sample sentences).
We en-code these heterogeneous properties into an objec-tive function that guides the search for the optimalmapping.Having defined a quality measure for mappings,our goal is to find the optimal mapping.
However,such partition optimization problems2 are NP hard(Garey and Johnson, 1979).
A naive approach tothe problem is to greedily improve the map, but itturns out that this approach yields poor quality map-pings.
We therefore develop a method for optimiz-ing over soft mappings, and use entropy regulariza-tion to drive those towards hard mappings.
We con-struct the objective in a way that facilitates simplemonotonically improving updates corresponding tosolving convex optimization problems.We evaluate our mapping approach on 19languages that include representatives of Indo-European, Semitic, Basque, Japonic and Turkic fam-ilies.
We measure mapping quality based on thetarget language parsing accuracy.
In addition toconsidering gold POS tags for the target language,2Instances of related hard problems are 3-partition andsubset-sum.we also evaluate the mapping algorithm on auto-matically induced POS tags.
In all evaluation sce-narios, our model consistently rivals the qualityof manually induced mappings.
We also demon-strate that the proposed inference procedure outper-forms greedy methods by a large margin, highlight-ing the importance of good optimization techniques.We further show that while all characteristics ofthe mapping contribute to the objective, our largestgain comes from distributional features that captureglobal statistics.
Finally, we establish that the map-ping quality has a significant impact on the accuracyof syntactic transfer, which motivates further studyof this topic.2 Related WorkMultilingual Parsing Early approaches for multi-lingual parsing used parallel data to bridge the gapbetween languages when modeling syntactic trans-fer.
In this setup, finding the mapping between var-ious POS annotation schemes was not essential; in-stead, the transfer algorithm could induce it directlyfrom the parallel data (Hwa et al 2005; Xi andHwa, 2005; Burkett and Klein, 2008).
However,more recent transfer approaches relinquish this datarequirement, learning to transfer from non-paralleldata (Zeman and Resnik, 2008; McDonald et al2011; Cohen et al 2011; Naseem et al 2010).These approaches assume access to a common inputrepresentation in the form of universal tags, whichenables the model to connect patterns observed inthe source language to their counterparts in the tar-get language.Despite ongoing efforts to standardize POS tagsacross languages (e.g., EAGLES initiative (Eynde,2004)), many corpora are still annotated withlanguage-specific tags.
In previous work, their map-ping to universal tags was performed manually.
Yet,even though some of these mappings have been de-veloped for the same CoNLL dataset (Buchholz andMarsi, 2006; Nivre et al 2007), they are not identi-cal and yield different parsing performance (Zemanand Resnik, 2008; Petrov et al 2011; Naseem et al2010).
The goal of our work is to automate this pro-cess and construct mappings that are optimized forperformance on downstream tasks (here we focus onparsing).
As our results show, we achieve this goal1369Noun Verb Det.
Prep.
Adj.00.050.10.150.20.250.30.35UnigramFrequencyEnglishGerman -Investors [are appealing] to the Securities and Exchange Commission not to [limit] their access to information [about stock purchases]and sales [by corporate insiders]-Einer der sich [f?r den Milliard?r] [ausspricht] [ist] Steve Jobs dem Perot [f?r den aufbau]der Computerfirma Next 20 Millionen Dollar [bereitstellte](a) (b)Figure 1: Illustration of similarities in POS tag statistics across languages.
(a) The unigram frequency statistics on fivetags for two close languages, English and German.
(b) Sample sentences in English and German.
Verbs are shown inblue, prepositions in red and noun phrases in green.
It can be seen that noun phrases follow prepositions.on a broad range of languages and evaluation sce-narios.Syntactic Category Refinement Our work alsorelates to work in syntactic category refinement inwhich POS categories and parse tree non-terminalsare refined in order to improve parsing perfor-mance (Finkel et al 2007; Klein and Manning,2003; Matsuzaki et al 2005; Petrov et al 2006;Petrov and Klein, 2007; Liang et al 2007).
Ourwork differs from these approaches in two ways.First, these methods have been developed in themonolingual setting, while our mapping algorithm isdesigned for multilingual parsing.
Second, these ap-proaches are trained on the syntactic trees of the tar-get language, which enables them to directly link thequality of newly induced categories with the qualityof syntactic parsing.
In contrast, we are not giventrees in the target language.
Instead, our model isinformed by mappings derived for other languages.3 Task FormulationThe input to our task consists of a target corpus writ-ten in a language T , and a set of non-parallel sourcecorpora written in languages {S1, .
.
.
, Sn}.
In thesource corpora, each word is annotated with botha language-specific POS tag and a universal POStag (Petrov et al 2011).
In the target corpus eachword is annotated only with a language-specific POStag, either gold or automatically induced.Our goal is to find a map from the set of LT targetlanguage tags to the set of K universal tags.
We as-sume that each language-specific tag is only mappedto one universal tag, which means we never split alanguage-specific tag and LT ?
K holds for everylanguage.
We represent the map by a matrix A ofsize K ?
LT where A(c|f) = 1 if the target lan-guage tag f is mapped to the universal tag c, andA(c|f) = 0 otherwise.3 Note that each column ofA should contain a single value of 1.
We will laterrelax the requirement thatA(c|f) ?
{0, 1}.
A candi-date mappingA can be applied to the target languageto produce sentences labeled with universal tags.4 ModelIn this section we describe an objective that reflectsthe quality of an automatic mapping.Our key insight is that for a good mapping, thestatistics over the universal tags should be similar forsource and target languages because these tags playthe same role cross-linguistically.
For example, weshould expect the frequency of a particular universaltag to be similar in the source and target languages.One choice to make when constructing an objec-tive is the source languages to which we want to besimilar.
It is clear that choosing all languages is not agood idea, since they are not all expected to have dis-tributional properties similar to the target language.There is strong evidence that projecting from sin-gle languages can lead to good parsing performance3We use c and f to reflect the fact that universal tags area coarse version (hence c) of the language specific fine tags(hence f ).1370(McDonald et al 2011).
Therefore, our strategy isto choose a single source language for comparison.The choice of the source language is based on sim-ilarity between typological properties; we describethis in detail in Section 5.We must also determine which statistical proper-ties we expect to be preserved across languages.
Ourmodel utilizes three linguistic phenomena which areconsistent across languages: POS tag global distri-butional statistics, POS tag per sentence statistics,and typology-based ordering statistics.
We defineeach of these below.4.1 Mapping CharacterizationWe focus on three categories of mapping properties.For each of the relevant statistics we define a func-tion Fi(A) that has low values if the source and tar-get statistics are similar.Global distributional statistics: The unigram andbigram statistics of the universal tags are expectedto be similar across languages with close typologicalprofiles.
We use pS(c1, c2) to denote the bigram dis-tribution over universal tags in the source language,and pT (f1, f2) to denote the bigram distribution overlanguage specific tags in the target language.
Thebigram distribution over universal tags in the targetlanguage depends on A and pT (f1, f2) and is givenby:pT (c1, c2;A) =?f1,f2A(c1|f1)A(c2|f2)pT (f1, f2)(1)To enforce similarity between source and target dis-tributions, we wish to minimize the KL divergencebetween the two: 4Fbi(A) = DKL[pS(c1, c2)|pT (c1, c2;A)] (2)We similarly define Funi(A) as the distance be-tween unigram distributions.Per sentence statistics: Another defining propertyof POS tags is their average count per sentence.Specifically, we focus on the verb count per sen-tence, which we expect be similar across languages.4We use the KL divergence because it assigns low weightsto infrequent universal tags.
Furthermore, this choice results ina simple, EM-like parameter estimation algorithm as discussedin Section 5.To express this constraint, we use nv(s,A) todenote the number of verbs (i.e., the universaltags corresponding to verbs according to A) insentence s. This is a linear function of A.
We alsouse E[nv(s,A)] to denote the average number ofverbs per sentence, and V [nv(s,A)] to denote thevariance.
We estimate these two statistics fromthe source language and denote them by ESv, VSv.Good mappings are expected to follow thesepatterns by having a variance upper bounded byVSv and an average lower bounded by ESv.5 Thiscorresponds to minimizing the following objectives:FEv(A) = max [0, ESv ?
E[nv(s,A)]]FV v(A) = max [0, V [nv(s,A)]?
VSv]Note that the above objectives are convex in A,which will make optimization simpler.
We refer tothe two terms jointly as Fverb(A).Typology-based ordering statistics: Typolog-ical features can be useful for determining therelative order of different tags.
If we know thatthe target language has a particular typologicalfeature, we expect its universal tags to obey thegiven relative ordering.
Specifically, we expect it toagree with ordering statistics for source languageswith a similar typology.
We consider two suchfeatures here.
First, in pre-position languages thepreposition is followed by the noun phrase.
Thus, ifT is such a language, we expect the probability ofa noun phrase following the adposition to be high,i.e., cross some threshold.
Formally, we define C1 ={noun, adj, num, pron, det} and consider the set ofbigram distributions Spre that satisfy the followingconstraint:?c?C1pT (adp,c) ?
apre (3)where apre =?c?C1 pS(adp,c) is calculated fromthe source language.
This constraint set is non-convex in A due to the bilinearity of the bi-gram term.
To simplify optimization6 we take an5The rationale is that we want to put a lower bound on thenumber of verbs per sentence, and induce it from the sourcelanguage.
Furthermore, we expect the number of verbs to bewell concentrated, and we induce its maximal variance fromthe source language.6In Section 5 we shall see that this makes optimization eas-ier.1371approach inspired by the posterior regularizationmethod (Ganchev et al 2010) and use the objective:Fc(A) = minr(c1,c2)?SpreDKL[r(c1, c2)|pT (c1, c2;A)](4)The above objective will attain lower values for Asuch that pT (c1, c2;A) is close to the constraint set.Specifically, it will have a value of zero when thebigram distribution induced by A has the propertyspecified in Spre.
We similarly define a set Spostfor post-positional languages.As a second typological feature, we consider theDemonstrative-Noun ordering.
In DN languages wewant the probability of a determiner to come be-fore C2 = {noun, adj, num}, (i.e., frequent universalnoun-phrase tags), to cross a threshold.
This con-straint translates to:?c?C2pT (det, c) ?
adet (5)where adet =?c?C2 pS(det, c) is a threshold de-termined from the source language.
We denote theset of distributions that have this property by SDN,and add them to the constraint in (4).
The overallconstraint set is denoted by S.4.2 The Overall ObjectiveWe have defined a set of functions Fi(A) that areexpected to have low values for good mappings.
Tocombine those, we use a weighted sum: F?
(A) =?i ?i ?
Fi(A).
(The weights in this equation arelearned; we discussed the procedure in Section 5)Optimizing over the set of mappings is difficultsince each mapping is a discrete set whose size isexponential size in LT .
Technically, the difficultycomes from the requirement that elements of A areintegral and its columns sum to one.
To relax thisrestriction, we will allow A(c|f) ?
[0, 1] and en-courage A to correspond to a mapping by adding anentropy regularization term:H[A] = ?
?f?cA(c|f) logA(c|f) (6)This term receives its minimal value when the con-ditional probability of the universal tags given alanguage-specific tag is 1 for one universal tag andzero for the others.The overall objective is then: F (A) = F?
(A) +?
?H[A], where ?
is the weight of the entropy term.7The resulting optimization problem is:minA?
?F (A) (7)where ?
is the set of non-negative matrices whosecolumns sum to one:?
={A :A(c|f) ?
0 ?c, f?Kc=1A(c|f) = 1 ?f}(8)5 Parameter EstimationIn this section we describe the parameter estimationprocess for our model.
We start by describing howto optimize A.
Next, we discuss the weight selec-tion algorithm, and finally the method for choosingsource languages.5.1 Optimizing the Mapping ARecall that our goal is to solve the optimizationproblem in Eq.
(7).
This objective is non convexsince the function H[A] is concave, and the objec-tive F (A) involves bilinear terms in A and loga-rithms of their sums (see Equations (1) and (2)).While we do not attempt to solve the problemglobally, we do have a simple update scheme thatmonotonically decreases the objective.
The updatecan be derived in a similar manner to expectationmaximization (EM) (Neal and Hinton, 1999) andconvex concave procedures (Yuille and Rangarajan,2003).
Figure 2 describes our optimization algo-rithm.
The key ideas in deriving it are using pos-terior distributions as in EM, and using a variationalformulation of entropy.
The term Fc(A) is handledin a similar way to the posterior regularization algo-rithm derivation.
A detailed derivation is providedin the supplementary file.
8The kth iteration of the algorithm involves severalsteps:?
In step 1, we calculate the current esti-mate of the bigram distribution over tags,pT (c1, c2;Ak).7Note that as ?
?
?, only valid maps will be selected bythe objective.8The supplementary file is available at http://groups.csail.mit.edu/rbg/code/unitag/emnlp2012.1372?
In step 2, we find the bigram distribution inthe constraint set S that is closest in KL di-vergence to pT (c1, c2;Ak), and denote it byrk(c1, c2).
This optimization problem is con-vex in r(c1, c2).?
In step 3, we calculate the bigram posteriorover language specific tags given a pair of uni-versal tags.
This is analogous to the standardE-step in EM.?
In step 4, we use the posterior in step 3 and thebigram distributions pS(c1, c2) and rk(c1, c2)to obtain joint counts over language specificand universal bigrams.?
In step 5, we use the joint counts from step 4to obtain counts over pairs of language specificand universal tags.?
In step 6, analogous to the M-step in EM, weoptimize over the mapping matrix A.
The ob-jective is similar to the Q function in EM, andalso includes the Fverb(A) term, and a linearupper bound on the entropy term.
The objec-tive can be seen to be convex in A.As mentioned above, each of the optimization prob-lems in steps 2 and 6 is convex, and can therefore besolved using standard convex optimization solvers.Here, we use the CVX package (Grant and Boyd,2008; Grant and Boyd, 2011).
It can be shown thatthe algorithm improves F (A) at every iteration andconverges to a local optimum.The above algorithm generates a mapping A thatmay contain fractional entries.
To turn it into a hardmapping we round A by mapping each f to the cthat maximizes A(c|f) and then perform greedy im-provement steps (one f at a time) to further improvethe objective.
The regularization constant ?
is tunedto minimize the F?
(A) value of the rounded A.5.2 Learning the Objective WeightsOur F?
(A) objective is a weighted sum of the in-dividual Fi(A) functions.
In the following, we de-scribe how to learn the ?i weights for every targetlanguage.
We would like F?
(A) to have low valueswhen A is a good map.
Since our performance goalis parsing accuracy, we consider a map to be goodInitialize A0.RepeatStep 1 (calculate current bigram estimate):pT (c1, c2;Ak) =?f1,f2Ak(c1|f1)Ak(c2|f2)pT (f1, f2)Step 2 (incorporate constraints):rk(c1, c2) = arg minr?SDKL[r(c1, c2)|pT (c1, c2;Ak)]Step 3 (calculate model posterior):p(f1, f2|c1, c2;Ak) ?
Ak(c1|f1)Ak(c2|f2)pT (f1, f2)Step 4: (complete joint counts):Nk(c1, c2, f1, f2) = p(f1, f2|c1, c2;Ak)(rk(c1, c2) + pS(c1, c2))Step 5 (obtain pairwise):Mk(c, f) = Nk1 (c, f) +Nk2 (c, f)where Nk1 (c, f) =?c2,f2Nk(c, c2, f, f2) and similarly forNk2 (c, f).Step 6 (M step with entropy linearization): Set Ak+1 to be thesolution ofminA???
?c,f[Mk(c, f) logA(c|f) + A(c|f) logAk(c|f)]+ Fverb(A)Until Convergence of AkFigure 2: An iterative algorithm for minimizing our ob-jective in Eq.
(7).
For simplicity we assume that all theweights ?i and ?
are equal to one.
It can be shown thatthe objective monotonically decreases in every iteration.if it results in high parsing accuracy, as measuredwhen projecting a parser from to S to T .Since we do not have annotated parses in T , weuse the other source languages S = {S1, .
.
.
, Sn}to learn the weight.
For each Si as the target, wefirst train a parser for each language in S \ {Si} asif it was the source, using the map of Petrov et al(2011), and choose S?i ?
S \ {Si} which gives thehighest parsing accuracy on Si.
Next we generate7000 candidate mappings for Si by randomly per-turbing the map of (Petrov et al 2011).
We evalu-ate the quality of each candidate A by projecting theparser of S?i to Si, and recording the parsing accu-racy.
Among all the candidates we choose the high-est accuracy one and denote it by A?(Si).
We nowwant the score F (A?
(Si)) to be lower than that of allother candidates.
To achieve this, we train a rankingSVM whose inputs are pairs of mapsA?
(Si) and an-1373other worse A(Si).
These map pairs are taken frommany different traget languages, i.e.
many differentSi.
The features given to the SVM are the terms ofthe score Fi(A).
The goal of the SVM is to weightthese terms such that the better map A?
(Si) has alower score.
The weights assigned by the SVM aretaken as ?i.5.3 Source Language SelectionAs noted in Section 4 we construct F (A) by choos-ing a single source language S. Here we describe themethod for choosing S. Our goal is to choose S thatis closest to T in terms of typology.
Assume thatlanguages are described by binary typological vec-tors vL.
We would like to learn a diagonal matrixD such that d(S, T ;D) = (vS ?
vT )TD(vS ?
vT )reflects the similarity between the languages.
In ourcontext, a good measure of similarity is the perfor-mance of a parser trained on S and projected on T(using the optimal map A).
We thus seek a matrixD such that d(S, T ;D) is ranked according to theparsing accuracy.
The matrix D is trained using anSVM ranking algorithm that tries to follow the rank-ing of parsing accuracy.
Similar to the technique forlearning the objective weights, we train across manypairs of source languages.9The typological features we use are a subsetof the features described in ?The World Atlas ofLanguages Structure?
(WALS, (Haspelmath et al2005)), and are shown in Table 1.6 Evaluation Set-UpDatasets We test our model on 19 languages: Ara-bic, Basque, Bulgarian, Catalan, Chinese, Czech,Danish, Dutch, English, German, Greek, Hungar-ian, Italian, Japanese, Portuguese, Slovene, Span-ish, Swedish, and Turkish.
Our data is taken fromthe CoNLL 2006 and 2007 shared tasks (Buch-holz and Marsi, 2006; Nivre et al 2007).
TheCoNLL datasets consist of manually created depen-dency trees and language-specific POS tags.
Fol-lowing Petrov et al(2011), our model maps theselanguage-specific tags to a set of 12 universal tags:noun, verb, adjective, adverb, pronoun, determiner,adposition, numeral, conjunction, particle, punctua-tion mark and X (a general tag).9Ties are broken using the F (A) objective.Evaluation Procedure We perform a separate ex-periment for each of the 19 languages as the tar-get and a source language chosen from the rest (us-ing the method from Section 5.3).
For the selectedsource language, we assume access to the mappingof Petrov et al(2011).Evaluation Measures We evaluate the quality ofthe derived mapping in the context of the target lan-guage parsing accuracy.
In both the training andtest data, the language-specific tags are replacedwith universal tags: Petrov?s tags for the source lan-guages and learned tags for the target language.
Wetrain two non-lexicalized parsers using source anno-tations and apply them to the target language.
Thefirst parser is a non-lexicalized version of the MSTparser (McDonald et al 2005) successfully used inthe multilingual context (McDonald et al 2011).
Inthe second parser, parameters of the target languageare estimated as a weighted mixture of parameterslearned from supervised source languages (Cohen etal., 2011).
For the parser of Cohen et al(2011), wetrained the model on the four languages used in theoriginal paper ?
English, German, Czech and Ital-ian.
When measuring the performance on each ofthese four languages, we selected another set of fourlanguages with a similar level of diversity.10Following the standard evaluation practice inparsing, we use directed dependency accuracy as ourmeasure of performance.Baselines We compare mappings induced by ourmodel against three baselines: the manually con-structed mapping of Petrov et al(2011), a randomlyconstructed mapping and a greedy mapping.
Thegreedy mapping uses the same objective as our fullmodel, but optimizes it using a greedy method.
Ineach iteration, this method makes |LT | passes overthe language-specific tags, selecting a substitutionthat contributes the most to the objective.Initialization To reduce the dimension of our al-gorithm?s search space and speed up our method, westart by clustering the language-specific POS tags ofthe target into |K| = 12 clusters using an unsuper-10We also experimented with a version of the Cohen et al(2011) model trained on all the source languages.
This set-upresulted in decreased performance.
For this reason, we chose totrain the model on the four languages.1374ID Feature Description Values81A Order of Subject, Object and Verb SVO, SOV, VSO, VOS, OVS, OSV85A Order of Adposition and Noun Postpositions, Prepositions, Inpositions86A Order of Genitive and Noun Genitive-Noun, Noun-Genitive87A Order of Adjective and Noun Adjective-Noun, Noun-Adjective88A Order of Demonstrative and Noun Demonstrative-Noun, Noun-Demonstrative, before and afterTable 1: The set of typological features that we use for source language selection.
The first column gives the ID ofthe feature as listed in WALS.
The second column describes the feature and the last column enumerates the allowablevalues for each feature; besides these values each feature can also have a value of ?No dominant order?.vised POS induction algorithm (Lee et al 2010).11Our mapping algorithm then learns the connectionbetween these clusters and universal tags.For initialization, we perform multiple randomrestarts and select the one with the lowest final ob-jective score.7 ResultsWe first present the results of our model using thegold POS tags for the target language.
Table 2 sum-marizes the performance of our model and the base-lines.Comparison against Baselines On average, themapping produced by our model yields parsers withhigher accuracy than all of the baselines.
These re-sults are consistent for both parsers (McDonald etal., 2011; Cohen et al 2011).
As expected, randommappings yield abysmal results ?
20.2% and 12.7%for the two parsers.
The low accuracy of parsers thatrely on the Greedy mapping ?
29.9% and 25.4% ?show that a greedy approach is a poor strategy formapping optimization.Surprisingly, our model slightly outperforms themapping of (Petrov et al 2011), yielding an aver-age accuracy of 56.7% as compared to the 55.4%achieved by its manually constructed counterpart forthe direct transfer method (McDonald et al 2011).Similar results are observed for the mixture weightsparser (Cohen et al 2011).
The main reason forthese differences comes from mistakes introduced inthe manual mapping.
For example, in Czech tag ?R?is labeled as ?pronoun?, while actually it should bemapped to ?adposition?.
By correcting this mistake,we gain 5% in parsing accuracy for the direct trans-fer parser.11This pre-clustering results in about 3% improvement, pre-sumably since it uses contextual information beyond what ouralgorithm does.Overall, the manually constructed mapping andour model?s output disagree on 21% of the assign-ments (measured on the token level).
However,the extent of disagreement is not necessarily predic-tive of the difference in parsing performance.
Forinstance, the manual and automatic mappings forCatalan disagree on 8% of the tags and their pars-ing accuracy differs by 5%.
For Greek on the otherhand, the disagreement between mappings is muchhigher ?
17%, yet the parsing accuracy is veryclose.
This phenomenon shows that not all mistakeshave equal weight.
For instance, a confusion be-tween ?pronoun?
and ?noun?
is less severe in theparsing context than a confusion between ?pronoun?and ?adverb?.Impact of Language Selection To assess thequality of our language selection method, we com-pare the model against an oracle that selects the bestsource for a given target language.
As Table 2 showsour method is very close to the oracle performance,with only 0.7% gap between the two.
In fact, for10 languages our method correctly predicts the bestpairing.
This result is encouraging in other contextsas well.
Specifically, McDonald et al(2011) havedemonstrated that projecting from a single oracle-chosen language can lead to good parsing perfor-mance, and our technique may allow such projectionwithout an oracle.Relations between Objective Values and Opti-mization Performance The suboptimal perfor-mance of the Greedy method shows that choosinga good optimization strategy plays a critical role infinding the desired mapping.
A natural question toask is whether the objective value is predictive of theend goal parsing performance.
Figure 3 shows theobjective values for the mappings computed by ourmethod and the baselines for four languages.
Over-1375Direct Transfer Parser (Accuracy) Mixture Weight Parser (Accuracy)Tag Diff.Random Greedy Petrov Model Best Pair Random Greedy Petrov Model.Catalan 15.9 32.5 74.8 79.3 79.3 12.6 24.6 65.6 73.9 8.8Italian 16.4 41.0 68.7 68.3 71.4 11.7 33.5 64.2 61.9 6.7Portuguese 15.8 24.6 72.0 75.1 75.1 10.7 14.1 70.4 72.6 12.2Spanish 11.5 27.4 72.1 68.9 68.9 6.4 26.5 58.8 62.8 7.5Danish 35.5 23.7 46.6 46.5 49.2 4.2 23.7 51.4 51.7 5.0Dutch 18.0 22.1 58.2 56.8 57.3 7.1 15.3 54.9 53.2 4.9English 14.7 19.0 51.6 49.0 49.0 13.3 15.1 47.5 41.8 17.7German 15.8 24.3 55.7 50.4 51.6 20.9 18.7 52.4 51.8 15.0Swedish 15.1 26.3 63.1 63.1 63.1 9.1 36.5 55.7 55.9 8.2Bulgarian 17.4 28.0 51.6 63.4 63.4 22.6 39.9 64.6 60.4 35.7Czech 19.0 34.4 47.7 57.3 57.3 12.7 26.2 48.3 55.7 28.5Slovene 15.6 21.8 43.5 51.4 52.8 11.3 20.7 42.2 53.0 38.8Greek 17.3 19.5 62.3 59.7 59.8 22.0 15.2 56.2 57.0 17.0Hungarian 28.4 44.1 53.8 52.3 52.3 4.0 43.8 46.4 51.7 18.1Arabic 22.1 45.4 51.5 51.2 52.9 3.9 40.9 48.3 51.1 15.7Basque 18.0 19.2 27.9 33.1 35.1 6.3 8.3 32.3 30.6 43.8Chinese 22.4 34.1 46.0 47.6 49.5 17.7 34.9 44.0 40.4 38.1Japanese 36.5 46.2 51.4 53.6 53.6 15.4 18.0 25.7 28.7 73.8Turkish 28.8 34.9 53.2 49.8 49.8 19.7 20.3 27.7 27.5 9.9Average 20.2 29.9 55.4 56.7 57.4 12.7 25.4 50.8 51.7 21.3Table 2: Directed dependency accuracy of our model and the baselines using gold POS tags for the target language.The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011).
The second sectionis for the weighted mixture parsing model (Cohen et al 2011).
The first two columns (Random and Greedy) of eachsection present the parsing performance with a random or a greedy mapping.
The third column (Petrov) shows theresults when the mapping of Petrov et al(2011) is used.
The fourth column (Model) shows the results when ourmapping is used and the fifth column in the first section (Best Pair) shows the performance of our model when the bestsource language is selected for every target language.
The last column (Tag Diff.)
presents the difference between ourmapping and the mapping of Petrov et al(2011) by showing the percentage of target language tokens for which thetwo mappings select a different universal tag.all, our method and the manual mappings reach sim-ilar values, both considerably better than other base-lines.
While the parsing performance correlates withthe objective, the correlation is not perfect.
For in-stance, on Greek our mapping has a better objectivevalue, but lower parsing performance.Ablation Analysis We next analyze the contribu-tion of each component of our objective to the result-ing performance.12 The strongest factor in our ob-jective is the distributional features capturing globalstatistics.
Using these features alone achieves anaverage accuracy of 51.1%, only 5.6% less thanthe full model score.
Adding just the verb-relatedconstraints to the distributional similarity objectivesimproves the average model performance by 2.1%.12The results are consistent for both parsers, here we reportthe accuracy for the direct transfer method (McDonald et al2011).Adding just the typological constraints yields a verymodest performance gain of 0.5%.
This is not sur-prising ?
the source language is selected to be typo-logically similar to the target language, and thus itsdistributional properties are consistent with typolog-ical features.
However, adding both the verb-relatedconstraints and the typological constraints results ina synergistic performance gain of 5.6% over the dis-tributional similarity objective, a gain which is muchbetter than the sum of the two individual gains.Application to Automatically Induced POS TagsA potential benefit of the proposed method is to re-late automatically induced clusters in the target lan-guage to universal tags.
In our experiments, we in-duce such clusters using Brown clustering,13 which13In our experiments, we employ Liang?s implementationhttp://cs.stanford.edu/?pliang/software/.
The number of clus-ters is set to 30.1376Catalan German Greek Arabic00.20.40.60.811.21.41.61.8ModelPetrovGreedyRandomFigure 3: Objective values for the different mappingsused in our experiments for four languages.
Note thatthe goal of the optimization procedure is to minimize theobjective value.has been successfully used for similar purposes inparsing research (Koo et al 2008).
We then mapthese clusters to the universal tags using our algo-rithm.The average parsing accuracy on the 19 languagesis 45.5%.
Not surprisingly, automatically inducedtags negatively impact parsing performance, yield-ing a decrease of 11% when compared to mappingsobtained using manual POS annotations (see Ta-ble 2).
To further investigate the impact of inaccu-rate tags on the mapping performance, we compareour model against the oracle mapping model thatmaps each cluster to the most common universal tagof its members.
Parsing accuracy obtained using thismethod is 45.1%, closely matching the performanceof our mapping algorithm.An alternative approach to mapping words intouniversal tags is to directly partition words into Kclusters (without passing through language specifictags).
In order for these clusters to be meaningfulas universal tags, we can provide several prototypesfor each cluster (e.g., ?walk?
is a verb etc.).
To testthis approach we used the prototype driven tagger ofHaghighi and Klein (2006) with 15 prototypes peruniversal tag.14 The resulting universal tags yieldan average parsing accuracy of 40.5%.
Our method(using Brown clustering as above) outperforms this14Oracle prototypes were obtained by taking the 15 mostfrequent words for each universal tag.
This yields almost thesame total number of prototypes as those in the experiment of(Haghighi and Klein, 2006).baseline by about 5%.8 ConclusionsWe present an automatic method for mappinglanguage-specific part-of-speech tags to a set of uni-versal tags.
Our work capitalizes on manually de-signed conversion schemes to automatically createmappings for new languages.
Our experimental re-sults demonstrate that automatically induced map-pings rival the quality of their hand-crafted coun-terparts.
We also establish that the mapping qualityhas a significant impact on the accuracy of syntactictransfer, which motivates further study of this topic.Finally, our experiments show that the choice ofmapping optimization scheme plays a crucial role inthe quality of the derived mapping, highlighting theimportance of optimization for the mapping task.AcknowledgmentsThe authors acknowledge the support of the NSF(IIS-0835445), the MURI program (W911NF-10-1-0533) and the DARPA BOLT program.
We thankTommi Jaakkola, the members of the MIT NLPgroup and the ACL reviewers for their suggestionsand comments.
Any opinions, findings, conclu-sions, or recommendations expressed in this paperare those of the authors, and do not necessarily re-flect the views of the funding organizations.ReferencesSabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of CoNLL, pages 149?164.David Burkett and Dan Klein.
2008.
Two languages arebetter than one (for syntactic parsing).
In Proceedingsof EMNLP, pages 877?886.Shay B. Cohen, Dipanjan Das, and Noah A. Smith.
2011.Unsupervised structure prediction with non-parallelmultilingual guidance.
In Proceedings of EMNLP,pages 50?61.Frank Van Eynde.
2004.
Part of speech tagging en lem-matisering van het corpus gesproken nederlands.
InTechnical report.Jenny Rose Finkel, Trond Grenager, and Christopher D.Manning.
2007.
The infinite tree.
In Proceedings ofACL, pages 272?279.Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
JMLR, 11:2001?2049.1377Michael Garey and David S. Johnson.
1979.
Comput-ers and Intractability: A Guide to the Theory of NP-Completeness.
W. H. Freeman & Co.Michael C. Grant and Stephen P. Boyd.
2008.
Graph im-plementations for nonsmooth convex programs.
In Re-cent Advances in Learning and Control, Lecture Notesin Control and Information Sciences, pages 95?110.Springer-Verlag Limited.Michael C. Grant and Stephen P. Boyd.
2011.
CVX:Matlab software for disciplined convex programming,version 1.21.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofNAACL, pages 320?327.Martin Haspelmath, Matthew S. Dryer, David Gil, andBernard Comrie, editors.
2005.
The World Atlas ofLanguage Structures.
Oxford University Press.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Journal of Natural Language Engineering, 11:311?325.Dan Klein and Christopher Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL, pages423?430.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Pro-ceedings of ACL, pages 595?603.Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.2010.
Simple type-level unsupervised pos tagging.
InProceedings of EMNLP, pages 853?861.Percy Liang, Slav Petrov, Michael I. Jordan, and DanKlein.
2007.
The infinite pcfg using hierarchi-cal dirichlet processes.
In Proceedings of EMNLP-CoNLL, pages 688?697.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic cfg with latent annotations.
InProceedings of ACL, pages 75?82.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof EMNLP, pages 523?530.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.Multi-source transfer of delexicalized dependencyparsers.
In Proceedings of EMNLP, pages 62?72.Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using universal linguistic knowl-edge to guide grammar induction.
In Proceedings ofEMNLP, pages 1234?1244.Radford M. Neal and Geoffrey E. Hinton.
1999.
A viewof the em algorithm that justifies incremental, sparse,and other variants.
In Michael I. Jordan, editor, Learn-ing in Graphical Models, pages 355?368.
MIT Press.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 shared task on dependencyparsing.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007, pages 915?932.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACL,pages 404?411.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings of ACL-COLING, pages 433?440.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011.A universal part-of-speech tagset.
In ArXiv, April.Chenhai Xi and Rebecca Hwa.
2005.
A backoff modelfor bootstrapping resources for non-english languages.In Proceedings of EMNLP, pages 851?858.Alan Yuille and Anand Rangarajan.
2003.
The concave-convex procedure (cccp).
In Proceedings of NeuralComputation, volume 15, pages 915?936.Daniel Zeman and Philip Resnik.
2008.
Cross-languageparser adaptation between related languages.
In Pro-ceedings of IJCNLP-08 Workshop on NLP for LessPrivileged Languages, pages 35?42.1378
