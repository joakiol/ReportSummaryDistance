First Joint Conference on Lexical and Computational Semantics (*SEM), pages 552?556,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsIRIT: Textual Similarity Combining Conceptual Similarity with an N-GramComparison MethodDavide Buscaldi, Ronan Tournier, Nathalie Aussenac-Gilles and Josiane MotheIRIT118 Route de NarbonneToulouse (France){davide.buscaldi,ronan.tournier}@irit.fr,{nathalie.aussenac,josiane.mothe}@irit.frAbstractThis paper describes the participation of theIRIT team to SemEval 2012 Task 6 (Seman-tic Textual Similarity).
The method used con-sists of a n-gram based comparison methodcombined with a conceptual similarity mea-sure that uses WordNet to calculate the sim-ilarity between a pair of concepts.1 IntroductionThe system used for the participation of the IRITteam (composed by members of the research groupsSIG and MELODI) to the Semantic Textual Similar-ity (STS) task (Agirre et al, 2012) is based on twosub-modules:?
a module that calculates the similarity betweensentences using n-gram based similarity;?
a module that calculates the similarity betweenconcepts in the two sentences, using a conceptsimilarity measure and WordNet (Miller, 1995)as a resource.In Figure 1, we show the structure of the sys-tem and the connections between the main compo-nents.
The input phrases are passed on one handdirectly to the n-gram similarity module, and on theother they are annoted with the Stanford POS Tag-ger (Toutanova et al, 2003).
All nouns and verbs areextracted from the tagged phrases and WordNet issearched for synsets corresponding to the extractednouns and nouns associated to the verbs by the de-rived terms relationship.
The synsets are the con-cepts used by the conceptual similarity module toPhrasesN-gram similarity modulePOS TaggerGoogle Web 1TWordNetConcept similarity moduleScoreGeometric Average and NormalisationConcept ExtractionFigure 1: Schema of the system.calculate the concept similarity.
Each module cal-culates a similarity score using its own method; thefinal similarity value is calculated as the geometricaverage between the two scores, multiplied by 5 inorder to comply with the task specifications.The n-gram based similarity relies on the ideathat two sentences are semantically related if theycontain a long enough sub-sequence of non-emptyterms.
Google Web 1T (Brants and Franz, 2006)has been used to calculate term idf, which is usedas a measure of the importance of the terms.
Theconceptual similarity is based on the idea that, givenan ontology, two concepts are semantically similarif their distance from a common ancestor is smallenough.
We used three different measures: the Wu-Palmer similarity measure (Wu and Palmer, 1994)and two ?Proxigenea?
measures (Dudognon et al,2010).
In the following we will explain in detail how552each similarity module works.2 N-Gram based SimilarityN-gram based similarity is based on the ClusteredKeywords Positional Distance (CKPD) model pro-posed in (Buscaldi et al, 2009).
This model wasoriginally proposed for passage retrieval in the fieldof Question Answering (QA), and it has been im-plemented in the JIRS system1.
In (Buscaldi et al,2006), JIRS showed to be able to obtain a better an-swer coverage in the Question Answering task thanother traditional passage retrieval models based onVector Space Model, such as Lucene2.
The modelhas been adapted for this task by calculating the idfweights for each term using the frequency value pro-vided by Google Web 1T.The similarity between a text fragment (or pas-sage) p and another text fragment q is calculated as:Sim(p, q) =?
?x?Qh(x, P )1d(x, xmax)?ni=1 wi(1)Where P is the set of n-grams with the highestweight in p, where all terms are also contained in q;Q is the set of all the possible j-grams in q and nis the total number of terms in the longest passage.The weights for each term and each n-gram are cal-culated as:?
wi calculates the weight of the term tI as:wi = 1?log(ni)1 + log(N)(2)Where ni is the frequency of term ti in theGoogle Web 1T collection, and N is the fre-quency of the most frequent term in the GoogleWeb 1T collection.?
the function h(x, P ) measures the weight ofeach n-gram and is defined as:h(x, Pj) ={ ?jk=1 wk if x ?
Pj0 otherwise(3)1http://sourceforge.net/projects/jirs/2http://lucene.apache.org/Where wk is the weight of the k-th term (seeEquation 2) and j is the number of terms thatcompose the n-gram x;?
1d(x,xmax) is a distance factor which reduces theweight of the n-grams that are far from theheaviest n-gram.
The function d(x, xmax) de-termines numerically the value of the separa-tion according to the number of words betweena n-gram and the heaviest one.
That function isdefined as show in Equation 4 :d(x, xmax) = 1 + k?
ln(1 + L) (4)Where k is a factor that determines the impor-tance of the distance in the similarity calcula-tion and L is the number of words between an-gram and the heaviest one (see Equation 3).In our experiments, k was set to 0.1, a defaultvalue used in JIRS.For instance, given the following two sentences:?Mr.
President, enlargement is essential for the con-struction of a strong and united European continent?and ?Mr.
President, widening is essential for theconstruction of a strong and plain continent of Eu-rope?, the longest n-grams shared by the two sen-tences are: ?Mr.
President?, ?is essential for theconstruction of a strong and?, ?continent?.term w(term)Mr 0.340President 0.312is 0.159essential 0.353for 0.153the 0.104construction 0.332of 0.120a 0.139strong 0.329and 0.121continent 0.427of 0.120Europe 0.308widening 0.464Table 1: Term weights (idf) calculated using the fre-quency for each term in Google Web 1T unigrams set.553Figure 2: Visualisation of depth calculation.The weights have been calculated with Formula2, using the frequencies from Google Web 1T.
Theweights for each of the longest n-grams are 0.652,1.809 and 0.427 respectively; their sum is 2.888which divided by all the term weights containedin the sentence gives 0.764 which is the similarityscore between the two sentences as calculated by then-gram based method.3 Conceptual SimilarityGiven Cp and Cq as the sets of concepts contained insentence p and q, respectively, with |Cp| ?
|Cq|, theconceptual similarity between p and q is calculatedas:ss(p, q) =?c1?Cpmaxc2?Cqs(c1, c2)|Cp|(5)where s(c1, c2) is a concept similarity measure.Concept similarity can be calculated by differentways.
Wu and Palmer introduced in (Wu andPalmer, 1994) a concept similarity measure definedas:s(c1, c2) =2 ?
d(c0)d(c1) + d(c2)(6)c0 is the most specific concept that is present bothin the synset path of c1 and c2 (see Figure 2 for de-tails).
The function returning the depth of a conceptis noted with d.3.1 ProxiGeneaBy making an analogy between a family tree andthe concept hierarchy in WordNet, (Dudognon et al,2010; Ralalason, 2010) proposed a concept similar-ity measure based on the principle of evaluating theproximity between two members of the same fam-ily.
The measure has been named ?ProxiGenea?
(from the french Proximite?
Ge?ne?alogique, genealog-ical proximity).
We took into account three versionsof the ProxiGenea measure:pg1(c1, c2) =d(c0)2d(c1) ?
d(c2)(7)This measure is very similar to the Wu-Palmer sim-ilarity measure, but it emphasizes the distances be-tween concepts;pg2(c1, c2) =d(c0)d(c1) + d(c2)?
d(c0)(8)In this measure, the more are the elements which arenot shared between the paths of c1 and c2, the morethe score decreases.
However, if the elements areplaced more deeply in the ontology, the decrease isless important.pg3(c1, c2) =11 + d(c1) + d(c2)?
2 ?
d(c0)(9)In Table 2 we show the weights that have beencalculated for each concept, using all the above sim-ilarity measures, and the concept that provided themaximum weight.
No Word Sense Disambiguationprocess is carried out; therefore, the scores are cal-culated taking into account all the possible sensesfor the word.
If the same concept is present in bothsentences, it obtains always a score of 1.
In the othercases, the maximum similarity value obtained withany other concept is retained.From the example in Table 2 we can see that Wu-Palmer tends to give to the concepts a higher simi-larity value than Proxigenea3.The final score for the above example is cal-culated as the geometric mean between the scoresobtained in Table 2 and 0.764 obtained from then-gram based similarity module, multiplied by 5.Therefore, for each similarity measure, the finalscores of the example are, respectively: 4.029,3.869, 3.921 and 3.703.
The correct similarity value,according to the gold standard, was 4.600.554c1, c2 wp pg1 pg2 pg3Mr1.000 1.000 1.000 1.000MrPresident1.000 1.000 1.000 1.000Presidentconstruction1.000 1.000 1.000 1.000constructioncontinent1.000 1.000 1.000 1.000continentEurope0.400 0.160 0.250 0.143continentwidening0.737 0.544 0.583 0.167enlargementscore 0.850 0.784 0.805 0.718Table 2: Maximum conceptual similarity weights usingthe different formulae for the concepts in the example.c1: first concept, c2: concept for which the maximumsimilarity value was calculated.
wp: Wu-Palmer similar-ity; pgX : Proxigenea similarity.
score is the result of (5).4 EvaluationBefore the official runs we carried out an evalua-tion to select the best similarity measures over thetraining set provided by the organisers.
The resultsof this evaluation are shown in Table 3.
The mea-sure selected is the normalised Pearson correlation(Agirre et al, 2012).
We evaluated also the use ofthe product instead of the geometric mean for thecombination of the two scores.Geometric meanMSRpar MSRvid SMT-Eur Allpg1 0.489 0.602 0.587 0.559pg2 0.490 0.596 0.586 0.558pg3 0.470 0.657 0.552 0.560wp 0.494 0.572 0.592 0.552Scalar productMSRpar MSRvid SMT-Eur Allpg1 0.469 0.601 0.487 0.519pg2 0.471 0.597 0.487 0.518pg3 0.447 0.637 0.459 0.514wp 0.476 0.577 0.492 0.515Table 3: Results on training corpus, comparison of dif-ferent conceptual similarity measures and combinationmethod.
Top: geometric mean, bottom: product.We used these results to select the final config-urations for our participation to the STS task: weselected to exclude Proxigenea 2 and to use the ge-ometric mean to combine the scores of the n-grambased similarity module and the conceptual similar-ity module.
Wu-Palmer similarity allowed to obtainthe best results on two train sets but Proxigenea 3was the similarity measure that obtained the best av-erage score thanks to the good result on MSRvid.The official results obtained by our system areshown in Table 4, with the ranking obtained for eachtest set.
We could observe that the system was wellr best pg3 pg1 wpMSRPar 60 0.734 0.417 0.429 0.433MSRvid 58 0.880 0.673 0.612 0.583SMTeur 7 0.567 0.518 0.495 0.486OnWN 64 0.727 0.553 0.539 0.532SMTnews 55 0.608 0.369 0.361 0.348All 58 0.677 0.520 0.501 0.490Table 4: Results obtained on each test set, grouped byconceptual similarity method.
r indicates the rankingamong all the participants teams.behind the best system in most test sets, except forSMTeur.
This was expected since our system doesnot use a machine learning approach and is com-pletely unsupervised, while the best systems usedsupervised learning.
We observed also that the be-haviour of the concept similarity measures was dif-ferent from the behaviour on the training sets.
In thecompetition, the best results were always obtainedwith Proxigenea3 instead of Wu-Palmer, except forthe MSRpar test set.In Table 4 we extrapolated the results for the com-posing methods and compared them with the resultobtained after their combination.
We used the pg3configuration for the conceptual similarity measure.From these results, we can observe that MSRvidwas a test set where the conceptual similarity alonewould have resulted better than the combination ofscores, while SMT-news was the test set where theCKPD measure obtained the best results in compar-ison to the result obtained by the conceptual simi-larity alone.
It was quite surprising to observe sucha good result for a method that does not take intoaccount any information about the structure of thesentences, actually viewing them as ?bags of con-555Combined pg3 CKPDMSRPar 0.417 0.412 0.417MSRvid 0.673 0.777 0.548SMTeuroparl 0.518 0.486 0.467OnWN 0.553 0.544 0.505SMTnews 0.369 0.266 0.408Table 5: Results obtained for each test set using only theconceptual similarity measure (pg3) and only the struc-tural similarity measure (CKPD), compared to the re-sult obtained by the complete system (Combined).cepts?.
This is probably due to the fact that SMT-news is a corpus composed of automatically trans-lated sentences, where structural similarity is an im-portant clue for determining overall semantic sim-ilarity.
On the other hand, MSRvid sentences arevery short, and CKPD is in most cases unable to cap-ture the semantic similarity.5 ConclusionsThe proposed method combined a measure of struc-tural similarity and a measure of conceptual simi-larity based on WordNet.
With the participation tothis task, we were interested in studying the differ-ences between different conceptual similarity mea-sures and in determining whether they can be usedto effectively measure the semantic similarity of textfragments.
The obtained results showed that Proxi-genea 3 allowed us to obtain the best results, indicat-ing that under the test conditions and with WordNetas a resource it overperforms the Wu-Palmer mea-sure.
Further studies may be required in order todetermine if these results can be generalised to othercollections and in using different ontologies.
We arealso interested in comparing the method to the Linconcept similarity measure (Lin, 1998) which takesinto account also the importance of the local rootconcept.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and Aitor Gon-zalez.
2012.
A pilot on semantic textual similarity.In Proceedings of the 6th International Workshop onSemantic Evaluation (SemEval 2012), in conjunctionwith the First Joint Conference on Lexical and Compu-tational Semantcis (*SEM 2012), Montreal, Quebec,Canada.Thorsten Brants and Alex Franz.
2006.
Web 1t 5-gramcorpus version 1.1.Davide Buscaldi, Jose?
Manuel Go?mez, Paolo Rosso, andEmilio Sanchis.
2006.
N-gram vs. keyword-basedpassage retrieval for question answering.
In CLEF,pages 377?384.Davide Buscaldi, Paolo Rosso, Jose?
Manuel Go?mez, andEmilio Sanchis.
2009.
Answering questions with ann-gram based passage retrieval engine.
Journal of In-telligent Information Systems (JIIS), 34(2):113?134.Damien Dudognon, Gilles Hubert, and Bachelin JhonnVictorino Ralalason.
2010.
Proxige?ne?a : Une mesurede similarite?
conceptuelle.
In Proceedings of the Col-loque Veille Strate?gique Scientifique et Technologique(VSST 2010).Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the Fifteenth In-ternational Conference on Machine Learning, ICML?98, pages 296?304, San Francisco, CA, USA.
Mor-gan Kaufmann Publishers Inc.George A. Miller.
1995.
Wordnet: a lexical database forenglish.
Commun.
ACM, 38(11):39?41, November.Bachelin Ralalason.
2010.
Repre?sentation multi-facettedes documents pour leur acce`s se?mantique.
Ph.D. the-sis, Universite?
Paul Sabatier, Toulouse, September.
inFrench.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1,NAACL ?03, pages 173?180.Zhibiao Wu and Martha Palmer.
1994.
Verbs semanticsand lexical selection.
In Proceedings of the 32nd an-nual meeting on Association for Computational Lin-guistics, ACL ?94, pages 133?138, Stroudsburg, PA,USA.
Association for Computational Linguistics.556
