Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 127?132,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsDCU-UVT: Word-Level Language Classification with Code-Mixed DataUtsab Barman, Joachim Wagner, Grzegorz Chrupa?a?and Jennifer FosterCNGL Centre for Global Intelligent Content, National Centre for Language TechnologySchool of Computing, Dublin City University, Dublin, Ireland?Tilburg School of Humanities, Department of Communication and Information SciencesTilburg University, Tilburg, The Netherlands{ubarman,jwagner,jfoster}@computing.dcu.ieG.A.Chrupala@uvt.nlAbstractThis paper describes the DCU-UVTteam?s participation in the Language Iden-tification in Code-Switched Data sharedtask in the Workshop on ComputationalApproaches to Code Switching.
Word-level classification experiments were car-ried out using a simple dictionary-basedmethod, linear kernel support vector ma-chines (SVMs) with and without con-textual clues, and a k-nearest neighbourapproach.
Based on these experiments,we select our SVM-based system withcontextual clues as our final system andpresent results for the Nepali-English andSpanish-English datasets.1 IntroductionThis paper describes DCU-UVT?s participationin the shared task Language Identification inCode-Switched Data (Solorio et al., 2014) atthe Workshop on Computational Approaches toCode Switching, EMNLP, 2014.
The task is tomake word-level predictions (six labels: lang1,lang2, ne, mixed, ambiguous and other) for mixed-language user generated content.
We submit pre-dictions for Nepali-English and Spanish-Englishdata and perform experiments using dictionaries, ak-nearest neighbour (k-NN) classifier and a linear-kernel SVM classifier.In our dictionary-based approach, we investi-gate the use of different English dictionaries aswell as the training data.
In the k-NN basedapproach, we use string edit distance, character-n-gram overlap and context similarity to makepredictions.
For the SVM approach, we experi-ment with context-independent (word, character-n-grams, length of a word and capitalisation in-formation) and context-sensitive (adding the pre-vious and next word as bigrams) features in differ-ent combinations.
We also experiment with addingfeatures from the k-NN approach and another setof features from a neural network.
Based on per-formance in cross-validation, we select the SVMclassifier with basic features (word, character-n-grams, length of a word, capitalisation informationand context) as our final system.2 BackgroundWhile the problem of automatically identify-ing and analysing code-mixing has been iden-tified over 30 years ago (Joshi, 1982), it hasonly recently drawn wider attention.
Specificproblems addressed include language identifica-tion in multilingual documents, identification ofcode-switching points and POS tagging (Solorioand Liu, 2008b) of code-mixing data.
Ap-proaches taken to the problem of identifying code-mixing include the use of dictionaries (Nguyenand Do?gru?oz, 2013; Barman et al., 2014; El-fardy et al., 2013; Solorio and Liu, 2008b), lan-guage models (Alex, 2008; Nguyen and Do?gru?oz,2013; Elfardy et al., 2013), morphological andphonological analysis (Elfardy et al., 2013; El-fardy and Diab, 2012) and various machine learn-ing algorithms such as sequence labelling withHidden Markov Models (Farrugia, 2004; Ros-ner and Farrugia, 2007) and Conditional RandomFields (Nguyen and Do?gru?oz, 2013; King andAbney, 2013), as well as word-level classifica-tion using Naive Bayes (Solorio and Liu, 2008a),logistic regression (Nguyen and Do?gru?oz, 2013)and SVMs (Barman et al., 2014), using featuressuch as word, POS, lemma and character-n-grams.Language pairs that have been explored includeEnglish-Maltese (Farrugia, 2004; Rosner and Far-rugia, 2007), English-Spanish (Solorio and Liu,2008b), Turkish-Dutch (Nguyen and Do?gru?oz,1272013), modern standard Arabic-Egyptian di-alect (Elfardy et al., 2013), Mandarin-English (Liet al., 2012; Lyu et al., 2010), and English-Hindi-Bengali (Barman et al., 2014).3 Data StatisticsThe training data provided for this task consists oftweets.
Unfortunately, because of deleted tweets,the full training set could not be downloaded.
Outof 9,993 Nepali-English training tweets, we wereable to download 9,668 and out of 11,400 Spanish-English training tweets, we were able to download11,353.
Table 1 shows the token-level statistics ofthe two datasets.Label Nepali-English Spanish-Englishlang1 (en) 43,185 76,204lang2 (ne/es) 59,579 32,477ne 3,821 2,814ambiguous 125 341mixed 112 51other 34,566 21,813Table 1: Number of tokens in the Nepali-Englishand Spanish-English training data for each labelNepali (lang2) is the dominant language inthe Nepali-English training data but for Spanish-English, English (lang1) is dominant.
The thirdlargest group contains tokens with the label other.These are mentions (@username), punctuationsymbols, emoticons, numbers (except numbersthat represent words such as 2 for to), words in alanguage other than lang1 and lang2 and unintel-ligible words.
Named entities (ne) are much lessfrequent and mixed language words (e.g.
ramri-ness) and words for which there is not enough con-text to disambiguate them are rare.
Hash tags areannotated as if the hash symbol was not there, e.g.#truestory is labelled lang1.4 ExperimentsAll experiments are carried out for Nepali-Englishdata.
Later we apply the best approach to Spanish-English.
We train our systems in a five-fold cross-validation and obtain best parameters based onaverage cross-validation results.
Cross-validationsplits are made based on users, i.e.
we avoid theoccurrence of a user?s tweets both in training andtest splits for each cross-validation run.
We ad-dress the task with the following approaches:1. a simple dictionary-based classifier,Resource AccuracyBNC 43.61LexNorm 54.60TrainingData 89.53TrainingData+BNC+LexNorm 90.71Table 2: Average cross-validation accuracy ofdictionary-based prediction for Nepali-English2.
classification using supervised machinelearning with k-nearest neighbour, and3.
classification using supervised machinelearning with SVMs.4.1 Dictionary-Based DetectionWe start with a simple dictionary-based approachusing as dictionaries (a) the British National Cor-pus (BNC) (Aston and Burnard, 1998), (b) Hanet al.
?s lexical normalisation dictionary (LexNorm)(Han et al., 2012) and (c) the training data.The BNC and LexNorm dictionaries are built byrecording all words occurring in the respectivecorpus or word list as English.
For the BNC, wealso collect word frequency information.
For thetraining data, we obtain dictionaries for each of thesix labels and each of the five cross-validation runs(using the relevant 4/5 of training data).To make a prediction, we consult all dictionar-ies.
If there are more than one candidate label,we choose the label for which the frequency forthe query token is highest.
To account for the factthat the BNC is much larger than the training data,we normalise all frequencies before comparison.LexNorm has no frequency information, hence itis added to our system as a simple word list (weconsider the language of a word to be English if itappears in LexNorm).
If a word appears in multi-ple dictionaries with the same frequency or if theword does not appear in any dictionary or list, thepredicted language is chosen based on the domi-nant language(s)/label(s) of the corpus.We experiment with the individual dictionar-ies and the combination of all three dictionaries,among which the combination achieves the high-est cross-validation accuracy (90.71%).
Table 2shows the results of dictionary-based detection ob-tained in five-fold cross-validation.4.2 Classification with k-NNFor Nepali-English, we also experiment with asimple k-nearest neighbour (k-NN) approach.
Foreach test item, we select a subset of the trainingdata using string edit distance and n-gram overlap128and choose the majority label of the subset as ourprediction.
For efficiency, we first select k1itemsthat share an n-gram with the token to be classi-fied.1The set of k1items is then re-ranked ac-cording to string edit distance to the test item andthe best k2matches are used to make a prediction.Apart from varying k1and k2, we experimentwith (a) lowercasing strings, (b) including contextby concatenating the previous, current and nexttoken, and (c) weighting context by first calcu-lating edit distances for the previous, current andnext token separately and using a weighted aver-age.
The best configuration we found in cross-validation uses lowercasing with k1= 800 andk2= 16 but no context information.
It achievesan accuracy of 94.97%.4.3 SVM ClassificationWe experiment with linear kernel SVM classifiersusing Liblinear (Fan et al., 2008).
Parameter opti-misation2is performed for each feature set combi-nation to obtain best cross-validation accuracy.4.3.1 Basic FeaturesFollowing Barman et al.
(2014), our basic featuresare:Char-N-Grams (G): We start with a charac-ter n-gram-based approach (Cavnar and Trenkle,1994).
Following King and Abney (2013), we se-lect lowercased character n-grams (n=1 to 5) andthe word as the features in our experiments.Dictionary-Based Labels (D): We use presencein the dictionary of the 5,000 most frequent wordsin the BNC and presence in the LexNorm dictio-nary as binary features.3Length of words (L): We create multiple fea-tures for token length using a decision tree (J48).We use length as the only feature to train a deci-sion tree for each fold and use the nodes obtainedfrom the tree to create boolean features (Rubino etal., 2013; Wagner et al., 2014).1Starting with n = 5, we decrease n until there are atleast k1items and then we randomly remove items added inthe last augmentation step to arrive at exactly k1items.
(Forn = 0, we randomly sample from the full training data.
)2C = 2iwith i = ?15,?14, ..., 103We chose these parameters based on experiments witheach dictionary, combinations of dictionaries and various fre-quency thresholds.
We apply a frequency threshold to theBNC to increase precision.
We rank the words according tofrequency and used the rank as a threshold (e.g.
top-5K, top-10K etc.).
With the top 5,000 ranked words and C = 0.25,we obtained best accuracy (96.40%).Features Accuracy Features AccuracyG 96.02 GD 96.27GL 96.11 GDL 96.32GC 96.15 GDC 96.20GLC 96.21 GDLC 96.40Table 3: Average cross-validation accuracy of 6-way SVMs on the Nepali-English data set; G =char-n-gram, L = binary length features, D = dict.-based labels and C = capitalisation featuresContext Accuracy(%)GDLC + P196.41GDLC + P296.38GDLC + N196.41GDLC + N296.41GDLC + P1+ N196.42GDLC + P2+ N296.41Table 4: Average cross-validation accuracy of 6-way SVMs using contextual features for Nepali-EnglishCapitalisation (C): We choose 3 booleanfeatures to encode capitalisation information:whether any letter in the word is capitalised,whether all letters in the word are capitalised andwhether the first letter is capitalised.Context (Piand Nj): We consider the previousi and next j token to be combined with the currenttoken, forming an (i+1)-gram and a (j+1)-gram,which we add as features.
Six settings are tested.Table 4 shows that using the bigrams formed withthe previous and next word are the best combina-tion for the task (among those tested).Among the eight combinations of the first fourfeature sets that contain the first set (G), Table 3shows that the 6-way SVM classifier4performsbest with all features sets (GDLC), achieving96.40% accuracy.
Adding contextual informationPiNjto GDLC, Table 4 shows best results fori=j=1, achieving 96.42% accuracy, only slightlyahead of the context-independent system.4.3.2 Neural Network (Elman) and k-NNFeaturesWe experiment with two additional features setsnot covered by Barman et al.
(2014):Neural Network (Elman): We extract featuresfrom the hidden layer of a recurrent neural net-4We also test 3-way SVM classification (lang1, lang2 andother) and heuristic post-processing, but it does not outper-form our 6-way classification runs.129Systems AccuracyGDLC 96.40k-NN 95.10Elman 89.96GDLC+k-NN 96.31GDLC+Elman 96.46GDLC+k-NN+Elman 96.40GDLC+P1N196.42k-NN+P1N195.11Elman+P1N191.53GDLC+P1N1+k-NN 96.33GDLC+P1N1+Elman 96.45GDLC+P1N1+k-NN+Elman 96.40Table 5: Average cross-validation accuracy of 6-way SVMs of combinations of GDLC, k-NN, El-man and P1N1features for Nepali-Englishwork that has been trained to predict the next char-acter in a string (Chrupa?a, 2014).
The 10 most ac-tive units of the hidden layer for each of the initial4 bytes and final 4 bytes of each token are bina-rised by using a threshold of 0.5.k-Nearest Neighbour (kNN): We obtain fea-tures from our basic k-NN approach (Section 4.2),encoding the prediction of the k-NN model withsix binary features (one for each label) and a nu-meric feature for each label stating the relativenumber of votes for the label, e.g.
if k2= 16and 12 votes are for lang1 the value of the fea-ture votes4lang1 will be 0.75.
Furthermore, weadd two features stating the minimum and maxi-mum edit distance between the test token and thek2selected training tokens.Table 5 shows cross-validation results for thesenew feature sets with and without the P1N1con-text features.
Excluding the GDLC features, wecan see that best accuracy is with k-NN and P1N1features (95.11%).
For Elman features, the accu-racy is lower (91.53% with context).
In combina-tion with the GDLC features, however, the Elmanfeatures can achieve a small improvement overthe GDLC+P1N1combination (+0.04 percentagepoints): 96.46% accuracy for the GDLC+Elmansetting (without P1N1features).
Furthermore, thek-NN features do not combine well.54.3.3 Final System and Test ResultsAt the time of submission of predictions, we hadan error in our GDLC+Elman feature combiner re-5A possible explanation may be that the k-NN featuresare based on only 3 of 5 folds for the training data (3 foldsare used to make predictions for the 4th set) but 4 of 5 foldsare used for test data predictions in each cross-validation run.TweetsToken-Level Tweet-LevelNepali-English 96.3 95.8Spanish-English 84.4 80.4Surprise GenreToken-Level Post-LevelNepali-English 85.6 77.5Spanish-English 94.4 80.0Table 6: Test set results (overall accuracy) forNepali-English and Spanish-English tweet dataand surprise genresulting in slightly lower performance.
Therefore,we selected SVM-GDLC-P1N1as our final ap-proach and trained the final two systems using thefull training data for Nepali-English and Spanish-English respectively.
While we knew that C =0.125 is best for Nepali-English from our experi-ments, we had to re-tune parameter C for Spanish-English using cross-validation on the training data.We found best accuracy of 94.16% for Spanish-English with C = 128.
Final predictions for thetest sets are made using these systems.Table 6 shows the test set results.
The testset for this task is divided into tweets and a sur-prise genre.
For the tweets, we achieve 96.3%and 84.4% accuracy (overall token-level accuracy)in Nepali-English and in Spanish-English respec-tively.
For this surprise genre (a collection of postsfrom Facebook and blogs), we achieve 85.6% forNepali-English and 94.4% for Spanish-English.5 ConclusionTo summarise, we achieved reasonable accuracywith a 6-way SVM classifier by employing basicfeatures only.
We found that using dictionariesis helpful, as are contextual features.
The perfor-mance of the k-NN classifier is also notable: it isonly 1.45 percentage points behind the final SVM-based system (in terms of cross-validation accu-racy).
Adding neural network features can furtherincrease the accuracy of systems.Briefly opening the test files to check for for-matting issues, we notice that the surprise genredata contains language-specific scripts that couldeasily be addressed in an English vs. non-Englishscenario.AcknowledgmentsThis research is supported by the Science Founda-tion Ireland (Grant 12/CE/I2267) as part of CNGL(www.cngl.ie) at Dublin City University.130ReferencesBeatrice Alex.
2008.
Automatic detection of Englishinclusions in mixed-lingual data with an applicationto parsing.
Ph.D. thesis, School of Informatics, TheUniversity of Edinburgh, Edinburgh, UK.Guy Aston and Lou Burnard.
1998.
The BNC hand-book: exploring the British National Corpus withSARA.
Capstone.Utsab Barman, Amitava Das, Joachim Wagner, andJennifer Foster.
2014.
Code-mixing: A challengefor language identification in the language of so-cial media.
In Proceedings of the First Workshopon Computational Approaches to Code-Switching.EMNLP 2014, Conference on Empirical Methods inNatural Language Processing, Doha, Qatar, Octo-ber.
Association for Computational Linguistics.William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Theo Pavlidis,editor, Proceedings of SDAIR-94, Third AnnualSymposium on Document Analysis and InformationRetrieval, pages 161?175.Grzegorz Chrupa?a.
2014.
Normalizing tweets withedit scripts and recurrent neural embeddings.
InProceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), pages 680?686, Baltimore, Mary-land, June.
Association for Computational Linguis-tics.Heba Elfardy and Mona Diab.
2012.
Token levelidentification of linguistic code switching.
In Pro-ceedings of Proceedings of COLING 2012: Posters(the 24th International Conference on Computa-tional Linguistics), pages 287?296, Mumbai, India.Heba Elfardy, Mohamed Al-Badrashiny, and MonaDiab.
2013.
Code switch point detection in Ara-bic.
In Natural Language Processing and Informa-tion Systems, pages 412?416.
Springer.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Paulseph-John Farrugia.
2004.
TTS pre-processing is-sues for mixed language support.
In Proceedingsof CSAW?04, the second Computer Science AnnualWorkshop, pages 36?41.
Department of ComputerScience & A.I., University of Malta.Bo Han, Paul Cook, and Timothy Baldwin.
2012.Automatically constructing a normalisation dictio-nary for microblogs.
In Proceedings of the 2012Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Natu-ral Language Learning, pages 421?432.
Associationfor Computational Linguistics.Aravind K. Joshi.
1982.
Processing of sentences withintra-sentential code-switching.
In J. Horeck?y, ed-itor, Proceedings of the 9th conference on Compu-tational linguistics - Volume 1 (COLING?82), pages145?150.
Academia Praha, North-Holland Publish-ing Company.Ben King and Steven Abney.
2013.
Labeling the lan-guages of words in mixed-language documents us-ing weakly supervised methods.
In Proceedings ofthe 2013 Conference of the North American Chap-ter of the Association for Computational Linguis-tics: Human Language Technologies, pages 1110?1119, Atlanta, Georgia, June.
Association for Com-putational Linguistics.Ying Li, Yue Yu, and Pascale Fung.
2012.
Amandarin-english code-switching corpus.
In Nico-letta Calzolari (Conference Chair), Khalid Choukri,Thierry Declerck, Mehmet Uur Doan, Bente Mae-gaard, Joseph Mariani, Asuncion Moreno, JanOdijk, and Stelios Piperidis, editors, Proceedingsof the Eight International Conference on LanguageResources and Evaluation (LREC?12), Istanbul,Turkey, may.
European Language Resources Asso-ciation (ELRA).Dau-Cheng Lyu, Tien Ping Tan, Engsiong Chng, andHaizhou Li.
2010.
SEAME: A Mandarin-Englishcode-switching speech corpus in South-East Asia.In INTERSPEECH 2010, 11th Annual Conferenceof the International Speech Communication Asso-ciation, volume 10, pages 1986?1989, Makuhari,Chiba, Japan.
ISCA Archive.Dong Nguyen and A. Seza Do?gru?oz.
2013.
Wordlevel language identification in online multilingualcommunication.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2013), pages 857?862, Seattle,Washington, USA, October.
Association for Com-putational Linguistics.Mike Rosner and Paulseph-John Farrugia.
2007.
Atagging algorithm for mixed language identifica-tion in a noisy domain.
In INTERSPEECH-2007,8th Annual Conference of the International SpeechCommunication Association, pages 190?193.
ISCAArchive.Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-hann Roturier, Rasoul Samad Zadeh Kaljahi, andFred Hollowood.
2013.
DCU-Symantec at theWMT 2013 quality estimation shared task.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 392?397, Sofia, Bulgaria.Association for Computational Linguistics.Thamar Solorio and Yang Liu.
2008a.
Learning to pre-dict code-switching points.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 973?981.
Association forComputational Linguistics.Thamar Solorio and Yang Liu.
2008b.
Part-of-speechtagging for English-Spanish code-switched text.
In131Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 1051?1060.
Association for Computational Linguistics.Thamar Solorio, Elizabeth Blair, Suraj Maharjan, SteveBethard, Mona Diab, Mahmoud Gonheim, AbdelatiHawwari, Fahad AlGhamdi, Julia Hirshberg, AlisonChang, and Pascale Fung.
2014.
Overview for thefirst shared task on language identification in code-switched data.
In Proceedings of the First Workshopon Computational Approaches to Code-Switching.EMNLP 2014, Conference on Empirical Methods inNatural Language Processing, Doha, Qatar, Octo-ber.
Association for Computational Linguistics.Joachim Wagner, Piyush Arora, Santiago Cortes, UtsabBarman, Dasha Bogdanova, Jennifer Foster, andLamia Tounsi.
2014.
DCU: Aspect-based polarityclassification for SemEval task 4.
In Proceedings ofthe International Workshop on Semantic Evaluation(SemEval-2014), pages 392?397, Dublin, Ireland,August.
Association for Computational Linguistics.132
