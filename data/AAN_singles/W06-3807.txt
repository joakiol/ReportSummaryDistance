Workshop on TextGraphs, at HLT-NAACL 2006, pages 37?44,New York City, June 2006. c?2006 Association for Computational LinguisticsLearning of Graph-based Question Answering RulesDiego Molla?Department of ComputingMacquarie UniversitySydney 2109, Australiadiego@ics.mq.edu.auAbstractIn this paper we present a graph-basedapproach to question answering.
Themethod assumes a graph representationof question sentences and text sentences.Question answering rules are automati-cally learnt from a training corpus of ques-tions and answer sentences with the an-swer annotated.
The method is indepen-dent from the graph representation formal-ism chosen.
A particular example is pre-sented that uses a specific graph represen-tation of the logical contents of sentences.1 IntroductionText-based question answering (QA) is the processof automatically finding the answers to arbitraryquestions in plain English by searching collectionsof text files.
Recently there has been intensive re-search in this area, fostered by evaluation-basedconferences such as the Text REtrieval Conference(TREC) (Voorhees, 2001b), the Cross-Lingual Eval-uation Forum (CLEF) (Vallin et al, 2005), and theNII-NACSIS Test Collection for Information Re-trieval Systems workshops (NTCIR) (Kando, 2005).Current research focuses on factoid question an-swering, whereby the answer is a short string thatindicates a fact, usually a named entity.
An exam-ple of a factoid question is Who won the 400m racein the 2000 Summer Olympic games?, which has ashort answer: Cathy Freeman.There are various approaches to question answer-ing.
The focus of this paper is on rule-based sys-tems.
A rule could be, say, ?if the question is ofthe form Who is the <position> of <country>?
anda text sentence says <position> of <country> Yand Y consists of two capitalised words, then Y isthe answer?).
Such a rule was used by Soubbotin(2001), who developed a system who obtained thebest accuracy in the 2001 Text REtrieval Conference(Voorhees, 2001a).
The system developed by Soub-botin (2001) relied on the development of a largeset of patterns of potential answer expressions, andthe allocation of those patterns to types of questions.The patterns were developed by hand by examiningthe data.Soubbotin (2001)?s work shows that a rule-basedQA system can produce good results if the rule set iscomprehensive enough.
Unfortunately, if the systemis ported to a new domain the set of rules needs tobe ported as well.
It has not been proven that ruleslike the ones developed by Soubbotin (2001), whichwere designed for the TREC QA task, can be portedto other domains.
Furthermore, the process of pro-ducing the rules was presumably very labour inten-sive.
Consequently, the cost of manually producingnew rules for a specialised domain could become tooexpensive for some domains.In this paper we present a method for the auto-matic learning of question answering rules by apply-ing graph manipulation methods.
The method relieson the representation of questions and answer sen-tences as graphs.
Section 2 describes the generalformat of the graph-based QA rules and section 3describes the method to learn the rules.
The meth-ods described on the above two sections are indepen-dent of the actual sentence representation formalism,37as long as the representation is a graph.
Section 4presents a specific application using logical graphs.Finally, sections 5 and 6 focus on related researchand final conclusions, respectively.2 Question Answering RulesIn one form or another, a question answering rulemust contain the following information:1. a pattern that matches the question;2. a pattern that matches the corresponding an-swer sentence; and3.
a pointer to the answer in the answer sentenceThe patterns in our rules are expressed as graphswith vertices containing variables.
A vertex witha variable can unify with a subgraph.
For exam-ple, Figure 1 shows two graphs and a pattern thatmatches both graphs.Graph 11 2 345Graph 21 2 7 89Pattern1 2 XYFigure 1: Two graphs and a pattern (variables in up-percase)Such patterns are used to match the graph repre-sentation of the question.
If a pattern defined in arule matches a question sentence, then the rule ap-plies to the sentence.Our rules specify the pattern of the answer sen-tence in an unusual way.
Instead of keeping a pat-tern to match the answer sentence, our rules definean extension graph that will be added to the graph ofthe question.
The rationale for this is that we want toreward answer sentences that have a high similaritywith the question.
Therefore, the larger the num-ber of vertices and edges that are shared between thequestion and the answer, the better.
The extensiongraph contains information that simulates the dif-ference between a question sentence and a sentencecontaining an answer.For example, lets us use graph representationsof syntactic dependency structures.
We will baseour representation on the output of Connexor(Tapanainen and Ja?rvinen, 1997), but the choice ofparser is arbitrary.
The same method applies to theoutput of any parser, as long as it can be representedas a graph.
In our choice, the dependency structureis represented as a bipartite graph where the lexi-cal entries are the vertices represented in boxes andthe dependency labels are the vertices represented inovals.
Figure 2 shows the graphs of a question andan answer sentence, and an extension of the questiongraph.
The answer is shown in thick lines, and theextension is shown in dashed lines.
This is what weaim to reproduce with our graph rules.
In particular,the extension of the question graph is such that thegraph of the answer sentence becomes a subgraph ofthe extended question graph.The question and answer sentence of Figure 2have an almost identical dependency graph and con-sequently the extension required to the questiongraph is very small.
Sentence pairs with more dif-ferences would induce a more substantial extensiongraph.Note that the extended graph still contains the rep-resentation of information that does not appear inthe answer sentence, namely the question term whatbook.
There is no need to remove any element fromthe question graph because, as we will see later, thecriteria to score the answer extracted are based onthe overlap between graphs.In sum, a graph rule has the following compo-nents:Rp a question pattern;Re an extension graph, which is a graph to be added38Q: What book did Rachel Carson write in 1962?
A: In 1962 Rachel Carson wrote ?Silent Spring?writev ch obj locdo book insubj det pcompcarson what 1962attrrachelwritesubj obj tmpcarson inattr1962springattrsilentQ extendedwritev ch obj locdo book insubj det pcompcarson what 1962attrrachelspringattr2silentFigure 2: Graph of a question, an answer sentence, and an extension of the question graph39to the question graph; andRa a pointer to the answer in the extension graphAn example of a rule is shown in Figure 3.
Thisrule is derived from the pair of question and answersentence shown in Figure 2.XobjYdetwhatANSWERFigure 3: Example of a QA rule.
Rp is in solid lines,Re is in dashed lines, and Ra is in thick lines.The rule can be used with a fresh pair of questionqi and answer sentence asi.
Let us use the notationGr(s) to denote the graph that represents the strings.
Also, unless said explicitly, names starting withuppercase denote graphs, and names starting withlowercase denote strings.
Informally, the process tofind the answer is:1.
If Gr(qi) matches Rp then the rule applies.Otherwise try a new rule.2.
Extend Gr(qi) with re to produce a new graphEReqi .3.
Compute the overlap between EReqi andGr(asi).4.
If a part of Ra is in the resulting overlap, thenexpand its projection on Gr(asi).The crucial point in the process is to determinethe projection of an overlap on the answer sentence,and then to extend it.
Once the overlap is found instep 3, if this overlap includes part of the annotatedanswer, that is if it includes Ra, then part of the an-swer will be the string in the answer sentence thatcorresponds to the overlap.
The full answer can beretrieved by expanding the answer found in the over-lap by following the outgoing edges in the graph ofqi What book did Michael Ende write in 1984?
ex-tended with the extension graph (Re) of Figure 3writev ch obj locdo book insubj det pcompende what 1984attrmichaelANSWERasi In 1984 Michael Ende wrote the novel titled?The Neverending Story?writev ch obj locdo novel insubj det mod pcompende the title 1984attr modmichael storydet attrthe neverendingFigure 4: An extended graph of a question and agraph of an answer sentence40writev ch obj locdo insubj pcompende 1984attrmichaelnovelFigure 5: Overlap of the graphs of Figure 4the answer.
Part of the process is shown in Figures 4and 5.In Figure 5 the overlap between the extendedquestion graph and the answer sentence graph con-tains the answer fragment novel.
After expanding itwe obtain the full answer the novel titled ?The NeverEnding Story?.13 Learning of Graph RulesTo learn a QA rule we need to determine the in-formation that is common between a question anda sentence containing an answer.
In terms of graphs,this is a variant of the well-known problem of find-ing the maximum common subgraph (MCS) of twographs (Bunke et al, 2002).The problem of finding the MCS of two graphs isknown to be NP-complete, but there are implemen-tations that are fast enough for practical uses, espe-cially if the graphs are not particularly large (Bunkeet al, 2002).
Given that our graphs are used to repre-sent sentences, their size would usually stay withina few tens of vertices.
This size is acceptable.There is an algorithm based on ConceptualGraphs (Myaeng and Lo?pez-Lo?pez, 1992) which isparticularly efficient for our purposes.Their methodfollows the traditional procedure of building the as-sociation graph of the two input graphs.
However, in1Note that this answer is not an exact answer according tothe TREC definition since it contains the string the novel titled;one further step would be needed to extract the exact answer;this is work for further research.contrast with the traditional approach, which findsthe cliques of the association graph (and this is thepart that is NP-complete), the method by Myaengand Lo?pez-Lo?pez (1992) first simplifies the associa-tion graph by merging some of its vertices, and thenit proceeds to searching the cliques.
By so doing thealgorithm is still exponential on the size of n, butnow n is smaller than with the traditional approachfor the same input graphs.The method presented by Myaeng and Lo?pez-Lo?pez (1992) finds connected graphs but we alsoneed to find overlaps that form unconnected graphs.For example, Figure 6 shows two graphs and theirMCS.
The resulting MCS is an unconnected graph,though Myaeng and Lo?pez-Lo?pez (1992)?s algo-rithm returns the two parts of the graph as indepen-dent MCSs.
It is easy to modify the original algo-rithm to obtain the desired output, as we did.Graph 1 Graph 212345123 45MCS (overlap)1245Figure 6: MCS of two graphsGiven two graphs G1 and G2, then their MCSis MCS(G1, G2).
To simplify the notation, wewill often refer to the MCS of two sentencesas MCS(s1, s2).
This is to be understood tobe the MCS of the graphs of the two sentencesMCS(Gr(s1), Gr(s2)).Let us now assume that the graph rule R is origi-nated from a pair (q,as) in the training corpus, whereq is a question and as a sentence containing the an-swer a.
The rule components are built as follows:Rp is the MCS of q and as, that is, MCS(q, as).Re is the path between the projection of Rp inGr(as) and the actual answer Gr(a).Ra is the graph representation of the exact answer.41Note that this process defines Rp as the MCS ofquestion and answer sentence.
Consequently, Rpis a subgraph of both the question and the answersentence.
This constraint is stronger than that of atypical QA rule, where the pattern needs to matchthe question only.
The resulting question pattern istherefore more general than it could be had one man-ually built the rule.
Rp does not include question-only elements in the question pattern because it isdifficult to determine what components of the ques-tion are to be added to the pattern, and what compo-nents are idiosyncratic to the specific question usedin the training set.Rules learnt this way need to be generalised in or-der to form generic patterns.
We currently use a sim-ple method of generalisation: convert a subset of thevertices into variables.
To decide whether a vertexcan be generalised a list of very common vertices isused.
This is the list of ?stop vertices?, in analogy tothe concept of stop words in methods to detect key-words in a string.
Thus, if a vertex is not in the listof stop vertices, then the vertex can be generalised.The list of stop vertices is fixed and depends on thegraph formalism used.For the question answering process it is useful toassociate a weight to every rule learnt.
The ruleweight is computed by testing the accuracy of therule in the training corpus.
This way, rules that over-generalise acquire a low weight.
The weight W(r)of a rule r is computed according to its precision onthe training set:W(r) =# correct answers found# answers found4 Application: QA with Logical GraphsThe above method has been applied to graphs rep-resenting the logical contents of sentences.
Therehas been a long tradition on the use of graphs forthis kind of sentence representation, such as Sowa?sConceptual Graphs (Sowa, 1979), and Quillian?s Se-mantic Nets (Quillian, 1968).
In our particular ex-periment we have used a graph representation thatcan be built automatically and that can be used effi-ciently for QA (Molla?
and van Zaanen, 2006).A Logical Graph (LG) is a directed, bipartitegraph with two types of vertices, concepts and re-lations.Concepts Examples of concepts are objects dog, ta-ble, events and states run, love, and propertiesred, quick.Relations Relations act as links between concepts.To facilitate the production of the LGs we havedecided to use relation labels that representverb argument positions.
Thus, the relation 1indicates the link to the first argument of a verb(that is, what is usually a subject).
The re-lation 2 indicates the link to the second argu-ment of a verb (usually the direct object), andso forth.
Furthermore, relations introduced byprepositions are labelled with the prepositionsthemselves.
Our relations are therefore close tothe syntactic structure.An example of a LG is shown in Figure 7, wherethe concepts are pictured in boxes and the relationsare pictured in ovals.The example in Figure 7 shows LG?s ability toprovide the graph representation of sentences withembedded clauses.
In contrast, other theories (suchas Sowa (1979)?s Conceptual Graphs) would rep-resent the sentence as a graph containing verticesthat are themselves graphs.
This departs from theusual definition of a graph, and therefore standardGraph Theory algorithms would need to be adaptedfor Conceptual Graphs.
An advantage of our LGs,therefore, is that they can be manipulated with stan-dard Graph Theory algorithms such as the ones de-scribed in this paper.Using the LG as the graph representation ofquestions and answer sentences, we implemented aproof-of-concept QA system.
The implementationand examples of graphs are described by Molla?
andvan Zaanen (2005) and here we only describe themethod to generalise rules and the decisions takento choose the exact answer.The process to generalise rules takes advantageof the two kinds of vertices.
Basically, relation ver-tices represent names of relations and we consideredthese to be important in the rule.
Consequently rela-tions edges were left unmodified in the generalisedrule.
Concept vertices are generalised by replacingthem with generic variables, except for a specific setof ?stop concepts?
which were not generalised.
Thelist of stop concepts is very small:42tom 1 believe 2want1mary 2marry1 2sailorTom believes that Mary wants to marry a sailorFigure 7: Example of a Logical Graphand, or, not, nor, if, otherwise, have, be,become, do, makeEvery question/answer pair in the training corpusgenerates one rule (or more if we use a process ofincreasingly generalising the rules).
Since the rule isbased on deep linguistic information, it generalisesover syntactic paraphrases.
Consequently, a smalltraining corpus suffices to produce a relatively largenumber of rules.The QA system was trained with an annotatedcorpus of 560 pairs of TREC questions and answersentences where the answers were manually anno-tated.
We only tested the ability of the system to ex-tract the exact answers.
Thus, the system acceptedpairs of question and answer sentences (where thesentence is guaranteed to contain an answer), andreturned the exact answer.
Given a question and an-swer sentence pair, the answer is found by applyingall matching rules.
All strings found as answers areranked by multiplying the rule weights and the sizesof the overlaps.
If an answer is found by severalrules, its score is the sum of all scores of each indi-vidual sentence.
Finally, if an answer occurs in thequestion it is ignored.
The results of a five-fold crossvalidation on the annotated corpus gave an accuracy(percentage of questions where the correct answerwas found) of 21.44%.
Given that the QA systemdoes not do any kind of question classification and itdoes not use any NE recogniser, the results are sat-isfactory.5 Related ResearchThere have been other attempts to learn QA rules au-tomatically.
For example, Ravichandran and Hovy(2002) learns rules based on simple surface patterns.Given that surface patterns ignore much linguisticinformation, it becomes necessary to gather a largecorpus of questions together with their answers andsentences containing the answers.
To obtain sucha corpus Ravichandran and Hovy (2002) mine theWeb to gather the relevant data.Other methods learn patterns based on syntacticinformation.
For example, Shen et al (2005) de-velop a method of extracting dependency paths con-necting answers with words found in the question.However we are not aware of any method that at-tempts to learn patterns based on logical informa-tion, other than our own.There is recent interest on the use of graphmethods for Natural Language Processing, suchas document summarisation (Mihalcea, 2004) doc-ument retrieval (Montes-y-Go?mez et al, 2000;Mishne, 2004), and recognition of textual entailment(Pazienza et al, 2005).
The present very workshopshows the current interest on the area.
However,we are not aware of any significant research aboutthe use of conceptual graphs (or any other form ofgraph representation) for question answering otherthan our own.6 ConclusionsWe have presented a method to learn question an-swering rules by applying graph manipulation meth-ods on the representations of questions and answersentences.
The method is independent of the actualgraph representation formalism.We are studying to combine WordNet with aNamed Entity Recogniser to produce generalisedrules.
This way it becomes possible to replace ver-tices with vertex types (e.g.
?PERSON?, ?DATE?,etc).
We are also exploring the use of machine learn-ing techniques to learn classes of vertices.
In par-ticular, grammar induction techniques (van Zaanen,2002) could be applied to learn types of regularitiesin the strings.43Further research will also focus on developingmethods to extend the question pattern Rp with in-formation found in the question only.
A possibilityis to keep a database of question subgraphs that areallowed to be added to Rp.
This database could bebuilt by hand, but ideally it should be learnt auto-matically.Additional research efforts will be allocated to de-termine degrees of word similarity or paraphrasing,such as the connection between was born in and ?sbirthplace is.
In particular, we will explore the useof nominalisations.
We will also study paraphrasingmethods to detect these connections.Considering that text information as complex assyntactic information or even logic and semanticinformation can be expressed in graphs (Quillian,1968; Schank, 1972; Sowa, 1979), we are convincedthat the time is ripe to explore the use of graphs forquestion answering.ReferencesH.
Bunke, P. Foggia, C. Guidobaldi, C. Sansone, andM.
Vento.
2002.
A comparison of algorithms formaximum common subgraph on randomly connectedgraphs.
In Lecture Notes on Computer Science, vol-ume 2396, pages 123?132.
Springer-Verlag, Heidel-berg.Noriko Kando.
2005.
Overview of the fifth NTCIRworkshop.
In Proceedings NTCIR 2005.Rada Mihalcea.
2004.
Graph-based ranking algorithmsfor sentence extraction applied to text summarization.In Proceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics, companionvolume (ACL 2004).Gilad Mishne.
2004.
Source code retrieval using concep-tual graphs.
Master?s thesis, University of Amsterdam.Diego Molla?
and Menno van Zaanen.
2005.
Learningof graph rules for question answering.
In Proc.
ALTW2005, Sydney.Diego Molla?
and Menno van Zaanen.
2006.
An-swerfinder at TREC 2005.
In Proceedings TREC2005.
NIST.Manuel Montes-y-Go?mez, Aurelio Lo?pez-Lo?pez, andAlexander Gelbukh.
2000.
Information retrieval withconceptual graph matching.
In Proc.
DEXA-2000,number 1873 in Lecture Notes in Computer Science,pages 312?321.
Springer-Verlag.Sung H. Myaeng and Aurelio Lo?pez-Lo?pez.
1992.
Con-ceptual graph matching: a flexible algorithm and ex-periments.
Journal of Experimentation and Theoreti-cal Artificial Intelligence, 4:107?126.Maria Teresa Pazienza, Marco Pennacchiotti, andFabio Massimo Zanzotto.
2005.
Textual entailmentas syntactic graph distance: a rule based and a SVMbased approach.
In Proceedings PASCAL RTE chal-lenge 2005.Ross Quillian.
1968.
Semantic memory.
In SemanticInformation Processing, pages 216?270.
MIT Press.Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a question answering system.In Proc.
ACL2002.Roger C. Schank.
1972.
Conceptual dependency: A the-ory of natural language understanding.
Cognitive Psy-chology, 3(4):532?631.Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow.2005.
Exploring syntactic relation patterns for ques-tion answering.
In Robert Dale, Kam-Fai Wong, JianSu, and Oi Yee Kwong, editors, Natural LanguageProcessing IJCNLP 2005: Second International JointConference, Jeju Island, Korea, October 11-13, 2005.Proceedings.
Springer-Verlag.M.
M. Soubbotin.
2001.
Patterns of potential answerexpression as clues to the right answers.
In Voorheesand Harman (Voorhees and Harman, 2001).John F. Sowa.
1979.
Semantics of conceptual graphs.
InProc.
ACL 1979, pages 39?44.Pasi Tapanainen and Timo Ja?rvinen.
1997.
A non-projective dependency parser.
In Proc.
ANLP-97.ACL.Alessandro Vallin, Bernardo Magnini, Danilo Giampic-colo, Lili Aunimo, Christelle Ayache, Petya Osenova,Anselmo Pe nas, Maarten de Rijke, Bogdan Sacaleanu,Diana Santos, and Richard Sutcliffe.
2005.
Overviewof the CLEF 2005 multilingual question answeringtrack.
In Proceedings CLEF 2005.
Working note.Menno van Zaanen.
2002.
Bootstrapping Structure intoLanguage: Alignment-Based Learning.
Ph.D. thesis,University of Leeds, Leeds, UK.Ellen M. Voorhees and Donna K. Harman, editors.
2001.The Tenth Text REtrieval Conference (TREC 2001),number 500-250 in NIST Special Publication.
NIST.Ellen M. Voorhees.
2001a.
Overview of the TREC 2001question answering track.
In Voorhees and Harman(Voorhees and Harman, 2001).Ellen M. Voorhees.
2001b.
The TREC question answer-ing track.
Natural Language Engineering, 7(4):361?378.44
