Retrieving Collocations from Text: XtractFrank  Smadja*Columbia UniversityNatural languages are full of collocations, recurrent combinations ofwords that co-occur moreoften than expected by chance and that correspond to arbitrary word usages.
Recent work inlexicography indicates that collocations are pervasive in English; apparently, they are commonin all types of writing, including both technical and nontechnical genres.
Several approacheshave been proposed to retrieve various types of collocations from the analysis of large samples oftextual data.
These techniques automatically produce large numbers of collocations along withstatistical figures intended to reflect the relevance of the associations.
However, noue of thesetechniques provides functional information along with the collocation.
Also, the results producedoften contained improper word associations reflecting some spurious aspect of the training corpusthat did not stand for true collocations.In this paper, we describe a set of techniques based on statistical methods for retrievingand identifying collocations from large textual corpora.
These techniques produce a wide rangeof collocations and are based on some original filtering methods that allow the production ofricher and higher-precision output.
These techniques have been implemented and resulted in alexicographic tool, Xtract.
The techniques are described and some results are presented on a 10million-word corpus of stock market news reports.
A lexicographic evaluation of Xtract as acollocation retrieval tool has been made, and the estimated precision of Xtract is 80%.1.
IntroductionConsider the following sentences:1.
"The Dow Jones average of 30 industrialsrose 26.28 points to 2,304.69 on Tuesday."2.
"The Dow average rose 26.28 points to 2,304.69on Tuesday."3.
"The Dow industrials rose 26.28 points to 2,304.69on Tuesday."4.
"The Dow Jones industrial rose 26.28 pointsto 2,304.69 on Tuesday.
".5 .
"The Jones industrials rose 26.28 pointsto 2,304.69 on Tuesday.
"* Computer Science Department, Columbia University, New York, NY 10027.
smadja@cs.columbia.edu.
@ 1993 Association for Computational LinguisticsComputational Linguistics Volume 19, Number 1Table 1Cross linguistic omparisons of collocations.Language English Translation English correspondenceFrench to see the door voir la porte to see the doorGerman to see the door die Ttir sehen to see the doorItalian to see the door vedere la porta to see the doorSpanish to see the door ver la puerta to see the doorTurkish to see the door kapiyi g6rmek to see the doorFrench to break down/force the door enfoncer la porteGerman to break down/force the door die Ttir aufbrechenItalian to break down/force the door sfondare la portaSpanish to break down/force the door tumbar la puertaTurkish to break down/force the door kapiyi kirmak* to push the door through, to break the door, to hit/demolish the door* to fall the door, to break the door,6 .
"The industrial Dow rose 26.28 points to2,304.69 on Tuesday.
"* 7.
"The Dow of 30 industrials rose 26.28 points to2,304.69 on Tuesday."8.
"The Dow industrial rose 26.28 points to2,304.69 on Tuesday.
"The above sentences contain expressions that are difficult to handle for nonspecial-ists.
For example, among the eight different expressions referring to the famous WallStreet index, only those used in sentences 1--4 are correct.
The expressions used in thestarred sentences 5-8 are all incorrect.
The rules violated in sentences 5--8 are neitherrules of syntax nor of semantics but purely lexical rules.
The word combinations usedin sentences 5-8 are invalid simply because they do not exist; similarly, the ones usedin sentences 1-4 are correct because they exist.Expressions uch as these are called collocations.
Collocations vary tremendouslyin the number of words involved, in the syntactic categories of the words, in thesyntactic relations between the words, and in how rigidly the individual words areused together.
For example, in some cases, the words of a collocation must be adjacent,as in sentences 1-5 above, while in others they can be separated by a varying number ofother words.
Unfortunately, with few exceptions (e.g., Benson, Benson, and Ilson 1986a)collocations are generally unavailable in compiled form.
This creates a problem forpersons not familiar with the sublanguage 1 as well as for several machine applicationssuch as language generation.In this paper we describe a set of techniques for automatically retrieving suchcollocations from naturally occurring textual corpora.
These techniques are based onstatistical methods; they have been implemented in a tool, Xtract, which is able toretrieve a wide range of collocations with high performance.
Preliminary results ob-tained with parts of Xtract have been described in the past (e.g., Smadja and McKeown1990); this paper gives a complete description of the system and the results obtained.1 This is true for laymen and also for non-native speakers familiar with the domain but not familiar withthe English expressions.144Frank Smadja Retrieving Collocations from Text: Xtract"Our firm made/did a deal with them""The swimmer had/got acramp""Politicians are always on/in the firing lane""These decisions are to be made/taken rapidly""The children usually set/lay the table""You have to break in/run in your new car"Figure 1British English or American English?
from Benson (1990).sentences candidates"If a fire breaks out, the alarm will ??
""The boy doesn't know how to ?
?
his bicycle""The American congress can ??
a presidential veto""Before ating your bag of microwavable popcorn,you have to ?
?
it""ring, go off, sound, start""drive, ride, conduct""ban~cancel~delete~reject""turn down~abrogate~overrule""cook/nuke/broil/fry/bake"Figure 2Fill-in-the-blank test, from Benson (1990).Xtract now works in three stages.
In the first stage, pairwise lexical relations are re-trieved using only statistical information.
This stage is comparable to Church andHanks (1989) in that it evaluates a certain word association between pairs of words.As in Church and Hanks (1989), the words can appear in any order and they canbe separated by an arbitrary number of other words.
However, the statistics we useprovide more information and allow us to have more precision in our output.
The out-put of this first stage is then passed in parallel to the next two stages.
In the secondstage, multiple-word combinations and complex expressions are identified.
This stageproduces output comparable to that of Choueka, Klein, and Neuwitz (1983); howeverthe techniques we use are simpler and only produce relevant data.
Finally, by com-bining parsing and statistical techniques the third stage labels and filters collocationsretrieved at stage one.
The third stage has been evaluated to raise the precision ofXtract from 40% to 80% with a recall of 94%.Section 2 is an introductory section on collocational knowledge, Section 3 describesthe type of collocations that are retrieved by Xtract, and Section 4 briefly surveys re-lated efforts and contrasts our work to them.
The three stages of Xtract are then in-troduced in Section 5 and described respectively in Sections 6, 7, and 8.
Some resultsobtained by running Xtract on several corpora are listed and discussed in Section 9.Qualitative and quantitative evaluations of our methods and of our results are dis-cussed in Sections 10 and 11.
Finally, several possible applications and tasks for Xtractare discussed in Section 12.2.
What Are Col locations?There has been a great deal of theoretical nd applied work related to collocationsthat has resulted in different characterizations (e.g., Allerton 1984; Cruse 1986; Mel'~uk1981).
Depending on their interests and points of view, researchers have focused ondifferent aspects of collocations.
One of the most comprehensive d finition that has145Computational Linguistics Volume 19, Number 1been used can be found in the lexicographic work of Benson and his colleagues (Benson1990).
The definition is the following:DefinitionA collocation is an arbitrary and recurrent word combination (Benson 1990).This definition, however, does not cover some aspects and properties of colloca-tions that have consequences for a number of machine applications.
For example, ithas been shown that collocations are difficult to translate across languages--this factobviously has a direct application for machine translation.
Many properties of col-locations have been identified in the past; however, the tendency was to focus on arestricted type of collocation.
In this section, we present four properties of collocationsthat we have identified and discuss their relevance to computational linguistics.2.1 Collocations Are ArbitraryCollocations are difficult o produce for second language learners (Nakhimovsky andLeed 1979).
In most cases, the learner cannot simply translate word-for-word whats/he would say in her/his native language.
As we can see in Table 1, the word-for-word translation of "to open the door" works well in both directions in all five languages.In contrast, translating word-for-word the expression: "to break down~force the door" isa poor strategy in both directions in all five languages.
The co-occurrence of "door"and "open" is an open or free combination, whereas the combination "door" and "breakdown" is a collocation.
Learners of English would not produce "to break down a door"whether their first language is French, German, Italian, Spanish, or Turkish, if theywere not aware of the construct.Figure 1 illustrates disagreements between British English and American English.Here the problem is even finer than in Table I since the disagreement is not across twodifferent languages, but across dialects of English.
In each of the sentences given inthis figure, there is a different word choice for the American (left side) and the BritishEnglish (right side).
The word choices do not correspond to any syntactic or semanticvariation of English but rather to different word usages in both dialects of English.Translating from one language to another equires more than a good knowledgeof the syntactic structure and the semantic representation.
Because collocations arearbitrary, they must be readily available in both languages for effective machine trans-lation.2.2 Collocations Are Domain-DependentIn addition to nontechnical collocations uch as the ones presented before, domain-specific ollocations are numerous.
Technical jargons are often totally unintelligible forthe layman.
They contain alarge number of technical terms.
In addition, familiar wordsseem to be used differently.
In the domain of sailing (Dellenbaugh and Dellenbaugh1990), for example, some words are unknown to the nonfamiliar reader: rigg, jib, andleeward are totally meaningless tothe layman.
Some other combinations apparently donot contain any technical words, but these words take on a totally different meaningin the domain.
For example, a dry suit is not a suit that is dry but a special type ofsuit used by sailors to stay dry in difficult weather conditions.
Similarly a wet suitis a special kind of suit used for several marine activities.
Native speakers are oftenunaware of the arbitrariness of collocations in nontechnical core English; however,this arbitrariness becomes obvious to the native speaker in specific sublanguages.146Frank Smadja Retrieving Collocations from Text: XtracttypeN-adjN-AdjN-AdjSVSVSVV-AdvV-AdvVOVOV-PartVVVVexample"heavy/light \[\] trading/smoker/traffic""high/low \[\] fertility/pressure/bounce""large/small \[\] crowd/retailer/client""index \[\] rose"stock \[\] \[rose, fell, jumped, continued, declined, crashed .... \]""advancers \[\] \[outnumbered, outpaced, overwhelmed, outstripped\]""trade 4=~ actively," mix 4=~ narrowly,""use ~=~ widely," "watch ~=~ closely""posted \[\] gain"momentum \[\] \[pick up, build, carry over, gather, loose, gain\]""take \[\] from," "raise \[\] by," "mix \[\] with""offer to \[acquire, buy"\]"agree to \[acquire, buy"\]Figure 3Some examples of predicative collocations.Linguistically mastering a domain such as the domain of sailing thus requires morethan a glossary, it requires knowledge of domain-dependent collocations.2.3 Collocations Are RecurrentThe recurrent property simply means that these combinations are not exceptions, butrather that they are very often repeated in a given context.
Word combinations such as"to make a decision, to hit a record, to perform an operation" are typical of the language, andcollocations such as "to buy short," "to ease the jib" are characteristic of specific domains.Both types are repeatedly used in specific contexts.2.4 Collocations Are Cohesive Lexical ClustersBy cohesive 2 clusters, we mean that the presence of one or several words of the collo-cations often implies or suggests the rest of the collocation.
This is the property mostlyused by lexicographers when compiling collocations (Cowie 1981; Benson 1989a).
Lexi-cographers use other people's linguistic judgment for deciding what is and what is nota collocation.
They give questionnaires to people such as the one given in Figure 2.
Thisquestionnaire contains entences used by Benson for compiling collocational knowl-edge for the BBI (Benson 1989b).
Each sentence contains an empty slot that can easilybe filled in by native speakers.
In contrast, second language speakers would not findthe missing words automatically but would consider a long list of words having the ap-propriate semantic and syntactic features uch as the ones given in the second column.As a consequence, collocations have particular statistical distributions (e.g., Hal-liday 1966; Cruse 1986).
This means that, for example, the probability that any twoadjacent words in a sample will be "red herring" is considerably larger than the prob-ability of "red" times the probability of "herring."
The words cannot be considered asindependent variables.
We take advantage of this fact to develop a set of statisticaltechniques for retrieving and identifying collocations from large textual corpora.3.
Three Types of CollocationsCollocations come in a large variety of forms.
The number of words involvedas well as the way they are involved can vary a great deal.
Some collocations are2 This notion of cohesion should not be confused with the cohesion as defined by Halliday (Hallidayand Hasan 1976).
Here we are dealing with a more lexical type of cohesion.147Computational Linguistics Volume 19, Number 1"The NYSE's composite index of all its listed common stocks roseNUMBER* to *NUMBER*""On the American Stock Exchange the market value index was upNUMBER* at *NUMBER*""The Dow Jones average of 30 industrials fellNUMBER* points to *NUMBER*""The closely watched index had been down about *NUMBER* points inthe first hour of trading""The average finished the week with a net loss of *NUMBER*"Figure 4Some examples of phrasal templates.very rigid, whereas others are very flexible.
For example, a collocation such as theone linking "to make" and "decision" can appear as "to make a decision," decisions tobe made," "made an important decision," etc.
In contrast, a collocation such as "The NewYork Stock Exchange" can only appear under one form; it is a very rigid collocation,a fixed expression.
We have identified three types of collocations: rigid noun phrases,predicative r lations, and phrasal templates.
We discuss the three types in turn, and givesome examples of collocations.3.1 Predicative RelationsA predicative relation consists of two words repeatedly used together in a similarsyntactic relation.
These lexical relations are the most flexible type of collocation.
Theyare hard to identify since they often correspond to interrupted word sequences in thecorpus.
For example, a noun and a verb will form a predicative relation if they arerepeatedly used together with the noun as the object of the verb.
"Make-decision" is agood example of a predicative relation.
Similarly, an adjective repeatedly modifyinga given noun such as "hostile-takeover" also forms a predicative relation.
Examplesof automatically extracted predicative relations are given in Figure 3.
3 This class ofcollocations is related to Mel'~uk's lexical functions (Mel'~uk 1981), and Benson's L-type relations (Benson, Benson, and Ilson 1986b).3.2 Rigid Noun PhrasesRigid noun phrases involve uninterrupted sequences of words such as "stock market,""foreign exchange," "New York Stock Exchange," The Dow Jones average of 30 industrials.
"They can include nouns and adjectives as well as closed class words, and are similarto the type of collocations retrieved by Choueka (1988) and Amsler (1989).
They arethe most rigid type of collocation.
Examples of rigid noun phrases are: 4 "The NYSE'scomposite index of all its listed common stocks," The NASDAQ composite index for the over thecounter market," levera ged buyout ," "the gross national product," "White House spokesmanMarlin Fitzwater.
"In general, rigid noun phrases cannot be broken into smaller fragments withoutlosing their meaning; they are lexical units in and of themselves.
Moreover, they oftenrefer to important concepts in a domain, and several rigid noun phrases can be used toexpress the same concept.
In the New York Stock Exchange domain, for example, "The3 In the examples, the "\[\]" sign represents a gap of zero, one or several words.
The "4=~" sign means thatthe two words can be in any order.4 All the examples related to the stock market domain have actually been retrieved by Xtract.148Frank Smadja Retrieving Collocations from Text: XtractDow industrials," "The Dow Jones average of 30 industrial stocks," "the Dow Jones industrialaverage," and "The Dow Jones industrials" represent several ways to express a singleconcept.
As we have seen before, these rigid noun phrases do not seem to follow anysimple construction rule, as, for example, the examples given in sentences 6-8 at thebeginning of the paper are all incorrect.3.3 Phrasal TemplatesPhrasal templates consist of idiomatic phrases containing one, several, or no emptyslots.
They are phrase-long collocations.
Figure 4 lists some examples of phrasal tem-plates in the stock market domain.
In the figure, the empty slots must be filled in bya number (indicated by *NUMBER* in the figure).
More generally, phrasal templatesspecify the parts of speech of the words that can fill the empty slots.
Phrasal templatesare quite representative of a given domain and are very often repeated in a rigid wayin a given sublanguage.
In the domain of weather eports, for example, the sentence"Temperatures indicate previous day's high and overnight low to 8 a.m." is actually repeatedbefore each weather eport, sUnlike rigid noun phrases and predicative relations, phrasal templates are specif-ically useful for language generation.
Because of their slightly idiosyncratic structure,generating them from single words is often a very difficult task for a language gener-ator.
As pointed out by Kukich (1983), in general, their usage gives an impression offluency that could not be equaled with compositional generation alone.4.
Related WorkThere has been a recent surge of research interest in corpus-based computational lin-guistics methods; that is, the study and elaboration of techniques using large realtext as a basis.
Such techniques have various applications.
Speech recognition (Bahl,Jelinek, and Mercer 1983) and text compression (e.g., Bell, Witten, and Cleary 1989;Guazzo 1980) have been of long-standing interest, and some new applications arecurrently being investigated, such as machine translation (Brown et al 1988), spellingcorrection (Mays, Damerau, and Mercer 1990; Church and Gale 1990), parsing (Debili1982; Hindle and Rooth 1990).
As pointed out by Bell, Witten, and Cleary (1989), theseapplications fall under two research paradigms: statistical approaches and lexical ap-proaches.
In the statistical approach, language is modeled as a stochastic process andthe corpus is used to estimate probabilities.
In this approach, a collocation is simplyconsidered as a sequence of words (or n-gram) among millions of other possible se-quences.
In contrast, in the lexical approach, a collocation is an element of a dictionaryamong a few thousand other lexical items.
Collocations in the lexicographic meaningare only dealt with in the lexical approach.
Aside from the work we present in thispaper, most of the work carried out within the lexical approach has been done incomputer-assisted l xicography by Choueka, Klein, and Neuwitz (1983) and Churchand his colleagues (Church and Hanks 1989).
Both works attempted to automaticallyacquire true collocations from corpora.
Our work builds on Choueka's, and has beendeveloped contemporarily to Church's.Choueka, Klein, and Neuwitz (1983) proposed algorithms to automatically retrieveidiomatic and collocational expressions.
A collocation, as defined by Choueka, is a se-quence of adjacent words that frequently appear together.
In theory the sequencescan be of any length, but in actuality, they contain two to six words.
In Choueka5 Taken from the daily reports transmitted daily by The Associated Press newswire.149Computational Linguistics Volume 19, Number 1(1988), experiments performed on an 11 million-word corpus taken from the NewYork Times archives are reported.
Thousands of commonly used expressions such as"fried chicken," "casual sex," "chop suey," home run," and "Magic Johnson" were retrieved.Choueka's methodology for handling large corpora can be considered as a first steptoward computer-aided lexicography.
The work, however, has some limitations.
First,by definition, only uninterrupted sequences of words are retrieved; more flexible col-locations uch as "make-decision," in which the two words can be separated by anarbitrary number of words, are not dealt with.
Second, these techniques simply ana-lyze the collocations according to their observed frequency in the corpus; this makesthe results too dependent on the size of the corpus.
Finally, at a more general level,although disambiguation was originally considered as a performance task, the collo-cations retrieved have not been used for any specific computational task.Church and Hanks (1989) describe a different set of techniques to retrieve col-locations.
A collocation as defined in their work is a pair of correlated words.
Thatis, a collocation is a pair of words that appear together more often than expected.Church et al (1991) improve over Choueka's work as they retrieve interrupted as wellas uninterrupted sequences of words.
Also, these collocations have been used by anautomatic parser in order to resolve attachment ambiguities (Hindle and Rooth 1990).They use the notion of mutual information as defined in information theory (Shannon1948; Fano 1961) in a manner similar to what has been used in speech recognition(e.g., Ephraim and Rabiner 1990), or text compression (e.g., Bell, Witten, and Cleary1989), to evaluate the correlation of common appearances of pairs of words.
Theirwork, however, has some limitations too.
First, by definition, it can only retrieve col-locations of length two.
This limitation is intrinsic to the technique used since mu-tual information scores are defined for two items.
The second limitation is that manycollocations identified in Church and Hanks (1989) do not really identify true collo-cations, but simply pairs of words that frequently appear together such as the pairs"doctor-nurse," "doctor-bill," doctor-honorary," "doctors-dentists," "doctors-hospitals," etc.These co-occurrences are mostly due to semantic reasons.
The two words are used inthe same context because they are of related meanings; they are not part of a singlecollocational construct.The work we describe in the rest of this paper is along the same lines of research.
Itbuilds on Choueka's work and attempts to remedy the problems identified above.
Thetechniques we describe retrieve the three types of collocations discussed in Section 2,and they have been implemented in a tool, Xtract.
Xtract retrieves interrupted as wellas uninterrupted sequences of words and deals with collocations of arbitrary length(1 to 30 in actuality).
The following four sections describe and discuss the techniquesused for Xtract.5.
Xtract: IntroductionXtract consists of a set of tools to locate words in context and make statistical observa-tion to identify collocations.
In the upgraded version we describe here, Xtract has beenextended and refined.
More information is computed and an effort has been made toextract more functional information.
Xtract now works in three stages.The three-stage analysis is described in Sections 6, 7, and 8.
In the first stage,described in Section 6, Xtract uses straight statistical measures to retrieve from a corpuspairwise lexical relations whose common appearance within a single sentence arecorrelated.
A pair (or bigram) is retrieved if its frequency of occurrence is above acertain threshold and if the words are used in relatively rigid ways.
The output ofstage one is then passed to both the second and third stage in parallel.
In the second150Frank Smadja Retrieving Collocations from Text: Xtractstage, described in Section 7, Xtract uses the output bigrams to produce collocationsinvolving more than two words (or n-grams).
It analyzes all sentences containing thebigram and the distribution of words and parts of speech for each position around thepair.
It retains words (or parts of speech) occupying a position with probability greaterthan a given threshold.
For example, the bigram "average-industrial" produces the n-gram "the Dow Jones industrial average," since the words are always used withinrigid noun phrases in the training corpus.
In the third stage, described in Section 8,Xtract adds syntactic information to collocations retrieved at the first stage and filtersout inappropriate ones.
For example, if a bigram involves a noun and a verb, thisstage identifies it either as a subject-verb or as a verb-object collocation.
If no suchconsistent relation is observed, then the collocation is rejected.6.
Xtract Stage One: Extracting Significant BigramsAccording to Cruse's definition (Cruse 1986), a syntagmatic lexical relation consistsof a pair of words whose common appearances within a single phrase structure arecorrelated.
In other words, those two words appear together within a single syntacticconstruct more often than expected by chance.
The first stage of Xtract attempts toidentify such pairwise lexical relations and produce statistical information on pairs ofwords involved together in the corpus.Ideally, in order to identify lexical relations in a corpus one would need to firstparse it to verify that the words are used in a single phrase structure.
However,in practice, free-style texts contain a great deal of nonstandard features over whichautomatic parsers would fai l .
6 Fortunately, there is strong lexicographic evidence thatmost syntagmatic lexical relations relate words separated by at most five other words(Martin, A1, and Van Sterkenburg 1983).
In other words, most of the lexical relationsinvolving a word w can be retrieved by examining the neighborhood of w, whereverit occurs, within a span of five ( -5  and +5 around w) words.
7 In the work presentedhere, we use this simplification and consider that two words co-occur if they are in asingle sentence and if there are fewer than five words between them.In this first stage, we thus use only statistical methods to identify relevant pairs ofwords.
These techniques are based on the assumptions that if two words are involvedin a collocation then:?
the words must appear together significantly more often than expectedby chance.?
because of syntactic onstraints the words should appear in a relativelyrigid way.
8These two assumptions are used to analyze the word distributions, and we base ourfiltering techniques on them.6.1 Presentation of the MethodIn this stage as well as in the two others, we often need part-of-speech information forseveral purposes.
Stochastic part-of-speech taggers uch as those in Church (1988) and6 This fact is being seriously challenged by current research (e.g., Abney 1990; Hindle 1983), and mightnot be true in the near future.7 Not crossing sentence boundaries.8 This is obviously not true for nonconfigurational languages.
Although we do believe that the methodsdescribed in this paper can be applied to many languages, we have only used them on English texts.151Computational Linguistics Volume 19, Number 1Garside and Leech (1987) have been shown to reach 95-99% performance on free-styletext.
We preprocessed the corpus with a stochastic part-of-speech tagger developed atBell Laboratories by Ken Church (Church 1988).
9In the rest of this section, we describe the algorithm used for the first stage ofXtract in some detail.
We assume that the corpus is preprocessed by a part of speechtagger and we note wi a collocate of w if the two words appear in a common sentencewithin a distance of 5 words.Step 1.1: Producing ConcordancesInput: The tagged corpus, a given word w.Output: All the sentences containing w.Description: This actually encompasses the task of identifying sentence boundaries,and the task of selecting sentences containing w. The first task is not simple and is stillan open problem.
It is not enough to look for a period followed by a blank space as,for example, abbreviations and acronyms such as S.B.F., U.S.A., and A.T.M.
often posea problem.
The basic algorithm for isolating sentences i described and implementedby a finite-state recognizer.
Our implementation could easily be improved in manyways.
For example, it performs poorly on acronyms and often considers them as endof sentences; giving it a list of currently used acronyms uch as N.B.A., E.I.K., etc.,would significantly improve its performance.Step 1.2: Compile and SortInput: Output of Step 1.1, i.e., a set of tagged sentences containing w.Output: A list of words wi with frequency information on how w and wi co-occur.This includes the raw frequency as well as the breakdown into frequencies for eachpossible position.
See Table 2 for example outputs.Description: For each input sentence containing w, we make a note of its collocatesand store them along with their position relative to w, their part of speech, and theirfrequency of appearance.
More precisely, for each prospective l xical relation, or foreach potential collocate wi, we maintain a data structure containing this information.The data structure is shown in Figure 5.
It contains freqi, the frequency of appearanceof wi with w so far in the corpus, PP, the part of speech of wi, and p~, (-5 _< j < 5,j ~ 0), the frequency of appearance of wi with w such that they are j words apart.The p~s represent the histogram of the frequency of appearances of w and wi in givenpositions.
This histogram will be used in later stages.As an example, if sentence (9) is the current input to step 1.2 and w = takeover,then, the prospective l xical relations identified in sentence (9) are as shown in Table 3.9.
"The pill would make a takeover attempt more expensive by allowing the retailer'sshareholders to .
.
.
"In Table 3, distance is the distance between "takeover" and wi, and PP is the partof speech of wi.
The closed class words are not considered at this stage and the other9 We are grateful to Ken Church and to Bell Laboratories for providing us with this tool.152Frank Smadja Retrieving Collocations from Text: Xtract(w, wl ) freql, PP1(w, w2) freq2, PP2(w, wi ) freqi, PPibigram Freq, PPf, (7"p-5 p-4 p-3 p-2 p-1 p~ p2 p3 p4 p5V~Figure 5Data structure maintained atstage one by Xtract.words, such as "shareholders," are rejected because they are more than five words awayfrom "takeover."
For each of the above word pairs, we maintain the associated atastructure as indicated in Figure 5.
For takeover pill, for example, we would incrementfreqpill, and the p4 column in the histogram.
Table 2 shows the output for the adjectivecollocates of the word "takeover.
"Step 1.3: AnalyzeInput: Output of Step 1.2, i.e., a list of words wi with information on how often andhow w and wi co-occur.
See Table 2 for an example input.Output: Significant word pairs, along with some statistical information describing howstrongly the words are connected and how rigidly they are used together.
A separate(but similar) statistical analysis is done for each syntactic ategory of collocates.
SeeTable 4 for an example output.Description: At this step, the statistical distribution of the collocates of w is analyzed,and the interesting word pairs are automatically selected.
If part of speech informationis available, a separate analysis is made depending on the part of speech of the collo-cates.
This balances the fact that verbs, adjectives, and nouns are simply not equallyfrequent.For each word w, we first analyze the distribution of the frequencies freqi of itscollocates wi, and then compute its average frequency f and standard eviation craround f. We then replace freqi by its associated z -score  ki.
ki is called the strength ofthe word pair in Figure 4; it represents he number of standard eviation above the153Computational Linguistics Volume 19, Number 1Table 2Output of stage 1, step 3.
Noun-adjective associations.W Wi Freq p-s p-4 p-3 p-2 p-1 pl p2 p3 p4 p5takeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeovertakeoverpossiblecorporateunsolicitedseveralrecentnewunwantedexpensivepotentialbigfriendlyunsuccessfulbiggestlargestoldunfriendlyrivalinadequateinitialunwelcomepreviousfederalbitterstronghostileattractiveunfair178 0 13 4 23 138 0 0 0 0 093 2 2 2 1 63 3 2 9 4 583 5 30 5 0 42 0 0 1 0 081 2 6 6 6 45 0 0 12 0 476 5 4 6 5 17 0 0 36 2 175 4 3 6 28 27 0 1 4 2 053 5 0 0 2 46 0 0 0 0 052 1 0 0 0 2 0 23 23 3 050 1 0 1 3 42 0 0 0 2 147 0 0 0 4 15 0 0 5 21 241 0 3 3 1 25 0 0 2 3 440 0 1 5 6 27 0 0 0 0 135 1 2 1 4 20 0 0 0 5 232 0 1 3 20 3 0 0 0 0 528 0 8 6 0 14 0 0 0 0 026 0 0 0 0 18 0 0 0 0 826 0 1 3 0 3 0 8 5 5 126 5 10 2 0 0 0 0 9 0 025 0 6 0 0 13 0 0 4 0 224 4 0 0 0 20 0 0 0 0 024 0 2 0 4 18 0 0 0 0 022 4 2 2 0 0 0 2 2 8 222 0 0 0 7 14 0 0 0 1 019 0 4 3 5 4 0 0 1 0 216 0 6 0 0 10 0 0 0 0 016 1 0 5 3 7 0 0 0 0 013 0 0 0 0 13 0 0 0 0 0Table 3The collocates of "takeover" as retrievedfrom sentence (9).w wi distance PPtakeover pill 4 Ntakeover make 2 Vtakeover attempt -1 Ntakeover expensive -3 Jtakeover allowing -5 Vaverage of the f requency of the word  pair  w and  wi and is def ined as:ki - freqi - f ( la)O"Then, we  ana lyze  the d is t r ibut ion  of the p)s and produce  their  average \]~i and var ianceUi around \]~i.
In F igure 4 spread represents  Ui on a scale of 1 to 100.
Ui character izesthe shape of the ~ h istogram.
If Ui is small ,  then the h i s togram wi l l  tend to be flat,154Frank Smadja Retrieving Collocations from Text: Xtraetwhich means that wi can be used equivalently in almost any position around w. Incontrast, if Ui is large, then the histogram will tend to have peaks, which means thatwi can only be used in one (or several) specific position around w. Ui is defined by:10 (lb)These analyses are then used to sort out the retrieved ata.
First, using (la), collo-cates with strength smaller than a given threshold/co are eliminated.
Then, using (lb),we filter out the collocates having a variance Ui smaller than a given threshold U0.Finally, we keep the interesting collocates by pulling out the peaks of the p'j distri-butions.
These peaks correspond to the js such that the z-score of p~j is bigger than agiven threshold kl.
These thresholds have to be determined by the experimenter andare dependent on the use of the retrieved collocations.
As described in Smadja (1991),for language generation we found that (k0, kl, U0) = (1, 1, 10) gave good results, but forother tasks different hresholds might be preferable.
In general, the lower the thresh-old the more data are accepted, the higher the recall, and the lower the precision ofthe results.
Section 10 describes an evaluation of the results produced with the abovethresholds.More formally, a peak, or lexical relation containing w, at this point is defined asa tuple (wi, distance, strength, spread,j) verifying the following set of inequalities:(c)strength = #eq~-f > ko (C1) }spread >_ Uo (C2)P~ ~ \]~i q- (kl x v/-~) (C3)Some example results are given in Table 4.As shown in Smadja (1991), the whole first stage of Xtract as described above canbe performed in O(S log S) time, in which S is the size of the corpus.
The third step ofcounting frequencies and maintaining the data structure dominates the whole processand as pointed out by Ken Church (personal communication), it can be reduced to asorting problem.6.2 What Exactly Is Filtered Out?The inequality set (C) is used to filter out irrelevant data, that is pairs of words sup-posedly not used consistently within a single syntactic structure.
This section discussesthe importance of each inequality in (C) on the filtering process.strength - freq - f  >_ ko (C1)(YCondition (C1) helps eliminate the collocates that are not frequent enough.
This con-dition specifies that the frequency of appearance of wi in the neighborhood of w mustbe at least one standard eviation above the average.
In most statistical distributions,this thresholding eliminates the vast majority of the lexical relations.
For example,for w = "takeover," among the 3385 possible collocates only 167 were selected, whichgives a proportion of 95% rejected.
In the case of the standard normal distribution, thiswould reject some 68% of the cases.
This indicates that the actual distribution of the155Computational Linguistics Volume 19, Number 1Table 4Output of stage 1, step 4.wi w~ distance strength spreadhostile takeovers 1 13 97hostile takeover 1 13 90corporate takeovers 1 8 90possible takeover 1 6 73hostile takeovers 2 2 70corporate takeover 1 3 63unwanted takeover 1 1 83potential takeover 1 1 80several takeover 1 2 50unsolicited takeover 1 2 53his takeover 1 3 44unsuccessful takeover 1 1 63takeover recent 3 2 46unsolicited takeover 4 2 53takeover last 2 2 46friendly takeover 1 1 60takeover expensive 3 1 60takeover expensive 2 1 60new takeover 2 2 46new takeover 1 2 46takeover big 4 1 47takeovers other 2 1 43big takeover 1 1 46takeovers major 4 1 46biggest takeover 1 .93 53largest takeover 2 .82 60collocates of "takeover" has a large kurtosis.
1?
Among the eliminated collocates were"dormant, dilute, ex., defunct," which obviously are not typical of a takeover.
Althoughthese rejected collocations might be useful for applications uch as speech recognition,for example, we do not consider them any further here.
We are looking for recurrentcombinations and not casual ones.spread >>_ Uo (C2)Condit ion (C2) requires that the histogram of the 10 relative frequencies of appearanceof wi within five words of w (or p}s) have at least one spike.
If the histogram isflat, it will be rejected by this condition.
For example, in Figure 5, the histogramassociated with w2 would be rejected, whereas the one associated with Wl or wi wouldbe accepted.
In Table 2, the histogram for "takeover-possible" is clearly accepted (thereis a spike for p- l ) ,  whereas the one for "takeover-federal" is rejected.
The assumptionhere is that, if the two words are repeatedly used together within a single syntacticconstruct, then they will have a marked pattern of co-appearance, i.e., they will notappear in all the possible positions with an equal probability.
This actually eliminatespairs such as "telephone-television," "bomb-soldier, .
.
.
.trouble-problem," "big-small," and10 The kurtosis of the distribution of the collocates probably depends on the word, and there is currentlyno agreement on the type of distribution that would describe them.156Frank Smadja Retrieving Collocations from Text: Xtract"doctor-nurse" where the two words co-occur with no real structural consistency.
Thetwo words are often used together because they are associated with the same contextrather than for pure structural reasons.
Many collocations retrieved in Church andHanks (1989) were of this type, as they retrieved octors-dentists, doctors-nurses, doctor-bills, doctors-hospitals, nurses-doctor, etc., which are not collocations in the sense definedabove.
Such collocations are not of interest for our purpose, although they could beuseful for disambiguation r other semantic purposes.
Condition (C2) filters out exactlythis type of collocations.p~ ~ \]~i q- (kl x V~i) (C3)Condition (C3) pulls out the interesting relative positions of the two words.
Conditions(C2) and (C1) eliminate rows in the output of Step 1.2.
(See Figure 2).
In contrast,Condition (C3) selects columns from the remaining rows.
For each pair of words,one or several positions might be favored and thus result in several PI selected.
Forexample, the pair "expensive-takeover" p oduced two different peaks, one with only oneword in between "expensive" and "takeover," and the other with two words.
Examplesentences containing the two words in the two possible positions are:" The provision is aimed at making ahostile takeover prohibitively expensive byenabling Borg Warner's tockholders to buy the.
.
.
""The pill would make a takeover attempt more expensive by allowing theretailer's hareholders tobuy more company stock..."Let us note that this filtering method is an original contribution of our work.Other works such as Church and Hanks (1989) simply focus on an evaluation of thecorrelation of appearance ofa pair of words, which is roughly equivalent to condition(C1).
(See next section).
However, taking note of their pattern of appearance allows usto filter out more irrelevant collocations with (C2) and (C3).
This is a very importantpoint that will allow us to filter out many invalid collocations and also produce morefunctional information at stages 2 and 3.
A graphical interpretation of the filteringmethod used for Xtract is given in Smadja (1991).7.
Xtract Stage Two: From 2-Grams to N-GramsThe role of the second stage of Xtract is twofold.
It produces collocations involvingmore than two words, and it filters out some pairwise relations.
Stage 2 is related tothe work of Choueka (1988), and to some extent o what has been done in speechrecognition (e.g., Bahl, Jelinek, and Mercer 1983; Merialdo 1987; Ephraim and Rabiner1990).7.1 Presentation of the MethodIn this second stage, Xtract uses the same components used for the first stage but ina different way.
It starts with the pairwise lexical relations produced in stage 1 andproduces multiple word collocations, uch as rigid noun phrases or phrasal templates,from them.
To do this, Xtract studies the lexical relations in context, which is exactlywhat lexicographers do.
For each bigram identified at the previous tage, Xtract ex-amines all instances of appearance of the two words and analyzes the distributions ofwords and parts of speech in the surrounding positions.Input: Output of Stage 1.
Similar to Table 4, i.e., a list of bigrams with their statisticalinformation as computed in stage 1.157Computational Linguistics Volume 19, Number 1Output: Sequences of words and parts of speech.
See Figure 8.Stage 2 has three steps:Step 2.1: Produce ConcordancesIdentical to Stage 1, Step 1.1.
Given a pair of words w and wi, and an integer specifyingthe distance of the two words, n This step produces all the sentences containing themin the given position.
For example, given the bigram takeover-thwart and the distance2, this step produces entences like:"Under the recapitalization plan it proposed to thwart the takeover.
"Step 2.2: Compile and SortIdentical to Stage 1, Step 1.2.
We compute the frequency of appearance of each of thecollocates of w by maintaining a data structure similar to the one given in Figure 5,Step 2.3: Analyze and FilterInput: Output of Step 2.2.Output: N-grams such as in Figure 8.Discussion: Here, the analyses are simpler than for Stage 1.
We are only interested inpercentage frequencies and we only compute the moment of order 1 of the frequencydistributions.Tables produced in Step 2.2 (such as in Figure 5) are used to compute the frequencyof appearance of each word in each position around w. For each of the possible relativedistances from w, we analyze the distribution of the words and only keep the wordsoccupying the position with a probability greater than a given threshold T. 12 If partof speech information is available, the same analysis is also performed with parts ofspeech instead of actual words.
In short, a word w or a part of speech pos is kept inthe final n-gram at position i if and only if it satisfies the following inequation:p(word\[i\] = Wo) > T (4a)p(e) denotes the probability of event e. Consider the examples given in Figures 6and 7 that show the concordances (output of step 2.1) for the input pairs: "average-industrial" and "index-composite.
"In Figure 6, the same words are always used from position -4  to position 0.However, at position +1, the words used are always different.
"Dow" is used at position-3  in more than 90% of the cases.
It is thus part of the produced rigid noun phrases.But "down" is only used a couple of times (out of several hundred) at position +1,11 The distance isactually optional and can be given in various ways.
We can specify the word order, themaximum distance, the exact distance, tc.12 This threshold must also be determined by the experimenter.
In the following we use T = 0.75.
Asdiscussed previously, the choice of the threshold isarbitrary, and the general rule is that the lower thethreshold, the higher the recall and the lower the precision of the results.
The choice of 0.75 is based onthe manual observations of several samples and it has effected the overall results, as discussed inSection 10.158Frank Smadja Retrieving Collocations from Text: XtractConcordances for: "average" industrial"Tuesday the Dow Jones industrial averageThe Dow Jones industrial average... that sent the Dow Jones industrial averageMonday the Dow Jones industrial averageThe Dow Jones industrial average... in the Dow Jones industrial averagerose 26.28 points to 2 304.69.went up 11.36 points today.down sharply ...showed some strength as ...was down 17.33 points to 2,287.36 ...was the biggest since ...=~ "the Dow Jones industrial average"Figure 6Producing: "the Dow Jones industrial average"Concordances for "composite index"The NYSE s composite indexThe NYSE s composite indexThe NYSE s composite indexThe NYSE s composite indexThe NYSE s composite indexThe NYSE s composite indexThe NYSE s composite indexThe NYSE s composite indexThe NYSE s composite indexof all its'l'isted common stocks fell 1.76 to 164.13.of all its listed common stocks fell 0.98 to 164.91.of all its listed common stocks fell 0.96 to 164.93.of all its listed common stocks fell 0.91 to 164.98.of all its listed common stocks rose 1.04 to 167.08.of all its listed common stocks rose 0.76of all its listed common stocks rose 0.50 to 166.54.of all its listed common stocks rose 0.69 to 166.73.of all its listed common stocks fell 0.33 to 170.63.
"the NYSE's composite index of all its listed common stocksFigure 7Producing: "the NYSE's composite index of all its listed common stocks"and will not be part of the produced rigid noun phrases.
From those concordances,Xtract produced the five-word rigid noun phrases: "The Dow Jones Industrial Average.
"Figure 7 shows that from position -3  to position +7 the words used are alwaysthe same.
In all the example sentences in which "composite" and "index" are adjacent,the two words are used within a bigger construct of 11 words (also called an 11-gram).However, if we look at position +8 for example, we see that although the words usedare different, in all the cases they are verbs.
Thus, after the 11-gram we expect o finda verb.
In short, Figure 7 helps us produce both the rigid noun phrases "The NYSE'scomposite index of all its listed common stocks," as well as the phrasal template "The NYSE'scomposite index of all its listed common stocks *VERB* *NUMBER* to *NUMBER*.
"Figure 8 shows some sample phrasal templates and rigid noun phrases that wereproduced at this stage.
The leftmost column gives the input lexical relations.
Someother examples are given in Figure 3.7.2 DiscussionThe role of stage 2 is to filter out many lexical relations and replace them by validones.
It produces both phrasal templates and rigid noun phrases.
For example, asso-ciations such as "blue-stocks, " "air-controller," or "advancing-market" were filtered out159Computational Linguistics Volume 19, Number 1lexical relationcomposite-indexcomposite-index"close-industrial"collocation"The NYSE's composite index of all its listed commonstocks fell *NUMBER* to *NUMBER*""the NYSE's composite index of all its listed commonstocks rose *NUMBER* to *NUMBER*.
""Five minutes before the close the Dow Jones average of 30 industrialswas up/down *NUMBER* to/from *NUMBER*""average industrial""advancing-market""block-trading""blue-stocks""cable-television""consumer index""the Dow Jones industrial average.
"the broader market in the NYSE advancing issues""Jack Baker head of block trading in Shearson Lehman Brothers Inc.""blue chip stocks""cable television""The consumer p ice index"Figure 8Example output collocations of stage two.and respectively replaced by: "blue chip stocks," air traffic controllers," and "the broadermarket in the NYSE advancing issues.
"Thus stage 2 produces n-word collocations from two-word associations.
Producingn-word collocations has already been done (e.g., Choueka 1988).
13 The general methodused by Choueka is the following: for each length n, (1 < n < 6), produce all the wordsequences of length n and sort them by frequency.
On a 12 million-word corpus,Choueka retrieved 10 collocations of length six, 115 collocations of length five, 1,024collocations of length four, 4,777 of length three, and some 15,973 of length two.
Thethreshold imposed was 14.
The method we presented in this section has three mainadvantages when compared to a straight n-gram method like Choueka's.1.
Stage 2 retrieves phrasal templates in addition to simple rigid nounphrases.
Using part of speech information, we allow categories andwords in our templates, thus retrieving a more flexible type ofcollocation.
It is not clear how simple n-gram techniques could beadapted to obtain the same results.2.
Stage 2 gets rid of subsumed m-grams of a given n-gram (m < n).
Sincestage 2 works from bigrams, and produces the biggest n-gram containingit, there is no m-gram (m < n) produced that is subsumed by it.
Forexample, although "shipments of arms to Iran" is a collocation of lengthfive, "arms to Iran" is not an interesting collocation.
It is not opaque, anddoes not constitute a modifier-modified syntactic relation.
A straightn-gram method would retrieve both, as well as many other subsumedm-grams, such as "of arms to Iran."
A sophisticated filtering methodwould then be necessary to eliminate the invalid ones (See Choueka1988).
Our method avoids this problem and only produces the biggestpossible n-gram, namely: "shipment of arms to Iran."3.
Stage 2 is a simple way of compiling n-gram data.
Retrieving an 11-gramby the methods used in speech, for example, would require a great deal13 Similar approaches have been done for several pplications such as Bahl, Jelinek, and Mercer (1983) andCerf-Danon et al (1989) for speech recognition, and Morris and Cherry (1975), Angell (1983), Kukich(1990), and Mays, Damerau, and Mercer (1990) for spelling correction (with letters instead of words).160Frank Smadja Retrieving Collocations from Text: Xtractof CPU time and space.
In a 10 million-word corpus, with about 60,000different words, there are about 3.6 x 109 possible bigrams, 2.16 x 1014trigrams, and 3 x 1033 7-grams.
This rapidly gets out of hand.
Choueka,for example, had to stop at length six.
In contrast, he rigid noun phraseswe retrieve are of arbitrary length and are retrieved very easily and inone pass.
The method we use starts from bigrams and produces thebiggest possible subsuming n-gram.
It is based on the fact that if ann-gram is statistically significant, hen the included bigrams must also besignificant.
For example, to identify "The Dow Jones average of 30industrials," a traditional n-gram method would compare it to the other7-grams and determine that it is significant.
In contrast, we start from anincluded significant bigram (for example, "Dow-30") and we directlyretrieve the surrounding n-grams.
148.
Xtract Stage Three: Adding Syntax to the CollocationsThe collocations as produced in the previous tages are already useful for lexicography.For computational use, however, functional information is needed.
For example, thecollocations should have some syntactic properties.
It is not enough to say that "make"goes with "decision"; we need to know that "decision" is used as the direct object ofthe verb.The advent of robust parsers such as Cass (Abney 1990) and Fidditch (Hindle1983) has made it possible to process large text corpora with good performance andthus combine statistical techniques with more symbolic analysis.
In the past, somesimilar attempts have been done.
Debili (1982) parsed corpora of French texts to iden-tify nonambiguous predicate argument relations.
He then used these relations fordisambiguation.
Hindle and Rooth (1990) later refined this approach by using bigramstatistics to enhance the task of prepositional phrase attachment.
Church et al (1989,1991) have yet another approach; they consider questions uch as what does a boat typ-ically do?
They are preprocessing a corpus with the Fidditch parser (Hindle 1983) inorder to produce a list of verbs that are most likely associated with the subject "boat.
"Our goal here is different, as we analyze collocations automatically produced bythe first stage of Xtract to either add syntactic information or reject hem.
For example,if a lexical relation identified at stage 1 involves a noun and a verb, the role of stage3 is to determine whether it is a subject-verb or a verb-object collocation.
If no suchconsistent relation is observed, then the collocation is rejected.
Stage 3 uses a parserbut it does not require a complete parse tree.
Given a number of sentences, Xtract onlyneeds to know pairwise syntactic (modifier-modified) relations.
The parser we usedin the experiment reported here is Cass (Abney 1989, 1990), a bottom-up incrementalparser.
Cass 15 takes input sentences labeled with part of speech and attempts o identifysyntactic structure.
One of the subtasks performed by Cass is to identify predicateargument relations, and this is the task we are interested in here.
Stage 3 works in thefollowing three steps.14 Actually, this 7-gram could be retrieved several times, one for each pair of open class word it contains.But a simple sorting algorithm gets rid of such repetitions.15 The parser developed at Bell Communication Research by Steve Abney, Cass stands for CascadedAnalysis of Syntactic Structure.
We are grateful to Steve for helping us with the use of Cass andcustomizing its output for us.161Computational Linguistics Volume 19, Number 1label bigramVO faced testSV investors awaitedNN year marketNN stock tradersJN old marketJN last sellofflabel bigramVO awaited signsSV Street facedNN week selloffNN bull marketJN major testJN epic selloffFigure 9All the syntactic labels produced by Cass on sentence (10).Step 3.1: Produce Tagged ConcordancesIdentical to what we did at Stage 2, Step 2.1.
Given a pair of words w and wi, adistance of the two words (optional), and a tagged corpus, Xtract produces all the(tagged) sentences containing them in the given position specified by the distance.Step 3.2: ParseInput: Output of Step 3.1.
A set of tagged sentences each containing both w and wi.Output: For each sentence, a set of syntactic labels such as those shown in Figure 9.Discussion: Cass is called on the concordances.
From Cass output, we only retrievebinary syntactic relations (or labels) such as "verb-object" or "verb-subject," noun-adjective," and "noun-noun."
To simplify, we abbreviate them respectively: VO, SV, NJ,NN.
For sentence (10) below, for example, the labels produced are shown in Figure 9.10.
"Wall Street faced a major test with stock traders returning to action for the firsttime since last week's epic selloff and investors awaited signs of life from the5-year-old bull market.
"Step 3.3: Label SentencesInput: A set of sentences each associated with a set of labels as shown in Figure 9.Output: Collocations with associated syntactic labels as shown in Figure 10.Discussion: For any given sentence containing both w and wi, two cases are possible:either there is a label for the bigram (w, wi), or there is none.
For example, for sen-tence (10), there is a syntactic label for the bigram faced-test, but there is none for thebigram stock-returning.
Faced-test enters into a verb object relation, and stock-returningdoes not enter into any type of relation.
If no label is retrieved for the bigram, itmeans that the parser could not identify a relation between the two words.
In thiscase we introduce a new label: U (for undefined) to label the bigram.
At this point,we associate with the sentence the label for the bigram (w, wi).
With each of the inputsentences, we associate a label for the bigram (w, wi).
For example, the label associatedwith sentence (10) for the bigram faced-test would be VO.
A list of labeled sentencesfor the bigram w = "rose" and wi = "prices" is shown in Figure 10.162Frank Smadja Retrieving Collocations from Text: XtractSome Concordances for (rose, prices) label... when they rose pork prices 1.1 percent ... VOAnalysts said stock prices rose because of a rally in Treasury bonds.
SVBond prices rose because many traders took the report as a signal ... SVStock prices rose in moderate trading today with little news ... SVBond prices rose in quiet trading SVStock prices rose sharply Friday in response to a rally in ... SV... soft drink prices rose 0.5 percent ... SVStock prices rose broadly in early trading today as a rising dollar ... SVFigure 10Producing the "prices \[\] rose," SV predicative relation at stage 3.Step 3.4: Filter and Label CollocationInput: A set of sentences containing w and wi each associated with a label as shownin Figure 10.Output: Labeled collocations as shown in Figure 11.Discussion on Step 3.4: At this step, we count the frequencies of each possible labelidentified for the bigram (w} wi) and perform a statistical analysis of order two for thisdistribution.
We compute the average frequency for the distribution of labels: ~ andthe standard eviation crt.
We finally apply a filtering method similar to (C2).
Let t bea possible label.
We keep t if and only if it satisfies inequality (4b) similar to (4a) givenbefore:p(labelIi \] = t) ~ T (4b)A collocation is thus accepted if and only if it has a label g satisfying inequality(4b), and g # U.
Similarly, a collocation is rejected if no label satisfies inequality (4b)or if U satisfies it.Figure 10 shows part of the output of Step 3.3 for w = "rose" and wi = "prices."
Asshown in the figure, SV labels are a large majority.
Thus, we would label the relationprice-rose as an SV relation.
An example output of this stage is given in Figure 11.The bigrams labeled U were rejected at this stage.Stage 3 thus produces very useful results.
It filters out collocations and rejectsmore than half of them, thus improving the quality of the results.
It also labels thecollocations it accepts, thus producing a more functional and usable type of knowledge.For example, if the first two stages of Xtract produce the collocation "make-decision,"the third stage identifies it as a verb-object collocation.
If no such relation can beobserved, then the collocation is rejected.
The produced collocations are not simpleword associations but complex syntactic structures.
Labeling and filtering are twouseful tasks for automatic use of collocations as well as for lexicography.
The wholeof stage 3 (both as a filter and as a labeler) is an original contribution of our work.Retrieving syntactically labeled collocations is a relatively new concern.
Moreover,filtering greatly improves the quality of the results.
This is also a possible use of theemerging new parsing technology.8.1 Xtract: The ToolkitXtract is actually a library of tools implemented using standard C-Unix libraries.
Thetoolkit has several utilities useful for analyzing corpora.
Without making any effort163Computational Linguistics Volume 19, Number 1W W isavingssavingssavingssavingssavingssavingssavingsmanufacturingmanufacturingsecuritieslabelailing Uappears Ucontinue Udip Udipped Ufailing Ufell SVsector NNsector NNbusiness NNFigure 11Some examples of syntactically labeled bigrams.W W isecuritiesdenominatedsecuritiessecuritiessecuritiessecuritiessecuritiessecuritiessecuritiessecuritiesdealersecuritiesfirmsfixedfraudindustrylawlawmakerslawyerlawyerslabelUVONNUNNNNNNUNNNNto make Xtract efficient in terms of computing resources, the first stage as well as thesecond stage of Xtract only takes a few minutes to run on a ten-megabyte (pre-tagged)corpus.
Xtract is currently being used at Columbia University for various lexical tasks.And it has been tested on many corpora, among them: several ten-megabyte corporaof news stories, a corpus, consisting of some twenty megabytes of New York Timesarticles, which has already been used by Choueka (1988), the Brown corpus (Francisand Ku~era 1982), a corpus of the proceedings of the Canadian Parliament, also calledthe Hansards corpus, which amounts to several hundred megabytes.
We are currentlyworking on packaging Xtract to make it available to the research community.
Thepackaged version will be portable, reusable, and faster than the one we used to writethis paper.
16We evaluate the filtering power of stage 3 in the evaluation section, Section 10.Section 9 presents ome results that we obtained with the three stages of Xtract.9.
Some ResultsResults obtained from The Jerusalem Post corpus have already been reported (e.g.,Smadja 1991).
Figure 12 gives some results for the three-stage process of Xtract ona 10 million-word corpus of stock market reports taken from the Associated Pressnewswire.
The collocations are given in the following format.
The first line containsthe bigrams with the distance, so that "sales fell -1"  says that the two words underconsideration are "sales" and "fell," and that the distance we are considering is -1.The first line is thus the output of stage 1.
The second line gives the output of stage2, i.e., the n-grams.
For example, "takeover-thwart" is retrieved as "44 .
.
.
.
.
to thwart ATtakeover NN .
.
.
.
.
.
.  "
AT stands for article, NN stands for nouns, and 44 is the numberof times this collocation has been retrieved in the corpus.
The third line gives theretrieved tags for this collocation, so that the syntactic relation between "takeover" and"thwart" is an SV relation.
And finally, the last line is an example sentence containingthe collocation.
Output of the type of Figure 12 is automatically produced.
This kindof output is about as far as we have gone automatically.
Any further analysis and/oruse of the collocations would probably require some manual intervention.16 Please contact he author if you are interested in getting a copy of the software.164Frank Smadja Retrieving Collocations from Text: Xtractsales fell -1158 .
.
.
.
.
.
.
sales fell .
.
.
.
158TAG: SV34New home sales fell 2.7 percent in February following an 8.6 percent drop in Januarythe Commerce Department reported.study said -140 .
.
.
.
.
.
.
AT study said .
.
.
.
.
.
40TAG: SV56A private study said Americans are eating about the same amount of red meat they didfour years ago.sense makes 126 .
.
.
.
.
.
makes sense .
.
.
.
26TAG: VO20 19Murray Drabkin of Washington lawyer for the Dalkon Shield claimants committee saidnow that Robins has agreed it makes sense to sell the company we are finally downto the real questions How much will the company bring in the open market andhow much of that amount will the claimantsallow to go to shareholders?steps take 175 .
.
.
.
.
.
take steps TO VB .
.
.
.
.
75TAG: VO15 14Officials also are hopeful that individual nations particularly West Germany and Japan willtake steps to stimulate their own economies.takeover thwart 244 .
.
.
.
.
to thwart AT takeover NN .
.
.
.
.
.
.
4413 11TAG: VOThe 48.50 a share offer announced Sunday is designed to thwart a takeover bid byGAF Corp.telephone return 153 .
.
.
.
.
return telephone calls .
.
.
.
.
.
.
5322 21TAG: VOMesa did not indicate the average price it paid for its 4.4 percent stake and Mesaofficials did not immediately return telephone calls seeking comment.Figure 12Some complete output on the stock market corpus.For  the 10 mi l l i on -word  stock market  corpus,  there were  some 60,000 di f ferentword  forms.
Xtract has been able to retr ieve some 15,000 col locat ions in total.
We wou ldl ike to note,  however ,  that Xtraet has on ly  been effective at retr iev ing col locat ions forwords  appear ing  at least several  dozen  t imes in the corpus.
This means  that low-f requency words  were not  p roduct ive  in terms of col locations.
Out  of the 60,000 wordsin the corpus,  on ly  8,000 were repeated  more  than 50 t imes.
This means  that for a target165Computational Linguistics Volume 19, Number 1YY=20% Y=20% N = 60 % T = 40% U = 60%T=94% T=94%U UU = 95%Y = 40%YY = 40%YN = 92%Figure 13Overlap of the manual and automatic evaluationslexicon of size N = 8,000, one should expect at least as many collocations to be added,and Xtract can help retrieve most of them.10.
A Lexicographic EvaluationThe third stage of Xtract can thus be considered as a retrieval system that retrievesvalid collocations from a set of candidates.
This section describes an evaluation ex-per iment of the third stage of Xtract as a retrieval system as well as an evaluationof the overall output of Xtract.
Evaluation of retrieval systems is usually done withthe help of two parameters: precision and recall (Salton 1989).
Precision of a retrievalsystem is defined as the ratio of retrieved valid elements divided by the total numberof retrieved elements (Salton 1989).
It measures the quality of the retrieved material.Recall is defined as the ratio of retrieved valid elements divided by the total numberof valid elements.
It measures the effectiveness of the system.
This section presents anevaluation of the retrieval performance of the third stage of Xtract.Deciding whether a given word combination is a valid or invalid collocation isactually a difficult task that is best done by a lexicographer.
Jeffery Triggs is a lex-icographer working for the Oxford English Dictionary (OED) coordinating the NorthAmerican Readers program of OED at Bell Communicat ion Research.
Jeffery Triggsagreed to go over manual ly several thousands of collocations.
17In order to have an unbiased experiment we had to be able to evaluate the per-formance of Xtract against a human expert.
We had to have the lexicographer andXtract perform the same task.
To do this in an unbiased way we randomly selecteda subset of about 4,000 collocations after the first two stages of Xtract.
This set ofcollocations thus contained some good collocations and some bad ones.
This data setwas then evaluated by the lexicographer and the third stage of Xtract.
This al lowed17 1 am grateful to Jeffery, whose professionalism and kindness helped me understand some of thedifficulty of lexicography.
Without him this evaluation would not have been possible.166Frank Smadja Retrieving Collocations from Text: Xtractus to evaluate the performances of the third stage of Xtract and the overall quality ofthe total output of Xtract in a single experiment.
The experiment was as follows:We gave the 4,000 collocations to evaluate to the lexicographer, asking him toselect he ones that he would consider for a domain-specific dictionary and to crossout the others.
The lexicographer came up with three simple tags, YY, Y, and N. BothY and YY include good collocations, and N includes bad collocations.
The differencebetween YY and Y is that Y collocations are of better quality than YY collocations.YY collocations are often too specific to be included in a dictionary, or some wordsare missing, etc.
After stage 2, about 20% of the collocations are Y, about 20% are YY,and about 60% are N. This told us that the precision of Xtract at stage 2 was onlyabout 40%.Although this would seem like a poor precision, one should compare it with themuch lower rates currently in practice in lexicography.
For compiling new entries forthe OED, for example, the first stage roughly consists of reading numerous documentsto identify new or interesting expressions.
This task is performed by professional read-ers.
For the OED, the readers for the American program alone produce some 10,000expressions a month.
These lists are then sent off to the dictionary and go throughseveral rounds of careful analysis before actually being submitted to the dictionary.The ratio of proposed candidates to good candidates i  usually low.
For example, outof the 10,000 expressions proposed each month, fewer than 400 are serious candidatesfor the OED, which represents a current rate of 4%.
Automatically producing listsof candidate xpressions could actually be of great help to lexicographers, and evena precision of 40% would be helpful.
Such lexicographic tools could, for example,help readers retrieve sublanguage-specific expressions by providing them with listsof candidate collocations.
The lexicographer then manually examines the list to re-move the irrelevant data.
Even low precision is useful for lexicographers, asmanualfiltering is much faster than manual scanning of the documents (Marcus 1990).
Suchtechniques are not able to replace readers, though, as they are not designed to identifylow-frequency expressions, whereas ahuman reader immediately identifies interestingexpressions with as few as one occurrence.The second stage of this experiment was to use Xtract stage 3 to filter out andlabel the sample set of collocations.
As described in Section 8, there are several validlabels (VO~ VS~ NN, etc.).
In this experiment, we grouped them under a single label: T.There is only one nonvalid label: U (for unlabeled).
A T collocation is thus accepted byXtract stage 3, and a U collocation is rejected.
The results of the use of stage 3 on thesample set of collocations are similar to the manual evaluation in terms of numbers:about 40% of the collocations were labeled (T) by Xtract stage 3, and about 60% wererejected (U).Figure 13 shows the overlap of the classifications made by Xtract and the lexicog-rapher.
In the figure, the first diagram on the left represents he breakdown in T andU of each of the manual categories (Y-YY and N).
The diagram on the right representsthe breakdown in Y-YY and N of the T and U categories.
For example, the first col-umn of the diagram on the left represents he application of Xtract stage 3 on the YYcollocations.
It shows that 94% of the collocations accepted by the lexicographer werealso accepted by Xtract.
In other words, this means that the recall of the third stage ofXtract is 94%.
The first column of the diagram on the right represents he lexicographicevaluation of the collocations automatically accepted by Xtract.
It shows that about80% of the T collocations were accepted by the lexicographer and that about 20% wererejected.
This shows that precision was raised from 40% to 80% with the addition ofXtract stage 3.
In summary, these experiments allowed us to evaluate Stage 3 as aretrieval system.
The results are: precision = 80% and recall -- 94%.167Computational Linguistics Volume 19, Number 1NYT d wpay 2 568rises -1  568raise 2 527cutting -1  522declines -1  492freeze -1  481offered 1 443increases -1  338closing 1 231fell 2 224DJ d w AP d wclosing 1 4615 gouging -1  1713rose -1  3704 get 3 551fell -1  3161 kindle 4 422tumbled -1  865 increases -1  357moved -1  850 pay 2 335declined -1  811 sell 5 293finished -3  710 finished -3  293closed - 1 648 declining 2 293measures 1 644 rose -5  291edged - 1 620 trading 3 207Figure 14Top associations with "price" in NYT, DJ, and AP.11.
Influence of the Corpus on the ResultsIn this section, we discuss the extent o which the results are dependent on the corpusused.
To illustrate our purpose here, we are using results collected from three differentcorpora.
The first one, DJ, for Dow Jones, is the corpus we used in this paper; it contains(mostly) stock market stories taken from the Associated Press newswire.
DJ contains8-9 million words.
The second corpus, NYT, contains articles published in the NewYork Times during the years 1987 and 1988.
The articles are on various subjects.
Thisis the same corpus that was used by Choueka (1988).
NYT contains 12 million words.The third corpus, AP, contains stories from the Associated Press newswire on variousdomains such as weather eports, politics, health, finances, etc.
AP is 4 million words.Figure 14 represents the top 10 word associations retrieved by Xtract stage 1 for thethree corpora with the word "price."
In this figure, d represents the distance betweenthe two words and w represents the weight associated with the bigram.
The weight is acombined index of the statistical distribution as discussed in Section 6, and it evaluatesthe collocation.
There are several differences and similarities among the three columnsof the figure in terms of the words retrieved, the order of the words retrieved, and thevalues of w. We identified two main ways in which the results depend on the corpus.We discuss them in turn.11.1 Results Are Dependent on the Size of the CorpusFrom the different corpora we used, we noticed that our statistical methods were noteffective for low-frequency words.
More precisely, the statistical methods we use do notseem to be effective on low frequency words (fewer than 100 occurrences).
If the wordis not frequently used in the corpus or if the corpus is too small, then the distributionof its collocates will not be big enough.
For example, from AP, which contains about1,000 occurrences of the word "rain," Xtract produced over 170 collocations at stage 1involving it.
In contrast, DJ only contains some 50 occurrences of "rain "is and Xtractcould only produce a few collocations with it.
Some collocations with "rain" and"hurricane" extracted from AP are listed in Figure 15.
Both words are high-frequencywords in AP and low-frequency words in DJ.18 The corpus actually contains ome stories not related to Wall Street.168Frank Smadja Retrieving Collocations from Text: XtractIn short, to build a lexicon for a computational linguistics application in a givendomain, one should make sure that the important words in the domain are frequentenough in the corpus.
For a subdomain of the stock market describing only the fluc-tuations of several indexes and some of the major events of the day at Wall Street,a corpus of 10 million words appeared to be sufficient.
This 10 million-token corpuscontains only 5,000 words each repeated more than 100 times.11.2 Results Are Dependent on the Contents of the CorpusSize and frequency are not the only important criteria.
For example, even though "food"is a high-frequency word in DJ, "eat" is not among its collocates, whereas it is amongthe top ones in the two other corpora.
Food is not eaten at Wall Street but rather traded,sold, offered, bought, etc.
If the corpus only contains tories in a given domain, mostof the collocations retrieved will also be dependent on this domain.
We have seen inSection 2 that in addition to jargonistic words, there are a number of more familiarterms that form collocations when used in different domains.
A corpus containingstock market stories is obviously not a good choice for retrieving collocations relatedto weather eports or for retrieving domain independent collocations such as "make-decision.
"For a domain-specific application, domain-dependent collocations are of interest,and a domain-specific corpus is exactly what is required.
To build a system that gen-erates tock market reports, it is a good choice to use a corpus containing only stockmarket reports.There is a danger in choosing a too specific corpus however.
For example, inFigure 14, we see that the first collocate of "price" in AP is "gouging," which is notretrieved in either DJ or in NYT.
"Price gouging" is not a current practice at Wall Streetand this collocation could not be retrieved even on some 20,000 occurrences of theword.
An example use of "price gouging" is the following:"The Charleston City Council passed an emergency ordinance barring pricegouging later Saturday after learning of an incident in which 5 pound bags ofice were being sold for 10.
"More formally, if we compare the columns in Figure 14, we see that the num-bers are much higher for DJ than for the other two corpora.
This is not due to asize/frequency factor, since "price" occurs about 10,000 times in both NYT and DJ,whereas it only occurs 4,500 times in AP.
It rather says that the distribution of collo-cates around "price" has a much higher variance in DJ than in the other corpora.
DJhas much bigger weights because it is focused; the stories are almost all about WallStreet.
In contrast, NYT contains a large number of stories with "price," but they havevarious origins.
"Price" has 4,627 collocates in NYT, whereas it only has 2,830 in DJ.Let us call Gorpus the variety of a given corpus.
One way to measure the variety isto use the information theory measure of entropy for a given language model.
Entropyis defined (Shannon 1948) as:~o~us = -~p(w) logp(w)Wwhere p(w) is the probability of appearance ofa given word, w. Entropy measures thepredictability of a corpus, in other words, the bigger the entropy of a corpus the lesspredictable it is.In an ideal language model, the entropy of a corpus should not depend on itssize.
However, word probabilities are difficult to approximate (see, for example, Bell169Computational Linguistics Volume 19, Number 1. .
.
.
.
CD inches of rain .
.
.
.. .
.
.
.
acid rain .
.
.
.. .
.
.
.
CD inches of rain fell .
.
.
.
.
.. .
.
.
.
heavy rain .
.
.
.. .
.
.
.
the Atlantic hurricane season .
.
.
.
.
.
.. .
.
.
.
hurricane force winds .
.
.
.
.
.. .
.
.
.
rain forests .
.
.
.
.
.
.. .
.
.
.
to reduce acid rain .
.
.
.. .
.
.
.
a major hurricane .
.
.
.
.
.. .
.
.
.
light rain .
.
.
.. .
.
.
.
the most powerful  hurricane to hit the .
.
.
.. .
.
.
.
an inch of rain .
.
.
.. .
.
.
.
to save the world s rain forests .
.
.
.
.
.
.. .
.
.
.
wind and rain .
.
.
.. .
.
.
.
a cold rain .
.
.
.
.
.Figure 15Some collocations retrieved from AE\[1987\] for a thorough discussion on probabil ity estimation), and in most cases entropygrows with the size of the corpus.
In this section, we use a simple unigram languagemodel trained on the corpus and we approximate the variety of a given corpus by:Oco us = - Zff(w)/S) log(f(w)/Slwin which f(w) is the frequency of appearance of the word w in the corpus and S isthe total number  of different word forms in the corpus.
In addition, to be fair in ourcomparison of the three corpora, we have used three (sub)corpora of about one millionwords for DJ, NYT, and Brown.
The I mi l l ion-word Brown corpus (Francis and Ku~era1982) contains 43,300 different words, of which only 1091 are repeated more than 100times.
The 0 of the Brown corpus is: OBrown = 10.5.
In comparison, the size of DJ is8,000,000.
It contains 59,233 different words of which 5,367 are repeated more than 100times.
DJ 0 ratio is: 0DI = 9.6.
And the 0 ratio of NYT which contains tories pertainingto various domains has been estimated at ONyT = 10.4.
According to this measure, DJis much more focused than both the Brown Corpus and NYT because the differencein variety is 1 in the logarithmic scale.
This is not a surprise since the subjects it coversare much more restricted, the genre is of only one kind, and the setting is constant.
Incontrast, the Brown corpus has been designed to be of mixed and rich composition,and NYT is made up of stories and articles related to various subjects and domains.Let us note that several factors might also influence the overall entropy of a givencorpus; for example the number  of writers, the time span covered by the corpus, etc.In any case, the success of statistical methods uch as the ones described in this reportalso depends on the sublanguage used in the corpus.For a sublanguage-dependent application, the training corpus must be focused,mainly because its vocabulary being restricted, the important words will be morefrequent han in a nonrestricted corpus (of equivalent size), and thus the collocationswill be easier to retrieve.
Other applications might require less focused corpora.
Forthose applications, the problem is even more touchy, as a perfectly balanced corpus isvery difficult to compile.
A sample of the 1987 DJ text is certainly not a good sample170Frank Smadja Retrieving Collocations from Text: Xtractof general English; however, a balanced sample, such as the Brown Corpus, may alsobe a poor sample.
It is doubtful that even a balanced corpus contains enough data onall possible domains, and the very effort of artificially balancing the corpus might alsobias the results.12.
Some ApplicationsCorpus-based techniques are still rarely used in the fields of linguistics, lexicography,and computational linguistics, and the main thrust of the work presented here is topromote its use for any text based application.
In this section we discuss everal usesof Xtract.12.1 Language GenerationLanguage generation is a novel application for Corpus-Based Computational Linguis-tics (Boguraev 1989).
In Smadja (1991) we show how collocations enhance the task oflexical selection in language generation.
Previous language generation works did notuse collocations mainly because they did not have the information i  compiled formand the lexicon formalisms available did not handle the variability of collocationalknowledge.
In contrast, we use Xtract to produce the collocations and we use Func-tional Unification Grammars (FUGs) (Kay 1979) as a representation formalism and aunification engine.
We show how the use of FUGs allows us to properly handle the in-teractions of collocational nd various other constraints.
We have implemented Cook,a surface sentence generator that uses a flexible lexicon for expressing collocationalconstraints in the stock market domain.
Using Ana (Kukich 1983) as a deep generator,Cook is implemented in FUF (Elhadad 1990), an extended implementation f FUG,and uniformly represents he lexicon and syntax as originally suggested by Halliday(1966).
For a more detailed escription of Cook the reader is referred to Smadja (1991).12.2 Retrieving Grammatical CollocationsAccording to Benson, Benson, and Ilson (1986a), collocations fall into two major groups:lexical collocations and grammatical collocations.
The difference between these twogroups lies in the types of words involved.
Lexical collocations roughly consist ofsyntagmatic affinities among open class words such as verbs, nouns, adjectives, andadverbs.
In contrast, grammatical collocations generally involve at least one closedclass word among particles, prepositions, and auxiliary verbs.
Examples of grammat-ical collocations are: put-up, as in "I can't put up with this anymore," and fill-out, as in"You have to fill out your 1040 form.
"19Consider the sentences below:1.
"The comparison to job hunting is certainly a valid one.
"2., "The comparison with job hunting is certainly a valid one."3.
"The association with job hunting is certainly a valid one."4.
* "The association to job hunting is certainly a valid one."5. "
.
.
.a  new initiative in the aftermath of the PLO's evacuation fromBeirut.
"19 Note that British English uses rather "to fill in a form.
"171Computational Linguistics Volume 19, Number 16., "...  a new initiative in the aftermath from the PLO's evacuation fromBeirut."7.
"...  a new initiative in the aftershocks from the PLO's evacuation fromBeirut.
"8., "...  a new initiative in the aftershocks of the PLO's evacuation fromBeirut.
"These examples clearly show that the choices of the prepositions are arbitrary.Sentences (1)-(2) and (3)-(4) compare the word associations comparison with~to withassociation with/to.
Although very similar in meaning, the two words select differentprepositions.
Moreover, the difference of meaning of the two prepositions does notaccount for the wording choices.
Similarly, sentences (5)-(6) and (7)-(8) illustrate thefact that "aftermath" selects the preposition "of" and "aftershock" selects "from.
"Grammatical collocations are very similar to lexical collocations in the sense thatthey also correspond to arbitrary and recurrent word co-occurrences (Benson 1990).
Interms of structure, grammatical collocations are much simpler: since many of the gram-matical collocations only include one open class word, the separation base-collocatorbecomes trivial.
The open class word is the meaning bearing element, it is the base; andthe closed class word is the collocator.
For lexicographers, grammatical collocations aresomehow simpler than lexical collocations.
A large number of dictionaries actually in-clude them.
For example, The Random House Dictionary of the English Language (RHDEL)(Flexner 1987) gives: "abreast of, accessible to, accustomed to, careful about, conducive to, con-scious of, equal to, expert at, fond of, jealous of," etc.
However, a large number are missingand the information provided is inconsistent and spotty.
For example, RHDEL doesnot include: appreciative of, available to, certain of, clever at, comprehensible to,curious about,difficult for, effective against, faithful to, friendly with, furious at, happy about, hostile to, etc.As demonstrated by Benson, even the most complete learners' dictionaries miss veryimportant grammatical collocations and treat the others inconsistently.
2?Xtract can be used without modification to retrieve noun-preposition collocations.Figure 16 lists such collocations as retrieved by Xtract.
Many of the associations re-trieved are effectively collocations: "absence of, accordance with, accuracy of, advantage of,aftershock from, agreement on, allegations of, anxiety about, aspect of," etc.12.3 Some Determiner-Noun ProblemsDeterminers are lexical elements that are used in conjunction with a noun to bring intocorrespondence with it a certain sector of reality (Ducrot and Todorov 1979).
A nounwithout determiner has no referent.
The role of determiner can be played by severalclasses of items: articles, (e.g., "a," "the"), possessives (e.g., "my, .... your"), indefiniteadjectives (e.g., "some," "many," "few," "certain"), demonstratives (e.g., "this," "those"),numbers, etc.
Determiner-noun combinations are often based simply on semantic orsyntactic riteria.
For example in the expression "my left foot," the determiner "my" ishere for semantic reasons.
Any other determiner would fail to identify the correct object(my left foot).
Classes of nouns such as mass and count are supposed to determine thetype of determiners tobe used in conjunction with the nouns (Quirk et al 1972).
Massnouns often refer to objects or ideas that can be divided into smaller parts withoutlosing their meaning.
In contrast, count nouns refer to objects that are not dividable.For example, "water" is a mass noun, if you spill half a glass of water you still have20 For a detailed case study the reader isreferred to Benson (1989b).172Frank Smadja Retrieving Collocations from Text: XtractNoun partability ofabsence ofacceleration ofacceptance ofaccordance withaccount ofaccounts inaccuracy ofacquisition ofacres ofaction byactions byactions ofadvance fromadvance ofadvancers withadvances inadvances onadvantage ofadviser inaftermath ofaftershocks fromNoun partafternoon fromaftershocks fromage ofagency foragency withagreement byagreements withalarm aboutalternatives foramount ofamounts ofanalysis foranalysis ofannouncement byannouncement ofanxiety aboutappetite forapplications forappointment ofappraisal ofapproval fromapproval ofFigure 16Some noun-preposition associations retrieved by Xtract.Noun partarbitrage inarea ofarea withareas ofargument byarguments inarticle inarticles onaspects ofassault onassessment ofassociation withassumption ofattempts byattention onattorney forattractiveness ofauction forauction inauction ofauthor ofauthority forsome water left in your glass.
In contrast if you cut a book in two halves and discardone half, you do not have a book any more; "book" is a count noun.
Count nouns areoften used with numbers and articles, and mass nouns are often used with no articles(or the zero article noted 0) (Quirk et al 1972).As with other types of word combinations, noun-determiner combinations oftenlead to collocations.
Consider the table given in Table 5.
In the table, some noun-determiner combinations are compared.
The first four determiners (a, the, 0, some)represent a singular use of the noun, and the last four (many, few, a lot of, a great dealof) represent a plural use.
1 and 300 are numbers.
0 is the zero article.
In the table,a '+ '  sign means that the combination is frequent and normal; a ' - '  sign means thatthe combination is very rare if not forbidden.
A '?'
sign means that the combinationis very low probability and that it would probably require an unusual context.
Forexample, one does not say ,"a butter," one says "some butter," and the combinationbutter-many is rather unusual and would only occur in unusual contexts.
For example,if one refers to several types of butter, one could say: "Many butters are based on regularbutter and an additional spice or flavor, such as rosemary, sage, basil, garlic, etc.
""Book" is a typical count noun in that it can combine with "a" and "many."
"Butter"is a typical mass noun in that it combines with the zero determiner and "a greatdeal."
However, words such as "police, people, traffic, opinion, weather," etc.
share somecharacteristics of both mass nouns and count nouns.
For example, "weather" is neither acount noun----~"a weather" is incorrect--nor a mass noun---*"a lot of weather" is incorrect(Quirk et al 1972).
However, it shares some characteristics of both types of nouns.Mass noun features include the premodified structures "a lot of good weather, .... some badweather," and "what lovely weather."
Count noun features include the plural "go out inall weathers," in the worst of weathers.
"173Computational Linguistics Volume 19, Number 1Table 5Some noun-determiner collocations.Noun/Det a the 0 some many few a lot of a great deal of 1 300butter - + + + -?
- ?
+ +book + + - - + + + - + +economics - + + -?
- - + +police - + + + + + + - - +people + + + + + + + + + +opinion + + + + + -?
+ +traffic - + + + - - + + - -weather - + .
.
.
.
.
+?
- -The problem with such combinations is that, if the word is irregular then theinformation will probably not be in the dictionary.
21Moreover, even if the word isregular, the word itself might not be in the dictionary or the information could simplybe difficult to retrieve automatically.Simple tools such as Xtract can hopefully provide such information.
Based ona large number of occurrences of the noun, Xtract will be able to make statisticalinferences as to the determiners used with it.
Such analysis is possible without anymodification to Xtract.
Actually, only a subpart of Xtract is necessary to retrieve them.12.4 Multilingual LexicographyWe have seen that collocations are difficult to handle for non-native speakers, andthat they require special handling for computational pplications.
In a multil ingualenvironment the problems become even more complex, as each language imposes itsown collocational constraints.
Consider, for example, the English expressions "House  ofPar l iament"  and "House  pa inter . "
The natural French translation for "house"  is "maison .
"However, the two expressions do not use this translation, but respectively "chambre"( " room"  in English) and "bdt iment"  ( "bu i ld ing"  in English).
Translations have to be pro-vided for collocations, and should not be word-based but rather expression-based.Bilingual dictionaries are generally inadequate in dealing with such issues.
They gen-erally limit such context-sensitive translations to ambiguous words (e.g., "number"  or" rock")  or highly complex words such as "make , "  "have , "  etc.
Moreover, even in thesecases, coverage is limited to semantic variants, and lexical collocations are generallyomitted.
One possible application is the development of compilation techniques forbilingual dictionaries.
This would require compiling two monolingual collocationaldictionaries and then developing some automatic or assisted translation methods.Those translation methods could be based on the statistical analysis of bilingual cor-pora currently available.
A simple algorithm for translating collocations is given inSmadja (1992).Several other applications uch as information retrieval, automatic thesauri com-pilation, and speech recognition are also discussed in Smadja (1991).21 Note that it might be in some grammar book.
For example, Quirk et al in their extensive grammarbook (1972) devote some 100 pages to such noun-determiner combinations.
They include a largenumber of rules and list exceptions tothose rules.174Frank Smadja Retrieving Collocations from Text: Xtract13.
Summary and ConclusionCorpus analysis is a relatively recent domain of research.
With the availability of largesamples of textual data and automated tools such as part-of-speech taggers, it hasbecome possible to develop and use automatic techniques for retrieving lexical infor-mation from textual corpora.
In this paper some original techniques for the automaticextraction of collocations have been presented.
The techniques have been implementedin a system, Xtract, and tested on several corpora.
Although some other attempts havebeen made to retrieve collocations from textual corpora, no work has been able to re-trieve the full range of the collocations that Xtract retrieves.
Thanks to our filteringmethods, the collocations produced by Xtract are of better quality.
And finally, be-cause of the syntactic labeling, the collocations we produce are richer than the onesproduced by other methods.The number and size of available textual corpora is constantly growing.
Dictionar-ies are available in machine-readable form, news agencies provide subscribers withdaily reports on various events, publishing companies use computers and providemachine-readable v rsions of books, magazines, and journals.
This amounts to a vastquantity of language data with unused and virtually unlimited, implicit and explicitinformation about the English language.
These textual data can thus be used to re-trieve important information that is not available in other forms.
The primary goalof the research we presented is to provide a comprehensive l xicographic toolkit toassist in implementing natural anguage processing, as well as to assist lexicographersin compiling general-purpose dictionaries, as most of the work is still manually per-formed in this domain.
The abundance of text corpora allows a shift toward moreempirical studies of language that emphasize the development of automated tools.We think that more research should be conducted in this direction and hope that ourwork will stimulate research projects along these lines.AcknowledgmentsI would like to thank Steve Abney, KenChurch, Karen Kukich, and MichaelElhadad for making their software toolsavailable to us.
Without hem, most of thework reported here would not have beenpossible.
Kathy McKeown read earlierversions of this paper and was helpful inboth the writing and the research.
Finally,the anonymous reviewers for ComputationalLinguistics made insightful comments onearlier versions of the paper.Part of this work has been done incollaboration with Bell CommunicationResearch, and part of this work has beensupported by DARPA grantN00039-84-C-0165, byNSF grantIRT-84-51438, and by ONR grantN00014-89-J-1782.ReferencesAbney, S. (1989).
Parsing by Chunks.
In TheMIT Parsing Volume, edited by C. Tenny.MIT Press.Abney, S. (1990).
"Rapid incrementalparsing with repair."
In Proceedings,Waterloo Conference on Electronic TextResearch, 1990.Allerton, D. J.
(1984).
"Three or four levelsof co-occurrence r lations."
Lingua, 63,17-40.Amsler, B.
(1989).
"Research towards thedevelopment of a lexical knowledge basefor natural language processing."
InProceedings, 1989 SIGIR Conference.Cambridge, MA.Angell, R. C. (1983).
"Automating spellingcorrection using a trigram similaritymeasure."
Information Processing andManagement, 19, 255-261.Bahl, L.; Jelinek, F.; and Mercer, R.
(1983).
"A maximum likelihood approach tocontinuous speech recognition."
IEEETransactions on Pattern Analysis and MachineIntelligence, 5(2), 179-190.Bell, T.; Witten, I.; and Cleary, J.
(1989).
"Modelling for text compression."
ACMComputing Surveys, 21(4), 557-591.Bell, T. (1987).
"A unifying theory andimprovement for existing approaches totext compression."
Doctoral dissertation,University of Canterbury, Christchurch,New Zealand.175Computational Linguistics Volume 19, Number 1Benson, M.; Benson, E.; and Ilson, R.(1986a).
The BBI Combinatory Dictionary ofEnglish: A Guide to Word Combinations.
JohnBenjamins.Benson, M.; Benson, E.; and Ilson, R.(1986b).
The Lexicographic Description ofEnglish.
John Benjamins.Benson, M. (1989a).
"The collocationaldictionary and the advanced learner."
InLearner's Dictionaries: State of the Art, editedby M. Tickoo, 84--93.
SEAMEO.Benson, M. (1989b).
"The structure of thecollocational dictionary."
InternationalJournal of Lexicography, 2 1-14.Benson, M. (1990).
"Collocations andgeneral-purpose dictionaries.
"International Journal of Lexicography, 3(1),23-35.Boguraev, B.
(1989).
"Introduction."
InComputational Lexicography for NaturalLanguage Processing, Chapter 1, edited byT.
Boguraev and B. Briscoe.
Longman.Brown, P.; Cocke, J.; Della Pietra, V.;Della Pietra, S.; Jelinek, F.; Mercer, R.; andRoossin, P. (1988).
"A statistical approachto language translation."
In Proceedings ofthe 13th International Conference onComputational Linguistics (COLING-88),71-76.Cerf-Danon, H.; Derouault, A. M.; Elbeze,M.
; and Merialdo, B.
(1989).
"Speechrecognition i  French with a very largedictionary."
In Eurospeech.Choueka, Y.; Klein, T.; and Neuwitz, E.(1983).
"Automatic retrieval of frequentidiomatic and collocational expressions ina large corpus."
Journal for Literary andLinguistic Computing, 4, 34-38.Choueka, Y.
(1988).
"Looking for needles ina haystack."
In Proceedings, RIAOConference on User-Oriented Context BasedText and Image Handling, 609-623.Cambridge, MA.Church, K., and Gale, W. (1990).
"Poorestimates of context are worse thannone."
In Darpa Speech and NaturalLanguage Workshop, Hidden Valley, PA.Church, K., and Hanks, P. (1989).
"Wordassociation orms, mutual information,and lexicography."
In Proceedings, 27thMeeting of the ACL, 76--83.
Also inComputational Linguistics, 16(1).Church, K. W.; Gale, W.; Hanks, P.; andHindle, D. (1989).
"Parsing, wordassociations and typicalpredicate-argument relations."
InProceedings ofthe International Workshop onParsing Technologies, 103-112.
CarnegieMellon University, Pittsburgh, PA.Church, K.; Gale, W.; Hanks, P.; and Hindle,D.
(1991).
"Using statistics in lexicalanalysis."
In Lexical Acquisition: UsingOn-Line Resources to Build a Lexicon, editedby Uri Zernik.
Lawrence Erlbaum.Church, K. (1988).
"Stochastic partsprogram and noun phrase parser forunrestricted text."
In Proceedings, SecondConference on Applied Natural LanguageProcessing.
Austin, TX.Cowie, A. P. (1981).
"The treatment ofcollocations and idioms in learner'sdictionaries."
Applied Linguistics, 2(3),223--235.Cruse, D. A.
(1986).
Lexical Semantics.Cambridge University Press.Debili, F. (1982).
AnalyseSyntactico-Sdmantique Fondde sur uneAcquisition Automatique de RelationsLexicales Sdmantiques.
Doctoraldissertation, Paris XI University, Orsay,France.
Th6se de Doctorat D'6tat.Dellenbaugh, D., and Dellenbaugh, B.(1990).
Small Boat Sailing, a Complete Guide.Sports Illustrated Winner's Circle Books.Ducrot, O., and Todorov, T. (1979).Encyclopedic Dictionary of the Sciences ofLanguage.
John Hopkins University Press.Elhadad, M. (1990).
"Types in functionalunification grammars."
In Proceedings,28th Meeting of the Association forComputational Linguistics.Ephraim, Y., and Rabiner, L. (1990).
"On therelations between modeling approachesfor speech recognition."
IEEE Transactionson Information Theory, 36(2), 372-380.Fano, R. (1961).
Transmission ofInformation: AStatistical Theory of Information.
MIT Press.Flexner, S., ed.
(1987).
The Random HouseDictionary of the English Language, SecondEdition.
Random House.Francis, W., and Ku~era, H. (1982).Frequency Analysis of English Usage.Houghton Mifflin.Garside, R., and Leech, G. (1987).
TheComputational Analysis of English, a CorpusBased Approach.
Longman.Guazzo, M. (1980).
"A generalminimum-redundancy source-codingalgorithm."
IEEE Transactions onInformation Theory, IT-26(1), 15-25.HaUiday, M. A. K., and Hasan, R. (1976).Cohesion in English.
Longman.Halliday, M. A. K. (1966).
"Lexis as alinguistic level."
In In Memory of J. R. Firth,edited by C. E. Bazell, J. C. Catford,M.
A. K. Halliday, and R. H. Robins,148-162.
Longmans Linguistics Library.Hindle, D., and Rooth, M.
(1990).
"Structural ambiguity and lexicalrelations."
In DARPA Speech and NaturalLanguage Workshop, Hidden Valley, PA.Hindle, D. (1983).
"User manual for176Frank Smadja Retrieving Collocations from Text: Xtractfidditch, a deterministic parser."
TechnicalMemorandum 7590-142, Naval ResearchLaboratory.Kay, M. (1979).
"Functional grammar."
InProceedings, 5th Meeting of the BerkeleyLinguistics Society.
Berkeley LinguisticsSociety.Kukich, K. (1983).
"Knowledge-based reportgeneration: A technique for automaticallygenerating natural language reports fromdatabases."
In Proceedings, SixthInternational ACM SIGIR, Conference onResearch and Development in InformationRetrieval.
Washington, D.C.Kukich, K. (1990).
"A comparison of somenovel and traditional lexical distancesmetrics for spelling correction."
InProceedings, International Neural NetworksConference (INNC).
Paris, France.Marcus, M. (1990).
"Tutorial on tagging andprocessing large textual corpora.
"Presented at the 28th Annual Meeting ofthe ACL.Martin, W. J. R.; A1, B. P. E; andVan Sterkenburg, P. J. G. (1983).
"On theprocessing of a text corpus: from textualdata to lexicographical information."
InLexicography: Principles and Practice,Applied Language Studies Series, editedby R. R. K. Hartmann.
Academic Press.Mays, E.; Damerau, F.; and Mercer, R.(1990).
"Context-based spellingcorrection."
In IBM Natural Language ITL,Paris, France.Mel'~uk, I.
A.
(1981).
"Meaning-text models:a recent rend in Soviet linguistics."
TheAnnual Review of Anthropology.Merialdo, B.
(1987).
"Speech recognitionwith very large size dictionary."
InProceedings, International Conference onAcoustics, Speech, and Signal Processing(ICASSP), Dallas, TX.Morris, R., and Cherry, L. L.
(1975).
"Computer detection of typographicalerrors."
IEEE Transactions on ProfessionalCommunications, PC-18(1), 54-63.Nakhimovsky, A. D., and Leed, R. L.
(1979).
"Lexical functions and languagelearning."
Slavic and East European Journal,23(1).Quirk, R.; Greenbaum, S.; Leech, G.; andSvartvik, J.
(1972).
A ComprehensiveGrammar of the English Language.
Longman.Salton, J.
(1989).
Automatic Text Processing,The Transformation, Analysis, and Retrieval ofInformation by Computer.
Addison-Wesley.Shannon, C. E. (1948).
"A mathematicaltheory of communication."
Bell SystemTech., 27, 379-423, 623-656.Smadja, E, and McKeown, K.
(1990).
"Automatically extracting andrepresenting collocations for languagegeneration."
In Proceedings ofthe 28thAnnual Meeting of the Association forComputational Linguistics, Pittsburgh, PA.Smadja, E (1991).
"Retrieving collocationalknowledge from textual corpora.
Anapplication: Language generation.
"Doctoral dissertation, Computer ScienceDepartment, Columbia University.Smadja, E (1992).
"How to compile abilingual collocational lexiconautomatically."
In Proceedings ofthe AAAIWorkshop on Statistically-Based NLPTechniques, San Jose, CA.177
