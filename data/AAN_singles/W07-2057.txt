Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 264?267,Prague, June 2007. c?2007 Association for Computational LinguisticsPNNL: A Supervised Maximum Entropy Approach to Word SenseDisambiguationStephen Tratz, Antonio Sanfilippo, Michelle Gregory, Alan Chappell, ChristianPosse, Paul WhitneyPacific Northwest National Laboratory902 Battelle Blvd, PO Box 999Richland, WA 99352, USA{stephen.tratz, antonio.sanfilippo, michelle, alan.chap-pell, christian.posse, paul.whitney}@pnl.govAbstractIn  this  paper,  we  described  the  PNNLWord Sense Disambiguation system as ap-plied  to  the  English  all-word  task  in  Se-mEval 2007.
We use a supervised learningapproach,  employing  a  large  number  offeatures and using Information Gain for di-mension  reduction.
The  rich  feature  setcombined with a Maximum Entropy classi-fier  produces results  that  are significantlybetter than baseline and are the highest F-score  for  the  fined-grained  English  all-words subtask of SemEval.1 IntroductionAccurate  word  sense  disambiguation  (WSD)  cansupport  many  natural  language  processing  andknowledge management  tasks.
The  main goal  ofthe  PNNL  WSD system  is  to  support  SemanticWeb applications, such as semantic-driven searchand  navigation,  through  a  reliable  mapping  ofwords  in  naturally  occurring  text  to  ontologicalclasses.
As described  in  Sanfilippo et  al.
(2006),this goal is achieved by defining a WordNet-based(Fellbaum,  1998)  ontology that  offers  a manage-able set of concept classes, provides an extensivecharacterization of concept class in terms of lexicalinstances, and integrates an automated class recog-nition algorithm.
We found that the same featuresthat are useful for predicting word classes are alsouseful in distinguishing individual word senses.Our main objective in this paper is to predict in-dividual word senses using a large combination offeatures  including contextual,  semantic,  and syn-tactic information.
In our earlier paper (Sanfilippoet al, 2006), we reported that the PNNL WSD sys-tem exceeded the performance of the best perform-ers  for  verbs  in  the  SENSEVAL-3  English  all-words task dataset.
SemEval 2007 is our first op-portunity  to  enter  a  word  sense  disambiguationcompetition.2 ApproachWhile many unsupervised word sense disambigua-tion systems have been created, supervised systemshave generally produced superior  results  (Snyderand Palmer, 2004; Mihalcea et al, 2004).
Our sys-tem is based on a supervised WSD approach thatuses  a  Maximum  Entropy  classifier  to  predictWordNet senses.We  use  SemCor1,  OMWE 1.0  (Chklovski  andMihalcea, 2002), and example sentences in Word-Net  as  the  training  corpus.
We  utilize  theOpenNLP MaxEnt  implementation2 of  the  maxi-mum  entropy  classification  algorithm  (Berger  etal.,  1996)  to  train  classification  models  for  eachlemma and part-of-speech combination in the train-ing  corpus.
These  models  are  used  to  predictWordNet  senses  for  words found in natural  text.For  lemma  and  part-of-speech  combinations  thatare not present in the training corpus, the PNNLWSD system defaults to the most frequent Word-Net sense.2.1 FeaturesWe use a rich set of features to predict individualword senses.
A large number of  features are ex-tracted for each word sense instance in the trainingdata.
Following  Dang & Palmer  (2005)  and Ko-homban & Lee (2005), we use contextual, syntac-tic and semantic  information to inform our word1http://www.cs.unt.edu/~rada/downloads.html.2http://maxent.sourceforge.net/.264sense disambiguation system.
However,  there aresignificant  differences  between the specific  typesof contextual,  syntactic  and semantic informationwe use in our system and those proposed by Dang& Palmer  (2005)  and Kohomban  & Lee (2005).More specifically,  we employ novel  features  andfeature combinations, as described below.?
Contextual information.
The contextual infor-mation we use includes the word under analy-sis plus the three tokens found on each side ofthe word, within sentence boundaries.
Tokensinclude both words and punctuation.?
Syntactic information.
We include grammaticaldependencies  (e.g.
subject,  object)  and  mor-pho-syntactic  features such as part of speech,case, number and tense.
We use the Connexorparser3 (Tapanainen and J?rvinen, 1997) to ex-tract lemma information, parts of speech, syn-tactic  dependencies,  tense,  case,  and  numberinformation.
A sample output  of  a  Connexorparse is given in Table 1.
Features are extract-ed  for  all  tokens  that  are  related  through nomore than 3 levels of dependency to the wordto be disambiguated.?
Semantic  information.
The semantic  informa-tion  we  incorporate  includes  named  entitytypes (e.g.
PERSON, LOCATION, ORGANI-ZATION) and hypernyms.
We use OpenNLP4and  LingPipe5 to  identify  named  entities,  re-placing the strings identified as named entities(e.g., Joe Smith) with the corresponding entitytype  (PERSON).
We also  substitute  personalpronouns  that  unambiguously  denote  peoplewith the entity type PERSON.
Numbers in thetext  are  replaced  with  type  label  NUMBER.Hypernyms  are  retrieved  from WordNet  andadded to the feature set for all noun tokens se-lected by the contextual and syntactic rules.
Incontrast to Dang & Palmer (2005), we only in-clude  the  hypernyms  of  the  most  frequentsense,  and  we  include  the  entire  hypernymchain (e.g.
motor, machine, device, instrumen-tality, artifact, object, whole, entity).To address feature extraction processes specificto  noun and verbs,  we add the  following  condi-tions.3http://www.connexor.com/.4http://opennlp.sourceforge.nt/.5http://www.alias-i.com/lingpipe/.?
Syntactic  information  for  verbs.
If  the  verbdoes not have a subject, the subject of the clos-est ancestor verb in the syntax tree is used in-stead.?
Syntactic information for nouns.
The first verbancestor in the syntax tree is also used to gen-erate features.?
Semantic information for nouns.
A feature in-dicating whether a token is capitalized for eachof the tokens used to generate features.A sample of the resulting feature vectors that areused by the PNNL word sense disambiguation sys-tem is presented in Table 2.ID Word Lemma GrammaticalDependen-ciesMorphosyntacticFeatures123456theenginethrobbedintolife.theenginethrobintolife.det:>2subj:>3main:>0goa:>3pcomp:>4@DN> %>N DET@SUBJ %NH N NOM SG@+FMAINV %VA V PAST@ADVL %EH PREP@<P %NH N NOM SGTable 1.
Connexor sample output for the sentence?The engine throbbed into life?.the pre:2:the, pre:2:pos:DET, det:the, det:pos:DET,hassubj:det:engine pre:1:instrumentality, pre:1:object, pre:1:artifact,pre:1:device, pre:1:engine, pre:1:motor, pre:1:whole,pre:1:entity, pre:1:machine, pre:1:pos:N,pre:1:case:NOM,pre:1:num:SG,subj:instrumentality,subj:object, subj:arti-fact, subj:device, subj:engine, subj:motor, subj:whole,subj:entity, subj:machine, subj:pos:N, hassubj:,subj:case:NOM, subj:num:SG,throbbed haspre:1:,haspre:2:,haspost:1:, haspost:2:, haspost:3:,self:throb, self:pos:V, main:,throbbed, self:tense:PASTinto post:1:into, post:1:pos:PREP, goa:into, goa:pos:PREP,life post:2:life, post:2:state, post:2:being, post:2:pos:N,post:2:case:NOM, post:2:num:SG, hasgoa:, pcomp:life,pcomp:state, pcomp:being, pcomp:pos:N,hasgoa:pcomp:, goa:pcomp:case:NOM,goa:pcomp:num:SG.
post:3:.Table  2.
Feature  vector  for  throbbed in the sen-tence ?The engine throbbed into life?.As the example in Table 2 indicates, the combi-nation of contextual, syntactic, and semantic infor-mation types results in a large number of features.Inspection  of  the  training data  reveals  that  somefeatures may be more important than others in es-tablishing word sense assignment for each choiceof word lemma.
We use a feature selection proce-265dure to reduce the full set of features to the featuresubset that is most relevant to word sense assign-ment for each lemma.
This practice improves theefficiency of our word sense disambiguation algo-rithm.
The feature selection procedure we adoptedconsists of scoring each potential feature accordingto  a  particular  feature  selection  metric,  and  thentaking the best k features.We choose Information Gain as our feature se-lection metric.
Information Gain measures the de-crease in entropy when the feature is given versuswhen it is absent.
Yang and Pederson (1997) reportthat  Information Gain outperformed other featureselection  approaches  in  their  multi-class  bench-marks,  and  Foreman  (2003)  showed  that  it  per-formed amongst the best for his 2-class problems.3 EvaluationTo evaluate our approach and feature set, we ranour  model  on  the  SENSEVAL-3  English  all-wordstask test data.
Using data provided by the SENSE-VAL website6, we were able to compare our resultsfor  verbs  to  the  top  performers  on  verbs  alone.Upali S. Kohomban and Wee Sun Lee provided uswith  the  results  file  for  the  Simil-Prime  system(Kohomban and Lee, 2005).
As reported in Sanfil-ippo et al (2006) and shown in table 3, our resultsfor verbs rival those of top performers.
We had asignificant  improvement  (p-value<0.05)  over  thebaseline of  52.9%, a marginal  improvement  overthe second best performer (SenseLearner) (Mihal-cea and Faruque, 2004), and we were as good asthe top performer (GAMBL) (Decadt et al, 2004).7System Precision Fraction ofRecallOur system 61% 22%GAMBL 59.0% 21.3%SenseLearner 56.1% 20.2%Baseline 52.9% 19.1%Table 3.
Results for verb sense disambiguation onSENSEVAL-3 data, adapted from Sanfilippo et al(2006).Since then, we have expanded our evaluation toall parts of speech.
Table 4 provides the evaluation6http://www.senseval.org/.7The 2% improvement in precision which our systemshowed as  compared to GAMBL was not statisticallysignificant (p=0.21).of our system as compared  to  the three top per-formers on the SENSEVAL-3 data and the baseline.The baseline of 0.631 F-score8 was computed us-ing the most frequent WordNet sense.
The PNNLWSD system performs significantly better than thebaseline (p-value<0.05) and rivals the top perform-ers.
The performance of the PNNL WSD systemrelative to the other three systems and the baselineremains unchanged when the unknown sense an-swers  (denoted  by a  ?U?)
are  excluded  from theevaluation.System Precision RecallPNNL 0.670 0.670Simil-Prime 0.661 0.663GAMBL 0.652 0.652SenseLearner 0.646 0.646Baseline 0.631 0.631Table 4.
SENSEVAL-3 English all-words.System Recall PrecisionPNNL 0.669 0.671GAMBL 0.651 0.651Simil-Prime 0.644 0.657SenseLearner 0.642 0.651Baseline 0.631 0.631Table 5.
SENSEVAL-3 English all-words, No ?U?.4 Experimental  results  on  SemEval  all-words subtaskThis was our first opportunity to test our model ina WSD competition.
For this competition, we fo-cused our efforts  on the fine-grained English all-words task because our system was set up to per-form fine-grained WordNet  sense  prediction.
Weare  pleased that  our  system achieved the  highestscore for this subtask.
Our results for the SemEvaldataset as compared to baseline are reported in Ta-ble 6.
The PNNL WSD system did not assign theunknown sense, ?U?, to any word instances in theSemEval dataset.8This baseline is slightly higher than that reported byothers (Snyder and Palmer 2004).266System F-scorePNNL 0.591Baseline 0.514p-value <0.01Table 6.
SemEval Results.5 DiscussionAlthough these results are promising, there is stillmuch work to be done.
For example, we need toinvestigate the contribution of each feature to theoverall performance of the system in terms of pre-cision and recall.
Such a feature sensitivity analysiswill provide us with a better understanding of howthe algorithm can be further improved and/or mademore efficient by leaving out features whose con-tribution is negligible.Another important point to make is that, whileour system shows the best precision/recall resultsoverall,  we  can  only  claim  statistical  relevancewith  reference  to  the  baseline  and  results  worsethan  baseline.
The  size  of  the  SemEval  data  set(N=465) is too small to establish whether the dif-ference in precision/recall results with the other topsystems is statistically significant.AcknowledgementsWe would like to thank Upali  S. Kohomban andWee Sun Lee for  providing us with their  SENSE-VAL-3 English all-words task results file for Simil-Prime.
Many thanks also to Patrick Paulson, BobBaddeley, Ryan Hohimer, and Amanda White fortheir  help  in  developing  the  word  class  disam-biguation system on which the work presented inthis paper is based.ReferencesBerger, A., S. Della Pietra and V. Della Pietra (1996) AMaximum  Entropy  Approach  to  Natural  LanguageProcessing.
Computational  Linguistics,  volume  22,number 1, pages 39-71.Chklovski, T. and R. Mihalcea (2002) Building a sensetagged corpus with open mind word expert.
In  Pro-ceedings of the ACL-02 workshop on Word sense dis-ambiguation: recent successes and future directions.Dang, H. T. and M. Palmer (2005) The Role of Semant-ic Roles in Disambiguating Verb Senses.
In Proceed-ings of the 43rd Annual Meeting of the Associationfor Computational Linguistics,  Ann Arbor MI, June26-28, 2005.Decadt,  B., V. Hoste,  W. Daelemans and A.
Van denBosch (2004) GAMBL, genetic  algorithm optimiza-tion of memory-based WSD.
SENSEVAL-3: Third In-ternational Workshop on the Evaluation of Systemsfor the Semantic Analysis of Text.
Barcelona, Spain.Fellbaum,  C.,  editor.
(1998)  WordNet:  An  ElectronicLexical Database.
MIT Press, Cambridge, MA.Foreman, G. (2003) An Extensive Empirical Study ofFeature  Selection  Metrics  for  Text  Classification.Journal  of  Machine  Learning  Research,  3,  pages1289-1305.Kohomban, U. and  W. Lee (2005) Learning semanticclasses  for  word sense disambiguation.
In Proceed-ings of the 43rd Annual meeting of the Association forComputational Linguistics, Ann Arbor, MI.Mihalcea,  R.,  T.  Chklovski,  and  A.  Kilgarriff  (2004)The  SENSEVAL-3  English   Lexical  Sample  Task,SENSEVAL-3: Third International Workshop on theEvaluation of  Systems for the Semantic Analysis ofText.
Barcelonna, Span.Mihalcea,  R.  and  E.  Faruque   (2004)  SenseLearner:Minimally supervised word sense disambiguation forall words in open text.
SENSEVAL-3: Third Interna-tional Workshop on the Evaluation of Systems for theSemantic Analysis of Text.
Barcelona, Spain.Sanfilippo, A.,  S.  Tratz,  M. Gregory, A.  Chappell,  P.Whitney, C. Posse, P. Paulson, B. Baddeley, R. Hohi-mer,  A.
White  (2006)  Automating  Ontological  An-notation with WordNet.
Proceedings to the Third In-ternational WordNet Conference, Jan 22-26, Jeju Is-land, Korea.Snyder,  B.   and  M.  Palmer.
2004.
The  English  All-Words  Task.
SENSEVAL-3:  Third  InternationalWorkshop on the Evaluation of  Systems for the Se-mantic Analysis of Text.
Barcelona, Spain.Tapanainen, P. and Timo J?rvinen (1997) A nonproject-ive  dependency  parser.
In  Proceedings  of  the  5thConference on Applied Natural Language Processing,pages 64?71, Washington D.C. Association for Com-putational Linguistics.Yang,  Y.  and  J.  O.  Pedersen  (1997)  A  ComparativeStudy on Feature Selection in Text Categorization.
InProceedings of the 14th International Conference onMachine Learning (ICML), pages 412-420, 1997.267
