Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 880?888,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsFast Generation of Translation Forestfor Large-Scale SMT Discriminative TrainingXinyan Xiao, Yang Liu, Qun Liu, and Shouxun LinKey Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of Sciences{xiaoxinyan,yliu,liuqun,sxlin}@ict.ac.cnAbstractAlthough discriminative training guarantees toimprove statistical machine translation by in-corporating a large amount of overlapping fea-tures, it is hard to scale up to large data due todecoding complexity.
We propose a new al-gorithm to generate translation forest of train-ing data in linear time with the help of wordalignment.
Our algorithm also alleviates theoracle selection problem by ensuring that aforest always contains derivations that exactlyyield the reference translation.
With millionsof features trained on 519K sentences in 0.03second per sentence, our system achieves sig-nificant improvement by 0.84 BLEU over thebaseline system on the NIST Chinese-Englishtest sets.1 IntroductionDiscriminative model (Och and Ney, 2002) caneasily incorporate non-independent and overlappingfeatures, and has been dominating the research fieldof statistical machine translation (SMT) in the lastdecade.
Recent work have shown that SMT benefitsa lot from exploiting large amount of features (Lianget al, 2006; Tillmann and Zhang, 2006; Watanabeet al, 2007; Blunsom et al, 2008; Chiang et al,2009).
However, the training of the large numberof features was always restricted in fairly small datasets.
Some systems limit the number of training ex-amples, while others use short sentences to maintainefficiency.Overfitting problem often comes when trainingmany features on a small data (Watanabe et al,2007; Chiang et al, 2009).
Obviously, using muchmore data can alleviate such problem.
Furthermore,large data also enables us to globally train millionsof sparse lexical features which offer accurate cluesfor SMT.
Despite these advantages, to the best ofour knowledge, no previous discriminative trainingparadigms scale up to use a large amount of trainingdata.
The main obstacle comes from the complexityof packed forests or n-best lists generation whichrequires to search through all possible translationsof each training example, which is computationallyprohibitive in practice for SMT.To make normalization efficient, contrastive esti-mation (Smith and Eisner, 2005; Poon et al, 2009)introduce neighborhood for unsupervised log-linearmodel, and has presented positive results in varioustasks.
Motivated by these work, we use a translationforest (Section 3) which contains both ?reference?derivations that potentially yield the reference trans-lation and also neighboring ?non-reference?
deriva-tions that fail to produce the reference translation.1However, the complexity of generating this transla-tion forest is up to O(n6), because we still need bi-parsing to create the reference derivations.Consequently, we propose a method to fast gener-ate a subset of the forest.
The key idea (Section 4)is to initialize a reference derivation tree with maxi-mum score by the help of word alignment, and thentraverse the tree to generate the subset forest in lin-ear time.
Besides the efficiency improvement, sucha forest allows us to train the model without resort-1Exactly, there are no reference derivations, since derivationis a latent variable in SMT.
We call them reference derivationjust for convenience.8800,40,12,43,41 30 42135462hyper-edge rulee1 r1 X ?
?X1 bei X2, X1 was X2?e2 r2 X ?
?qiangshou bei X1,the gunman was X1?e3 r3 X ?
?jingfang X1, X1 by the police?e4 r4 X ?
?jingfang X1, police X1 ?e5 r5 X ?
?qiangshou, the gunman?e6 r6 X ?
?jibi, shot dead?Figure 1: A translation forest which is the running example throughout this paper.
The reference translation is ?thegunman was killed by the police?.
(1) Solid hyperedges denote a ?reference?
derivation tree t1 which exactly yieldsthe reference translation.
(2) Replacing e3 in t1 with e4 results a competing non-reference derivation t2, which fails toswap the order ofX3,4.
(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3.
Generally,this is done by deleting a node X0,1.ing to constructing the oracle reference (Liang et al,2006; Watanabe et al, 2007; Chiang et al, 2009),which is non-trivial for SMT and needs to be deter-mined experimentally.
Given such forests, we glob-ally learn a log-linear model using stochastic gradi-ent descend (Section 5).
Overall, both the generationof forests and the training algorithm are scalable, en-abling us to train millions of features on large-scaledata.To show the effect of our framework, we globallytrain millions of word level context features moti-vated by word sense disambiguation (Chan et al,2007) together with the features used in traditionalSMT system (Section 6).
Training on 519K sentencepairs in 0.03 seconds per sentence, we achieve sig-nificantly improvement over the traditional pipelineby 0.84 BLEU.2 Synchronous Context Free GrammarWe work on synchronous context free grammar(SCFG) (Chiang, 2007) based translation.
The el-ementary structures in an SCFG are rewrite rules ofthe form:X ?
?
?, ?
?where ?
and ?
are strings of terminals and nonter-minals.
We call ?
and ?
as the source side and thetarget side of rule respectively.
Here a rule means aphrase translation (Koehn et al, 2003) or a transla-tion pair that contains nonterminals.We call a sequence of translation steps as aderivation.
In context of SCFG, a derivation is a se-quence of SCFG rules {ri}.
Translation forest (Miet al, 2008; Li and Eisner, 2009) is a compact repre-sentation of all the derivations for a given sentenceunder an SCFG (see Figure 1).
A tree t in the forestcorresponds to a derivation.
In our paper, tree meansthe same as derivation.More formally, a forest is a pair ?V,E?, where Vis the set of nodes, E is the set of hyperedge.
Fora given source sentence f = fn1 , Each node v ?
Vis in the form Xi,j , which denotes the recognitionof nonterminal X spanning the substring from the ithrough j (that is fi+1...fj).
Each hyperedge e ?
Econnects a set of antecedent to a single consequentnode and corresponds to an SCFG rule r(e).3 Our Translation ForestWe use a translation forest that contains both ?ref-erence?
derivations that potentially yield the refer-ence translation and also some neighboring ?non-reference?
derivations that fail to produce the ref-erence translation.
Therefore, our forest only repre-sents some of the derivations for a sentence given anSCFG rule table.
The motivation of using such a for-est is efficiency.
However, since this space containsboth ?good?
and ?bad?
translations, it still providesevidences for discriminative training.First see the example in Figure 1.
The derivationtree t1 represented by solid hyperedges is a referencederivation.
We can construct a non-reference deriva-tion by making small change to t1.
By replacing thee3 of t1 with e4, we obtain a non-reference deriva-881tion tree t2.
Considering the rules in each derivation,the difference between t1 and t2 lies in r3 and r4.
Al-though r3 has a same source side with r4, it producesa different translation.
While r3 provides a swap-ping translation, r4 generates a monotone transla-tion.
Thus, the derivation t2 fails to move the sub-ject ?police?
to the behind of verb ?shot dead?, re-sulting a wrong translation ?the gunman was policeshot dead?.
Given such derivations, we hope thatthe discriminative model is capable to explain whyshould use a reordering rule in this context.Generally, our forest contains all the referencederivationsRT for a sentence given a rule table, andsome neighboring non-reference derivations NT ,which can be defined fromRT .More formally, we call two hyperedges e1 and e2are competing hyperedges, if their correspondingrules r(e1) = ?
?1, ?1?
and r(e2) = ?
?2, ?2?
:?1 = ?2 ?
?1 ?= ?2 (1)This means they give different translations for asame source side.
We use C(e) to represent the setof competing hyperedges of e.Two derivations t1 = ?V 1, E1?
and t2 =?V 2, E2?
are competing derivations if there existse1 ?
E1 and e2 ?
E2: 2V 1 = V 2 ?
E1 ?
e1 = E2 ?
e2?
e2 ?
C(e1) (2)In other words, derivations t1 and t2 only differ ine1 and e2, and these two hyperedges are competinghyperedges.
We useC(t) to represent the set of com-peting derivations of tree t, and C(t,e) to representthe set of competing derivations of t if the competi-tion occurs in hyperedge e in t.Given a rule table, the set of reference derivationsRT for a sentence is determined.
Then, the set ofnon-reference derivations NT can be defined fromRT :?t?RT C(t) (3)Overall, our forest is the compact representation ofRT and NT .2The definition of derivation tree is similar to forest, exceptthat the tree contains exactly one tree while forest contains ex-ponentially trees.
In tree, the hyperedge degrades to edge.Algorithm 1 Forest Generation1: procedure GENERATE(t)2: list?
t3: for v ?
t in post order do4: e?
incoming edge of v5: append C(t, e) to list;6: for u ?
child(v) from left to right do7: tn?
OPERATE(t, u)8: if tn ?= t then9: append tn to list10: for e?
?
tn ?
e?
/?
t do11: append C(tn,e?)
to list12: if SCORE(t) < SCORE(tn) then13: t?
tn14: return t,list4 Fast GenerationIt is still slow to calculate the entire forest definedin Section 3, therefore we use a greedy decoding forfast generating a subset of the forest.
Starting forma reference derivation, we try to slightly change thederivation into a new reference derivation.
Duringthis process, we collect the competing derivationsof reference derivations.
We describe the details oflocal operators for changing a derivation in section4.1, and then introduce the creation of initial refer-ence derivation with max score in Section 4.2.For example, given derivation t1, we delete thenode X0,1 and the related hyperedge e1 and e5.
Fix-ing the other nodes and edges, we try to add a newedge e2 to create a new reference translation.
In thiscase, if rule r2 really exists in our rule table, we geta new reference derivation t3.
After constructing t3,we first collect the new tree and C(t3, e2).
Then, wewill move to t3, if the score of t3 is higher than t2.Notably, if r2 does not exist in the rule table, we failto create a new reference derivation.
In such case,we keep the origin derivation unchanged.Algorithm 1 shows the process of generation.3The input is a reference derivation t, and the out-put is a new derivation and the generated derivations.3For simplicity, we list all the trees, and do not compressthem into a forest in practice.
It is straight to extent the algo-rithm to get a compact forest for those generated derivations.Actually, instead of storing the derivations, we call the generatefunction twice to calculate gradient of log-linear model.8820,40,1 2,40,42,40,42,40,2Figure 2: Lexicalize and generalize operators over t1 (part) in Figure 1.
Although here only shows the nodes, we alsoneed to change relative edges actually.
(1) Applying lexicalize operator on the non-terminal node X0,1 in (a) results anew derivation shown in (b).
(2) When visiting bei in (b), the generalize operator changes the derivation into (c).The list used for storing forest is initialized with theinput tree (line 2).
We visit the nodes in t in post-order (line 3).
For each node v, we first append thecompeting derivations C(t,e) to list, where e is in-coming edge of v (lines 4-5).
Then, we apply oper-ators on the child nodes of v from left to right (lines6-13).
The operators returns a reference derivationtn (line 7).
If it is new (line 8), we collect both the tn(line 9), and also the competing derivationsC(tn, e?
)of the new derivation on those edges e?
which onlyoccur in the new derivation (lines 10-11).
Finally, ifthe new derivation has a larger score, we will replacethe origin derivation with new one (lines 12-13).Although there is a two-level loop for visitingnodes (line 3 and 6), each node is visited only onetime in the inner loops.
Thus, the complexity islinear with the number of nodes #node.
Consid-ering that the number of source word (also leaf nodehere) is less than the total number of nodes and ismore than ?
(#node+1)/2?, the time complexity ofthe process is also linear with the number of sourceword.4.1 Lexicalize and GeneralizeThe function OPERATE in Algorithm 1 uses two op-erators to change a node: lexicalize and generalize.Figure 2 shows the effects of the two operators.
Thelexicalize operator works on nonterminal nodes.
Itmoves away a nonterminal node and attaches thechildren of current node to its parent.
In Figure 2(b),the node X0,1 is deleted, requiring a more lexical-ized rule to be applied to the parent node X0,4 (onemore terminal in the source side).
We constrain thelexicalize operator to apply on pre-terminal nodeswhose children are all terminal nodes.
In contrast,the generalize operator works on terminal nodes andinserts a nonterminal node between current node andits parent node.
This operator generalizes over thecontinuous terminal sibling nodes left to the currentnode (including the current node).
Generalizing thenode bei in Figure 2(b) results Figure 2(c).
A newnode X0,2 is inserted as the parent of node qiang-shou and node bei.Notably, there are two steps when apply an oper-ator.
Suppose we want to lexicalize the node X0,1in t1 of Figure 1, we first delete the node X0,1 andrelated edge e1 and e5, then we try to add the newedge e2.
Since rule table is fixed, the second stepis a process of decoding.
Therefore, sometimes wemay fail to create a new reference derivation (liker2 may not exist in the rule table).
In such case, wekeep the origin derivation unchanged.The changes made by the two operators are local.Considering the change of rules, the lexicalize oper-ator deletes two rules and adds one new rule, whilethe generalize operator deletes one rule and adds twonew rules.
Such local changes provide us with a wayto incrementally calculate the scores of new deriva-tions.
We use this method motivated by Gibbs Sam-pler (Blunsom et al, 2009) which has been used forefficiently learning rules.
The different lies in thatwe use the operator for decoding where the rule ta-ble is fixing.4.2 Initialize a Reference DerivationThe generation starts from an initial referencederivation with max score.
This requires bi-parsing(Dyer, 2010) over the source sentence f and the ref-erence translation e. In practice, we may face threeproblems.First is efficiency problem.
Exhaustive searchover the space under SCFG requires O(|f |3|e|3).883To parse quickly, we only visit the tight consistent(Zhang et al, 2008) bi-spans with the help of wordalignment a.
Only visiting tight consistent spansgreatly speeds up bi-parsing.
Besides efficiency,adoption of this constraint receives support from thefact that heuristic SCFG rule extraction only extractstight consistent initial phrases (Chiang, 2007).Second is degenerate problem.
If we only usethe features as traditional SCFG systems, the bi-parsing may end with a derivation consists of somegiant rules or rules with rare source/target sides,which is called degenerate solution (DeNero et al,2006).
That is because the translation rules with raresource/target sides always receive a very high trans-lation probability.
We add a prior score log(#rule)for each rule, where #rule is the number of occur-rence of a rule, to reward frequent reusable rules andderivations with more rules.Finally, we may fail to create reference deriva-tions due to the limitation in rule extraction.
Wecreate minimum trees for (f , e,a) using shift-reduce(Zhang et al, 2008).
Some minimum rules in thetrees may be illegal according to the definition ofChiang (2007).
We also add these rules to the ruletable, so as to make sure every sentence is reachablegiven the rule table.
A source sentence is reachablegiven a rule table if reference derivations exists.
Werefer these rules as added rules.
However, this mayintroduce rules with more than two variables and in-crease the complexity of bi-parsing.
To tackle thisproblem, we initialize the chart with minimum par-allel tree from the Zhang et al (2008) algorithm,ensuring that the bi-parsing has at least one path tocreate a reference derivation.
Then we only need toconsider the traditional rules during bi-parsing.5 TrainingWe use the forest to train a log-linear model with alatent variable as describe in Blunsom et al(2008).The probability p(e|f) is the sum over all possiblederivations:p(e|f) =?t??
(e,f)p(t, e|f) (4)where ?
(e, f) is the set of all possible derivationsthat translate f into e and t is one such derivation.44Although the derivation is typically represent as d, we de-notes it by t since our paper use tree to represent derivation.Algorithm 2 Training1: procedure TRAIN(S)2: Training Data S = {fn, en,an}Nn=13: Derivations T = {}Nn=14: for n = 1 to N do5: tn ?
INITIAL(fn, en,an)6: i?
07: for m = 0 to M do8: for n = 0 to N do9: ?
?
LEARNRATE(i)10: (?L(wi, tn), tn)?GENERATE(tn)11: wi ?
wi + ?
?
?L(wi, tn)12: i?
i + 113: return?MNi=1 wiMNThis model defines the conditional probability ofa derivation t and the corresponding translation egiven a source sentence f as:p(t, e|f) = exp?i ?ihi(t, e, f)Z(f) (5)where the partition function isZ(f) =?e?t??
(e,f)exp?i?ihi(t, e, f) (6)The partition function is approximated by our for-est, which is labeled as Z?
(f), and the derivationsthat produce reference translation is approximatedby reference derivations in Z?
(f).We estimate the parameters in log-linear modelusing maximum a posteriori (MAP) estimator.
Itmaximizes the likelihood of the bilingual corpusS = {fn, en}Nn=1, penalized using a gaussian prior(L2 norm) with the probability density functionp0(?i) ?
exp(?
?2i /2?2).
We set ?2 to 1.0 in ourexperiments.
This results in the following gradient:?L?
?i= Ep(t|e,f)[hi]?
Ep(e|f)[hi]?
?i?2 (7)We use an online learning algorithm to train theparameters.
We implement stochastic gradient de-scent (SGD) recommended by Bottou.5 The dy-namic learning rate we use is N(i+i0) , where N is the5http://leon.bottou.org/projects/sgd884number of training example, i is the training itera-tion, and i0 is a constant number used to get a initiallearning rate, which is determined by calibration.Algorithm 2 shows the entire process.
We firstcreate an initial reference derivation for every train-ing examples using bi-parsing (lines 4-5), and thenonline learn the parameters using SGD (lines 6-12).We use the GENERATE function to calculate the gra-dient.
In practice, instead of storing all the deriva-tions in a list, we traverse the tree twice.
The firsttime is calculating the partition function, and thesecond time calculates the gradient normalized bypartition function.
During training, we also changethe derivations (line 10).
When training is finishedafter M epochs, the algorithm returns an averagedweight vector (Collins, 2002) to avoid overfitting(line 13).
We use a development set to select totalepoch m, which is set as M = 5 in our experiments.6 ExperimentsOur method is able to train a large number of fea-tures on large data.
We use a set of word contextfeatures motivated by word sense disambiguation(Chan et al, 2007) to test scalability.
A word levelcontext feature is a triple (f, e, f+1), which countsthe number of time that f is aligned to e and f+1 oc-curs to the right of f .
Triple (f, e, f?1) is similar ex-cept that f?1 locates to the left of f .
We retain wordalignment information in the extracted rules to ex-ploit such features.
To demonstrate the importanceof scaling up the size of training data and the effectof our method, we compare three types of trainingconfigurations which differ in the size of featuresand data.MERT.
We use MERT (Och, 2003) to training 8features on a small data.
The 8 features is the sameas Chiang (2007) including 4 rule scores (direct andreverse translation scores; direct and reverse lexi-cal translation scores); 1 target side language modelscore; 3 penalties for word counts, extracted rulesand glue rule.
Actually, traditional pipeline oftenuses such configuration.Perceptron.
We also learn thousands of contextword features together with the 8 traditional featureson a small data using perceptron.
Following (Chianget al, 2009), we only use 100 most frequent wordsfor word context feature.
This setting use CKY de-TRAIN RTRAIN DEV TEST#Sent.
519,359 186,810 878 3,789#Word 8.6M 1.3M 23K 105KAvg.
Len.
16.5 7.3 26.4 28.0Lon.
Len.
99 95 77 116Table 1: Corpus statistics of Chinese side, where Sent.,Avg., Lon., and Len.
are short for sentence, longest,average, and length respectively.
RTRAIN denotes thereachable (given rule table without added rules) subset ofTRAIN data.coder to generate n-best lists for training.
The com-plexity of CKY decoding limits the training data intoa small size.
We fix the 8 traditional feature weightsas MERT to get a comparable results as MERT.Our Method.
Finally, we use our method to trainmillions of features on large data.
The use of largedata promises us to use full vocabulary of trainingdata for the context word features, which results mil-lions of fully lexicalized context features.
Duringdecoding, when a context feature does not exit, wesimply ignore it.
The weights of 8 traditional fea-tures are fixed the same as MERT also.
We fix theseweights because the translation feature weights fluc-tuate intensely during online learning.
The main rea-son may come from the degeneration solution men-tioned in Section 4.2, where rare rules with very hightranslation probability are selected as the referencederivations.
Another reason could be the fact thattranslation features are dense intensify the fluctua-tion.
We leave learning without fixing the 8 featureweights to future work.6.1 DataWe focus on the Chinese-to-English translation taskin this paper.
The bilingual corpus we use con-tains 519, 359 sentence pairs, with an average lengthof 16.5 in source side and 20.3 in target side,where 186, 810 sentence pairs (36%) are reach-able (without added rules in Section 4.2).
Themonolingual data includes the Xinhua portion ofthe GIGAWORD corpus, which contains 238M En-glish words.
We use the NIST evaluation sets of2002 (MT02) as our development set, and sets ofMT03/MT04/MT05 as test sets.
Table 2 shows thestatistics of all bilingual corpus.We use GIZA++ (Och and Ney, 2003) to perform885System #DATA #FEAT MT03 MT04 MT05 ALLMERT 878 8 33.03 35.12 32.32 33.85Perceptron 878 2.4K 32.89 34.88 32.55 33.76Our Method 187K 2.0M 33.64 35.48 32.91* 34.41*519K 13.9M 34.19* 35.72* 33.09* 34.69*Improvement over MERT +1.16 +0.60 +0.77 +0.84Table 2: Effect of our method comparing with MERT and perceptron in terms of BLEU.
We also compare our fastgeneration method with different data (only reachable or full data).
#Data is the size of data for training the featureweights.
* means significantly (Koehn, 2004) better than MERT (p < 0.01).word alignment in both directions, and grow-diag-final-and (Koehn et al, 2003) to generate symmet-ric word alignment.
We extract SCFG rules as de-scribed in Chiang (2007) and also added rules (Sec-tion 4.2).
Our algorithm runs on the entire trainingdata, which requires to load all the rules into thememory.
To fit within memory, we cut off thosecomposed rules which only happen once in the train-ing data.
Here a composed rule is a rule that can beproduced by any other extracted rules.
A 4-gramslanguage model is trained by the SRILM toolkit(Stolcke, 2002).
Case-insensitive NIST BLEU4 (Pa-pineni et al, 2002) is used to measure translationperformance.The training data comes from a subset of theLDC data including LDC2002E18, LDC2003E07,LDC2003E14, Hansards portion of LDC2004T07,LDC2004T08 and LDC2005T06.
Since the rule ta-ble of the entire data is too large to be loaded tothe memory (even drop one-count rules), we removemany sentence pairs to create a much smaller datayet having a comparable performance with the entiredata.
The intuition lies in that if most of the sourcewords of a sentence need to be translated by theadded rules, then the word alignment may be highlycrossed and the sentence may be useless.
We cre-ate minimum rules from a sentence pair, and countthe number of source words in those minimum rulesthat are added rules.
For example, suppose the resultminimum rules of a sentence contain r3 which is anadded rule, then we count 1 time for the sentence.
Ifthe number of such source word is more than 10%of the total number, we will drop the sentence pair.We compare the performances of MERT settingon three bilingual data: the entire data that contains42.3M Chinese and 48.2M English words; 519Kdata that contains 8.6M Chinese and 10.6M En-glish words; FBIS (LDC2003E14) parts that con-tains 6.9M Chinese and 9.1M English words.
Theyproduce 33.11/32.32/30.47 BLEU tested on MT05respectively.
The performance of 519K data is com-parable with that of entire data, and much higherthan that of FBIS data.6.2 ResultTable 3 shows the performance of the three differenttraining configurations.
The training of MERT andperceptron run on MT02.
For our method, we com-pare two different training sets: one is trained onall 519K sentence pairs, the other only uses 186Kreachable sentences.Although the perceptron system exploits 2.4Kfeatures, it fails to produce stable improvementsover MERT.
The reason may come from overfitting,since the training data for perceptron contains only878 sentences.
However, when use our method tolearn the word context feature on the 519K data,we significantly improve the performance by 0.84points on the entire test sets (ALL).
The improve-ments range from 0.60 to 1.16 points on MT03-05.
Because we use the full vocabulary, the num-ber of features increased into 13.9 millions, which isimpractical to be trained on the small developmentset.
These results confirm the necessity of exploitingmore features and learning the parameters on largedata.
Meanwhile, such results also demonstrate thatwe can benefits from the forest generated by our fastmethod instead of traditional CKY algorithm.Not surprisingly, the improvements are smallerwhen only use 186K reachable sentences.
Some-times we even fail to gain significant improvement.This verifies our motivation to guarantee all sentence88603060901201501800  10  20  30  40  50  60  70  80  90TrainingTime(Milliseconds)Sentence LengthFigure 3: Plot of training times (including forest genera-tion and SGD training) versus sentence length.
We ran-domly select 1000 sentence from the 519K data for plot-ting.are reachable, so as to use all training data.6.3 SpeedHow about the speed of our framework?
Our methodlearns in 32 mlliseconds/sentence.
Figure 3 showstraining times (including forest generation and SGDtraining) versus sentence length.
The plot confirmsthat our training algorithm scales linearly.
If weuse n-best lists which generated by CKY decoderas MERT, it takes about 3105 milliseconds/sentencefor producing 100-best lists.
Our method acceleratesthe speed about 97 times (even though we searchtwice to calculate the gradient).
This shows the effi-ciency of our method.The procedure of training includes two steps.
(1)Bi-parsing to initialize a reference derivation withmax score.
(2) Training procedure which generatesa set of derivations to calculate the gradient and up-date parameters.
Step (1) only runs once.
The av-erage time of processing a sentence for each stepis about 9.5 milliseconds and 30.2 milliseconds re-spectively.For simplicity we do not compress the generatedderivations into forests, therefore the size of result-ing derivations is fairly small, which is about 265.8for each sentence on average, where 6.1 of them arereference derivations.
Furthermore, we use lexical-ize operator more often than generalize operator (theration between them is 1.5 to 1).
Lexicalize operatoris used more frequently mainly dues to that the ref-erence derivations are initialized with reusable (thussmall) rules.7 Related WorkMinimum error rate training (Och, 2003) is perhapsthe most popular discriminative training for SMT.However, it fails to scale to large number of features.Researchers have propose many learning algorithmsto train many features: perceptron (Shen et al, 2004;Liang et al, 2006), minimum risk (Smith and Eisner,2006; Li et al, 2009), MIRA (Watanabe et al, 2007;Chiang et al, 2009), gradient descent (Blunsom etal., 2008; Blunsom and Osborne, 2008).
The com-plexity of n-best lists or packed forests generationhamper these algorithms to scale to a large amountof data.For efficiency, we only use neighboring deriva-tions for training.
Such motivation is same as con-trastive estimation (Smith and Eisner, 2005; Poon etal., 2009).
The difference lies in that the previouswork actually care about their latent variables (postags, segmentation, dependency trees, etc), whilewe are only interested in their marginal distribution.Furthermore, we focus on how to fast generate trans-lation forest for training.The local operators lexicalize/generalize are usefor greedy decoding.
The idea is related to ?peg-ging?
algorithm (Brown et al, 1993) and greedy de-coding (Germann et al, 2001).
Such types of localoperators are also used in Gibbs sampler for syn-chronous grammar induction (Blunsom et al, 2009;Cohn and Blunsom, 2009).8 Conclusion and Future WorkWe have presented a fast generation algorithm fortranslation forest which contains both referencederivations and neighboring non-reference deriva-tions for large-scale SMT discriminative training.We have achieved significantly improvement of 0.84BLEU by incorporate 13.9M feature trained on 519Kdata in 0.03 second per sentence.In this paper, we define the forest based on com-peting derivations which only differ in one rule.There may be better classes of forest that can pro-duce a better performance.
It?s interesting to modifythe definition of forest, and use more local operatorsto increase the size of forest.
Furthermore, since thegeneration of forests is quite general, it?s straight to887apply our forest on other learning algorithms.
Fi-nally, we hope to exploit more features such as re-ordering features and syntactic features so as to fur-ther improve the performance.AcknowledgementWe would like to thank Yifan He, Xianhua Li, DaqiZheng, and the anonymous reviewers for their in-sightful comments.
The authors were supported byNational Natural Science Foundation of China Con-tracts 60736014, 60873167, and 60903138.ReferencesPhil Blunsom and Miles Osborne.
2008.
Probabilisticinference for machine translation.
In Proc.
of EMNLP2008.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In Proc.
of ACL-08.Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A gibbs sampler for phrasal synchronousgrammar induction.
In Proc.
of ACL 2009.Peter F. Brown, Vincent J.Della Pietra, Stephen A. DellaPietra, and Robert.
L. Mercer.
1993.
The mathemat-ics of statistical machine translation.
ComputationalLinguistics, 19:263?311.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word sense disambiguation improves statistical ma-chine translation.
In Proc.
of ACL 2007, pages 33?40.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine translation.In Proc.
of NAACL 2009.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Trevor Cohn and Phil Blunsom.
2009.
A Bayesian modelof syntax-directed tree to string grammar induction.
InProc.
of EMNLP 2009.Michael Collins.
2002.
Discriminative training methodsfor hidden markov models: Theory and experimentswith perceptron algorithms.
In Proc.
of EMNLP 2002.John DeNero, Dan Gillick, James Zhang, and Dan Klein.2006.
Why generative phrase models underperformsurface heuristics.
In Proc.
of the HLT-NAACL 2006Workshop on SMT.Chris Dyer.
2010.
Two monolingual parses are betterthan one (synchronous parse).
In Proc.
of NAACL2010.Ulrich Germann, Michael Jahr, Kevin Knight, DanielMarcu, and Kenji Yamada.
2001.
Fast decoding andoptimal decoding for machine translation.
In Proc.
ofACL 2001.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of HLT-NAACL 2003.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
of EMNLP2004.Zhifei Li and Jason Eisner.
2009.
First- and second-orderexpectation semirings with applications to minimum-risk training on translation forests.
In Proc.
of EMNLP2009.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009.Variational decoding for statistical machine transla-tion.
In Proc.
of ACL 2009.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminative ap-proach to machine translation.
In Proc.
of ACL 2006.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proc.
of ACL 2008.Franz J. Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statisticalmachine translation.
In Proc.
of ACL 2002.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL 2003.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proc.
of ACL 2002.Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentation withlog-linear models.
In Proc.
of NAACL 2009.Libin Shen, Anoop Sarkar, and Franz Josef Och.
2004.Discriminative reranking for machine translation.
InProc.
of NAACL 2004.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: Training log-linear models on unlabeled data.In Proc.
of ACL 2005.David A. Smith and Jason Eisner.
2006.
Minimum riskannealing for training log-linear models.
In Proc.
ofCOLING/ACL 2006.Andreas Stolcke.
2002.
Srilm ?
an extensible languagemodeling toolkit.
In Proc.
of ICSLP 2002.Christoph Tillmann and Tong Zhang.
2006.
A discrim-inative global training algorithm for statistical mt.
InProc.
of ACL 2006.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for statis-tical machine translation.
In Proc.
of EMNLP-CoNLL2007.Hao Zhang, Daniel Gildea, and David Chiang.
2008.
Ex-tracting synchronous grammar rules from word-levelalignments in linear time.
In Proc.
of Coling 2008.888
