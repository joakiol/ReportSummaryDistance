Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1559?1568,Denver, Colorado, May 31 ?
June 5, 2015. c?2015 Association for Computational LinguisticsDo We Really Need Lexical Information?
Towards a Top-downApproach to Sentiment Analysis of Product ReviewsYulia OtmakhovaComputational Linguistics LabDepartment of LinguisticsHyopil ShinComputational Linguistics LabDepartment of LinguisticsSeoul National University Seoul National UniversityGwanakro 1, Gwanak-gu Gwanakro 1, Gwanak-guSeoul, 151-742, South Korea Seoul, 151-742, South Koreajulia.nixie@gmai1.com hpshin@snu.ac.krAbstractMost of the current approaches to sentimentanalysis of product reviews are dependent onlexical sentiment information and proceed in abottom-up way, adding new layers of featuresto lexical data.
In this paper, we maintain thata typical product review is not a bag of senti-ments, but a narrative with an underlyingstructure and reoccurring patterns, which al-lows us to predict its sentiments knowing onlyits general polarity and discourse cues thatoccur in it.
We hypothesize that knowing onlythe review?s score and its discourse patternswould allow us to accurately predict thesentiments of its individual sentences.
Theexperiments we conducted prove this hypoth-esis and show a substantial improvement overthe lexical baseline.1 IntroductionFor years, sentiment analysis has heavily relied onlexical resources, whether compiled by hand(Wilson et al, 2005) or automatically extractedfrom a large corpus (Hu and Lui, 2004).
In addi-tion to an overwhelming task of trying to captureall words and expressions that can convey a senti-ment there are many other problems to solve:resolving the scope of negation to determine theshift of polarity (Lapponi et al, 2012), determiningif an opinion is present in interrogative or condi-tional sentences (Narayanan et al, 2009), dealingwith irony (Tsur, 2010), etc.
But even if wemanage to solve all aforementioned problems andcreate an efficient classifier, there will always becases where reliance on lexical cues forsubjectivity will betray us.
Consider, for instance,the following examples from reviews of onlineuniversities1:(1) The lectures are interactive and recorded.So, if you can't attend you can listen inlater.
(2) I assure you, online learning at Capella wasthe most difficult form of education I haveundergone!
(3) UMUC provided really good quality educa-tion until about 5 years ago.In the first example, the author expresses a positiveopinion of the university, but it will fail to bedetected because it does not include any explicitsentiment cues (such opinions are referred to as?implicit?
by Liu (2012) or as ?polar facts?
byToprak et al (2010)).
Because the sentiment (andits presence) of such sentences is highly domain-dependent, they cannot be covered by any lexiconsor learned in a supervised or a non-supervised way.The second example does have a sentiment cuedifficult, and judging by it the sentiment should be1  The examples in this section are taken from DarmstadtService Review Corpus, available from https://www.ukp.tu-darmstadt.de/data/sentiment-analysis/darmstadt-service-review-corpus (Toprak et al, 2010).
The corpus was also usedas a development set for extracting features for this study.1559negative.
However, in this case the author actuallyexpresses a positive view of an online university,defending it from people who claim that onlineeducation is ?too easy?.
In the third example, thecorrect sentiment (negative) would again be im-possible to determine because of a complicatedstructure.These are just a few examples of what is cur-rently impossible to classify correctly relying onlexical resources.
To improve the classificationresults, there have been attempts to use localdiscourse information, such as discourse cues andpolarity of adjacent sentences, in order to correctsome of the misclassified sentences(Somasundaran, 2010).
However, though suchattempts resulted in some improvements, they alsorequired quite complicated frameworks.While such bottom-up approach (starting fromlexical polarity and adding supplementaryinformation to improve classification on a phraseand text level) is commonly used in sentimentanalysis, we are wondering if it is the only validone.
Provided that we have a reliable externalmeasure of a text?s general polarity (such as aproduct rating for a product review) and thenarrative has a predictable discourse structure,would not it be possible to classify its sentences ina top-down manner, without using any sentimentlexicons?
In this paper, we experiment with thisapproach and compare its results with those of thetraditional bottom-up method.This paper is organized as follows.
Section 2presents a brief overview of previous studiesrelated to sentiment analysis of product reviews,while section 3 explains the motivation behindtaking an alternative approach.
In section 4 wegive the details of the experiments, and then insection 5 present their results.
Lastly, section 6summarizes our findings.2 Previous StudiesSentiment analysis so far has largely relied onexplicit lexical information, either in the form ofsentiment dictionaries and lexicons, such asSentiWordNet 2  or Subjectivity lexicon 3 , opinionphrases extracted from a manually-annotatedcorpus or a dataset compiled in real time using2 http://sentiwordnet.isti.cnr.it/ (Baccianella et al, 2010)3 http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/ (Wilson et al,2005)machine learning with such lexical features as bag-of-words features, n-grams, collocations, or moresophisticated lexical patterns (Tang, 2009).
Asresearchers realized the limitations of a purelylexical approach, they tried to augment it by usingnegation resolution, word meaning disambiguationor hand-crafted rules (Ding, 2008).
However,though such efforts improved classification on thesentence level, they were not able to deal with thesentences where an opinion was implicit (i.e.
therewere no appraisal words or other lexical cues, seeexample 1 above) or the polarity of the sentimentword was different from the usual one (see exam-ple 2 above).
To correct such misclassified in-stances, another level of complexity was added byusing discourse features.
Somasundaran (2010)defines opinion frames to enforce discourse con-straints on the polarity of segments with the sameor alternative target relations.
Using a similarapproach, Zhou et al (2011) employ simplifiedRhetorical Structure Theory (RST) relation cues(contrast, condition, continuation, cause andpurpose) to eliminate polarity ambiguities.
Yang(2014) concentrates on discovering opinionatedsentences which do not have strong sentimentsignals (implicit opinions), using discourseknowledge to improve the results of a ConditionalRandom Fields classifier.
While such approachesare a definite improvement over the lexicalbaseline, they are computationally complex andstill overly dependent on the lexical cues.While machine learning algorithms such asNa?ve Bayes or SVM are still the primary toolsused for sentiment analysis, lately such texts asproduct reviews have been recognized as having aninternal structure and inter-sentential relations, andthus structural conditional frameworks have beenused for their classification.
One popular tool isConditional Random Fields (CRF), which wasused, among others, by Zhao (2008) to classifysentiments on a sentence level, by Breck (2007) toidentify subjective expressions, and by Li (2010) tosummarize product reviews taking their structureinto account.3 Motivation behind Top-down ApproachThough most of the previous studies treat productreviews as a bag of sentences or even words, infact they are narratives that have a specificstructure.
While their structure is less rigid and1560predicable than, say, that of research papers, itnevertheless has some recurring patterns whichlend themselves to generalization.The same principle applies to sentiments appear-ing in product reviews.
The authors of reviews donot simply pile up some random facts about theproduct or their evaluations of it.
To the best oftheir abilities, they try to convince the reader tobuy or not to buy a particular product, and,according to Grice?s Maxims (Grice, 1975), theydo it in the clearest and most effective waypossible.
Thus, if an author has in general apositive opinion of a product, the probability of anegative sentence appearing in a review is lowerthan that of a positive sentence, and even if anegative sentence is introduced, it is likely toappear together with a concession or a contrastmarker, such as although or but, or be modified bya hedging expression, such as might, only, could be,which mitigate the negative effect on the reader.Thus the author makes us understand that hisprimary opinion of the product is still positive, anduses the discourse relation of contrast to present anopposite opinion.Likewise, if an objective sentence appears in areview, it is not a random event, but a tool servingsome purpose, such as interacting with a reader byasking questions which do not require an answer(Where do I start?)
or supporting one?s view byshowing that you have some expertise necessary toprovide a valid opinion.
While in this paper wecover only objective sentences that are used toprovide background information (the discourserelation of background), it is clear that other rea-sons for usage of objective sentences are presentand capable of being formalized.The facts mentioned above make us consider aproduct review as a text which has a primarypolarity and optionally includes some segmentswhich have an opposite polarity or no polarity atall (objective sentences).
Instead of relying onlexical sentiment information, which makes itdifficult to distinguish between objective andsubjective sentences on one hand (implicit opin-ions) and between positive and negative opinionson the other (sarcasm, context-dependent polarity),we suggest using a top-down approach: determin-ing the primary polarity of a review based on anexternal source of information, such as productrating, and then locating segments which do notconform with this polarity (have no polarity or anopposite polarity) by finding cues that mark achange in a discourse flow.In the next section we describe an experimentwhich we conducted to confirm that this approachis viable.4 Experiment4.1 Data and TaskFor the experiments in this study we use Filatova?sAmazon product reviews corpus (so called Sar-casm Corpus4), consisting of 817 ironic and regularreviews.
We chose to use this corpus because webelieved that segments in ironic reviews would bedifficult to classify by purely lexical means.
Out ofthese 817 reviews we randomly selected 100reviews for training and 20 reviews for test data.We did not use the whole dataset because thenumber of reviews with a particular review scorediffers greatly (60% of reviews are 5-star, whileonly 5% are 2-star).
To prevent a skew towardspositive labels we used equal-size random samplesof reviews with all possible scores.
Reviews wereannotated by one of the authors and an externalannotator on a clause level if a sentence containedopinions with opposite polarities, and on asentence level otherwise.
The inter-annotatoragreement was measured by using Fleiss?
kappaand Krippendorff's alpha, and the results showedthat the annotation was highly reliable (?
= 0.912,?
= 0.913).
Overall the training corpus consisted of843 segments (438 negative, 268 positive and 137objective), while the test set contained 145segments (78 negative, 41 positive and 26objective).While the studies in sentiment analysis usuallymake distinction between subjective and objectivesentences on one hand and between negative,positive and neutral sentences on the other, in thispaper we make a twofold distinction, first classi-fying a segment as objective or subjective, andthen, in case of subjective (polar) sentences, fur-ther subdividing them into positive and negative.To our mind the classification into positive, nega-tive and neutral sentences, commonly adopted forproduct reviews, is incorrect, as neutral sentimentsrarely, if ever, appear in reviews.
What is com-4 http://storm.cis.fordham.edu/~filatova/SarcasmCorpus.html(Filatova, 2012).1561monly referred to as neutral sentences should beclassified as objective segments, as they do notcarry any sentiment related to the subject matter.When annotating the corpus we considered theintended semantic orientation of a segment, not itsliteral meaning and the presence and polarity oflexical cues.
Thus, segments without any lexicalcues could be annotated both as subjective andobjective:(4) I bought this mobo from Amazon, afterbuying the same month the DG31PRClassic for my wife.
(objective)(5) After I install my new PC, the 2do.
day ofuse, the LAN failed.
(subjective, negative)Segments with a lexical cue of a certain polaritycould be annotated both as positive and negative:(6) The ring is nice and heavy.
(positive)(7) It's going to be a nice paperweight.
(nega-tive, from a review of a camera)Finally, segments where an alternative product waspraised or preferred were understood to be a criti-cism towards the reviewed product:(8) I will never buy another Panasonic product.There are plenty of other brands that areloyal to their customers.
(both segments arenegative)We view each of the reviews as a separatediscourse with its own sentiment flow, and thustreat the sentiment analysis problem as a sequenceclassification task.
We employ the CRF method,which outperforms other methods of sequencelabeling (Lafferty, 2001).
In CRFs the probabilityof a sequence is defined as)(),(exp)|( XZXYFXYp???
??where?
?
?yxyFxZ ),(exp)( ?
?where X is a set of input random variables, Y is aset of labels, and ?
is a weight for the featurefunction F(Y,X).
(Sha, 2003).All experiments in this paper were conductedusing a C++ implementation of a linearConditional Random Fields classifier (CRF++) 5 .Though more complex or constrained types ofCRF classifiers and models based on them provedto be more suitable for sentiment analysis (Mao,2006; Yang, 2014), we use the simplest model as aproof of concept in this study.Each review in the training and test data is con-verted into a sequence of polarity segments as-signed to it.
For example, the following shortreview:(9) The ring is nice and heavy.
Have beenwearing if for almost a month and still not ascratch!is presented as a sequence of tokens POSITIVEPOSITIVE, based on the sentiment labels from theannotation.
The tokens are assigned features, asdefined in the following sections, which are thenfed into the classifier.4.2 Features for Experiments4.2.1 Lexical FeaturesTo set a baseline, we use a state-of-art lexicalclassifier ?
Stanford Sentiment Analysis Classifierfrom Stanford CoreNLP toolkit6 ?
to determine thelexical polarity of each individual sentence.
Thusthe lexical classifier considers only lexical featuresavailable in a particular sentence without lookingat neighboring sentences or discourse cues.
For thelocal context classifier we also determine thelexical polarity of the previous and next sentencesand use the sequence of {prev_polarity,current_polarity, next_polarity} as a feature (asimilar approach is taken by Somasundaran(2010)).
This is done to disambiguate and, ifnecessary, to correct the polarity of misclassifiedinstances that are sandwiched between thecorrectly classified ones.
For example, if thelexical classifier fails to detect an implicit opinion5 Available from http://taku910.github.io/crfpp/6 Available from http://nlp.stanford.edu/sentiment/code.html1562in a sentence that appears between two explicitopinions, it might correct it as follows:POSITIVE OBJECTIVE POSITIVE ->POSITIVE POSITIVE POSITIVE4.2.2 Contrast FeaturesThe main drawback of the local context classifieris that it can misclassify sentences with theopposite polarity, lumping them together withsentences of the primary polarity.
To prevent this,for the contrast classifier we add another set offeatures ?
discourse cues with a Rhetoric StructureTheory (RST) (Mann and Thompson, 1988)relation of contrast.
We consider both explicit andimplicit discourse markers of contrast for this setof features:4.2.2.1 Explicit Contrast MarkersContrast relations are primarily realized by usingexplicit discourse markers, which, depending ontheir type, mark the sentence they appear in (incase of although type) or the previous sentence (incase of but type) as contrasting:(10) The Phillips screwdriver on the end of oneof the tines is helpful for things like tighten-ing eyeglasses, POSITIVE CONTRbut it is slightly offset from the opposingblade and I've nicked or jabbed myself withit more than once while it's in my pocket.NEGATIVE NCONTR(11) Although it has 10 workable buttonswhich come in handy for some games,POSITIVE CONTRit has some major flaws.
NEGATIVENCONTRThe segment with the NCONTR marker usuallyhas the primary polarity of the review, while thesegment with a CONTR marker presents a con-trasting opinion.4.2.2.2 Implicit Contrast RelationsContrast relations can also be manifested implicitlythrough the use of hedges.
Hedging is often usedwhen the review?s author wants to mention somenegative side of a product they like (or a positiveaspect of a product they hate), but does not want toput an unnecessary emphasis on it.
Such hedgingexpressions as the only good/bad point, the onlydrawback, I would only recommend it?
etc areused for this purpose:(12) With all the upgrades that Apple has donewith their macbooks, I think I finally feelthat it's worth the spending to buy my firstmac.
NHEDGEMy only complain is that it's still a lot moreexpensive than PC laptops with similarspecs.
HEDGE4.2.3 Background ClassifierThe background classifier allows us to capturesome of the objective sentences that are related tothe polar ones using a background RST relation.We identify three types of patterns where back-ground relations are used:1.
Acquirement patterns: people often start re-views with an explanation of how they gotthe product.2.
Personal background patterns: people oftensupport their evaluation of a product bystating who there are, what they do for aliving, what kind of lifestyle they lead etc.3.
Personal experience patterns: again, tosupport their views the writers prime theirreaders by describing their experiences orachievements.Unfortunately, background relations, unlike con-trast relations, are almost never explicit.
They areparatactic and lack discourse cues, so we need torely on lexical and grammatical features for classi-fication.
However, we believe that because back-ground information is usually presented in easy-to-predict patterns, it is more feasible andcomputationally inexpensive to use lexical cues tosingle out objective sentences than to try to captureinfinitely large number of ways sentiments can beexpressed.
In the following subsections, wedescribe these patterns in more detail and explainwhich lexical and grammatical cues can be used todetect them.15634.2.3.1 Acquirement PatternsAt the beginning of a review people often explainhow they acquired the product:(13) I bought this camera for my deployment toIraq.
(objective)It was in my cargo pants pocket one day Itook it out and the lens was cracked and thesilver trim ring had fallen off.
(negative)We formalize this feature as follows:[I | we] [verb synonymous to ?acquire?|verb ofdecision + verb synonymous to ?acquire?
],or, more specifically:[I | we] [ordered | bought | got .
* as a gift | pur-chased | decided to buy?
]All verbs are in past simple tense, as only in thistense they are unlikely to bear any sentiment(compare, for example, sentences with the sameverbs in present perfect tense:(14) However, I am glad that I have bought amac.
(positive)(15) This is probably the worst book I?vebought.
(negative)Moreover, this pattern is likely to be used at thebeginning of the review, so we add ACQUIREfeature only to those segments which appear in thefirst 25% of a review.4.2.3.2 Personal Background PatternsIn these patterns, the authors offer their personalinformation that is relevant to the subject matter ofthe review and can support their opinion.
Forinstance, in the following review the author refersto his pets as the major reason for buying aparticular vacuum cleaner:(16) I have a cat and a dog, and there is lots ofshedding hair, all the time.
(objective, per-sonal background)When I saw the DC25 Animal, I decided tospend the money hoping that this vacuumwould do the job.
(objective, acquirement)It has lived up to my wildest dreams, it iswonderfully easy to handle, so easy to ma-neuver, the 16 lbs make such a differencecompared to those very heavy machines Ihad before, I had no problem carrying it up-stairs.
(positive)We formalize this feature as follows:[I|we] [am (a|an)|have (a|an)|'m (a|an)|am not(a|an)]The indefinite article is used to prevent matchingsuch polar expressions, as I?m very pleased withthe quality of this product.
Again, such patterns aresearched for only at the beginning of a review.4.2.3.3 Personal Experience PatternsThese patterns also serve to provide some back-ground information about user?s experiences toback up his opinion on a product:(17) Usually I am a huge fan of hats that looklike food.
(objective, personal background).My meatloaf hat has been a hit for years.
(objective, personal experience)When I received my turkey hat I carefullyunwrapped the bubble wrap and gazed uponits tan beauty.
(positive).To capture this pattern we search for verbs inperfect forms (except for the verbs of possession).We exclude verbs in perfect continuous forms, asthey are more often used to describe positive ornegative results of using a product.
Compare, forexample:(18) I have been using it for almost a monthand my lashes are so long, they touch myeyebrows... (positive)We also exclude phrases that have ?should/could?before ?have?, as they often express negativesentiments (Liu, 2014):1564(19) Would have been nice if the stilts couldaccommodate multiple/varying heights.
(negative)4.2.4 Primary Polarity FeaturesLastly, we use reviews?
scores to predict theirglobal semantic orientation (primary polarity).
Theintuition behind this is that the reviews with ahigher score will contain more positive sentencesthan reviews with a lower score, and thus globalpolarity information might help us to amendincorrect predictions of a lexical classifier (asimilar approach was taken, among others, by(Yang, 2014)).
This is supported by the statistics ofour corpus: the polarity of sentences in a review ingeneral correlates with its score.
Highly positive(5-star) and highly negative (1-star) reviewscontain few segments of the opposite polarity, andeven reviews with a less extreme scoredemonstrate a clear preference of one of thepolarities (see Table 1).
Thus it can be predictedthat the classifier using this feature will tend toassign the primary polarity (positive for 4- and 5-star reviews, negative for 1-, 2-, and 3-starreviews) unless there is some strong evidencepreventing it.Review score Positive Negative Objective1 0.01 0.85 0.132 0.10 0.77 0.123 0.22 0.65 0.134 0.62 0.23 0.155 0.68 0.04 0.27Table 1.
Percentage of positive, negative andobjective sentences in reviews with different productratings4.3 Bottom-up vs Top-down Approach:Experiment Design4.3.1 Bottom-up ApproachThis is a widely-used approach which relies on alexical polarity classifier to determine the semanticorientation of each segment and then corrects themisclassified segments by employing more generalfeatures: discourse features (in our study ?
contrastand background) and global semantic orientationfeatures (called primary polarity features in thispaper).The bottom-up approach has become a standard insentiment analysis, so we believe there is no needto explain it in detail.
The main focus of this studyis on the top-down approach, which we describebelow.4.3.2.
Top-down ApproachIn this set of experiments, we do not use anylexical information about the presence ofsentiments in segments and their types.
Instead, werely on rating scores assigned to the reviews todetermine their primary polarity, and then correctthe misclassified instances using discoursefeatures.
In general, the feature set used for thisclassifier is the same as for the bottom-upapproach.
The only important exception is thatlexical features are completely omitted.We adopt the following process for sentimentclassification:1.
All sentences in a review are assigned apolarity label determined by the corre-sponding review rating.2.
We look for discourse patterns that are as-sociated with a change of the primary po-larity (POSITIVE -> NEGATIVE,NEGATIVE -> POSITIVE).
These areusually manifested through contrastrelation and enable us to correct some ofmisclassified polarity labels.3.
We look for discourse patterns where a po-lar statement is accompanied by an objec-tive statement.
A common example ofsuch discourse relations in product reviewsis background.
At this stage, unnecessaryPOSITIVE and NEGATIVE labels arechanged into OBJECTIVE.Schematically this can be shown as follows usingan arbitrary example of a 4-star review, wherelight-gray blocks stand for positive segments, dark-gray for negative segments and white for objectiveones (here we assume that all segments will beinitialized as positive, as it is the primary class for4-star reviews):1565Figure 1.
Top-down approach classification flowIn the next section, we discuss the results of exper-iments conducted to show that such features canimprove sentiment classification.5 Experiment ResultsIn this section, we compare the results achieved byusing the top-down approach with those of thetraditional bottom-up method.5.1 Bottom-up ApproachThe lexical classifier results, used as a baseline inthis study, are listed in Table 2 below.
As can beseen from the results, the recall and precision ofpositive and especially objective segments is low,which shows that purely lexical classifier cannotreliably distinguish between objective andsubjective sentences and between positive andnegative ones.
The accuracy of the classifier is alsolow (0.6138).When we add local discourse context, the recallof positive and negative segments improves, asdoes the overall accuracy (to 0.6758).
However,the local discourse classifier completely ignoresthe objective sentences, assigning polarity to them.The precision of the negative class and overallprecision also gets lower, as some positivesegments sandwiched between negative ones areassigned a negative label.Adding contrast discourse cues does not help toimprove this situation, because it leads tooverestimation of positive segments and loweraccuracy (0.6620).
In fact, the contrast classifierperforms even worse than the local discourse one.It seems that lexical information introduces toomuch noise, and building up on such an unreliablebasis does not produce expected results.The background classifier improves theperformance, especially for objective sentences,classifying them with a high precision and at leastsome recall.
It also improves the overall accuracy(to 0.6896).However, the most significant improvement isseen after adding the review scores (primarypolarity) as features.
It helps improve almost allscores, including accuracy, which reaches 0.7241.5.2 Top-down ApproachThe primary polarity classifier, which uses thereview?s rating to predict its overall polarity, has ahigh recall and an accuracy of 0.7379 (see Table3).
However, it again ignores the objective class,because it is distributed more or less evenlybetween reviews with different ratings and thuscannot be correlated with a particular review score.SubjectiveObjective TotalNegative PositivePrec Rec Prec Rec Prec Rec Prec Rec F1 AccLexical 0.71 0.77 0.61 0.54 0.29 0.27 0.60 0.61 0.61 0.6138Local discourse 0.69 0.87 0.64 0.73 0.00 0.00 0.55 0.68 0.61 0.6758+ Contrast 0.69 0.87 0.62 0.68 0.00 0.00 0.55 0.66 0.60 0.6620+ Background 0.73 0.85 0.60 0.75 1.00 0.12 0.74 0.69 0.65 0.6896+ Primary pol.
0.78 0.87 0.62 0.82 1.00 0.12 0.77 0.72 0.68 0.7241Table 2.
Precision, recall, F1 and accuracy scores for the bottom-up approach1566SubjectiveObjective TotalNegative PositivePrec Rec Prec Rec Prec Rec Prec Rec F1 AccPrimary polarity 0.75 0.94 0.71 0.83 0.00 0.00 0.61 0.74 0.66 0.7379+ Contrast 0.81 0.90 0.68 0.95 0.00 0.00 0.63 0.75 0.68 0.7517+ Background 0.82 0.91 0.73 0.95 1.00 0.19 0.83 0.79 0.76 0.7931Table 3.
Precision, recall, F1 and accuracy scores for the top-down approachAlso, because 3-star reviews contain more negativesentences than positive ones, all of them arelumped into the negative class.
Thus the recall forthe positive class is substantially low than for thenegative class.To single out the segments whose polarity isdifferent from the primary one, we add explicit andimplicit contrast features and train the contrastclassifier.
Contrast features help to raise recall forpositive and precision for negative sentences,though, as might be expected, they do not affectthe classification of objective segments.
However,the overall precision and recall are improved, aswell as the overall accuracy (to 0.7517)The background classifier allows us to findsome of objective sentences.
Acquirement,personal background and personal experiencepatterns turn out to be precise features that alsoguarantee us at least some recall for objectivesentences.
Overall precision, recall and F1 scoresare improved accordingly, as well as accuracy (to0.7931).As can be seen from comparing these results,even the most primitive rating-based classifier(primary polarity) achieves better recall andaccuracy than any of lexical classifiers (even theone with primary polarity features).
Moreover,adding discourse features to it consistentlyimproves the results, allowing us to build a high-precision, high-recall sentiment classifier.
On theother hand, building up on the lexical classifierdoes not show such consistent improvements.6 ConclusionUntil now the sentiment analysis has beenprimarily done in a bottom-up way, starting withthe classification of lexical items, then resolvingthe polarity of the sentence, then using discourseinformation to improve the lexical classification.However, lexical classifiers so far produce resultsthat are too unreliable to become a basis of adiscourse-level classification.
We assert thatstarting from the top by roughly defining a text?spolarity and assigning it to all its segments, andthen fine-tuning the classification by ?chiselingout?
incorrect bits based on reliable discourse rela-tions can be a more productive and effectiveapproach.
Our experiments show that such ap-proach can lead to a substantial improvement overlexical baseline at least in texts with a predictablestructure and recurring patterns, such as productreviews.
Because each of the discourse features wetested led to improvement, we believe that the top-down classifier can be made even more accurate byemploying other discourse relations in the form ofcarefully selected linguistic features.ReferencesBaccianella, S., Esuli, A., & Sebastiani, F. (2010).SentiWordNet 3.0: An Enhanced Lexical Resourcefor Sentiment Analysis and Opinion Mining.
LREC,10, 2200-2204.Breck E., Choi Y., & Cardie C. (2007).
IdentifyingExpressions of Opinion in Context.
IJCAI, 7, 2683-2688.Ding, X., Liu, B., & Yu, P. S. (2008).
A holisticlexicon-based approach to opinion mining.
WSDM,231-240.Filatova, E. (2012).
Irony and Sarcasm: CorpusGeneration and Analysis Using Crowdsourcing.LREC, 392-398.Grice, P. (1975).
Logic and conversation.
Syntax andsemantics, 3, 41-58.Hu, M., & Liu, B.
(2004).
Mining opinion features incustomer reviews.
AAAI, 4(4), 755-760.Lafferty, J., McCallum, A., & Pereira, F. C. (2001).Conditional random fields: Probabilistic models forsegmenting and labeling sequence data.
Proceedingsof the Eighteenth International Conference onMachine Learning, 282-289.1567Lapponi, E., Read, J., & Ovrelid, L. (2012).Representing and resolving negation for sentimentanalysis.
Proceedings of the 2012 ICDM Workshop onSentiment Elicitation from Natural Text for InformationRetrieval and Extraction, 687-692.Li F., Han C., Huang M., Zhu X., Xia Y. J., Zhang S., &Yu H. (2010).
Structure-aware review mining andsummarization.
Proceedings of the 23rdInternational Conference on ComputationalLinguistics, 653-661.Liu, B.
(2012).
Sentiment Analysis and Opinion Mining.Morgan & Claypool Publishers.Liu, Y., Yu, X., Liu, B., & Chen, Z.
(2014).
Sentence-Level Sentiment Analysis in the Presence ofModalities.
Computational Linguistics and IntelligentText Processing, 1-16.Mann, W. & Thompson, S. (1988).
Rhetorical structuretheory: Towards a functional theory of textorganization.
Text, 8(3), 243-281.Mao, Y., & Lebanon, G. (2006).
Isotonic conditionalrandom fields and local sentiment flow.
Advances inneural information processing systems, 961-968.Narayanan, R., Liu, B., & Choudhary, A.
(2009).Sentiment analysis of conditional sentences.Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, 1, 180-189.Polanyi, L., & Zaenen, A.
(2006).
Contextual valenceshifters.
Computing attitude and affect in text:Theory and applications, 1-10.Sha, F., & Pereira, F. (2003).
Shallow parsing withconditional random fields.
Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics on HumanLanguage Technology, 1, 134-141.Somasundaran S. (2010).
Discourse-Level Relations forOpinion Analysis (Doctoral dissertation).
Universityof Pittsburgh.Tang, H. F., Tan, S. B., & Cheng, X. Q.
(2009).
Asurvey on sentiment detection of reviews.
ExpertSystems with Applications, 36(7), 10760-10773.Toprak, C., Jakob, N., & Gurevych, I.
(2010).
Sentenceand expression level annotation of opinions in user-generated discourse.
Proceedings of the 48th AnnualMeeting of the Association for ComputationalLinguistics, 575-584.Tsur, O., Davidov, D., & Rappoport, A.
(2010).
GreatCatchy Name: Semi-Supervised Recognition ofSarcastic Sentences in Online Product Reviews.ICWSM, 162-169.Wilson, T., Wiebe, J., & Hoffmann, P. (2005).Recognizing contextual polarity in phrase-levelsentiment analysis.
Proceedings of the conference onhuman language technology and empirical methodsin natural language processing, 347-354.Yang, B., & Cardie, C. (2014).
Context-aware learningfor sentence-level sentiment analysis with posteriorregularization.
Proceedings of ACL.Zhao J., Liu K. & Wang G. (2008).
Adding RedundantFeatures for CRFs-based Sentence SentimentClassification.
Proceedings of the Conference onEmpirical Methods in Natural Language Processing,117-126.Zhou L., Li B., Gao W., Wei Z.
& Wong K. F. (2011).Unsupervised Discovery of Discourse Relations forEliminating Intra-sentence Polarity Ambiguities.Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, 162-171.1568
