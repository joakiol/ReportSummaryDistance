Mining WordNet for Fuzzy Sentiment:Sentiment Tag Extraction from WordNet GlossesAlina Andreevskaia and Sabine BerglerConcordia UniversityMontreal, Quebec, Canada{andreev, bergler}@encs.concordia.caAbstractMany of the tasks required for semantictagging of phrases and texts rely on a listof words annotated with some semanticfeatures.
We present a method for ex-tracting sentiment-bearing adjectives fromWordNet using the Sentiment Tag Extrac-tion Program (STEP).
We did 58 STEPruns on unique non-intersecting seed listsdrawn from manually annotated list ofpositive and negative adjectives and evalu-ated the results against other manually an-notated lists.
The 58 runs were then col-lapsed into a single set of 7, 813 uniquewords.
For each word we computed aNet Overlap Score by subtracting the totalnumber of runs assigning this word a neg-ative sentiment from the total of the runsthat consider it positive.
We demonstratethat Net Overlap Score can be used as ameasure of the words degree of member-ship in the fuzzy category of sentiment:the core adjectives, which had the high-est Net Overlap scores, were identifiedmost accurately both by STEP and by hu-man annotators, while the words on theperiphery of the category had the lowestscores and were associated with low ratesof inter-annotator agreement.1 IntroductionMany of the tasks required for effective seman-tic tagging of phrases and texts rely on a list ofwords annotated with some lexical semantic fea-tures.
Traditional approaches to the developmentof such lists are based on the implicit assumptionof classical truth-conditional theories of meaningrepresentation, which regard all members of a cat-egory as equal: no element is more of a mem-ber than any other (Edmonds, 1999).
In this pa-per, we challenge the applicability of this assump-tion to the semantic category of sentiment, whichconsists of positive, negative and neutral subcate-gories, and present a dictionary-based SentimentTag Extraction Program (STEP) that we use togenerate a fuzzy set of English sentiment-bearingwords for the use in sentiment tagging systems 1.The proposed approach based on the fuzzy logic(Zadeh, 1987) is used here to assign fuzzy sen-timent tags to all words in WordNet (Fellbaum,1998), that is it assigns sentiment tags and a degreeof centrality of the annotated words to the senti-ment category.
This assignment is based on Word-Net glosses.
The implications of this approach forNLP and linguistic research are discussed.2 The Category of Sentiment as a FuzzySetSome semantic categories have clear membership(e.g., lexical fields (Lehrer, 1974) of color, bodyparts or professions), while others are much moredifficult to define.
This prompted the developmentof approaches that regard the transition frommem-bership to non-membership in a semantic categoryas gradual rather than abrupt (Zadeh, 1987; Rosch,1978).
In this paper we approach the category ofsentiment as one of such fuzzy categories wheresome words ?
such as good, bad ?
are very cen-tral, prototypical members, while other, less cen-tral words may be interpreted differently by differ-ent people.
Thus, as annotators proceed from thecore of the category to its periphery, word mem-1Sentiment tagging is defined here as assigning positive,negative and neutral labels to words according to the senti-ment they express.209bership in this category becomes more ambiguous,and hence, lower inter-annotator agreement can beexpected for more peripheral words.
Under theclassical truth-conditional approach the disagree-ment between annotators is invariably viewed as asign of poor reliability of coding and is eliminatedby ?training?
annotators to code difficult and am-biguous cases in some standard way.
While thisprocedure leads to high levels of inter-annotatoragreement on a list created by a coordinated teamof researchers, the naturally occurring differencesin the interpretation of words located on the pe-riphery of the category can clearly be seen whenannotations by two independent teams are com-pared.
The Table 1 presents the comparison of GI-H4 (General Inquirer Harvard IV-4 list, (Stone etal., 1966)) 2 and HM (from (Hatzivassiloglou andMcKeown, 1997) study) lists of words manuallyannotated with sentiment tags by two different re-search teams.GI-H4 HMList composition nouns, verbs,adj., adv.adj.
onlyTotal list size 8, 211 1, 336Total adjectives 1, 904 1, 336Tags assigned Positiv, Nega-tiv or no tagPositiveor Nega-tiveAdj.
with 1, 268 1, 336non-neutral tagsIntersection 774 (55% 774 (58%(% intersection) of GI-H4 adj) of HM)Agreement on tags 78.7%Table 1: Agreement between GI-H4 and HM an-notations on sentiment tags.The approach to sentiment as a category withfuzzy boundaries suggests that the 21.3% dis-agreement between the two manually annotatedlists reflects a natural variability in human an-notators?
judgment and that this variability is re-lated to the degree of centrality and/or relative im-portance of certain words to the category of sen-timent.
The attempts to address this difference2The General Inquirer (GI) list used in this study wasmanually cleaned to remove duplicate entries for words withsame part of speech and sentiment.
Only the Harvard IV-4list component of the whole GI was used in this study, sinceother lists included in GI lack the sentiment annotation.
Un-less otherwise specified, we used the full GI-H4 list includingthe Neutral words that were not assigned Positiv or Negativannotations.in importance of various sentiment markers havecrystallized in two main approaches: automaticassignment of weights based on some statisticalcriterion ((Hatzivassiloglou and McKeown, 1997;Turney and Littman, 2002; Kim and Hovy, 2004),and others) or manual annotation (Subasic andHuettner, 2001).
The statistical approaches usu-ally employ some quantitative criterion (e.g., mag-nitude of pointwise mutual information in (Turneyand Littman, 2002), ?goodness-for-fit?
measure in(Hatzivassiloglou and McKeown, 1997), probabil-ity of word?s sentiment given the sentiment if itssynonyms in (Kim and Hovy, 2004), etc.)
to de-fine the strength of the sentiment expressed by aword or to establish a threshold for the member-ship in the crisp sets 3 of positive, negative andneutral words.
Both approaches have their limi-tations: the first approach produces coarse resultsand requires large amounts of data to be reliable,while the second approach is prohibitively expen-sive in terms of annotator time and runs the risk ofintroducing a substantial subjective bias in anno-tations.In this paper we seek to develop an approachfor semantic annotation of a fuzzy lexical cate-gory and apply it to sentiment annotation of allWordNet words.
The sections that follow (1) de-scribe the proposed approach used to extract sen-timent information from WordNet entries usingSTEP (Semantic Tag Extraction Program) algo-rithm, (2) discuss the overall performance of STEPon WordNet glosses, (3) outline the method fordefining centrality of a word to the sentiment cate-gory, and (4) compare the results of both automatic(STEP) and manual (HM) sentiment annotationsto the manually-annotated GI-H4 list, which wasused as a gold standard in this experiment.
Thecomparisons are performed separately for each ofthe subsets of GI-H4 that are characterized by adifferent distance from the core of the lexical cat-egory of sentiment.3 Sentiment Tag Extraction fromWordNet EntriesWord lists for sentiment tagging applications canbe compiled using different methods.
Automaticmethods of sentiment annotation at the word levelcan be grouped into two major categories: (1)corpus-based approaches and (2) dictionary-based3We use the term crisp set to refer to traditional, non-fuzzy sets210approaches.
The first group includes methodsthat rely on syntactic or co-occurrence patternsof words in large texts to determine their senti-ment (e.g., (Turney and Littman, 2002; Hatzivas-siloglou and McKeown, 1997; Yu and Hatzivas-siloglou, 2003; Grefenstette et al, 2004) and oth-ers).
The majority of dictionary-based approachesuse WordNet information, especially, synsets andhierarchies, to acquire sentiment-marked words(Hu and Liu, 2004; Valitutti et al, 2004; Kimand Hovy, 2004) or to measure the similaritybetween candidate words and sentiment-bearingwords such as good and bad (Kamps et al, 2004).In this paper, we propose an approach to senti-ment annotation of WordNet entries that was im-plemented and tested in the Semantic Tag Extrac-tion Program (STEP).
This approach relies bothon lexical relations (synonymy, antonymy and hy-ponymy) provided in WordNet and on the Word-Net glosses.
It builds upon the properties of dic-tionary entries as a special kind of structured text:such lexicographical texts are built to establish se-mantic equivalence between the left-hand and theright-hand parts of the dictionary entry, and there-fore are designed to match as close as possible thecomponents of meaning of the word.
They haverelatively standard style, grammar and syntacticstructures, which removes a substantial source ofnoise common to other types of text, and finally,they have extensive coverage spanning the entirelexicon of a natural language.The STEP algorithm starts with a small set ofseed words of known sentiment value (positiveor negative).
This list is augmented during thefirst pass by adding synonyms, antonyms and hy-ponyms of the seed words supplied in WordNet.This step brings on average a 5-fold increase inthe size of the original list with the accuracy of theresulting list comparable to manual annotations(78%, similar to HM vs. GI-H4 accuracy).
At thesecond pass, the system goes through all WordNetglosses and identifies the entries that contain intheir definitions the sentiment-bearing words fromthe extended seed list and adds these head words(or rather, lexemes) to the corresponding category?
positive, negative or neutral (the remainder).
Athird, clean-up pass is then performed to partiallydisambiguate the identified WordNet glosses withBrill?s part-of-speech tagger (Brill, 1995), whichperforms with up to 95% accuracy, and eliminateserrors introduced into the list by part-of-speechambiguity of some words acquired in pass 1 andfrom the seed list.
At this step, we also filter outall those words that have been assigned contradict-ing, positive and negative, sentiment values withinthe same run.The performance of STEP was evaluated usingGI-H4 as a gold standard, while the HM list wasused as a source of seed words fed into the sys-tem.
We evaluated the performance of our sys-tem against the complete list of 1904 adjectives inGI-H4 that included not only the words that weremarked as Positiv, Negativ, but also those that werenot considered sentiment-laden by GI-H4 annota-tors, and hence were by default considered neutralin our evaluation.
For the purposes of the evalua-tion we have partitioned the entire HM list into 58non-intersecting seed lists of adjectives.
The re-sults of the 58 runs on these non-intersecting seedlists are presented in Table 2.
The Table 2 showsthat the performance of the system exhibits sub-stantial variability depending on the compositionof the seed list, with accuracy ranging from 47.6%to 87.5% percent (Mean = 71.2%, Standard Devi-ation (St.Dev) = 11.0%).Average Averagerun size % correct# of adj StDev % StDevPASS 1 103 29 78.0% 10.5%(WN Relations)PASS 2 630 377 64.5% 10.8%(WN Glosses)PASS 3 435 291 71.2% 11.0%(POS clean-up)Table 2: Performance statistics on STEP runs.The significant variability in accuracy of theruns (Standard Deviation over 10%) is attributableto the variability in the properties of the seed listwords in these runs.
The HM list includes somesentiment-marked words where not all meaningsare laden with sentiment, but also the words wheresome meanings are neutral and even the wordswhere such neutral meanings are much more fre-quent than the sentiment-laden ones.
The runswhere seed lists included such ambiguous adjec-tives were labeling a lot of neutral words as sen-timent marked since such seed words were morelikely to be found in the WordNet glosses in theirmore frequent neutral meaning.
For example, run# 53 had in its seed list two ambiguous adjectives1dim and plush, which are neutral in most of thecontexts.
This resulted in only 52.6% accuracy(18.6% below the average).
Run # 48, on theother hand, by a sheer chance, had only unam-biguous sentiment-bearing words in its seed list,and, thus, performed with a fairly high accuracy(87.5%, 16.3% above the average).In order to generate a comprehensive list cov-ering the entire set of WordNet adjectives, the 58runs were then collapsed into a single set of uniquewords.
Since many of the clearly sentiment-ladenadjectives that form the core of the category ofsentiment were identified by STEP in multipleruns and had, therefore, multiple duplicates in thelist that were counted as one entry in the com-bined list, the collapsing procedure resulted ina lower-accuracy (66.5% - when GI-H4 neutralswere included) but much larger list of English ad-jectives marked as positive (n = 3, 908) or neg-ative (n = 3, 905).
The remainder of WordNet?s22, 141 adjectives was not found in any STEP runand hence was deemed neutral (n = 14, 328).Overall, the system?s 66.5% accuracy on thecollapsed runs is comparable to the accuracy re-ported in the literature for other systems run onlarge corpora (Turney and Littman, 2002; Hatzi-vassiloglou and McKeown, 1997).
In order tomake a meaningful comparison with the resultsreported in (Turney and Littman, 2002), we alsodid an evaluation of STEP results on positives andnegatives only (i.e., the neutral adjectives from GI-H4 list were excluded) and compared our labels tothe remaining 1266 GI-H4 adjectives.
The accu-racy on this subset was 73.4%, which is compara-ble to the numbers reported by Turney and Littman(2002) for experimental runs on 3, 596 sentiment-marked GI words from different parts of speechusing a 2x109 corpus to compute point-wise mu-tual information between the GI words and 14manually selected positive and negative paradigmwords (76.06%).The analysis of STEP system performancevs.
GI-H4 and of the disagreements between man-ually annotated HM and GI-H4 showed thatthe greatest challenge with sentiment tagging ofwords lies at the boundary between sentiment-marked (positive or negative) and sentiment-neutral words.
The 7% performance gain (from66.5% to 73.4%) associated with the removal ofneutrals from the evaluation set emphasizes theimportance of neutral words as a major source ofsentiment extraction system errors 4.
Moreover,the boundary between sentiment-bearing (positiveor negative) and neutral words in GI-H4 accountsfor 93% of disagreements between the labels as-signed to adjectives in GI-H4 and HM by two in-dependent teams of human annotators.
The viewtaken here is that the vast majority of such inter-annotator disagreements are not really errors buta reflection of the natural ambiguity of the wordsthat are located on the periphery of the sentimentcategory.4 Establishing the degree of word?scentrality to the semantic categoryThe approach to sentiment category as a fuzzyset ascribes the category of sentiment some spe-cific structural properties.
First, as opposed to thewords located on the periphery, more central ele-ments of the set usually have stronger and morenumerous semantic relations with other categorymembers 5.
Second, the membership of these cen-tral words in the category is less ambiguous thanthe membership of more peripheral words.
Thus,we can estimate the centrality of a word in a givencategory in two ways:1.
Through the density of the word?s relation-ships with other words ?
by enumerating itssemantic ties to other words within the field,and calculating membership scores based onthe number of these ties; and2.
Through the degree of word membership am-biguity ?
by assessing the inter-annotatoragreement on the word membership in thiscategory.Lexicographical entries in the dictionaries, suchas WordNet, seek to establish semantic equiva-lence between the word and its definition and pro-vide a rich source of human-annotated relation-ships between the words.
By using a bootstrap-ping system, such as STEP, that follows the linksbetween the words in WordNet to find similarwords, we can identify the paths connecting mem-bers of a given semantic category in the dictionary.With multiple bootstrapping runs on different seed4It is consistent with the observation by Kim and Hovy(2004) who noticed that, when positives and neutrals werecollapsed into the same category opposed to negatives, theagreement between human annotators rose by 12%.5The operationalizations of centrality derived from thenumber of connections between elements can be found in so-cial network theory (Burt, 1980)212lists, we can then produce a measure of the den-sity of such ties.
The ambiguity measure de-rived from inter-annotator disagreement can thenbe used to validate the results obtained from thedensity-based method of determining centrality.In order to produce a centrality measure, weconducted multiple runs with non-intersectingseed lists drawn from HM.
The lists of wordsfetched by STEP on different runs partially over-lapped, suggesting that the words identified by thesystem many times as bearing positive or negativesentiment are more central to the respective cate-gories.
The number of times the word has beenfetched by STEP runs is reflected in the GrossOverlap Measure produced by the system.
Insome cases, there was a disagreement between dif-ferent runs on the sentiment assigned to the word.Such disagreements were addressed by comput-ing the Net Overlap Scores for each of the foundwords: the total number of runs assigning the worda negative sentiment was subtracted from the to-tal of the runs that consider it positive.
Thus, thegreater the number of runs fetching the word (i.e.,Gross Overlap) and the greater the agreement be-tween these runs on the assigned sentiment, thehigher the Net Overlap Score of this word.The Net Overlap scores obtained for each iden-tified word were then used to stratify these wordsinto groups that reflect positive or negative dis-tance of these words from the zero score.
The zeroscore was assigned to (a) the WordNet adjectivesthat were not identified by STEP as bearing posi-tive or negative sentiment 6 and to (b) the wordswith equal number of positive and negative hitson several STEP runs.
The performance measuresfor each of the groups were then computed to al-low the comparison of STEP and human annotatorperformance on the words from the core and fromthe periphery of the sentiment category.
Thus, foreach of the Net Overlap Score groups, both auto-matic (STEP) and manual (HM) sentiment annota-tions were compared to human-annotated GI-H4,which was used as a gold standard in this experi-ment.On 58 runs, the system has identified 3, 908English adjectives as positive, 3, 905 as nega-tive, while the remainder (14, 428) of WordNet?s22, 141 adjectives was deemed neutral.
Of these14, 328 adjectives that STEP runs deemed neutral,6The seed lists fed into STEP contained positive or neg-ative, but no neutral words, since HM, which was used as asource for these seed lists, does not include any neutrals.Figure 1: Accuracy of word sentiment tagging.884 were also found in GI-H4 and/or HM lists,which allowed us to evaluate STEP performanceand HM-GI agreement on the subset of neutrals aswell.
The graph in Figure 1 shows the distributionof adjectives by Net Overlap scores and the aver-age accuracy/agreement rate for each group.Figure 1 shows that the greater the Net Over-lap Score, and hence, the greater the distance ofthe word from the neutral subcategory (i.e., fromzero), the more accurate are STEP results and thegreater is the agreement between two teams of hu-man annotators (HM and GI-H4).
On average,for all categories, including neutrals, the accuracyof STEP vs. GI-H4 was 66.5%, human-annotatedHM had 78.7% accuracy vs. GI-H4.
For the wordswith Net Overlap of ?7 and greater, both STEPand HM had accuracy around 90%.
The accu-racy declined dramatically as Net Overlap scoresapproached zero (= Neutrals).
In this category,human-annotated HM showed only 20% agree-ment with GI-H4, while STEP, which deemedthese words neutral, rather than positive or neg-ative, performed with 57% accuracy.These results suggest that the two measures ofword centrality, Net Overlap Score based on mul-tiple STEP runs and the inter-annotator agreement(HM vs. GI-H4), are directly related 7.
Thus, theNet Overlap Score can serve as a useful tool inthe identification of core and peripheral membersof a fuzzy lexical category, as well as in predic-7In our sample, the coefficient of correlation between thetwo was 0.68.
The Absolute Net Overlap Score on the sub-groups 0 to 10 was used in calculation of the coefficient ofcorrelation.213tion of inter-annotator agreement and system per-formance on a subgroup of words characterized bya given Net Overlap Score value.In order to make the Net Overlap Score measureusable in sentiment tagging of texts and phrases,the absolute values of this score should be nor-malized and mapped onto a standard [0, 1] inter-val.
Since the values of the Net Overlap Scoremay vary depending on the number of runs used inthe experiment, such mapping eliminates the vari-ability in the score values introduced with changesin the number of runs performed.
In order to ac-complish this normalization, we used the value ofthe Net Overlap Score as a parameter in the stan-dard fuzzy membership S-function (Zadeh, 1975;Zadeh, 1987).
This function maps the absolutevalues of the Net Overlap Score onto the intervalfrom 0 to 1, where 0 corresponds to the absence ofmembership in the category of sentiment (in ourcase, these will be the neutral words) and 1 reflectsthe highest degree of membership in this category.The function can be defined as follows:S(u;?, ?, ?)
=??????
?0 for u ?
?2(u?????
)2 for?
?
u ?
?1?
2(u?????
)2 for ?
?
u ?
?1 for u ?
?where u is the Net Overlap Score for the wordand ?, ?, ?
are the three adjustable parameters: ?is set to 1, ?
is set to 15 and ?, which represents acrossover point, is defined as ?
= (?
+ ?
)/2 = 8.Defined this way, the S-function assigns highestdegree of membership (=1) to words that have thethe Net Overlap Score u ?
15.
The accuracy vs.GI-H4 on this subset is 100%.
The accuracy goesdown as the degree of membership decreases andreaches 59% for values with the lowest degrees ofmembership.5 Discussion and conclusionsThis paper contributes to the development of NLPand semantic tagging systems in several respects.?
The structure of the semantic category ofsentiment.
The analysis of the categoryof sentiment of English adjectives presentedhere suggests that this category is structuredas a fuzzy set: the distance from the coreof the category, as measured by Net Over-lap scores derived from multiple STEP runs,is shown to affect both the level of inter-annotator agreement and the system perfor-mance vs. human-annotated gold standard.?
The list of sentiment-bearing adjectives.
Thelist produced and cross-validated by multipleSTEP runs contains 7, 814 positive and neg-ative English adjectives, with an average ac-curacy of 66.5%, while the human-annotatedlist HM performed at 78.7% accuracy vs.the gold standard (GI-H4) 8.
The remaining14, 328 adjectives were not identified as sen-timent marked and therefore were consideredneutral.The stratification of adjectives by their NetOverlap Score can serve as an indicatorof their degree of membership in the cate-gory of (positive/negative) sentiment.
Sincelow degrees of membership are associatedwith greater ambiguity and inter-annotatordisagreement, the Net Overlap Score valuecan provide researchers with a set of vol-ume/accuracy trade-offs.
For example, byincluding only the adjectives with the NetOverlap Score of 4 and more, the researchercan obtain a list of 1, 828 positive and nega-tive adjectives with accuracy of 81% vs. GI-H4, or 3, 124 adjectives with 75% accuracyif the threshold is set at 3.
The normalizationof the Net Overlap Score values for the use inphrase and text-level sentiment tagging sys-tems was achieved using the fuzzy member-ship function that we proposed here for thecategory of sentiment of English adjectives.Future work in the direction laid out by thisstudy will concentrate on two aspects of sys-tem development.
First further incrementalimprovements to the precision of the STEPalgorithm will be made to increase the ac-curacy of sentiment annotation through theuse of adjective-noun combinatorial patternswithin glosses.
Second, the resulting list ofadjectives annotated with sentiment and withthe degree of word membership in the cate-gory (as measured by the Net Overlap Score)will be used in sentiment tagging of phrasesand texts.
This will enable us to compute thedegree of importance of sentiment markersfound in phrases and texts.
The availability8GI-H4 contains 1268 and HM list has 1336 positive andnegative adjectives.
The accuracy figures reported here in-clude the errors produced at the boundary with neutrals.214of the information on the degree of central-ity of words to the category of sentiment mayimprove the performance of sentiment deter-mination systems built to identify the senti-ment of entire phrases or texts.?
System evaluation considerations.
The con-tribution of this paper to the developmentof methodology of system evaluation is two-fold.
First, this research emphasizes the im-portance of multiple runs on different seedlists for a more accurate evaluation of senti-ment tag extraction system performance.
Wehave shown how significantly the system re-sults vary, depending on the composition ofthe seed list.Second, due to the high cost of manual an-notation and other practical considerations,most bootstrapping and other NLP systemsare evaluated on relatively small manuallyannotated gold standards developed for agiven semantic category.
The implied as-sumption is that such a gold standard repre-sents a random sample drawn from the pop-ulation of all category members and hence,system performance observed on this goldstandard can be projected to the whole se-mantic category.
Such extrapolation is notjustified if the category is structured as a lex-ical field with fuzzy boundaries: in this casethe precision of both machine and human an-notation is expected to fall when more pe-ripheral members of the category are pro-cessed.
In this paper, the sentiment-bearingwords identified by the system were stratifiedbased on their Net Overlap Score and eval-uated in terms of accuracy of sentiment an-notation within each stratum.
These strata,derived from Net Overlap scores, reflect thedegree of centrality of a given word to thesemantic category, and, thus, provide greaterassurance that system performance on otherwords with the same Net Overlap Score willbe similar to the performance observed on theintersection of system results with the goldstandard.?
The role of the inter-annotator disagree-ment.
The results of the study presented inthis paper call for reconsideration of the roleof inter-annotator disagreement in the devel-opment of lists of words manually annotatedwith semantic tags.
It has been shown herethat the inter-annotator agreement tends tofall as we proceed from the core of a fuzzysemantic category to its periphery.
There-fore, the disagreement between the annota-tors does not necessarily reflect a qualityproblem in human annotation, but rather astructural property of the semantic category.This suggests that inter-annotator disagree-ment rates can serve as an important sourceof empirical information about the structuralproperties of the semantic category and canhelp define and validate fuzzy sets of seman-tic category members for a number of NLPtasks and applications.ReferencesEric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
Computational Lin-guistics, 21(4):543?565.R.S.
Burt.
1980.
Models of network structure.
AnnualReview of Sociology, 6:79?141.Philip Edmonds.
1999.
Semantic representations ofnear-synonyms for automatic lexical choice.
Ph.D.thesis, University of Toronto.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,MA.Gregory Grefenstette, Yan Qu, David A. Evans, andJames G. Shanahan.
2004.
Validating the Cover-age of Lexical Resources for Affect Analysis andAutomatically Classifying New Words along Se-mantic Axes.
In Yan Qu, James Shanahan, andJanyce Wiebe, editors, Exploring Attitude and Af-fect in Text: Theories and Applications, AAAI-2004Spring Symposium Series, pages 71?78.Vasileios Hatzivassiloglou and Kathleen B. McKeown.1997.
Predicting the Semantic Orientation of Adjec-tives.
In 35th ACL, pages 174?181.Minqing Hu and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In KDD-04, pages 168?177.Jaap Kamps, Maarten Marx, Robert J. Mokken, andMaarten de Rijke.
2004.
UsingWordNet to measuresemantic orientation of adjectives.
In LREC 2004,volume IV, pages 1115?1118.Soo-Min Kim and Edward Hovy.
2004.
Determiningthe sentiment of opinions.
In COLING-2004, pages1367?1373, Geneva, Switzerland.215Adrienne Lehrer.
1974.
Semantic Fields and Lexi-cal Structure.
North Holland, Amsterdam and NewYork.Eleanor Rosch.
1978.
Principles of Categorization.
InEleanor Rosch and Barbara B. Lloyd, editors, Cog-nition and Categorization, pages 28?49.
LawrenceErlbaum Associates, Hillsdale, New Jersey.P.J.
Stone, D.C. Dumphy, M.S.
Smith, and D.M.Ogilvie.
1966.
The General Inquirer: a computerapproach to content analysis.
M.I.T.
studies in com-parative politics.
M.I.T.
Press, Cambridge, MA.Pero Subasic and Alison Huettner.
2001.
Affect Anal-ysis of Text Using Fuzzy Typing.
IEEE-FS, 9:483?496.Peter Turney and Michael Littman.
2002.
Un-supervised learning of semantic orientation froma hundred-billion-word corpus.
Technical ReportERC-1094 (NRC 44929), National Research Coun-cil of Canada.Alessandro Valitutti, Carlo Strapparava, and OlivieroStock.
2004.
Developing Affective Lexical Re-sources.
PsychNology Journal, 2(1):61?83.Hong Yu and Vassileios Hatzivassiloglou.
2003.
To-wards Answering Opinion Questions: SeparatingFacts from Opinions and Identifying the Polarity ofOpinion Sentences.
In Conference on EmpiricalMethods in Natural Language Processing (EMNLP-03).Lotfy A. Zadeh.
1975.
Calculus of Fuzzy Restric-tions.
In L.A. Zadeh, K.-S. Fu, K. Tanaka, andM.
Shimura, editors, Fuzzy Sets and their Applica-tions to cognitive and decision processes, pages 1?40.
Academic Press Inc., New-York.Lotfy A. Zadeh.
1987.
PRUF ?
a Meaning Rep-resentation Language for Natural Languages.
InR.R.
Yager, S. Ovchinnikov, R.M.
Tong, and H.T.Nguyen, editors, Fuzzy Sets and Applications: Se-lected Papers by L.A. Zadeh, pages 499?568.
JohnWiley & Sons.216
