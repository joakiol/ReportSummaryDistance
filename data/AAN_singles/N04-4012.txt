UI on the Fly: Generating a Multimodal User InterfaceDavid ReitterMedia Lab EuropeDublin, Ireland{reitter,erin}@mle.media.mit.eduErin Marie PanttajaMedia Lab EuropeDublin, IrelandFred CumminsUniversity College DublinDublin, Irelandfred.cummins@ucd.ieAbstractUI on the Fly is a system that dynami-cally presents coordinated multimodal contentthrough natural language and a small-screengraphical user interface.
It adapts to theuser?s preferences and situation.
MultimodalFunctional Unification Grammar (MUG) is aunification-based formalism that uses rules togenerate content that is coordinated across sev-eral communication modes.
Faithful variantsare scored with a heuristic function.1 IntroductionMultimodal user interfaces are everywhere.
The use ofa keyboard and mouse on a desktop PC is ubiquitous,if not natural.
However, the click-then-type paradigmof common interfaces misses the cross-modal synchro-nization of timing and meaning that is evident in human-human communication.
With coordinated output, noviceusers could get explanations (redundant content) and ex-perienced users could receive additional (complemen-tary) information, increasing the bandwidth of the inter-face.
Coordinated input (?put that there!?)
speeds up in-put and relieves speech recognition of notoriously hard-to-recognize referring expressions such as names.
If auser interface is generated on the fly, it can adapt to thesituation and special needs of the user as well as to thedevice.While users are not necessarily prone to make multi-modal inputs (Oviatt, 1999), they can still integrate com-plementary output or use redundant output in noisy sit-uations.
Consequently, this paper deals with generatingoutput.
We propose a grammar formalism that general-izes decisions about how to deliver content in an adapt-able multimodal user interface.
We demonstrate it in thecontext of a user interface for a mobile personal informa-tion manager.2 Related WorkSince Bolt?s (1980) Put-That-There system introducedcross-modal coordination in multimodal user input, vari-ous projects have investigated multimodal input and out-put methods.
Users display a preference for the touch-screen in map-based positioning acts and object selection(Oviatt et al, 1997).
WIP (Andre?
et al, 1993) and othersystems (Feiner and McKeown, 1990; Roth and Hefley,1993) generate static multimodal documents.
In an in-teractive user interface, however, layout should remainconsistent (Woods and Roth, 1988, perceived stability).SmartKom (Wahlster, 2002) is a recent effort that pro-duces a multimodal user interface, using XML/XSLTtechniques to render the output.
These are determinis-tic, which makes soft constraints such as usability hard toimplement.
SUPPLE (Gajos and Weld, 2004) overcomesthis problem in its model of the user and the expectedworkload for various interfaces, generating a unimodal(graphical) user interface without natural language gener-ation elements.
On the integration side, Johnston (1998)presents a unification-based grammar that recasts multi-modal signal fusion as a parsing problem.Our approach employs a non-deterministic grammar toderive variants which are evaluated with a comparativelysimple user and situation model according to their utility(information conveyed) and the projected cognitive loadimposed on the user.
It also removes the requirement in-herent in Johnston?s system of explicitly defining rules tointegrate multimodal information.In the following, we discuss the grammar formalismused to create output, as well as consistency and adapta-tion considerations.3 FormalismIn this section, we will explain how the Multimodal Func-tional Unification Grammar (MUG) allows us to generatecontent.
Our formalism and the associated evaluation al-gorithm work closely with a dialogue manager.
As in-put, they receive an unambiguous, language- and mode-independent representation of the next dialogue turn.26666666666666666666666666666666666666666666664type askconfirmationinitiative implicitexperience noviceerror noneaction2666666666666666666666666666666666664type taskcontexttype emailtask26666666666666666666666666666664type send-emailemail26666666666666666666666666664type emailto264type contactfirstname Fredlastname Cummins375cc264type contactfirstname Erinlastname Panttaja375from"type emailaddressadr reitter@mle.ie#subject"type textcontent Aussie weather#body"type textcontent G?day mates....-Dave#3777777777777777777777777777537777777777777777777777777777775377777777777777777777777777777777777537777777777777777777777777777777777777777777775Figure 1: Input representation: confirmation of sendingof an email3.1 Dialogue acts as inputAlthough the semantic input is independent of mode(screen, voice) and language (Portuguese), the input se-mantics are domain-specific.
The representation uses thefollowing types of dialogue acts at the top level: ask formissing information, ask for a confirmation of an actionor data, inform the user about the state of objects, or givecontext-dependent help.An example is shown in Figure 1.
The input-FD spec-ifies type of act in progress (askconfirmation), and thedetails of the interaction type.
It then specifies the detailsof the current action, in this case, the email that the useris sending.Furthermore, the dialogue manager may indicate theneed to realize a certain portion of an utterance with anattribute realize.
The input format integrates with princi-pled, object-oriented dialogue managers.3.2 The domain: a personal assistant.In this example, we have constructed a personal assistantto be used in the domain of sending email messages.We implemented a MUG for a PDA-size handheld de-vice with a color touch-screen (see Figure 2a).
The initialsteps to adapt it to a mobile phone (Figure 2b) involvedcreating a device profile that uses no GUI widgets andassociates a higher cost (see Section 5) with the screen(a) (b)Figure 2: a) Voice: ?Do you want to send the email?
Yesor No??.
b) Voice: ?Send the email regarding Aussieweather to Fred Cummins now?
?output, as the screen is smaller.
All devices used haveserver-driven TTS output capabilities.3.3 The grammarMUG is a collection of components.
Each of them spec-ifies a realization variant for a given partial semantic orsyntactic representation.
This representation may be spe-cific to a mode or general.
We call these componentsfunctional descriptions (FDs) in the tradition of the Func-tional Unification Grammar (Kay, 1979), from whichMUG is derived.For each output, the MUG identifies an utterance plan,consisting of separate constituents in the output.
Forexample, when we ask for missing information (?Whowould you like to send the e-mail to??
), the utterance con-sists of an instruction and an interaction section.
Such aplan is defined in a component, as is each more specificgeneration level down to the choice of GUI widgets orlexicon entries.MUG is based on the unification of such attribute-valuestructures.
Unification can be seen as a process that aug-ments an FD with additional information.
FDs are re-cursive: a value can be atomic or a nested FD.
Values inan FD can be bound to the values in a substructure FD(structure sharing).To realize a semantic representation R, we unify a suit-able grammar component FD with each m-constituentsubstructure F in R, until all substructures have been ex-panded.
An m-constituent is an FD that has an attributepath m|cat, that is, which has been designated as a con-stituent for mode m. Note that zero or one grammar com-ponents for a given mode can be unified with F .Components from the grammar invoke each other byinstantiating the cat attribute in the mode-specific part ofa substructure.
Figure 3 shows a component that appliesto all modes.There may be several competing components in thegrammar.
This creates the ambiguity needed to gener-ate a variety of outputs from the same input.
Each out-put will be faithful to the original input.
However, onlyone variant will be optimally adapted to the given situa-tion, user, and device (see Section 5).
Our final markupis text for the text to speech system as well as HTML tobe displayed in a browser, similar to the MATCH system(Johnston et al, 2002).The nested attribute-value structures and unificationare powerful principles that allow us to cover a broadrange of planning tasks, including syntactic and lexicalchoices.
The declarative nature of the grammar allows usto easily add new ways to express a given semantic en-tity.
The information that each component has access tois explicitly encapsulated by an FD.A grammar workbench allows us to debug the genera-tion grammar.
We could improve the debugging processwith a type-hierarchy, which defines allowed attributesfor each type.266666666666666666666664action 324Modehcat 1itype 135instruction2664action 3Mode"cat confirm-modtext 4#3775user-input24Mode"cat yesnolisttext 5#35Mode"cat askconfirmationtext concat([ 4 , 5 ])#377777777777777777777775Figure 3: A MUG component that handles the confirma-tion of tasks or user input.
The mode in variable Modemay be voice or screen.4 Planning for CoherenceCoherence is a key element in designing a multimodaluser interface, where the potential for confusion is in-creased.
Our user interface attempts to be both consis-tent and coherent.
For example, lexical choice does notvary: it is either ?mobile phone?
or ?cell phone,?
but itis the same whether it is in text or voice.
This is in linewith priming effects, which are known to occur in human-human dialogue.Like humans (McNeill, 1992; Oviatt et al, 1997),our system aims to be coherent and consistent across allmodes.
We present redundant content, for example, bychoosing the same lexical realizations (never mix cellphone and mobile phone).
We present complementaryinput in linked components.
If, for example, a deicticexpression such as these two e-mails (by voice) requiresthe e-mails to be put in focus on the screen, it will set afeature accordingly in the complementary mode.This is possible because of a very simple principle en-coded in the generation algorithm: all components real-izing one semantic entity must unify.
Components maystill specify mode-specific information.
This is done ina feature named after the mode, so it will not interferewith the realization instructions of a component that real-izes the same semantic entity in another mode.
The FDsallow us to distinguish information a) that needs to beshared across all output modes, b) that is specific to a par-ticular output mode, or c) that requires collaboration be-tween two modes, such as deictic pronouns.
The unifica-tion principle replaces explicit integration rules for eachcoordination scheme, such as the ones used by Johnston(1998), which accounts for the integration of user input.5 Adaptively Choosing the Best VariantThe application of the MUG generates several outputvariants.
They may include or exclude pieces of infor-mation, which may be of more or less utility to the user.
(When information is being confirmed, it should be fullydescribed, but in later interactions, the email could be re-ferred to as ?it.?
)For example, several components applied to the sub-FD for task in Figure 1 may depend more on the screen(Figure 2a) or be redundant in screen and voice output(Figure 2b).
This allows the system to reflect a low ben-efit for output on the screen if the user is driving a caror to increase the cost of voice output if the user is in ameeting, or reflect the fact that one doesn?t hear the voiceoutput on a mobile phone while reading the screen.The system adapts to the user?s abilities, her prefer-ences, and the situation she is in by choosing an appro-priate variant.
These properties are scalar, and the result-ing constraints are to be weighted against each other inour objective function.
Each piece of output is scored ac-cording to a simple trade-off: a) realize content where re-quested, b) maximize utility to the user, and c) minimizecognitive load in perceiving and analyzing the output.These constraints are formalized in a score that is as-signed to each variant ?, given a set of available ModesM , a situation model < ?, ?
>, a device model ?
and autility/time trade-off coefficient ?:s(?)
= ??<e,d>?E(?
)u(e, d) +maxm?M (?mtm(?
))u(e, d) = P (d,?m?M(?m?mem|realized), erealize)The first part of the sum in s describes the utility ben-efit.
The function E returns a set of semantic entitiesin e (substructures) and their embedding depths in d.The function P penalizes the non-realization of requested(attribute realize) semantic entities, while rewarding the(possibly redundant) realization of an entity.
The rewarddecreases with the embedding depth d of the semantic en-tity.
(Deeper entities give less relevant details by default.
)The cognitive load (second part of the sum) is repre-sented by a prediction of the time tm(?)
it would take tointerpret the output.
This is the utterance output time fortext spoken by the text-to-speech system, or an estimatedreading time for text on the screen.Further work will allow us to cover the range ofnovice to experienced users by relying on natural lan-guage phrases versus graphical user interface widgets.6 ConclusionWe have demonstrated a formalism that generates coher-ent multimodal user interfaces, as well its application ina small-screen email client.
As the generation algorithmmakes use of both hard constraints and scalar scores, itcaters for adaptability.
We have proven its functionalityand efficiency in a series of examples in the context of adialogue system, where content is generated in real-timefor various usage situations and different devices.Further evaluation will show whether the fitness func-tion can accurately mirror user satisfaction with a givenoutput variant and whether our form of adaptivity is ac-tually an advantage to users on the go.
Without a goldstandard for a generation system for dynamic multimodaluser interfaces to qualitatively compare against, con-trolled user trials will allow us to evaluate the usabilityof the interfaces we have created.
Task completion times,user frustration levels, and user satisfaction can then beused to evaluate the success of this model of multimodalinteractions.The underlying formalism is intended to be used in cre-ating, using the MUG Workbench, any multimodal sys-tem that can be constructed compositionally, using natu-ral language and other auditory and visual components.As possible examples for future applications, we see amultimodal interface that allows mobile users or userswith sensory impairments to traverse information-rich so-cial networks, and a kiosk for multimodal, multilingualaccess to public transportation options.7 AcknowledgementThe authors would like to thank Stefan Agamanolis,Robert Dale, John Kelleher, Kerry Robinson, and theanonymous reviewers.
This research was partially fundedby the European Commission under the FASiL project,contract number: IST-2001-38685.ReferencesE.
Andre?, W. Finkler, W. Graf, T. Rist, A. Schauder, andW.
Wahlster.
1993.
Wip: The automatic synthesis ofmultimodal presentations.
In M. T. Maybury, editor,Intelligent Multimedia Interfaces.
AAAI Press, MenloPark, CA.Richard A. Bolt.
1980.
Put-that-there:voice and gestureat the graphics interface.
In Proceedings of the 7th an-nual conference on Computer graphics and interactivetechniques, pages 262 ?
270, Seattle.Steven Feiner and Kathleen McKeown.
1990.
Coordi-nating text and graphics in explanation generation.
InProc.
of AAAI-90, pages 442?449, Boston, MA.Krzysztof Gajos and Daniel S. Weld.
2004.
Supple: Au-tomatically generating user interfaces.
In Proceedingsof IUI-2004, Funchal, Portugal.M.
Johnston, S. Bangalore, G. Vasireddy, A. Stent,P.
Ehlen, M. Walker, S. Whittaker, and P. Maloor.2002.
Match: An architecture for multimodal dialoguesystems.
In Proceedings of ACL-2002.Michael Johnston.
1998.
Unification-based multimodalparsing.
In Proceedings of COLING-ACL 1998, pages624?630.Martin Kay.
1979.
Functional grammar.
In Proceedingsof the Fifth Meeting of the Berkeley Linguistics Society,pages 142?158, Berkeley, CA.David McNeill.
1992.
Hand and mind: What gesturesreveal about thought.
University of Chicago Press.Sharon Oviatt, Antonella DeAngeli, and Karen Kuhn.1997.
Integration and synchronization of input modesduring multimodal human-computer interaction.
InProceedings of the SIGCHI conference on Humanfactors in computing systems, pages 415?422.
ACMPress.Sharon Oviatt.
1999.
Ten myths of multimodal interac-tion.
Communications of the ACM, 42(11):74?81.Steven F. Roth and William E. Hefley.
1993.
Intelligentmultimedia presentation systems: Research and princi-ples.
In M. T. Maybury, editor, Intelligent MultimediaInterfaces.
AAAI Press, Menlo Park, CA.Wolfgang Wahlster.
2002.
Smartkom: Fusion and fissionof speech, gestures, and facial expressions.
In Pro-ceedings of the 1st International Workshop on Man-Machine Symbiotic Systems, Kyoto, Japan.David Woods and Emilie Roth.
1988.
Cognitive sys-tems engineering.
In M. Helander, editor, Handbookof Human-Computer Interaction, pages 1?43.
Elsevier,North Holland.
