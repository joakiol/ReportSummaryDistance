Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1001?1012,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA Dependency Parser for TweetsLingpeng Kong Nathan Schneider Swabha SwayamdiptaArchna Bhatia Chris Dyer Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{lingpenk,nschneid,swabha,archna,cdyer,nasmith}@cs.cmu.eduAbstractWe describe a new dependency parser forEnglish tweets, TWEEBOPARSER.
Theparser builds on several contributions: newsyntactic annotations for a corpus of tweets(TWEEBANK), with conventions informedby the domain; adaptations to a statisticalparsing algorithm; and a new approach toexploiting out-of-domain Penn Treebankdata.
Our experiments show that the parserachieves over 80% unlabeled attachmentaccuracy on our new, high-quality test setand measure the benefit of our contribu-tions.Our dataset and parser can be found athttp://www.ark.cs.cmu.edu/TweetNLP.1 IntroductionIn contrast to the edited, standardized language oftraditional publications such as news reports, socialmedia text closely represents language as it is usedby people in their everyday lives.
These informaltexts, which account for ever larger proportions ofwritten content, are of considerable interest to re-searchers, with applications such as sentiment anal-ysis (Greene and Resnik, 2009; Kouloumpis et al.,2011).
However, their often nonstandard contentmakes them challenging for traditional NLP tools.Among the tools currently available for tweets area POS tagger (Gimpel et al., 2011; Owoputi et al.,2013) and a named entity recognizer (Ritter et al.,2011)?but not a parser.Important steps have been taken.
The EnglishWeb Treebank (Bies et al., 2012) represents anannotation effort on web text?which likely liessomewhere between newspaper text and social me-dia messages in formality and care of editing?thatwas sufficient to support a shared task (Petrov andMcDonald, 2012).
Foster et al.
(2011b) annotateda small test set of tweets to evaluate parsers trainedon the Penn Treebank (Marcus et al., 1993), aug-mented using semi-supervision and in-domain data.Others, such as Soni et al.
(2014), have used exist-ing Penn Treebank?trained models on tweets.In this work, we argue that the Penn Treebankapproach to annotation?while well-matched toedited genres like newswire?is poorly suited tomore informal genres.
Our starting point is thatrapid, small-scale annotation efforts performedby imperfectly-trained annotators should provideenough evidence to train an effective parser.
Wesee this starting point as a necessity, given observa-tions about the rapidly changing nature of tweets(Eisenstein, 2013), the attested difficulties of do-main adaptation for parsing (Dredze et al., 2007),and the expense of creating Penn Treebank?styleannotations (Marcus et al., 1993).This paper presents TWEEBOPARSER, the firstsyntactic dependency parser designed explicitly forEnglish tweets.
We developed this parser follow-ing current best practices in empirical NLP: weannotate a corpus (TWEEBANK) and train the pa-rameters of a statistical parsing algorithm.
Ourresearch contributions include:?
a survey of key challenges posed by syntacticanalysis of tweets (by humans or machines) anddecisions motivated by those challenges and byour limited annotation-resource scenario (?2);?
our annotation process and quantitative mea-sures of the quality of the annotations (?3);?
adaptations to a statistical dependency parsingalgorithm to make it fully compatible with theabove, and also to exploit information from out-of-domain data cheaply and without a strongcommitment (?4); and?
an experimental analysis of the parser?s unla-beled attachment accuracy?which surpasses80%?and contributions of various importantcomponents (?5).The dataset and parser can be found at http://www.ark.cs.cmu.edu/TweetNLP.10012 Annotation ChallengesBefore describing our annotated corpus of tweets(?3), we illustrate some of the challenges of syn-tactic analysis they present.
These challenges moti-vate an approach to annotation that diverges signif-icantly from conventional approaches to treebank-ing.
Figure 1 presents a single example illustratingfour of these: token selection, multiword expres-sions, multiple roots, and structure within nounphrases.
We discuss each in turn.2.1 Token SelectionMany elements in tweets have no syntactic function.These include, in many cases, hashtags, URLs, andemoticons.
For example:RT @justinbieber : now Hailee get a twitterThe retweet discourse marker, username, and colonshould not, we argue, be included in the syntacticanalysis.
By contrast, consider:Got #college admissions questions ?
Ask themtonight during #CampusChat I?m lookingforward to advice from @collegevisithttp://bit.ly/cchOTkHere, both the hashtags and the at-mentioned user-name are syntactically part of the utterances, whilethe punctuation and the hyperlink are not.
In theexample of Figure 1, the unselected tokens includeseveral punctuation tokens and the final token #be-lieber, which marks the topic of the tweet.Typically, dependency parsing evaluations ig-nore punctuation token attachment (Buchholz andMarsi, 2006), and we believe it is a waste of an-notator (and parser) time to decide how to attachpunctuation and other non-syntactic tokens.
Maet al.
(2014) recently proposed to treat punctua-tion as context features rather than dependents, andfound that this led to state-of-the-art performancein a transition-based parser.
A small adaptationto our graph-based parsing approach, described in?4.2, allows a similar treatment.Our approach to annotation (?3) forces annota-tors to explicitly select tokens that have a syntacticfunction.
75.6% tokens were selected by the anno-tators.
Against the annotators?
gold standard, wefound that a simple rule-based filter for usernames,hashtags, punctuation, and retweet tokens achieves95.2% (with gold-standard POS tags) and 95.1%(with automatic POS tags) average accuracy in thetask of selecting tokens with a syntactic functionin a ten-fold cross-validation experiment.
To takecontext into account, we developed a first-ordersequence model and found that it achieves 97.4%average accuracy (again, ten-fold cross-validated)with either gold-standard or automatic POS tags.Features include POS; shape features that recog-nize the retweet marker, hashtags, usernames, andhyperlinks; capitalization; and a binary featurefor tokens that include punctuation.
We trainedthe model using the structured perceptron (Collins,2002).2.2 Multiword ExpressionsWe consider multiword expressions (MWEs) oftwo kinds.
The first, proper names, have beenwidely modeled for information extraction pur-poses, and even incorporated into parsing (Finkeland Manning, 2009).
(An example found in Fig-ure 1 is LA Times.)
The second, lexical idioms,have been a ?pain in the neck?
for many years (Saget al., 2002) and have recently received shallowtreatment in NLP (Baldwin and Kim, 2010; Con-stant and Sigogne, 2011; Schneider et al., 2014).Constant et al.
(2012), Green et al.
(2012), Canditoand Constant (2014), and Le Roux et al.
(2014)considered MWEs in parsing.
Figure 1 providesLA Times and All the Rage as examples.Penn Treebank?style syntactic analysis (and de-pendency representations derived from it) doesnot give first-class treatment to this phenomenon,though there is precedent for marking multiwordlexical units and certain kinds of idiomatic relation-ships (Haji?c et al., 2012; Abeill?
et al., 2003).1We argue that internal analysis of MWEs is notcritical for many downstream applications, andtherefore annotators should not expend energy ondeveloping and respecting conventions (or mak-ing arbitrary decisions) within syntactically opaqueor idiosyncratic units.
We therefore allow annota-tors to decide to group words as explicit MWEs,including: proper names (Justin Bieber, WorldSeries), noncompositional or entrenched nominalcompounds (belly button, grilled cheese), connec-tives (as well as), prepositions (out of), adverbials(so far), and idioms (giving up, make sure).From an annotator?s perspective, a MWE func-tions as a single node in the dependency parse,with no internal structure.
For idioms whose in-ternal syntax is easily characterized, the parse canbe used to capture compositional structure, an at-1The popular Stanford typed dependencies (de Marneffeand Manning, 2008) scheme includes a special dependencytype for multiwords, though this is only applied to a small list.1002Helvetica font or similar (such as Arial).
No italics.
The current gray color for unselected tokens is fine.#1385F0#17AD3F#C82506#FA9F1BROOTCOORD MWEmultiword expressionscoordination noun phrase internal structuremultiple rootsOMG I <3 the Biebs & want to have his babies !
?> LATimes : Social Media ?
#belieberNOUN PHRASE INTERNAL STRUCTURENOUN PHRASE ?INTERNAL STRUCTUREOMG I ?
the Biebs & want to have his babies !
?> LA Times : Teen Pop Star Heartthrob is All the Rage on Social Media ?
#belieberROOT MULTIPLE ROOTSCOORD ROOT MWE MWEROOT ROOT?
#belieberHelvetica font or similar (such as Arial).
No italics.
The current gray color for unselected tokens is fine.#1385F0#17AD3F#C82506#FA9F1BROOTCOORD MWEmultiword expressionscoordination noun phrase internal structuremultiple rootsOMG I <3 the Biebs & want to have his babies !
?> LATimes : Social Media ?
#belieberNOUN PHRASE INTERNAL STRUCTURENOUN PHRASE ?INTERNAL STRUCTUREOMG I ?
the Biebs & want to have his babies !
?> LA Times : Teen Pop Star Heartthrob is All the Rage on Social Media ?
#belieberROOT MULTIPLE ROOTSCOORD ROOT MWE MWEROOT ROOTFigure 1: Parse tree for a (constructed) example illustrating annotation challenges discussed in ?2.
Colors highlight tokenselection (gray; ?2.1), multiword expressions (blue; ?2.2), multiple roots (red; ?2.3), coordination (dotted arcs, green; ?3.2), andnoun phrase internal structure (orange; ?2.4).
The internal structure of multiword expressions (dashed arcs below the sentence)was predicted automatically by a parser, as described in ?2.2.tractive property from the perspective of semanticprocessing.To allow training a fairly conventional statisti-cal dependency parser from these annotations, wefind it expedient to apply an automatic conversionto the MWE annotations, in the spirit of Johnson(1998).
We apply an existing dependency parser,the first-order TurboParser (Martins et al., 2009)trained on the Penn Treebank, to parse each MWEindependently, assigning structures like those forLA Times and All the Rage in Figure 1.
Arcsinvolving the MWE in the annotation are then re-connected to the MWE-internal root, so that the re-sulting tree respects the original tokenization.
TheMWE-internal arcs are given a special label so thatthe transformation can be reversed and MWEs re-constructed from parser output.2.3 Multiple RootsFor news text such as that found in the Penn Tree-bank, sentence segmentation is generally consid-ered a very easy task (Reynar and Ratnaparkhi,1997).
Tweets, however, often contain multiplesentences or fragments, which we call ?utterances,?each with its own syntactic root disconnected fromthe others.
The selected tokens in Figure 1 com-prise four utterances.Our approach to annotation allows multiple ut-terances to emerge directly from the connectednessproperties of the graph implied by an annotator?sdecisions.
Our parser allows multiple attachmentsto the ?wall?
symbol, so that multi-rooted analysescan be predicted.2.4 Noun Phrase Internal StructureA potentially important drawback of deriving de-pendency structures from phrase-structure annota-tions, as is typically done using the Penn Treebank,is that flat annotations lead to loss of information.This is especially notable for noun phrases in thePenn Treebank (Vadas and Curran, 2007).
ConsiderTeen Pop Star Heartthrob in Figure 1; Penn Tree-bank conventions would label this as a single NPwith four NN children and no internal structure.
De-pendency conversion tools would likely attach thefirst three words in the NP to Heartthrob.
Direct de-pendency annotation (rather than phrase-structureannotation followed by automatic conversion) al-lows a richer treatment of such structures, which ispotentially important for semantic analysis (Vecchiet al., 2013).3 A Twitter Dependency CorpusIn this section, we describe the TWEEBANK cor-pus, highlighting data selection (?3.1), the annota-tion process (?3.2), important convention choices(?3.3), and measures of quality (?3.4).3.1 Data SelectionWe added manual dependency parses to 929 tweets(12,318 tokens) drawn from the POS-tagged Twit-ter corpus of Owoputi et al.
(2013), which are tok-enized and contain manually annotated POS tags.Owoputi et al.
?s data consists of two parts.
Thefirst, originally annotated by Gimpel et al.
(2011),consists of tweets sampled from a particular day,October 27, 2010?this is known as OCT27.
Dueto concerns about overfitting to phenomena specificto that day (e.g., tweets about a particular sportsgame), Owoputi et al.
(2013) created a new set of547 tweets (DAILY547) consisting of one randomEnglish tweet per day from January 2011 throughJune 2012.Our corpus is drawn roughly equally fromOCT27 and DAILY547.2Despite its obvious tem-poral skew, there is no reason to believe this sampleis otherwise biased; our experiments in ?5 suggestthat this property is important.3.2 AnnotationUnlike a typical treebanking project, which maytake years and involve thousands of person-hoursof work by linguists, most of TWEEBANK was builtin a day by two dozen annotators, most of whomhad only cursory training in the annotation scheme.2This results from a long-term goal to fully annotate both.1003(1) RT @FRIENDSHlP : Friendship is love withoutkissing ...Friendship > is < love < without < kissing(2) bieber is an alien !
:O he went down to earth .bieber > is** < alien < anhe > [went down]** < to < earth(3) RT @YourFavWhiteGuy : Helppp meeeee .
I?mmmmeltiiinngggg ?
http://twitpic.com/316cjgHelppp** < meeeeeI?mmm** < meltiiinnggggFigure 2: Examples of GFL annotations from the corpus.Our annotators used the Graph Fragment Lan-guage (GFL), a text-based notation that facilitateskeyboard entry of parses (Schneider et al., 2013).
APython Flask web application allows the annotatorto validate and visualize each parse (Mordowanecet al., 2014).
Some examples are shown in Fig-ure 2.
Note that all of the challenges in ?2 arehandled easily by GFL notation: ?retweet?
infor-mation, punctuation, and a URL are not selected byvirtue of their exclusion from the GFL expression;in (2) went down is annotated as a MWE usingGFL?s square bracket notation; in (3) the tokensare grouped into two utterances whose roots aremarked by the ** symbol.Schneider et al.
?s GFL offers some additional fea-tures, only some of which we made use of in thisproject.
One important feature allows an annotatorto leave the parse underspecified in some ways.
Weallowed our annotators to make use of this feature;however, we excluded from our training and test-ing data any parse that was incomplete (i.e., anyparse that contained multiple disconnected frag-ments with no explicit root, excluding unselectedtokens).
Learning to parse from incomplete anno-tations is a fascinating topic explored in the past(Hwa, 2001; Pereira and Schabes, 1992) and, in thecase of tweets, left for future work.An important feature of GFL that we did use isspecial notation for coordination structures.
Forthe coordination structure in Figure 1, for example,the notation is:$a :: {?
want} :: {&}where $a creates a new node in the parse tree as it isvisualized for the annotator, and this new node at-taches to the syntactic parent of the conjoined struc-ture, avoiding the classic forced choice betweencoordinator and conjunct as parent.
For learning toparse, we transform GFL?s coordination structuresinto specially-labeled dependency parses collaps-ing nodes like $a with the coordinator and labelingthe attachments specially for postprocessing, fol-lowing Schneider et al.
(2013).
In our evaluation(?5), these are treated like other attachments.3.3 Annotation ConventionsA wide range of dependency conventions are in use;in many cases these are conversion conventionsspecifying how dependency trees can be derivedfrom phrase-structure trees.
For English, the mostpopular are due to Yamada and Matsumoto (2003)and de Marneffe and Manning (2008), known as?Yamada-Matsumoto?
(YM) and ?Stanford?
depen-dencies, respectively.
The main differences be-tween them are in whether the auxiliary is the par-ent of the main verb (or vice versa) and whether thepreposition or its argument heads a prepositionalphrase (Elming et al., 2013).A full discussion of our annotation conventionsis out of scope.
We largely followed the conven-tions suggested by Schneider et al.
(2013), which inturn are close to those of YM.
Auxiliary verbs areparents of main verbs, and prepositions are parentsof their arguments.
The key differences from YMare in coordination structures (discussed in ?3.2;YM makes the first conjunct the head) and posses-sive structures, in which the possessor is the childof the clitic, which is the child of the semantic head,e.g., the > king > ?s > horses.3.4 Intrinsic QualityOur approach to developing this initial corpus ofsyntactically annotated tweets was informed by anaversion to making the perfect the enemy of thegood; that is, we sought enough data of sufficientquality to build a usable parser within a relativelyshort amount of time.
If our research goals hadbeen to develop a replicable process for annotation,more training and more quality control would havebeen called for.
Under our budgeted time and anno-tator resources, this overhead was simply too costly.Nonetheless, we performed a few analyses that givea general picture of the quality of the annotations.Inter-annotator agreement.
170 of the tweetswere annotated by multiple users.
By the softCom-Prec measure (Schneider et al., 2013),3the agree-ment rate on dependencies is above 90%.Expert linguistic judgment.
A linguist co-author examined a stratified sample (balanced3softComPrec is a generalization of attachment accuracythat handles unselected tokens and MWEs.1004across annotators) of 60 annotations and rated theirquality on a 5-point scale.
30 annotations weredeemed to have ?no obvious errors,?
15 only minorerrors, 3 a major error (i.e., clear violation of an-notation guidelines),44 a major error and at leastone minor error, and 8 as containing multiple majorerrors.
Thus, 75% are judged as having no majorerrors.
We found this encouraging, considering thatthis sample is skewed in favor of people who anno-tated less (including many of the less experiencedand/or lower-proficiency annotators).Pairwise ranking.
For 170 of the doubly anno-tated tweets, an experienced annotator examinedwhether one or the other was markedly better.
In100 cases the two annotations were of comparablequality (neither was obviously better) and did notcontain any obvious major errors.
In only 7 pairsdid both of the annotations contain a serious error.Qualitatively, we found several unsurprisingsources of error or disagreement, including em-bedded/subordinate clauses, subject-auxiliary in-version, predeterminers, and adverbial modifiersfollowing a modal/auxiliary verb and a main verb.Clarification of the conventions, or even explicitrule-based checking in the validation step, mightlead to quality improvements in further annotationefforts.4 Parsing AlgorithmFor parsing, we start with TurboParser, which isopen-source and has been found to perform well ona range of parsing problems in different languages(Martins et al., 2013; Kong and Smith, 2014).
Theunderlying model allows for flexible incorporationof new features and changes to specification in theoutput space.
We briefly review the key ideas inTurboParser (?4.1), then describe decoder modifi-cations required for our problem (?4.2).
We thendiscuss features we added to TurboParser (?4.3).4.1 TurboParserLet an input sentence be denoted by x and the setof possible dependency parses for x be denoted byYx.
A generic linear scoring function based on a4What we deemed major errors included, for example,an incorrect dependency relation between an auxiliary verband the main verb (like ima > [have to]).
Minor errorsincluded an incorrect attachment between two modifiers ofthe same head, as in the > only > [grocery store]?thecorrect annotation would have two attachments to a singlehead, i.e.
the > [grocery store] < only (or equivalent).feature vector representation g is used in parsingalgorithms that seek to find:parse?
(x) = argmaxy?Yxw?g(x,y) (1)The score is parameterized by a vector w ofweights, which are learned from data (most com-monly using MIRA, McDonald et al., 2005a).The decomposition of the features into local?parts?
is a critical choice affecting the computa-tional difficulty of solving Eq.
1.
The most aggres-sive decomposition leads to an ?arc-factored?
or?first-order?
model, which permits exact, efficientsolution of Eq.
1 using spanning tree algorithms(McDonald et al., 2005b) or, with a projectivityconstraint, dynamic programming (Eisner, 1996).Second- and third-order models have also beenintroduced, typically relying on approximations,since less-local features increase the computationalcost, sometimes to the point of NP-hardness (Mc-Donald and Satta, 2007).
TurboParser attacks theparsing problem using a compact integer linear pro-gramming (ILP) representation of Eq.
1 (Martinset al., 2009), then employing alternating directionsdual decomposition (AD3; Martins et al., 2011).This enables inclusion of second-order features(e.g., on a word with its sibling or grandparent;Carreras, 2007) and third-order features (e.g., aword with its parent, grandparent, and a sibling, orwith its parent and two siblings; Koo and Collins,2010).For a collection of (possibly overlapping) partsfor input x, Sx(which includes the union of allparts of all trees in Yx), we will use the followingnotation.
Letg(x,y) = ?s?Sxfs(x,y), (2)where fsonly considers part s and is nonzero onlyif s is present in y.
In the ILP framework, each shas a corresponding binary variable zsindicatingwhether part s is included in the output.
A col-lection of constraints relating zsdefine the set offeasible vectors z that correspond to valid outputsand enfore agreement between parts that overlap.Many different versions of these constraints havebeen studied (Riedel and Clarke, 2006; Smith andEisner, 2008; Martins et al., 2009, 2010).A key attraction of TurboParser is that manyoverlapping parts can be handled, making use ofseparate combinatorial algorithms for efficientlyhandling subsets of constraints.
For example, theconstraints that force z to encode a valid tree canbe exploited within the framework by making calls1005to classic arborescence algorithms (Chu and Liu,1965; Edmonds, 1967).
As a result, when describ-ing modifications to TurboParser, we need only toexplain additional constraints and features imposedon parts.4.2 Adapted Parse PartsThe first collection of parts we adapt are simplearcs, each consisting of an ordered pair of indicesof words in x; arc(p,c) corresponds to the attach-ment of xcas a child of xp(iff zarc(p,c) = 1).
Our rep-resentation explicitly excludes some tokens frombeing part of the syntactic analysis (?2.1); to han-dle this, we constrain zarc(i, j) = 0 whenever xi or x jis excluded.The implication is that excluded tokens are still?visible?
to feature functions that involve otheredges.
For example, some conventional first-orderfeatures consider the tokens occurring between aparent and child.
Even if a token plays no syntacticrole of its own, it might still be informative aboutthe syntactic relationships among other tokens.
Wenote three alternative methods:1.
We might remove all unselected tokens fromx before running the parser.
In ?5.6 we findthis method to fare 1.7?2.3% worse than ourmodified decoding algorithm.2.
We might remove unselected tokens but usethem to define new features, so that they stillserve as evidence.
This is the approach takenby Ma et al.
(2014) for punctuation.
We judgeour simple modification to the decoding algo-rithm to be more expedient, and leave the trans-lation of existing context-word features into thatframework for future exploration.3.
We might incorporate the token selection deci-sions into the parser, performing joint inferencefor selection and parsing.
The AD3algorithmwithin TurboParser is well-suited to this kindof extension: z-variables for each token?s se-lection could be added, and similar scores tothose of our token selection sequence model(?2.1) could be integrated into parsing.
Given,however, that the sequence model achieves over97% accuracy, and that perfect token selectionwould gain only 0.1?1% in parsing accuracy (re-ported in ?5.5), we leave this option for futurework as well.For first-order models, the above change is allthat is necessary.
For second- and third-ordermodels, TurboParser makes use of head automata,in particular ?grand-sibling head automata?
thatassign scores to word tuples of xg, its child xp,and two of xp?s adjacent children, xcand x?c(Kooet al., 2010).
The second-order models in ourexperiments include parts for sibling(p,c,c?)
andgrandparent(p,c,g) and use the grand-sibling headautomaton to reason about these together.
Au-tomata for an unselected xpor xg, and transitionsthat consider unselected tokens as children, areeliminated.
In order to allow the scores to dependon unselected tokens between xcand x?c, we addedthe binned counts of unselected tokens (mostlypunctuation) joint with the word form and POStag of xpand the POS tag of xcand x?cas featuresscored in the sibling(p,c,c?)
part.
The changes dis-cussed above comprise the totality of adaptationswe made to the TurboParser algorithm; we refer tothem as ?parsing adaptations?
in the experiments.4.3 Additional FeaturesBrown clusters.
Owoputi et al.
(2013) found thatBrown et al.
(1992) clusters served as excellent fea-tures in Twitter POS tagging.
Others have foundthem useful in parsing (Koo et al., 2008) and othertasks (Turian et al., 2010).
We therefore followKoo et al.
in incorporating Brown clusters as fea-tures, making use of the publicly available Twitterclusters from Owoputi et al.5We use 4 and 6 bitcluster representations to create features whereverPOS tags are used, and full bit strings to createfeatures wherever words were used.Penn Treebank features.
A potential danger ofour choice to ?start from scratch?
in developinga dependency parser for Twitter is that the result-ing annotation conventions, data, and desired out-put are very different from dependency parses de-rived from the Penn Treebank.
Indeed, Foster et al.
(2011a) took a very different approach, applyingPenn Treebank conventions in annotation of a testdataset for evaluation of a parser trained using PennTreebank trees.
In ?5.4, we replicate, for depen-dencies, their finding that a Penn Treebank?trainedparser is hard to beat on their dataset, which wasnot designed to be topically representative of En-glish Twitter.
When we turn to a more realisticdataset like ours, we find the performance of thePenn Treebank?trained parser to be poor.Nonetheless, it is hard to ignore such a largeamount of high-quality syntactic data.
We there-5http://www.ark.cs.cmu.edu/TweetNLP/clusters/50mpaths21006fore opted for a simple, stacking-inspired incor-poration of Penn Treebank information into ourmodel.6We define a feature on every candidate arcwhose value is the (quantized) score of the same arcunder a first-order model trained on the Penn Tree-bank converted using head rules that are as closeas possible to our conventions (discussed in moredetail in ?5.1).
This lets a Penn Treebank modelliterally ?weigh in?
on the parse for a tweet, andlets the learning algorithm determine how muchconsideration it deserves.5 ExperimentsOur experiments quantify the contributions of vari-ous components of our approach.5.1 SetupWe consider two test sets.
The first, TEST-NEW,consists of 201 tweets from our corpus annotatedby the most experienced of our annotators (oneof whom is a co-author of this work).
Given verylimited data, we believe using the highest qualitydata for measuring performance, and lower-qualitydata for training, is a sensibly realistic choice.Our second test set, TEST-FOSTER, is the datasetannotated by Foster et al.
(2011b), which consistsof 250 sentences.
Recall that their corpus wasannotated with phrase structures according to PennTreebank conventions.
Conversion to match ourannotation conventions was carried out as follows:1.
We used the PennConverter tool with head ruleoptions selected to approximate our annotationconventions as closely as possible.72.
An experienced annotator manually modifiedthe automatically converted trees by:(a) Performing token selection (?2.1) to removethe tokens which have no syntactic function.
(b) Grouping MWEs (?2.2).
Here, most of theMWEs are named entities such as Manch-ester United.
(c) Attaching the roots of the utterance in tweetsto the ?wall?
symbol (?2.3).86Stacking is a machine learning method where the predic-tions of one model are used to create features for another.
Thesecond model may be from a different family.
Stacking hasbeen found successful for dependency parsing by Nivre andMcDonald (2008) and Martins et al.
(2008).
Johansson (2013)describes further advances that use path features.7http://nlp.cs.lth.se/software/treebank_converter; run with -rightBranching=false-coordStructure=prague -prepAsHead=true-posAsHead=true -subAsHead=true -imAsHead=true-whAsHead=false.8This was infrequent; their annotations split most multi-TRAIN TEST-NEW TEST-FOSTERtweets 717 201 < 250?unique tweets 569 201 < 250?tokens 9,310 2,839 2,841selected tokens 7,015 2,158 2,366types 3,566 1,461 1,230utterances 1,473 429 337multi-root tweets 398 123 60MWEs 387 78 109Table 1: Statistics of our datasets.
(A tweet with k annotationsin the training set is counted k times for the totals of tokens,utterances, etc.).
?TEST-FOSTER contains 250 manually splitsentences.
The number of tweets should be smaller but is notrecoverable from the data release.
(d) Recovering the internal structure of the nounphrases.
(e) Fixing a difference in conventions with re-spect to subject-auxiliary inversion.9We consider two training sets.
TRAIN-NEW con-sists of the remaining 717 tweets from our corpus(?3) annotated by the rest of the annotators.
Someof these tweets have annotations from multiple an-notators; 11 annotations for tweets that also oc-curred in TEST-NEW were excluded.
TRAIN-PTBis the conventional training set from the Penn Tree-bank (?2?21).
The PennConverter tool was usedto extract dependencies, with head rule options se-lected to approximate our annotation conventionsas closely as possible (see footnote 7).
The result-ing annotations lack the same attention to nounphrase?internal structure (?2.4) and handle subject-auxiliary inversions differently than our data.
Part-of-speech tags were coarsened to be compatiblewith the Twitter POS tags, using the mappings spec-ified by Gimpel et al.
(2011).Statistics for the in-domain datasets are given inTable 1.
As we can see in the table, more than halfof the tweets in our corpus have multiple utterances.The out-of-vocabulary rate for our TRAIN/TEST-NEW split is 33.7% by token and 62.5% by type;for TRAIN/TEST-FOSTER it is 41.4% and 64.6%respectively.
These are much higher than the 2.5%and 13.2% in the standard Penn Treebank split.All evaluations here are on unlabeled attachmentF1scores.10Our parser provides labels for coordi-nation structures and MWEs (?2), but we do notpresent detailed evaluations of those due to spaceconstraints.utterance tweets into separate sentence-instances.9For example, in the sentence Is he driving, we attachedhe to driving while PennConverter attaches it to Is.10Because of token selection, precision and recall may notbe equal.10075.2 PreprocessingBecause some of the tweets in our test set werealso in the training set of Owoputi et al.
(2013),we retrained their POS tagger on all the annotateddata they have minus the 201 tweets in our testset.
Its tagging accuracy was 92.8% and 88.7% onTEST-NEW and TEST-FOSTER, respectively.
Thetoken selection model (?2.1) achieves 97.4% onTEST-NEW with gold or automatic POS tagging;and on TEST-FOSTER, 99.0% and 99.5% with goldand automatic POS tagging, respectively.As noted in ?4.3, Penn Treebank features weredeveloped using a first-order TurboParser trainedon TRAIN-PTB; Brown clusters were included incomputing these Penn Treebank features if theywere available in the parser to which the features(i.e.
Brown clusters) were added.5.3 Main ParserThe second-order TurboParser described in ?4,trained on TRAIN-NEW (default hyperparametervalues), achieves 80.9% unlabeled attachment ac-curacy on TEST-NEW and 76.1% on TEST-FOSTER.The experiments consider variations on this mainapproach, which is the version released as TWEE-BOPARSER.The discrepancy between the two test sets iseasily explained: as noted in ?3.1, the datasetfrom which our tweets are drawn was designedto be representative of English on Twitter.
Fos-ter et al.
(2011b) selected tweets from Berming-ham and Smeaton?s (2010) corpus, which uses fiftypredefined topics like politics, business, sports,and entertainment?in short, topics not unlikethose found in the Penn Treebank.
Relative tothe Penn Treebank training set, the by-type out-of-vocabulary rates are 45.2% for TEST-NEW andonly 21.6% for TEST-FOSTER (cf.
13.2% for thePenn Treebank test set).Another mismatch is in the handling of utter-ances.
In our corpus, utterance segmentationemerges from multi-rooted annotations (?2.3).
Fos-ter et al.
(2011b) manually split each tweet intoutterances and treat those as separate instances intheir corpus, so that our model trained on oftenmulti-rooted tweets from TRAIN is being testedonly on single-rooted utterances.5.4 Experiment: Which Training Set?We consider the direct use of TRAIN-PTB insteadof TRAIN-NEW.
Table 2 reports the results on bothUnlabeled Attachment F1(%)mod.
POS POS as-isTEST-NEWBaseline 73.0 73.5+ Brown 73.7 73.3+ Brown & PA 72.9 73.1TEST-FOSTERBaseline 76.3 75.2+ Brown 75.5 76.7+ Brown & PA 76.9 77.0Table 2: Performance of second-order TurboParser trained onTRAIN-PTB, with various preprocessing options.
The mainparser (?5.3) achieves 80.9% and 76.1% on the two test sets,respectively; see ?5.4 for discussion.test sets, with various options.
?Baseline?
is off-the-shelf second-order TurboParser.
We consideraugmenting it with Brown cluster features (?4.3;?+ Brown?)
and then also with the parsing adapta-tions of ?4.2 (?+ Brown & PA?).
Another choiceis whether to modify the POS tags at test time; themodified version (?mod.
POS?)
maps at-mentionsto pronoun, and hashtags and URLs to noun.We note that comparing these scores to our mainparser (?5.3) conflates three very important inde-pendent variables: the amount of training data(39,832 Penn Treebank sentences vs. 1,473 Twitterutterances), the annotation method, and the sourceof the data.
However, we are encouraged that, onwhat we believe is the superior test set (TEST-NEW),our overall approach obtains a 7.8% gain with anorder of magnitude less annotated data.5.5 Experiment: Effect of PreprocessingTable 3 (second block, italicized) shows the per-formance of the main parser on both test sets withgold-standard and automatic POS tagging and to-ken selection.
On TEST-NEW, with either gold-standard POS tags or gold-standard token selection,performance increases by 1.1%; with both, it in-creases by 2.3%.
On TEST-FOSTER, token selec-tion matters much less, but POS tagging accountsfor a drop of more than 6%.
This is consistent withFoster et al.
?s finding: using a fine-grained PennTreebank?trained POS tagger (achieving around84% accuracy on Twitter), they saw 5?8% improve-ment in unlabeled dependency attachment accuracyusing gold-standard POS tags.5.6 Experiment: AblationsWe ablated each key element of our main parser?PTB features, Brown features, second order fea-tures and decoding, and the parsing adaptations of10080.760.780.80.82UnlabeledAttachment F 1Baseline ?PA?PTB ?PA?Brown ?PTB?Brown ?PA ?PTB ?Brown MainFirst?OrderSecond?Order(a) TEST-NEW0.70.720.740.76UnlabeledAttachment F 1Baseline ?PA?PTB ?PA?Brown ?PTB?Brown ?PA ?PTB ?Brown MainFirst?OrderSecond?Order(b) TEST-FOSTERFigure 3: Feature ablations; these charts present the same scores shown in Table 3 and more variants of the first-order model.Unlabeled Attachment F1(%)TEST-NEW TEST-FOSTERMain parser 80.9 76.1Gold POS and TS 83.2 82.8Gold POS, automatic TS 82.0 82.3Automatic POS, gold TS 82.0 76.2Single ablations:?
PTB 80.2 72.6?
Brown 81.2 75.4?
2nd order 80.1 75.6?
PA 79.2 73.7Double ablations:?
PTB, ?
Brown 79.5 72.8?
PTB, ?
2nd order 78.5 72.2?
PTB, ?
PA 77.4 69.6?
Brown, ?
2nd order 80.7 74.5?
Brown, ?
PA 78.2 73.7?
2nd order, ?
PA 77.7 73.5Baselines:Second order 76.5 70.4First order 76.1 70.4Table 3: Effects of gold-standard POS tagging and tokenselection (TS; ?5.5) and of feature ablation (?5.6).
The ?base-lines?
are TurboParser without the parsing adaptations in ?4.2and without Penn Treebank or Brown features.
The best resultin each column is bolded.
See also Figure 3.?4.2?as well as each pair of these.
These condi-tions use automatic POS tags and token selection.The ??
PA?
condition, which ablates parsing adap-tations, is accomplished by deleting punctuation(in training and test data) and parsing using Turbo-Parser?s existing algorithm.Results are shown in Table 3.
Further resultswith first- and second-order TurboParsers are plot-ted in Figure 3.
Notably, a 2?3% gain is obtained bymodifying the parsing algorithm, and our stacking-inspired use of Penn Treebank data contributes inboth cases, quite a lot on TEST-FOSTER (unsur-prisingly given that test set?s similarity to the PennTreebank).
More surprisingly, we find that Browncluster features do not consistently improve perfor-mance, at least not as instantiated here, with oursmall training set.6 ConclusionWe described TWEEBOPARSER, a dependencyparser for English tweets that achieves over 80%unlabeled attachment score on a new, high-qualitytest set.
This is on par with state-of-the-art re-ported results for news text in Turkish (77.6%;Koo et al., 2010) and Arabic (81.1%; Martinset al., 2011).
Our contributions include impor-tant steps taken to build the parser: a considera-tion of the challenges of parsing tweets that in-formed our annotation process, the resulting newTWEEBANK corpus, adaptations to a statisticalparsing algorithm, a new approach to exploitingdata in a better-resourced domain (the Penn Tree-bank), and experimental analysis of the decisionswe made.
The dataset and parser can be found athttp://www.ark.cs.cmu.edu/TweetNLP.AcknowledgmentsThe authors thank the anonymous reviewersand Andr?
Martins, Yanchuan Sim, Wang Ling,Michael Mordowanec, and Alexander Rush forhelpful feedback, as well as the annotators WaleedAmmar, Jason Baldridge, David Bamman, DallasCard, Shay Cohen, Jesse Dodge, Jeffrey Flanigan,Dan Garrette, Lori Levin, Wang Ling, Bill Mc-Dowell, Michael Mordowanec, Brendan O?Connor,Rohan Ramanath, Yanchuan Sim, Liang Sun, SamThomson, and Dani Yogatama.
This research wassupported in part by the U. S. Army Research Lab-oratory and the U. S. Army Research Office undercontract/grant number W911NF-10-1-0533 and byNSF grants IIS-1054319 and IIS-1352440.1009ReferencesAnne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.2003.
Building a treebank for French.
In Treebanks,pages 165?187.
Springer.Timonthy Baldwin and Su Nam Kim.
2010.
Multi-word expressions.
In Handbook of Natural Lan-guage Processing, Second Edition.
CRC Press, Tay-lor and Francis Group.Adam Bermingham and Alan F. Smeaton.
2010.
Clas-sifying sentiment in microblogs: Is brevity an advan-tage?
In Proc.
of CIKM.Ann Bies, Justin Mott, Colin Warner, and SethKulick.
2012.
English Web Treebank.
Techni-cal Report LDC2012T13, Linguistic Data Consor-tium.
URL http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2012T13.Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProc.
of CoNLL.Marie Candito and Matthieu Constant.
2014.
Strate-gies for contiguous multiword expression analysisand dependency parsing.
In Proc.
of ACL.Xavier Carreras.
2007.
Experiments with a higher-order projective dependency parser.
In Proc.
ofEMNLP-CoNLL.Yoeng-Jin Chu and Tseng-Hong Liu.
1965.
On shortestarborescence of a directed graph.
Scientia Sinica,14(10):1396.Michael Collins.
2002.
Discriminative training meth-ods for Hidden Markov Models: theory and ex-periments with perceptron algorithms.
In Proc.
ofEMNLP.Matthieu Constant and Anthony Sigogne.
2011.
MWU-aware part-of-speech tagging with a CRF model andlexical resources.
In Proc.
of the Workshop on Multi-word Expressions: from Parsing and Generation tothe Real World.Matthieu Constant, Anthony Sigogne, and Patrick Wa-trin.
2012.
Discriminative strategies to integrate mul-tiword expression recognition and parsing.
In Proc.of ACL.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies repre-sentation.
In Proc.
of COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation.Mark Dredze, John Blitzer, Partha Pratim Taluk-dar, Kuzman Ganchev, Joao Graca, and FernandoPereira.
2007.
Frustratingly hard domain adapta-tion for dependency parsing.
In Proc.
of EMNLP-CoNLL.Jack Edmonds.
1967.
Optimum branchings.
Journalof Research of the National Bureau of Standards B,71(233-240):160.Jacob Eisenstein.
2013.
What to do about bad languageon the internet.
In Proc.
of NAACL-HLT.Jason Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proc.
ofCOLING.Jakob Elming, Anders Johannsen, Sigrid Klerke,Emanuele Lapponi, H?ctor Mart?nez Alonso, andAnders S?gaard.
2013.
Down-stream effects of tree-to-dependency conversions.
In Proc.
of NAACL-HLT.Jenny Rose Finkel and Christopher D. Manning.
2009.Joint parsing and named entity recognition.
In Procof ACL-HLT.Jennifer Foster, ?zlem ?etinoglu, Joachim Wagner,Joseph Le Roux, Stephen Hogan, Joakim Nivre,Deirdre Hogan, and Josef van Genabith.
2011a.#hardtoparse: POS tagging and parsing the Twitter-verse.
In Proc.
of AAAI Workshop on Analyzing Mi-crotext.Jennifer Foster, ?zlem ?etino?glu, Joachim Wagner,Joseph Le Roux, Joakim Nivre, Deirdre Hogan, andJosef van Genabith.
2011b.
From news to comment:resources and benchmarks for parsing the languageof Web 2.0.
In Proc.
of IJCNLP.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor Twitter: annotation, features, and experiments.In Proc.
of ACL-HLT.Spence Green, Marie-Catherine de Marneffe, andChristopher D. Manning.
2012.
Parsing models foridentifying multiword expressions.
ComputationalLinguistics, 39(1):195?227.Stephan Greene and Philip Resnik.
2009.
Syntac-tic packaging and implicit sentiment.
In Proc.
ofNAACL.1010Jan Haji?c, Eva Haji?cov?, Jarmila Panevov?, Petr Sgall,Silvie Cinkov?, Eva Fu?c?kov?, Marie Mikulov?,Petr Pajas, Jan Popelka, Ji?r?
Semeck`y, Jana?indlerov?, Jan ?t?ep?nek, Josef Toman, Zde?nkaUre?ov?, and Zden?ek ?abokrtsk?.
2012.
PragueCzech-English Dependency Treebank 2.0.
Techni-cal Report LDC2012T08, Linguistic Data Consor-tium.
URL http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2012T08.Rebecca Hwa.
2001.
Learning Probabilistic Lexical-ized Grammars for Natural Language Processing.Ph.D.
thesis, Harvard University.Richard Johansson.
2013.
Training parsers on incom-patible treebanks.
In Proc.
of NAACL-HLT.Mark Johnson.
1998.
PCFG models of linguistictree representations.
Computational Linguistics,24(4):613?632.Lingpeng Kong and Noah A. Smith.
2014.
An empiri-cal comparison of parsing methods for Stanford de-pendencies.
ArXiv:1404.4314.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
InProc.
of ACL.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proc.
of ACL.Terry Koo, Alexander M. Rush, Michael Collins,Tommi Jaakkola, and David Sontag.
2010.
Dual de-composition for parsing with non-projective head au-tomata.
In Proc.
of EMNLP.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: The goodthe bad and the OMG!
In Proc.
of ICWSM.Joseph Le Roux, Matthieu Constant, and AntoineRozenknop.
2014.
Syntactic parsing and compoundrecognition via dual decomposition: application toFrench.
In Proc.
of COLING.Ji Ma, Yue Zhang, and Jingbo Zhu.
2014.
Punctua-tion processing for projective dependency parsing.In Proc.
of ACL.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional linguistics, 19(2):313?330.Andr?
F.T.
Martins, Miguel Almeida, and Noah A.Smith.
2013.
Turning on the turbo: Fast third-ordernon-projective turbo parsers.
In Proc.
of ACL.Andr?
F.T.
Martins, Dipanjan Das, Noah A. Smith, andEric P. Xing.
2008.
Stacking dependency parsers.
InProc.
of EMNLP.Andr?
F.T.
Martins, Noah A. Smith, Pedro M.Q.Aguiar, and M?rio A.T. Figueiredo.
2011.
Dual de-composition with many overlapping components.
InProc.
of EMNLP.Andr?
F.T.
Martins, Noah A. Smith, and Eric P. Xing.2009.
Concise integer linear programming formu-lations for dependency parsing.
In Proc.
of ACL-IJCNLP.Andr?
F.T.
Martins, Noah A. Smith, Eric P. Xing, Pe-dro M.Q.
Aguiar, and M?rio A.T. Figueiredo.
2010.Turbo parsers: Dependency parsing by approximatevariational inference.
In Proc.
of EMNLP.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005a.
Online large-margin training of de-pendency parsers.
In Proc.
of ACL.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Haji?c.
2005b.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proc.
of HLT-EMNLP.Ryan McDonald and Giorgio Satta.
2007.
On the com-plexity of non-projective data-driven dependencyparsing.
In Proc.
of IWPT.Michael T. Mordowanec, Nathan Schneider, Chris.Dyer, and Noah A. Smith.
2014.
Simplified depen-dency annotations with GFL-Web.
In Proc.
of ACL,demonstration track.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proc.
of ACL-HLT.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah A.Smith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProc.
of NAACL-HLT.Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed cor-pora.
In Proc.
of ACL.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 Shared Task on Parsing the Web.
In Notesof the First Workshop on Syntactic Analysis of Non-Canonical Language.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.
Amaximum entropy approach to identifying sentenceboundaries.
In Proc.
of ANLP.1011Sebastian Riedel and James Clarke.
2006.
Incremen-tal integer linear programming for non-projective de-pendency parsing.
In Proc.
of EMNLP.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: an ex-perimental study.
In Proc.
of EMNLP.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
Multiwordexpressions: A pain in the neck for NLP.
In Proc.
ofCICLing.Nathan Schneider, Emily Danchik, Chris Dyer, andNoah A. Smith.
2014.
Discriminative lexical se-mantic segmentation with gaps: Running the MWEgamut.
Transactions of the Association for Compu-tational Linguistics, 2:193?206.Nathan Schneider, Brendan O?Connor, Naomi Saphra,David Bamman, Manaal Faruqui, Noah A. Smith,Chris Dyer, and Jason Baldridge.
2013.
A frame-work for (under)specifying dependency syntax with-out overloading annotators.
In Proc.
of the 7th Lin-guistic Annotation Workshop and Interoperabilitywith Discourse.David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In Proc.
of EMNLP.Sandeep Soni, Tanushree Mitra, Eric Gilbert, and JacobEisenstein.
2014.
Modeling factuality judgments insocial media text.
In Proc.
of ACL.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proc.
of ACL.David Vadas and James Curran.
2007.
Adding nounphrase structure to the Penn Treebank.
In Proc.
ofACL.Eva Maria Vecchi, Roberto Zamparelli, and Marco Ba-roni.
2013.
Studying the recursive behaviour ofadjectival modification with compositional distribu-tional semantics.
In Proc.
of EMNLP.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proc.
of IWPT.1012
