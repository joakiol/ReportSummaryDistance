Proceedings of the First Workshop on Multilingual Modeling, pages 25?31,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsA Comparable Corpus Based on Aligned Multilingual OntologiesRoger GranadaPUCRS (Brazil)roger.granada@acad.pucrs.brLucelene LopesPUCRS (Brazil)lucelene.lopes@pucrs.brCarlos RamischUniversity of Grenoble (France)ceramisch@inf.ufrgs.brCassia TrojahnUniversity of Grenoble (France)cassia.trojahn@inria.frRenata VieiraPUCRS (Brazil)renata.vieira@pucrs.brAline VillavicencioUFRGS (Brazil)alinev@gmail.comAbstractIn this paper we present a methodology forbuilding comparable corpus, using multilin-gual ontologies of a scpecific domain.
Thisresource can be exploited to foster research onmultilingual corpus-based ontology learning,population and matching.
The building re-source process is exemplified by the construc-tion of annotated comparable corpora in En-glish, Portuguese, and French.
The corpora,from the conference organization domain, arebuilt using the multilingual ontology conceptlabels as seeds for crawling relevant docu-ments from the web through a search engine.Using ontologies allows a better coverage ofthe domain.
The main goal of this paper isto describe the design methodology followedby the creation of the corpora.
We present apreliminary evaluation and discuss their char-acteristics and potential applications.1 IntroductionOntological resources provide a symbolic model ofthe concepts of a scientific, technical or generaldomain (e.g.
Chemistry, automotive industry, aca-demic conferences), and of how these concepts arerelated to one another.
However, ontology creationis labour intensive and error prone, and its mainte-nance is crucial for ensuring the accuracy and util-ity of a given resource.
In multilingual contexts, itis hard to keep the coherence among ontologies de-scribed in different languages and to align them ac-curately.
These difficulties motivate the use of semi-automatic approaches for cross-lingual ontology en-richment and population, along with intensive reuseand interoperability between ontologies.
For that, itis crucial to have domain-specific corpora available,or the means of automatically gathering them.Therefore, this paper describes an ontology-basedapproach for the generation of multilingual compa-rable corpora.
We use a set of multilingual domain-dependent ontologies, which cover different aspectsof the conference domain.
These ontologies providethe seeds for building the domain specific corporafrom the web.
Using high-level background knowl-edge expressed in concepts and relations, which arerepresented as natural language descriptions in thelabels of the ontologies, allow focused web crawl-ing with a semantic and contextual coverage of thedomain.
This approach makes web crawling moreprecise, which is crucial when exploiting the web asa huge corpus.Our motivation is the need of such resourcesin tasks related to semi-automatic ontology cre-ation and maintenance in multilingual domains.We exemplify our methodology focusing on theconstruction of three corpora, one in English,one in Portuguese, and one in French.
Thiseffort is done in the context of a larger re-search project which aims at investigating meth-ods for the construction of lexical resources, in-tegrating multilingual lexica and ontologies, fo-cusing on collaborative and automatic techniques(http://cameleon.imag.fr/xwiki/bin/view/Main/).In the next section, we present some relevant re-lated work (?2).
This is followed by a descriptionof the methodology used to build the corpora (?3).Finally, the application example expressed by theresulting corpora are evaluated (?4) and discussed25(?5).
We conclude by outlining their future applica-tions (?
6).2 Related WorkWeb as corpus (WAC) approaches have been suc-cessfully adopted in many cases where data sparse-ness plays a major limiting role, either in specificlinguistic constructions and words in a language(e.g.
compounds and multiword expressions), or forless resourced languages in general1.For instance, Grefenstette (1999) uses WAC formachine translation of compounds from French intoEnglish, Keller et al (2002) for adjective-noun,noun-noun and verb-object bigram discovery, andKim and Nakov (2011) for compound interpretation.Although a corpus derived from the web may con-tain noise, the sheer size of data available shouldcompensate for that.
Baroni and Ueyama (2006)discuss in details the process of corpus construc-tion from web pages for both generic and domain-specific corpora.
In particular, they focus on thecleaning process applied to filter the crawled webpages.
Much of the methodology applied in ourwork is similar to their proposed approach (see ?3).Moreover, when access to parallel corpora is lim-ited, comparable corpora can minimize data sparse-ness, as discussed by Skadina et al (2010).
Theycreate bilingual comparable corpora for a variety oflanguages, including under-resourced ones, with 1million words per language.
This is used as ba-sis for the definition of metrics for comparability oftexts.
Forsyth and Sharoff (2011) compile compa-rable corpora for terminological lexicon construc-tion.
An initial verification of monolingual compa-rability is done by partitioning the crawled collec-tion into groups.
Those are further extended throughthe identification of representative archetypal textsto be used as seeds for finding documents of thesame type.Comparable corpora is a very active research sub-ject, being in the core of several European projects(e.g.
TTC2, Accurat3).
Nonetheless, to date most of1Kilgarriff (2007) warns about the dangers of statistics heav-ily based on a search engine.
However, since we use the down-loaded texts of web pages instead of search engine count esti-mators, this does not affect the results obtained in this work.2www.ttc-project.eu3www.accurat-project.euthe research on comparable corpora seems to focuson lexicographic tasks (Forsyth and Sharoff, 2011;Sharoff, 2006), bilingual lexicon extraction (Morinand Prochasson, 2011), and more generally on ma-chine translation and related applications (Ion et al,2011).
Likewise, there is much to be gained fromthe potential mutual benefits of comparable corporaand ontology-related tasks.Regarding multilingually aligned ontologies, veryfew data sets have been made available for use inthe research community.
Examples include the vlcr4and the mldirectory5 datasets.
The former con-tains a reduced set of alignments between the the-saurus of the Netherlands Institute for Sound andVision and two other resources, English WordNetand DBpedia.
The latter consists of a set of align-ments between web site directories in English andin Japanese.
However, these data sets provide sub-sets of bilingual alignments and are not fully pub-licly available.
The MultiFarm dataset6, a multilin-gual version of the OntoFarm dataset (S?va?b et al,2005), has been designed in order to overcome thelack of multilingual aligned ontologies.
MultiFarmis composed of a set of seven ontologies that coverthe different aspects of the domain of organizing sci-entific conferences.
We have used this dataset as thebasis for generating our corpora.3 MethodologyThe main contribution of this paper is the proposalof the methodology to build corpora.
This sec-tion describes the proposed methodology present-ing our own corpus crawler, but also its applicationto construct three corpora, in English, Portuguese,and French.
These corpora are constructed from theMultiFarm dataset.3.1 Tools and ResourcesInstead of using an off-the-shelf web corpus toolsuch as BootCaT (Baroni and Bernardini, 2004), weimplemented our own corpus crawler.
This allowedus to have more control on query and corpus con-struction process.
Even though our corpus construc-4www.cs.vu.nl/?laurah/oaei/20095oaei.ontologymatching.org/2008/mldirectory6web.informatik.uni-mannheim.de/multifarm26tion strategy is similar to the one implemented inBootCaT, there are some significant practical issuesto take into account, such as:?
The predominance of multiword keywords;?
The use of the fixed keyword conference;?
The expert tuning of the cleaning process;?
The use of a long term support search AP[b].Besides, BootCaT uses the Bing search API,which will no longer work in 2012.
As our workis part of a long-term project, we preferred to useGoogle?s search API as part of the University Re-search Program.The set of seed domain concepts comes fromthe MultiFarm dataset.
Seven ontologies from theOntoFarm project (Table 1), together with the align-ments between them, have been translated from En-glish into eight languages (Chinese, Czech, Dutch,French, German, Portuguese, Russian, and Span-ish).
As shown in Table 1, the ontologies differin numbers of classes, properties, and in their log-ical expressivity.
Overall, the ontologies have a highvariance with respect to structure and size and theywere based upon three types of resources:?
actual conferences and their web pages (type?web?),?
actual software tools for conference organisa-tion support (type ?tool?
), and?
experience of people with personal participa-tion in organisation of actual conferences (type?insider?
).Currently, our comparable corpus generation ap-proach focuses on a subset of languages, namely En-glish (en), Portuguese (pt) and French (fr).
Thelabels of the ontology concepts, like conference andcall for papers, are used to generate queries and re-trieve the pages in our corpus.
In the current imple-mentation, the structure and relational properties ofthe ontologies were ignored.
Concept labels wereour choice of seed keywords since we intended tohave comparable, heterogeneous and multilingualdomain resources.
This means that we need a corpusand an ontology referring to the same set of terms orconcepts.
We want to ensure that the concept labelsName Type C DP OPEkaw insider 74 0 33Sofsem insider 60 18 46Sigkdd web 49 11 17Iasted web 140 3 38ConfTool tool 38 23 13Cmt tool 36 10 49Edas tool 104 20 30Table 1: Ontologies from the OntoFarm dataset in termsof number of classes (C), datatype properties (DP) andobject properties (OP).are present in the corresponding natural language,textual sources.
This combination of resources is es-sential for our goals, which involve problems such asontology learning and enriching from corpus.
Thus,the original ontology can serve as a reference forautomatically extracted resources.
Moreover, weintend to use the corpus as an additional resourcefor ontology (multilingual) matching, and again thepresence of the labels in the corpus is of great rele-vance.3.2 Crawling and PreprocessingIn each language, a concept label that occurs intwo or more ontologies provides a seed keywordfor query construction.
This results in 49 en key-words, 54 pt keywords and 43 fr keywords.
Be-cause many of our keywords are formed by morethan one word (average length of keywords is re-spectively 1.42, 1.81 and 1.91 words), we combinethree keywords regardless of their sizes to form aquery.
The first keyword is static, and correspondsto the word conference in each language.
The queryset is thus formed by permuting keywords two bytwo and concatenating the static keyword to them(e.g.
conference reviewer program committee).
Thisresults in 1 ?
48 ?
47 = 2, 256 en queries, 2,756pt queries and 1,892 fr queries.
Average querylength is 3.83 words for en, 4.62 words for pt and4.91 words for fr.
This methodology is in line withthe work of Sharoff (2006), who suggests to buildqueries by combining 4 keywords and downloadingthe top 10 URLs returned for each query.The top 10 results returned by Google?s search27API7 are downloaded and cleaned.
Duplicate URLsare automatically removed.
We did not filter outURLs coming from social networks or Wikipediapages because they are not frequent in the corpus.Results in formats other than html pages (like .docand .pdf documents) are ignored.
The first clean-ing step is the extraction of raw text from the htmlpages.
In some cases, the page must be discarded forcontaining malformed html which our page cleaneris not able to parse.
In the future, we intend to im-prove the robustness of the HTML parser.3.3 Filtering and Linguistic AnnotationAfter being downloaded and converted to raw text,each page undergoes a two-step processing.
In thefirst step, markup characters as interpunctuation,quotation marks, etc.
are removed leaving only let-ters, numbers and punctuation.
Further heuristicsare applied to remove very short sentences (less than3 words), email addresses, URLs and dates, sincethe main purpose of the corpus is related to concept,instance and relations extraction.
Finally, heuristicsto filter out page menus and footnotes are included,leaving only the text of the body of the page.
Theraw version of the text still contains those expres-sions in case they are needed for other purposes.In the second step, the text undergoes linguisticannotation, where sentences are automatically lem-matized, POS tagged and parsed.
Three well-knownparsers were employed: Stanford parser (Klein andManning, 2003) for texts in English, PALAVRAS(Bick, 2000) for texts in Portuguese, and Berkeleyparser (Petrov et al, 2006) for texts in French.4 EvaluationThe characteristics of the resulting corpora are sum-marized in tables 2 and 3.
Column D of table 2shows that the number of documents retrieved ismuch higher in en than in pt and fr, and this isnot proportional to the number of queries (Q).
In-deed, if we look in table 3 at the average ratio ofdocuments retrieved per query (D/Q), the en queriesreturn much more documents than queries in otherlanguages.
This indicates that the search engine re-turns more distinct results in en and more duplicateURLs in fr and in pt.
The high discrepancy in7research.google.com/university/searchQ D W token W typeen 2,256 10,127 15,852,650 459,501pt 2,756 5,342 12,876,344 405,623fr 1,892 5,154 9,482,156 362,548Table 2: Raw corpus dimensions: number of queries (Q),documents (D), and words (W).D/Q S/D W/S TTRen 4.49 110.59 14.15 2.90%pt 1.94 120.08 20.07 3.15%fr 2.72 115.63 15.91 3.82%Table 3: Raw corpus statistics: average documents perquery (D/Q), sentences per document (S/D), words persentence (W/S) and type-token ration (TTR).the number of documents has a direct impact in thesize of the corpus in each language.
However, thisis counterbalanced by the average longer documents(S/D) and longer sentences (W/S) in pt and fr withrespect to en.
The raw corpus contains from 9.48million words in fr, 12.88 million words in pt to15.85 million words in en, constituting a large re-source for research on ontology-related tasks.A preliminary semi-automated analysis of the cor-pus quality was made by extracting the top-100 mostfrequent n-grams and unigrams for each language.Using the parsed corpora, the extraction of the top-100 most frequent n-grams for each language fo-cused on the most frequent noun phrases composedby at least two words.
The lists with the top-100most frequent unigrams was generated by extract-ing the most frequent nouns contained in the parsedcorpus for each language.
Four annotators manuallyjudged the semantic adherence of these lists to theconference domain.We are aware that semantic adherence is a vaguenotion, and not a straightforward binary classifica-tion problem.
However, such a vague notion wasconsidered useful at this point of the research, whichis ongoing work, to give us an initial indicationof the quality of the resulting corpus.
Examplesof what we consider adherent terms are appel a?communication (call for papers), conference pro-gram and texto completo (complete text), examples28# of adherent termsLower Upperen words 46 85en n-grams 57 94fr words 21 69fr n-grams 24 45pt words 32 70pt n-grams 11 45Table 4: Number of words and n-grams judged as seman-tically adherent to the domain.of nonadherent terms extracted from the corpus wereproduits chimiques (chemical products), followingcase, projeto de lei (law project).
In the three lan-guages, the annotation of terms included misparsedand mistagged words (ad hoc), places and dates typ-ical of the genre (but not necessarily of the domain),general-purpose terms frequent in conference web-sites (email, website) and person names.Table 4 shows the results of the annotation.
Thelower bound considers an n-gram as semanticallyadherent if all the judges agree on it.
The upperbound, on the other hand, considers as relevant n-grams all those for which at least one of the fourjudges rated it as relevant.
As a result of our anal-ysis, we found indications that the English corpuswas more adherent, followed by French and Por-tuguese.
This can be explained by the fact thatthe amount of internet content is larger for English,and that the number of international conferencesis higher than national conferences adopting Por-tuguese and French as their official languages.
Weconsidered the adherence of Portuguese and Frenchcorpora rather low.
There are indications that mate-rial related to political meetings, law and public in-stitutions was also retrieved on the basis of the seedterms.The next step in our evaluation is verifying itscomparable nature, by counting the proportion oftranslatable words.
Thus, we will use existing bilin-gual dictionaries and measure the rank correlation ofequivalent words in each language pair.5 DiscussionThe first version of the corpus containing the to-tality of the raw pages, the tools used to processthem, and a sample of 1,000 annotated texts foreach language are freely available for download atthe CAMELEON project website8.
For the rawfiles, each page is represented by an URL, a lan-guage code, a title, a snippet and the text of thepage segmented into paragraphs, as in the originalHTML file.
A companion log file contains informa-tion about the download dates and queries used to re-trieve each URL.
The processed files contain the fil-tered and parsed texts.
The annotation format variesfor each language according to the parser used.
Thefinal version of this resource will be available withthe totality of pages parsed.Since the texts were extracted from web pages,there is room for improvement concerning some im-portant issues in effective corpus cleaning.
Some ofthese issues were dealt with as described in the ?
3,but other issues are still open and are good candi-dates for future refinements.
Examples already fore-seen are the removal of foreign words, special char-acters, and usual web page expressions like ?site un-der construction?, ?follow us on twitter?, and ?clickhere to download?.
However, the relevance of someof these issues depends on the target application.
Forsome domains, foreign expressions may be genuinepart of the vocabulary (e.g.
parking or weekend incolloquial French and deadline in Portuguese), andas such, should be kept, while for other domainsthese expressions may need to be removed, sincethey do not really belong to the domain.
Therefore,the decision of whether to implement these filtersor not, and to deal with truly multilingual texts, de-pends on the target application.Another aspect that was not taken into account inthis preliminary version is related to the use of therelations between concepts in the ontologies to guidethe construction of the queries.
Exploiting the con-textual and semantic information expressed in theserelations may have an impact in the set of retrieveddocuments and will be exploited in future versionsof the corpus.6 Conclusions and Future WorkThis paper has described an ontology-based ap-proach for the generation of a multilingual compara-8cameleon.imag.fr/xwiki/bin/view/Main/Resources29ble corpus in English, Portuguese and French.
Thecorpus constructed and discussed here is an impor-tant resource for ontology learning research, freelyavailable to the research community.
The work onterm extraction that we are doing for the initial as-sessment of the corpus is indeed the initial step to-wards more ambitious research goals such as multi-lingual ontology learning and matching in the con-text of our long-term research project.The initial ontologies (originally built by hand)and resulting corpora can serve as a reference, a re-search resource, for information extraction tasks re-lated to ontology learning (term extraction, conceptformation, instantiation, etc).
The resource also al-lows the investigation of ontology enriching tech-niques, due to dynamic and open-ended nature oflanguage, by which relevant terms found in the cor-pus may not be part of the original ontology.
We canalso assess the frequencies (relevance) of the labelsof the ontology element with respect to the corpus,thus assessing the quality of the ontology itself.
An-other research that can be developed on the basis ofour resource is to evaluate the usefulness of a corpusin the improvement of existing multilingual ontol-ogy matching techniques9.Regarding to our own crawler implementation,we plan to work on its evaluation by using otherweb crawlers, as BootCaT, and compare both ap-proaches, specially on what concerns the use of on-tologies.From the point of view of NLP, several techniquescan be compared showing the impact of adoptingdifferent tools in terms of depth of analysis, fromPOS tagging to parsing.
This is also an important re-source for comparable corpora research, which canbe exploited for other tasks such as natural languagetranslation and ontology-based translation.
So farthis corpus contains English, Portuguese and Frenchversions, but the ontology data set includes 8 lan-guages, to which this corpus may be extended in thefuture.9An advantage of this resource is that the Multilingual Onto-Farm is to be included in the OAEI (Ontology Alignment Eval-uation Initiative) evaluation campaign.ReferencesMarco Baroni and Silvia Bernardini.
2004.
BootcaT:Bootstrapping corpora and terms from the web.
InProc.
of the Fourth LREC (LREC 2004), Lisbon, Por-tugal, May.
ELRA.Marco Baroni and Motoko Ueyama.
2006.
Buildinggeneral- and special-purpose corpora by web crawling.In Proceedings of the 13th NIJL International Sympo-sium on Language Corpora: Their Compilation andApplication, pages 31?40.Eckhard Bick.
2000.
The parsing system Palavras.Aarhus University Press.Richard Forsyth and Serge Sharoff.
2011.
From crawledcollections to comparable corpora: an approach basedon automatic archetype identification.
In Proc.
of Cor-pus Linguistics Conference, Birmingham, UK.Gregory Grefenstette.
1999.
The World Wide Web as aresource for example-based machine translation tasks.In Proc.
of the Twenty-First Translating and the Com-puter, London, UK, Nov. ASLIB.Radu Ion, Alexandru Ceaus?u, and Elena Irimia.
2011.An expectation maximization algorithm for textualunit alignment.
In Zweigenbaum et al (Zweigenbaumet al, 2011), pages 128?135.Frank Keller, Maria Lapata, and Olga Ourioupina.
2002.Using the Web to overcome data sparseness.
In JanHajic?
and Yuji Matsumoto, editors, Proc.
of the 2002EMNLP (EMNLP 2002), pages 230?237, Philadel-phia, PA, USA, Jul.
ACL.Adam Kilgarriff.
2007.
Googleology is bad science.Comp.
Ling., 33(1):147?151.Su Nam Kim and Preslav Nakov.
2011.
Large-scale nouncompound interpretation using bootstrapping and theweb as a corpus.
In Proc.
of the 2011 EMNLP(EMNLP 2011), pages 648?658, Edinburgh, Scotland,UK, Jul.
ACL.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proc.
of the 41st ACL (ACL2003), pages 423?430, Sapporo, Japan, Jul.
ACL.Emmanuel Morin and Emmanuel Prochasson.
2011.Bilingual lexicon extraction from comparable corporaenhanced with parallel corpora.
In Zweigenbaum et al(Zweigenbaum et al, 2011), pages 27?34.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
of the 21st COLINGand 44th ACL (COLING/ACL 2006), pages 433?440,Sidney, Australia, Jul.
ACL.Serge Sharoff, 2006.
Creating general-purpose cor-pora using automated search engine queries.
Gedit,Bologna, Italy.30Inguna Skadina, Ahmed Aker, Voula Giouli, Dan Tufis?,Robert Gaizauskas, Madara Mieirina, and Nikos Mas-tropavlos.
2010.
A Collection of Comparable Cor-pora for Under-resourced Languages.
In Inguna Skad-ina and Andrejs Vasiljevs, editors, Frontiers in Artifi-cial Intelligence and Applications, volume 219, pages161?168, Riga, Latvia, Oct. IOS Press.Ondr?ej S?va?b, Vojte?ch Sva?tek, Petr Berka, Dus?an Rak,and Petr Toma?s?ek.
2005.
Ontofarm: Towards an ex-perimental collection of parallel ontologies.
In PosterTrack of ISWC 2005.Pierre Zweigenbaum, Reinhard Rapp, and Serge Sharoff,editors.
2011.
Proc.of the 4th Workshop on Buildingand Using Comparable Corpora: Comparable Cor-pora and the Web (BUCC 2011), Portland, OR, USA,Jun.
ACL.31
