An Eff icient Generat ion  A lgor i thm for Lexical ist  MTVic tor  Poznaf isk i ,  J ohn  L. Beaven &: Pete  Whi te lock  *SHARP Laborator ies of Europe Ltd.Oxford Science Park,  Oxford OX4 4GAUni ted K ingdom{vp ~i lb ,pete }@sharp.
co.ukAbst rac tThe lexicalist approach to Machine Trans-lation offers significant advantages inthe development of linguistic descriptions.However, the Shake-and-Bake generationalgorithm of (Whitelock, 1992) is NP-complete.
We present a polynomial timealgorithm for lexicalist MT generation pro-vided that sufficient information can betransferred to ensure more determinism.1 IntroductionLexicalist approaches to MT, particularly those in-corporating the technique of Shake-and-Bake gen-eration (Beaven, 1992a; Beaven, 1992b; Whitelock,1994), combine the linguistic advantages of transfer(Arnold et al, 1988; Allegranza et al, 1991) andinterlingual (Nirenburg et al, 1992; Dorr, 1993) ap-proaches.
Unfortunately, the generation algorithmsdescribed to date have been intractable.
In this pa-per, we describe an alternative generation compo-nent which has polynomial time complexity.Shake-and-Bake translation assumes a sourcegrammar, a target grammar and a bilingual dictio-nary which relates translationally equivalent sets oflexical signs, carrying across the semantic dependen-cies established by the source language analysis tageinto the target language generation stage.The translation process consists of three phases:1.
A parsing phase, which outputs a multiset,or bag, of source language signs instantiatedwith sufficiently rich linguistic information es-tablished by the parse to ensure adequate trans-lations.2.
A lexical-semantic transfer phase which em-ploys the bilingual dictionary to map the bag*We wish to thank our colleagues Kerima Benkerimi,David Elworthy, Peter Gibbins, Inn Johnson, AndrewKay and Antonio Sanfilippo at SLE, and our anonymousreviewers for useful feedback and discussions on the re-search reported here and on earlier drafts of this paper.of instantiated source signs onto a bag of targetlanguage signs.3.
A generation phase which imposes an order onthe bag of target signs which is guaranteedgrammatical ccording to the monolingual tar-get grammar.
This ordering must respect helinguistic constraints which have been trans-ferred into the target signs.The Shake-an&Bake generation algorithm of(Whitelock, 1992) combines target language signsusing the technique known as generate-and-test.
Ineffect, an arbitrary permutation ofsigns is input to ashift-reduce parser which tests them for grammaticalwell-formedness.
If they are well-formed, the systemhalts indicating success.
If not, another permutationis tried and the process repeated.
The complexity ofthis algorithm is O(n!)
because all permutations (n!for an input of size n) may have to be explored tofind the correct answer, and indeed must be exploredin order to verify that there is no answer.Proponents of the Shake-and-Bake approach aveemployed various techniques to improve generationefficiency.
For example, (Beaven, 1992a) employsa chart to avoid recalculating the same combina-tions of signs more than once during testing, and(Popowich, 1994) proposes a more general techniquefor storing which rule applications have been at-tempted; (Brew, 1992) avoids certain pathologicalcases by employing lobal constraints on the solu-tion space; researchers such as (Brown et al, 1990)and (Chen and Lee, 1994) provide a system for baggeneration that is heuristically guided by probabil-ities.
However, none of these approaches i guar-anteed to avoid protracted search times if an exactanswer is required, because bag generation is NP-complete (Brew, 1992).Our novel generation algorithm has polynomialcomplexity (O(n4)).
The reduction in theoreticalcomplexity is achieved by placing constraints onthe power of the target grammar when operatingon instantiated signs, and by using a more restric-tive data structure than a bag, which we call atarget language normalised commutative bracketing261(TNCB).
A TNCB records dominance informationfrom derivations and is amenable to incremental up-dates.
This allows us to employ a greedy algorithmto refine the structure progressively until either atarget constituent is found and generation has suc-ceeded or no more changes can be made and gener-ation has failed.In the following sections, we will sketch the basicalgorithm, consider how to provide it with an initialguess, and provide an informal proof of its efficiency.2 A Greedy  Incrementa l  Generat ionA lgor i thmWe begin by describing the fundamentals of a greedyincremental generation algorithm.
The cruciM datastructure that it employs is the TNCB.
We give somedefinitions, state some key assumptions about suit-able TNCBs for generation, and then describe thealgorithm itself.2.1 TNCBsWe assume a sign-based grammar with binary rules,each of which may be used to combine two signsby unifying them with the daughter categories andreturning the mother.
Combination is the commuta-tive equivalent of rule application; the linear order-ing of the daughters that leads to successful rule ap-plication determines the orthography of the mother.Whitelock's Shake-and-Bake g neration algorithmattempts to arrange the bag of target signs untila grammatical ordering (an ordering which allowsall of the signs to combine to yield a single sign) isfound.
However, the target derivation informationitself is not used to assist the algorithm.
Even in(Beaven, 1992a), the derivation information is usedsimply to cache previous results to avoid exact re-computation at a later stage, not to improve on pre-vious guesses.
The reason why we believe such im-provement is possible is that, given adequate infor-mation from the previous stages, two target signscannot combine by accident; they must do so be-cause the underlying semantics within the signs li-censes it.If the linguistic data that two signs contain allowsthem to combine, it is because they are providinga semantics which might later become more spec-ified.
For example, consider the bag of signs thathave been derived through the Shake-and-Bake pro-cess which represent the phrase:(1) The big brown dogNow, since the determiner and adjectives all mod-ify the same noun, most grammars will allow us toconstruct he phrases:(2) The dog(3) The big dog(4) The brown dogas well as the 'correct' one.
Generation will fail ifall signs in the bag are not eventually incorporatedin tile final result, but in the naive algorithm, theintervening computation may be intractable.In the algorithm presented here, we start from ob-servation that the phrases (2) to (4) are not incorrectsemantically; they are simply under-specifications of(1).
We take advantage of this by recording theconstituents hat have combined within the TNCB,which is designed to allow further constituents o beincorporated with minimal recomputation.A TNCB is composed of a sign, and a history ofhow it was derived from its children.
The structureis essentially a binary derivation tree whose childrenare unordered.
Concretely, it is either NIL, or atriple:TNCB = NILlValue ?
TNCB x TNCBValue = Sign IINCONSISTENT IUNDETERMINEDThe second and third items of the TNCB tripleare the child TNCBs.
The value of a TNCB isthe sign that is formed from the combination of itschildren, or INCONSISTENT, representing the factthat they cannot grammatically combine, or UN-DETERMINED, i.e.
it has not yet been establishedwhether the signs combine.Undetermined TNCBs are commutative, .g.
theydo not distinguish between the structures hown inFigure 1.Figure 1: Equivalent TNCBsIn section 3 we will see that this property is im-portant when starting up the generation process.Let us introduce some terminology.A TNCB is?
well-formed iff its value is a sign,?
ill-formed iff its value is INCONSISTENT,?
undetermined (and its value is UNDETER-MINED) iff it has not been demonstratedwhether it is well-formed or ill-formed.?
maximal iff it is well-formed and its parent (if ithas one) is ill-formed.
In other words, a maxi-mal TNCB is a largest well-formed componentof a TNCB.262Since TNCBs are tree-like structures, if aTNCB is undetermined or ill-formed then so areall of its ancestors (the TNCBs that contain it).We define five operations on a TNCB.
The firstthree are used to define the fourth transformation(move) which improves ill-formed TNCBs.
The fifthis used to establish the well-formedness of undeter-mined nodes.
In the diagrams, we use a cross torepresent ill-formed nodes and a black circle to rep-resent undetermined ones.De let ion :  A maximal TNCB can be deletedfrom its current position.
The structure aboveit must be adjusted in order to maintain binarybranching.
In figure 2, we see that when node4 is deleted, so is its parent node 3.
The newnode 6, representing the combination of 2 and5, is marked undetermined.t*5 2 5I .
.
- - - -  JFigure 2 :4  is deleted, raising 5Con junct ion :  A maximal TNCB can be con-joined with another maximal TNCB if they maybe combined by rule.
In figure 3, it can be seenhow the maximal TNCB composed of nodes 1,2, and 3 is conjoined with the maximal TNCBcomposed of nodes 4, 5 and 6 giving the TNCBmade up of nodes 1 to 7.
The new node, 7, iswell-formed.1 4 72 3 5 6 2 35 6Figure 3 :1  is conjoined with 4 giving 7Ad junct ion :  A maximal TNCB can be in-serted inside a maximal TNCB, i.e.
conjoinedwith a non-maximal TNCB, where the combina-tion is licensed by rule.
In figure 4, the TNCBcomposed of nodes 1, 2, and 3 is inserted in-side the TNCB composed of nodes 4, 5 and 6.All nodes (only 8 in figure 4) which dominatethe node corresponding to the new combination(node 7) must be marked undetermined - -  suchnodes are said to be disrupted.12 3485 2 3 6Figure 4 :1  is adjoined next to 6 inside 4Movement :  This is a combination of a deletionwith a subsequent conjunction or adjunction.
Infigure 5, we illustrate a move via conjunction.In the left-hand figure, we assume we wish tomove the maximal TNCB 4 next to the maximalTNCB 7.
This first involves deleting TNCB 4(noting it), and raising node 3 to replace node2.
We then introduce node 8 above node 7, andmake both nodes 7 and 4 its children.
Notethat during deletion, we remove a surplus node(node 2 in this case) and during conjunction oradjunction we introduce a new one (node 8 inthis case) thus maintaining the same number ofnodes in the tree.9 /L3 7Figure 5: A conjoining move from 4 to 7Evaluat ion:  After a movement, the TNCBis undetermined as demonstrated in figure 5.The signs of the affected parts must be recal-culated by combining the recursively evaluatedchild TNCBs.2.2 Su i tab le  GrammarsThe Shake-and-Bake system of (Whitelock, 1992)employs a bag generation algorithm because it is as-sumed that the input to the generator is no morethan a collection of instantiated signs.
Full-scale baggeneration is not necessary because sufficient infor-mation can be transferred from the source languageto severely constrain the subsequent search duringgeneration.The two properties required of TNCBs (and hencethe target grammars with instantiated lexicM signs)are:1.
P recedence  Monoton ic i ty .
The order of the263orthographies of two combining signs in the or-thography of the result must be determinate - -it must not depend on any subsequent combi-nation that the result may undergo.
This con-straint says that if one constituent fails to com-bine with another, no permutation of the ele-ments making up either would render the com-bination possible.
This allows bottom-up eval-uation to occur in linear time.
In practice, thisrestriction requires that sufficiently rich infor-mation be transferred from the previous trans-lation stages to ensure that sign combination isdeterministic.2.
Dominance  Monoton ic i ty .
If a maximalTNCB is adjoined at the highest possible placeinside another TNCB, the result will be well-formed after it is re-evaluated.
Adjunction isonly attempted if conjunction fails (in fact con-junction is merely a special case of adjunctionin which no nodes are disrupted); an adjunctionwhich disrupts i nodes is attempted before onewhich disrupts i + 1 nodes.
Dominance mono-tonicity merely requires all nodes that are dis-rupted under this top-down control regime tobe well-formed when re-evaluated.
We will seethat this will ensure the termination of the gen-eration algorithm within n -  1 steps, where n isthe number of lexical signs input to the process.We are currently investigating the mathematicalcharacterisation f grammars and instantiated signsthat obey these constraints.
So far, we have notfound these restrictions particularly problematic.2.3 The  Generat ion  A lgor i thmThe generator cycles through two phases: a testphase and a rewrite phase.
Imagine a bag of signs,corresponding to "the big brown dog barked", hasbeen passed to the generation phase.
The first stepin the generation process is to convert it into somearbitrary TNCB structure, say the one in figure 6.In order to verify whether this structure is valid,we evaluate the TNCB.
This is the test phase.
Ifthe TNCB evaluates uccessfully, the orthographyof its value is the desired result.
If not, we enter therewrite phase.If we were continuing in the spirit of the origi-nal Shake-and-Bake generation process, we wouldnow form some arbitrary mutation of the TNCB andretest, repeating this test-rewrite cycle until we ei-ther found a well-formed TNCB or failed.
However,this would also be intractable due to the undirected-ness of the search through the vast number of possi-bilities.
Given the added derivation information con-tained within TNCBs and the properties mentionedabove, we can direct this search by incrementallyimproving on previously evaluated results.We enter the rewrite phase, then, with an ill-formed TNCB.
Each move operation must improvep lgFigure 6: An arbitrary right-branching TNCB struc-tureit.
Let us see why this is so.The move operation maintains the same numberof nodes in the tree.
The deletion of a maximalTNCB removes two ill-formed nodes (figure 2).
Atthe deletion site, a new undetermined node is cre-ated, which may or may not be ill-formed.
At thedestination site of the movement (whether conjunc-tion or adjunction), a new well-formed node is cre-ated.The ancestors of the new well-formed node willbe at least as well-formed as they were prior to themovement.
We can verify this by case:1.
When two maximal TNCBs are conjoined,nodes dominating the new node, which werepreviously ill-formed, become undetermined.When re-evaluated, they may remain ill-formedor some may now become well-formed.2.
When we adjoin a maximal TNCB within an-other TNCB, nodes dominating the new well-formed node are disrupted.
By dominancemonotonicity, all nodes which were disruptedby the adjunction must become well-formed af-ter re-evaluation.
And nodes dominating themaximal disrupted node, which were previouslyill-formed, may become well-formed after re-evaluation.We thus see that rewriting and re-evaluating mustimprove the TNCB.Let us further consider the contrived worst-casestarting point provided in figure 6.
After the testphase, we discover that every single interior node isill-formed.
We then scan the TNCB, say top-downfrom left to right, looking for a maximal TNCB tomove.
In this case, the first move will be PAST tobark, by conjunction (figure 7).Once again, the test phase fails to provide a well-formed TNCB, so we repeat he rewrite phase, thistime finding dog to conjoin with the (figure 8 showsthe state just after the second pass through the testphase).After further testing, we again re-enter the rewritephase and this time note that brown can be insertedin the maximal TNCB the dog barked adjoined withdog (figure 9).
Note how, after combining dog andthe, the parent sign reflects the correct orthography264Figure 7: The initial guessL___ t /  \PAST bark ~ brown .
tgFigure 8: The TNCB after "PAST" is moved to"bark"even though they did not have the correct linearprecedence.PAST bark the = browmt - ___ - JbigFigure 9: The TNCB after "dog" is moved to "the"After finding that big may not be conjoined withthe brown dog, we try to adjoin it within the latter.Since it will combine with brown dog, no adjunctionto a lower TNCB is attempted.The final result is the TNCB in figure 11, whoseorthography is "the big brown dog barked".We thus see that during generation, we formed abasic constituent, he dog, and incrementally refinedit by adjoining the modifiers in place.
At the heart ofthis approach is that, once well-formed, constituentscan only grow; they can never be dismantled.Even if generation ultimately fails, maximal well-formed fragments will have been built; the lattermay be presented to the user, allowing gracefuldegradation of output quality.the b~PAST bXark d'og b~o.n ~he ~'bfg,Figure 10: The TNCB after "brown" is moved to"dog"the big brown dog barkedPA k heFigure 11: The final TNCB after "big" is moved to"brown dog"3 Initialising the GeneratorConsidering the algorithm described above, we notethat the number of rewrites necessary to repair theinitial guess is no more than the number of ill-formedTNCBs.
This can never exceed the number of inte-rior nodes of the TNCB formed from n lexical signs(i.e.
n -2 ) .
Consequently, the better formed the ini-tial TNCB used by the generator, the fewer the num-ber of rewrites required to complete generation.
Inthe last section, we deliberately illustrated an initialguess which was as bad as possible.
In this section,we consider a heuristic for producing a motivatedguess for the initial TNCB.Consider the TNCBs in figure 1.
If we interpretthe S, O and V as Subject, Object and Verb we canobserve an equivalence between the structures withthe bracketings: (S (V O)), (S (O V)), ((V O) S),and ((O V) S).
The implication of this equivalenceis that if, say, we are translating into a (S (V O))language from a head-finM language and have iso-morphic dominance structures between the sourceand target parses, then simply mirroring the sourceparse structure in the initial target TNCB will pro-vide a correct initiM guess.
For example, the Englishsentence (5):(5) the book is red265has a corresponding Japanese quivalent (6):(6) ((hon wa) (akai desu))((book TOP) (red is))If we mirror the Japanese bracketing structure inEnglish to form the initial TNCB, we obtain: ((bookthe) (red is)).
This will produce the correct answerin the test phase of generation without he need torewrite at all.Even if there is not an exact isomorphism betweenthe source and target commutative bracketings, thefirst guess is still reasonable as long as the majorityof child commutative bracketings in the target lan-guage are isomorphic with their equivalents in thesource language.
Consider the French sentence:(7) ((le ((grandchien) brun)) aboya)(8) ((the ((big dog) brown)) barked)The TNCB implied by the bracketing in (8) isequivalent to that in figure 10 and requires just onerewrite in order to make it well-formed.
We thussee how the TNCBs can mirror the dominance in-formation in the source language parse in order tofurnish the generator with a good initial guess.
Onthe other hand, no matter how the SL and TL struc-tures differ, the algorithm will still operate correctlywith polynomial complexity.
Structural transfer canbe incorporated to improve the efficiency of genera-tion, but it is never necessary for correctness oreventractability.4 The  Complex i ty  o f  the  GeneratorThe theoretical complexity of the generator isO (n4),where n is the size of the input.
We give an informalargument for this.
The complexity of the test phaseis the number of evaluations that have to be made.Each node must be tested no more than twice in theworst case (due to precedence monotonicity), asonemight have to try to combine its children in eitherdirection according to the grammar rules.
There arealways exactly n - 1 non-leaf nodes, so the complex-ity of the test phase is O(n).
The complexity ofthe rewrite phase is that of locating the two TNCBsto be combined.
In the worst case, we can imaginepicking an arbitrary child TNCB (O(n)) and thentrying to find another one with which it combines(O(n)).
The complexity of this phase is thereforethe product of the picking and combining complex-ities, i.e.
O(n2).
The combined complexity of thetest-rewrite cycle is thus O(n3).
Now, in section 3,we argued that no more than n - 1 rewrites wouldever be necessary, thus the overall complexity of gen-eration (even when no solution is found) is O(n4).Average case complexity isdependent on the qual-ity of the first guess, how rapidly the TNCB struc-ture is actually improved, and to what extent theTNCB must be re-evaluated after rewriting.
In theSLEMaT system (Poznarlski et al, 1993), we havetried to form a good initial guess by mirroring thesource structure in the target TNCB, and allowingsome local structural modifications in the bilingualequivalences.Structural transfer operations only affect the ef-ficiency and not the functionality of generation.Transfer specifications may be incrementally refinedand empirically tested for efficiency.
Since completespecification of transfer operations i not requiredfor correct generation of grammatical target text,the version of Shake-and-Bake translation presentedhere maintains its advantage over traditional trans-fer models, in this respect.The monotonicity constraints, on the other hand,might constitute a dilution of the Shake-and-Bakeideal of independent grammars.
For instance, prece-dence monotonicity requires that the status of aclause (strictly, its lexical head) as main or sub-ordinate has to be transferred into German.
It isnot that the transfer of information per se compro-mises the ideal - -  such information must often ap-pear in transfer entries to avoid grammatical butincorrect ranslation (e.g.
a great man translatedas un homme grand).
The problem is justifyingthe main/subordinate distinction in every languagethat we might wish to translate into German.
Thisdistinction can be justified monolingually for theother languages that we treat (English, French, andJapanese).
Whether the constraints will ultimatelyrequire monolingual grammars to be enriched withentirely unmotivated features will only become clearas translation coverage is extended and new lan-guage pairs are added.5 Conc lus ionWe have presented a polynomial complexity gener-ation algorithm which can form part of any Shake-and-Bake style MT system with suitable grammarsand information transfer.
The transfer module isfree to attempt structural transfer in order to pro-duce the best possible first guess.
We tested aTNCB-based generator in the SLEMaT MT sys-tem with the pathological cases described in (Brew,1992) against Whitelock's original generation algo-rithm, and have obtained speed improvements ofseveral orders of magnitude.
Somewhat more sur-prisingly, even for short sentences which were notproblematic for Whitelock's ystem, the generationcomponent has performed consistently better.Re ferencesV.
Allegranza, P. Bennett, J. Durand, F. van Eynde,L.
Humphreys, P. Schmidt, and E. Steiner.
1991.Linguistics for Machine Translation: The EurotraLinguistic Specifications.
In C. Copeland, J. Du-rand, S. Krauwer, and B. Maegaard, editors, TheEurotra Formal Specifications.
Studies in Machine266Translation and Natural Language Processing 2,pages 15-124.
Office for Official Publications ofthe European Communities.D.
Arnold, S. Krauwer, L. des Tombe, and L. Sadler.1988.
'Relaxed' Compositionality in MachineTranslation.
In Second International Conferenceon Theoretical and Methodological Issues in Ma-chine Translation of Natural Languages, CarnegieMellon Univ, Pittsburgh.John L. Beaven.
1992a.
Lexicalist Unification-basedMachine Translation.
Ph.D. thesis, University ofEdinburgh, Edinburgh.John L. Beaven.
1992b.
Shake-and-Bake MachineTranslation.
In Proceedings of COLING 92, pages602-609, Nantes, France.Chris Brew.
1992.
Letting the Cat out of the Bag:Generation for Shake-and-Bake MT.
In Proceed-ings of COLING 92, pages 29-34, Nantes, France.Peter F. Brown, John Cocke, A Della Pietra, Vin-cent J. Della Pietra, Fredrick Jelinek, John D.Lafferty, Robert L. Mercer, and Paul S. Roossin.1990.
A Statistical Approach to Machine Trans-lation.
Computational Linguistics, 16(2):79-85,June.Hsin-Hsi Chen and Yue-Shi Lee.
1994.
A Correc-tive Training Algorithm for Adaptive Learning inBag Generation.
In International Conference onNew Methods in Language Processing (NeMLaP),pages 248-254, Manchester, UK.
UMIST.Bonnie Jean Dorr.
1993.
Machine Translation: AView from the Lexicon.
Artificial Intelligence Se-ries.
The MIT Press, Cambridge, Mass.Sergei Nirenburg, Jaime Carbonell, Masaru Tomita,and Kenneth Goodman.
1992.
Machine Trans-lation: A Knowledge-Based Approach.
MorganKaaufmann, San Mateo, CA.Fred Popowich.
1994.
Improving the Efficiencyof a Generation Algorithm for Shake and BakeMachine Translation using Head-Driven PhraseStructure Grammar.
TechnicM Report CMPT-TR 94-07, School of Computing Science, SimonFraser University, Burnaby, British Columbia,CANADA V5A 1S6.V.
Poznariski, John L. Beaven, and P. Whitelock.1993.
The Design of SLEMaT Mk II.
TechnicalReport IT-1993-19, Sharp Laboratories of Europe,LTD, Edmund Halley Road, Oxford Science Park,Oxford OX4 4GA, July.P.
Whitelock.
1992.
Shake and Bake Translation.In Proceedings of COLING 92, pages 610-616,Nantes, France.P.
Whitelock.
1994.
Shake-and-Bake Translation.In C. J. Rupp, M. A. Rosner, and R. L. Johnson,editors, Constraints, Language and Computation,pages 339-359.
Academic Press, London.267
