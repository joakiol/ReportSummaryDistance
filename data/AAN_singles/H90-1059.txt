DARPA Resource ManagementBenchmark Test ResultsJune 1990D.
S. Pallett, J. G. Fiscus, and J. S. GarofoloRoom A 216 Technology BuildingNational Institute of Standards and Technology (NIST)Gaithersburg, MD 20899IntroductionThe June 1990 DARPA Resource ManagementBenchmark Test makes use of the first ofseveral test sets provided with the ExtendedResource Management Speaker-DependentCorpus (RM2) [I].
The corpus was designedas a speaker-dependent extension to theResource Management (RM1) Corpus [2],consisting of (only) four speakers, but with alarge number (2400) of sentence utterancesfor each of these speakers for system trainingpurposes.
The corpus was produced on CD-ROM by NIST in April 1990, and distributedto DARPA contractors.
Results have beenreported to NIST for both speaker-dependentand speaker-independent systems, and theresults of NIST scoring and preliminaryanalysis of these data are included in thispaper.
In addition to the June 1990 (RM2)test set results, some sites also reported theresults of tests of new algorithms on test setsthat have been used in previous results ("test-retest" results), or for new (first-time) use ofprevious test sets, cr for new systems indevelopment.
Those results are also tabulated.Test ProtocolTest results were submitted to NIST forscoring by the same "standard scoringsoftware used in previous tests [3] andcontained on the CD-ROM version of the RM2corpus.
Minor modifications had to be madein order to accommodate the larger volume oftest data.
(For each of the four speakers,there were a total of 120 sentence utterances,so that the test consisted of a total of 480sentence utterances, in contrast to the test setsize of 300 sentence utterances used inprevious tests.)
Scoring options were notchanged from previous tests.Tabulated ResultsTable 1 presents results of NIST scoring of theJune 1990 RM2 Test Set results received byNIST as of June 21, 1990.For speaker-dependent systems, results arepresented for systems from BBN and MIT/LL[4] for two conditions of training: the set of600 sentence texts used in previous (e.g., RM1corpus) tests, and another condition makinguse of an additional 1800 sentence utterancesfor each speaker, for a total of 2400 trainingutterances.
For speaker-independent systems,results were reported from AT&T [S], BBN[6], CMU [7], MIT/LL [4], SFU [8] and SSI[9].
Most sites made use of the 109-speakersystem training condition used for previoustests and reported results on the RM2 test set.BBN's Speaker Independent and SpeakerAdaptive results [6] were reported for theFebruary 1989 Test sets, and are tabulated inTable 2.
SRI also reported results for the caseof having used the 12 speaker (7200 sentenceutterance) training material from the speaker-dependent RM1 corpus in addition to the 109speaker (3990 sentence utterance) speakerindependent system training set, for a total of11,190 sentence utterances for systemtraining.Table 2 presents results of NIST scoring ofother results reported by several sites on testsets other than the June 1990 (RM2) Test Set.In some cases (e.g., some of the "test-retest"cases) the results may reflect the benefits ofhaving used these test sets for retest purposesmore than one time.Signif icance Test ResultsNIST has implemented some of thesignificance tests \[3\] contained on the RMseries of CD-ROMs for some of the data sentfor these tests.
In general these tests serve toindicate that the differences in measuredperformance between many of these systemsare small -- certainly for systems that aresimilarly trained and/or share similaralgorithmic approaches to speech recognition.As a case in point, consider the sentence-levelMcNemar test results shown in Table 3,comparing the BBN and MIT/LL speakerdependent systems, when using the word-pairgrammar.
For the two systems that weretrained on 2400 sentence utterances, the BBNsystem had 426 (out of 480) sentencescorrect, and the MIT/LL system had 427correct.
In comparing these systems with theMcNemar test, there are subsets of 399responses that were identically correct, and 26identically incorrect.
The two systems differedin the number of unique errors by only onesentence (i.e., 27 vs. 28).
The significancetest obviously results in a "same" judgement.A similar comparison shows that the twosystems trained on 600 sentence utterancesyield a "same" judgement.
However,comparisons involving differently-trainedsystems do result in significant performancedifferences -- both within site, and across sites.Table 4 shows the results of implementationof the sentence-level McNemar test forspeaker-independent systems trained on the109 speaker/3990 sentence utterance trainingset, using the word-pair grammar, for theRM2 test set.For the no-grammar case for the speaker-independent systems, the sentence-levelMcNemar test indicates that the performancedifferences between these systems are notsignificant.
However, when implementing theword-level matched-pair sentence-segmentword error (MAPSSWE) test, the CMU systemhas significantly better performance than othersystems in this category.Note that the data for the SRI system trainedon 11,190 sentence utterances are notincluded in these comparisons, since thecomparisons are limited to systems trained on3990 sentence utterances.Other AnalysesSince release of the "standard scoringsoftware" used for the results reported at thismeeting, NIST has developed additionalscoring software tools.
One of these toolsperforms an analysis of the results reportedfor each lexical item.By focussing on individual lexical items("words") we can investigate lexical coverageas well as performance for individual wordsfor each individual test (such as the June1990 test).
In this RM2 test set there wereoccurrences of 226 mono-syllabic words and503 polysyllabic words -- larger coverage ofthe lexicon than in previous test sets.
Themost frequently appearing word was "THE",with 297 occurrences.In the case of the system we refer to as "BBN(2400 train)" with the word pair grammar, inthe case of the word "THE" -- 97.6% of theoccurrences of this word were correctlyrecognized, with 0.0% substitution errors,2.4% deletions, and 0.7% "resultant299insertions", for a total of 3.0% word error forthis lexical item.
What we term "resultantinsertions" correspond to cases for which aninsertion error of this lexical item occurred,but for which the cause is not known.The conventional scoring software providesdata on a "weighted" frequency-of-occurrencebasis.
All errors are counted equally, and themore frequently occurring words -- such as the"function" words -- typically contribute moreto the overall system performance measures.However, when comparing results from onetest set to another it is sometimes desirable tolook at measures that are not weighted byfrequency of occurrence.
Our recentlydeveloped scoring software permits us to dothis, and, by looking at results for the subsetof words that have appeared on all tests todate, some measures of progress over the pastseveral years are provided, without thecomplications introduced by variable coverageand different frequencies-of-occurrence oflexical items in different tests.
Furtherdiscussion of this is to appear in an SLS Notein preparation at NIST.By further partitioning the results of such ananalysis into those for mono- and poly-syllabicword subsets, some insights can be gained intothe state-of-the art as evidenced by the presenttests.For the speaker-dependent systems trained on2400 sentence utterances using the word-pairgrammar, the unweighted total word error formono-syllabic word subset is between 1.6%and 2.2% (with the MIT/LL system having aslightly (but not significantly) larger numberof "resultant insertions".
For thecorresponding case of poly-syllabic words, theunweighted total word error is 0.2% for eachsystem.For the CMU speaker independent system,using the word-pair grammar, the unweightedtotal word error for mono-syllabic words is5.6%, and for poly-syllabic words, 1.7%.By comparing the CMU speaker-independentsystem results to the best-trained speaker-dependent systems, one can observe that theerror rates for mono-syllabic words aretypically 3 to 4 times greater than for thespeaker-dependent systems, and for poly-syllabic words, approximately 8 times larger.When making similar comparisons, usingresults for other speaker-independent systemsand the best-trained speaker-dependentsystems, the mono-syllabic word error ratesare typically 4 to 6 times greater, and forpoly-syllabic words, 12 times larger.It is clear from such comparisons that thewell-trained speaker-dependent systems haveachieved substantially greater success inmodelling the poly-syllabic words than thespeaker-independent systems.Comparisons With Other RM Test SetsSeveral sites have noted that the four speakersof the RM2 Corpus are significantly differentfrom the speakers of the RM1 corpus.
Onespeaker in particular appears to be a "goat",and there may be two "sheep" -- to varyingdegrees for both speaker-dependent andspeaker-independent systems.
An ANOVA testshould be implemented to address thesignificance of this effect.It has been noted that there appears to be a"within-session effect" -- with later sentenceutterances being more difficult to recognizethan earlier.It has been argued that overall performance isworse for this test set than for other recenttest sets in the RM corpora, but thisconclusion does not appear to be supportedfor all systems.
Some sites have noted thatperformance for this test set is worse than forthe RM2 Development Test Set, but thesignificance of this effect is unknown.
Datafor the current AT&T system are available forboth the Feb 89 and Oct 89 SpeakerIndependent Test Sets, and indicate total worderrors of 5.2% and 4.7%, respectively (seeTable 2) vs. 5.7% for the June 1990 RM2test set (see Table 1), suggesting that the RM2test set is more difficult.
A similar comparisoninvolving the current CMU data for the Feb 89and Oct 89 Speaker Independent Test Setsindicates word error rates of 4.6% and 4.8%,respectively vs. 4.3% for the June 1990 testset, suggesting that for the current CMUsystem there is (probably insignificantly)better performance on the June 1990 test set.The significance of these differences is notknown, but appears to vary from system tosystem.SummaryThis paper has presented NIST's tabulationand preliminary analysis of results reported forDARPA Resource Management benchmarkspeech recognition tests just prior to the June1990 DARPA Speech and Natural LanguageWorkshop at Hidden Valley, PA.
The resultsare provided for both speaker-dependent,speaker-adaptive, and speaker-independentsystems, using both RM2 and RM1 testmaterial.
All results reported in this documentwere scored at NIST using NIST scoringsoftware.
The reader is referred to otherpapers in the Proceedings (e.g., references \[4 -9\]) for details of the systems and additionaldiscussion of these results.AcknowledgementsWe would like to acknowledge the cooperationof the following individuals who served aspoints-of-contact (and in many cases, principalresearcher) at their site: Jay Wilpon (AT&T),Francis Kubala (BBN), Kai-Fu Lee (CMU),Doug Paul (MIT/LL), Hy Murveit (SRI), and(for SSI), Bill Meisel and Kai-Fu Lee.
AtNIST, Jon Fiscus has been responsible fordevelopment and implementation of scoringtools, and John Garofolo has been responsiblefor production of the Resource ManagementCorpora on CD-ROM.References\[1\] "DARPA Extended Resource ManagementContinuous Speech Speaker-Dependent Corpus(RM2)", NIST speech discs 3-1.1 and 3-2.1,April 1990.\[2\] "DARPA Resource Management ContinuousSpeech Database (RM1)", NIST speech discs 2-1.1/2-2.1, 2-3.1, and 2-4.1, 1989-1990.\[3\] Pallett, D. S., "Tools for the Analysis ofBenchmark Speech Recognition Tests", paper$2.16 in Proceedings of ICASSP 90,International Conference on Acoustics, Speechand Signal Processing, April 3-6, 1990, pp.
97-100.\[4\] Paul, D. B., ''The Lincoln Tied-MixtureHMM Continuous Speech Recognizer",Proceedings of DARPA Speech and NaturalLanguage Workshop, June 1990.\[5\] C. H. Lee et al, "Improved AcousticModeling for Continuous peech Recognition",Proceedings of DARPA Speech and NaturalLanguage Workshop, June 1990.\[6\] Kubala, F. and Schwartz, R., "A NewParadigm for Speaker-Independent Trainingand Speaker Adaptation", Proceedings ofDARPA Speech and Natural LanguageWorkshop, June 1990.\[7\] Huang, X. et al, "Improved HiddenMarkov Modeling for Speaker-IndependentContinuous Speech Recognition", Proceedingsof DARPA Speech and Natural LanguageWorkshop, June 1990.\[8\] Murveit, H. Weintraub, M. and Cohen, M.,"Training Set Issues in SRI's DECIPHERSpeech Recognition System", Proceedings ofDARPA Speech and Natural LanguageWorkshop, June 1990.\[9\] Anikst, M. T. et al, "Experiments withTree-Structured MMI Encoders on the RMTask", Proceedings of DARPA Speech andNatural Language Workshop, June 1990.30..I.June 1990 RM2 (Four Speaker) Test SetSpeaker-Dependent Systemsa.
Word-Pair Grammar:BBN (2400 train)BBN (600 train)MIT/LL (2400 train)MIT/LL (600 train)b.
No Grammar:MIT/LL (2400 train)MIT/LL (600 train)Total SentCorr Sub Del Ins Err Err98.5 1.1 0.5 0.1 1.7 11.397.3 2.1 0.6 0.4 3.1 20.098.7 0.9 0.4 0.2 1.5 11.097.4 1.7 0.9 0.5 3.1 20.0Total SentCorr Sub Del Ins Err Err95.9 3.3 0.9 0.8 4.9 28.889.5 8.3 2.2 2.2 12.7 58.3S~eaker-Independent Systemsa.
Word-Pair Grammar, 109-Speaker Training:TotalCorr Sub Del Ins ErrAT&T (first run) 94.8 3.9 1.2 0.8 6.0AT&T (2nd ruddebugged) 94.9 3.7 1.4 0.6 5.7CMU 96.2 2.9 0.9 0.5 4.3MIT/LL 94.8 3.8 1.3 0.7 5.9SRI 94.1 4.8 1.1 0.6 6.5SRI (109 + 12 train) 95.6 3.4 0.9 0.4 4.8SSI (VQ FE, CI HMM BE) 81.8 11.5 6.7 1.2 19.5SSI (SSI FE, CI HMM BE) 85.8 10.4 3.9 1.3 15.6SSI (SSI FE, CD HMM BE) 92.4 5.3 2.4 0.4 8.0SentErr32.331.527.131.932.127.169.859.641.3b.
No Grammar, 109-Speaker Training:Total SentCorr Sub Del Ins Err ErrAT&T (first run) 77.7 16.7 5.6 1.5 23.8 78.3AT&T (2nd rddebugged) 77.7 16.7 5.6 1.5 23.8 78.3CMU 81.9 14.8 3.4 1.8 19.9 74.4MIT/LL 79.1 16.5 4.4 2.1 22.9 74.6SRI 75.7 18.3 6.0 1.5 25.7 77.3Table 1.Results Reported to NIST for Previous Test Setsa.
AT&T (109-speaker training - 2nd ruddebugged retest):Total SentCorr Sub Del Ins Err ErrAT&T (Feb '89 SI WPG) 95.5 3.4 1.1 0.7 5.2 28.0AT&T (Feb '89 SI NG) 80.5 15.0 4.5 2.3 21.7 75.3AT&T (Oct '89 SI WPG) 96.2 2.9 0.9 0.9 4.7 27.3AT&T (Oct '89 SI NG) 80.6 14.5 5.0 2.6 22.0 76.7b.
BBN (Feb '89 SI set - not previously reported upon):Total SentCorr Sub Del Ins Err ErrBBN (Feb '89 3-12 WPG) 93.7 4.9 1.4 1.1 7.4 37.0BBN (Feb '89 SI-109* WPG) 94.8 4.3 1.0 1.2 6.5 34.3(log* = > 4360 sentence utterances used for training)c. BBN (Feb '89 SD set, speaker-adaptive):Total SentCorr Sub Del Ins Err ErrBBN (Feb '89 SA-1 WPG) 95.6 3.4 1.0 0.7 5.2 25.7BBN (Feb '89 SA-4 WPG) 96.4 2.5 1.1 0.7 4.3 23.3d.
CMU (109-speaker training retest):Total SentCorr Sub Del Ins Err ErrCMU (Feb '89 SI WPG) 96.1 3.2 0.6 0.7 4.6 24.0CMU (Oct '89 SI WPG) 96.2 2.7 1.0 1.0 4.8 28.0e.
SSI: (June '88 set, 109 speaker training)Total SentCorr Sub Del Ins Err ErrVQ FE, CI HMM BE, WPG 80.3 14.9 4.8 2.2 22.0 71.3SSI FEY CI HMM BE, WPG 86.3 10.8 2.9 1.5 15.2 55.7SSI FEY CD HMM BE, WPG 93.6 5.1 1.3 0.7 7.1 36.7Table 2.Speaker-Dependent Word-Pair GrammarSentence-Level McNemar Test Analysis11 1bbn366 6018 36bbn11bbnl11111368 5916 37same339 4545 51Legendbbnbbn => BBN, 2400 training utterances11 = > LL, 2400 training utterancesbbnl = > BBN, 600 training utterances111 = > LL, 600 training utterancesTable 3.3 0 411same399 272826bbnlbbn373 53114311358 6926 27Speaker-Independent Word-Pair GrammarSentence-Level McNemar Test AnalysisI a t ta t ta t t lcrnu11srissil-I---ssi 1 a t t lsame320 59 146a t t122 20323 132cmuc mu274 5176 79same272 5754 97crnu272 7854 76same275 5475 76a t t l126 20319 132crnu134 21611 11911same268 5759 96same268 6159 92same275 7552 78Legendsrisame271 5455 100att = > AT&T (first run)attl = > AT&T (2nd ruddebugged)crnu = > CMU11 = > MIT/LLsri = > SRIssil = > SSI (VQ FE - CI HMM BE)ssi2 => SSI (SSI FE - CI HMM BE)ssi3 => SSI (SSI FE - CD HMM BE)Table 4.ssi2a t t163 16231 134a t t l164 16530 121cmu173 17721 109ssi3a t t234 9148 107a t t l237 9245 106cmu248 10234 9611 11163 164 236 9131 122 46 107sri sri166 160 228 9828 126 54 100ssi2 ssi 3115 30 127 1879 256 155 180ssi 3183 1199 187
