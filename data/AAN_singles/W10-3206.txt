Proceedings of the 8th Workshop on Asian Language Resources, pages 38?46,Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language ProcessingConstruction of bilingual multimodal corpora of referring expressions incollaborative problem solvingTOKUNAGA Takenobu IIDA Ryu YASUHARA Masaaki TERAI Asuka{take,ryu-i,yasuhara}@cl.cs.titech.ac.jp asuka@nm.hum.titech.ac.jpTokyo Institute of TechnologyDavid MORRIS Anja BELZD.Morris@brighton.ac.uk a.s.belz@itri.brighton.ac.ukUniversity of BrightonAbstractThis paper presents on-going work onconstructing bilingual multimodal corporaof referring expressions in collaborativeproblem solving for English and Japanese.The corpora were collected from dia-logues in which two participants collab-oratively solved Tangram puzzles witha puzzle simulator.
Extra-linguistic in-formation such as operations on puzzlepieces, mouse cursor position and piecepositions were recorded in synchronisa-tion with utterances.
The speech datawas transcribed and time-aligned with theextra-linguistic information.
Referringexpressions in utterances that refer to puz-zle pieces were annotated in terms of theirspans, their referents and their other at-tributes.
The Japanese corpus has alreadybeen completed, but the English counter-part is still undergoing annotation.
Wehave conducted a preliminary comparativeanalysis of both corpora, mainly with re-spect to task completion time, task suc-cess rates and attributes of referring ex-pressions.
These corpora showed signif-icant differences in task completion timeand success rate.1 IntroductionA referring expression (RE) is a linguistic de-vice that refers to a certain object of interest (e.g.used in describing where the object is located inspace).
REs have attracted a great deal of atten-tion in both language analysis and language gen-eration research.
In language analysis research,reference resolution, particularly anaphora resolu-tion (Mitkov, 2002), has a long research history asfar back as the mid-1970s (Hobbs, 1978).
Muchresearch has been conducted from both theoreticaland empirical perspectives, mainly concerning theidentification of antecedents or entities mentionedwithin the same text.
This trend, targeting refer-ence resolution in written text, is still dominant inthe language analysis, perhaps because such tech-niques are intended for use in applications such asinformation extraction.In contrast, in language generation research in-terest has recently shifted from the generation ofone-off references to entities to generation of REsin discourse context (Belz et al, 2010) and inves-tigating human referential behaviour in real worldsituations, with the aim of using such techniquesin applications like human-robot interaction (Pi-wek, 2007; Foster et al, 2008; Bard et al, 2009).In both analysis and generation, machine-learning approaches have come to replace rule-based approaches as the predominant researchtrend since the 1990s.
This trend has made anno-tated corpora an indispensable component of re-search for training and evaluating proposed meth-ods.
In fact, research on reference resolution hasdeveloped significantly as a result of large scalecorpora, e.g.
those provided by the Message Un-derstanding Conference (MUC)1 and the Auto-matic Content Extraction (ACE)2 project.
Thesecorpora were constructed primarily for informa-tion extraction research, thus were annotated withco-reference relations within texts.
Also in thelanguage generation community, several corpora1http://www.nlpir.nist.gov/related projects/muc/2http://www.itl.nist.gov/iad/tests/ace/38have been developed (Di Eugenio et al, 2000; By-ron, 2005; van Deemter et al, 2006; Foster andOberlander, 2007; Foster et al, 2008; Stoia et al,2008; Spanger et al, 2009a; Belz et al, 2010).Unlike the corpora of MUC and ACE, many arecollected from situated dialogues, and thereforeinclude multimodal information (e.g.
gestures andeye-gaze) other than just transcribed text (Martinet al, 2007).
Foster and Oberlander (2007) em-phasised that any corpus for language generationshould include all possible contextual informationat the appropriate granularity.
Since constructinga dialogue corpus generally requires experimentsfor data collection, this kind of corpus tends to besmall-scale compared with corpora for referenceresolution.Against this background, we have been de-veloping multimodal corpora of referring expres-sions in collaborative problem-solving settings.This paper presents on-going work of construct-ing bilingual (English and Japanese) comparablecorpora in this domain.
We achieve our goal byreplicating, for the English corpus, the same pro-cess of data collection and annotation as we usedfor our existing Japanese corpus (Spanger et al,2009a).
Our aim is to create bilingual multimodalcorpora collected from dialogues in dynamic situ-ations.
From the point of view of reference anal-ysis, our corpora contribute to augmenting the re-sources of multimodal dialogue corpora annotatedwith reference relations which have been minorin number compared to other types of text cor-pora.
From the point of view of reference gen-eration, our corpora contribute to increasing theresources available that can be used to further re-search of this kind.
In addition, our corpora con-tribute to comparative studies of human referentialbehaviour in different languagesThe structure of the paper is as follows.
Sec-tion 2 describes the experimental set-up for datacollection which was introduced in our previouswork (Spanger et al, 2009a).
The setting is basi-cally the same for the construction of the Englishcorpus.
Section 3 explains the annotation schemeadopted in our corpora, followed by a descriptionof a preliminary analysis of the corpora in sec-tion 4.
Section 5 briefly mentions related workto highlight the characteristics of our corpora.
Fi-nally, Section 6 concludes the paper and looks atpossible future directions.!"#$%&'#()*"+,-.
!%#+)#Figure 1: Screenshot of the Tangram simulator2 Data collection2.1 Experimental set-upWe recruited subjects in pairs of friends and col-leagues.
Each pair was instructed to solve Tan-gram puzzles collaboratively.
Tangram puzzlesare geometrical puzzles that originated in ancientChina.
The goal of a Tangram puzzle is to con-struct a given goal shape by arranging seven sim-ple shapes, as shown in Figure 1.
The pieces in-clude two large triangles, a medium-sized trian-gle, two small triangles, a parallelogram and asquare.With the aim of recording the precise positionof every piece and every action the participantsmade during the solving process, we implementeda Tangram simulator in which the pieces can bemoved, rotated and flipped with simple mouse op-erations on a computer display.
The simulator dis-plays two areas: a goal shape area and a work-ing area where the pieces can be manupulated andtheir movements are shown in real time.We assigned a different role to each participantof a pair: one acted as the solver and the other asthe operator.
The operator has a mouse for manip-ulating Tangram pieces, but does not have a goalshape on the screen.
The solver has a goal shapeon the screen but does not have a mouse.
This set-ting naturally leads to a situation where given acertain goal shape, the solver thinks of the neces-sary arrangement of the pieces and gives instruc-tions to the operator how to move them, while theoperator manipulates the pieces with the mouse39according to the solver?s instructions.Figure 2: Picture of the experiment settingAs we mentioned in our previousstudy (Spanger et al, 2009a), this interactionproduces frequent use of referring expressionsintended to distinguish specific pieces of thepuzzle.
In our Tangram simulator, all piecesare of the same color, thus color is not usefulin identifying a specific piece, i.e.
only sizeand shape are discriminative object-intrinsicattributes.
Instead, we can expect other attributessuch as spatial relations and deictic reference tobe used more often.Each pair of participants sat side by side asshown in Figure 2.
Each participant had his/herown computer display showing the shared work-ing area.
A room-divider screen was set betweenthe solver (right side) and operator (left side) toprevent the operator from seeing the goal shape onthe solver?s screen, and to restrict their interactionto speech only.!
"#$%&'()"*(!+#$,-"$.&/0!.#$1/"(!2#$34"*5$Figure 3: The goal shapes given to the subjectsEach participant pair was assigned 4 trials con-sisting of two symmetric and two asymmetricgoal shapes as shown in Figure 3.
In Cogni-tive Science, a wide variety of different kinds ofpuzzles have been employed extensively in thefield of Insight Problem solving.
This has beentermed the ?puzzle-problem approach?
(Sternbergand Davidson, 1996; Suzuki et al, 2001) andin the case of physical puzzles has relatively of-ten involved puzzle tasks of symmetric shapeslike the so-called T-puzzle, e.g.
(Kiyokawa andNakazawa, 2006).
In more recent work Tangrampuzzles have been used as a means to study var-ious new aspects of human problem solving ap-proaches, including collection of of eye-gaze in-formation (Baran et al, 2007).
In order to col-lect data as broadly as possible in this context, weset up puzzle-problems including both symmetri-cal as well as asymmetrical ones as shown in Fig-ure 3.The participants exchanged their roles after twotrials, i.e.
a participant first solves a symmetric andthen an asymmetric puzzle as the solver and thendoes the same as the operator, and vice versa.
Theorder of the puzzle trials is the same for all pairs.Before starting the first trial as the operator,each participant had a short training exercise inorder to learn how to manipulate pieces with themouse.
The initial arrangement of the pieces wasrandomised every time.
We set a time limit of 15minutes for the completion of each trial (i.e.
con-struction of the goal shape).
In order to prevent thesolver from getting into deep thought and keepingsilent, the simulator is designed to give a hint ev-ery five minutes by showing a correct piece posi-tion in the goal shape area.
After 10 minutes havepassed, a second hint is provided, while the pre-vious hint disappears.
A trial ends when the goalshape is complete or the time is up.
Utterances bythe participants are recorded separately in stereothrough headset microphones in synchronisationwith the position of the pieces and the mouse op-erations.
Piece positions and mouse actions wereautomatically recorded by the simulator at inter-vals of 10 msec.40Table 1: The ELAN Tiers of the corpusTier meaningOP-UT utterances by the operatorSV-UT utterances by the solverOP-REX referring expressions by the operatorOP-Ref referents of OP-REXOP-Attr attributes of OP-REXSV-REX referring expressions by the solverSV-Ref referents of SV-REXSV-Attr attributes of SV-REXAction action on a pieceTarget the target piece of ActionMouse the piece on which the mouse is hovering?
Indentation of Tier denotes parent-child relations.2.2 Subjects and collected dataFor our Japanese corpus, we recruited 12 Japanesegraduate students of the Cognitive Science depart-ment, 4 females and 8 males, and split them into 6pairs.
All pairs knew each other previously andwere of the same sex and approximately sameage3.
We collected 24 dialogues (4 trials by 6pairs) of about 4 hours and 16 minutes.
The av-erage length of a dialogue was 10 minutes 40 sec-onds (SD = 3 minutes 18 seconds).For the comparable English corpus, we re-cruited 12 native English speakers of various oc-cupations, 6 males and 6 females.
Their aver-age age was 30.
There were 6 pairs all of whomknew each other beforehand except for one pair.Whereas during the creation of the Japanese cor-pus we had to give extra attention to ensuring thatsocial relationships did not have an impact on howthe subjects communicated with one another, forthe English corpus there was no such concern.
Wecollected 24 dialogues (4 trials by 6 pairs) of 5hours and 7 minutes total length.
The averagelength of a dialogue was 12 minutes 47 seconds(SD = 3 minutes 34 seconds).3 AnnotationThe recorded speech data was transcribed andthe referring expressions were annotated withthe Web-based multi-purpose annotation tool3In Japan, the relationship of senior to junior or sociallyhigher to lower placed might affect the language use.
Wecarefully recruited pairs to avoid the effects of this social re-lationship such as the possible use of overly polite and indi-rect language, reluctance to correct mistakes etc.Table 2: Attributes of referring expressionsdpr : demonstrative pronoun, e.g.
?the same one?,?this?, ?that?, ?it?dad : demonstrative adjective, e.g.
?that triangle?siz : size, e.g.
?the large triangle?typ : type, e.g.
?the square?dir : direction of a piece, e.g.
?the triangle facing theleft?.prj : projective spatial relation (including directionalprepositions or nouns such as ?right?, ?left?,?above?.
.
. )
e.g.
?the triangle to the left of thesquare?tpl : topological spatial relation (including non-directional prepositions or nouns such as ?near?,?middle?.
.
.
), e.g.
?the triangle near the square?ovl : overlap, e.g.
?the small triangle under the largeone?act : action on pieces, e.g ?the triangle that you areholding now?, ?the triangle that you just rotated?cmp : complement, e.g.
?the other one?sim : similarity, e.g.
?the same one?num : number, e.g.
?the two triangle?rpr : repair, e.g.
?the big, no, small triangle?err : obvious erroneous expression, e.g.
?the square?referring to a trianglenest : nested expression; when a referring expressionincludes another referring expression, only theoutermost expression is annotated with this at-tribute, e.g.
?
(the triangle to the left of (the smalltriangle))?meta: metaphorical expression, e.g.
?the leg?, ?thehead?SLAT (Noguchi et al, 2008)4.
Our target expres-sions in this corpus are referring expressions re-ferring to a puzzle piece or a set of puzzle pieces.We do not deal with expressions referring to a lo-cation, a part of a piece or a constructed shape.These expressions are put aside for future work.The annotation of referring expressions is three-fold: (1) identification of the span of expressions,(2) identification of their referents, and (3) assign-ment of a set of attributes to each referring expres-sion.Using the multimodal annotation tool ELAN,5the annotations of referring expressions were thenmerged with extra-linguistic data recorded by theTangram simulator.
The available extra-linguisticinformation from the simulator consists of (1) theaction on a piece, (2) the coordinates of the mousecursor and (3) the position of each piece in the4We did not use SLAT for English corpus annotation.
In-stead, ELAN was directly used for annotating referring ex-pressions.5http://www.lat-mpi.eu/tools/elan/41Table 3: Summary of trialsID time success OP-REX SV-REX ID time success OP-REX SV-REXE01 15:00 J01 8:40 o 10 48E02 15:00 J02 11:49 o 7 55E03 15:00 J03 11:36 o 5 26E04 15:00 J04 7:31 o 2 21E05 15:00 J05 15:00 23 78E06 15:00 J06 11:12 o 5 60E07 15:00 J07 12:11 o 3 59E08 15:00 J08 11:20 o 4 61E09 10:39 o J09 14:59 o 36 84E10 15:00 J10 6:20 o 3 47E11 15:00 J11 5:21 o 2 14E12 8:30 o J12 13:40 o 37 77E13 14:33 o 8 95 J13 15:00 8 56E14 7:27 o 1 62 J14 4:48 o 1 29E15 14:02 o 16 127 J15 9:30 o 20 39E16 3:57 o 1 31 J16 5:07 o 3 17E17 13:00 o J17 13:37 o 10 46E18 6:40 o J18 8:57 o 4 51E19 15:00 J19 8:02 o 0 37E20 12:32 o J20 11:23 o 1 59E21 15:00 J21 10:12 o 7 71E22 15:00 J22 10:24 o 9 64E23 15:00 J23 15:00 0 69E24 5:36 o J24 14:22 o 0 76Ave.
12:47 6.5 78.8 Ave. 10:40 8.3 51.8SD 3:34 7.14 41.4 SD 3:18 10.4 20.1Total 5:06:56 10 26 315 Total 4:16:01 21 200 1,244working area.
Actions and mouse cursor positionsare recorded at intervals of 10 msec, and are ab-stracted into (1) a time span labeled with an actionsymbol (?move?, ?rotate?
or ?flip?)
and its targetpiece number (1?7), and (2) a time span labeledwith a piece number which is under the mousecursor during that span.
The position of pieces isupdated and recorded with a timestamp when theposition of any piece changes.
Information aboutpiece positions is not merged into the ELAN filesand is kept in separate files.
As a result, we have11 time-aligned ELAN Tiers as shown in Table 1.Two annotators (two of the authors) first an-notated four Japanese dialogues separately andbased on a discussion of discrepancies, decidedon the following criteria to identify a referring ex-pression.?
The minimum span of a noun phrase in-cluding necessary information to identify areferent is annotated.
The span might in-clude repairs with their reparandum and dis-fluency (Nakatani and Hirschberg, 1993) ifneeded.?
Demonstrative adjectives are included in ex-pressions.?
Erroneous expressions are annotated with aspecial attribute.?
An expression without a definite referent (i.e.a group of possible referents or none) is as-signed a referent number sequence consist-ing of a prefix, followed by the sequence ofpossible referents as its referent, if any arepresent.?
All expressions appearing in muttering tooneself are excluded.Table 2 shows a list of attributes of referringexpressions used in annotating the corpus.The rest of the 20 Japanese dialogues were an-notated by two of the authors and discrepancieswere resolved by discussion.
Four English dia-logues have been annotated so far by one of theauthors.4 Preliminary corpus analysisWe have already completed the Japanese corpus,which is named REX-J (2008-08), but only 4 outof 24 dialogues have been annotated for the En-glish counterpart (REX-E (2010-03)).
Table 3shows a summary of the trials.
The horizontal42lines divide the trials by pairs, ?o?
in the ?suc-cess?
column denotes that the trial was success-fully completed in the time limit (15 minutes), andthe ?OP-REX?
and ?SV-REX?
columns show thenumber of referring expressions used by the op-erator and the solver respectively.
The followingsubsections describe a preliminary comparison ofthe English and Japanese corpora.Table 4: Task completion timeLang.\Shape (a) (b) (c) (d)English 832.0 741.2 890.3 605.8(105.4) (246.5) (23.7) (287.2)Japanese 774.7 535.0 571.7 633.8(167.3) (168.5) (242.2) (215.2)* Average (SD)4.1 Task performanceWe conducted a two-way ANOVA with the taskcompletion time as the dependent variable, andthe goal shape and the language as the indepen-dent variables.
Only the main effect of the lan-guage was significant (F (1, 40) = 5.82, p <0.05).
Table 4 shows the average and the standarddeviation of the completion time.
Note that we seta time limit (15 minutes) for solving the puzzle.We considered the completion time as 15 minuteseven when a puzzle was not actually solved in thetime limit.
We also conducted a two-way ANOVAusing only the successful cases.
Both main effectsand their interaction were not significant.We then conducted an ANOVA with the num-ber of successfully solved puzzles by each pair asthe dependent variable and the language as the in-dependent variable.
The main effect was signifi-cant (F (1, 10) = 6.79, p < 0.05).
Table 5 showsthe average number of success goals per pair andthe success rate with their standard deviations inparentheses.Finally, we conducted an ANOVA with thenumber of pairs who succeeded in solving a goalTable 5: The number of solved trials and successratesLang.
solved trials success rate [%]Japanese 3.50 (0.55) 87.5 (13.7)English 1.67 (1.63) 41.7 (40.8)* Average (SD)shape as the dependent variable and the goal shapeas the independent variable.
The main effect wasnot significant.In summary, we found a difference in the taskperformance between the languages in terms ofthe task completion time and the success rate, butno difference among the goal shapes.
This dif-ference could be explained by the diversity of thesubjects rather than the difference of languages.The Japanese subject group consisted of univer-sity graduate students from the same department(Cognitive Science) and roughly of the same age(Average = 23.3, SD = 1.5).
In contrast, the En-glish subjects have diverse backgrounds (e.g.
highschool students, university faculty, writer, pro-grammer, etc.)
and age (Average = 30.8, SD =11.7).
In addition, a familiarity with this kind ofgeometric puzzle might have some effect.
How-ever, we collected a familiarity with the puzzleonly from the English subjects, we could not con-duct further analysis on this viewpoint.
Anyhow,in this respect, the independent variable shouldhave been named ?subject group?
instead of ?lan-guage?.4.2 Referring expressionsIt is important to note that since we have onlycompleted the annotation of four dialogs, all byone pair of subjects, our analyses of referring ex-pressions are tentative and pending further analy-sis.We have 200 and 1,243 referring expressions bythe operator and the solver respectively, 1,444 intotal in the 24 Japanese dialogues.
On the otherhand we have 26 (operator) and 315 (solver) re-ferring expressions in 4 English dialogues.
Theaverage number of referring expressions per di-alogue in Table 3 suggests that English subjectsuse more referring expressions than Japanese sub-jects.
Since we have only the data from a singlepair, we cannot say whether this tendency appliesto the other pairs.
We cannot draw a decisive con-clusion until we complete the annotation of theEnglish corpus.Table 6 shows the total frequencies of the at-tributes and their frequencies per dialogue.
Thetable gives us an impression of significantly fre-quent use of demonstrative pronouns (dpr) by the43Table 6: Comparison of attribute distributionEnglish Japanese(4 dialogues) (24 dialogues)attribute frq frq/dlg frq frq/dlgdpr 226 56.5 678 28.3dad 29 7.3 178 7.4siz 68 17.0 288 12.0typ 103 25.8 655 27.3dir 0 0 7 0.3prj 10 2.5 141 5.9tpl 4 1 9 0.4ovl 0 0 2 0.1act 5 1.3 103 4.3cmp 17 4.3 33 1.4sim 0 0 7 0.3num 22 5.5 35 1.5rpr 0 0 1 0err 0 0 1 0nest 1 0.3 31 1.3meta 1 0.3 6 0.3English subjects.
The Japanese subjects use moreattributes of projective spatial relations (prj) andactions on the referent (act).6 The English subjectsuse more complement attributes (cmp) as well asmore number attributes (num).5 Related workOver the last decade, with a growing recogni-tion that referring expressions frequently appearin collaborative task dialogues (Clark and Wilkes-Gibbs, 1986; Heeman and Hirst, 1995), a num-ber of corpora have been constructed to study thenature of their use.
This tendency also reflectsthe recognition that this area yields both challeng-ing research topics as well as promising applica-tions such as human-robot interaction (Foster etal., 2008; Kruijff et al, 2010).The COCONUT corpus (Di Eugenio et al,2000) was collected from keyboard-dialogs be-tween two participants, who worked together ona simple 2-D design task, buying and arrangingfurniture for two rooms.
The COCONUT cor-pus is limited in annotations which describe sym-bolic object information such as object intrinsicattributes and location in discrete co-ordinates.
Asan initial work of constructing a corpus for collab-orative tasks, the COCONUT corpus can be char-acterised as having a rather simple domain as well6We called such expressions as action-mentioning expres-sions (AME) in our previous work.as limited annotation.The QUAKE corpus (Byron, 2005) and its suc-cessor, the SCARE corpus (Stoia et al, 2008) dealwith a more complex domain, where two partici-pants collaboratively play a treasure hunting gamein a 3-D virtual world.
Despite the complexityof the domain, the participants were only allowedlimited actions, e.g.
moving step forward, pushinga button etc.As a part of the JAST project, the Joint Con-struction Task (JCT) corpus was created based ondialogues in which two participants constructed apuzzle (Foster et al, 2008).
The setting of theexperiment is quite similar to ours except thatboth participants have even roles.
Since our mainconcern is referring expressions, we believe ourasymmetric setting elicits more referring expres-sions than the symmetric setting of the JCT cor-pus.In contrast to these previous corpora, our cor-pora record a wide range of information usefulfor analysis of human reference behaviour in situ-ated dialogue.
While the domain of our corpora issimple compared to the QUAKE and SCARE cor-pora, we allowed a comparatively large flexibil-ity in the actions necessary for achieving the goalshape (i.e.
flipping, turning and moving of puzzlepieces at different degrees), relative to the com-plexity of the domain.
Providing this relativelylarger freedom of actions to the participants to-gether with the recording of detailed informationallows for research into new aspects of referringexpressions.As for a multilingual aspect, all the above cor-pora are English.
There have been several recentattempts at collecting multilingual corpora in situ-ated domains.
For instance, (Gargett et al, 2010)collected German and English corpora in the samesetting.
Their domain is similar to the QUAKEcorpus.
Van der Sluis et al (2009) aim at a com-parative study of referring expressions betweenEnglish and Japanese.
Their domain is still staticat the moment.
Our corpora aim at dealing withthe dynamic nature of situated dialogues betweenvery different languages, English and Japanese.44Table 7: The REX-J corpus familyname puzzle #pairs #dialg.
#valid statusT2008-08 Tangram 6 24 24 completedT2009-03 Tangram 10 40 16 completedT2009-11 Tangram 10 36 27 validatingN2009-11 Tangram 5 20 8 validatingP2009-11 Polyomino 7 28 24 annotatingD2009-11 2-Tangram 7 42 24 annotating6 Conclusion and future workThis paper presented an overview of our English-Japanese bilingual multimodal corpora of refer-ring expressions in a collaborative problem solv-ing setting.
The Japanese corpus was completedand has already been used for research (Spanger etal., 2009b; Spanger et al, 2010; Iida et al, 2010),but the English counterpart is still undergoing an-notation.
We have also presented a preliminarycomparative analysis of these corpora in terms ofthe task performance and usage of referring ex-pressions.
We found a significant difference of thetask performance, which could be attributed to thedifference in diversity of subjects.
We have tenta-tive results on the usage of referring expressions,since only four English dialogues are available atthe moment.The data collection experiments were con-ducted in August 2008 for Japanese and in March2010 for English.
Between these periods, weconducted various data collections to build differ-ent types of Japanese corpora (March, 2009 andNovember 2009).
These experiments involve cap-turing eye-gaze information of participants duringproblem solving, and introducing variants of puz-zles (Polyomino, Double Tangram and Tangramwithout any hints7).
They are also under prepa-ration for publication.
Table 7 gives an overviewof the REX-J corpus family, where ?#valid?
de-notes the number of dialogues with valid eye-gaze data.
Eye-gaze data is difficult to capturecleanly throughout a dialogue.
We discarded di-alogues in which eye-gaze was captured success-fully less than 70% of the total time of the dia-logue.
Namely, we annotated or will annotate di-alogues with validated eye-gaze data only.These corpora enable research on utilising eye-gaze information in reference resolution and gen-7N2009-11 in Table 7eration, and evaluation in different tasks (puzzles)as well.
We are planning to distribute the REX-Jcorpus family through GSK (Language ResourcesAssociation in Japan)8, and the REX-E corpusfrom both University of Brighton and GSK.ReferencesBaran, Bahar, Berrin Dogusoy, and Kursat Cagiltay.2007.
How do adults solve digital tangram prob-lems?
Analyzing cognitive strategies through eyetracking approach.
InHCI International 2007 - 12thInternational Conference - Part III, pages 555?563.Bard, Ellen Gurman, Robin Hill, Manabu Arai, andMary Ellen Foster.
2009.
Accessibility and atten-tion in situated dialogue: Roles and regulations.
InProceedings of the Workshop on Production of Re-ferring Expressions Pre-CogSci 2009.Belz, Anja, Eric Kow, Jette Viethen, and Albert Gatt.2010.
Referring expression generation in context:The GREC shared task evaluation challenges.
InKrahmer, Emiel and Marie?t Theune, editors, Empir-ical Methods in Natural Language Generation, vol-ume 5980 of Lecture Notes in Computer Science.Springer-Verlag, Berlin/Heidelberg.Byron, Donna K. 2005.
The OSU Quake 2004 cor-pus of two-party situated problem-solving dialogs.Technical report, Department of Computer Scienceand Enginerring, The Ohio State University.Clark, H. Herbert.
and Deanna Wilkes-Gibbs.
1986.Referring as a collaborative process.
Cognition,22:1?39.Di Eugenio, Barbara, Pamela W. Jordan, Richmond H.Thomason, and Johanna.
D. Moore.
2000.
Theagreement process: An empirical investigation ofhuman-human computer-mediated collaborative di-alogues.
International Journal of Human-ComputerStudies, 53(6):1017?1076.Foster, Mary Ellen and Jon Oberlander.
2007.
Corpus-based generation of head and eyebrow motion foran embodied conversational agent.
Language Re-sources and Evaluation, 41(3?4):305?323, Decem-ber.Foster, Mary Ellen, Ellen Gurman Bard, Markus Guhe,Robin L. Hill, Jon Oberlander, and Alois Knoll.2008.
The roles of haptic-ostensive referring ex-pressions in cooperative, task-based human-robotdialogue.
In Proceedings of 3rd Human-Robot In-teraction, pages 295?302.8http://www.gsk.or.jp/index e.html45Gargett, Andrew, Konstantina Garoufi, AlexanderKoller, and Kristina Striegnitz.
2010.
The give-2 corpus of giving instructions in virtual environ-ments.
In Proceedings of the Seventh conference onInternational Language Resources and Evaluation(LREC 2010), pages 2401?2406.Heeman, Peter A. and Graeme Hirst.
1995.
Collabo-rating on referring expressions.
Computational Lin-guistics, 21:351?382.Hobbs, Jerry R. 1978.
Resolving pronoun references.Lingua, 44:311?338.Iida, Ryu, Shumpei Kobayashi, and Takenobu Toku-naga.
2010.
Incorporating extra-linguistic informa-tion into reference resolution in collaborative taskdialogue.
In Proceedings of 48th Annual Meetingof the Association for Computational Linguistics,pages 1259?1267.Kiyokawa, Sachiko and Midori Nakazawa.
2006.
Ef-fects of reflective verbalization on insight problemsolving.
In Proceedings of 5th International Con-ference of the Cognitive Science, pages 137?139.Kruijff, Geert-Jan M., Pierre Lison, Trevor Ben-jamin, Henrik Jacobsson, Hendrik Zender, andIvana Kruijff-Korbayova.
2010.
Situated dialogueprocessing for human-robot interaction.
In Cogni-tive Systems: Final report of the CoSy project, pages311?364.
Springer-Verlag.Martin, Jean-Claude, Patrizia Paggio, Peter Kuehnlein,Rainer Stiefelhagen, and Fabio Pianesi.
2007.
Spe-cial issue on Mulitmodal corpora for modeling hu-man multimodal behaviour.
Language Resourcesand Evaluation, 41(3-4).Mitkov, Ruslan.
2002.
Anaphora Resolution.
Long-man.Nakatani, Christine and Julia Hirschberg.
1993.
Aspeech-first model for repair identification and cor-rection.
In Proceedings of 31th Annual Meeting ofACL, pages 200?207.Noguchi, Masaki, Kenta Miyoshi, Takenobu Toku-naga, Ryu Iida, Mamoru Komachi, and KentaroInui.
2008.
Multiple purpose annotation usingSLAT ?
Segment and link-based annotation tool.In Proceedings of 2nd Linguistic Annotation Work-shop, pages 61?64.Piwek, Paul L. A.
2007.
Modality choise for gen-eration of referring acts.
In Proceedings of theWorkshop on Multimodal Output Generation (MOG2007), pages 129?139.Spanger, Philipp, Masaaki Yasuhara, Ryu Iida, andTakenobu Tokunaga.
2009a.
A Japanese corpusof referring expressions used in a situated collab-oration task.
In Proceedings of the 12th EuropeanWorkshop on Natural Language Generation (ENLG2009), pages 110 ?
113.Spanger, Philipp, Masaaki Yasuhara, Ryu Iida, andTakenobu Tokunaga.
2009b.
Using extra linguisticinformation for generating demonstrative pronounsin a situated collaboration task.
In Proceedings ofPreCogSci 2009: Production of Referring Expres-sions: Bridging the gap between computational andempirical approaches to reference.Spanger, Philipp, Ryu Iida, Takenobu Tokunaga,Asuka Teri, and Naoko Kuriyama.
2010.
Towardsan extrinsic evaluation of referring expressions insituated dialogs.
In Kelleher, John, Brian MacNamee, and Ielka van der Sluis, editors, Proceed-ings of the Sixth International Natural LanguageGeneration Conference (INGL 2010), pages 135?144.Sternberg, Robert J. and Janet E. Davidson, editors.1996.
The Nature of Insight.
The MIT Press.Stoia, Laura, Darla Magdalene Shockley, Donna K.Byron, and Eric Fosler-Lussier.
2008.
SCARE:A situated corpus with annotated referring expres-sions.
In Proceedings of the Sixth InternationalConference on Language Resources and Evaluation(LREC 2008), pages 28?30.Suzuki, Hiroaki, Keiga Abe, Kazuo Hiraki, andMichiko Miyazaki.
2001.
Cue-readiness in in-sight problem-solving.
In Proceedings of the 23rdAnnual Meeting of the Cognitive Science Society,pages 1012 ?
1017.van Deemter, Kees, Ielka van der Sluis, and AlbertGatt.
2006.
Building a semantically transparentcorpus for the generation of referring expressions.In Proceedings of the Fourth International NaturalLanguage Generation Conference, pages 130?132.van der Sluis, Ielka, Junko Nagai, and Saturnino Luz.2009.
Producing referring expressions in dialogue:Insights from a translation exercise.
In Proceedingsof PreCogSci 2009: Production of Referring Ex-pressions: Bridging the gap between computationaland empirical approaches to reference.46
