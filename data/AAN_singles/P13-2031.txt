Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 171?176,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsCo-regularizing character-based and word-based models forsemi-supervised Chinese word segmentationXiaodong Zeng?
Derek F. Wong?
Lidia S. Chao?
Isabel Trancoso?
?Department of Computer and Information Science, University of Macau?INESC-ID / Instituto Superior Te?cnico, Lisboa, Portugalnlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo,isabel.trancoso@inesc-id.ptAbstractThis paper presents a semi-supervisedChinese word segmentation (CWS) ap-proach that co-regularizes character-basedand word-based models.
Similarly tomulti-view learning, the ?segmentationagreements?
between the two differen-t types of view are used to overcome thescarcity of the label information on unla-beled data.
The proposed approach train-s a character-based and word-based mod-el on labeled data, respectively, as the ini-tial models.
Then, the two models are con-stantly updated using unlabeled examples,where the learning objective is maximiz-ing their segmentation agreements.
The a-greements are regarded as a set of valuableconstraints for regularizing the learning ofboth models on unlabeled data.
The seg-mentation for an input sentence is decod-ed by using a joint scoring function com-bining the two induced models.
The e-valuation on the Chinese tree bank revealsthat our model results in better gains overthe state-of-the-art semi-supervised mod-els reported in the literature.1 IntroductionChinese word segmentation (CWS) is a criticaland a necessary initial procedure with respect tothe majority of high-level Chinese language pro-cessing tasks such as syntax parsing, informa-tion extraction and machine translation, since Chi-nese scripts are written in continuous characterswithout explicit word boundaries.
Although su-pervised CWS models (Xue, 2003; Zhao et al,2006; Zhang and Clark, 2007; Sun, 2011) pro-posed in the past years showed some reasonablyaccurate results, the outstanding problem is thatthey rely heavily on a large amount of labeled da-ta.
However, the production of segmented Chi-nese texts is time-consuming and expensive, sincehand-labeling individual words and word bound-aries is very hard (Jiao et al, 2006).
So, one can-not rely only on the manually segmented data tobuild an everlasting model.
This naturally pro-vides motivation for using easily accessible rawtexts to enhance supervised CWS models, in semi-supervised approaches.
In the past years, however,few semi-supervised CWS models have been pro-posed.
Xu et al (2008) described a Bayesian semi-supervised model by considering the segmentationas the hidden variable in machine translation.
Sunand Xu (2011) enhanced the segmentation result-s by interpolating the statistics-based features de-rived from unlabeled data to a CRFs model.
An-other similar trial via ?feature engineering?
wasconducted by Wang et al (2011).The crux of solving semi-supervised learningproblem is the learning on unlabeled data.
In-spired by multi-view learning that exploits redun-dant views of the same input data (Ganchev etal., 2008), this paper proposes a semi-supervisedCWS model of co-regularizing from two dif-ferent views (intrinsically two different models),character-based and word-based, on unlabeled da-ta.
The motivation comes from that the two typesof model exhibit different strengths and they aremutually complementary (Sun, 2010; Wang et al,2010).
The proposed approach begins by train-ing a character-based and word-based model onlabeled data respectively, and then both modelsare regularized from each view by their segmen-tation agreements, i.e., the identical outputs, ofunlabeled data.
This paper introduces segmenta-tion agreements as gainful knowledge for guidingthe learning on the texts without label information.Moreover, in order to better combine the strengthsof the two models, the proposed approach uses ajoint scoring function in a log-linear combinationform for the decoding in the segmentation phase.1712 Segmentation ModelsThere are two classes of CWS models: character-based and word-based.
This section briefly re-views two supervised models in these categories,a character-based CRFs model, and a word-basedPerceptrons model, which are used in our ap-proach.2.1 Character-based CRFs ModelCharacter-based models treat word segmentationas a sequence labeling problem, assigning label-s to the characters in a sentence indicating theirpositions in a word.
A 4 tag-set is used in thispaper: B (beginning), M (middle), E (end) andS (single character).
Xue (2003) first proposedthe use of CRFs model (Lafferty et al, 2001) incharacter-based CWS.
Let x = (x1x2...x|x|) ?
Xdenote a sentence, where each character and y =(y1y2...y|y|) ?
Y denote a tag sequence, yi ?
Tbeing the tag assigned to xi.
The goal is to achievea label sequence with the best score in the form,p?c(y|x) =1Z(x; ?c)exp{f(x, y) ?
?c} (1)where Z(x; ?c) is a partition function that normal-izes the exponential form to be a probability distri-bution, and f(x, y) are arbitrary feature functions.The aim of CRFs is to estimate the weight param-eters ?c that maximizes the conditional likelihoodof the training data:?
?c = argmax?cl?i=1log p?c(yi|xi)?
??
?c?22 (2)where ??
?c?22 is a regularizer on parameters tolimit overfitting on rare features and avoid degen-eracy in the case of correlated features.
In thispaper, this objective function is optimized by s-tochastic gradient method.
For the decoding, theViterbi algorithm is employed.2.2 Word-based Perceptrons ModelWord-based models read a input sentence from leftto right and predict whether the current piece ofcontinuous characters is a word.
After one wordis identified, the method moves on and searchesfor a next possible word.
Zhang and Clark (2007)first proposed a word-based segmentation mod-el using a discriminative Perceptrons algorithm.Given a sentence x, let us denote a possible seg-mented sentence as w ?
w, and the function thatenumerates a set of segmentation candidates asGEN:w = GEN(x) for x.
The objective is tomaximize the following problem for all sentences:?
?w = argmaxw=GEN(x)|w|?i=1?
(x,wi) ?
?w (3)where it maps the segmented sentencew to a glob-al feature vector ?
and denotes ?w as its cor-responding weight parameters.
The parameter-s ?w can be estimated by using the Perceptron-s method (Collins, 2002) or other online learningalgorithms, e.g., Passive Aggressive (Crammer etal., 2006).
For the decoding, a beam search decod-ing method (Zhang and Clark, 2007) is used.2.3 Comparison Between Both ModelsCharacter-based and word-based models presentdifferent behaviors and each one has its ownstrengths and weakness.
Sun (2010) carried out athorough survey that includes theoretical and em-pirical comparisons from four aspects.
Here, twocritical properties of the two models supportingthe co-regularization in this study are highlight-ed.
Character-based models present better predic-tion ability for new words, since they lay moreemphasis on the internal structure of a word andthereby express more nonlinearity.
On the oth-er side, it is easier to define the word-level fea-tures in word-based models.
Hence, these modelshave a greater representational power and conse-quently better recognition performance for in-of-vocabulary (IV) words.3 Semi-supervised Learning viaCo-regularizing Both ModelsAs mentioned earlier, the primary challenge ofsemi-supervised CWS concentrates on the unla-beled data.
Obviously, the learning on unlabeleddata does not come for ?free?.
Very often, it isnecessary to discover certain gainful information,e.g., label constraints of unlabeled data, that is in-corporated to guide the learner toward a desiredsolution.
In our approach, we believe that the seg-mentation agreements (?
3.1) from two differen-t views, character-based and word-based models,can be such gainful information.
Since each of themodels has its own merits, their consensuses signi-fy high confidence segmentations.
This naturallyleads to a new learning objective that maximizessegmentation agreements between two models onunlabeled data.172This study proposes a co-regularized CWSmodel based on character-based and word-basedmodels, built on a small amount of segmented sen-tences (labeled data) and a large amount of rawsentences (unlabeled data).
The model inductionprocess is described in Algorithm 1: given labeleddataset Dl and unlabeled dataset Du, the first t-wo steps are training a CRFs (character-based) andPerceptrons (word-based) model on the labeleddata Dl , respectively.
Then, the parameters ofboth models are continually updated using unla-beled examples in a learning cycle.
At each iter-ation, the raw sentences in Du are segmented bycurrent character-based model ?c and word-basedmodel ?w.
Meanwhile, all the segmentation agree-ments A are collected (?
3.1).
Afterwards, theagreements A are used as a set of constraints tobias the learning of CRFs (?
3.2) and Perceptron(?
3.3) on the unlabeled data.
The convergencecriterion is the occurrence of a reduction of seg-mentation agreements or reaching the maximumnumber of learning iterations.
In the final segmen-tation phase, given a raw sentence, the decodingrequires both induced models (?
3.4) in measuringa segmentation score.Algorithm 1 Co-regularized CWS model inductionRequire: n labeled sentencesDl;m unlabeled sentencesDuEnsure: ?c and ?w1: ?0c ?
crf train(Dl)2: ?0w ?
perceptron train(Dl)3: for t = 1...Tmax do4: At ?
agree(Du, ?t?1c , ?t?1w )5: ?tc ?
crf train constraints(Du,At, ?t?1c )6: ?tw ?
perceptron train constraints(Du,At, ?t?1w )7: end for3.1 Agreements Between Two ModelsGiven a raw sentence, e.g., ??????????????
(I am watching the opening ceremonyof the Olympics in Beijing.
)?, the two segmenta-tions shown in Figure 1 are the predictions froma character-based and word-based model.
Thesegmentation agreements between the two mod-els correspond to the identical words.
In this ex-ample, the five words, i.e.
??
(I)?, ???
(Bei-jing)?, ??
(watch)?, ????
(opening ceremony)?and ??(.
)?, are the agreements.3.2 CRFs with ConstraintsFor the character-based model, this paper fol-lows (Ta?ckstro?m et al, 2013) to incorporate thesegmentation agreements into CRFs.
The mainidea is to constrain the size of the tag sequencelattice according to the agreements for achievingsimplified learning.
Figure 2 demonstrates an ex-ample of the constrained lattice, where the boldnode represents that a definitive tag derived fromthe agreements is assigned to the current charac-ter, e.g., ??
(I)?
has only one possible tag ?S?because both models segmented it to a word witha single character.
Here, if the lattice of all admis-sible tag sequences for the sentence x is denotedas Y(x), the constrained lattice can be defined byY?
(x, y?
), where y?
refers to tags inferred from theagreements.
Thus, the objective function on unla-beled data is modeled as:??
?c = argmax?cm?i=1log p?c(Y?
(xi, y?i)|xi)?
??
?c?22(4)It is a marginal conditional probability given bythe total probability of all tag sequences consistentwith the constrained lattice Y?
(x, y?).
This objec-tive can be optimized by using LBFGS-B (Zhu etal., 1997), a generic quasi-Newton gradient-basedoptimizer.Figure 1: The segmentations given by character-based and word-based model, where the words in?2?
refer to the segmentation agreements.Figure 2: The constrained lattice representationfor a given sentence, ??????????????
?.3.3 Perceptrons with ConstraintsFor the word-based model, this study incorporatessegmentation agreements by a modified parame-ter update criterion in Perceptrons online training,as shown in Algorithm 2.
Because there are no?gold segmentations?
for unlabeled sentences, theoutput sentence predicted by the current model iscompared with the agreements instead of the ?an-swers?
in the supervised case.
At each parameter173update iteration k, each raw sentence xu is decod-ed with the current model into a segmentation zu.If the words in output zu do not match the agree-ments A(xu) of the current sentence xu, the pa-rameters are updated by adding the global featurevector of the current training example with the a-greements and subtracting the global feature vec-tor of the decoder output, as described in lines 3and 4 of Algorithm 2.Algorithm 2 Parameter update in word-based model1: for k = 1...K, u = 1...m do2: calculate zu = argmaxw=GEN(x)?|w|i=1 ?
(xu, wi) ?
?k?1w3: if zu 6= A(xu)4: ?kw = ?k?1w + ?(A(xu))?
?
(zu)5: end for3.4 The Joint Score Function for DecodingThere are two co-regularized models as results ofthe previous induction steps.
An intuitive idea isthat both induced models are combined to conductthe segmentation, for the sake of integrating theirstrengths.
This paper employs a log-linear inter-polation combination (Bishop, 2006) to formulatea joint scoring function based on character-basedand word-based models in the decoding:Score(w) = ?
?
log(p?c(y|x))+(1?
?)
?
log(?
(x,w) ?
?w) (5)where the two terms of the logarithm are the s-cores of character-based and word-based model-s, respectively, for a given segmentation w. Thiscomposite function uses a parameter ?
to weightthe contributions of the two models.
The ?
valueis tuned using the development data.4 Experiment4.1 SettingThe experimental data is taken from the Chinesetree bank (CTB).
In order to make a fair compar-ison with the state-of-the-art results, the versionsof CTB-5, CTB-6, and CTB-7 are used for the e-valuation.
The training, development and testingsets are defined according to the previous works.For CTB-5, the data split from (Jiang et al, 2008)is employed.
For CTB-6, the same data split asrecommended in the CTB-6 official document isused.
For CTB-7, the datasets are formed accord-ing to the way in (Wang et al, 2011).
The cor-responding statistic information on these data s-plits is reported in Table 1.
The unlabeled data inour experiments is from the XIN CMN portion ofChinese Gigaword 2.0.
The articles published in1991-1993 and 1999-2004 are used as unlabeleddata, with 204 million words.The feature templates in (Zhao et al, 2006)and (Zhang and Clark, 2007) are used in train-ingthe CRFs model and Perceptrons model, respec-tively.
The experimental platform is implement-ed based on two popular toolkits: CRF++ (Kudo,2005) and Zpar (Zhang and Clark, 2011).Data #Sent-train#Sent-dev#Sent-testOOV-devOOV-testCTB-5 18,089 350 348 0.0811 0.0347CTB-6 23,420 2,079 2,796 0.0545 0.0557CTB-7 31,131 10,136 10,180 0.0549 0.0521Table 1: Statistics of CTB-5, CTB-6 and CTB-7data.4.2 Main ResultsThe development sets are mainly used to tune thevalues of the weight factor ?
in Equation 5.
Weevaluated the performance (F-score) of our modelon the three development sets by using differen-t ?
values, where ?
is progressively increased insteps of 0.1 (0 < ?
< 1.0).
The best performedsettings of ?
for CTB-5, CTB-6 and CTB-7 on de-velopment data are 0.7, 0.6 and 0.6, respectively.With the chosen parameters, the test data is usedto measure the final performance.Table 2 shows the F-score results of word seg-mentation on CTB-5, CTB-6 and CTB-7 testingsets.
The line of ?ours?
reports the performanceof our semi-supervised model with the tuned pa-rameters.
We first compare it with the supervised?baseline?
method which joints character-basedand word-based model trained only on the trainingset1.
It can be observed that our semi-supervisedmodel is able to benefit from unlabeled data andgreatly improves the results over the supervisedbaseline.
We also compare our model with twostate-of-the-art semi-supervised methods of Wang?11 (Wang et al, 2011) and Sun ?11 (Sun and X-u, 2011).
The performance scores of Wang ?11 aredirectly taken from their paper, while the results ofSun ?11 are obtained, using the program providedby the author, on the same experimental data.
The1The ?baseline?
uses a different training configuration sothat the ?
values in the decoding are also need to be tuned onthe development sets.
The tuned ?
values are {0.6, 0.6, 0.5}for CTB-5, CTB-6 and CTB-7.174bold scores indicate that our model does achievesignificant gains over these two semi-supervisedmodels.
This outcome can further reveal that us-ing the agreements from these two views to regu-larize the learning can effectively guide the mod-el toward a better solution.
The third compari-son candidate is Hatori ?12 (Hatori et al, 2012)which reported the best performance in the litera-ture on these three testing sets.
It is a supervisedjoint model of word segmentation, POS taggingand dependency parsing.
Impressively, our modelstill outperforms Hatori ?12 on all three datasets.Although there is only a 0.01 increase on CTB-5,it can be seen as a significant improvement whenconsidering Hatori ?12 employs much richer train-ing resources, i.e., sentences tagged with syntacticinformation.Method CTB-5 CTB-6 CTB-7Ours 98.27 96.33 96.72Baseline 97.58 94.71 94.87Wang ?11 98.11 95.79 95.65Sun ?11 98.04 95.44 95.34Hatori ?12 98.26 96.18 96.07Table 2: F-score (%) results of five CWS modelson CTB-5, CTB-6 and CTB-7.5 ConclusionThis paper proposed an alternative semi-supervised CWS model that co-regularizes acharacter- and word-based model by using theirsegmentation agreements on unlabeled data.
Weperform the agreements as valuable knowledgefor the regularization.
The experiment resultsreveal that this learning mechanism results in apositive effect to the segmentation performance.AcknowledgmentsThe authors are grateful to the Science and Tech-nology Development Fund of Macau and the Re-search Committee of the University of Macau forthe funding support for our research, under the ref-erence No.
017/2009/A and MYRG076(Y1-L2)-FST13-WF.
The authors also wish to thank theanonymous reviewers for many helpful comments.ReferencesChristopher M. Bishop.
2006.
Pattern recognition andmachine learning.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP, pages 1-8, Philadelphia, USA.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of ma-chinelearning research, 7:551-585.Kuzman Ganchev, Joao Graca, John Blitzer, and BenTaskar.
2008.
Multi-View Learning over Struc-tured and Non-Identical Outputs.
In Proceedings ofCUAI, pages 204-211, Helsinki, Finland.Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, andJun?ichi Tsujii.
2012.
Incremental Joint Approachto Word Segmentation, POS Tagging, and Depen-dency Parsing in Chinese.
In Proceedings of ACL,pages 1045-1053, Jeju, Republic of Korea.Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Liu.2008.
A Cascaded Linear Model for Joint ChineseWord Segmentation and Part-of-Speech Tagging.
InProceedings of ACL, pages 897-904, Columbus, O-hio.Wenbin Jiang, Liang Huang, and Qun Liu.
2009.
Au-tomatic Adaptation of Annotation Standards: Chi-nese Word Segmentation and POS Tagging - A CaseStudy.
In Proceedings of ACL and the 4th IJCNLPof the AFNLP, pages 522-530, Suntec, Singapore.Feng Jiao, Shaojun Wang and Chi-Hoon Lee.
2006.Semi-supervised conditional random fields for im-proved sequence segmentation and labeling.
In Pro-ceedings of ACL and the 4th IJCNLP of the AFNLP,pages 209-216, Strouds-burg, PA, USA.Taku Kudo.
2005.
CRF++: Yet another CRF toolkit.Software available at http://crfpp.sourceforge.
net.John Lafferty, Andrew McCallum, and Fernando Pe-reira.
2001.
Conditional Random Field: Prob-abilistic Models for Segmenting and Labeling Se-quence Data.
In Proceedings of ICML, pages 282-289, Williams College, USA.Weiwei Sun.
2001.
Word-based and character-basedword segmentation models: comparison and com-bination.
In Proceedings of COLING, pages 1211-1219, Bejing, China.Weiwei Sun.
2011.
A stacked sub-word model forjoint Chinese word segmentation and part-of-speechtagging.
In Proceedings of ACL, pages 1385-1394,Portland, Oregon.Weiwei Sun and Jia Xu.
2011.
Enhancing Chineseword segmentation using unlabeled data.
In Pro-ceedings of EMNLP, pages 970-979, Scotland, UK.Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan M-cDonald, and Joakim Nivre.
2013.
Token and TypeConstraints for Cross-Lingual Part-of-Speech Tag-ging.
In Transactions of the Association for Compu-tational Linguistics, 1:1-12.175Kun Wang, Chengqing Zong, and Keh-Yih Su.
2010.A Character-Based Joint Model for Chinese WordSegmentation.
In Proceedings of COLING, pages1173-1181, Bejing, China.Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.2011.
Improving Chinese word segmentation andPOS tagging with semi-supervised methods usinglarge auto-analyzed data.
In Proceedings of IJC-NLP, pages 309-317, Hyderabad, India.Jia Xu, Jianfeng Gao, Kristina Toutanova and Her-mann Ney.
2008.
Bayesian semi-supervised chineseword segmentation for statistical machine transla-tion.
In Proceedings of COLING, pages 1017-1024,Manchester, UK.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, 8(1):29-48.Yue Zhang and Stephen Clark.
2007.
Chinese segmen-tation using a word-based perceptron algorithm.
InProceedings of ACL, pages 840-847, Prague, CzechRepublic.Yue Zhang and Stephen Clark.
2010.
A fast decoderfor joint word segmentation and POS-tagging usinga single discriminative model.
In Proceedings ofEMNLP, pages 843-852, Massachusetts, USA.Yue Zhang and Stephen Clark.
2011.
Syntactic pro-cessing using the generalized perceptron and beamsearch.
Computational Linguistics, 37(1):105-151.Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-LiangLu.
2006.
Effective tag set selection in Chineseword segmentation via conditional random fieldmodeling.
In Proceedings of PACLIC, pages 87-94,Wuhan, China.Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and JorgeNocedal.
2006.
L-BFGS-B: Fortran subroutines forlarge scale bound constrained optimization.
ACMTransactions on Mathematical Software, 23:550-560.176
