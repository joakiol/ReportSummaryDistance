A Connectionist Model of Anticipation in Visual WorldsMarshall R. Mayberry, III, Matthew W. Crocker, and Pia KnoeferleDepartment of Computational Linguistics,Saarland University, Saarbru?cken, Germany{martym, crocker, knoferle}@coli.uni-sb.deAbstract.
Recent ?visual worlds?
studies, wherein researchers study language incontext by monitoring eye-movements in a visual scene during sentence process-ing, have revealed much about the interaction of diverse information sources andthe time course of their influence on comprehension.
In this study, five experi-ments that trade off scene context with a variety of linguistic factors are modelledwith a Simple Recurrent Network modified to integrate a scene representationwith the standard incremental input of a sentence.
The results show that the modelcaptures the qualitative behavior observed during the experiments, while retain-ing the ability to develop the correct interpretation in the absence of visual input.1 IntroductionThere are two prevalent theories of language acquisition.
One view emphasizes syntac-tic and semantic bootstrapping during language acquisition that enable children to learnabstract concepts from mappings between different kinds of information sources [1,2].Another view emerges from connectionist literature and emphasizes the learning of lin-guistic structure from purely distributional properties of language usage [3,4].
While theperspectives are often taken to be diametrically opposed, both can be seen as cruciallyrelying on correlations between words and their immediate context, be it the sentenceas a whole or extra-linguistic input, such as a scene.We combine insights from both distributional and bootstrapping accounts in mod-elling the on-line comprehension of utterances in both the absence and presence of a vi-sual scene.
This is an important achievement in at least two regards.
First, it emphasizesthe complementarity between distributional and bootstrapping approaches?discoveringstructure across linguistic and scene contexts [5].
Further, it is an important first step inlinking situated models of on-line utterance comprehension more tightly to accounts oflanguage acquisition, thus emphasizing the continuity of language processing.We present results from two simulations on a Simple Recurrent Network (SRN; [3]).Modification of the network to integrate input from a scene together with the charac-teristic incremental processing of such networks allowed us to model people?s abilityto adaptively use the contextual information in order to more rapidly interpret and dis-ambiguate a sentence.
The model draws on recent studies that appeal to theories of lan-guage acquisition to account for the comprehension of scene-related utterances [6,7].Recent research within the visual worlds paradigm, wherein participants?
gazes to ascene while listening to an utterance are monitored, provides support for this view.
Find-ings from this paradigm support an account of scene-related utterance comprehension inR.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
849?861, 2005.c?
Springer-Verlag Berlin Heidelberg 2005850 M.R.
Mayberry, III, M.W.
Crocker, and P. Knoeferlewhich the rapid coordinated interaction of information from the immediate scene, andlinguistic knowledge plays a major role in incremental and anticipatory comprehension.2 Simulation 1In Simulation 1, we simultaneously model four experiments that show the rapid influ-ence of diverse informational sources?linguistic and world knowledge as well as sceneinformation ?
on utterance comprehension.
All experiments were conducted in German,a language that allows both subject-verb-object (SVO) and object-verb-subject (OVS)sentence types.
In the face of word order ambiguity, case marking indicates the subjector object grammatical function, except in the case of feminine and neuter noun phraseswhere the article does not distinguish the nominative and accusative cases.2.1 Anticipation Depending on StereotypicalityThe first two experiments that we modeled examined how linguistic and world knowl-edge or stereotypicality enabled rapid thematic role assignment in unambiguous sen-tences, thus determining who-does-what-to-whom in a scene.Fig.
1.
Selectional RestrictionsExperiment 1: Morphosyntactic and lexical verb information.
To examine the influ-ence of case-marking and verb plausibility on thematic role assignment, [8] presentedparticipants with utterances such as (1) or (2) that described a scene showing a hare, acabbage, a fox, and a distractor (see Figure 1) :(1) Der Hase frisst gleich den Kohl.The harenom eats shortly the cabbageacc.
(2) Den Hasen frisst gleich der Fuchs.The hareacc eats shortly the foxnom.After hearing ?The harenom eats ...?
and ?The hareacc eats ...?, people made anticipatoryeye-movements to the cabbage and fox respectively.
This reveals that people were ableto predict role fillers in a scene through linguistic/world knowledge that identified who-does-what-to-whom.A Connectionist Model of Anticipation in Visual Worlds 851Experiment 2: Verb type information.
To further investigate the role of verb infor-mation, the authors replaced the agent/patient verbs like frisst (?eats?)
with experi-encer/theme verbs like interessiert (?interests?).
This manipulation interchanged agent(experiencer) and patient (theme) roles from Experiment 1.
For Figure 1 and the subject-first (3) or object-first sentence (4), participants showed gaze fixations complementaryto those of Experiment 1, confirming that both case and semantic verb information areused to predict relevant role fillers.
(3) Der Hase interessiert ganz besonders den Fuchs.The harenom interests especially the foxacc.
(4) Den Hasen interessiert ganz besonders der Kohl.The hareacc interests especially the cabbagenom.2.2 Anticipation Depending on Depicted EventsThe second set of experiments investigated whether depicted events showing who-does-what-to-whom can establish a scene character?s role as agent or patient when syntacticand thematic role relations are temporarily ambiguous in the utterance.Fig.
2.
Depicted EventsExperiment 3: Verb-mediated depicted role relations.
[9] presented such initiallyambiguous spoken SVO (5) and OVS sentences (6) together with a scene in which aprincess both paints a fencer and is washed by a pirate (Figure 2):(5) Die Princessin malt offensichtlich den Fechter.The princessnom paints obviously the fenceracc.
(6) Die Princessin wa?scht offensichtlich der Pirat.The princessacc washes obviously the piratenom.Linguistic disambiguation occurred on the second NP; disambiguation prior to the sec-ond NP was only possible through use of the depicted events.
When the verb identifiedan action, the depicted role relations disambiguated towards either an agent-patient (5)or patient-agent (6) role relation, as indicated by anticipatory eye-movements to the pa-tient (pirate) or agent (fencer) respectively for (5) and (6).
This gaze-pattern showed the852 M.R.
Mayberry, III, M.W.
Crocker, and P. Knoeferlerapid influence of verb-mediated depicted events on the assignment of thematic roles toa temporarily ambiguous sentence-initial noun phrase.Experiment 4: Weak temporal adverb constraint.
[9] also investigated German verb-final active (7) and passive (8) constructions.
In this type of sentence, the initial subjectnoun phrase is role-ambiguous, and the auxiliary wird can have a passive or futureinterpretation.
(7) Die Princessin wird sogleich den Pirat washen.The princessnom will right away wash the pirateacc.
(8) Die Princessin wird soeben von dem Fechter gemalt.The princessacc is just now painted by the fencernom.To evoke early linguistic disambiguation, temporal adverbs biased the auxiliary wird to-ward either the future (?will?)
or passive (?is -ed?)
reading.
Since the verb was sentence-final, the interplay of scene and linguistic cues (e.g., temporal adverbs) were rather moresubtle.
When the listener heard a future-biased adverb such as sogleich, after the aux-iliary wird, he interpreted the initial NP as agent of a future active construction, as evi-denced by anticipatory eye-movements to the patient in the scene.
Conversely, listenersinterpreted the passive-biased construction soeben with these roles exchanged.2.3 ArchitectureThe Simple Recurrent Network is a type of neural network typically used to processtemporal sequences of patterns such as words in a sentence.
A common approach isfor the modeller to train the network on prespecified targets, such as verbs and theirarguments, that represent what the network is expected to produce upon completing asentence.
Processing is incremental, with each new input word interpreted in the con-text of the sentence processed so far, represented by a copy of the previous hiddenlayer serving as additional input or context to the current hidden layer.
Because thesetypes of associationist models automatically develop correlations among the data theyare trained on, they will generally develop expectations about the output even beforeprocessing is completed because sufficient information occurs early in the sentence towarrant such predictions.
Moreover, during the course of processing a sentence theseexpectations can be overridden with subsequent input, often abruptly revising an inter-pretation in a manner reminiscent of how humans seem to process language.
Indeed,it is these characteristics of incremental processing, the automatic development of ex-pectations, seamless integration of multiple sources of information, and nonmonotonicrevision that have endeared neural network models to cognitive researchers.In Simulation 1, the four experiments described above have been modelled simul-taneously using a single network.
The goal of modelling all experimental results by asingle architecture required enhancements to the SRN, the development and presenta-tion of the training data, as well as the training regime itself.
We describe these next.In two of the experiments, only three characters are depicted, the representation ofwhich can be propagated directly to the network?s hidden layer.
In the other two exper-iments, the scene featured three characters involved in two events (e.g., pirate-washes-princess and princess-paints-fencer, as shown in Figure 3).
The middle character wasA Connectionist Model of Anticipation in Visual Worlds 853hidden layercontext layerevent layerswaescht Prinzessin Pirat PATinput layerwaeschtFig.
3.
Scene Integrationinvolved in both events, either as an agent or a patient (e.g., princess).
Only one of theevents, however, corresponded to the spoken linguistic input.The representation of this scene information and its integration into the model?sprocessing was the primary modification to the SRN.
Connections between representa-tions for the depicted characters and the hidden layer were provided.
Encoding of thedepicted events, when present, required additional links from the characters and de-picted actions to event layers, and links from these event layers to the SRN?s hiddenlayer.
Representations for the events were developed in the event layers by compress-ing the scene representations of the involved characters and depicted actions throughweights corresponding to the action, its agent and its patient for each event.
This eventrepresentation was kept simple and only provided conceptual input to the hidden layer:who did what to whom was encoded for both events, when depicted; richer grammaticalinformation (e.g., case and gender on articles) only came from the linguistic input.Neural networks will usually encode any correlations in the data that help to min-imize error.
In order to prevent the network from encoding regularities in its weightsregarding the position of the characters and events given in the scene (such as, for ex-ample, that the central character in the scene corresponds to the first NP in the presentedsentence) which are not relevant to the role-assignment task, one set of weights was usedfor all characters, and another set of weights used for both events.
This weight-sharingensured that the network had to access the information encoded in the event layers, ordetermine the relevant characters itself, thus improving generalization.
The representa-tions for the characters and actions were the same for both input (scene and sentence)and output.854 M.R.
Mayberry, III, M.W.
Crocker, and P. KnoeferleThe input assemblies were the scene representations and the current word from theinput sentence.
The output assemblies were the verb, the first and second nouns, and anassembly that indicated whether the first noun was the agent or patient of the sentence(token PAT in Figure 3).
Typically, agent and patient assemblies would be fixed in acase-role representation without such a discriminator, and the model required to learnto instantiate them correctly [10].
However, we found that the model performed muchbetter when the task was recast as having to learn to isolate the nouns in the order inwhich they are introduced, and separately mark how those nouns relate to the verb.
Theinput and output assemblies had 100 units each, the event layers contained 200 unitseach, and the hidden and context layers consisted of 400 units.2.4 Input Data, Training, and ExperimentsWe trained the network to correctly handle sentences involving non-stereotypical eventsas well as stereotypical ones, both when visual context was present and when it was ab-sent.
As over half a billion sentence/scene combinations were possible for all of theexperiments, we adopted a grammar-based approach to randomly generate sentencesand scenes based on the materials from each experiment while holding out the actualmaterials to be used for testing.
Because of the complementary roles that stereotypical-ity played in the two sets of experiments, there was virtually no lexical overlap betweenthem.
In order to accurately model the first two experiments involving selectional re-strictions on verbs, two additional words were added to the lexicon for each charac-ter selected by a verb.
For example, in the sentence Der Hase frisst gleich den Kohl,the nouns Hase1, Hase2, Kohl1, and Kohl2 were used to develop training sentences.These were meant to represent, for example, words such as ?rabbit?
and ?jackrabbit?
or?carrot?
and ?lettuce?
in the lexicon that have the same distributional properties as theorignal words ?hare?
and ?cabbage?.
With these extra tokens the network could learnthat Hase, frisst, and Kohl were correlated without ever encountering all three wordsin the same training sentence.
The experiments involving non-stereotypicality did notpose this constraint, so training sentences were simply generated to avoid presentingexperimental items.Some standard simplifications to the words have been made to facilitate modelling.For example, multi-word adverbs such as fast immer were treated as one word throughhyphenation so that sentence length within a given experimental set up is maintained.Nominal case markings such as -n in Hasen were removed to avoid sparse data as thesemarkings are idiosyncratic, and the case markings on the determiners are more infor-mative overall.
More importantly, morphemes such as the infinitive marker -en andpast participle ge- were removed, because, for example, the verb forms malt, malen,and gemalt, would all be treated as unrelated tokens, again contributing unnecessarilyto the problem with sparse data.
The result is that one verb form is used, and to per-form accurately, the network must rely on its position in the sentence (either second orsentence-final), as well as whether the word von occurs to indicate a participial readingrather than infinitival.
All 326 words in the lexicon for the first four experiments weregiven random representations.We trained the network by repeatedly presenting the model with 1000 randomlygenerated sentences from each experiment (constituting one epoch) and testing everyA Connectionist Model of Anticipation in Visual Worlds 855100 epochs against the held-out test materials for each of the five experiments.
Sceneswere provided half of the time to provide an unbiased approximation to linguistic expe-rience.
The network was initialized with weights between -0.01 and 0.01.
The learningrate was initially set to 0.05 and gradually reduced to 0.002 over the course of 15000epochs.
Four splits took a little less than two weeks to complete on 1.6Ghz PCs.2.5 ResultsFigure 4 reports the percentage of targets at the network?s output layer that the modelcorrectly matches, both as measured at the adverb and at the end of the sentence.
Themodel clearly demonstrates the qualitative behavior observed in all four experiments inthat it is able to access the information either from the encoded scene or stereotypicalityand combine it with the incrementally presented sentence to anticipate forthcomingarguments.859095100Exp 1 Exp 2 Exp 3 Exp 4P erce ntag eCo rrec tAdverbNP2Fig.
4.
ResultsFor the two studies using stereotypical information (experiments 1 and 2), the net-work achieved just over 96% at sentence end, and anticipation accuracy was just over95% at the adverb.
Because these sentences are unambiguous, the model is able tocorrectly identify the role of the upcoming argument, but makes errors in token iden-tification, confusing words that are within the selectionally restricted set, such as, forexample, Kohl and Kohl2.
Thus, the model has not quite mastered the stereotypicalknowledge, particularly as it relates to the presence of the scene.In the other two experiments using non-stereotypical characters and depicted events(experiments 3 and 4), accuracy was 100% at the end of the sentence.
More impor-tantly, the model achieved over 98% early disambiguation on experiment 3, where thesentences were simple, active SVO and OVS.
Early disambiguation on experiment 4was somewhat harder because the adverb is the disambiguating point in the sentenceas opposed to the verb in the other three experiments.
As nonlinear dynamical sys-tems, neural networks sometimes require an extra step to settle after a decision point isreached due to the attractor dynamics of the weights.
For both experiments, most errorsoccurred on role-assignment due to the initially-ambiguous first noun phrase.856 M.R.
Mayberry, III, M.W.
Crocker, and P. KnoeferleThe difference in performance between the first two experiments and second twoexperiments can be attributed to the event layer that was only available in experiments3 and 4.
Closer inspection of the model?s behavior during processing revealed that finerdiscrimination was encoded in the links between the event layers and hidden layer thanthat encoded in the weights between the characters and the hidden layer.3 Simulation 2The previous set of experiments demonstrated the rapid use of either linguistic knowl-edge or depicted events to anticipate forthcoming arguments in a sentence.
A furtherimportant question is the relative importance of these two informational sources whenthey conflict.
We first review an experimental study by [6] designed to address this issueand then report relevant modelling results.Fig.
5.
Scene vs Stored KnowledgeScene vs Stored Knowledge.
One goal of the study by [6] was to verify that storedknowledge about non-depicted events and information from depicted, but non-stereo-typical, events each enable rapid thematic interpretation.
Case-marking on the first NPalways identified the pilot as a patient.
After hearing the verb in (9) more inspections tothe only food-serving agent (detective) than to the other agent showed the influence ofdepicted events.
In contrast, when people heard the verb in condition two (10), a higherproportion of anticipatory eye-movements to the only stereotypical agent (wizard) thanto the other agent revealed the influence of stereotypical knowledge (see Figure 5).
(9) Den Piloten verko?stigt gleich der Detektiv.The pilotacc serves-food-to shortly the detectivenom.
(10) Den Piloten verzaubert gleich der Zauberer.The pilotacc jinxes shortly the wizardnom.Second, the study determined the relative importance of depicted events and verb-basedthematic role knowledge when these information sources competed.
In conditions threeand four ((11) & (12)) participants heard an utterance in which the verb identified both aA Connectionist Model of Anticipation in Visual Worlds 857stereotypical (detective) and a depicted agent (wizard).
In this case, people preferred torely on the immediate event depictions over stereotypical knowledge, and looked moreoften at the wizard, the agent of the depicted event, than at the other, stereotypical agentof the spying-action (the detective).
(11) Den Piloten bespitzelt gleich der Zauberer.The pilotacc spies-on shortly the wizardnom.
(12) Den Piloten bespitzelt gleich der Detektiv.The pilotacc spies-on shortly the detectivenom.3.1 Architecture, Data, Training, and ResultsIn simulation 1, we modelled experiments that depended on stereotypicality or depictedevents, but not both.
The experiment modelled in simulation 2, however, was specif-ically designed to investigate how these two information sources interacted.
Accord-ingly, the network needed to learn to use either information from the scene or stereotyp-icality when available, and, moreover, favor the scene when the two sources conflicted,as observed in the empirical results.
Recall that the network is trained only on the finalinterpretation of a sentence.
Thus, capturing the observed behavior required manipula-tion of the frequencies of the four conditions described above during training.
In orderto train the network to develop stereotypical agents for verbs, the frequency that a verboccurs with its stereotypical agent, such as Detektiv and bespitzelt from example (12)above, had to be greater than for a non-stereotypical agent.
However, the frequencyshould not be so great as to override the influence from the scene.The solution we adopted is motivated by theories of language acquisition that takeinto account the importance of early linguistic experience in a visual environment (seethe General Discussion).
We found a small range of frequencies that permitted the net-work to develop an early reliance on the information from the scene while it graduallylearned the stereotypical associations.
Figure 6 shows the effect this training regime hadover 6000 epochs on the ability of the network to accurately anticipate the missing argu-.0.70.750.80.850.90.9510  1000  2000  3000  4000  5000  6000Cond 1Cond 2Cond 3Cond 4EpochsP erce ntag eFig.
6.
Acquisition of Stereotypicality858 M.R.
Mayberry, III, M.W.
Crocker, and P. Knoeferlement in each of the four conditions described above when the ratio of non-stereotypicalto stereotypical sentences was 8:1.
The network quickly learns to use the scene for con-ditions 2-4 (examples 10-12), where the action in the linguistic input stream is alsodepicted, allowing the network to determine the relevant event and deduce the missingargument.
(Because the graph shows the accuracy of the network at anticipating theupcoming argument at the adverb, the lines for conditions 3 and 4 are, in fact, identi-cal.)
But condition 1 (sentence 9) requires only stereotypical knowledge.
The accuracyof condition 1 remains close to 75% (correctly producing the verb, first NP, and rolediscriminator, but not the second NP) until around epoch 1800 or so and then graduallyimproves as the network learns the appropriate stereotypical associations.Results from several separate runs with different training parameters (such as learn-ing rate and stereotypicality ratio) show that the network does indeed model the ob-served experimental behavior.
The best results thus far exceed 99% accuracy in cor-rectly anticipating the proper roles and 100% accuracy at the end of sentence.As in simulation 1, the training corpus was generated by exhaustively combiningparticipants and actions for all experimental conditions while holding out all test sen-tences.
However, we found that we were able to use a larger learning rate, 0.1, than 0.05as in the first simulation.Analysis of the network after successful training suggests why this training pol-icy works.
Early in training, before stereotypicality has been encoded in the network?sweights, patterns are developed in the hidden layer once the verb is read in from the in-put stream that enable the network to accurately decode that verb in the output layer.
Notsurprisingly, the network uses these same patterns to encode the stereotypical agent; theonly constraint for the network is to ensure that the scene can still override this stereo-typicality when the depicted event so dictates.4 General Discussion and Future WorkThe model demonstrates that reliance on correlations from distributional informationin the linguistic input and the scene during training of the model enabled successfulmodelling of on-line utterance comprehension both in the presence and absence of richvisual contexts.
The model that we present acquires stereotypical knowledge from dis-tributional properties of language during training.
The mapping from words to the scenerepresentations is established through cooccurrence of scene-related utterances and de-picted events during training.
The network that emerges from this training regime suc-cessfully models five visual worlds eye-tracking experiments in two simulations.
A firstsimulation of four experiments models the influence of either thematic and syntacticknowledge in the utterance [8], or of depicted events showing who-does-what-to-whomon incremental thematic role assignment [9].
Crucially in modelling the fifth experi-ment we are able to account for the greater relative priority of depicted events whenevent depictions and event knowledge conflict with each other.The simple accuracy results belie the complexity of the task in both simulations.
Forexperiments 3 and 4, the network has to demonstrate anticipation of upcoming roleswhen the scene is present, showing that it can indeed access the proper role and fillerfrom the compressed representation of the event associated with the verb processed inA Connectionist Model of Anticipation in Visual Worlds 859the linguistic stream when available.
This task is rendered more difficult because theappropriate event must be extracted from the superimposition of the two events in thescene, which is what is propagated into the model?s hidden layer.
In addition, it mustalso still be able to process all sentences correctly when the scene is not present.Simulation 2 is more challenging still.
The experiment shows that information fromthe scene takes precedence when there is a conflict with stereotypical knowledge; oth-erwise, each source of knowledge is used when it is available.
In the training regimeused in this simulation, the dominance of the scene is established early because it ismuch more frequent than the more particular stereotypical knowledge.
As training pro-gresses, stereotypical knowledge is gradually learned because it is sufficiently frequentfor the network to capture the relevant associations.
As the network weights graduallysaturate, it becomes more difficult to retune them.
But encoding stereotypical knowl-edge requires far fewer weight adjustments, so the network is able to learn that tasklater during training.According to the ?Coordinated Interplay?
account in [7,6,11], the rapid integrationof scene and utterance information and the observed preferred reliance of the compre-hension system on the visual context over stored knowledge might best be explainedby appealing to bootstrapping accounts of language acquisition.
The development ofa child?s world knowledge occurs in a visual environment, which accordingly plays aprominent role during language acquisition.
The fact that the child can draw on two in-formational sources (utterance and scene) enables it to infer information that it has notyet acquired from what it already knows.
Bootstrapping accounts for the fact that a childcan correlate event structure from the world around it with descriptions of events.
Whena child perceives an event, the structural information it extracts from it can determinehow the child interprets a sentence that describes the event in question.
The incrementalinterpretation of a sentence can in turn direct the child?s attention to relevant entitiesand events in the environment.
Events are only present for a limited time when utter-ances refer to such events during child language acquisition.
This time-limited pres-ence might determine the tight coordination with which attention in the scene interactswith utterance comprehension and information extracted from the scene during adultlanguage comprehension.
This contextual development may have shaped both our cog-nitive architecture (i.e., providing for rapid, seamless integration of scene and linguisticinformation), and comprehension mechanisms (e.g., people rapidly avail themselves ofinformation from the immediate scene when the utterance identifies it).The model presented in this paper extends current models of on-line utterance com-prehension when utterances relate to a scene [12] in several ways.
Existing models ac-count for processes of establishing reference in scene-sentence integration when scenescontain only objects.
Our network accounts for processes of establishing reference, andfurthermore models the rapid assignment of thematic roles based on linguistic and worldknowledge, as well as scene events.
In this way, it achieves rapid scene-utterance inte-gration for increasingly rich visual contexts, including the construction of propositionalrepresentations on the basis of scene events.
It models the integration of utterancesand relatively rich scenes (that contain actions and events) in addition to objects.
Fur-thermore, the model?in line with experimental findings ?
successfully accounts for therelative priority of depicted events in thematic interpretation.
It importantly achieves860 M.R.
Mayberry, III, M.W.
Crocker, and P. Knoeferlethis through a modification of the training regime that prioritizes scene information.This confirms suggestions from [7] that a rapid interplay between utterance compre-hension and the immediate scene context during acquisition is one potential cause forthe relative priority of depicted events during on-line comprehension.Connectionist models such as the SRN have been used to model aspects of cogni-tive development, including the time-course of emergent behaviors [13], making themhighly suitable for simulating developmental stages in child language acquisition (e.g.,first learning names of objects in the immediate scene, and later proceeding to the acqui-sition of stereotypical knowledge).
The finding that modelling this aspect of develop-ment provides an efficient way to naturally reproduce the observed adult comprehensionbehavior promises to offer deeper insight into how adult performance is at least partiallya consequence of the acquisition process.Future research will focus on combining all of the experiments in one model, andexpand the range of sentence types and fillers to which the network is exposed.
Thearchitecture itself is being redesigned to scale up to much more complex linguistic con-structions and have greater coverage while retaining the cognitively plausible behaviordescribed in this study [14].AcknowledgementsThe first two authors were supported by SFB 378 (project ?ALPHA?
), and the third au-thor by a PhD studentship (GRK 715), all awarded by the German Research Foundation(DFG).References1.
Steven Pinker.
How could a child use verb syntax to learn verb semantics?
In Lila Gleitmanand Barbara Landau, editors, The acquisition of the lexicon, pages 377?410.
MIT Press,Cambridge, MA, 1994.2.
Cynthia Fisher, D. G. Hall, S. Rakowitz, and Lila Gleitman.
When it is better to receivethan to give: Syntactic and conceptual constraints on vocabulary growth.
In Lila Gleitmanand Barbara Landau, editors, The acquisition of the lexicon, pages 333?375.
MIT Press,Cambridge, MA, 1994.3.
Jeffrey L. Elman.
Finding structure in time.
Cognitive Science, 14:179?211, 1990.4.
Martin Redington, Nick Chater, and Steven Finch.
Distributional information: A powerfulcue for acquiring syntactic categories.
Cognitive Science, 22:425?469, 1998.5.
Deb Roy and Alex Pentland.
Learning words from sights and sounds: A computationalmodel.
Cognitive Science, 26(1):113?146, 2002.6.
Pia Knoeferle and Matthew W. Crocker.
Stored knowledge versus depicted events: whatguides auditory sentence comprehension.
In Proceedings of the 26th Annual Conference ofthe Cognitive Science Society.
Mahawah, NJ: Erlbaum, 2004.
714?719.7.
Pia Knoeferle and Matthew W. Crocker.
The coordinated interplay of scene, utterance, andworld knowledge: evidence from eye-tracking.
submitted.8.
Yuki Kamide, Christoph Scheepers, and Gerry T. M. Altmann.
Integration of syntactic andsemantic information in predictive processing: Cross-linguistic evidence from German andEnglish.
Journal of Psycholinguistic Research, 32(1):37?55, 2003.A Connectionist Model of Anticipation in Visual Worlds 8619.
Pia Knoeferle, Matthew W. Crocker, Christoph Scheepers, and Martin J. Pickering.
Theinfluence of the immediate visual context on incremental thematic role-assignment: evidencefrom eye-movements in depicted events.
Cognition, 95:95?127, 2005.10.
Risto Miikkulainen.
Natural language processing with subsymbolic neural networks.
InAntony Browne, editor, Neural Network Perspectives on Cognition and Adaptive Robotics,pages 120?139.
Institute of Physics Publishing, Bristol, UK; Philadelphia, PA, 1997.11.
Pia Knoeferle and Matthew W. Crocker.
The coordinated processing of scene and utterance:evidence from eye-tracking in depicted events.
In Proceedings of International Conferenceon Cognitive Science, Allahabad, India, 2004.12.
Deb Roy and Niloy Mukherjee.
Towards situated speech understanding: Visual context prim-ing of language models.
Computer Speech and Language, 19(2):227?248, 2005.13.
Jeffrey L. Elman, Elizabeth A. Bates, Mark H. Johnson, Annette Karmiloff-Smith, DomenicoParisi, and Kim Plunkett.
Rethinking Innateness: A Connectionist Perspective on Develop-ment.
MIT Press, Cambridge, MA, 1996.14.
Marshall R. Mayberry and Matthew W. Crocker.
Generating semantic graphs through self-organization.
In Proceedings of the AAAI Symposium on Compositional Connectionism inCognitive Science, pages 40?49, Washington, D.C., 2004.
