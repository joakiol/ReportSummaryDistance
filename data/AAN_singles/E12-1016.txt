Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 152?161,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsDoes more data always yield better translations?Guillem Gasco?, Martha-Alicia Rocha, Germa?n Sanchis-Trilles,Jesu?s Andre?s-Ferrer and Francisco CasacubertaDepartament de Sistemes Informa`tics i Computacio?Universitat Polite`cnica de Vale`nciaCam??
de Vera s/n, 46022 Vale`ncia, Spain{ggasco,mrocha,gsanchis,jandres,fcn}@dsic.upv.esAbstractNowadays, there are large amounts of dataavailable to train statistical machine trans-lation systems.
However, it is not clearwhether all the training data actually helpor not.
A system trained on a subset of suchhuge bilingual corpora might outperformthe use of all the bilingual data.
This paperstudies such issues by analysing two train-ing data selection techniques: one basedon approximating the probability of an in-domain corpus; and another based on in-frequent n-gram occurrence.
Experimentalresults not only report significant improve-ments over random sentence selection butalso an improvement over a system trainedwith the whole available data.
Surprisingly,the improvements are obtained with just asmall fraction of the data that accounts forless than 0.5% of the sentences.
After-wards, we show that a much larger room forimprovement exists, although this is doneunder non-realistic conditions.1 IntroductionGlobalisation and the popularisation of the Inter-net have lead to a rapid increase in the amount ofbilingual corpora available.
Entities such as theEuropean Union, the United Nations and othermultinational organisations need to translate allthe documentation they generate.
Such transla-tions happen every day and provide very largemultilingual corpora, which are oftentimes diffi-cult to process and significantly increase the com-putational requirements needed to train statisticalmachine translation (SMT) systems.
For instance,the corpora made available for recent machinetranslation evaluations are in the order of 1 billionrunning words (Callison-Burch et al 2010).However, two main problems arise when at-tempting to use this huge pool of sentences fortraining SMT systems: firstly, a large portion ofthis data is obtained from domains that differ fromthat in which the SMT system is to be used or as-sessed; secondly, the use of all this data for train-ing the system increases the computational train-ing requirements.
Despite the previous remarks,the de facto standard consists in training SMT sys-tems with all the available data.
This is due tothe widespread misconception that the more dataa system is trained with, the better its performanceshould be.
Although the previous statement is the-oretically true if all the data belongs to the samedomain, this is not the case in the problems tack-led by most of the SMT systems.
For instance,enterprises often need to build on-demand sys-tems (Yuste et al 2010).
In this case, since weare interested in translating some specific text, itis not clear whether training a system with all datayields better performance than training it with awisely selected subset of bilingual sentences.The bilingual sentence selection (BSS) task isstated as the problem of selecting the best sub-set of bilingual sentences from an available poolof sentences, with which to train a SMT system.This paper is concerned to BSS, and mainly twoideas are developed.On the one hand, two BSS strategies that at-tempt to build better translation systems are anal-ysed.
Such strategies are able to improve state-of-the-art translation quality without the very highcomputational resources that are required whenusing the complete pool of sentences.
Both tech-niques span through two orthogonal criteria whenselecting bilingual sentences from the availablepool: avoiding to introduce a bias in the originaldata distribution, and increasing the informative-ness of the corpus.On the other hand, we prove that among all pos-sible subsets from the sentence pool, there is atleast a small one that yields large improvements(up to 10 BLEU points) with respect to a systemtrained with all the data.
In order to retrieve suchsubset, we had to use an oracle that employs infor-mation extracted from the reference translations152only for the purpose of selecting bilingual sen-tences.
However, references are not used at anystage within the translation system for obtainingthe hypotheses.
Note that although we are notable to achieve such an improvement without anoracle, this result restates the BSS problem as aninteresting approach not only for reducing com-putational effort but also for significantly boost-ing performance.
To our knowledge, no previouswork has quantified the room of improvement inwhich BSS techniques could incur.In order to assess the performance of the dif-ferent BSS techniques, translation results are ob-tained by using a standard state-of-the-art SMTsystem (Koehn et al 2007).
The most recent lit-erature defines the SMT problem (Papineni et al1998; Och and Ney, 2002) as follows: given aninput sentence f from a certain source language,the purpose is to find an output sentence e?
in acertain target language such thate?
= argmaxeK?k=1?khk(f , e) (1)where hk(f , e) is a score function representing animportant feature for the translation of f into e,as for example the language model of the targetlanguage, a reordering model or several transla-tion models.
?k are the log-linear combinationweights.The main contributions of this paper are:?
A BSS technique is analysed, which im-proves the results obtained with a randombilingual sentence selection strategy whenthe specific domain to be translated signifi-cantly differs from that of the pool of sen-tences.?
Another BSS technique is analysed that, us-ing less than 0.5% of the sentences avail-able, significantly improves over random se-lection, beating a system trained with all thepool of sentences.?
We prove, by means of an oracle, that a wiseBSS technique can yield large improvementswhen compared with systems trained with alldata available.The remaining of the paper is structured as fol-lows.
Section 2 summarises the related work.Sections 3 and 4 present two BSS techniques,namely, probabilistic sampling and recovery ofinfrequent n-grams.
In Section 5 experimental re-sults are reported.
Finally, the main results of thework and several future work directions are dis-cussed in Section 6.2 Related WorkTraining data selection has been receiving an in-creasing amount of attention within the SMTcommunity.
For instance, in (Li et al 2010;Gasco?
et al 2010) several BSS techniques, sim-ilar to those analysed in this paper, have beenapplied for training MT systems when there arelarge training corpora available.
However, nei-ther such techniques have been formalised, nor itsperformance thoroughly analysed.
A similar ap-proach that gives weights to different subcorporawas proposed in (Matsoukas et al 2009).In (Lu et al 2007), information retrieval meth-ods are used in order to produce different sub-models which are then weighted according to thesentence to be translated.
In such work, authorsdefine the baseline as the result obtained train-ing only with the corpus that share the same do-main of the test.
Afterwards they claim that theyare able to improve baseline translation quality byadding new sentences retrieved with their method.However, they neither compare their techniquewith random sentence selection, nor with a modeltrained with all the corpora.Although the techniques that are applied forBSS are often very similar to those applied for ac-tive learning (AL), both problems are essentiallydifferent.
Since the AL strategies assume thatthe pool of sentences are not translated, they areusually interested in finding the best monolingualsubset of sentences to be translated by a humanannotator.
In contrast, in BSS, it is assumed that afairly large amount of bilingual corpora is readilyavailable, and the main goal consists in selectingonly those sentences which will maximise systemperformance.Some works have applied sentence selection insmall scale AL frameworks.
These works extendthe training corpora at most with 5000 sentences.In (Ananthakrishnan et al 2010), sentences areselected by means of discriminative techniques.In (Haffari et al 2009) a technique is proposedfor increasing the counts of phrases that are con-sidered infrequent.
Both works significantly dif-fer from the current work not only on the frame-work, but also on the scale of the experiments, the153proposed techniques and the obtained improve-ments.
Similar ideas applied to adaptation prob-lems have been proposed in (Moore and Lewis,2010; Axelrod et al 2011).3 Probabilistic SamplingAs discussed in Section 2, BSS has inherentlyattached many meaningful links with AL tech-niques.
Selecting samples for learning our mod-els, incurs in a well-known difficulty in AL, theso-called sample bias problem (Dasgupta, 2009).This problem, which is spread to the BSS case,is summarised as the distortion introduced by theactive strategy into the probability distribution un-derlying the training corpus.
This bias forces thetraining algorithm to learn a distorted probabilitymodel which can significantly differ from the ac-tual one.In order to further analyse the sampling biasproblem, consider the maximum likelihood esti-mation (MLE) of a probability model, p?
(e, f)for a given corpus of N data points,{(en, fn)},sampled from the actual probability distribution,Pr(e, f).
Recall that e denotes a target sen-tence whereas f stands for its source counter-part.
MLE techniques aims at minimising theKullback-Leibler divergence between the actualunknown probability distribution and the proba-bility model (Bishop, 2006), defined asKL(Pr | p?)
=?e,fPr(e, f) log(Pr(e, f)p?
(e, f))(2)When minimising, Eq.
(2) is simplified to??
= argmax?
?e,fPr(e, f) log(p?
(e, f)) (3)which is approximated by a sufficiently largedataset under the commonly hold assumption thatit is independently and identically distributed ac-cording to Pr(e, f) as??
= argmax??nlog(p?
(en, fn)) (4)Therefore, by perturbing the sample {(en, fn)}with an active strategy, we are, in fact, modifyingthe approximation to Eq.
(3) and learning a differ-ent underlying probability distribution.In this section a statistical framework is pro-posed to build systems with BSS while avoidingthe sample bias.
The proposed approach relies inconserving the probability distribution of the taskdomain by wisely selecting the bilingual pairs tobe used from the whole pool of sentences.
Hence,it is mandatory to exclude sentences from the poolthat distort the actual probability.
In order to ap-proximate the probability distribution, we assumethat a small but representative corpus is avail-able from the task domain.
This corpus, referredhenceforth as the in-domain corpus, provides away to build an initial model which approximatesthe actual probability of the system.
The pool ofsentences will be oppositely denoted as the out-of-domain corpus.The actual probability of the task domain, theso called in-domain probability, is approximatedwith the following modelp(e, f , |e|, |f |) = p(e, f | |e|, |f |) ?
p(|e|, |f |) (5)where p(|e|, |f |) denotes the in-domain lengthprobability, and p(e, f | |e|, |f |) the in-domainbilingual probability.The length probability is estimated by MLEp(|e|, |f |) =N(|e|+ |f |)N(6)where N(|e|+|f |) is the number of bilingual pairsin the in-domain corpus such that their lengthssum up to |e|+|f | and N denotes the total num-ber of sentences.
Note that no distinction is madebetween source and target lengths since the modelis intended for sampling.The complexity of the in-domain bilingualprobability distribution, p(e, f | |e|, |f |), requiresa more sophisticated approximationp(e, f/|e|, |f |) =exp(?k ?kfk(e, f))Z(7)being Z a normalisation constant; and wherefk(.
.
.)
and ?k are the features of the model andtheir respective parametric weights.
Specifically,four logarithmic features were considered for thissampling technique: a direct and an inverse IBMmodel 4 (Brown et al 1994); and both, sourceand target, 5-gram language models.
All fea-ture models are estimated in the in-domain cor-pus with standard techniques (Brown et al 1994;Stolcke, 2002).
As a first approach, the parame-ters of the log-linear model in Eq.
(7), ?k, wereuniformly fixed to 1.154Once we have an appropriate model for thein-domain probability distribution, the proposedmethod randomly samples a given number ofbilingual pairs from the out-of-domain corpora(the pool of sentences).
The process of extend-ing the in-domain corpus with additional bilin-gual pairs from the out-of-domain corpus is sum-marised as follows:?
Decide according to the in-domain lengthprobability in Eq.
(6), how many samplesshould be drawn for each length, i.e.
dividethe number of sentences to add into lengthdependent buckets.?
Randomly draw the number of samplesspecified in each bucket according to thein-domain bilingual probability in Eq.
(7)among all the bilingual sentences that sharethe current bucket length.Although the pool of sentences is typicallylarge, it is not large enough to gather a signifi-cant amount of probability mass.
Consequently,a small set of sentences accumulate most of theprobability mass and tend to be selected multi-ple times.
To avoid this awkward and undesiredbehaviour, the sampling is performed without re-placement.4 Infrequent n-gram RecoveryAnother criterion when confronting the BSS taskis to increase the informativeness of the trainingset.
Thus, it seems important to choose sentencesthat provide information not seen in the trainingcorpus.
Note that this criterion is sometimes op-posed to the one presented in Section 3.The performance of phrase-based machinetranslation systems strongly relies in the qualityof the phrases extracted from the training sam-ples.
In most of the cases, the inference of suchphrases or rules is based on word alignments,which cannot be computed accurately when ap-pearing rarely in the training corpus.
The extremecase are the out-of-vocabulary words: words thatdo not appear in the training set, cannot be trans-lated.
Moreover, this problem can be extended tosequences of words (n-grams).
Consider a 2-gramfifj appearing few or no times in the training set.Although fi and fj may appear separately in thetraining set, the system might not be able to in-fer the translation of the 2-gram fifj , which maybe different from the concatenation of the transla-tions of both words separately.When selecting sentences from the pool it isimportant to choose sentences that contain n-grams that have never been seen (or have beenseen just a few times) in the training set.
Suchn-grams will be henceforth referred to as infre-quent n-grams .
An n-gram is considered infre-quent when it appears less times than an infre-quent threshold t. If the source language sen-tences to be translated are known beforehand, theset of infrequent n-grams can be reduced to thosepresent in such sentences.
Then, the techniqueconsists in selecting from the pool those sentenceswhich contain infrequent n-grams present in thesource sentences to be translated.Sentences in the pool are sorted by their infre-quency score in order to select first the most in-formative.
Let X the set of n-grams that appearin the sentences to be translated and w one ofthem; C(w) the counts of w in the source lan-guage training set; and N(w) the counts of win the source sentence f to be scored.
The infre-quency score of f is:i(f) =?w?Xmin(1, N(w))max(0, t?C(w)) (8)In order to avoid giving a high score to noisysentences with a lot of occurrences of the same in-frequent n-gram, only one occurrence of each n-gram is taken into account to compute the score.In addition, the score gives more importance tothe n-grams with lowest counts in the trainingset.
Although it could be possible to select thehighest scored sentences, we updated the scoreseach time a sentence is selected.
This decisionwas taken to avoid the selection of too many sen-tences with the same infrequent n-gram.
First,sentences in the pool are scored using Equation(8).
Then, in each iteration, the sentence f?
withthe highest score is selected, added to the trainingset and removed from the pool.
In addition, thecounts of the n-grams present in f?
are updatedand, hence, the scores of the rest of the sentencesin the pool.
Since rescoring the whole pool wouldincur in a very high computational cost, a subop-timal search strategy was followed, in which thesearch was constrained to a given set of highestscoring sentences.
Here it was set to one million.155t = 1 t = 10 t = 25tr all tr all tr all1-gr 11.6 1.3 40.5 3.5 59.9 5.12-gr 38 9.8 73.2 21.3 84.9 27.93-gr 66.8 33.5 91.1 55.7 96.4 64.94-gr 87.1 65.8 98.2 85.5 99.4 90.7Table 1: Percentage of infrequent n-grams in the TEDtest set when considering only the TED training set(tr), and when adding the out-of-domain pool (all),for different infrequency thresholds t.Table 1 shows the percentage of source lan-guage infrequent n-grams for the test of a rela-tively small corpus such as the TED corpus (fordetails see Section 5) when considering just thein-domain training set (?
40K sentences) and thesame percentage when adding the larger out of do-main corpora.
The percentages in the table havebeen computed separately for different values ofthe threshold t and for n-grams of order from 1 to4.
Note that the reduction in the number of infre-quent n-grams is very high for the 1-grams but de-creases progressively when considering n-gramsof higher order.
This indicates that the infrequentn-grams recovery technique should be very effec-tive for lower order n-grams, but might have lesseffect for higher order n-grams.
Therefore, andin order to lower the computational cost involved,the experiments carried out for this paper wereperformed considering only infrequent 1-grams,2-grams and 3-grams.5 ExperimentsIn the present Section, we first describe the exper-imental framework employed to assess the perfor-mance of the BSS techniques described.
Then, re-sults for the probabilistic sentence selection strat-egy are shown, followed by results obtained withthe infrequent n-grams technique.
Some exam-ple translations are shown and, finally, we alsoreport experiments using the infrequent n-gramstechnique in Oracle mode, in order to establishthe potential improvement for such technique andfor BSS in general.5.1 Experimental SetupAll experiments were carried out using theopen-source SMT toolkit Moses (Koehn et al2007), in its standard non-monotonic configura-tion.
The phrase tables were generated by meansof symmetrised word alignments obtained withSubset Language |S| |W | |V |trainEnglish47.5K747K 24.6KFrench 793K 31.7KdevEnglish5719.2K 1.9KFrench 10.3K 2.2KtestEnglish64112.6K 2.4KFrench 12.8K 2.7KTable 2: TED corpus main figures.
K denotes thou-sands of elements.
|S| stands for number of sentences,|W | for number of running words, and |V | for vocab-ulary size.Subset Language |S| |W | |V |trainEnglish77.2K1.71M 29.9KFrench 1.99M 48Kdev 08English2.1K49.8K 8.7KFrench 55.4K 7.7Ktest 09English2.5K65.6K 8.9KFrench 72.5K 10.6Ktest 10English2.5K62K 8.9KFrench 70.5K 10.3KTable 3: News Commentary corpus main figures.GIZA++ (Och and Ney, 2003).
The languagemodel used was a 5-gram with modified Kneser-Ney smoothing (Kneser and Ney, 1995), builtwith SRILM toolkit (Stolcke, 2002).
The log-linear combination weights in Eq.
(1) were opti-mised using Minimum Error Rate Training (Ochand Ney, 2002) on the corresponding develop-ment sets.Experiments were carried out on two corpora:TED (Paul et al 2010) and News Commentary(NC) (Callison-Burch et al 2010).
TED is anEnglish-French corpus composed of subtitles fora collection of public speeches on a variety of top-ics.
The same partitions as in the IWSLT2010evaluation task (Paul et al 2010) have been used.Subtitles have been concatenated into completesentences.
NC is a slightly larger English-Frenchcorpus in the news domain.
Main figures of bothcorpora are shown in Tables 2 and 3.
As for thepool of sentences, three large corpora have beenused: Europarl (Euro), United Nations (UN) andGigaword (Giga), in the partition established forthe 2010 workshop on SMT of the ACL (Callison-Burch et al 2010).
Sentences of length greaterthan 50 have been pruned.
Table 4 shows the mainfigures of the tokenised and lowercased corpora.When translating between some languagepairs, there are words that remain invariable, likefor example numbers or punctuation marks in thecase of European languages.
In fact, an easy and156Corpus Language |S| |W | |V |EuroEnglish1.25M25.6M 81KFrench 28.2M 101KUNEnglish5M94.4M 302KFrench 107M 283KGigaEnglish15.5M303M 1.6MFrench 361M 1.6MTable 4: Figures of the corpora used as sentence pool.M stands for millions of elements.effective technique that is commonly used is to re-produce out-of-vocabulary words from the sourcesentence in the target hypothesis.
However, in-variable n-grams are usually infrequent as well,which implies that the infrequent n-grams tech-nique would select sentences containing such n-grams, even though they do not provide furtherinformation.
As a first approach, we exclude n-grams without any letter.Baseline experiments have been carried out forTED and NC corpora using the correspondingtraining set.
For comparison purposes, we alsoincluded results for a purely random sentence se-lection without replacement.
In the plots, eachpoint corresponding to random selection representthe average of 10 repetitions.
Experiments usingall data are also reported, although a 64GB ma-chine was necessary, even with binarized phraseand distortion tables.Experiments were conducted by selecting afixed amount of sentences according to each oneof the techniques described above.
Then, thesesentences were included into the training data andsubsequent SMT systems were built for translat-ing the test set.Results are shown in terms of BLEU (Papineniet al 2001), which is an accuracy metric thatmeasures n-gram precision, with a penalty forsentences that are too short.
Although it couldbe argued that improvements obtained might bedue to a side effect of the brevity penalty, thiswas not found to be true: the BSS techniques (in-cluding random) and considering all data yieldedvery similar brevity penalties (?0.005), withineach corpus.
In addition, TER scores (Snover etal., 2006) were also computed, but are omittedfor clarity purposes and since they were found tobe coherent with BLEU.
TER is an error metricthat computes the minimum number of edits re-quired to modify the system hypotheses so thatthey match the references translations.00.010.020.030  10  20  30  40  50  60  70  80  90  100RelativefrequencyCombined sentence lengthEuroparlGigawordUNTEDNCFigure 2: Combined length relative frequency.5.2 Results for Probabilistic SamplingIn addition to the probabilistic sampling tech-nique proposed in Section 3, we also analysed theeffect of sampling only according to the combinedsource-reference length, with the purpose of es-tablishing whether potential improvements wereonly due to the length component, or rather to thecomplete sampling model.
Results for the 2009test set are shown in Figure 1.
Several thingsshould be noted:?
Performing sentence selection only accordingto sentence lengths does not achieve betterperformance than random selection.?
Selecting sentences according to probabilis-tic sampling is able to improve random se-lection in the case of the TED corpus, butis not able to do so in the case of the NCcorpus.
Significance tests for the 500K casereported that the differences were significantin the case of the TED corpus, but not in thecase of the NC corpus.?
In the case of the TED corpus, the perfor-mance achieved with the system built bysampling 500K sentences is only 0.5 BLEUpoints below the performance achieved bythe system built with all the data available.The explanation to the fact that probabilisticsampling is able to improve over random sam-pling only in the case of the TED corpus, but notin the case of NC, relies in the nature of the cor-pora.
Although both of them belong to a verygeneric domain, their characteristics are very dif-ferent.
In fact, the NC data is very similar to thesentences in the pool, but, in contrast, the sen-tences present in the TED corpus have a muchmore different structure.
This difference is illus-trated in Figure 2, where the relative frequency of157212223240 100K 200K 300K 400K 500KBLEUNumber of sentences addedTED corpusin domainallrandomlengthsampling192021220 100K 200K 300K 400K 500KBLEUNumber of sentences addedNC corpusin domainallrandomlengthsamplingFigure 1: Effect of adding sentences over the BLEU score using the probabilistic sampling, length sampling andrandom selection techniques for the two corpora, TED and News Commentary.
Horizontal lines represent thescores when using just the in domain training set and all the data available.2122232425260 50k 100k 200kBLEUNumber of sentences addedTED corpusallt=10 in domaint=25 random19202122230 50k 100k 200kBLEUNumber of sentences addedNC corpusallin domainrandomt=10t=25Figure 3: Effect of adding sentences over the BLEU score using the infrequent n-grams (with different thresh-olds) and random selection techniques for the two corpora, TED and News Commentary.
Horizontal lines repre-sent the scores when using just the in domain training set and all the data available.each combined sentence length is shown.
In thisplot, it stands out clearly that the TED corpus hasa very different length distribution than the otherfour corpora considered, whereas the NC corpuspresents a very similar distribution.
This impliesthat, when considering TED, an intelligent dataselection strategy will have better chances to im-prove random selection than in the case of NC.5.3 Results for Infrequent n-grams RecoveryFigure 3 shows the effect of adding sentences us-ing the infrequent n-grams and the random se-lection techniques on the 2009 test set.
Onceall the infrequent n-grams have been coveredt times, the infrequency score for all the sen-tences remaining in the pool is 0, and none ofthem can be selected.
Hence, the number ofsentences that can be selected for each t is lim-ited.
Although for clarity we only show resultsfor t = {10, 25}, experiments have also been car-ried out for t = {1, 5, 10, 25}.
Such results pre-sented similar curves, although less sentences canbe selected and hence improvements obtained areslightly lower.
Several conclusions can be drawn:?
The translation quality provided by the in-frequent n-grams technique is significantlybetter than the results achieved with randomselection, comparing similar amount of sen-tences.
Specifically, the improvements ob-tained are in the range of 3 BLEU points.?
Results for the TED corpus are more irreg-ular.
The best performance is achieved fort = 25 and 50K sentences added.
In NC, thebest result is for t = 10 and 112K.?
Selecting sentences with the infrequent n-grams technique provides better results thanincluding all the available data.
While usingless than 0.5% of the data, improvements be-tween 0.5 and 1 BLEU points are achieved.When looking at Figure 3, one might suspectthat t needs to be set specifically for a given test158set, and that results from one set are not to be ex-trapolated to other test sets.
For this reason, weselected the best configuration in Figure 3 andused it to build a new system for translating theunseen NC 2010 test set.
Such experiment, witht = 10 and including all sentences with scoregreater than 0 (?
110K), is shown in Table 5 andevidences that improvements are actually coher-ent among different test sets.technique BLEU TER #phrasesin-domain 19.0 65.2 5.1Mall data 22.7 60.8 1236Minfreq.
t = 10 23.6 59.2 16.5MTable 5: Effect of the infrequent n-gram recovery tech-nique for an unseen test set, when setting t = 10 andnumber of phrases (parameters) of the models.5.4 Oracle ResultsIn order to analyse the potential of BSS tech-niques, the infrequent n-grams recovery tech-nique in Section 4 was implemented in oraclemode.
In this way, sentences from the poolwere selected according to the infrequent n-gramspresent in the reference translations of the test set.Note that test references were not included intothe training data as such, but were rather usedto establish which bilingual sentences within thepool were best suitable for training the SMT sys-tem.
In this way, we were able to establish the po-tential for improvement of a BSS technique.
In-terestingly, the SMT system trained in this wayachieved 31 BLEU points on the News Commen-tary 2009 test set, i.e.
an 8 BLEU points improve-ment over the system trained with all the dataavailable.
This result would have beaten all thesystems that took part in the 2009 Workshop onMachine translation (Callison-Burch et al 2009).This result is really important: although we areaware that the sentences were selected in a non-realistic manner, it proves that an appropriate BSStechnique would be able to boost SMT perfor-mance in a very significant manner.
Similar re-sults were obtained with the TED and NC 2010test sets, with 10 and 7 points improvement, re-spectively.5.5 Example TranslationsExample translations are shown in Figure 4.
Inthe first example, the baseline system is not ableSrc the budget has also been criticised by klaus .Bsl le budget a e?galement e?te?
criticised par m. klaus .Rdm le budget a e?galement e?te?
critique?es par m. klaus .PS le budget a e?galement e?te?
critique?e par klaus .All le budget a e?galement e?te?
critique?
par klaus .Infr le budget a e?galement e?te?
critique?
par klaus .Ref klaus critique e?galement le budget .Src and one has come from music .Bsl et un a de la musique .Rdm et on vient de musique .PS et on a viennent de musique .All et de la musique .Infr et un est venu de la musique .Ref et un vient du monde de la musique .Figure 4: Examples of two translations for each of theSMT systems built: Src (source sentence), Bsl (base-line), Rdm (random selection), PS (probabilistic sam-pling), All (all the data available), Infr (Infrequent n-grams) and Ref (reference).to translate criticised, which is considered out-of-vocabulary.
Even though random selection is ableto solve this problem (luckily), it does not achieveto translate it correctly, introducing a concordanceerror.
A similar thing happens when using prob-abilistic sampling, where a grammatical error isalso present, and only Infr and All are ableto present a correct translation.
This is not onlycasual, since, by ensuring that a given n-gram ap-pears at least a certain number of times t, the oddsof including all possible translations of criticisedare incremented significantly.
Note that, even ifthe Infr translation is different from the refer-ence, it is equally correct.
In the second example,the baseline translation is pretty much correct, buthas a different meaning (something like ?and onehas music?).
Similarly, when including all datathe translation obtained by the system means ?andsome music?.
In this case, both random and prob-abilistic selection present grammatically incorrectsentences, and only Infr is able to provide a cor-rect translation, although pretty literal and differ-ent from the reference.6 DiscussionBilingual sentence selection (BSS) might be un-derstood to be closely related to adaptation, eventhough both paradigms tackle problems whichare, in essence, different.
The goal of an adap-tation technique is to adapt model parameters,which have been estimated on a large out-of-domain (or generic) data set, so that they are159best suitable for dealing with a domain-specifictest set.
This adaptation process is ought to beachieved by means of a (potentially small) adapta-tion set, which belongs to the same domain as thetest data.
In contrast, BSS tackles with the prob-lem of how to select samples from a large poolof training data, regardless of whether such poolof data is in-domain or out-of-domain.
Hence, inone case we can assume to have a fairly well es-timated translation model, which is to be adapted,whereas in BSS we still have full control over theestimation of such model and need not to aim at aspecific domain, although it might often be so.BSS is related with instance weighting (Jiangand Zhai, 2007; Foster et al 2010).
Adapta-tion and BSS can be considered to be orthogo-nal (yet complementary) problems under the in-stance weighting paradigm.
In such case, instanceweighting can be considered to span a completeparadigmatic space between both.
At one end,there is sample selection (BSS for SMT), while atthe other end there is adaptation.
For instance, itis quite common to confront the adaptation prob-lem by extracting different phrase-tables from dif-ferent corpora, and then interpolate such tables.This technique could be also applied to promotethe performance of the system built by means ofBSS.
However, this is left out as future work.We thoroughly analysed two BSS approachesthat obtain competitive results, while using asmall fraction of the training data, although thereis still much to be gained.
For instance, oracle re-sults have also been reported in this work, yield-ing improvements of up to 10 BLEU points.
Eventhough the use of an oracle typically implies thatthe results obtained are not realistic, recall thatthe proposed oracle is special, in the sense that itonly uses the reference sentences for the specificpurpose of selecting training samples, but the ref-erences are not included into the training data assuch.
This is useful for assessing the potential be-hind BSS: ideally, if we were able to design a BSSstrategy that, without using the references, wouldselect exactly those training samples, we would beboosting system performance by 10 BLEU points.This re-states BSS as a compelling technique thathas not yet received the attention it deserves.BSS is not aimed at optimising computationalrequirements, but does so as a byproduct.
Thismay seem despicable but it would allow to runmore experiments with the same resources, uselarger corpora or even more complex techniques,such as synchronous grammars or hierarchicalmodels.
For instance, the infrequent n-gramstechnique has beaten all the other systems usingjust a small fraction of the corpus, only 0.5%, andis yet able to outperform a system trained with allthe data by 0.9 BLEU points and the random base-line by 3 points.
This baseline has been proved tobe difficult to beat by other works.Preliminary experiments were performed in or-der to analyse the perplexity of the references, thenumber of out of vocabulary words (OoVs) andthe ratio of target-source phrases.
These exper-iments revealed that the improvements obtainedare largely correlated with a decrease in perplex-ity and in the number of OoVs.
On the one hand,reducing the amount of OoVs was mirrored byan important improvement in BLEU when theamount of additional data was small, and alsoentailed a decrease in perplexity.
However, areduction in perplexity by itself did not alwaysimply significant improvements.
Moreover, noreal conclusion could be drawn from the analy-sis of target-source phrase ratio.
Hence, we un-derstand that the improvements obtained are pro-vided mainly by a more specialised estimation ofthe model parameters.
However, further experi-ments should still be conducted in order to verifythis conclusion.AcknowledgmentsThe research leading to these results has re-ceived funding from the European Union SeventhFramework Programme (FP7/2007-2013) undergrant agreement nr.
287755.
This work wasalso supported by the Spanish MEC/MICINN un-der the MIPRCV ?Consolider Ingenio 2010?
pro-gram (CSD2007-00018), and iTrans2 (TIN2009-14511) project.
Also supported by the Span-ish MITyC under the erudito.com (TSI-020110-2009-439) project and Instituto Tecnolo?gico deLeo?n, DGEST-PROMEP y CONACYT, Me?xico.160ReferencesSankaranarayanan Ananthakrishnan, Rohit Prasad,David Stallard, and Prem Natarajan.
2010.
Dis-criminative sample selection for statistical machinetranslation.
In Proc.
of the EMNLP, pages 626?635,Cambridge, MA, October.Amittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domaindata selection.
In Proc of the EMNLP, pages 355?362.Christopher M. Bishop.
2006.
Pattern Recognitionand Machine Learning.
Springer.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1994.
The mathe-matics of statistical machine translation: Parameterestimation.
Computational Linguistics, 19(2):263?311.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProc of the WSMT, pages 1?28, Athens, Greece,March.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.2010.
Findings of the 2010 joint Workshop on Sta-tistical Machine Translation and Metrics for Ma-chine Translation.
In Proc.
of the MATR(ACL),pages 17?53, Uppsala, Sweden, July.Sanjoy Dasgupta.
2009.
The two faces of active learn-ing.
In Proc.
of The twentieth Conference on Algo-rithmic Learning Theory, page 1, Porto (Portugal),October.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adap-tation in statistical machine translation.
In Proc.
ofthe EMNLP, pages 451?459, Cambridge, MA, Oc-tober.Guillem Gasco?, Vicent Alabau, Jesu?s Andre?s-Ferrer,Jesu?s Gonza?lez-Rubio, Martha-Alicia Rocha,Germa?n Sanchis-Trilles, Francisco Casacuberta,Jorge Gonza?lez, and Joan-Andreu Sa?nchez.
2010.ITI-UPV system description for IWSLT 2010.
InProc.
of the IWSLT 2010, Paris, France, December.Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.2009.
Active learning for statistical phrase-basedmachine translation.
In Proc.
of HLT/NAACL?09,pages 415?423, Morristown, NJ, USA.Jing Jiang and ChengXiang Zhai.
2007.
Instanceweighting for domain adaptation in NLP.
In Proc.of ACL?07, pages 264?271.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
Proc.of ICASSP, II:181?184, May.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristieMoran, Richard Zens, Chris Dyer, Ontraj Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proc.
of ACL, pages 177?180.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Ann Irvine, Sanjeev Khudanpur, LaneSchwartz, Wren Thornton, Ziyuan Wang, JonathanWeese, and Omar Zaidan.
2010.
Joshua 2.0: Atoolkit for parsing-based machine translation withsyntax, semirings, discriminative training and othergoodies.
In Proc.
of the MATR(ACL), pages 139?143, Uppsala, Sweden, July.Yajuan Lu, Jin Huang, and Qun Liu.
2007.
Improv-ing statistical machine translation performance bytraining data selection and optimization.
In Proc.
ofthe EMNLP-CoNLL, pages 343?350, Prague, CzechRepublic, June.Spyros Matsoukas, Antti-Veikko I. Rosti, and BingZhang.
2009.
Discriminative corpus weight es-timation for machine translation.
In Proc.
of theEMNLP, pages 708?717, Singapore, August.Robert C. Moore and William Lewis.
2010.
Intelli-gent selection of language model training data.
InACL (Short Papers), pages 220?224.Franz J. Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proc.
of ACL, pages295?302.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.In Computational Linguistics, volume 29, pages19?51.Kishore Papineni, Salim Roukos, and Todd Ward.1998.
Maximum likelihood and discriminativetraining of direct translation models.
In Proc.
ofICASSP?98, pages 189?192.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2001.
Bleu: A method for automaticevaluation of machine translation.
In Technical Re-port RC22176 (W0109-022).Michael Paul, Marcello Federico, and Sebastian Stker.2010.
Overview of the IWSLT 2010 evaluationcampaign.
In Proc.
of the IWSLT 2010, Paris,France, December.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A studyof translation edit rate with targeted human annota-tion.
In Proc.
of AMTA?06.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proc.
of ICSLP.Elia Yuste, Manuel Herranz, Antonio Lagarda, Li-onel Tarazo?n, Isa?
?as Sa?nchez-Cortina, and Fran-cisco Casacuberta.
2010.
Pangeamt - puttingopen standards to work... well.
In Proc.
of theAMTA2010.
Denver, CO, USA, November.161
