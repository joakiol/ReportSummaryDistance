Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 461?466,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsWhy Initialization Matters for IBM Model 1:Multiple Optima and Non-Strict ConvexityKristina ToutanovaMicrosoft ResearchRedmond, WA 98005, USAkristout@microsoft.comMichel GalleyMicrosoft ResearchRedmond, WA 98005, USAmgalley@microsoft.comAbstractContrary to popular belief, we show that theoptimal parameters for IBM Model 1 are notunique.
We demonstrate that, for a largeclass of words, IBM Model 1 is indifferentamong a continuum of ways to allocate prob-ability mass to their translations.
We study themagnitude of the variance in optimal modelparameters using a linear programming ap-proach as well as multiple random trials, anddemonstrate that it results in variance in testset log-likelihood and alignment error rate.1 IntroductionStatistical alignment models have become widelyused in machine translation, question answering,textual entailment, and non-NLP application areassuch as information retrieval (Berger and Lafferty,1999) and object recognition (Duygulu et al, 2002).The complexity of the probabilistic modelsneeded to explain the hidden correspondence amongwords has necessitated the development of highlynon-convex and difficult to optimize models, suchas HMMs (Vogel et al, 1996) and IBM Models 3and higher (Brown et al, 1993).
To reduce the im-pact of getting stuck in bad local optima the orig-inal IBM paper (Brown et al, 1993) proposed theidea of training a sequence of models from simplerto complex, and using the simpler models to initial-ize the more complex ones.
IBM Model 1 was thefirst model in this sequence and was considered areliable initializer due to its convexity.In this paper we show that although IBM Model 1is convex, it is not strictly convex, and there is a largespace of parameter values that achieve the same op-timal value of the objective.We study the magnitude of this problem by for-mulating the space of optimal parameters as solu-tions to a set of linear equalities and seek maximallydifferent parameter values that reach the same objec-tive, using a linear programming approach.
This letsus quantify the percentage of model parameters thatare not uniquely defined, as well as the number ofword types that have uncertain translation probabil-ities.
We additionally study the achieved variance inparameters resulting from different random initial-ization in EM, and the impact of initialization on testset log-likelihood and alignment error rate.
Theseexperiments suggest that initialization does matterin practice, contrary to what is suggested in (Brownet al, 1993, p. 273).12 PreliminariesIn Appendix A we define convexity and strict con-vexity of functions following (Boyd and Vanden-berghe, 2004).
In this section we detail the gener-ative model for Model 1.2.1 IBM Model 1IBM Model 1 (Brown et al, 1993) defines a genera-tive process for a source sentences f = f1 .
.
.
fm andalignments a = a1 .
.
.
am given a corresponding tar-get translation e = e0 .
.
.
el.
The generative processis as follows: (i) pick a length m using a uniformdistribution with mass function proportional to ; (ii)for each source word position j, pick an alignment1When referring to Model 1, Brown et al (1993) state that?details of our initial guesses for t(f |e) are unimportant?.461position in the target sentence aj ?
0, 1, .
.
.
, l froma uniform distribution; and (iii) generate a sourceword using the translation probability distributiont(fj |eaj ).
A special empty word (NULL) is assumedto be part of the target vocabulary and to occupythe first position in each target language sentence(e0=NULL).The trainable parameters of Model 1 are the lex-ical translation probabilities t(f |e), where f and erange over the source and target vocabularies, re-spectively.
The log-probability of a single sourcesentence f given its corresponding target sentence eand values for the translation parameters t(f |e) canbe written as follows (Brown et al, 1993):m?j=1logl?i=0t(fj |ei)?m log(l + 1) + log The parameters of IBM Model 1 are usu-ally derived via maximum likelihood estimationfrom a corpus, which is equivalent to negativelog-likelihood minimization.
The negative log-likelihood for a parallel corpus D is:LD(T ) = ?
?f ,em?j=1logl?i=0t(fj |ei) +B (1)where T is the matrix of translation probabilitiesand B represents the other terms of Model 1 (stringlength probability and alignment probability), whichare constant with respect to the translation parame-ters t(f |e).We can define the optimization problem as theone of minimizing negative log-likelihood LD(T )subject to constraints ensuring that the parametersare well-formed probabilities, i.e., that they are non-negative and summing to one.
It is well-known thatthe EM algorithm for this problem converges to a lo-cal optimum of the objective function (Dempster etal., 1977).3 Convexity analysis for IBM Model 1In this section we show that, contrary to the claim in(Brown et al, 1993), the optimization problem forIBM Model 1 is not strictly convex, which meansthat there could be multiple parameter settings thatachieve the same globally optimal value of the ob-jective.2The function ?
log(x) is strictly convex (Boydand Vandenberghe, 2004).
Each term in the nega-tive log-likelihood is a negative logarithm of a sumof parameters.
The negative logarithm of a sum isnot strictly convex, as illustrated by the followingsimple counterexample.
Let?s look at the function?
log(x1 +x2).
We can express it in vector notationusing ?
log(1Tx), where 1 is a vector with all ele-ments equal to 1.
We will come up with two param-eter settings x,y and a value ?
that violate the defini-tion of strict convexity.
Take x = [x1, x2] = [.1, .2],y = [y1, y2] = [.2, .1] and ?
= .5.
We havez = ?x + (1 ?
?
)y = [z1, z2] = [.15, .15].
Also?
log(1T (?x + (1 ?
?
)y)) = ?
log(z1 + z2) =?
log(.3).
On the other hand, ??
log(x1 + x2) ?(1??)
log(y1+y2) = ?
log(.3).
Strict convexity re-quires that the former expression be strictly smallerthan the latter, but we have equality.
Therefore, thisfunction is not strictly convex.
It is however con-vex as stated in (Brown et al, 1993), because it is acomposition of log and a linear function.We thus showed that every term in the negativelog-likelihood objective is convex but not strictlyconvex and thus the overall objective is convex, butnot strictly convex.
Because the objective is con-vex, the inequality constraints are convex, and theequality constraints are affine, the IBM Model 1 op-timization problem is a convex optimization prob-lem.
Therefore every local optimum is a global op-timum.
But since the objective is not strictly con-vex, there might be multiple distinct parameter val-ues achieving the same optimal value.
In the nextsection we study the actual space of optima for smalland realistically-sized parallel corpora.2Brown et al (1993, p. 303) claim the following aboutthe log-likelihood function (Eq.
51 and 74 in their paper, andEq.
1 in ours): ?The objective function (51) for this model is astrictly concave function of the parameters?, which is equivalentto claiming that the negative log-likelihood function is strictlyconvex.
In this section, we will theoretically demonstrate thatBrown et al?s claim is in fact incorrect.
Furthermore, we willempirically show in Sections 4 and 5 that multiple distinct pa-rameter values can achieve the global optimum of the objectivefunction, which also disproves Brown et al?s claim about thestrict convexity of the objective function.
Indeed, if a functionis strictly convex, it admits a unique globally optimum solution(Boyd and Vandenberghe, 2004, p. 151), so our experimentsprove by modus tollens that Brown et al?s claim is wrong.4624 Solution SpaceIn this section, we characterize the set of parametersthat achieve the maximum of the log-likelihood ofIBM Model 1.
As illustrated with the followingsimple example, it is relatively easy to establishcases where the set of optimal parameters t(f |e) isnot unique:e : short sentence f : phrase courteIf the above sentence pair represents the entiretraining data, Model 1 likelihood (ignoring NULLwords) is proportional to[t(phrase|short) + t(phrase|sentence)]?
[t(courte|short) + t(courte|sentence)]which can be maximized in infinitely many differ-ent ways.
For instance, setting t(phrase|sentence) =t(courte|short) = 1 yields the maximum likelihoodvalue with (0 + 1)(1 + 0) = 1, but the mostdivergent set of parameters (t(courte|sentence) =t(phrase|sentence) = 1) also reaches the same op-timum: (1+0)(0+1) = 1.
While this example maynot seem representative given the small size of thisdata, the laxity of Model 1 that we observe in thisexample also surfaces in real and much larger train-ing sets.
Indeed, it suffices that a given pair of targetwords (e1,e2) systematically co-occurs in the data(as with e1 = short e2 = sentence) to cause Model 1to fail to distinguish the two.3To characterize the solution space, we use the def-inition of IBM Model 1 log-likelihood from Eq.
1 inSection 2.1.
We ask whether distinct sets of parame-ters yield the same minimum negative log-likelihoodvalue of Eq.
1, i.e., whether we can find distinctmodels t(f |e) and t?
(f |e) so that:?f ,em?j=1logl?i=0t(fj |ei) =?f ,em?j=1logl?i=0t?
(fj |ei)Since the negative logarithm is strictly convex, the3Since e1 and e2 co-occur with exactly the same sourcewords, one can redistribute the probability mass betweent(f |e1) and t(f |e2) without affecting the log-likelihood.This is true if (a) the two distributions remain well-formed:?j t(fj |ei) = 1 for i ?
{1, 2}; (b) any adjustments to param-eters of fj leave each estimate t(fj |e1) + t(fj |e2) unchanged.above equation can be satisfied for optimal parame-ters only if the following holds for each f , e pair:l?i=0t(fj |ei) =l?i=0t?
(fj |ei), j = 1 .
.
.m (2)We can further simplify the above equation if we re-call that both t(f |e) and t?
(f |e) are maximum log-likelihood parameters, and noting it is generally easyto obtain one such set of parameters, e.g., by run-ning the EM algorithm until convergence.
Usingthese EM parameters (?)
in the right hand side ofthe equation, we replace these right hand sides withEM?s estimate t?
(fj |e).
This finally gives us the fol-lowing linear program (LP), which characterizes thesolution space of the maximum log-likelihood:4l?i=0t(fj |ei) = t?
(fj |e), j = 1 .
.
.m ?f , e (3)?ft(f |e) = 1, ?e (4)t(f |e) ?
0, ?e, f (5)The two conditions in Eq.
4-5 are added to ensurethat t(f |e) is well-formed.
To solve this LP, we usethe interior-point method of (Karmarkar, 1984).To measure the maximum divergence in optimalmodel parameters, we solve the LP of Eq.
3-5 byminimizing the linear objective function xTk?1xk,where xk is the column-vector representing all pa-rameters of the model t(f |e) currently optimized,and where xk?1 is a pre-existing set of maximumlog-likelihood parameters.
Starting with x0 definedusing EM parameters, we are effectively searchingfor the vector x1 with lowest cosine similarity to x0.We repeat with k > 1 until xk doesn?t reduce thecosine similarity with any of the previous parametervectors x0 .
.
.xk?1 (which generally happens withk = 3).54In general, an LP admits either (a) an infinity of solutions,when the system is underconstrained; (b) exactly one solution;(c) zero solutions, when it is ill-posed.
The latter case neveroccurs in our case, since the system was explicitly constructedto allow at least one solution: the parameter set returned by EM.5Note that this greedy procedure is not guaranteed to find thetwo points of the feasible region (a convex polytope) with mini-mum cosine similarity.
This problem is related to finding the di-ameter of this polytope, which is known to be NP-hard when thenumber of variables is unrestricted (Kaibel et al, 2002).
Never-theless, divergences found by this procedure are fairly substan-tial, as shown in Section 5.4630%10%20%30%40%50%60%70%80%90%100%0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1EM-LP-1EM-LP-8EM-LP-32EM-LP-128EM-rand-1EM-rand-8EM-rand-32EM-rand-128EM-rand-1KEM-rand-10Kcumulativepercentagecosine similarity [c]Figure 1: Percentage of target words for which we foundpairs of distributions t(f |e) and t?
(f |e) whose cosinesimilarity drops below a given threshold c (x-axis).5 ExperimentsIn this section, we show that the solution spacedefined by the LP of Eq.
3-5 can be fairly large.We demonstrate this with Bulgarian-English paral-lel data drawn from the JRC-AQUIS corpus (Stein-berger et al, 2006).
Our training data consists of upto 10,000 sentence pairs, which is representative ofthe amount of data used to train SMT systems forlanguage pairs that are relatively resource-poor.Figure 1 relies on two methods for determining towhat extent the model t(f |e) can vary while remain-ing optimal.
The EM-LP-N method consists of ap-plying the method described at the end of Section 4with N training sentence pairs.
For EM-rand-N , weinstead run EM 100 times (also onN sentence pairs)until convergence using different random startingpoints, and then use cosine similarity to compare theresulting models.6 Figure 1 shows some surprisingresults: First, EM-LP-128 finds that, for about 68%of target token types, cosine similarity between con-trastive models is equal to 0.
A cosine of zero es-sentially means that we can turn 1?s into 0?s with-out affecting log-likelihood, as in the short sentenceexample in Section 4.
Second, with a much largertraining set, EM-rand-10K finds a cosine similaritylower or equal to 0.5 for 30% of word types, whichis a large portion of the vocabulary.6While the first method is better at finding divergent optimalmodel parameters, it needs to construct large linear programsthat do not scale with large training sets (linear systems quicklyreach millions of entries, even with 128 sentence pairs).
We useEM-rand to assess the model space on larger training set, whilewe use EM-LP mainly to illustrate that divergence between op-timal models can be much larger than suggested by EM-rand.train coupled non-unique log-likall c. non-c. stdev unif1 100 100 100 - 2.9K -4.9K8 83.6 89.0 100 33.3 2.3K -2.3K32 77.8 81.8 100 17.9 874 74.4128 67.8 73.3 99.7 17.7 270 2721K 52.6 64.1 99.8 24.0 220 28110K 30.3 47.33 99.9 24.4 150 300Table 1: Results using 100 random initialization trials.In Table 1 we show additional statistics computedfrom the EM-rand-N experiments.
Every row repre-sents statistics for a given training set size (in num-ber of sent.
pairs, first column); the second columnshows the percent of target word types that alwaysco-occur with another word type (we term thesewords coupled); the third, fourth, and fifth columnsshow the percent of word types whose translationdistributions were found to be non-unique, wherewe define the non-unique types to be ones where theminimum cosine between any two different optimalparameter vectors was less than .95.
The percentof non-unique types are reported overall, as well asonly among coupled words (c.) and non-coupledwords (non-c.).
The last two columns show the stan-dard deviation in test set log-likelihood across differ-ent random trials, as well as the difference betweenthe log-likelihood of the uniformly initialized modeland the best model from the random trials.We can see that as the training set size increases,the percentage of words that have non-unique trans-lation probabilities goes down but is still very large.The coupled words almost always end up havingvarying translation parameters at convergence (morethan 99.5% of these words).
This also happens fora sizable portion of the non-coupled words, whichsuggests that there are additional patterns of co-occurrence that result in non-determinism.7 We alsocomputed the percent of word types that are coupledfor two more-realistically sized data-sets: we foundthat in a 1.6 million sent pair English-Bulgarian cor-pus 15% of Bulgarian word types were coupled andin a 1.9 million English-German corpus from theWMT workshop (Callison-Burch et al, 2010), 13%of the German word types were coupled.The log-likelihood statistics show that although7We did not perform such experiments for larger data-sets,since EM takes thousands of iterations to converge.464the standard deviation goes down with training setsize, it is still large at reasonable data sizes.
Inter-estingly, the uniformly initialized model performsworse for a very small data size, but it catches up andsurpasses the random models at data sizes greaterthan 100 sentence pairs.To further evaluate the impact of initialization forIBM Model 1, we report on a set of experimentslooking at alignment error rate achieved by differ-ent models.
We report the performance of Model 1,as well as the performance of the more competitiveHMM alignment model (Vogel et al, 1996), initial-ized from IBM-1 parameters.
The dataset for theseexperiments is English-French parallel data fromHansards.
The manually aligned data for evaluationconsists of 137 sentences (a development set from(Och and Ney, 2000)).We look at two different training set sizes, asmall set consisting of 1000 sentence pairs, anda reasonably-sized dataset containing 100,000 sen-tence pairs.
In each data size condition, we report onthe performance achieved by IBM-1, and the perfor-mance achieved by HMM initialized from the IBM-1 parameters.
For IBM Model 1 training, we eitherperform only 5 EM iterations (the standard settingin GIZA++), or run it to convergence.
For each ofthese two settings, we either start training from uni-form t(f |e) parameters, or random parameters.
Ta-ble 2 details the results of these experiments.Each row in the table represents an experimentalcondition, indicating the training data size (1K in thefirst four rows and 100K in the next four rows), thetype of initialization (uniform versus random) andthe number of iterations EM was run for Model 1 (5iterations versus unlimited (to convergence, denoted?)).
The numbers in the table are alignment errorrates, achieved at the end of Model 1 training, andat 5 iterations of HMM.
When random initializationis used, we run 20 random trials with different ini-tialization, and report the min, max, and mean AERachieved in each setting.From the table, we can draw several conclusions.First, in agreement with current practice using only5 iterations of Model 1 training results in better fi-nal performance of the HMM model (even thoughthe performance of Model 1 is higher when ran toconvergence).
Second, the minimum AER achievedby randomly initialized models was always smallersetting IBM-1 HMMmin mean max min mean max1K-unif-5 42.99 - - 22.53 - -1K-rand-5 42.90 44.07 45.08 22.26 22.99 24.011K-unif-?
42.10 - - 28.09 - -1K-rand-?
41.72 42.61 43.63 27.88 28.47 28.89100K-unif-5 28.98 - - 12.68 - -100K-rand-5 28.63 28.99 30.13 12.25 12.62 12.89100K-unif-?
28.18 - - 16.84 - -100K-rand-?
27.95 28.22 30.13 16.66 16.78 16.85Table 2: AER results for Model 1 and HMM using uni-form and random initialization.
We do not report meanand max for uniform, since they are identical to min.than the AER of the uniform-initialized models.
Insome cases, even the mean of the random trials wasbetter than the corresponding uniform model.
Inter-estingly, the advantage of the randomly initializedmodels in AER does not seem to diminish with in-creased training data size like their advantage in testset perplexity.6 ConclusionsThrough theoretical analysis and three sets of ex-periments, we showed that IBM Model 1 is notstrictly convex and that there is large variance inthe set of optimal parameter values.
This varianceimpacts a significant fraction of word types and re-sults in variance in predictive performance of trainedmodels, as measured by test set log-likelihood andword-alignment error rate.
The magnitude of thisnon-uniqueness further supports the development ofmodels that can use information beyond simple co-occurrence, such as positional and fertility informa-tion like higher order alignment models, as well asmodels that look beyond the surface form of a wordand reason about morphological or other properties(Berg-Kirkpatrick et al, 2010).In future work we would like to study the im-pact of non-determinism on higher order models inthe standard alignment model sequence and to gainmore insight into the impact of finer-grained featuresin alignment.AcknowledgementsWe thank Chris Quirk and Galen Andrew for valu-able discussions and suggestions.465ReferencesTaylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,John DeNero, and Dan Klein.
2010.
Painless unsu-pervised learning with features.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics.
Association for ComputationalLinguistics.Adam Berger and John Lafferty.
1999.
Information re-trieval as statistical translation.
In Proceedings of the1999 ACM SIGIR Conference on Research and Devel-opment in Information Retrieval.Stephen Boyd and Lieven Vandenberghe.
2004.
ConvexOptimization.
Cambridge University Press.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert.
L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19:263?311.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, and Omar Zaidan, editors.
2010.
Pro-ceedings of the Joint Fifth Workshop on Statistical Ma-chine Translation and MetricsMATR.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the emalgorithm.
Journal of the royal statistical society, se-ries B, 39(1).Pinar Duygulu, Kobus Barnard, Nando de Freitas,P.
Duygulu, K. Barnard, and David Forsyth.
2002.Object recognition as machine translation: Learning alexicon for a fixed image vocabulary.
In Proceedingsof ECCV.Volker Kaibel, Marc E. Pfetsch, and TU Berlin.
2002.Some algorithmic problems in polytope theory.
InDagstuhl Seminars, pages 23?47.N.
Karmarkar.
1984.
A new polynomial-time algorithmfor linear programming.
Combinatorica, 4:373?395,December.Franz Josef Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Tomaz Erjavec, and Dan Tufis.
2006.The JRC-acquis: A multilingual aligned parallel cor-pus with 20+ languages.
In Proceedings of the 5thInternational Conference on Language Resources andEvaluation (LREC).Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of the 16th Int.
Conf.
onComputational Linguistics (COLING).
Association forComputational Linguistics.Appendix A: Convex functions and convexoptimization problemsWe denote the domain of a function f by dom f .Definition A function f : Rn ?
R is convex if and onlyif dom f is a convex set and for all x, y ?
dom f and?
?
0, ?
?
1:f(?x+ (1?
?
)y) ?
?f(x) + (1?
?
)f(y) (6)Definition A function f is strictly convex iff dom f is aconvex set and for all x 6= y ?
dom f and ?
> 0, ?
< 1:f(?x+ (1?
?
)y) < ?f(x) + (1?
?
)f(y) (7)Definition A convex optimization problem is defined by:min f0(x)subject tofi(x) ?
0, i = 1 .
.
.
kaTj x = bj , j = 1 .
.
.
lWhere the functions f0 to fk are convex and the equal-ity constraints are affine.It can be shown that the feasible set (the set of pointsthat satisfy the constraints) is convex and that any localoptimum for the problem is a global optimum.
If f0is strictly convex then any local optimum is the uniqueglobal optimum.466
