Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1933?1942,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsIdentifying Manipulated Offerings on Review PortalsJiwei LiSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAjiweil@cs.cmu.eduMyle Ott Claire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853, USAmyleott,cardie@cs.cornell.eduAbstractRecent work has developed supervised meth-ods for detecting deceptive opinion spam?fake reviews written to sound authentic anddeliberately mislead readers.
And whereaspast work has focused on identifying individ-ual fake reviews, this paper aims to identifyofferings (e.g., hotels) that contain fake re-views.
We introduce a semi-supervised man-ifold ranking algorithm for this task, whichrelies on a small set of labeled individual re-views for training.
Then, in the absence ofgold standard labels (at an offering level),we introduce a novel evaluation procedurethat ranks artificial instances of real offer-ings, where each artificial offering contains aknown number of injected deceptive reviews.Experiments on a novel dataset of hotel re-views show that the proposed method outper-forms state-of-art learning baselines.1 IntroductionConsumers increasingly rely on user-generatedonline reviews when making purchase deci-sions (Cone, 2011; Ipsos, 2012).
Unfortunately,the ease of posting content to the Web, potentiallyanonymously, combined with the public?s trust andgrowing reliance on opinions and other informationfound online, create opportunities and incentives forunscrupulous businesses to post deceptive opinionspam?fraudulent or fictitious reviews that aredeliberately written to sound authentic, in order todeceive the reader (Ott et al2011).Unlike other kinds of spam, such asWeb (Martinez-Romo and Araujo, 2009; Castilloet al2006) and e-mail spam (Chirita et al2005),recent work has found that deceptive opinion spamis neither easily ignored nor easily identified byhuman readers (Ott et al2011).
Accordingly, thereis growing interest in developing automatic, usuallylearning-based, methods to help users identifydeceptive opinion spam (see Section 2).
Evenin fully-supervised settings, however, automaticmethods are imperfect at identifying individualdeceptive reviews, and erroneously labeling genuinereviews as deceptive may frustrate and alienatehonest reviewers.An alternative approach, not yet considered inprevious work, is to instead identify those prod-uct or service offerings where fake reviews appearwith high probability.
For example, a hotel managermay post fake positive reviews to promote their ownhotel, or fake negative reviews to demote a com-petitor?s hotel.
In both cases, rather than identify-ing these deceptive reviews individually, it may bepreferable to identify the manipulated offering (i.e.,the hotel) so that review portal operators, such asTripAdvisor or Yelp, can further investigate the sit-uation without alienating users.1Accordingly, this paper addresses the novel taskof identifying manipulated offerings, which weframe as a ranking problem, where the goal is to rankofferings by the proportion of their reviews that arebelieved to be deceptive.
We propose a novel three-layer graph model, based on manifold ranking (Zhouet al2003a; 2003b), to jointly model deceptive lan-guage at the offering-, review- and term-level.
Inparticular, rather than treating reviews within thesame offering as independent units, there is a rein-forcing relationship between offerings and reviews.1Manipulating online reviews may also have legal conse-quences.
For example, the Federal Trade Commission (FTC)has updated their guidelines on the use of endorsements andtestimonials in advertising to suggest that posting deceptive re-views may be unlawful in the United States (FTC, 2009).1933Figure 1: Mutual Reinforcement Graph Model for HotelRanking using the Manifold-Ranking MethodOur manifold ranking approach is semi-supervised in that it requires no supervisoryinformation at the offering level; rather, it requiresonly a small amount of labeled data at a reviewlevel.
Intuitively, and as depicted in Figure 1 forhotel offerings, we represent hotels, reviews andterms as nodes in a graph, where each hotel isconnected to its reviews, and each review, in turn, isconnected to the terms used within it.
The influenceof labeled data is propagated along the graph tounlabeled data, such that a hotel is considered moredeceptive if it is heavily linked with other deceptivereviews, and a review, in turn, is more deceptive if itis generated by a deceptive hotel.The success of our semi-supervised approach fur-ther depends on the ability to learn patterns of truth-ful and deceptive reviews that generalize across re-views of different offerings.
This is challenging, be-cause reviews often contain offering-specific vocab-ulary.
For example, reviews of hotels in Los Angelesare more likely to include keywords such as ?beach?,?sea?, ?sunshine?
or ?LA?, while reviews of Juneauhotels may contain ?glacier?, ?Juneau?, ?bear?
or?aurora borealis.?
A hotel review might also men-tion the hotel?s restaurant or bar by name.Unfortunately, it is unclear how important (ordetrimental) offering-specific features are when de-ciding whether a review is fake.
Accordingly, wepropose a dimensionality-reduction approach, basedon Latent Dirichlet Allocation (LDA) (Blei et al2003), to obtain a vector representation of reviewsfor the ranking algorithm that generalizes across re-views of different offerings.
Specifically, we trainan LDA-based topic model to view each review as amixture of aspect-, city-, hotel- and review-specifictopics (see Section 6).
We then reduce the dimen-sionality of our data (i.e., labeled and unlabeled re-views) by replacing each review term vector with avector that corresponds to its term distribution overjust its aspect-specific topics, i.e., excluding city-,hotel- and review-specific topics.
We find that, com-pared to models trained either on the full vocabulary,or trained on standard LDA document-topic vectors,this representation allows our models to generalizebetter across reviews of different offerings.We evaluate our approach on the task of identi-fying (ranking) manipulated hotels.
In particular, inthe absence of gold standard offering-level labels,we introduce a novel evaluation procedure for thistask, in which we rank numerous versions of eachhotel, where each hotel version contains a differ-ent number of injected, known deceptive reviews.Thus, we expect hotel versions with larger propor-tions of deceptive reviews to be ranked higher thanthose with smaller proportions.For labeled training data, we use the Ott etal.
(2011) dataset of 800 positive (5-star) reviews of20 Chicago hotels (400 deceptive and 400 truthful).For evaluation, we construct a new FOUR-CITIESdataset, containing 40 deceptive and 40 truthful re-views for each of eight hotels in four different cities(640 reviews total), following the procedure out-lined in Ott et al(2011).
We find that our manifoldranking approach outperforms several state-of-the-art learning baselines on this task, including trans-ductive Support Vector Regression.
We addition-ally apply our approach to a large-scale collectionof real-world reviews from TripAdvisor and explorethe resulting ranking.In the sections below, we discuss related work(Section 2) and describe the datasets used in thiswork (Section 3), the dimensionality-reduction ap-proach for representing reviews (Section 4), and thesemi-supervised manifold ranking approach (Sec-tion 5).
We then evaluate the methods quantitatively(Sections 6 and 7) and qualitatively (Section 8).2 Related WorkA number of recent approaches have focused onidentifying individual fake reviews or users who post1934fake reviews.
For example, Jindal and Liu (2008)train machine learning classifiers to identify dupli-cate (or near duplicate) reviews.
Yoo and Gretzel(2009) gathered 40 truthful and 42 deceptive hotelreviews and manually compare the psychologicallyrelevant linguistic differences between them.
Lim etal.
(2010) propose an approach based on abnormaluser behavior to predict spam users, without usingany textual features.
Ott et al(2011) solicit decep-tive reviews from workers on Amazon MechanicalTurk, and built a dataset containing 400 deceptiveand 400 truthful reviews, which they use to trainand evaluate supervised SVM classifiers.
Ott et al(2012) expand upon this work to estimate preva-lences of deception in a review community.
Mukher-jee et al(2012) study spam produced by groups offake reviewers.
Li et al(2013) use topic modelsto detect differences between deceptive and truthfultopic-word distributions.
In contrast, in this work weaim to identify fake reviews at an offering level.2LDA Topic Models.
LDA topic models (Blei etal, 2003) have been employed for many NLP tasksin recent years.
Here, we build on earlier workthat uses topic models to (a) separate backgroundinformation from information discussing the vari-ous ?aspects?
of products (e.g., Chemudugunta etal.
(2007)) and (b) identify different levels of infor-mation (e.g., user-specific, location-specific, time-specific) (Ramage et al 2009).Manifold Ranking Algorithm.
The manifold-ranking method (Zhou et al2003a; Zhou et al2003b) is a mutual reinforcement ranking approachinitially proposed to rank data points along their un-derlying manifold structure.
It has been widely usedin many different ranking applications, such as sum-marization (Wan et al2007; Wan and Yang, 2007).3 DatasetIn this paper, we train all of our models using theCHICAGO dataset of Ott et al2011), which contains20 deceptive and 20 truthful reviews from each of 20Chicago hotels (800 reviews total).
This dataset is2Approaches for identifying individual fake reviews may beapplied to our task, for example, by averaging the review-levelpredictions for an offering.
This averaging approach is one ofour baselines in Section 7.City HotelsChicago W Chicago, Palomar ChicagoNew York Hotel Pennsylvania, Waldorf AstoriaLos AngelesSheraton Gateway,The Westin Los Angeles AirportHoustonMagnolia Hotel,Crowne Plaza Houston River OaksTable 1: Details of our FOUR-CITIES evaluation data.unique in that it contains known (gold standard) de-ceptive reviews, solicited through Amazon Mechan-ical Turk, and is publicly-available.3Unfortunately, the CHICAGO dataset is limited,both in size (800 reviews) and scope, in that it onlycontains reviews of hotels in one city: Chicago.Accordingly, in order to perform a more realisticevaluation for our task, we construct a new dataset,FOUR-CITIES, that contains 40 deceptive and 40truthful reviews from each of eight hotels in four dif-ferent cities (640 reviews total).We build the FOUR-CITIES dataset using the sameprocedure as Ott et al2011), by creating and di-viding 320 Mechanical Turk jobs, called Human-Intelligence Tasks (HITs), evenly across eight of themost popular hotels in our four chosen cities (see Ta-ble 1).
Each HIT presents a worker with the name ofa hotel and a link to the hotel?s website.
Workers areasked to imagine that they work for the marketingdepartment of the hotel and that their boss has askedthem to write a fake positive review, as if they werea customer, to be posted on a travel review website.Each worker is allowed to submit a single review,and is paid $1 for an acceptable submission.Finally, we augment our deceptive FOUR-CITIESreviews with a matching set of truthful reviews fromTripAdvisor by randomly sampling 40 positive (5-star) reviews for each of the eight chosen hotels.While we cannot know for sure that the sampled re-views are truthful, previous work has suggested thatrates of deception among popular hotels is likely tobe low (Jindal and Liu, 2008; Lim et al2010).4 Topic Models for DimensionalityReductionAs mentioned in the introduction, we want to learnpatterns of truthful and deceptive reviews that apply3We use the dataset available at: http://www.cs.cornell.edu/?myleott/op_spam.1935Figure 2: Graphical illustration of the RLDA topic model.across hotels in different locations.
This is challeng-ing, however, because hotel reviews often containspecific information about the hotel or city, and itis unclear whether these features will generalize toreviews of other hotels.We therefore investigate an LDA-baseddimensionality-reduction approach (RLDA) toderive effective vector representations of reviews.Specifically, we model each document as a bag ofwords, generated from a mixture of: (a) ?aspect?topics (that discuss various dimensions of theoffering); (b) city-specific topics; (c) hotel-specifictopics; (d) review-specific topics;4 and (e) a back-ground topic.
We use this model to reduce thedimensionality of the review representation in ourtraining and test sets, by replacing each review?sterm vector with a vector corresponding to thedistribution over only the aspect-based topics, i.e.,we exclude city, hotel and review-specific topics, aswell as the background topic.Below we present specific details of our model(Sections 4.1 and 4.2).
The effectiveness of ourdimensionality-reduction approach will be directlyevaluated in Section 6, by comparing the perfor-mance of various classifiers trained either on the fullvocabulary, or on our reduced feature representation.4.1 RLDA Model DetailsThe plate diagram and generative story for ourmodel are given in Figures 2 and 3, respectively.Our model has a similar general structure to stan-dard LDA, but with additional machinery to handledifferent levels of information.
In particular, in or-der to model K aspects in a collection of R reviews,4These will be terms used in just a small number of reviews.?
Draw ?B ?
Dir(?)?
For each aspect z = 1, 2, ...,K: draw ?z ?
Dir(?)?
For each city c = 1, 2, ..., C: draw ?c ?
Dir(?)?
For each hotel h = 1, 2, ..., H: draw ?h ?
Dir(?)?
For each review r:?
Draw pir ?
Dir(?)?
Draw ?r ?
Dir(?)?
Draw ?r ?
Dir(?)?
For each word w in d:?
Draw yw ?Multi(pir)?
If yw = 0:?
Draw zw ?Multi(?)?
Draw w ?Multi(?zw )?
If yw = 1: draw w ?Multi(?B)?
If yw = 2: draw w ?Multi(?d)?
If yw = 3: draw w ?Multi(?h)?
If yw = 4: draw w ?Multi(?c)Figure 3: Generative story for the RLDA topic model.of H hotels, in C cities, we first draw multinomialword distributions corresponding to: the backgroundtopic, ?B; aspect topics, ?k for k ?
[1,K]; review-specific topics, ?r for r ?
[1, R]; hotel-specific top-ics, ?h for h ?
[1, H]; and city-specific topics, ?cfor c ?
[1, C].
Then, for each word w in reviewR, we sample a switch variable, y ?
[0, 4], indicat-ing whether w comes from one of the aspect topics(y = 0), or the background topic (y = 1), review-specific topic (y = 2), hotel-specific topic (y = 3)or city-specific topic (y = 4).
If the word comesfrom one of the aspect topics, then we further sam-ple the specific aspect topic, zw ?
[1,K].
Finally,we generate the word, w, from the corresponding ?.4.2 Inference for RLDAGiven the review collection, our goal is to find themost likely assignment yw (and zw if yw = 0) foreach word, w, in each review.
We perform infer-ence using Gibbs sampling.
It is relatively straight-forward to derive Gibbs sampling equations that al-low joint sampling of the zw and yw latent variablesfor each word token w:P (yw = 0, Zw = k) =Nar,?w + ?Nr,?w + 5?
?Ckr,?w + ?
?k Ckr,?w +K?
?Ewk + ?
?w Ewk + V ?,P (yw = m,m = 1, 2, 3, 4) =Nmr,?w + ?Nr,?w + 5?
?Ewm + ?
?w Ewm + V ?,Note that the subscript ?w indicates that thecount for word token w is excluded.
Also, Nr1936denotes the number of words in review r andNar,?w, N1r,?w, N2r,?w, N3r,?w, N4r,?w are the number ofwords in review r assigned to the aspect, background,review-specific, hotel-specific and city-specific topics, re-spectively, excluding the current word.
Ckr,?w denotesthe number of words in review r assigned to aspect topick.
Ewk , Ew1 , Ew2 , Ew3 , Ew4 denote the number of times thatthe word w is assigned to aspect k, the background topic,review-specific topic r, hotel-specific topic h, and city-specific topic c, respectively.
We set hyperparameter ?to 1, ?
to 0.5, ?
to 0.01.
We run 200 iterations of Gibbssampling until the topic distribution stabilizes.
After eachiteration in Gibbs sampling, we obtain:piir =N ir + ?
?iNir + 5?
?kr =Ckr + ?
?k Ckr +K?
?wz =Ewz + ?
?w Ewz + V ?
?wm =Ewm + ?
?w Ewm + V ?
(1)Finally, at the end of Gibbs sampling, we filter outbackground, document-specific, hotel-specific andcity-specific information, by replacing each docu-ment?s term vector with a 1?K aspect-topic vector,~Gr = ?
?1r , ?2r , ?
?
?
, ?Kr ?.5 Manifold Ranking for HotelsIn this section, we describe our ranking algorithm ?based on manifold ranking (Zhou et al2003a; Zhouet al2003b) ?
that tries to jointly model deceptivelanguage at the hotel-, review- and term-level.5.1 Graph ConstructionWe use a three-layer (hotel layer, review layer andterm layer) mutual reinforcement model (see Fig-ure 1).
Formally, we represent our three-layer graphas G = ?VH , VR, VT , EHR, ERR, ERT , ETT ?,where VH = {Hi}i=NHi=1 , VR = {R}i=HRi=1 andVT = {Ti}i=Vi=1 correspond to the set of hotels, re-views and terms respectively.
EHR, ERR and ERTrespectively denote the edges between hotels and re-views, reviews and reviews and reviews and terms.Each edge is associated with a weight that denotesthe similarity between two nodes.Let sim(Hi, Rj), where Hi ?
VH and Rj ?
VR,denote the edge weight between hotelHi and reviewRj , calculated as follows:sim(Hi, Rj) ={1 if Ri ?
Hj0 if Ri 6?
Hj(2)Then we get row normalized matrices DHR ?RNH?NR and DRH ?
RNR?NH as follows:DHR(i, j) =sim(Hi, Rj)?i?
sim(Hi?
, Rj)DRH(i, j) =sim(Hi, Rj)?j?
sim(Hi, Rj?
)(3)As described in Section 4.2, each review is rep-resented with a 1 ?
K aspect vector Gr after fil-tering undesired information.
The edge weight be-tween two reviews is then the cosine similarity,sim(Ri, Rj), between two reviews and can be cal-culated as follows:sim(Ri, Rj) =?t=Kt=1 Gti ?Gtj?
?t=Kt=1 Gt2i ??
?t=Kt=1 Gt2j(4)Since the normalization process will make thereview-to-review relation matrix asymmetric, weadopt the following strategy: let P denote the sim-ilarity matrix between reviews, where P (i, j) =sim(Ri, Rj) and M denotes the diagonal matrixwith (i,i)-element equal to the sum of the ith rowof SIM .
The normalized matrix between reviewsDRR ?
RNR?NR is calculated as follows:DRR =M?
12 ?
P ?M?12 (5)sim(Ri, wj) denotes the similarity between re-view Ri and term wj and is the conditional prob-ability of word wj given review Ri.
If wj ?
Rj ,sim(Ri, wj) is calculated according to Eq.
(6) byintegrating out latent parameters ?
and pi.
Else ifwj 6?
Rj , sim(Ri, wj) = 0.sim(Ri, wj) =k=K?k=1p(z = k|ri)?
p(wj |z = k)+?t?
{B,h,c,d}p(wj |yi = t)p(yi = t|ri)= pi(a)dk=K?k=1?zd ?
?
(wj)z +?t?
{B,h,c,d}pi(t)d ?
(wj)t(6)Similar to Eq.
(3), we further get the normalized ma-trix DRT ?
RHR?V and DTR ?
RV?HR .Similarity between terms sim(wi, wj) is given bythe WordNet path-similarity,5 normalized to createthe matrix DV V .5Path-similarity is based on the shortest path that con-nects the senses in the ?is-a?
(hypernym/hyponym) tax-onomy.
See http://nltk.googlecode.com/svn/trunk/doc/howto/wordnet.html.1937Input: The hotel set VD, review set VR, termset VT , normalized transition probability matrixDHR, DRR, DRH , DRT , DTT , DTR.Output: the ranking vectors SR, SH , ST .Begin:1.
Initialization: set the score labeled reviews to+1 or ?1 and other unlabeled reviews 0: S0R =[+1, ...,+1,?1, ...,?1, 0, ..., 0].
Set S0H andS0T to 0.
Normalize the score vector.2.
update SkR, SkH and SkT according to Eq.
(7).3. normalize SkR, SkH and SkT .4. fix the score of labeled reviews to +1 and ?1.Go to step (2) until convergence.Figure 4: Semi-Supervised Reinforcement Ranking.5.2 Reinforcement Ranking Based on theManifold MethodBased on the set of labeled reviews, nodes for truth-ful reviews (positive) are initialized with a highscore (1) and nodes for deceptive reviews, a lowscore (-1).
Given the weighted graph, our task isto assign a score to the each hotel, each term, andthe remaining unlabeled reviews.
Let SH , SR andST denote the ranking scores of hotels, reviews andterms, which are updated during each iteration asfollows until convergence6:????
?Sk+1H = DHR ?
SkRSk+1R = 1DRR ?
SkR + 2DRH ?
SkH + 3DRT ?
SktSk+1T = 4DTT ?
SkT + 5DTR ?
SkR(7)where 1 + 2 + 3 = 1 and 4 + 5 = 1.
(The scoreof labeled reviews will be fixed to +1 or ?1.
)6 Learning Generalizable ClassifiersIn Section 4, we introduced RLDA to filter outreview-, hotel- and city-specific information fromour vector-based review representation.
Here, wewill directly evaluate the effectiveness of RLDAby comparing the performance of binary deceptivevs.
truthful classifiers trained on three feature sets:(a) the full vocabulary, encoded as unigrams andbigrams (N-GRAMS); (b) a reduced-dimensionalityfeature space, based on standard LDA (Blei etal, 2003); and (c) a reduced-dimensionality feature6Convergence is achieved if the difference between rankingscores in two consecutive iterations is less than 0.00001.space, based on our proposed revised LDA approach(RLDA).We compare two kinds of classifiers, which aretrained on only the labeled CHICAGO dataset andtested on the FOUR-CITIES dataset.
First, we useSVMlight (Joachims, 1999) to train linear SVM clas-sifiers, which have been shown to perform well inrelated work (Ott et al2011).
Second, we train atwo-layer manifold classifier, which is a simplifiedversion of the model presented in Section 5.
In thismodel, the graph consists of only review and termlayers, and the score of a labeled review is fixed to1 or -1 in each iteration.
After convergence, reviewswith scores greater than 0 are classified as truthful,and less than 0 as deceptive.Results and Discussion The results are shown inTable 2 and show the average accuracy and preci-sion/recall w.r.t.
the truthful (positive) class.
We findthat SVM and MANIFOLD are comparable in all sixconditions, and not surprisingly, perform best whenevaluated on reviews from the two Chicago hotels inour FOUR-CITIES data.
However, the N-GRAM andLDA feature sets perform much worse than RDLAwhen evaluation is performed on reviews from theother three (non-Chicago) cities.
This confirms thatclassifiers trained on n-gram features overfit to thetraining data (CHICAGO) and do not generalize wellto reviews from other cities.
In addition, the stan-dard LDA-based method for dimensionality reduc-tion is not sufficient for our specific task.7 Identifying Manipulated HotelsIn this section, we evaluate the performance of ourmanifold ranking approach (see Section 5) on thetask of identifying manipulated hotels.Baselines.
We consider several baseline rankingapproaches to compare to our manifold ranking ap-proach.
Like the manifold ranking approach, thebaselines also employ both the CHICAGO dataset (la-beled) and FOUR-CITIES dataset (without labels).7For fair comparison, we use identical processingtechniques for each approach.
Topic number is set7While we have not investigated the effects of unlabeled datain detail, providing additional unlabeled data (beyond the testset) boosts the manifold ranking performances reported belowby 1-2%.1938city feature setSVM ManifoldAccuracy Precision Recall Accuracy Precision RecallChicagoN-GRAMS 0.831 0.844 0.818 0.835 0.844 0.825LDA 0.833 0.846 0.819 0.817 0.832 0.802RLDA 0.830 0.838 0.822 0.841 0.819 0.863Non-ChicagoN-GRAMS 0.728 0.744 0.714 0.733 0.738 0.727LDA 0.714 0.696 0.732 0.728 0.715 0.741RLDA 0.791 0.799 0.780 0.801 0.787 0.815Table 2: Binary classification results showing that n-gram features overfit to the CHICAGO training data.
Resultscorrespond to evaluation on reviews for the two Chicago hotels from FOUR-CITIES and non-Chicago FOUR-CITIESreviews (six hotels).to five for all topic-model-based approaches.
Eachbaseline makes review-level predictions and thenranks each hotel by the average of those predictions.?
Review-SVR: Uses linear Tranductive SupportVector Regression with unigram and bigram fea-tures, similar to Ott et al(2011).?
Review-SVR+LDA (R): Similar to REVIEW-SVR but uses our revised LDA (RLDA) topicmodel for dimensionality reduction (R).?
Two-Layer Manifold (S): A simplified version ofour model where the hotel-level is removed fromthe graph.
Dimensionality reduction is performedusing standard LDA (S).?
Two-Layer Manifold (R): Similar to TWO-LAYER MANIFOLD (S) but uses the revised LDA(RLDA) model for dimensionality reduction.?
Three-layer Manifold (tf-idf): Our three-layermanifold ranking model, except with each reviewrepresented as a TF-IDF term vector.
Review sim-ilarity is calculated based on the cosine similaritybetween these vectors.Evaluation Method.
To evaluate ranking perfor-mance in the absence of a gold standard set of ma-nipulated hotels, we rearrange the FOUR-CITIES testset of 40 truthful and 40 deceptive reviews for eachof eight hotels: we create 41 versions of each hotel,where each hotel version contains a different num-ber of injected deceptive reviews, ranging from 0 to40.
For example, the first version of a hotel will have40 truthful and 0 deceptive reviews, the second ver-sion 39 truthful and 1 deceptive, and the 41st ver-sion 0 truthful and 40 deceptive.
In total, we gen-erate 41 ?
8 = 328 versions of hotel reviews.
Weexpect versions with larger proportions of deceptivereviews to receive lower scores by the ranking mod-els (i.e., they are ranked higher/more deceptive).Metrics.
To qualitatively evaluate the ranking re-sults, we use the Normalized Discounted Cumula-tive Gain (NDCG), which is commonly used to eval-uate retrieval algorithms with respect to an idealrelevance-based ranking.
In particular, NDCG re-wards rankings with the most relevant results at thetop positions (Liu, 2009), which is also our objec-tive, namely, to rank versions that have higher pro-portions of deceptive reviews nearer to the top.Let R(m) denote the relevance score of mthranked hotel version.
Then, NDCGN is defined as:NDCGN =1IDCGNm=N?m=12R(m) ?
1log2(1 +m)(8)where IDCGN refers to discounted cumulative gain(DCG) of the ideal ranking of the top N results.
Wedefine the ideal ranking according to the proportionof deceptive reviews in different versions, and re-port NDCG scores for theNth ranked hotel versions(N = 8 to 321), at intervals of 8 (to account for tiesamong the eight hotels).Results and Discussion.
NDCG results are shownin Figure 5.
We observe that our approach (using2, 5 or 10 topics) generally outperforms the otherapproaches.
In particular, approaches that use ourRLDA text representation (OUR APPROACH, TWO-LAYER MANIFOLD (R), and REVIEW-SVR+LDA(R)), which tries to remove city- and hotel-specificinformation, perform better than those that usethe full vocabulary (REVIEW-SVR, TWO-LAYERMANIFOLD (S), and THREE-LAYER MANIFOLD(TF-IDF)).
This further confirms that our RLDAdimensionality reduction technique allows models,1939Figure 5: NDCGN results for different approaches.
Kindicates the number of topics.trained on limited data, to generalize to reviews ofdifferent hotels and in different locations.
We alsofind that approaches that model a reinforcing rela-tionship between hotels and their reviews are bet-ter than approaches that model reviews as inde-pendent units, e.g., TWO-LAYER MANIFOLD (R)vs. REVIEW-SVR+LDA and TWO-LAYER MANI-FOLD (S) vs. REVIEW-SVR.
This confirms our in-tuition that a hotel is more deceptive if it is con-nected with many deceptive reviews, and, in turn,a review is more deceptive if from a deceptive hotel.8 Qualitative EvaluationWe now present qualitative evaluations for theRLDA topic model and the manifold ranking model.Topic Quality.
Table 3 gives the top words forfour aspect topics and four city-specific topics in theRLDA topic model; Table 4 gives the highest andlowest ranking term weights in our three-layer man-ifold model.
By comparing the first row of topics inTable 3, corresponding to aspect topics, to the topwords in Table 4, we observe that the learned top-ics relate to truthful and deceptive classes.
For ex-ample, Topics 1 and 4 share many terms with thetop truthful terms in the manifold model, e.g., spa-tial terms, such as location, floor and block,and punctuation, such as (, ), and $.
Similarly,Topics 2 and 7 share many terms with the top de-ceptive terms in the manifold model, e.g., hotel,husband, wife, amazing, experience andrecommend.
This makes sense, since topic modelshave been shown to produce discriminative topics onTopic1 Topic2 Topic4 Topic7location hotel ( hotel$ stay room servicewalk staff ) husbandnight restaurant park amazingblock friendly bed willfloor room night weekendquiet recommend shower friendlynice love view travellobby excellent minute experiencebreakfast wife pillow friendNYC Chicago LA HoustonYork Chicago los Houstonny Michigan Angeles downtowntime mile la Texassquare tower lax cabnyc Illinois shuttling Westsidestreet avenue hollywood centerempire Rogers plane NorthwestChinatown river morning ststation Burnham California museumWall Goodman downtown missionTable 3: Top words in topics extracted from RLDA topicmodel (see Section 4).
The top row presents topic wordsfrom four aspect topics (K = 10) and the bottom rowpresents top words from four city-specific topics.Deceptive Truthfulterm score term scoremy -1.063 $ 0.964visit -0.944 location 0.922we -0.882 ( 0.884hotel -0.863 ) 0.884husband -0.828 bathroom 0.842family -0.824 floor 0.810amazing -0.782 breakfast 0.784experience -0.740 bar 0.762recommend -0.732 block 0.747wife -0.680 small 0.721relax -0.678 but 0.720vacation -0.651 walk 0.707will -0.651 lobby 0.707friendly -0.646 quiet 0.684Table 4: Term scores from our ranking algorithm.this data in previous work (Li et al 2013).With respect to the second row in Table 4, con-taining top words from city-specific topics, we ob-serve that each topic does contain primarily city-specific information.
This helps to explain why re-moving terms associated with these topics resultedin a better vector representation for reviews.1940Figure 6: Hotel Ranking Distribution on TripAdvisorFigure 7: Proportion of Singletons vs. Hotel Ranking.Real-world Evaluation.
Finally, we apply ourranking model to a large-scale collection of real-world reviews from TripAdvisor.
We crawl 878,561reviews from 3,945 hotels in 25 US cities from Tri-pAdvisor excluding all non-5-star reviews and re-moving hotels with fewer than 100 reviews.
In theend, we collect 244,810 reviews from 838 hotels.We apply our manifold ranking model and rankall 838 hotels.
First, we present a histogram of theresulting manifold ranking scores in Figure 6.
Weobserve that the distribution reaches a peak around0.04, which in our quantitative evaluation (Sec-tion 7) corresponded to a hotel with 34 truthful and6 deceptive reviews.
These results suggest that themajority of reviews in TripAdvisor are truthful, inline with previous findings by Ott et al(2011).Next, we note that previous work has hypothe-sized that deceptive reviews are more likely to beposted by first-time review writers, or singleton re-viewers (Ott et al2011; Wu et al2011).
Accord-ingly, if this hypothesis were valid, then manipu-lated hotels would have an above-average proportionof singleton reviews.
Figure 7 shows a histogramof the average proportion of singleton reviews, asa function of the ranking scores produced by ourmodel.
Noting that lower scores correspond to ahigher predicted proportion of deceptive reviews, weobserve that hotels that are ranked as being more de-ceptive by our model have much higher proportionsof singleton reviews, on average, compared to hotelsranked as less deceptive.9 ConclusionWe study the problem of identifying manipulated of-ferings on review portals and propose a novel three-layer graph model, based on manifold ranking forranking offerings based on the proportion of reviewsexpected to be instances of deceptive opinion spam.Experimental results illustrate the effectiveness ofour model over several learning-based baselines.AcknowledgmentsThis work was supported in part by National Sci-ence Foundation Grant BCS-0904822, a DARPADeft grant, as well as a gift from Google.
We alsothank the EMNLP reviewers for their helpful com-ments and advice.ReferencesDavid Blei, Ng Andrew and Michael Jordan.
LatentDirichlet alcation.
2003.
In Journal of MachineLearning Research.Carlos Castillo, Debora Donato, Luca Becchetti, PaoloBoldi, Stefano Leonardi, Massimo Santini and Sebas-tiano Vigna.
A reference collection for web spam.
InACM Sigir Forum.
2006.Paul-Alexandru Chirita, Jrg Diederich and Wolfgang Ne-jdl.
MailRank: using ranking for spam detection.
InProceedings of the 14th ACM international conferenceon Information and knowledge management.
2005.Cone.
2011 Online Influence Trend Tracker.http://www.coneinc.com/negative-reviews-online-reverse-purchase-decisions.
August.Yajuan Duan, Zhumin Chen, Furu Wei, Ming Zhou andHeung-Yeung Shum.
Twitter Topic Summarization byRanking Tweets Using Social Influence and ContentQuality.
In Proceedings of 24th International Confer-ence on Computational Linguistics 2012.Federal Trade Commission.
Guides Concerning Use ofEndorsements and Testimonials in Advertising.
InFTC 16 CFR Part 255.
2009.1941Socialogue: Five Stars?
Thumbs Up?
A+ orJust Average?
URL:http://www.ipsos-na.com/news-polls/pressrelease.aspx?id=5929gNitin Jindal, and Bing Liu.
Opinion spam and analysis.
InProceedings of the 2008 International Conference onWeb Search and Data Mining.
2008.Nitin Jindal, Bing Liu and Ee-Peng Lim.
Finding UnusualReview Patterns Using Unexpected Rules.
In Proceed-ings of the 19th ACM international conference on In-formation and knowledge management.2010.Thorsten Joachims.
Making large-scale support vectormachine learning practical.
In Advances in kernelmethods.1999.Fangtao Li, Minlie Huang, Yi Yang and Xiaoyan Zhu.Learning to identify review Spam.
In Proceedings ofthe Twenty-Second international joint conference onArtificial Intelligence.
2011.Jiwei Li, Claire Cardie and Sujian Li.
TopicSpam: aTopic-Model-Based Approach for Spam Detection.
InProceedings of the 51th Annual Meeting of the Associ-ation for Computational Linguis- tics.
2013.Peng Li, Jing Jiang and Yinglin Wang.
Generating tem-plates of entity summaries with an entity-aspect modeland pattern mining.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics.
2010.Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,and Hady Wirawan Lauw.
Detecting Product ReviewSpammers Using Rating Behavior.
In Proceedings ofthe 19th ACM international conference on Informationand knowledge management.
2010.Tieyan Liu.
Learning to Rank for Information Retrieval.In Foundations and Trends in Information Retrieval2009.Arjun Mukherjee, Bing Liu and Natalie Glance.
SpottingFake Reviewer Groups in Consumer Reviews .
In Pro-ceedings of the 21st international conference on WorldWide Web.
2012.Juan Martinez-Romo and Lourdes Araujo.
Web spamidentification through language model analysis.
InProceedings of the 5th international workshop on ad-versarial information retrieval on the web.
2009.Myle Ott, Claire Cardie and Jeffrey Hancock.
Estimatingthe Prevalence of Deception in Online Review Com-munities.
In Proceedings of the 21st international con-ference on World Wide Web.
2012.Myle Ott, Yejin Choi, Claire Cardie and Jeffrey Hancock.Finding Deceptive Opinion Spam by Any Stretch ofthe Imagination.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics.
2011.Daniel Ramage, David Hall, Ramesh Nallapati andChristopher Manning.
Labeled LDA: A supervisedtopic model for credit attribution in multi-labeledcorpora.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing.2009.Michal Rosen-zvi, Thomas Griffith, Mark Steyvers andPadhraic Smyth.
The author-topic model for authorsand documents.
In Proceedings of the 20th conferenceon Uncertainty in artificial intelligence.2004.Xiaojun Wan and Jianwu Yang.
Multi-Document Sum-marization Using Cluster-Based Link Analysis.
InProceedings of the 31st annual international ACM SI-GIR conference on Research and development in in-formation retrieval.
2008.Xiaojun Wan, Jianwu Yang and Jianguo Xiao Manifold-Ranking Based Topic-Focused Multi-Document Sum-marization.
In Proceedings of International Joint Con-ferences on Artificial Intelligence,2007.Guan Wang, Sihong Xie, Bing Liu and Philip Yu.
Re-view Graph based Online Store Review Spammer De-tection.
In Proceedings of International Conference ofData Mining.
2011.Guangyu Wu, Derek Greene and , Padraig Cunningham.Merging multiple criteria to identify suspicious re-views.
In Proceedings of the fourth ACM conferenceon Recommender systems.
2011.Kyung-Hyan Yoo and Ulrike Gretzel.
Comparison of De-ceptive and Truthful Travel Reviews.
In Informationand Communication Technologies in Tourism.
2009.Dengyong Zhou, Olivier Bousquet, Thomas Navin andJason Weston.
Learning with local and global consis-tency.
In Proceedings of Advances in neural informa-tion processing systems.2003.Dengyong Zhou, Jason Weston, Arthur Gretton andOlivier Bousquet.
Ranking on data manifolds.
In Pro-ceedings of Advances in neural information processingsystems.2003.1942
