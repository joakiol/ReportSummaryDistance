QuickSet: Multimodal Interaction forSimulation Set-up and ControlPhilip R. Cohen, Michael Johnston, David McGee, Sharon Oviatt, Jay Pittman,Ira Smith, Liang Chen and Josh ClowCenter for Human Computer CommunicationOregon Graduate Institute of Science and TechnologyP.O.Box 91000Portland, OR 97291-1000 USATel: 1-503-690-1326E-mail: pcohen@cse.ogi.eduhttp://www.cse.ogi.edu/CHCCABSTRACTThis paper presents a novel multimodal system applied tothe setup and control of distributed interactive simulations.We have developed the QuickSet prototype, a pen/voicesystem running on a hand-held PC, communicating througha distributed agent architecture to NRaD's ~ LeatherNetsystem, adistributed interactive training simulator built forthe US Marine Corps (USMC).
The paper briefly describesthe system and illustrates its use in multimodal simulationsetup.KEYWORDS: multimodal interfaces, agent architecture,gesture recognition, speech recognition, natural languageprocessing, distributed interactive simulation.1.
INTRODUCTIONIn order to train personnel more effectively, the US militaryis developing large-scale distributed simulation capabilities.Begun as SIMNET in the 1980's \[23\], these distributed,interactive environments attempt to provide a high degree offidelity in simulating combat, including simulations of theindividual combatants, the equipment, entity movements,atmospheric effects, etc.
There are four general phases ofuser interaction with these simulations: Creating entities,supplying their initial behavior, interacting with the entitiesduring a running simulation, and reviewing the results.
Thepresent research concentrates on the first two of thesestages.Our contribution to the distributed interactive simulation(DIS) effort is to rethink the nature of the user interaction.As with most modern simulators, DISs are controlled viagraphical user interfaces (GUIs).
However, the simulationGUI is showing signs of strain, since even for a small-scalescenario, it requires users to choose from hundreds ofentities in order to select he desired ones to place on a map.To compound these interface problems, the military isintending to increase the scale of the simulationsI NRaD = US Navy Command and Control Ocean Systems CenterResearch Development Test and Evaluation (San Diego).20dramatically, while at the same time, for reasons ofmobility and affordability, desiring that simulations houldbe creatable from small devices (e.g., PDAs).
Thisimpending collision of trends for smaller screen size and formore entities requires a different paradigm for human-computer interaction.We have argued generically that GUI technologies offeradvantages in allowing users to manipulate objects that areon the screen, in reminding users of their options, and inminimizing errors \[7\].
However, GUIs are often weak insupporting interactions with many objects, or objects noton the screen.
In contrast, it was argued that linguistically-based interface technologies offer the potential to describelarge sets of objects, which may not all be present on ascreen, and can be used to create more complex behaviorsthrough specification of rule invocation conditions.Simulation is one type of application for which theselimitations of GUIs, as well as the strengths of naturallanguage, specially spoken language, are apparent \[6\].It has become clear, however, that speech-only interaction isnot optimal for spatial tasks.
Using a high-fidelity"Wizard-of-Oz" methodology \[20\], recent empirical resultsdemonstrate clear language processing and task performanceadvantages for multimodal (pen/voice) input over speech-only input for map-based systems \[17,18\].3.
QUICKSETTo address these simulation interface problems, andmotivated by the above results, we have developedQuickSet (see Figure 1) a collaborative, handheld,multimodal system for configuring military simulationsbased on LeatherNet \[5\], a system used in training platoonleaders and company commanders at the USMC base at 29Palms, California.
LeatherNet simulations are created usingthe ModSAF simulator \[10\] and can be visualized in aCAVE-based virtual reality environment \[11, 26\] calledCommandVu (see Figure 2 - -  QuickSet systems are onthe soldiers' tables).
In addition to LeatherNet, QuickSet isbeing used in a second effort called Exlnit (ExerciseInitialization), that will enable users to create division-sizedexercises.
Because of the use of OAA, QuickSet caninteroperate with agents from CommandTalk [14], whichprovides a speech-only interface to ModSAF.QuickSet runs on both desktop and hand-held PC's,communicating over wired and wireless LAN's, or modemlinks.
The system combines peech and pen-based gestureinput on multiple 3-1b hand-held PCs (Fujitsu Stylistic1000), which communicate via wireless LAN through theOpen Agent Architecture (OAA) 2 [8], to ModSAF, and alsoto CommandVu.
With this highly portable device, a usercan create entities, establish "control measures" (e.g.,objectives, checkpoints, etc.
), draw and label various linesand areas, (e.g., landing zones) and give the entitiesbehavior.agent-based architecture was chosen to support thisapplication because it offers easy connection to legacyapplications, and the ability to run the same set of softwarecomponents in a variety of hardware configurations, rangingfrom stand-alone on the handheld PC, to distributedoperation across numerous workstations and PCs.Additionally, the architecture supports mobility in thatlighter weight agents can run on the handheld, while morecomputationally-intensive processing can be migratedelsewhere on the network.
The agents may be written inany programming language (here, Quintus Prolog, VisualC++, Visual Basic, and Java), as long as they communicatevia an interagent communication language.
Theconfiguration of agents used in the Quickset system isillustrated in Figure 3.
A brief description of each agentfollows:Figure 1: QuickSet running on a wireless handheld PC.In the remainder of the paper, we illustrate the systembriefly, describe its components, and discuss its application.QuickSet interface: On the handheld PC is a get-referenced map of the region such that entities displayed onthe map are registered to their positions on the actualterrain, and thereby to their positions on each of the varioususer interfaces connected to the simulation.
The mapinterface agent provides the usual pan and zoom capabilities,multiple overlays, icons, etc.
The user can draw directly onthe map, in order to create points, lines, and areas.
The usercan create entities, give them behavior, and watch thesimulation unfold from the handheld.
When the pen isplaced on the screen, the speech recognizer is activated,thereby allowing users to speak and gesture simultaneously.Speech recognition agent: The speech recognitionagent used in QuickSet employs either IBM's VoiceTypeApplication Factory or VoiceType 3.0 recognizers.
Therecognizers use an HMM-based continuous speaker-independent speech recognition technology for PC's underWindows 95/NT.
Currently, the system has a vocabulary of450 words.
It produces a single most likely interpretationof an utterance.Gesture recognition agent: OGI's gesture recognitionagent processes all pen input from a PC screen or tablet.The agent weights the results of both HMM and neural netrecognizers, producing a combined score for each of thepossible recognition results.
Currently, 45 gestures can berecognized, resulting in the creation of 21 military symbols,irregular shapes, and various types of lines.Figure 2: Artist's rendition of QuickSet used withCommandVu virtual display of distributedinteractive simulation.4.
SYSTEM ARCHITECTUREArchitecturally, QuickSet uses distributed agenttechnologies based on the Open Agent Architecture forinteroperation, information brokering and distribution.
Anz Open Agent Architecture is a trademark of SRI International.Natural language agent: The natural anguage agentcurrently employs a definite clause grammar and producestyped feature structures as a representation f the utterancemeaning.
Currently, for this task, the language consists ofnoun phrases that label entities, as well as a variety o fimperative constructs for supplying behavior.Muitimodal integration agent: The multimodalinterpretation agent accepts typed feature structure meaningrepresentations from the language and gesture recognitionagents, and produces a unified multimodal interpretation.21QuickSet Brokered Architecture More detail on the architecture and the individual agents =re provided in [12, 22].5.
EXAMPLEHolding QuickSet in hand, the user views a map from theModSAF simulation, and with spoken language coupledwith pen gestures, issues commands to ModSAF.
In otterto create a unit in QuickSet, the user would hold the pen atthe desired location and utter (for instance): "led T72platoon" resulting in a new platoon of the specified typebeing created at that location.Figure 3: A blackboard is used by a facilitatoragent, who routes queries to appropriate agents forsolution.Simulation agent: The simulation agent, developedprimarily by SRI International, but modified by us formultimodal interaction, serves as the communicationchannel between the OAA-brokered agents and the ModSAFsimulation system.
This agent offers an API for ModSAFthat other agents can use.Web display agent: The Web display agent can be usedto create ntities, points, lines, and areas.
It posts queriesfor updates to the state of the simulation via Java code thatinteracts with the blackboard and facilitator.
The queries amrouted to the running ModSAF simulation, and theavailable ntities can be viewed over a WWW connectionusing a suitable browser.Other user interfaces: When another user interfaceconnected to the facilitator subscribes to and produces thesame set of events as others, it immediately becomes part ofa collaboration.
One can view this as human-humancollaboration mediated by the agent architecture, oras agent-agent collaboration.CommandVu agent: Since the CommandVu virtualreality system is an agent, the same multimodal interface onthe handheld PC can be used to create entities and to fly theuser through the 3-D terrain.
For example, the user can ask"CommandVu, fly me to this platoon <gesture on themap>.
"Application bridge agent: The bridge agentgeneralizes the underlying applications' API to typed featurestructures, thereby providing an interface to the variousapplications uch as ModSAF, CommandVu, and Exinit.This allows for a domain-independent integrationarchitecture in which constraints on multimodalinterpretation are stated in terms of higher-level constructssuch as typed feature structures, greatly facilitating reuse.CORBA bridge agent: This agent converts OAAmessages to CORBA IDL (Interface Definition Language)for the Exercise Initialization project.Figure 4: The QuickSet interface as the userestablishes two platoons, a barbed-wire fence, abreached minefield, and then issues a command to oneplatoon to follow a traced route,The user then adds a barbed-wire fence to the simulation bydrawing a line at the desired location while uttering '"oarbedwire."
Similarly a fortified line is ~ .
A minefield of anamorphous hape is drawn and is labeled verbally, andfinally an M1A1 platoon is created as above.
Then the usercan assign a task to the new platoon by saying "M1A1platoon follow this route" while drawing the route with thepen.
The results of these commands are visible on theQuickSet screen, as seen in Figure 4, in the ModSAFsimulation, and in the CommandVu 3D rendering of thescene.
In addition to multimodal input, unimodal spokenlanguage and gestural commands can be given at any time,depending on the user's task and preference.6.
MULTIMODAL INTEGRATIONSince any unimodal recognizer will make mistakes, theoutput of the gesture recognizer is not accepted as a simpleunilateral decision.
Instead the recognizer produces a set ofprobabilities, one for each possible interpretation of thegesture.
The recognized entities, as well as the i rrecognition probabilities, are sent to the facilitator, whichforwards them to the multimodal interpretation agent.
Incombining the meanings of the gestural and spokeninterpretations, we attempt o satisfy an important designconsideration, amely that the communicative modalitiesshould compensate for each other's weaknesses [7, 16].This is accomplished by selecting the highest scoringunified interpretation of speech and gesture.
Importantly,22the unified interpretation might not include the highestscoring gestural (or spoken language) interpretation becauseit might not be semantically compatible with the othermode.
The key to this interpretation process is the use of atyped feature structure \[1, 3\] as a meaning representationlanguage that is common to the natural language andgestural interpretation agents.
Johnston et al \[12\] presentthe details of multimodal integration of continuous peechand pen-based gesture, guided by research in users'multimodal integration and synchronization strategies \[19\].Unlike many previous approaches tomultimodal integration(e.g, \[2, 9, 12, 15, 25\]) speech is not "in charge," in thesense of relegating esture a secondary and dependent role.This mutually-compensatory interpretation process iscapable of analyzing multimodal constructions, as well asspeech-only and pen-only constructions when they occur.Vo and Wood's system \[24\] is similar to the one reportedhere, though we believe the use of typed feature structuresprovides a more generally usable and formal integrationmechanism than their frame-merging strategy.
Cheyer andJulia \[4\] sketch a system based on Oviatt's \[17\] results andthe OAA \[8\], but do not discuss the integration strategy normultimodal compensation.7.
CONCLUDING REMARKSQuickSet has been delivered to the US Navy (NRaD) andUS Marine Corps.
for use at 29 Palms, California, where itis primarily used to set up training scenarios and to controlthe virtual environment.
It is also installed at NRaD'sCommand Center of the Future.
The system was used bythe US Army's 82nd Airborne Corps.
at Ft. Bragg duringthe Royal Dragon Exercise.
There, QuickSet was deployedin a tent, where it was subjected to an extreme noiseenvironment, including explosions, low-flying jet aircraft,generators, and the like.
Not surprisingly, spokeninteraction with QuickSet was not feasible, although usersgestured successfully.
Instead, users wanted to gesture.Although we had provided amultimodal interface for use inless hostile conditions, nevertheless we needed toprovide,and in fact have provided, a complete overlap infunctionality, such that any task can be accomplished justwith pen or just with speech when necessary.
Finally,QuickSet is now being extended for use in the Exlnitsimulation initialization system for DARPA's STOW-97Advanced Concept Demonstration that is intended forcreation of division-sized exercises.Regarding the multimodal interface itself, QuickSet hasundergone a "proactive" interface valuation in that thestudies that were performed in advance of building thesystem predicted the utility of multimodal over unimodalspeech as an input to map-based systems \[17, 18\].
Inparticular, it was discovered in this research that multimodalinteraction generates simpler language than unimodalspoken commands to maps.
For example, to create a"phase line" between two three-digit <x,y> grid coordinates,a user would have to say: "create a line from nine four threenine six one to nine five seven nine six eight and call itphase line green" \[14\].
In contrast, a QuickSet user wouldsay "phase line green" while drawing a line.
Creation of areafeatures with unimodal speech would be more complex still,if not infeasible.
Given that numerous difficult-to-processlinguistic phenomena (such as utterance disfluencies) areknown to be elevated in lengthy utterances, and also to beelevated when people speak locative constituents \[17, 18\],multimodal interaction that permits pen input to specifylocations and that results in brevity offers the possibility ofmore robust recognition.Further development of QuickSet's poken, gestural, andmultimodal integration capabilites are continuing.
Researchis also ongoing to examine and quantify the benefits ofmultimodal interaction in general, and our architecture inparticular.ACKNOWLEDGMENTSThis work is supported in part by the InformationTechnology and Information Systems offices of DARPAunder contract number DABT63-95-C-007, in part by ONRgrant number N00014-95-I-1164, and has been done incollaboration with the US Navy's NCCOSC RDT&EDivision (NRaD), Ascent Technologies, Mitre Corp., MRJCorp., and SRI International.REFERENCES1.
Calder, J. Typed unification for natural languageprocessing.
In E. Klein and J. van Benthem (Eds.
),Categories, Polymorphisms, and Unification.
Centrefor Cognitive Science, University of Edinburgh,Edinburgh, 1987, 65-72.. Brison, E. and N. Vigouroux.
(unpublished ms.).Multimodal references: A generic fusion process.URIT-URA CNRS.
Universit6 Paul Sabatier,Toulouse, France.3.
Carpenter, R. The logic of typed feature structures.Cambridge University Press, Cambridge, 1992.. Cheyer, A., and L. Julia.
Multimodal maps: An agent-based approach.
International Conference onCooperative Multimodal Communication (CMC/95),May 1995.
Eindhoven, The Netherlands, 1995, 24-26.. Clarkson, J. D., and Yi., J., LeatherNet: A syntheticforces tactical training system for the USMCcommander.
Proceedings of the Sixth Conference onComputer Generated Forces and BehavioralRepresentation.
Institute for simulation and training.Technical Report IST-TR-96-18, 1996, 275-281.6.
Cohen, P. R. Integrated Interfaces for Decision Supportwith Simulation, Proceedings of the Winter Simulation :Conference, Nelson, B. and Kelton, W. D. and Clark,G.
M., (eds.
), ACM, New York, December, 1991,1066-1072.7.
Cohen, P. R. The Role of Natural Language in aMultimodal Interface.
Proceedings of UIST'92, ACMPress, New York, 1992, 143-149.238.
Cohen, P.R., Cheyer, A., Wang, M., and Baeg, S.C.An Open Agent Architecture.
Working notes of theAAAI Spring Symposium Series on Software AgentsStanford Univ., CA, March, 1994, 1-8.9.
Cohen, P. R., Dalrymple, M., Moran, D.B., Pereira,F.
C. N., Sullivan, J. W., Gargan, R. A., Schlossberg,J.
L., and Tyler, S.W.
Synergistic Use of DirectManipulation and Natural Language, Human Factorsin Computing Systems: CHI'89 ConferenceProceedings, ACM, Addison Wesley Publishing CoNew York, 227-234, 1989.10.
Courtemanche, A.J.
and Ceranowicz, A. ModSAFDevelopment Status.
Proceedings of the FifthConference on Computer Generated Forces andBehavioral Representation, Univ.
Central Florida,Orlando, 1995, 3-13.11.
Cruz-Neira, C. D.J.
Sandin, T.A.
DeFanti, "Surround-Screen Projection-Based Virtual Reality: The Designand Implementation f the CAVE," Computer Graphics(Proceedings of SIGGRAH'93), ACM SIGGRAPH,August 1993, 135-142.12.
Johnston, M., Cohen, P. R., McGee, D., Oviatt, S.L., Pittman, J., and Smith, I.. Unification-basedmultimodal integration, in submission.13.
Koons, D.B., C.J.
Sparrell and K.R.
Thorisson.
1993.Integrating simultaneous input from speech, gaze, andhand gestures.
In Mark T. Maybury (ed.)
IntelligentMultimedia Interfaces.
AAAI Press/ MIT Press,Cambridge, MA, 257-276.14.
Moore, R., Dowding, J. Bratt, H. Gawron, J. M., andCbeyer, A., CommandTalk: A Spoken-LanguageInterface for Battlefield Simulations, 1997, (thisvolume).15.
Neal, J.G.
and Shapiro, S.C.
Intelligent multi-mediainterface technology.
In J.W.
Sullivan and S.W.
Tyler,editors, Intelligent User Interfaces, chapter 3, pages 45-68.
ACM Press Frontier Series, Addison WesleyPublishing Co., New York, New York, 1991.16.
Oviatt, S. L., Pen/Voice: Complementary multimodalcommunication, Proceedings of SpeechTech'92, NewYork, February, 1992, 238-241.17.
Oviatt, S.L.
Multimodal interfaces for dynamicinteractive maps.
Proceedings of CHI'96 HumanFactors in Computing Systems (April 13-18,Vancouver, Canada), ACM Press, N'Y, 1996, 95-102.18.
Oviatt, S. L., Multimodal interactive maps: Designingfor human performance, Human-Computer Interaction,in press.19.
Oviatt, S. L, A. DeAngeli, and K. Kuhn.
In press.Integration and synchronization of input modes duringmultimodal human-computer interaction.
Proceedingsof the Conference on Human Factors in ComputingSystems (CHI '97), ACM Press, New York.20.
Oviatt, S. L., Cohen, P. R, Fong, M. W. and Frank,M.
P., A rapid semi-automatic simulation technique forinteractive speech and handwriting, Proceedings of the1992 International Conference Spoken LanguageProcessing, vol.
2, University of Alberta, J.
Ohala(ed.
), October, 1992, 1351-1354.21.
Oviatt, S. L., Cohen, P. R., Wang, M. Q.,Towardinterface design for human language technology:Modality and structure as determinants of linguisticcomplexity, Speech Communication, 15 (3-4), 1994.22.
Pittman, J.A., Smith, I.A., Cohen, P.R., Oviatt, S.L.,and Yang, T.C.
QuickSet: A Multimodal Interface forMilitary Simulation.
in Proceedings of the SixthConference on Computer-Generated Forces andBehavioral Representation, Orlando, Florida, 1996.23.
Thorpe, J.
A., The new technology of large scalesimulator networking: Implications for mastering theart of warfighting.
Proceedings of the 9thInterservice/industry Training Systems Conference,Orlando, Florida, December, 1987, 492-501.24.
Vo, M. T. and C. Wood.
Building an applicationframework for speech and pen input integration inmultimodal learning interfaces.
InternationalConference on Acoustics, Speech, and SignalProcessing, Atlanta, GA, 1996.25.
Wauchope, K. Eucalyptus: Integrating natural anguageinput with a graphical user interface.
Naval ResearchLaboratory, Report NRL/FR/5510--94-9711, 994.26.
Zyda, M. J., Pratt, D. R., Monahan, J. G., andWilson, K. P., NPSNET: Constructing a 3-D virtualworld, Proceedings of the 1992 Symposium onInteractive 3-D Graphics, March, 1992.24
