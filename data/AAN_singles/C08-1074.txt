Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 585?592Manchester, August 2008Random Restarts in Minimum Error Rate Training for StatisticalMachine TranslationRobert C. Moore and Chris QuirkMicrosoft ResearchRedmond, WA 98052, USAbobmoore@microsoft.com, chrisq@microsoft.comAbstractOch?s (2003) minimum error rate training(MERT) procedure is the most commonlyused method for training feature weights instatistical machine translation (SMT) mod-els.
The use of multiple randomized start-ing points in MERT is a well-establishedpractice, although there seems to be nopublished systematic study of its bene-fits.
We compare several ways of perform-ing random restarts with MERT.
We findthat all of our random restart methods out-perform MERT without random restarts,and we develop some refinements of ran-dom restarts that are superior to the mostcommon approach with regard to resultingmodel quality and training time.1 IntroductionOch (2003) introduced minimum error rate train-ing (MERT) for optimizing feature weights in sta-tistical machine translation (SMT) models, anddemonstrated that it produced higher translationquality scores than maximizing the conditionallikelihood of a maximum entropy model using thesame features.
Och?s method performs a seriesof one-dimensional optimizations of the featureweight vector, using an innovative line search thattakes advantage of special properties of the map-ping from sets of feature weights to the resultingtranslation quality measurement.
Och?s line searchis guaranteed to find a global optimum, whereasmore general line search methods are guaranteedonly to find a local optimum.c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.Global optimization along one dimension at atime, however, does not insure global optimizationin all dimensions.
Hence, as Och briefly men-tions, ?to avoid finding a poor local optimum,?MERT can be augmented by trying multiple ran-dom starting points for each optimization searchwithin the overall algorithm.
However, we are notaware of any published study of the effects of ran-dom restarts in the MERT optimization search.We first compare a variant of Och?s method withand without multiple starting points for the op-timization search, selecting initial starting pointsrandomly according to a uniform distribution.
Wefind that using multiple random restarts can sub-stantially improve the resulting model in terms oftranslation quality as measured by the BLEU met-ric, but that training time also increases substan-tially.
We next try selecting starting points by arandom walk from the last local optimum reached,rather than by sampling from a uniform distrib-ution.
We find this provides a slight additionalimprovement in BLEU score, and is significantlyfaster, although still slower than training withoutrandom restarts.Finally we look at two methods for speeding uptraining by pruning the set of hypotheses consid-ered.
We find that, under some circumstances, thiscan speed up training so that it takes very littleadditional time compared to the original methodwithout restarts, with no significant reduction inBLEU score compared to the best training methodsin our experiments.2 Och?s MERT procedureWhile minimum error rate training for SMT istheoretically possible by directly applying gen-eral numerical optimization techniques, such asthe downhill simplex method or Powell?s method585(Press, 2002), naive use of these techniques wouldinvolve repeated translation of the training sen-tences using hundreds or thousands of combina-tions of feature weights, which is clearly impracti-cal given the speed of most SMT decoders.Och?s optimization method saves expensiveSMT decoding time by generating lists of n-besttranslation hypotheses, and their feature values ac-cording to the SMT model, and then optimizingfeature weights just with respect to those hypothe-ses.
In this way, as many different feature weightsettings as necessary can be explored without re-running the decoder.
The translation quality mea-surement for the training corpus can be estimatedfor a given point in feature weight space by find-ing the highest scoring translation hypothesis, outof the current set of hypotheses, for each sentencein the training set.
This is typically orders of mag-nitude faster than re-running the decoder for eachcombination of feature weights.Since the resulting feature weights are opti-mized only for one particular set of translation hy-potheses, the decoder may actually produce differ-ent results when run with those weights.
There-fore Och iterates the process, re-running the de-coder with the optimized feature weights to pro-duce new sets of n-best translation hypotheses,merging these with the previous sets of hypothe-ses, and re-optimizing the feature weights relativeto the expanded hypothesis sets.
This process is re-peated until no more new hypotheses are obtainedfor any sentence in the training set.Another innovation by Och is a method of nu-merical optimization that takes advantage of thefact that, while translation quality metrics mayhave continous values, they are always applied tothe discrete outputs of a translation decoder.
Thismeans that any measure of translation quality canchange with variation in feature weights only atdiscrete points where the decoder output changes.Och takes advantage of this through an efficientprocedure for finding all the points along a one-dimensional line in feature weight space at whichthe highest scoring translation hypothesis changes,given the current set of hypotheses for a particu-lar sentence.
By merging the lists of such pointsfor all sentences in the training set, he finds allthe points at which the highest scoring hypothesischanges for any training sentence.Finding the optimal value of the feature weightsalong the line being optimized then requires sim-ply evaluating the translation quality metric foreach range of values for the feature weights be-tween two such consecutive points.
This can bedone efficiently by tracking incremental changesin the sufficient statistics for the translation qual-ity metric as we iterate through the points wherethings change.
Och uses this procedure as a linesearch method in an iterative optimization proce-dure, until no additional improvement in the trans-lation quality metric is obtained, given the currentsets of translation hypotheses.3 Optimization with Random RestartsAlthough Och?s line search is globally optimal,this is not sufficient to guarantee that a series ofline searches will find the globally optimal com-bination of all feature weights.
To avoid gettingstuck at an inferior local optimum during MERT, itis usual to perform multiple optimization searchesover each expanded set of translation hypothesesstarting from different initial points.
Typically, oneof these points is the best point found while op-timizing over the previous set of translation hy-potheses.1 Additional starting points are then se-lected by independently choosing initial values foreach feature weight according to a uniform distri-bution over a fixed interval, say ?1.0 to +1.0.
Thebest point reached, starting from either the previ-ous optimum or one of the random restart points,is selected as the optimum for the current set of hy-potheses.
This widely-used procedure is describedby Koehn et al (2007, p. 50).3.1 Preliminary evaluationIn our first experiments, we compared a variantof Och?s MERT procedure with and without ran-dom restarts as described above.
For our trainingand test data we used the English-French subsetof the Europarl corpus provided for the shared task(Koehn and Monz, 2006) at the Statistical MachineTranslation workshop held in conjunction with the2006 HLT-NAACL conference.
We built a stan-dard baseline phrasal SMT system, as describedby Koehn et al (2003), for translating from Eng-lish to French (E-to-F), using the word alignmentsand French target language model provided by theworkshop organizers.We trained a model with the standard eight fea-tures: E-to-F and F-to-E phrase translation log1Since additional hypotheses have been added, initiatingan optimization search from this point on the new set of hy-potheses will often lead to a higher local optimum.586probabilities, E-to-F and F-to-E phrase translationlexical scores, French language model log proba-bilities, phrase pair count, French word count, anddistortion score.
Feature weight optimization wasperformed on the designated 2000-sentence-pairdevelopment set, and the resulting feature weightswere evaluated on the designated 2000-sentence-pair development test set, using the BLEU-4 metricwith one reference translation per sentence.At each decoding iteration we generated the100-best translation hypotheses found by ourphrasal SMT decoder.
To generate the initial100-best list, we applied the policy of setting theweights for features we expected to be positivelycorrelated with BLEU to 1, the weights for fea-tures we expected to be negatively correlated withBLEU to ?1, and the remaining weights to 0.
Inthis case, we set the initial distortion score weightto ?1, the phrase count weight to 0, and all otherfeature weights to 1.We made a common modification of the MERTprocedure described by Och, by replacing Pow-ell?s method (Press, 2002) for selecting the direc-tions in which to search the feature weight space,with simple co-ordinate ascent?following Koehnet al (2007)?repeatedly optimizing one featureweight at a time while holding the others fixed, un-til all feature weights are at optimum values, giventhe values of the other feature weights.
Powell?smethod is not designed to reach a better local op-timum than co-ordinate ascent, but does have con-vergence guarantees under certain idealized condi-tions.
However, we have observed informally that,in MERT, co-ordinate ascent always seems to con-verge relatively quickly, with Powell?s method of-fering no clear advantage.We also modified Och?s termination test slightly.As noted above, Och suggests terminating theoverall procedure when n-best decoding fails toproduce any hypotheses that have not already beenseen.
Without random restarts, this will guaranteeconvergence because the last set of feature weightsselected will still be a local optimum.2 However,if we go through the coordinate ascent procedurewithout finding a better set of feature weights, thenwe do not have to perform the last iteration of n-best decoding, because it will necessarily producethe same n-best lists as the previous iteration, as2With random restarts, there can be no guarantee of con-vergence, unless we have a true global optimization method,or we enumerate all possible hypotheses permitted by themodel.long as the decoder is deterministic.
Thus we canterminate the overall procedure if either we eitherfail to generate any new hypotheses in n-best de-coding, or the optimium set of feature weights doesnot change in the coordinate ascent phase of train-ing.
In fact, we relax the termination test a bitmore than this, and terminate if no feature weightchanges by more than 1.0%.Without random restarts, we found that MERTconverged in 8 decoding iterations, with the result-ing model producing a BLEU score of 31.12 onthe development test set.
For the experiment withrandom restarts, after each iteration of 100-bestdecoding, we searched from 20 initial points, 19points selected by uniform sampling over the in-terval [?1, 1] for each feature weight, plus the op-timum point found for the previous set of hypothe-ses.
This procedure converged in 10 decoding it-erations, with a BLEU score of 32.02 on the de-velopment test set, an improvement of 0.90 BLEU,compared to MERT without random restarts.While the difference in BLEU score with andwithout random restarts was substantial, trainingwith random restarts took much longer.
Withour phrasal decoder and our MERT implementa-tion, optimizing feature weights took 3894 secondswithout random restarts and 12690 seconds withrandom restarts.3 We therefore asked the questionwhether there was some other way to invest extratime in training feature weights that might be justas effective as performing random restarts.
The ob-vious thing to try is using larger n-best lists, so were-ran the training without random restarts, usingn-best lists of 200 and 300.Using n-best lists of 200 produced a noticeableimprovement in training without restarts, converg-ing in 9 decoding iterations taking 7877 seconds,and producing a BLEU score of 31.83 on the de-velopment test set.
Using n-best lists of 300 con-verged in 8 decoding iterations taking 8973 sec-onds, but the BLEU score on the development testset fell back to 31.16.
Thus, simply increasing thesize of the n-best list does not seem to be a reli-able method for improving the results obtained byMERT without random restarts.4 Random Walk RestartsIn the procedure described above, the initial valuesfor each feature weight are independently sampled3Timings are for single-threaded execution using a desk-top PC with 3.60 GHz Intel Xeon processors.587from a uniform distribution over the range [?1, 1].We have observed anecdotally, however, that ifthe selected starting point itself produces a BLEUscore much below the best we have seen so far, co-ordinate ascent search is very unlikely to take us toa point that is better than the current best.
In orderto bias the selection of restarting points towardsbetter scores, we select starting points by randomwalk from the ending point of the last coordinateascent search.The idea is to perform a series of cautious stepsin feature weight space guided by training setBLEU.
We begin the walk at the ending point of thelast coordinate ascent search; let us call this point~w(0).
Each step updates the feature weights in amanner inspired by Metropolis-Hastings sampling(Hastings, 1970).
Starting from the current featureweight vector ~w(i), we sample a small update froma multivariate Gaussian distribution with mean of0 and diagonal covariance matrix ?2I .
This updateis added to the current value to produce a new po-tential feature weight vector.
The BLEU scores forthe old and the new feature weight vector are com-pared.
The new feature weight vector is alwaysaccepted if the BLEU score on the training set isimproved; however if the BLEU score drops, thenew vector is accepted with a probability that de-pends on how close the new BLEU score is to theprevious one.
After a fixed number of steps, thewalk is terminated, and we produce a value to useas the initial point for the next round of coordinateascent.There are several wrinkles, however.
First, weprefer that the scores not fall substantially duringthe random walk.
Therefore we establish a base-line value of m = BLEU(~w(0)) ?
0.005 (i.e., 1/2BLEU point below the initial value) and do not al-low a step to go below this baseline value.
To en-sure this, each step progresses as follows:~d(i)?
GAUSSIAN(0, ?2I)~v(i)= ~w(i)+~d(i)u(i)?
UNIFORM(0, 1)~w(i+1)=??
?~v(i) if BLEU(~v(i))?mBLEU(~w(i))?m ?
u(i)~w(i) otherwise.With this update rule, we know that ~w(i+1) willnever go below m, since the initial value is not be-low m, and any step moving below m will resultin a negative ratio and therefore not be accepted.So far, ?2 is left as a free parameter.
An ini-tial value of 0.001 performs well in our experi-ence, though in general it may result in steps thatare consistently too small (so that only a very lo-cal neighborhood is explored) or too large (so thatthe vast majority of steps are rejected).
Thereforewe devote the first half of the steps to ?burn-in?
;that is, tuning the variance parameter so that ap-proximately 60% of the steps are accepted.
Duringburn-in, we compute the acceptance rate after eachstep.
If it is less than 60%, we multiply ?2 by 0.99;if greater, we multiply by 1.01.The final twist is in selection of the point usedfor the next iteration of coordinate ascent.
Ratherthan using the final point of the random walk ~w(n),we return the feature weight vector that achievedthe highest BLEU score after burn-in: ~w?
=argmax~w(i),n/2<i?nBLEU(~w).
This ensures thatthe new feature weight vector has a relatively highobjective function value yet is likely very differentfrom the initial point.4.1 Preliminary evaluationTo evaluate the random walk selection procedure,we used a similar experimental set-up to the previ-ous one, testing on the 2006 English-French Eu-roparl corpus, using n-best lists of 100, and 20starting points for each coordinate ascent search?one being the best point found for the previoushypothesis set, and the other 19 selected by ourrandom walk procedure.
We set the number ofsteps to be used in each random walk to 500.
Thisprocedure converged in 6 decoding iterations tak-ing 8458 seconds, with a BLEU score of 32.13 onthe development test set.
This is an improvementof 0.11 BLEU over the uniform random restartmethod, and it also took only 67% as much time.The speed up was due to the fact that random walkmethod converged in 2 fewer decoding iterations,although the average time per iteration was greater(1410 seconds vs. 1269 seconds) because of theextra time needed for the random walk.5 Hypothesis Set PruningMERT with random walk restarts seems to pro-duce better models than either MERT with uniformrandom restarts or with no restarts, but it is stillslower than MERT with no restarts by more thana factor of 2.
The difference between 3894 sec-onds (1.08 hours) and 8458 seconds (2.35 hours) tooptimize feature weights may not seem important,given how long the rest of the process of build-588ing and training an SMT system takes; however,to truly optimize an SMT system would actuallyrequire performing feature weight training manytimes to find optimum values of hyper-parameterssuch as maximum phrase size and distortion limit.This kind of optimization is rarely done for everysmall model change, because of how long featureweight optimization takes; so it seems well worththe effort to speed up the optimization process asmuch as possible.To try to speed up the feature weight optimiza-tion process, we have tried pruning the set of hy-potheses that MERT is applied to.
The time takenby the random walk and coordinate ascent phasesof MERT with random walk restarts is roughly lin-ear in the number of translation hypotheses exam-ined.
In the experiment described in Section 4.1,after the first 100-best decoding iteration therewere 196,319 hypotheses in the n-best lists, andMERT took 347 seconds.
After merging all hy-potheses from 6 iterations of 100-best decodingthere were 800,580 hypotheses, and MERT took1380 seconds.We conjectured that a large proportion of thesehypotheses are both low scoring according to mostsubmodels and low in measured translation qual-ity, so that omitting them would make little differ-ence to the feature weight optimization optimiza-tion process.4 We attempt to identify such hy-potheses by extracting some additional informa-tion from Och?s line search procedure.Och?s line search procedure takes note of everyhypothesis that is the highest scoring hypothesisfor a particular sentence for some value of the fea-ture weight being optimized by the line search.The hypotheses that are never the highest scoringhypothesis for any combination of feature valuesexplored effectively play no role in the MERT pro-cedure.
We conjectured that hypotheses that arenever selected as potentially highest scoring in aparticular round of MERT could be pruned fromthe hypothesis set without adversely affecting thequality of the feature weights eventually produced.We tested two implementations of this type ofhypothesis pruning.
In the more conservative im-plementation, after each decoding iteration, wenote all the hypotheses that are ever ?touched?
(i.e., ever the highest scoring) during the coordi-nate ascent search either from the initial starting4Hypotheses that are of low translation quality, but highscoring according to some submodels, need to be retained sothat the feature weights are tuned to avoid selecting them.point or from one of the random restarts.
Any hy-pothesis that is never touched is pruned from thesets of hypotheses that are merged with the resultsof subsequent n-best decoding iterations.
We referto this as ?post-restart?
pruning.In the more aggressive implementation, aftereach decoding iteration, we note all the hypothe-ses that are touched during the coordinate ascentsearch from the initial starting point.
The hypothe-ses that are not touched are pruned from the hy-pothesis set before any random restarts.
We referto this as ?pre-restart?
pruning.5.1 Preliminary evaluationWe evaluated both post- and pre-restart pruningwith random walk restarts, under the same condi-tions used to evaluate random walk restarts withoutpruning.
With post-restart pruning, feature weighttraining converged in 8 decoding iterations taking7790 seconds, with a BLEU score of 32.14 on thedevelopment test set.
The last set of restarts ofMERT had 276,134 hypotheses to consider, a re-duction of more than 65% compared to no prun-ing.
With pre-restart pruning, feature weight train-ing converged in 7 decoding iterations taking 4556seconds, with a BLEU score of 32.17 on the devel-opment test set.
The last set of restarts of MERThad only 64,346 hypotheses to consider, a reduc-tion of 92% compared to no pruning.Neither method of pruning degraded translationquality as measured by BLEU; in fact, BLEU scoresincreased by a trivial amount with pruning.
Post-restart pruning speeded up training only slightly,primarily because it took more decoding iterationsto converge.
Time per decoding iteration was re-duced from 1409 seconds to 974 seconds.
Pre-restart pruning was substantially faster overall, aswell as in terms of time per decoding iteration,which was 650 seconds.Additional insight into the differences betweenfeature weight optimization methods can be gainedby evaluating the feature weight sets produced af-ter each decoding iteration.
Figure 1 plots theBLEU score obtained on the development test setas a function of the cumulative time taken to pro-duce the corresponding feature weights, for eachof the training runs we have described so far.We observe a clear gap between the resultsobtained from random walk restarts and thosefrom either uniform random restarts or no restarts.Note in particular that, although the uniform ran-589Restart Pruning Num Num Decoding Total MERT Dev-test ConfMethod Method Starts N-best Iterations Seconds Seconds BLEU Levelnone none 1 100 8 3894 276 31.12 > 0.999none none 1 300 8 8973 1173 31.17 > 0.999none none 1 200 9 7877 717 31.83 0.999uniform rand none 5 100 7 4294 917 31.95 0.993uniform rand none 30 100 11 19345 13306 31.98 0.995uniform rand none 20 100 10 12690 7613 32.02 0.984uniform rand none 10 100 10 9059 3898 32.02 0.984random walk pre-restart 30 100 12 9963 3016 32.04 0.999random walk pre-restart 5 100 11 7696 619 32.10 0.962random walk none 30 100 7 14055 10887 32.10 0.959random walk pre-restart 10 100 14 8581 1254 32.10 0.986random walk post-restart 10 100 9 6985 2236 32.11 0.938random walk none 20 100 6 8458 5766 32.13 0.909random walk post-restart 20 100 8 7790 3965 32.14 0.857random walk none 10 100 8 8338 4280 32.15 0.840random walk pre-restart 20 100 7 4556 1179 32.17 0.712random walk post-restart 5 100 10 6103 1114 32.18 0.794random walk post-restart 30 100 8 9811 6218 32.20 0.554random walk none 5 100 10 7741 3047 32.21 0.000Table 1: Extended Evaluation Results.dom restart method eventually comes within 0.15BLEU points of the best result using random walkrestarts, it takes far longer to get there.
The uni-form restart run produces noticably inferior BLEUscores until just before convergence, while with therandom walk method, the BLEU score increasesquite quickly and then stays essentially flat for sev-eral iterations before convergence.We also note that there appears to be less realdifference among our three variations on randomwalk restarts than there might seem to be fromtheir times to convergence.
Although pre-restartpruning was much faster to convergence than ei-ther of the other variants, all three reached approx-imately the same BLEU score in the same amountof time, if intermediate points are considered.
Thissuggests that our convergence test, while more lib-eral than Och?s, still may be more conservativethan necessary when using random walk restarts.6 Extended EvaluationWe now extend the previous evaluations in twoways.
First, we repeat all the experiments on opti-mization with 20 starting points, using 5, 10, and30 starting points, to see whether we can trade offtraining time for translation quality by changingthat parameter setting, and if so, whether any set-tings seem clearly better than others.Second, we note that different optimizationmethods lead to convergence at different numbersof decoding iterations.
This means that whichmethod produces the shortest total training timewill depend on the relative time taken by n-bestdecoding and the MERT procedure itself (includ-ing the random walk selection procedure, if thatis used).
By co-incidence, these times happenedto be roughly comparable in our experiments.5 Ina situation where decoding is much slower thanMERT, however, the main determinant of overalltraining time would be how many decoding iter-ations are needed.
On the other hand, if decod-ing was made much faster, say, through algorith-mic improvements or by using a compute cluster,total training time would be dominated by MERTproper.
We therefore report number of decoding it-erations to convergence and pure MERT time (ex-cluding decoding and hypothesis set merging) foreach of our experiments, in addition to total featureweight training time.Table 1 reports these three measures of com-5In our complete set of training experiments encompass-ing 187 decoding iterations, decoding averaged 521 secondsper iteration, and MERT (excluding decoding and hypothesisset merging) averaged 421 seconds per (decoding) iteration.590puational effort, plus BLEU score on the devel-opment test set, sorted by ascending BLEU score,for 19 variations on MERT: 3 n-best list sizes forMERT without restarts, and 4 different numbersof restarts for 4 different versions of MERT withrestarts (uniform random selection, random walkselection without pruning, random walk selectionwith post-restart pruning, and random walk selec-tion with pre-restart pruning).The final column of Table 1 is a confidencescore reflecting the estimated probability that thetranslation model produced (at convergence) bythe MERT variant for that row of the table is notas good in terms of BLEU score as the variant thatyielded the highest BLEU score (at convergence)that we observed in these experiments.
Theseprobabilities were estimated by Koehn?s (2004)paired bootstrap resampling method, run for atleast 100,000 samples per comparison.The 11 models obtaining a BLEU score of 31.10or less are all estimated to be at least 95% likely tohave worse translation quality than the best scor-ing model.
We therefore dismiss these modelsfrom further consideration,6 including all modelstrained without random restarts, as well as all mod-els trained with uniform random restarts, leavingonly models trained with random walk restarts.With random walk restarts, post-restart prun-ing remains under consideration at all numbersof restarts tried.
For random walk restarts with-out pruning, only the model produced by 30 start-ing points has been eliminated, and for pre-restartpruning, only the model produced by 20 starts re-mains under consideration.
This suggests that pre-restart pruning may be too aggressive and, thus,overly sensitive to the number of restarts.To get a better picture of the remaining 8 mod-els, see Figures 2?4.
Despite convergence timesranging from 4456 to 9811 seconds, in Figure 2 weobserve that, if feature weights after each decodingiteration are considered, the relationships betweentraining time and BLEU score are remarkably sim-ilar.
In Figure 3, BLEU score varies considerablyup to 3 decoding iterations, but above that, BLEUscores are very close, and almost flat.
In fact, wesee almost no benefit from more than 7 decodingiterations for any model.Finally, Figure 4 shows some noticeable differ-ences between random walk variants in respect to6We estimate the joint probability that these 11 models areall worse than our best scoring model to be 0.882, by multi-plying the confidence scores for all these models.MERT time proper.
Thus, while the choice of ran-dom walk variant chosen may matter little if de-coding is slow, it seems that it can have an im-portant impact if decoding is fast.
If we com-bine the results shown here with the observationfrom Figure 3 that there seems to be no benefit totrying more than 7 decoding iterations, it appearsthat perhaps the best trade-off between translationquality and training time would be obtained by us-ing post-restart pruning, with 5 starting points perdecoding iteration, cutting off training after 7 iter-ations.
This took a total of 4009 seconds to train,compared to 7741 seconds for the highest scoringmodel on the development test set considered inTable 1 (produced by random walk restarts with nopruning, 5 starting points per decoding iteration, atconvergence after 10 iterations).To validate the proposal to use the suggestedfaster training procedure, we compared the twomodels under discussion on the 2000 in-domainsentence pairs for the designated final test set forour corpus.
The model produced by the sug-gested training procedure resulted in a BLEU scoreof 31.92, with the model that scored highest onthe development test set scoring an insignificantlyworse 31.89.
In contast, the highest scoring modelof the three trained with no restarts produced aBLEU score of 31.55 on the final test set, whichwas worse than either of the random walk methodsevaluated on the final test set, at confidence levelsexceeding 0.996 according to the paired bootstrapresampling test.7 ConclusionsWe believe that our results show very convinc-ingly that using random restarts in MERT im-proves the BLEU scores produced by the result-ing models.
They also seem to show that startingpoint selection by random walk is slightly superiorto uniform random selection.
Finally, our exper-iments suggest that time to carry out MERT canbe significantly reduced by using as few as 5 start-ing points per decoding iteration, performing post-restart pruning of hypothesis sets, and cutting offtraining after a fixed number of decoding iterations(perhaps 7) rather than waiting for convergence.ReferencesHastings, W. Keith.
1970.
Monte Carlo samplingmethods using Markov chains and their applica-tions.
Biometrika 57: 97?109.591Koehn, Philipp, and Christof Monz.
2006.
Manualand automatic evaluation of machine translationbetween European languages.
In Proceedingsof the Workshop on Statistical Machine Trans-lation, New York City, USA, 102?121.Koehn, Philipp, Franz Josep Och, and DanielMarcu.
2003.
Statistical Phrase-Based Trans-lation.
In Proceedings of Human LanguageTechnology Conference of the North AmericanChapter of the Association for ComputationalLinguistics, Edmondton, Alberta, Canada, 127?133.Koehn, Philipp.
2004.
Statistical significancetests for machine translation evaluation.
In Pro-ceedings of the 2004 Conference on Empiri-cal Methods in Natural Language Processing,Barcelona, Spain, 388?395.Koehn, Philipp, et al 2007.
Open Source Toolkitfor Statistical Machine Translation: FactoredTranslation Models and Confusion Network De-coding.
Final Report of the 2006 Language En-gineering Workshop, Johns Hopkins University,Center for Speech and Language Processing.Och, Franz Josef.
2003.
Minimum error rate train-ing in statistical machine translation.
In Pro-ceedings of the 41st Annual Meeting of the Asso-ciation for Computational Linguistics, Sapporo,Japan, 160?167.Press, William H., et al 2002.
Numerical Recipesin C++.
Cambridge University Press, Cam-bridge, UK.303132330 2000 4000 6000 8000 10000 12000 14000BLEUscoretotal time in secondsFigure 1random walk, pre-restart pruning, 20iterationsrandom walk, post-restart pruning, 20iterationsrandom walk, nopruning, 20 iterationsuniform randomrestarts, 20 iterationsno restarts, n-best = 100no restarts, n-best = 200no restarts, n-best = 300303132330 5000 10000BLEUscoretotal time in secondsFigure 2random walk, pre-restartpruning, 20 iterationsrandom walk, post-restart pruning, 5iterationsrandom walk, post-restart pruning, 10iterationsrandom walk, post-restart pruning, 20iterationsrandom walk, post-restart pruning, 30iterationsrandom walk, nopruning, 5 iterationsrandom walk, nopruning, 10 iterationsrandom walk, nopruning, 20 iterations303132330 1 2 3 4 5 6 7 8 9 10 11BLEUscoredecoding iterationsFigure 3random walk, pre-restartpruning, 20 iterationsrandom walk, post-restart pruning, 5iterationsrandom walk, post-restart pruning, 10iterationsrandom walk, post-restart pruning, 20iterationsrandom walk, post-restart pruning, 30iterationsrandom walk, nopruning, 5 iterationsrandom walk, nopruning, 10 iterationsrandom walk, nopruning, 20 iterations303132330 1000 2000 3000 4000 5000 6000 7000BLEUscoreMERT time in secondsFigure 4random walk, pre-restartpruning, 20 iterationsrandom walk, post-restartpruning, 5 iterationsrandom walk, post-restartpruning, 10 iterationsrandom walk, post-restartpruning, 20 iterationsrandom walk, post-restartpruning, 30 iterationsrandom walk, nopruning, 5 iterationsrandom walk, nopruning, 10 iterationsrandom walk, nopruning, 20 iterations592
