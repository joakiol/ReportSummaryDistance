Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 653?661,Beijing, August 2010Structure-Aware Review Mining and SummarizationFangtao Li1, Chao Han1, Minlie Huang1, Xiaoyan Zhu1,Ying-Ju Xia2, Shu Zhang2 and Hao Yu21State Key Laboratory of Intelligent Technology and Systems?1Tsinghua National Laboratory for Information Science and Technology?1Department of Computer Science and Technology, Tsinghua University2Fujitsu Research and Development Centerfangtao06@gmail.com; zxy_dcs@tsinghua.edu.cnAbstractIn this paper, we focus on object feature 11 Introductionbased review summarization.
Different frommost of previous work with linguistic rules orstatistical methods, we formulate the reviewmining task as a joint structure tagging prob-lem.
We propose a new machine learningframework based on Conditional RandomFields (CRFs).
It can employ rich features tojointly extract positive opinions, negative opi-nions and object features for review sentences.The linguistic structure can be naturally inte-grated into model representation.
Besides li-near-chain structure, we also investigate con-junction structure and syntactic tree structurein this framework.
Through extensive experi-ments on movie review and product reviewdata sets, we show that structure-aware mod-els outperform many state-of-the-art ap-proaches to review mining.With the rapid expansion of e-commerce, peopleare more likely to express their opinions andhands-on experiences on products or servicesthey have purchased.
These reviews are impor-tant for both business organizations and personalcostumers.
Companies can decide on their strat-egies for marketing and products improvement.Customers can make a better decision when pur-1 Note that there are two meanings for word ?feature?.We use ?object feature?
to represent the target entity,which the opinion expressed on, and use ?feature?
asthe input for machine learning methods.chasing products or services.
Unfortunately,reading through all customer reviews is difficult,especially for popular items, the number of re-views can be up to hundreds or even thousands.Therefore, it is necessary to provide coherentand concise summaries for these reviews.Figure 1.
Feature based Review SummarizationInspired by previous work (Hu and Liu, 2004;Jin and Ho, 2009), we aim to provide object fea-ture based review summarization.
Figure 1shows a summary example for movie ?Gonewith the wind?.
The object (movie) features,such as ?movie?, ?actor?, with their correspond-ing positive opinions and negative opinions, arelisted in a structured way.
The opinions areranked by their frequencies.
This provides a con-cise view for reviews.
To accomplish this goal,we need to do three tasks:  1), extract all the ob-ject features and opinions; 2), determine the sen-timent polarities for opinions; 3), for each objectfeature, determine the relevant opinions, i.e.
ob-ject feature-opinion pairs.For the first two tasks, most previous studiesemploy linguistic rules or statistical methods (Huand Liu, 2004; Popescu and Etzioni 2005).
Theymainly use unsupervised learning methods,which lack an effective way to address infre-quent object features and opinions.
They are alsohard to incorporate rich overlapping features.Gone With The Wind:Movie:Positive: great, good, amazing, ?
, breathtakingNegative: bad, boring, waste time, ?
, mistakeActor:Positive: charming , brilliant , great, ?
, smartNegative: poor, fail, dirty, ?
, lameMusic:Positive: great, beautiful, very good, ?
, topNegative: annoying, noise, too long, ?
, unnecessary?
?653Actually, there are many useful features, whichhave not been fully exploited for review mining.Meanwhile, most of previous methods extractobject features, opinions, and determine the po-larities for opinions separately.
In fact, the objectfeatures, positive opinions and negative opinionscorrelate with each other.In this paper, we formulate the first two tasks,i.e.
object feature, opinion extraction and opi-nion polarity detection, as a joint structure tag-ging problem, and propose a new machine learn-ing framework based on Conditional RandomFields (CRFs).
For each sentence in reviews, weemploy CRFs to jointly extract object features,positive opinions and negative opinions, whichappear in the review sentence.
This frameworkcan naturally encode the linguistic structure.
Be-sides the neighbor context with linear-chainCRFs, we propose to use Skip-chain CRFs andTree CRFs to utilize the conjunction structureand syntactic tree structure.
We also propose anew unified model, Skip-Tree CRFs to integratethese structures.
Here, ?structure-aware?
refersto the output structure, which model the relation-ship among output labels.
This is significantlydifferent from the previous input structure me-thods, which consider the linguistic structure asheuristic rules (Ding and Liu, 2007) or input fea-tures for classification (Wilson et al 2009).
Ourproposed framework has the following advan-tages: First, it can employ rich features for re-view mining.
We will analyze the effect of fea-tures for review mining in this framework.Second, the framework can utilize the relation-ship among object features, positive opinionsand negative opinions.
It jointly extracts thesethree types of expressions in a unified way.Third, the linguistic structure information can benaturally integrated into model representation,which provides more semantic dependency foroutput labels.
Through extensive experiments onmovie review and product review, we show ourproposed framework is effective for review min-ing.The rest of this paper is organized as follows:In Section 2, we review related work.
We de-scribe our structure aware review mining me-thods in Section 3.
Section 4 demonstrates theprocess of summary generation.
In Section 5, wepresent and discuss the experiment results.
Sec-tion 6 is the conclusion and future work.2 Related WorkObject feature based review summary has beenstudied in several papers.
Zhuang et al (2006)summarized movie reviews by extracting objectfeature keywords and opinion keywords.
Objectfeature-opinion pairs were identified by using adependency grammar graph.
However, it used amanually annotated list of keywords to recognizemovie features and opinions, and thus the systemcapability is limited.
Hu and Liu (2004) pro-posed a statistical approach to capture objectfeatures using association rules.
They only con-sidered adjective as opinions, and the polaritiesof opinions are recognized with WordNet expan-sion to manually selected opinion seeds.
Popescuand Etzioni (2005) proposed a relaxation labe-ling approach to utilize linguistic rules for opi-nion polarity detection.
However, most of thesestudies focus on unsupervised methods, whichare hard to integrate various features.
Some stu-dies (Breck et al 2007; Wilson et al 2009; Ko-bayashi et al 2007) have used classificationbased methods to integrate various features.
Butthese methods separately extract object featuresand opinions, which ignore the correlationamong output labels, i.e.
object features and opi-nions.
Qiu et al (2009) exploit the relations ofopinions and object features by adding some lin-guistic rules.
However, they didn?t care the opi-nion polarity.
Our framework can not only em-ploy various features, but also exploit the corre-lations among the three types of expressions, i.e.object features, positive opinions, and negativeopinions, in a unified framework.
Recently, Jinand Ho (2009) propose to use Lexicalized HMMfor review mining.
Lexicalized HMM is a va-riant of HMM.
It is a generative model, which ishard to integrate rich, overlapping features.
Itmay encounter sparse data problem, especiallywhen simultaneously integrating multiple fea-tures.
Our framework is based on ConditionalRandom Fields (CRFs).
CRFs is a discriminativemodel, which can easily integrate various fea-tures.These are some studies on opinion mining withConditional Random Fields.
For example, withCRFs, Zhao et al(2008) and McDonald et al(2007) performed sentiment classification in sen-tence and document level; Breck et al(2007)identified opinion expressions from newswiredocuments; Choi et al (2005) determined opi-654nion holders to opinions also from newswire da-ta.
None of previous work focuses on jointly ex-tracting object features, positive opinions andnegative opinions simultaneously from reviewdata.
More importantly, we also show how toencode the linguistic structure, such as conjunc-tion structure and syntactic tree structure, intomodel representation in our framework.
This issignificantly different from most of previousstudies, which consider the structure informationas heuristic rules (Hu and Liu, 2004) or inputfeatures (Wilson et al 2009).Recently, there are some studies on joint sen-timent/topic extraction (Mei et al 2007; Titovand McDonald, 2008; Snyder and Barzilay,2007).
These methods represent reviews as sev-eral coarse-grained topics, which can be consi-dered as clusters of object features.
They arehard to indentify the low-frequency object fea-tures and opinions.
While in this paper, we willextract all the present object features and corres-ponding opinions with their polarities.
Besides,the joint sentiment/topic methods are mainlybased on review document for topic extraction.In our framework, we focus on sentence-levelreview extraction.3 Structure Aware Review Mining3.1 Problem DefinitionTo produce review summaries, we need to firstfinish two tasks: identifying object features, opi-nions, and determining the polarities for opi-nions.
In this paper, we formulate these twotasks as a joint structure tagging problem.
Wefirst describe some related definitions:Definition (Object Feature): is defined as wholetarget expression that the subjective expressionshave been commented on.
Object features can beproducts, services or their elements and proper-ties, such as ?character?, ?movie?, ?director?
formovie review, and ?battery?, ?battery life?,?memory card?
for product review.Definition (Review Opinion): is defined as thewhole subjective expression on object features.For example, in sentence ?The camera is easy touse?, ?easy to use?
is a review opinion.
?opinion?is used for short.Definition (Opinion Polarity): is defined as thesentiment category for review opinion.
In thispaper, we consider two types of polarities: posi-tive opinion and negative opinion.
For example,?easy to use?
belongs to positive opinion.For our review mining task, we need torepresent three types of expressions: object fea-tures, positive opinions, and negative opinions.These expressions may be words, or wholephrases.
We use BIO encoding for tag represen-tation, where the non-opinion and neutral opi-nion words are represented as ?O?.
With Nega-tion (N), which is only one word, such as ?not?,?don?t?, as an independent tag, there are totally 8tags, as shown in Table 1.
The following is anexample to denote the tags:The/O camera/FB comes/O with/O a/O piti-ful/CB 32mb/FB compact/FI flash/FI card/FI ./OFB Feature Beginning CB Negative BeginningFI Feature Inside CI Negative InsidePB Positive Beginning N Negation WordPI Positive Inside O OtherTable 1.
Basic Tag Set for Review Mining3.2 Structure Aware ModelIn this section, we describe how to encode dif-ferent linguistic structure into model representa-tion based on our CRFs framework.3.2.1 Using Linear CRFs.For each sentence in a review, our task is to ex-tract all the object features, positive opinions andnegative opinions.
This task can be modeled as aclassification problem.
Traditional classificationtools, e.g.
Maximum Entropy model (Berger etal, 1996), can be employed, where each word orphrase will be treated as an instance.
However,they independently consider each word orphrase, and ignore the dependency relationshipamong them.Actually, the context information plays an im-portant role for review mining.
For example,given two continuous words with same part ofspeech, if the previous word is a positive opi-nion, the next word is more likely a positive opi-nion.
Another example is that if the previousword is an adjective, and it is an opinion, thenext noun word is more likely an object feature.To this end, we formulate the review miningtask as a joint structure tagging problem, andpropose a general framework based on Condi-tional Random Fields (CRFs) (Lafferty et al,2001) which are able to model the dependencies655y1 yn-1y3y2 ynx1 xn-1x3x2 xn(a) Linear-chain  CRFsy4x1 xn-1x3x2 xnxn-2?x4y1 yn-2y3 yny2 yn-1(c) Tree-CRFsy4x1 xn-1x3x2 xnxn-2?x4y1 yn-2y3 yny2 yn-1(d) Skip-Tree CRFs(b) Skip-chain  CRFsFigure 2 CRFs modelsbetween nodes.
(See Section 3.2.5 for moreabout CRFs)In this section, we propose to use linear-chainCRFs to model the sequential dependencies be-tween continuous words, as discussed above.
Itviews each word in the sentence as a node, andadjacent nodes are connected by an edge.
Thegraphical representation is shown in Figure 2(a).Linear CRFs can make use of dependency rela-tionship among adjacent words.3.2.2 Leveraging Conjunction StructureWe observe that the conjunctions play importantroles on review mining: If the words or phrasesare connected by conjunction ?and?, they mostlybelong to the same opinion polarity.
If the wordsor phrases are connected by conjunction ?but?,they mostly belong to different opinion polarity,as reported in (Hatzivassiloglou and McKeown,1997; Ding and Liu, 2007).
For example, ?Thisphone has a very cool and useful feature ?
thespeakerphone?, if we only detect ?cool?, it ishard to determine its opinion polarity.
But if wesee ?cool?
is connected with ?useful?
by con-junction ?and?, we can easily acquire the polari-ty of ?cool?
as positive.
This conjunction struc-ture not only helps to determine the opinions, butalso helps to recognize object features.
For ex-ample, ?I like the special effects and music inthis movie?, with word ?music?
and conjunction?and?, we can easily detect that ?special effects?as an object feature.To model the long distance dependency withconjunctions, we use Skip-chain CRFs model todetect object features and opinions.
The graphi-cal representation of a Skip-chain CRFs, given inFigure 2(b), consists of two types of edges: li-near-edge (to ) and skip-edge (to ).The linear-edge is described as linear CRFs.
Theskip-edge is imported as follows:We first identify the conjunctions in the re-view sentence, with a collected conjunction set,including ?and?, ?but?, ?or?, ?however?, ?al-though?
etc.
For each conjunction, we extract itsconnected two text sequences.
The nearest twowords with same part of speech from the twotext sequences are connected with the skip-edge.Here, we just consider the noun, adjective, andadverb.
For example, in ?good pictures andbeautiful music?, there are two skip-edges: oneconnects two adjective words ?good?
and ?beau-tiful?
; the other connects two nouns ?pictures?and ?music?.
We also employ the general senti-ment lexicons, SentiWordNet (Esuli and Sebas-tiani, 2006), to connect opinions.
Two nearestopinion words, detected by sentiment lexicon,from two sequences, will also be connected byskip-edge.
If the nearest distance exceeds thethreshold, this skip edge will be discarded.
Here,we consider the threshold as nine.Skip-chain CRFs improve the performance ofreview mining, because it naturally encodes theconjunction structure into model representationwith skip-edges.3.2.3 Leveraging Syntactic Tree StructureBesides the conjunction structure, the syntactictree structure also helps for review mining.
Thetree denotes the syntactic relationship amongwords.
In a syntactic dependency representation,each node is a surface word.
For example, thecorresponding dependency tree (Klein and Man-ning, 2003) for the sentence, ?I really like thislong movie?, is shown in Figure 3.y1 yn-1y3y2 ynx1 xn-1x3x2 xn656likelongthisreally movieInsubj dobjadvmoddet amodFigure 3.
Syntactic Dependency Tree RepresentationIn linear-chain structure and skip-chain structure,?like?
and ?movie?
have no direct edge, but insyntactic tree, ?movie?
is directly connectedwith ?like?, and their relationship ?dobj?
is alsoincluded, which shows ?movie?
is an objectiveof ?like?.
It can provide deeper syntactic depen-dencies for object features, positive opinions andnegative opinions.
Therefore, it is important toconsider the syntactic structure in the reviewmining task.In this section, we propose to use Tree CRFs tomodel the syntactic tree structure for reviewmining.
The representation of a Tree CRFs isshown in Figure 2(c).
The syntactic tree structureis encoded into our model representation.
Eachnode is corresponding to a word in the depen-dency tree.
The edge is corresponding to depen-dency tree edge.
Tree CRFs can make use of de-pendency relationship in syntactic tree structureto boost the performance.3.2.4 Integrating Conjunction Structure andSyntactic Tree StructureConjunction structure provides the semantic re-lations correlated with conjunctions.
Syntactictree structure provides dependency relation inthe syntactic tree.
They represent different se-mantic dependencies.
It is interesting to considerthese two dependencies in a unified model.
Wepropose Skip-Tree CRFs, to combine these twostructure information.
The graphical representa-tion of a Skip-Tree CRFs, given in Figure 2(d),consists of two types of edges: tree edges andconjunction skip-edges.
We hope to simulta-neously model the dependency in conjunctionstructure and syntactic tree structure.We also notice that there is a relationship?conj?
in syntactic dependency tree.
However,we find that it only connects two head words fora few coordinating conjunction, such as ?and",?or", ?but?.
Our designed conjunction skip-edgeprovides more information for joint structuretagging.
We analyze more conjunctions to con-nect not only two head words, but also the wordswith same part of speech.
We also connect thewords with sentiment lexicon.
We will show thatthe skip-tree CRFs, which combine the twostructures, is effective in the experiment section.3.2.5 Conditional Random FieldsA CRFs is an undirected graphical model G ofthe conditional distribution (	|).
Y are therandom variables over the labels of the nodesthat are globally conditioned on X, which are therandom variables of the observations.
The condi-tional probability is defined as:P(|)=1()   (, 	|,),+   (, 	|,),where Z(x) is the normalization factor, is thestate function on node, is the transition func-tions on edge, and ?and are parameters toestimate (Sutton and McCallum, 2006).Inference and Parameter Estimation.
For Li-near CRFs, dynamic programming is used tocompute the maximum a posteriori (MAP) of Ygiven X.
For more complicated graphs withcycles, we employ Tree Re-Parameterization(TRP) algorithm (Wainwright et al 2001) forapproximate inference.Given the training Data  =  {(), ()},the parameter estimation is to determine the pa-rameters based on maximizing the log-likelihood!
"=#$%& (()|()).
In Linear CRFsmodel, dynamic programming and L-BFGS al-gorithm can be used to optimize objective func-tion !
", while for complicated CRFs, TRP isused instead to calculate the marginal probabili-ty.3.3 Feature SpaceIn this section, we describe the features used inthe learning methods.
All the features are listedin Figure 4.
Word features include the word?stoken, lemma, and part of speech.
The adjacentwords?
information is considered.
We detectwhether the negation words appear in the pre-vious four words as a binary feature.
We alsodetect whether this word is the superlative form,such as ?best?, and comparative form, such as?better?, as binary features.
Two types of dictio-naries are employed.
We use WordNet to acquirethe synonyms and antonyms for each word.
Sen-tiWordNet (Esuli and Sebastiani, 2006) is usedto acquire the prior polarity for each word.
Weuse the words with positive or negative score657Figure 4.
Features for learning Methodsabove a threshold (0.6).
Sentence Feature pro-vides sentence level information.
It includes thecount of positive words and negative words,which are detected by SentiWordNet.
We alsoincorporate the count of negation words as a fea-ture.
There are some syntactic features from de-pendency tree.
Parent word and its polarity areconsidered.
We also detect if the word is subject,object or copular.
For edge features, the conjunc-tion words are incorporated as correspondingskip-edge features.
The syntactic relationship isconsidered as a feature for corresponding tree-edge.
For classification and linear CRFs models,we just add this edge features as general features.4 Review Summary GenerationAfter extracting the object features and opinions,we need to extract the relevant opinions for eachfeature.
In this paper, we identify the nearestopinion word/phrase for each object feature asobject feature-opinion pair, which is widely usedin previous work (Hu and Liu, 2004; Jin and Ho,2009).
The review summary is generated as alist of structured object feature-opinion pairs, asshown in Figure 1.5 Experiment5.1 Experiment setupData Set: For our structure tagging task, weneed to know the labels for all the words in re-views.
In this paper, we manually annotate twotypes of these review data sets.
One is moviereview, which contains five movies with totally500 reviews.
The other is product review, whichcontains four products with totally 601 reviews.We need to label all object features, positiveopinions, negative opinions, and the object fea-ture-opinion pairs for all sentences.
Each sen-tence is labeled by two annotators.
The conflictis checked by the third person.
Finally, we ac-quire 2207 sentences for movie review and 2533sentences for product review.
For each type, in-cluding movie and product, the data set is di-vided into five parts.
We select four parts astraining data, and the fifth part as testing data.Evaluation Metric:Precision, Recall and F measure are used to testour results, as Jin and Ho (2009).5.2 BaselinesFirst word Second Word Third WordJJ NN or NNS AnythingRB, RBR or RBS JJ NN or NNSJJ JJ NN or NNSNN or NNS JJ Not NN or NNSTable 2.
Rules in rule  based methodRule based Method:The rule based method is used in Jin and Ho(2009), which is motivated by (Hu and Liu, 2004;Turney, 2002).
The employed rules are shown inTable 2.
The matching adjective is identified asopinion, and matching nouns are extracted asobject features.
To determine the polarities of theopinions, 25 positive adjectives and 25 negativeadjectives are used as seeds, and then expandedby searching synonyms and antonyms in Word-Net.
The polarity of a word is detected by check-ing the collected lists.Lexicon based Method:The object features and opinions extraction issame as rule based method.
The general senti-ment lexicon SentiWordNet is employed todetect the polarity for each word.Lexicalized HMM:The object features and opinions are identifiedby Lexicalized HMM (L-HMM), as Jin and Ho(2009).
L-HMM is a variant of HMM.
It has twoobservations.
The current tag is not only relatedWord Feature:Word tokenWord lemmaWord part of speechPrevious word token, lemma, part of speechNext word token, lemma, part of speechNegation word appears in previous 4 wordsIs superlative degreeIs comparative degreeDictionary FeatureWordNet SynonymWordNet AntonymSentiWordNet Prior PolaritySentence FeatureNum of positive words in SentiWordNetNum of negative words in SentiWordNetNum of Negation wordSyntactic Features:Parent wordParent SentiWordnet Prior PolarityIn subjectIn copularIn objectEdge FeatureConjunction wordSyntactic relationship658Methods Object Features Positive Opinions Negative Opinions OverallP(%) R(%) F(%) P(%) R(%) F(%) P(%) R(%) F(%) P(%) R(%) F(%)MovieReviewRule 41.2 32.3 36.2 82.9 31.1 45.3 23.5 13.7 17.3 49.2 25.7 33.8Lexicon 41.2 32.3 36.2 64.0 38.1 47.8 19.6 6.8 10.2 41.6 25.8 31.8L-HMM 88.0 52.6 65.9 82.1 49.6 61.9 65.9 41.1 50.6 78.7 47.8 59.5MaxEnt 83.4 75.1 79.1 82.2 65.0 72.6 74.1 29.5 42.2 79.9 56.5 66.2Linear CRFs 81.8 78.4 80.1 79.1 63.9 70.7 75.8 32.2 45.2 79.0 58.2 67.0ProductReviewRule 53.5 35.6 42.8 74.4 22.5 34.6 17.1 8.9 11.7 48.3 22.3 30.6Lexicon 53.5 35.6 42.8 48.9 29.7 40.0 14.7 3.7 5.9 39.1 23.0 29.0L-HMM 83.9 48.7 61.6 90.3 56.8 69.8 47.2 25.2 32.9 73.8 43.6 54.8MaxEnt 83.4 55.1 66.4 82.2 65.0 72.6 64.1 30.0 40.4 76.6 49.9 60.4Linear CRFs 91.1 56.3 69.6 88.7 70.4 78.5 67.7 32.6 44.0 82.5 53.1 64.6Table 3.
Comparison Results with Baselines(the learning methods only employ word token and part of speech as features).Methods Object Features Positive Opinions Negative Opinions OverallP(%) R(%) F(%) P(%) R(%) F(%) P(%) R(%) F(%) P(%) R(%) F(%)MovieReviewMaxEnt 82.8 76.6 79.6 80.3 67.8 73.5 82.8 36.3 50.5 81.9 60.2 69.4Linear CRFs 83.5 75.4 79.2 77.8 71.4 74.5 70.9 53.4 60.9 77.4 66.8 71.7Skip CRFs 83.9 78.7 81.2 81.8 73.4 77.4 75.2 62.3 68.2 80.3 71.5 75.7Tree CRFs 84.1 79.0 81.5 82.7 75.4 78.9 76.7 61.0 67.9 81.2 72.2 76.2SkipTreeCRFs 85.5 82.0 83.7 82.3 80.0 81.1 80.2 66.4 72.7 82.6 76.2 79.3ProductReviewMaxEnt 80.0 70.8 75.1 85.6 65.7 74.3 65.1 37.8 47.8 76.9 58.1 66.2Linear CRFs 84.0 72.9 78.1 86.7 72.0 78.6 60.4 49.6 54.5 77.0 64.8 70.4Skip CRFs 84.8 73.5 78.7 87.8 74.5 80.6 73.1 50.4 59.6 81.2 66.1 73.2Tree CRFs 83.0 72.7 77.5 86.6 73.4 79.4 64.3 54.8 59.2 78.0 67.0 72.1SkipTreeCRFs 87.1 74.1 80.1 91.8 76.7 83.6 81.1 57.0 67.0 86.6 69.3 77.0Table 4.
Comparative experiments with all featureswith the previous tag, but also correlates withprevious observations.
They use word token andpart of speech as two features.Classification based Method:We also formulate the review mining as aclassification task.
Each word is considered as aninstance.
Maximum Entropy (MaxEnt) is used inthis paper.5.3 Experiment resultsSince Lexicalized HMM employ word token andpart of speech as features (Jin and Ho, 2009), wefirst conduct comparative experiments with thesetwo features for learning methods.
Table 3shows the results.
The rule based method is alittle better than lexicon based method.
Senti-WordNet is designed for general opinion mining,which may be not suitable for domain specificreview mining task.
For rule based method, theseeds are selected in the review domain, which ismore suitable for domain specific task.
However,both methods achieve low performance.
Thisbecause that they only employ simple linguisticrules to extract object features and opinions,which is not effective for infrequent cases andphrase cases.
Lexicalized HMM is an extensionof HMM.
It uses word token and part of speechas two observations.
The current tag is not onlyrelated with the previous tag, but also correlateswith previous two observations.
LexicalizedHMM can employ dependency relationshipamong adjacent words.
However, it doesn?tachieve the expected result.
This is because thatLexicalized HMM is a generative model, whichis hard to incorporate rich overlapping features.Even Lexicalized HMM uses linear interpolationsmoothing technique.
The data sparsity problemseriously hurt the performance.
There are manysentences with zero probability.
MaxEnt classifi-er is a discrimitive model, which can incorporatevarious features.
However, it independently clas-sifies each word, and ignores the dependencyamong successive words.
The linear CRFs mod-el achieves best performances for movie review,and product review in overall F-score.
This isbecause that, in our joint structure taggingframework, linear CRFs can employs the globalstructure to make use of the adjacent dependencyrelation, and easily incorporate various featuresto boost the performance.We also conduct the comparative experimentswith all features.
From Table 4, we can see thatlinear CRFs, which consider the chain structure,659Object Features Positive Opinions Negative Opinions OverallP(%) R(%) F(%) P(%) R(%) F(%) P(%) R(%) F(%) P(%) R(%) F(%)Basic 83.8 79.2 81.4 79.5 71.0 75.0 76.1 37.0 49.8 79.8 62.4 70.0Basic +Word Feature 84.0 81.4 82.7 79.2 75.6 77.4 78.9 48.6 60.2 80.7 68.6 74.1Basic +Dictionary 80.5 76.6 78.5 82.7 76.3 79.4 76.5 60.3 67.4 80.0 71.0 75.2Basic +Sentence 82.5 75.6 78.9 80.4 75.4 77.8 84.0 46.7 60.0 82.3 65.9 73.2Basic +Syntactic 84.5 70.8 77.0 79.6 73.9 76.7 79.5 47.9 59.8 81.2 64.2 71.7Basic + Edge 84.1 80.1 82.1 79.5 75.4 77.4 82.4 47.9 60.6 82.0 67.8 74.2All Features 85.5 82.0 83.7 82.3 80.0 81.1 80.2 66.4 72.7 82.6 76.2 79.3Table 5.
Feature Evaluations with Skip Tree CRFs (movie)still achieve better results than MaxEnt classifiermethod.
Skip-chain CRFs model the conjunctionstructure in the sentence.
We can see that theSkip-chain CRFs achieve better results than li-near CRFs.
This shows that conjunction struc-ture is really important for review mining.
Forexample ?although this camera takes great pic-tures, it is extremely fragile.
?, ?fragile?
is notcorrectly classified by MaxEnt and Linear CRFs.But the Skip-chain CRFs can correctly classify?fragile?
as negative opinion, with conjunction?although?, and the skip edge between ?great?and ?fragile?.
Tree CRFs encode the syntactictree structure into model representation.
Com-pared with linear-CRFs, the performances areimproved for most of expression identificationtasks, except for a little decline for product ob-ject feature, which may be because that the tags?FB?
and ?FI?
are out of order when transferringto tree structure.
These are no significant differ-ence between Skip-Chain CRFs and Tree CRFs.Conjunction structure and syntactic structurerepresent the semantic dependency from differ-ent views.
When integrating these two types ofdependencies, the Skip-Tree CRFs achieve betteroverall results than both Skip-Chain CRFs andTree CRFs.Table 5 shows the movie review result forSkip Tree model for different types of features.The basic feature only employs word token asfeature set.
Other features are defined as shownin Figure 4.
By adding different features, we findthat they all achieve overall improvements thanbasic feature.
The dictionary features are themost important features, especially for positiveopinion and negative opinion identification,which shows the importance of prior word?s sen-timent.
Word features also play important roles:Part of speech is reported useful in several pa-pers (such as Jin and Ho, 2009); the superlativeand comparative forms are good indicators foropinion words.
Syntactic features acquire limitedimprovement in this experiment.
They may over-lap with CRF based structure model.
We alsofind that sentence level features contribute to thereview mining task.
Edge feature is also impor-tant.
It makes the skip edge and tree edge withthe semantic representation.
When combing allthe features, the result is significantly improvedcompared with any single feature set, whichshows that it is crucial to integrate various fea-tures for review mining.A review summary example, generated by ourmethods, is shown in Figure 1.6 ConclusionIn this paper, we formulate the review miningtask as a joint structure tagging problem.
A newframework based on Conditional Random Fieldsis proposed.
The framework can employ richfeatures to simultaneously extract object fea-tures, positive opinions and negative opinions.With this framework, we investigate the chainstructure, conjunction structure and syntactic treestructure for review mining.
A new unified mod-el, called skip tree CRFs, is proposed for reviewmining.
Through extensive experiments, weshow that our proposed framework is effective.It outperforms many state-of-the-art methods.In future work, we will improve the objectfeature-opinion pair detection with other learn-ing methods.
We also want to cluster the relatedobject features to provide more concise reviewsummary.AcknowledgementThis work was partly supported by ChineseNSF grant No.60973104 and No.
60803075, agrand from Fujitsu Research Center, and a grantfrom the International Development ResearchCenter, Ottawa, Canada.
We also thank the ano-nymous reviewers, Qiang Yang, Lei Zhang andQiu Long for their valuable comments.660ReferencesA.
Berger and Vincent Della Pietra and Stephen A.Della Pietra.
1996.
A Maximum Entropy Approachto Natural Language Processing.
ComputationalLinguistics.E.
Breck, Y. Choi, and C. Cardie.
2007.
Identifyingexpressions of opinion in context.
Proceedings ofthe International Joint Conference on Artificial In-telligence (IJCAI).Y.
Choi, Claire Cardie, Ellen Riloff, and SiddharthPatwardhan.
2005.
Identifying Sources of Opi-nions with Conditional Random Fields and Extrac-tion Patterns.
In Proceedings of HLT-EMNLP.X.
Ding and Bing Liu.
2007.
The Utility of LinguisticRules in Opinion Mining.
In Proceedings of SIGIR.A.
Esuli and Fabrizio Sebastiani.
2006.
SENTI-WORDNET: A Publicly Available Lexical Re-source for Opinion Mining.
In Proceedings ofLREC.V.
Hatzivassiloglou and K. McKeown.
1997.
Predict-ing the semantic orientation of adjectives.
Proceed-ings of the Joint ACL/EACL Conference.M.
Hu and B. Liu.
2004.
Mining and SummarizingCustomer Reviews.
Proceedings of the 10th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining (KDD?04).W.
Jin, H.H.
Ho.
2009.
A novel lexicalized HMM-based learning framework for web opinion mining.Proceedings of the 26th Annual International Con-ference on Machine Learning (ICML 2009).J.
Lafferty, A. McCallum, F. Pereira.
2001.
Condi-tional random fields: Probabilistic models for seg-menting and labeling sequence data.
In: Proc.
18thInternational Conf.
on Machine Learning (ICML).D.
Klein and Christopher D. Manning.
2003.
FastExact Inference with a Factored Model for NaturalLanguage Parsing.
In Advances in Neural Informa-tion Processing Systems 15 (NIPS 2002),N. Kobayashi, K. Inui, and Y. Matsumoto.
2007.Opinion Mining from Web documents: Extractionand Structurization.
Journal of Japanese society forartificial intelligence.R.
McDonald, K. Hannan, T. Neylon, M. Wells, and J.Reynar.
2007.
Structured models for fine-to-coarsesentiment analysis.
Proceedings of the Associationfor Computational Linguistics (ACL).Q.
Mei, X. Ling, M. Wondra, H. Su, and C. Zhai.2007.
Topic sentiment mixture: modeling facetsand opinions in weblogs.
In Proceedings of the16th international conference on World Wide Web.A.
Popescu and O. Etzioni.
2005.
Extracting ProductFeatures and Opinions from Reviews.
Proceedingsof 2005 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP?05), 339-346.G.
Qiu, B. Liu, J. Bu and C. Chen.
2009.
ExpandingDomain Sentiment Lexicon through Double Prop-agation, International Joint Conference on Artifi-cial Intelligence (IJCAI-09).B.
Snyder and R. Barzilay.
2007.
Multiple AspectRanking using the Good Grief Algorithm", In Proc.of NAACLC.
Sutton, A. McCallum.
2006.
An Introduction toConditional Random Fields for Relational Learn-ing.
In "Introduction to Statistical RelationalLearning".
Edited by Lise Getoor and Ben Taskar.MIT Press.I.
Titov and R. McDonald.
2008.
A Joint Model ofText and Aspect Ratings for Sentiment Summari-zation.In Proceeding of the Association for Com-putational Linguistics (ACL).P.
D. Turney.
2002.
Thumbs up or Thumbs Down?Semantic Orientation Applied to UnsupervisedClassification of Reviews.
Proceedings of Associa-tion for Computational Linguistics (ACL?02).M.
Wainwright, T. Jaakkola, and A. Willsky.
2001.Tree-based reparameterization for approximate es-timation on graphs with cycles.
In Proceedings ofAdvances in Neural Information Processing Sys-tems (NIPS'2001).
pp.
1001-1008.T.
Wilson, Janyce Wiebe, and Paul Hoffmann 2009.Recognizing Contextual Polarity: an exploration offeatures for phrase-level sentiment analysis.
Com-putational Linguistics 35(3).J.
Zhao, Kang Liu, Gen Wang.
2008.
Adding Redun-dant Features for CRFs-based Sentence SentimentClassification.
In Proceedings of EMNLP.L.
Zhuang, Feng Jing and Xiaoyan Zhu.
2006.
MovieReview Mining and Summarization.
In Proceed-ings of CIKM.661
