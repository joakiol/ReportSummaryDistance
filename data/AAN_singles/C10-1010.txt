Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 80?88,Beijing, August 2010Self-Annotation for Fine-Grained Geospatial Relation ExtractionAndre Blessing Hinrich Schu?tzeInstitute for Natural Language ProcessingUniversita?t Stuttgartner@ifnlp.orgAbstractA great deal of information on the Web isrepresented in both textual and structuredform.
The structured form is machine-readable and can be used to augment thetextual data.
We call this augmentation?
the annotation of texts with relationsthat are included in the structured data ?self-annotation.
In this paper, we intro-duce self-annotation as a new supervisedlearning approach for developing and im-plementing a system that extracts fine-grained relations between entities.
Themain benefit of self-annotation is that itdoes not require manual labeling.
The in-put of the learned model is a represen-tation of the free text, its output struc-tured relations.
Thus, the model, oncelearned, can be applied to any arbitraryfree text.
We describe the challenges forthe self-annotation process and give re-sults for a sample relation extraction sys-tem.
To deal with the challenge of fine-grained relations, we implement and eval-uate both shallow and deep linguistic anal-ysis, focusing on German.1 IntroductionIn the last years, information extraction has be-come more important in domains like context-aware systems (e.g.
Nexus (Du?rr et al, 2004)) thatneed a rich knowledge base to make the right de-cisions in different user contexts.
Geospatial dataare one of the key features in such systems andneed to be represented on different levels of de-tail.
Data providers do not cover all these lev-els completely.
To overcome this problem, fine-grained information extraction (IE) methods canbe used to acquire the missing knowledge.
Wedefine fine-grained IE as methods that recognizeentities at a finer grain than standard categorieslike person, location, and organization.
Further-more, the quality of the data in context-aware sys-tems plays an important role and updates by an in-formation extraction component can increase theoverall user acceptance.For both issues an information extraction sys-tem is required that can handle fine-grained rela-tions, e.g., ?X is a suburb of Y?
or ?the river Xis a tributary of Y?
?
as opposed to simple con-tainment.
The World Wide Web offers a wealth ofinformation about geospatial data and can be usedas source for the extraction task.
The extractioncomponent can be seen as a kind of sensor that wecall text senor (Blessing et al, 2006).In this paper, we address the problem of de-veloping a flexible system for the acquisition ofrelations between entities that meets the abovedesiderata.
We concentrate on geospatial entitieson a fine-grained level although the approach isin principle applicable to any domain.
We usea supervised machine learning approach, includ-ing several features on different linguistic lev-els, to build our system.
Such a system highlydepends on the quality and amount of labeleddata in the training phase.
The main contri-bution of this paper is the introduction of self-annotation, a novel approach that allows us toeliminate manual labeling (although training setcreation also involves costs other than labeling).Self-annotation is based on the fact that WordWide Web sites like Wikipedia include, in addi-80tion to unstructured text, structured data.
We usestructured data sources to automatically annotateunstructured texts.
In this paper, we use GermanWikipedia data because it is a good source for theinformation required for our context-aware sys-tem and show that a system created without man-ual labeling has good performance.Our trained model only uses text, not the struc-tured data (or any other markup) of the input doc-uments.
This means that we can train an informa-tion extractor on Wikipedia and then apply it toany text, regardless of whether this text also con-tains structured information.In the first part of this paper, we discussthe challenges of self-annotation including someheuristics which can easily be adapted to differentrelation types.
We then describe the architectureof the extraction system.
The components we de-velop are based on the UIMA (Unstructured In-formation Management Architecture) framework(Hahn et al, 2008) and include two linguistic en-gines (OpenNLP1, FSPar).
The extraction task isperformed by a supervised classifier; this classi-fier is also implemented as a UIMA componentand uses the ClearTK framework.
We evaluate ourapproach on two types of fine-grained relations.2 Related workJiang (2009) also addresses the issue of super-vised relation extraction when no large manuallylabeled data set is available.
They use only a fewseed instances of the target relation type to traina supervised relation extraction system.
However,they use multi-task transfer learning including alarge amount of labeled instances of other relationtypes for training their system.
In contrast, ourwork eliminates manual labeling by using struc-tured data to annotate the relations.Wu and Weld (2007) extract facts from in-foboxes and link them with their correspondingrepresentation in the text.
They discuss several is-sues that occur when using infoboxes as a knowl-edge base, in particular, (i) the fact that infoboxesare incomplete; and (ii) schema drift.
Schemadrift occurs when authors over time use differ-ent attribute names to model facts or the same1http://opennlp.sourceforge.net/attributes are used to model different facts.
Sothe semantics of the infoboxes changes slightlyand introduces noise into the structured informa-tion.
Their work differs from self-annotation inthat they are not interested in the creation of self-annotated corpora that can be used as training datafor other tasks.
Their goal is to develop methodsthat make infoboxes more consistent.Zhang and Iria (2009) use a novel entity extrac-tion method to automatically generate gazetteersfrom seed lists using Wikipedia as knowledgesource.
In contrast to our work they need struc-tured data for the extraction while our system fo-cuses on the extraction of information from un-structured text.
Methods that are applicable toany unstructured text (not just the text in theWikipedia) are needed to increase coverage be-yond the limited number of instances covered inWikipedia.Nothman et al (2009) also annotateWikipedia?s unstructured text using struc-tured data.
The type of structured data they use ishyperlinking (as opposed to infoboxes) and theyuse it to derive a labeled named entity corpus.They show that the quality of the annotation iscomparable to other manually labeled namedentity recognition gold standards.
We interprettheir results as evidence that self-annotation canbe used to create high quality gold standards.3 Task definitionIn this section, we describe the annotation task;give a definition of the relation types covered inthis paper; and introduce the extraction model.We focus on binary relations between two re-lation arguments occurring in the same sentence.To simplify the self-annotation process we restrictthe first argument of the relation to the main en-tity of the Wikipedia article.
As we are buildingtext sensors for a context aware system, relationsbetween geospatial entities are of interest.
Thuswe consider only relations that use a geospatialnamed entity as second argument.We create the training set by automaticallyidentifying all correct binary relations in the text.To this end, we extract the relations from thestructured part of the Wikipedia, the infoboxes.Then we automatically find the corresponding81sentences in the text and annotate the relations(see section 4).
All other not yet marked binaryrelations between the main entity and geospatialentities are annotated as negative samples.
Theresult of this step is a self-annotated training set.In the second step of our task, the self-annotated training set is used to train the extrac-tion model.
The model only takes textual featuresas input and can be applied to any free text.3.1 Classification task and relations usedOur relation extraction task is modeled as a classi-fication task which considers a pair of named en-tities and decides whether they occur in the re-quested relation or not.
The classifier uses ex-tracted features for this decision.
Features be-long to three different classes.
The first class con-tains token-based features and their linguistic la-bels like part-of-speech, lemma, stem.
In the sec-ond class, we have chunks that aggregate one ormore tokens into complex units.
Dependency re-lations between the tokens are represented in thethird class.Our classifier is applicable to a wide spectrumof geospatial relation types.
For the purposes ofa focused evaluation, we selected two relations.The first type contains rivers and the bodies ofwater into which they flow.
We call it river-bodyOfWater relation.
Our second type is com-posed of relations between towns and the corre-sponding suburb.
We call this town-suburb rela-tion.3.2 Wikipedia as resourceWikipedia satisfies all corpus requirements for ourtask.
It contains a lot of knowledge about geospa-tial data with unstructured (textual) and structuredinformation.
We consider only German Wikipediaarticles because our target application is a Germancontext aware system.
In relation extraction forGerman, we arguably face more challenges ?
e.g.,more complex morphology and freer word order ?than we would in English.For this work we consider only a subset of theGerman Wikipedia.
We use all articles that belongto the following categories: Rivers by country,Mountains by country, Valleys by country, Islandsby country, Mountain passes by country, Forestsby country and Settlements by country.For the annotation task we use the structuralcontent of Wikipedia articles.
Most articles be-longing to the same categories use similar tem-plates to represent structured information.
Onetype of template is the infobox, which con-tains pairs of attributes and their values.
Theseattribute-value pairs specify a wide range ofgeospatial relation types including fine-grainedrelations.
In this work we consider only the in-fobox data and the article names from the struc-tured data.For context-aware systems fine-grained relationtypes are particularly relevant.
Such relations arenot represented in resources like DBPedia (Aueret al, 2007) or Yago (Suchanek et al, 2007) al-though they also consist of infobox data.
Hence,we have to build our own extraction component(see section 5.2) when using infoboxes.4 Self-AnnotationSelf-annotation is a two-fold task.
First, the struc-tured data, in our case the infoboxes of Wikipediaarticles, must be analyzed to get al relevantattribute-value pairs.
Then all relevant geospatialentities are marked and extracted.
In a second stepthese entities must be matched with the unstruc-tured data.In most cases, the extraction of the named en-tities that correspond to the required relations istrivial because the values in the infoboxes con-sist only of one single entity or one single link.But in some cases the values contain mixed con-tent which can include links, entities and evenfree text.
In order to find an accurate extractionmethod for those values we have developed sev-eral heuristics.
See section 5.2 for discussion.The second task links the extracted structureddata to tokens in the textual data.
Pattern basedstring matching methods are not sufficient to iden-tify all relations in the text.
In many cases, mor-phological rules need to be applied to identifythe entities in the text.
In other cases, the pre-processed text must be retokenized because theborders of multi-word expressions are not consis-tent with the extracted names in step one.
Oneother issue is that some named entities are a subsetof other named entities (Lonau vs. kleine Lonau;82Figure 1: Infobox of the German Wikipedia articleabout Gollach.similar to York vs. New York).
We have to use alongest match strategy to avoid such overlappingannotations.The main goal of the self-annotation task isto reach the highest possible annotation quality.Thus, only complete extracted relations are usedfor the annotation process while incomplete dataare excluded from the training set.
This procedurereduces the noise in the labeled data.4.1 ExampleWe use the river-bodyOfWater relation betweenthe two rivers Gollach and Tauber to describe theself-annotation steps.Figure 1 depicts a part of the infobox for theGerman Wikipedia article about the river Gollach.For this relation the attribute Mu?ndung ?mouth?
isrelevant.
The value contains unstructured infor-mation (i.e., text, e.g.
bei ?at?
Bieberehren) andstructured information (the link from Bieberehrento its Wikipedia page).
The relation we want toextract is that the river Gollach flows into the riverTauber.BieberehrensiesieTauberGollachGollach TauberSieGollachTauberFigure 2: Textual content of the GermanWikipedia article about Gollach.
All named enti-ties which are relevant for the river-bodyOfWaterrelation are highlighted.
This article contains twoinstances for the relation between Gollach andTauber.Figure 2 shows the textual content of the Gol-lach article.
We have highlighted all relevantnamed entities for the self-annotation process.This includes the name of the article and instancesof the pronoun sie referring to Gollach.
Ourmatching algorithm identifies two sentences aspositive samples for the relation between Gollachand Tauber:?
(i) Die Gollach ist ein rechter Nebenfluss derTauber in Mittel- und Unterfranken.
(TheGollach is a right tributary of the Tauber inMiddle and Lower Franconia.)?
(ii) Schlie?lich mu?ndet sie in Bieberehrenauf 244 m in die Tauber.
(Finally, it dis-charges in Bieberehren at 244 m above MSLinto the Tauber.
)5 ProcessingIn this section we describe how the self-annotationmethod and relation extraction is implemented.First we introduce the interaction with theWikipedia resource to acquire the structuredand unstructured information for the processing83pipeline.
Second we present the components ofthe UIMA pipeline which are used for the relationextraction task.5.1 Wikipedia interactionWe use the JWPL API (Zesch et al, 2008) topre-process the Wikipedia data.
This interfaceprovides functions to extract structured and un-structured information from Wikipedia.
How-ever, many Wikipedia articles do not adhere tovalid Wikipedia syntax (missing closing brack-ets etc.).
The API also does not correctly handleall Wikipedia syntax constructions.
We thereforehave enhanced the API for our extraction task toget high quality data for German Wikipedia arti-cles.5.2 Infobox extractionAs discussed in section 4 infoboxes are the keyresource for the self-annotation step.
Howeverthe processing of infoboxes that include attribute-value pairs with mixed content is not trivial.For each new relation type an initial manual ef-fort is required.
However, in comparison to thecomplete annotation of a training corpus, this ef-fort is small.
First the attributes used in the in-foboxes of the Wikipedia articles relevant for aspecific relation have to be analyzed.
The resultsof this analysis simplify the choice of the cor-rect attributes.
Next, the used values of these at-tributes must be investigated.
If they contain onlysingle entries (links or named entities) the extrac-tion is trivial.
However, if they consist of mixedcontent (see section 4.1) then specific extractionmethods have to be applied.
We investigated dif-ferent heuristics for the self-annotation process toget a method that can easily be adapted to new re-lation types.Our first heuristic includes a set of rules spec-ifying the extraction of the values from the in-foboxes.
This heuristic gives an insufficient basisfor the self-annotation task because the rich mor-phology and free word order in German can notbe modeled with simple rules.
Moreover, hand-crafted rules are arguably not as robust and main-tainable as a statistical classifier trained on self-annotated training material.Our second heuristic is a three step process.
Instep one we collect all links in the mixed con-tent and replace them by a placeholder.
In thesecond step we tag the remaining content withthe OpenNLP tokenizer to get al named entities.Both collected lists are then looked up in a lexiconthat contains named entities and the correspond-ing geospatial classes.
This process requires a nor-malization procedure that includes the applicationof morphological methods.
The second methodcan be easily adapted to new relation types.5.3 UIMAThe self-annotated corpora are processed by sev-eral components of the UIMA (Mu?ller et al,2008) pipeline.
The advantage of exchangeablecollection readers is that they seamlessly handlestructured and unstructured data.
Another advan-tage of using UIMA is the possibility to sharecomponents with other research groups.
We caneasily exchange different components, like the us-age of the commonly known OpenNLP process-ing tools or the FSPar NLP engine (Schiehlen,2003) (which includes the TreeTagger (Schmid,1995)).
This allows us to experiment with dif-ferent approaches, e.g., shallow vs. deep analy-sis.
The components we use provide linguisticanalysis on different levels: tokens, morphology,part of speech (POS), chunking and partial depen-dency analysis.
Figure 4 shows the results afterthe linguistic processing of our sample sentence.For this work only a few annotations are wrappedas UIMA types: token (incl.
lemma, POS), multi-word, sentence, NP, PP and dependency relations(labeled edges between tokens).
We will intro-duce our machine learning component in section5.5.
Finally, the CAS consumers allow us to storeextracted facts in a context model.Figure 3 shows the article about Gollach afterlinguistic processing.
In the legend all annotatedcategories are listed.
We highlighted all markedrelations, all references to the article name (re-ferred to as subject in the figure) and links.
Afterselection of the Tauber relation, all annotations forthis token are listed in the right panel.5.4 Coreference resolutionUsing anaphora to refer to the main entity is acommon practice of the authors of Wikipedia ar-84Figure 3: Screenshot of the UIMA Annotation-Viewer.ticles.
Coreference resolution is therefore neces-sary for our annotation task.
A shallow linguis-tic analysis showed that the writing style is simi-lar throughout Wikipedia articles.
Based on thisobservation, we empirically investigated somegeospatial articles and came to the conclusion thata simple heuristic is sufficient for our coreferenceresolution problem.
In almost all articles, pro-nouns refer to the main entity of the article.
Inaddition we include some additional rules to beable to establish coreference of markables such asder Fluss ?the river?
or der Bach ?the creek?
withthe main entity.5.5 Supervised relation extractionWe use the ClearTK (Ogren et al, 2008) toolkit,which is also an UIMA component, for the rela-tion extraction task.
It contains wrappers for dif-ferent machine learning suites.
Our initial exper-iments showed that the MaximumEntropy clas-sifier achieved the best results for our classifi-cation task.
The toolkit provides additional ex-tensible feature methods.
Because we view self-annotation and fine-grained named entity recogni-tion as our main contributions, not feature selec-tion, we only give a brief overview of the featureswe use.F1 is a window based bag-of-words feature(window size = 3).
It considers lemma and part-of-speech tag of the tokens.
F2 is a phrase basedextractor that uses the parent phrase of both enti-ties (max 2 levels).
F3 is a representation of allsiesheSchlie?lichFinallyaufonMetermeter244ininBieberehreninTauberdiethe12341 3 1 2 3 41 2 2TOPm?ndenflowSUBJADVFigure 4: Dependency parser output of the FSParframework.linguistic effort descriptionF1 pos-tagging window size 3, LEMMAF2 chunk-parse parent chunksF3 dependency-parse dependency paths betw.
NEsTable 1: List of feature typespossible dependency paths between the article?smain entity and a target entity, where each pathis represented as a feature vector.
In most cases,more than one path is returned by the partial de-pendency parser (which makes no disambiguationdecisions) and included in the feature representa-tion.
Figure 4 depicts the dependency parser out-put of our sample sentence.
Each pair of squareand circle with the same number corresponds toone dependency.
These different possible depen-dency combinations give rise to 8 possible pathsbetween the relation entities Tauber and sie ?she?although our example sentence is a very simplesentence.6 EvaluationWe evaluate the system in two experiments.
Thefirst considers the relation between suburbs andtheir parent towns.
In the second experiment theriver-bodyOfWater relation is extracted.
The ex-periments are based on the previously describedextracted Wikipedia corpus.
For each experimenta new self-annotated corpus is created that is splitinto three parts.
The first part (60%) is used astraining corpus.
The second part (20%) is usedas development corpus.
The remaining 20% isused for the final evaluation and was not inspectedwhile we were developing the extraction algo-rithms.856.1 Metric usedOur gold standard includes all relations of eacharticle.
Our metric works on the level of typeand is independent of how often the same relationoccurs in the article.
The metric counts a rela-tion as true positive (TP) if the system extractedit at least once.
If the relation was not found bythe system a false negative (FN) is counted.
Afalse positive (FP) is given if the system extractsa relation between two entities that is not part ofthe (infobox-derived) gold standard for the article.All three measures are used to calculate precision(P = TPTP+FP ), recall (R = TPTP+FN ), and F1-score (F1 = 2 P?RP+R ).6.2 Town-suburb extractionThe town-suburb extractor uses one attribute ofthe infobox to identify the town-suburb relation.There is no schema drift in the infobox data andthe values contain only links.
Therefore the self-annotation works almost perfectly.
The only ex-ceptions are articles without an infobox whichcannot be used for training.
However, this is not areal issue because the amount of remaining data issufficient: 9000 articles can be used for this task.The results in table 2 show that the classifier thatuses F1, F2 and F3 (that is, including the depen-dency features) performs best.engine features F1 recall precisionFSPar F1 64.9 79.0% 55.7%FSPar F1, F2 89.6 90.2% 89.5%FSPar F1, F2, F3 98.3 98.8% 97.8%Table 2: Results of different feature combinationson the test set for town-suburb relation6.3 River-bodyOfWater extractionFor the extraction of the river-bodyOfWater re-lation the infobox processing is more difficult.We have to handle more attributes because thereis schema drift between the different users.
Itis hence necessary to merge information comingfrom different attribute values.
The other diffi-culty is the usage of mixed contents in the values.Another main difference to the town-suburb rela-tion is that the river-bodyOfWater relation is oftennot mentioned in the first sentence (which usuallygives a short definition about the the main entity).Thus, the self-annotation method has to deal withthe more complex sentences that are common laterin the article.
This also contributes to a more chal-lenging extraction task.Our river-bodyOfWater relation corpus consistsof 3000 self-annotated articles.Table 3 shows the performance of the extrac-tor using two different linguistic components asdescribed in section 5.3.
As in the case of town-suburb extraction the classifier that uses all fea-tures, including dependency features, performsbest.engine features F1 recall precisionFSPar F1 51.8% 56.6% 47.8%FSPar F1,F2 72.1% 68.9% 75.7%FSPar F1,F2,F3 78.3% 74.1% 83.0%OpenNLP F1 48.0% 62.8% 38.8%OpenNLP F1,F2 73.3% 71.7% 74.7%Table 3: Results of different feature combinationson the test set for river-bodyOfWater extraction6.4 Evaluation of self-annotationTo evaluate the quality of self-annotation, we ran-domly selected one set of 100 self-annotated ar-ticles from each data set and labeled these setsmanually.
These annotations are used to calcu-late the inter-annotator agreement between the hu-man annotated and machine annotated instances.We use Cohen?s ?
as measure and get a result of1.00 for the town-suburb relation.
For the river-bodyOfWater relation we got a ?-value of 0.79,which also indicates good agreement.We also use a gazetteer to evaluate the qual-ity of all town-suburb relations that were extractedfor our self-annotated training set.
The accuracyis nearly perfect (only one single error), which isgood evidence for the high quality of Wikipedia.Required size of self-annotated training set.The performance of a supervised system dependson the size of the training data.
In the self-annotation step a minimum of instances has to beannotated, but it is not necessary to self-annotateall available articles.We reduced the number of articles used inthe training size to test this hypothesis.
Reduc-ing the entire training set of 9000 (respectively,3000) self-annotated articles to 1000 reduces F186by 2.0% for town-suburb and by 2.4% for river-bodyOfWater; a reduction to 100 reduces F1 by8.5% for town-suburb and by 9.3% for river-bodyOfWater (compared to the 9000/3000 base-line).7 DiscussionWu and Weld (2007) observed schema drift intheir work: Wikipedia authors do not not use in-fobox attributes in a consistent manner.
However,we did not find schema drift to be a large prob-lem in our experiments.
The variation we foundcan easily be handled with a small number ofrules.
This can be due to the fact that the qual-ity of Wikipedia articles improved a lot in the lastyears through the introduction of automatic main-tenance tools like bots2.
Nevertheless, the devel-opment of self-annotation for a new relation typerequires some manual work.
The developer has tocheck the quality of the extraction relations in theinfoboxes.
This can lead to some additional adap-tation work for the used attributes such as mergingor creating rules.
However, a perfect coverage isnot required because the extraction system is onlyused for training purposes; we only need to finda sufficiently large number of positive training in-stances and do not require exhaustive labeling ofall articles.It is important to note that considering par-tially found relations as negative samples has tobe avoided.
Wrong negative samples have a gen-erally unwanted impact on the performance of thelearned extraction model.
A developer has to beaware of this fact.
In one experiment, the learnedclassifiers were applied to the training data andreturned a number of false positive results ?
40in case of the river-bodyOfWater relation.
31 ofthese errors were not actual errors because theself-annotation missed some true instances.
Nev-ertheless, the trained model recognizes these sam-ples as correct; this could perhaps be used to fur-ther improve the quality of self-annotation.Manually labeled data also includes noise andthe benefit of self-annotation is substantial when2See en.wikipedia.org/wiki/Wikipedia:Bots.
The edit his-tory of many articles shows that there is a lot of automaticmaintenance by bots to avoid schema drift.the aim is to build a fine-grained relation extrac-tion system in a fast and cheap way.The difference of the results between OpenNLPand FSPar engines are smaller than expected.Although sentence splitting is poorly done byOpenNLP the effect on the extraction result israther low.
Another crucial point is that thelexicon-based named entity recognizer of the FS-Par engine that was optimized for named entitiesused in Wikipedia has no significant impact on theoverall performance.
Thus, a basic set of NLPcomponents with moderate error rates may be suf-ficient for effective self-annotation.8 ConclusionThis paper described a new approach to develop-ing and implementing a complete system to ex-tract fine-grained geospatial relations by using asupervised machine learning approach without ex-pensive manual labeling.
Using self-annotation,systems can be rapidly developed and adapted fornew relations without expensive manual annota-tion.
Only some manual work has to be doneto find the right attributes in the infoboxes.
Thematching process between infoboxes and text isnot in all cases trivial and for some attributes ad-ditional rules have to be modeled.9 AcknowledgmentThis project was funded by DFG as part of Nexus(Collaborative Research Centre, SFB 627).ReferencesAuer, So?ren, Christian Bizer, Georgi Kobilarov, JensLehmann, and Zachary Ives.
2007.
Dbpedia: Anucleus for a web of open data.
In In 6th Intl Se-mantic Web Conference, Busan, Korea, pages 11?15.
Springer.Blessing, Andre, Stefan Klatt, Daniela Nicklas, Stef-fen Volz, and Hinrich Schu?tze.
2006.
Language-derived information and context models.
In Pro-ceedings of 3rd IEEE PerCom Workshop on ContextModeling and Reasoning (CoMoRea) (at 4th IEEEInternational Conference on Pervasive Computingand Communication (PerCom?06)).Du?rr, Frank, Nicola Ho?nle, Daniela Nicklas, ChristianBecker, and Kurt Rothermel.
2004.
Nexus?a plat-form for context-aware applications.
In Roth, Jo?rg,87editor, 1.
Fachgespra?ch Ortsbezogene Anwendun-gen und Dienste der GI-Fachgruppe KuVS, pages15?18, Hagen, Juni.
Informatik-Bericht der Fer-nUniversita?t Hagen.Hahn, Udo, Ekaterina Buyko, Rico Landefeld,Matthias Mu?hlhausen, Michael Poprat, KatrinTomanek, and Joachim Wermter.
2008.
Anoverview of JCoRe, the JULIE lab UIMA compo-nent repository.
In Proceedings of the LREC?08Workshop ?Towards Enhanced Interoperability forLarge HLT Systems: UIMA for NLP?, Marrakech,Morocco, May.Jiang, Jing.
2009.
Multi-task transfer learning forweakly-supervised relation extraction.
In ACL-IJCNLP ?09: Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 2, pages 1012?1020, Morristown, NJ, USA.
Association for Com-putational Linguistics.Mu?ller, Christof, Torsten Zesch, Mark-ChristophMu?ller, Delphine Bernhard, Kateryna Ignatova,Iryna Gurevych, and Max Mu?hlha?user.
2008.
Flex-ible uima components for information retrieval re-search.
In Proceedings of the LREC 2008 Work-shop ?Towards Enhanced Interoperability for LargeHLT Systems: UIMA for NLP?, Marrakech, Mo-rocco, May 31, 2008.
24?27.Nothman, Joel, Tara Murphy, and James R. Curran.2009.
Analysing wikipedia and gold-standard cor-pora for ner training.
In EACL ?09: Proceedingsof the 12th Conference of the European Chapterof the Association for Computational Linguistics,pages 612?620, Morristown, NJ, USA.
Associationfor Computational Linguistics.Ogren, Philip V., Philipp G. Wetzler, and StevenBethard.
2008.
Cleartk: A uima toolkit for sta-tistical natural language processing.
In UIMA forNLP workshop at Language Resources and Evalua-tion Conference (LREC).Schiehlen, Michael.
2003.
Combining deep and shal-low approaches in parsing german.
In ACL ?03:Proceedings of the 41st Annual Meeting on Asso-ciation for Computational Linguistics, pages 112?119, Morristown, NJ, USA.
Association for Com-putational Linguistics.Schmid, Helmut.
1995.
Improvements in part-of-speech tagging with an application to german.
In InProceedings of the ACL SIGDAT-Workshop, pages47?50.Suchanek, Fabian M., Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: A Core of Semantic Knowl-edge.
In 16th international World Wide Web con-ference (WWW 2007), New York, NY, USA.
ACMPress.Wu, Fei and Daniel S. Weld.
2007.
Autonomouslysemantifying wikipedia.
In Proceedings of the Six-teenth ACM Conference on Information and Knowl-edge Management, CIKM 2007, Lisbon, Portugal,November 6-10, 2007, pages 41?50.Zesch, Torsten, Christof Mu?ller, and Iryna Gurevych.2008.
Extracting Lexical Semantic Knowledgefrom Wikipedia and Wiktionary.
In Proceedings ofthe Conference on Language Resources and Evalu-ation (LREC).Zhang, Ziqi and Jose?
Iria.
2009.
A novel approach toautomatic gazetteer generation using wikipedia.
InPeople?s Web ?09: Proceedings of the 2009 Work-shop on The People?s Web Meets NLP, pages 1?9,Morristown, NJ, USA.
Association for Computa-tional Linguistics.88
