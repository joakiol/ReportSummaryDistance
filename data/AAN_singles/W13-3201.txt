Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 1?10,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsVector Space Semantic Parsing: A Framework forCompositional Vector Space ModelsJayant KrishnamurthyCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213jayantk@cs.cmu.eduTom M. MitchellCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213tom.mitchell@cmu.eduAbstractWe present vector space semantic parsing(VSSP), a framework for learning compo-sitional models of vector space semantics.Our framework uses Combinatory Cate-gorial Grammar (CCG) to define a cor-respondence between syntactic categoriesand semantic representations, which arevectors and functions on vectors.
Thecomplete correspondence is a direct con-sequence of minimal assumptions aboutthe semantic representations of basic syn-tactic categories (e.g., nouns are vectors),and CCG?s tight coupling of syntax andsemantics.
Furthermore, this correspon-dence permits nonuniform semantic repre-sentations and more expressive composi-tion operations than previous work.
VSSPbuilds a CCG semantic parser respectingthis correspondence; this semantic parserparses text into lambda calculus formulasthat evaluate to vector space representa-tions.
In these formulas, the meanings ofwords are represented by parameters thatcan be trained in a task-specific fashion.We present experiments using noun-verb-noun and adverb-adjective-noun phraseswhich demonstrate that VSSP can learncomposition operations that RNN (Socheret al 2011) and MV-RNN (Socher et al2012) cannot.1 IntroductionVector space models represent the semantics ofnatural language using vectors and operations onvectors (Turney and Pantel, 2010).
These modelsare most commonly used for individual words andshort phrases, where vectors are created using dis-tributional information from a corpus.
Such mod-els achieve impressive performance on standard-ized tests (Turney, 2006; Rapp, 2003), correlatewell with human similarity judgments (Griffiths etal., 2007), and have been successfully applied to anumber of natural language tasks (Collobert et al2011).While vector space representations for indi-vidual words are well-understood, there remainsmuch uncertainty about how to compose vectorspace representations for phrases out of their com-ponent words.
Recent work in this area raisesmany important theoretical questions.
For exam-ple, should all syntactic categories of words berepresented as vectors, or are some categories,such as adjectives, different?
Using distinct se-mantic representations for distinct syntactic cate-gories has the advantage of representing the opera-tional nature of modifier words, but the disadvan-tage of more complex parameter estimation (Ba-roni and Zamparelli, 2010).
Also, does semanticcomposition factorize according to a constituencyparse tree (Socher et al 2011; Socher et al2012)?
A binarized constituency parse cannot di-rectly represent many intuitive intra-sentence de-pendencies, such as the dependence between averb?s subject and its object.
What is needed toresolve these questions is a comprehensive theo-retical framework for compositional vector spacemodels.In this paper, we observe that we already havesuch a framework: Combinatory Categorial Gram-mar (CCG) (Steedman, 1996).
CCG provides atight mapping between syntactic categories andsemantic types.
If we assume that nouns, sen-tences, and other basic syntactic categories arerepresented by vectors, this mapping prescribessemantic types for all other syntactic categories.1For example, we get that adjectives are functionsfrom noun vectors to noun vectors, and that prepo-1It is not necessary to assume that sentences are vectors.However, this assumption simplifies presentation and seemslike a reasonable first step.
CCG can be used similarly toexplore alternative representations.1Input: Log.
Form:?red ball?
?semanticparsing ?Aredvball?
evaluation ??????
?Lexicon: red:= ?x.Aredxball:= vballParams.
:Ared =??
??
?
?vball =???
?Figure 1: Overview of vector space semantic pars-ing (VSSP).
A semantic parser first translates nat-ural language into a logical form, which is thenevaluated to produce a vector.sitions are functions from a pair of noun vectorsto a noun vector.
These semantic type specifica-tions permit a variety of different composition op-erations, many of which cannot be represented inpreviously-proposed frameworks.
Parsing in CCGapplies these functions to each other, naturally de-riving a vector space representation for an entirephrase.The CCG framework provides function typespecifications for each word?s semantics, given itssyntactic category.
Instantiating this frameworkamounts to selecting particular functions for eachword.
Vector space semantic parsing (VSSP) pro-duces these per-word functions in a two-step pro-cess.
The first step chooses a parametric func-tional form for each syntactic category, which con-tains as-yet unknown per-word and global param-eters.
The second step estimates these parametersusing a concrete task of interest, such as predictingthe corpus statistics of adjective-noun compounds.We present a stochastic gradient algorithm for thisstep which resembles training a neural networkwith backpropagation.
These parameters may alsobe estimated in an unsupervised fashion, for ex-ample, using distributional statistics.Figure 1 presents an overview of VSSP.
Theinput to VSSP is a natural language phrase anda lexicon, which contains the parametrized func-tional forms for each word.
These per-word repre-sentations are combined by CCG semantic pars-ing to produce a logical form, which is a sym-bolic mathematical formula for producing the vec-tor for a phrase ?
for example, Aredvball is a for-mula that performs matrix-vector multiplication.This formula is evaluated using learned per-wordand global parameters (values for Ared and vball)to produce the language?s vector space representa-tion.The contributions of this paper are threefold.First, we demonstrate how CCG provides a the-oretical basis for vector space models.
Second,we describe VSSP, which is a method for con-cretely instantiating this theoretical framework.Finally, we perform experiments comparing VSSPagainst other compositional vector space mod-els.
We perform two case studies of compositionusing noun-verb-noun and adverb-adjective-nounphrases, finding that VSSP can learn compositionoperations that existing models cannot.
We alsofind that VSSP produces intuitively reasonable pa-rameters.2 Combinatory Categorial Grammar forVector Space ModelsCombinatory Categorial Grammar (CCG) (Steed-man, 1996) is a lexicalized grammar formalismthat has been used for both broad coverage syntac-tic parsing and semantic parsing.
Like other lexi-calized formalisms, CCG has a rich set of syntac-tic categories, which are combined using a smallset of parsing operations.
These syntactic cate-gories are tightly coupled to semantic represen-tations, and parsing in CCG simultaneously de-rives both a syntactic parse tree and a seman-tic representation for each node in the parse tree.This coupling between syntax and semantics moti-vates CCG?s use in semantic parsing (Zettlemoyerand Collins, 2005), and provides a framework forbuilding compositional vector space models.2.1 SyntaxThe intuition embodied in CCG is that, syntac-tically, words and phrases behave like functions.For example, an adjective like ?red?
can com-bine with a noun like ?ball?
to produce anothernoun, ?red ball.?
Therefore, adjectives are natu-rally viewed as functions that apply to nouns andreturn nouns.
CCG generalizes this idea by defin-ing most parts of speech in terms of such func-tions.Parts of speech in CCG are called syntactic cat-egories.
CCG has two kinds of syntactic cat-egories: atomic categories and functional cate-gories.
Atomic categories are used to representphrases that do not accept arguments.
These cate-gories includeN for noun,NP for noun phrase, Sfor sentence, and PP for prepositional phrase.
Allother parts of speech are represented using func-tional categories.
Functional categories are writtenas X/Y or X\Y , where both X and Y are syn-2Part of speech Syntactic category Example usage Semantic type Example log.
formNoun N person : N Rd vpersonAdjective N/Nx good person : N ?Rd,Rd?
?x.AgoodxDeterminer NP/Nx the person : NP ?Rd,Rd?
?x.xIntrans.
Verb S\NPx the person ran : S ?Rd,Rd?
?x.Aranx+ branTrans.
Verb S\NPy/NPx the person ran home : S ?Rd, ?Rd,Rd??
?x.?y.
(Tranx)yAdverb (S\NP )\(S\NP ) ran lazily : S\NP ?
?Rd,Rd?, ?Rd,Rd??
[?y.Ay ?
?y.
(TlazyA)y](S\NP )/(S\NP ) lazily ran : S\NP ?
?Rd,Rd?, ?Rd,Rd??
[?y.Ay ?
?y.
(TlazyA)y](N/N)/(N/N) very good person : N ?
?Rd,Rd?, ?Rd,Rd??
[?y.Ay ?
?y.
(TveryA)y]Preposition (N\Ny)/Nx person in France : N ?Rd, ?Rd,Rd??
?x.?y.
(Tinx)y(S\NPy)\(S\NP )f/NPx ran in France : S\NP ?Rd, ?
?Rd,Rd?, ?Rd,Rd???
?x.?f.?y.
(Tinx)(f(y))Table 1: Common syntactic categories in CCG, paired with their semantic types and example logicalforms.
The example usage column shows phrases paired with the syntactic category that results fromusing the exemplified syntactic category for the bolded word.
For ease of reference, each argument toa syntactic category on the left is subscripted with its corresponding semantic variable in the examplelogical form on the right.
The variables x, y, b, v denote vectors, f denotes a function, A denotes amatrix, and T denotes a tensor.
Subscripted variables (Ared) denote parameters.
Functions in logicalforms are specified using lambda calculus; for example ?x.Ax is the function that accepts a (vector)argument x and returns the vector Ax.
The notation [f ?
g] denotes the higher-order function that,given input function f , outputs function g.tactic categories.
These categories represent func-tions that accept an argument of category Y andreturn a phrase of category X .
The direction ofthe slash defines the expected location of the argu-ment: X/Y expects an argument on the right, andX\Y expects an argument on the left.2 For ex-ample, adjectives are represented by the categoryN/N ?
a function that accepts a noun on the rightand returns a noun.The left part of Table 1 shows examples ofcommon syntactic categories, along with exam-ple uses.
Note that some intuitive parts of speech,such as prepositions, are represented by multiplesyntactic categories.
Each of these categories cap-tures a different use of a preposition, in this casethe noun-modifying and verb-modifying uses.2.2 SemanticsSemantics in CCG are given by first associating asemantic type with each syntactic category.
Eachword in a syntactic category is then assigned asemantic representation of the corresponding se-mantic type.
These semantic representations areknown as logical forms.
In our case, a logical formis a fragment of a formula for computing a vectorspace representation, containing word-specific pa-rameters and specifying composition operations.In order to construct a vector space model, weassociate all of the atomic syntactic categories,2As a memory aid, note that the top of the slash points inthe direction of the expected argument.N , NP , S, and PP , with the type Rd.
Then,the logical form for a noun like ?ball?
is a vec-tor vball ?
Rd.
The functional categories X/Yand X\Y are associated with functions from thesemantic type of X to the semantic type of Y .
Forexample, the semantic type of N/N is ?Rd,Rd?,representing the set of functions from Rd to Rd.3This semantic type captures the same intuition asadjective-noun composition models: semantically,adjectives are functions from noun vectors to nounvectors.The right portion of Table 1 shows semantictypes for several syntactic categories, along withexample logical forms.
All of these mappingsare a direct consequence of the assumption thatall atomic categories are semantically representedby vectors.
Interestingly, many of these semantictypes contain functions that cannot be representedin other frameworks.
For example, adverbs havetype ?
?Rd,Rd?, ?Rd,Rd?
?, representing functionsthat accept an adjective argument and return anadjective.
In Table 1, the example logical formapplies a 4-mode tensor to the adjective?s matrix.Another powerful semantic type is ?Rd, ?Rd,Rd?
?,which corresponds to transitive verbs and prepo-3The notation ?A,B?
represents the set of functionswhose domain isA and whose range isB.
Somewhat confus-ingly, the bracketing in this notation is backward relative tothe syntactic categories ?
the syntactic category (N\N)/Nhas semantic type ?Rd, ?Rd,Rd?
?, where the inner ?Rd,Rd?corresponds to the left (N\N).3theNP/N?x.xredN/N?x.AredxballNvballN : AredvballNP : Aredvballon(NP\NP )/NP?x.
?y.Aonx+BonytheNP/N?x.xtableNvtableNP : vtableNP\NP : ?y.Aonvtable +BonyNP : Aonvtable +BonAredvballFigure 2: Syntactic CCG parse and corresponding vector space semantic derivation.sitions.
This type represents functions from twoargument vectors to an output vector, which havebeen curried to accept one argument vector at atime.
The example logical form for this type usesa 3-mode tensor to capture interactions betweenthe two arguments.Note that this semantic correspondence permitsa wide range of logical forms for each syntacticcategory.
Each logical form can have an arbitraryfunctional form, as long as it has the correct se-mantic type.
This flexibility permits experimenta-tion with different composition operations.
For ex-ample, adjectives can be represented nonlinearlyby using a logical form such as ?x.
tanh(Ax).
Or,adjectives can be represented nonparametricallyby using kernel regression to learn the appropriatefunction from vectors to vectors.
We can also in-troduce simplifying assumptions, as demonstratedby the last entry in Table 1.
CCG treats preposi-tions as modifying intransitive verbs (the categoryS\N ).
In the example logical form, the verb?ssemantics are represented by the function f , theverb?s subject noun is represented by y, and f(y)represents the sentence vector created by compos-ing the verb with its argument.
By only operatingon f(y), this logical form assumes that the actionof a preposition is conditionally independent of theverb f and noun y, given the sentence f(y).2.3 LexiconThe main input to a CCG parser is a lexicon, whichis a mapping from words to syntactic categoriesand logical forms.
A lexicon contains entries suchas:ball := N : vballred := N/N : ?x.Aredxred := N : vredflies := ((S\NP )/NP ) : ?x.?y.
(Tfliesx)yEach entry of the lexicon associates a word(ball) with a syntactic category (N ) and a logicalform (vball) giving its vector space representation.Note that a word may appear multiple times in thelexicon with distinct syntactic categories and log-ical forms.
Such repeated entries capture wordswith multiple possible uses; parsing must deter-mine the correct use in the context of a sentence.2.4 ParsingParsing in CCG has two stages.
First, a categoryfor each word in the input is retrieved from the lex-icon.
Second, adjacent categories are iterativelycombined by applying one of a small number ofcombinators.
The most common combinator isfunction application:X/Y : f Y : g =?
X : f(g)Y : g X\Y : f =?
X : f(g)The function application rule states that a cate-gory of the form X/Y behaves like a function thataccepts an input category Y and returns categoryX .
The rule also derives a logical form for the re-sult by applying the function f (the logical formfor X/Y ) to g (the logical form for Y ).
Figure 2shows how repeatedly applying this rule producesa syntactic parse tree and logical form for a phrase.The top row of the parse represents retrieving alexicon entry for each word in the input.
Eachfollowing line represents a use of the function ap-plication combinator to syntactically and semanti-cally combine a pair of adjacent categories.
Theorder of these operations is ambiguous, and dif-ferent orderings may result in different parses ?
aCCG parser?s job is to find a correct ordering.
Theresult of parsing is a syntactic category for the en-tire phrase, coupled with a logical form giving thephrase?s vector space representation.3 Vector Space Semantic ParsingVector space semantic parsing (VSSP) is anapproach for constructing compositional vectorspace models based on the theoretical frameworkof the previous section.
VSSP concretely instanti-ates CCG?s syntactic/semantic correspondence byadding appropriately-typed logical forms to a syn-tactic CCG parser?s lexicon.
Parsing a sentencewith this lexicon and evaluating the resulting logi-4Semantic type Example syntactic categories Logical form templateRd N,NP, PP, S vw?Rd,Rd?
N/N , NP/N , S/S, S\NP ?x.?
(Awx)?Rd, ?Rd,Rd??
(S\NP )/NP , (NP\NP )/NP ?x.?y.?((Twx)y)?
?Rd,Rd?, ?Rd,Rd??
(N/N)/(N/N) [?y.?(Ay)?
?y.?
((TwA)y)]Table 2: Lexicon templates used in this paper to produce a CCG semantic parser.
?
represents thesigmoid function, ?
(x) = ex1+ex .cal form produces the sentence?s vector space rep-resentation.While it is relatively easy to devise vector spacerepresentations for individual nouns, it is morechallenging to do so for the fairly complex func-tion types licensed by CCG.
VSSP defines thesefunctions in two phases.
First, we create a lexi-con mapping words to parametrized logical forms.This lexicon specifies a functional form for eachword, but leaves free some per-word parame-ters.
Parsing with this lexicon produces logicalforms that are essentially functions from theseper-word parameters to vector space representa-tions.
Next, we train these parameters to pro-duce good vector space representations in a task-specific fashion.
Training performs stochastic gra-dient descent, backpropagating gradient informa-tion through the logical forms.3.1 Producing the Parametrized LexiconWe create a lexicon using a set of manually-constructed templates that associate each syntacticcategory with a parametrized logical form.
Eachtemplate contains variables that are instantiated todefine per-word parameters.
The output of thisstep is a CCG lexicon which can be used in abroad coverage syntactic CCG parser (Clark andCurran, 2007) to produce logical forms for inputlanguage.4Table 2 shows some templates used to createlogical forms for syntactic categories.
To reduceannotation effort, we define one template per se-mantic type, covering all syntactic categories withthat type.
These templates are instantiated by re-placing the variable w in each logical form withthe current word.
For example, instantiating thesecond template for ?red?
produces the logicalform ?x.?
(Aredx), where Ared is a matrix of pa-rameters.4In order to use the lexicon in an existing parser, the gen-erated syntactic categories must match the parser?s syntac-tic categories.
Then, to produce a logical form for a sen-tence, simply syntactically parse the sentence, generate log-ical forms for each input word, and retrace the syntacticderivation while applying the corresponding semantic oper-ations to the logical forms.Note that Table 2 is a only starting point ?
devis-ing appropriate functional forms for each syntacticcategory is an empirical question that requires fur-ther research.
We use these templates in our ex-periments (Section 4), suggesting that they are areasonable first step.
More complex data sets willrequire more complex logical forms.
For example,to use high-dimensional vectors, all matrices andtensors will have to be made low rank.
Anotherpossible improvement is to tie the parameters fora single word across related syntactic categories(such as the transitive and intransitive forms of averb).3.2 Training the Logical Form ParametersThe training problem in VSSP is to optimize thelogical form parameters to best perform a giventask.
Our task formulation subsumes both clas-sification and regression: we assume the input isa logical form, and the output is a vector.
Given adata set of this form, training can be performed us-ing stochastic gradient descent in a fashion similarto backpropagation in a neural network.The data set for training consists of tuples,{(`i, yi)}ni=1, where ` is a logical form and y is alabel vector representing the expected task output.Each logical form ` is treated as a function fromparameter vectors ?
to vectors in Rd.
For example,the logical form Aredvball is a function from Aredand vball to a vector.
We use ?
to denote the setof all parameters; for example, ?
= {Ared, vball}.We further assume a loss function L defined overpairs of label vectors.
The training problem istherefore to minimize the objective:O(?)
=n?i=1L(yi, g(`i(?))
+?2||?||2Above, g represents a global postprocessing func-tion which is applied to the output of VSSP tomake a task-specific prediction.
This function mayalso be parametrized, but we suppress these pa-rameters for simplicity.
As a concrete example,consider a classification task (as in our evaluation).In this case, y represents a target distribution overlabels, L is the KL divergence between the pre-5dicted and target distributions, and g represents asoftmax classifier.We optimize the objective O by runningstochastic gradient descent.
The gradients of theparameters ?
can be computed by iteratively ap-plying the chain rule to `, which procedurallyresembles performing backpropagation in a neu-ral network (Rumelhart et al 1988; Goller andKu?chler, 1996).4 Comparing Models of SemanticCompositionThis section compares the expressive power ofVSSP to previous work.
An advantage of VSSPis its ability to assign complex logical forms tocategories like adverbs and transitive verbs.
Thissection examines cases where such complex logi-cal forms are necessary, using synthetic data sets.Specifically, we create simple data sets mimick-ing expected forms of composition in noun-verb-noun and adverb-adjective-noun phrases.
VSSPis able to learn the correct composition operationsfor these data sets, but previously proposed mod-els cannot.We compare VSSP against RNN (Socher et al2011) and MV-RNN (Socher et al 2012), tworecursive neural network models which factorizecomposition according to a binarized constituencyparse tree.
The RNN model represents the seman-tics of each parse tree node using a single vector,while the MV-RNN represents each node usingboth a matrix and a vector.
These representationsseem sufficient for adjectives and nouns, but it isunclear how they generalize to other natural lan-guage constructions.In these experiments, each model is used to mapan input phrase to a vector, which is used to train asoftmax classifier that predicts the task output.
ForVSSP, we use the lexicon templates from Table 2.All nouns are represented as two-dimensional vec-tors, and all matrices and tensors are full rank.
Theparameters of each model (i.e., the per-word vec-tors, matrices and tensors) and the softmax classi-fier are trained as described in Section 3.2.4.1 Propositional LogicThe propositional logic experiment examines theimpact of VSSP?s representation of transitiveverbs.
VSSP directly represents these verbs astwo-argument functions, allowing it to learn op-erations with complex interactions between bothfalse and false 0,1 false or false 0,1 false xor false 0,1true and false 0,1 true or false 1,0 true xor false 1,0false and true 0,1 false or true 1,0 false xor true 1,0true and true 1,0 true or true 1,0 true xor true 0,1Table 3: Data for propositional logic experiment.Composition Formula KL divergenceRNN 0.44MV-RNN 0.12VSSP 0.01Table 4: Training error on the propositional logicdata set.
VSSP achieves zero error because itsverb representation can learn arbitrary logical op-erations.arguments.
In contrast, the RNN and MV-RNNmodels learn a set of global weights which areused to combine the verb with its arguments.
Thefunctional forms of these models limit the kinds ofinteractions that can be captured by verbs.We evaluated the learnability of argument in-teractions using the simple data set shown in Ta-ble 3.
In this data set, the words ?and,?
?or,?
and?xor?
are treated as transitive verbs, while ?true?and ?false?
are nouns.
The goal is to predict thelisted truth values, which are represented as two-dimensional distributions over true and false.Table 4 shows the training error of each modelon this data set, measured in terms of KL diver-gence between the model?s predictions and thetrue values.
VSSP achieves essentially zero train-ing error because its 3-mode tensor representa-tion of transitive verbs is trivially able to learnarbitrary logical operations.
RNN and MV-RNNcan learn each logical operation independently, butcannot learn all three at the same time ?
this phe-nomenon occurs because XOR requires differentglobal weight matrices than AND/OR.
As a re-sult, these models learn both AND and OR, but failto learn XOR.
This result suggests that much ofthe learning in these models occurs in the globalweight matrices, while the verb representationscan have only limited influence.Although this data set is synthetic, the interac-tion given by XOR seems necessary to representreal verbs.
To learn AND and OR, the argumentsneed not interact ?
it is sufficient to detect a setof appropriate subject and object arguments, thenthreshold the number of such arguments.
Thisinformation is essentially type constraints for thesubject and object of a verb.
However, type con-straints are insufficient for real verbs.
For exam-ple, consider the verb ?eats.?
All animals eat and6very big elephant 1,0 very big mouse 0.3,0.7pretty big elephant 0.9,0.1 pretty big mouse 0.2,0.8pretty small elephant 0.8,0.2 pretty small mouse 0.1,0.9very small elephant 0.7,0.3 very small mouse 0,1Table 5: Data for adverb-adjective-noun compo-sition experiment.
Higher first dimension valuesrepresent larger objects.Composition Model KL divergenceRNN 0.10MV-RNN 0.10VSSP 0.00Table 6: Training error of each composition modelon the adverb-adjective-noun experiment.can be eaten, but not all animals eat all other an-imals; whether or not ?X eats Y ?
is true dependson an interaction between X and Y .4.2 Adverb-Adjective-Noun CompositionAdverbs can enhance or attenuate the propertiesof adjectives, which in turn can enhance or attenu-ate the properties of nouns.
The adverb-adjective-noun experiment compares each model?s abilityto learn these effects using a synthetic object sizedata set, shown in Table 5.
The task is to predictthe size of each described object, which is repre-sented as a two-dimensional distribution over bigand small.
The challenge of this data set is thatan adverb?s impact on size depends on the adjec-tive being modified ?
a very big elephant is big-ger than a big elephant, but a very small elephantis smaller than a small elephant.
Note that thistask is more difficult than adverb-adjective com-position (Socher et al 2012), since in this taskthe adverb has to enhance/attenuate the enhanc-ing/attenuating properties of an adjective.Table 6 shows the training error of each modelon this data set.
VSSP achieves zero training errorbecause its higher-order treatment of adverbs al-lows it to accurately represent their enhancing andattenuating effects.
However, none of the othermodels are capable of representing these effects.This result is unsurprising, considering that theRNN and MV-RNN models essentially add theadverb and adjective parameters using a learnedlinear operator (followed by a nonlinearity).
Suchadditive combination forces adverbs to have a con-sistent direction of effect on the size of the noun,which is incompatible with the desired enhancingand attenuating behavior.Examining VSSP?s learned parameters clearlydemonstrates its ability to learn enhancing and?elephant??1.6?0.1??mouse???0.11.6??small?
?
0.22 00 1.7?
?big?
?
1.7 ?1.10 0.22?
?very small?
?
0.25 ?.12?1.34 2.3?
?very big??
2.3 ?1.34?0.12 0.25?Figure 3: Parameters for nouns, adjectives and ad-jective phrases learned by VSSP.
When the adverb?very?
is applied to ?small?
and ?big,?
it enhancestheir effect on a modified noun.attenuating phenomena.
Figure 3 demonstratesVSSP?s learned treatment of ?very.?
In the fig-ure, a high first dimension value represents a largeobject, while a high second dimension value rep-resents a small object; hence the vectors for ele-phant and mouse show that, by default, elephantsare larger than mice.
Similarly, the matrices forbig and small scale up the appropriate dimensionwhile shrinking the other dimension.
Finally, weshow the computed matrices for ?very big?
and?very small?
?
this operation is possible becausethese phrases have an adjective?s syntactic cate-gory, N/N .
These matrices have the same di-rection of effect as their unenhanced versions, butproduce a larger scaling in that direction.5 Related WorkSeveral models for compositionality in vectorspaces have been proposed in recent years.
Muchwork has focused on evaluating composition oper-ations for word pairs (Mitchell and Lapata, 2010;Widdows, 2008).
Many operations have been pro-posed, including various combinations of addition,multiplication, and linear operations (Mitchell andLapata, 2008), holographic reduced representa-tions (Plate, 1991) and others (Kintsch, 2001).Other work has used regression to train modelsfor adjectives in adjective-noun phrases (Baroniand Zamparelli, 2010; Guevara, 2010).
All of thiswork is complementary to ours, as these composi-tion operations can be used within VSSP by appro-priately choosing the logical forms in the lexicon.A few comprehensive frameworks for compo-sition have also been proposed.
One approachis to take tensor outer products of word vec-tors, following syntactic structure (Clark and Pul-man, 2007).
However, this approach results indifferently-shaped tensors for different grammati-cal structures.
An improvement of this frameworkuses a categorial grammar to ensure that similarly-7typed objects lie in the same vector space (Clarket al 2008; Coecke et al 2010; Grefenstetteand Sadrzadeh, 2011).
VSSP generalizes thiswork by allowing nonlinear composition opera-tions and considering supervised parameter esti-mation.
Several recent neural network models im-plicitly use a framework which assumes that com-position factorizes according to a binarized con-stituency parse, and that words and phrases haveuniform semantic representations (Socher et al2011; Socher et al 2012).
Notably, Hermannand Blunsom (2013) instantiate such a frameworkusing CCG.
VSSP generalizes these approaches,as they can be implemented within VSSP bychoosing appropriate logical forms.
Furthermore,our experiments demonstrate that VSSP can learncomposition operations that cannot be learned bythese approaches.The VSSP framework uses semantic parsing todefine a compositional vector space model.
Se-mantic parsers typically map sentences to logi-cal semantic representations (Zelle and Mooney,1996; Kate and Mooney, 2006), with many sys-tems using CCG as the parsing formalism (Zettle-moyer and Collins, 2005; Kwiatkowski et al2011; Krishnamurthy and Mitchell, 2012).
Al-though previous work has focused on logical se-mantics, it has demonstrated that semantic parsingis an elegant technique for specifying models ofcompositional semantics.
In this paper, we showhow to use semantic parsing to produce composi-tional models of vector space semantics.6 Discussion and Future WorkWe present vector space semantic parsing (VSSP),a general framework for building compositionalmodels of vector space semantics.
Our frame-work is based on Combinatory Categorial Gram-mar (CCG), which defines a correspondence be-tween syntactic categories and semantic types rep-resenting vectors and functions on vectors.
Amodel in VSSP instantiates this mapping in a CCGsemantic parser.
This semantic parser parses nat-ural language into logical forms, which are in turnevaluated to produce vector space representations.We further propose a method for constructing sucha semantic parser using a small number of logi-cal form templates and task-driven estimation ofper-word parameters.
Synthetic data experimentsshow that VSSP?s treatment of adverbs and tran-sitive verbs can learn more functions than priorwork.An interesting aspect of VSSP is that it high-lights cases where propositional semantics seemsuperior to vector space semantics.
For example,compare ?the ball that I threw?
and ?I threw theball.?
We expect the semantics of these phrasesto be closely related, differing only in that onephrase refers to the ball, while the other refers tothe throwing event.
Therefore, our goal is to de-fine a logical form for ?that?
which appropriatelyrelates the semantics of the above expressions.
It iseasy to devise such a logical form in propositionalsemantics, but difficult in vector space semantics.Producing vector space solutions to such problemsis an area for future work.Another direction for future work is joint train-ing of both the semantic parser and vector spacerepresentations.
Our proposed approach of addinglogical forms to a broad CCG coverage parser hasthe advantage of allowing VSSP to be applied togeneral natural language.
However, using the syn-tactic parses from this parser may not result inthe best possible factorization of semantic com-position.
Jointly training the semantic parser andthe vector space representations may lead to bettermodels of semantic composition.We also plan to apply VSSP to real data sets.We have made some progress applying VSSP toSemEval Task 8, learning to extract relations be-tween nominals (Hendrickx et al 2010).
Al-though our work thus far is preliminary, we havefound that the generality of VSSP makes it easyto experiment with different models of composi-tion.
To swap between models, we simply mod-ify the CCG lexicon templates ?
all of the remain-ing infrastructure is unchanged.
Such preliminaryresults suggest the power of VSSP as a generalframework for learning vector space models.AcknowledgmentsThis research has been supported in part byDARPA under award FA8750-13-2-0005, and inpart by a gift from Google.
We thank MattGardner, Justin Betteridge, Brian Murphy, ParthaTalukdar, Alona Fyshe and the anonymous review-ers for their helpful comments.ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: representingadjective-noun constructions in semantic space.
In8Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Stephen Clark and Stephen Pulman.
2007.
Combiningsymbolic and distributional models of meaning.
InProceedings of AAAI Spring Symposium on Quan-tum Interaction.Stephen Clark, Bob Coecke, and MehrnooshSadrzadeh.
2008.
A compositional distribu-tional model of meaning.
Proceedings of theSecond Symposium on Quantum Interaction.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2010.
Mathematical Foundations for a Com-positional Distributed Model of Meaning.
LambekFestschirft, Linguistic Analysis, 36.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537, November.Christoph Goller and Andreas Ku?chler.
1996.
Learn-ing task-dependent distributed representations bybackpropagation through structure.
In Proceedingsof the International Conference on Neural Networks(ICNN-96), pages 347?352.
IEEE.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical composi-tional distributional model of meaning.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing.Thomas L. Griffiths, Joshua B. Tenenbaum, and MarkSteyvers.
2007.
Topics in semantic representation.Psychological Review 114.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of the 2010 Workshop onGeometrical Models of Natural Language Seman-tics.Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,Preslav Nakov, Diarmuid O?.
Se?aghdha, SebastianPado?, Marco Pennacchiotti, Lorenza Romano, andStan Szpakowicz.
2010.
Semeval-2010 task 8:Multi-way classification of semantic relations be-tween pairs of nominals.
In Proceedings of the 5thInternational Workshop on Semantic Evaluation.Karl Moritz Hermann and Phil Blunsom.
2013.
TheRole of Syntax in Vector Space Models of Composi-tional Semantics.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics.Rohit J. Kate and Raymond J. Mooney.
2006.
Us-ing string-kernels for learning semantic parsers.
In21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Asso-ciation for Computational Linguistics, Proceedingsof the Conference.Walter Kintsch.
2001.
Predication.
Cognitive Science,25(2).Jayant Krishnamurthy and Tom M. Mitchell.
2012.Weakly supervised training of semantic parsers.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2011.
Lexical general-ization in ccg grammar induction for semantic pars-ing.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin Distributional Models of Semantics.
CognitiveScience, 34(8):1388?1429.Tony Plate.
1991.
Holographic reduced represen-tations: convolution algebra for compositional dis-tributed representations.
In Proceedings of the 12thInternational Joint Conference on Artificial Intelli-gence - Volume 1.Reinhard Rapp.
2003.
Word sense discovery based onsense descriptor dissimilarity.
In Proceedings of theNinth Machine Translation Summit.David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.Williams.
1988.
Neurocomputing: foundations ofresearch.
chapter Learning internal representationsby error propagation.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011.Semi-Supervised Recursive Autoencoders for Pre-dicting Sentiment Distributions.
In Proceedings ofthe 2011 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP).Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic Composi-tionality Through Recursive Matrix-Vector Spaces.In Proceedings of the 2012 Conference on Em-pirical Methods in Natural Language Processing(EMNLP).Mark Steedman.
1996.
Surface Structure and Inter-pretation.
The MIT Press.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: vector space models of se-mantics.
Journal of Artificial Intelligence Research,37(1), January.9Peter D. Turney.
2006.
Similarity of semantic rela-tions.
Computational Linguistics, 32(3), September.Dominic Widdows.
2008.
Semantic vector products:Some initial investigations.
In Proceedings of theSecond AAAI Symposium on Quantum Interaction.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In Proceedings of the thirteenth na-tional conference on Artificial Intelligence.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: struc-tured classification with probabilistic categorialgrammars.
In UAI ?05, Proceedings of the 21st Con-ference in Uncertainty in Artificial Intelligence.10
