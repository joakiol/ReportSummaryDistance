Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 247?257,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsOn the Role of Seed Lexicons in Learning Bilingual Word EmbeddingsIvan Vuli?c and Anna KorhonenLanguage Technology LabDTAL, University of Cambridge{iv250, alk23}@cam.ac.ukAbstractA shared bilingual word embedding space(SBWES) is an indispensable resource ina variety of cross-language NLP and IRtasks.
A common approach to the SB-WES induction is to learn a mapping func-tion between monolingual semantic spaces,where the mapping critically relies on aseed word lexicon used in the learning pro-cess.
In this work, we analyze the impor-tance and properties of seed lexicons forthe SBWES induction across different di-mensions (i.e., lexicon source, lexicon size,translation method, translation pair relia-bility).
On the basis of our analysis, wepropose a simple but effective hybrid bilin-gual word embedding (BWE) model.
Thismodel (HYBWE) learns the mapping be-tween two monolingual embedding spacesusing only highly reliable symmetric trans-lation pairs from a seed document-levelembedding space.
We perform bilinguallexicon learning (BLL) with 3 languagepairs and show that by carefully selectingreliable translation pairs our new HYBWEmodel outperforms benchmarking BWElearning models, all of which use moreexpensive bilingual signals.
Effectively,we demonstrate that a SBWES may be in-duced by leveraging only a very weak bilin-gual signal (document alignments) alongwith monolingual data.1 IntroductionDense real-valued vector representations of wordsor word embeddings (WEs) have recently gainedincreasing popularity in natural language process-ing (NLP), serving as invaluable features in a broadMonolingual                 vs                   BilingualFigure 1: A toy example of a 3-dimensional mono-lingual vs shared bilingual word embedding space(further SBWES) from Gouws et al (2015).range of NLP tasks, e.g., (Turian et al, 2010; Col-lobert et al, 2011; Chen and Manning, 2014).
Sev-eral studies have showcased a direct link and com-parable performance to ?more traditional?
distribu-tional models (Turney and Pantel, 2010).
Yet thewidely used skip-gram model with negative sam-pling (SGNS) (Mikolov et al, 2013b) is consideredas the state-of-the-art word representation model,due to its simplicity, fast training, as well as itssolid and robust performance across a wide varietyof semantic tasks (Baroni et al, 2014; Levy andGoldberg, 2014b; Levy et al, 2015).Research interest has recently extended to bilin-gual word embeddings (BWEs).
BWE learningmodels focus on the induction of a shared bilingualword embedding space (SBWES) where wordsfrom both languages are represented in a uniformlanguage-independent manner such that similarwords (regardless of the actual language) have sim-ilar representations (see Fig.
1).
A variety of BWElearning models have been proposed, differing inthe essential requirement of a bilingual signal nec-essary to construct such a SBWES (discussed laterin Sect.
2).
SBWES may be used to support manytasks, e.g., computing cross-lingual/multilingualsemantic word similarity (Faruqui and Dyer, 2014),learning bilingual word lexicons (Mikolov et al,2013a; Gouws et al, 2015; Vuli?c et al, 2016),cross-lingual entity linking (Tsai and Roth, 2016),247parsing (Guo et al, 2015; Johannsen et al, 2015),machine translation (Zou et al, 2013), or cross-lingual information retrieval (Vuli?c and Moens,2015; Mitra et al, 2016).BWE models should have two desirable prop-erties: (P1) leverage (large) monolingual trainingsets tied together through a bilingual signal, (P2)use as inexpensive bilingual signal as possible inorder to learn a SBWES in a scalable and widelyapplicable manner across languages and domains.While we provide a classification of related work,that is, different BWE models according to theseproperties in Sect.
2.1, the focus of this work ison a popular class of models labeled Post-HocMapping with Seed Lexicons.
These models op-erate as follows (Mikolov et al, 2013a; Dinu et al,2015; Lazaridou et al, 2015; Ammar et al, 2016):(1) two separate non-aligned monolingual embed-ding spaces are induced using any monolingual WElearning model (SGNS is the typical choice), (2)given a seed lexicon of word translation pairs as thebilingual signal for training, a mapping functionis learned which ties the two monolingual spacestogether into a SBWES.All existing work on this class of models as-sumes that high-quality training seed lexicons arereadily available.
In reality, little is understoodregarding what constitutes a high quality seed lexi-con, even with ?traditional?
distributional models(Gaussier et al, 2004; Holmlund et al, 2005; Vuli?cand Moens, 2013).
Therefore, in this work we askwhether BWE learning could be improved by mak-ing more intelligent choices when deciding overseed lexicon entries.
In order to do this we delvedeeper into the cross-lingual mapping problem byanalyzing a spectrum of seed lexicons with respectto controllable parameters such as lexicon source,its size, translation method, and translation pairreliability.The contributions of this paper are as follows:(C1) We present a systematic study on the impor-tance of seed lexicons for learning mapping func-tions between monolingual WE spaces.
(C2) Given the insights gained, we propose a sim-ple yet effective hybrid BWE model HYBWE thatremoves the need for readily available seed lexi-cons, and satisfies properties P1 and P2.
HYBWErelies on an inexpensive seed lexicon of highly reli-able word translation pairs obtained by a document-level BWE model (Vuli?c and Moens, 2016) fromdocument-aligned comparable data.
(C3) Using a careful pair selection process whenconstructing a seed lexicon, we show that in theBLL task HYBWE outperforms a BWE modelof Mikolov et al (2013a) which relies on readilyavailable seed lexicons.
HYBWE also outperformsstate-of-the-art models of (Hermann and Blunsom,2014b; Gouws et al, 2015) which require sentence-aligned parallel data.2 Learning SBWES using Seed LexiconsGiven source and target language vocabularies VSand VT, all BWE models learn a representation ofeach word w ?
VSunionsq VTin a SBWES as a real-valued vector: w = [f1, .
.
.
, fd], where fk?
Rdenotes the value for the k-th cross-lingual fea-ture for w within a d-dimensional SBWES.
Se-mantic similarity sim(w, v) between two wordsw, v ?
VSunionsq VTis then computed by applyinga similarity function (SF), e.g.
cosine (cos) ontheir representations in the SBWES: sim(w, v) =SF (w,v) = cos(w,v).2.1 Related Work: BWE Models andBilingual SignalsBWE models may be clustered into four differenttypes according to bilingual signals used in train-ing, and properties P1 and P2 (see Sect.
1).
Upad-hyay et al (2016) provide a similar overview ofrecent bilingual embedding learning architecturesregarding different bilingual signals required forthe embedding induction.
(Type 1) Parallel-Only: This group of BWE mod-els relies on sentence-aligned and/or word-alignedparallel data as the only data source (Zou et al,2013; Hermann and Blunsom, 2014a; Ko?cisk?
etal., 2014; Hermann and Blunsom, 2014b; Chandaret al, 2014).
In addition to an expensive bilingualsignal (colliding with P2), these models do notleverage larger monolingual datasets for training(not satisfying P1).
(Type 2) Joint Bilingual Training: These modelsjointly optimize two monolingual objectives, withthe cross-lingual objective acting as a cross-lingualregularizer during training (Klementiev et al, 2012;Gouws et al, 2015; Soyer et al, 2015; Shi et al,2015; Coulmance et al, 2015).
The idea may besummarized by the simplified formulation (Luonget al, 2015): ?(MonoS+MonoT)+?Bi.
The mono-lingual objectives MonoSand MonoTensure thatsimilar words in each language are assigned similar248embeddings and aim to capture the semantic struc-ture of each language, whereas the cross-lingualobjective Bi ensures that similar words across lan-guages are assigned similar embeddings.
It ties thetwo monolingual spaces together into a SBWES(thus satisfying P1).
Parameters ?
and ?
govern theinfluence of the monolingual and bilingual compo-nents.1The main disadvantage of Type 2 modelsis the costly parallel data needed for the bilingualsignal (thus colliding with P2).
(Type 3) Pseudo-Bilingual Training: This setof models requires document alignments as bilin-gual signal to induce a SBWES.
Vuli?c and Moens(2016) create a collection of pseudo-bilingual docu-ments by merging every pair of aligned documentsin training data, in a way that preserves impor-tant local information: words that appeared next toother words within the same language and thosethat appeared in the same region of the documentacross different languages.
This collection is thenused to train word embeddings with monolingualSGNS from word2vec.With pseudo-bilingual documents, the ?context?of a word is redefined as a mixture of neighbouringwords (in the original language) and words thatappeared in the same region of the document (inthe ?foreign?
language).
The bilingual contextsfor each word in each document steer the finalmodel towards constructing a SBWES.
The advan-tage over other BWE model types lies in exploitingweaker document-level bilingual signals (satisfyingP2), but these models are unable to exploit mono-lingual corpora during training (unlike Type 2 orType 4; thus colliding with P1).
(Type 4) Post-Hoc Mapping with Seed Lexicons:These models learn post-hoc mapping functions be-tween monolingual WE spaces induced separatelyfor two different languages (e.g., by SGNS).
AllType 4 models (Mikolov et al, 2013a; Faruquiand Dyer, 2014; Dinu et al, 2015; Lazaridou etal., 2015) rely on readily available seed lexiconsof highly frequent words obtained by e.g.
GoogleTranslate (GT) to learn the mapping (again collid-ing with P2), but they are able to satisfy P1.1Type 1 models may be considered a special case of Type2 models: Setting ?
= 0 reduces Type 2 models to Type 1models trained solely on parallel data, e.g., (Hermann andBlunsom, 2014b; Chandar et al, 2014).
?
= 1 results in themodels from (Klementiev et al, 2012; Gouws et al, 2015;Soyer et al, 2015; Coulmance et al, 2015).2.2 Post-Hoc Mapping with Seed Lexicons:Methodology and LexiconsKey Intuition One may infer that a type-hybridprocedure which would retain only highly reliabletranslation pairs obtained by a Type 3 model as aseed lexicon for Type 4 models effectively satisfiesboth requirements: (P1) unlike Type 1 and Type3, it can learn from monolingual data and tie twomonolingual spaces using the highly reliable trans-lation pairs, (P2) unlike Type 1 and Type 2, it doesnot require parallel data; unlike Type 4, it does notrequire external lexicons and translation systems.The only bilingual signal required are documentalignments.
Therefore, our focus is on novel lessexpensive Type 4 models.Overview The standard learning setup we useis as follows: First, two monolingual embeddingspaces, RdSand RdT, are induced separately ineach of the two languages using a standard mono-lingual WE model such as CBOW or SGNS.
dSand dTdenote the dimensionality of monolingualWE spaces.
The bilingual signal is a seed lexicon,i.e., a list of word translation pairs (xi, yi), wherexi?
VS, yi?
VT, and xi?
RdS, yi?
RdT.Learning Objectives Training is cast as a mul-tivariate regression problem: it implies learninga function that maps the source language vectorsfrom the training data to their corresponding targetlanguage vectors.
A standard approach (Mikolovet al, 2013a; Dinu et al, 2015) is to assume a lin-ear map W ?
RdS?dT, where a L2-regularizedleast-squares error objective (i.e., ridge regression)is used to learn the map W. The map is learned bysolving the following optimization problem (typi-cally by stochastic gradient descent (SGD)):minW?RdS?dT||XW ?Y||2F+ ?||W||2F(1)X and Y are matrices obtained through the re-spective concatenation of source language and tar-get language vectors from training pairs.
Once thelinear map W is estimated, any previously unseensource language word vector xumay be straightfor-wardly mapped into the target language embeddingspace RdTas Wxu.
After mapping all vectors x,x ?
VS, the target embedding space RdTin factserves as SBWES.22Another possible objective (found in the zero-shot learn-ing literature) is a margin-based ranking loss (Weston et al,2011; Lazaridou et al, 2015).
We omit the results with thisobjective for brevity, and due to the fact that similar trends areobserved as with (more standard) linear maps.249Seed Lexicon Source and Translation MethodPrior work on post-hoc mapping with seed lexi-cons used a translation system (i.e., GT) to translatehighly frequent English words to other languagessuch as Czech, Spanish (Mikolov et al, 2013a;Gouws et al, 2015) or Italian (Dinu et al, 2015;Lazaridou et al, 2015).
This method presupposesthe availability and high quality of such an exter-nal translation system.
To simulate this setup, wetake as a starting point the BNC word frequencylist from Kilgarriff (1997) containing 6, 318 mostfrequent English lemmas.
The list is then translatedto other languages via GT.
We call the BNC-basedlexicons obtained by employing Google TranslateBNC+GT.In this paper, we propose another option: first,we learn the ?first?
SBWES (i.e., SBWES-1) us-ing another BWE model (see Sect.
2.1), and thentranslate the BNC list through SBWES-1 by re-taining the nearest cross-lingual neighbor yi?
VTfor each xiin the BNC list which is represented inSBWES-1.
The pairs (xi, yi) constitute the seedlexicon needed for learning the mapping betweenmonolingual spaces, that is, to induce the finalSBWES-2.Although in theory any BWE induction modelmay be used to induce SBWES-1, we rely ona document-level Type 3 BWE induction modelfrom (Vuli?c and Moens, 2016), since it requiresonly document alignments as (weak) bilingual sig-nal.
The resulting hybrid BWE induction model(HYBWE) combines the output of a Type 3 model(SBWES-1) and a Type 4 model (SBWES-2).This seed lexicon and BWE learning variant iscalled BNC+HYB.Our new hybrid model allows us to also usesource language words occurring in SBWES-1sorted by frequency as seed lexicon source, againleaning on the intuition that higher frequency phe-nomena are more reliably translated using statisti-cal models.
Their translations can also be foundthrough SBWES-1 to obtain seed lexicon pairs(xi, yi).
This variant is called HFQ+HYB.Another possibility, recently introduced by Kiroset al (2015) for vocabulary expansion in monolin-gual settings, relies on all words shared betweentwo vocabularies to learn the mapping.
In this work,we test the ability and limits of such orthographicevidence in cross-lingual settings: seed lexiconpairs are (xi, xi), where xi?
VSand xi?
VT.This seed lexicon variant is called ORTHO.Seed Lexicon Size While all prior reported onlyresults with restricted seed lexicon sizes only (i.e.,1K, 2K and 5K lexicon pairs are used as standard),in this work we provide a full-fledged analysis ofthe influence of seed lexicon size on the SBWESperformance in cross-lingual tasks.
More extremesettings are also investigated, in the attempt to an-swer two important questions: (1) Can a Type 4SBWES be induced in a limited setting with onlya few hundred lexicon pairs available (e.g., 100-500)?
(2) Can the Type 4 models profit from theinclusion of more seed lexicon pairs (e.g., morethan 5K, even up to 40K-50K lexicon pairs)?Translation Pair Reliability When buildingseed lexicons through SBWES-1 (i.e., BNC+HYBand HFQ+HYB methods), it is possible to con-trol for the reliability of translation pairs to be in-cluded in the final lexicon, with the idea that theuse of only highly reliable pairs can potentiallylead to an improved SBWES-2.
A simple yeteffective reliability reliability feature for transla-tion pairs is the symmetry constraint (Peirsman andPad?, 2010; Vuli?c and Moens, 2013) : two wordsxi?
VSand yi?
VSare used as seed lexiconpairs only if they are mutual nearest neighboursgiven their representations in SBWES-1.
The twovariants of seed lexicons with only symmetric pairsare BNC+HYB+SYM and HFREQ+HYB+SYM.We also test the variants without the sym-metry constraint (i.e., BNC+HYB+ASYM andHFQ+HYB+ASYM).Even more conservative reliability measures maybe applied by exploiting the scores in the lists oftranslation candidates ranked by their similarityto the cue word xi.
We investigate a symmetryconstraint with a threshold: two words xi?
VSand yi?
VSare included as seed lexicon pair(xi, yi) iff they are mutual nearest neighbours inSBWES-1 and it holds:sim(xi, yi)?
sim(xi, zi) > THR (2)sim(yi, xi)?
sim(yi, wi) > THR (3)where zi?
VTis the second best translation can-didate for xi, and wi?
VSfor yi.
THR is a param-eter which specifies the margin between the twobest translation candidates.
The intuition is thathighly unambiguous and monosemous translationpairs (which is reflected in higher score margins)are also highly reliable.33Other (more elaborate) reliability measures exist in the2503 Experimental SetupTask: Bilingual Lexicon Learning (BLL) Af-ter the final SBWES is induced, given a list of nsource language words xu1, .
.
.
, xun, the task is tofind a target language word t for each xuin the listusing the SBWES.
t is the target language wordclosest to the source language word xuin the in-duced SBWES, also known as the cross-lingualnearest neighbor.
The set of learned n (xu, t) pairsis then run against a gold standard BLL test set.Following the standard practice (Mikolov et al,2013a; Dinu et al, 2015), for all Type 4 models, allpairs containing any of the test words xu1, .
.
.
, xunare removed from training seed lexicons.Test Sets For each language pair, we evaluate onstandard 1,000 ground truth one-to-one translationpairs built for three language pairs: Spanish (ES)-,Dutch (NL)-, Italian (IT)-English (EN) by Vuli?cand Moens (2013).
The dataset is generally con-sidered a benchmarking test set for BLL modelsthat learn from non-parallel data, and is availableonline.4We have also experimented with two otherbenchmarking BLL test sets (Bergsma and Durme,2011; Leviant and Reichart, 2015) observing a verysimilar relative performance of all the models inour comparison.Evaluation Metrics We measure the BLL per-formance using the standard Top 1 accuracy (Acc1)metric (Gaussier et al, 2004; Mikolov et al, 2013a;Gouws et al, 2015).5Baseline Models To induce SBWES-1, we re-sort to document-level embeddings of Vuli?c andMoens (2016) (Type 3).
We also compare to re-sults obtained directly by their model (BWESG) tomeasure the performance gains with HYBWE.To compare with a representative Type 2 model,we opt for the BilBOWA model of Gouws et al(2015) due to its solid performance and robustnessin the BLL task when trained on general-domaincorpora such as Wikipedia (Luong et al, 2015), itsreduced complexity reflected in fast computationson massive datasets, as well as its public availabil-literature (Smith and Eisner, 2007; Tu and Honavar, 2012;Vuli?c and Moens, 2013), but we do not observe any significantgains when resorting to the more complex reliability estimates.4http://people.cs.kuleuven.be/~ivan.vulic/5Similar trends are observed within a more lenient settingwith Acc5and Acc10scores, but we omit these results forclarity and the fact that the actual BLL performance is bestreflected in Acc1scores (i.e., best translation only).ity.6In short, BilBOWA combines the adaptedSGNS for monolingual objectives together with across-lingual objective that minimizes the L2-lossbetween the bag-of-word vectors of parallel sen-tences.
BilBOWA uses the same training setup asHYBWE (monolingual datasets plus a bilingualsignal), but relies on a stronger bilingual signal(sentence alignments as opposed to HYBWE?s doc-ument alignments).We also compare with a benchmarking Type 1model from sentence-aligned parallel data calledBiCVM (Hermann and Blunsom, 2014b).
Finally,a SGNS-based BWE model with the BNC+GTseed lexicon is taken as a baseline Type 4 model(Mikolov et al, 2013a).7Training Data and Setup We use standard train-ing data and suggested settings to obtain BWEsfor all models involved in comparison.
We retainthe 100K most frequent words in each languagefor all models.
To induce monolingual WE spaces,two monolingual SGNS models were trained on thecleaned and tokenized Wikipedias from the Poly-glot website (Al-Rfou et al, 2013) using SGD witha global learning rate of 0.025.
For BilBOWA,as in the original work (Gouws et al, 2015), thebilingual signal for the cross-lingual regularizationis provided by the first 500K sentences from Eu-roparl.v7 (Tiedemann, 2012).
We use SGD witha global rate of 0.15.8The window size is variedfrom 2 to 16 in steps of 2, and the best scoringmodel is always reported in all comparisons.BWESG was trained on the cleaned and tok-enized document-aligned Wikipedias available on-line9, SGD on pseudo-bilingual documents witha global rate 0.025.
For BiCVM, we use the toolreleased by its authors10and train on the wholeEuroparl.v7 for each language pair: we train anadditive model, with hinge loss margin set to d(i.e., dimensionality) as in the original paper, batchsize of 50, and noise parameter of 10.
All BiCVMmodels are trained with 200 iterations.For all models, we obtain BWEs with d =40, 64, 300, 500, but we report only results with300-dimensional BWEs as similar trends were ob-served with other d-s. Other parameters are: 15epochs, 15 negatives, subsampling rate 1e?
4.6https://github.com/gouwsmeister/bilbowa7For details concerning all baseline models, the reader isencouraged to check the relevant literature.8Suggested by the authors (personal correspondence).9http://linguatools.org/tools/corpora/10https://github.com/karlmoritz/bicvm251BNC+GT BNC+HYB+ASYM BNC+HYB+SYM HFQ+HYB+ASYM HFQ+HYB+SYM ORTHOcasamiento casamiento casamiento casamiento casamiento casamientomarriage marry marriage marriage marriage mar?amarry marriage marry marry marry se?ormarrying marrying marrying betrothal betrothal do?abetrothal wed wedding marrying marrying juanawedding wedding betrothal wedding wedding nochewed betrothal wed daughter wed amorelopement remarry marriages betrothed elopement guerraTable 1: Nearest EN neighbours of the Spanish word casamiento (marriage) with different seed lexicons.Model ES-EN NL-EN IT-ENBICVM (TYPE 1) 0.532 0.583 0.569BILBOWA (TYPE 2) 0.632 0.636 0.647BWESG (TYPE 3) 0.676 0.626 0.643BNC+GT (Type 4) 0.677 0.641 0.646ORTHO 0.233 0.506 0.224BNC+HYB+ASYM 0.673 0.626 0.644BNC+HYB+SYM 0.681 0.658* 0.663*(3388; 2738; 3145)HFQ+HYB+ASYM 0.673 0.596 0.635HFQ+HYB+SYM 0.695* 0.657* 0.667*Table 2: Acc1scores in a standard BLL setup(for Type 4 models): all seed lexicons contain 5Ktranslation pairs, except for BNC+HYB+SYM (itssizes provided in parentheses).
* denotes a statisti-cally significant improvement over baselines andBNC+GT using McNemar?s statistical significancetest with the Bonferroni correction, p < 0.05.4 Results and DiscussionExp.
I: Standard BLL Setting First, we replicatethe previous BLL setups with Type 4 models from(Mikolov et al, 2013a; Dinu et al, 2015) by relyingon seed lexicons of exactly 5K word pairs (exceptfor BNC+HYB+SYM which exhausts all possiblepairs before the 5K limit) sorted by frequency ofthe source language word.
Results with differentlexicons for the three language pairs are summa-rized in Table 2, while Table 1 shows examples ofnearest neighbour words for a Spanish word notpresent in any of the training lexicons.Table 1 provides evidence for our first insight:Type 4 models do not necessarily require externallexicons (such as the BNC+GT model) to learn asemantically plausible SBWES (i.e., the lists ofnearest neighbours are similar for all lexicons ex-cluding ORTHO).
Table 1 also suggests that thechoice of seed lexicon pairs may strongly influencethe properties of the resulting SBWES.
Due to itsdesign, ORTHO finds a mapping which naturallybrings foreign words appearing in the English vo-cabulary closer in the induced SBWES.This first batch of quantitative results alreadyshows that Type 4 models with inexpensive auto-matically induced lexicons (i.e., HYBWE) are on apar with or even better than Type 4 models relyingon external resources or translation systems.
In ad-dition, the best reported scores using the more con-strained symmetric BNC/HFQ+HYB+SYM lexi-con variants are higher than those for three baselinemodels (of Type 1, Type 2, and Type 3) that pre-viously held highest scores on the BLL test sets(Vuli?c and Moens, 2016).
These improvementsover the baseline models and BNC+GT are sta-tistically significant (using McNemar?s statisticalsignificance test, p < 0.05).
Table 2 also suggeststhat a careful selection of reliable pairs can lead topeak performances even with a lower number ofpairs, i.e., see the results of BNC+HYB+SYM.Exp.
II: Lexicon Size BLL results for ES-ENand NL-EN obtained by varying the seed lexiconsizes are displayed in Fig.
2(a) and 2(b).
Results forIT-EN closely follow the patterns observed with ES-EN.
BNC+HYB+SYM and HFQ+HYB+ASYM?
the two models that do not blindly use all po-tential training pairs, but rely on sets of symmet-ric pairs (i.e., they include the simple measure oftranslation pair reliability) ?
display the best per-formance across all lexicon sizes.
The finding con-firms the intuition that a more intelligent pair selec-tion strategy is essential for Type 4 BWE models.HFQ+HYB+SYM ?
a simple hybrid BWE model(HYBWE) combining a document-level Type 3model with a Type 4 model and translation reliabil-ity detection ?
is the strongest BWE model overall(see also Table 2 again).HYBWE-based models which do not performany pair selection (i.e., BNC/HFQ+HYB+ASYM)closely follow the behaviour of the GT-basedmodel.
This demonstrates that an external lexi-con or translation system may be safely replaced25200.10.20.30.40.50.60.70.1k 0.2k 0.5k 1k 2k 5k 10k 20k 50kAcc 1scoresLexicon sizeBNC+GTBNC+HYB+ASYMBNC+HYB+SYMHFQ+HYB+ASYMHFQ+HYB+SYMORTHO(a) Spanish-English00.10.20.30.40.50.60.70.1k 0.2k 0.5k 1k 2k 5k 10k 20k 50kLexicon sizeBNC+GTBNC+HYB+ASYMBNC+HYB+SYMHFQ+HYB+ASYMHFQ+HYB+SYMORTHO(b) Dutch-EnglishFigure 2: BLL results (Acc1) across different seed lexicon sizes for all lexicons.
x axes are in log scale.by a document-level embedding model without anysignificant performance loss in the BLL task.
TheORTHO-based model falls short of its competitors.However, we observe that even this model with thelearning setting relying on the cheapest bilingualsignal may lead to reasonable BLL scores, espe-cially for the more related NL-EN pair.The two models with the symmetry constraintdisplay a particularly strong performance with set-tings relying on scarce resources (i.e., only a smallportion of training pairs is available).
For instance,HFQ+HYB+SYM scores 0.129 for ES-EN withonly 200 training pairs (vs 0.002 with BNC+GT),and 0.529 with 500 pairs (vs 0.145 with BNC+GT).On the other hand, adding more pairs does notlead to an improved BLL performance.
In fact,we observe a slow and steady decrease in perfor-mance with lexicons containing 10, 000 and moretraining pairs for all HYBWE variants.
The phe-nomenon may be attributed to the fact that highlyfrequent words receive more accurate representa-tions in SBWES-1, and adding less frequent and,consequently, less accurate training pairs to theSBWES-2 learning process brings in additionalnoise.
In plain language, when it comes to seed lex-icons Type 4 models prefer quality over quantity.Exp.
III: Translation Pair Reliability In thenext experiment, we vary the threshold valueTHR (see sect.
2.2) in the HFQ+HYB+SYMvariant with the following values in comparison:0.0 (None), 0.01, 0.025, 0.05, 0.075, 0.1.
We in-vestigate whether retaining only highly unambigu-ous pairs would lead to even better BLL perfor-mance.
The results for all three language pairsare summarized in Fig.
3(a)-3(c).
The results forall variant models again decrease when employ-ing larger lexicons (due to the usage of less fre-quent word pairs in training).
We observe that aslightly stricter selection criterion (i.e., THR =0.01, 0.025) also leads to slightly improved peakBLL scores for ES-EN and IT-EN around the 5Kregion.
The improvements, however, are not statis-tically significant.
On the other hand, a too conser-vative pair selection criterion with higher thresholdvalues significantly deteriorates the overall perfor-mance of HYBWE with HFQ+HYB+SYM.
Theconservative criteria discard plenty of potentiallyuseful training pairs.
Therefore, as one line offuture research, we plan to investigate more sophis-ticated models for the selection of reliable seedlexicon pairs that will lead to a better trade-off be-tween the lexicon size and reliability of the pairs.Exp.
IV: Another Task - Suggesting WordTranslations in Context (SWTC) In the finalexperiment, we test whether the findings originat-ing from the BLL task generalize to another cross-lingual semantic task: suggesting word translationsin context (SWTC) recently proposed by Vuli?c andMoens (2014).
Given an occurrence of a polyse-mous word w ?
VS, the SWTC task is to choosethe correct translation in the target language ofthat particular occurrence of w from the given setT C(w) = {t1, .
.
.
, ttq}, T C(w) ?
VT, of its tqpossible translations/meanings.
Whereas in theBLL task the candidate search is performed overthe entire vocabulary VT, the set TC(w) typicallycomprises only a few pre-selected words/senses.One may refer to T C(w) as an inventory of transla-tion candidates for w. The best scoring translationcandidate in the ranked list is then the correct trans-lation for that particular occurrence of w observingits local context Con(w).
SWTC is an extended2530.60.620.640.660.680.71k 2k 4k 5k 10k 20k 40kAcc 1scoresLexicon sizeTHR=NoneTHR=0.01THR=0.025THR=0.05THR=0.075THR=0.1(a) Spanish-English0.540.560.580.60.620.640.661k 2k 4k 5k 10k 20k 40kLexicon sizeTHR=NoneTHR=0.01THR=0.025THR=0.05THR=0.075THR=0.1(b) Dutch-English0.560.580.60.620.640.660.681k 2k 4k 5k 10k 20k 40kLexicon sizeTHR=NoneTHR=0.01THR=0.025THR=0.05THR=0.075THR=0.1(c) Italian-EnglishFigure 3: BLL results across different threshold (THR) values with the HFQ+HYB+SYM seed lexicons.Higher thresholds imply less ambiguous word translation pairs.
Thicker horizontal lines denote the bestscore from any of the baseline models.
x axes are in log scale.Model ES-EN NL-EN IT-ENNO CONTEXT 0.406 0.433 0.408BEST SYSTEM 0.703 0.712 0.789(Vuli?c and Moens, 2014)BICVM (TYPE 1) 0.506 0.586 0.522BILBOWA (TYPE 2) 0.586 0.656 0.589BWESG (TYPE 3) 0.783 0.858 0.792BNC+GT (TYPE 4) 0.794 0.858 0.783ORTHO 0.647 0.794 0.678BNC+HYB+ASYM 0.806* 0.872 0.778BNC+HYB+SYM 0.808* 0.875* 0.814*(3839; 3117; 3693)HFQ+HYB+ASYM 0.789 0.864 0.781HFQ+HYB+SYM (THR = None) 0.792 0.869 0.786HFQ+HYB+SYM (THR=0.01) 0.792 0.858 0.789HFQ+HYB+SYM (THR=0.025) 0.800 0.853 0.792Table 3: Acc1scores in the SWTC task.
All seedlexicons contain 6K translation pairs, except forBNC+HYB+SYM (its sizes provided in parenthe-ses).
* denotes a statistically significant improve-ment over baselines and BNC+GT using McNe-mar?s statistical significance test with the Bonfer-roni correction, p < 0.05.cross-lingual variant of the task proposed by Huanget al (2012) which evaluates monolingual context-sensitive semantic similarity of words in sententialcontext, and it is also very related to cross-linguallexical substitution (Mihalcea et al, 2010).To isolate the performance of each BWE induc-tion model from the details of the SWTC setup,we use the same approach with all models: weopt for the SWTC framework proven to yieldexcellent results with BWEs in the SWTC task(Vuli?c and Moens, 2016).
In short, the context bagCon(w) = {cw1, .
.
.
, cwr} is obtained by harvest-ing all r words that occur with w in the sentence.The vector representation of Con(w) is the d-dimensional embedding computed by aggregatingover all word embeddings for each cwj?
Con(w)using standard addition as the compositional opera-tor (Mitchell and Lapata, 2008) which was provena robust choice (Milajevs et al, 2014):Con(w) = cw1+ cw2+ .
.
.+ cwr(4)where cwjis the embedding of the j-th con-text word, and Con(w) is the resulting embed-ding of the context bag Con(w).
Finally, foreach tj?
T C(w), the context-sensitive similar-ity with w is computed as: sim(w, tj, Con(w)) =cos(Con(w), tj), where Con(w) and tjare rep-resentations of the (sentential) context bag and thecandidate translation tjin the same SBWES.11The evaluation set consists of 360 sentences for15 polysemous nouns (24 sentences for each noun)in each of the three languages: Spanish, Dutch, Ital-ian, along with the single gold standard single wordEnglish translation given the sentential context.12Table 3 summarizes the results (Acc1scores) in theSWTC task.
NO-CONTEXT refers to the context-insensitive majority baseline obtained by BNC+GT(i.e., it always chooses the most semantically sim-ilar translation candidate at the word type level).We also report the results of the best SWTC modelfrom Vuli?c and Moens (2014).The results largely support the claims estab-lished with the BLL evaluation.
An exter-11The same ranking of different models (with lower abso-lute scores) is observed when adapting the monolingual lexicalsubstitution framework of Melamud et al (2015) to the SWTCtask as done by Vuli?c and Moens (2016).12The SWTC evaluation set is available online at:http://aclweb.org/anthology/attachments/D/D14/D14-1040.Attachment.zip254nal seed lexicon of BNC+GT may be safelyreplaced by an automatically induced inex-pensive seed lexicon (as in HYBWE withBNC+HYB+SYM/ASYM).
The best perform-ing models are again BNC+HYB+SYM andHFQ+HYB+SYM.
The comparison of ASYM andSYM lexicon variants further suggests that filter-ing translation pairs using the symmetry constraintagain leads to consistent improvements, but stricterselection criteria with higher thresholds do not leadto significant performance boosts, and may evenhurt the performance (see the results for NL-EN).Various HYBWE variants significantly improveover baseline BWE models (Types 1-4), also out-performing previous best SWTC results.5 Conclusions and Future WorkWe presented a detailed analysis of the importanceand properties of seed bilingual lexicons in learn-ing bilingual word embeddings (BWEs) which arevaluable for many cross-lingual/multilingual NLPtasks.
On the basis of the analysis, we proposed asimple yet effective hybrid bilingual word embed-ding model called HYBWE.
It learns the mappingbetween two monolingual embedding spaces us-ing only highly reliable symmetric translation pairsfrom an inexpensive seed document-level embed-ding space.
The results in the tasks of (1) bilinguallexicon learning and (2) suggesting word transla-tions in context demonstrate that ?
due to its carefulselection of reliable translation pairs for seed lexi-cons ?
HYBWE outperforms benchmarking BWEinduction models, all of which use more expensivebilingual signals for training.In future work, we plan to investigate other meth-ods for seed pairs selection, settings with scarceresources (Agi?c et al, 2015; Zhang et al, 2016),other context types inspired by recent work in themonolingual settings (Levy and Goldberg, 2014a;Melamud et al, 2016), as well as model adapta-tions that can work with multi-word expressions.Encouraged by the excellent results, we also planto test the portability of the approach to more lan-guage pairs, and other tasks and applications.AcknowledgmentsThis work is supported by ERC Consolidator GrantLEXICAL: Lexical Acquisition Across Languages(no 648909).
The authors are grateful to Roi Re-ichart and the anonymous reviewers for their help-ful comments and suggestions.References?eljko Agi?c, Dirk Hovy, and Anders S?gaard.
2015.If all you have is a bit of the Bible: Learning POStaggers for truly low-resource languages.
In ACL,pages 268?272.Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.2013.
Polyglot: Distributed word representationsfor multilingual NLP.
In CoNLL, pages 183?192.Waleed Ammar, George Mulcaire, Yulia Tsvetkov,Guillaume Lample, Chris Dyer, and Noah A. Smith.2016.
Massively multilingual word embeddings.CoRR, abs/1602.01925.Marco Baroni, Georgiana Dinu, and Germ?nKruszewski.
2014.
Don?t count, predict!
Asystematic comparison of context-counting vs.context-predicting semantic vectors.
In ACL, pages238?247.Shane Bergsma and Benjamin Van Durme.
2011.Learning bilingual lexicons using the visual similar-ity of labeled web images.
In IJCAI, pages 1764?1769.Sarath A.P.
Chandar, Stanislas Lauly, Hugo Larochelle,Mitesh M. Khapra, Balaraman Ravindran, Vikas C.Raykar, and Amrita Saha.
2014.
An autoencoderapproach to learning bilingual word representations.In NIPS, pages 1853?1861.Danqi Chen and Christopher D. Manning.
2014.
Afast and accurate dependency parser using neural net-works.
In EMNLP, pages 740?750.Ronan Collobert, Jason Weston, L?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel P. Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Jocelyn Coulmance, Jean-Marc Marty, Guillaume Wen-zek, and Amine Benhalloum.
2015.
Trans-gram,fast cross-lingual word embeddings.
In EMNLP,pages 1109?1113.Georgiana Dinu, Angeliki Lazaridou, and Marco Ba-roni.
2015.
Improving zero-shot learning by miti-gating the hubness problem.
In ICLR Workshop Pa-pers.Manaal Faruqui and Chris Dyer.
2014.
Improvingvector space word representations using multilingualcorrelation.
In EACL, pages 462?471.
?ric Gaussier, Jean-Michel Renders, Irina Matveeva,Cyril Goutte, and Herv?
D?jean.
2004.
A geometricview on bilingual lexicon extraction from compara-ble corpora.
In ACL, pages 526?533.Stephan Gouws, Yoshua Bengio, and Greg Corrado.2015.
BilBOWA: Fast bilingual distributed repre-sentations without word alignments.
In ICML, pages748?756.255Jiang Guo, Wanxiang Che, David Yarowsky, HaifengWang, and Ting Liu.
2015.
Cross-lingual depen-dency parsing based on distributed representations.In ACL, pages 1234?1244.Karl Moritz Hermann and Phil Blunsom.
2014a.
Mul-tilingual distributed representations without wordalignment.
In ICLR.Karl Moritz Hermann and Phil Blunsom.
2014b.
Mul-tilingual models for compositional distributed se-mantics.
In ACL, pages 58?68.Jon Holmlund, Magnus Sahlgren, and Jussi Karlgren.2005.
Creating bilingual lexica using referencewordlists for alignment of monolingual semanticvector spaces.
In NODALIDA, pages 71?77.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In ACL, pages 873?882.Anders Johannsen, H?ctor Mart?nez Alonso, and An-ders S?gaard.
2015.
Any-language frame-semanticparsing.
In EMNLP, pages 2062?2066.Adam Kilgarriff.
1997.
Putting frequencies in thedictionary.
International Journal of Lexicography,10(2):135?155.Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,Richard S. Zemel, Antonio Torralba, Raquel Urta-sun, and Sanja Fidler.
2015.
Skip-thought vectors.In NIPS.Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.2012.
Inducing crosslingual distributed representa-tions of words.
In COLING, pages 1459?1474.Tom??
Ko?cisk?, Karl Moritz Hermann, and Phil Blun-som.
2014.
Learning bilingual word representationsby marginalizing alignments.
In ACL, pages 224?229.Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-roni.
2015.
Hubness and pollution: Delving intocross-space mapping for zero-shot learning.
In ACL,pages 270?280.Ira Leviant and Roi Reichart.
2015.
Judgment lan-guage matters: Multilingual vector space models forjudgment language aware lexical semantics.
CoRR,abs/1508.00106.Omer Levy and Yoav Goldberg.
2014a.
Dependency-based word embeddings.
In ACL, pages 302?308.Omer Levy and Yoav Goldberg.
2014b.
Neural wordembedding as implicit matrix factorization.
In NIPS,pages 2177?2185.Omer Levy, Yoav Goldberg, and Ido Dagan.
2015.
Im-proving distributional similarity with lessons learnedfrom word embeddings.
Transactions of the ACL,3:211?225.Thang Luong, Hieu Pham, and Christopher D. Man-ning.
2015.
Bilingual word representations withmonolingual quality in mind.
In Proceedings of the1st Workshop on Vector Space Modeling for NaturalLanguage Processing, pages 151?159.Oren Melamud, Omer Levy, and Ido Dagan.
2015.
Asimple word embedding model for lexical substitu-tion.
In Proceedings of the 1st Workshop on VectorSpace Modeling for Natural Language Processing,pages 1?7.Oren Melamud, David McClosky, Siddharth Patward-han, and Mohit Bansal.
2016.
The role of contexttypes and dimensionality in learning word embed-dings.
In NAACL-HLT.Rada Mihalcea, Ravi Sinha, and Diana McCarthy.2010.
SemEval-2010 task 2: Cross-lingual lexicalsubstitution.
In SEMEVAL, pages 9?14.Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013a.Exploiting similarities among languages for ma-chine translation.
CoRR, abs/1309.4168.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.Corrado, and Jeffrey Dean.
2013b.
Distributed rep-resentations of words and phrases and their compo-sitionality.
In NIPS, pages 3111?3119.Dmitrijs Milajevs, Dimitri Kartsaklis, MehrnooshSadrzadeh, and Matthew Purver.
2014.
Evaluatingneural word representations in tensor-based compo-sitional settings.
In EMNLP, pages 708?719.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In ACL, pages236?244.Bhaskar Mitra, Eric T. Nalisnick, Nick Craswell,and Rich Caruana.
2016.
A dual embed-ding space model for document ranking.
CoRR,abs/1602.01137.Yves Peirsman and Sebastian Pad?.
2010.
Cross-lingual induction of selectional preferences withbilingual vector spaces.
In NAACL, pages 921?929.Tianze Shi, Zhiyuan Liu, Yang Liu, and Maosong Sun.2015.
Learning cross-lingual word embeddings viamatrix co-factorization.
In ACL, pages 567?572.David A. Smith and Jason Eisner.
2007.
Bootstrappingfeature-rich dependency parsers with entropic priors.In EMNLP-CoNLL, pages 667?677.Hubert Soyer, Pontus Stenetorp, and Akiko Aizawa.2015.
Leveraging monolingual data for crosslingualcompositional word representations.
In ICLR.J?rg Tiedemann.
2012.
Parallel data, tools and inter-faces in OPUS.
In LREC, pages 2214?2218.Chen-Tse Tsai and Dan Roth.
2016.
Cross-lingualwikification using multilingual embeddings.
InNAACL-HLT.256Kewei Tu and Vasant Honavar.
2012.
Unambiguityregularization for unsupervised learning of proba-bilistic grammars.
In EMNLP-CoNLL, pages 1324?1334.Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Ben-gio.
2010.
Word representations: A simple and gen-eral method for semi-supervised learning.
In ACL,pages 384?394.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: vector space models of se-mantics.
Journal of Artifical Intelligence Research,37(1):141?188.Shyam Upadhyay, Manaal Faruqui, Chris Dyer, andDan Roth.
2016.
Cross-lingual models of word em-beddings: An empirical comparison.
In ACL.Ivan Vuli?c and Marie-Francine Moens.
2013.
A studyon bootstrapping bilingual vector spaces from non-parallel data (and nothing else).
In EMNLP, pages1613?1624.Ivan Vuli?c and Marie-Francine Moens.
2014.
Proba-bilistic models of cross-lingual semantic similarityin context based on latent cross-lingual concepts in-duced from comparable data.
In EMNLP, pages349?362.Ivan Vuli?c and Marie-Francine Moens.
2015.
Mono-lingual and cross-lingual information retrieval mod-els based on (bilingual) word embeddings.
In SIGIR,pages 363?372.Ivan Vuli?c and Marie-Francine Moens.
2016.Bilingual distributed word representations fromdocument-aligned comparable data.
Journal of Ar-tificial Intelligence Research, 55:953?994.Ivan Vuli?c, Douwe Kiela, Stephen Clark, and Marie-Francine Moens.
2016.
Multi-modal representa-tions for improved bilingual lexicon learning.
InACL.Jason Weston, Samy Bengio, and Nicolas Usunier.2011.
WSABIE: scaling up to large vocabulary im-age annotation.
In IJCAI, pages 2764?2770.Yuan Zhang, David Gaddy, Regina Barzilay, andTommi Jaakkola.
2016.
Ten pairs to tag - Multilin-gual POS tagging via coarse mapping between em-beddings.
In NAACL-HLT.Will Y. Zou, Richard Socher, Daniel Cer, and Christo-pher D. Manning.
2013.
Bilingual word em-beddings for phrase-based machine translation.
InEMNLP, pages 1393?1398.257
