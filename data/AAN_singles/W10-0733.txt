Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 208?211,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUsing Mechanical Turk to Build Machine Translation Evaluation SetsMichael BloodgoodHuman Language TechnologyCenter of ExcellenceJohns Hopkins Universitybloodgood@jhu.eduChris Callison-BurchCenter for Language andSpeech ProcessingJohns Hopkins Universityccb@cs.jhu.eduAbstractBuilding machine translation (MT) test sets isa relatively expensive task.
As MT becomesincreasingly desired for more and more lan-guage pairs and more and more domains, itbecomes necessary to build test sets for eachcase.
In this paper, we investigate using Ama-zon?s Mechanical Turk (MTurk) to make MTtest sets cheaply.
We find that MTurk canbe used to make test sets much cheaper thanprofessionally-produced test sets.
More im-portantly, in experiments with multiple MTsystems, we find that the MTurk-producedtest sets yield essentially the same conclu-sions regarding system performance as theprofessionally-produced test sets yield.1 IntroductionMachine translation (MT) research is empiricallyevaluated by comparing system output against refer-ence human translations, typically using automaticevaluation metrics.
One method for establishing atranslation test set is to hold out part of the trainingset to be used for testing.
However, this practice typ-ically overestimates system quality when comparedto evaluating on a test set drawn from a different do-main.
Therefore, it?s necessary to make new test setsnot only for new language pairs but also for new do-mains.Creating reasonable sized test sets for new do-mains can be expensive.
For example, the Workshopon Statistical Machine Translation (WMT) uses amix of non-professional and professional translatorsto create the test sets for its annual shared translationtasks (Callison-Burch et al, 2008; Callison-Burchet al, 2009).
For WMT09, the total cost of creat-ing the test sets consisting of roughly 80,000 wordsacross 3027 sentences in seven European languageswas approximately $39,800 USD, or slightly morethan $0.08 USD/word.
For WMT08, creating testsets consisting of 2,051 sentences in six languageswas approximately $26,500 USD or slightly morethan $0.10 USD/word.In this paper we examine the use of Amazon?sMechanical Turk (MTurk) to create translation testsets for statistical machine translation research.Snow et al (2008) showed that MTurk can be usefulfor creating data for a variety of NLP tasks, and thata combination of judgments from non-experts canattain expert-level quality in many cases.
Callison-Burch (2009) showed that MTurk could be used forlow-cost manual evaluation of machine translationquality, and suggested that it might be possible touse MTurk to create MT test sets after an initial pi-lot study where turkers (the people who completethe work assignments posted on MTurk) producedtranslations of 50 sentences in five languages.This paper explores this in more detail by ask-ing turkers to translate the Urdu sentences of theUrdu-English test set used in the 2009 NIST Ma-chine Translation Evaluation Workshop.
We evalu-ate multiple MT systems on both the professionally-produced NIST2009 test set and our MTurk-produced test set and find that the MTurk-producedtest set yields essentially the same conclusions aboutsystem performance as the NIST2009 set yields.2082 Gathering the Translations viaMechanical TurkThe NIST2009 Urdu-English test set1 is a pro-fessionally produced machine translation evalua-tion set, containing four human-produced referencetranslations for each of 1792 Urdu sentences.
Weposted the 1792 Urdu sentences onMTurk and askedfor translations into English.
We charged $0.10 USDper translation, giving us a total translation cost of$179.20 USD.
A challenge we encountered duringthis data collection was that many turkers wouldcheat, giving us fake translations.
We noticed thatmany turkers were pasting the Urdu into an onlinemachine translation system and giving us the outputas their response even though our instructions saidnot to do this.
We manually monitored for this andrejected these responses and blocked these workersfrom computing any of our future work assignments.In the future, we plan to combat this in a more prin-cipled manner by converting our Urdu sentences intoan image and posting the images.
This way, thecheating turkers will not be able to cut and paste intoa machine translation system.We also noticed that many of the translations hadsimple mistakes such as misspellings and typos.
Wewanted to investigate whether these would decreasethe value of our test set so we did a second phaseof data collection where we posted the translationswe gathered and asked turkers (likely to be com-pletely different people than the ones who providedthe initial translations) to correct simple grammarmistakes, misspellings, and typos.
For this post-editing phase, we paid $0.25 USD per ten sentences,giving a total post-editing cost of $44.80 USD.In summary, we built two sets of reference trans-lations, one with no editing, and one with post-editing.
In the next section, we present the resultsof experiments that test how effective these test setsare for evaluating MT systems.3 Experimental ResultsA main purpose of an MT test set is to evaluate vari-ousMT systems?
performances relative to each otherand assist in drawing conclusions about the relative1http://www.itl.nist.gov/iad/894.01/tests/mt/2009/ResultsRelease/currentUrdu.htmlquality of the translations produced by the systems.2Therefore, if a given system, say System A, out-performs another given system, say System B, ona high-quality professionally-produced test set, thenwe would want to see that System A also outper-forms System B on our MTurk-produced test set.
Itis also desirable that the magnitudes of the differ-ences in performance between systems also be main-tained.In order to measure the differences in perfor-mance, using the differences in the absolute mag-nitudes of the BLEU scores will not work well be-cause the magnitudes of the BLEU scores are af-fected by many factors of the test set being used,such as the number of reference translations per for-eign sentence.
For determining performance differ-ences between systems and especially for compar-ing them across different test sets, we use percentageof baseline performance.
To compute percentage ofbaseline performance, we designate one system asthe baseline system and use percentage of that base-line system?s performance.
For example, Table 1shows both absolute BLEU scores and percentageperformance for three MT systems when tested onfive different test sets.
The first test set in the tableis the NIST-2009 set with all four reference trans-lations per Urdu sentence.
The next four test setsuse only a single reference translation per Urdu sen-tence (ref 1 uses the first reference translation only,ref 2 the second only, etc.).
Note that the BLEUscores for the single-reference translation test setsare much lower than for the test set with all four ref-erence translations and the difference in the absolutemagnitudes of the BLEU scores between the threedifferent systems are different for the different testsets.
However, the percentage performance of theMT systems is maintained (both the ordering of thesystems and the amount of the difference betweenthem) across the different test sets.We evaluated three different MT systems on theNIST2009 test set and on our two MTurk-producedtest sets (MTurk-NoEditing and MTurk-Edited).Two of the MT systems (ISI Syntax (Galley et al,2Another useful purpose would be to get some absolutesense of the quality of the translations but that seems out ofreach currently as the values of BLEU scores (the defacto stan-dard evaluation metric) are difficult to map to precise levels oftranslation quality.209Eval ISI JHU JoshuaSet (Syntax) (Syntax) (Hier.
)NIST-2009 33.10 32.77 26.65(4 refs) 100% 99.00% 80.51%NIST-2009 17.22 16.98 14.25(ref 1) 100% 98.61% 82.75%NIST-2009 17.76 17.14 14.69(ref 2) 100% 96.51% 82.71%NIST-2009 16.94 16.54 13.80(ref 3) 100% 97.64% 81.46%NIST-2009 13.63 13.67 11.05(ref 4) 100% 100.29% 81.07%Table 1: This table shows three MT systems evaluatedon five different test sets.
For each system-test set pair,two numbers are displayed.
The top number is the BLEUscore for that system when using that test set.
For ex-ample, ISI-Syntax tested on the NIST-2009 test set hasa BLEU score of 33.10.
The bottom number is the per-centage of baseline system performance that is achieved.ISI-Syntax (the highest-performing system on NIST2009to our knowledge) is used as the baseline.
Thus, it willalways have 100% as the percentage performance for allof the test sets.
To illustrate computing the percentageperformance for the other systems, consider for JHU-Syntax tested on NIST2009, that its BLEU score of 32.77divided by the BLEU score of the baseline system is32.77/33.10 ?
99.00%2004; Galley et al, 2006) and JHU Syntax (Li et al,2009) augmented with (Zollmann and Venugopal,2006)) were chosen because they represent state-of-the-art performance, having achieved the highestscores on NIST2009 to our knowledge.
They alsohave very similar performance on NIST2009 so wewant to see if that similar performance is maintainedas we evaluate on our MTurk-produced test sets.The third MT system (Joshua-Hierarchical) (Li etal., 2009), an open source implementation of (Chi-ang, 2007), was chosen because though it is a com-petitive system, it had clear, markedly lower perfor-mance on NIST2009 than the other two systems andwe want to see if that difference in performance isalso maintained if we were to shift evaluation to ourMTurk-produced test sets.Table 2 shows the results.
There are a numberof observations to make.
One is that the absolutemagnitude of the BLEU scores is much lower forall systems on the MTurk-produced test sets than onEval ISI JHU JoshuaSet (Syntax) (Syntax) (Hier.
)NIST- 33.10 32.77 26.652009 100% 99.00% 80.51%MTurk- 13.81 13.93 11.10NoEditing 100% 100.87% 80.38%MTurk- 14.16 14.23 11.68Edited 100% 100.49% 82.49%Table 2: This table shows three MT systems evaluated us-ing the official NIST2009 test set and the two test sets weconstructed (MTurk-NoEditing and MTurk-Edited).
Foreach system-test set pair, two numbers are displayed.
Thetop number is the BLEU score for that system when usingthat test set.
For example, ISI-Syntax tested on the NIST-2009 test set has a BLEU score of 33.10.
The bottomnumber is the percentage of baseline system performancethat is achieved.
ISI-Syntax (the highest-performing sys-tem on NIST2009 to our knowledge) is used as the base-line.the NIST2009 test set.
This is primarily because theNIST2009 set had four translations per foreign sen-tence whereas the MTurk-produced sets only haveone translation per foreign sentence.
Due to thisdifferent scale of BLEU scores, we compare perfor-mances using percentage of baseline performance.We use the ISI Syntax system as the baseline sinceit achieved the highest results on NIST2009.
Themain observation of the results in Table 2 is thatboth the relative performance of the various MT sys-tems and the amount of the differences in perfor-mance (in terms of percentage performance of thebaseline) are maintained when we use the MTurk-produced test sets as when we use the NIST2009 testset.
In particular, we can see that whether using theNIST2009 test set or the MTurk-produced test sets,one would conclude that ISI Syntax and JHU Syn-tax perform about the same and Joshua-Hierarchicaldelivers about 80% of the performance of the twosyntax systems.
The post-edited test set did notyield different conclusions than the non-edited testset yielded so the value of post-editing for test setcreation remains an open question.4 Conclusions and Future WorkIn conclusion, we have shown that it is feasible touse MTurk to build MT evaluation sets at a sig-210nificantly reduced cost.
But the large cost sav-ings does not hamper the utility of the test set forevaluating systems?
translation quality.
In exper-iments, MTurk-produced test sets lead to essen-tially the same conclusions about multiple MT sys-tems?
translation quality as much more expensiveprofessionally-produced MT test sets.It?s important to be able to build MT test setsquickly and cheaply because we need new ones fornew domains (as discussed in Section 1).
Now thatwe have shown the feasibility of using MTurk tobuild MT test sets, in the future we plan to buildnew MT test sets for specific domains (e.g., enter-tainment, science, etc.)
and release them to the com-munity to spur work on domain-adaptation for MT.We also envision using MTurk to collect addi-tional training data to tune an MT system for a newdomain.
It?s been shown that active learning can beused to reduce training data annotation burdens fora variety of NLP tasks (see, e.g., (Bloodgood andVijay-Shanker, 2009)).
Therefore, in future work,we plan to use MTurk combined with an activelearning approach to gather new data in the new do-main to investigate improving MT performance forspecialized domains.
But we?ll need new test sets inthe specialized domains to be able to evaluate the ef-fectiveness of this line of research and therefore, wewill need to be able to build new test sets.
In light ofthe findings we presented in this paper, it seems wecan build those test sets using MTurk for relativelylow costs without sacrificing much in their utility forevaluating MT systems.AcknowledgementsThis research was supported by the EuroMatrix-Plus project funded by the European Commission,by the DARPA GALE program under Contract No.HR0011-06-2-0001, and the NSF under grant IIS-0713448.
Thanks to Amazon Mechanical Turk forproviding a $100 credit.ReferencesMichael Bloodgood and K Vijay-Shanker.
2009.
Takinginto account the differences between actively and pas-sively acquired data: The case of active learning withsupport vector machines for imbalanced datasets.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 137?140, Boulder, Colorado, June.
Associationfor Computational Linguistics.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2008.
Furthermeta-evaluation of machine translation.
In Proceed-ings of the Third Workshop on Statistical MachineTranslation (WMT08), Colmbus, Ohio.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation (WMT09), March.Chris Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using Amazon?s Me-chanical Turk.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Process-ing, pages 286?295, Singapore, August.
Associationfor Computational Linguistics.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of the Human Language Technology Con-ference of the North American chapter of the Asso-ciation for Computational Linguistics (HLT/NAACL-2004), Boston, Massachusetts.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics (ACL-CoLing-2006), Sydney, Australia.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenThornton, Jonathan Weese, and Omar Zaidan.
2009.Joshua: An open source toolkit for parsing-based ma-chine translation.
In Proceedings of the Fourth Work-shop on Statistical Machine Translation, pages 135?139, Athens, Greece, March.
Association for Compu-tational Linguistics.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast - but is itgood?
Evaluating non-expert annotations for natu-ral language tasks.
In Proceedings of EMNLP-2008,Honolulu, Hawaii.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of the NAACL-2006 Workshop on Statis-tical Machine Translation (WMT-06), New York, NewYork.211
