Proceedings of the Workshop on Statistical Machine Translation, pages 94?101,New York City, June 2006. c?2006 Association for Computational LinguisticsHow Many Bits Are Needed To Store Probabilitiesfor Phrase-Based Translation?Marcello Federico and Nicola BertoldiITC-irst - Centro per la Ricerca Scientica e Tecnologica38050 Povo - Trento, Italy{federico,bertoldi}@itc.itAbstractState of the art in statistical machine trans-lation is currently represented by phrase-based models, which typically incorpo-rate a large number of probabilities ofphrase-pairs and word n-grams.
In thiswork, we investigate data compressionmethods for efficiently encoding n-gramand phrase-pair probabilities, that are usu-ally encoded in 32-bit floating point num-bers.
We measured the impact of com-pression on translation quality through aphrase-based decoder trained on two dis-tinct tasks: the translation of EuropeanParliament speeches from Spanish to En-glish, and the translation of news agenciesfrom Chinese to English.
We show thatwith a very simple quantization scheme allprobabilities can be encoded in just 4 bitswith a relative loss in BLEU score on thetwo tasks by 1.0% and 1.6%, respectively.1 IntroductionIn several natural language processing tasks, such asautomatic speech recognition and machine transla-tion, state-of-the-art systems rely on the statisticalapproach.Statistical machine translation (SMT) is basedon parametric models incorporating a large num-ber of observations and probabilities estimated frommonolingual and parallel texts.
The current state ofthe art is represented by the so-called phrase-basedtranslation approach (Och and Ney, 2004; Koehn etal., 2003).
Its core components are a translationmodel that contains probabilities of phrase-pairs,and a language model that incorporates probabilitiesof word n-grams.Due to the intrinsic data-sparseness of languagecorpora, the set of observations increases almost lin-early with the size of the training data.
Hence, toefficiently store observations and probabilities in acomputer memory the following approaches can betackled: designing compact data-structures, pruningrare or unreliable observations, and applying datacompression.In this paper we only focus on the last approach.We investigate two different quantization methodsto encode probabilities and analyze their impact ontranslation performance.
In particular, we addressthe following questions:?
How does probability quantization impact onthe components of the translation system,namely the language model and the translationmodel??
Which is the optimal trade-off between datacompression and translation performance??
How do quantized models perform under dif-ferent data-sparseness conditions??
Is the impact of quantization consistent acrossdifferent translation tasks?Experiments were performed with our phrase-based SMT system (Federico and Bertoldi, 2005) ontwo large-vocabulary tasks: the translation of Euro-pean Parliament Plenary Sessions from Spanish to94English, and the translation of news agencies fromChinese to English, according to the set up definedby the 2005 NIST MT Evaluation Workshop.The paper is organized as follows.
Section 2 re-views previous work addressing efficiency in speechrecognition and information retrieval.
Section 3 in-troduces the two quantization methods consideredin this paper, namely the Lloyd?s algorithm and theBinning method.
Section 4 briefly describes ourphrase-based SMT system.
Sections 5 reports anddiscusses experimental results addressing the ques-tions in the introduction.
Finally, Section 6 drawssome conclusions.2 Previous workMost related work can be found in the area of speechrecognition, where n-gram language models havebeen used for a while.Efforts targeting efficiency have been mainly fo-cused on pruning techniques (Seymore and Rosen-feld, 1996; Gao and Zhang, 2002), which permitto significantly reduce the amount of n-grams to bestored at a negligible cost in performance.
More-over, very compact data-structures for storing back-off n-gram models have been recently proposed byRaj and Whittaker (2003).Whittaker and Raj (2001) discuss probability en-coding as a means to reduce memory requirementsof an n-gram language model.
Quantization of a3-gram back-off model was performed by applyingthe k-means Lloyd-Max algorithm at each n-gramlevel.
Experiments were performed on several large-vocabulary speech recognition tasks by consideringdifferent levels of compression.
By encoded proba-bilities in 4 bits, the increase in word-error-rate wasonly around 2% relative with respect to a baselineusing 32-bit floating point probabilities.Similar work was carried out in the field of in-formation retrieval, where memory efficiency is in-stead related to the indexing data structure, whichcontains information about frequencies of terms inall the individual documents.
Franz and McCarley(2002) investigated quantization of term frequenciesby applying a binning method.
The impact on re-trieval performance was analyzed against differentquantization levels.
Results showed that 2 bits aresufficient to encode term frequencies at the cost of anegligible loss in performance.In our work, we investigate both data compres-sion methods, namely the Lloyd?s algorithm and thebinning method, in a SMT framework.3 QuantizationQuantization provides an effective way of reducingthe number of bits needed to store floating pointvariables.
The quantization process consists in par-titioning the real space into a finite set of k quantiza-tion levels and identifying a center ci for each level,i = 1, .
.
.
, k. A function q(x) maps any real-valuedpoint x onto its unique center ci.
Cost of quantiza-tion is the approximation error between x and ci.If k = 2h, h bits are enough to represent a floatingpoint variable; as a floating point is usually encodedin 32 bits (4 byte), the compression ratio is equalto 32/h1 .
Hence, the compression ratio also givesan upper bound for the relative reduction of mem-ory use, because it assumes an optimal implemen-tation of data structures without any memory waste.Notice that memory consumption for storing the k-entry codebook is negligible (k ?
32 bits).As we will apply quantization on probabilisticdistribution, we can restrict the range of real val-ues between 0 and 1.
Most quantization algorithmsrequire a fixed (although huge) amount of pointsin order to define the quantization levels and theircenters.
Probabilistic models used in SMT satisfythis requirement because the set of parameters largerthan 0 is always limited.Quantization algorithms differ in the way parti-tion of data points is computed and centers are iden-tified.
In this paper we investigate two differentquantization algorithms.Lloyd?s AlgorithmQuantization of a finite set of real-valued data pointscan be seen as a clustering problem.
A large fam-ily of clustering algorithms, called k-means algo-rithms (Kanungo et al, 2002), look for optimal cen-ters ci which minimize the mean squared distancefrom each data point to its nearest center.
The mapbetween points and centers is trivially derived.1In the computation of the compression ratio we take intoaccount only the memory needed to store the probabilities of theobservations, and not the memory needed to store the observa-tions themselves which depends on the adopted data structures.95As no efficient exact solution to this problemis known, either polynomial-time approximation orheuristic algorithms have been proposed to tacklethe problem.
In particular, Lloyd?s algorithm startsfrom a feasible set of centers and iteratively movesthem until some convergence criterion is satisfied.Finally, the algorithm finds a local optimal solution.In this work we applied the version of the algorithmavailable in the K-MEANS package2 .Binning MethodThe binning method partitions data points into uni-formly populated intervals or bins.
The center ofeach bin corresponds to the mean value of all pointsfalling into it.
If Ni is the number of points of thei-th bin, and xi the smallest point in the i-th bin, apartition [xi, xi+1] results such that Ni is constantfor each i = 0, .
.
.
, k ?
1, where xk = 1 by default.The following map is thus defined:q(x) = ci if xi <= x < xi+1.Our implementation uses the following greedystrategy: bins are build by uniformly partition alldifferent points of the data set.4 Phrase-based Translation SystemGiven a string f in the source language, our SMTsystem (Federico and Bertoldi, 2005; Cettolo et al,2005), looks for the target string e maximizing theposterior probability Pr(e,a | f) over all possibleword alignments a.
The conditional distribution iscomputed with the log-linear model:p?
(e,a | f) ?
exp{ R?r=1?rhr(e, f ,a)},where hr(e, f ,a), r = 1 .
.
.
R are real valued featurefunctions.The log-linear model is used to score translationhypotheses (e,a) built in terms of strings of phrases,which are simple sequences of words.
The transla-tion process works as follows.
At each step, a targetphrase is added to the translation whose correspond-ing source phrase within f is identified through threerandom quantities: the fertility which establishes itslength; the permutation which sets its first position;2www.cs.umd.edu/?mount/Projects/KMeans.the tablet which tells its word string.
Notice that tar-get phrases might have fertility equal to zero, hencethey do not translate any source word.
Moreover,untranslated words in f are also modeled throughsome random variables.The choice of permutation and tablets can beconstrained in order to limit the search space un-til performing a monotone phrase-based translation.In any case, local word reordering is permitted byphrases.The above process is performed by a beam-searchdecoder and is modeled with twelve feature func-tions (Cettolo et al, 2005) which are either esti-mated from data, e.g.
the target n-gram languagemodels and the phrase-based translation model, orempirically fixed, e.g.
the permutation models.While feature functions exploit statistics extractedfrom monolingual or word-aligned texts from thetraining data, the scaling factors ?
of the log-linearmodel are empirically estimated on developmentdata.The two most memory consuming feature func-tions are the phrase-based Translation Model (TM)and the n-gram Language Model (LM).Translation ModelThe TM contains phrase-pairs statistics computedon a parallel corpus provided with word-alignmentsin both directions.
Phrase-pairs up to length 8 areextracted and singleton observations are pruned off.For each extracted phrase-pair (f?
, e?
), four transla-tion probabilities are estimated:?
a smoothed frequency of f?
given e??
a smoothed frequency of e?
given f??
an IBM model 1 based probability of e?
given f??
an IBM model 1 based probability of f?
given e?Hence, the number of parameters of the transla-tion models corresponds to 4 times the number ofextracted phrase-pairs.
From the point of view ofquantization, the four types of probabilities are con-sidered separately and a specific codebook is gener-ated for each type.Language ModelThe LM is a 4-gram back-off model estimated withthe modied Kneser-Ney smoothing method (Chenand Goodman, 1998).
Singleton pruning is appliedon 3-gram and 4-gram statistics.
In terms of num-96task parallel resources mono resources LM TMsrc trg words 1-gram 2-gram 3-gram 4-gram phrase pairsNIST 82,168 88,159 463,855 1,408 20,475 29,182 46,326 10,410EPPS 34,460 32,951 3,2951 110 2,252 2,191 2,677 3,877EPPS-800 23,611 22,520 22,520 90 1,778 1,586 1,834 2,499EPPS-400 11,816 11,181 11,181 65 1,143 859 897 1,326EPPS-200 5,954 5,639 5,639 47 738 464 439 712EPPS-100 2,994 2,845 2,845 35 469 246 213 387Table 1: Figures (in thousand) regarding the training data of each translation task.ber of parameters, each n-gram, with n < 4, hastwo probabilities associated with: the probability ofthe n-gram itself, and the back-off probability of thecorresponding n + 1-gram extensions.
Finally, 4-grams have only one probability associated with.For the sake of quantization, two separate code-books are generated for each of the first three lev-els, and one codebook is generated for the last level.Hence, a total of 7 codebooks are generated.
In alldiscussed quantized LMs, unigram probabilities arealways encoded with 8 bits.
The reason is that uni-gram probabilities have indeed the largest variabilityand do not contribute significantly to the total num-ber of parameters.5 ExperimentsData and Experimental FrameworkWe performed experiments on two large vocabularytranslation tasks: the translation of European Parlia-mentary Plenary Sessions (EPPS) (Vilar et al, 2005)from Spanish to English, and the translation of doc-uments from Chinese to English as proposed by theNIST MT Evaluation Workshops3 .Translation of EPPS is performed on the so-calledfinal text editions, which are prepared by the trans-lation office of the European Parliament.
Both thetraining and testing data were collected by the TC-STAR4 project and were made freely available toparticipants in the 2006 TC-STAR Evaluation Cam-paign.
In order to perform experiments under differ-ent data sparseness conditions, four subsamples ofthe training data with different sizes were generated,too.Training and test data used for the NIST task are3www.nist.gov/speech/tests/mt/.4www.tc-star.orgtask sentences src words ref wordsEPPS 840 22725 23066NIST 919 25586 29155Table 2: Statistics of test data for each task.available through the Linguistic Data Consortium5.Employed training data meet the requirements setfor the Chinese-English large-data track of the 2005NIST MT Evaluation Workshop.
For testing weused instead the NIST 2003 test set.Table 1 reports statistics about the training data ofeach task and the models estimated on them.
Thatis, the number of running words of source and targetlanguages, the number of n-grams in the languagemodel and the number phrase-pairs in the transla-tion model.
Table 2 reports instead statistics aboutthe test sets, namely, the number of source sentencesand running words in the source part and in the goldreference translations.Translation performance was measured in termsof BLEU score, NIST score, word-error rate (WER),and position independent error rate (PER).
Scorecomputation relied on two and four reference trans-lations per sentence, respectively, for the EPPSand NIST tasks.
Scores were computed in case-insensitive modality with punctuation.
In general,none of the above measures is alone sufficiently in-formative about translation quality, however, in thecommunity there seems to be a preference towardreporting results with BLEU.
Here, to be on the safeside and to better support our findings we will reportresults with all measures, but will limit discussionon performance to the BLEU score.In order to just focus on the effect of quantiza-5www.ldc.upenn.edu97LM-h32 8 6 5 4 3 232 54.78 54.75 54.73 54.65 54.49 54.24 53.828 54.78 54.69 54.69 54.79 54.55 54.18 53.656 54.57 54.49 54.76 54.57 54.63 54.26 53.60TM-h 5 54.68 54.68 54.56 54.61 54.60 54.10 53.394 54.37 54.36 54.47 54.44 54.23 54.06 53.263 54.28 54.03 54.22 53.96 53.75 53.69 53.032 53.58 53.51 53.47 53.35 53.39 53.41 52.41Table 3: BLEU scores in the EPPS task with different quantization levels of the LM and TM.tion, all reported experiments were performed witha plain configuration of the ITC-irst SMT system.That is, we used a single decoding step, no phrasere-ordering, and task-dependent weights of the log-linear model.Henceforth, LMs and TM quantized with h bitsare denoted with LM-h and TM-h, respectively.Non quantized models are indicated with LM-32and TM-32.Impact of Quantization on LM and TMA first set of experiments was performed on theEPPS task by applying probability quantization ei-ther on the LM or on the TMs.
Figures 1 and 2compare the two proposed quantization algorithms(LLOYD and BINNING) against different levels ofquantization, namely 2, 3, 4, 5, 6, and 8 bits.The scores achieved by the non quantized models(LM-32 and TM-32) are reported as reference.The following considerations can be drawn fromthese results.
The Binning method works slightly,but not significantly, better than the Lloyd?s algo-rithm, especially with the highest compression ra-tios.In general, the LM seems less affected by datacompression than the TM.
By comparing quantiza-tion with the binning method against no quantiza-tion, the BLEU score with LM-4 is only 0.42% rel-ative worse (54.78 vs 54.55).
Degradation of BLEUscore by TM-4 is 0.77% (54.78 vs 54.36).
For all themodels, encoding with 8 bits does not affect transla-tion quality at all.In following experiments, binning quantizationwas applied to both LM and TM.
Figure 3 plotsall scores against different levels of quantization.As references, the curves corresponding to onlyLM-h TM-h BLEU NIST WER PER32 32 28.82 8.769 62.41 42.308 8 28.87 8.772 62.39 42.194 4 28.36 8.742 62.94 42.452 2 25.95 8.491 65.87 44.04Table 4: Translation scores on the NIST task withdifferent quantization levels of the LM and TM.LM quantization (LM-h) and only TM quantization(TM-h) are shown.
Independent levels of quantiza-tion of the LM and TM were also considered.
BLEUscores related to several combinations are reportedin Table 3.Results show that the joint impact of LM and TMquantization is almost additive.
Degradation with4 bits quantization is only about 1% relative (from54.78 to 54.23).
Quantization with 2 bits is sur-prisingly robust: the BLEU score just decreases by4.33% relative (from 54.78 to 52.41).Quantization vs. Data SparsenessQuantization of LM and TM was evaluated with re-spect to data-sparseness.
Quantized and not quan-tized models were trained on four subset of the EPPScorpus with decreasing size.
Statistics about thesesub-corpora are reported in Table 1.
Quantizationwas performed with the binning method using 2,4, and 8 bit encodings.
Results in terms of BLEUscore are plotted in Figure 4.
It is evident that thegap in BLEU score between the quantized and notquantized models is almost constant under differenttraining conditions.
This result suggests that perfor-mance of quantized models is not affected by datasparseness.98Consistency Across Different TasksA subset of quantization settings tested with theEPPS tasks was also evaluated on the NIST task.Results are reported in Table 4.Quantization with 8 bits does not affect perfor-mance, and gives even slightly better scores.
Alsoquantization with 4 bits produces scores very closeto those of non quantized models, with a loss inBLEU score of only 1.60% relative.
However, push-ing quantization to 2 bits significantly deterioratesperformance, with a drop in BLEU score of 9.96%relative.In comparison to the EPPS task, performancedegradation due to quantization seems to be twice aslarge.
In conclusion, consistent behavior is observedamong different degrees of compression.
Absoluteloss in performance, though quite different from theEPPS task, remains nevertheless very reasonable.Performance vs. CompressionFrom the results of single versus combined com-pression, we can reasonably assume that perfor-mance degradation due to quantization of LM andTM probabilities is additive.
Hence, as memory sav-ings on the two models are also independent we canlook at the optimal trade-off between performanceand compression separately.
Experiments on theNIST and EPPS tasks seem to show that encodingof LM and TM probabilities with 4 bits provides thebest trade-off, that is a compression ratio of 8 with arelative loss in BLEU score of 1% and 1.6%.
It canbe seen that score degradation below 4 bits growsgenerally faster than the corresponding memory sav-ings.6 ConclusionIn this paper we investigated the application of datacompression methods to the probabilities stored bya phrase-based translation model.
In particular,probability quantization was applied on the n-gramlanguage model and on the phrase-pair translationmodel.
Experimental results confirm previous find-ings in speech recognition: language model proba-bilities can be encoded in just 4 bits at the cost ofa very little loss in performance.
The same resolu-tion level seems to be a good compromise even forthe translation model.
Remarkably, the impact ofquantization on the language model and translationmodel seems to be additive with respect to perfor-mance.
Finally, quantization does not seems to beaffected by data sparseness and behaves similarly ondifferent translation tasks.ReferencesM Cettolo, M. Federico, N. Bertoldi, R. Cattoni, and B.Chen.
2005.
A Look Inside the ITC-irst SMT Sys-tem.
In Proc.
of MT Summit X, pp.
451?457, Pukhet,Thailand.S.
F. Chen and J. Goodman.
1998.
An EmpiricalStudy of Smoothing Techniques for Language Mod-eling.
Technical Report TR-10-98, Computer ScienceGroup, Harvard University, Cambridge, MA, USA.M.
Federico and N. Bertoldi.
2005.
A Word-to-PhraseStatistical Translation Model.
ACM Transaction onSpeech Language Processing, 2(2):1?24.M.
Franz and J. S. McCarley.
2002.
How Many Bits areNeeded to Store Term Frequencies.
In Proc.
of ACMSIGIR, pp.
377?378, Tampere, Finland.J.
Gao and M. Zhang.
2002.
Improving Language ModelSize Reduction using Better Pruning Criteria.
In Proc.of ACL, pp.
176?182, Philadelphia, PA.T.
Kanungo, D. M. Mount, N. Netanyahu, C. Piatko,R.
Silverman, , and A. Y. Wu.
2002.
An EfficientK-Means Clustering Algorithm: Analysis and Imple-mentation.
IEEE Transaction on Pattern Analysis andMachine Intelligence, 24(7):881?892.P.
Koehn, F. J. Och, and D. Marcu.
2003.
StatisticalPhrase-Based Translation.
In Proc.
of HLT/NAACL2003, pp.
127?133, Edmonton, Canada.F.
J. Och and H. Ney.
2004.
The Alignment TemplateApproach to Statistical Machine Translation.
Compu-tational Linguistics, 30(4):417?449.B.
Raj and E. W. D. Whittaker.
2003.
Lossless Compres-sion of Language Model Structure and Word Identi-fiers.
In Proc.
of ICASSP, pp.
388?391, Honk Kong.K.
Seymore and R. Rosenfeld.
1996.
Scalable BackoffLanguage Models.
In Proc.
of ICSLP, vol.
1, pp.
232?235, Philadelphia, PA.D.
Vilar, E. Matusov, S. Hasan, R .
Zens, , and H. Ney.2005.
Statistical Machine Translation of EuropeanParliamentary Speeches.
In Proc.
of MT Summit X,pp.
259?266, Pukhet, Thailand.E.
W. D. Whittaker and B. Raj.
2001.
Quantization-based Language Model Compression.
In Proc.
of Eu-rospeech, pp.
33?36, Aalborg, Denmark.995151.55252.55353.55454.55532865432BLEUSCOREBITSBINNINGLLOYD10.310.410.510.610.710.832865432NISTSCOREBITSBINNINGLLOYD34.53535.53636.532865432WERBITSBINNINGLLOYD2525.52626.52727.52832865432PERBITSBINNINGLLOYDFigure 1: EPPS task: translation scores vs. quantization level of LM.
TM is not quantized.5151.55252.55353.55454.55532865432BLEUSCOREBITSBINNINGLLOYD10.310.410.510.610.710.832865432NISTSCOREBITSBINNINGLLOYD34.53535.53636.532865432WERBITSBINNINGLLOYD2525.52626.52727.52832865432PERBITSBINNINGLLOYDFigure 2: EPPS task: translation scores vs. quantization level of TM.
LM is not quantized.1005252.55353.55454.55532865432BLEUSCOREBITSLM-h+TM-hLM-hTM-h10.410.4510.510.5510.610.6510.710.7510.832865432NISTSCOREBITSLM-h+TM-hLM-hTM-h34.634.83535.235.435.635.83632865432WERBITSLM-h+TM-hLM-hTM-h2525.52626.52732865432PERBITSLM-h+TM-hLM-hTM-hFigure 3: EPPS task: translation scores vs. quantization level of LM and TM.
Quantization was performedwith the Binning algorithm.444648505254EPPSEPPS-800EPPS-400EPPS-200EPPS-100BLEUSCORELM-32+TM32LM-8+TM-8LM-4+TM-4LM-2+TM-29.69.81010.210.410.610.8EPPSEPPS-800EPPS-400EPPS-200EPPS-100NISTSCORELM-32+TM-32LM-8+TM-8LM-4+TM-4LM-2+TM-23435363738394041EPPSEPPS-800EPPS-400EPPS-200EPPS-100WERSCORELM-32+TM-32LM-8+TM-8LM-4+TM-4LM-2+TM-2252627282930EPPSEPPS-800EPPS-400EPPS-200EPPS-100PERSCORELM-32+TM-32LM-8+TM-8LM-4+TM-4LM-2+TM-2Figure 4: EPPS task: translation scores vs. amount of training data.
Different levels of quantization weregenerated with the Binning algorithm.101
