Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 33?41,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsContinuous Measurement Scales inHuman Evaluation of Machine TranslationYvette Graham Timothy Baldwin Alistair Moffat Justin ZobelDepartment of Computing and Information Systems, The University of Melbourne{ygraham,tbaldwin,ammoffat,jzobel}@unimelb.edu.auAbstractWe explore the use of continuous rat-ing scales for human evaluation in thecontext of machine translation evaluation,comparing two assessor-intrinsic quality-control techniques that do not rely onagreement with expert judgments.
Ex-periments employing Amazon?s Mechan-ical Turk service show that quality-controltechniques made possible by the use ofthe continuous scale show dramatic im-provements to intra-annotator agreementof up to +0.101 in the kappa coefficient,with inter-annotator agreement increasingby up to +0.144 when additional standard-ization of scores is applied.1 IntroductionHuman annotations of language are often requiredin natural language processing (NLP) tasks forevaluation purposes, in order to estimate how wella given system mimics activities traditionally per-formed by humans.
In tasks such as machinetranslation (MT) and natural language generation,the system output is a fully-formed string in a tar-get language.
Annotations can take the form ofdirect estimates of the quality of those outputs orbe structured as the simpler task of ranking com-peting outputs from best-to-worst (Callison-Burchet al 2012).A direct estimation method of assessment, asopposed to ranking outputs from best-to-worst,has the advantage that it includes in annotationsnot only that one output is better than another,but also the degree to which that output was bet-ter than the other.
In addition, direct estimationof quality within the context of machine transla-tion extends the usefulness of the annotated datato other tasks such as quality-estimation (Callison-Burch et al 2012).For an evaluation to be credible, the annotationsmust be credible.
The simplest way of establish-ing this is to have the same data point annotated bymultiple annotators, and measure the agreementbetween them.
There has been a worrying trendin recent MT shared tasks ?
whether the evalu-ation was structured as ranking translations frombest-to-worst, or by direct estimation of fluencyand adequacy ?
of agreement between annotatorsdecreasing (Callison-Burch et al 2008; Callison-Burch et al 2009; Callison-Burch et al 2010;Callison-Burch et al 2011; Callison-Burch et al2012).
Inconsistency in human evaluation of ma-chine translation calls into question conclusionsdrawn from those assessments, and is the targetof this paper: by revising the annotation process,can we improve annotator agreement, and hencethe quality of human annotations?Direct estimates of quality are intrinsically con-tinuous in nature, but are often collected using aninterval-level scale with a relatively low numberof categories, perhaps to make the task cognitivelyeasier for human assessors.
In MT evaluation,five and seven-point interval-level scales are com-mon (Callison-Burch et al 2007; Denkowski andLavie, 2010).
However, the interval-level scalecommonly used for direct estimation of translationquality (and other NLP annotation tasks) forceshuman judges to discretize their assessments intoa fixed number of categories, and this processcould be a cause of inconsistency in human judg-ments.
In particular, an assessor may be repeatedlyforced to choose between two categories, neitherof which really fits their judgment.
The contin-uous nature of translation quality assessment, aswell as the fact that many statistical methods ex-ist that can be applied to continuous data but notinterval-level data, motivates our trial of a contin-uous rating scale.We use human judgments of translation fluencyas a test case and compare consistency levels when33the conventional 5-point interval-level scale and acontinuous visual analog scale (VAS) are used forhuman evaluation.
We collected data via Ama-zon?s Mechanical Turk, where the quality of an-notations is known to vary considerably (Callison-Burch et al 2010).
As such, we test two quality-control techniques based on statistical significance?
made possible by the use of the continuous ratingscale ?
to intrinsically assess the quality of individ-ual human judges.
The quality-control techniquesare not restricted to fluency judgments and are rel-evant to more general MT evaluation, as well asother NLP annotation tasks.2 Machine Translation FluencyMeasurement of fluency as a component of MTevaluation has been carried out for a number ofyears (LDC, 2005), but it has proven difficultto acquire consistent judgments, even from ex-pert assessors.
Evaluation rounds such as the an-nual Workshop on Statistical Machine Translation(WMT) use human judgments of translation qual-ity to produce official rankings in shared tasks, ini-tially using an two-item assessment of fluency andadequacy as separate attributes, and more recentlyby asking judges to simply rank system outputsagainst one another according to ?which transla-tion is better?.
However, the latter method also re-ports low levels of agreement between judges.
Forexample, the 2007 WMT reported low levels ofconsistency in fluency judgments in terms of bothintra-annotator agreement (intra-aa), with a kappacoefficient of ?
= 0.54 (moderate), and inter-annotator agreement (inter-aa), with ?
= 0.25(slight).
Adequacy judgments for the same datareceived even lower scores: ?
= 0.47 for intra-aa,and ?
= 0.23 for inter-aa.While concerns over annotator agreement haveseen recent WMT evaluations move away from us-ing fluency as an evaluation component, there canbe no question that fluency is a useful means ofevaluating translation output.
In particular, it is notbiased by reference translations.
The use of auto-matic metrics is often criticized by the fact thata system that produces a good translation whichhappens not to be similar to the reference trans-lations will be unfairly penalized.
Similarly, ifhuman annotators are provided with one or morereference sentences, they may inadvertently favortranslations that are similar to those references.
Iffluency is judged independently of adequacy, noreference translation is needed, and the bias is re-moved.In earlier work, we consider the possibilitythat translation quality is a hypothetical construct(Graham et al 2012), and suggest applying meth-ods of validating measurement of psychologicalconstructs to the validation of measurements oftranslation quality.
In psychology, a scale that em-ploys more items as opposed to fewer is consid-ered more valid.
Under this criteria, a two-item(fluency and adequacy) scale is more valid than asingle-item translation quality measure.3 Measurement ScalesDirect estimation methods are designed to elicitfrom the subject a direct quantitative estimate ofthe magnitude of an attribute (Streiner and Nor-man, 1989).
We compare judgments collectedon a visual analog scale (VAS) to those using aninterval-level scale presented to the human judgeas a sequence of radio-buttons.
The VAS was firstused in psychology in the 1920?s, and prior to thedigital age, scales used a line of fixed length (usu-ally 100mm in length), with anchor labels at bothends, and to be marked by hand with an ?X?
at thedesired location (Streiner and Norman, 1989).When an interval-scale is used in NLP evalua-tion or other annotation tasks, it is commonly pre-sented in the form of an adjectival scale, wherecategories are labeled in increasing/decreasingquality.
For example, an MT evaluation of fluencymight specify 5 = ?Flawless English?, 4 = ?GoodEnglish?, 3 = ?Non-native English?, 2 = ?DisfluentEnglish?, and 1 = ?Incomprehensible?
(Callison-Burch et al 2007; Denkowski and Lavie, 2010).With both a VAS and an adjectival scale, thechoice of labels can be critical.
In medical re-search, patients?
ratings of their own health havebeen shown to be highly dependent on the ex-act wording of descriptors (Seymour et al 1985).Alexandrov (2010) provides a summary of the ex-tensive literature on the numerous issues associ-ated with adjectival scale labels, including biasresulting from positively and negatively wordeditems not being true opposites of one another, anditems intended to have neutral intensity in factproving to have unique conceptual meanings.Likert scales avoid the problems associated withadjectival labels, by structuring the question asa simple statement that the respondent registerstheir level of (dis)agreement with.
Figure 1 shows34Figure 1: Amazon Mechanical Turk interface for fluency judgments with a Likert-type scale.Figure 2: Continuous rating scale for fluency judgments with two anchors.the Likert-type interval-level scale we use to col-lect fluency judgments of MT output, and Fig-ure 2 shows an equivalent VAS using the twomost extreme anchor labels, strongly disagree andstrongly agree.4 Crowd-sourcing JudgmentsThe volume of judgments required for evaluationof NLP tasks can be large, and employing expertsto undertake those judgments may not always befeasible.
Crowd-sourcing services via the Web of-fer an attractive alternative, and have been used inconjunction with a range of NLP evaluation andannotation tasks.
Several guides exist for instruct-ing researchers from various backgrounds on us-ing Amazon?s Mechanical Turk (AMT) (Gibson etal., 2011; Callison-Burch, 2009), and allowancefor the use of AMT is increasingly being madein research grant applications, as a cost-effectiveway of gathering data.
Issues remain in connec-tion with low payment levels (Fort et al 2011);nevertheless, Ethics Approval Boards are typicallydisinterested in projects that make use of AMT, re-garding AMT as being a purchased service ratherthan a part of the experimentation that may affecthuman subjects.The use of crowd-sourced judgments does,however, introduce the possibility of increased in-consistency, with service requesters typically hav-ing no specific or verifiable knowledge about anygiven worker.
Hence, the possibility that a workeris acting in good faith but not performing the taskwell must be allowed for, as must the likelihoodthat some workers will quite ruthlessly seek tominimize the time spent on the task, by deliber-ately giving low-quality or fake answers.
Someworkers may even attempt to implement auto-mated responses, so that they get paid without hav-ing to do the work they are being paid for.For example, if the task at hand is that of assess-ing the fluency of text snippets, it is desirable toemploy native speakers.
With AMT the requesterhas the ability to restrict responses to only workerswho have a specified skill.
But that facility doesnot necessarily lead to confidence ?
there is noth-ing stopping a worker employing someone elseto do the test for them.
Devising a test that reli-ably evaluates whether or not someone is a nativespeaker is also not at all straightforward.Amazon allow location restrictions, based onthe registered residential address of the Turker,which can be used to select in favor of those likelyto have at least some level of fluency (Callison-Burch et al 2010).
We initially applied this re-striction to both sets of judgments in experiments,setting the task up so that only workers regis-tered in Germany could evaluate the to-Germantranslations, for example.
However, very low re-35sponse rates for languages other than to-Englishwere problematic, and we also received a numberof apparently-genuine requests from native speak-ers residing outside the target countries.
As aresult, we removed all location restrictions otherthan for the to-English tasks.1Crowd-sourcing judgments has the obvious riskof being vulnerable to manipulation.
On the otherhand, crowd-sourced judgments also offer the po-tential of being more valid than those of experts,since person-in-the-street abilities might be a moreuseful yardstick for some tasks than informed aca-demic judgment, and because a greater number ofjudges may be available.Having the ability to somehow evaluate thequality of the work undertaken by a Turker is thushighly desirable.
We would like to be able to putin place a mechanism that filters out non-nativespeakers; native speakers with low literacy levels;cheats; and robotic cheats.
That goal is consideredin the next section.5 Judge-Intrinsic Quality ControlOne common method of quality assessment for anew process is to identify a set of ?gold-standard?items that have been judged by experts and whosemerits are agreed, present them to the new processor assessor, and then assess the degree to whichthe new process and the experts ?agree?
on theoutcomes (Snow et al 2008; Callison-Burch etal., 2010).
A possible concern is that even expertscan be expected to disagree (and hence have lowinter-aa levels), meaning that disagreement withthe new process will also occur, even if the newprocess is a reliable one.
In addition, the qual-ity of the judgments collected is also assessed viaagreement levels, meaning that any filtering basedon a quality-control measure that uses agreementwill automatically increase consistency, even tothe extent of recalibrating non-expert workers?
re-sponses to more closely match expert judgments(Snow et al 2008).
Moreover, if an interval-levelscale is used, standardized scores cannot be em-ployed, so a non-expert who is more lenient thanthe experts, but in a reliable and systematic man-ner, might still have their assessments discarded.For judgments collected on a continuous scale,statistical tests based on difference of means (overassessors) are possible.
We structure our human1It has also been suggested that AMT restricts Turker reg-istration by country; official information is unclear about this.T1 initial :T1 repeat :d1Bad Ref for T2 :d2T2 initial :Figure 3: Intrinsic quality-control distributions foran individual judge.intelligence tasks (HITs) on Mechanical Turk ingroups of 100 in a way that allows us to controlassignment of repeat item pairs to workers, so thatstatistical tests can later be applied to an individ-ual worker?s score distributions for repeat items.Workers were made aware of the task structurebefore accepting it ?
the task preview included amessage This HIT consists of 100 fluency assess-ments, you have 0 so far complete.We refer to the repeat items in a HIT asask again translations.
In addition, we inserted anumber of bad reference pairs into each HIT, witha bad reference pair consisting of a genuine MTsystem output, and a distorted sentence derivedfrom it, expecting that its fluency was markedlyworse than that of the corresponding system out-put.
This was done by randomly selecting twowords in the sentence and duplicating them in ran-dom locations not adjacent to the original wordand not in the initial or sentence-final position.Any other degradation method could also be used,so long as it has a high probability of reducing thefluency of the text, and provided that it is not im-mediately obvious to the judges.Insertion of ask again and bad reference pairsinto the HITs allowed two measurements to bemade for each worker: when presented withan ask again pair, we expect a conscientiousjudge to give similar scores (but when usinga continuous scale, certainly not identical), andon bad reference pairings a conscientious judgeshould reliably give the altered sentence a lowerscore.
The wide separation of the two appear-ances of an ask again pair makes it unlikely thata judge would remember either the sentence ortheir first reaction to it, and backwards movementthrough the sentences comprising each HIT wasnot possible.
In total, each HIT contained 100 sen-36Figure 4: Welch?s t-test reliability estimates plot-ted against mean seconds per judgment.tences, including 10 bad reference pairs, and 10ask again pairs.Figure 3 illustrates these two types of pairs,presuming that over the course of one or moreHITs each worker has assessed multiple ask againpairs generating the distribution indicated by d1,and also multiple bad reference pairs, generatingthe distribution indicated by d2.
As an estimateof the reliability of each individual judge we ap-ply a t-test to compare ask again differences withbad reference differences, with the expectationthat for a conscientious worker the latter shouldbe larger than the former.
Since there is no guar-antee that the two distributions of d1 and d2 havethe same variance, we apply Welch?s adaptation ofthe Student t-test.The null hypothesis to be tested for each AMTworker is that the score difference for ask againpairs is not less than the score difference forbad reference pairs.
Lower p values mean morereliable workers; in the experiments that are re-ported shortly, we use p < 0.05 as a thresholdof reliability.
We also applied the non-parametricMann-Whitney test to the same data, for the pur-pose of comparison, since there is no guaranteethat d1 and d2 will be normally distributed for agiven assessor.The next section provides details of the experi-mental structure, and then describes the outcomesin terms of their effect on overall system rank-ings.
As a preliminary indication of Turker be-havior, Figure 4 summarizes some of the data thatwas obtained.
Each plotted point represents oneAMT worker who took part in our experiments,and the horizontal axis reflects their average per-judgment time (noting that this is an imprecisemeasurement, since they may have taken phonecalls or answered email while working through aHIT, or simply left the task idle to help obscurea lack of effort).
The vertical scale is the p valueobtained for that worker when the ask again distri-bution is compared to their bad reference distribu-tion, with a line at p = 0.05 indicating the upperlimit of the zone for which we are confident thatthey had a different overall response to ask againpairs than they did to bad reference pairs.
Note thesmall number of very fast, very inaccurate work-ers at the top left; we have no hesitation in call-ing them unconscientious (and declining to paythem for their completed HITs).
Note also the verysmall number of workers for which it was possi-ble to reliably distinguish their ask again behaviorfrom their bad reference behavior.6 ExperimentsHIT StructureA sample of 560 translations was selected atrandom from the WMT 2012 published sharedtask dataset for a range of language pairs, withsegments consisting of 70 translations, each as-signed to a total of eight distinct HITs.
The sen-tences were generated as image files, as recom-mended for judgment of translations (Callison-Burch, 2009).
Each HIT was presented to a workeras a set of 100 sentences including a total of 30quality control items, with only one sentence visi-ble on-screen at any given time.
Each quality con-trol item comprised a pair of corresponding trans-lations, widely separated within the HIT.
Threekinds of quality control pairs were used:?
ask again: system output and exact repeat;?
bad reference: system output and an alteredversion of it with noticeably lower fluency;and?
good reference: system output and the corre-sponding human produced reference transla-tion (as provided in the released WMT data).Each HIT consisted of 10 groups, each containing10 sentences: 7 ?normal?
translations, plus oneof each type of quality control translation drawn37from one of the other groups in the HIT in sucha way that 40?60 judgments would be completedbetween the elements of any quality-control pair.Consistency of Human JudgmentsUsing judgments collected on the continuous rat-ing scale, we first examine assessor consistencybased on Welch?s t-test and the non-parametricMann-Whitney U-test.
In order to examine the de-gree to which human assessors assign consistentscores, we compute mean values of d1 (Figure 3)when ask again pairs are given to the same judge,and across pairs of judges.
Three sets of resultsare shown: the raw unfiltered data; data filteredaccording to p < 0.05 according to the quality-control regime described in the previous sectionusing the Welch?s t-test; and data filtered usingthe Mann-Whitney U-test.
Table 1 shows that thet-test indicates that only 13.1% of assessors meetquality control hurdle, while a higher proportion,35.7%, of assessors are deemed acceptable.The stricter filter, Welch?s t-test, yields moreconsistent scores for same-judge repeat items: de-creases of 4.5 (mean) and 4.2 (sd) are observedwhen quality control is applied.
In addition, re-sults for Welch?s t-test show high levels of con-sistency for same-judge repeat items: an averagedifference of only 9.5 is observed, which is notunreasonable, given that the scale is 100 pointsin length and a 10-point difference corresponds tojust 60 pixels on the screen.For repeat items rated by distinct judges, bothfiltering methods decrease the mean difference inscores compared to the unfiltered baseline, withthe two tests giving similar improvements.When an interval-level scale is used to evaluatethe data, the Kappa coefficient is commonly usedto evaluate consistency levels of human judges(Callison-Burch et al 2007), where Pr(a) is therelative observed agreement among raters, andPr(e) is the hypothetical probability of chanceagreement:?
=Pr(a)?
Pr(e)1?
Pr(e)In order to use the Kappa coefficient to compareagreement levels for the interval-level and contin-uous scales, we convert continuous scale scores toa target number of interval categories.
We do thisprimarily for a target number of five, as this bestprovides a comparison between scores for the 5-point interval-level scale.
But we also present re-sults for targets of four and two categories, sincethe continuous scale is marked at the midway andquarter points, providing implicit intervals.
A two-category is also interesting if the assessment pro-cess is regarded as dichotomizing to only includefor each translation whether or not the judge con-sidered it to be ?good?
or ?bad?.
Use of statisti-cal difference of means tests on interval-level datais not recommended; but for the purpose of illus-tration, we also applied Welch?s t-test to qualitycontrol workers that completed the interval-levelHITs, with the same threshold of p < 0.05.Tables 2 and 3 show intra-annotator agreementfor the five-point interval scale and continuousscales, with and without quality control.2 Resultsfor repeat items on the interval-level scale showthat quality control only alters intra-aa marginally(Pr(a) increases by 1%), and that inter-aa levelsworsen (Pr(a) decreases by 6.2%).
This confirmsthat applying statistical tests to interval-level datais not a suitable way of filtering out low qualityworkers.When comparing consistency levels of asses-sors using the interval-level scale to those of thecontinuous scale, we observe marginally lower ?coefficients for both intra-aa (?0.009) and inter-aa (?0.041) for the continuous scale.
However,this is likely to be in part due to the fact that thecontinuous scale corresponds more intuitively to 4categories, and agreement levels for the unfiltered4-category continuous scale are higher than thosecollected on the interval-level scale by +0.023intra-aa and +0.014 inter-aa.Applying quality-control on the continuousscale results in dramatic increases in intra-aa lev-els: +0.152 for 5-categories (5-cat), +0.100 for4-categories (4-cat) and +0.096 for 2-categories(2-cat).
When considering inter-aa levels, quality-control does not directly result in as dramatic anincrease, as inter-aa levels increase by +0.010for 5-cat, +0.006 for 4-cat and +0.004 for 2-cat.It is likely, however, that apparent disagreementbetween assessors might be due to different as-sessors judging fluency generally worse or betterthan one another.
The continuous scale allows forscores to be standardized by normalizing scoreswith respect to the mean and standard deviationof all scores assigned by a given individual judge.We therefore transform scores of each judge into2Note that the mapping from continuous scores to cate-gories was not applied for quality control.38same judge distinct judgesworkers judgments mean sd mean sdUnfiltered 100.0% 100.0% 14.0 18.4 28.9 23.5Welch?s t-test 13.1% 23.5% 9.5 14.2 25.2 21.0Mann-Whitney U-test 35.7% 48.8% 13.1 17.7 25.0 22.6Table 1: Mean and standard deviation of score differences for continuous scale with ask again itemswithin a given judge and across two distinct judges, for no quality control (unfiltered), Welch?s t-test andMann-Whitney U-test with a quality-control threshold of p < 0.05.# 5-pt.
interval 5-pt.
interval continuous continuouscateg- unfiltered filtered unfiltered filteredories Pr(a) ?
Pr(a) ?
Pr(a) ?
Pr(a) ?5 60.4% 0.505 61.4% 0.517 59.7% 0.496 71.8% 0.6474 - - - - 64.6% 0.528 72.1% 0.6292 - - - - 85.2% 0.704 90.0% 0.800Table 2: Intra-annotator (same judge) agreement levels for 5-point interval and continuous scales forunfiltered judgments and judgments of workers with p < 0.05 for Welch?s t-test.corresponding z-scores and use percentiles of thecombined set of all scores to map z-scores to cat-egories where a score falling in the bottom 20 thpercentile corresponds to strongly disagree, scoresbetween the 20 th and 40 th percentile to disagree,and so on.
Although this method of transformationis somewhat harsh on the continuous scale, sincescores no longer correspond to different locationson the original scale, it nevertheless shows an in-crease in consistency of +0.05 (5-cat), +0.086 (4-cat) and +0.144 (2-cat).
However, caution mustbe taken when interpreting consistency for stan-dardized scores, as can be seen from the increasein agreement observed when unfiltered scores arestandardized.Table 4 shows a breakdown by target languageof the proportion of judgments collected whosescores met the significance threshold of p < 0.05.Results appear at first to have shockingly low lev-els of high quality work, especially for English andGerman.
When running the tasks in MechanicalTurk, it is worth noting that we did not adopt statis-tical tests to automatically accept/reject HITs andwe believe this would be rather harsh on workers.Our method of quality control is a high bar to reachand it is likely that many workers that do not meetthe significance threshold would still have beenworking in good faith.
In practice, we individuallyexamined mean scores for reference translation,system outputs and bad reference pairs, and onlydeclined payment when there was no doubt the re-English German French Spanish10.0% 0% 57.9% 62.5%Table 4: High quality judgments, by language.sponse was either automatic or extremely careless.The structure of the task and the fact that thequality-control items were somewhat hidden mayhave lulled workers into a false sense of compla-cency, and perhaps encouraged careless responses.However, even taking this into consideration, thefact that none of the German speaking asses-sors and just 10% of English speaking assessorsreached our standards serves to highlight the im-portance of good quality-control techniques whenemploying services like AMT.
In addition, the riskof getting low quality work for some languagesmight be more risky than for others.
The responserate for high quality work for Spanish and Frenchwas so much higher than German and English,perhaps by chance, or perhaps the result of factorsthat will be revealed in future experimentation.System RankingsAs an example of the degree to which systemrankings are affected by applying quality control,for the language direction for which we achievedthe highest number of high quality assessments,English-to-Spanish, we include system rankingsby mean score with each measurement scale, withand without quality control and for mean z-scores39# 5-pt.
interval 5-pt.
interval continuous continuous cont.
standrdzed.
cont.
standrdzed.categ- unfiltered qual.-controlled unfiltered qual.-controlled unfiltered qual.-controlledories Pr(a) ?
Pr(a) ?
Pr(a) ?
Pr(a) ?
Pr(a) ?
Pr(a) ?5 33.0% 0.16 26.8% 0.084 29.5% 0.119 30.3% 0.128 30.2% 0.1272 33.5% 0.1694 - - - - 38.1% 0.174 38.5% 0.180 35.5% 0.1403 44.5% 0.2602 - - - - 66.5% 0.331 66.8% 0.335 75.5% 0.5097 73.8% 0.475Table 3: Inter-annotator (distinct judge) agreement levels for 5-point interval and continuous scales forunfiltered judgments and judgments of workers with p < 0.05 for Welch?s t-test.z-scores5-pt.
5-pt.
continuous continuous continuousunfiltered qual.-controlled unfiltered qual.-controlled qual.-controlledSys A 2.00 Sys A 2.00 Sys E 69.60 Sys E 74.39 Sys E 0.43Sys B 1.98 Sys D 1.97 Sys B 61.78 Sys F 65.07 Sys B 0.16Sys C 1.98 Sys F 1.95 Sys G 60.21 Sys G 64.51 Sys G 0.08Sys D 1.98 Sys C 1.95 Sys F 59.38 Sys B 63.68 Sys D 0.06Sys E 1.98 Sys E 1.95 Sys D 59.05 Sys D 63.52 Sys C 0.02Sys F 1.97 Sys B 1.94 Sys A 57.44 Sys C 61.33 Sys F 0.01Sys G 1.97 Sys G 1.93 Sys I 56.31 Sys A 58.43 Sys H ?0.03Sys H 1.96 Sys H 1.90 Sys C 55.82 Sys I 57.46 Sys I ?0.07Sys I 1.96 Sys I 1.88 Sys H 55.27 Sys H 57.04 Sys A ?0.10Sys J 1.94 Sys J 1.81 Sys J 50.46 Sys J 50.73 Sys J ?0.23Sys K 1.90 Sys K 1.76 Sys K 44.62 Sys K 41.25 Sys K ?0.47Table 5: WMT system rankings based on approximately 80 randomly-selected fluency judgments persystem, with and without quality control for radio button and continuous input types, based on German-English.
The quality control method applied is annotators who score worsened system output and gen-uine system outputs with statistically significant lower scores according to paired Student?s t-test.when raw scores are normalized by individual as-sessor mean and standard deviation.
The resultsare shown in Table 5.
(Note that we do not claimthat these rankings are indicative of actual systemrankings, as only fluency of translations was as-sessed, using an average of just 55 translations persystem.
)When comparing system rankings for unfilteredversus quality-controlled continuous scales, firstlythe overall difference in ranking is not as dramaticas one might expect, as many systems retain thesame rank order, with only a small number of sys-tems changing position.
This happens becauserandom-clickers cannot systematically favor anysystem, and positive and negative random scorestend to cancel each other out.
However, even hav-ing two systems ordered incorrectly is of concern;careful quality control, and the use of normaliza-tion of assessors?
scores may lead to more consis-tent outcomes.
We also note that incorrect systemorderings may lead to flow-on effects for evalua-tion of automatic metrics.The system rankings in Table 5 also show howthe use of the continuous scale can be used to ranksystems according to z-scores, so that individualassessor preferences over judgments can be ame-liorated.
Interestingly, the system that scores clos-est to the mean, Sys F, corresponds to the baselinesystem for the shared task with a z-score of 0.01.7 ConclusionWe have compared human assessor consistencylevels for judgments collected on a five-pointinterval-level scale to those collected on a contin-uous scale, using machine translation fluency asa test case.
We described a method for quality-controlling crowd-sourced annotations that resultsin marked increases in intra-annotator consistencyand does not require judges to agree with experts.In addition, the use of a continuous scale allowsscores to be standardized to eliminate individualjudge preferences, resulting in higher levels ofinter-annotator consistency.40AcknowledgmentsThis work was funded by the Australian ResearchCouncil.
Ondr?ej Bojar, Rosa Gog, Simon Gog,Florian Hanke, Maika Vincente Navarro, PavelPecina, and Djame Seddah provided translationsof task instructions, and feedback on publishedHITs.ReferencesA.
Alexandrov.
2010.
Characteristics of single-itemmeasures in Likert scale format.
The ElectronicJournal of Business Research Methods, 8:1?12.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz, andJ.
Schroeder.
2007.
(Meta-) evaluation of machinetranslation.
In Proc.
2nd Wkshp.
Statistical MachineTranslation, pages 136?158, Prague, Czech Repub-lic.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz,and J. Schroeder.
2008.
Further meta-evaluation ofmachine translation.
In Proc.
3rd Wkshp.
Statisti-cal Machine Translation, pages 70?106, Columbus,Ohio.C.
Callison-Burch, P. Koehn, C. Monz, andJ.
Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProc.
4th Wkshp.
Statistical Machine Translation,pages 1?28, Athens, Greece.C.
Callison-Burch, P. Koehn, C. Monz, K. Peterson,M.
Przybocki, and O. Zaidan.
2010.
Findings of the2010 Joint Workshop on Statistical Machine Trans-lation and Metrics for Machine Translation.
In Proc.5th Wkshp.
Statistical Machine Translation, pages17?53, Uppsala, Sweden.C.
Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.2011.
Findings of the 2011 Workshop on StatisticalMachine Translation.
In Proc.
6th Wkshp.
Statisti-cal Machine Translation, pages 22?64, Edinburgh,Scotland.C.
Callison-Burch, P. Koehn, C. Monz, M. Post,R.
Soricut, and L. Specia.
2012.
Findings of the2012 Workshop on Statistical Machine Translation.In Proc.
7th Wkshp.
Statistical Machine Translation,pages 10?51, Montreal, Canada.C.
Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using Amazon?s Me-chanical Turk.
In Proc.
Conf.
Empirical Methods inNatural Language Processing, pages 286?295, Sin-gapore.M.
Denkowski and A. Lavie.
2010.
Choosing the rightevaluation for machine translation: An examinationof annotator and automatic metric performance onhuman judgement tasks.
In Proc.
9th Conf.
Assoc.Machine Translation in the Americas (AMTA), Den-ver, Colorado.K.
Fort, G. Adda, and K. B. Cohen.
2011.
AmazonMechanical Turk: Gold mine or coal mine?
Com-putational Linguistics, 37(2):413?420.E.
Gibson, S. Piantadosi, and K. Fedorenko.
2011.
Us-ing Mechanical Turk to obtain and analyze Englishacceptability judgments.
Language and LinguisticsCompass, 5/8:509?524.Y.
Graham, T. Baldwin, A. Harwood, A. Moffat, andJ.
Zobel.
2012.
Measurement of progress in ma-chine translation.
In Proc.
Australasian LanguageTechnology Wkshp., pages 70?78, Dunedin, NewZealand.LDC.
2005.
Linguistic data annotation specification:Assessment of fluency and adequacy in translations.Technical report, Linguistic Data Consortium.
Re-vision 1.5.R.
A. Seymour, J. M. Simpson, J. E. Charlton, andM.
E. Phillips.
1985.
An evaluation of length andend-phrase of visiual analogue scales in dental pain.Pain, 21:177?185.R.
Snow, B. O?Connor, D. Jursfsky, and A. Y. Ng.2008.
Cheap and fast ?
but is it good?
Evalu-ating non-expert annotations for natural languagetasks.
In Proc.
Conf.
Empirical Methods in Natu-ral Language Processing, pages 254?263, Honolulu,Hawaii.D.
L. Streiner and G. R. Norman.
1989.
Health Mea-surement Scales: A Practical Guide to their Devel-opment and Use.
Oxford University Press, fourthedition.41
