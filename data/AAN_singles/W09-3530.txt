Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 132?135,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPName Transliteration with Bidirectional Perceptron Edit ModelsDayne FreitagSRI Internationalfreitag@ai.sri.comZhiqiang (John) WangSRI Internationaljohnwang@ai.sri.comAbstractWe report on our efforts as part of theshared task on the NEWS 2009 MachineTransliteration Shared Task.
We appliedan orthographic perceptron character editmodel that we have used previously forname transliteration, enhancing it in twoways: by ranking possible transliterationsaccording to the sum of their scores ac-cording to two models, one trained to gen-erate left-to-right, and one right-to-left;and by constraining generated strings tobe consistent with character bigrams ob-served in the respective language?s train-ing data.
Our poor showing in the of-ficial evaluation was due to a bug inthe script used to produce competition-compliant output.
Subsequent evaluationshows that our approach yielded compara-tively strong performance on all alphabeticlanguage pairs we attempted.1 IntroductionWhile transliteration is a much simpler prob-lem than another linguistic transduction prob-lem, language translation, it is rarely trivial.
Atleast three phenomena complicate the automatictransliteration between two languages using dif-ferent scripts?differing phoneme sets, lossy or-thography, and non-alphabetic orthographies (e.g.,syllabaries).For most language pairs, these difficulties standin the way of a rule-based treatment of the prob-lem.
For this reason, many machine learning ap-proaches to the problem have been proposed.
Wecan draw a rough distinction between learning ap-proaches that attempt to model the phonetics ofa transliteration problem explicitly, and those thattreat the problem as simply one of orthographictransduction, leaving it to the learning algorithmto acquire phonetic distinctions directly from or-thographic features of the training data.
For exam-ple, Knight and Graehl (1998) address the prob-lem through cascaded finite state transducers, withexplicit representations of the phonetics.
Sub-sequently, Al-Onaizan and Knight (2002) realizeimprovements by adding a ?spelling?
(i.e., ortho-graphic) model.
There has been an increasing em-phasis on purely orthographic models, probablybecause they require less detailed domain knowl-edge (e.g., (Lee and Chang, 2003)).2 ApproachThe approach we explored as part of the NEWS2009 Machine Transliteration Shared Task (Li etal., 2009) is strictly orthographic.
We view theconversion of a name in one language to its rep-resentation in another as the product of a seriesof single-character edits, and seek to learn a char-acter edit model that maximizes the score of cor-rect name pairs.
Our approach follows that de-scribed in Freitag and Khadivi (2007), a ?struc-tured perceptron?
with cheaply computed charac-ter n-grams as features.
Here, we give a briefdescription, and present the successful enhance-ments we tried specifically for the shared task.2.1 Perceptron Edit ModelSuppose we are given two sequences, sm1 ?
?
?sand tn1 ?
?
?t .
We desire a function A(s, t) 7?
Nwhich assigns high scores to correct pairs s, t. Ifwe stipulate that this score is the sum of the indi-vidual scores of a series of edits, we can find thehighest-scoring such series through a generaliza-tion of the standard edit distance:A(si1, tj1) =max????
?a?,tj(s, i, t, j) + A(si1, tj?11 )asi,?
(s, i, t, j) + A(si?11 , tj1)asi,tj(s, i, t, j) + A(si?11 , tj?11 )(1)132with A(?, ?)
= 0.
The function asi,tj (s, i, t, j)represents the score of substituting tj for si; a?,tjand asi,?
represent insertion and deletion, respec-tively.In the experiments reported in this paper, we as-sume that each local function a is defined in termsof p + q features, {f1, ?
?
?
, fp, fp+1, ?
?
?
, fp+q},defined over the source and target alhabets, re-spectively, and that these features have the func-tional form ??
?N 7?
R.In this paper we exclusively use character n-gram indicator features.
The ?order?
of a modelis the size of the largest n-grams; for a model oforder 2, features would be the bigrams and uni-grams immediately adjacent to a given string posi-tion.
Since the shared task is to generate targetstrings, only features for preceding n-grams areused in the target language.The score of a particular edit is a linear combi-nation of the corresponding feature values:a(s, i, t, j) =p?k=1?k ?fk(s, i)+p+q?k=p+1?k ?fk(t, j)(2)The weights ?k are what we seek to optimize inorder to tune the model for our particular applica-tion.We optimize these weights through an exten-sion of perceptron training for sequence labeling,due to Collins (2002).
Take ?
to be a model pa-rameterization, and let A?
(s, t) return an optimaledit sequence e, with its score v, given input se-quences s and t under ?.
Elements of sequencee are character pairs ?cs, ct?, with cs ?
?s ?
{?
}and ct ?
?t ?
{?
}, where ?
represents the emptystring.
Let ?
(s, t,e) be a feature vector for asource string, target string, and corresponding editsequence.Table 1 shows the training algorithm.
Startingwith a zero parameter vector, we iterate throughthe collection of source sequences.
For each se-quence, we pick two target sequences, one the?true?
transliteration t of the source string s, andone chosen by searching for a string t?
that yields amaximal score according to the current model A?
(Line 6).
If the model fails to assign t a higherscore than t?
(Line 9), we apply the perceptrontraining update (Line 10).Note that because generation constructs the tar-get sequence, the search in Line 6 for a targetstring t?
that yields maximal A?
(s, t?)
is not triv-ial, and does not correspond to a simple recurrence1: Given training set S = {?s, t?
}2: V ?
[], an empty list3: ??
0, a weight vector4: for some number of iterations do5: for ?s, t?
in S do6: t?
?
maxargt?A?
(s, t?
)7: ?e, v?
?
A?
(s, t)8: ?e?, v??
?
A?
(s, t?
)9: if v?
?
v then10: ??
?+?
(s, t,e)??
(s, t?,e?
)11: end if12: Append ?
to V13: end for14: end for15: Return the mean ?
from VTable 1: The training algorithm.
A?
is the affinityfunction under model parameters ?, returning editsequence e and score v.relation like Equation 1.
Both in training and test-ing, we use a beam search for target string gener-ation.
In training, this may mean that we find a t?with lower score than the correct target t. In suchcases (Line 9 returns false), the model has cor-rectly ordered the two alternative transliterations,and does not require updating.2.2 Shared Task ExtensionsThis approach has been used effectively for prac-tical transliteration of names from English to Ara-bic and Mandarin (and vice versa).
As part ofthe NEWS shared task, we experimented with twosimple extensions, both of which yielded improve-ments over the baseline described above.
Theseextensions were used in our official submission foralphabetic language pairs.
We treated English-to-Chinese somewhat differently, as described below.Simple character n-gram constraints.
Thedescribed approach sometimes violates targetlanguage spelling conventions by interpolatingclearly inappropriate characters into a string thatis otherwise a reasonable transliteration.
We takethis behavior as symptomatic of a kind of under-training in some portion of the problem space,a possible byproduct of 1-best perceptron train-ing.
One principled solution may be to optimizeagainst n-best lists (Bellare et al, 2009).Instead, we address this shortcoming in astraightforward way?by prohibiting the creationof n-grams, for some small n, that do not occur133in the training data.
Under a bigram restriction, if?ab?
is not seen in training, then an operation thatinserts ?b?
after ?a?
is disallowed.
In essence, weimpose a very simple character language model ofthe target domain.Our non-standard English-to-Chinese contribu-tions, which involved transliterating from Englishto pinyin, employed a similar idea.
In these exper-iments, rather than character bigrams, the modelwas constrained to produce only legal pinyin se-quences.Bidirectional generation.
Character n-gramrestrictions yielded modest but universal improve-ments on development data.
Larger improvementswere obtained through an equally simple idea: In-stead of a single left-to-right model, we trainedtwo models, one generating left-to-right, the otherright-to-left, each model constrained by n-gram ta-bles, as described above.
At evaluation time, eachof the constituent models was used to generate alarge number of candidate strings (100, typically).All strings in the union of these two sets were thenassigned a score, which was the unweighted sumof scores according to the constituent models, andreranked according to this score.
The 10 highest-scoring were retained for evaluation.A buggy implementation of this two-model ideaaccounts for our poor showing in the official eval-uation.
Because of a trivial error in the script weused to produce output, right-to-left models weretreated as if they were left-to-right.
The resultingstrings and scores were consequently erroneous.3 EvaluationWe experimented with models of order 2 and3 (2-gram and 3-gram features) on shared taskdata for English to Hindi, Kannada, Russian, andTamil (Kumaran and Kellner, 2007).
Based ondevelopment accuracy scores, we found modelsof order 3 to be consistently better than order 2,and our submitted results use only order-3 models,with one exception.
English-to-native-Chinese (Liet al, 2004) was treated as a special case.
Usingtrigram features in the target language results inan explosion in the feature space, and a model thatis slow to train and performs poorly.
Thus, onlyfor this language pair, we devised a mixed-ordermodel, one using trigram features over Englishstrings, and unigram features over Chinese.
Be-cause of the large target-language branching fac-tor, the mixed-order native Chinese model remain-Languages Accuracy DeltaEnHi 0.465 -0.033EnHi baseline 0.421 -0.077EnKa 0.396 -0.002EnKa baseline 0.370 -0.028EnRu 0.609 -0.004EnRu baseline 0.588 -0.025EnTa 0.475 +0.001EnTa baseline 0.422 -0.052EnCh standard 0.672 -0.059EnCh non-standard 1 0.673 -0.236EnCh non-standard 2 0.5 -0.409Table 2: Post-contest accuracy on evaluation set,including delta from highest-scoring contest par-ticipant.ing one of the slowest to train.We trained all models for 20 epochs, evaluatingthe 1-best accuracy of intervening models on thedevelopment data.
In all cases, we observed thataccuracy increased steadily for some number of it-erations, after which it plateaued.
Consequently,for all language pairs, we submitted the predic-tions of the latest model.Table 2 lists accuracy reported by the officialevaluation script on the contest evaluation data.All non-Chinese runs in the table are ?standard,?and are trained exclusively on shared task train-ing data.
Those labeled ?baseline?
are left-to-right models with no character n-gram constraints.These results were obtained after release of theevaluation data, but differ from our official sub-mission in only two ways: First, and most im-portantly, the bug described previously was cor-rected.
Second, in some cases training runs thathad not completed at evaluation time were allowedto run to the full 20 epochs, and the resulting mod-els were used.
The exceptions are Hindi and na-tive Chinese, each of which reflect performanceat approximately 10 epochs.
Without exception, abeam of size 100 was used to generate these re-sults.With the exception of ?EnCh standard,?
allresults in the table employed the bidirectionalscheme described above.
The Chinese non-standard runs differ from standard only in thatmodels were trained to perform English-to-pinyintransliteration, followed by a conversion frompinyin to native Chinese using tables provided by134the Unicode consortium.
Non-standard Run 1 re-tains pinyin tonal diacritics, while Run 2 omitsthem.
The mapping from pinyin to native Chi-nese characters introduces indeterminacy, whichwe accounted for in a simple fashion: First, in con-structing a pinyin-to-Chinese conversion table, wediscarded any Chinese characters that were usedin the training data fewer than some small frac-tion of cases.
Then, given a ranked list of pinyintransliterations, we generated all possible nativeChinese sequences, ranked by the product of ob-served pinyin-to-Chinese probabilities, accordingto training frequencies.It will be observed that our non-standardEnglish-to-Chinese results lag considerably be-hind the best results.
We suspect this is due inpart to the fact that no additional training data wasused in these experiments?only a change in repre-sentation.4 Discussion and ConclusionTreating the transliteration problem as one of or-thographic transduction appears viable, particu-larly for conversion between alphabetic languages.An empirical character edit model based on astructured perceptron and character n-gram fea-tures, and using a simple training procedure thatdiscovers appropriate weights for latent characteralignment operations, yields performance that isgenerally as good as alternative approaches ex-plored in the shared task.
The key is model com-bination, particularly the combination of left-to-right and right-to-left models, respectively.In contrast to the alphabetic language pairs,our performance on Chinese falls somewhat short.Nevertheless, it is possible that simple modifica-tions of the basic procedure would render it com-petitive on English-Chinese, as well.
In convert-ing from English to native Chinese, we relied ona mixed-order model, with order-3 English fea-tures.
It is possible that trigrams are too smallin some cases to identify the appropriate Chinesecharacter, and that 4-grams, if we can afford them,will make the difference.
There is virtue in theidea of transliterating to pinyin as an intermediatestep; converting to tonal pinyin yields accuracy atthe same level as English-to-native-Chinese, evenwith the indeterminacy it introduces.
Future workincludes more principled approaches to resolvingthis indeterminacy, and combined pinyin/nativemodels.AcknowledgmentsThis material is based upon work supported bythe Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-06-C-0023 (approved for public release, distribution un-limited).
Any opinions, findings and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthe view of DARPA.ReferencesY.
Al-Onaizan and K. Knight.
2002.
Machine translit-eration of names in Arabic text.
In Proceedings ofthe ACL-02 workshop on computational approachesto semitic languages.K.
Bellare, K. Crammer, and D. Freitag.
2009.
Loss-sensitive discriminative training of machine translit-eration models.
In Proceedings of the StudentResearch Workshop and Doctoral Consortium atNLT/NAACL 2009.M.
Collins.
2002.
Discriminative training meth-ods for hidden Markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof EMNLP-2002.D.
Freitag and S. Khadivi.
2007.
A sequence align-ment model based on the averaged perceptron.
InProceedings of EMNLP-CoNLL 2007.K.
Knight and J. Graehl.
1998.
Machine translitera-tion.
Computational Linguistics, 24(4).A.
Kumaran and T. Kellner.
2007.
A generic frame-work for machine transliteration.
In Proceedings ofthe 30th SIGIR.C.-J.
Lee and J.S.
Chang.
2003.
Acquisitionof English-Chinese transliterated word pairs fromparallel-aligned texts using a statistical machinetransliteration model.
In Proceedings of the HLT-NAACL 2003 Workshop on Building and Using Par-allel Texts.H.
Li, M. Zhang, and J. Su.
2004.
A joint source chan-nel model for machine transliteration.
In Proceed-ings of the 42nd ACL.H.
Li, A. Kumaran, M. Zhang, and V. Pervouch-ine.
2009.
Whitepaper of NEWS 2009 MachineTransliteration Shared Task.
In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS2009).135
