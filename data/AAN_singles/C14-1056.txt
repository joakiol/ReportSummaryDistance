Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 588?599, Dublin, Ireland, August 23-29 2014.Enforcing Topic Diversity in a Document Recommender for ConversationsMaryam HabibiIdiap Research Institute and EPFLRue Marconi 19, CP 5921920 Martigny, Switzerlandmaryam.habibi@idiap.chAndrei Popescu-BelisIdiap Research InstituteRue Marconi 19, CP 5921920 Martigny, Switzerlandandrei.popescu-belis@idiap.chAbstractThis paper addresses the problem of building concise, diverse and relevant lists of documents,which can be recommended to the participants of a conversation to fulfill their information needswithout distracting them.
These lists are retrieved periodically by submitting multiple implicitqueries derived from the pronounced words.
Each query is related to one of the topics identifiedin the conversation fragment preceding the recommendation, and is submitted to a search engineover the English Wikipedia.
We propose in this paper an algorithm for diverse merging of theselists, using a submodular reward function that rewards the topical similarity of documents tothe conversation words as well as their diversity.
We evaluate the proposed method throughcrowdsourcing.
The results show the superiority of the diverse merging technique over severalothers which not enforce the diversity of topics.1 IntroductionWe present a diverse retrieval technique for ranking documents that are spontaneously retrieved andrecommended to people during a conversation.
These documents represent potentially useful informationfor the conversation participants.
The information needs of the participants are represented by implicitqueries which are built in the background based on their current speech, specifically from keywordsobtained from the conversation transcripts.
Since people usually mention several topics even during ashort conversation span, such keyword sets are made of content words related to different topics.
Whenjuxtaposed in an implicit query, these topics may have noisy effects on the retrieval results (Bhogal et al.,2007; Carpineto and Romano, 2012).The purpose of this paper is to present a method for merging lists of documents retrieved throughmultiple implicit queries prepared for short conversations spans.
Several topically-separated queries areconstructed from keywords, and generate several lists of documents.
The goal of the method proposedhere is to generate a unique and concise list of documents that can be recommended in real time to theconversation participants.
The list should cover the maximum number of implicit queries and thereforetopics.
To merge the lists of documents according to these criteria, we use inspiration from extractivetext summarization (Lin and Bilmes, 2011; Li et al., 2012) and from our own previous work on diversekeyword extraction (Habibi and Popescu-Belis, 2013).
The method proposed here rewards at the sametime topic similarity ?
to select the most relevant documents to the conversation fragment ?
and topicdiversity ?
to cover the maximum number of implicit queries and therefore topics in a concise andrelevant list of recommendations, if more than one topic is discussed in the conversation fragment.Several studies have been previously carried out on merging lists of results in information retrieval.Despite the superficial similarity, the problem here is in fact different from distributed information re-trieval, where several lists of results from different search engines for the same query must be merged.Moreover, many studies addressed the topic diversification approach for re-ranking the retrieved resultsof a single query.
However, these approaches are not directly applicable to multiple queries.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/588The paper is organized as follows.
In Section 2 we review existing techniques for merging and re-ranking lists of search results which are applicable here.
We then explain the general framework of ourdocument recommender system in Section 3.
In Section 4 we describe the proposed algorithm for diversemerging of lists of recommendations.
Section 5 presents the data, the parameters setting, and evaluationtasks for comparing document lists.
In Section 6 we first demonstrate empirically the benefits, for just-in-time document recommendation, of separating users?
information needs into multiple topically-separatedqueries rather than using a unique query.
Then, we compare the proposed diverse merging technique withseveral alternative ones, showing that it outperforms them according to human judgments of relevance,and also exemplify the results on one conversation fragment given in the Appendix A.2 Related WorkJust-in-time document retrieval systems have been designed to recommend to their users documentswhich are potentially relevant to their activities, e.g.
individual users authoring documents or browsingvarious repositories, or small groups holding business or private meetings (Hart and Graham, 1997;Rhodes and Maes, 2000; Popescu-Belis et al., 2008).
When using a document recommender system,people are generally unwilling to examine a large number of recommended documents, mainly becausethis would distract them from their main activity.
Several solutions to this problem have been proposed.For instance, the Watson document recommender system (Budzik and Hammond, 2000), designedfor reading or writing activities, clustered the document results and selected from each cluster the bestrepresentative to generate a list of recommendations.
Clustering results is not suitable for our applicationwhere the mixture of topics in a single query will degrade the document results aimed to be clustered(Bhogal et al., 2007; Carpineto and Romano, 2012), and consequently may have a damaging effect onthe clusters?
representatives.
The second part of the method, which selected the best representative ofthe clusters in the final document list can be helpful; however, its effectiveness relies on having clusterswith the same level of importance (Wu and McClean, 2007).Many studies in information retrieval addressed the problem of diverse ranking, which can be stated asa tradeoff between finding relevant versus diverse information (Robertson, 1997).
The existing diverseranking proposals differ in their diversifying policies and definitions, which can be categorized into im-plicit methods (Carbonell and Goldstein, 1998; Zhai et al., 2003; Radlinski and Dumais, 2006; Wangand Zhu, 2009) or explicit ones (Agrawal et al., 2009; Carterette and Chandar, 2009; Santos et al., 2010;Vargas et al., 2012).
The implicit approaches assume that similar documents will cover similar aspectsof a query, and have to be demoted in the ranking to promote relative novelty and reduce overall redun-dancy.
In one of the earliest approaches, Carbonell and Goldstein (1998) introduced Maximal MarginalRelevance (MMR) to re-rank documents based on a tradeoff between the relevance of document resultsand relative novelty as a measure of diversity.
MMR was also used by Radlinski and Dumais (2006) tore-rank results from a query set which is generated for a user query and represents a variety of potentialuser intents.Instead of implicitly accounting for the aspects covered by each document, another option is to ex-plicitly model these aspects within the diversification approach.
Agrawal et al.
(2009) introduced asubmodular objective function to minimize the probability of average user dissatisfaction by assuminga taxonomy of information and modeling user query aspects at the topical level of this taxonomy.
Al-ternatively, Santos et al.
(2010) proposed another submodular objective function to maximize coverageand minimize redundancy with respect to query aspects modeled in a keyword-based representation forminstead of a predefined taxonomy.In our case, the recommender system for conversational environments requires diversity in the resultsof multiple topically-separated queries, rather than of a single ambiguous query.
Therefore, a new ap-proach will be proposed, and will be compared in particular to a version of the explicit diversificationapproach (Santos et al., 2010) adapted to our problem.3 Framework of our Document Recommender SystemWe have designed the Automatic Content Linking Device (ACLD), a speech-based just-in-time documentrecommender system for business meetings (Popescu-Belis et al., 2008; Popescu-Belis et al., 2011).589Transcript of conversation fragmentExtract the best k keywords that cover all themain topics with high probabilityTopical clustering of keywords to prepare Mmultiple topic-aware queries},...,{1 kccC Retrieval system},...,{},,...,{11 MM wwWqqQ Retrieval system},...,{1 MllL  lDiverse ranking(DivS)Similarity merging(SimM)Round-robinmergingDiverse merging(DivM)},...,{1 NddS },...,{ 1 NddS },...,{ 1 NddS },...,{ 1 NddS list of relevant documents,(1)(2)(3)(4)Figure 1: The four stages of our document recommendation approach (shown vertically: 1?4) and thefour options considered in this paper (bottom line: SimM, Round-robin, DivM, and DivS).The ACLD monitors the ongoing conversation, and formulates queries based on the words detected bya real-time automatic speech recognition (ASR) system (Garner et al., 2009).
The queries are firedperiodically to retrieve documents which are then recommended to users by displaying their titles alongwith relevant excerpts.
As these queries are built and triggered in the background, they are referred to as?implicit queries?, as opposed to ?explicit?
ones that could be formulated by users.
Just-in-time documentrecommendation in the ACLD system proceeds according to the steps shown in Figure 1, which displaysat step 4 the various options for merging lists of results that are the focus of this paper.Prior to the first processing step outlined in Figure 1, the ACLD must decide when to make a recom-mendation, and what portion of the conversation prior to that moment should be used.
This question isbeyond the scope of this paper, and remains to be fully investigated, using verbal and non-verbal criteria.Here, for the reasons explained in Section 5.2, the ACLD recommends documents every two minutes,segmenting the conversation at the end of the nearest utterance and using the entire conversation frag-ment since the previous recommendation.
Although in practice the results of the current recommendationprocess are merged with the previous ones (using a weighted mechanism that embodies the idea of ?per-sistence?
of documents over time), in this paper we will consider the recommendation for each fragmentindependently of the previous one.The recommendation process represented in Figure 1 starts by extracting a set of keywords, C, fromthe words recognized by the ASR system from the users?
conversations.
The keywords are extractedusing the diverse keyword extraction technique that we proposed (Habibi and Popescu-Belis, 2013),which maximizes the coverage of the topics of a text by the extracted keyword set, as we also target inthis paper.
Then, implicit queries which express the users?
information needs are formulated using thekeyword set, following two alternative approaches depicted in step 2 of Figure 1.
In a baseline model(right side of the figure), a single query is built for the conversation fragment using the entire keywordlist as an implicit query.
In the approach we are advocating, multiple topically-separated queries areproduced for the conversation fragment (step 2, left side of the figure).
This is described in a separatedocument (Habibi and Popescu-Belis, submitted), but can be outlined as follows.
The implicit queries areobtained by clustering the above-mentioned keyword set into several topically-separated subsets, eachone corresponding to an abstract topic obtained using topic modeling techniques (similarly to the model590presented in Subsection 4.1).
Each subset is an implicit query, and is weighted based on the importanceof the topic to which it is associated.In step 3, we separately submit each implicit query to the Apache Lucene search engine over theEnglish Wikipedia and obtain several lists of relevant articles.
Finally, we merge and re-rank theselists before recommendation (step 4).
One baseline alternative is the explicit diverse ranking techniqueproposed by Santos et al.
(2010) for diversifying the primary search results retrieved for a single query,shown on the right side of the figure.
To compare the methods, we adapted this latter method to makeit applicable to our system when a single implicit query is built for a conversation fragment, by definingquery aspects using the abstract topics employed for query and document representation.
The method isnoted DivS as it diversifies documents from a single list.Our proposal lies at step 4.
As represented on the left side of Figure 1, in our system, we merge the listsof documents retrieved for multiple implicit queries.
We thus propose a new method noted DivM and wecompare it with two other merging techniques.
The first one, noted SimM, ignores the diversity of topicsin the list of results and ranks documents only by considering their topic similarity to the conversationfragment.
The second one is the merging technique used by the above-mentioned Watson system (Budzikand Hammond, 2000), which uses Round robin merging, hence it is noted Round-robin.
In contrast, ourproposed method, DivM, is a diverse merging technique which we now proceed to define formally.4 Diverse Merging of the Results of Multiple QueriesThe diverse merging of retrieved document lists is the process of creating a short, diverse and relevant listof recommended documents which covers the maximum number of topics of each conversation fragment.The merging algorithm rewards diversity by decreasing the gain of selecting documents from a list asthe number of its previously selected documents increases.
The method proceeds in two steps.
First,we represent queries and the corresponding list of candidate documents from the Apache Lucene searchengine using topic modeling techniques, and then we rank documents by using topical similarity andrewarding the coverage of different lists.4.1 Document and Query RepresentationA topic model represents the abstract topics which occur in a collection of documents ?
here, preferably,a collection that is representative of the domain of the conversations.
Once trained, topic models suchas Probabilistic Latent Semantic Analysis (PLSA) or Latent Dirichlet Allocation (LDA) can be usedto determine the distribution of abstract topics in each set of words composing either a conversationfragment, or a query, or a document.
LDA implemented in the Mallet toolkit (McCallum, 2002) is usedhere to train topic models because it does not suffer from the over-fitting of PLSA (Blei et al., 2003).We first learn a probability model for observing a word v in a document d through the set of abstracttopics T = {t1, ..., tz, ..., tZ}, where Z is the number of topics, using the Mallet toolkit:p(v|d) =Z?z=1p(v|tz) ?
p(tz|d) (1)The topic-word distribution p(v|tz) and the document-topic distribution p(tz|d), which are obtainedusing topic modeling, respectively show the contribution of the word v in the construction of the topictz, and the distribution of topic tzin the document d with respect to the other topics.We represent each new text or fragment A (e.g.
from a conversation or document) by a set of proba-bility distributions over all abstract topics T noted as P (A) = {p(t1|A), ..., p(tz|A), ..., p(tZ|A)} wherep(tz|A) is inferred using the Gibbs sampling implemented by the Mallet toolkit given the topic modelspreviously learned.
We associate to each new document diand query qja set of topic probabilities ac-cording to the above definition noted respectively as P (di) = {p(t1|di), ..., p(tz|di), ..., p(tZ|di)} andP (qj) = {p(t1|qj), ..., p(tz|qj), ..., p(tZ|qj)}.4.2 Diverse Merging ProblemAs stated above, our goal is to recommend a short ranked list of documents answering the users?
informa-tion needs hypothesized in a conversation fragment, which are modeled by multiple topic-aware implicit591queries as described in Section 3.
We build the final list of recommended documents by merging thedocument lists, one from each implicit query, with the objective of the maximum coverage of the topicsof the conversation fragment.
Since each document list contains documents found by a search enginegiven an implicit query, which was prepared for one of the main topics of the conversation fragment,we merge the lists by selecting documents from the maximum number of lists in addition to maximizingtheir topical similarity to the conversation fragment.The problem of diverse merging of lists thus amounts to finding a ranked subset of documents S ?
?Mi=1li, which are the most representative of all the result lists li, and potentially the most informative withrespect to the conversation fragment and the information needs that are implicitly stated.
This problemis an instance of the maximum coverage problem, which is known to be NP-hard.
Our formulation andsolution proceed as follows.Let us consider a set of implicit queries Q = {q1, ..., qM}, and the corresponding set of documentlists L = {l1, ..., lM} resulting from each query.
M is the number of implicit queries of the fragment,and each liis a list of documents {d1, ..., dNi} which are retrieved for query qi.
We define the weightwiof each query qias the importance within the conversation fragment of the topics represented in thequery qi, and compute it as the topical similarity of qito the fragment, as shown in Equation 2.
In thisequation, q is the query made from the whole keyword set, which we call a collective query, and includeskeywords for all the main topics of the conversation fragment in one query.
In turn, we associate to q aset of probabilities over abstract topics, P (q) = {p(t1|q), ..., p(tZ|q)}, similar to the representation ofimplicit queries explained in Subsection 4.1.wi=Z?z=1p(tz|qi) ?
p(tz|q) (2)4.3 Defining a Diverse Reward FunctionAlthough the maximum coverage problem is NP-hard, it has been shown that a greedy algorithm canfind an approximate solution guaranteed to be within a factor of (1 ?
1/e) ' 0.63 of the optimal one ifthe coverage function is submodular and monotone non-decreasing1(Nemhauser et al., 1978).
Severalmonotone submodular functions have been proposed in various domains for a similar underlying prob-lem, such as explicit diverse re-ranking of retrieval results (Agrawal et al., 2009; Santos et al., 2010;Vargas et al., 2012), extractive summarization of a text (Lin and Bilmes, 2011; Li et al., 2012), or ourown model of diverse keyword extraction from a text (Habibi and Popescu-Belis, 2013).We define a monotone submodular function for diverse merging of document lists inspired by the lattertwo applications, who proposed a power function with a scaling exponent between 0 and 1 for diverseselection of sentences (or keywords) covering the maximum number of topics of a given document with afixed number of items.
To adapt these techniques to the problem of diverse merging, from the perspectiveof capturing users?
information needs in the set of recommended documents, we define here a rewardfunction enforcing the diverse merging of the lists of document results.We first estimate the topical similarity of the document subset Si= S ?
lito the collective query q(see Subsection 4.2) as rSi:rSi=?d?SiZ?z=1p(tz|d) ?
p(tz|q) (3)We then propose the following reward function f for each Sicontaining relevant documents selectedfrom li(results of implicit query qi), where wiis the topical similarity of qito the conversation fragment(see Equation 2), and ?
is an exponent parameter between 0 and 1.
This reward function is submodularbecause it has the diminishing returns property when rSiincreases.f : rSi?
wi?
r?Si(4)1A function F is submodular if ?A ?
B ?
T \ t, F (A + t) ?
F (A) ?
F (B + t) ?
F (B) (diminishing returns) and ismonotone non-decreasing if ?A ?
B, F (A) ?
F (B).592The set S is ultimately ranked by maximizing the cumulative reward function R(S) over all the lists,written as follows:R(S) =M?i=1wi?
r?Si(5)The probability of selecting documents from the list of results for qithus depends on wi, the topicalsimilarity of the query to the conversation fragment.
This is in contrast to choosing the best representativedocument from the list of documents relevant to each query, like in the Watson system, which does notselect more documents for queries with higher weight before considering lower weight ones.
Our modelrewards diversity to increase the chance of choosing documents from all the lists of results retrieved forimplicit queries.4.4 Finding the Optimal Document ListSince R(S) is a monotone submodular function, we propose a greedy algorithm (Alg.
1) to maximizeR(S).
If ?
= 1, the reward function ignores the diversity constraint, because it does not penalize multipleselections from the same list liand ranks documents only depending on their similarity to the collectivequery and on the weights of implicit queries.
However, when 0<?<1, as soon as a document is selectedfrom the list of results of an implicit query, other documents from the same list start having diminishingreturns as competitors for selection.
Decreasing the value of ?
increases the impact of the diversityconstraint on ranking documents, which augments the chance of recommending documents from otherdocument lists.Input : query set Q of size M with probabilities, set of weights W , set of lists of document resultsL with probabilities, number of recommended documents kOutput: set of recommended documents SS ?
?
;for i = 1 toM step 1 doSi?
?
;endwhile |S| ?
k doS ?
S ?
argmaxd?
((?Mi=1li)\S)(g(d)) where g(d) =?Mi=1wi?
[r{d}?li+ rSi]?
;for i = 1 toM step 1 doSi= li?
S;endendreturn S;Algorithm 1: Diverse merging of document results for recommendation.5 Data, Settings and Evaluation MethodThe experiments were performed on conversational data from the ELEA Corpus (Emergent LEader Anal-ysis, Sanchez-Cortes et al.
(2012)).
Implicit queries were formulated as presented above in Figure 1 usingkeywords extracted from each conversation fragment, defined as below (Subsection 5.1).
Each subsetof keywords obtained by topical clustering of the keyword set resulted in an implicit query.
The lists ofdocument results for each implicit query were obtained by submitting the query to the Apache Lucenesearch engine2over the English Wikipedia3.
These initial lists of results were ultimately merged intofinal recommendation lists of documents using the four alternative methods from Figure 1, including theone we proposed.
This section presents the data, system parameters, and evaluation methods used in ourexperiments.2Available from http://lucene.apache.org.3A local copy was downloaded from http://dumps.wikimedia.org.5935.1 Conversational CorpusThe ELEA Corpus comprises nearly ten hours of recorded meetings in English and French.
Each meetingconsists in a role play game in which participants play survivors of an airplane crash in a mountainousregion.
They must rank a list of 12 items with respect to their utility for surviving until they are rescued.We used from the ELEA corpus four English conversations of around fifteen minutes each, which havebeen manually transcribed and segmented at the speaker turn level.One of the most important issues for a just-in-time document recommender system is to determinethe appropriate timing of the recommendations, and the size of the context to use for computing them.Here, awaiting future investigations4, we decided to make recommendations approximately every twominutes, at the end of an ongoing speaker turn, and consider as input the words uttered since the previousrecommendation.
A segment size of two minutes enables us to collect an appropriate number of words(neither too small nor too large) in order to extract keywords, model the topics, and formulate implicitqueries.
Based on our experience with the ACLD, it also corresponds to an acceptable frequency forreceiving suggestions.Therefore, our test data comprises 26 two-minute segments, each of them ending at a speaker change.On average, segments contain 278 words (including stop words).
Once topic modeling is applied, theaverage number of topics per fragment is 5, with an observed minimum of 3 and a maximum of 9.5.2 Parameter Settings for ExperimentationAs document search is performed over the English Wikipedia, we trained our topic models on this corpusas well.
We used only a subset of it for tractability reasons, i.e.
about 125,000 articles as in other studies(Hoffman et al., 2010).
The subset is randomly selected from the entire English Wikipedia.
As inprevious studies, we fixed the number of topics at 100 (Boyd-Graber et al., 2009; Hoffman et al., 2010).The exponent of the submodular function was set to ?
= 0.75, as in our diverse keyword extractionstudy (Habibi and Popescu-Belis, 2013).
This was found to be the best value for diverse merging of listsof results, as it leads to a reasonable balance between relevance and diversity in the aggregated list ofdocuments.
Of course, if sufficient training data were available, this could be used to optimize ?.The number of recommended documents was fixed at five in our experiments.
This value was selectedagain based on user preferences observed with the ACLD.
Moreover, this is also the value of the averagenumber of topics in a conversation fragment, which allows the system to cover on average one result pertopic.
Experiments with other values were not carried out due to the cost of evaluation.5.3 Evaluation Protocol and MetricsWe designed a task that measures the relevance of recommended document lists for each of the testconversation fragment.
Based on validation experiments in our previous work (Habibi and Popescu-Belis, 2012), the task requires subjects to compare two lists obtained by two different methods.
Using aweb browser, the subjects had to read the conversation transcript, answer several control questions aboutits content, and then decide which of the two lists provides more relevant documents, with the followingoptions: the first list is better than the second one; the second is better than the first; both are equallyrelevant; or both are equally irrelevant.
The position of each system (first or second) was randomizedacross the tasks.The 26 comparison tasks (one for each ELEA fragment) were crowdsourced via Amazon?s MechanicalTurk as ?human intelligence tasks?
(HITs).
For each HIT we recruited ten workers, only acceptingthose with greater than 95% approval rate and more than 1000 previously approved HITs (qualificationcontrol).
We only kept answers from the workers who answered correctly our control questions abouteach HIT.
Each worker could answer the entire set of 26 HITs, or part of it.
We observed that the averagetime spent per HIT was around 90 seconds.4For instance, they could combine an analysis of non-verbal information to detect ?interruptibility?
and of verbal informationto detect topic changes and perform online segmentation (Mohri et al., 2010).
Topic changes, however, are not appropriatemoments to make recommendations because it would be useless to recommend documents about a topic that the users nolonger discuss (Jones and Brown, 2004).594To consolidate the comparative judgments over a large number of subjects and conversation fragments,and compute an aggregated score, we applied a qualification control factor to the human judgments (toreduce the effect of judgments which disagree with the majority vote) and another one to the HITs (toreduce the impact of undecided HITs on the global scores).
This was done by using the PCC-H metric,defined and validated in our previous work (Habibi and Popescu-Belis, 2012), which provides two scores,one for each document list, summing up to 100%; a higher value indicates a better list.
In addition toPCC-H, we also provide below (Table 1) the raw preference scores for each comparison, i.e.
the numberof times a system was preferred over another one, although PCC-H was shown to be a more reliableindicator of quality.6 Experimental ResultsWe merged and re-ranked the document lists intended to be recommended during a conversation by thefour methods presented above in Section 3 and Figure 1.
Three methods merge lists of results fromtopically-separated queries: SimM only considers their similarity with the fragment; Round-robin picksthe best document in each list; and our proposal, DivM, considers the diversity and importance of topics.A fourth method, DivS, uses one query made of all keywords extracted from the conversation fragment,and ranks the documents using the diverse re-ranking technique proposed by Santos et al.
(2010).Binary comparisons were performed between pairs of techniques, using crowdsourcing over 26 con-versation fragments of the ELEA Corpus, and aiming to minimize the number of binary comparisonswhile still ordering completely the methods according to their perceived quality.6.1 Diverse Re-ranking vs. Similarity MergingWe first performed a comparison between the top five documents generated by two recommendationstrategies, DivS and SimM, over 26 conversation fragments of the ELEA Corpus.
The consolidated rele-vance score (PCC-H) is 75% for SimM vs. 25% for DivS, as shown in Table 1.
These scores indicate thesuperiority of SimM over DivS.
In other words, separating the mixture of topics of a fragment into mul-tiple topically-separated queries mitigates the negative effect of the mixture of topics on the suggestions.6.2 Comparison across Merging TechniquesBinary comparisons were then performed between pairs of merging techniques (SimM, Round-robin,and DivM), using the same experimental settings.
The PCC-H scores are 62% for DivM vs. 38% forRound-robin, 59% for DivM vs. 41% for SimM, and 56% for Round-robin vs. 44% for SimM, as shownin Table 1.
The scores show that the diverse merging of lists of documents improves recommendations,and indicate the following high to low ranking: DivM > Round-robin > SimM.SimM ranks lowest in this ordering, likely because of the ignorance of diversity in the list of results.Round-robin is second, likely because it disregards the major differences of importance among implicitqueries in a conversation fragment.
The results of the comparisons confirm that the DivM technique,which merges lists of documents by considering the diversity of topics in the list of recommendations,in proportion to their importance in the conversation, is the most satisfying to the majority of humansubjects.6.3 Impact of the Topical Diversity of FragmentsTo further examine the benefits of our method, we studied its sensitivity to the number of topics in theconversation fragments.
For this purpose, we divided the set of test fragments into two subsets.
The firstone (noted ?A?
in Table 1) gathers the fragments for which fewer than or exactly five main topics (andtherefore implicit queries) have been computed.
The other fragments, with more than five main topics,form the second subset (noted ?B?).
The value of five corresponds to the average number of main topicsper fragment as well as to the number of recommended documents in our experiments.As shown in Table 1, although there is an improvement in the comparison scores of DivS over SimMwhen the number of conveyed topics in the fragments is higher than the number of recommended doc-uments (subset B), the comparison scores indicate the superiority of SimM over DivS in both cases, and595PCC-H relevance score (%) Raw preferences (%)Compared methods A B A ?
B A ?
B(m1vs.m2) m1m2m1m2m1m2m1m2SimM vs. DivS 80 20 70 30 75 25 70 30Round-robin vs. SimM 33 67 68 32 56 44 52 48DivM vs. Round-robin 64 36 60 40 62 38 58 42DivM vs. SimM 54 46 60 40 59 41 58 42Table 1: Comparative scores of the recommended document lists from four methods: DivS, SimM,Round-robin, and DivM, evaluated by human judges over the ELEA Corpus.
Subset A gathers frag-ments with fewer than or exactly five topics, while subset B gathers all the other fragments.
The resultsimply the following ranking: DivM > Round-robin > SimM > DivS.confirm the benefit of the diverse merging techniques.
When comparing Round-robin versus SimM, thescores show the superiority of the former method when the number of conveyed topics in fragments ishigher than the number of recommended documents, because it provides a diverse lists of documentsin which documents relevant to less important topics are not displayed.
However, when the number oftopics is smaller than the number of recommendations, SimM provides better results.
The reason of thedecrease in the scores of Round-robin is likely the ignorance of the actual importance of the main topicswhen ranking documents.
Overall, as shown in Table 1, regardless of the number of topics conveyed inthe fragments, DivM always outperforms Round-robin and SimM.6.4 Example of Document ResultsTo illustrate how DivM surpasses the other techniques, we consider an example from one of the conver-sation fragments of the ELEA Corpus.
The manual transcript of this conversation fragment is given inthe Appendix A.
As described in Section 5, the conversation participants had to select a list of 12 itemsvital to survive in winter while waiting to be rescued.
The keywords extracted from the manual transcriptof this fragment by our method (Habibi and Popescu-Belis, 2013) are: fire, lighter, cloth, shoe, cold, die,igloo, walking.
As our keyword extraction method was shown to be robust to ASR noise, we only usehere the reference transcripts (Habibi and Popescu-Belis, submitted).We display the topically-aware implicit queries prepared by our method from this keyword list alongwith their weights in Table 2.
Then, in Table 3 we show the retrieval results (five highest-rankedWikipedia pages) obtained by the four methods using the reference transcript of this fragment.As shown in Table 2, each implicit query corresponds to one of the main topics of the fragment witha specific weight.
In this example, the main topics spoken in the fragment are about making an igloo,lightening a fire, having warm clothes, and suitable shoes for walking.As shown in Table 3, DivS provides two irrelevant documents likely because the single (collective)query does not separate the mixture of topics in the conversation fragment, and leads to some poor results(Wikipedia pages) such as ?Cold Fire (Koontz novel)?.
SimM slightly improves the results by separatingthe discussed topics of the conversation fragment into multiple queries.
However, it does not cover all theImplicit queries Weightsq1= {fire, cold, igloo, lighter} w1= 0.110q2= {shoe, lighter, walking} w2= 0.097q3= {cloth} w3= 0.058q4= {die} w4= 0.040q5= {igloo} w5= 0.026Table 2: Example of implicit queries built from the keyword list extracted from a sample fragment of theELEA Corpus.
Each query covers one of the main topics of the fragment and has a different weight.596DivS SimM Round-robin DivMFlint spark lighter Igloo Igloo IglooExtended Cold Flint spark lighter Shoe ShoeWeather Clothing SystemCold Fire (Koontz novel) Lighter Jersey (clothing) Flint spark lighterIgloo Lighter (barge) Die Hard Jersey (clothing)Walking Worcester Cold Flint spark lighter LighterStorage Warehouse fireTable 3: Example of retrieved Wikipedia pages from the four different methods tested in this paper.Results of diverse merging (DivM) appear to cover more topics relevant to the conversation fragmentthan other methods.
The average ranking (DivM > Round-robin > SimM > DivS) is also observed inthis example.topics mentioned in the fragment due to mostly focusing on the single topic represented by q1.
Round-robin further enhances the results by adding diversity, but as it gives the same level of importance to alltopics, it provides a poor result like ?Die Hard?
from a topic of the conversation fragment with a smallweight.
The results of DivM appear to be the most useful ones, as they include other articles relevantto q1, q2, and q3before showing results relevant to the low weight queries q4and q5.
Therefore, in thisexample, DivM provides better ranking of documents by covering the largest number of main topicsmentioned in the fragment.7 ConclusionWe proposed a diverse merging technique for combining lists of documents from multiple topically-separated implicit queries, prepared using keyword lists obtained from the transcripts of conversationfragments.
Our diverse merging method DivM provides a short, diverse, and relevant list of recommen-dations, which avoids distracting participants that would consider it during the conversation.
We alsocompared DivM to existing merging techniques, in terms of comprehensiveness and relevance of thefinal recommended list of documents to the conversation fragment.
The human judgments collected viaAmazon Mechanical Turk showed that DivM outperforms all other methods.Moreover, these results emphasized the benefit of splitting the keyword set into multiple topically-separated queries: the suggested lists of documents from DivS (which accounts for the diversity of resultsby re-ranking the documents of a single list) were indeed found less relevant than those from SimM andthe other two methods, which merged results from multiple queries.In the future, the diverse merging method DivM will be integrated in the ACLD just-in-time retrievalsystem for conversational environments, with implicit queries that are prepared from the ASR transcriptof users?
conversation.
User-oriented evaluation experiments will be conducted.
We will also enable thesystem to answer explicit queries asked by users, considering contextual factors to improve the relevanceof the answers, which will complement the recommendation functionality based on implicit queries.AcknowledgmentsThe authors are grateful to the Swiss National Science Foundation for its support through the IM2 NCCRon Interactive Multimodal Information Management (2002-2013, see http://www.im2.ch), and tothe Hasler Foundation for its support through the REMUS project (Re-ranking Multiple Search Resultsfor Just-in-Time Document Recommendation, 2014).
The authors also thank the anonymous reviewersfor their helpful suggestions.ReferencesRakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong.
2009.
Diversifying search results.
InProceedings of the Second ACM International Conference on Web Search and Data Mining, pages 5?14.
ACM.597Jagdev Bhogal, Andy Macfarlane, and Peter Smith.
2007.
A review of ontology based query expansion.
Informa-tion and Processing Management, 43:866?886.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003.
Latent Dirichlet Allocation.
Journal of MachineLearning Research, 3:993?1022.Jonathan Boyd-Graber, Jordan Chang, Sean Gerrish, Chong Wang, and David Blei.
2009.
Reading tea leaves:How humans interpret topic models.
In Proceedings of the 23rd Annual Conference on Neural InformationProcessing Systems (NIPS), pages 1?9.Jay Budzik and Kristian J. Hammond.
2000.
User interactions with everyday applications as context for just-in-time information access.
In Proceedings of the 5th International Conference on Intelligent User Interfaces(IUI), pages 44?51.
ACM.Jaime Carbonell and Jade Goldstein.
1998.
The use of MMR, diversity-based reranking for reordering documentsand producing summaries.
In Proceedings of the 21st annual international ACM SIGIR conference on Researchand development in information retrieval, pages 335?336.
ACM.Claudio Carpineto and Giovanni Romano.
2012.
A survey of automatic query expansion in information retrieval.ACM Computing Surveys (CSUR), 44(1):1?56.Ben Carterette and Praveen Chandar.
2009.
Probabilistic models of ranking novel documents for faceted topicretrieval.
In Proceedings of the 18th ACM conference on Information and knowledge management, pages 1287?1296.Philip N. Garner, John Dines, Thomas Hain, Asmaa El Hannani, Martin Karafiat, Danil Korchagin, Mike Lincoln,Vincent Wan, and Le Zhang.
2009.
Real-time ASR from meetings.
In Proceedings of Interspeech 2009 (10thAnnual Conference of the International Speech Communication Association), pages 2119?2122.Maryam Habibi and Andrei Popescu-Belis.
2012.
Using crowdsourcing to compare document recommendationstrategies for conversations.
In Workshop on Recommendation Utility Evaluation: Beyond RMSE (RUE 2011),pages 15?20.Maryam Habibi and Andrei Popescu-Belis.
2013.
Diverse keyword extraction from conversations.
In Proceedingsof the ACL 2013 (51th Annual Meeting of the Association for Computational Linguistics), pages 651?657.Maryam Habibi and Andrei Popescu-Belis.
submitted.
Keyword extraction and clustering for document recom-mendation in conversations.
Manuscript submitted for publication.Peter E. Hart and Jamey Graham.
1997.
Query-free information retrieval.
International Journal of IntelligentSystems Technologies and Applications, 12(5):32?37.Matthew D. Hoffman, David M. Blei, and Francis Bach.
2010.
Online learning for Latent Dirichlet Allocation.Proceedings of 24th Annual Conference on Neural Information Processing Systems, 23:856?864.Gareth J.F.
Jones and Peter J.
Brown.
2004.
Context-aware retrieval for ubiquitous computing environments.
InMobile and ubiquitous information access, pages 227?243.
Springer.Jingxuan Li, Lei Li, and Tao Li.
2012.
Multi-document summarization via submodularity.
Applied Intelligence,37(3):420?430.Hui Lin and Jeff Bilmes.
2011.
A class of submodular functions for document summarization.
In Proceedings ofthe ACL 2011 (49th Annual Meeting of the Association for Computational Linguistics), pages 510?520.Andrew K. McCallum.
2002.
MALLET: A machine learning for language toolkit.
http://mallet.cs.umass.edu.Mehryar Mohri, Pedro Moreno, and Eugene Weinstein.
2010.
Discriminative topic segmentation of text andspeech.
In International Conference on Artificial Intelligence and Statistics, pages 533?540.George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher.
1978.
An analysis of approximations formaximizing submodular set functions.
Mathematical Programming Journal, 14(1):265?294.Andrei Popescu-Belis, Erik Boertjes, Jonathan Kilgour, Peter Poller, Sandro Castronovo, Theresa Wilson, Alejan-dro Jaimes, and Jean Carletta.
2008.
The AMIDA Automatic Content Linking Device: Just-in-time documentretrieval in meetings.
In Proceedings of MLMI 2008 (Machine Learning for Multimodal Interaction), LNCS5237, pages 272?283.598Andrei Popescu-Belis, Majid Yazdani, Alexandre Nanchen, and Philip N. Garner.
2011.
A speech-based just-in-time retrieval system using semantic search.
In Proceedings of 49th Annual Meeting of the ACL, pages 80?85.Filip Radlinski and Susan Dumais.
2006.
Improving personalized web search using result diversification.
In Pro-ceedings of the 29th annual international ACM SIGIR conference on Research and development in informationretrieval, pages 691?692.
ACM.Bradley J. Rhodes and Pattie Maes.
2000.
Just-in-time information retrieval agents.
IBM Systems Journal,39(3.4):685?704.Stephen E. Robertson.
1997.
The probability ranking principle in IR.
In Karen Sparck Jones and Peter Willett,editors, Readings in information retrieval, pages 281?286.
Morgan Kaufmann Publishers Inc.Dairazalia Sanchez-Cortes, Oya Aran, Marianne Schmid Mast, and Daniel Gatica-Perez.
2012.
A nonverbalbehavior approach to identify emergent leaders in small groups.
IEEE Trans.
on Multimedia, 14(3):816?832.Rodrygo L.T.
Santos, Craig Macdonald, and Iadh Ounis.
2010.
Exploiting query reformulations for web searchresult diversification.
In Proceedings of the 19th Int.
Conf.
on the World Wide Web, pages 881?890.
ACM.Sa?ul Vargas, Pablo Castells, and David Vallet.
2012.
Explicit relevance models in intent-oriented informationretrieval diversification.
In Proceedings of the 35th international ACM SIGIR conference on Research anddevelopment in information retrieval, pages 75?84.
ACM.Jun Wang and Jianhan Zhu.
2009.
Portfolio theory of information retrieval.
In Proceedings of the 32nd interna-tional ACM SIGIR conference on Research and development in information retrieval, pages 115?122.
ACM.Shengli Wu and Sally McClean.
2007.
Result merging methods in distributed information retrieval with overlap-ping databases.
Information Retrieval, 10(3):297?319.Cheng Xiang Zhai, William W. Cohen, and John Lafferty.
2003.
Beyond independent relevance: methods andevaluation metrics for subtopic retrieval.
In Proceedings of the 26th annual international ACM SIGIR confer-ence on Research and development in information retrieval, pages 10?17.
ACM.Appendix A. Transcript of a Conversation Fragment from the ELEA CorpusThe following transcript of a conversation fragment (speakers noted A through C) was submitted to thedocument recommender system and is exemplified in Section 6.4.
The corresponding implicit queriesand recommendations are respectively shown in Tables 2 and 3.A: okay I start.B: how how do you want to proceed?A: I guess -C: yes what is the most important?A: I guess fire light.B: fire lighter?A: fire, yes.
I would say if we had something we can fire with -- I guess thatthe lighter is useful in getting some sparks.B: hopefully.A: so we can use either newspaper or -- something like that.C: but again - first it is more important to have enough err clothes.A: and for me, more important to know where to go.
I would say that the compass.C: I mean -- if you don?t have enough clothes so -- at one point you can --B: you can die.C: yes you can -- you will die.
so first issue, try to keep yourself alive andthen you can --A: but -- but you already have some --B: basics.
you everything.
you have enormous which is and so is no shoes here.C: okay that we have shoes so -- okay.B: because seventy kilometers will take you how many days?
err in the snow --what do you think?A: two or three.B: it can be two or three days?C: yes, but okay you cannot always have fire with you -- but you need alwayshave clothes with you.
I mean it is the only thing that protects you when you arewalking.B: oh yes.
and erm you can make an igloo during the evening.
not that cold.only about five degrees.
so lighting a fire is not so important.C: I guess fire is an extra.
I mean it is important but err for me first it isimportant that when you keep walking you should be protected.599
