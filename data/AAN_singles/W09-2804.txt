Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 7?14,Suntec, Singapore, 6 August 2009.c?2009 ACL and AFNLPOptimization-based Content Selection for Opinion SummarizationJackie Chi Kit CheungDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canadajcheung@cs.toronto.eduGiuseppe Carenini and Raymond T. NgDepartment of Computer ScienceUniversity of British ColumbiaVancouver, BC, V6T 1Z4, Canada{carenini,rng}@cs.ubc.caAbstractWe introduce a content selection methodfor opinion summarization based on awell-studied, formal mathematical model,the p-median clustering problem from fa-cility location theory.
Our method re-places a series of local, myopic steps tocontent selection with a global solution,and is designed to allow content and re-alization decisions to be naturally inte-grated.
We evaluate and compare ourmethod against an existing heuristic-basedmethod on content selection, using humanselections as a gold standard.
We find thatthe algorithms perform similarly, suggest-ing that our content selection method isrobust enough to support integration withother aspects of summarization.1 IntroductionIt is now possible to find a large amount of in-formation on people?s opinions on almost everysubject online.
The ability to analyze such infor-mation is critical in complex, high-stakes decisionmaking processes.
At the individual level, some-one wishing to buy a laptop may read customerreviews from others who have purchased and usedthe product.
At the corporate level, customer feed-back on a newly launched product may help toidentify weaknesses and features that are in needof improvement (Dellarocas et al, 2004).Effective summarization systems are thusneeded to convey people?s opinions to users.
Achallenging problem in implementing this ap-proach in a particular domain is to devise a con-tent selection strategy that identifies what key in-formation should be presented.
In general, contentselection is a critical task at the core of both sum-marization and NLG and it represents a promisingarea for cross-fertilization.Existing NLG systems tend to approach con-tent selection by defining a heuristic based on sev-eral relevant factors, and maximizing this heuristicfunction.
ILEX (Intelligent Labelling Explorer) isa system for generating labels for sets of objectsdefined in a database, such as for museum arti-facts (O?Donnell et al, 2001).
Its content selectionstrategy involves computing a heuristic relevancescore for knowledge elements, and returning theitems with the highest scores.In GEA (Generator of Evaluative Arguments),evaluative arguments are generated to describe anentity as positive or negative (Carenini and Moore,2006).
An entity is decomposed into a hierarchyof features, and a relevance score is independentlycalculated for each feature, based on the prefer-ences of the user and the value of that feature forthe product.
Content selection involves selectingthe most relevant features for the current user.There is also work in sentiment analysis relyingon optimization or clustering-based approaches.Pang and Lee (2004) frame the problem of detect-ing subjective sentences as finding the minimumcut in a graph representation of the sentences.They produce compressed versions of movie re-views using just the subjective sentences, whichretain the polarity information of the review.
Ga-mon et al (2005) use a heuristic approach tocluster sentences drawn from car reviews, group-ing sentences that share common terms, especiallythose salient in the domain such as ?drive?
or ?han-dling?.
The resulting clusters are displayed by aTreemap visualization.Our work is most similar to the content se-lection method of the multimedia conversationsystem RIA (Responsive Information Architect)(Zhou and Aggarwal, 2004).
In RIA, contentselection involves selecting dimensions (such asprice in the real estate domain) in response to aquery such that the desirability of the dimensionsselected for the query is maximized while respect-7ing time and space constraints.
The maximizationof desirability is implemented as an optimizationproblem similar to a knapsack problem.
RIA?scontent selection method performs similarly to ex-pert human designers, but the evaluation is limitedin scale (two designers, each annotating two se-ries of queries to the system), and no heuristic al-ternative is compared against it.
Our work alsoframes content selection as a formal optimizationproblem, but we apply this model to the domain ofopinion summarization.A key advantage of formulating a content selec-tion strategy as a p-median optimization problemis that the resulting framework can be extended toselect other characteristics of the summary at thesame time as the information content, such as therealization strategy with which the content is ex-pressed.
The p-median clustering works as a mod-ule separate from its interpretation as the solutionto a content selection problem, so we can freelymodify the conversion process from the selectionproblem to the clustering problem.
Work in NLGand summarization has shown that content andrealization decisions (including media allocation)are often dependent on each other, which shouldbe reflected in the summarization process.
Forexample, in multi-modal summarization, complexinformation can be more effectively conveyed bycombining graphics and text (Tufte et al, 1998).While graphics can present large amounts of datacompactly and support the discovery of trends andrelationships, text is much more effective at ex-plaining key points about the data.
In another casespecific to opinion summarization, the controver-siality of the opinions in a corpus was found to cor-relate with the type of text summary, with abstrac-tive summarization being preferred when the con-troversiality is high (Carenini and Cheung, 2008).We first test whether our optimization-basedapproach can achieve reasonable performance oncontent selection alone.
As a contribution of thispaper, we compare our optimization-based ap-proach to a previously proposed heuristic method.Because our approach replaces a set of myopic de-cisions with an extensively studied procedure (thep-median problem) that is able to find a global so-lution, we hypothesized our approach would pro-duce better selections.
The results of our studyindicate that our optimization-based content selec-tion strategy performs about as well as the heuris-tic method.
These results suggest that our frame-work is robust enough for integrating other aspectsof summarization with content selection.2 Previous Heuristic Approach2.1 Assumed Input InformationWe now define the expected input into the summa-rization process, then describe a previous greedyheuristic method.
The first phase of the summa-rization process is to extract opinions about an en-tity from free text or some other source, such assurveys.
and express the extracted information in astructured format for further processing.
We adoptthe approach to opinion extraction described byCarenini et al (2006), which we summarize here.Given a corpus of documents expressing opin-ions about an entity, the system extracts a set ofevaluations on aspects or features of the product.An evaluation consists of a polarity, a score forthe strength of the opinion, and the feature be-ing evaluated.
The polarity expresses whether theopinion is positive or negative, and the strengthexpresses the degree of the sentiment, which isrepresented as an integer from 1 to 3.
Possi-ble polarity/strength (P/S) scores are thus [-3,-2,-1,+1,+2,+3], with +3 being the most positiveevaluation, and -3 the most negative.
For exam-ple, using a DVD player as the entity, the com-ment ?Excellent picture quality?on par with myPioneer, Panasonic, and JVC players.?
contains anopinion on the picture quality, and is a very posi-tive evaluation (+3).The features and their associated opinions areorganized into a hierarchy of user-defined features(UDFs), so named because they can be defined bya user according to the user?s needs or interests.1The outcome of the process of opinion extractionand structuring is a UDF hierarchy in which eachnode is annotated with all the evaluations it re-ceived in the corpus (See Figure 1 for an example).2.2 Heuristic Content Selection StrategyUsing the input information described above, con-tent selection is framed as the process of selectinga subset of those features that are deemed more1Actually, the system first extracts a set of surface-levelcrude features (CFs) on which opinions were expressed, us-ing methods described by Hu and Liu (2004).
Next, the CFsare mapped onto the UDFs using term similarity scores.
Theprocess of mapping CFs to UDFs groups together semanti-cally similar CFs and reduces redundancy.
Our study ab-stracts away from this mapping process, as well as the pro-cess of creating the UDF structure.
We leave the explanationof the details to the original papers.8CameraLens [+1,+1,+3,-2,+2]Digital ZoomOptical Zoom.
.
.Editing/Viewing[+1,+1]Viewfinder [-2,-2,-1].
.
.Flash[+1,+1,+3,+2,+2].
.
.ImageImage TypeTIFFJPEG.
.
.ResolutionEffective PixelsAspect Ratio.
.
.Figure 1: Partial view of assumed input informa-tion (UDF hierarchy annotated with user evalua-tions) for a digital camera.important and relevant to the user.
This is doneusing an importance measure defined on the avail-able features (UDFs).
This measure is calculatedfrom the P/S scores of the evaluations associatedto each UDF.
Let PS(u) be the set of P/S scoresthat UDF u receives.
Then, a measure of im-portance is defined as some function of the P/Sscores.
Previous work considered only summingthe squares of the scores.
In this work, we alsoconsider summing the absolute value of the scores.So, the importance measure is defined asdir moi(u) =?psPS(u)ps2or?psPS(u)|ps|where the term ?direct?
means the importance isderived only from that feature and not from itsdescendant features.
The basic premises of thesemetrics are that a feature?s importance should beproportional to the number of evaluations of thatfeature in the corpus, and that stronger evaluationsshould be given more weight.
The two versionsimplement the latter differently, using the sum ofsquares or the absolute values respectively.
No-tice that each non-leaf node in the feature hierar-chy effectively serves a dual purpose.
It is both afeature upon which a user might comment, as wellas a category for grouping its sub-features.
Thus,a non-leaf node should be important if either itsdescendants are important or the node itself is im-portant.
To this end, a total measure of importancemoi(u) is defined asmoi(u) =??????
?dir moi(u) if CH(u) = ?[?
dir moi(u) +(1?
?)?
?v?CH(u)moi(v)] otherwisewhere CH(u) refers to the children of u inthe hierarchy and ?
is some real parameter in therange [0.5, 1] that adjusts the relative weights ofthe parent and children.
We found in our experi-mentation that the parameter setting does not sub-stantially change the performance of the system,so we select the value 0.9 for ?, following previ-ous work.
As a result, the total importance of anode is a combination of its direct importance andof the importance of its children.The selection procedure proceeds as follows.First, the most obvious simple greedy selectionstrategy was considered?sort the nodes in the UDFby the measure of importance and select the mostimportant node until a desired number of featuresis included.
However, since a node derives partof its ?importance?
from its children, it is possiblefor a node?s importance to be dominated by one ormore of its children.
Including both the child andparent node would be redundant because most ofthe information is contained in the child.
Thus, adynamic greedy selection algorithm was devisedin which the importance of each node was recal-culated after each round of selection, with all pre-viously selected nodes removed from the tree.
Inthis way, if a node that dominates its parent?s im-portance is selected, its parent?s importance willbe reduced during later rounds of selection.
No-tice, however, that this greedy selection consists ofa series of myopic steps to decide which featuresto include in the summary next, based on what hasbeen selected already and what remains to be se-lected at this step.
Although this series of localdecisions may be locally optimal, it may result ina suboptimal choice of contents overall.3 Clustering-Based OptimizationStrategyTo address the limitation of local optimality ofthis initial strategy, we explore if the content se-lection problem for opinion summarization canbe naturally and effectively solved by a globaloptimization-based approach.
Our approach as-sumes the same input information as the previ-ous approach, and we also use the direct measure9of importance defined above.
Our framework isUDF-based in the following senses.
First, a UDFis the basic unit of content that is selected for in-clusion in the summary.
Also, the informationcontent that needs to be ?covered?
by the summaryis the sum of the information content in all of theUDFs in the UDF hierarchy.To reduce content selection to a clustering prob-lem, we need the following components.
First, weneed a cost function to quantify how well a UDF(if selected) can express the information contentin another UDF.
We call this measure the infor-mation coverage cost.
To define this cost func-tion, we need to define the semantic relatednessbetween the selected content and the covered con-tent, which is domain-dependent.
For example, wecan rely on similarity metrics such as ones basedon WordNet similarity scores (Fellbaum and oth-ers, 1998).
In the consumer product domain inwhich we test our method, we use the UDF hi-erarchy of the entity being summarized.Second, we need a clustering paradigm that de-fines the quality of a proposed clustering; that is,a way to globally quantify how well all the infor-mation content is represented by the set of UDFsthat we select.
The clustering paradigm that wefound to most naturally fit our task is the p-medianproblem (also known as the k-median problem),from facility location theory.
In its original in-terpretation, p-median is used to find optimal lo-cations for opening facilities which provide ser-vices to customers, such that the cost of servingall of the customers with these facilities is mini-mized.
This matches our intuition that the qualityof a summary of opinions depends on how well itrepresents all of the opinions to be summarized.Formally, given a set F of m potential locationsfor facilities, a set U of n customers, a cost func-tion d : F ?
U ??
< representing the cost ofserving a customer u ?
U with a facility f ?
F ,and a constant p ?
m, an optimal solution to thep-median problem is a subset S of F , such that theexpression?u?Uminf?Sd(f, u)is minimized, and |S| = p. The subset S is exactlythe set of UDFs that we would include in the sum-mary, and the parameter p can be set to determinethe summary length.Although solving the p-median problem is NP-hard in general (Kariv and Hakimi, 1979), viableapproximation methods do exist.
We use POP-STAR, an implementation of an approximate so-lution (Resende and Werneck, 2004) which hasan average error rate of less than 0.4% on all theproblem classes it was tested on in terms of the p-median problem value.
As an independent test ofthe program?s efficacy, we compare the program?soutput to solutions which we obtained by brute-force search on 12 of the 36 datasets we workedwith which are small enough such that an exact so-lution can be feasibly found.
POPSTAR returnedthe exact solution in all 12 instances.We now reinterpret the p-median problem forsummarization content selection by specifying thesets U , F , and the information coverage cost d interms of properties of the summarization process.We define the basic unit of the summarization pro-cess to be UDFs, so the sets U and F correspondto the set of UDFs describing the product.
Theconstant p is a parameter to the p-median prob-lem, determining the summary size in terms of thenumber of features.The cost function is d(u, v), where u is a UDFthat is being considered for inclusion in the sum-mary, and v is the UDF to be ?covered?
by u. Tospecify this cost, we need to consider both the to-tal amount of information in v as well as the se-mantic relationship between the two features.
Weuse the importance measure defined earlier, basedon the number and strength of evaluations of thecovered feature to quantify the former.
The rawimportance score is modified by multipliers whichdepend on the relationship between u and v. Oneis the semantic relatedness between the two fea-tures, which is modelled by the UDF tree hierar-chy.
We hypothesize that it is easier for a moregeneral feature to cover information about a morespecific feature than the reverse, and that featuresthat are not in a ancestor-descendant relationshipcannot cover information about each other becauseof the tenuous semantic connection between them.For example, knowing that a camera is well-likedin general provides stronger evidence that its dura-bility is also well-liked than the reverse.
Based onthese assumptions, we define a multiplier for theabove measure of importance based on the UDFtree structure, T (u, v), as follows.T (u, v) =???Tup?
k, if u is a descendant of vk, if u is an ancestor of v?, otherwisek is the length of the path from u to v in the UDF10hierarchy.
Tupis a parameter specifying the rela-tive difficulty of covering information in a featurethat is an ancestor in the UDF hierarchy.
Mirror-ing our experience with the heuristic method, thevalue of the parameter does not affect performancevery much.
In our experiments and the example tofollow, we pick the values Tup= 3, meaning thatcovering information in an ancestor node is threetimes more difficult than covering information ina descendant node.Another multiplier to the opinion domain is thedistribution of evaluations of the features.
Cover-age is expected to be less if the features are evalu-ated differently; for example, if users rated a cam-era well overall but the feature zoom poorly, a sen-tence about how well the camera is rated in gen-eral does not provide much evidence that the zoomis not well liked, and vice versa.
Since evalua-tions are labelled with P/S ratings in our data, it isnatural to define this multiplier based on the dis-tributions of ratings for the features.
Given theseP/S ratings between -3 and +3, we first aggregatethe positive and negative evaluations.
As before,we test both summing absolute values and squaredvalues.
Define:imp pos(u) =?ps?PS(u)?ps>0ps2or |ps|imp neg(u) =?ps?PS(u)?ps<0ps2or |ps|Then, we calculate the parameter to the Bernoullidistribution corresponding to the ratio of the im-portance of the two polarities.
That is, Bernoulliwith parameter?
(u) = imp pos(u)/(imp pos(u)+imp neg(u))The distribution-based multiplier E(u, v) is theJensen-Shannon divergence from Ber(?
(u)) toBer(?
(v)), plus one for multiplicative identitywhen the divergence is zero.E(u, v) = JS(?
(u), ?
(v)) + 1The final formula for the information coveragecost is thusd(u, v) = dir moi(v)?
T (u, v)?
E(u, v)Consider the following example consisting offour-node UDF tree and importance scores.i.
Covered ii.
SolutionsA B C D p Selected Val.CoveringA 0 50 30 240 1 A 320B 165 0 ?
120 2 A,D 80C 165 ?
0 ?
3 A,B,D 30D 330 150 ?
0 4 A,B,C,D 0Table 1: i.
Information coverage cost scores forthe worked example.
Rows represent the coveringfeature, while columns represent the covered fea-ture.
ii.
Optimal solution to p-median problem inthe worked example at different numbers of fea-tures selected.A dir moi(A) = 55?
?B C dir moi(B) = 50, dir moi(C) = 30?D dir moi(D) = 120With parameter Tup= 3 and setting thedistribution-based multiplier E to 1 to simplifycalculations (or for example, if the features re-ceived the same distributions of evaluations), thistree yields the information coverage cost scoresfound in Table 1i.
Running p-median on these val-ues produces the optimal results found in Table 1ii.This method trades off selecting centrally locatednodes near the root of the UDF tree and the im-portance of the individual nodes.
In this example,D is selected after the root node A even though Dhas a greater importance value.4 Comparative Evaluation4.1 Stochastic Data GenerationIn our experiments we wanted to compare the twocontent selection strategies (heuristic vs. p-medianoptimization) on datasets that were both realisticand diverse.
Despite the widespread adoption ofuser reviews in online websites, there is to ourknowledge no publicly available corpus of cus-tomer reviews of sufficient size which is annotatedwith features arranged in a hierarchy.
While small-scale corpora do exist for a small number of prod-ucts, the size of the corpora is too small to be rep-resentative of all possible distributions of evalu-ations and feature hierarchies of products, whichlimits our ability to draw any meaningful conclu-sion from the dataset.2Thus, we stochastically2Using a constructed dataset based on real data where noresources or agreed-upon evaluation methodology yet existshas been done in other NLP tasks such as topic boundary de-tection (Reynar, 1994) and local coherence modelling (Barzi-lay and Lapata, 2005).
We are encouraged, however, that sub-sequent to our experiment, more resources for opinion anal-11mean std.# Features 55.3889 8.5547# Evaluated Features 21.6667 5.9722# Children (depth 0) 11.3056 0.7753# Children (depth 1 fertile) 5.5495 1.7724Table 2: Statistics on the 36 generated data sets.At depth 1, 134 of the 407 features in total acrossthe trees were barren.
The generated tree hierar-chies were quite flat, with a maximum depth of 2.generated the data for the products to mimic realproduct feature hierarchies and evaluations.
Wedid this by gathering statistics from existing cor-pora of customer reviews about electronics prod-ucts (Hu and Liu, 2004), which contain UDF hier-archies and evaluations that have been defined andannotated.
Using these statistics, we created dis-tributions over the characteristics of the data, suchas the number of nodes in a UDF hierarchy, andsampled from these distributions to generate newUDF hierarchies and evaluations.
In total, we gen-erated 36 sets of data, which covered a realistic setof possible scenarios in term of feature hierarchystructures as well as in term of distribution of eval-uations for each feature.
Table 2 presents somestatistics on the generated data sets.4.2 Building a Human Performance ModelWe adopt the evaluation approach that a good con-tent selection strategy should perform similarly tohumans, which is the view taken by existing sum-marization evaluation schemes such as ROUGE(Lin, 2004) and the Pyramid method (Nenkova etal., 2007).
For evaluating our content selectionstrategy, we conducted a user study asking humanparticipants to perform a selection task to create?gold standard?
selections.
Participants viewedand selected UDF features using a Treemap infor-mation visualization.
See Figure 2 for an example.We recruited 25 university students or gradu-ates, who were each presented with 19 to 20 ofthe cases we generated as described above.
Eachcase represented a different hypothetical product,which was represented by a UDF hierarchy, aswell as P/S evaluations from -3 to +3.
These weredisplayed to the participants by a Treemap visual-ization (Shneiderman, 1992), which is able to givean overview of the feature hierarchy and the eval-uations that each feature received.
Treemaps havebeen shown to be a generally successful tool forysis such as a user review corpus by Constant et al (2008)have been released, as an anonymous reviewer pointed out.visualizing data in the customer review domain,even for novice users (Carenini et al, 2006).
Ina Treemap, the feature hierarchy is represented bynested rectangles, with parent features being largerrectangles, and children features being smallerrectangles contained within its parent rectangle.The size of the rectangles depends on the numberof evaluations that this feature received directly,as well as indirectly through its children features.Each evaluation is also shown as a small rectangle,coloured according to its P/S rating, with -3 beingbright red, and +3 being bright green.Participants received 30 minutes of interactivetraining in using Treemaps, and were presentedwith a scenario in which they were told to take therole of a friend giving advice on the purchase ofan electronics product based on existing customerreviews.
They were then shown 22 to 23 scenar-ios corresponding to different products and eval-uations, and asked to select features which theythink would be important to include in a summaryto send to a friend.
We discarded the first threeselections that participants made to allow them tobecome further accustomed to the visualization.The number of features that participants wereasked to select from each tree was 18% of thenumber of selectable features.
A feature is con-sidered selectable if it appears in the Treemap vi-sualization; that is, the feature receives at leastone evaluation, or one of its descendant featuresdoes.
This proportion was the average propor-tion at which the selections made by the heuristicgreedy strategy and p-median diverged the mostwhen we were initially testing the algorithms.
Be-cause each tree contained a different number offeatures, the actual number of features selectedranged from two to seven.
Features were givengeneric labels like Feature 34, so that participantscannot rely on preexisting knowledge about thatFigure 2: A sample Treemap visualization of thecustomer review data sets shown to participants.12Selection method Cohen?s Kappaheuristic, squared moi 0.4839heuristic, abs moi 0.4841p-median, squared moi 0.4679p-median, abs moi 0.4821Table 3: Cohen?s kappa for heuristic greedy andp-median methods against human selections.
Twoversions of the measure of importance were tested,one using squared P/S scores, the other using ab-solute values.kind of product in their selections.4.3 Evaluation MetricsUsing this human gold standard, we can now com-pare the greedy heuristic and the p-median strate-gies.
We report the agreement between the hu-man and machine selections in terms of kappaand a version of the Pyramid method.
The Pyra-mid method is a summarization evaluation schemebuilt upon the observation that human summariescan be equally informative despite being divergentin content (Nenkova et al, 2007).
In the Pyramidmethod, Summary Content Units (SCUs) in a setof human-written model summaries are manuallyidentified and annotated.
These SCUs are placedinto a pyramid with different tiers, correspondingto the number of model (i.e.
human) summariesin which each SCU appears.
A summary to beevaluated is similarly annotated by SCUs and isscored by the scores of its SCUs, which are thetier of the pyramid in which the SCU appears.
ThePyramid score is defined as the sum of the weightsof the SCUs in the evaluated summary divided bythe maximum score achievable with this numberof SCUs, if we were to take SCUs starting fromthe highest tier of the pyramid.
Thus, a summaryscores highly if its SCUs are found in many ofthe model summaries.
We use UDFs rather thantext passages as SCUs, since UDFs are the ba-sic units of content in our selections.
Moderateinter-annotator agreement between human featureselections shows that our data fits the assumptionof the Pyramid method (i.e.
diversity of human an-notations); the Fleiss?
kappa (1971) scores for thehuman selections ranged from 0.2984 to 0.6151,with a mean of 0.4456 among all 33 sets whichwere evaluated.
A kappa value above 0.6 is gener-ally taken to indicate substantial agreement (Lan-dis and Koch, 1977).Figure 3: Pyramid scores for the two selection ap-proaches at different numbers of features i. usingthe squared importance measure, ii.
using the ab-solute value importance measure.4.4 ResultsThe greedy heuristic method and p-median per-form similarly at the number of features that thehuman participants were asked to select.
The dif-ference is not statistically significant by a two-tailed t-test.
Table 3 shows that using absolutevalues of P/S scores in the importance measureis better than using squares.
Squaring seems togive too much weight to extreme evaluations overmore neutral evaluations.
P-median is particu-larly affected, which is not surprising as it uses themeasure of importance both in the raw importancescore and in the distribution-based multiplier.The Pyramid method allows us to compare thealgorithms at different numbers of features.
Fig-ure 3 shows the average pyramid score for thetwo methods over the proportion of features thatare selected.
Overall, both algorithms performwell, and reach a score of about 0.9 at 10% offeatures selected.
The heuristic method performsslightly better when the proportion is below 25%,but slightly worse above that proportion.We consider several possible explanations forthe surprising result that the heuristic greedymethod and p-median methods perform similarly.One possibility is that the approximate p-mediansolution we adopted (POPSTAR) is error-prone onthis task, but this is unlikely as the approximatemethod has been rigorously tested both externallyon much larger problems and internally on a sub-set of our data.
Another possibility is that the au-tomatic methods have reached a ceiling in perfor-mance by these evaluation metrics.Nevertheless, these results are encouraging inshowing that our optimization-based method is aviable alternative to a heuristic strategy for con-tent selection, and validate that incorporating other13summarization decisions into content selection isan option worth exploring.5 Conclusions and Future WorkWe have proposed a formal optimization-basedmethod for summarization content selection basedon the p-median clustering paradigm, in whichcontent selection is viewed as selecting clustersof related information.
We applied the frame-work to opinion summarization of customer re-views.
An experiment evaluating our p-medianalgorithm found that it performed about as wellas a comparable existing heuristic approach de-signed for the opinion domain in terms of similar-ity to human selections.
These results suggest thatthe optimization-based approach is a good startingpoint for integration with other parts of the sum-marization/NLG process, which is a promising av-enue of research.6 AcknowledgementsWe would like to thank Lucas Rizoli, Gabriel Mur-ray and the anonymous reviewers for their com-ments and suggestions.ReferencesR.
Barzilay and M. Lapata.
2005.
Modeling Local Co-herence: An Entity-based Approach.
In Proc.
43rdACL, pages 141?148.G.
Carenini and J.C.K.
Cheung.
2008.
Extractive vs.NLG-based abstractive summarization of evaluativetext: The effect of corpus controversiality.
In Proc.5th INLG.G.
Carenini and J.D.
Moore.
2006.
Generating andevaluating evaluative arguments.
Artificial Intelli-gence, 170(11):925?952.G.
Carenini, R.T. Ng, and A. Pauls.
2006.
Interac-tive multimedia summaries of evaluative text.
InProc.
11th Conference on Intelligent User Inter-faces, pages 124?131.N.
Constant, C. Davis, C. Potts, and F. Schwarz.2008.
The pragmatics of expressive content: Evi-dence from large corpora.
Sprache und Datenverar-beitung.C.
Dellarocas, N. Awad, and X. Zhang.
2004.
Explor-ing the Value of Online Reviews to Organizations:Implications for Revenue Forecasting and Planning.In Proc.
24th International Conference on Informa-tion Systems.C.
Fellbaum et al 1998.
WordNet: an electronic lexi-cal database.
Cambridge, Mass: MIT Press.J.L.
Fleiss et al 1971.
Measuring nominal scale agree-ment among many raters.
Psychological Bulletin,76(5):378?382.M.
Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.2005.
Pulse: Mining customer opinions from freetext.
Lecture Notes in Computer Science, 3646:121?132.M.
Hu and B. Liu.
2004.
Mining and summarizingcustomer reviews.
In Proc.
2004 ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, pages 168?177.
ACM Press NewYork, NY, USA.O.
Kariv and S.L.
Hakimi.
1979.
An algorithmicapproach to network location problems.
II: the p-medians.
SIAM Journal on Applied Mathematics,37(3):539?560.J.R.
Landis and G.G.
Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33(1):159?174.C.Y.
Lin.
2004.
ROUGE: A Package for AutomaticEvaluation of Summaries.
In Proc.
Workshop onText Summarization Branches Out, pages 74?81.A.
Nenkova, R. Passonneau, and K. McKeown.
2007.The Pyramid Method: Incorporating human con-tent selection variation in summarization evaluation.ACM Transactions on Speech and Language Pro-cessing (TSLP), 4(2).M.
O?Donnell, C. Mellish, J. Oberlander, and A. Knott.2001.
ILEX: an architecture for a dynamic hypertextgeneration system.
Natural Language Engineering,7(03):225?250.B.
Pang and L. Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proc.
42nd ACL, pages271?278.M.G.C.
Resende and R.F.
Werneck.
2004.
A Hy-brid Heuristic for the p-Median Problem.
Journalof Heuristics, 10(1):59?88.J.C.
Reynar.
1994.
An automatic method of findingtopic boundaries.
In Proc.
32nd ACL, pages 331?333.B.
Shneiderman.
1992.
Tree visualization with tree-maps: 2-d space-filling approach.
ACM Transac-tions on Graphics (TOG), 11(1):92?99.E.R.
Tufte, S.R.
McKay, W. Christian, and J.R Matey.1998.
Visual Explanations: Images and Quanti-ties, Evidence and Narrative.
Computers in Physics,12(2):146?148.M.X.
Zhou and V. Aggarwal.
2004.
An optimization-based approach to dynamic data content selectionin intelligent multimedia interfaces.
In Proc.
17thannual ACM symposium on User interface softwareand technology, pages 227?236.
ACM Press NewYork, NY, USA.14
