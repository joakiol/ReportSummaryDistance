Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1169?1179,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Random Walk Approach to Selectional Preferences Based onPreference Ranking and Propagation?Zhenhua Tian?, Hengheng Xiang, Ziqi Liu, Qinghua Zheng?Ministry of Education Key Lab for Intelligent Networks and Network SecurityDepartment of Computer Science and TechnologyXi?an Jiaotong UniversityXi?an, Shaanxi 710049, China{zhhtian?,qhzheng?
}@mail.xjtu.edu.cnAbstractThis paper presents an unsupervised ran-dom walk approach to alleviate data spar-sity for selectional preferences.
Based onthe measure of preferences between predi-cates and arguments, the model aggregatesall the transitions from a given predicate toits nearby predicates, and propagates theirargument preferences as the given predi-cate?s smoothed preferences.
Experimen-tal results show that this approach out-performs several state-of-the-art method-s on the pseudo-disambiguation task, andit better correlates with human plausibilityjudgements.1 IntroductionSelectional preferences (SP) or selectional restric-tions capture the plausibility of predicates andtheir arguments for a given relation.
Kaze andFodor (1963) describe that predicates and theirarguments have strict boolean restrictions, eithersatisfied or violated.
Sentences are semanticallyanomalous and not consistent in reading if theyviolated the restrictions.
Wilks (1973) argues that?rejecting utterances is just what humans do not.They try to understand them.?
He further states s-electional restrictions as preferences between thepredicates and arguments, where the violation canbe less preferred, but not fatal.
For instance, giventhe predicate word eat, word food is likely to beits object, iPhone is likely to be implausible for it,and tiger is less preferred but not curious.SP have been proven to help many natural lan-guage processing tasks that involve attachment de-?Partial of this work was done when the first author vis-iting at Language Technologies Institute of Carnegie MellonUniversity sponsored by the China Scholarship Council.cisions, such as semantic role labeling (Resnik,1993; Gildea and Jurafsky, 2002), word sense dis-ambiguation (Resnik, 1997), human plausibilityjudgements (Spasic?
and Ananiadou, 2004), syn-tactic disambiguation (Toutanova et al, 2005),word compositionality (McCarthy et al, 2007),textual entailment (Pantel et al, 2007) and pro-noun resolution (Bergsma et al, 2008) etc.A direct approach to acquire SP is to extracttriples (q, r, a) of predicates, relations, and argu-ments from a syntactically analyzed corpus, andthen conduct maximum likelihood estimation (M-LE) on the data.
However, this strategy is infea-sible for many plausible triples due to data spar-sity.
For example, given the relation <verb-dobj-noun> in a corpus, we may see plausible triples:eat - {food, cake, apple, banana, candy...}But we may not see plausible and implausibletriples such as:eat - {watermelon, ziti, escarole, iPhone...}Then how to use a smooth model to alleviatedata sparsity for SP?Random walk models have been successful-ly applied to alleviate the data sparsity issue oncollaborative filtering in recommender systems.Many online businesses, such as Netflix, Ama-zon.com, and Facebook, have used recommendersystems to provide personalized suggestions onthe movies, books, or friends that the users mayprefer and interested in (Liben-Nowell and Klein-berg, 2007; Yildirim and Krishnamoorthy, 2008).In this paper, we present an extension of usingthe random walk model to alleviate data sparsi-ty for SP.
The main intuition is to aggregate allthe transitions from a given predicate to its near-by predicates, and propagate their preferences onarguments as the given predicate?s smoothed argu-1169ment preferences.
Our work and contributions aresummarized as follows:?
We present a framework of random walk ap-proach to SP.
It contains four components withflexible configurations.
Each component is cor-responding to a specific functional operation onthe bipartite and monopartite graphs which rep-resenting the SP data;?
We propose an adjusted preference rankingmethod to measure SP based on the popularityand association of predicate-argument pairs.
Itbetter correlates with human plausibility judge-ments.
It also helps to discover similar predi-cates more precisely;?
We introduce a probability function for randomwalk based on the predicate distances.
It con-trols the influence of nearby and distant predi-cates to achieve more accurate results;?
We find out that propagate the measured prefer-ences of predicate-argument pairs is more prop-er and natural for SP smooth.
It helps to im-prove the final performance significantly.We conduct experiments using two sections ofthe LDC English gigaword corpora as the general-ization data.
For the pseudo-disambiguation task,we evaluate it on the Penn TreeBank-3 data.
Re-sults show that our model outperforms several pre-vious methods.
We further investigate the correla-tions of smoothed scores with human plausibili-ty judgements.
Again our method achieves bettercorrelations on two third party data.The remainder of the paper is organized as fol-lows: Section 2 introduces related work.
Section 3briefly formulates the overall framework of ourmethod.
Section 4 describes the detailed modelconfigurations, with discussions on their roles andimplications.
Section 5 provides experiments onboth the pseudo-disambiguation task and humanplausibility judgements.
Finally, Section 6 sum-marizes the conclusions and future work.2 Related Work2.1 WordNet-based ApproachResnik (1996) conducts the pioneer work oncorpus-driven SP induction.
For a given predi-cate q, the system firstly computes its distributionof argument semantic classes based on WordNet.Then for a given argument a, the system collectsthe set of candidate semantic classes which con-tain the argument a, and ensures they are seen inq.
Finally the system picks a semantic class fromthe candidates with the maximal selectional asso-ciation score, and defines the score as smoothedscore of (q, a).Many researchers have followed the so-calledWordNet-based approach to SP.
One of the keyissues is to induce the set of argument semanticclasses that are acceptable by the given predicate.Li and Abe (1998) propose a tree cut model basedon minimal description length (MDL) principlefor the induction of semantic classes.
Clark andWeir (2002) suggest a hypothesis testing methodby ascending the noun hierarchy of WordNet.
Cia-ramita and Johnson (2000) model WordNet as aBayesian network to solve the ?explain away?
am-biguity.
Beyond induction on argument classes on-ly, Agirre and Martinez (2001) propose a class-to-class model that simultaneously learns SP on boththe predicate and argument classes.WordNet-based approach produces human in-terpretable output, but suffers the poor lexical cov-erage problem.
Gildea and Jurafsky (2002) showthat clustering-based approach has better cover-age than WordNet-based approach.
Brockman-n and Lapata (2003) find out that sophisticatedWordNet-based methods do not always outperfor-m simple frequency-based methods.2.2 Distributional Models without WordNetAlternatively, Rooth et al (1999) propose an EM-based clustering smooth for SP.
The key idea is touse the latent clusterings to take the place ofWord-Net semantic classes.
Where the latent clusteringsare automatically derived from distributional da-ta based on EM algorithm.
Recently, more so-phisticated methods are innovated for SP based ontopic models, where the latent variables (topics)take the place of semantic classes and distribution-al clusterings (Se?aghdha, 2010; Ritter et al, 2010).Without introducing semantic classes and laten-t variables, Keller and Lapata (2003) use the webto obtain frequencies for unseen bigrams smooth.Pantel et al (2007) apply a collection of rules tofilter out incorrect inferences for SP.
Specifically,Dagan et al (1999) introduce a general similarity-based model for word co-occurrence probabilities,which can be interpreted for SP.
Similarly, Erk etal.
propose an argument-oriented similarity modelbased on semantic or syntactic vector spaces (Erk,11702007; Erk et al, 2010).
They compare several sim-ilarity functions and weighting functions in theirmodel.
Furthermore, instead of employing varioussimilarity functions, Bergsma et al (2008) pro-pose a discriminative approach to learn the weight-s between the predicates, based on the verb-nounco-occurrences and other kinds of features.Random walk model falls into the non-classbased distributional approach.
Previous literatureshave fully studied the selection of distance or sim-ilarity functions to find out similar predicates andarguments (Dagan et al, 1999; Erk et al, 2010), orlearn the weights between the predicates (Bergsmaet al, 2008).
Instead, we put effort in followingissues: 1) how to measure SP; 2) how to trans-fer between predicates using random walk; 3) howto propagate the preferences for smooth.
Experi-ments show these issues are important for SP andthey should be addressed properly to achieve bet-ter results.3 RSP: A Random Walk Model for SPIn this section, we briefly introduce how to addressSP using random walk.
We propose a frameworkof RSP with four components (functions).
Each ofthem are flexible to be configured.
In summary,Algorithm 1 describes the overall process.Algorithm 1 RSP: Random walk model for SPRequire: Init bipartite graph G with raw counts1: // Ranking on the bipartite graph G;2: R = ?
(G); // ranking function3: // Project R to monopartite graph D4: D = ?
(R); // distance function5: // Transform D to stochastic matrix P6: P = ?
(D); // probability function7: // Get the convergence P?8: P?
=?
?t=1(dP )t|(dP )t| = dP (I ?
dP )?1;9: return Smoothed bipartite graph R?10: R?
= P?
?R; // propagation functionBipartite Graph Construction: For a giv-en relation r, the observed predicate-argumentpairs can be represented by a bipartite graphG=(X,Y,E).
Where X={q1 , q2 , ..., qm} are them predicates, and Y ={a1 , a2 , ..., an} are the n ar-guments.
We initiate the links E with the rawco-occurrence counts of seen predicate-argumentpairs in a given generalization data.
We representthe graph by an adjacency matrix with rows repre-senting predicates and columns as arguments.
Forconvenience, we use indices i, j to represent pred-icates qi , qj , and k, l for arguments ak , al .We employ a preference ranking function ?
tomeasure the SP between the predicates and argu-ments.
It transforms G to a corresponding bipar-tite graph R, with links representing the strengthof SP.
Each row of the adjacency matrix R denotesthe predicate vector q?i or q?j .
We discuss the selec-tion of ?
in section 4.1.?
:= G 7?
R (1)ArgumentNodesPredicateNodescanfishfoodcropflowersoilfruiteatcookharvestcultivateirrigateconsumeharvestconsumecook eat cultivate irrigatechickencropfood fruit flowercanchickenfishPredicate Projection Argument  ProjectionsoilFigure 1: Illustration of (R) the bipartitegraph of the verb-dobj-noun relation, (Q) thepredicate-projection monopartite graph, and (A)the argument-projection monopartite graph.Monopartite Graph Projection: In order toconduct random walk on the graph, we projectthe bipartite graph R onto a monopartite graphQ=(X,E) between the predicates, or A=(Y,E)between the arguments (Zhou et al, 2007).
Fig-ure 1 illustrates the intuition of the projection.
Thelinks in Q represent the indirect connects betweenthe predicates in R. Two predicates are connectedin Q if they share at least one common neighborargument in R. The weight of the links in Q couldbe set by arbitrary distance measures.
We refer Das an instance of the projection Q by a given dis-tance function ?.?
:= R 7?
D (2)Stochastic Walking Strategy: We introduce aprobability function ?
to transform the predicatedistances D into transition probabilities P .
WhereP is a stochastic matrix, with each element pijrepresents the transition probability from predicateqi to qj .
Generally speaking, nearby predicatesgain higher probabilities to be visited, while dis-tant predicates will be penalized.?
:= D 7?
P (3)1171Follow Equation 4, we aggregate over all ordersof the transition probabilities P as the final sta-tionary probabilities P?
.
According to the Perron-Frobenius theory, one can verify that it convergesto dP (I ?
dP )?1 when P is non-negative andregular matrix (Li et al, 2009).
Where t repre-sents the orders: the length of the path betweentwo nodes in terms of edges.
The damp factord ?
(0, 1), and its value mainly depends on the da-ta sparsity level.
Typically d prefers small valuessuch as 0.005.
It means higher order transitionsare much less reliable than lower orders (Liben-Nowell and Kleinberg, 2007).P?
=?
?t=1(dP )t|(dP )t| = dP (I ?
dP )?1 (4)Preference Propagation: in Equation 5, wecombine the converged transition probabilities P?with the measured preferences R as the propa-gation function: 1) for a given predicate, firstlyit transfers to all nearby predicates with designedprobabilities; 2) then it sums over the argumentspreferred by these predicates with quantified s-cores to get smoothed R?.
We further describe it-s configuration details in Section 4.4 and Equa-tion 12 with two propagation modes.R?
= P?
?R (5)4 Model Configurations4.1 Preference Ranking: Measure theSelectional PreferencesIn collaborative filtering, usually there are explic-it and scaled user ratings on their item prefer-ences.
For instance, a user ratings a movie witha score?
[0,10] on IMDB site.
But in SP, the pref-erences between the predicates and arguments areimplicit: their co-occurrence counts follow thepower law distribution and vary greatly.Therefore, we employ a ranking function ?
tomeasure the SP of the seen predicate-argumentpairs.
We suppose this could bring at least twobenefits: 1) a proper measure on the preferencescan make the discovering of nearby predicateswith similar preferences to be more accurate; 2)while propagation, we propagate the scored pref-erences, rather than the raw counts or condition-al probabilities, which could be more proper andagree with the nature of SP smooth.
We denoteSelPref(q, a) as Pr(q, a) for short.SelPref(q, a) = ?
(q, a) (6)Previous literatures have well studied on varioussmooth models for SP.
However, they vary great-ly on the measure of preferences.
It is still notclear how to do this best.
Lapata et al investigatethe correlations between the co-occurrence counts(CT) c(q, a), or smoothed counts with the humanplausibility judgements (Lapata et al, 1999; Lap-ata et al, 2001).
Some introduce conditional prob-ability (CP) p(a|q) for the decision of preferencejudgements (Chambers and Jurafsky, 2010; Erk etal., 2010; Se?aghdha, 2010).
Meanwhile, the point-wise mutual information (MI) is also employedby many researchers to filter out incorrect infer-ences (Pantel et al, 2007; Bergsma et al, 2008).
?CT = c(q, a) ?MI = logp(q, a)p(q)p(a)?CP =c(q, a)c(q, ?)
?TD = c(q, a)log(m|a|)(7)In this paper, we present an adjusted rankingfunction (AR) in Equation 8 to measure the SP ofseen predicate-argument pairs.
Intuitively, it mea-sures the preferences by combining both the pop-ularity and association, with parameters controlthe uncertainty of the trade-off between the two.We define the popularity as the joint probabilityp(q, a) based on MLE, and the association as MI.This is potentially similar to the process of humanplausibility judgements.
One may judge the plau-sibility of a predicate-argument collocation fromtwo sides: 1) if it has enough evidences and com-monly to be seen; 2) if it has strong associationaccording to the cognition based on kinds of back-ground knowledge.
This metric is also similar tothe TF-IDF (TD) used in information retrieval.
?AR(q, a) = p(q, a)?1( p(q, a)p(q)p(a))?2s.t.
?1 , ?2 ?
[0, 1](8)We verify if a metric is better by two tasks:1) how well it correlates with human plausibilityjudgements; 2) how well it helps with the smoothinference to disambiguate plausible and implausi-ble instances.
We conduct empirical experimentson these issues in Section 5.3 and Section 5.4.4.2 Distance Function: Projection of theMonopartite GraphIn Equation 9, the distance function ?
is used todiscover nearby predicates with distance dij .
Itweights the links on the monopartite graph Q. It1172guides the walker to transfer between predicates.We calculate ?
based on the vectors q?i, q?j repre-sented by the measured preferences in R.dij = ?
(q?i, q?j) (9)Where ?
can be distance functions such as Eu-clidean (norm) distance or Kullback-Leibler diver-gence (KL) etc., or one minus the similarity func-tions such as Jaccard and Cosine etc.
The selectionof distributional functions has been fully studiedby previous work (Lee, 1999; Erk et al, 2010).
Inthis paper, we do not focus on this issue due topage limits.
We simply use the Cosine function:?cosine(q?i, q?j) = 1 ?q?i ?
q?j?q?i??q?j?
(10)4.3 Probability Function: the Walk StrategyWe define the probability function ?
as Equa-tion 11.
Where the transition probability p(qj |qi)in P is defined as a function of the distance dijwith a parameter ?.
Intuitively, it means in a givenwalk step, a predicate qj which is far away fromqi will get much less probability to be visited, andqi has high probabilities to start walk from itselfand its nearby predicates to pursue good precision.Once we get the transition matrix P , we can com-pute P?
according to Equation 4.p(qj |qi) = ?
(dij) =(1 ?
dij)?Z(qi)s.t.
?
?
0, dij ?
[0, 1](11)Where the parameter ?
is used to control the bal-ance of nearby and distant predicates.
Z(qi) is thenormalize factor.
Typically, ?
around 2 can pro-duce good enough results in most cases.
We verifythe settings of ?
in section 5.3.2.4.4 Propagation FunctionThe propagation function in Equation 5 is repre-sented by the matrix form.
It can be expanded andrewritten as Equation 12.
Where p?
(qj |qi) is theconverged transition probability from predicate qito qj .
Pr(ak, qj) is the measured preference ofpredicate qj with argument ak.P?r(ak, qi) =m?j=1p?
(qj |qi) ?
Pr(ak, qj) (12)We employ two propagation modes (PropMode)for the preference propagation function.
One is?CP?
mode.
In this mode, we always set Pr(q, a)as the conditional probability p(a|q) for the prop-agation function, despite what ?
is used for thedistance function.
This mode is similar to previ-ous methods (Dagan et al, 1999; Keller and Lap-ata, 2003; Bergsma et al, 2008).
The other is ?PP?mode.
We set ranking function ?=Pr(q, a) alwaysto be the same in both the distance function and thepropagation function.
That means what we propa-gated is the designed and scored preferences.
Thiscould be more proper and agree with the natureof SP smooth.
We show the improvement of thisextension in section 5.3.1.5 Experiments5.1 Data SetGeneralization Data: We parsed the AgenceFrance-Presse (AFP) and New York Times (NYT)sections of the LDC English Gigaword corpo-ra (Parker et al, 2011), each from year 2001-2010.The parser is provided by the Stanford CoreNLPpackage1.
We filter out all tokens containingnon-alphabetic characters, collect the <verb-dobj-noun > triples from the syntactically analyzed da-ta.
Predicates (verbs) whose frequency lower than30 and arguments (noun headwords) whose fre-quency less than 5 are excluded out.
No other fil-ters have been done.
The resulting data consist of:?
AFP: 26, 118, 892 verb-dobj-noun observa-tions with 1, 918, 275 distinct triples, totally4, 771 predicates and 44, 777 arguments.?
NYT: 29, 149, 574 verb-dobj-noun observa-tions with 3, 281, 391 distinct triples, totally5, 782 predicates and 57, 480 arguments.Test Data: For pseudo-disambiguation, we em-ploy Penn TreeBank-3 (PTB) as the test data (Mar-cus et al, 1999)2.
We collect the 36, 400 manu-ally annotated verb-dobj-noun dependencies (with23, 553 distinct ones) from PTB.
We keep depen-dencies whose predicates and arguments are seenin the generalization data.
We randomly selec-t 20% of these dependencies as the test set.
Wesplit the test set equally into two parts: one as thedevelopment set and the other as the final test set.Human Plausibility Judgements Data: Weemploy two human plausibility judgements data1http://nlp.stanford.edu/software/corenlp.shtml2PTB includes 2, 499 stories from the Wall Street Journal(WSJ).
It is different with our two generalization data.1173for the correlation evaluation.
In each they col-lect a set of predicate-argument pairs, and anno-tate with two kinds of human ratings: one for anargument takes the role as the patient of a predi-cate, and the other for the argument as the agent.The rating values are between 1 and 7: e.g.
theyassign hunter-subj-shoot with a rating 6.9 but 2.8for shoot-dobj-hunter.?
PBP: Pado?
et al (2007) develop a set of hu-man plausibility ratings on the basis of thePenn TreeBank and FrameNet respectively.We refer PBP as their 212 patient ratingsfrom the Penn TreeBank.?
MRP: This data are originally contributed byMcRae et al (1998).
We use all their 723patient-nn ratings.Without explicit explanation, we remove all theselected PTB tests and human plausibility pairsfrom AFP and NYT to treat them unseen.5.2 Comparison MethodsSince RSP falls into the unsupervised distribu-tional approach, we compare it with previoussimilarity-based methods and unsupervised gener-ative topic model 3.Erk et al (Erk, 2007; Erk et al, 2010) arethe pioneers to address SP using similarity-basedmethod.
For a given (q, a) in relation r, the mod-el sums over the similarities between a and theseen headwords a?
?
Seen(q, r).
They investi-gated several similarity functions sim(a, a?)
suchas Jaccard, Cosine, Lin, and nGCM etc., and dif-ferent weighting functions wtq,r(a?
).S(q, r, a) =?a?wtq,r(a?)Zq,r?
sim(a, a?)
(13)For comparison, we suppose the primary cor-pus and generalization corpus in their model to bethe same.
We set the similarity function of theirmodel as nGCM, use both the FREQ and DISCRweighting functions.
The vector space is in SYN-PRIMARY setting with 2, 000 basis elements.Dagan et al (1999) propose state-of-the-artsimilarity based model for word co-occurrenceprobabilities.
Though it is not intended for SP, butit can be interpreted and rewritten for SP as:Pr(a|q) =?q?
?Simset(q)sim(q, q?
)Z(q) p(a|q?)
(14)3The implementation of RSP and listed previous methodsare available at https://github.com/ZhenhuaTian/RSPThey use the k-closest nearbys as Simset(q),with a parameter ?
to revise the similarity func-tion.
For comparison, we use the Jensen-Shannondivergence (Lin, 1991) which shows the best per-formance in their work as sim(q, q?
), and optimizethe settings of k and ?
in our experiments.LDA-SP: Another kind of sophisticated unsu-pervised approaches for SP are latent variablemodels based on Latent Dirichlet Allocation (L-DA).
O?
Se?aghdha (2010) applies topic modelsfor the SP induction with three variations: LDA,Rooth-LDA, and Dual-LDA; Ritter et al (2010)focus on inferring latent topics and their distribu-tions over multiple arguments and relations (e.g.,the subject and direct object of a verb).In this work, we compare with O?
Se?aghdha?soriginal LDA approach to SP.
We use the Mat-lab Topic Modeling Toolbox4 for the inferenceof latent topics.
The hyper parameters are set assuggested ?=50/T and ?=200/n, where T is thenumber of topics and n is the number of argu-ments.
We test T=100, 200, 300, each with 1, 000iterations of Gibbs sampling.5.3 Pseudo-DisambiguationPseudo-disambiguation has been used for SP e-valuation by many researchers (Rooth et al, 1999;Erk, 2007; Bergsma et al, 2008; Chambers andJurafsky, 2010; Ritter et al, 2010).
First the sys-tem removes a portion of seen predicate-argumentpairs from the generalization data to treat them asunseen positive tests (q, a+).
Then it introducesconfounder selection to create a pseudo negativetest (q, a?)
for each positive (q, a+).
Finally itevaluates a SP model by how well the model dis-ambiguates these positive and negative tests.Confounder Selection: for a given (q, a+), thesystem selects an argument a?
from the argumen-t vocabulary.
Then by ensure (q, a?)
is unseen inthe generalization data, it treats a?
as pseudo a?.This process guarantees that (q, a?)
to be negativein real case with very high probability.
Previouswork have made advances on confounder selec-tion with random, bucket and nearest confounder-s. Random confounder (RND) most closes to therealistic case; While nearest confounder (NER) isreproducible and it avoids frequency bias (Cham-bers and Jurafsky, 2010).In this work, we employ both RND and NERconfounders: 1) for RND, we randomly select4psiexp.ss.uci.edu/research/programs data/toolbox.htm1174confounders according to the occurrence probabil-ity of arguments.
We sample confounders on boththe development and final test data with 100 itera-tions.
2) for NER, firstly we sort the arguments bytheir frequency.
Then we select the nearest con-founders with two iterations.
One iteration selectsthe confounder whose frequency is more than orequal to a+, and the other iteration with frequencylower than or equal to a+.Evaluation Metric: we evaluate performanceon both the pairwise and pointwise settings:1) On pairwise setting, we combine correspond-ing (q, a+, a?)
together as test instances.
The per-formance is evaluated based on the accuracy (AC-C) metric.
It computes the portion of test instances(q, a+, a?)
which correctly predicted by the s-mooth model with score(q, a+) > score(q, a?
).We weight each instance equally for macroACC,and weight each by the frequency of the positivepair (q, a+) for microACC.2) On pointwise setting, we use each positivetest (q, a+) or negative test (q, a?)
as test in-stances independently.
We treat it as a binaryclassification task, and evaluate using the standardarea-under-the-curve (AUC) metric.
This metricis firstly employed for the SP evaluation by Ritteret al(2010).
For macroAUC, we weight each in-stance equally; for microAUC, we weight each byits argument frequency (Bergsma et al, 2008).Parameters Tuning: The parameters are tunedon the PTB development set, using AFP as thegeneralization data.
We report the overall perfor-mance on the final test set.
While using NYT asthe generalization data, we hold the same parame-ter settings as AFP to ensure the results are robust.Note that indeed the parameter settings would varyamong different generalization and test data.5.3.1 Verify Ranking Function andPropagation MethodThis experiment is conducted on the PTB devel-opment set with RND confounders.
We use AFPand NYT as the generalization data.
For compari-son, we set the distance function ?
as Cosine, withdefault d=0.005, and ?=1.In Table 1, the evaluation metric is Accuracy.The first 4 rows are the results of ?CP?
PropMode,and the latter 3 rows are the ?PP?
PropMode.
Withrespect to the ranking function ?, CP performsthe worst as it considers only the popularity ratherthan association.
The heavy bias on frequent pred-icates and arguments has two major drawbacks: a)The computation of predicate distances would re-ly much more on frequent arguments, rather thanthose arguments they preferred; b) While propaga-tion, it may bias more on frequent arguments, too.Even these frequent arguments are less preferredand not proper to be propagated.Crit.
AFP NYTmacro micro macro micro?CP 71.7 76.7 78.2 81.2?MI 70.9 75.8 79.1 81.8?TD 73.4 78.2 80.9 83.4?AR 72.9 77.8 81.0 83.5?MI 76.8 80.6 81.9 83.8?TD 74.4 79.1 81.8 84.2?AR 82.5 85.2 87.7 88.6Table 1: Comparing different ranking functions.For MI, it biases infrequent arguments withstrong association, without regarding to the popu-lar arguments with more evidences.
Furthermore,the generalization data is automatically parsed andkind of noisy, especially on infrequent predicatesand arguments.
The noises could yield unreliableestimations and decrease the performance.
For T-D, it outperforms MI method on ?CP?
PropMode,but it not always outperforms MI on ?PP?
Prop-Mode.
It is no surprise to find out the adjustedranking AR achieves better results on both AFPand NYT data, with ?1=0.2 and ?2=0.6.
Finally,it shows the ?PP?
mode, which propagating the de-signed preference scores, gains significantly betterperformance as discussed in Section 4.4.5.3.2 Verify ?
of the Probability FunctionThis experiment is conducted on the PTB develop-ment tests with both RND and NER confounders.The generalization data is AFP.0 0.5 1 1.5 2 2.5 3 3.5 4 4.57678808284868890accuracy (%)deltaRND macro accuracyRND micro accuracyNER macro accuracyNER micro accuracyFigure 2: Performance variation on different ?.1175CriterionAFP NYTRND NER RND NERmacro micro macro micro macro micro macro microErk et al FREQ 73.7 73.6 73.9 73.6 68.3 68.4 63.8 63.0Erk et alDISCR 76.0 78.3 79.1 78.1 83.3 84.2 82.4 82.6Dagan et al 80.6 82.8 84.7 85.0 87.0 87.6 86.9 87.3LDA-SP 82.0 83.5 83.7 82.9 89.1 89.0 87.9 87.8RSPnaive 72.6 76.4 79.4 81.1 78.5 80.4 74.8 78.0+Rank 74.0 77.7 83.5 85.2 81.4 83.1 84.5 86.9+Rank+PP 83.5 85.2 87.2 87.0 88.2 88.2 88.0 88.3+Rank+PP+Delta 86.2 87.3 88.4 88.1 90.6 90.1 91.1 89.3Table 2: Pseudo-disambiguation results of different smooth models.
Macro and micro Accuracy.0 0.2 0.4 0.6 0.8 100.20.40.60.81False Positive (FP)TruePositive(TP)Erk et al      macroAUC=0.72Dagan et al macroAUC=0.80LDA?SP       macroAUC=0.77RSP?ALL     macroAUC=0.840 0.2 0.4 0.6 0.8 100.20.40.60.81False Positive (FP)TruePositive(TP)Erk et al      microAUC=0.62Dagan et al microAUC=0.83LDA?SP       microAUC=0.73RSP?ALL     microAUC=0.89Figure 3: Marco and micro ROC curves of different smooth models.We set the ranking function ?
as AR (withtuned ?1=0.2 and ?2=0.6), the distance function?
as Cosine, default d=0.005, and we restrict ?
?
[0.5, 4].
Figure 2 shows ?
has significant impacton the performance.
Starting from ?=0.5, the sys-tem gains better performance while ?
increasing.It achieves good results around ?=2.
This mean-s for a given predicate, the penalty on its distantpredicates helps to get more accurate smooth.
Theperformance will drop if ?
becomes too big.
Thismeans closest predicates are useful for smooth.
Itit not better to penalize them heavily.5.3.3 Overall PerformanceFinally we compare the overall performance of d-ifferent models.
We report the results on the PTBfinal test set, with RND and NER confounders.Table 2 shows the overall performance on Accu-racy metric.
Among previous methods in the first4 rows, LDA-SP performs the best in most cas-es.
In the last 4 rows, RSPnaive means both theranking function and PropMode are set as ?CP?and ?=1.
This configuration yields poor perfor-mance.
Iteratively, by employing the adjustedranking function, smoothing with preference prop-agation method, and revising the probability func-tion with the parameter ?, RSP outperforms allprevious methods.
The parameter settings of RSP-All are ?1=0.2, ?2=0.6, ?=1.75 and d=0.005.Figure 3 show the macro (left) and micro (right)receiver-operating-characteristic (ROC) curves ofdifferent models, using AFP as the generalizationdata and RND confounders.
For each kind ofprevious methods, we show the best AUC theyachieved.
RASP-All still performs the best onthe terms of AUC metric, achieving macroAUCat 84% and microAUC at 89%.
We also verifiedthe AUC metric using NYT as the generalizationdata.
The results are similar to the AFP data.
Itis also interesting to find out that the ACC met-ric is not always bring into correspondence withthe AUC metric.
The difference mainly raise onthe pointwise and pairwise test settings of pseudo-disambiguation.5.4 Human Plausibility JudgementsWe conduct empirical studies on the correla-tions between different preference ranking func-1176CriterionAFP NYTSpearman?s ?
Kendall?s ?
Spearman?s ?
Kendall?s ?PBP MRP PBP MRP PBP MRP PBP MRPCT 0.49 0.36 0.37 0.28 0.54 0.44 0.41 0.34CP 0.47 0.39 0.35 0.30 0.51 0.48 0.39 0.37MI 0.56 0.39 0.43 0.31 0.54 0.49 0.41 0.38TD 0.53 0.36 0.39 0.28 0.56 0.45 0.42 0.34AR 0.58 0.40 0.44 0.31 0.58 0.50 0.44 0.39Erk et al FREQ 0.30 0.08 0.22 0.06 0.25 0.09 0.18 0.06Erk et alDISCR 0.06 0.21 0.04 0.15 0.16 0.23 0.11 0.16Dagan et al 0.32 0.24 0.24 0.18 0.46 0.29 0.34 0.21LDA-SP 0.31 0.32 0.23 0.23 0.38 0.38 0.28 0.28LDA-SP+Bayes 0.39 0.25 0.30 0.18 0.40 0.32 0.30 0.23RSP-All 0.46 0.31 0.34 0.23 0.53 0.38 0.40 0.28Table 3: Correlation results on the human plausibility judgements data.tions and human ratings.
Follow Lapata etal.
(2001), we first collect the co-occurrencecounts of predicate-argument pairs in the humanplausibility data from AFP and NYT (before re-moving them as unseen pairs).
Then we scorethem with different ranking functions (describedin Section 4.1) based on MLE.
Inspired by Erk etal.
(2010), we do not suppose linear correlationsbetween the estimated scores and human ratings.We use the Spearman?s ?
and Kendal?s ?
rankcorrelation coefficient.We also compare the correlations between thesmoothed scores of different models with humanratings.
With respect to upper bounds, Pado?
etal.
(2007) suggest that the typical agreement ofhuman participants is around a correlation of 0.7on their plausibility data.
We hold that automaticmodels of plausibility can not be expected to sur-pass this upper bound.In Table 3, all coefficients are verified at signif-icant level p<0.01.
The first 5 rows are the corre-lations between the preference ranking function-s and human ratings based on MLE.
On both thePBP and MRP data, the proposed AR metric bettercorrelates with human ratings than others, with ?2>0.5 and ?1 around [0.2, 0.35].
The latter 6 rowsare the results of smooth models.
It shows LDA-SP performs good correlation with human ratings,where LDA-SP+Bayes refers to the Bayes predic-tion method of Ritter et al (2010).
RSP modelgains the best correlation on the two plausibilitydata in most cases, where the parameter settingsare the same as pseudo-disambiguation.6 Conclusions and Future WorkIn this work we present an random walk approachto SP.
Experiments show it is efficient and effec-tive to address data sparsity for SP.
It is also flex-ible to be applied to new data.
We find out that aproper measure on SP between the predicates andarguments is important for SP.
It helps with thediscovering of nearby predicates and it makes thepreference propagation to be more accurate.
An-other issue is that it is not good enough to direct-ly applies the similarity or distance functions forsmooth.
Potential future work including but notlimited to follows: investigate argument-orientedand personalized random walk, extend the modelin heterogenous network with multiple link types,discover soft clusters using random walk for se-mantic induction, and combine it with discrimina-tive learning approach etc.AcknowledgmentsThe research is supported in part by the Na-tional High Technology Research and Devel-opment Program 863 of China under GrantNo.2012AA011003; Key Projects in the Nation-al Science and Technology Pillar Program underGrant No.2011BAK08B02; Chinese GovernmentGraduate Student Overseas Study Program spon-sored by the China Scholarship Council (CSC).We also gratefully acknowledge the anonymousreviewers for their helpful comments.1177ReferencesEneko Agirre and David Martinez.
2001.
Learningclass-to-class selectional preferences.
In Proceed-ings of the 2001 workshop on Computational Natu-ral Language Learning.Shane Bergsma, Dekang Lin, and Randy Goebel.2008.
Discriminative learning of selectional pref-erence from unlabeled text.
In EMNLP.Carsten Brockmann and Mirella Lapata.
2003.
Evalu-ating and combining approaches to selectional pref-erence acquisition.
In EACL.Nathanael Chambers and Dan Jurafsky.
2010.
Improv-ing the use of pseudo-words for evaluating selection-al preferences.
In ACL.Massimiliano Ciaramita and Mark Johnson.
2000.
Ex-plaining away ambiguity: Learning verb selectionalpreference with bayesian networks.
In COLING.Stephen Clark and David J. Weir.
2002.
Class-basedprobability estimation using a semantic hierarchy.Computational Linguistics, 28(2):187?206.Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.1999.
Similarity-Based Models of Word Cooccur-rence Probabilities.
Machine Learning, 34:43?69.Katrin Erk, Sebastian Pado?, and Ulrike Pado?.
2010.
Aflexible, corpus-driven model of regular and inverseselectional preferences.
Computational Linguistics,36(4):723?763.Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.
In ACL.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Jerrold J. Katz and Jerry A. Fodor.
1963.
The structureof a semantic theory.
Language, 39(2):170?210.Frank Keller and Mirella Lapata.
2003.
Using the webto obtain frequencies for unseen bigrams.
Computa-tional Linguistics, 29(3):459?484.Maria Lapata, Scott McDonald, and Frank Keller.1999.
Determinants of adjective-noun plausibility.In EACL, pages 30?36.
Association for Computa-tional Linguistics.Maria Lapata, Frank Keller, and Scott McDonald.2001.
Evaluating smoothing algorithms againstplausibility judgements.
In ACL, pages 354?361.Association for Computational Linguistics.Lillian Lee.
1999.
Measures of distributional similar-ity.
In ACL, pages 25?32, Stroudsburg, PA, USA.Association for Computational Linguistics.Hang Li and Naoki Abe.
1998.
Generalizing caseframes using a thesaurus and the mdl principle.Computational linguistics, 24(2):217?244.Ming Li, Benjamin M Dias, Ian Jarman, Wael El-Deredy, and Paulo JG Lisboa.
2009.
Grocery shop-ping recommendations based on basket-sensitiverandom walk.
In SIGKDD, pages 1215?1224.ACM.David Liben-Nowell and Jon Kleinberg.
2007.
Thelink-prediction problem for social networks.
Jour-nal of the American society for information scienceand technology, 58(7):1019?1031.Jianhua Lin.
1991.
Divergence measures based on theshannon entropy.
IEEE Transactions on InformationTheory, 37(1):145?151.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor.
1999.
Treebank-3.Diana McCarthy, Sriram Venkatapathy, and Aravind K.Joshi.
2007.
Detecting compositionality of verb-object combinations using selectional preferences.In EMNLP-CoNLL.Ken McRae, Michael J. Spivey-Knowltonb, andMichael K. Tanenhausc.
1998.
Modeling the influ-ence of thematic fit (and other constraints) in on-linesentence comprehension.
Journal of Memory andLanguage, 38(3):283?312.Sebastian Pado?, Ulrike Pado?, and Katrin Erk.
2007.Flexible, corpus-based modelling of human plausi-bility judgements.
In EMNLP/CoNLL, volume 7.Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,Timothy Chklovski, and Eduard Hovy.
2007.
Is-p: Learning inferential selectional preferences.
InNAACL-HLT.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English gigaword fifth edi-tion.Philip Resnik.
1993.
Selection and information: aclass-based approach to lexical relationships.
IRCSTechnical Reports Series.Philip Resnik.
1996.
Selectional constraints: Aninformation-theoretic model and its computationalrealization.
Cognition, 61(1):127?159.Philip Resnik.
1997.
Selectional preference and sensedisambiguation.
In Proceedings of the ACL SIGLEXWorkshop on Tagging Text with Lexical Semantics:Why, What, and How.
Washington, DC.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A la-tent dirichlet alocation method for selectional pref-erences.
In ACL.Mats Rooth, Stefan Riezler, Detlef Prescher, GlennCarroll, and Franz Beil.
1999.
Inducing a semanti-cally annotated lexicon via em-based clustering.
InACL.Diarmuid O?
Se?aghdha.
2010.
Latent variable modelsof selectional preference.
In ACL.1178Irena Spasic?
and Sophia Ananiadou.
2004.
Us-ing automatically learnt verb selectional preferencesfor classification of biomedical terms.
Journal ofBiomedical Informatics, 37(6):483?497.Kristina Toutanova, Christopher D. Manning, DanFlickinger, and Stephan Oepen.
2005.
Stochas-tic hpsg parse disambiguation using the redwood-s corpus.
Research on Language & Computation,3(1):83?105.Yorick Wilks.
1973.
Preference semantics.
Technicalreport, DTIC Document.Hilmi Yildirim and Mukkai S. Krishnamoorthy.
2008.A random walk method for alleviating the sparsityproblem in collaborative filtering.
In Proceedings ofthe 2008 ACM conference on Recommender system-s, pages 131?138.
ACM.Tao Zhou, Jie Renan, Matu?s?
Medo, and Yi-ChengZhang.
2007.
Bipartite network projection andpersonal recommendation.
Physical Review E,76(4):046115.1179
