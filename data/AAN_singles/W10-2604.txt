Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 23?30,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsExploring Representation-Learning Approaches to Domain AdaptationFei Huang and Alexander YatesTemple UniversityComputer and Information Sciences324 Wachman HallPhiladelphia, PA 19122{fei.huang,yates}@temple.eduAbstractMost supervised language processing sys-tems show a significant drop-off in per-formance when they are tested on textthat comes from a domain significantlydifferent from the domain of the trainingdata.
Sequence labeling systems like part-of-speech taggers are typically trained onnewswire text, and in tests their errorrate on, for example, biomedical data cantriple, or worse.
We investigate techniquesfor building open-domain sequence label-ing systems that approach the ideal of asystem whose accuracy is high and con-stant across domains.
In particular, we in-vestigate unsupervised techniques for rep-resentation learning that provide new fea-tures which are stable across domains, inthat they are predictive in both the train-ing and out-of-domain test data.
In exper-iments, our novel techniques reduce errorby as much as 29% relative to the previousstate of the art on out-of-domain text.1 IntroductionSupervised natural language processing (NLP)systems exhibit a significant drop-off in perfor-mance when tested on domains that differ fromtheir training domains.
Past research in a vari-ety of NLP tasks, like parsing (Gildea, 2001) andchunking (Huang and Yates, 2009), has shown thatsystems suffer from a drop-off in performance onout-of-domain tests.
Two separate experimentswith part-of-speech (POS) taggers trained on WallStreet Journal (WSJ) text show that they can reachaccuracies of 97-98% on WSJ test sets, but achieveaccuracies of at most 90% on biomedical text(R.Codena et al, 2005; Blitzer et al, 2006).The major cause for poor performance on out-of-domain texts is the traditional representationused by supervised NLP systems.
Most systemsdepend to varying degrees on lexical features,which tie predictions to the words observed ineach example.
While such features have been usedin a variety of tasks for better in-domain perfor-mance, they are pitfalls for out-of-domain tests fortwo reasons: first, the vocabulary can differ greatlybetween domains, so that important words in thetest data may never be seen in the training data.And second, the connection between words andlabels may also change across domains.
For in-stance, ?signaling?
appears only as a present par-ticiple (VBG) in WSJ text (as in, ?signaling that...?
), but predominantly as a noun (as in ?signalingpathway?)
in biomedical text.Representation learning is a promising new ap-proach to discovering useful features that are sta-ble across domains.
Blitzer et al (2006) and ourprevious work (2009) demonstrate novel, unsu-pervised representation learning techniques thatproduce new features for domain adaptation of aPOS tagger.
This framework is attractive for sev-eral reasons: experimentally, learned features canyield significant improvements over standard su-pervised models on out-of-domain tests.
Sincethe representation learning techniques are unsu-pervised, they can be applied to arbitrary new do-mains to yield the best set of features for learningon WSJ text and predicting on the new domain.There is no need to supply additional labeled ex-amples for each new domain.
This reduces the ef-fort for domain adaptation, and makes it possibleto apply systems to open-domain text collectionslike the Web, where it is prohibitively expensiveto collect a labeled sample that is truly representa-tive of all domains.Here we explore two novel directions in therepresentation-learning framework for domainadaptation.
Specifically, we investigate empiri-cally the effects of representation learning tech-niques on POS tagging to answer the following:231.
Can we produce multi-dimensional represen-tations for domain adaptation?
Our previous ef-forts have provided only a single new feature inthe learned representations.
We now show howwe can perform a multi-dimensional clusteringof words such that each dimension of the clus-tering forms a new feature in our representation;such multi-dimensional representations dramati-cally reduce the out-of-domain error rate of ourPOS tagger from 9.5% to 6.7%.2.
Can maximum-entropy models be used to pro-duce representations for domain adaptation?
Re-cent work on contrastive estimation (Smith andEisner, 2005) has shown that maximum-entropy-based latent variable models can yield more accu-rate clusterings for POS tagging than more tradi-tional generative models trained with Expectation-Maximization.
Our preliminary results show thatsuch models can be used effectively as represen-tations for domain adaptation as well, matchingstate-of-the-art results while using far less data.The next section provides background informa-tion on learning representations for NLP tasks us-ing latent-variable language models.
Section 3 de-scribes our experimental setup.
In Sections 4 and5, we empirically investigate our two questionswith a series of representation-learning methods.Section 6 analyzes our best learned representationto help explain its effectiveness.
Section 7 presentsprevious work, and Section 8 concludes and out-lines directions for future work.2 Open-Domain Sequence Labeling byLearning RepresentationsLet X be an instance set for a learning problem;for POS tagging, for instance, this could be the setof all English sentences.
Let Y be the space ofpossible labels for an instance, and let f : X ?
Zbe the target function to be learned.
A represen-tation is a function R: X ?
Y , for some suitablefeature space Y (such as Rd).
A domain is definedas a distribution D over the instance set X .
Anopen-domain system observes a set of training ex-amples (R(x), f(x)), where instances x ?
X aredrawn from a source domain, to learn a hypothe-sis for classifying examples drawn from a separatetarget domain.Previous work by Ben-David et al (2007) usesVapnik-Chervonenkis (VC) theory to show thatthe choice of representation is crucial to open-domain learning.
As is customary in VC the-ory, a good choice of representation must allowa learning machine to achieve low error rates dur-ing training.
Just as important, however, is thatthe representation must simultaneously make thesource and target domains look as similar to oneanother as possible.For open-domain sequence-labeling, then, thetraditional representations are problematic.
Typ-ical representations in NLP use functions of thelocal context to produce features.
Although manyprevious studies have shown that such lexicalfeatures allow learning systems to achieve im-pressively low error rates during training, theyalso make texts from different domains look verydissimilar.
For instance, a sentence containing?bank?
is almost certainly from the WSJ ratherthan biomedical text; a sentence containing ?path-way?
is almost certainly from a biomedical textrather than from the WSJ.Our recent work (2009) shows how to buildsystems that learn new representations for open-domain NLP using latent-variable language mod-els like Hidden Markov Models (HMMs).
In POS-tagging and chunking experiments, these learnedrepresentations have proven to meet both of Ben-David et al?s criteria for representations.
Theyhelp discriminate among classes of words, sinceHMMs learn distributional similarity classes ofwords that often correlate with the labels that needto be predicted.
Moreover, it would be difficult totell apart two domains based on the set of HMMstates that generated the texts, since a given HMMstate may generate words from any number of do-mains.In the rest of this paper, we investigate ways toimprove the predictive power of the learned rep-resentations, without losing the essential propertythat the features remain stable across domains.
Westay within the framework of using graphical mod-els to learn representations, and demonstrate sig-nificant improvements on our original technique.3 Experimental SetupWe use the same experimental setup as Blitzeret al (2006): the Penn Treebank (Marcus et al,1993) Wall Street Journal portion for our labeledtraining data; 561 MEDLINE sentences (9576words) from the Penn BioIE project (PennBioIE,2005) for our labeled test set; and all of the un-labeled text from the Penn Treebank WSJ portionplus Blitzer et al?s MEDLINE corpus of 71,30624unlabeled sentences to train our latent variablemodels.
The two texts come from two very dif-ferent domains, making this data a tough test fordomain adaptation.
23% of the word types in thetest text are Out-Of-Vocabulary (OOV), meaningthat they are never observed in the training data.We use a number of unsupervised representa-tion learning techniques to discover features fromour unlabeled data, and a supervised classifier totrain on the training set annotated with learned fea-tures.
We use an open source Conditional RandomField (CRF) (Lafferty et al, 2001) software pack-age1 designed by Sunita Sajarwal and William W.Cohen to implement our supervised models.
Werefer to the baseline system with feature set fol-lowing our previous work (2009) as PLAIN-CRF.Our learned features will supplement this set.For comparison, we also report on the perfor-mance of Blitzer et al?s Structural Correspon-dence Learning (SCL) (2006), our HMM-basedmodel (2009)(HY09), and two other baselines:?
TEST-CRF: Our baseline model, trained andtested on the test data.
This is our upperbound.?
SELF-CRF: Following the self-trainingparadigm (e.g., (McClosky et al, 2006b;McClosky et al, 2006a)), we train ourbaseline first on the training set, then apply itto the test set, then retrain it on the trainingset plus the automatically labeled test set.We perform only one iteration of retraining,although in general multiple iterations arepossible, usually with diminishing marginalreturns.4 Multi-dimensional RepresentationsFrom a linguistic perspective, words are multi-dimensional objects.
For instance, the word ?we?in ?We like doing domain adaptation research?
is apronoun, a subject, first person, and plural, amongother things.
Each of these properties is a sepa-rate feature of this word, which can be changedwithout changing the other features.
For exam-ple, if ?we?
is changed to ?they?
in the previ-ous example, it is exactly the same as ?we?
inall aspects, except that it is third person; if ?we?is changed to ?us?, then it changes from subjectcase to object case.
In morphologically rich lan-guages, many syntactic distinctions are marked in1Available from http://sourceforge.net/projects/crf/the surface forms of words; in more analytic orisolating languages like English, the distinctionsare still there, but must often be inferred from con-text rather than word form.
Beyond syntactic di-mensions, numerous semantic properties can alsodistinguish words, such as nouns that refer to cog-nitive agents versus nouns that refer to materialsand tools.We seek to learn multidimensional representa-tions of words.
Our HMM-based model is able tocategorize words in one dimension, by assigninga single HMM latent state to each word.
Sincethe HMM is trained on unlabeled data, this di-mension may partially reflect POS categories, butmore likely represents a mixture of many differentword dimensions.
By adding in multiple hiddenlayers to our sequence model, we aim to learn amulti-dimensional representation that may help usto capture word features from multiple perspec-tives.
The supervised CRF system can then sortout which dimensions are relevant to the sequence-labeling task at hand.A Factorial HMM (FHMM) can be used tomodel multiple hidden dimensions of a word.However, the memory requirements of an FHMMincrease exponentially with the number of lay-ers in the graphical model, making it hard to use(see Table 1).
Although other parameterizationsmay require much less memory, like using a log-linear output distribution conditioned on the fac-tors, exact inference is still computationally in-tractable; exploring FHMMs with approximate in-ference and learning is an interesting area for fu-ture work.
Here, we choose to create severalsingle-layer HMMs separately.
Figure 1 showsour Independent-HMM model (I-HMM).
I-HMMhas several copies of the observation sequence andeach copy is associated with its own hidden labelsequence.
To encourage each layer of the I-HMMmodel to find a different local maximum in pa-rameter space during training (and thus a differentmodel of the observation sequence), we initializethe parameters randomly.Suppose there are L independent layers in an I-HMM model for corpus x = (x1, .
.
.
, xN ), andeach layer is (yl1,yl2,...ylN ), where l = 1...L andeach y can have K states.
The distribution of thecorpus and one hidden layer l isP (x,yl) =?iP (xi|yli)P (yli|yli?1)For each layer l, for each position i, each HMM25XN??
?XNX2 XNX1 X2X1X1X2Y11Y12Y1LY21Y22Y2L YNLYN2YN1Figure 1: Graphical models of an Independent HiddenMarkov Model.
The dash line rectangle indicates that theyare copies of the observation sequenceModel Number of Memorylayers words statesHMM 1 W K O(WK + K2)FHMM L W K O(WKL + LK2)I-HMM L W K O(WKL + LK2)Table 1: The memory requirement for HMM, FHMM, andI-HMM models.state y and each POS tag z, we add a new booleanfeature to our CRF system that indicates whetherY li = y and Zi=z.We experiment with two versions of I-HMM:first, we fix the number of states in each layer at80 states, and increase the number of HMM lay-ers from 1 to 8 (I-HMM(80)).
Second, to providegreater encouragement for each layer to representseparate information, we vary the number of statesin each layer (I-HMM(vary)).
The detailed config-uration for this model is shown in Table 2.The results for our two models are shown in Fig-ure 2.
We can see that the accuracy of I-HMM(80)model keeps increasing from 90.5% to 93.3% until7 layers of HMM features (we call this 7-layer rep-resentation I-HMM*).
This is a dramatic 29% de-crease in the best reported error rate for this datasetwhen no labeled data from the biomedical domainis used.
Unlike with an FHMM, there is no guar-antee that the different layers of an I-HMM willmodel different aspects of the observation signal,but our results indicate that for at least several lay-ers, the induced models are complementary.
After7 layers, results begin to decrease, most likely be-cause the added layer is no longer complementaryto the existing latent-variable models and is caus-ing the supervised CRF to overfit the training data.For the I-HMM(vary) model with up to 5 lay-Number Number of Statesof Layers in each Layer1 102 10 203 10 20 404 10 20 40 605 10 20 40 60 80Table 2: The configuration of HMM layers and HMM statesfor the I-HMM(vary) model8687888990919293941 2 3 4 5 6 7 8AccuracyNumber of HMM layersAccuracy on different number ofHMM layersI-HMM(80)I-HMM(vary)HY09(90.5%)Figure 2: Our best multi-dimensional smoothed-HMM tag-ger with 7 layers reaches 93.3% accuracy, a drop of nearly 3%in the error rate from the previous state of the art (HY09).ers, the accuracy is not as good as I-HMM(80), al-though the 5-layer model still outperforms HY09.Individually, HMM models with fewer than 80states perform worse than the 80-state model (amodel with 40 states achieved 89.4% accuracy,and a model with 20 states achieved 88.9%).
Wehad hoped that by using layers with differentnumbers of states, we could force the layers tolearn complementary models, but the results indi-cate that any benefit from complementarity is out-weighed by the lower performance of the individ-ual layers.5 Learning Representations withContrastive EstimationIn recent years, many NLP practitioners have be-gun using discriminative models, and especiallymaximum-entropy-based models like CRFs, be-cause they allow the modeler to incorporate ar-bitrary, interacting features of the observation se-quence while still providing tractable inference.To see if the same benefit can carry over to our rep-resentation learning, we aim to build maximum-entropy-based linear-chain models that, unlike26most discriminative models, train on unannotateddata.
We follow Smith and Eisner (2005) intraining our models using a technique called con-trastive estimation, which we explain below.
Wecall the resulting model the Smith and EisnerModel (SEM).The key to SEM is that the contrastive estima-tion training procedure forces the model to explainwhy the given training data are better than per-turbed versions of the data, called neighbor points.For example, the sentence ?We like doing domainadaptation research?
is a valid sentence, but if weswitched ?like?
and ?doing?, the new sentence?We doing like domain adaptation research?
is notvalid.
SEM learns a model of the original sen-tence by contrasting it with the invalid neighborsentences.Let ~x =< x1, x2, ..., xN > be the observed ex-ample sentences, and let Y be the space of possiblehidden structures for xi.
Let N (xi) be a ?neigh-borhood?
for xi, or a set of negative examples ob-tained by perturbing xi, plus xi itself.
Given a vec-tor of feature functions ~f(x, y), SEM tries to finda set of weights ~?
that maximize a log-likelihoodfunction:LN (~?)
= log?i?y?Y u(xi, y|~?)?
(x,y)?N (xi)?Y u(x, y|~?
)where u(x, y|~?)
= exp(~?
?
~f(x, y)) is the ?un-normalized probability?
of an (example, hiddenstructure) pair (x,y).
Following Smith and Eisner,we use the best performing neighborhood, calledTRANS1, to conduct our experiments.
TRANS1is the set of sentences resulting from transposingany pair of adjacent words for any given trainingexample.The base feature space for SEM includes twokinds of boolean features analogous to HMMemission and transition probabilities.
For an ob-servation sequence x1, .
.
.
, xT and a label se-quence y1, .
.
.
, yT , a boolean emission feature in-dicates whether xt = x and yt = y for all possiblet, x, and y.
A boolean transition feature indicateswhether yt?1 = y and yt = y?
for all possible t, y,and y?.Because contrastive estimation is a computa-tionally expensive training procedure, we take twosteps to reduce the computational cost: we reducethe unlabeled data set, and we prune the featureset of SEM.
For our training data, we use only thesentences with length less than or equal to 10.
Wealso get rid of punctuation and the correspondingtags, change all words to lowercase and change allnumbers into a single symbol.To reduce the feature space, we create a tag-ging dictionary from Penn Treebank sections 02-21: for every word in these sections, the dictionaryrecords the set of POS tags that were ever asso-ciated with that word.
We then prune the emis-sion features for words that appear in this dic-tionary to include only the features that associatewords with their corresponding POS tags in thedictionary.
For the words that don?t appear in thePenn Treebank, they are associated with all pos-sible POS tags.
This procedure reduces the totalnumber of features in our SEM model from over500,000 to just over 60,000.After we train the model, we use a Viterbi-likealgorithm to decode it on the testing set.
Unlikethe HMM model, the decoded states of SEM arealready meaningful POS tags, so we can use thesedecoded states as POS tags (PLAIN-SEM), or usethem as features for a CRF model (SEM-CRF).We show the result of both models, as well asseveral comparison models, in Table 3.
From theresult, we can see that the unsupervised PLAIN-SEM outperforms the supervised PLAIN-CRF onboth all words and OOV words.
This impres-sive performance results from its ability to adaptto the new domain through the unlabeled train-ing examples and the contrastive estimation train-ing procedure.
In addition, the SEM-CRF modelsignificantly outperforms the SCL model (88.9%)and the HMM-based CRF with 40 hidden states(89.4%) while using only 36 hidden states, al-though it does not quite reach the performanceof HY09.
These results, which use a subset ofthe available unlabeled training text, suggest thatmaximum-entropy-style representation learning isa promising area for further investigation.6 AnalysisAs we mention in Section 2, the choice of repre-sentation is crucial to open-domain learning.
InSections 4 and 5, we demonstrate empirically thatlearned representations based on latent-variablegraphical models can significantly improve the ac-curacy of a POS tagger on a new domain, com-pared with using the traditional word-level repre-sentations.
We now examine our best representa-tion, I-HMM*, in light of the theoretical predic-tions made by VC theory.27All OOVModel words wordsPLAIN-CRF 88.3 67.3SELF-CRF 88.5 70.4PLAIN-SEM 88.5 69.8SCL 88.9 72.0SEM-CRF 90.0 71.9HY09 90.5 75.2I-HMM* 93.3 76.3TEST-CRF 98.9 NATable 3: SEM-CRF reduces error compared withSCL by 1.1% on all words; I-HMM* closes 33%of the gap between the state-of-the-art HY09 andthe upper-bound, TEST-CRF.In particular, Ben-David et al?s analysis showsthat the distance between two domains under arepresentation R of the data is crucial to domainadaptation.
However, their analysis depends ona particular notion of distance, the H-divergence,that is computationally intractable to calculate.For our analysis, we resort instead to a crudebut telling approximation of this measure, using amore standard notion of distance: Jensen-ShannonDivergence (DJS).To calculate the distance between domains un-der a representation R, we represent a domain Das a multinomial probability distribution over theset of features in R. We take maximum-likelihoodestimates of this distribution using our samplesfrom the WSJ and MEDLINE domains.
We thenmeasure the Jensen-Shannon Divergence betweenthe two distributions, which for discrete distribu-tions is calculated asDJS(p||q) =12?i[pilog(pimi)+ qilog(qimi)]where m = p+q2 .Figure 3 shows the divergence between thesetwo domains under purely lexical features, and un-der only HMM-based features.
OOV words makeup a substantial portion of the divergence betweenthe two domains under the lexical representation,but even if we ignore them the HMM features aresubstantially less variable across the two domains,which helps to explain their ability to provide su-pervised classifiers with stable features for domainadaptation.
Because there are so few HMM statescompared with the number of word types, there isno such thing as an OOV HMM state, and the word00.050.10.150.20.250.30.350.40.450.5Words I-HMM* StatesDistancebetweenDomainsOOVNon-OOVFigure 3: The Jensen-Shannon Divergence be-tween the newswire domain and the biomedicaldomain, according to a word-based representationof the domains and a HMM-based representation.The portion of the distance that is due to wordswhich appear in the biomedical domain but not thenewswire domain is shown in gray.states that appear in training data appear roughlyas often in test data.
This means that any asso-ciations that the CRF might learn between HMMstates and predicted outcomes is likely to remainuseful on the test data, but associations betweenwords and outcomes are less likely to be useful.7 Previous WorkPrevious work on artificial neural networks(ANNs) (Fahlman and Lebiere, 1990) has shownthat it is possible to learn effectively by addingmore hidden units to the neural network that cor-relate with the residual error of the existing hiddenunits (Cascade-Correlation learning).
Like our I-HMM technique, this work aims to build a multi-dimensional model, and it is capable of learningthe number of appropriate dimensions.
Unlikethe ANN scenario, our multi-dimensional learn-ing techniques must handle unlabeled data, andthey rely on the sequential structure of languageto learn effectively, whereas Cascade-Correlationlearning assumes samples are independent andidentically distributed.
Our techniques do not (yet)automatically determine the best number of layersin the model.Unlike our techniques for domain adaptation, inmost cases researchers have focused on the sce-nario where labeled training data is available inboth the source and the target domain (e.g., (Bac-chiani et al, 2006; Daume?
III, 2007; Chelba and28Acero, 2004; Daume?
III and Marcu, 2006; Blitzeret al, 2007)).
Our techniques use only raw textfrom the target domain.
This reduces the costof domain adaptation and makes the techniquesmore widely applicable to new domains like webprocessing, where the domain and vocabulary ishighly variable, and it is extremely difficult toobtain labeled data that is representative of thetest distribution.
When labeled target-domain datais available, instance weighting and similar tech-niques can potentially be used in combination withour techniques to improve our results further.Several researchers have previously studiedmethods for using unlabeled data for sequence la-beling, either alone or as a supplement to labeleddata.
Ando and Zhang develop a semi-supervisedchunker that outperforms purely supervised ap-proaches on the CoNLL 2000 dataset (Ando andZhang, 2005).
Recent projects in semi-supervised(Toutanova and Johnson, 2007) and unsupervised(Biemann et al, 2007; Smith and Eisner, 2005)tagging also show significant progress.
HMMshave been used many times for POS tagging insupervised, semi-supervised, and in unsupervisedsettings (Banko and Moore, 2004; Goldwater andGriffiths, 2007; Johnson, 2007).
The REALM sys-tem for sparse information extraction has also usedunsupervised HMMs to help determine whetherthe arguments of a candidate relation are of theappropriate type (Downey et al, 2007).
Schu?tze(1994) has presented an algorithm that categorizesword tokens in context instead of word types fortagging words.
We take a novel perspective on theuse of unsupervised latent-variable models by us-ing them to compute features of each token thatrepresent the distribution over that token?s con-texts.
These features prove to be highly usefulfor supervised sequence labelers in out-of-domaintests.In the deep learning (Bengio, 2009) paradigm,researchers have investigated multi-layer latent-variable models for language modeling, amongother tasks.
While n-gram models have tradition-ally dominated in language modeling, two recentefforts develop latent-variable probabilistic mod-els that rival and even surpass n-gram models inaccuracy (Blitzer et al, 2005; Mnih and Hinton,2007).
Several authors investigate neural networkmodels that learn a vector of latent variables torepresent each word (Bengio et al, 2003; Emamiet al, 2003; Morin and Bengio, 2005).
And facto-rial Hidden Markov Models (Ghahramani and Jor-dan, 1997) are a multi-layer variant of the HMMthat has been used in speech recognition, amongother things.
We use simpler mixtures of single-layer models for the sake of memory-efficiency,and we use our models as representations in a su-pervised task, rather than as language models.8 Conclusion and Future WorkOur representation learning approach to domainadaptation yields state-of-the-art results in POStagging experiments.
Our best models use multi-dimensional clustering to find several latent cate-gories for each word; the latent categories serveas useful and domain-independent features forour supervised learner.
Our exploration hasyielded significant progress already, but it has onlyscratched the surface of possible models for thistask.
The current representation learning tech-niques we use are unsupervised, meaning that theyprovide the same set of categories, regardless ofwhat task they are to be used for.
Semi-supervisedlearning approaches could be developed to guidethe representation learning process towards fea-tures that are best-suited for a particular task, butare still useful across domains.
Our current ap-proach also requires retraining of a CRF for everynew domain; incremental retraining techniques fornew domains would speed up the process andmake domain adaptation much more accessible.Finally, there are cases where small amounts of la-beled data are available for new domains; modelsthat combine our representation learning approachwith instance weighting and other forms of super-vised domain adaptation may take better advan-tage of these cases.AcknowledgmentsWe wish to thank the anonymous reviewers fortheir helpful comments and suggestions.ReferencesRie Kubota Ando and Tong Zhang.
2005.
A high-performance semi-supervised learning method fortext chunking.
In ACL.Michiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat.
2006.
MAP adaptation of stochas-tic grammars.
Computer Speech and Language,20(1):41?68.Michele Banko and Robert C. Moore.
2004.
Part ofspeech tagging in context.
In COLING.29Shai Ben-David, John Blitzer, Koby Crammer, and Fer-nando Pereira.
2007.
Analysis of representationsfor domain adaptation.
In Advances in Neural In-formation Processing Systems 20, Cambridge, MA.MIT Press.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.Y.
Bengio.
2009.
Learning deep architectures for AI.Foundations and Trends in Machine Learning, 2.C.
Biemann, C. Giuliano, and A. Gliozzo.
2007.
Un-supervised pos tagging supporting supervised meth-ods.
Proceeding of RANLP-07.J.
Blitzer, A. Globerson, and F. Pereira.
2005.
Dis-tributed latent variable models of lexical cooccur-rences.
In Proceedings of the Tenth InternationalWorkshop on Artificial Intelligence and Statistics.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP.John Blitzer, Koby Crammer, Alex Kulesza, FernandoPereira, and Jenn Wortman.
2007.
Learning boundsfor domain adaptation.
In Advances in Neural Infor-mation Processing Systems.Ciprian Chelba and Alex Acero.
2004.
Adaptation ofmaximum entropy classifier: Little data can help alot.
In EMNLP.Hal Daume?
III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, 26.Hal Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In ACL.Doug Downey, Stefan Schoenmackers, and Oren Et-zioni.
2007.
Sparse information extraction: Unsu-pervised language models to the rescue.
In ACL.A.
Emami, P. Xu, and F. Jelinek.
2003.
Using aconnectionist model in a syntactical based languagemodel.
In Proceedings of the International Confer-ence on Spoken Language Processing, pages 372?375.Scott E. Fahlman and Christian Lebiere.
1990.
Thecascade-correlation learning architecture.
Advancesin Neural Information Processing Systems 2.Zoubin Ghahramani and Michael I. Jordan.
1997.
Fac-torial hidden markov models.
Machine Learning,29(2-3):245?273.Daniel Gildea.
2001.
Corpus Variation and Parser Per-formance.
In Conference on Empirical Methods inNatural Language Processing.Sharon Goldwater and Thomas L. Griffiths.
2007.A fully bayesian approach to unsupervised part-of-speech tagging.
In ACL.Fei Huang and Alexander Yates.
2009.
Distributionalrepresentations for handling sparsity in supervisedsequence labeling.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics.Mark Johnson.
2007.
Why doesn?t EM find goodHMM POS-taggers.
In EMNLP.J.
Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.In Proceedings of the International Conference onMachine Learning.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: the Penn Treebank.
Com-putational Linguistics, 19(2):313?330.David McClosky, Eugene Charniak, and Mark John-son.
2006a.
Effective self-training for parsing.
InProc.
of HLT-NAACL.David McClosky, Eugene Charniak, and Mark John-son.
2006b.
Reranking and self-training for parseradaptation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the ACL, pages 337?344.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.In Proceedings of the 24th International Conferenceon Machine Learning, pages 641?648, New York,NY, USA.
ACM.F.
Morin and Y. Bengio.
2005.
Hierarchical probabilis-tic neural network language model.
In Proceedingsof the International Workshop on Artificial Intelli-gence and Statistics, pages 246?252.PennBioIE.
2005.
Mining the bibliome project.http://bioie.ldc.upenn.edu/.Anni R.Codena, Serguei V.Pakhomovb, Rie K.Andoa,Patrick H.Duffyb, and Christopher G.Chute.
2005.Domain-specific language models and lexiconsfor tagging.
Journal of Biomedical Informatics,38(6):422?430.Hinrich Schu?tze.
1994.
Distributional part-of-speechtagging.
In Proceedings of the 7th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics (EACL).Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 354?362, Ann Arbor, Michigan, June.Kristina Toutanova and Mark Johnson.
2007.
Abayesian LDA-based model for semi-supervisedpart-of-speech tagging.
In NIPS.30
