c?
2003 Association for Computational LinguisticsWord Reordering and a DynamicProgramming Beam Search Algorithm forStatistical Machine TranslationChristoph Tillmann?
Hermann Ney?IBM T. J. Watson Research Center RWTH AachenIn this article, we describe an efficient beam search algorithm for statistical machine translationbased on dynamic programming (DP).
The search algorithm uses the translation model presentedin Brown et al (1993).
Starting from a DP-based solution to the traveling-salesman problem,we present a novel technique to restrict the possible word reorderings between source and targetlanguage in order to achieve an efficient search algorithm.
Word reordering restrictions especiallyuseful for the translation direction German to English are presented.
The restrictions are gener-alized, and a set of four parameters to control the word reordering is introduced, which then caneasily be adopted to new translation directions.
The beam search procedure has been successfullytested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the CanadianHansards task (French to English, 100,000-word vocabulary).
For the medium-sized Verbmobiltask, a sentence can be translated in a few seconds, only a small number of search errors occur,and there is no performance degradation as measured by the word error criterion used in thisarticle.1.
IntroductionThis article is about a search procedure for statistical machine translation (MT).
Thetask of the search procedure is to find the most likely translation given a source sen-tence and a set of model parameters.
Here, we will use a trigram language model andthe translation model presented in Brown et al (1993).
Since the number of possibletranslations of a given source sentence is enormous, we must find the best outputwithout actually generating the set of all possible translations; instead we would liketo focus on the most likely translation hypotheses during the search process.
For thispurpose, we present a data-driven beam search algorithm similar to the one used inspeech recognition search algorithms (Ney et al 1992).
The major difference betweenthe search problem in speech recognition and statistical MT is that MT must take intoaccount the different word order for the source and the target language, which doesnot enter into speech recognition.
Tillmann, Vogel, Ney, and Zubiaga (1997) proposesa dynamic programming (DP)?based search algorithm for statistical MT that mono-tonically translates the input sentence from left to right.
The word order difference isdealt with using a suitable preprocessing step.
Although the resulting search proce-dure is very fast, the preprocessing is language specific and requires a lot of manual?
IBM T. J. Watson Research Center, Yorktown Heights, NY 10598.
E-mail: ctill@us.ibm.com.
The researchreported here was carried out while the author was with Lehrstuhl fu?r Informatik VI, ComputerScience Department, RWTH Aachen.?
Lehrstuhl fu?r Informatik VI, Computer Science Department, RWTH Aachen, D-52056 Aachen,Germany.
E-mail: ney@informatik.rwth-aachen.de.98Computational Linguistics Volume 29, Number 1work.
Currently, most search algorithms for statistical MT proposed in the literatureare based on the A?
concept (Nilsson 1971).
Here, the word reordering can be easilyincluded in the search procedure, since the input sentence positions can be processedin any order.
The work presented in Berger et al (1996) that is based on the A?
concept,however, introduces word reordering restrictions in order to reduce the overall searchspace.The search procedure presented in this article is based on a DP algorithm to solvethe traveling-salesman problem (TSP).
A data-driven beam search approach is pre-sented on the basis of this DP-based algorithm.
The cities in the TSP correspond tosource positions of the input sentence.
By imposing constraints on the possible wordreorderings similar to that described in Berger et al (1996), the DP-based approachbecomes more effective: when the constraints are applied, the number of word re-orderings is greatly reduced.
The original reordering constraint in Berger et al (1996)is shown to be a special case of a more general restriction scheme in which the wordreordering constraints are expressed in terms of simple combinatorical restrictions onthe processed sets of source sentence positions.1 A set of four parameters is given tocontrol the word reordering.
Additionally, a set of four states is introduced to dealwith grammatical reordering restrictions (e.g., for the translation direction German toEnglish, the word order difference between the two languages is mainly due to theGerman verb group.
In combination with the reordering restrictions, a data-drivenbeam search organization for the search procedure is proposed.
A beam search prun-ing technique is conceived that jointly processes partial hypotheses according to twocriteria: (1) The partial hypotheses cover the same set of source sentence positions,and (2) the partial hypotheses cover sets C of source sentence positions of equal car-dinality.
A partial hypothesis is said to cover a set of source sentence positions whenexactly the positions in the set have already been processed in the search process.
Toverify the effectiveness of the proposed techniques, we report and analyze results fortwo translation tasks: the German to English Verbmobil task and French to EnglishCanadian Hansards task.The article is structured as follows.
Section 2 gives a short introduction to the trans-lation model used and reports on other approaches to the search problem in statisticalMT.
In Section 3, a DP-based search approach is presented, along with appropriatepruning techniques that yield an efficient beam search algorithm.
Section 4 reportsand analyzes translation results for the different translation directions.
In Section 5,we conclude with a discussion of the achieved results.2.
Previous Work2.1 IBM Translation ApproachIn this article, we use the translation model presented in Brown et al (1993), and themathematical notation we use here is taken from that paper as well: a source stringf J1 = f1 ?
?
?
fj ?
?
?
fJ is to be translated into a target string eI1 = e1 ?
?
?
ei ?
?
?
eI.
Here, I is thelength of the target string, and J is the length of the source string.
Among all possibletarget strings, we will choose the string with the highest probability as given by Bayes?1 The word reordering restriction used in the search procedure described in Berger et al (1996) is notmentioned in Brown et al (1993), although exactly the translation model described there is used.Equivalently, we use exactly the translation model described in Brown et al (1993) but try differentreordering restrictions for the DP-based search procedure.99Tillmann and Ney DP Beam Search for Statistical MTFigure 1Architecture of the statistical translation approach based on Bayes?
decision rule.decision rule:e?I1 = arg maxeI1{Pr(eI1 | fJ1)}= arg maxeI1{Pr(eI1) ?
Pr(fJ1 | eI1)} (1)Pr(eI1) is the language model of the target language, whereas Pr(fJ1 | eI1) is the stringtranslation model.
The language model probability is computed using a trigram lan-guage model.
The string translation probability Pr(f J1 | eI1) is modeled using a series offive models of increasing complexity in training.
Here, the model used for the trans-lation experiments is the IBM-4 model.
This model uses the same parameter set asthe IBM-5 model, which in preliminary experiments did not yield better translationresults.
The actual implementation used during the experiments is described in Al-Onaizan et al (1999) and in Och and Ney (2000).
The argmax operation denotes thesearch problem (i.e., the generation of the output sentence in the target language).
Theoverall architecture of the statistical translation approach is summarized in Figure 1.In general, as shown in this figure, there may be additional transformations to makethe translation task simpler for the algorithm.
The transformations may range fromsimple word categorization to more complex preprocessing steps that require someparsing of the source string.
In this article, however, we will use only word catego-100Computational Linguistics Volume 29, Number 1rization as an explicit transformation step.
In the search procedure both the languageand the translation model are applied after the text transformation steps.
The following?types?
of parameters are used for the IBM-4 translation model:Lexicon probabilities: We use the lexicon probability p(f | e) for translating thesingle target word e as the single source word f .
A source word f maybe translated by the ?null?
word e0 (i.e., it does not produce any targetword e).
A translation probability p(f | e0) is trained along with the regulartranslation probabilities.Fertilities: A single target word e may be aligned to n = 0, 1 or more source words.This is explicitly modeled by the fertility parameter ?
(n | e): the probabilitythat the target word e is translated by n source words is ?
(n | e).
Thefertility for the ?null?
word is treated specially (for details see Brown et al[1993]).
Berger et al (1996) describes the extension of a partial hypothesisby a pair of target words (e?, e), where e?
is not connected to any sourceword f .
In this case, the so-called spontaneous target word e?
is accountedfor with the fertility.
Here, the translation probability ?
(0 | e?)
and no-translation probability p(f | e?
).Class-based distortion probabilities: When covering a source sentence positionj, we use distortion probabilities that depend on the previously coveredsource sentence positions (we say that a source sentence position j is cov-ered for a partial hypothesis when it is taken account of in the translationprocess by generating a target word or the ?null?
word e0 ).
In Brown etal.
(1993), two types of distortion probabilities are distinguished: (1) theleftmost word of a set of source words f aligned to the same target worde (which is called the ?head?)
is placed, and (2) the remaining sourcewords are placed.
Two separate distributions are used for these two cases.For placing the ?head?
the center function center(i) (Brown et al [1993]uses the notation i) is used: the average position of the source wordswith which the target word ei?1 is aligned.
The distortion probabilitiesare class-based: They depend on the word class F(f ) of a covered sourceword f as well as on the word class E(e) of the previously generated targetword e. The classes are automatically trained (Brown et al 1992).When the IBM-4 model parameters are used during search, an input sentence can beprocessed one source position at a time in a certain order primarily determined by thedistortion probabilities.
We will use the following simplified set of translation modelparameters: lexicon probabilities p(f | e) and distortion probabilities p(j | j?, J).
Here, jis the currently covered input sentence position and j?
is the previously covered inputsentence position.
The input sentence length J is included, since we would like to thinkof the distortion probability as normalized according to J.
No fertility probabilities or?null?
word probabilities are used; thus each source word f is translated as exactly onetarget word e and each target word e is translated as exactly one source word f .
Thesimplified notation will help us to focus on the most relevant details of the DP-basedsearch procedure.
The simplified set of parameters leads to an unrealistic assumptionabout the length of the source and target sentence, namely, I = J.
During the translationexperiments we will, of course, not make this assumption.
The implementation detailsfor using the full set of IBM-4 model parameters are given in Section 3.9.2.101Tillmann and Ney DP Beam Search for Statistical MT2.2 Search Algorithms for Statistical Machine TranslationIn this section, we give a short overview of search procedures used in statistical MT:Brown et al (1990) and Brown et al (1993) describe a statistical MT system that is basedon the same statistical principles as those used in most speech recognition systems(Jelinek 1976).
Berger et al (1994) describes the French-to-English Candide translationsystem, which uses the translation model proposed in Brown et al (1993).
A detaileddescription of the decoder used in that system is given in Berger et al (1996) but hasnever been published in a paper: Throughout the search process, partial hypothesesare maintained in a set of priority queues.
There is a single priority queue for eachsubset of covered positions in the source string.
In practice, the priority queues areinitialized only on demand; far fewer than the full number of queues possible are actu-ally used.
The priority queues are limited in size, and only the 1,000 hypotheses withthe highest probability are maintained.
Each priority queue is assigned a thresholdto select the hypotheses that are going to be extended, and the process of assigningthese thresholds is rather complicated.
A restriction on the possible word reorderings,which is described in Section 3.6, is applied.Wang and Waibel (1997) presents a search algorithm for the IBM-2 translationmodel based on the A?
concept and multiple stacks.
An extension of this algorithmis demonstrated in Wang and Waibel (1998).
Here, a reshuffling step on top of theoriginal decoder is used to handle more complex translation models (e.g., the IBM-3model is added).
Translation approaches that use the IBM-2 model parameters but arebased on DP are presented in Garc?
?a-Varea, Casacuberta, and Ney (1998) and Niessenet al (1998).
An approach based on the hidden Markov model alignments as usedin speech recognition is presented in Tillmann, Vogel, Ney, and Zubiaga (1997) andTillmann, Vogel, Ney, Zubiaga, and Sawaf (1997).
This approach assumes that sourceand target language have the same word order, and word order differences are dealtwith in a preprocessing stage.
The work by Wu (1996) also uses the original IBM modelparameters and obtains an efficient search algorithm by restricting the possible wordreorderings using the so-called stochastic bracketing transduction grammar.Three different decoders for the IBM-4 translation model are compared in Germannet al (2001).
The first is a reimplementation of the stack-based decoder described inBerger et al (1996).
The second is a greedy decoder that starts with an approximatesolution and then iteratively improves this first rough solution.
The third convertsthe decoding problem into an integer program (IP), and a standard software packagefor solving IP is used.
Although the last approach is guaranteed to find the optimalsolution, it is tested only for input sentences of length eight or shorter.This article will present a DP-based beam search decoder for the IBM-4 translationmodel.
The decoder is designed to carry out an almost full search with a small numberof search errors and with little performance degradation as measured by the word errorcriterion.
A preliminary version of the work presented here was published in Tillmannand Ney (2000).3.
Beam Search in Statistical Machine Translation3.1 Inverted Alignment ConceptTo explicitly describe the word order difference between source and target language,Brown et al (1993) introduced an alignment concept, in which a source position j ismapped to exactly one target position i:regular alignment: j ?
i = aj102Computational Linguistics Volume 29, Number 1e.InthiscasemycolleaguecanknotvisitonthefourthofMaym K Syoua v M nchtbFesuchen.ndeannoegee eni aiIiiimesirtelllla mnFigure 2Regular alignment example for the translation direction German to English.
For each Germansource word there is exactly one English target word on the alignment path.An example for this kind of alignment is given in Figure 2, in which each Germansource position j is mapped to an English target position i.
In Brown et al (1993), thisalignment concept is used for model IBM-1 through model IBM-5.
For search purposes,we use the inverted alignment concept as introduced in Niessen et al (1998) and Neyet al (2000).
An inverted alignment is defined as follows:inverted alignment: i ?
j = biHere, a target position i is mapped to a source position j.
The coverage constraint foran inverted alignment is not expressed by the notation: Each source position j shouldbe ?hit?
exactly once by the path of the inverted alignment bI1 = b1 ?
?
?
bi ?
?
?
bI.
Theadvantage of the inverted alignment concept is that we can construct target sentencehypotheses from bottom to top along the positions of the target sentence.
Using theinverted alignments in the maximum approximation, we rewrite equation (1) to obtainthe following search criterion, in which we are looking for the most likely target103Tillmann and Ney DP Beam Search for Statistical MTFigure 3Illustration of the transitions in the regular and in the inverted alignment model.
The regularalignment model (left figure) is used to generate the sentence from left to right; the invertedalignment model (right figure) is used to generate the sentence from bottom to top.sentence eI1 of length I = J for an observed source sentence fJ1 of length J:maxI{p(J | I) ?
maxeI1{p(eI1) ?
p(fJ1 | eI1)}}(2)?= maxI{p(J | I) ?
maxeI1{I?i=1p(ei | ei?1, ei?2) ?
maxbI1I?i=1[p(bi | bi?1, J) ?
p(fbi | ei)]}}= maxI{p(J | I) ?
maxeI1,bI1{I?i=1[p(ei | ei?1, ei?2) ?
p(bi | bi?1, J) ?
p(fbi | ei)]}}The following notation is used: ei?1, ei?2 are the immediate predecessor target words,ei is the word to be hypothesized, p(ei | ei?1, ei?2) denotes the trigram language modelprobability, p(fbi | ei) denotes the lexicon probability for translating the target word eias source word fbi , and p(bi | bi?1, J) is the distortion probability for covering sourceposition bi after source position bi?1.
Note that in equation (2) two products over i aremerged into a single product over i.
The translation probability p(f J1 | eI1) is computed inthe maximum approximation using the distortion and the lexicon probabilities.
Finally,p(J | I) is the sentence length model, which will be dropped in the following (it is notused in the IBM-4 translation model).
For each source sentence f J1 to be translated, weare searching for the unknown mapping that optimizes equation (2):i ?
(bi, ei)In Section 3.3, we will introduce an auxiliary quantity that can be evaluated recursivelyusing DP to find this unknown mapping.
We will explicitly take care of the coverageconstraint by introducing a coverage set C of source sentence positions that havealready been processed.
Figure 3 illustrates the concept of the search algorithm usinginverted alignments: Partial hypotheses are constructed from bottom to top along thepositions of the target sentence.
Partial hypotheses of length i?1 are extended to obtainpartial hypotheses of the length i.
Extending a partial hypothesis means covering asource sentence position j that has not yet been covered.
For a given grid point in the104Computational Linguistics Volume 29, Number 1Table 1DP-based algorithm for solving traveling-salesman problems due to Held and Karp.
Theoutermost loop is over the cardinality of subsets of already visited cities.input: cities j = 1, .
.
.
, J with distance matrix djj ?initialization: D({k}, k) := d1kfor each path length c = 2, .
.
.
, J dofor each pair (C, j), where C ?
{2, .
.
.
, J} and j ?
C and |C| = c doD(C, j) = minj?
?C\{j}{djj ?
+ D(C\{j}, j?)}traceback:?
find shortest tour: D?
= mink?
{2,...,J}[D({2, .
.
.
, J}, k) + dk1]?
recover optimal sequence of citiestranslation lattice, the unknown target word sequence can be obtained by tracing backthe translation decisions to the partial hypothesis at stage i = 1.
The grid points aredefined in Section 3.3.
In the left part of the figure the regular alignment concept isshown for comparison purposes.3.2 Held and Karp Algorithm for Traveling-Salesman ProblemHeld and Karp (1962) presents a DP approach to solve the TSP, an optimization prob-lem that is defined as follows: Given are a set of cities {1, .
.
.
, J} and for each pairof cities j, j?
the cost djj ?
> 0 for traveling from city j to city j?.
We are looking forthe shortest tour, starting and ending in city 1, that visits all cities in the set of citiesexactly once.
We are using the notation C for the set of cities, since it corresponds toa coverage set of processed source positions in MT.
A straightforward way to findthe shortest tour is by trying all possible permutations of the J cities.
The resultingalgorithm has a complexity of O(J!).
DP can be used, however, to find the shortest tourin O(J2 ?
2J), which is a much smaller complexity for larger values of J.
The approachrecursively evaluates the quantity D(C, j):D(C, j) := costs of the partial tour starting in city 1, endingin city j, and visiting all cities in CSubsets of cities C of increasing cardinality c are processed.
The algorithm, shown inTable 1, works because not all permutations of cities have to be considered explicitly.During the computation, for a pair (C, j), the order in which the cities in C have beenvisited can be ignored (except j); only the costs for the best path reaching j has to bestored.
For the initialization the costs for starting from city 1 are set: D({k}, k) = d1k foreach k ?
{2, .
.
.
, |C|}.
Then, subsets C of increasing cardinality are processed.
Finally,the cost for the optimal tour is obtained in the second-to-last line of the algorithm.The optimal tour itself can be found using a back-pointer array in which the optimaldecision for each grid point (C, j) is stored.Figure 4 illustrates the use of the algorithm by showing the ?supergraph?
that issearched in the Held and Karp algorithm for a TSP with J = 5 cities.
When traversingthe lattice from left to right following the different possibilities, a partial path to a nodej corresponds to the subset C of all cities on that path together with the last visited105Tillmann and Ney DP Beam Search for Statistical MTFigure 4Illustration of the algorithm by Held and Karp for a traveling salesman problem with J = 5cities.
Not all permutations of cities have to be evaluated explicitly.
For a given subset of citiesthe order in which the cities have been visited can be ignored.city j.
Of all the different paths merging into the node j, only the partial path with thesmallest cost has to be retained for further computation.3.3 DP-Based Algorithm for Statistical Machine TranslationIn this section, the Held and Karp algorithm is applied to statistical MT.
Using theconcept of inverted alignments as introduced in Section 3.1, we explicitly take care ofthe coverage constraint by introducing a coverage set C of source sentence positionsthat have already been processed.
Here, the correspondence is according to the fact thateach source sentence position has to be covered exactly once, fulfilling the coverageconstraint.
The cities of the more complex translation TSP correspond roughly to triples(e?, e, j), the notation for which is given below.
The final path output by the translationalgorithm will contain exactly one triple (e?, e, j) for each source position j.The algorithm processes subsets of partial hypotheses with coverage sets C ofincreasing cardinality c. For a trigram language model, the partial hypotheses are ofthe form (e?, e, C, j), where e?, e are the last two target words, C is a coverage set forthe already covered source positions, and j is the last covered position.
The targetword sequence that ends in e?, e is stored as a back pointer to the predecessor partialhypothesis (and recursively to its predecessor hypotheses) and is not shown in thenotation.
Each distance in the TSP now corresponds to the negative logarithm of theproduct of the translation, distortion, and language model probabilities.
The following106Computational Linguistics Volume 29, Number 1Table 2DP-based algorithm for statistical MT that consecutively processes subsets C of sourcesentence positions of increasing cardinality.input: source language string f1 ?
?
?
fj ?
?
?
fJinitializationfor each cardinality c = 1, 2, .
.
.
, J dofor each pair (C, j), where C ?
{1, .
.
.
, J} and j ?
C and |C| = c dofor each pair of target words e?, e ?
EQe?
(e, C, j) = p(fj | e) maxe??j?
?C\{j}{p(j | j?, J) ?
p(e | e?, e??)
?
Qe??
(e?, C\{j}, j?)}traceback:?
find best end hypothesis: maxe,e?
,j{p($ | e, e?)
?
Qe?
(e, {1, .
.
.
, J}, j)}?
recover optimal word sequenceauxiliary quantity is defined:Qe?
(e, C, j) := probability of the best partial hypothesis (ei1, bi1), whereC = {bk | k = 1, .
.
.
, i}, bi = j, ei = e, and ei?1 = e?The above auxiliary quantity satisfies the following recursive DP equation:Qe?
(e, C, j) = p(fj | e) ?
maxe??j?
?C\{j}{p(j | j?, J) ?
p(e | e?, e??)
?
Qe??
(e?, C\{j}, j?
)}Here, j?
is the previously covered source sentence position and e?, e??
are the predecessorwords.
The DP equation is evaluated recursively for each hypothesis (e?, e, C, j).
Theresulting algorithm is depicted in Table 2.
Some details concerning the initializationand the finding of the best target language string are presented in Section 3.4. p($ | e, e?
)is the trigram language probability for predicting the sentence boundary symbol $.
Thecomplexity of the algorithm is O(E3 ?
J2 ?
2J), where E is the size of the target languagevocabulary.3.4 Verb Group Reordering: German to EnglishThe above search space is still too large to translate even a medium-length inputsentence.
On the other hand, only very restricted reorderings are necessary; for ex-ample, for the translation direction German to English, the word order difference ismostly restricted to the German verb group.
The approach presented here assumes amostly monotonic traversal of the source sentence positions from left to right.2 A smallnumber of positions may be processed sooner than they would be in that monotonictraversal.
Each source position then generates a certain number of target words.
Therestrictions are fully formalized in Section 3.5.A typical situation is shown in Figure 5.
When translating the sentence monotoni-cally from left to right, the translation of the German finite verb kann, which is the leftverbal brace in this case, is skipped until the German noun phrase mein Kollege, whichis the subject of the sentence, is translated.
Then, the right verbal brace is translated:2 Also, this assumption is necessary for the beam search pruning techniques to work efficiently.107Tillmann and Ney DP Beam Search for Statistical MTi.fourththeofMaythiscasecolleaguecannotvisitIneeFaknnmidInsllam KoS .youonmynieiniemllegebesuchenchtiertena vaMFigure 5Word reordering for the translation direction German to English: The reordering is restrictedto the German verb group.The infinitive besuchen and the negation particle nicht.
The following restrictions areused: One position in the source sentence may be skipped for a distance of up to L = 4source positions, and up to two source positions may be moved for a distance of atmost R = 10 source positions (the notation L and R shows the relation to the handlingof the left and right verbal brace).
To formalize the approach, we introduce four verbgroup states S:?
Initial : A contiguous initial block of source positions is covered.?
Skip: One word may be skipped, leaving a ?hole?
in the monotonictraversal.?
Move: Up to two words may be ?moved?
from later in the sentence.?
Cover : The sentence is traversed monotonically until the state Initial isreached.108Computational Linguistics Volume 29, Number 1InitialSkipMove Cover11.
vierten5.
Kollege4.
mein1.
In3.
Fall2.
diesem 12.
Mai6.
kann13.
.8. besuchen7.
nicht10.
am9.
SieFigure 6Order in which the German source positions are covered for the German-to-English reorderingexample given in Figure 5.The states Move and Skip both allow a set of upcoming words to be processed soonerthan would be the case in the monotonic traversal.
The state Initial is entered wheneverthere are no uncovered positions to the left of the rightmost covered position.
Thesequence of states needed to carry out the word reordering example in Figure 5 isgiven in Figure 6.
The 13 source sentence words are processed in the order shown.A formal specification of the state transitions is given in Section 3.5.
Any number ofconsecutive German verb phrases in a sentence can be processed by the algorithm.
Thefinite-state control presented here is obtained from a simple analysis of the German-to-English word reordering problem and is not estimated from the training data.
Itcan be viewed as an extension of the IBM-4 model distortion probabilities.Using the above states, we define partial hypothesis extensions of the followingtype:(S ?, C\{j}, j?)
?
(S, C, j)Not only the coverage set C and the positions j, j?, but also the verb group states S,S ?,are taken into account.
For the sake of brevity, we have omitted the target languagewords e, e?
in the notation of the partial hypothesis extension.
For each extension anuncovered position is added to the coverage set C of the partial hypothesis, and theverb group state S may change.
A more detailed description of the partial hypoth-esis extension for a certain state S is given in the next section in a more generalcontext.
Covering the first uncovered position in the source sentence, we use the lan-109Tillmann and Ney DP Beam Search for Statistical MTguage model probability p(e | $, $).
Here, $ is the sentence boundary symbol, whichis thought to be at position 0 in the target sentence.
The search starts in the hypoth-esis (Initial , {?
}, 0).
{?}
denotes the empty set, where no source sentence position iscovered.
The following recursive equation is evaluated:Qe?
(e,S, C, j) (3)= p(fj | e) maxe??
,S?
,j?(S?
,C\{j},j?)?(S,C,j)j?
?C\{j}{p(j | j?, J) ?
p(e | e?, e??)
?
Qe??
(e?,S ?, C\{j}, j?
)}The search ends in the hypotheses (Initial , {1, .
.
.
, J}, j); the last covered position maybe in the range j ?
{J?L, .
.
.
, J}, because some source positions may have been skippedat the end of the input sentence.
{1, .
.
.
, J} denotes a coverage set including all positionsfrom position 1 to position J.
The final translation probability QF isQF = maxe,e?j?
{J?L,...,J}p($ | e, e?)
?
Qe?
(e, Initial , {1, .
.
.
, J}, j) (4)where p($ | e, e?)
denotes the trigram language model, which predicts the sentenceboundary $ at the end of the target sentence.
QF can be obtained using an algorithmvery similar to the one given in Table 2.
The complexity of the verb group reorderingfor the translation direction German to English is O(E3 ?
J ?
(R2 ?
L ?
R)), as shown inTillmann (2001).3.5 Word Reordering: GeneralizationFor the translation direction English to German, the word reordering can be restrictedin a similar way as for the translation direction German to English.
Again, the wordorder difference between the two languages is mainly due to the German verb group.During the translation process, the English verb group is decomposed as shown inFigure 7.
When the sentence is translated monotonically from left to right, the trans-lation of the English finite verb can is moved, and it is translated as the German leftverbal brace before the English noun phrase my colleague, which is the subject of thesentence.
The translations of the infinitive visit and of the negation particle not areskipped until later in the translation process.
For this translation direction, the trans-lation of one source sentence position may be moved for a distance of up to L = 4source positions, and the translation of up to two source positions may be skippedfor a distance of up to R = 10 source positions (we take over the L and R notationfrom the previous section).
Thus, the role of the skipping and the moving are simplyreversed with respect to their roles in German-to-English translation.
For the exampletranslation in Figure 7, the order in which the source sentence positions are coveredis given in Figure 8.We generalize the two approaches for the different translation directions as fol-lows: In both approaches, we assume that the source sentence is mainly processedmonotonically.
A small number of upcoming source sentence positions may be pro-cessed earlier than they would be in the monotonic traversal: The states Skip and Moveare used as explained in the preceding section.
The positions to be processed outsidethe monotonic traversal are restricted as follows:?
The number of positions dealt with in the states Move and Skip isrestricted.?
There are distance restrictions on the source positions processed in thosestates.110Computational Linguistics Volume 29, Number 1.IndiesemFallkannmeinKollegeSieamviertenMainichtbesuchen.n hconov youon ouuo MI tiscasemylleagecan tisitthefthrf ayFigure 7Word reordering for the translation direction English to German: The reordering is restrictedto the English verb group.These restrictions will be fully formalized later in this section.
In the state Move, somesource sentence positions are ?moved?
from later in the sentence to earlier.
After sourcesentence positions are moved, they are marked, and the translation of the sentence iscontinued monotonically, keeping track of the positions already covered.
To formalizethe approach, we introduce four reordering states S:?
Initial : A contiguous initial block of source positions is covered.?
Skip: A restricted number of source positions may be skipped, leaving?holes?
in the monotonic traversal.?
Move: A restricted number of words may be ?moved?
from later in thesentence.?
Cover : The sentence is traversed monotonically until the state Initial isreached.To formalize the approach, the following notation is introduced:rmax(C) = maxc?Cc111Tillmann and Ney DP Beam Search for Statistical MTInitialSkipCoverMove1.
In13.
not2.
this3.
case6.
colleague14.
visit15.
.
4. can5.
my7.
you8.
on9.
the10.
fourth11.
of12.
MayFigure 8Order in which the English source positions are covered for the English-to-German reorderingexample given in Figure 7.lmin(C) = minc/?Ccu(C) = card({c | c /?
C and c < rmax(C)})m(C) = card({c | c ?
C and c > lmin(C)})w(C) = rmax(C)?
lmin(C)rmax(C) is the rightmost covered and lmin(C) is the leftmost uncovered source position.u(C) is the number of ?skipped?
positions, and m(C) is the number of ?moved?
po-sitions.
The function card(?)
returns the cardinality of a set of source positions.
Thefunction w(C) describes the ?window?
size in which the word reordering takes place.A procedural description for the computation of the set of successor hypotheses fora given partial hypothesis (S, C, j) is given in Table 3.
There are restrictions on thepossible successor states: A partial hypothesis in state Skip cannot be expanded intoa partial hypothesis in state Move and vice versa.
If the coverage set for the newlygenerated hypothesis covers a contiguous initial block of source positions, the stateInitial is entered.
No other state S is considered as a successor state in this case (hencethe use of the continue statement in the procedural description).
The set of successorhypotheses Succ by which to extend the partial hypothesis (S, C, j) is computed usingthe constraints defined by the values for numskip, widthskip, nummove, and widthmove ,as explained in the Appendix.
In particular, a source position k is discarded for ex-tension if the ?window?
restrictions are violated.
Within the restrictions all possiblesuccessors are computed.
It can be observed that the set of successors, as computedin Table 3, is never empty.112Computational Linguistics Volume 29, Number 1Table 3Procedural description to compute the set Succ of successor hypotheses by which to extend apartial hypothesis (S, C, j).input: partial hypothesis (S, C, j)Succ := {?
}for each k /?
C doSet C?
= C ?
{k}if u(C?)
= 0Succ := Succ ?
(Initial, C?, k)continueif (S = Initial) or (S = Skip)if w(C?)
?
widthskip and u(C?)
?
numskipSucc := Succ ?
(Skip, C?, k)if (S = Initial) or (S = Move)if k = lmin(C?)
and w(C?)
?
widthmove and m(C?)
?
nummoveSucc := Succ ?
(Move, C?, k)if (S = Move) or (S = Cover)if (lmin(C?)
= k)Succ := Succ ?
(Cover, C?, k)output: set Succ of successor hypothesesThere is an asymmetry between the two reordering states Move and Skip: While instate Move, the algorithm is not allowed to cover the position lmin(C).
It must first enterthe state Cover to do so.
In contrast, for the state Skip, the newly generated hypothesisalways remains in the state Skip (until the state Initial is entered.)
This is motivatedby the word reordering for the German verb group.
After the right verbal brace hasbeen processed, no source words may be moved into the verbal brace from later inthe sentence.
There is a redundancy in the reorderings: The same reordering might becarried out using either the state Skip or Move, especially if widthskip and widthmoveare about the same.
The additional computational burden is alleviated somewhat bythe fact that the pruning, as introduced in Section 3.8, does not distinguish hypothesesaccording to the states.
A complexity analysis for different reordering constraints isgiven in Tillmann (2001).3.6 Word Reordering: IBM-Style RestrictionsWe now compare the new word reordering approach with the approach used in Bergeret al (1996).
In the approach presented in this article, source sentence words are alignedwith hypothesized target sentence words.3 When a source sentence word is aligned, wesay its position is covered.
During the search process, a partial hypothesis is extendedby choosing an uncovered source sentence position, and this choice is restricted.
Onlyone of the first n uncovered positions in a coverage set may be chosen, where n isset to 4.
This choice is illustrated in Figure 9.
In the figure, covered positions aremarked by a filled circle, and uncovered positions are marked by an unfilled circle.Positions that may be covered next are marked by an unfilled square.
The restrictionsfor a coverage set C can be expressed in terms of the expression u(C) defined in theprevious section: The number of uncovered source sentence positions to the left ofthe rightmost covered position.
Demanding u(C) ?
3, we obtain the S3 restriction3 In Berger et al (1996), a morphological analysis is carried out and word morphemes are processedduring the search.
Here, we process only full-form words.113Tillmann and Ney DP Beam Search for Statistical MTuncovered position for extensioncovered positionuncovered positionJ1 jFigure 9Illustration of the IBM-style reordering constraint.introduced in the Appendix.
An upper bound of O(E3 ?
J4) for the word reorderingcomplexity is given in Tillmann (2001).3.7 Empirical Complexity CalculationsIn order to demonstrate the complexity of the proposed reordering constraints, wehave modified our translation algorithm to show, for the different reordering con-straints, the overall number of successor states generated by the algorithm given inTable 3.
The number of successors shown in Figure 10 is counted for a pseudotransla-tion task in which a pseudo?source word x is translated into the identically pseudo?target word x.
No actual optimization is carried out; the total number of successorsis simply counted as the algorithm proceeds through subsets of increasing cardinality.The complexity differences for the different reordering constraints result from the dif-ferent number of coverage subsets C and corresponding reordering states S allowed.For the different reordering constraints we obtain the following results (the abbrevia-tions MON, GE, EG, and S3 are taken from the Appendix):?
MON: For this reordering restriction, a partial hypothesis is alwaysextended by the position lmin(C), hence the number of processed arcs is J.?
GE, EG: These two reordering constraints are very similar in terms ofcomplexity: The number of word reorderings is heavily restricted ineach.
Actually, since the distance restrictions (expressed by the variableswidthskip and widthmove) apply, the complexity is linear in the length ofthe input sentence J.?
S3: The S3 reordering constraint has a complexity close to J4.
Since nodistance restrictions for the skipped positions apply, the overall searchspace is significantly larger than for the GE or EG restriction.114Computational Linguistics Volume 29, Number 11101001000100001000001e+061e+070 5 10 15 20 25 30 35 40 45 50"J4""S3""EG""GE""MON"Figure 10Number of processed arcs for the pseudotranslation task as a function of the input sentencelength J (y-axis is given in log scale).
The complexity for the four different reorderingconstraints MON, GE, EG, and S3 is given.
The complexity of the S3 constraint is close to J4.3.8 Beam Search Pruning TechniquesTo speed up the search, a beam search strategy is used.
There is a direct analogy tothe data-driven search organization used in continuous-speech recognition (Ney et al1992).
The full DP search algorithm proceeds cardinality-synchronously over subsetsof source sentence positions of increasing cardinality.
Using the beam search concept,the search can be focused on the most likely hypotheses.
The hypotheses Qe?
(e, C, j)are distinguished according to the coverage set C, with two kinds of pruning basedon this coverage set:1.
The coverage pruning is carried out separately for each coverage set C.2.
The cardinality pruning is carried out jointly for all coverage sets C withthe same cardinality c = c(C).After the pruning is carried out, we retain for further consideration only hypothe-ses with a probability close to the maximum probability.
The number of survivinghypotheses is controlled by four kinds of thresholds:?
the coverage pruning threshold tC?
the coverage histogram threshold nC?
the cardinality pruning threshold tc?
the cardinality histogram threshold ncFor the coverage and the cardinality pruning, the probability Qe?
(e, C, j) is adjusted totake into account the uncovered source sentence positions C?
= {1, .
.
.
, J}\C.
To make115Tillmann and Ney DP Beam Search for Statistical MTthis adjustment, for a source word f at an uncovered source position, we precomputean upper bound p?
(f ) for the product of language model and lexicon probability:p?
(f ) = maxe?
?,e?,e{p(e | e?, e??)
?
p(f | e)}The above optimization is carried out only over the word trigrams (e, e?, e??)
that haveactually been seen in the training data.
Additionally, the observation pruning describedbelow is applied to the possible translations e of a source word f .
The upper boundis used in the beam search concept to increase the comparability between hypothesescovering different coverage sets.
Even more benefit from the upper bound p?
(f ) can beexpected if the distortion and the fertility probabilities are taken into account (Tillmann2001).
Using the definition of p?
(f ), the following modified probability Q?e?
(e, C, j) is usedto replace the original probability Qe?
(e, C, j), and all pruning is applied to the newprobability:Q?e?
(e, C, j) = Qe?
(e, C, j) ??j?C?p?
(fj)For the translation experiments, equation (3) is recursively evaluated over subsets ofsource positions of equal cardinality.
For reasons of brevity, we omit the state descrip-tion S in equation (3), since no separate pruning according to the states S is carried out.The set of surviving hypotheses for each cardinality c is referred to as the beam.
Thesize of the beam for cardinality c depends on the ambiguity of the translation task forthat cardinality.
To fully exploit the speedup of the DP beam search, the search spaceis dynamically constructed as described in Tillmann, Vogel, Ney, Zubiaga, and Sawaf(1997), rather than using a static search space.
To carry out the pruning, the maximumprobabilities with respect to each coverage set C and cardinality c are computed:?
Coverage pruning: Hypotheses are distinguished according to the subsetof covered positions C. The probability Q?
(C) is defined:Q?
(C) = maxe,e?,jQ?e?
(e, C, j)?
Cardinality pruning: Hypotheses are distinguished according to thecardinality c(C) of subsets C of covered positions.
The probability Q?
(c) isdefined for all hypotheses with c(C) = c:Q?
(c) = maxCc(C)=cQ?
(C)The coverage pruning threshold tC and the cardinality pruning threshold tc are usedto prune active hypotheses.
We call this pruning translation pruning.
Hypotheses arepruned according to their translation probability:Q?e?
(e, C, j) < tC ?
Q?(C)Q?e?
(e, C, j) < tc ?
Q?
(c)For the translation experiments presented in Section 4, the negative logarithms of theactual pruning thresholds tc and tC are reported.
A hypothesis (e?, e, C, j) is discarded ifits probability is below the corresponding threshold.
For the current experiments, the116Computational Linguistics Volume 29, Number 1coverage and the cardinality threshold are constant for different coverage sets C andcardinalities c. Together with the translation pruning, histogram pruning is carriedout: The overall number N(C) of active hypotheses for the coverage set C and theoverall number N(c) of active hypotheses for all subsets of a given cardinality maynot exceed a given number; again, different numbers are used for coverage and cardi-nality pruning.
The coverage histogram pruning is denoted by nC , and the cardinalityhistogram pruning is denoted by nc:N(C) > nCN(c) > ncIf the numbers of active hypotheses for each coverage set C and cardinality c, N(C)and N(c), exceed the above thresholds, only the partial hypotheses with the highesttranslation probabilities are retained (e.g., we may use nC = 1,000 for the coveragehistogram pruning).The third type of pruning conducted observation pruning: The number of wordsthat may be produced by a source word f is limited.
For each source language wordf the list of its possible translations e is sorted according top(f | e) ?
puni(e)where puni(e) is the unigram probability of the target language word e. Only the best notarget words e are hypothesized during the search process (e.g., during the experimentsto hypothesize, the best no = 50 words was sufficient.3.9 Beam Search ImplementationIn this section, we describe the implementation of the beam search algorithm presentedin the previous sections and show how it is applied to the full set of IBM-4 modelparameters.3.9.1 Baseline DP Implementation.
The implementation described here is similar tothat used in beam search speech recognition systems, as presented in Ney et al (1992).The similarities are given mainly in the following:?
The implementation is data driven.
Both its time and memoryrequirements are strictly linear in the number of path hypotheses(disregarding the sorting steps explained in this section).?
The search procedure is developed to work most efficiently when theinput sentences are processed mainly monotonically from left to right.The algorithm works cardinality-synchronously, meaning that all thehypotheses that are processed cover subsets of source sentence positionsof equal cardinality c.?
Since full search is prohibitive, we use a beam search concept, as inspeech recognition.
We use appropriate pruning techniques in connectionwith our cardinality-synchronous search procedure.Table 4 shows a two-list implementation of the search algorithm given in Table 2 inwhich the beam pruning is included.
The two lists are referred to as S and Snew: Sis the list of hypotheses that are currently expanded, and Snew is the list of newly117Tillmann and Ney DP Beam Search for Statistical MTTable 4Two-list implementation of a DP-based search algorithm for statistical MT.input: source string f1 ?
?
?
fj ?
?
?
fJinitial hypothesis lists: S = {($, $, {?
}, 0)}for each cardinality c = 1, 2, .
.
.
, J doSnew = {?
}for each hypothesis (e?, e, C, j?)
?
S, where j?
?
C and |C| = c doExpand (e?, e, C, j?)
using probabilities p(fj | e) ?
p(j | j?, J) ?
p(e | e?, e??
)Look up and add or update expanded hypothesis in SnewSort hypotheses in Snew according to translation scoreCarry out cardinality pruningSort hypotheses in Snew according to coverage set C and translation scoreCarry out coverage pruningBookkeeping of surviving hypotheses in SnewS := Snewoutput: get best target word sequence eI1 from bookkeeping arraygenerated hypotheses.
The search procedure processes subsets of covered source sen-tence positions of increasing cardinality.
The search starts with S = {($, $, {?
}, 0)},where $ denotes the sentence start symbol for the immediate two predecessor wordsand {?}
denotes the empty coverage set, in which no source position is covered yet.For the initial search state, the position last covered is set to 0.
A set S of activehypotheses is expanded for each cardinality c using lexicon model, language model,and distortion model probabilities.
The newly generated hypotheses are added to thehypothesis set Snew; for hypotheses that are not distinguished according to our DPapproach, only the best partial hypothesis is retained for further consideration.
Thisso-called recombination is implemented as a set of simple lookup and update opera-tions on the set Snew of partial hypotheses.
During the partial hypothesis extensions,an anticipated pruning is carried out: Hypotheses are discarded before they are con-sidered for recombination and are never added to Snew.
(The anticipated pruning is notshown in Table 4.
It is based on the pruning thresholds described in Section 3.8.)
Afterthe extension of all partial hypotheses in S, a pruning step is carried out for the hy-potheses in the newly generated set Snew.
The pruning is based on two simple sortingsteps on the list of partial hypotheses Snew.
(Instead of sorting the partial hypothe-ses, we might have used hashing.)
First, the partial hypotheses are sorted accordingto their translation scores (within the implementation, all probabilities are convertedinto translation scores by taking the negative logarithm ?
log()).
Cardinality prun-ing can then be carried out simply by running down the list of hypotheses, startingwith the maximum-probability hypothesis, and applying the cardinality thresholds.Then, the partial hypotheses are sorted a second time according to their coverage setC and their translation score.
After this sorting step, all partial hypotheses that coverthe same subset of source sentence positions are located in consecutive fragments inthe overall list of partial hypotheses.
Coverage pruning is carried out in a single runover the list of partial hypotheses: For each fragment corresponding to the same cov-erage set C, the coverage pruning threshold is applied.
The partial hypotheses thatsurvive the two pruning stages are then written into the so-called bookkeeping array(Ney et al 1992).
For the next expansion step, the set S is set to the newly generatedlist of hypotheses.
Finally, the target translation is constructed from the bookkeepingarray.118Computational Linguistics Volume 29, Number 13.9.2 Details for IBM-4 Model.
In this section, we outline how the DP-based beamsearch approach can be carried out using the full set of IBM-4 parameters.
(Moredetails can be found in Tillmann [2001] or in the cited papers.)
First, the full set ofIBM-4 parameters does not make the simplifying assumption given in Section 3.1,namely, that source and target sentences are of equal length: Either a target word emay be aligned with several source words (its fertility is greater than one) or a singlesource word may produce zero, one, or two target words, as described in Berger etal.
(1996), or both.
Zero target words are generated if f is aligned to the ?null?
worde0.
Generating a single target word e is the regular case.
Two target words (e?, e??
)may be generated.
The costs for generating the target word e?
are given by its fertility?
(0 | e?)
and the language model probability; no lexicon probability is used.
During theexperiments, we restrict ourselves to triples of target words (e, e?, e??)
actually seen in thetraining data.
This approach is used for the French-to-English translation experimentspresented in this article.Another approach for mapping a single source language word to several targetlanguage words involves preprocessing by the word-joining algorithm given in Till-mann (2001), which is similar to the approach presented in Och, Tillmann, and Ney(1999).
Target words are joined during a training phase, and several joined target lan-guage words are dealt with as a new lexicon entry.
This approach is used for theGerman-to-English translation experiments presented in this article.In order to deal with the IBM-4 fertility parameters within the DP-based concept,we adopt the distinction between open and closed hypotheses given in Berger et al(1996).
A hypothesis is said to be open if it is to be aligned with more source positionsthan it currently is (i.e., at least two).
Otherwise it is called closed.
The differencebetween open and closed is used to process the input sentence one position a time(for details see Tillmann 2001).
The word reordering restrictions and the beam searchpruning techniques are directly carried over to the full set of IBM-4 parameters, sincethey are based on restrictions on the coverage vectors C only.To ensure its correctness, the implementation was tested by carrying out forcedalignments on 500 German-to-English training sentence pairs.
In a forced alignment,the source sentence f J1 and the target sentence eI1 are kept fixed, and a full search with-out re-ordering restrictions is carried out only over the unknown alignment aJ1.
Thelanguage model probability is divided out, and the resulting probability is compared tothe Viterbi probability as obtained by the training procedure.
For 499 training sentencesthe Viterbi alignment probability as obtained by the forced-alignment search was ex-actly the same as the one produced by the training procedure.
In one case the forced-alignment search did obtain a better Viterbi probability than the training procedure.4.
Experimental ResultsTranslation experiments are carried out for the translation directions German to En-glish and English to German (Verbmobil task) and for the translation directions Frenchto English and English to French (Canadian Hansards task).
Section 4.1 reports on theperformance measures used.
Section 4.2 shows translation results for the Verbmobiltask.
Sections 4.2.1 and 4.2.2 describe that task and the preprocessing steps applied.In Sections 4.2.3 through 4.2.5, the efficiency of the beam search pruning techniques isshown for German-to-English translation, as the most detailed experiments are con-ducted for that direction.
Section 4.2.6 gives translation results for the translation direc-tion English to German.
In Section 4.3, translation results for the Canadian Hansardstask are reported.119Tillmann and Ney DP Beam Search for Statistical MT4.1 Performance Measures for Translation ExperimentsTo measure the performance of the translation methods, we use three types of au-tomatic and easy-to-use measures of the translation errors.
Additionally, a subjectiveevaluation involving human judges is carried out (Niessen et al 2000).
The followingevaluation criteria are employed:?
WER (word error rate): The WER is computed as the minimum number ofsubstitution, insertion, and deletion operations that have to beperformed to convert the generated string into the reference target string.This performance criterion is widely used in speech recognition.
Theminimum is computed using a DP algorithm and is typically referred toas edit or Levenshtein distance.?
mWER (multireference WER): We use the Levenshtein distance betweenthe automatic translation and several reference translations as a measureof the translation errors.
For example, on the Verbmobil TEST-331 testset, an average of six reference translations per automatic translation areavailable.
The Levenshtein distance between the automatic translationand each of the reference translations is computed, and the minimumLevenshtein distance is taken.
The resulting measure, the mWER, ismore robust than the WER, which takes into account only a singlereference translation.?
PER (position-independent word error rate): In the case in which only asingle reference translation per sentence is available, we introduce as anadditional measure the position-independent word error rate (PER).
Thismeasure compares the words in the two sentences without taking theword order into account.
Words in the reference translation that have nocounterpart in the translated sentence are counted as substitution errors.Depending on whether the translated sentence is longer or shorter thanthe reference translation, the remaining words result in either insertion(if the translated sentence is longer) or deletion (if the translatedsentence is shorter) errors.
The PER is guaranteed to be less than orequal to the WER.
The PER is more robust than the WER since it ignorestranslation errors due to different word order in the translated andreference sentences.?
SSER (subjective sentence error rate): For a more fine-grained evaluation ofthe translation results and to check the validity of the automaticevaluation measures subjective judgments by test persons are carried out(Niessen et al 2000).
The following scale for the error count per sentenceis used in these subjective evaluations:0.0 : semantically correct and syntactically correct?
?
?
: ?
?
?0.5 : semantically correct and syntactically wrong?
?
?
: ?
?
?1.0 : semantically wrong (independent of syntax)Each translated sentence is judged by a human examiner according tothe above error scale; several human judges may be involved in judgingthe same translated sentence.
Subjective evaluation is carried out onlyfor the Verbmobil TEST-147 test set.120Computational Linguistics Volume 29, Number 1Table 5Training and test conditions for the German-to-English Verbmobil corpus (*number of wordswithout punctuation).German EnglishTraining: Sentences 58,073Words 519,523 549,921Words* 418,979 453,632Vocabulary: Size 7,911 4,648Singletons 3,453 1,699TEST-331: Sentences 331Words 5,591 6,279Bigram/Trigram Perplexity 84.0/68.2 49.3/38.3TEST-147: Sentences 147Words 1,968 2,173Bigram/Trigram Perplexity ?
34.6/28.14.2 Verbmobil Translation Experiments4.2.1 The Task and the Corpus.
The translation system is tested on the Verbmobil task(Wahlster 2000).
In that task, the goal is the translation of spontaneous speech in face-to-face situations for an appointment scheduling domain.
We carry out experiments forboth translation directions: German to English and English to German.
Although theVerbmobil task is still a limited-domain task, it is rather difficult in terms of vocabularysize, namely, about 5,000 words or more for each of the two languages; second, thesyntactic structures of the sentences are rather unrestricted.
Although the ultimate goalof the Verbmobil project is the translation of spoken language, the input used for thetranslation experiments reported on in this article is mainly the (more or less) correctorthographic transcription of the spoken sentences.
Thus, the effects of spontaneousspeech are present in the corpus; the effect of speech recognition errors, however, isnot covered.
The corpus consists of 58,073 training pairs; its characteristics are given inTable 5.
For the translation experiments, a trigram language model with a perplexity of28.1 is used.
The following two test corpora are used for the translation experiments:TEST-331: This test set consists of 331 test sentences.
Only automatic evaluation iscarried out on this test corpus: The WER and the mWER are computed.
Foreach test sentence in the source language there is a range of acceptablereference translations (six on average) provided by a human translator,who is asked to produce word-to-word translations wherever it is possi-ble.
Part of the reference sentences are obtained by correcting automatictranslations of the test sentences that are produced using the approach pre-sented in this article with different reordering constraints.
The other partis produced from the source sentences without looking at any of theirtranslations.
The TEST-331 test set is used as held-out data for parameteroptimization (for the language mode scaling factor and for the distortionmodel scaling factor).
Furthermore, the beam search experiments in whichthe effect of the different pruning thresholds is demonstrated are carriedout on the TEST-331 test set.TEST-147: The second, separate test set consists of 147 test sentences.
Translationresults are given in terms of mWER and SSER.
No parameter optimization121Tillmann and Ney DP Beam Search for Statistical MTis carried out on the TEST-147 test set; the parameter values as obtainedfrom the experiments on the TEST-331 test set are used.4.2.2 Preprocessing Steps.
To improve the translation performance the followingpreprocessing steps are carried out:Categorization: We use some categorization, which consists of replacing a singleword by a category.
The only words that are replaced by a category labelare proper nouns denoting German cities.
Using the new labeled corpus,all probability models are trained anew.
To produce translations in the?normal?
language, the categories are translated by rule and are insertedinto the target sentence.Word joining: Target language words are joined using a method similar to the onedescribed in Och, Tillmann, and Ney (1999).
Words are joined to handlecases like the German compound noun ?Zahnarzttermin?
for the English?dentist?s appointment,?
because a single word has to be mapped to twoor more target words.
The word joining is applied only to the target lan-guage words; the source language sentences remain unchanged.
Duringthe search process several joined target language words may be generatedby a single source language word.Manual lexicon: To account for unseen words in the test sentences and to obtain agreater number of focused translation probabilities p(f | e), we use a bilin-gual German-English dictionary.
For each word e in the target vocabulary,we create a list of source translations f according to this dictionary.
Thetranslation probability pdic(f | e) for the dictionary entry (f , e) is defined aspdic(f | e) =??
?1Neif (f , e) is in dictionary0 otherwisewhere Ne is the number of source words listed as translations of the tar-get word e. The dictionary probability pdic(f | e) is linearly combinedwith the automatically trained translation probabilities paut(f | e) to ob-tain smoothed probabilities p(f | e):p(f | e) = (1 ?
?)
?
pdic(f | e) + ?
?
paut(f | e)For the translation experiments, the value of the interpolation parameteris fixed at ?
= 0.5.4.2.3 Effect of the Scaling Factors.
In speech recognition, in which Bayes?
decision ruleis applied, a language model scaling factor ?LM is used; a typical value is ?LM ?
15.This scaling factor is employed because the language model probabilities are morereliably estimated than the acoustic probabilities.
Following this use of a languagemodel scaling factor in speech recognition, such a factor is introduced into statisticalMT, too.
The optimization criterion in equation (1) is modified as follows:e?I1 = arg maxeI1{p(eI1)?LM ?
p(fJ1 | eI1)}where p(eI1) is the language model probability of the target language sentence.
In theexperiments presented here, a trigram language model is used to compute p(eI1).
The122Computational Linguistics Volume 29, Number 1Table 6Computing time, mWER, and SSER for three different reordering constraints on the TEST-147test set.
During the translation experiments, reordered words are not allowed to crosspunctuation marks.Reordering CPU time mWER SSERconstraint [sec] [%] [%]MON 0.2 40.6 28.6GE 5.2 33.3 21.0S3 13.7 34.4 19.9effect of the language model scaling factor ?LM is studied on the TEST-331 test set.
Aminimum mWER is obtained for ?LM = 0.8, as reported in Tillmann (2001).
Unlike inspeech recognition, the translation model probabilities seem to be estimated as reliablyas the language model probabilities in statistical MT.A second scaling factor ?D is introduced for the distortion model probabilitiesp(j | j?, J).
A minimum mWER is obtained for ?D = 0.4, as reported in Tillmann(2001).
The WER and mWER on the TEST-331 test set increase significantly, if nodistortion probability is used, for the case ?D = 0.0.
The benefit of a distortion prob-ability scaling factor of ?D = 0.4 comes from the fact that otherwise, a low distor-tion probability might suppress long-distant word reordering that is important forGerman-to-English verb group reordering.
The setting ?LM = 0.8 and ?D = 0.4 is usedfor all subsequent translation results (including the translation direction English toGerman).4.2.4 Effect of the Word Reordering Constraints.
Table 6 shows the computing time,mWER, and SSER on the TEST-147 test set as a function of three reordering constraints:MON, GE, and S3 (as discussed in the Appendix).
The computing time is given interms of central processing unit (CPU) time per sentence (on a 450 MHz PentiumIII personal computer).
For the SSER, it turns out that restricting the word reorder-ing such that it may not cross punctuation marks improves translation performancesignificantly.
The average length of the sentence fragments that are separated by punc-tuation marks is rather small: 4.5 words per fragment.
A coverage pruning thresholdof tC = 5.0 and an observation pruning of no = 50 are applied during the experiments.4No other type of pruning is used.5The MON constraint performs worst in terms of both mWER and SSER.
Thecomputing time is small, since no reordering is carried out.
Constraints GE and S3perform nearly identically in terms of both mWER and SSER.
The GE constraint,however, works about three times as fast as the S3 constraint.Table 7 shows example translations obtained under the three different reorderingconstraints.
Again, the MON reordering constraint performs worst.
In the second andthird translation examples, the S3 word reordering constraint performs worse than theGE reordering constraint, since it cannot take the word reordering due to the Germanverb group properly into account.
The German finite verbs bin (second example) andko?nnten (third example) are too far away from the personal pronouns ich and Sie (six4 For the translation experiments, the negative logarithm of the actual pruning thresholds tc and tC isreported; for simplicity reasons we do not change the notation.5 In a speech-to-speech demo system, we would use the GE reordering restriction and a slightly sharperpruning in order to achieve translation times of about one second per sentence.123Tillmann and Ney DP Beam Search for Statistical MTTable 7Example translations for the translation direction German to English using three differentreordering constraints: MON, GE, and S3.Input: Ja, wunderbar.
Ko?nnen wir machen.MON: Yes, wonderful.
Can we do.GE: Yes, wonderful.
We can do that.S3: Yes, wonderful.
We can do that.Input: Das ist zu knapp , weil ich ab dem dritten in Kaiserslautern bin.Genaugenommen nur am dritten.Wie wa?re es denn am a?hm Samstag, dem zehnten Februar?MON: That is too tight , because I from the third in Kaiserslautern.In fact only on the third.
How about a?hm Saturday , the tenthof February?GE: That is too tight, because I am from the third in Kaiserslautern.In fact only on the third.
A?hm how about Saturday, Februarythe tenth?S3: That is too tight, from the third because I will be inKaiserslautern.
In fact only on the third.
A?hm howabout Saturday, February the tenth?Input: Wenn Sie dann noch den siebzehnten ko?nnten, wa?re das toll, ja.MON: If you then also the seventeenth could, would be the great, yes.GE: If you could then also the seventeenth, that would be great, yes.S3: Then if you could even take seventeenth, that would be great, yes.Input: Ja, das kommt mir sehr gelegen.
Machen wir es dannam besten so.MON: Yes, that suits me perfectly.
Do we should best like that.GE: Yes, that suits me fine.
We do it like that then best.S3: Yes, that suits me fine.
We should best do it like that.and five source sentence positions, respectively) to be reordered properly.
In the lastexample, the less restrictive S3 reordering constraint leads to a better translation; theGE translation is still acceptable, though.4.2.5 Effect of the Beam Search Pruning Thresholds.
In this section, the effect of thebeam search pruning is demonstrated.
Translation results on the TEST-331 test set arepresented to evaluate the effectiveness of the pruning techniques.6 The quality of thesearch algorithm with respect to the GE and S3 reordering constraints is evaluatedusing two criteria:1.
The number of search errors for a certain combination of pruningthresholds is counted.
A search error occurs for a test sentence if thefinal translation probability QF for a candidate translation eI1 as given inequation (4) is smaller than a reference probability for that test sentence.We will compute reference probabilities two ways, as explained below.2.
The mWER performance measure is computed as a function of thepruning thresholds used.
Generally, decreasing the pruning threshold6 The CPU times on the TEST-331 set are higher, since the average fragment length is greater than for theTEST-147 set.124Computational Linguistics Volume 29, Number 1Table 8Effect of the coverage pruning threshold tC on the number of search errors and mWER on theTEST-331 test set (no cardinality pruning carried out: tc = ?).
A cardinality histogram pruningof 200,000 is applied to restrict the maximum overall size of the search space.
The negativelogarithm of tC is reported.Reordering tC CPU time Search errors mWERconstraint [sec] Qref > QF QF?
> QF [%]GE 0.01 0.21 318 323 73.50.1 0.43 231 301 53.11.0 1.43 10 226 30.32.5 4.75 5 142 25.85.0 29.6 ?
35 24.67.5 156 ?
2 24.910.0 630 ?
?
24.912.5 1300 ?
?
24.9S3 0.01 5.48 314 324 70.00.1 9.21 225 303 50.91.0 46.2 4 223 31.62.5 190 ?
129 28.45.0 830 ?
?
28.3leads to a higher word error rate, since the optimal path through thetranslation lattice is missed, resulting in translation errors.Two automatically generated reference probabilities are used.
These probabilities arecomputed separately for the reordering constraints GE and S3 (the difference is notshown in the notation, but will be clear from the context):Qref: A forced alignment is carried out between each of the test sentences andits corresponding reference translation; only a single reference translationfor each test sentence is used.
The probability obtained for the referencetranslation is denoted by Qref.QF?
: A translation is carried out with conservatively large pruning thresholds,yielding a translation close to the one with the maximum translation prob-ability.
The translation probability for that translation is denoted by QF?
.First, in a series of experiments we study the effect of the coverage and cardinalitypruning for the reordering constraints GE and S3.
(When we report on the differentpruning thresholds, we will show the negative logarithm of those pruning thresholds.
)The experiments are carried out on two different pruning ?dimensions?:1.
In Table 8, only coverage pruning using threshold tC is carried out; nocardinality pruning is applied: tc = ?.2.
In Table 9, only cardinality pruning using threshold tc is carried out; nocoverage pruning is applied: tC = ?.Both tables use an observation pruning of no = 50.
The effect of the coverage prun-ing threshold tC is demonstrated in Table 8.
For the translation experiments reportedin this table, the cardinality pruning threshold is set to tc = ?
; thus, no compari-son between partial hypotheses that do not cover the same set C of source sentence125Tillmann and Ney DP Beam Search for Statistical MTTable 9Effect of the cardinality pruning threshold tc on the number of search errors and mWER onthe TEST-331 test set (no coverage pruning is carried out: tC = ?).
A coverage histogrampruning of 1,000 is applied to restrict the overall size of the search space.
The negativelogarithm of tc is shown.Reordering tc CPU time Search errors mWERconstraint [sec] Qref > QF QF?
> QF [%]GE 1.0 0.03 45 287 48.52.0 0.06 20 277 41.93.0 0.13 16 266 37.74.0 0.30 6 239 34.15.0 0.55 2 212 30.57.5 3.2 ?
106 26.610.0 14.2 ?
32 25.112.5 42.2 ?
5 24.915.0 93.9 ?
?
24.917.5 176.7 ?
?
24.9S3 1.0 0.02 10 331 51.42.0 0.05 1 283 46.23.0 0.10 1 274 43.34.0 0.22 ?
251 40.25.0 0.50 ?
227 37.57.5 4.3 ?
171 32.910.0 26.8 ?
99 30.812.5 123.3 ?
49 28.915.0 430 ?
?
28.2positions is carried out.
To restrict the overall size of the search space in terms ofCPU time and memory requirements, a cardinality pruning of nc = 200,000 is ap-plied.
As can be seen from Table 8, mWER and the number of search errors decreasesignificantly as the coverage pruning threshold tC increases.
For the GE reorderingconstraint, mWER decreases from 73.5% to 24.9%.
For a coverage pruning thresholdtC ?
5.0, mWER remains nearly constant at 25.0%, although search errors still occur.For the S3 reordering constraint, mWER decreases from 70.0% to 28.3%.
The largestcoverage threshold tested for the S3 constraint is tC = 5.0, since for larger thresholdvalues tC , the search procedure cannot be carried out because of memory and timerestrictions.
The number of search errors is reduced as the coverage pruning thresh-old is increased.
It turns out to be difficult to verify search errors by looking at thereference translation probabilities Qref alone.
The translation with the maximum trans-lation probability seems to be quite narrowly defined.
The coverage pruning is moreeffective for the GE constraint than for the S3 constraint, since the overall search spacefor the GE reordering is smaller.Table 9 shows the effect of the cardinality pruning threshold tc on mWER whenno coverage pruning is carried out (a histogram coverage pruning of 1,000 is appliedto restrict the overall size of the search space).
The cardinality threshold tc has astrong effect on mWER, which decreases significantly as the cardinality threshold tcincreases.
For the GE reordering constraint, mWER decreases from 48.5% to 24.9%; forthe S3 reordering constraint, mWER decreases from 51.4% to 28.2%.
For the coveragethreshold t = 15.0, the GE constraint works about four times as fast as the S3 constraint,since the overall search space for the S3 constraint is much larger.
Although the overallsearch space is much larger for the S3 constraint, for smaller values of the coverage126Computational Linguistics Volume 29, Number 1Table 10Effect of observation pruning on the number of search errors and mWER on the TEST-331 testset (parameter setting: tc = ?, tC = 10.0 ).
No histogram pruning is applied.
The results arereported for the GE constraint.Observation CPU time Search errors mWERpruning no [sec] Qref > QF QF?
> QF [%]1 2.0 13 284 29.32 5.9 6 239 26.93 10.8 2 196 25.75 23.6 2 140 25.310 62.9 ?
99 24.825 238 ?
44 24.550 630 ?
?
24.9threshold tC ?
5.0, the S3 constraint works as fast as the GE constraint or even faster,because only a very small portion of the overall search space is searched for smallvalues of the cardinality pruning threshold tc.
There is some computational overheadin expanding a partial hypothesis for the GE constraint because the finite-state controlhas to be handled.
No results are obtained for the S3 constraint and the coveragethreshold tc = 17.5 because of memory restrictions.
The number of search errors isreduced as the cardinality pruning threshold is increased.
Again, it is difficult to verifysearch errors by looking at the reference translation probabilities alone.Both coverage and cardinality pruning are more efficient for the GE reorderingconstraint than for the S3 reordering constraint.
For the S3 constraint, no translationresults are obtained for a coverage threshold tc > 5.0 without cardinality pruningapplied because of memory and computing time restrictions.
For the GE constraintvirtually a full search can be carried out where only observation pruning is applied:Identical target translations and translation probabilities are produced for the hypoth-esis files for the two cases (1) tC = 10.0, tc = ?, and (2) tC = ?, tc = 15.0.
(Actually,for one test sentence in the TEST-331 test set, the translations are different, althoughthe translation probabilities are exactly the same.)
Since the pruning is carried outindependently on two different pruning dimensions, no search errors will occur if thethresholds are further increased.Table 10 shows the effect of the observation pruning parameter no on mWER forthe reordering constraint GE.
mWER is significantly reduced by hypothesizing up tothe best 50 target words e for a source language word f .
mWER increases from 24.9%to 29.3% when the number of hypothesized words is decreased to only a single word.Table 11 demonstrates the effect of the combination of the coverage pruning thresh-old tC = 5.0 and the cardinality pruning threshold tc = 12.5, where the actual valuesare found in informal experiments: In a typical setting of the two parameters tc shouldbe at least twice as big as tC .
For the GE reordering constraint, the average computingtime is about seven seconds per sentence without any loss in translation performanceas measured in terms of mWER.
For the S3 reordering constraint, the average comput-ing time per sentence is 27 seconds.
Again, the combination of coverage and cardinalitypruning works more efficiently for the GE constraint.
The memory requirement forthe algorithm is about 100 MB.4.2.6 English-to-German Translation Experiments.
A series of translation experimentsfor the translation direction English to German are also carried out.
The results, given127Tillmann and Ney DP Beam Search for Statistical MTTable 11Demonstration of the combination of the two pruning thresholds tC = 5.0 and tc = 12.5 tospeed up the search process for the two reordering constraints GE and S3 (no = 50).
Thetranslation performance is shown in terms of mWER on the TEST-331 test set.Reordering tC tc CPU time Search errors mWERconstraint [sec] Qref > QF QF?
> QF [%]GE 5.0 12.5 6.9 0 38 24.7S3 5.0 12.5 26.9 0 65 29.2Table 12Translation results for the translation direction English to German on the TEST-331 test set.The results are given in terms of computing time, WER, and PER for three different reorderingconstraints: MON, EG, and S3.Reordering CPU time WER PERconstraint [sec] [%] [%]MON 0.5 70.6 57.0EG 10.1 70.1 55.9S3 53.2 70.1 55.8in terms of WER and PER, are shown in Table 12.
For the English-to-German translationdirection, a single reference translation for each test sentence is used to carry outthe automatic evaluation.
The translation task for the translation direction Englishto German is more difficult than for the translation direction German to English; thetrigram language model perplexity increases from 38.3 to 68.2 on the TEST-331 test set,as can be seen in Table 5.
No parameter optimization is carried out for this translationdirection; the parameter settings are carried over from the results obtained in Table 11.The word error rates for the translation direction English to German are signif-icantly higher than those for the translation direction German to English.
There areseveral reasons for this: German vocabulary and perplexity are significantly largerthan those for English, and only a single reference translation per test sentence isavailable for English-to-German translation.
There is only a very small difference interms of word error rates for the reordering constraints EG and S3; in particular, WERis 70.1% for both.
The reordering constraint MON performs slightly worse: WER in-creases to 70.6%, and PER increases to 57.0%.
Table 13 shows translation examplesfor the translation direction English to German.
The MON constraint performs worst;there is no significant difference in quality of translations produced under the EG andthe S3 constraints.4.3 Canadian Hansards Translation Experiments4.3.1 The Task and the Corpus.
The second corpus on which we perform translationexperiments is the Hansard corpus.
By law, the proceedings of the Canadian parlia-ment are recorded in both French and English.
(For historical reasons, these proceed-ings are called ?Hansards.?)
The remarks of the parliament members are written downin whichever of the two languages they use.
They are then translated into the otherlanguage to produce complete sets of the proceedings, one in French and the otherin English.
The resulting bilingual data have been sentence-aligned using statisticalmethods (Brown et al 1990).
Originally, about three million sentences were selected.Here, we use a subset of the original training data; the details regarding this subset128Computational Linguistics Volume 29, Number 1Table 13Example translations for the translation direction English to German using three differentreordering constraints: MON, EG, and S3.Input: Yeah , that wouldn?t be bad.
Do you have any ideas whereI could stay?MON: Ja, das wa?re schade.
Haben Sie irgendwelche Ideen wo ichko?nnten u?bernachten?EG: Ja, das wa?re nicht schlecht.
Haben Sie irgendwelche Ideen wowir wohnen ko?nnten?S3: Ja, das wa?re nicht schlecht.
Haben Sie irgendwelche Ideenwo wir wohnen ko?nnten?Input: Oh, that sounds great .
Could you arrange a suite for me?MON: Oh, das klingt gut.
Ko?nnten Sie unbedingt ein Suite bei mir?EG: Oh, das klingt gut.
Ko?nnten Sie einen Suite ausmachen fu?r mich?S3: Oh, das klingt gut.
Ko?nnten Sie mir einen Suite ausmachen?Input: Well, I still need your signature here and then I will checkwith your company.MON: Also, ich konnte Arbeitskraft Unterschrift hier und ich werdenachsehen mit Ihrer Firma.EG: Also, ich bra?uchte noch Ihre Unterschrift und dann gucke ich hiermit Ihrer Firma.S3: Also, ich brauche hier noch Ihre Unterschrift und dann werde ichveranlassen mit Ihrer Firma.Table 14Training and test conditions for the Hansards task (*number of words without punctuation).French EnglishTrain: Sentences 1,470,473Words 24,338,195 22,163,092Words* 22,175,069 20,063,378Vocabulary: Size 100,269 78,332Singletons 40,199 31,319Test: Sentences 5,432Words 97,646 80,559Bigr./Tri.
Perplexity 196.9/121.8 269.9/179.8are given in Table 14.
The Hansards corpus presents by far a more difficult task thanthe Verbmobil corpus in terms of vocabulary size and number of training sentences.The training and test sentences are less restrictive than for the Verbmobil task.
For thetranslation experiments on the Hansards corpus, no word joining is carried out.
Twotarget words can be produced by a single source word, as described in Section 3.9.2.4.3.2 Translation Results.
As can be seen in Table 15 for the translation directionFrench to English and in Table 16 for the translation direction English to French, theword error rates are rather high compared to those for the Verbmobil task.
The reasonfor the higher error rates is that, as noted in the previous section, the Hansards taskis by far less restrictive than the Verbmobil task, and the vocabulary size is much129Tillmann and Ney DP Beam Search for Statistical MTTable 15Computing time, WER, and PER for the translation direction French to English using the tworeordering constraints MON and S3.
An almost ?full?
search is carried out.Reordering CPU time WER PERconstraint [sec] [%] [%]MON 2.5 65.5 53.0S3 580.0 64.9 51.4Table 16Computing time, WER, and PER for the translation direction English to French using the tworeordering constraints MON and S3.
An almost ?full?
search is carried out.Reordering CPU time WER PERconstraint [sec] [%] [%]MON 2.2 66.6 56.3S3 189.1 66.0 54.4larger.
There is only a slight difference in performance between the MON and theS3 reordering constraints on the Hansards task.
The computation time is also ratherhigh compared to the Verbmobil task: For the S3 constraint, the average translationtime is about 3 minutes per sentence for the translation direction English to Frenchand about 10 minutes per sentence for the translation direction French to English.The following parameter setting is used for the experiment conducted here: tC = 5.0,tc = 10.0, nC = 250, and to = 12.
(The actual parameters are chosen in informalexperiments to obtain reasonable CPU times while permitting only a small number ofsearch errors.)
No cardinality histogram pruning is carried out.
As for the German-to-English translation experiments, word reordering is restricted so that it may notcross punctuation boundaries.
The resulting fragment lengths are much larger forthe translation direction English to French, and still larger for the translation directionFrench to English, when compared to the fragment lengths for the translation directionGerman to English, hence the high CPU times.
In an additional experiment for thetranslation direction French to English and the reordering constraint S3, we find we canspeed up the translation time to about 18 seconds per sentence by using the followingparameter setting: tC = 3.0, tc = 7.5, nC = 20, nc = 400, and no = 5.
For the resultinghypotheses file, PER increases only slightly, from 51.4% to 51.6%.Translation examples for the translation direction French to English under the S3reordering constraint are given in Table 17.
The French input sentences show somepreprocessing that is carried out beforehand to simplify the translation task (e.g., desis transformed into de les and l?est is transformed into le est).
The translations pro-duced are rather approximative in some cases, although the general meaning is oftenpreserved.5.
ConclusionsWe have presented a DP-based beam search algorithm for the IBM-4 translation model.The approach is based on a DP solution to the TSP, and it gains efficiency by imposingconstraints on the allowed word reorderings between source and target language.
Adata-driven search organization in conjunction with appropriate pruning techniques130Computational Linguistics Volume 29, Number 1Table 17Example translations for the translation direction French to English using the S3 reorderingconstraint.Input Je crois que cela donne une bonne ide?e de les principes a`retenir et de ce que devraient e?tre nos responsabilite?s.S3 I think it is a good idea of the principles and to whatshould be our responsibility.Input Je pense que, inde?pendamment de notre parti, nous trouvonstous cela inacceptable.S3 I think, regardless of our party, we find that unacceptable.Input Je ai le intention de parler surtout aujourd?
hui de les nombreusesame?liorations apporte?es a` les programmes de pensions de tous lesCanadiens.S3 I have the intention of speaking today about the many improvementsin pensions for all Canadians especially those programs.Input Chacun en lui - me?me est tre`s complexe et le lien entre les deux leest encore davantage de sorte que pour beaucoup la situationpre?sente est confuse.S3 Each in itself is very complex and the relationship between the two is moreso much for the present situation is confused.is proposed.
For the medium-sized Verbmobil task, a sentence can be translated in afew seconds on average, with a small number of search errors and no performancedegradation as measured by the word error criterion used.Word reordering is parameterized using a set of four parameters, in such a waythat it can easily be adopted to new translation directions.
A finite-state control isadded, and its usefulness is demonstrated for the translation direction German toEnglish, in which the word order difference between the two languages is mainly dueto the German verb group.
Future work might aim at a tighter integration of the IBM-4model distortion probabilities and the finite-state control; the finite-state control itselfmay be learned from training data.The applicability of the algorithm applied in the experiments in this article isnot restricted to the IBM translation models or to the simplified translation modelused in the description of the algorithm in Section 3.
Since the efficiency of the beamsearch approach is based on restrictions on the allowed coverage vectors C alone,the approach may be used for different types of translation models as well (e.g., forthe multiword-based translation model proposed in Och, Tillmann, and Ney [1999]).On the other hand, since the decoding problem for the IBM-4 translation model isprovably NP-complete, as shown in Knight (1999) and Germann et al (2001), wordreordering restrictions as introduced in this article are essential for obtaining an effi-cient search algorithm that guarantees that a solution close to the optimal one will befound.Appendix: Quantification of Reordering RestrictionsTo quantify the reordering restrictions in Section 3.5, the four non-negative num-bers numskip, widthskip, nummove, and widthmove are used (widthskip correspondsto L, widthmove corresponds to R in Section 3.4; here, we use a more intuitive nota-tion).
Within the implementation of the DP search, the restrictions are provided to the131Tillmann and Ney DP Beam Search for Statistical MTalgorithm as an input parameter of the following type:S numskip widthskip M nummove widthmoveThe meaning of the reordering string is as follows: The two numbers following S thatare separated by an underscore describe the way words may be skipped; the twonumbers following M that are separated by an underscore describe the way wordsmay be moved during word reordering.
The first number after S and M denotesthe number of positions that may be skipped or moved, respectively (e.g., for thetranslation direction German to English [GE in the chart below], one position maybe skipped and two positions may be moved).
The second number after S and Mrestricts the distance a word may be skipped or moved, respectively.
These ?width?parameters restrict the word reordering to take place within a ?window?
of a certainsize, established by the distance between the positions lmin(C) and rmax(C) as definedin Section 3.5.
In the notation, either the substring headed by S or that headed by M(or both) may be omitted altogether to indicate that the corresponding reordering isnot allowed.
Any numerical value in the string may be set to INF, denoting that anarbitrary number of positions may be skipped/moved or that the moving or skippingdistance may be arbitrarily large.
The following reordering strings are used in thisarticle:Word reordering Descriptionstring The empty string denotes the reordering restriction in which(short: MON) no reordering is allowed.S 01 04 M 02 10 This string describes the German-to-English word reordering.
(short: GE) Up to one word may be skipped for at most 4 positions,and up to two words may be moved up to 10 positions.S 02 10 M 01 04 This string describes the English-to-German word reordering.
(short: EG) Up to two words may be skipped for at most 10 positionsand up to one word may be moved for up to 4 positions.S 03 INF This string describes the IBM-style word reordering(short: S3) given in Section 3.6.
Up to three words may be skipped foran unrestricted number of positions.
No words may be moved.S INF INF or These strings denote the word re-ordering withoutM INF INF restrictions.
(short: NO)The word reordering strings can be directly used as input parameters to the DP-basedsearch procedure to test different reordering restrictions within a single implementa-tion.AcknowledgmentsThis work has been supported as part of theVerbmobil project (contract number01 IV 601 A) by the German FederalMinistry of Education, Science, Researchand Technology and as part of the Eutrans132Computational Linguistics Volume 29, Number 1project (ESPRIT project number 30268) bythe European Community.
Some of theexperiments on the Canadian Hansards taskhave been carried out by Nicola Ueffingusing the existing implementation of thesearch algorithm (Och, Ueffing, and Ney[2001]).
We would like to thank theanonymous reviewers for their detailedcomments on an earlier version of thisarticle.
Also, we would like to thank NiyuGe, Scott McCarley, Salim Roukos, NicolaUeffing, and Todd Ward for their valuableremarks.ReferencesAl-Onaizan, Yaser, Jan Curin, Michael Jahr,Kevin Knight, John Lafferty, DanMelamed, Franz-Josef Och, David Purdy,Noah Smith, and David Yarowsky.
1999.Statistical machine translation.
FinalReport, Johns Hopkins UniversitySummer Workshop (WS 99) on LanguageEngineering, Center for Language andSpeech Processing, Baltimore.Berger, Adam L., Peter F. Brown, StephenA.
Della Pietra, Vincent J. Della Pietra,John R. Gillett, John D. Lafferty, Robert L.Mercer, Harry Printz, and Lubos Ures.1994.
The Candide system for machinetranslation.
In Proceedings of the ARPAHuman Language Technology Workshop,pages 152?157, San Mateo, California,March.Berger, Adam L., Peter F. Brown, StephenA.
Della Pietra, Vincent J. Della Pietra,Andrew S. Kehler, and Robert L. Mercer.1996.
Language translation apparatus andmethod of using context-based translationmodels.
U.S. Patent 5510981.Brown, Peter F., John Cocke, Vincent J. DellaPietra, Stephen A. Della Pietra, FredJelinek, John Lafferty, Robert L. Mercer,and Paul S. Roosin.
1990.
A statisticalapproach to machine translation.Computational Linguistics, 16(2):79?85.Brown, Peter F., Vincent J. Della Pietra,Stephen A. Della Pietra, and Robert L.Mercer.
1993.
The mathematics ofstatistical machine translation: Parameterestimation.
Computational Linguistics,19(2):263?311.Brown, Peter F., Peter V. deSouza, VincentJ.
Della Pietra, and Robert L. Mercer.1992.
Class-based n-gram models ofnatural language.
ComputationalLinguistics, 18(4):467?479.Garc?
?a-Varea, Ismael, Francisco Casacuberta,and Hermann Ney.
1998.
An iterativeDP-based search algorithm for statisticalmachine translation.
In Proceedings of theFifth International Conference on SpokenLanguage Processing (ICSLP 98),pages 1135?1139, Sydney, Australia,November.Germann, Ulrich, Michael Jahr, KevinKnight, Daniel Marcu, and Kenji Yamada.2001.
Fast decoding and optimal decodingfor machine translation.
In Proceedings ofthe 38th Annual Conference of the Associationfor Computational Linguistics (ACL 2001),pages 228?235, Toulouse, France, July.Held, Michael and Richard M. Karp.
1962.A dynamic programming approach tosequencing problems.
SIAM,10(1):196?210.Jelinek, Fred.
1976.
Speech recognition bystatistical methods.
Proceedings of the IEEE,64:532?556.Knight, Kevin.
1999.
Decoding complexityin word-replacement translation models.Computational Linguistics, 25(4):607?615.Ney, Hermann, Dieter Mergel, AndreasNoll, and Annedore Paeseler.
1992.
Datadriven search organization for continuousspeech recognition in the SPICOS system.IEEE Transactions on Signal Processing,40(2):272?281.Ney, Hermann, Sonja Niessen, Franz-JosefOch, Hassan Sawaf, Christoph Tillmann,and Stefan Vogel.
2000.
Algorithms forstatistical translation of spoken language.IEEE Transactions on Speech and AudioProcessing, 8(1):24?36.Niessen, Sonja, Franz-Josef Och, GregorLeusch, and Hermann Ney.
2000.
Anevaluation tool for machine translation:Fast evaluation for MT research.
InProceedings of the Second InternationalConference on Language Resources andEvaluation, pages 39?45, Athens, Greece,May.Niessen, Sonja, Stefan Vogel, Hermann Ney,and Christoph Tillmann.
1998.
ADP-based search algorithm for statisticalmachine translation.
In Proceedings of the36th Annual Conference of the Association forComputational Linguistics and the 17thInternational Conference on ComputationalLinguistics (ACL/COLING 98),pages 960?967, Montreal, Canada, August.Nilsson, Nils J.
1971.
Problem Solving Methodsin Artificial Intelligence.
McGraw Hill, NewYork.Och, Franz-Josef and Hermann Ney.
2000.
Acomparison of alignment models forstatistical machine translation.
InProceedings of the 18th InternationalConference on Computational Linguistics(COLING 2000), pages 1086?1090,Saarbru?cken, Germany, July?August.133Tillmann and Ney DP Beam Search for Statistical MTOch, Franz-Josef, Christoph Tillmann, andHermann Ney.
1999.
Improved alignmentmodels for statistical machine translation.In Proceedings of the Joint Conference onEmpirical Methods in Natural LanguageProcessing and Very Large Corpora(EMNLP/VLC 99), pages 20?28, CollegePark, Maryland, June.Och, Franz-Josef, Nicola Ueffing, andHermann Ney.
2001.
An efficient (A)*search algorithm for statistical machinetranslation.
In Proceedings of theData-Driven Machine Translation Workshop,39th Annual Meeting of the Association forComputational Linguistics (ACL),pages 55?62, Toulouse, France, July.Tillmann, Christoph.
2001.
Word Re-orderingand Dynamic Programming Based SearchAlgorithm for Statistical Machine Translation.Ph.D.
thesis, University of Technology,Aachen, Germany.Tillmann, Christoph and Hermann Ney.2000.
Word re-ordering and DP-basedsearch in statistical machine translation.In Proceedings of the 18th InternationalConference on Computational Linguistics(COLING 2000), pages 850?856,Saarbru?cken, Germany, July?August.Tillmann, Christoph, Stefan Vogel, HermannNey, and Alex Zubiaga.
1997.
A DP-basedsearch using monotone alignments instatistical translation.
In Proceedings of the35th Annual Conference of the Association forComputational Linguistics (ACL 97),pages 289?296, Madrid, July.Tillmann, Christoph, Stefan Vogel, HermannNey, Alex Zubiaga, and Hassan Sawaf.1997.
Accelerated DP-based search forstatistical translation.
In Proceedings of theFifth European Conference on SpeechCommunication and Technology (Eurospeech97), pages 2667?2670, Rhodos, Greece,September.Wahlster, Wolfgang.
2000.
Verbmobil:Foundations of Speech-to-Speech Translation.Springer Verlag, Berlin.Wang, Ye-Yi and Alex Waibel.
1997.Decoding algorithm in statisticaltranslation.
In Proceedings of the 35thAnnual Conference of the Association forComputational Linguistics (ACL 97),pages 366?372, Madrid, July.Wang, Ye-Yi and Alex Waibel.
1998.
Fastdecoding for statistical machinetranslation.
In Proceedings of the FifthInternational Conference on Spoken LanguageProcessing (ICSLP 98), pages 2775?2778,Sydney, Australia, December.Wu, Dekai.
1996.
A polynomial-timealgorithm for statistical machinetranslation.
In Proceedings of the 34thAnnual Conference of the Association forComputational Linguistics (ACL 96),pages 152?158, Santa Cruz, California,June.
