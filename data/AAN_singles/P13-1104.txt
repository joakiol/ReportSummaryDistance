Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1052?1062,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsTransition-based Dependency Parsing with Selectional BranchingJinho D. ChoiDepartment of Computer ScienceUniversity of Massachusetts AmherstAmherst, MA, 01003, USAjdchoi@cs.umass.eduAndrew McCallumDepartment of Computer ScienceUniversity of Massachusetts AmherstAmherst, MA, 01003, USAmccallum@cs.umass.eduAbstractWe present a novel approach, called selec-tional branching, which uses confidence es-timates to decide when to employ a beam,providing the accuracy of beam search atspeeds close to a greedy transition-baseddependency parsing approach.
Selectionalbranching is guaranteed to perform a fewernumber of transitions than beam search yetperforms as accurately.
We also present anew transition-based dependency parsingalgorithm that gives a complexity of O(n)for projective parsing and an expected lin-ear time speed for non-projective parsing.With the standard setup, our parser showsan unlabeled attachment score of 92.96%and a parsing speed of 9 milliseconds persentence, which is faster and more accuratethan the current state-of-the-art transition-based parser that uses beam search.1 IntroductionTransition-based dependency parsing has gainedconsiderable interest because it runs fast and per-forms accurately.
Transition-based parsing givescomplexities as low as O(n) and O(n2) for projec-tive and non-projective parsing, respectively (Nivre,2008).1 The complexity is lower for projective pars-ing because a parser can deterministically skip to-kens violating projectivity, while this property isnot assumed for non-projective parsing.
Nonethe-less, it is possible to perform non-projective parsingin expected linear time because the amount of non-projective dependencies is notably smaller (Nivreand Nilsson, 2005) so a parser can assume projec-tivity for most cases while recognizing ones forwhich projectivity should not be assumed (Nivre,2009; Choi and Palmer, 2011).1We refer parsing approaches that produce only projectivedependency trees as projective parsing and both projective andnon-projective dependency trees as non-projective parsing.Greedy transition-based dependency parsing hasbeen widely deployed because of its speed (Cer etal., 2010); however, state-of-the-art accuracies havebeen achieved by globally optimized parsers usingbeam search (Zhang and Clark, 2008; Huang andSagae, 2010; Zhang and Nivre, 2011; Bohnet andNivre, 2012).
These approaches generate multipletransition sequences given a sentence, and pick onewith the highest confidence.
Coupled with dynamicprogramming, transition-based dependency parsingwith beam search can be done very efficiently andgives significant improvement to parsing accuracy.One downside of beam search is that it alwaysuses a fixed size of beam even when a smaller sizeof beam is sufficient for good results.
In our exper-iments, a greedy parser performs as accurately as aparser that uses beam search for about 64% of time.Thus, it is preferred if the beam size is not fixed butproportional to the number of low confidence pre-dictions that a greedy parser makes, in which case,fewer transition sequences need to be explored toproduce the same or similar parse output.We first present a new transition-based parsingalgorithm that gives a complexity of O(n) for pro-jective parsing and an expected linear time speedfor non-projective parsing.
We then introduce se-lectional branching that uses confidence estimatesto decide when to employ a beam.
With our new ap-proach, we achieve a higher parsing accuracy thanthe current state-of-the-art transition-based parserthat uses beam search and a much faster speed.2 Transition-based dependency parsingWe introduce a transition-based dependency pars-ing algorithm that is a hybrid between Nivre?s arc-eager and list-based algorithms (Nivre, 2003; Nivre,2008).
Nivre?s arc-eager is a projective parsing al-gorithm showing a complexity of O(n).
Nivre?slist-based algorithm is a non-projective parsing al-gorithm showing a complexity of O(n2).
Table 1shows transitions in our algorithm.
The top 4 and1052Transition Current state ?
Resulting stateLEFTl-REDUCE ( [?|i], ?, [j|?
], A ) ?
( ?, ?, [j|?
], A ?
{i l?
j} )RIGHTl-SHIFT ( [?|i], ?, [j|?
], A ) ?
( [?|i|?|j], [ ], ?, A ?
{i l?
j} )NO-SHIFT ( [?|i], ?, [j|?
], A ) ?
( [?|i|?|j], [ ], ?, A )NO-REDUCE ( [?|i], ?, [j|?
], A ) ?
( ?, ?, [j|?
], A )LEFTl-PASS ( [?|i], ?, [j|?
], A ) ?
( ?, [i|?
], [j|?
], A ?
{i l?
j} )RIGHTl-PASS ( [?|i], ?, [j|?
], A ) ?
( ?, [i|?
], [j|?
], A ?
{i l?
j} )NO-PASS ( [?|i], ?, [j|?
], A ) ?
( ?, [i|?
], [j|?
], A )Table 1: Transitions in our dependency parsing algorithm.Transition PreconditionsLEFTl-?
[i 6= 0] ?
?[?k.
(i?
k) ?
A] ?
?[(i??
j) ?
A]RIGHTl-?
?[?k.
(k ?
j) ?
A] ?
?
[(i ??
j) ?
A]?-SHIFT ?
[?k ?
?.
(k 6= i) ?
((k ?
j) ?
(k ?
j))]?-REDUCE [?h.
(h?
i) ?
A] ?
?
[?k ?
?.
(i?
k)]Table 2: Preconditions of the transitions in Table 1 (?
is a wildcard representing any transition).the bottom 3 transitions are inherited from Nivre?sarc-eager and list-based algorithms, respectively.2Each parsing state is represented as a tuple (?,?, ?, A), where ?
is a stack containing processedtokens, ?
is a deque containing tokens popped outof ?
but will be pushed back into ?
in later parsingstates to handle non-projectivity, and ?
is a buffercontaining unprocessed tokens.
A is a set of labeledarcs.
(i, j) represent indices of their correspondingtokens (wi, wj), l is a dependency label, and the 0identifier corresponds to w0, introduced as the rootof a tree.
The initial state is ([0], [ ], [1, .
.
.
, n], ?
),and the final state is (?, ?, [ ], A).
At any parsingstate, a decision is made by comparing the top of?, wi, and the first element of ?, wj .
This decisionis consulted by gold-standard trees during trainingand a classifier during decoding.LEFTl-?
and RIGHTl-?
are performed when wjis the head ofwi with a dependency label l, and viceversa.
After LEFTl-?
or RIGHTl-?, an arc is addedto A. NO-?
is performed when no dependency isfound for wi and wj .
?-SHIFT is performed whenno dependency is found for wj and any token in?
other than wi.
After ?-SHIFT, all tokens in ?as well as wj are pushed into ?.
?-REDUCE isperformed when wi already has the head, and wi isnot the head of any token in ?.
After ?-REDUCE,wi is popped out of ?.
?-PASS is performed whenneither ?-SHIFT nor ?-REDUCE can be performed.After ?-PASS, wi is moved to the front of ?
so it2The parsing complexity of a transition-based dependencyparsing algorithm is determined by the number of transitionsperformed with respect to the number of tokens in a sentence,say n (K?bler et al, 2009).can be compared to other tokens in ?
later.
Eachtransition needs to satisfy certain preconditions toensure the properties of a well-formed dependencygraph (Nivre, 2008); they are described in Table 2.(i?
j) and (i ??
j) indicate that wj is the headand an ancestor of wi with any label, respectively.When a parser is trained on only projective trees,our algorithm learns only the top 4 transitions andproduces only projective trees during decoding.
Inthis case, it performs at most 2n ?
1 transitionsper sentence so the complexity is O(n).
When aparser is trained on a mixture of projective and non-projective trees, our algorithm learns all transitionsand produces both kinds of trees during decoding.In this case, it performs at most n(n+1)2 transitionsso the complexity is O(n2).
However, because ofthe presence of ?-SHIFT and ?-REDUCE, our al-gorithm is capable of skipping or removing tokensduring non-projective parsing, which allows it toshow a linear time parsing speed in practice.700 10 20 30 40 50 60130020406080100Sentence lengthTransitionsFigure 1: The # of transitions performed duringtraining with respect to sentence lengths for Dutch.1053Transition ?
?
?
A0 Initialization [0] [ ] [1|?]
?1 NO-SHIFT [?|1] [ ] [2|?
]2 NO-SHIFT [?|2] [ ] [3|?
]3 NO-SHIFT [?|3] [ ] [4|?
]4 LEFT-REDUCE [?|2] [ ] [4|?]
A ?
{3?NSUBJ?
4}5 NO-PASS [?|1] [2] [4|?
]6 RIGHT-SHIFT [?|4] [ ] [5|?]
A ?
{1 ?RCMOD?
4}7 NO-SHIFT [?|5] [ ] [6|?
]8 LEFT-REDUCE [?|4] [ ] [6|?]
A ?
{5?AUX?
6}9 RIGHT-PASS [?|2] [4] [6|?]
A ?
{4 ?XCOMP?
6}10 LEFT-REDUCE [?|1] [4] [6|?]
A ?
{2?DOBJ?
6}11 NO-SHIFT [?|6] [ ] [7|?
]12 NO-REDUCE [?|4] [ ] [7|?
]13 NO-REDUCE [?|1] [ ] [7|?
]14 LEFT-REDUCE [0] [ ] [7|?]
A ?
{1?NSUBJ?
7}15 RIGHT-SHIFT [?|7] [ ] [8] A ?
{0 ?ROOT?
7}16 RIGHT-SHIFT [?|8] [ ] [ ] A ?
{7 ?ADV?
8}Table 3: A transition sequence generated by our parsing algorithm using gold-standard decisions.Figure 1 shows the total number of transitions per-formed during training with respect to sentencelengths for Dutch.
Among all languages distributedby the CoNLL-X shared task (Buchholz and Marsi,2006), Dutch consists of the highest number ofnon-projective dependencies (5.4% in arcs, 36.4%in trees).
Even with such a high number of non-projective dependencies, our parsing algorithm stillshows a linear growth in transitions.Table 3 shows a transition sequence generatedby our parsing algorithm using gold-standard deci-sions.
Afterw3 andw4 are compared, w3 is poppedout of ?
(state 4) so it is not compared to any othertoken in ?
(states 9 and 13).
After w2 and w4 arecompared, w2 is moved to ?
(state 5) so it can becompared to other tokens in ?
(state 10).
After w4and w6 are compared, RIGHT-PASS is performed(state 9) because there is a dependency betweenw6 and w2 in ?
(state 10).
After w6 and w7 arecompared, w6 is popped out of ?
(state 12) becauseit is not needed for later parsing states.3 Selectional branching3.1 MotivationFor transition-based parsing, state-of-the-art accu-racies have been achieved by parsers optimized onmultiple transition sequences using beam search,which can be done very efficiently when it is cou-pled with dynamic programming (Zhang and Clark,2008; Huang and Sagae, 2010; Zhang and Nivre,2011; Huang et al, 2012; Bohnet and Nivre, 2012).Despite all the benefits, there is one downside ofthis approach; it generates a fixed number of tran-sition sequences no matter how confident the one-best sequence is.3 If every prediction leading tothe one-best sequence is confident, it may not benecessary to explore more sequences to get the bestoutput.
Thus, it is preferred if the beam size is notfixed but proportional to the number of low confi-dence predictions made for the one-best sequence.The selectional branching method presented hereperforms at most d ?
t?
e transitions, where t is themaximum number of transitions performed to gen-erate a transition sequence, d = min(b, |?|+1), b isthe beam size, |?| is the number of low confidencepredictions made for the one-best sequence, ande = d(d?1)2 .
Compared to beam search that alwaysperforms b ?
t transitions, selectional branching isguaranteed to perform fewer transitions given thesame beam size because d ?
b and e > 0 except ford = 1, in which case, no branching happens.
Withselectional branching, our parser shows slightly3The ?one-best sequence?
is a transition sequence gener-ated by taking only the best prediction at each parsing state.1054higher parsing accuracy than the current state-of-the-art transition-based parser using beam search,and performs about 3 times faster.3.2 Branching strategyFigure 2 shows an overview of our branching strat-egy.
sij represents a parsing state, where i is theindex of the current transition sequence and j isthe index of the current parsing state (e.g., s12 rep-resents the 2nd parsing state in the 1st transitionsequence).
pkj represents the k?th best prediction(in our case, it is a predicted transition) given s1j(e.g., p21 is the 2nd-best prediction given s11).s11s12p11s22?
?
s1tp12?
?
s2tp21s33p22?
s3tsdt??
?p2jT1=T2=T3=Td=p1jFigure 2: An overview of our branching strategy.Each sequence Ti>1 branches from T1.Initially, the one-best sequence T1 = [s11, ... , s1t]is generated by a greedy parser.
While generatingT1, the parser adds tuples (s1j , p2j), ... , (s1j , pkj)to a list ?
for each low confidence prediction p1jgiven s1j .4 Then, new transition sequences are gen-erated by using the b highest scoring predictions in?, where b is the beam size.
If |?| < b, all predic-tions in ?
are used.
The same greedy parser is usedto generate these new sequences although it nowstarts with s1j instead of an initial parsing state,applies pkj to s1j , and performs further transitions.Once all transition sequences are generated, a parsetree is built from a sequence with the highest score.For our experiments, we set k = 2, which gavenoticeably more accurate results than k = 1.
Wealso experimented with k > 2, which did not showsignificant improvement over k = 2.
Note that as-signing a greater k may increase |?| but not the totalnumber of transition sequences generated, whichis restricted by the beam size, b.
Since each se-quence Ti>1 branches from T1, selectional branch-ing performs fewer transitions than beam search:at least d(d?1)2 transitions are inherited from T1,4?
is initially empty, which is hidden in Figure 2.where d = min(b, |?| + 1); thus, it performs thatmany transitions less than beam search (see theleft lower triangle in Figure 2).
Furthermore, se-lectional branching generates a d number of se-quences, where d is proportional to the number oflow confidence predictions made by T1.
To sum up,selectional branching generates the same or fewertransition sequences than beam search and eachsequence Ti>1 performs fewer transitions than T1;thus, it performs faster than beam search in generalgiven the same beam size.3.3 Finding low confidence predictionsFor each parsing state sij , a prediction is made bygenerating a feature vector xij ?
X , feeding it intoa classifier C1 that uses a feature map ?
(x, y) anda weight vector w to measure a score for each labely ?
Y , and choosing a label with the highest score.When there is a tie between labels with the highestscore, the first one is chosen.
This can be expressedas a logistic regression:C1(x) = arg maxy?Y{f(x, y)}f(x, y) = exp(w ?
?
(x, y))?y?
?Y exp(w ?
?
(x, y?
))To find low confidence predictions, we use the mar-gins (score differences) between the best predictionand the other predictions.
If all margins are greaterthan a threshold, the best prediction is consideredhighly confident; otherwise, it is not.
Given thisanalogy, the k-best predictions can be found asfollows (m ?
0 is a margin threshold):Ck(x,m) = K arg maxy?Y{f(x, y)}s.t.
f(x,C1(x))?
f(x, y) ?
m?K arg max?
returns a set of k?
labels whose mar-gins to C1(x) are smaller than any other label?smargin to C1(x) and also ?
m, where k?
?
k.When m = 0, it returns a set of the highest scoringlabels only, including C1(x).
When m = 1, it re-turns a set of all labels.
Given this, a prediction isconsidered not confident if |Ck(x,m)| > 1.3.4 Finding the best transition sequenceLet Pi be a list of all predictions that lead to gen-erate a transition sequence Ti.
The predictions inPi are either inherited from T1 or made specifi-cally for Ti.
In Figure 2, P3 consists of p11 as itsfirst prediction, p22 as its second prediction, and1055further predictions made specifically for T3.
Thescore of each prediction is measured by f(x, y) inSection 3.3.
Then, the score of Ti is measured byaveraging scores of all predictions in Pi.score(Ti) =?p?Pi score(p)|Pi|Unlike Zhang and Clark (2008), we take the av-erage instead of the sum of all prediction scores.This is because our algorithm does not guaranteethe same number of transitions for every sequence,so the sum of all scores would weigh more on se-quences with more transitions.
We experimentedwith both the sum and the average, and taking theaverage led to slightly higher parsing accuracy.3.5 Bootstrapping transition sequencesDuring training, a training instance is generatedfor each parsing state sij by taking a feature vec-tor xij and its true label yij .
To generate multipletransition sequences during training, the bootstrap-ping technique of Choi and Palmer (2011) is used,which is described in Algorithm 1.5Algorithm 1 BootstrappingInput: Dt: training set, Dd: development set.Output: A model M .1: r ?
02: I ?
getTrainingInstances(Dt)3: M0 ?
buildModel(I)4: S0 ?
getScore(Dd,M0)5: while (r = 0) or (Sr?1 < Sr) do6: r ?
r + 17: I ?
getTrainingInstances(Dt,Mr?1)8: Mr ?
buildModel(I)9: Sr ?
getScore(Dd,Mr)10: return Mr?1First, an initial model M0 is trained on all data bytaking the one-best sequences, and its score is mea-sured by testing on a development set (lines 2-4).Then, the next model Mr is trained on all data butthis time, Mr?1 is used to generate multiple tran-sition sequences (line 7-8).
Among all transitionsequences generated by Mr?1, training instancesfrom only T1 and Tg are used to trainMr, where T1is the one-best sequence and Tg is a sequence giv-ing the most accurate parse output compared to thegold-standard tree.
The score of Mr is measured(line 9), and repeat the procedure if Sr?1 < Sr;otherwise, return the previous model Mr?1.5Alternatively, the dynamic oracle approach of Goldbergand Nivre (2012) can be used to generate multiple transitionsequences, which is expected to show similar results.3.6 Adaptive subgradient algorithmTo build each model during bootstrapping, we usea stochastic adaptive subgradient algorithm calledADAGRAD that uses per-coordinate learning ratesto exploit rarely seen features while remaining scal-able (Duchi et al, 2011).This is suitable for NLPtasks where rarely seen features often play an im-portant role and training data consists of a largenumber of instances with high dimensional features.Algorithm 2 shows our adaptation of ADAGRADwith logistic regression for multi-class classifica-tion.
Note that when used with logistic regression,ADAGRAD takes a regular gradient instead of a sub-gradient method for updating weights.
For our ex-periments, ADAGRAD slightly outperformed learn-ing algorithms such as average perceptron (Collins,2002) or Liblinear SVM (Hsieh et al, 2008).Algorithm 2 ADAGRAD + logistic regressionInput: D = {(xi, yi)}ni=1 s.t.
xi ?
X , yi ?
Y?
(x, y) ?
Rd s.t.
d = dimension(X )?
|Y|T : iterations, ?
: learning rate, ?
: ridgeOutput: A weight vector w ?
Rd.1: w?
0, where w ?
Rd2: G?
0, where G ?
Rd3: for t?
1 .
.
.
T do4: for i?
1 .
.
.
n do5: Q?y?Y ?
I(yi, y)?
f(xi, y), s.t.
Q ?
R|Y|6: ?
??y?Y(?
(xi, y) ?Qy)7: G?
G + ?
?
?8: for j ?
1 .
.
.
d do9: wj ?
wj + ?
?
1?+?Gj ?
?jI(y, y?)
={1 y = y?0 otherwiseThe algorithm takes three hyper-parameters; T isthe number of iterations, ?
is the learning rate, and?
is the ridge (T > 0, ?
> 0, ?
?
0).
G is our run-ning estimate of a diagonal covariance matrix forthe gradients (per-coordinate learning rates).
Foreach instance, scores for all labels are measuredby the logistic regression function f(x, y) in Sec-tion 3.3.
These scores are subtracted from an outputof the indicator function I(y, y?
), which forces ourmodel to keep learning this instance until the pre-diction is 100% confident (in other words, untilthe score of yi becomes 1).
Then, a subgradientis measured by taking all feature vectors togetherweighted by Q (line 6).
This subgradient is used toupdate G and w, where ?
is the Hadamard product(lines 7-9).
?
is a ridge term to keep the inversecovariance well-conditioned.10564 Experiments4.1 CorporaFor projective parsing experiments, the Penn En-glish Treebank (Marcus et al, 1993) is used withthe standard split: sections 2-21 for training, 22 fordevelopment, and 23 for evaluation.
All constituenttrees are converted with the head-finding rules ofYamada and Matsumoto (2003) and the labelingrules of Nivre (2006).
For non-projective pars-ing experiments, four languages from the CoNLL-X shared task are used: Danish, Dutch, Slovene,and Swedish (Buchholz and Marsi, 2006).
Theselanguages are selected because they contain non-projective trees and are publicly available from theCoNLL-X webpage.6 Since the CoNLL-X data wehave does not come with development sets, the last10% of each training set is used for development.4.2 Feature engineeringFor English, we mostly adapt features from Zhangand Nivre (2011) who have shown state-of-the-artparsing accuracy for transition-based dependencyparsing.
Their distance features are not includedin our approach because they do not seem to showmeaningful improvement.
Feature selection is doneon the English development set.For the other languages, the same features areused with the addition of morphological featuresprovided by CoNLL-X; specifically, morphologicalfeatures from the top of ?
and the front of ?
areadded as unigram features.
Moreover, all POS tagfeatures from English are duplicated with coarse-grained POS tags provided by CoNLL-X.
No morefeature engineering is done for these languages; itis possible to achieve higher performance by usingdifferent features, especially when these languagescontain non-projective dependencies whereas En-glish does not, which we will explore in the future.4.3 DevelopmentSeveral parameters need to be optimized during de-velopment.
For ADAGRAD, T , ?, and ?
need to betuned (Section 3.6).
For bootstrapping, the numberof iterations, say r, needs to be tuned (Section 3.5).For selectional branching, the margin threshold mand the beam size b need to be tuned (Section 3.3).First, all parameters are tuned on the English devel-opment set by using grid search on T = [1, .
.
.
, 10],?
= [0, 01, 0, 02], ?
= [0.1, 0.2], r = [1, 2, 3],6http://ilk.uvt.nl/conll/m = [0.83, .
.
.
, 0.92], and b = [16, 32, 64, 80].As a result, the following parameters are found:?
= 0.02, ?
= 0.1, m = 0.88, and b = 64|80.
Forthis development set, the beam size of 64 and 80gave the exact same result, so we kept the one witha larger beam size (b = 80).0.920.83 0.86 0.88 0.991.29191.0491.0891.1291.16MarginAccuracy64|803216b =Figure 3: Parsing accuracies with respect to mar-gins and beam sizes on the English development set.b = 64|80: the black solid line with solid circles,b = 32: the blue dotted line with hollow circles,b = 16: the red dotted line with solid circles.Figure 3 shows parsing accuracies with respect todifferent margins and beam sizes on the English de-velopment set.
These parameters need to be tunedjointly because different margins prefer differentbeam sizes.
For instance, m = 0.85 gives the high-est accuracy with b = 32, but m = 0.88 gives thehighest accuracy with b = 64|80.140 2 4 6 8 10 129288.58989.59090.59191.5IterationAccuracyUASLASFigure 4: Parsing accuracies with respect to ADA-GRAD and bootstrap iterations on the English de-velopment set when ?
= 0.02, ?
= 0.1, m = 0.88,and b = 64|80.
UAS: unlabeled attachment score,LAS: labeled attachment score.Figure 4 shows parsing accuracies with respect toADAGRAD and bootstrap iterations on the Englishdevelopment set.
The range 1-5 shows results of5 ADAGRAD iterations before bootstrapping, therange 6-9 shows results of 4 iterations during the1057first bootstrapping, and the range 10-14 shows re-sults of 5 iterations during the second bootstrap-ping.
Thus, the number of bootstrap iteration is2 where each bootstrapping takes a different num-ber of ADAGRAD iterations.
Using an Intel Xeon2.57GHz machine, it takes less than 40 minutesto train the entire Penn Treebank, which includestimes for IO, feature extraction and bootstrapping.800 10 20 30 40 50 60 701,200,0000200,000400,000600,000800,0001,000,000Beam size = 1, 2, 4, 8, 16, 32, 64, 80TransitionsFigure 5: The total number of transitions performedduring decoding with respect to beam sizes on theEnglish development set.Figure 5 shows the total number of transitions per-formed during decoding with respect to beam sizeson the English development set (1,700 sentences,40,117 tokens).
With selectional branching, thenumber of transitions grows logarithmically as thebeam size increases whereas it would have grownlinearly if beam search were used.
We also checkedhow often the one best sequence is chosen as thefinal sequence during decoding.
Out of 1,700 sen-tences, the one best sequences are chosen for 1,095sentences.
This implies that about 64% of time,our greedy parser performs as accurately as ournon-greedy parser using selectional branching.For the other languages, we use the same valuesas English for ?, ?, m, and b; only the ADAGRADand bootstrap iterations are tuned on the develop-ment sets of the other languages.4.4 Projective parsing experimentsBefore parsing, POS tags were assigned to the train-ing set by using 20-way jackknifing.
For the auto-matic generation of POS tags, we used the domain-specific model of Choi and Palmer (2012a)?s tagger,which gave 97.5% accuracy on the English evalua-tion set (0.2% higher than Collins (2002)?s tagger).Table 4 shows comparison between past and cur-rent state-of-the-art parsers and our approach.
Thefirst block shows results from transition-based de-pendency parsers using beam search.
The secondblock shows results from other kinds of parsingapproaches (e.g., graph-based parsing, ensembleparsing, linear programming, dual decomposition).The third block shows results from parsers usingexternal data.
The last block shows results fromour approach.
The Time column show how manyseconds per sentence each parser takes.7Approach UAS LAS TimeZhang and Clark (2008) 92.1Huang and Sagae (2010) 92.1 0.04Zhang and Nivre (2011) 92.9 91.8 0.03Bohnet and Nivre (2012) 93.38 92.44 0.4McDonald et al (2005) 90.9Mcdonald and Pereira (2006) 91.5Sagae and Lavie (2006) 92.7Koo and Collins (2010) 93.04Zhang and McDonald (2012) 93.06 91.86Martins et al (2010) 93.26Rush et al (2010) 93.8Koo et al (2008) 93.16Carreras et al (2008) 93.54Bohnet and Nivre (2012) 93.67 92.68Suzuki et al (2009) 93.79bt = 80, bd = 80, m = 0.88 92.96 91.93 0.009bt = 80, bd = 64, m = 0.88 92.96 91.93 0.009bt = 80, bd = 32, m = 0.88 92.96 91.94 0.009bt = 80, bd = 16, m = 0.88 92.96 91.94 0.008bt = 80, bd = 8, m = 0.88 92.89 91.87 0.006bt = 80, bd = 4, m = 0.88 92.76 91.76 0.004bt = 80, bd = 2, m = 0.88 92.56 91.54 0.003bt = 80, bd = 1, m = 0.88 92.26 91.25 0.002bt = 1, bd = 1, m = 0.88 92.06 91.05 0.002Table 4: Parsing accuracies and speeds on the En-glish evaluation set, excluding tokens containingonly punctuation.
bt and bd indicate the beam sizesused during training and decoding, respectively.UAS: unlabeled attachment score, LAS: labeledattachment score, Time: seconds per sentence.For evaluation, we use the model trained with b =80 and m = 0.88, which is the best setting foundduring development.
Our parser shows higher ac-curacy than Zhang and Nivre (2011), which isthe current state-of-the-art transition-based parserthat uses beam search.
Bohnet and Nivre (2012)?stransition-based system jointly performs POS tag-ging and dependency parsing, which shows higheraccuracy than ours.
Our parser gives a comparativeaccuracy to Koo and Collins (2010) that is a 3rd-order graph-based parsing approach.
In terms ofspeed, our parser outperforms all other transition-based parsers; it takes about 9 milliseconds per7Dhillon et al (2012) and Rush and Petrov (2012) alsohave shown good results on this data but they are excludedfrom our comparison because they use different kinds ofconstituent-to-dependency conversion methods.1058Approach Danish Dutch Slovene SwedishLAS UAS LAS UAS LAS UAS LAS UASNivre et al (2006) 84.77 89.80 78.59 81.35 70.30 78.72 84.58 89.50McDonald et al (2006) 84.79 90.58 79.19 83.57 73.44 83.17 82.55 88.93Nivre (2009) 84.2 - - - 75.2 - - -F.-Gonz?lez and G.-Rodr?guez (2012) 85.17 90.10 - - - - 83.55 89.30Nivre and McDonald (2008) 86.67 - 81.63 - 75.94 84.66Martins et al (2010) - 91.50 - 84.91 - 85.53 - 89.80bt = 80, bd = 1, m = 0.88 86.75 91.04 80.75 83.59 75.66 83.29 86.32 91.12bt = 80, bd = 80, m = 0.88 87.27 91.36 82.45 85.33 77.46 84.65 86.80 91.36Table 5: Parsing accuracies on four languages with non-projective dependencies, excluding punctuation.sentence using the beam size of 80.
Our parser isimplemented in Java and tested on an Intel Xeon2.57GHz.
Note that we do not include input/outputtime for our speed comparison.For a proof of concept, we run the same model,trained with bt = 80, but decode with differentbeam sizes using the same margin.
Surprisingly,our parser gives the same accuracy (0.01% higherfor labeled attachment score) on this data even withbd = 16.
More importantly, bd = 16 shows aboutthe same parsing speed as bd = 80, which indicatesthat selectional branching automatically reduceddown the beam size by estimating low confidencepredictions, so even if we assigned a larger beamsize for decoding, it would have performed as effi-ciently.
This implies that we no longer need to beso conscious about the beam size during decoding.Another interesting part is that (bt = 80, bd = 1)shows higher accuracy than (bt = 1, bd = 1); thisimplies that our training method of bootstrappingtransition sequences can improve even a greedyparser.
Notice that our greedy parser shows higheraccuracy than many other greedy parsers (Hall etal., 2006; Goldberg and Elhadad, 2010) becauseit uses the non-local features of Zhang and Nivre(2011) and the bootstrapping technique of Choiand Palmer (2011) that had not been used for mostother greedy parsing approaches.4.5 Non-projective parsing experimentsTable 5 shows comparison between state-of-the-artparsers and our approach for four languages withnon-projective dependencies.
Nivre et al (2006)uses a pseudo-projective transition-based parsingapproach.
McDonald et al (2006) uses a 2nd-ordermaximum spanning tree approach.
Nivre (2009)and Fern?ndez-Gonz?lez and G?mez-Rodr?guez(2012) use different non-projective transition-basedparsing approaches.
Nivre and McDonald (2008)uses an ensemble model between transition-basedand graph-based parsing approaches.
Martins etal.
(2010) uses integer linear programming for theoptimization of their parsing model.Some of these approaches use greedy parsers, sowe include our results from models using (bt = 80,bd = 1, m = 0.88), which finds only the one-bestsequences during decoding although it is trained onmultiple transition sequences (see Section 4.4).
Ourparser shows higher accuracies for most languagesexcept for unlabeled attachment scores in Danishand Slovene.
Our greedy approach outperformsboth Nivre (2009) and Fern?ndez-Gonz?lez andG?mez-Rodr?guez (2012) who use different non-projective parsing algorithms.600 10 20 30 40 50130020406080100Sentence lengthTransitionsFigure 6: The # of transitions performed during de-coding with respect to sentence lengths for Dutch.Figure 6 shows the number of transitions performedduring decoding with respect to sentence lengthsfor Dutch using bd = 1.
Our parser still shows alinear growth in transition during decoding.5 Related workOur parsing algorithm is most similar to Choi andPalmer (2011) who integrated our LEFT-REDUCEtransition into Nivre?s list-based algorithm.
Ouralgorithm is distinguished from theirs because oursgives different parsing complexities of O(n) andO(n2) for projective and non-projective parsing,respectively, whereas their algorithm gives O(n2)1059for both cases; this is possible because of the newintegration of the RIGHT-SHIFT and NO-REDUCEtransitions.
There are other transition-based de-pendency parsing algorithms that take a similar ap-proach; Nivre (2009) integrated a SWAP transitioninto Nivre?s arc-standard algorithm (Nivre, 2004)and Fern?ndez-Gonz?lez and G?mez-Rodr?guez(2012) integrated a buffer transition into Nivre?sarc-eager algorithm to handle non-projectivity.Our selectional branching method is most rele-vant to Zhang and Clark (2008) who introduceda transition-based dependency parsing model thatuses beam search.
Huang and Sagae (2010) laterapplied dynamic programming to this approachand showed improved efficiency.
Zhang and Nivre(2011) added non-local features to this approachand showed improved parsing accuracy.
Bohnetand Nivre (2012) introduced a transition-based sys-tem that jointly performed POS tagging and de-pendency parsing.
Our work is distinguished fromtheirs because we use selectional branching instead.6 ConclusionWe present selectional branching that uses confi-dence estimates to decide when to employ a beam.Coupled with our new hybrid parsing algorithm,ADAGRAD, rich non-local features, and bootstrap-ping, our parser gives higher parsing accuracy thanmost other transition-based dependency parsers inmultiple languages and shows faster parsing speed.It is interesting to see that our greedy parser out-performed most other greedy dependency parsers.This is because our parser used both bootstrappingand Zhang and Nivre (2011)?s non-local features,which had not been used by other greedy parsers.In the future, we will experiment with more ad-vanced dependency representations (de Marneffeand Manning, 2008; Choi and Palmer, 2012b) toshow robustness of our approach.
Furthermore, wewill evaluate individual methods of our approachseparately to show impact of each method on pars-ing performance.
We also plan to implement thetypical beam search approach to make a direct com-parison to our selectional branching.8AcknowledgmentsSpecial thanks are due to Luke Vilnis of the Uni-versity of Massachusetts Amherst for insights on8Our parser is publicly available under an open sourceproject, ClearNLP (clearnlp.googlecode.com).the ADAGRAD derivation.
We gratefully acknowl-edge a grant from the Defense Advanced ResearchProjects Agency (DARPA) under the DEFT project,solicitation #: DARPA-BAA-12-47.ReferencesBernd Bohnet and Joakim Nivre.
2012.
A Transition-Based System for Joint Part-of-Speech Tagging andLabeled Non-Projective Dependency Parsing.
InProceedings of the 2012 Joint Conference on Em-pirical Methods in Natural Language Processingand Computational Natural Language Learning,EMNLP?12, pages 1455?1465.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X shared task on multilingual dependency parsing.In Proceedings of the Tenth Conference on Com-putational Natural Language Learning, CoNLL?06,pages 149?164.Xavier Carreras, Michael Collins, and Terry Koo.
2008.TAG, Dynamic Programming, and the Perceptronfor Efficient, Feature-rich Parsing.
In Proceedingsof the 12th Conference on Computational NaturalLanguage Learning, CoNLL?08, pages 9?16.Daniel Cer, Marie-Catherine de Marneffe, Daniel Ju-rafsky, and Christopher D. Manning.
2010.
Pars-ing to Stanford Dependencies: Trade-offs betweenspeed and accuracy.
In Proceedings of the 7th In-ternational Conference on Language Resources andEvaluation, LREC?10.Jinho D. Choi and Martha Palmer.
2011.
Getting theMost out of Transition-based Dependency Parsing.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, ACL:HLT?11, pages 687?692.Jinho D. Choi and Martha Palmer.
2012a.
Fast and Ro-bust Part-of-Speech Tagging Using Dynamic ModelSelection.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguistics,ACL?12, pages 363?367.Jinho D. Choi and Martha Palmer.
2012b.
Guidelinesfor the Clear Style Constituent to Dependency Con-version.
Technical Report 01-12, University of Col-orado Boulder.Michael Collins.
2002.
Discriminative Training Meth-ods for Hidden Markov Models: Theory and Exper-iments with Perceptron Algorithms.
In Proceedingsof the conference on Empirical methods in naturallanguage processing, EMNLP?02, pages 1?8.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependenciesrepresentation.
In Proceedings of the COLINGworkshop on Cross-Framework and Cross-DomainParser Evaluation.1060Paramveer S. Dhillon, Jordan Rodu, Michael Collins,Dean P. Foster, and Lyle H. Ungar.
2012.
SpectralDependency Parsing with Latent Variables.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing, EMNLP?12, pages205?213.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive Subgradient Methods for Online Learningand Stochastic Optimization.
The Journal of Ma-chine Learning Research, 12(39):2121?2159.Daniel Fern?ndez-Gonz?lez and Carlos G?mez-Rodr?guez.
2012.
Improving Transition-BasedDependency Parsing with Buffer Transitions.
InProceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Processingand Computational Natural Language Learning,EMNLP?12, pages 308?319.Yoav Goldberg and Michael Elhadad.
2010.
An Effi-cient Algorithm for Easy-First Non-Directional De-pendency Parsing.
In Human Language Technolo-gies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, HLT:NAACL?10, pages 742?750.Yoav Goldberg and Joakim Nivre.
2012.
A DynamicOracle for Arc-Eager Dependency Parsing.
In Pro-ceedings of the 24th International Conference onComputational Linguistics, COLING?12.Johan Hall, Joakim Nivre, and Jens Nilsson.
2006.Discriminative Classifiers for Deterministic Depen-dency Parsing.
In In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics, COLING-ACL?06, pages 316?323.Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin,S.
Sathiya Keerthi, and S. Sundararajan.
2008.
ADual Coordinate Descent Method for Large-scaleLinear SVM.
In Proceedings of the 25th interna-tional conference on Machine learning, ICML?08,pages 408?415.Liang Huang and Kenji Sagae.
2010.
Dynamic Pro-gramming for Linear-Time Incremental Parsing.
InProceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, ACL?10.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured Perceptron with Inexact Search.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL-HLT?12, pages 142?151.Terry Koo and Michael Collins.
2010.
Efficient Third-order Dependency Parsers.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, ACL?10.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple Semi-supervised Dependency Parsing.
InProceedings of the Annual Meeting of the Associ-ation for Computational Linguistics, ACL:HLT?08,pages 595?603.Sandra K?bler, Ryan T. McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Synthesis Lectureson Human Language Technologies.
Morgan & Clay-pool Publishers.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a Large Anno-tated Corpus of English: The Penn Treebank.
Com-putational Linguistics, 19(2):313?330.Andr?
F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-dro M. Q. Aguiar, and M?rio A. T. Figueiredo.
2010.Turbo Parsers: Dependency Parsing by ApproximateVariational Inference.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP?10, pages 34?44.Ryan Mcdonald and Fernando Pereira.
2006.
OnlineLearning of Approximate Dependency Parsing Algo-rithms.
In Proceedings of the Annual Meeting of theEuropean American Chapter of the Association forComputational Linguistics, EACL?06, pages 81?88.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online Large-Margin Training ofDependency Parsers.
In Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, pages 91?98.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual Dependency Analysis with aTwo-Stage Discriminative Parser.
In Proceedingsof the Tenth Conference on Computational NaturalLanguage Learning, CoNLL?06, pages 216?220.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing Graph-based and Transition-based DependencyParsers.
In Proceedings of the 46th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, ACL:HLT?08,pages 950?958.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-Projective Dependency Parsing.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics, ACL?05, pages 99?106.Joakim Nivre, Johan Hall, Jens Nilsson, G?ls?en Eryig?it,and Svetoslav Marinov.
2006.
Labeled pseudo-projective dependency parsing with support vec-tor machines.
In Proceedings of the 10th Confer-ence on Computational Natural Language Learning,CoNLL?06, pages 221?225.Joakim Nivre.
2003.
An Efficient Algorithm for Pro-jective Dependency Parsing.
In Proceedings of the8th International Workshop on Parsing Technolo-gies, IWPT?03, pages 149?160.1061Joakim Nivre.
2004.
Incrementality in DeterministicDependency Parsing.
In Proceedings of the ACL?04Workshop on Incremental Parsing: Bringing Engi-neering and Cognition Together, pages 50?57.Joakim Nivre.
2006.
Inductive Dependency Parsing.Springer.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
Computational Lin-guistics, 34(4):513?553.Joakim Nivre.
2009.
Non-Projective Dependency Pars-ing in Expected Linear Time.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, ACL-IJCNLP?09, pages 351?359.Alexander M. Rush and Slav Petrov.
2012.
Vine Prun-ing for Efficient Multi-Pass Dependency Parsing.
InProceedings of the 12th Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, NAACL:HLT?12.Alexander M. Rush, David Sontag, Michael Collins,and Tommi Jaakkola.
2010.
On Dual Decomposi-tion and Linear Programming Relaxations for Nat-ural Language Processing.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP?10, pages 1?11.Kenji Sagae and Alon Lavie.
2006.
Parser Combina-tion by Reparsing.
In In Proceedings of the HumanLanguage Technology Conference of the NAACL,NAACL?06, pages 129?132.Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichael Collins.
2009.
An Empirical Study ofSemi-supervised Structured Conditional Models forDependency Parsing.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP?09, pages 551?560.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chine.
In Proceedings of the 8th International Work-shop on Parsing Technologies, IWPT?03, pages 195?206.Yue Zhang and Stephen Clark.
2008.
A Tale ofTwo Parsers: investigating and combining graph-based and transition-based dependency parsing us-ing beam-search.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP?08, pages 562?571.Hao Zhang and Ryan McDonald.
2012.
GeneralizedHigher-Order Dependency Parsing with Cube Prun-ing.
In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,EMNLP-CoNLL?12, pages 320?331.Yue Zhang and Joakim Nivre.
2011.
Transition-basedDependency Parsing with Rich Non-local Features.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, ACL?11, pages 188?193.1062
