Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 83?88,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsCross-lingual projection for class-based language modelsBeat Gfeller and Vlad Schogol and Keith HallGoogle Inc.{beatg,vlads,kbhall}@google.comAbstractThis paper presents a cross-lingual pro-jection technique for training class-basedlanguage models.
We borrow from pre-vious success in projecting POS tags andNER mentions to that of a trained class-based language model.
We use a CRFto train a model to predict when a se-quence of words is a member of a givenclass and use this to label our languagemodel training data.
We show that we cansuccessfully project the contextual cuesfor these classes across pairs of languagesand retain a high quality class model inlanguages with no supervised class data.We present empirical results that show thequality of the projected models as wellas their effect on the down-stream speechrecognition objective.
We are able toachieve over 70% of the WER reductionwhen using the projected class models ascompared to models trained on human an-notations.1 IntroductionClass-based language modeling has a long historyof being used to improve the quality of speechrecognition systems (Brown et al, 1992; Knesserand Ney, 1993).
Recent work on class-based mod-els has exploited named entity recognition (NER)approaches to label language model training datawith class labels (Levit et al, 2014; Vasserman etal., 2015), providing a means to assign words andphrases to classes based on their context.
Thesecontextually assigned classes have been shownto improve speech recognition significantly overgrammar-based, deterministic class assignments.In this work, we address the problem of la-beling training data in order to build a class se-quence tagger.
We borrow from the successes ofprevious cross-lingual projection experiments forlabeling tasks (Yarowsky et al, 2001; Yarowskyand Ngai, 2001; Burkett et al, 2010; Pad?o andLapata, 2009).
We focus on numeric classes(e.g., address numbers, dates, currencies, times,etc.)
as the sequence-based labeling approach hasbeen shown to be effective for identifying them.Given a model trained from human-labeled datain one language (we refer to this as the high-resource language), we label translations of sen-tences from another language (referred to as thelow-resource language).
We show that we canproject the numeric entity boundaries and labelsacross the aligned translations with a phrase-basedtranslation model.
Furthermore, we show that ifwe train a class labeling model on the projectedlow-resource language and then use that to build aclass-based speech recognition system, we achievebetween 70% and 85% of the error reduction aswe would have achieved with human-labeled ex-amples in the low-resource language.We present empirical results projecting numericentity labels from English to Russian, Indonesian,and Italian.
We present full speech recognitionresults for using human annotated data (the idealperformance) and projected data with various sizesof training data.2 Related workThere is an increasingly large body of work basedon exploiting alignments between translations ofsentences in multiple languages (Yarowsky et al,2001; Yarowsky and Ngai, 2001; Burkett et al,2010; Das and Petrov, 2011).
In this work weemploy the simple approach of projecting anno-tations across alignments of translated sentences.Our cross-lingual approach is closely related toother NER projection approaches (Huang et al,83?
?Figure 1: Examples of cross-lingual projection fornumeric entities.2003; Moore, 2003); however, we have focusedon a limited class of entities which may explainwhy the simple approach works reasonably well.Our projection approach is most closely relatedto that presented in (Yarowsky et al, 2001) and(Pad?o and Lapata, 2009).
In each of these, la-bels over sequences of words are projected acrossalignments directly from one language to theother.
While we follow a similar approach, ourgoal is not necessarily to get the exact projection,but to get a projection which allows us to learncontextual cues for the classes we are labeling.Additionally, we focus on the case where we aregenerating the translated data rather that identify-ing existing parallel data.
Similar to (Yarowskyand Ngai, 2001), we filter out poor alignments (de-tails are described in Section 3.2).3 Methodology3.1 Training class taggers for languagemodelingWe use a statistical sequence tagger to identify andreplace class instances in raw text with their la-bel.
For example, the tokens 10 thousand dollarsin the raw training text may be replaced with aplaceholder class symbol.
The decision is context-dependent: the tagger is able to resolve ambi-guities among possible labels, or even leave thetext unchanged.
Next, this modified text is usedto train a standard n-gram language model.
Fi-0 1<time> 2sixseveneightnine 3tentwentythirtyforty 5</time> </time>Figure 2: This FST is a small excerpt of the fullgrammar for TIME.
Arc weights are not shown.nally, all placeholders become non-terminals inthe language model and are expanded either stat-ically or dynamically with stochastic finite-stateclass grammars (see Figure 2 for an example).Decorator tokens inside the grammars are used tomark class instances in the word lattice so thatthey can be converted (after recognition) to the de-sired written forms using deterministic spoken-to-written text-normalization rules.3.2 Cross-lingual Projection TechniquesThe starting point for cross-lingual projection is totrain a statistical sentence tagger of high quality ina high-resource language, i.e., a language whereboth a lot of training data and human annotatorsare readily available.
We use English in our exper-iments.To obtain annotated sentences in a low-resourcelanguage, we translate unlabeled sentences intothe high-resource language.
We use an in-housephrase-based statistical machine translation sys-tem (Koehn et al, 2003) which is trained with par-allel texts extracted from web pages; described indetail in Section 4.1 of (Nakagawa, 2015).
Thetranslation system we use provides token-by-tokenalignments as part of the output.
This is achievedby keeping alignments along with phrase-pairsduring the phrase extraction stage of training thealignment system.The high quality sentence tagger is applied tothe translated sentences.
Then, using the align-ments between the translated sentences, we mapclass tags back to the low-resource language.
SeeFigure 1 for examples of actual mappings pro-duced by this procedure.With this approach, we can produce arbitrar-ily large in-domain annotated training sets forthe low-resource language.
These annotated sen-tences are then used to train a class tagger forthe low-resource language.
The main question iswhether the resulting class tagger is of sufficientquality for our down-stream objective.84For the goal of training a class-based languagemodel in a low-resource language, one may con-sider a different approach than the one just de-scribed: instead of training a tagger in the low-resource language, each sentence in the languagemodel training data could be translated to the high-resource language, tagged using the statistical tag-ger, and projected back to the low-resource lan-guage.
The primary reason for not pursuing thisapproach is the size of the language model train-ing data (tens of billions of sentences).
Translat-ing a corpus this large is prohibitive.
As the high-resource language tagger is trained on approxi-mately 150K tokens, we believe that we have cov-ered a large number of the predictive cues for theset of classes.Alignment detailsWhen projecting the class labels back from atranslated sentence to the original sentence, vari-ous subtle issues arise.
We describe these and oursolutions for each in this section.To tag a token in the low-resource language, wesee which tokens in the high-resource languageare aligned to it in the translation, and look attheir class tags.
If all of these tokens have thesame class tag, we assign the same tag to the low-resource language token.
Otherwise, we use thefollowing rules:?
If some tokens have no class tag but othershave some class tag, we still assign the classtag to the original token.?
If multiple tokens with different class tagsmap to the original token, we consider thetagging ambiguous.
In such a case, we sim-ply skip the sentence and do not use it fortraining the low-resource tagger.
We can af-ford to do so because there is no shortage ofunlabeled training sentences.In a number of cases, we ignore sentence pairswhich may have contained alignments allowing usto project labels, but also contained noise (e.g.,spurious many-to-one alignments).
We rejectedpoor alignments 2%, 31% and 14% of the time forIndonesian, Russian and Italian respectively.
Dateand time expressions were often affected by thesenoisy alignments.4 Empirical evaluation4.1 DataWe trained an English conditional random field(CRF) (Lafferty et al, 2001) tagger to be used inall experiments in order to provide labels for thesentences produced by translation.
To train thistagger we obtained a data set of 24,503 manuallylabeled sentences (150K tokens) sampled from acorpus of British English language model trainingmaterial.
Each token is labeled with one of 17 pos-sible tags.
About 95% of the tokens are labeledwith a ?none?
tag, meaning that the token is not inany of the pre-determined non-lexical classes.Separately, we obtained similar training setsto create Italian, Indonesian and Russian taggers.The models trained from these labeled data setswere used only to create baseline systems for com-parison with the cross-lingual systems.To provide input into our cross-lingual projec-tion procedure, we also sampled datasets of unla-beled sentences of varying sizes for each evalua-tion language, using the same sampling procedureas used for the human-labeled sets.Note that these tagger training sets have incon-sistent sizes across languages (see Table 2) due tothe nature of the sampling procedure: Each train-ing source is searched for sentences matching anextensive list of patterns of numeric entities.
Sen-tences from each training source are collected upto a source-specific maximum number (which maynot always be reached).
We also apply a flatteningstep to increase diversity of the sample.4.2 CRF modelOur CRF tagger model was trained online usinga variant of the MIRA algorithm (Crammer andSinger, 2003).
Our feature set includes isolatedfeatures (for word identity wi, word type di, andword cluster ci) as well as features for neighboringwords wi?2, wi?1, wi+1, wi+2, wi+3, neighbor-ing clusters ci?2, ci?1, ci+1, ci+2, ci+3, pair fea-tures (wi, di?1), (wi, di+1), (di, di?1), (di, di+1),and domain-specific features (indicators for tokenswithin a given numeric range, or tokens that end ina certain number of zero digits).
We also includeclass bias features, which capture the class priordistribution found in the training set.4.3 MetricsWe use two manually transcribed test sets to eval-uate the performance of our approach in the con-85Test Set Utts Words % Numeric wordsNUM ID 9,744 60,781 19%NUM RU 10,988 59,933 22%NUM IT 8,685 48,195 18%VS ID 9,841 36,276 2%VS RU 12,467 49,403 3%VS IT 12,625 47,867 2%Table 1: NUM refers to the NUMERIC entities testset and VS refers to the VOICE-SEARCH test set.text of numeric transcription.
The first test setVOICE-SEARCH (approximately 48K words forItalian and Russian, and approximately 36K wordsfor Indonesian) is a sample from general voice-search traffic, and tracks any regressions that ap-pear as a result of biasing too heavily toward theselected classes.
The other test set NUMERIC (ap-proximately 48K words for Italian, and approxi-mately 60K for Russian and Indonesian) containsutterances we expect to benefit from class-basedmodeling of numeric entities.
See Table 1 for de-tails on these test sets.We report word-error-rate (WER) on each testset for each model evaluated, including two base-line systems (one built without classes at all andanother that has classes identified by a taggertrained on human-labeled data).
We also reporta labeled-bracket F1 score to show the perfor-mance of the tagger independent of the speech-recognition task.
For each language, the test setused for labeled-bracket F1 is a human-labeledcorpus of approximately 2K sentences that wereheld out from the human-labeled corpora for thebaseline systems.4.4 ResultsThe results in Table 2 show that all class-basedsystems outperform the baseline in WER on theNUMERIC test set, while performance on theVOICE-SEARCH test set was mostly flat.
The flatperformance on VOICE-SEARCH is expected: asseen in Table 1 this test set has a very low propor-tion of words that are numeric in form.
We pro-vide results on this test set in order to confirm thatour approach does not harm general voice-searchqueries.
As for performance on the NUMERICtest set, larger cross-lingual data sets led to betterperformance for Russian and Italian, but causeda slight regression for Indonesian.
The trans-lation system we use for these experiments hasbeen optimized for a general-purpose web searchNUM VSModel F1 WER WERID Baseline (no classes) - 20.0 10.1ID Cross-lingual 15K 0.64 19.3 10.1ID Cross-lingual 37K 0.65 19.4 10.1ID Cross-lingual 77K 0.64 19.5 10.1ID Human-labeled 0.83 19.1 10.1RU Baseline (no classes) - 28.7 17.1RU Cross-lingual 16K 0.37 26.4 17.0RU Cross-lingual 98K 0.39 26.2 17.1RU Human-labeled 0.87 25.3 16.8IT Baseline (no classes) - 23.0 14.8IT Cross-lingual 18K 0.55 19.7 14.8IT Cross-lingual 104K 0.57 19.6 14.8IT Human-labeled 0.88 19.0 14.8Table 2: NUM refers to the NUMERIC entities testset and VS refers to the VOICE-SEARCH test set.All NUM WER results are statistically significant(p < 0.1%) using a paired random permutationsignificance test.translation task rather than for an academic task.When evaluated on a test set matched to the trans-lation task, performance for Russian-to-Englishwas considerably worse than for Indonesian-to-English or Italian-to-English.For Indonesian (ID), the human-labeled sys-tem achieved a 4.5% relative WER reduction onNUMERIC, while the best cross-lingual systemachieved a 3.5% relative reduction.For Russian (RU), the human-labeled systemimproved more, achieving an 11.8% relative re-duction on NUMERIC, while the best cross-lingualsystem achieved an 8.7% relative reduction.Finally, for Italian (IT), the human-labeled sys-tem gave an impressive 17.4% relative reductionon NUMERIC, while the best cross-lingual systemachieved a 14.8% relative reduction on the sametest set.Across the three languages, the cross-lingualsystems achieved relative error reductions on theNUMERIC test set that were between 70% and85% of the reduction achieved when using onlyhuman-labeled data for training the class tagger.4.5 Error AnalysisWe noticed that the Russian cross-lingual-derivedtraining set was of lower quality than those ofthe other languages, as seen in the labeled-bracketF1 metric in Table 2.
Looking more closely, we86noticed that the per-class F1 scores tended to belower for labels used for dates and times.
This ob-servation also concides with the observation thatthe alignment procedure frequently ran into am-biguity issues when aligning month, day and yeartokens between Russian and English, thus signifi-cantly reducing the coverage of these labels in theinduced cross-lingual training set.5 ConclusionWe presented a cross-lingual projection techniquefor training class-based language models.
We ex-tend a previously successful sequence-modeling-based class labeling approach for identifyingcontextually-dependent class assignments by pro-jecting labels from a high-resource language to alow-resources language.
This allows us to buildclass-based language models in low-resource lan-guages with no annotated data.
Our empirical re-sults show that we are able to achieve between70% and 85% of the error reduction that we wouldhave obtained had we used human-labeled data.While cross-lingual projection for sequence-labeling techniques are well known in the com-munity, our approach exploits the fact that we aregenerating training data from the projection ratherthan using the projected result directly.
Further-more, noise in the class-labeling system does notcripple the language model as it learns a distribu-tion over labels (including no label).In future work, we will experiment withalternative projection approaches including pro-jecting the training data and translating from thehigh-resource language to the low-resource lan-guage.
We also plan to experiment with differentprojection approaches to address the ambiguityissues we observed when aligning time and dateexpressions.6 AcknowledgmentsWe would like to thank the anonymous reviewersfor their detailed reviews and suggestions.ReferencesPeter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18:467?479.David Burkett, Slav Petrov, John Blitzer, and DanKlein.
2010.
Learning better monolingual modelswith unannotated bilingual text.
In Proceedings ofthe Fourteenth Conference on Computational Natu-ral Language Learning, CoNLL ?10, pages 46?54.Association for Computational Linguistics.Koby Crammer and Yoram Singer.
2003.
Ultraconser-vative online algorithms for multiclass problems.
J.Mach.
Learn.
Res., 3:951?991, March.Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-basedprojections.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies - Volume1, HLT ?11, pages 600?609.
Association for Com-putational Linguistics.Fei Huang, Stephan Vogel, and Alex Waibel.
2003.Automatic extraction of named entity translingualequivalence based on multi-feature cost minimiza-tion.
In Proceedings of the ACL 2003 Workshopon Multilingual and Mixed-language Named EntityRecognition - Volume 15, MultiNER ?03, pages 9?16.
Association for Computational Linguistics.Reinhard Knesser and Hermann Ney.
1993.
Im-proved clustering techniques for class-based statisti-cal language modelling.
In Proc.
Eurospeech.
ISCA- International Speech Communication Association,September.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology, vol-ume 1 of NAACL ?03, pages 48?54.
Association forComputational Linguistics.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth Inter-national Conference on Machine Learning, ICML?01, pages 282?289, San Francisco, CA, USA.
Mor-gan Kaufmann Publishers Inc.Michael Levit, Sarangarajan Parthasarathy, ShuangyuChang, Andreas Stolcke, and Benoit Dumoulin.2014.
Word-phrase-entity language models: Gettingmore mileage out of n-grams.
In Proc.
Interspeech.ISCA - International Speech Communication Asso-ciation, September.Robert C. Moore.
2003.
Learning translations ofnamed-entity phrases from parallel corpora.
InProceedings of the Tenth Conference on European87Chapter of the Association for Computational Lin-guistics - Volume 1, EACL ?03, pages 259?266.
As-sociation for Computational Linguistics.Tetsuji Nakagawa.
2015.
Efficient top-down BTGparsing for machine translation preordering.
In Pro-ceedings of the 53rd Annual Meeting of the Associa-tion for Computational Linguistics, ACL 2015, Vol-ume 1: Long Papers, pages 208?218.Sebastian Pad?o and Mirella Lapata.
2009.
Cross-lingual annotation projection of semantic roles.Journal of Artificial Intelligence Research,36(1):307?340, September.Lucy Vasserman, Vlad Schogol, and Keith Hall.
2015.Sequence-based class tagging for robust transcrip-tion in asr.
In Proc.
Interspeech.
ISCA - Interna-tional Speech Communication Association, Septem-ber.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual pos taggers and np bracketers via robustprojection across aligned corpora.
In Proceedingsof the Second Meeting of the North American Chap-ter of the Association for Computational Linguisticson Language Technologies, NAACL ?01, pages 1?8.Association for Computational Linguistics.David Yarowsky, Grace Ngai, and Richard Wicen-towski.
2001.
Inducing multilingual text analy-sis tools via robust projection across aligned cor-pora.
In Proceedings of the First International Con-ference on Human Language Technology Research,HLT ?01, pages 1?8.
Association for ComputationalLinguistics.88
