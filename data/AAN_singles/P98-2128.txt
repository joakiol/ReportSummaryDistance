Learning Constraint Grammar-style disambiguation rules usingInductive Logic ProgrammingNikolaj  L indbergCentre for Speech TechnologyRoyal Institute of TechnologySE-100 44 Stockholm, Swedennikolaj ~speech.
kth.
seMart in  E ineborgTelia Research ABSpoken Language ProcessingSE-136 80 Haninge, SwedenMart in.
E. Eineborg?telia.
seAbstractThis paper reports a pilot study, in whichConstraint Grammar inspired rules were learntusing the Progol machine-learning system.Rules discarding faulty readings of ambiguouslytagged words were learnt for the part of speechtags of the Stockholm-Ume?
Corpus.
Severalthousand isambiguation rules were induced.When tested on unseen data, 98% of the wordsretained the correct reading after tagging.
How-ever, there were ambiguities pending after tag-ging, on an average 1.13 tags per word.
Theresults suggest hat the Progol system can beuseful for learning tagging rules of good qual-ity.1 In t roduct ionThe success of the Constraint Grammar (CG)(Karlsson et al, 1995) approach to part ofspeech tagging and surface syntactic depen-dency parsing is due to the minutely hand-crafted grammar and two-level morphology lex-icon, developed over several years.In the study reported here, the Progolmachine-learning system was used to induceCG-style tag eliminating rules from a one mil-lion word part of speech tagged corpus ofSwedish.
Some 7 000 rules were induced.
Whentested on unseen data, 98% of the words re-tained the correct ag.
There were still ambi-guities left in the output, on an average 1.13readings per word.In the following sections, the CG frameworkand the Progol machine learning system will bepresented very briefly.1.1 Constraint Grammar POS taggingConstraint Grammar is a system for part ofspeech tagging and (shallow) syntactic depen-dency analysis of unrestricted text.
In the fol-lowing, only the part of speech tagging step willbe discussed.The following as a typical 'reductionistic' ex-ample of a CG rule which discards averbal read-ing of a word following a word unambiguouslytagged as determiner (Tapanainen, 1996, page12):REMOVE (V) IF (-iC DET) ;where V is the target tag to be discarded and -ICDET denotes the word immediately to the left(-I), unambiguously (C) tagged as determiner(DET).
There are several types of rules, not only'reductionistic' ones, making the CG formalismquite powerful.
A full-scale CG has hundreds ofrules.
The developers ofEnglish CG report hat99.7% of the words retain their correct reading,and that 93-97% of the words are unambiguousafter tagging (Karlsson et al, 1995, page 186).A parser applying the constraints i describedin Tapanainen (1996).1.2 Inductive Logic ProgrammingInductive Logic Programming (ILP) is a combi-nation of machine learning and logic program-ming, where the goal is to find a hypothesis,H, given examples, E, and background knowl-edge, B, such that the hypothesis along withthe background knowledge logically implies theexamples (Muggleton, 1995, page 2):BAH~EThe examples are usually split into a positive,E +, and a negative, E- ,  subset.The ILP system used in this paper, CPro-gol Version 4.2, uses Horn clauses as the repre-sentational language.
Progol creates, for eachE +, a most specific clause -l-i and then searchesthrough the lattice of hypotheses, from specific775to more general, bounded by\[\] -< H -<-l-ito find the clause that maximally compressesthe data where "< (0-subsumption) is defined asCl .-<C2 -' ~ ~O:c IOCC 2and 12 is the empty clause.
As an example, con-sider the two clauses:ci : p(X,Y) :- q(X,Y).c2: p(a,b) :- q(a,b), r(Z).where Cl -< c2 under the substitution 0 ={Xla, YIb}.When Progol has found the clause that com-presses the data the most, it is added to thebackground knowledge and all examples thataxe redundant with respect to this new back-ground knowledge are removed.More informally, Progol builds the most spe-cific clause for each positive example.
It thentries to find a more general version of the clause(with respect o the background knowledge andmode declarations, ee below) that explains asmany positive and as few negative xamples aspossible.Mode declarations pecifying the propertiesof the rules have to be given by the user.
Amodeh declaration specifies the head of the rules,while modeb declarations specify what the bod-ies of the rules to induce might contain.
Theuser also declares the types of arguments, andwhether they are input or output arguments, orif an argument should be instantiated by Pro-gol.
Progol is freely available and documentedin Muggleton (1995) and Roberts (1997).1.3 The  Stockholm-Umefi  CorpusThe training material in the experiments re-ported here is sampled from a pre-release ofthe Stockholm-Ume?
Corpus (SUC).
SUC cov-ers just over one million words of part of speechtagged Swedish text, sampled from differenttext genres (largely following the Brown corpustext categories).
The first official release is nowavailable on CD-ROM.The SUC tagset has 146 different tags, andthe tags consist of a part of speech tag, e.g.
VB(the verb) followed by a (possibly empty) set ofmorphological features, such as PRS (the presenttense) and AKT (the active voice), etc.
There are25 different part of speech tags.
Thus, many ofthe 146 tags represent different inflected forms.Examples of the tags are found in Table 1.
TheSUC tagging scheme is presented in Ejerhed etal.
(1992).2 P rev ious  workTwo previous tudies on the induction of rulesfor part of speech tagging are presented in thissection.Samuelsson et al (1996) describe experi-ments of inducing English CG rules, intendedmore as a help for the grammarian, rather thanas an attempt o induce a full-scale CG.
Thetraining corpus consisted of some 55 000 wordsof English text, morphologically and syntacti-cally tagged according to the EngCG tagset.Constraints of the form presented in Sec-tion 1.1 were induced based on bigram statistics.Also lexical rules, discarding unlikely readingsfor certain word forms, were induced.
In addi-tion to these, 'barrier' rules were learnt.
Whilethe induced 'remove' rules were based on bi-grams, the barrier rules utilized longer contexts.When tested on a 10 000 word test corpus, therecall of the induced grammar was 98.2% witha precision of 87.3%, which means that some ofthe ambiguities were left pending after tagging(1.12 readings per word).Cussens (1997) describes a project in whichCG inspired rules for tagging English text wereinduced using the Progol machine-learning sys-tem.
To its help the Progol system had a smallhand-crafted syntactic grammar.
The grammarwas used as background knowledge to the Pro-gol system only, and was not used for producingany syntactic structure in the final output.
Theexamples consisted of the tags of all of the wordson each side of the word to be disambiguated(the target word).
Given no unknown wordsand a tag set of 43 different tags, the systemtagged 96.4% of the words correctly.3 P resent  workThe current work was inspired by Cussens(1997) as well as Samuelsson et al (1996), butdeparts from both in several respects.
It alsofollows up an initial experiment conducted bythe current authors (Eineborg and Lindberg,7761998).Following Samuelsson et al (1996) local-context and lexical rules were induced.
In thepresent work, no barrier rules were induced.
Incontrast o their study, a TWOL lexicon and anannotated training text using the same tagsetwere not available.
Instead, a lexicon was cre-ated from the training corpus.Just as in Cussens work, Progol was usedto induce tag elimination rules from an anno-tated corpus.
In contrast o his study, no gram-matical background knowledge is given to thelearner and also word tokens, and not only partof speech tags, are in the training data.In order to induce the new rules, the contexthas been limited to a window of maximally fivewords, with the target word to disambiguate inthe middle.
A motivation for using a rathersmall window size can be found in Karlsson etal.
(1995, page 59) where it is pointed out thatsensible constraints referring to a position rel-ative to the target word utilize close context,typically 1-3 words.Some further restrictions on how the learn-ing system may use the information in the win-dow have been applied in order to reduce thecomplexity of the problem.
This is described inSection 3.2.A pre-release of the Stockholm-Ume?
Corpuswas used.
Some 10% of the corpus was put asideto be used as test data, and the rest of the cor-pus made up the training data.
The test datafiles were evenly distributed over the differenttext genres.3.1 P reprocess ingBefore starting the learning of constraints, thetraining data was preprocessed in differentways.
Following Cusseus (1997), a lexicon wasproduced from the training corpus.
All differentword forms in the corpus were represented in thelexicon by one look-up word and an ambiguityclass, the set of different tags which occurredin the corpus for the word form.
The lexiconended up just over 86 000 entries big.Similar to Karlsson et al (1995), the firststep of the tagging process was to identify 'id-ioms', although the term is used somewhat dif-ferently in this study; bi- and trigrams whichwere always tagged with one specific tag se-quence (unambiguously tagged, i.e.)
were ex-tracted from the training text.
Example 'id-ioms' are given in Table 1.
1 530 such bi- andtrigrams were used.Following Samuelsson et al (1996), a list ofvery unlikely readings for certain words was pro-duced ('lexicai rules').
For a word form plus tagto qualify as a lexical rule, the word form shouldhave a frequency of at least 100 occurrences inthe training data, and the word should occurwith the tag to discard in no more than 1% ofthe cases.
355 lexical rules were produced thisway.
The role of lexical rules and 'idioms' is toremove the simple cases of ambiguities, makingit possible for the induced rules to fire, sincethese rules are all 'careful', meaning that theycan refer to unambiguous contexts only (if theyrefer to tag features, and not word forms only,i.e.
).3.2 Ru le  induct ionRules were induced for all part of speech cat-egories.
Allowing the rules to refer to spe-cific morphological features (and not necessar-ily a complete specification) has increased theexpressive power of the rules, compared tothe initial experiments (Eineborg and Lindberg,1998).
The rules can look at word form, part ofspeech, morphological features, and whether aword has an upper or lower case initial charac-ter.
Although we used a window of size 5, therules can look at maximally four positions atthe same time within the window.
Another re-striction has been put on which combination offeatures the system may select from a contextword.
The closer a context word is to the targetthe more features it may use.
This is done inorder to reduce the search space.
Each contextword is represented as a prolog term with argu-ments for word form, upper/lower case charac-ter and part of speech tag along with a set ofmorphological features (if any).A different set of training data was producedfor each of the 24 part speech categories.
Thetraining data was pre-processed by applying thebi- and trigrams and the lexical rules, describedabove (Section 3.1).
This step was taken in or-der to reduce the amount of training data - -rules should not be learnt for ambiguities whichwould be taken care of anyway.Progol is able to induce a hypothesis usingonly positive examples, or using both positiveand negative xamples.
Since we are inducingtag eliminating rules, an example is considered777BI- AND TRIGRAMSeft pardet ~ri saraband reedp& Erund avPOS READINGS (UNAMBIGUOUS TAG SEQUENCE)ett/DT NEU SIN IND par/NN NEU SIN IND NOMdet/PN NEU SIN DEF SUB/0BJ ~r/VB PRS AKT?/PP samband/NN NEU SIN IND NOM med/PPp&/PP Erund/NNUTR SIN IND N0M av/PPTable 1: 'Idioms'.
Unambiguous word sequences found in the training data.positive when a word is incorrectly tagged andthe reading should be discarded.
A negativeexample is a correctly tagged word where thereading should be retained.
The training datafor each part of speech tag consisted of between4000 and 6000 positive examples with an equiv-alent number of negative xamples.
The exam-ples for each part of speech category were ran-domly drawn from all examples available in thetraining data.A noise level of 1% was tolerated to make surethat Progol could find important rules despitethe fact that some examples could be incorrect.3.3 Ru le  fo rmatThe induced rules code two types of informa-tion: Firstly, the rules state the number andpositions of the context words relative to thetarget word (the word to disambiguate).
Sec-ondly, for each context word referred to by arule, and possibly also for the target word, therule states under what conditions the rule isapplicable.
These conditions can be the wordform, morphological features or whether a wordis spellt with an initial capital etter or not, andcombinations of these things.
Examples of in-duced rules areremove (vb,A) : -const r  (A, le f t  ( feats  ( \[dr\] ) ) ) .remove ( ie ,A)  : -constr (A, right_right (feats ( \[def\] ),feats ( \[vb\] ) )).remove(vb, A) :-context (A, left_target (word (art),feat list ( \[imp, akt\] ) ) ).where the first rule eliminates all verbal (vb)readings of a word immediately preceded by aword tagged as determiner (dr).
The secondrule deletes the infinitive marker (ie) readingof a word followed by any word which has thefeature 'definite' (clef), followed by a verb (vb).The third rule deletes verb tags which have thefeatures 'imperative' (imp) and 'active voice'(aRt) if the preceding word is att (word(atl;)).As alredy been mentioned, the scope of therules has been limited to a window of five words,the target word included.
In an earlier attempt,the window was seven words, but these ruleswere less expressive in other respects (Eineborgand Lindberg, 1998).4 Resu l tsJust under 7 000 rules were induced.
The taggerwas tested on a subset of the unseen data.
Onlysentences in which all words were in the lexiconwere allowed.
Sentences including words taggedas U0 were discarded.
The U0 tag is a peculiarityof the SUC tagset, and conveys no grammaticalinformation; it stands for 'foreign word' and isused e.g.
for the words in passages quoting textwhich is not in Swedish.The test data consisted of 42 925 words, in-cluding punctuation marks.
After lexicon look-up the words were assigned 93 810 readings,i.e., on average 2.19 readings per word.
41 926words retained the correct reading after disam-biguation, which means that the correct ag sur-vived for 97.7% of the words.
After tagging,there were 48 691 readings left, 1.13 readingsper word.As a comparison to these results, a prelim-inary test of the Brill tagger also trained onthe Stockholm-Ume?
Corpus, tagged 96.9% ofthe words correctly, and Oliver Mason's QTaggot 96.3% on the same data (Ridings, 1998).Neither of these two taggers leave ambigui-ties pending and both handles unknown words,which makes a direct comparison of the fguresgiven above hard.The processing times were quite long for mostof the rule sets - -  few of them were actuallyallowed to continue until all examples were ex-hausted.5 D iscuss ion  and  fu ture  workThe figures of the experimental tagger are notoptimal, but promising, considering that the778rules induced is a limited subset of possible ruletypes.Part of the explanation for the figure of am-biguities pending after tagging is that there aresome ambiguity classes which are very hard todeal with.
For example, there is a tag for the ad-verb, hB, and one tag for the verbal particle, PL.In the lexicon built from the corpus, there are 83word forms which can have at least both thesereadings.
Thus, turning a corpus into a lexiconmight lead to the introduction of ambiguitieshard to solve.
A lexicon better tailored to thetask would be of much use.
Another importantissue is that of handling unknown words.To reduce the error rate, the bad rules shouldbe identified by testing all rules against thetraining data.
To tackle the residual ambigu-ities, the next step will be to learn also differentkinds of rules, for example 'select' rules whichretain a given reading, but discard all others.Also rules scoping longer contexts than a win-dow of 5-7 words must be considered.6 Conc lus ionsUsing the Progol ILP system, some 7 000tag eliminating rules were induced from theStockholm-Ume?
Corpus.
A lexicon was builtfrom the corpus, and after lexicon look-up, testdata (including only known words) was disam-biguated with the help of the induced rules.
Of42 925 known words, 41 926 (98%) retained thecorrect reading after disambiguation.
Some am-biguities remained inoutput: on an average 1.13readings per word.
Considering the experimen-tal status of the tagger, we find the results en-couraging.AcknowledgmentsBritt Hartmann (Stockholm University) an-swered many corpus related questions.
HenrikBostr6m (Stockholm University/Royal Instituteof Technology) helped us untangle a few ILPmysteries.Re ferencesEric BriU.
1994.
Some advances intransformation-based part of speech tagging.In Proceedings of the Twelfth National Con-ference on Artificial Intelligence (AAAI-94).James Cussens.
1997.
Part of speech taggingusing Progol.
In Proceedings of the 7th Inter-national Workshop on Inductive Logic Pro-gramming (ILP-97), pages 93-108.Martin Eineborg and Nikolaj Lindberg.
1998.Induction of Constraint Grammar-rules u ingProgol.
In Proceedings of The Eighth Inter-national Conference on Inductive Logic Pro-gramming (ILP'98), Madison, Wisconsin.Eva Ejerhed, Gunnel Kiillgren, Wennstedt Ola,and Magnus ~,strSm.
1992.
The LinguisticAnnotation System of the Stockholm-Ume~Project.
Department of General Linguistics,University of Ume?.Fred Karlsson, Atro Voutilainen, Juha Heikkil?,and Arto Anttila, editors.
1995.
ConstraintGrammar: A language-independent systemfor parsing unrestricted text.
Mouton deGruyter, Berlin and New York.Oliver Manson, 1997.
QTAG--A portable prob-abilistic tagger.
Corpus Research, The Uni-versity of Birmingham, U.K.Stephen Muggleton.
1995.
Inverse entailmentand Progol.
New Generation ComputingJournal, 13:245-286.Daniel Ridings.
1998.
SUC and the Brill tagger.GU-ISS-98-1 (Research Reports from the De-partment of Swedish, GSteborg University).Sam Roberts, 1997.
An introduction to Progol.Christer Samuelsson, Pasi Tapanainen, andAtro Voutilainen.
1996.
Inducing Con-straint Grammars.
In Miclet Laurent andde la Higuera Colin, editors, GrammaticalInference: Learning Syntax from Sentences,pages 146-155.
Springer Verlag.Pasi Tapanainen.
1996.
The Constraint Gram-mar Parser CG-2.
Department of GeneralLinguistics, University of Helsinki.779
