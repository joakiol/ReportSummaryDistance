Proceedings of the 43rd Annual Meeting of the ACL, pages 149?156,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsModelling the substitutability of discourse connectivesBen HutchinsonSchool of InformaticsUniversity of EdinburghB.Hutchinson@sms.ed.ac.ukAbstractProcessing discourse connectives is im-portant for tasks such as discourse parsingand generation.
For these tasks, it is use-ful to know which connectives can signalthe same coherence relations.
This paperpresents experiments into modelling thesubstitutability of discourse connectives.It shows that substitutability effects dis-tributional similarity.
A novel variance-based function for comparing probabilitydistributions is found to assist in predict-ing substitutability.1 IntroductionDiscourse coherence relations contribute to themeaning of texts, by specifying the relationships be-tween semantic objects such as events and propo-sitions.
They also assist in the interpretation ofanaphora, verb phrase ellipsis and lexical ambigu-ities (Hobbs, 1985; Kehler, 2002; Asher and Las-carides, 2003).
Coherence relations can be implicit,or they can be signalled explicitly through the use ofdiscourse connectives, e.g.
because, even though.For a machine to interpret a text, it is impor-tant that it recognises coherence relations, and so asexplicit markers discourse connectives are of greatassistance (Marcu, 2000).
When discourse con-nectives are not present, the task is more difficult.For such cases, unsupervised approaches have beendeveloped for predicting relations, by using sen-tences containing discourse connectives as trainingdata (Marcu and Echihabi, 2002; Lapata and Las-carides, 2004).
However the nature of the relation-ship between the coherence relations signalled bydiscourse connectives and their empirical distribu-tions has to date been poorly understood.
In par-ticular, one might wonder whether connectives withsimilar meanings also have similar distributions.Concerning natural language generation, texts areeasier for humans to understand if they are coher-ently structured.
Addressing this, a body of researchhas considered the problems of generating appropri-ate discourse connectives (for example (Moser andMoore, 1995; Grote and Stede, 1998)).
One suchproblem involves choosing which connective to gen-erate, as the mapping between connectives and re-lations is not one-to-one, but rather many-to-many.Siddharthan (2003) considers the task of paraphras-ing a text while preserving its rhetorical relations.Clauses conjoined by but, or and when are sepa-rated to form distinct orthographic sentences, andthese conjunctions are replaced by the discourse ad-verbials however, otherwise and then, respectively.The idea underlying Siddharthan?s work is thatone connective can be substituted for another whilepreserving the meaning of a text.
Knott (1996)studies the substitutability of discourse connectives,and proposes that substitutability can motivate the-ories of discourse coherence.
Knott uses an empiri-cal methodology to determine the substitutability ofpairs of connectives.
However this methodology ismanually intensive, and Knott derives relationshipsfor only about 18% of pairs of connectives.
It wouldthus be useful if substitutability could be predictedautomatically.149This paper proposes that substitutability can bepredicted through statistical analysis of the contextsin which connectives appear.
Similar methods havebeen developed for predicting the similarity of nounsand verbs on the basis of their distributional similar-ity, and many distributional similarity functions havebeen proposed for these tasks (Lee, 1999).
Howeversubstitutability is a more complex notion than simi-larity, and we propose a novel variance-based func-tion for assisting in this task.This paper constitutes a first step towards predict-ing substitutability of cnonectives automatically.
Wedemonstrate that the substitutability of connectiveshas significant effects on both distributional similar-ity and the new variance-based function.
We then at-tempt to predict substitutability of connectives usinga simplified task that factors out the prior likelihoodof being substitutable.2 Relationships between connectivesTwo types of relationships between connectives areof interest: similarity and substitutability.2.1 SimilarityThe concept of lexical similarity occupies an impor-tant role in psychology, artificial intelligence, andcomputational linguistics.
For example, in psychol-ogy, Miller and Charles (1991) report that psycholo-gists ?have largely abandoned ?synonymy?
in favourof ?semantic similarity?.?
In addition, work in au-tomatic lexical acquisition is based on the proposi-tion that distributional similarity correlates with se-mantic similarity (Grefenstette, 1994; Curran andMoens, 2002; Weeds and Weir, 2003).Several studies have found subjects?
judge-ments of semantic similarity to be robust.
Forexample, Miller and Charles (1991) elicit similar-ity judgements for 30 pairs of nouns such ascord?smile, and found a high correlation withjudgements of the same data obtained over 25years previously (Rubenstein and Goodenough,1965).
Resnik (1999) repeated the experiment,and calculated an inter-rater agreement of 0.90.Resnik and Diab (2000) also performed a similarexperiment with pairs of verbs (e.g.
bathe?kneel).The level of inter-rater agreement was again signifi-cant (r = 0.76).1.
Take an instance of a discourse connectivein a corpus.
Imagine you are the writerthat produced this text, but that you need tochoose an alternative connective.2.
Remove the connective from the text, andinsert another connective in its place.3.
If the new connective achieves the same dis-course goals as the original one, it is consid-ered substitutable in this context.Figure 1: Knott?s Test for SubstitutabilityGiven two words, it has been suggested that ifwords have the similar meanings, then they can beexpected to have similar contextual distributions.The studies listed above have also found evidencethat similarity ratings correlate positively with thedistributional similarity of the lexical items.2.2 SubstitutabilityThe notion of substitutability has played an impor-tant role in theories of lexical relations.
A defini-tion of synonymy attributed to Leibniz states thattwo words are synonyms if one word can be used inplace of the other without affecting truth conditions.Unlike similarity, the substitutability of dis-course connectives has been previously studied.Halliday and Hasan (1976) note that in certain con-texts otherwise can be paraphrased by if not, as in(1) It?s the way I like to go to work.One person and one line of enquiry at a time.Otherwise/if not, there?s a muddle.They also suggest some other extended paraphrasesof otherwise, such as under other circumstances.Knott (1996) systematises the study of the substi-tutability of discourse connectives.
His first step isto propose a Test for Substitutability for connectives,which is summarised in Figure 1.
An application ofthe Test is illustrated by (2).
Here seeing as wasthe connective originally used by the writer, how-ever because can be used instead.150w1w2(a) w1 and w2 areSYNONYMSw1 w2(b) w1 is aHYPONYM of w2w1w2(c) w1 and w2 areCONTINGENTLYSUBSTITUTABLEw1w2(d) w1 and w2 areEXCLUSIVEFigure 2: Venn diagrams representing relationships between distributions(2) Seeing as/because we?ve got nothing butcircumstantial evidence, it?s going to bedifficult to get a conviction.
(Knott, p. 177)However the ability to substitute is sensitive to thecontext.
In other contexts, for example (3), the sub-stitution of because for seeing as is not valid.
(3) It?s a fairly good piece of work, seeingas/#because you have been under a lot ofpressure recently.
(Knott, p. 177)Similarly, there are contexts in which because canbe used, but seeing as cannot be substituted for it:(4) That proposal is useful, because/#seeing as itgives us a fallback position if the negotiationscollapse.
(Knott, p. 177)Knott?s next step is to generalise over all contextsa connective appears in, and to define four substi-tutability relationships that can hold between a pairof connectives w1 and w2.
These relationships areillustrated graphically through the use of Venn dia-grams in Figure 2, and defined below.?
w1 is a SYNONYM of w2 if w1 can always besubstituted for w2, and vice versa.?
w1 and w2 are EXCLUSIVE if neither can everbe substituted for the other.?
w1 is a HYPONYM of w2 if w2 can always besubstituted for w1, but not vice versa.?
w1 and w2 are CONTINGENTLY SUBSTI-TUTABLE if each can sometimes, but not al-ways, be substituted for the other.Given examples (2)?
(4) we can conclude that be-cause and seeing as are CONTINGENTLY SUBSTI-TUTABLE (henceforth ?CONT.
SUBS.?).
Howeverthis is the only relationship that can be establishedusing a finite number of linguistic examples.
Theother relationships all involve generalisations overall contexts, and so rely to some degree on the judge-ment of the analyst.
Examples of each relationshipgiven by Knott (1996) include: given that and see-ing as are SYNONYMS, on the grounds that is a HY-PONYM of because, and because and now that areEXCLUSIVE.Although substitutability is inherently a morecomplex notion than similarity, distributional simi-larity is expected to be of some use in predicting sub-stitutability relationships.
For example, if two dis-course connectives are SYNONYMS then we wouldexpect them to have similar distributions.
On theother hand, if two connectives are EXCLUSIVE, thenwe would expect them to have dissimilar distribu-tions.
However if the relationship between two con-nectives is HYPONYMY or CONT.
SUBS.
then weexpect to have partial overlap between their distribu-tions (consider Figure 2), and so distributional simi-larity might not distinguish these relationships.The Kullback-Leibler (KL) divergence functionis a distributional similarity function that is of par-ticular relevance here since it can be described in-formally in terms of substitutability.
Given co-occurrence distributions p and q, its mathematicaldefinition can be written as:D(p||q) =?xp(x)(log1q(x)?
log1p(x)) (5)151w1w2(a) w1 and w2are SYNONYMSw2w1(b) w2 is a HY-PONYM of w1w1 w2(c) w1 is a HY-PONYM of w2w1w2(d) w1 and w2 areCONT.
SUBS.w2w1(e) w1 and w2 areEXCLUSIVEFigure 3: Surprise in substituting w2 for w1 (darker shading indicates higher surprise)The value log 1p(x) has an informal interpretation asa measure of how surprised an observer would beto see event x, given prior likelihood expectationsdefined by p. Thus, if p and q are the distributions ofwords w1 and w2 thenD(p||q) = Ep(surprise in seeing w2?
surprise in seeing w1) (6)where Ep is the expectation function over the distri-bution of w1 (i.e.
p).
That is, KL divergence mea-sures how much more surprised we would be, onaverage, to see word w2 rather than w1, where theaveraging is weighted by the distribution of w1.3 A variance-based function fordistributional analysisA distributional similarity function provides onlya one-dimensional comparison of two distributions,namely how similar they are.
However we can ob-tain an additional perspective by using a variance-based function.
We now introduce a new function Vby taking the variance of the surprise in seeing w2,over the contexts in which w1 appears:V (p, q) = V ar(surprise in seeing w2)= Ep((Ep(log1q(x)) ?
log1q(x))2) (7)Note that like KL divergence, V (p, q) is asymmetric.We now consider how the substitutability of con-nectives affects our expectations of the value of V .If two connectives are SYNONYMS then each canalways be used in place of other.
Thus we wouldalways expect a low level of surprise in seeing oneRelationship Functionof w1 to w2 D(p||q) D(q||p) V (p, q) V (q, p)SYNONYM Low Low Low LowHYPONYM Low Medium Low HighCONT.
SUBS.
Medium Medium High HighEXCLUSIVE High High Low LowTable 1: Expectations for distributional functionsconnective in place of the other, and this low level ofsurprise is indicated via light shading in Figure 3a.It follows that the variance in surprise is low.
On theother hand, if two connectives are EXCLUSIVE thenthere would always be a high degree of surprise inseeing one in place of the other.
This is indicatedusing dark shading in Figure 3e.
Only one set isshaded because we need only consider the contextsin which w1 is appropriate.
In this case, the vari-ance in surprise is again low.
The situation is moreinteresting when we consider two connectives thatare CONT.
SUBS..
In this case substitutability (andhence surprise) is dependent on the context.
Thisis illustrated using light and dark shading in Fig-ure 3d.
As a result, the variance in surprise is high.Finally, with HYPONYMY, the variance in surprisedepends on whether the original connective was theHYPONYM or the HYPERNYM.Table 1 summarises our expectations of the val-ues of KL divergence and V , for the various sub-stitutability relationships.
(KL divergence, unlikemost similarity functions, is sensitive to the order ofarguments related by hyponymy (Lee, 1999).)
The152Something happened and something else happened.Something happened or something else happened.?
0 ?
1 ?
2 ?
3 ?
4 ?
5Figure 4: Example experimental itemexperiments described below test these expectationsusing empirical data.4 ExperimentsWe now describe our empirical experiments whichinvestigate the connections between a) subjects?
rat-ings of the similarity of discourse connectives, b)the substitutability of discourse connectives, and c)KL divergence and the new function V applied tothe distributions of connectives.
Our motivation isto explore how distributional properties of wordsmight be used to predict substitutability.
The ex-periments are restricted to connectives which relateclauses within a sentence.
These include coordinat-ing conjunctions (e.g.
but) and a range of subordina-tors including conjunctions (e.g.
because) as well asphrases introducing adverbial clauses (e.g.
now that,given that, for the reason that).
Adverbial discourseconnectives are therefore not considered.4.1 Experiment 1: Subject ratings of similarityThis experiment tests the hypotheses that 1) subjectsagree on the degree of similarity between pairs ofdiscourse connectives, and 2) similarity ratings cor-relate with the degree of substitutability.4.1.1 MethodologyWe randomly selected 48 pairs of discourse con-nectives such that there were 12 pairs standing ineach of the four substitutability relationships.To dothis, we used substitutability judgements made byKnott (1996), supplemented with some judgementsof our own.
Each experimental item consisted ofthe two discourse connectives along with dummyclauses, as illustrated in Figure 4.
The format of theexperimental items was designed to indicate how aphrase could be used as a discourse connective (e.g.it may not be obvious to a subject that the phrasethe moment is a discourse connective), but withoutMean HYP CONT.
SUBS.
EXCLSYNONYM 3.97 * * *HYPONYM 3.43 * *CONT.
SUBS.
1.79 *EXCLUSIVE 1.08Table 2: Similarity by substitutability relationshipproviding complete semantics for the clauses, whichmight bias the subjects?
ratings.
Forty native speak-ers of English participated in the experiment, whichwas conducted remotely via the internet.4.1.2 ResultsLeave-one-out resampling was used to compareeach subject?s ratings are with the means of theirpeers?
(Weiss and Kulikowski, 1991).
The averageinter-subject correlation was 0.75 (Min = 0.49, Max= 0.86, StdDev = 0.09), which is comparable to pre-vious results on verb similarity ratings (Resnik andDiab, 2000).
The effect of substitutability on simi-larity ratings can be seen in Table 2.
Post-hoc Tukeytests revealed all differences between means in Ta-ble 2 to be significant.The results demonstrate that subjects?
ratings ofconnective similarity show significant agreementand are robust enough for effects of substitutabilityto be found.4.2 Experiment 2: Modelling similarityThis experiment compares subjects?
ratings of sim-ilarity with lexical co-occurrence data.
It hypothe-sises that similarity ratings correlate with distribu-tional similarity, but that neither correlates with thenew variance in surprise function.4.2.1 MethodologySentences containing discourse connectives weregathered from the British National Corpus and theworld wide web, with discourse connectives identi-fied on the basis of their syntactic contexts (for de-tails, see Hutchinson (2004b)).
The mean numberof sentences per connective was about 32, 000, al-though about 12% of these are estimated to be er-rors.
From these sentences, lexical co-occurrencedata were collected.
Only co-occurrences with dis-15300.511.522.50  1  2  3  4  5DivergenceofDMco-occurrencesSimilarity judgementsbest fitSYNONYMHYPONYMCONT SUBSEXCLUSIVEFigure 5: Similarity versus distributional divergencecourse adverbials and other structural discourse con-nectives were stored, as these had previously beenfound to be useful for predicting semantic featuresof connectives (Hutchinson, 2004a).4.2.2 ResultsA skewed variant of the Kullback-Leibler diver-gence function was used to compare co-occurrencedistributions (Lee, 1999, with ?
= 0.95).
Spear-man?s correlation coefficient for ranked data showeda significant correlation (r = ?0.51, p < 0.001).
(The correlation is negative because KL divergenceis lower when distributions are more similar.)
Thestrength of this correlation is comparable with sim-ilar results achieved for verbs (Resnik and Diab,2000), but not as great as has been observed fornouns (McDonald, 2000).
Figure 5 plots the meansimilarity judgements against the distributional di-vergence obtained using discourse markers, and alsoindicates the substitutability relationship for eachitem.
(Two outliers can be observed in the upper leftcorner; these were excluded from the calculations.
)The ?variance in surprise?
function introduced inthe previous section was applied to the same co-occurrence data.1 These variances were comparedto distributional divergence and the subjects?
simi-larity ratings, but in both cases Spearman?s correla-tion coefficient was not significant.In combination with the previous experiment,1In practice, the skewed variant V (p, 0.95q + 0.05p) wasused, in order to avoid problems arising when q(x) = 0.these results demonstrate a three way correspon-dence between the human ratings of the similar-ity of a pair of connectives, their substitutabil-ity relationship, and their distributional similarity.Hutchinson (2005) presents further experiments onmodelling connective similarity, and discusses theirimplications.
This experiment also provides empiri-cal evidence that the new variance in surprise func-tion is not a measure of similarity.4.3 Experiment 3: Predicting substitutabilityThe previous experiments provide hope that sub-stitutability of connectives might be predicted onthe basis of their empirical distributions.
Howeverone complicating factor is that EXCLUSIVE is by farthe most likely relationship, holding between about70% of pairs.
Preliminary experiments showedthat the empirical evidence for other relationshipswas not strong enough to overcome this prior bias.We therefore attempted two pseudodisambiguationtasks which eliminated the effects of prior likeli-hoods.
The first task involved distinguishing be-tween the relationships whose connectives subjectsrated as most similar, namely SYNONYMY and HY-PONYMY.
Triples of connectives ?p, q, q??
werecollected such that SYNONYM(p, q) and either HY-PONYM(p, q?)
or HYPONYM(q?, p) (we were not at-tempting to predict the order of HYPONYMY).
Thetask was then to decide automatically which of q andq?
is the SYNONYM of p.The second task was identical in nature to the first,however here the relationship between p and q waseither SYNONYMY or HYPONYMY, while p and q?were either CONT.
SUBS.
or EXCLUSIVE.
Thesetwo sets of relationships are those corresponding tohigh and low similarity, respectively.
In combina-tion, the two tasks are equivalent to predicting SYN-ONYMY or HYPONYMY from the set of all four rela-tionships, by first distinguishing the high similarityrelationships from the other two, and then making afiner-grained distinction between the two.4.3.1 MethodologySubstitutability relationships between 49 struc-tural discourse connectives were extracted fromKnott?s (1996) classification.
In order to obtain moreevaluation data, we used Knott?s methodology to ob-tain relationships between an additional 32 connec-154max(D1, D2) max(V1, V2) (V1 ?
V2)2SYN 0.627 4.44 3.29HYP 0.720 5.16 8.02CONT 1.057 4.85 7.81EXCL 1.069 4.79 7.27Table 3: Distributional analysis by substitutabilitytives.
This resulted in 46 triples ?p, q, q??
for the firsttask, and 10,912 triples for the second task.The co-occurrence data from the previous sectionwere re-used.
These were used to calculate D(p||q)and V (p, q).
Both of these are asymmetric, so forour purposes we took the maximum of applyingtheir arguments in both orders.
Recall from Table 1that when two connectives are in a HYPONYMY re-lation we expect V to be sensitive to the order inwhich the connectives are given as arguments.
Totest this, we also calculated (V (p, q) ?
V (q, p))2,i.e.
the square of the difference of applying the argu-ments to V in both orders.
The average values aresummarised in Table 3, with D1 and D2 (and V1 andV2) denoting different orderings of the arguments toD (and V ), and max denoting the function whichselects the larger of two numbers.These statistics show that our theoretically moti-vated expectations are supported.
In particular, (1)SYNONYMOUS connectives have the least distribu-tional divergence and EXCLUSIVE connectives themost, (2) CONT.
SUBS.
and HYPONYMOUS connec-tives have the greatest values for V , and (3) V showsthe greatest sensitivity to the order of its argumentsin the case of HYPONYMY.The co-occurrence data were used to construct aGaussian classifier, by assuming the values for Dand V are generated by Gaussians.2 First, normalfunctions were used to calculate the likelihood ratioof p and q being in the two relationships:P (syn|data)P (hyp|data)=P (syn)P (hyp)?P (data|syn)P (data|hyp)(8)= 1?n(max(D1, D2);?syn, ?syn)n(max(D1, D2);?hyp, ?hyp)(9)2KL divergence is right skewed, so a log-normal model wasused to model D, whereas a normal model used for V .Input to Gaussian SYN vs SYN/HYP vsModel HYP EX/CONTmax(D1, D2) 50.0% 76.1%max(V1, V2) 84.8% 60.6%Table 4: Accuracy on pseudodisambiguation taskwhere n(x;?, ?)
is the normal function with mean?
and standard deviation ?, and where ?syn, for ex-ample, denotes the mean of the Gaussian model forSYNONYMY.
Next the likelihood ratio for p andq was divided by that for p and q?.
If this valuewas greater than 1, the model predicted p and qwere SYNONYMS, otherwise HYPONYMS.
The sametechnique was used for the second task.4.3.2 ResultsA leave-one-out cross validation procedure wasused.
For each triple ?p, q, q?
?, the data concern-ing the pairs p, q and p, q?
were held back, and theremaining data used to construct the models.
Theresults are shown in Table 4.
For comparison, a ran-dom baseline classifier achieves 50% accuracy.The results demonstrate the utility of the newvariance-based function V .
The new variance-basedfunction V is better than KL divergence at dis-tinguishing HYPONYMY from SYNONYMY (?2 =11.13, df = 1, p < 0.001), although it performsworse on the coarser grained task.
This is consis-tent with the expectations of Table 1.
The two clas-sifiers were also combined by making a naive Bayesassumption.
This gave an accuracy of 76.1% on thefirst task, which is significantly better than just us-ing KL divergence (?2 = 5.65, df = 1, p < 0.05),and not significantly worse than using V .
The com-bination?s accuracy on the second task was 76.2%,which is about the same as using KL divergence.This shows that combining similarity- and variance-based measures can be useful can improve overallperformance.5 ConclusionsThe concepts of lexical similarity and substitutabil-ity are of central importance to psychology, ar-tificial intelligence and computational linguistics.155To our knowledge this is the first modelling studyof how these concepts relate to lexical items in-volved in discourse-level phenomena.
We found athree way correspondence between data sources ofquite distinct types: distributional similarity scoresobtained from lexical co-occurrence data, substi-tutability judgements made by linguists, and thesimilarity ratings of naive subjects.The substitutability of lexical items is importantfor applications such as text simplification, where itcan be desirable to paraphrase one discourse con-nective using another.
Ultimately we would like toautomatically predict substitutability for individualtokens.
However predicting whether one connectivecan either a) always, b) sometimes or c) never besubstituted for another is a step towards this goal.Our results demonstrate that these general substi-tutability relationships have empirical correlates.We have introduced a novel variance-based func-tion of two distributions which complements distri-butional similarity.
We demonstrated the new func-tion?s utility in helping to predict the substitutabil-ity of connectives, and it can be expected to havewider applicability to lexical acquisition tasks.
Inparticular, it is expected to be useful for learningrelationships which cannot be characterised purelyin terms of similarity, such as hyponymy.
In futurework we will analyse further the empirical proper-ties of the new function, and investigate its applica-bility to learning relationships between other classesof lexical items such as nouns.AcknowledgementsI would like to thank Mirella Lapata, Alex Las-carides, Alistair Knott, and the anonymous ACL re-viewers for their helpful comments.
This researchwas supported by EPSRC Grant GR/R40036/01 anda University of Sydney Travelling Scholarship.ReferencesNicholas Asher and Alex Lascarides.
2003.
Logics of Conver-sation.
Cambridge University Press.James R. Curran and M. Moens.
2002.
Improvements in auto-matic thesaurus extraction.
In Proceedings of the Workshopon Unsupervised Lexical Acquisition, Philadelphia, USA.Gregory Grefenstette.
1994.
Explorations in Automatic The-saurus Discovery.
Kluwer Academic Publishers, Boston.Brigitte Grote and Manfred Stede.
1998.
Discourse markerchoice in sentence planning.
In Eduard Hovy, editor, Pro-ceedings of the Ninth International Workshop on NaturalLanguage Generation, pages 128?137, New Brunswick,New Jersey.
Association for Computational Linguistics.M.
Halliday and R. Hasan.
1976.
Cohesion in English.
Long-man.Jerry A Hobbs.
1985.
On the coherence and structure of dis-course.
Technical Report CSLI-85-37, Center for the Studyof Language and Information, Stanford University.Ben Hutchinson.
2004a.
Acquiring the meaning of discoursemarkers.
In Proceedings of the 42nd Annual Meeting ofthe Association for Computational Linguistics (ACL 2004),pages 685?692.Ben Hutchinson.
2004b.
Mining the web for discourse mark-ers.
In Proceedings of the Fourth International Conferenceon Language Resources and Evaluation (LREC 2004), pages407?410, Lisbon, Portugal.Ben Hutchinson.
2005.
Modelling the similarity of discourseconnectives.
To appear in Proceedings of the the 27th An-nual Meeting of the Cognitive Science Society (CogSci2005).Andrew Kehler.
2002.
Coherence, Reference and the Theory ofGrammar.
CSLI publications.Alistair Knott.
1996.
A data-driven methodology for motivat-ing a set of coherence relations.
Ph.D. thesis, University ofEdinburgh.Mirella Lapata and Alex Lascarides.
2004.
Inferring sentence-internal temporal relations.
In In Proceedings of the HumanLanguage Technology Conference and the North AmericanChapter of the Association for Computational LinguisticsAnnual Meeting, Boston, MA.Lillian Lee.
1999.
Measures of distributional similarity.
InProceedings of ACL 1999.Daniel Marcu and Abdessamad Echihabi.
2002.
An unsuper-vised approach to recognizing discourse relations.
In Pro-ceedings of the 40th Annual Meeting of the Association forComputational Linguistics (ACL-2002), Philadelphia, PA.Daniel Marcu.
2000.
The Theory and Practice of DiscourseParsing and Summarization.
The MIT Press.Scott McDonald.
2000.
Environmental determinants of lexicalprocessing effort.
Ph.D. thesis, University of Edinburgh.George A. Miller and William G. Charles.
1991.
Contextualcorrelates of semantic similarity.
Language and CognitiveProcesses, 6(1):1?28.M.
Moser and J. Moore.
1995.
Using discourse analysis andautomatic text generation to study discourse cue usage.
InProceedings of the AAAI 1995 Spring Symposium on Empir-ical Methods in Discourse Interpretation and Generation.Philip Resnik and Mona Diab.
2000.
Measuring verb similarity.In Proceedings of the Twenty Second Annual Meeting of theCognitive Science Society, Philadelphia, US, August.Philip Resnik.
1999.
Semantic similarity in a taxonomy: Aninformation-based measure and its application to problemsof ambiguity in natural language.
Journal of Artificial Intel-ligence Research, 11:95?130.H.
Rubenstein and J.
B. Goodenough.
1965.
Contextual corre-lates of synonymy.
Computational Linguistics, 8:627?633.Advaith Siddharthan.
2003.
Preserving discourse structurewhen simplifying text.
In Proceedings of the 2003 EuropeanNatural Language Generation Workshop.Julie Weeds and David Weir.
2003.
A general frameworkfor distributional similarity.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Processing(EMNLP 2003), Sapporo, Japan, July.Sholom M. Weiss and Casimir A. Kulikowski.
1991.
Computersystems that learn.
Morgan Kaufmann, San Mateo, CA.156
