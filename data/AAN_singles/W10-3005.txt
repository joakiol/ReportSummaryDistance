Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 32?39,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsDetecting Hedge Cues and their Scopes with Average PerceptronFeng Ji, Xipeng Qiu, Xuanjing HuangFudan University{fengji,xpqiu,xjhuang}@fudan.edu.cnAbstractIn this paper, we proposed a hedge de-tection method with average perceptron,which was used in the closed challengein CoNLL-2010 Shared Task.
There aretwo subtasks: (1) detecting uncertain sen-tences and (2) identifying the in-sentencescopes of hedge cues.
We use the unifiedlearning algorithm for both subtasks sincethat the hedge score of sentence can be de-composed into scores of the words, espe-cially the hedge words.
On the biomedicalcorpus, our methods achieved F-measurewith 77.86% in detecting in-domain un-certain sentences, 77.44% in recognizinghedge cues, and 19.27% in identifying thescopes.1 IntroductionDetecting hedged information in biomedical lit-eratures has received considerable interest in thebiomedical natural language processing (NLP)community recently.
Hedge information indicatesthat authors do not or cannot back up their opin-ions or statements with facts (Szarvas et al, 2008),which exists in many natural language texts, suchas webpages or blogs, as well as biomedical liter-atures.For many NLP applications, such as questionanswering and information extraction, the infor-mation extracted from hedge sentences would beharmful to their final performances.
Therefore,the hedge or speculative information should bedetected in advance, and dealt with different ap-proaches or discarded directly.In CoNLL-2010 Shared Task (Farkas et al,2010), there are two different level subtasks: de-tecting sentences containing uncertainty and iden-tifying the in-sentence scopes of hedge cues.For example, in the following sentence:These results suggest that the IRE motifin the ALAS mRNA is functional andimply that translation of the mRNA iscontrolled by cellular iron availabilityduring erythropoiesis.The words suggest and imply indicate that thestatements are not supported with facts.In the first subtask, the sentence is consideredas uncertainty.In the second subtask, suggest and imply areidentified as hedge cues, while the consecutiveblocks suggest that the IRE motif in the ALASmRNA is functional and imply that translation ofthe mRNA is controlled by cellular iron availabil-ity during erythropoiesis are recognized as theircorresponding scopes.In this paper, we proposed a hedge detec-tion method with average perceptron (Collins,2002), which was used in the closed challenges inCoNLL-2010 Shared Task (Farkas et al, 2010).Our motivation is to use a unified model to de-tect two level hedge information (word-level andsentence-level) and the model is easily expandedto joint learning of two subtasks.
Since that thehedge score of sentence can be decomposed intoscores of the words, especially the hedge words,we chosen linear classifier in our method and usedaverage perceptron as the training algorithm.The rest of the paper is organized as follows.
InSection 2, a brief review of related works is pre-sented.
Then, we describe our method in Section3.
Experiments and results are presented in thesection 4.
Finally, the conclusion will be presentedin Section 5.2 Related worksAlthough the concept of hedge information hasbeen introduced in linguistic community for along time, researches on automatic hedge detec-tion emerged from machine learning or compu-32tational linguistic perspective in recent years.
Inthis section, we give a brief review on the relatedworks.For speculative sentences detection, Medlockand Briscoe (2007) report their approach basedon weakly supervised learning.
In their method,a statistical model is initially derived from a seedcorpus, and then iteratively modified by augment-ing the training dataset with unlabeled samplesaccording the posterior probability.
They onlyemploy bag-of-words features.
On the publicbiomedical dataset1, their experiments achieve theperformance of 0.76 in BEP (break even point).Although they also introduced more linguistic fea-tures, such as part-of-speech (POS), lemma andbigram (Medlock, 2008), there are no significantimprovements.In Ganter and Strube (2009), the same task onWikipedia is presented.
In their system, score of asentence is defined as a normalized tangent valueof the sum of scores over all words in the sentence.Shallow linguistic features are introduced in theirexperiments.Morante and Daelemans (2009) present their re-search on identifying hedge cues and their scopes.Their system consists of several classifiers andworks in two phases, first identifying the hedgecues in a sentence and secondly finding the fullscope for each hedge cue.
In the first phase, theyuse IGTREE algorithm to train a classifier with3 categories.
In the second phase, three differentclassifiers are trained to find the first token and lasttoken of in-sentence scope and finally combinedinto a meta classifier.
The experiments shownthat their system achieves an F1 of nearly 0.85of identifying hedge cues in the abstracts sub cor-pus, while nearly 0.79 of finding the scopes withpredicted hedge cues.
More experiments couldbe found in their paper (Morante and Daelemans,2009).
They also provide a detail statistics onhedge cues in BioScope corpus2.3 Hedge detection with averageperceptron3.1 Detecting uncertain sentencesThe first subtask is to identify sentences con-taining uncertainty information.
In particular,1http://www.benmedlock.co.uk/hedgeclassif.html2http://www.inf.u-szeged.hu/rgai/bioscopethis subtask is a binary classification problem atsentence-level.We define the score of sentence as the confi-dence that the sentence contains uncertainty infor-mation.The score can be decomposed as the sum of thescores of all words in the sentence,S(x, y) = ?xi?xs(xi, y) =?xi?xwT?
(xi, y)where, x denotes a sentence and xi is the i-th word in the sentence x, ?
(xi, y) is a sparsehigh-dimensional binary feature vector of word xi.y ?
{uncertain, certain} is the category of thesentence.
For instance, in the example sentence,if current word is suggest while the category ofthis sentence is uncertain, the following feature ishired,?n(xi, y) ={1, if xi=??suggest??y=??uncertain??
,0, otherwisewhere n is feature index.This representation is commonly used in struc-tured learning algorithms.
We can combine thefeatures into a sparse feature vector ?
(x, y) =?i ?
(xi, y).S(x, y) = wT?
(x, y) = ?xi?xwT?
(xi, y)In the predicting phase, we assign x to the cate-gory with the highest score,y?
= argmaxywT?
(x, y)We learn the parameters w with online learningframework.
The most common online learner isthe perceptron (Duda et al, 2001).
It adjusts pa-rameters w when a misclassification occurs.
Al-though this framework is very simple, it has beenshown that the algorithm converges in a finitenumber of iterations if the data is linearly separa-ble.
Moreover, much less training time is requiredin practice than the batch learning methods, suchas support vector machine (SVM) or conditionalmaximum entropy (CME).Here we employ a variant perceptron algorithmto train the model, which is commonly namedaverage perceptron since it averages parametersw across iterations.
This algorithm is first pro-posed in Collins (2002).
Many experiments of33NLP problems demonstrate better generalizationperformance than non averaged parameters.
Moretheoretical proofs can be found in Collins (2002).Different from the standard average perceptron al-gorithm, we slightly modify the average strategy.The reason to this modification is that the origi-nal algorithm is slow since parameters accumulateacross all iterations.
In order to keep fast trainingspeed and avoid overfitting at the same time, wemake a slight change of the parameters accumu-lation strategy, which occurs only after each iter-ation over the training data finished.
Our trainingalgorithm is shown in Algorithm 1.input : training data set:(xn, yn), n = 1, ?
?
?
, N ,parameters: average number: K,maximum iteration number: T .output: average weight: cwInitialize: cw?
0,;for k = 0 ?
?
?K ?
1 dow0 ?
0 ;for t = 0 ?
?
?T ?
1 doreceive an example (xt, yt);predict: y?t = argmaxy wTt ?
(xt, y) ;if y?t 6= yt thenwt+1 = wt+?
(xt, yt)??
(xt, y?t)endendcw = cw +wT ;endcw = cw/K ;Algorithm 1: Average Perceptron algorithmBinary context features are extracted from 6predefined patterns, which are shown in Figure 1.By using these patterns, we can easily obtain thecomplicate features.
As in the previous example,if the current word is suggest, then a new com-pound feature could be extracted in the form ofw?1 =results//w0 =suggest by employing the pat-tern w?1w0.
// is the separate symbol.3.2 Identifying hedge cues and their scopesOur approach for the second subtask consists oftwo phases: (1) identifying hedge cues in a sen-tence, then (2) recognizing their correspondingscopes.3.2.1 Identifying hedge cuesHedge cues are the most important clues for de-termining whether a sentence contains uncertain?
unigram: w0,p0?
bigram: w0w1, w0p0, p0p1?
trigram: w?1w0w1Figure 1: Patterns employed in the sentence-levelhedge detection.
Here w denotes single word, p ispart of speech, and the subscript denotes the rela-tive offset compared with current position.?
unigram: w?2, w?1, w0, w1, w2, p0?
bigram: w?1w0, w0w1, w0p0, p?1p0, p0p1?
trigram: w?1w0w1Figure 2: Patterns employed in the word-levelhedge detection.information.
Therefore in this phase, we treat theproblem of identifying hedge cues as a classifica-tion problem.
Each word in a sentence would bepredicted a category indicating whether this wordis a hedge cue word or not.
In the previous ex-ample, there are two different hedge cues in thesentence (show in bold manner).
Words suggestand imply are assigned with the category CUE de-noting hedge cue word, while other words are as-signed with label O denoting non hedge cue word.In our system, this module is much similar tothe module of detecting uncertain sentences.
Theonly difference is that this phase is word level.
Sothat each training sample in this phase is a word,while in detecting speculative sentences trainingsample is a sentence.
The training algorithm is thesame as the algorithm shown in Algorithm 1.
12predefined patterns of context features are shownin Figure 2.3.2.2 Recognizing in-sentence scopesAfter identifying the hedge cues in the first phase,we need to recognize their corresponding in-sentence scopes, which means the boundary ofscope should be found within the same sentence.We consider this problem as a word-cue pairclassification problem, where word is any wordin a sentence and cue is the identified hedge cueword.
Similar to the previous phase, a word-levellinear classifier is trained to predict whether each34word-cue pair in a sentence is in the scope of thehedge cue.Besides base context features used in the pre-vious phase, we introduce additional syntactic de-pendency features.
These features are generatedby a first-order projective dependency parser (Mc-Donald et al, 2005), and listed in Figure 3.The scopes of hedge cues are always coveringa consecutive block of words including the hedgecue itself.
The ideal method should recognize onlyone consecutive block for each hedge cue.
How-ever, our classifier cannot work so well.
Therefore,we apply a simple strategy to process the outputof the classifier.
The simple strategy is to find amaximum consecutive sequence which covers thehedge cue.
If a sentence is considered to containseveral hedge cues, we simply combine the con-secutive sequences, which have at least one com-mon word, to a large block and assign it to therelative hedge cues.4 ExperimentsIn this section, we report our experiments ondatasets of CoNLL-2010 shared tasks, includingthe official results and our experimental resultswhen developing the system.Our system architecture is shown in Figure 4,which consists of the following modules.1.
corpus preprocess module, which employs atokenizer to normalize the corpus;2. sentence detection module, which uses a bi-nary sentence-level classifier to determinewhether a sentence contains uncertainty in-formation;3. hedge cues detection module, which identi-fies which words in a sentence are the hedgecues, we train a binary word-level classifier;4. cue scope recognition module, which recog-nizes the corresponding scope for each hedgecue by another word-level classifier.Our experimental results are obtained on thetraining datasets by 10-fold cross validation.
Themaximum iteration number for training the aver-age perceptron is set to 20.
Our system is imple-mented with Java3.3http://code.google.com/p/fudannlp/biomedical Wikipedia#sentences 14541 11111#words 382274 247328#hedge sentences 2620 2484%hedge sentences 0.18 0.22#hedge cues 3378 3133average number 1.29 1.26average cue length 1.14 2.45av.
scope length 15.42 -Table 1: Statistical information on annotated cor-pus.4.1 DatasetsIn CoNLL-2010 Shared Task, two differentdatasets are provided to develop the system: (1)biological abstracts and full articles from the Bio-Scope corpus, (2) paragraphs from Wikipedia.
Be-sides manually annotated datasets, three corre-sponding unlabeled datasets are also allowed forthe closed challenges.
But we have not employedany unlabeled datasets in our system.A preliminary statistics can be found in Ta-ble 1.
We make no distinction between sen-tences from abstracts or full articles in biomedi-cal dataset.
From Table 1, most sentences are cer-tainty while about 18% sentences in biomedicaldataset and 22% in Wikipedia dataset are spec-ulative.
On the average, there exists nearly 1.29hedge cues per sentence in biomedical dataset and1.26 in Wikipedia.
The average length of hedgecues varies in these two corpus.
In biomedicaldataset, hedge cues are nearly one word, but morethan two words in Wikipedia.
On average, thescope of hedge cue covers 15.42 words.4.2 Corpus preprocessThe sentence are processed with a maximum-entropy part-of-speech tagger4 (Toutanova et al,2003), in which a rule-based tokenzier is used toseparate punctuations or other symbols from reg-ular words.
Moreover, we train a first-order pro-jective dependency parser with MSTParser5 (Mc-Donald et al, 2005) on the standard WSJ trainingcorpus, which is converted from constituent treesto dependency trees by several heuristic rules6.4http://nlp.stanford.edu/software/tagger.shtml5http://www.seas.upenn.edu/?strctlrn/MSTParser/MSTParser.html6http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html35?
word-cue pair: current word and the hedge cue word pair,?
word-cue POS pair: POS pair of current word and the hedge cue word,?
path of POS: path of POS from current word to the hedge cue word along dependencytree,?
path of dependency: relation path of dependency from current word to the hedge cueword along dependency tree,?
POS of hedge cue word+direction: POS of hedge cue word with the direction to thecurrent word.
Here direction can be ?LEFT?
if the hedge cue is on the left to the currentword, or ?RIGHT?
on the right,?
tree depth: depth of current in the corresponding dependency tree,?
surface distance: surface distance between current word and the hedge cue word.
Thevalue of this feature is always 10 in the case of surface distance greater than 10,?
surface distance+tree depth: combination of surface distance and tree depth?
number of punctuations: number of punctuations between current word and the hedgecue word,?
number of punctuations + tree depth: combination of number of punctuations and treedepthFigure 3: Additional features used in recognizing in-sentence scope4.3 Uncertain sentences detectionIn the first subtask, we carry out the experimentswithin domain and cross domains.
As previouslymentioned, we do not use the unlabeled datasetsand make no distinction between abstracts and fullarticles in biomedical dataset.
This means wetrain the models only with the official annotateddatasets.
The model for cross-domain is trainedon the combination of annotated biomedical andWikipedia datasets.In this subtask, evaluation is carried out on thesentence level and F-measure of uncertainty sen-tences is employed as the chief metric.Table 2 shows the results within domain.
Af-ter 10-fold cross validation over training dataset,we achieve 84.39% of F1-measure on biomedicalwhile 56.06% on Wikipedia.We analyzed the low performance of our sub-mission result on Wikipedia.
The possible rea-son is our careless work when dealing with thetrained model file.
Therefore we retrain a modelfor Wikipedia and the performance is listed on thebottom line (Wikipedia?)
in Table 2.Dataset Precision Recall F110-fold cross validationbiomedical 91.03 78.66 84.39Wikipedia 66.54 48.43 56.06official evaluationbiomedical 79.45 76.33 77.86Wikipedia 94.23 6.58 1.23Wikipedia?
82.19 32.86 46.95Table 2: Results for in-domain uncertain sentencesdetectionTable 3 shows the results across domains.
Wesplit each annotated dataset into 10 folds.
Thentraining dataset is combined by individually draw-ing 9 folds out from the split datasets and therests are used as the test data.
On biomedicaldataset, F1-measure gets to 79.24% while 56.16%on Wikipedia dataset.
Compared with the resultswithin domain, over 5% performance decreasesfrom 84.39% to 79.24% on biomedical, but aslightly increase on Wikipedia.36Figure 4: System architecture of our systemDataset Precision Recall F110-fold cross validationbiomedical 87.86 72.16 79.24Wikipedia 67.78 47.95 56.16official evaluationbiomedical 62.81 79.11 70.03Wikipedia 62.66 55.28 58.74Table 3: Results for across-domain uncertain sen-tences detection4.3.1 Results analysisWe investigate the weights of internal features andfound that the words, which have no uncertaintyinformation, also play the significant roles to pre-dict the uncertainty of the sentence.Intuitively, the words without uncertainty infor-mation should just have negligible effect and thecorresponding features should have low weights.However, this ideal case is difficult to reached bylearning algorithm due to the sparsity of data.In Table 4, we list the top 10 words involvedin features with the largest weights for each cate-gory.
These words are ranked by the accumulativescores of their related features.In Table 5, we list the top 10 POS involved infeatures with the largest weight for each category.4.4 Hedge cue identificationHedge cues identification is one module for thesecond subtask, we also analyze the performanceon this module.Since we treat this problem as a binary classi-fication problem, we evaluate F-measure of hedgecue words.
The results are listed in Table 6.We have to point out that our evaluation isDataset Precision Recall F110-fold cross validation(word-level)biomedical 90.15 84.43 87.19Wikipedia 57.74 39.81 47.13official evaluation(phrase-level)biomedical 78.7 76.22 77.44Table 6: Results for in-domain hedge cue identifi-cationbased on word while official evaluation is basedon phrase.
That means our results would seemto be higher than the official results, especially onWikipedia dataset because average length of hedgecues in Wikipedia dataset is more than 2 words.4.4.1 Result AnalysisWe classify the results into four categories: falsenegative, false positive, true positive and true neg-ative.
We found that most mistakes are made be-cause of polysemy and collocation.In Table 7, we list top 10 words for each cate-gory.
For the false results, the words are difficult todistinguish without its context in the correspond-ing sentence.4.5 Scopes recognitionFor recognizing the in-sentence scopes, F-measureis also used to evaluate the performance of theword-cue pair classifier.
The results using goldhedge cues are shown in Table 8.
From the re-sults, F-measure achieves respectively 70.44% and75.94% when individually using the base contextfeatures extracted by 12 predefined patterns (seeFigure 1) and syntactic dependency features (seeFigure 3), while 79.55% when using all features.The results imply that syntactic dependency37biomedical Wikipedia cross domainuncertain certain uncertain certain uncertain certainwhether show probably the other suggest showmay demonstrate some often whether used tosuggest will many patients probably waslikely can one of another indicate CFSindicate role believed days appear demonstratepossible found possibly CFS putative the otherputative human considered are some of allappear known to such as any other thought ?
:?thought report several western possibly peoplepotential evidence said to pop likely could notTable 4: Top 10 significant words in detecting uncertain sentencesbiomedical Wikipedia cross domainuncertain certain uncertain certain uncertain certainMD SYM RB VBZ JJS SYMVBG PRP JJS CD RBS ?
:?VB NN RBS ?:?
RB JJRVBZ CD FW WRB EX WDTIN WDT VBP PRP CC CDTable 5: Top 5 significant POS in detecting uncertain sentencesDataset Precision Recall F1base context featuresbiomedical 66.04 75.48 70.44syntactic dependency featuresbiomedical 93.77 63.05 75.94all featuresbiomedical 78.72 80.41 79.55Table 8: Results for scopes recognizing with goldhedge cues (word-level)features contribute more benefits to recognizescopes than surface context features.Official results evaluated at block level are alsolisted in Table 9.dataset Precision Recall F1biomedical 21.87 17.23 19.27Table 9: Official results for scopes recognizing(block level)From Table 9 and the official result on hedgecue identification in Table 6, we believe that ourpost-processing strategy would be responsible forthe low performance on recognizing scopes.
Ourstrategy is to find a maximum consecutive blockcovering the corresponding hedge cue.
This strat-egy cannot do well with the complex scope struc-ture.
For example, a scope is covered by anotherscope.
Therefore, our system would generate ablock covering all hedge cues if there exists morethan one hedge cues in a sentence.5 ConclusionWe present our implemented system for CoNLL-2010 Shared Task in this paper.
We introducesyntactic dependency features when recognizinghedge scopes and employ average perceptron al-gorithm to train the models.
On the biomedi-cal corpus, our system achieves F-measure with77.86% in detecting uncertain sentences, 77.44%in recognizing hedge cues, and 19.27% in identi-fying the scopes.Although some results are low and beyond ourexpectations, we believe that our system can be atleast improved within the following fields.
Firstly,we would experiment on other kinds of features,such as chunk or named entities in biomedical.Secondly, the unlabeled datasets would be ex-plored in the future.38False Negative False Positive True Positive True Negativesupport considered suggesting chemiluminescenceof potential may rhinitisdemonstrate or proposal leukemogenica hope might ribosomalpostulate indicates indicating bpsupports expected likely nc82good can appear intronic/exonicadvocates should possible largeimplicated either speculate alleleputative idea whether endTable 7: Top 10 words with the largest scores for each category in hedge cue identificationAcknowledgmentsThis work was funded by Chinese NSF (GrantNo.
60673038), 973 Program (Grant No.2010CB327906, and Shanghai Committee of Sci-ence and Technology(Grant No.
09511501404).ReferencesMichael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing, pages 1?8.
Associa-tion for Computational Linguistics.Richard O. Duda, Peter E. Hart, and David G. Stork.2001.
Pattern classification.
Wiley, New York.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to Detect Hedges andtheir Scope in Natural Language Text.
In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning (CoNLL-2010): SharedTask, pages 1?12, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Viola Ganter and Michael Strube.
2009.
Findinghedges by chasing weasels: hedge detection usingWikipedia tags and shallow linguistic features.
InProceedings of the ACL-IJCNLP 2009 ConferenceShort Papers, pages 173?176.
Association for Com-putational Linguistics.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of the ACL, pages91?98.
Association for Computational Linguistics.Ben Medlock and Ted Briscoe.
2007.
Weakly super-vised learning for hedge classification in scientificliterature.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 992?999.
Association for Computational Lin-guistics.Ben Medlock.
2008.
Exploring hedge identification inbiomedical literature.
Journal of Biomedical Infor-matics, 41(4):636?654.Roser Morante andWalter Daelemans.
2009.
Learningthe scope of hedge cues in biomedical texts.
In Pro-ceedings of the Workshop on BioNLP, pages 28?36.Association for Computational Linguistics.Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, andJa?nos Csirik.
2008.
The BioScope corpus: anno-tation for negation, uncertainty and their scope inbiomedical texts.
In Proceedings of the Workshopon Current Trends in Biomedical Natural LanguageProcessing, pages 38?45.
Association for Computa-tional Linguistics.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.
Association for Compu-tational Linguistics.39
