Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 18?26,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsEstimating Annotation Cost for Active Learningin a Multi-Annotator EnvironmentShilpa Arora, Eric Nyberg and Carolyn P. Rose?Language Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{shilpaa,ehn,cprose}@cs.cmu.eduAbstractWe present an empirical investigation of theannotation cost estimation task for activelearning in a multi-annotator environment.
Wepresent our analysis from two perspectives:selecting examples to be presented to the userfor annotation; and evaluating selective sam-pling strategies when actual annotation costis not available.
We present our results on amovie review classification task with rationaleannotations.
We demonstrate that a combina-tion of instance, annotator and annotation taskcharacteristics are important for developing anaccurate estimator, and argue that both corre-lation coefficient and root mean square errorshould be used for evaluating annotation costestimators.1 IntroductionActive Learning is the process of selectively query-ing the user to annotate examples with the goalof minimizing the total annotation cost.
Annota-tion cost has been traditionally measured in termsof the number of examples annotated, but it hasbeen widely acknowledged that different examplesmay require different annotation effort (Settles et al,2008; Ringger et al, 2008).Ideally, we would use actual human annotationcost for evaluating selective sampling strategies, butthis will require conducting several user studies, oneper strategy on the same dataset.
Alternatively, wemay be able to simulate the real user by an annota-tion cost estimator that can then be used to evaluateseveral selective sampling strategies without havingto run a new user study each time.
An annotationcost estimator models the characteristics that candifferentiate the examples in terms of their annota-tion time.
The characteristics that strongly correlatewith the annotation time can be used as a criterionin selective sampling strategies to minimize the totalannotation cost.In some domains, the annotation cost of an ex-ample is known or can be calculated exactly beforequerying the user.
For example, in biological ex-periments it might be calculable from the cost ofthe equipment and the material used (King et al,2004).
In NLP, sometimes a simplifying assumptionis made that the annotation cost for an example canbe measured in terms of its length (e.g.
seconds ofvoicemail annotated (Kapoor et al, 2007); numberof tokens annotated (Tomanek et al, 2007)).
An-other assumption is that the number of user anno-tation actions can be used as a proxy for annota-tion cost of an example (e.g.
number of bracketsadded for parsing a sentence (Hwa, 2000); numberof clicks for correcting named entities (Kristjannsonet al, 2004)).
While these are important factors indetermining the annotation cost, none of them alonecan fully substitute for the actual annotation cost.For example, a short sentence with a lot of embed-ded clauses may be more costly to annotate than alonger sentence with simpler grammatical structure.Similarly, a short sentence with multiple verbs anddiscontinuous arguments may take more time to an-notate with semantic roles than a longer sentencewith a single verb and simple subject-verb-objectstructure (Carreras and Ma?rquez, 2004).What further complicates the estimation of anno-tation cost is that even for the same example, anno-tation cost may vary across annotators (Settles et al,2008).
For example, non-native speakers of Englishwere found to take longer time to annotate part of18speech tags (Ringger et al, 2008).
Often multipleannotators are used for creating an annotated cor-pus to avoid annotator bias, and we may not knowall our annotators beforehand.
Annotation cost alsodepends on the user interface used for annotation(Gweon et al, 2005), and the user interface maychange during an annotation task.
Thus, we needa general annotation cost estimator that can predictannotation cost for a given annotator and user inter-face.
A general estimator can be built by using an-notator and user interface characteristics in additionto the instance characteristics for learning an anno-tation cost model, and training on data from mul-tiple annotators and multiple user interfaces.
Sucha general estimator is important for active learningresearch where the goal is to compare selective sam-pling strategies independent of the annotator and theuser interface.In this work, we investigate the annotation cost es-timation problem for a movie review classificationtask in a multi-annotator environment with a fixeduser interface.
We demonstrate that a combinationof instance, annotation task and annotator charac-teristics is important for accurately estimating theannotation cost.
In the remainder of the paper, wefirst present a survey of related work and an analysisof the data collected.
We then describe the featuresused for our supervised learning approach to anno-tation cost estimation, followed by the experimentalsetup and results.
Finally, we conclude with somefuture directions we would like to explore.2 Related workThere has been some recent research effort in usingsupervised learning for estimating annotation cost.The most closely related work is that by Settles et al(2008) and Ringger et al (2008).
Settles et al (2008)present a detailed analysis of annotation cost for fourNLP applications: named entity recognition, imageretrieval, speculative vs. definite distinction, and in-formation extraction.
They study the effect of do-main, annotator, jitter, order of examples, etc., onthe annotation cost.Results from Settles et al (2008) are promisingbut leave much room for improvement.
They usedonly instance level features such as number of en-tities, length, number of characters, percentage ofnon-alpha numeric characters, etc.
for annotationcost estimation.
For three of their tasks, the corre-lation between the estimated and actual annotationtimes was in the range (R = 0.587 to 0.852).
Notethat the percentage of variance accounted for by amodel is obtained by squaring the R value from thecorrelation coefficient.
Thus, an R value of 0.587indicates that only about 34% (R2) of the varianceis accounted for, so the model will make incorrectpredictions about ranking in the majority of cases.Nevertheless, we acknowledge that our results arenot substantially better, although we argue that thiswork contributes to the pool of knowledge that willhopefully lead to better performance in the future.Settles et al (2008) train and test their estimatoron data from the same annotator.
Thus, in orderto use their model for a new annotator, we wouldneed to first collect data for that annotator and traina model.
In our work, a group of annotators anno-tate the same text, and we train and test on differentannotators.
We also show that using characteristicsof the annotators and annotation task in addition tothe instance characteristics improves performance.Ringger et al (2008) use linear regression for an-notation cost estimation for Part-Of-Speech (POS)tagging.
About 30 annotators annotated 36 differentinstances each.
The authors present about 13 de-scriptive statistics of the data, annotator and annota-tion task, but in their model they only used numberof tokens in the sentence and the number of correc-tions needed as features.
They report that the othervariables didn?t have a significant effect when eval-uated using a Bayesian Information Criterion (fromthe R package).Ringger et al (2008) noticed that nativeness ofthe annotator did have an effect on the annotationtime, but they chose not to include that featurein their model as they expected to have a similarmix of skills and background in their target anno-tators.
However, if annotation times differ substan-tially across annotators, then not accounting for thisdifference will reduce the performance of the model.Also, the low adjusted correlation value for theirmodel (R = 0.181) indicates that there is only aweak correlation between the annotation time and alinear combination of the length of the example andthe number of corrections.193 Analysis and ExperimentsIn this section, we present our annotation methodol-ogy and analysis of the data we collected, followedby a description of the features we used.We thenpresent our experimental setup followed by a dis-cussion of our results.3.1 Annotation Methodology and DataAnalysisIn this work, we estimate the annotation cost for amovie review classification task.
The data we usedwere collected as part of a graduate course.
Twentyannotators (students and instructors) were groupedinto five groups of four each.
The groups were cre-ated such that each group had similar variance inannotator characteristics such as department, educa-tional experience, programming experience, etc.
Weused the first 200 movie reviews from the datasetprovided by Zaidan et al (2007), with an equal dis-tribution of positive and negative examples.
Eachgroup annotated 25 movie reviews randomly se-lected from the 200 reviews and all annotators ineach group annotated all 25 reviews.
In additionto voting positive or negative for a review, annota-tors also annotated rationales (Zaidan et al, 2007),spans of text in the review that support their vote.Rationales can be used to guide the model by identi-fying the most discriminant features.
In related work(Arora and Nyberg, 2009), we ascertain that with ra-tionales the same performance can be achieved withless annotated data.
The annotation task with ra-tionales involved a variety of user actions: votingpositive or negative, highlighting spans of text andadding rationale annotations.
We used the same an-notation guidelines as Zaidan et al (2007).
The datahas been made available for research purposes1.
Fig-ure 1 shows a screenshot of the GUI used.
We per-formed an analysis of our data similar to that con-ducted by Settles et al (2008).
We address the fol-lowing main questions.Are the annotation times variable enough?
If allexamples take a similar time to annotate, then thenumber of examples can be used as an approxima-tion for the annotation cost.
Figure 2 shows the his-togram of averaged annotation times (averaged over1www.cs.cmu.edu/?shilpaa/datasets/ial/ial-uee-mr-v0.1.zipFigure 1: The GUI used for the annotation task.
The userselects the review (segment) to annotate from the list in theright panel.
The review text is displayed in the left panel.
Theuser votes positive or negative using the radio buttons.
Ratio-nales are added by selecting a span of text and right clickingto select the rationale tag.
The start/stop button can be used topause the current task.Figure 2: Distribution of averaged annotation times4 annotators in a group).
As can be seen from themean (?
= 165 sec.)
and the standard deviation(?
= 68.85), there is a meaningful variance in theannotation times.How do the annotation times vary across annota-tors?
A strong correlation between annotation timesfrom different annotators on a set of instances sug-gests that there are certain characteristics of these in-stances, independent of the annotator characteristics,that can determine their ranking based on the time ittakes to annotate them.
We evaluated the pairwisecorrelation for all pairs of annotators in each group(Table 1).
As can be seen, there is significant pair-wise correlation in more than half of the pairs of an-notators that differ in nativeness (10/16).
However,not all such pairs of annotators are associated withsignificant correlation.
This suggests that it is im-portant to consider both instance and annotator char-acteristics for estimating annotation time.20group Avg-Na(Std) Avg-CR(Std) #sign-pairs0 2.25(0.96) 0.54(0.27) 4/6 (4/5)1 1.75(0.5) 0.45(0.08) 5/6 (2/3)2 1(0) 0.13(0.17) 0/6 (0/0)3 1.75(0.96) 0.36(0.12) 2/6 (1/5)4 2.75(0.5) 0.47(0.04) 6/6 (3/3)Avg.
1.9(0.58) 0.39(0.21) 17/30 (10/16)Table 1: The Table shows the average nativeness and averagepairwise correlation between annotation times for the mem-bers of each group (and their standard deviation).
#sign-pairsshows the fraction of pairwise correlations within the groupsthat were significant (p < 0.05).
In brackets, is the fractionof correlations between annotators with different nativenesswithin the groups that were significant.The box plot in Figure 3 shows the distributionof annotation times across annotators.
As can beseen, some annotators take in general much longerthan others, and the distribution of times is very dif-ferent across annotators.
For some, the annotationtimes vary a lot, but not so much for others.
Thissuggests that using annotator characteristics as fea-tures in addition to the instance characteristics maybe important for learning a better estimator.Figure 3: Box plot shows the annotation time (in sec) dis-tribution (y-axis) for an annotator (x-axis) for a set of 25 doc-uments.
g0-a1 represents annotator 1 of group 0 and g0-avgrepresents the average annotation time.
A box represents themiddle 50% of annotation times, with the line representing themedian.
Whiskers on either side span the 1st and 4th quartilesand asterisks indicate the outliers.3.2 Feature DesignWe group the features in the following three cat-egories: Instance, Annotation Task and Annotatorcharacteristics.3.2.1 Instance characteristicsInstance characteristics capture the properties ofthe example the user annotates.
Table 2 describesthe instance based features we used and the intu-ition supporting their use for annotation cost esti-mation.
Table 3 shows the mean and standard de-viation of each of these characteristics, and as canbe seen, these characteristics do vary across exam-ples and hence these features can be beneficial fordistinguishing examples.3.2.2 Annotation Task characteristicsAnnotation task characteristics are those that canbe captured only during or after the annotation task.We used the number of rationales as a feature fromthis category.
In addition to voting for movie re-views as positive or negative, the user also adds ra-tionales that support their vote.
More rationales im-ply more work since the user must look for the rele-vant span of text and perform the physical action ofselecting the span and adding an annotation for eachrationale.
Table 3 shows the distribution of the aver-age Number of Rationales (NR) per example (aver-aged over the four annotators for a given set).3.2.3 Annotator characteristicsThe annotation cost of an example may varyacross annotators.
As reported in Table 1, the aver-age correlation for annotators on the same documentis low (R = 0.39) with 17 out of 30 pairwise correla-tions being significant.
Thus, it is important to con-sider annotator characteristics, such as whether theannotator is a native speaker of English, their educa-tion level, reading ability, etc.
In this work, we onlyuse nativeness of the annotator as a feature and planto explore other characteristics in the future.
We as-signed each annotator a nativeness value.
A valueof 3 was given to an annotator whose first languageis English.
A value of 2 was given to an annotatorwho has a different first language but has either beeneducated in English or has been in the United Statesfor a long time.
A value of 1 was assigned to the re-maining annotators.
Among the 20 annotators in thestudy, there were 8 annotators with nativeness valueof 1, and 6 each for nativeness values of 2 and 3.Table 1 shows the average and standard deviation ofthe nativeness score in each group.21Feature Definition IntuitionCharacterLength (CL)Length of review interms of number ofcharactersLonger documentstake longer to anno-tatePolar wordCount (PC)Number of wordsthat are polar (strongsubjective wordsfrom the lexicon(Wilson et al, 2005))More subjectivityimplies user wouldneed more time tojudge positive vs.negativeStop wordPercent (SC)Percentage of wordsthat are stop wordsA high percentageof stop words im-plies that the text isnot very complex andhence easier to read.Avg.
Sen-tence Length(SL)Average of the char-acter length of sen-tences in the reviewLong sentences in areview may make itharder to read.Table 2: Instance characteristicsFeature Mean Standard DeviationCL 2.25 0.92PC 41.50 20.39SP 0.45 0.03SL 121.90 28.72NR 4.80 2.30Table 3: Mean and the standard deviation for the feature oc-currences in the data.3.3 Evaluation MetricWe use both Root Mean Square (RMS) error andCorrelation Coefficient (CRCoef) to evaluate ourmodel, since the two metrics evaluate different as-pects of an estimate.
RMS is a way to quantify theamount by which an estimator differs from the truevalue of the quantity being estimated.
It tells us how?off?
our estimate is from the truth.
CRCoef on theother hand measures the strength and direction of alinear relationship between two random variables.
Ittells us how well correlated our estimate is with theactual annotation time.
Thus, for evaluating how ac-curate our model is in predicting annotation times,RMS is a more appropriate metric.
For evaluatingthe utility of the estimated annotation cost as a cri-terion for ranking and selecting examples for user?sannotation, CRCoef is a better metric.3.4 Experiments & ResultsWe learn an annotation cost estimator using the Lin-ear Regression and SMO Regression (Smola andScholkopf, 1998) learners from the Weka machinelearning toolkit (Witten and Frank, 2005).
As men-tioned earlier, we have 5 sets of 25 documents each,and each set was annotated by four annotators.
Theresults reported are averaged over five folds, whereeach set is one fold, and two algorithms (Linear Re-gression and SMO Regression).
Varying the algo-rithm helps us find the most predictive feature com-binations across different algorithms.
Since each setwas annotated by different annotators, we never trainand test on the data from same annotators.
We usedthe JMP2 and Minitab3 statistical tools for our analy-sis.
We used an ANOVA model with Standard LeastSquares fitting to compare the different experimen-tal conditions.
We make all comparisons in termsof both the CRCoef and the RMS metrics.
For sig-nificance results reported, we used 2-tailed pairedT-test, considering (p < 0.05) as significant.We present our results and analysis in three parts.We first compare the four instance characteristics,annotator and annotation task characteristics; andtheir combination.
We then present an analysisof the interaction between features and annotationtime.
Finally, we compare the ranking of featuresbased on the two evaluation metrics we used.3.4.1 Comparing characteristics for annotationcost estimationInstance Characteristics: We compare the fourinstance characteristics described in Section 3.2.1and select the most predictive characteristic for fur-ther analysis with annotator and annotation taskcharacteristics.
As can be seen in Table 4, characterlength performs the best, and it is significantly betterthan stop word percent and average sentence length.Character length also outperforms polar word count,but this difference is not significant.
Because of thelarge significant difference between the performanceof stop word percent and average sentence length,compared to character length, we do not considerthem for further analysis.Feature Combinations: In Table 5, we comparethe feature combinations of instance, annotator andannotation task characteristics.
The table also showsthe weights for the features used and the constant forthe linear regression model trained on all the data.
Amissing weight for a feature indicates that it wasn?tused in that feature combination.2http://www.jmp.com/software/3http://www.minitab.com/22Feature CR-Coef RMSCL 0.358 104.51PC 0.337 105.92SP -0.041* 114.34*SL 0.042* 114.50*Table 4: CR-Coef and RMS results for Character Length(CL), Polar word Count (PC), Stop word Percent (SP) and av-erage Sentence Length (SL).
Best performance is highlightedin bold.
?
marks the results significantly worse than the best.We use only the best performing instance charac-teristic, the character length.
The length of an ex-ample has often been substituted for the annotationcost (Kapoor et al, 2007; Tomanek et al, 2007).We show in Table 5 that certain feature combina-tions significantly outperform character length.
Thecombination of all three features (last row) performsthe best for both CRCoef and RMS, and this resultis significantly better than the character length (thirdrow).
The combination of number of rationales andnativeness (fourth row) also outperforms characterlength significantly in CRCoef.
This suggests thatthe number of rationales we expect or require in a re-view and the annotator characteristics are importantfactors for annotation cost estimation and should beconsidered in addition to the character length.CL NR AN Const.
CR-Coef RMS-29.33 220.77 0.135?
123.93?17.59 82.81 0.486 95.290.027 61.53 0.357?
104.51?19.11 -40.78 153.21 0.55+ 96.040.028 32.79 120.18 0.397?
109.85?0.02 15.15 17.57 0.553+ 90.27+0.021 16.64 -41.84 88.09 0.626+ 88.44+Table 5: CR-Coef and RMS results for seven feature com-binations of Character Length (CL), Number of Rationales(NR) and Annotator Nativeness (AN).
The values in featureand ?Const.?
columns are weights and constant for the linearregression model trained on all the data.
The numbers in boldare the results for the best feature combination.
?
marks theresults significantly worse than the best.
+ marks the resultssignificantly better than CL.The impact of the nativeness feature is somewhatmixed.
Adding the nativeness feature always im-proves the correlation and for RMS, it helps whenadded to the combined feature (CL+NR) but not oth-erwise.
Although this improvement with additionof the nativeness feature is not significant, it doessuggest that annotator characteristics might be im-portant to consider.
To investigate this further, weevaluated our assumption that native speakers takeless time to annotate.
For each set, we compared theaverage annotation times (averaged over examples)against the nativeness values.
For all sets, annotatorswith nativeness value of 3 always took less time onaverage than those with nativeness value of 2 or 1.Between 2 and 1, there were no reliable differences.Sometimes annotators with value of 1 took less timethan annotators with value of 2.
Also, for group 2which had all annotators with nativeness value of 1,we observed a poor correlation between annotators(Table 1).
This suggest two things: 1) our assign-ment of nativeness value may not be accurate andwe need other ways of quantifying nativeness, 2)there are other annotator characteristics we shouldtake into consideration.PC CL NR AN Const.
CR RMS0.027 61.53 0.358ab 104.5x2.2 74.20 0.337a 105.9x0.7 0.019 60.89 0.355b 104.9x0.028 -32.8 120.2 0.397ab 109.8x2.3 -35.5 135.1 0.382a 111.1x1.1 0.016 -34.3 121.8 0.395b 109.9x0.02 15.1 17.57 0.553a 90.27x1.5 15.1 32.02 0.542a 91.65x0.0 0.02 15.1 17.57 0.554a 90.40x0.021 16.6 -41.8 88.09 0.626a 88.44x1.6 16.5 -43.5 102.8 0.614a 90.42y0.0 0.021 16.6 -41.8 88.09 0.626a 88.78xTable 6: Each block of 3 rows in this table compares theperformance of Character Length (CL) and Polar word Count(PC) in combination with Number of Rationales (NR) and An-notator Nativeness (AN) features.
The values in feature and?Const.?
columns are weights and constant for the linear re-gression model trained on all the data.
Best performance ishighlighted in bold.
Results in a block not connected by sameletter are significantly different.Polar word Count and Character Length: As wesaw in Table 4, the difference between characterlength and polar word count is not significant.
Wefurther compare these two instance characteristicsin the presence of the annotator and annotation taskcharacteristics.
Our goal is to ascertain whethercharacter length performs better than polar wordcount, or vice versa, and whether this difference issignificant.
We also evaluate whether using bothperforms better than using any one of them alone.The results presented in Table 6 help us answer thesequestions.
For all feature combinations characterlength, with and without polar word count, performs23better than polar word count, but this difference isnot significant except in three cases.
These resultssuggests that polar word count can be used as an al-ternative to character length in annotation cost esti-mation.3.4.2 Interaction between Features andAnnotation TimeAs a post-experiment analysis, we studied theinteraction between the features we used and an-notation time, and the interaction among featuresthemselves.
Table 7 reports the pairwise correlation(Pearson, 1895) for these variables, calculated overall 125 reviews.
As can be seen, all features havesignificant correlation with annotation time exceptstop words percentage and average sentence length.Note that number of rationales has higher correla-tion with annotation time (R = 0.529) than charac-ter length (R = 0.417).
This suggests that numberof rationales may have more influence than charac-ter length on annotation time, and a low correlationbetween number of rationales and character length(R = 0.238) indicates that it might not be the casethat longer documents necessarily contain more ra-tionales.
Annotating rationales requires cognitiveeffort of identifying the right span and manual ef-fort to highlight and add an annotation, and hencemore rationales implies more annotation time.
Wealso found some examples in our data where docu-ments with substantially different lengths but samenumber of rationales took a similar time to anno-tate.
One possible explanation for this observation isuser?s annotation strategy.
If the annotator choosesto skim through the remaining text when enough ra-tionales are found, two examples with same numberof rationales but different lengths might take similartime.
We plan to investigate the effect of annotator?sstrategy on annotation time in the future.A negative correlation of nativeness with annota-tion time (R = ?0.219) is expected, since nativespeakers (AN = 3) are expected to take less anno-tation time than non-native speakers (AN = {2, 1}),although this correlation is low.
A low correla-tion between number of rationales and nativeness(R = 0.149) suggests that number of rationalesa user adds may not be influenced much by theirnativeness value.
A not significant low correlation(R = ?0.06) between character length and native-AT CL NR AN PC SP SLAT 1CL 0.42 1NR 0.53 0.24 1AN -0.22 0.06 0.15 1PC 0.4 0.89 0.28 0.11 1SP 0.03 0.06 0.14 0.03 0.04 1SL 0.08 0.15 0.01 -0.01 0.14 -0.13 1Table 7: Correlation between Character Length (CL), Num-ber of Rationales (NR), Annotator Nativeness (AN), Polarword Count (PC), Stop word Percent (SP), average SentenceLength (SL) and Annotation Time (AT), calculated over alldocuments (125) and all annotators (20).
Significant corre-lations are highlighted in bold.ness provides no evidence that reviews with differentlengths were distributed non-uniformly across anno-tators with different nativeness.The number of polar words in a document has asimilar correlation with annotation time as characterlength (R = 0.4).
There is also a strong correla-tion between character length and polar word count(R = 0.89).
Since reviews are essentially people?sopinions, we can expect longer documents to havemore polar words.
This also explains why there is nosignificant difference in performance for polar wordcount and character length (Table 4).
A more usefulfeature may be the information about the number ofpositive and negative polar words in a review, since areview with both positive and negative opinions canbe difficult to classify as positive or negative.
Weplan to explore these variations of the polar wordfeature in the future.
We also plan to investigate howwe can exploit this dependence between characteris-tics for annotation cost estimation.3.4.3 CRCoef Vs. RMSWe presented our results using correlation coef-ficient and root mean squared error metrics.
Ta-ble 8 shows the ranking of the feature combinationsfrom better to worse for both these metrics and aswe can see, there is a difference in the order of fea-ture combinations for the two metrics.
Also, signif-icance results differ in some cases for the two met-rics.
These differences suggest that features whichcorrelate well with the annotation times (higher CR-Coef rank) can give an accurate ranking of examplesbased on their annotation cost, but they may not beas accurate in their absolute estimate for simulatingannotators and thus might have a lower RMS rank.Thus, it is important to evaluate the user effort esti-24mator in terms of both these metrics so that the rightestimator can be chosen for a given objective.Rank CR-Coef RMS1 (CL+NR+AN) (CL+NR+AN)2 (CL+NR) (CL+NR)3 (NR+AN) (NR)4 (NR) (NR+AN)5 (CL+AN) (CL)6 (CL) (CL+AN)7 (AN) (AN)Table 8: Ranking of feature combinations.4 Towards a General Annotation CostEstimatorOur multi-annotator environment allows us to trainand test on data from different annotators by usingannotator characteristics as features in the annota-tion cost estimation.
A model trained on data from avariety of annotators can be used for recommend-ing examples to annotators not represented in ourtraining data but with similar characteristics.
Thisis important since we may not always know all ourannotators before building the model, and trainingan estimator for each new annotator is costly.
Also,in active learning research, the goal is to evaluateselective sampling approaches independently of theannotator.
Choosing annotators for supervised an-notation cost estimation such that the within groupvariance in annotator characteristics is high will giveus a more generic estimator and a stricter evaluationcriterion.
Thus, we have a framework that has thepotential to be used to build a user-independent an-notation cost estimator for a given task.However, this framework is specific to the UserInterface (UI) used.
A change in the user interfacemight require recollecting the data from all the an-notators and training a model on the new data.
Forexample, if annotating rationales was made signif-icantly faster in a new UI design, it would havea major impact on annotation cost.
An alternativewould be to incorporate UI features in our model andtrain it on several different UIs or modifications ofthe same UI, which will allow us to use our trainedmodel with a new user interface or modifications ofthe existing UIs, without having to recollect the dataand retrain the model.
A few UI features that can beused in our context are: adding a rationale annota-tion, voting positive or negative, etc.
The units forexpressing these features will be the low-level userinterface actions such as number of clicks, mousedrags, etc.
For example, in our task, adding a ra-tionale annotation requires one mouse drag and twoclicks, and adding a vote requires one click.
In a dif-ferent user interface, adding a rationale annotationmight require just one mouse drag.Using UI features raises a question of whetherthey can replace the annotation task features; e.g.,whether the UI feature for adding rationale anno-tation can replace the number of rationales feature.Our hypothesis is that number of rationales has moreinfluence on annotation time than just the manual ef-fort of annotating them.
It also requires the cognitiveeffort of finding the rationale, deciding its span, etc.We aim to explore incorporating UI features in ourannotation cost estimation model in the future.5 Conclusion and Future WorkIn this work we presented a detailed investigation ofannotation cost estimation for active learning withmultiple annotators.
We motivated the task from twoperspectives: selecting examples to minimize anno-tation cost and simulating annotators for evaluatingactive learning approaches.
We defined three cate-gories of features based on instance, annotation taskand annotator characteristics.
Our results show thatusing a combination of features from all three cate-gories performs better than any one of them alone.Our analysis was limited to a small dataset.
In thefuture, we plan to collect a larger dataset for this taskand explore more features from each feature group.With the multi-annotator annotation cost estima-tor proposed, we also motivated the need for a gen-eral estimator that can be used with new annotatorsor user interfaces without having to retrain.
We aimto explore this direction in the future by extendingour model to incorporate user interface features.
Wealso plan to use the annotation cost model we devel-oped in an active learning experiment.AcknowledgmentsWe would like to thank Hideki Shima for his helpwith the task setup and Jing Yang for helpful discus-sions.
We would also like to thank all the anony-mous reviewers for their helpful comments.25ReferencesShilpa Arora and Eric Nyberg.
2009.
Interactive An-notation Learning with Indirect Feature Voting.
InProceedings of NAACL-HLT 2009 (Student ResearchWorkshop).Xavier Carreras and Llu?`s Ma?rquez.
2004.
Intro-duction to the CoNLL-2004 Shared Task: Seman-tic Role Labeling.
http://www.lsi.upc.edu/?srlconll/st04/st04.html.Gahgene Gweon, Carolyn Penstein Ros?e, Joerg Wittwerand Matthias Nueckles.
2005.
Supporting Efficientand Reliable Content Analysis Using Automatic TextProcessing Technology.
In proceedings of INTER-ACT 2005: 1112-1115.Robbie A. Haertel, Kevin D. Seppi, Eric K. Ringger andJanes L. Cattoll.
2008.
Return on Investment for Ac-tive Learning.
In proceedings of NIPS Workshop onCost Sensitive Learning.Rebecca Hwa.
2000.
Sample Selection for StatisticalGrammar Induction.
In proceedings of joint SIGDATconference on Empirical Methods in NLP and VeryLarge Corpora.Ashish Kapoor, Eric Horvitz and Sumit Basu.
2007.
Se-lective supervision:Guiding supervised learning withdecision-theoretic active learning.
In proceedings ofIJCAI, pages 877-882.Ross D. King, Kenneth E. Whelan, Ffion M. Jones, PhilipG.
K. Reiser, Christopher H. Bryant, Stephen H. Mug-gleton, Douglas B. Kell and Stephen G. Oliver.
2004.Functional Genomics hypothesis generation and ex-perimentation by a robot scientist.
In proceedings ofNature, 427(6971):247-52.Trausti Kristjansson, Aron Culotta, Paul Viola and An-drew Mccallum.
2004.
Interactive Information Ex-traction with Constrained Conditional Random Fields.In proceedings of AAAI.Karl Pearson.
1895.
Correlation Coefficient.
Royal So-ciety Proceedings, 58, 214.Eric Ringger, Marc Carmen, Robbie Haertel, KevinSeppi, Deryle Lonsdale, Peter McClanahan, Janes L.Cattoll and Noel Ellison.
2008.
Assessing the Costs ofMachine-Assisted Corpus Annotation through a UserStudy.
In proceedings of LREC.Burr Settles, Mark Craven and Lewis Friedland.
2008.Active Learning with Real Annotation Costs.
In pro-ceedings of NIPS Workshop on Cost Sensitive Learn-ing.Alex J. Smola and Bernhard Scholkopf 1998.
A Tutorialon Support Vector Regression.
NeuroCOLT2 Techni-cal Report Series - NC2-TR-1998-030.Katrin Tomanek, Joachim Wermter and Udo Hahn.
2007.An approach to text corpus construction which cuts an-notation costs and maintains reusability of annotateddata.
In proceedings of EMNLP-CoNLL, pp.
486-495.Theresa Wilson, Janyce Wiebe and Paul Hoffmann.2005.
Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.
In proceedings ofHLT/EMNLP, Vancouver, Canada.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical machine learning tools and techniques.
2nd Edi-tion, Morgan Kaufmann, San Francisco.Omar Zaidan, Jason Eisner and Christine Piatko.
2007.Using ?annotator rationales?
to improve machinelearning for text categorization.
In Proceedings ofNAACL-HLT, pp.
260-267, Rochester, NY.26
