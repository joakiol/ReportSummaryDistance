Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 174?179,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsEvaluating Distant Supervision for Subjectivity and Sentiment Analysison Arabic Twitter FeedsEshrag Refaee and Verena RieserInteraction Lab, School of Mathematical and Computer Sciences,Heriot-Watt University,EH14 4AS Edinburgh, United Kingdom.eaar1@hw.ac.uk, v.t.rieser@hw.ac.ukAbstractSupervised machine learning methods forautomatic subjectivity and sentiment anal-ysis (SSA) are problematic when appliedto social media, such as Twitter, since theydo not generalise well to unseen topics.
Apossible remedy of this problem is to ap-ply distant supervision (DS) approaches,which learn from large amounts of auto-matically annotated data.
This researchempirically evaluates the performance ofDS approaches for SSA on Arabic Twitterfeeds.
Results for emoticon- and lexicon-based DS show a significant performancegain over a fully supervised baseline, es-pecially for detecting subjectivity, wherewe achieve 95.19% accuracy, which is a48.47% absolute improvement over previ-ous fully supervised results.1 IntroductionSubjectivity and sentiment analysis (SSA) aims todetermine the attitude of an author with respectto some topic, e.g.
objective or subjective, orthe overall contextual polarity of an utterance, e.g.positive or negative.
Previous work on automaticSSA has used manually annotated gold standarddata sets to analyse which feature sets and mod-els perform best for this task, e.g.
(Wilson et al.,2009; Wiebe et al., 1999).
Most of this work is inEnglish, but there have been first attempts to applysimilar techniques to Arabic, e.g.
(Abdul-Mageedet al., 2011; Mourad and Darwish, 2013).
Whilethese models work well when tested using cross-validation on limited static data sets, our previ-ous results reveal that these models do not gen-eralise to new data sets, e.g.
collected at a laterpoint in time, due to their limited coverage (Refaeeand Rieser, 2014).
While there is a growing inter-est within the NLP community in building Arabiccorpora by harvesting the web, e.g.
(Al-Sabbaghand Girju, 2012; Abdul-Mageed and Diab, 2012;Zaidan and Callison-Burch, 2013), these resourceshave not been publicly released yet and only smallamounts of these data-sets are (manually) anno-tated.
We therefore turn to an approach knownas distant supervision (DS), as first proposed by(Read, 2005), which uses readily available fea-tures, such as emoticons, as noisy labels in or-der to efficiently annotate large amounts of datafor learning domain-independent models.
This ap-proach has been shown to be successful for En-glish SSA, e.g.
(Go et al., 2009), and SSA forunder-resourced languages, such as Chinese (Yuanand Purver, 2012).The contributions of this paper are as follows:we first collect two large corpora using emoticonsand lexicon-based features as noisy labels, whichwe plan to release as part of this submission.
Sec-ond, this work is the first to apply and empiricallyevaluate DS approaches on Arabic Twitter feeds.We find that DS significantly outperforms fully su-pervised SSA on our held-out test set.
However,compared to a majority baseline, predicting nega-tive sentiment proves to be difficult using DS ap-proaches.
Third, we conduct an error analysis tocritically evaluate the results and give recommen-dations for future directions.2 Arabic Twitter SSA CorporaWe start by collecting three corpora at differenttimes over one year to account for the cyclic ef-fects of topic change in social media (Eisenstein,2013).
Table 1 shows the distributions of labels inour data-sets:1.
A gold standard data-set which we use fortraining and evaluation (spring 2013);2.
A data-set for DS using emoticon-basedqueries (autumn 2013);3.
Another data-set for DS using a lexicon-based approach (winter 2014).174Data set Neutral Polar Positive Negative TotalGold standard training 1,157 937 470 467 3,031Emoticon-based training 55,076 62,466 32,842 33,629 184,013Lexicon-based training 55,076 55,538 18,442 5,013 134,069Manually labelled test 422 579 278 301 1,580Table 1: Sentiment label distribution of the gold standard manually annotated and distant supervisiondata sets.Gold Standard Data-set: We harvest two goldstandard data sets at different time steps, whichwe label manually.
We first harvest a data set of3,031 multi-dialectal Arabic tweets randomly re-trieved over the period from February to March2013.
We use this set as a training set for ourfully supervised approach.
We also manually label1,580 tweets collected in autumn 2013, which weuse as an independent held-out test set.
Two na-tive speakers were recruited to manually annotatethe collected data for subjectivity and sentiment,where we define sentiment as a positive or nega-tive emotion, opinion or attitude, following (Wil-son et al., 2009).
Our gold standard annotationsreached a weighted ?
= 0.76, which indicates re-liable annotations (Carletta, 1996).
We also auto-matically annotate the corpus with a rich set of lin-guistically motivated features using freely avail-able processing tools for Arabic, such as MADA(Nizar Habash and Roth, 2009), see Table 2.
Formore details on gold standard corpus annotationplease see (Refaee and Rieser, 2014).1Type Feature-setsMorphological diacritic, aspect, gender, mood, per-son, part of speech (POS), state, voice,has morphological analysis.Syntactic n-grams of words and POS, lem-mas, including bag of words (BOW),bag of lemmas.Semantic has positive lexicon,has negative lexicon,has neutral lexicon, has negator,has positive emoticon,has negative emoticon.Table 2: Annotated Feature-setsEmoticon-Based Queries: In order to investi-gate DS approaches to SSA, we also collect amuch larger data set of Arabic tweets, wherewe use emoticons as noisy labels, following e.g.
(Read, 2005; Go et al., 2009; Pak and Paroubek,2010; Yuan and Purver, 2012; Suttles and Ide,2013).
We query Twitter API for tweets with vari-ations of positive and negative emoticons to ob-tain pairs of micro-blog texts (statuses) and usingEmoticon Sentiment label:) , :-) , :)), (: , (-: ,((:positive:( , :-( , :(( , :(( , ): ,)): )-:negativeTable 3: Emoticons used to automatically label thetraining data-set.emoticons as author-provided emotion labels.
Infollowing (Purver and Battersby, 2012; Zhang etal., 2011; Suttles and Ide, 2013), we also utilisesome sentiment-bearing hash tags to query emo-tional tweets, e.g.
hQ?
happiness and?Q ksadness.
Note that emoticons and hash-tags aremerely used to collect and build the training setand were replaced by the standard (positive/ neg-ative) labels.
In order to collect neutral instances,we query a set of official news accounts, followingan approach by (Pak and Paroubek, 2010).
Exam-ples of the accounts queried are: BBC-Arabic, Al-Jazeera Arabic, SkyNews Arabia, Reuters Arabic,France24-Arabic, and DW Arabic.
We then au-tomatically extract the same set of linguisticallymotivated features as for the gold standard corpus.Lexicon-Based Annotation: We also inves-tigate an alternative approach to DS, whichcombines rule-driven lexicon-based SSA, e.g.
(Taboada et al., 2011), with machine learning ap-proaches, following (Zhang et al., 2011).
Webuild a new training dataset by combining threelexica.
We first exploit two existing subjectiv-ity lexica: a manually annotated Arabic subjectiv-ity lexicon (Abdul-Mageed and Diab, 2012) anda publicly available English subjectivity lexicon,MPQA (Wilson et al., 2009), which we automati-cally translate using Google Translate, following a1This GS data-set has been shared viaa special LREC repository available athttp://www.resourcebook.eu/shareyourlr/index.php175similar technique to (Mourad and Darwish, 2013).The translated lexicon is manually corrected by re-moving translations with neutral or no clear senti-ment indicator.2This results in 2,627 translated in-stances after correction.
We then construct a thirddialectal lexicon of 484 words that we extractedfrom an independent Twitter development set andmanually annotated for sentiment.
All lexiconswere merged into a combined lexicon of 4,422 an-notated sentiment words (duplicates removed).
Inorder to obtain automatic labels for positive andnegative instances, we follow a simplified versionof the rule-based aggregation approach of Taboadaet al.
(2011).
First, all lexicons and tweets are lem-matised.
For each tweet, matched sentiment wordsare marked with either (+1) or (-1) to incorporatethe semantic orientation of individual constituents.This achieves a coverage level of 76.62% (whichis computed as a percentage of tweets with at leastone lexicon word) using the combined lexicon.The identified sentiment words are replaced byplace-holders to avoid bias.
To account for nega-tion, we reverse the polarity (switch negation) fol-lowing (Taboada et al., 2011).
The sentiment ori-entation of the entire tweet is then computed bysumming up the sentiment scores of all sentimentwords in a given tweet into a single score that au-tomatically determines the label as being: positiveor negative.
Instances where the score equals zeroare excluded from the training set as they representmixed-sentiment instances with an even number ofsentiment words.
We validate this lexicon-basedlabelling approach against a separate developmentset by comparing the automatically computed la-bels against manually annotated ones, reaching anaccuracy of 69.06%.3 Classification Experiments UsingDistant SupervisionWe experiment with a number of machine learn-ing methods and we report the results of the bestperforming scheme, namely Support Vector Ma-chines (SVMs), where we use the implementationprovided by WEKA (Witten and Frank, 2005).
Wereport the results on two metrics: F-score and ac-curacy.
We use paired t-tests to establish signifi-cant differences (p < .05).
We experiment withdifferent feature sets and report on the best results(Bag-of-Words (BOW) + morphological + seman-2For instance, the day of judgement is assigned with a neg-ative label while its Arabic translation is neutral consideringthe context-independent polarity.tic).
We compare our results against a majoritybaseline and against a fully supervised approach.It is important to mention the most prominent pre-vious work on SSA of Arabic tweets like (Abdul-Mageed et al., 2012) who trained SVM classifierson a nearly 3K manually labelled data-set to curryout two-stage binary classification attaining accu-racy up to 65.87% for the sentiment classificationtask.
In a later work, (Mourad and Darwish, 2013)employ SVM and Naive Bayes classifiers trainedon a set of 2,300 manually labelled Arabic tweets.With 10-fold cross-validation settings, the authorreported an accuracy score of 72.5% for the senti-ment classification task (positive vs. negative).We evaluate the approaches on a separate held-out test-set that is collected at a later point in time,as described in Section 2.3.1 Emoticon-Based Distant SupervisionWe first evaluate the potential of exploiting train-ing data that is automatically labelled using emoti-cons.
The results are summarised in Table 4.Polar vs. neutral: The results show a signifi-cant improvement over the majority baseline, aswell as over the classifier trained on the gold stan-dard data set: We achieve 95.19% accuracy onthe held-out set, which is a 48.47% absolute im-provement over our previous fully supervised re-sults.
We attribute this improvement to two fac-tors.
First, the emoticon-based data set is about 60times bigger than the gold standard data set (seeTable 1) and thus the emoticon-based model bettergeneralises to unseen events.
Note that this perfor-mance is comparable with (Suttles and Ide, 2013)who achieved up to 98% accuracy using emoticon-based DS on English tweets using 5.9 milliontweets.
Second, neutral instances were sampledfrom news accounts, which are mainly written inmodern standard Arabic (MSA), whereas we as-sume that tweets including emoticons (which weuse for acquiring polar instances) are mainly writ-ten in dialectal Arabic (DA).
In future work, weplan to investigate this hypothesis further by au-tomatically detecting MSA/DA for a given tweet,e.g.
(Zaidan and Callison-Burch, 2013).
Abdul-Mageed et al.
(2012) show that having such a fea-ture can result in no significant impact on the over-all performance of both subjectivity and sentimentanalysis tasks.Positive vs. negative: For sentiment classifica-tion, the performance of the emoticon-based ap-proach degrades notably to 51%, which is still176Data-set majoritybaselinefully super-visedemoticon DS lexicon-presencelexicon-aggr.F Acc.
F Acc.
F Acc.
F Acc.
F Acc.polar vs. neutral 0.69 53.0 0.43 46.62 0.95 95.19 0.95 95.09 0.91 91.09positive vs. negative 0.67 50.89 0.41 49.65 0.51 51.25 0.53 57.06 0.52 52.98Table 4: 2-level and single-level SSA classification using distant supervision (DS).significantly better that the fully supervised base-line, but nevertheless worse than a simple majoritybaseline.
These results are much lower than previ-ous results on emoticon-based sentiment analysison English tweets by (Go et al., 2009; Bifet andFrank, 2010) which both achieved around 83% ac-curacy.
The confusion matrix shows that mostlynegative instances are misclassified as positive,with a very low recall on negative instances, seeTable 5.
Next, we investigate possible reasons in adetailed error analysis.Data set Precision Recallemoticon DSpositive 0.479 0.81negative 0.556 0.212lexicon-presence DSpositive 0.521 0.866negative 0.733 0.317lexicon-aggregation DSpositive 0.496 0.650negative 0.583 0.426Table 5: Recall and precision for Sentiment Anal-ysis3.1.1 Error Analysis for Emoticon-Based DSIn particular, we investigate the use of sarcasm andthe direction emoticons face in right-to-left alpha-bets.Use of sarcasm and irony: Using an emoticonas a label is naturally noisy, since we cannot knowfor sure the intended meaning the author wishesto express.
This is especially problematic whenemoticons are used in a sarcastic way, i.e.
theirintended meaning is the opposite of the expressedemotion.
An example from our data set is:(1) ):???
@ AK?J?g.great job Ahli :( ?
refer-ring to a famous footballteam.Research in psychology shows that up to 31% ofthe time, emoticons are used sarcastically (Wolf,2000).
In order to investigate this hypothesiswe manually labelled a random sample of 303misclassified instances for neutral, positive, nega-tive, as well as sarcastic, mixed and unclear sen-timents, see Table 6.
Interestingly, the sarcas-tic instances represent only 4.29%, while tweetswith mixed (positive and negative) sentiments rep-resent 5.94% of the manually annotated sub-set.In 34.32% of the instances, the manual labelshave matched the automatic emoticon-based la-bels.
Surprisingly, automatic emoticon-based la-bel contrasts the manual labels in 36.63% of theinstances.
Instances labelled as neutral represent4.95%.
The rest of the instances were assigned?unclear sentiment orientation?.EmoticonLabelPredictedlabelManual label # in-stancesPositive Negative Mixed 8Negative Positive Mixed 10Positive Negative Negative 59Negative Positive Negative 42Positive Negative Neutral 29Negative Positive Neutral 7Positive Negative Positive 62Negative Positive Positive 52Positive Negative Sarcastic 8Negative Positive Sarcastic 5Positive Negative Unclear senti-ment indicator19Negative Positive Unclear senti-ment indicator2Table 6: Results of labelling sarcasm, mixed emo-tions and unclear sentiment for misclassified in-stances.Facing of emoticons: We therefore investigateanother possible error source following (Mouradand Darwish, 2013), who claim that the right-to-left alphabetic writing of Arabic might result inemoticons being mistakenly interchanged whiletyping.
On some Arabic keyboards, typing ?
)?will produce the opposite ?
(?
parentheses.
Thefollowing example (2) illustrates a case of a mis-classified instance, where we assume that the fac-ing of emoticons might have been interchanged ormistyped.
(2) (: ??
@ ??A?
?Cg no hope anymore :)3.2 Lexicon-Based Distant SupervisionTo avoid the issue of ambiguity in the directionof facing, we experiment with a lexicon-based ap-proach to DS: instead of using emoticons, we now177utilise the adjectives in our sentiment lexicon asnoisy labels.
We experiment with two differentsettings for the lexicon-based DS approach.
First,we experiment with a lexicon-presence approachthat automatically labels a tweet as a positive in-stance if it only includes positive lexicon(s) andthe same for the negative class.
Data instanceshaving mixed positive and negative lexicons or nosentiment lexicons are excluded from the trainingset.
The second approach is based on assigninga numerical value to sentiment words and aggre-gating the value into a single score, see Section 2.The results are summarised in Table 4.Polar vs. neutral: We can observe that the mod-els trained with the lexicon-presence approach sig-nificantly outperform the majority baseline, thefully supervised learning, as well as the lexicon-aggregation approach.
The lexicon-presence andthe emoticon-based DS approaches reach almostidentical performance on our test set.Positive vs. negative: Again, we observe thatit is difficult to discriminate negative instancesfor both lexicon-based approaches.
The lexicon-presence approach significantly outperforms themajority baseline, the fully supervised learn-ing, and the lexicon-aggregation approach.
Butthis time it also significantly outperforms theemoticon-based approach, which allows us to con-clude that lexicon-based labelling introduces lessnoise for sentiment analysis.
However, our re-sults are significantly worse than the lexicon-basedapproach of Taboada et al.
(2011), with up to80% accuracy, and the learning-based approachof Zhanh et al.
(2011), with up to 85% accu-racy on English tweets.
The lexicon-presence ap-proach achieves the highest precision for negativetweets, see table 5, but still has a low recall.
Thelexicon-aggregation approach has the highest re-call for negative tweets, but its precision is almostidentical to the emoticon-based approach.3.2.1 Error Analysis for Lexicon-Based DSWe conduct an error analysis in order to fur-ther investigate the difference in performancebetween the lexicon-presence and the lexicon-aggregation approach.
We hypothesise that thelexicon-aggregation might perform better on in-stances with mixed emotions, i.e.
tweets withpositive and negative indicators, but a clear over-all sentiment.
We therefore manually add 36 in-stances to the test set which contain mixed emo-tions (but a unique sentiment label).
However, theresults on the new test set confirm the superiorityof the lexicon-presence approach.
In general, bothlexicon-based approaches perform worse for sen-timent classification.
Taboada et al.
(2011) high-light the issue of ?positive bias?
associated withlexicon-based approaches of sentiment analysis,as people tend to prefer using positive expressionsand understate negative ones.4 Conclusion and Future WorkWe address the task of subjectivity and sentimentanalysis (SSA) for Arabic Twitter feeds.
We em-pirically investigate the performance of distant su-pervision (DS) approaches on a manually labelledindependent test set, in comparison to a fully su-pervised baseline, trained on a manually labelledgold standard data set.
Our experiments reveal:(1) DS approaches to SSA for Arabic Twitterfeeds show significantly higher performance in ac-curacy and F-score than a fully supervised ap-proach.
Despite providing noisy labels, they allowlarger amounts of data to be rapidly annotated, andthus, can account for the topic shifts observed insocial media.
(2) DS approaches which use a subjectivity lex-icon for labelling outperform approaches usingemoticon-based labels for sentiment analysis, butachieve a very similar performance for subjectiv-ity detection.
We hypothesise that this can be at-tributed to unclear facings of the emoticons.
(3) We also find that both our DS approachesachieve good results of up to 95% accuracy forsubjectivity analysis, which is comparable to pre-vious work on English tweets.
However, we detecta decrease in performance for sentiment analysis,where negative instances repeatedly get misclas-sified as positive.
We assume that this can be at-tributed to the more indirect ways adopted by peo-ple to express their emotions verbally via socialmedia (Taboada et al., 2011).
Other possible rea-sons for this, which we will explore in future work,include culturally specific differences (Hong et al.,2011), as well as pragmatic/ context-dependent as-pects of opinion (Sayeed, 2013).178ReferencesMuhammad Abdul-Mageed and Mona Diab.
2012.AWATIF: A multi-genre corpus for modern standardArabic subjectivity and sentiment analysis.
In Pro-ceedings of the Eight International Conference onLanguage Resources and Evaluation (LREC?12), Is-tanbul, Turkey.
European Language Resources As-sociation (ELRA).Muhammad Abdul-Mageed, Mona T. Diab, and Mo-hammed Korayem.
2011.
Subjectivity and senti-ment analysis of modern standard Arabic.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies: short papers - Volume 2, HLT?11, pages 587?591, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Muhammad Abdul-Mageed, Sandra Kuebler, andMona Diab.
2012.
SAMAR: A system for subjec-tivity and sentiment analysis of Arabic social me-dia.
In Proceedings of the 3rd Workshop in Com-putational Approaches to Subjectivity and SentimentAnalysis, pages 19?28.
Association for Computa-tional Linguistics.Rania Al-Sabbagh and Roxana Girju.
2012.
YADAC:Yet another dialectal Arabic corpus.
In Proceed-ings of the Eight International Conference on Lan-guage Resources and Evaluation (LREC?12), Istan-bul, Turkey.
European Language Resources Associ-ation (ELRA).Albert Bifet and Eibe Frank.
2010.
Sentiment knowl-edge discovery in twitter streaming data.
In Discov-ery Science, pages 1?15.
Springer.J.
Carletta.
1996.
Assessing agreement on classifica-tion tasks: the kappa statistic.
Computational Lin-guistics, 22(2):249?254.Jacob Eisenstein.
2013.
What to do about bad lan-guage on the internet.
In Proceedings of NAACL-HLT, pages 359?369.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.CS224N Project Report, Stanford, pages 1?12.Lichan Hong, Gregorio Convertino, and Ed H Chi.2011.
Language matters in twitter: A large scalestudy.
In ICWSM.Ahmed Mourad and Kareem Darwish.
2013.
Sub-jectivity and sentiment analysis of modern stan-dard Arabic and Arabic microblogs.
WASSA 2013,page 55.Owen Rambow Nizar Habash and Ryan Roth.
2009.MADA+TOKAN: A toolkit for Arabic tokenization,diacritization, morphological disambiguation, postagging, stemming and lemmatization.
In KhalidChoukri and Bente Maegaard, editors, Proceedingsof the Second International Conference on ArabicLanguage Resources and Tools, Cairo, Egypt, April.The MEDAR Consortium.A.
Pak and P. Paroubek.
2010.
Twitter as a corpus forsentiment analysis and opinion mining.
In Proceed-ings of LREC.Matthew Purver and Stuart Battersby.
2012.
Experi-menting with distant supervision for emotion classi-fication.
In Proceedings of the 13th Conference ofthe European Chapter of the Association for Com-putational Linguistics (EACL), pages 482?491, Avi-gnon, France, April.
Association for ComputationalLinguistics.Jonathon Read.
2005.
Using emoticons to reduce de-pendency in machine learning techniques for senti-ment classification.
In Proceedings of the ACL Stu-dent Research Workshop, pages 43?48.
Associationfor Computational Linguistics.Eshrag Refaee and Verena Rieser.
2014.
An Arabictwitter corpus for subjectivity and sentiment anal-ysis.
In Proceedings of the Ninth InternationalConference on Language Resources and Evalua-tion (LREC?14), Reykjavik, Iceland, may.
EuropeanLanguage Resources Association (ELRA).Asad Sayeed.
2013.
An opinion about opinions aboutopinions: subjectivity and the aggregate reader.
InProceedings of NAACL-HLT, pages 691?696.Jared Suttles and Nancy Ide.
2013.
Distant supervi-sion for emotion classification with discrete binaryvalues.
In Computational Linguistics and IntelligentText Processing, pages 121?136.
Springer.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
Computa-tional linguistics, 37(2):267?307.Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P.O?Hara.
1999.
Development and use of a gold-standard data set for subjectivity classifications.
InProceedings of the 37th annual meeting of the As-sociation for Computational Linguistics on Com-putational Linguistics, ACL ?99, pages 246?253,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analy-sis.
Computational Linguistics, 35(3):399?433.Ian H Witten and Eibe Frank.
2005.
Data Mining:Practical machine learning tools and techniques.Morgan Kaufmann.Alecia Wolf.
2000.
Emotional expression online: Gen-der differences in emoticon use.
CyberPsychology& Behavior, 3(5):827?833.Zheng Yuan and Matthew Purver.
2012.
Predictingemotion labels for chinese microblog texts.
In Pro-ceedings of the 1st International Workshop on Senti-ment Discovery from Affective Data (SDAD), pages40?47, Bristol, UK, September.Omar F. Zaidan and Chris Callison-Burch.
2013.
Ara-bic dialect identification.
Computational Linguis-tics.Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil, Me-ichun Hsu, and Bing Liu.
2011.
Combining lex-iconbased and learning-based methods for twittersentiment analysis.
HP Laboratories, Technical Re-port HPL-2011, 89.179
