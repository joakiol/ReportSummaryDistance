Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 38?47,Beijing, August 2010Learning Semantic Network Patterns for HypernymyExtractionTim vor der Bru?ckIntelligent Information and Communication Systems (IICS)FernUniversita?t in Hagentim.vorderbrueck@fernuni-hagen.deAbstractCurrent approaches of hypernymy ac-quisition are mostly based on syntacticor surface representations and extracthypernymy relations between surfaceword forms and not word readings.In this paper we present a purely se-mantic approach for hypernymy ex-traction based on semantic networks(SNs).
This approach employs a setof patternssub0(a1, a2) ?
premise where thepremise part of a pattern is given by aSN.
Furthermore this paper describeshow the patterns can be derived byrelational statistical learning followingthe Minimum Description Length prin-ciple (MDL).
The evaluation demon-strates the usefulness of the learnedpatterns and also of the entire hyper-nymy extraction system.1 IntroductionA concept is a hypernym of another conceptif the first concept denotes a superset of thesecond.
For instance, the class of animals is asuperset of the class of dogs.
Thus, animal isa hypernym of its hyponym dog and a hyper-nymy relation holds between animal and dog.A large collection of hypernymy (supertype)relations is needed for a multitude of tasksin natural language processing.
Hypernymsare required for deriving inferences in ques-tion answering systems, they can be employedto identify similar words for information re-trieval or they can be useful to avoid word-repetition in natural language generation sys-tems.
To build a taxonomy manually requiresa large amount of work.
Thus, automatic ap-proaches for their construction are preferable.In this work we introduce a semantically ori-ented approach where the hypernyms are ex-tracted using a set of patterns which are nei-ther syntactic nor surface-oriented but insteadpurely semantic and are based on a SN for-malism.
The patterns are applied on a setof SNs which are automatically derived fromthe German Wikipedia1 by a deep syntactico-semantic analysis.
Furthermore, these pat-terns are automatically created by a machinelearning approach based on the MDL princi-ple.2 Related WorkPatterns for hypernymy extraction were firstintroduced by Hearst (Hearst, 1992), the so-called Hearst patterns.
An example of such apattern is:NPhypo {,NPhypo}*{,} and other NPhyper.These patterns are applied on arbitrarytexts and the instantiated variables NPhypoand NPhyper are then extracted as a concretehypernymy relation.Apart from the handcrafted patterns therewas also some work to determine patternsautomatically from texts (Snow and others,2005).
For that, Snow et al collected sen-tences in a given text corpus with known hy-pernym noun pairs.
These sentences are thenparsed by a dependency parser.
Afterwards,the path in the dependency tree is extractedwhich connects the corresponding nouns witheach other.
To account for certain key wordsindicating a hypernymy relation like such (seefirst Hearst pattern) they added the links tothe word on either side of the two nouns (if notyet contained) to the path too.
Frequently oc-1Note that for better readability the examples aretranslated from German into English throughout thispaper.38curring paths are then learned as patterns forindicating a hypernymy relation.An alternative approach for learning pat-terns which is based on a surface instead ofa syntactic representation was proposed byMorin et al (Morin and Jaquemin, 2004).They investigate sentences containing pairs ofknown hypernyms and hyponyms as well.
Allthese sentences are converted into so-called?lexico-syntactic expressions?
where all NPsand lists of NPs are replaced by special sym-bols, e.g.
: NP find in NP such as LIST.
Asimilarity measure between two such expres-sions is defined as the sum of the maximallength of common substrings for the maxi-mum text windows before, between and afterthe hyponym/hypernym pair.
All sentencesare then clustered according to this similaritymeasure.
The representative pattern (calledcandidate pattern) of each cluster is defined tobe the expression with the lowest mean squareerror (deviation) to all other expressions inthe same similarity cluster.
The patterns tobe used for hyponymy detection are the can-didate patterns of all clusters found.3 MultiNetMultiNet is an SN formalism (Helbig, 2006).In contrast to SNs like WordNet (Fellbaum,1998) or GermaNet (Hamp and Feldweg,1997), which contain lexical relations betweensynsets, MultiNet is designed to comprehen-sively represent the semantics of natural lan-guage expressions.
An SN in the MultiNetformalism is given as a set of vertices andarcs where the vertices represent the concepts(word readings) and the arcs the relations (orfunctions) between the concepts.
A vertex canbe lexicalized if it is directly associated to alexical entry or non-lexicalized.
An exampleSN is shown in Fig.
1.
Note that each vertexof the SN is assigned both a unique ID (e.g.,c2 ) and a label which is the associated lexicalentry for lexicalized vertices and anon for non-lexicalized vertices.
Thus, two SNs differingonly by the IDs of the non-lexicalized verticesare considered equivalent.
Important Multi-Net relations/functions are (Helbig, 2006):?
agt: Conceptual role: Agent?
attr: Specification of an attribute?
val: Relation between a specific at-tribute and its value?
prop: Relation between object and prop-erty?
*itms: Function enumerating a set?
pred: Predicative concept characterizinga plurality?
obj: Neutral object?
sub0: Relation of conceptual subordi-nation (hyponymy) and hyperrelation tosubr, subs, and sub?
subs: Relation of conceptual subordina-tion (for situations)?
subr: Relation of conceptual subordina-tion (for relations)?
sub: Relation of conceptual subordina-tion other than subs and subrMultiNet is supported by a semantic lexicon(Hartrumpf and others, 2003) which defines,in addition to traditional grammatical entrieslike gender and number, semantic informationconsisting of one or more ontological sorts andseveral semantic features for each lexicon en-try.
The ontological sorts (more than 40) forma taxonomy.
In contrast to other taxonomies,ontological sorts are not necessarily lexical-ized, i.e., they need not denote lexical entries.The following list shows a small selection ofontological sorts which are inherited from ob-ject :?
Concrete objects: e.g., milk, honey?
Discrete objects: e.g., chair?
Substances: e.g.,, milk, honey?
Abstract objects: e.g., race, robberySemantic features denote certain semanticproperties for objects.
Such a property caneither be present, not present or underspeci-fied.
A selection of several semantic featuresis given below:animal, animate, artif (artificial), human,spatial, thconc (theoretical concept)Example for the concept bottle.1.1 2: dis-crete object; animal -, animate -, artif +,human -, spatial +, thconc -, .
.
.2the suffix .1.1 denotes the reading numbered .1.1of the word bottle.39c1c2c3c4c5c6c7c8c9c10*MODPpresent.0SUBPROPTEMPOBJSCARSUBSSUBdenote.1.1tall.1.1very.1.1SUBa2=house.1.1a1=skyscraper.1.1Figure 1: Matching a pattern to an SN.
Bold lines indicate matched arcs, the dashed line theinferred arc.The SNs as described here are automati-cally constructed from (German) texts by thedeep linguistic parser WOCADI3(Hartrumpf,2002) whose parsing process is based on aword class functional analysis.4 Application of Deep PatternsThe extraction of hyponyms as described hereis based on a set of patterns.
Each patternconsists of a conclusion part sub0(a1 , a2 ) anda premise part in form of an SN where both a1and a2 have to show up.
The patterns are ap-plied by a pattern matcher (or automated the-orem prover if axioms are used) which matchesthe premise with an SN.
The variable bindingsfor a1 and a2 are given by the matched con-cepts of the SN.
An example pattern whichmatches to the sentence: A skyscraper de-notes a very tall building.
is D4 (see Ta-ble 1).
The pattern matching process is il-lustrated in Fig.1.
The resulting instantiatedconclusion which is stored in the knowledgebase is sub0(skyscraper.1.1, house.1.1).
Ad-vantages by using the MultiNet SN formalism3WOCADI is the abbreviation for word classdisambiguation.for hypernym (and instance-of relation) acqui-sition consists of: learning relations betweenword readings instead of words, the possibil-ity to apply logical axioms and backgroundknowledge, and that person names are alreadyparsed.An example sentence from the Wikipediacorpus where a hypernymy relation was suc-cessfully extracted by our deep approach andwhich illustrates the usefulness of this ap-proach is: In any case, not all incidentsfrom the Bermuda Triangle or from otherworld areas are fully explained.
From this sen-tence, a hypernymy pair cannot be extractedby the Hearst pattern X or other Y.
The ap-plication of this pattern fails due to the wordfrom which cannot be matched.
To extractthis relation by means of shallow patterns anadditional pattern would have to be intro-duced.
This could also be the case if syntacticpatterns were used instead since the coordina-tion of Bermuda Triangle and world areas isnot represented in the syntactic constituencytree but only on a semantic level4.4Note that some dependency parsers normalizesome syntactic variations too.405 Graph Substructure Learning ByFollowing the MinimumDescription Length PrincipleIn this section, we describe how the patternscan be learned by a supervised machine learn-ing approach following the Minimum Descrip-tion Length principle.
This principle statesthat the best hypothesis for a given data setis that one which minimizes the descriptionof the data (Rissanen, 1989), i.e., compressesthe data the most.
Basically we follow thesubstructure learning approach of Cook andHolder (Cook and Holder, 1994).According to this approach, the descriptionlength to minimize is the number of bits re-quired to encode a certain graph which is com-pressed by means of a substructure.
If a lotof graph vertices can be matched with thesubstructure vertices, this description lengthwill be quite small.
For our learning scenariowe investigate collection of SNs containing aknown hypernymy relationship.
A pattern(given by a substructure in the premise) whichcompresses this set quite well is expected to beuseful for extracting hypernyms.Let us first determine the number of bits toencode the entire graph or SN.
A graph can berepresented by its adjacency matrix and a setof vertex and arc labels.
Since an adjacencymatrix consists only of ones and zeros, it iswell suitable for a binary encoding.
For theencoding process, we do not regard the labelnames directly but instead their number as-suming an ordering exists on the label names(e.g., alphabetical).c1 c2 c3 c4 c5 c6 c7 c8 c9 c10c1 0 0 0 0 0 0 0 0 0 0c2 0 0 0 0 0 0 0 0 0 0c3 0 0 0 0 0 0 0 0 0 0c4 1 1 0 0 0 0 0 0 0 0c5 0 0 1 0 0 0 0 0 0 0c6 0 0 0 0 0 0 1 0 0 0c7 0 0 0 0 0 0 0 0 0 0c8 0 0 0 0 1 1 0 0 1 1c9 0 0 0 0 0 0 0 0 0 0c10 0 0 0 0 0 0 0 0 0 0Figure 2: Adjacency matrix of the SN.To encode all labels the number of labelsand a list of all label numbers have to be spec-ified, e.g., 3,1,2,1 for 3 vertices with two dif-ferent label numbers5 (1,2).
The first numberencoding (3) starts at position 0 in the bitstring, the second (1) at position 2 = dlog2 3e,the third one at position 2+dlog2 2e, etc.
Sincethe graph actually need not to be encoded inthis way but only the length of the encodingis important, non-integer numbers of bits areaccepted for simplicity too.
If there are a totalof lu different labels, then each encoded labelnumber requires log2(lu) bits.
The total num-ber of bits to encode the vertex labels are thengiven by:vbits = log2(v) + v log2(lu) in which v denotesthe total number of vertices6.In the next step, the adjacency matrix is en-coded where each row is processed separately.A straightforward approach for encoding onerow would be to use v number of bits, one forevery column.
However, the number of zerosare generally much larger than the number ofones which means that a better compressionof the data is possible by exploiting this fact.Consider the case that a certain matrix rowcontains exactly m ones.
There are(vm)possibilities to distribute the ones to the indi-vidual cells.
All possible permutations couldbe specified in a list.
In this case it is onlynecessary to specify the position in this list touniquely describe one row.
Let b = maxi ki.Then the number of ones in one row can beencoded using log2(b + 1) bits.
log2(vki)bits are required to encode the distributionof ones in one row.
Additionally, log2(b + 1)bits are needed to encode b which is only nec-essary once for the matrix.
Let us considerthe adjacency matrix given in Fig.
2 of theSN shown in Fig.
1 with 10 rows and columnswhere each row contains at most four ones.To encode the row c4, containing two ones, re-5The commas are only included for better readabil-ity and are actually not encoded.6The approach of Cook and Holder is a bit inex-act here.
To be precise, the number of bits needed toencode v and b would have to be known a priori.41quires log2(4) + log2(102)=7.49 bits whichis smaller than 10 bits which were necessaryfor the na?
?ve approach.
The total length rbitsof the encoding is given by:rbits = log2(b + 1) +v?i=1[log2(b + 1)+log2(vki)](1)=(v + 1) log2(b + 1)+v?i=1log2(vki)Finally, the arcs need to be encoded.
Lete(i, j) be the number of arcs between vertexi and j in the graph and m := maxi,je(i, j).log2(m) bits are required to encode the num-ber of arcs between both vertices and log2(le)bits are needed for the arc label (out of a setof le elements).
Then the entire number ofbits is given by (e is the number of arcs in thegraph):ebits = log2(m) +v?i=1v?j=1[A[i, j]log2(m)+e(i, j) log2(le)]= log2(m) + e log2(le)+v?i=1v?j=1A[i, j] log2(m)=e(log2(le)) + (K + 1) log2(m)(2)where K is the number of ones in the adja-cency matrix.The total description length of the graph isthen given by: vbits + rbits + ebits.Now let us investigate how the descriptionlength of the compressed graph is determined.In the original algorithm the substructure isreplaced in the graph by a single vertex.
Thedescription length of the graph compressed bythe substructure is then given by the descrip-tion length of the substructure added by thedescription length of the modified graph.c1 c2 c3 c4 c5 c6 c7 c8 c9 c10c1 0 0 0 0 0 0 0 0 0 0c2 0 0 0 0 0 0 0 0 0 0c3 0 0 0 0 0 0 0 0 0 0c4 1 1 0 0 0 0 0 0 0 0c5 0 0 ?
0 0 0 0 0 0 0c6 0 0 0 0 0 0 ?
0 0 0c7 0 0 0 0 0 0 0 0 0 0c8 0 0 0 0 ?
?
0 0 ?
?c9 0 0 0 0 0 0 0 0 0 0c10 0 0 0 0 0 0 0 0 0 0Figure 3: Adjacency matrix of the compressedSN.
Vertices whose connections can be com-pletely inferred from the pattern are removed.In our method there are two major differ-ences from the graph learning approach ofCook and Holder.?
Not a single graph is compressed but aset of graphs.?
For the approach of Cook and Holder, itis unknown which vertex of the substruc-ture a graph node is actually connectedwith.
Thus, the description is not com-plete and the original graph could not bereconstructed using the substructure andthe compressed graph.
To make the de-scription complete we specify the bind-ings of the substructure vertices to thegraph vertices.The generalization of the Cook and Holder-algorithm to a set of graphs is quite straightforward.
The total description length of a setof compressed graphs is given by the descrip-tion length of the substructure (here pattern)added to the sum of the description lengths ofeach SN compressed by this pattern.Additional bits are needed to encode thevertex bindings (assuming the pattern premiseis contained in the SN).
First the numberof bindings bin ([1, vp], vp: number of non-lexicalized vertices appearing in a pattern) hasto be specified which requires log2(vp) bits.The number of bits needed to encode a singlebinding is given by log2(vp) + log2(v) (vertexindices: [0, vp?1] to [0, v?1]).
Thus, the total42number of required bits is given bybinbits =bin(log2(vp) + log2(v))+log2(vp)(3)Note that not all bindings need to be en-coded.
The number of required binding en-codings can be determined as follows.
Firstall bindings for all non-lexicalized pattern ver-tices are determined.
Then all cells from theadjacency matrix of the SN which contain aone and are also contained in the adjacencymatrix of the pattern, if this binding is ap-plied to the non-lexicalized pattern vertices,are set to zero.
Vertices which contain only ze-ros in the adjacency matrix on both columnsand rows are removed from the adjacency ma-trix/graph.
The arcs from and to this ver-tex can be completely inferred by the patternwhich means that all vertices this vertex isconnected with are also contained in the pat-tern.
Since SNs differing only by the IDs oftheir non-lexicalized vertices are consideredidentical, no binding has to be specified forsuch a vertex.
Additionally, the modified ad-jacency matrix is the result of the compres-sion by the pattern, i.e., vbits, rbits, and ebitsare determined from the modified adjacencymatrix/graph if the pattern was successfullymatched to the SN.Let us consider our example pattern D4(Table 1).
The following bindings are deter-mined: a1: c3 (a1); a: c8; c: c6; b: c5; a2: c7(a2)The bindings for a1 and a2 need not to beremembered since all hyponym vertices are re-named to a1 and the hypernym vertices toa2 in order to learn generic patterns for arbi-trary hypernyms/hyponyms.
The cells of theadjacency matrix which are associated to thearcs: scar(c8 , c5 ), sub(c5 , a1 ), obj(c8 , c6 ),subs(c8 , c9 ), temp(c8, c10) are set to zero(marked by a cross in Fig.
3) since these arcsare also represented in the pattern using thebindings stated above.
The rows and columnsof c3, c5, c7, and c9 of the modified graphadjacency matrix only contain zeros.
Thus,these rows can be removed from the adja-cency matrix and the associated concepts canbe eliminated from the vertex set of the SN.The findings of the optimal patterns is donecompositionally employing a beam search ap-proach.
First this approach starts with pat-terns containing only a single arc.
Thesepatterns are then extended by adding onearc after another preferring patterns lead-ing to small description lengths of the com-pressed SNs.
Note that only pattern premisesare allowed which are fully connected, e.g.,sub(a, c)?
sub(e, f) is no acceptable premise.Two lists are used during the search,local besti for guiding the search process andglobal best for storing the best global resultsfound so far:?
local besti : The k best patterns oflength i?
global best : The k best patterns of anylengthThe list local besti is determined by extend-ing all elements from local besti?1 by onearc and only keeping the k arcs leading tothe smallest description length.
The listglobal best is updated after each change ofthe list local besti.
This process is iteratedas long as the total description length can befurther reduced, i.e., DL(local besti+1[0]) <DL(local besti[0]), where DL : Pattern ?
Rdenotes the description length of a pattern and[0] accesses the first element of a list.The list global best contains as the result ofthis approach the k patterns with the smallestoverall compressed description length7.
Notehowever that it is often not recommendedto use all elements of global best since thislist contains oftentimes patterns where thepremise part is a subgraph (can be inferredby) another premise pattern part contained inthis list and their combination would actuallynot reduce the description length.
Thus, inaddition to the original approach of Cook andHolder, a dependency resolution is done.The following iterative approach is pro-posed to cancel out such dependent patterns:1.
Start with the first entry of the global list:depend best := {global best [0]}7compressed description length: short for descrip-tion length of the SNs compressed by the pattern43ID Definition Matching ExpressionD1sub0(a1 , a2 )?sub(g, a2 ) ?
attch(g, f)?subr(e, sub.0 ) ?
temp(e, present .0 )?arg2(e, f) ?
arg1(e, d)?sub(d, a1 )An applehypo is a typeof fruithyper.D2sub0(a1 , a2 )?sub(f, a2 ) ?
equ(g, f)?subr(e, equ.0 ) ?
temp(e, present .0 )?arg2(e, f) ?
arg1(e, d)?sub(d, a1 )Psycho-linguisticshypo is a sciencehyperof the human ability to speak.D3sub0(a1 , a2 )?pred(g, a2 ) ?
attch(g, f)?subr(e, pred .0 ) ?
arg2(e, f)?temp(e, present .0 ) ?
arg1(e, d)?pred(d, a1 )Hepialidaehypo are a kind of insectshyper .literal translation from: DieWurzelbohrer sind eine Familieder Schmetterlinge.D4sub0(a1 , a2 )?sub(f, a2 ) ?
subs(e, denote.1 .1 )?temp(e, present .0 ) ?
obj(e, f)?scar(e, d) ?
sub(d, a1 )A skyscraperhypodenotes a very tall buildinghyper .D5sub0(a1 , a2 )?prop(f, other .1 .1 ) ?
pred(f, a2 )?foll*itms(d, f) ?
pred(d, a1 )duckshypo and otheranimalshyperD6 sub0(a1 , a2 )?sub(d, a2 ) ?
sub(d, a1 ) the instrumenthyper cellohypoD7sub0(a1, a2)?
sub(f, a2 )?temp(e, present .0 ) ?
subr(e, sub.0 )?sub(d, a1 ) ?
arg2(e, f)?arg1(e, d)The Morton numberhypo is adimensionless indicatorhyper .Table 1: A selection of automatically learned patterns.2.
Set index :=13.
Calculate the combined (compressed)description length of depend best and{global best [index ]}4.
If the combined description lengthis reduced add global best [index ] todepend best , otherwise leave depend bestunchanged5.
If counter ?
length(global best) then re-turn depend best6.
index := index + 17.
Go back to step 36 System ArchitectureIn this section, we give an overview over ourhypernymy extraction system.
The followingprocedure is employed to identify hypernymyrelations in Wikipedia (see Fig.
4):1.
At first, all sentences of Wikipedia areanalyzed by the deep analyzer WOCADI(Hartrumpf, 2002).
As a result of theparsing process, a token list, a syntacticdependency tree, and an SN is created.Tokens SNShallow PatternApplicationShallow patternsDeep patternsHaGenLex TextDeep PatternApplicationValidation(Filter)Validation(Score)AnalysisWOCADIKBFigure 4: Activity diagram of the hypernymextraction process.442.
Shallow patterns based on regular expres-sions are applied to the token lists, anddeep patterns (learned and hand-crafted)are applied to the SNs to generate pro-posals for hypernymy relations.3.
A validation tool using ontological sortsand semantic features checks whether theproposals are technically admissible at allto reduce the amount of data to be storedin the knowledge base KB.4.
If the validation is successful, the hyper-nymy hypothesis is integrated into KB.Steps 2?4 are repeated until all sentencesare processed.5.
Each hypernymy hypothesis in KB is as-signed a confidence score estimating itsreliability.7 Validation FeaturesThe knowledge acquisition carried out is fol-lowed by a two-step validation.
In the firststep, we check the ontological sorts and se-mantic features of relational arguments forsubsumption.
For instance, a discrete con-cept (ontological sort: d) denoting a humanbeing (semantic feature: human +) can onlybe hypernym of an other object, if this objectis both discrete and a human being as well.Only relational candidates for which semanticfeatures and ontological sorts can be shownto be compatible are stored in the knowledgebase.In a second step, each relational candidatein the knowledge base is assigned a qualityscore.
This is done by means of a supportvector machine (SVM) on several features.The SVM determines the classification (hy-pernymy or non-hypernymy) and a probabil-ity value for each hypernymy hypothesis.
Ifthe classification is ?hypernymy?, the score isdefined by this probability value, otherwise asone minus this value.Correctness Rate: The feature CorrectnessRate takes into account that the assumed hy-pernym alone is already a strong indicationfor the correctness or incorrectness of the in-vestigated relation.
The same holds for theassumed hyponym as well.
For instance, re-lation hypotheses with hypernym liquid andtown are usually correct.
However, this isnot the case for abstract concepts.
Moreover,movie names are often extracted incompletelysince they can consist of several tokens.
Thus,this indicator determines how often a conceptpair is classified correctly if a certain conceptshows up in the first (hyponym) or second (hy-pernym) position.Frequency : The feature frequency regardsthe quotient of the occurrences of the hy-ponym in other extracted relations in hy-ponym position and the hypernym in hyper-nym position.This feature is based on two assumption.First, we assume that general terms normallyoccur more frequently in large text corporathan very specific ones (Joho and Sanderson,2007).
Second, we assume that usually a hy-pernym has more hyponyms than vice-versa.Context : Generally, the hyponym can ap-pear in the same textual context as its hyper-nym.
The textual context can be described asa set of other concepts (or words for shallowapproaches) which occur in the neighborhoodof the investigated hyponym/hypernym can-didate pair investigated on a large text cor-pus.
Instead of the textual context we re-gard the semantic context.
More specifically,the distributions of all concepts are regardedwhich are connected with the assumed hyper-nym/hyponym concept by the MultiNet-prop(property) relation.
The formula to estimatethe similarity was basically taken from (Cimi-ano and others, 2005).ID Precision First Sent.
# MatchesD1 0.275 0.323 5 484D2 0.183 0.230 35 497D3 0.514 0.780 937D4 0.536 0.706 1 581D5 0.592 - 3 461D6 0.171 0.167 37 655Table 2: Precision of hypernymy hypothesesextracted by patterns without usage of the val-idation component (D7 not yet evaluated).See (vor der Bru?ck, 2010) for a more de-45Score ?0.95 ?0.90 ?0.85 ?0.80 ?0.75 ?0.70 ?0.65 ?0.60 ?0.55Precision 1.0000 0.8723 0.8649 0.8248 0.8203 0.7049 0.6781 0.5741 0.5703Table 3: Precision of the extracted hypernymy relations for different confidence score intervals.tailed description of the validation features.8 EvaluationWe applied the pattern learning process ona collection of 600 SN, derived by WOCADIfrom Wikipedia, which contain hyponymicallyrelated concepts.
Table 1 contains some of theextracted patterns including a typical expres-sion to which this pattern could be matched.The predicate follf (a, b) used in this tablespecifies that argument a precedes argumentb in the argument list of function f .
PatternsD1-D4 and D7 contain concept definitionswhere the defined concept is, in many cases,the hyponym of the defining concept.
In pat-tern D1 and D7 the defining concept is directlyidentified by the parser as hypernym of the de-fined concept (subr(e, sub.0 )).
In pattern D2the defining concept is recognized as equiva-lent to the defined concept (subr(e, equ.0 )).However, in most of the cases the definingconcept consists of a meaning molecule, i.e.,a complex concept where some inner conceptis modified by an additional expression (oftena property or an additional subclause).
If thisexpression is dropped which is done by thepattern D2 the remaining concept becomes ahypernym of the defined concept.
Pattern D5is a well-known Hearst pattern.
Pattern D6is used to match to appositions.
However, forthat the representation of appositions in theSN, as provided by the parser, could be im-proved since the order of the two concepts ina sentence is not clear by regarding only theSN, i.e., from the expression the instrumentcello both sub0(instrument .1 .1 , cello.1 .1 )and sub0(cello.1 .1 , instrument .1 .1 ) could beextracted.
The incorrect relation hypoth-esis has to be filtered out (hopefully)by the validation component.
A bet-ter representation would be by employ-ing the tupl*(c1, .
.
.
, cn) predicate whichcombines several concepts with regard totheir order.
So the example expressionshould better be represented by sub(d, e) ?tupl*(e, instrument .1 .1 , cello.1 .1 ).Precision values for the hyponymy relationhypotheses extracted by the learned patterns,which are applied on a subset of the GermanWikipedia, are given in Table 2.
The firstprecision value specifies the overall precision,the second the precision if only hypernymy hy-potheses are considered which were extractedfrom first sentences of Wikipedia articles.
Theprecision is usually increased considerably ifonly such sentences are regarded.
Note thatthis precision value was not given for patternD5 which usually cannot be matched to suchsentences.
The last number specifies the to-tal amount of sentences a pattern could bematched to.Furthermore, besides the pattern extractionprocess, the entire hypernymy acquisition sys-tem was validated, too.
In total 391 153 dif-ferent hypernymy hypotheses were extractedemploying 22 deep and 19 shallow patterns.149 900 of the relations were only determinedby the deep but not by the shallow patternswhich shows that the recall can be consider-ably increased by using deep patterns in addi-tion.
But also precision profits from the usageof deep patterns.
The average precision of allrelations extracted by both shallow and deeppatterns is 0.514 that is considerably higherthan the average precision for the relationsonly extracted by shallow patterns (0.243).The correctness of an extracted relation hy-pothesis is given for several confidence scoreintervals in Table 3.
There are 89 944 con-cept pairs with a score above 0.7, 3 558 ofthem were annotated with the informationof whether the hypernymy relation actuallyholds.Note that recall is very difficult to specifysince for doing this the number of hypernymyrelations which are theoretically extractable46from a text corpus has to be known wheredifferent annotators can have very dissentingopinions about this number.
Thus, we justgave the number of relation hypotheses ex-ceeding a certain score.
However the precisionobtained by our system is quite competitiveto other approaches for hypernymy extrac-tion like the one of Erik Tjong and Kim Sangwhich extracts hypernyms in Dutch (Tjongand Sang, 2007) (Precision: 0.48).9 Conclusion and OutlookWe showed a method to automatically derivepatterns for hypernymy extraction in form ofSNs by following the MDL principle.
A listof such patterns together with precision andnumber of matches were given to show theusefulness of the applied approach.
The pat-terns were applied on the Wikipedia corpusto extract hypernymy hypotheses.
These hy-potheses were validated using several features.Depending on the score, an arbitrary high pre-cision can be reached.
Currently, we deter-mine confidence values for the precision valuesof the pattern example.
Further future workincludes the application of our learning algo-rithm to larger text corpora in order to findadditional patterns.
Also an investigation ofhow this method can be used for other typesof semantic relations is of interest.AcknowledgementsWe want to thank all of our departmentwhich contributed to this work, especiallySven Hartrumpf and Alexander Pilz-Lansleyfor proofreading this paper.
This work wasin part funded by the DFG project Semantis-che Duplikatserkennung mithilfe von TextualEntailment (HE 2847/11-1).ReferencesCimiano, P. et al 2005.
Learning taxonomic re-lations from heterogeneous sources of evidence.In Buitelaar, P. et al, editors, Ontology Learn-ing from Text: Methods, evaluation and applica-tions, pages 59?73.
IOS Press, Amsterdam, TheNetherlands.Cook, D. and L. Holder.
1994.
Substructure dis-covery using minimum description length andbackground knowledge.
Journal of Artificial In-telligence Research, 1:231?255.Fellbaum, C., editor.
1998.
WordNet An Elec-tronic Lexical Database.
MIT Press, Cam-bridge, Massachusetts.Hamp, B. and H. Feldweg.
1997.
Germanet - alexical-semantic net for german.
In Proc.
of theACL workshop of Automatic Information Ex-traction and Building of Lexical Semantic Re-sources for NLP Applications, Madrid, Spain.Hartrumpf, S. et al 2003.
The semantically basedcomputer lexicon HaGenLex ?
Structure andtechnological environment.
Traitement automa-tique des langues, 44(2):81?105.Hartrumpf, S. 2002.
Hybrid Disambiguation inNatural Language Analysis.
Ph.D. thesis, Fern-Universita?t in Hagen, Fachbereich Informatik,Hagen, Germany.Hearst, M. 1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proc.
ofCOLING, Nantes, France.Helbig, H. 2006.
Knowledge Representation andthe Semantics of Natural Language.
Springer,Berlin, Germany.Joho, H. and M. Sanderson.
2007.
Document fre-quency and term specificity.
In Proc.
of RIAO,Pittsburgh, Pennsylvania.Morin, E. and C. Jaquemin.
2004.
Automaticacquisition and expansion of hypernym links.Computers and the Humanities, 38(4):363?396.Rissanen, J.
1989.
Stochastic Complexity inStatistical Inquiry.
World Scientific PublishingCompany, Hackensack, New Jersey.Snow, R. et al 2005.
Learning syntactic patternsfor automatic hypernym discovery.
In Advancesin Neural Information Processing Systems 17,pages 1297?1304.
MIT Press, Cambridge, Mas-sachusetts.Tjong, E. and K. Sang.
2007.
Extracting hy-pernym pairs from the web.
In Proceedingsof the 45 Annual Meeting of the ACL on In-teractive Poster and Demonstration Sessions,Prague, Czech Republic.vor der Bru?ck, T. 2010.
Hypernymy extractionusing a semantic network representation.
Inter-national Journal of Computational Linguisticsand Applications (IJCLA), 1(1).47
