Proceedings of the 6th Workshop on Statistical Machine Translation, pages 470?477,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsSpell Checking Techniques for Replacement of Unknown Words and DataCleaning for Haitian Creole SMS TranslationSara StymneDepartment of Computer and Information ScienceLinko?ping University, Swedensara.stymne@liu.seAbstractWe report results on translation of SMS mes-sages from Haitian Creole to English.
Weshow improvements by applying spell check-ing techniques to unknown words and creatinga lattice with the best known spelling equiva-lents.
We also used a small cleaned corpus totrain a cleaning model that we applied to thenoisy corpora.1 IntroductionIn this paper we report results on the WMT 2011featured shared task on translation of SMS messagesfrom Haitian Creole into English, which featured anumber of challenges.
The in-domain data avail-able is small and noisy, with a lot of non-standardlanguage.
Furthermore, Haitian Creole is a low re-source language, for which there are few languagetechnology tools and corpora available.Our main focus has been to make the best pos-sible use of the available training data through dif-ferent ways of cleaning the data, and by replacingunknown words in the test data by plausible spellingequivalents.
We have also investigated effects of dif-ferent ways to combine the available data in transla-tion and language models.2 Baseline systemWe performed all our experiments using a stan-dard phrase-based statistical machine translation(PBSMT) system, trained using the Moses toolkit(Koehn et al, 2007), with SRILM (Stolcke, 2002)and KenLM (Heafield, 2011) for language model-ing, and GIZA++ (Och and Ney, 2003) for wordalignment.
We also used a lexicalized reorderingmodel (Koehn et al, 2005).
We optimized eachsystem separately using minimum error rate train-ing (Och, 2003).
The development and devtest datawere available in two versions, as raw, noisy data,and in a clean version, where the raw data had beencleaned by human post-editors.The different subcorpora had different tokeniza-tions and casing conventions.
We normalized punc-tuation by applying a tokenizer that separated mostpunctuation marks into separate tokens, excludingapostrophes that were suspected to belong to con-tracted words or Haitian short forms, periods for ab-breviations, and periods in URLs.
There were oftenmany consecutive punctuation marks; these were re-placed by only the first of the punctuation marks.In the English translations of the SMS data therewere often translator?s notes at the end of the transla-tions.
These were removed when introduced by twostandard formulations: Additional Notes or transla-tor?s note/interpretation.
In addition the translationmarker The SMS [ .
.
. ]
were removed.Case information was inconsistent, especially forSMS data, and for this reason we lower-cased allHaitian source data.
On the English target sidewe wanted to use true-cased data, since we wantedcase distinctions in the translation output.
We basedthe true-casing on Koehn and Haddow (2009), whochanged the case of the first word in each sentence,to the most common case variant of that word in thecorpus when it is not sentence initial.
In the noisySMS data, though, there were many sentences withall capital letters that would influence this truecasingmethod negatively.
To address this, we modified thealgorithm to exclude sentences with more than 40%capital letters when calculating corpus statistics, andto lowercase all unknown capitalized words.470Data Sentences Words TM LM Reo TCIn-domain SMS data 17,192 35k SMS SMS yes yesMedical domain 1,619 10k other other ?
?Newswire domain 13,517 30k other other ?
yesGlossary 35,728 85k other other ?
?Wikipedia parallel sentence 8,476 90k other other ?
yesWikipedia named entities 10,499 25k other other ?
?Haitisurf dictionary 1,687 3k other other ?
yesKrengle sentences 658 3k other other ?
yesThe Bible 30,715 850k bible bible ?
yesTable 1: Corpora used for training translation models (TM), language models (LM), lexicalized reordering model(Reo), and true-casing model (TC).
All corpora are bilingual English?Haitian Creole.All translation results are reported for the devtestcorpus, on truecased data.
We report results onthree metrics, Bleu (Papineni et al, 2002), NIST(Doddington, 2002), and Meteor optimized on flu-ency/adequacy (Lavie and Agarwal, 2007).3 Corpus UsageThe corpora available for the task was a smallbilingual in-domain corpus of SMT data, a limitedamount of bilingual out-of-domain corpora, suchas dictionaries and the Bible.
This is different tothe common situation of domain adaptation, as inthe standard WMT shared tasks, where there is asmall bilingual in-domain corpus, a larger in-domainmonolingual corpus, and possibly several out-of-domain corpora that can be both monolingual andbilingual.
In such a scenario it is often useful touse all available training data for both translationand language models, possibly in separate models(Koehn and Schroeder, 2007).Table 1 summarizes how we used the availablecorpora, in our different models.
For translationand language models we separated the bilingual datainto three parts, the SMS data, the Bible, and every-thing else.
For our lexicalized reordering model weonly used SMS data, since we believe word orderthere is likely to differ from the other corpora.
Forthe English true-casing model we concatenated theEnglish side of all bilingual corpora that were notlower-cased.Table 2 shows the results of the different modelcombinations on the clean devtest data.
When weused only the SMS data in the translation model,the scores changed only slightly regardless of whichcombinations of language models we used.
Usingtwo translation models for the SMS data and theother bilingual data overall gave better results thanwhen only using SMS data for the translation model.With double translation models it was best only touse the SMS data in the language model.
Includingthe Bible data had a minor impact.
Based on theseexperiments we will use all available training datain two translation models, one for SMS and one foreverything else, but only use SMS data in one lan-guage model, which corresponds to the line markedin bold in Table 2, and which we will call the dualsystem.We did not perform model combination experi-ments for the raw input data, since we believed thepattern would be similar as for the clean data.
Theresults for the raw devtest as input are considerablylower than for the clean data.
Using the best modelcombination, we got a Bleu score of only 26.25,which can be compared to 29.90 using the cleandata.4 Data Cleaning ModelWhile the training data is noisy, we had access tocleaned versions of dev, devtest and test data.
Wedecided to use the dev data to build a model forcleaning the noisy SMS data.
We did this by train-ing a standard PBSMT model from raw to clean devdata.
When inspecting this translation model wefound that it very often changed the place holdersfor names and phone numbers, and thus we filteredout all entries in the phrase table that did not havematching place holders.
We then used this model toperform monotone decoding of the raw SMS data,thus creating a cleaner version of it.This approach is similar to that of Aw et al471TMs LMs Bleu NIST MeteorSMS SMS 29.04 5.578 52.32SMS SMS, other 28.76 5.543 51.96SMS SMS, other+bible 29.18 5.696 51.77SMS, other SMS 29.78 5.808 52.86SMS, other+bible SMS 29.90 5.764 52.88SMS, other+bible SMS, other 29.59 5.742 52.28SMS, other+bible SMS, other+bible 28.75 5.587 52.52Table 2: Translation results, with different combinations of translation and language models.
Model names separatedby a comma stands for separate models, and names separated with a plus for one model built from concatenatedcorpora.Model Testset Bleu NIST MeteorDual clean 29.90 5.764 52.88Dual+CM clean 29.78 5.740 52.95Dual raw 26.25 5.231 50.79Dual raw+CM 26.26 5.348 51.30Dual+CM raw 25.64 5.120 50.01Dual+CM raw+CM 26.24 5.362 51.64Table 3: Translation results, with and without an addi-tional cleaning model (+CM) on the clean and raw devtestdata(2006), who trained a model for translation fromEnglish SMS language to standard written English,with very good results both on this task itself, andon a task of translating English SMS messages intoChinese.
For training they used up to 5000 sen-tences, but the results stabilized already when us-ing 3000 training sentences.
Our task is different,though, since we do not aim at standard writtenHaitian, but into cleaned up SMS language, and ourtraining corpus is a lot smaller, only 900 sentences.Table 3 shows the results of using the cleaningmodel on training data and raw translation input.
Forthe clean data using the cleaning model on the train-ing data had very little effect on any of the metricsused.
For the raw data translation results are im-proved as measured by NIST and Meteor when weuse the filter on the devtest data, compared to usingthe raw devtest data.
Using the filter on the trainingdata gives worse results for non-filtered devtest data,but the overall best results are had by filtering bothtraining and devtest data for raw translation input.Based on these experiments we used the cleaningmodel both on test and training data for raw input,but not at all for clean input, marked in bold in Table3.5 Spell Checking-based Replacement ofUnknown WordsThe SMS data is noisy, and there are often manyspelling variations of the same word.
One exam-ple is the word airport, which occur in the trainingcorpus in at least six spelling variants: the correctayeropo`, and aeoport, ayeopo`, aeroport, aeyopo`t,and aewopo, and in the devtest in a seventh variantaye?oport.
The non-standardized spelling means thatmany unknown words (out-of-vocabulary words,OOVs) have a known spelling variant in the train-ing corpus.
We thus decided to treat OOVs using amethod inspired by spell-checking techniques, andapplied an approximate string matching techniqueto OOVs in the translation input in order to changethem into known spelling variants.OOV replacement has been proposed by severalresearchers, replacing OOVs e.g.
by morphologicalvariants (Arora et al, 2008) or synonyms (Mirkin etal., 2009).
Habash (2008) used several techniquesfor expanding OOVs in order to extend the phrase-table.
Yang and Kirchhoff (2006) trained a morpho-logically based back-off model for OOVs.
Bertoldiet al (2010) created confusion networks as input oftranslation input with artificially created misspelledwords, not specifically targetting OOVs, however.The work most similar to ours is DeNeefe et al(2008), who also created lattices with spelling alter-natives for OOVs, which did not improve translationresults, however.
Contrary to us, they only consid-ered one edit per word, and did not weigh edits orlattice arcs.Many standard spell checkers are based on thenoisy channel model, which use an error (channel)model and a source model, which is normally mod-472eled by a language model.
The error model normallyuse some type of approximate string matching, suchas Levenshtein distance (Levenshtein, 1966), whichmeasures the distance between two strings as thenumber of insertions, deletions, and substitutions ofcharacters.
It is often normalized based on the lengthof the strings (Yujian and Bo, 2007), and the dis-tance calculation has also been improved by associ-ating different costs to individual error operations.Church and Gale (1991) used a large training corpusto assign probabilities to each unique error opera-tion, and also conditioned operations on one consec-utive character.
Brill and Moore (2000) introduced amodel that worked on character sequences, not onlyon character level, and was conditioned on wherein the word the sequences occurred.
They trainedweights on a corpus of misspelled words with cor-rections.Treating OOVs in the SMS corpus as a spellchecking problem differs from a standard spellchecking scenario in that the goal is not necessarilyto change an incorrectly spelled word into a correctword, but rather to change a word that is not in ourcorpus into a spelling variant that we have seen in thecorpus, but which might not necessarily be correctlyspelled.
It is also the case that many of the OOVs arenot wrong, but just happen to be unseen; for instancethere are many place names.
Thus we must makesure that our algorithm for finding spelling equiva-lents is bi-directional, so that it cannot only changeincorrect spellings into correct spellings, but also gothe other way, which could be needed in some cases.We also need to try not to suggest alternatives forwords that does not have any plausible alternativesin the corpus, such as unknown place names.5.1 Approximate String Matching AlgorithmThe approximate string matching algorithm we sug-gest is essentially that of Brill and Moore (2000),a modified weighted Levenshtein distance, wherewe allow error operations on character sequences aswell as on single characters.
We based our weightestimations on the automatically created list of lex-ical variants that was built as a step in building thecleaning model, described in section 4.
This list isvery noisy, but does also contain some true spellingequivalents.
We implemented two versions of thealgorithm, first a simple version which used manu-ally identified error operations, then a more complexvariant where error operations and weights werefound automatically.Manually Assigned WeightsWe went through the lexicon list manually to iden-tify edits that could correct the misspellings that oc-curred in the list.
We identified substitutions lim-ited to three characters in length, and at the begin-ning and end of words we also identified letter in-sertions and deletions.
The inspection showed thatit was very common for letters to be replaced by thesame letter but with a diacritic, or with a differentdiacritic, for instance to vary between [e, e?, e`].
An-other common operation was between a single char-acter and two consecutive occurrences of the samecharacter.
Table 4 shows the 46 identified opera-tions.
To account for the fact that we do not wantour error model to have a directionality from wrongto correct, we allow operations in both directions.Since the operations were found manually we didnot have a reliable way to estimate weights, and useduniform weights for all operations.
The operationsin Table 4 have the weights given in the table, sub-stitution of a letter with a diacritic variant 0, singleto double letters 0.1, insertions and deletions 1 andsubstitutions other than those in the table, 1.6.Automatically Assigned WeightsTo automatically train weights from the very noisylist of lexical variants, we filtered it by applyingthe edit distance with the manual weights describedabove to phrase pair that did not differ in length bymore than three characters.
We used a cut-off thresh-old of 2.8 for words where both versions had at leastsix characters, and 1 for shorter words.
This gaveus a list of 587 plausible spelling variants, from theoriginal list with 1635 word pairs.To find good character substitutions and assignweights to them, we used standard PBSMT tech-niques as implemented in Moses, but on characterlevel, with the filtered list of word pairs as train-ing data.
We inserted spaces between each characterof the words, and also added beginning and end ofword markers, e.g., the word proble?m was tokenizedas ?B p r o b l e?
m E?.
Thus we could train a PB-SMT system that aligned characters using GIZA++,and extracted and scored phrases, which in this case473Manual AutomaticType Instances Weight Examples+weights Countmid 1-1 e-i, a-o, i-y, a-e, i-u, s-c, r-w, c-k, j-g, s-z,n-m.2 n-m .90, e-c .74, j-g .62 12mid 1-2 z-sz, i-iy, m-nm, n-nm, y-il, i-ye, s-rs, t-th, o-an, x-ks, x-kz, e-a,.2 x-ks .35, i-ue .83, w-rr .74 107mid 1-3 ?
?
e-ait .75 e-eur .66 29mid 2-2 wa-oi, we-oi, en-un, xs-ks .2 we-oi .67, wo-ro .20, ie-ye .54 103mid 2-3 wa-oir, ye-ier, an-ent, eo-eyo .2 iv-eve .79, ey-eyi .18 160mid 3-3 syo-tio, syo-tyo .2 ant-ent .81, dyo,dia .67 116beg 0-1 -h, -l .2 -n .95, -m .90, -h .50 9beg 0-2 ?
?
-te .95, -pa .82 6beg 1-1 h-l .2 a-e .89, w-r .67 i-u .33 5beg 1-2,3 ?
?
e-ai .68, a-za .74 k-pak .48 30beg 2,3-2,3 ?
?
wo-ro 0, ex-ekz .65, ens-ins .17 58end 0-1 -e, -t, -n, -m, -r, -y .2 -r .57 -e .85, -v .75 12end 0-2 -te, -de, -ue, -le 1 -de .93, -le .75 7end 1-1 ?
?
e-o .74, n-m .86 5end 1-2,3 ?
?
i-li .81, c-se .62 n-nne .66 48end 2,3-2,3 ?
?
sm-me .67, ns-nce .38, wen-oin .36 70Table 4: Error operations at the middle, beginning and end of words.
For manually defined operations all instances areshown, with their uniform score.
For automaticcally identified operations examples are shown with their score, andthe total count of each operation type.amounts to creating a phrase-table with character se-quences.
The phrase probabilities are given in bothtranslation directions, P (S|T ) and P (T |S).
Sincewe do not want our scores to have any direction, weused the arithmetic mean of these two probabilitiesto calculate the score for the pair, which is calcu-lated as 1 ?
((P (S|T ) + P (T |S))/2), to also con-vert the probabilities to costs.
To compensate forerrors made in the extraction process, we filteredout phrase pairs where both probabilities were lowerthan 0.1.To get fair scores for character sequences of dif-ferent lengths we applied the phrase table construc-tion four times, while increasing the limit of themaximum phrase length from one to four.
From thefirst phrase table, with maximum length 1, we ex-tracted 1-1 substitutions, from the second table 1-2and 2-2 substitutions, and so on.
We used the begin-ning and end of word markers both to extract sub-stitutions that were only used at the beginning orend of sentences, and to extract deletions and inser-tions used at the beginning and end of words.
Again,we only allowed substitutions up to three charactersin length.
The fourth phrase-table, with phrases oflength four, were only used to allow us to extractsubstitutions of length three at the beginning and endof words, since the markers count as tokens.
Table 4shows the types of transformations extracted, someexamples of each with their score, and the countof each transformation.
A total of 777 operationswere found, compared to only 46 manual operations.There were few substitutions with diacritic variants,so again we allowed them with a zero cost.
The costsfor deletions, additions, and substitutions not givenany weights were the same as before, 1, 1, and 1.6.For the edit distance with the automatic weights, weused scores that were normalized by the length ofthe shortest string.Application to OOVsWe applied the edit distance operation on all OOVslonger than 3 characters, and calculated the distanceto all words in the training corpora that did not dif-fer in length with more than two characters.
We usedthe standard dynamic programming implementationof our edit distance, but extended to check the scoresnot only in directly neighbouring cells, but in cellsup to a distance of 3 away, to account for the maxi-mum length of the character sequence substitutions.It would have been possible to use a fast trie imple-474Clean devtest Raw devtestSystem Bleu NIST Meteor Bleu NIST MeteorNo OOV treatment 29.90 5.764 52.88 26.24 5.362 51.64Manual 1-best 29.76 5.721 52.91 26.60 5.417 52.17Automatic 1-best 29.90 5.746 52.83 26.26 5.351 51.60Manual lattice 30.53 5.957 54.06 27.12 5.574 53.27Automatic lattice 30.94 5.982 54.62 27.27 5.554 52.99Automatic lattice + LM 30.33 5.912 54.07 27.79 5.555 52.98Table 5: Translation results, using the approximate string matching algorithm for OOVs.
The submitted system ismarked with bold.mentation (Brill and Moore, 2000), however.We performed both 1-best substitution of OOVs,and lattice decoding where we kept the three bestalternatives for each word.
In both cases we only re-placed OOVs if the edit distance scores were below athreshold of 1.2 for the manual weights, which werenot normalized, and for the normalized automaticweights below 0.25, or below 0.33 for word pairswhere both words had at least 6 characters.
Thesethresholds were set by inspecting the results, but re-sulted in a different number of substitutions:?
clean (total 691)?
manual: 251?
automatic: 222?
raw (total 932)?
manual: 601?
automatic: 437The lattice arcs were weighted with the edit dis-tance score, normalized to fall between 0-1.
We alsotried to include a source language model score inthe weights in the lattice, to account for the sourcemodel that has been shown to be useful for spellingcorrection, but which has not been found useful forOOV replacement.
We trained a 3-gram languagemodel on the Haitian SMS text, and applied thismodel for a five-word context around the replacedOOV.
We used a single lattice weight where half thescore came from the edit distance, and the other halfrepresented the language model component.
A bet-ter approach though, would probably have been touse two weights.5.2 ResultsTable 5 shows the results of the OOV treatment.When using 1-best substitutions there are small dif-ferences compared to the baseline on both test sets,except for the system with manual weights on rawdata, which was improved on all metrics.
All threeways of applying the lattice substitutions led to largeimprovements on all metrics on both test sets.
Onthe clean test set it was better to use automatic thanmanual weights when not using the language modelscore, which made the results worse.
On the rawtest set the highest Meteor and NIST scores werehad by using manual weights, whereas the highestBleu score was had by using automatic weights withthe language model.
The system submitted to theworkshop is the system with a lattice with manualweights, marked in bold in Table 5, since the auto-matic weights were not ready in time for the submis-sion.6 ConclusionIn this article we presented methods for translat-ing noisy Haitian Creole SMS messages, which webelieve are generally suitable for small and noisycorpora and under-resourced languages.
We usedan automatically trained cleaning model, trained ononly 900 manually cleaned sentences, that led to im-provements for noisy translation input.
Our maincontribution was to apply methods inspired by spellchecking to suggest known spelling variants of un-known words, which we presented as a lattice tothe decoder.
Several versions of this method gaveconsistent improvements over the baseline system.There are still many questions left about which con-figuration that is best for weighting and pruning thelattice, however, which we intend to investigate infuture work.
In this work we only considered OOVsin the translation input, but it would also be interest-ing to address misspelled words in the training cor-pus.475ReferencesKarunesh Arora, Michael Paul, and Eiichiro Sumita.2008.
Translation of unknown words in phrase-basedstatistical machine translation for languages of richmorphology.
In Proceedings of the First Interna-tional Workshop on Spoken Languages Technologiesfor Under-resourced languages (SLTU-2008), pages70?75, Hanoi, Vietnam.AiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.
Aphrase-based statistical model for SMS text normaliza-tion.
In Proceedings of the 21st International Confer-ence on Computational Linguistics and 44th AnnualMeeting of the ACL, poster session, pages 33?40, Syd-ney, Australia.Nicola Bertoldi, Mauro Cettolo, and Marcello Federico.2010.
Statistical machine translation of texts withmisspelled words.
In Proceedings of Human Lan-guage Technologies: The 2010 Annual Conference ofthe NAACL, pages 412?419, Los Angeles, California,USA.Eric Brill and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling correction.
InProceedings of the 38th Annual Meeting of the ACL,pages 286?293, Hong Kong.Kenneth W. Church and William A. Gale.
1991.
Prob-ability scoring for spelling correction.
Statistics andComputing, 1:93?103.Steve DeNeefe, Ulf Hermjakob, and Kevin Knight.
2008.Overcoming vocabulary sparsity in MT using lattices.In Proceedings of the 8th Conference of the Associa-tion for Machine Translation in the Americas, pages89?96, Waikiki, Hawaii, USA.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurencestatistics.
In Proceedings of the Second InternationalConference on Human Language Technology, pages228?231, San Diego, California, USA.Nizar Habash.
2008.
Four techniques for online handlingof out-of-vocabulary words in Arabic-English statisti-cal machine translation.
In Proceedings of the 46thAnnual Meeting of the ACL: Human Language Tech-nologies, Short papers, pages 57?60, Columbus, Ohio,USA.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, Edin-burgh, UK.Philipp Koehn and Barry Haddow.
2009.
Edinburgh?ssubmission to all tracks of the WMT 2009 shared taskwith reordering and speed improvements to Moses.
InProceedings of the Fourth Workshop on Statistical Ma-chine Translation, pages 160?164, Athens, Greece.Philipp Koehn and Josh Schroeder.
2007.
Experiments indomain adaptation for statistical machine translation.In Proceedings of the Second Workshop on StatisticalMachine Translation, pages 224?227, Prague, CzechRepublic, June.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descriptionfor the 2005 IWSLT speech translation evaluation.
InProceedings of the International Workshop on Spo-ken Language Translation, Pittsburgh, Pennsylvania,USA.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL, demon-stration session, pages 177?180, Prague, Czech Re-public.Alon Lavie and Abhaya Agarwal.
2007.
METEOR: Anautomatic metric for MT evaluation with high levels ofcorrelation with human judgments.
In Proceedings ofthe Second Workshop on Statistical Machine Transla-tion, pages 228?231, Prague, Czech Republic.Vladimir Iosifovich Levenshtein.
1966.
Binary codes ca-pable of correcting deletions, insertions and reversals.Soviet Physics Doklady, 10(8):707?710.Shachar Mirkin, Lucia Specia, Nicola Cancedda, IdoDagan, Marc Dymetman, and Idan Szpektor.
2009.Source-language entailment modeling for translatingunknown terms.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 791?799, Sun-tec, Singapore.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the42nd Annual Meeting of the ACL, pages 160?167, Sap-poro, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting of the ACL, pages 311?318,Philadelphia, Pennsylvania, USA.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proceedings of the Seventh Inter-national Conference on Spoken Language Processing,pages 901?904, Denver, Colorado, USA.476Mei Yang and Katrin Kirchhoff.
2006.
Phrase-basedbackoff models for machine translation of highly in-flected languages.
In Proceedings of the 11th Confer-ence of the EACL, pages 41?48, Trento Italy.Li Yujian and Liu Bo.
2007.
A normalized Levenshteindistance metric.
IEEE Transactions on Pattern Analy-sis and Machine Intelligence, 29(6):1091?1095.477
