Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 797?806,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsPerplexity on Reduced CorporaHayato Kobayashi?Yahoo Japan Corporation9-7-1 Akasaka, Minato-ku, Tokyo 107-6211, Japanhakobaya@yahoo-corp.jpAbstractThis paper studies the idea of remov-ing low-frequency words from a corpus,which is a common practice to reducecomputational costs, from a theoreticalstandpoint.
Based on the assumption that acorpus follows Zipf?s law, we derive trade-off formulae of the perplexity of k-grammodels and topic models with respect tothe size of the reduced vocabulary.
In ad-dition, we show an approximate behaviorof each formula under certain conditions.We verify the correctness of our theory onsynthetic corpora and examine the gap be-tween theory and practice on real corpora.1 IntroductionRemoving low-frequency words from a corpus(often called cutoff) is a common practice to saveon the computational costs involved in learninglanguage models and topic models.
In the caseof language models, we often have to removelow-frequency words because of a lack of com-putational resources, since the feature space of k-grams tends to be so large that we sometimes needcutoffs even in a distributed environment (Brantset al, 2007).
In the case of topic models, the in-tuition is that low-frequency words do not make alarge contribution to the statistics of the models.Actually, when we try to roughly analyze a corpuswith topic models, a reduced corpus is enough forthe purpose (Steyvers and Griffiths, 2007).A natural question arises: How many low-frequency words can we remove while maintain-ing sufficient performance?
Or more generally,by how much can we reduce a corpus/model us-ing a certain strategy and still keep a sufficientlevel of performance?
There have been many stud-?This work was mainly carried out while the author waswith Toshiba Corporation.ies addressing the question as it pertains to differ-ent strategies (Stolcke, 1998; Buchsbaum et al,1998; Goodman and Gao, 2000; Gao and Zhang,2002; Ha et al, 2006; Hirsimaki, 2007; Churchet al, 2007).
Each of these studies experimen-tally discusses trade-off relationships between thesize of the reduced corpus/model and its perfor-mance measured by perplexity, word error rate,and other factors.
To our knowledge, however,there is no theoretical study on the question andno evidence for such a trade-off relationship, es-pecially for topic models.In this paper, we first address the question froma theoretical standpoint.
We focus on the cutoffstrategy for reducing a corpus, since a cutoff issimple but powerful method that is worth study-ing; as reported in (Goodman and Gao, 2000;Gao and Zhang, 2002), a cutoff is competitivewith sophisticated strategies such as entropy prun-ing.
As the basis of our theory, we assume Zipf?slaw (Zipf, 1935), which is an empirical rule repre-senting a long-tail property of words in a corpus.Our approach is essentially the same as those inphysics, in the sense of constructing a theory whilebelieving experimentally observed results.
For ex-ample, we can derive the distance to the landingpoint of a ball thrown up in the air with initialspeed v0and angle ?
as v02sin(2?
)/g by believ-ing in the experimentally observed gravity acceler-ation g. In a similar fashion, we will try to clarifythe trade-off relationship by believing Zipf?s law.The rest of the paper is organized as follows.
InSection 2, we define the notation and briefly ex-plain Zipf?s law and perplexity.
In Section 3, wetheoretically derive the trade-off formulae of thecutoff for unigram models, k-gram models, andtopic models, each of which represents its per-plexity with respect to a reduced vocabulary, un-der the assumption that the corpus follows Zipf?slaw.
In addition, we show an approximate behav-ior of each formula under certain conditions.
In797Section 4, we verify the correctness of our theoryon synthetic corpora and examine the gap betweentheory and practice on several real corpora.
Sec-tion 5 concludes the paper.2 PreliminariesLet us consider a corpus w := w1?
?
?wNof cor-pus size N and vocabulary size W .
We use anabridged notation {w} := {w ?
w} to repre-sent the vocabulary of w. Clearly, N = |w| andW = |{w}| hold.
When w has additional nota-tions, N and W inherit them.
For example, wewill use N ?
as the size of w?
without its definition.2.1 Power law and Zipf?s lawA power law is a mathematical relationship be-tween two quantities x and y, where y is propor-tional to the c-th power of x, i.e., y ?
xc, andc is a real number.
Zipf?s law (Zipf, 1935) is apower law discovered on real corpora, wherein forany word w ?
w in a corpus w, its frequency (orword count) f(w) is inversely proportional to itsfrequency ranking r(w), i.e.,f(w) =Cr(w).Here, f(w) := |{w?
?
w | w?
= w}|, andr(w) := |{w??
w | f(w?)
?
f(w)}|.
Fromthe definition, the constant C is the maximum fre-quency in the corpus.
Taking the natural loga-rithms ln(?)
of both sides of the above equation,we find that its plot becomes linear on a log-loggraph of r(w) and f(w).
In fact, the result basedon a statistical test in (Clauset et al, 2009) reportsthat the frequencies of words in a corpus com-pletely follow a power law, whereas many datasetswith long-tail properties, such as networks, actu-ally do not follow power laws.2.2 PerplexityPerplexity is a widely used evaluation measure ofk-gram models and topic models.
Let p be a pre-dictive distribution over words, which was learnedfrom a training corpus w based on a certain model.Formally, perplexity PP is defined as the geomet-ric mean of the inverse of the per-word likelihoodon the held-out test corpus w?, i.e.,PP :=(?w?w?1p(w))1N?.Intuitively, PP means how many possibilities onehas for estimating the next word in a test cor-pus.
According to the definition, a lower perplex-ity means better generalization performance of p.Another well-known evaluation measure is cross-entropy.
Since cross-entropy is easily calculatedas log2PP, we can apply many of the results ofthis paper to cross-entropy.3 Perplexity on Reduced CorporaNow let us consider what a cutoff is.
In our study,we simply define a corpus that has been reducedby removing low-frequency words from the origi-nal corpus with a certain threshold.
Formally, wesay w?
is a corpus reduced from the original cor-pus w, if w?
is the longest subsequence of w suchthat maxw??w?r(w?)
= W?.
Note that a sub-sequence can include gaps in contrast to a sub-string.
For example, supposing we have a corpusw = abcaba with a vocabulary {w} = {a, b, c},w?1= ababa is a reduced corpus, while w?2=aba and w?3= acaa are not.After learning a distribution p?
from a re-duced corpus w?, we need to infer the distri-bution p learned from the original corpus w.Here, we use constant restoring (defined below),which assumes the frequencies of the reduced low-frequency words are a constant.Definition 1 (Constant Restoring).
Given a pos-itive constant ?, a distribution p?
over a reducedcorpus w?, and a corpus w, we say that p?
isa ?-restored distribution of p?
from w?
to w, if?w?{w}p?
(w) = 1, and for any w ?
w,p?
(w) ?{p?
(w) (w ?
w?)?
(w /?
w?
).Constant restoring is similar to the additivesmoothing defined by p?
(w) ?
p?
(w)+?, which isused to solve the zero-frequency problem of lan-guage models (Chen and Goodman, 1996).
Theonly difference is the addition of a constant ?only to zero-frequency words.
We think con-stant restoring is theoretically natural in our set-ting, since we can derive the above equation byletting each frequency of reduced words be ?N ?and defining a restored frequency function as fol-lows:?f(w) ={f(w) (w ?
w?)?N?
(w /?
w?
).798Informally, constant restoring involves paddingthe vocabulary, while additive smoothing involvespadding the corpus.
Smoothing should be carriedout after restoring.3.1 Perplexity of Unigram ModelsLet us consider the perplexity of a unigram modellearned from a reduced corpus.
In unigram mod-els, a predictive distribution p?
on a reduced cor-pus w?
can be simply calculated as p?(w?)
=f(w?)/N?.
We shall start with an analysis oftraining-set perplexity, since we can derive an ex-act formula for it, which will give us a sufficientidea for making an approximate analysis of test-set perplexity.Let ?PP1:=(?w?w1p?
(w))1N be the perplexityof a ?-restored distribution p?
on a unigram model.The next lemma gives the optimal restoring con-stant ??
minimizing ?PP1.Lemma 2.
For any ?-restored distribution p?
of adistribution p?
from a reduced corpus w?
to theoriginal corpus w, its perplexity is minimized by?
?=N ?N?
(W ?W?)N?.Proof.
Let wRbe the longest subsequence suchthat minw??w?r(w?)
= W?+ 1.
Since wRis theremainder of w?, NR= N ?N?
and WR= W ?W?
hold.
After substituting the normalized formof p?
of Definition 1 into ?PP1, we have?PP1=(?w??w?1p?(w?)?wR?wR1p?(wR))1N=(?w?
?w?1 + WR?p?(w?
)?wR?wR1 + WR??
)1N=1 + WR??NRN(?w??w?1p?(w?
))1N.We obtain the optimal smoothing factor ??
when????PP1????
(1 + WR?
)/?NRN= 0.By using a similar argument to the one in theabove lemma, we can obtain the optimal constantof additive smoothing as ??
?
N?N ?WN?, when N issufficiently large.The next theorem gives the exact formula of thetraining-set perplexity of a unigram model learnedfrom a reduced corpus.Theorem 3.
For any distribution p?
on a unigrammodel learned from a corpus w?
reduced from theoriginal corpus w following Zipf?s law, the per-plexity ?PP1of the ?
?-restored distribution p?
of p?from w?
to w is calculated by?PP1(W?)
=H(W ) exp(B(W?
)H(W ))(W ?W?H(W )?H(W?))1?H(W?
)H(W ),where H(X) :=?Xx=11xand B(X) :=?Xx=1lnxx.Proof.
We expand the first part of ?PP1in the proofof Lemma 2 using ??
as follows:1 + WR???
?NRN=(1 +NRN?)(WRN?NR)NRN=(NN?
)((W ?W?
)N?N ?N?
)1?N?N.The second part of ?PP1is as follows:(?w??w?1p?(w?))1N=?w??{w?}(1p?(w?))f(w?)N=W??r=1(rN?C)CrN=W??r=1(N?C)CrNW??r=1rCrN=(N?C)N?Nexp(CNW?
?r=1ln rr).We obtain the objective formula by putting theabove two formulae together with N = CH(W )and N ?
= CH(W ?
), which are derived fromZipf?s law.The functions H(X) and B(X) are the X-thpartial sum of the harmonic series and Bertrandseries (special form), respectively.
An approxima-tion by definite integrals yields H(X) ?
lnX+?,where ?
is the Euler-Mascheroni constant, andB(X) ?12ln2X .
We may omit ?
from the ap-proximate analysis.Now let us consider an approximate form of?PP1(W?)
in Theorem 3.
For further discussion,799we define the last part of ?PP1(W?)
as follows:F (W,W?)
:=(W ?W?H(W )?H(W?))1?H(W?
)H(W ).Since W ?
= ?W holds for an appropriate ratio ?,we haveF (W, ?W ) =(W ?
?WH(W )?H(?W ))1?H(?W )H(W )?
(W ?
?WlnW ?
ln (?W ))1?ln (?W )lnW=(W (1?
?)?
ln ?)?
ln ?lnW?1?
(W ?
?
).Therefore, when W is sufficiently large, we canuse F (W,W ?)
?
WW?, since F (W, ?W ) ?
1?holdsfor any ratio ?
: 0 < ?
< 1.
Using this fact,we obtain an approximate formula ?PP1of ?PP1asfollows:?PP1(W?)
= lnW exp(ln2W?2 lnW)WW?=?W lnW exp(lnW??
lnW )22 lnW.The complexity of ?PP1is quasi-polynomial,i.e., ?PP1(W?)
= O(W?lnW?
), which behaves asa quadratic function on a log-log graph.
Since?PP1(W?)
is convex, i.e., ?2?W?2?PP1(W?)
> 0, andits gradient ??W??PP1(W?)
is zero when W ?
= W ,we infer that low-frequency words may not largelycontribute to the statistics.Considering the special case of W ?
= W , weobtain the perplexity PP1of the unigram modellearned from the original corpus w asPP1= H(W ) exp(B(W )H(W ))?
?W lnW.Interestingly, PP1is approximately expressed asa simple elementary function of vocabulary sizeW .
This suggests that models learned from cor-pora with the same vocabulary size theoreticallyhave the same perplexity.For the test-set perplexity, we assume that boththe training corpus w and test corpus w?are gen-erated from the same distribution based on Zipf?slaw.
This assumption is natural, considering thesituation of an in-domain test or cross validationtest.
Let w??
be the longest subsequence of w?such that for any w ?
w?
?, w ?
w?
holds.
For-mally, we assume p?
(w) ?
p??
(w) for any w ?
w?
?when W?> W?, where p??
is the true distribu-tion over w??.
Using similar arguments to thoseof Lemma 2 and Theorem 3 for w?, we obtainan approximation formula for the test-set perplex-ity, where we simply substitute W and W ?
in theexact formula for the training-set perplexity withW?and W?
?, respectively.
For simplicity, we willonly consider training-set perplexity from now on,since we can make a similar argument for the test-set perplexity in the later analysis.3.2 Perplexity of k-gram ModelsHere, we will consider the perplexity of a k-grammodel learned from a reduced corpus as a standardextension of a unigram model.
Our theory onlyassumes that the corpus is generated on the basisof Zipf?s law.
Thus, we can use a simple modelwhere k-grams are calculated from a random wordsequence based on Zipf?s law.
This model seemsto be stupid, since we can easily notice that thebigram ?is is?
is quite frequent, and the two bi-grams ?is a?
and ?a is?
have the same frequency.However, the experiments described later uncov-ered the fact that the model can roughly capturethe behavior of real corpora.The frequency fkof k-gram word wk?
wk inthe model is represented by the following formula:fk(wk) =Ckgk(rk(wk)),where Ckis the maximal frequency in k-grams, rkis the frequency ranking of wkover k-grams, andgkexpresses the frequency decay in k-grams.
Forexample, the decay function g2of bigrams is asfollows:(g2(i))i:= (g2(1), g2(2), g2(3), ?
?
?
)= (1 ?
1, 1 ?
2, 2 ?
1, 1 ?
3, 3 ?
1, ?
?
?
)= (1, 2, 2, 3, 3, 4, 4, 4, 5, 5, 6, ?
?
?
).This is an inverse of the sum of Piltz?s divisorfunctions d2(n) :=?i1?i2=n1, which representsthe number of divisors of an integer n (cf.
(OEIS,2001)).
In general, we formally define gkthroughits inverse: g?1k(?)
:= Sk(?
), where Sk(?)
:=?
?n=1dk(n) and dk(n) :=?i1?i2???ik=n1.
Since(gk(i))iis a sorted sequence of the elements of thek-th tensor power of vector (1, ?
?
?
,W ), we cancalculate the maximum frequency Ckas follows.800Lemma 4.
For any corpus w following Zipf?s law,the maximum frequency of k-grams in our modelis calculated byCk=N ?
(k ?
1)D(H(W ))k,where D denotes the number of documents in w.Proof.
We use?wkfk(wk) = Ck(?w1/r(w))k.The sum Sk(?)
of Piltz?s divisor functions canbe approximated by ?Pk(ln ?
), where Pk(x) is apolynomial of degree k ?
1 with respect to x,and the main term of ?Pk(ln ?)
is given by thefollowing residue Ress=1?k(s)xss, where ?
(s) isthe Riemann zeta function (Li, 2005).
Using thisfact, we obtain an approximation ln (g?1k(?))
?ln ?
+ O(ln (ln ?))
?
ln ?, when ?
is sufficientlylarge.
Thus, when the corpus is sufficiently large,we can see that the behavior of fkis roughly linearon a log-log graph, i.e., fk(wk) ?
rk(wk)?1, sinceif g?1k(?)
?
?c holds, then fk(r) ?
(gk(r))?1?r?1c holds.Unfortunately, however, most corpora in thereal world are not so large that the above-mentioned relation holds.
Actually, Ha et al (Haet al, 2002; Ha et al, 2006) experimentally foundthat although a k-gram corpus roughly follows apower law even when k > 1, its exponent issmaller than 1 (for Zipf?s law).
They pointed outthat the exponent of bigrams is about 0.66, andthat of 5-grams is about 0.59 in the Wall StreetJournal corpus (WSJ87).
Believing their claimthat there exists a constant ?ksuch that fk(wk) ?rk(wk)?
?k , we estimated the exponent of k-gramsin an actual situation in the form of the followinglemma.Lemma 5.
Assuming that fk(wk) ?
rk(wk)?
?kholds for any k-gram word wk?
wk in a corpusw following Zipf?s law, the optimal exponent inour model based on the least squares criterion iscalculated by?k=lnW(k ?
1) ln (lnW ) + lnW.Proof.
We find the optimal exponent ?kby mini-mizing the sum of squared errors between the gra-dients of g?1k(r) and r1pik on a log-log graph:?{?
?y(y + lnPk(y))??
?y(1?ky)}2dy,where y = ln r.In the case of unigrams (k = 1), the formulaexactly represents Zipf?s law.
In the case of k-grams (k > 1), we found that the formula ap-proaches Zipf?s law when W approaches infinity,i.e., limW??
?k= 1.Let us consider the perplexity of a k-grammodel learned from a reduced corpus.
We im-mediately obtain the following corollary usingLemma 5.Corollary 6.
For any distribution p?
on a k-grammodel learned from a corpus w?
reduced from theoriginal corpus w following Zipf?s law, assumingthat fk(wk) ?
rk(wk)?
?k holds for any k-gramword wk?
wk and the optimal exponent ?kinLemma 5, the perplexity ?PPkof the ?
?-restoreddistribution p?
of p?
from w?
to w is calculated by?PPk(W?)
=H?k(W ) exp(B?k(W?
)H?k(W ))(W ?W?H?k(W )?H?k(W?))1?Hpik(W?
)Hpik(W ),where Ha(X) :=?Xx=11xaand Ba(X) :=?Xx=1a lnxxa.Ha(X) is the X-th partial sum of the P-seriesor hyper-harmonic series, which is a generaliza-tion of the harmonic series H(X).
Ba(X) is theX-th partial sum of the Bertrand series (anotherspecial form of B(X)).
When 0 < a < 1, we caneasily calculate ?PPk(W?)
by using the followingapproximations:Ha(X) ?
(X + 1)1?a?
11?
aBa(X) ?a1?
a(X + 1)1?aln(X + 1)?a(1?
a)2(X + 1)1?a+a(1?
a)2.By putting the approximations of Ha(X) andBa(X) into the formula of Corollary 6, we ob-tain an approximation ?PPk(W?)
?
O(W?W?1?pik).This implies that ?PPk(W?)
is approximately linearon a log-log graph, when ?kis close to 1, i.e., k isrelatively small and W is sufficiently large.
Notethat we must use the approximation of H(X), notHa(X), when a = 1.The fact that the frequency of k-grams followsa power law leads us to an additional convenient801property, since the process of generating a cor-pus in our theory can be treated as a variant ofthe coupon collector?s problem.
In this problem,we consider how many trials are needed for col-lecting all coupons whose occurrence probabilitiesfollow some stable distribution.
According to awell-known result about power law distributions(Boneh and Papanicolaou, 1996), we need a cor-pus of size kWk1?
?klnW when ?k< 1, and W ln2 Wwhen ?k= 1 for collecting all of the k-grams, thenumber of which is W k. Using results in (Atso-nios et al, 2011), we can easily obtain a lower andupper bound of the actual vocabulary size ?Wkofk-grams from the corpus size N and vocabularysize W as?Wk?
(?k+ 1)(1?
e?(1?pik)NWk?1?lnWk?1Wk)?Wk??k?k?
1(NH?k(Wk))1pik?NW1??k(?k?
1)H?k(Wk).This means that we can determine the roughsparseness of k-grams and adjust some of the pa-rameters such as the gram size k in learning statis-tical language models.3.3 Perplexity of Topic ModelsIn this section, we consider the perplexity of thewidely used topic model, Latent Dirichlet Alloca-tion (LDA) (Blei et al, 2003), by using the nota-tion given in (Griffiths and Steyvers, 2004).
LDAis a probabilistic language model that generates acorpus as a mixture of hidden topics, and it allowsus to infer two parameters: the document-topicdistribution ?
that represents the mixture rate oftopics in each document, and the topic-word dis-tribution ?
that represents the occurrence rate ofwords in each topic.
For a given corpus w, themodel is defined as?di?
Dirichlet(?)zi|?di?
Multi(?di)?zi?
Dirichlet(?
)wi|zi, ?zi?
Multi(?zi),where diand ziare respectively the documentthat includes the i-th word wiand the hiddentopic that is assigned to wi.
In the case of infer-ence by Gibbs sampling presented in (Griffiths andSteyvers, 2004), we can sample a ?good?
topic as-signment zifor each word wiwith high probabil-ity.
Using the assignments z, we obtain the pos-terior distributions of two parameters as ?
?d(z) ?n(d)z+ ?
and ?
?z(w) ?
n(w)z+ ?, where n(d)zandn(w)zrespectively represent the number of timesassigning topic z in document d and the numberof times topic z is assigned to word w.Since an exact analysis is very hard, we willplace rough assumptions on ??
and ??
to reduce thecomplexity.
The assumption placed on ??
is that theword distribution ?
?zof each topic z follows Zipf?slaw.
We think this is acceptable since we can re-gard each topic as a corpus that follows Zipf?s law.Since ?
?zis normalized for each topic, we can as-sume that for any two topics, z and z?, and anytwo words, w and w?, ?
?z(w) ???z?(w?)
holds ifrz(w) = rz?(w?
), where rz(w) is the frequencyranking of w with respect to n(w)z.
Note that theabove assumption pertains to a posterior, and wedo not discuss the fact that a Pitman-Yor processprior is better suited for a power law (Goldwater etal., 2011).The assumption placed on ??
may not be reason-able in the case of ?
?, because we can easily thinkof a document with only one topic, and we usu-ally use a small number T of topics for LDA, e.g.,T = 20.
Thus, we consider two extreme cases.One is where each document evenly has all topics,and the other is where each document only has onetopic.
Although these two cases might be unreal-istic, the actual (theoretical) perplexity is expectedto be between their values.
We believe that analyz-ing such extreme cases is theoretically important,since it would be useful for bounding the compu-tational complexity and predictive performance.We can regard the former case as a unigrammodel, since the marginal predictive distribution?Tz=1??d(z)?
?z(w) ??Tz=1n(w)z+?T?
?f(w) is in-dependent of d; here we have used ?
?d(z) = 1/Tfrom the assumption.
In the latter case, we canobtain an exact formula for the perplexity of LDAwhen the topic assigned to each document followsa discrete uniform distribution, as shown in thenext theorem.
Note that a mixture of corpora fol-lowing Zipf?s law can be approximately regardedas following Zipf?s law, when W is sufficientlylarge.Theorem 7.
For any distribution p?
on the LDAmodel with T topics learned from a corpus w?
re-duced from the original corpus w following Zipf?slaw, assuming that each document only has onetopic which is assigned based on a discrete uni-form distribution, the perplexity ?PPMix of the ?
?-restored distribution p?
of p?
from w?
to w is calcu-802Table 1: Details of Reuters, 20news, Enwiki,Zipf1, and ZipfMix.vocab.
size corpus size doc.
sizeReuters 70,258 2,754,800 18,11820news 192,667 4,471,151 19,997Enwiki 409,902 16,711,226 51,231Zipf1 69,786 2,754,800 18,118ZipfMix 70,093 2,754,800 18,118lated by?PPMix(W ?)
=H(W/T ) exp(B(W?/T )H(W/T ))(W ?W?H(W/T )?H(W?/T ))1?H(W?/T )H(W/T )Proof.
We can prove this by using a similar argu-ment to that of Theorem 3 for each topic.The formula of the theorem is nearly identicalto the one of Theorem 3 for a 1/T corpus.
Thisimplies that the growth rate of the perplexity ofLDA models is larger than that of unigram mod-els, whereas the perplexity of LDA models forthe original corpus is smaller than that of unigrammodels.
In fact, a similar argument to the one inthe approximate analysis in Section 3.1 leads to anapproximate formula ?PPMix of ?PPMix as?PPMix(W ?)
=?WTlnWTexp(lnW??
lnW )22 ln (W/T ),when W is sufficiently large.
That is, ?PPMix(W ?
)also has a quadratic behavior in a log-log graph,i.e., ?PPMix(W ?)
= O(W ?lnW?
).4 ExperimentsWe performed experiments on three real corpora(Reuters, 20news, and Enwiki) and two syn-thetic corpora (Zipf1 and ZipfMix) to verifythe correctness of our theory and to examine thegap between theory and practice.
Reuters and20news here denote corpora extracted from theReuters-21578 and 20 Newsgroups data sets, re-spectively.
Enwiki is a 1/100 corpus of the En-glish Wikipedia.
Zipf1 is a synthetic corpus gen-erated by Zipf?s law, whose corpus is the same sizeas Reuters, and ZipfMix is a mixture of 20 syn-thetic corpora, sizes are 1/20th of Reuters.
Weused ZipfMix only for the experiments on topicmodels.
Table 1 lists the details of all five corpora.Fig.
1(a) shows the word frequency ofReuters, 20news, Enwiki, and Zipf1 versusfrequency ranking on a log-log graph.
In all cor-pora, we can regard each curve as linear with agradient close to 1.
This means that all corporaroughly follow Zipf?s law.
Furthermore, since thecurve of Zipf1 is similar to that of Reuters,Zipf1 can be regarded as acceptable.Fig.
1(b) plots the perplexity of unigram mod-els learned from Reuters, 20news, Enwiki,and Zipf1 versus the size of reduced vocabu-lary on a log-log graph.
Each value is the aver-age over different test sets of five-fold cross val-idation.
Theory1 is calculated using the for-mula in Theorem 3.
The graph shows that thecurve of Theory1 is nearly identical to that ofZipf1.
Since the vocabulary size W?of each testset is small in this experiment, some errors appearwhen W ?
is large, i.e., W?< W?.
This clearlymeans that our theory is theoretically correct foran ideal corpus Zipf1.
Comparing Zipf1 withReuters, however, we find that their perplex-ities are quite different.
The reason is that thegap between the frequencies of low-ranking (high-frequency) words is considerably large.
For ex-ample, the frequency of the 1st-rank word ofReuters is f(w) = 136, 371, while that ofZipf1 is f(w) = 234, 705.
Our theory seems tobe suited for inferring the growth rate of perplexityrather than the perplexity value itself.As for the approximate formula ?PP1of Theo-rem 3, we can surely regard the curve of Zipf1as being roughly quadratic.
The curves of realcorpora also have a similar tendency, althoughtheir gradients are slightly steeper.
This differencemight have been caused by the above-mentionederrors.
However, at least, we can ascertain theimportant fact that the results for the corpora re-duced by 1/100 are not so different from those ofthe original corpora from the perspective of theirperplexity measures.Fig.
1(c) plots the frequency of k-grams (k ?
{1, 2, 3}) in Reuters versus frequency rankingon a log-log graph.
TheoryFreq (1-3) are calcu-lated using Ckin Lemma 4 and ?kin Lemma 5.A comparison of TheoryFreq and Zipf verifiesthe correctness of our theory.
However, comparingZipf and Reuters, we see that Ckis poorly es-timated when the gram size is large, whereas ?kisroughly correct.
This may have happened becausewe did not put any assumptions on the word se-803100101102103104105106Frequency Ranking100101102103104105106107FrequencyReuters20newsEnwikiZipf1(a) Frequency of unigrams100101102103104105106Reduced Vocabulary Size103104105Test-setPerplexityReuters20newsEnwikiZipf1Theory1(b) Perplexity of unigram models100101102103104105Frequency Ranking101102103104105FrequencyReutersZipf1TheoryFreq1Reuters2Zipf2TheoryFreq2Reuters3Zipf3TheoryFreq3(c) Frequency of k-grams1 2 3 4 5 6 7 8 9 10Gram Size0.40.60.81.01.2ExponentReutersTheoryExp(d) Exponent of a power law over k-grams100101102103104105106107Reduced Vocabulary Size102103104105106Test-setPerplexityReutersZipf1Theory1Reuters2Zipf2Theory2Reuters3Zipf3Theory3(e) Perplexity of k-gram models100101102103104105106Reduced Vocabulary Size103104105Test-setPerplexityReuters20newsEnwikiZipf1Theory1ZipfMixTheoryMixTheoryAve(f) Perplexity of topic modelsFigure 1: (a) Word frequency of Reuters, 20news, Enwiki, and Zipf1 versus frequency ranking.
(b) Perplexity of unigram models learned from Reuters, 20news, Enwiki, and Zipf1 versus size ofreduced vocabulary.
Theory1 is calculated using the formula in Theorem 3.
(c) Frequency of k-grams(k ?
{1, 2, 3}) in Reuters and Zipf1 versus frequency ranking.
The suffix digit of each label meansits gram size.
TheoryFreq (1-3) are calculated using Lemma 4 and Lemma 5.
(d) Exponent of a powerlaw over k-grams in Reuters versus gram size.
TheoryGrad is calculated using ?kin Lemma 5.
(e)Perplexity of k-gram models learned from Reuters versus size of reduced vocabulary.
Theory2 andTheory3 are calculated using the formula in Corollary 6.
(f) Perplexity of topic models learned fromReuters, 20news, Enwiki, Zipf1, and ZipfMix versus size of reduced vocabulary.
TheoryMix iscalculated using the formula in Theorem 7.quences in our simple model.
The frequencies ofhigh-order k-grams tend to be lower than in real-ity.
We might need to place a hierarchical assump-tion on the a power law, as in done in hierarchicalPitman-Yor processes (Wood et al, 2011).Fig.
1(d) plots the exponent of the power lawover k-grams in Reuters versus the gram sizeon a normal graph.
We estimated each exponentof Reuters by using the least-squares method.TheoryGrad is calculated using ?kin Lemma 5.Surprisingly, the real exponents of Reuters arealmost the same as the theoretical estimate ?kbased on our ?stupid?
model that does not careabout the order of words.
Note that we do not useany information other than the vocabulary size Wand the gram size k for estimating ?k.Fig.
1(e) plots the perplexity of k-gram mod-els (k ?
{1, 2, 3}) learned from Reuters versusthe size of reduced vocabulary on a log-log graph.Theory2 and Theory3 are calculated using theformula in Corollary 6.
In the case of bigrams,the perplexities of Theory2 are almost the sameas that of Zipf2 when the size of reduced vocab-ulary is large.
However, in the case of trigrams,the perplexities of Theory3 are far from those ofZipf3.
This difference may be due to the sparse-ness of trigrams in Zipf3.
To verify the correct-ness of our theory for higher order k-gram models,we need to make assumptions that include backoffand smoothing.Fig.
1(f) plots the perplexity of LDA modelswith 20 topics learned from Reuters, 20news,Enwiki, Zipf1, and ZipfMix versus the size ofreduced vocabulary on a log-log graph.
We useda collapsed Gibbs sampler with 100 iterations toinfer the parameters and set the hyper parameters,?
= 0.1 and ?
= 0.1.
In evaluating the perplexity,we estimated a posterior document-topic distribu-804Table 2: Computational time and memory sizefor LDA learning on the original corpus, (1/10)-reduced corpus, and (1/20)-reduced corpus ofReuters.corpus time memory perplexityoriginal 4m3.80s 71,548KB 500(1/10) 3m55.70s 46,648KB 550(1/20) 3m42.63s 34,024KB 611tion ?
?dby using the first half of each test documentand calculated the perplexity on the second half,as is done in (Asuncion et al, 2009).
Each valueis the average over different test sets of five-foldcross validation.
Theory1 and TheoryMixare calculated using the formulae in Theorem 3and Theorem 7, respectively.
Comparing Zipf1with Theory1, and ZipfMix with TheoryMix,we find that our theory of the extreme casesdiscussed in Section 3.3 is theoretically cor-rect.
TheoryAve is the average of Theory1and TheoryMix.
Comparing Reuters andTheoryAve, we see that their curves are almostthe same.
If theoretical perplexity ?PP has asimilar tendency as real perplexity PP on alog-log graph, i.e., ln PP(W ?)
?
ln ?PP(W ?)
+ cfor some constant c, we can approximateits deterioration rate as PP(W ?
)/PP(W ) ?exp (ln?PP(W ?)
+ c)/ exp (ln ?PP(W ) + c) =?PP(W ?
)/ ?PP(W ).
Therefore, we can useTheoryAve as a heuristic function for estimat-ing the perplexity of topic models.
Since wecan calculate an inverse of TheoryAve fromthe bisection or Newton-Raphson method, wecan maximize the reduction rate and ensure anacceptable perplexity based on a user-specifieddeterioration rate.
According to the fact that thethree real corpora with different sizes have asimilar tendency, it is expected that we can useour theory for a larger corpus.Finally, let us examine the computational costsfor LDA learning.
Table 2 shows computa-tional time and memory size for LDA learningon the original corpus, (1/10)-reduced corpus, and(1/20)-reduced corpus of Reuters.
Comparingthe memory used in the learning with the origi-nal corpus and with the (1/10)-reduced corpus ofReuters, we find that the learning on the (1/10)-reduced corpus used 60% of the memory used bythe learning on the original corpus.
While thecomputational time decreased a little, we believethat reducing the memory size helps to reducecomputational time for a larger corpus in the sensethat it can relax the constraint for in-memory com-puting.
Although we did not examine the accuracyof real tasks in this paper, there is an interestingreport that the word error rate of language mod-els follows a power law with respect to perplexity(Klakow and Peters, 2002).
Thus, we conjecturethat the word error rate also has a similar tendencyas perplexity with respect to the reduced vocabu-lary size.5 ConclusionWe studied the relationship between perplexityand vocabulary size of reduced corpora.
We de-rived trade-off formulae for the perplexity of k-gram models and topic models with respect to thesize of reduced vocabulary and showed that eachformula approximately has a simple behavior on alog-log graph under certain conditions.
We veri-fied the correctness of our theory on synthetic cor-pora and examined the gap between theory andpractice on real corpora.
We found that the es-timation of the perplexity growth rate is reason-able.
This means that we can maximize the reduc-tion rate, thereby ensuring an acceptable perplex-ity based on a user-specified deterioration rate.Furthermore, this suggests the possibility that wecan theoretically derive empirical parameters, or?rules of thumb?, for different NLP problems, as-suming that a corpus follows Zipf?s law.
We be-lieve that our theoretical estimation has the advan-tages of computational efficiency and scalabilityespecially for very large corpora, although exper-imental estimations such as cross-validation maybe more accurate.In the future, we want to find out the cause ofthe gap between theory and practice and extendour theory to bridge the gap, in the same way thatwe can construct equations of motion with air re-sistance in the example of the landing point ofa ball in Section 1.
For example, promising re-search directions include using a general law suchas the Zipf-Mandelbrot law (Mandelbrot, 1965), asophisticated model that cares the order of wordssuch as hierarchical Pitman-Yor processes (Woodet al, 2011), and smoothing/backoff methods tohandle the sparseness problem.AcknowledgmentsThe author would like to thank the reviewers fortheir helpful comments.805ReferencesArthur Asuncion, Max Welling, Padhraic Smyth, andYee Whye Teh.
2009.
On smoothing and infer-ence for topic models.
In Proceedings of the 25thConference on Uncertainty in Artificial Intelligence(UAI2009), pages 27?34.
AUAI Press.Ioannis Atsonios, Olivier Beaumont, Nicolas Hanusse,and Yusik Kim.
2011.
On power-law distributedballs in bins and its applications to view size esti-mation.
In Proceedings of the 22nd InternationalSymposium on Algorithms and Computation (ISAAC2011), pages 504?513.
Springer-Verlag.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Shahar Boneh and Vassilis G. Papanicolaou.
1996.General asymptotic estimates for the coupon collec-tor problem.
Journal of Computational and AppliedMathematics, 67(2):277?289.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large LanguageModels in Machine Translation.
In Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL2007), pages 858?867.ACL.Adam L. Buchsbaum, Raffaele Giancarlo, and Jef-fery R. Westbrook.
1998.
Shrinking LanguageModels by Robust Approximation.
In Proceed-ings of the 1998 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP1998), pages 685?688.Stanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proceedings of the 34th annual meet-ing on Association for Computational Linguistics(ACL 1996), pages 310?318.
ACL.Ken Church, Ted Hart, and Jianfeng Gao.
2007.
Com-pressing Trigram Language Models with GolombCoding.
In Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning (EMNLP-CoNLL2007), pages 199?207.
ACL.Aaron Clauset, Cosma Rohilla Shalizi, and M. E. J.Newman.
2009.
Power-Law Distributions in Em-pirical Data.
SIAM Review, 51(4):661?703.Jianfeng Gao and Min Zhang.
2002.
Improving Lan-guage Model Size Reduction using Better PruningCriteria.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics(ACL 2002), pages 176?182.
ACL.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2011.
Producing Power-Law Distribu-tions and Damping Word Frequencies with Two-Stage Language Models.
Journal of Machine Learn-ing Research, 12:2335?2382.Joshua Goodman and Jianfeng Gao.
2000.
Lan-guage Model Size Reduction by Pruning and Clus-tering.
In Proceedings of the 6th InternationalConference on Spoken Language Processing (ICSLP2000), pages 110?113.
ISCA.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
In Proceedings of the NationalAcademy of Sciences of the United States of America(PNAS 2004), volume 101, pages 5228?5235.Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, and F. J.Smith.
2002.
Extension of Zipf?s Law to Words andPhrases.
In Proceedings of the 19th InternationalConference on Computational Linguistics (COLING2002), pages 1?6.
ACL.Le Quan Ha, P. Hanna, D. W. Stewart, and F. J. Smith.2006.
Reduced n-gram models for English and Chi-nese corpora.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics, Proceedings of the Conference(COLING-ACL 2006), pages 309?315.
ACL.Teemu Hirsimaki.
2007.
On Compressing N-GramLanguage Models.
In Proceedings of the 2007 IEEEInternational Conference on Acoustics, Speech andSignal Processing (ICASSP 2007), pages 949?952.Dietrich Klakow and Jochen Peters.
2002.
Testing thecorrelation of word error rate and perplexity.
SpeechCommunication, 38(1):19?28.Hailong Li.
2005.
On Generalized Euler Constantsand an Integral Related to the Piltz Divisor Problem.
?Siauliai Mathematical Seminar, 8:81?93.Benoit B. Mandelbrot.
1965.
Information Theoryand Psycholinguistics: A Theory of Word Frequen-cies.
In Scientific Psychology: Principles and Ap-proaches.
Basic Books.OEIS.
2001.
The on-line encyclopedia of inte-ger sequences (a061017).
http://oeis.org/A061017/.Mark Steyvers and Tom Griffiths.
2007.
Probabilis-tic Topic Models.
In Handbook of Latent SemanticAnalysis, pages 424?440.
Lawrence Erlbaum Asso-ciates.Andreas Stolcke.
1998.
Entropy-based Pruning ofBackoff Language Models.
In Proceedings of theDARPA Broadcast News Transcription and Under-standing Workshop, pages 270?274.Frank Wood, Jan Gasthaus, Ce?dric Archambeau,Lancelot James, and Yee Whye Teh.
2011.
The Se-quence Memoizer.
Communications of the Associa-tion for Computing Machines, 54(2):91?98.George Kingsley Zipf.
1935.
The Psychobiology ofLanguage.
Houghton-Mifflin.806
