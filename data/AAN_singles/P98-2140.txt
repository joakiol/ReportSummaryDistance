Feature Lattices for Maximum Entropy ModellingAndre i  M ikheev*HCRC, Language Technology Group, University of Edinburgh,2 Buccleuch Place, Edinburgh EH8 9LW, Scotland, UK.e-mail: Andrei.
Mikheev@ed.ac.ukAbst rac tMaximum entropy framework proved to be ex-pressive and powerful for the statistical an-guage modelling, but it suffers from the com-putational expensiveness of the model build-ing.
The iterative scaling algorithm that is usedfor the parameter estimation is computation-ally expensive while the feature selection pro-cess might require to estimate parameters formany candidate features many times.
In thispaper we present a novel approach for buildingmaximum entropy models.
Our approach usesthe feature collocation lattice and builds com-plex candidate features without resorting to it-erative scaling.1 In t roduct ionMaximum entropy modelling has been recentlyintroduced to the NLP community and provedto be an expressive and powerful framework.The maximum entropy model is a model whichfits a set of pre-defined constraints and assumesmaximum ignorance about everything which isnot subject o its constraints thus assigning suchcases with the most uniform distribution.
Themost uniform distribution will have the entropyon its maximumBecause of its ability to handle overlappingfeatures the maximum entropy framework pro-vides a principle way to incorporate informa-tion from multiple knowledge sources.
It issuperior totradit ionally used for this purposelinear interpolation and Katz back-off method.
(Rosenfeld, 1996) evaluates in detail a maxi-mum entropy language model which combinesunigrams, bigrams, trigrams and long-distancetrigger words, and provides a thorough analysisof all the merits of the approach.
* Now at Harlequin Ltd.The iterative scaling algorithm(Darroch&Ratcliff, 1972) applied for the pa-rameter estimation of maximum entropy mod-els computes a set of feature weights (As) whichensure that the model fits the reference distri-bution and does not make spurious assumptions(as required by the maximum entropy principle)about events beyond the reference distribution.It, however, does not guarantee that the fea-tures employed by the model are good featuresand the model is useful.
Thus the most im-portant part of the model building is the fea-ture selection procedure.
The key idea of thefeature selection is that if we notice an interac-tion between certain features we should build amore complex feature which will account for thisinteraction.
The newly added feature shouldimprove the model: its Kullback-Leibler diver-gence from the reference distribution should de-crease and the conditional maximum entropymodel will also have the greatest log-likelihood(L) value:The basic feature induction algorithm pre-sented in (Della Pietra et al, 1995) starts withan empty feature space and iterativety tries allpossible feature candidates.
These candidatesare either atomic features or complex featuresproduced as a combination of an atomic featurewith the features already selected to the model'sfeature space.
For every feature from the can-didate feature set the algorithm prescribes tocompute the maximum entropy model using theiterative scaling algorithm described above, andselect the feature which in the largest way min-imizes the Kullback-Leibler divergence or max-imizes the log-likelihood of the model.
This ap-proach, however, is not computationally feasi-ble since the iterative scaling is computation-ally expensive and to compute models for manycandidate features many times is unreal.
To848make feature ranking computationally tractablein (Della Pietra et al, 1995) and (Berger et al,1996) a simplified process proposed: at the fea-ture ranking stage when adding a new featureto the model, all previously computed parame-ters are kept fixed and, thus, we have to fit onlyone new constraint imposed by the candidatefeature.
Then after the best ranked feature hasbeen established, it is added to the feature spaceand the weights for all the features are recom-puted.
This approach estimates good featuresrelatively fast but it does not guarantee that atevery single point we add the best feature be-cause when we add a new feature to the modelall its parameters can change.In this paper we present a novel approach tofeature selection for the maximum entropy mod-els.
Our approach uses a feature collocation lat-tice and selects candidate features without re-sorting to the iterative scaling.2 Feature  Co l locat ion  Lat t i ceWe start the modelling process by building asample space w to train our model on.
The sam-ple space consists of observed events of interestmapped to a set of atomic features T which weshould define beforehand.
Thus every observa-tion from the sample space is a binary vectorof atomic features: if an observation includesa certain feature, its corresponding bit in thevector is turned on (set to 1) otherwise it is 0.When we have a set of atomic features T anda training sample of configurations w, we canbuild the feature collocation lattice.
Such collo-cation lattice will represent, in fact, the factorialconstraint space (X) for the maximum entropymodel and at the same time will contain all seenand logically implied configurations (w+).
For-mally, the feature collocation lattice is a 3-ple:(0, C_, ~w) where0 is a set of nodes of the lattice which corre-sponds to the union of the feature space ofthe maximum entropy model and the con-figuration space: 0 = XU~(w).
In fact, thenodes in the lattice (0) can have dual in-terpretation - on one hand they can act asmapped configurations from the extendedconfiguration space (w +) and on the otherhand they can act as features from the con-straint space (X);?
C_ is a transitive, antisymmetric relationover 0 x 0 - a partial ordering.
We also willneed the indicator function to flag whetherthe relation C holds from node i to node k:1 il OiC_Okfoi(Ok) = 0 otherwise~w is a set of configuration frequency countsof the nodes (0) of the lattice.
This repre-sents how many times we saw a particu-lar configuration in our training samples.Because of the dual interpretation of thenodes, a node can also be associated withits feature frequency count i.e.
the num-ber of times we see this feature combina-tion anywhere in the lattice.
The featurefrequency of a node will then be ~X(0k) =~0~e0 f0k(0i) * ~0~ which is the sum of allthe configuration frequency counts (~w) ofthe descendant nodes.Suppose we have a lattice of nodesA,B,\[AB\] with obvious relations: A C_\[AB\]; B C_ \[AB\]:A "~ \[AB\] ~, BThe configuration frequency ~,~ will be thenumber of times we saw A but not \[AB\]and then the feature frequency of A willbe: ~ = ~ + ~,~B i.e.
the number of timeswe saw A in all the nodes.When we construct the feature collocationlattice from a set of samples, each sample repre-sents a feature configuration which we must addto the lattice as its node (Ok).
To support gener-alizations over the domain we also want to addto the lattice the nodes which are shared partswith other nodes in the lattice.
Thus we addto the lattice all sub-configurations of a newlyadded configuration which are the intersectionswith the other nodes.
We increment he con-figuration frequency (~)  of a node each timewe see in the training samples this particularconfiguration in full.
For example, if a config-uration \[ABCD\] comes from a training sam-ple and it is still not in the lattice, we createa node \[ABCD\] and set its configuration fre-quency ~\[~ABCD\] to 1.
If by that time there is anode \[ABDE\] in the lattice, we then also create849the node \[ABD\], relate it to the nodes \[ABCD\]and \[ABDE\] and set its configuration frequencyto 0.
If \[ABCD\] had already existed in the lat-tice, we would simply incremented its configu-ration frequency: ~\[WABCD \] ~- ~\[WABcD \] + 1.Thus in the feature lattice we have nodes withnon-zero configuration frequencies, which wecall reference nodes and nodes with zero config-uration frequencies which we call latent or hid-den nodes.
Reference nodes actually representthe observed configuration space (w).
Hiddennodes are never observed on their own but onlyas parts of the reference nodes and representpossible generalizations about domain: low-complexity constraints (X) and logically possi-ble configurations (w+).This method of building the feature colloca-tion lattice ensures that along with true obser-vations it contains hidden nodes which can pro-vide generalizations about the domain.
At thesame time there is no over-generation f the hid-den nodes: no logically impossible feature com-binations and no hidden nodes without general-ization power are included.3 Feature  Se lec t ionAfter we constructed from a set of samples thefeature collocation lattice (0, C_,(~), which wewill call the empirical attice, we try to esti-mate which features contribute and which donot to the frequency distribution on the refer-ence nodes.
Thus only the predictive featureswill be retained in the lattice.
The optimizedfeature space can be seen as a feature lattice de-fined over the empirical feature lattice: 0' C_ 0and initially it is empty: 0' =j0.
We build theoptimized lattice by incrementally adding a fea-ture (atomic or complex) from the empirical lat-tice, together with the nodes which are the min-imal collocations of this feature with the nodesalready included into the optimized lattice.
Thenecessity to add the collocations comes from thefact that the features (or nodes) can overlapwith each other and we want to have a uniquenode for such overlap.
So if in the optimizedfeature lattice there is just one feature A, thenwhen we add the feature B we also have to addthe collocation \[AB\] if it exists in the empiricallattice.
The configuration frequency of a nodein the optimized lattice ((,w) then can be corn-puted as:tW__(1)Thus a node in the optimized lattice takes allconfiguration frequencies ((w) of itself and theabove related nodes if these nodes do not belongto the optimized lattice themselves and there isno higher node in the optimized lattice relatedto them.Figure i shows how the configuration frequen-cies in the optimized lattice are redistributedwhen adding a new feature.
First the lat-tice is empty.
When we add the feature Ato the optimized lattice (Figure 1.a), becauseno other features are present in the optimizedlattice, it takes all the configuration frequen-cies of the nodes where we see the feature A:~ = ~ + ~B + ~C + ~BC" Case b) of Fig-ure 1 represents the situation when we add thefeature B to the optimized lattice which alreadyincludes the feature A.
Apart from the node Bwe also add the collocation of the nodes A andB to the optimized lattice.
Now we have to re-distribute the configuration frequencies in theoptimized lattice.
The configuration frequencyof the node A now will become the number oftimes of seeing the feature A but not the fea-ture combination AB: ('A w = ~1 + ~C" Theconfiguration frequency of the node B will bethe number of times of seeing the node B butnot the nodeAB: ~ = ~ + w (BC" The con-figuration frequency of the node AB will be:~B = %ABCW -b %ABC'?W hen we add the feature Cto the optimized lattice (Figure 1.c) we producea fully saturated lattice identical to the empiri-cal lattice, since the node C will collocate withthe node A producing AC and will collocatewith the node B producing BC.
These nodesin their turn will collocate with each other andwith the node AB producing the node ABC.During the optimized lattice construction allthe features (atomic and complex) from the em-pirical lattice compete, and we include the onewhich results in a optimized lattice with thesmallest divergence D(p \[I P') and equation ??
)and therefore with the greatest log-likelihoodLp(p') , where:?
p(Oi) is the probability for the i-th node in850a)~'~' = ?~ + , ' .~ + ?~.c + ~'~.~cb) c)~ = ?~ + ~\ ]c  ~ = ~\] ~ = ~ = +~ ~= ,~=~(~c = ~c~T~c = ~cFigure 1: This figure shows the redistribution of the configuration frequencies in the optimized feature lattice whenadding new nodes.
Case a) stands for adding the feature A to the empty lattice, case b) stands for adding the featureB to the lattice with the feature A and case c) stands for adding the feature C to the lattice with the atomic featuresA and B and their collocations.
The unfilled nodes stand for the nodes in the empirical attice which don't havereference in the optimized lattice.
The nodes in bold stand for the nodes decided by the optimized lattice (i.e.
theycan be assigned with non-default probabilities).the empirical lattice:p(Oi)=--~?~ where N= ~_, ~ (2) N o~eo?
p'(Si) is the probability assigned to the i-thnode using only the nodes included into theoptimized lattice.but there is just one undecided node (C) whichis not shown in bold.
So the probabilities forthe nodes will be:I%~ %?t.u./ ?
iw=/(A)  / (ec )  = p'(B)/(ABC) =/(AB) /(C) -- ~N is the total count on the empirical lattice and{ ~_~_.
.
-, is calculated as shown in equation 2:~f u~Eup'(Oi) = ~ '/ O, ?
O' & N = ~.~ + ~ + ~ + ~B + ~'~C + ~'~BC"\[30~: oj e 0' & 0r c o,\] &\[~0k : 0k e 0' & "e~ c 0~ & 0j c 8k\] The presented above method provides us with1/\]Y \] oth~,.,,ise - an efficient way of selecting only important fea-(3)The optimized lattice assigns the probabil-ity to a node in the empirical lattice equalto that of its most specific sub-node fromthe optimized lattice.
For reference nodeswhich do not have sub-nodes in the opti-mized lattice at all (undecided nodes) ac-cording to the maximum entropy principlewe assign the uniform probability of mak-ing an arbitrary prediction.For instance, for the example on Figure 1.bthe optimized lattice includes only three nodestures from the initial set of candidate featureswithout resorting to iterative scaling.
Whenthis way we add the features to the optimizedlattice some candidate features might not suf-ficiently contribute to the probability distribu-tion on the lattice.
For instance, in the examplepresented on Figure 1, after we added the fea-ture \[B\] (case b) the only remaining undecidednode was IV\].
If the node \[C\] is truly hidden (i.e.it does not have its own observation frequency)and all other nodes are optimally decided, thereis no point to add the node \[C\] into the latticeand instead of having 9 nodes we will have only8513.
Another consideration which we apply duringthe lattice building is to penalize the develop-ment of low frequency (but not zero frequency)nodes i.e.
the nodes with no reliable statisticson them.
Thus we smooth the estimates on suchnodes with the uniform distribution (which hasthe entropy on its maximum):p"(Oi) = L * ~ + (1 - L) * p'(Oi) where  L =THRESHOLDTHRESHOLD+~'O~So for high frequency nodes this smoothing isvery minor but for nodes with frequencies lessthan two thresholds the penalty will be consid-erable.
This will favor nodes which do not cre-ate sparce collocations with other nodes.The described method is similar in spirit tothe method of word trigger incorporation to atrigram model suggested in (Rosenfeld, 1996):if a trigram predicts well enough there is noneed for an additional trigger.
The main differ-ence is that we do not recompute the maximumentropy model every time but use our own fre-quency redistribution method over the colloca-tion lattice.
This is the crucial difference whichmakes a tremendous aving in time.
We alsodo not require a newly added feature to be ei-ther atomic or a collocation of an atomic featurewith a feature already included into the modelas it was proposed in (Della Pietra et al, 1995)(Berger et al, 1996).
All the features are cre-ated equal and the model should decide on thelevel of granularity by itself.4 Mode l  Genera l i za t ionAfter we have chosen a subset of features for ourmodel, we restrict our feature lattice to the op-timized lattice.
Now we can compute the max-imum entropy model taking the reference prob-abilities (which are configuration probabilities)as in equation 3.The nodes from the optimized lattice serveboth as possible domain configurations and aspotential constraint features to our model.
We,however, want to constrain only the nodes withthe reliable statistics on them in order not tooverfit the model.
This in its turn will take offcertain computational load, since we expect aconsiderable number of fragmented (simply in-frequent) nodes in the optimized lattice.
Thiscomes from the requirement to build all the col-locations when we add a new node.
Althoughmany top-level nodes will not be constrained,the information from such infrequent nodes willnot be lost completely - it will contribute tomore general nodes since for every constrainednode we marginalize Over all its unconstraineddescendants (more specific nodes).
Thus aspossible constraints for the model we will con-sider only those nodes from the optimized lat-tice, whose marginalized over responses featurefrequency counts I are greater than a certainthreshold, e.g.
: ~0x__ ~ y > 5.
This considera- =( ,  )tion is slightly different from the one suggestedin (Ristad, 1996) where it was proposed to un-constrain nodes with infrequent jo int  featurefrequency counts.
Thus if we saw a certain fea-ture configuration say 5,000 times and it alwaysgave a single response we suggest o constrainas well the observation that we never saw thisconfiguration with the other responses.
If weapplied the suggestion of (Ristad, 1996) andcut out on the basis of the joint frequency wewould lose the negative vidence, which is quitereliable judging by the total frequency of theobservation.Initially we constrain all the nodes which sat-isfy the above requirement.
In order to gener-alize and simplify our maximum entropy model,we uncgnstrain the most specific features, com-pute a new simplified maximum entropy model,and if it still predicts well, we repeat the pro-cess.
So our aim is to remove from the con-straints as many top level nodes as possiblewithout losing the model fitness to the refer-ence distribution (15) of the optimized featurelattice.
The necessary condition for a node tobe taken as a candidate to unconstrain, is thatthis node shouldn't have any constrained nodesabove it.
There is also a natural ranking forthe candidate nodes: the closer to 1 the weight(),) of a such a node is, the less it is importantfor the model.
We can set a certain thresh-old on the weights, so all the candidate nodeswhose As differ from 1 less than this thresholdwill be unconstrained in one go.
Therefore wedon't have to use the iterative scaling for featureranking and apply it only for model recompu-tation, possibly un-constraining several featureconfigurations (nodes) at once.
This method, infact, resembles the Backward Sequential Search1~'~(Ok) = ~o,~o, Io, (o~) ?
gg852(BSS) proposed in (Pedersen&Bruce, 1997) fordecomposable models.
There is also a sig-nificant reduction in computational load sincethe generalized smaller model deviates from theprevious larger model only in a small number ofconstraints.
So we use the parameters of thatlarger model 2 as the initial values for the itera-tive scaling algorithm.
This proved to decreasethe number of required iterations by about ten-fold, which makes a tremendous saving in time.There can be many possible criteria when tostop the generalization algorithm.
The sim-plest one is just to set a predefined thresholdon the deviation D(fi II P) of the generalizedmodel from the reference distribution.
(Peder-sen&Bruce, 1997) suggest to use Akaike's Infor-mation Criteria (AIC) to judge the acceptabil-ity of a new model.
AIC rewards good model fitand penalizes models with high complexity mea-sured in the number of features.
We adoptedthe stop condition suggested in (Berger et al,1996) - the maximization of the likelihood on across-validation set of samples which is unseenat the parameter estimation.5 App l i ca t ion :  Fu l l s top  Prob lemSentence boundary disambiguation has recentlygained certain attention of the language ngi-neering community.
It is required for most textprocessing tasks such as, tagging, parsing, par-allel corpora alignment etc., and, as it turnedout to be, this is a non-trivial task itself.
Aperiod can act as the end of a sentence or bea part of an abbreviation, but when an abbre-viation is the last word in a sentence, the pe-riod denotes the end of a sentence as well.
Thesimplest "period-space-capital_letter" approachworks well for simple texts but is rather unre-liable for texts with many proper names andabbreviations at the end of sentence as, for in-stance, the Wall Street Journal (WSJ) corpus ((Marcus et al, 1993) ).One well-known trainable systems - SATZ- is described in (Palmer&Hearst, 1997).
Ituses a neural network with two layers of hid-den units.
It was trained on the most prob-able parts-of-speech of three words before andthree words after the period using 573 samplesfrom the WSJ corpus.
It was then tested on2instead of the uniform distribution as prescribed inthe step 1 of the Improved Iterative Scaling algorithm.853unseen 27,294 sentences from the same corpusand achieved 1.5% error rate.
Another auto-matically trainable system described in (Rey-nar&Ratnaparkhi, 1997).
This system is sim-ilar to ours in the model choice - it uses themaximum entropy framework.
It was trainedon two different feature sets and scored 1.2%error rate on the corpus tuned feature set and2% error rate on a more portable feature set.The features themselves were words and theirclasses in the immediate context of the periodmark.
(Reynar&Ratnaparkhi, 1997) don't re-port on the number of features utilized by theirmodel and don't describe their approach to fea-ture selection but judging by the time their sys-tem was trained (18 minutes 3) it did not aimto produce the best performing feature-set butestimated a given one.To tackle this problem we applied our methodto a maximum entropy model which used alexicon of words associated with one or morecategories from the set: abbreviation, propernoun, content word, closed-class word.
Thismodel employed atomic features uch as the lex-icon information for the words before and afterthe period, their capitalization and spellings.For training we collected from the WSJ cor-pus 51,000 samples of the form (Y, F..F) and(N, F..F), where Y stands for the end of sen-tence, N stands for otherwise and Fs stand forthe atomic features of the model.
We started tobuilt the model with 238 most frequent atomicfeatures which gave us the collocation lattice of8,245 nodes in 8 minutes of processor time onfive SUN Ultra-1 workstations working in par-allel by means of multi-threading and RemoteProcess Communication.
When we applied thefeature selection algorithm (section 3), we in 53minutes boiled the lattice down to 769 nodes.Then constraining all the nodes, we compileda maximum entropy model in about 15 minutesand then using the constraint removal process intwo hours we boiled the constraint space downto 283.
In this set only 31 atomic features re-mained.
This model was detected to achieve thebest performance on a specified cross-validationset.
For the evaluation we used the same 27,294sentences as in (Palmer&Hearst, 1997) 4 whichaPersonal communication4We would like to thank David Palmer for making histest data available to us.were also used by (Reynar&Ratnaparkhi, 1997)in the evaluation of their system.
These sen-tences, of course, were not seen at the train-ing phase of our model.
Our model achieved99,2477% accuracy which is the highest quotedscore on this test-set known to the authors.We attribute this to the fact that although westarted with roughly the same atomic featuresas (Reynar&Ratnaparkhi, 1997) our systemcreated complex features with higher predictionpower.6 Conc lus ionIn this paper we presented a novel approach forbuilding maximum entropy models.
Our ap-proach uses a feature collocation lattice and se-lects the candidate features without resortingto iterative scaling.
Instead we use our ownfrequency redistribution algorithm.
After thecandidate features have been selected we, us-ing the iterative scaling, compute a fully satu-rated model for the maximal constraint spaceand then apply relaxation to the most specificconstraints.We applied the described method to sev-eral language modelling tasks such as sentenceboundary disambiguation, part-of-speech tag-ging, stress prediction in continues peech gen-eration, etc., and proved its feasibility for select-ing and building the models with the complex-ity of tens of thousands constraints.
We see themajor achievement of our method in buildingcompact models with only a fraction of possi-ble features (usually there is a few hundred fea-tures) and at the same time performing at leastas good as state-of-the-art: in fact, our sen-tence boundary disambiguater scored the high-est known to the author accuracy (99.2477%)and our part-of-speech tagging model general-ized for a new domain with only a tiny degra-dation in performance.A potential drawback of our approach is thatwe require to build the feature collocation lat-tice for the whole observed feature-space whichmight not be feasible for applications with hun-dreds of thousands of features.
So one of thedirections in our future work is to find effi-cient ways for a decomposition of the featurelattice into non-overlapping sub-lattices whichthen can be handled by our method.
Anotheravenue for further improvement is to introduce854the "or" operation on the nodes of the lattice.This can provide a further generalization overthe features employed by the model.7 AcknowledgementsThe work reported in this paper was supportedin part by grant GR/L21952 (Text Tokenisa-tion Tool) from the Engineering and PhysicalSciences Research Council, UK.
We would alsolike to acknowledge that this work was based ona long-standing collaborative relationship withSteve Finch.ReferencesA.
Berger, S. Della Pietra, V. Della Pietra,1996.
A Maximum Entropy Approach to Nat-ural Language Processing In ComputationalLinguistics vol.22(1)J.N.
Darroch and D. Ratcliff 1972.
GeneralizedIterative Scaling for Log-Linear Models.
TheAnnals of Mathematical Statistics, 43(5).S.
Della Pietra, V.. Della Pietra, and J. Lafferty1995.
Inducing Features of Random FieldsTechnical report CMU-CS-95-144M.
Marcus, M.A.
Marcinkiewicz, and B. San-torini 1993.
Building a Large Annotated Cor-pus of English: The Penn Treebank.
In Com-putational Linguistics, vol 19(2), ACL.D.
D. Palmer and M. A. Hearst 1997.
AdaptiveMultilingual Sentence Boundary Disambigua-tion.
In Computational Linguistics, vol 23(2),ACL.
pp.
241-269T.
Pedersen and R. Bruce 1997.
A New Su-pervised Learning Algorithm for Word SenseDisambiguation.
In Proceedings of the Four-teenth National Conference on Artificial In-telligence, Providence, RI.J.
C. Reynar and A. Ratnaparkhi 1997.
AMaximum Entropy Approach to IdentifyingSentence Boundaries.
In Proceedings of theFifth A CL Conference on Applied NaturalLanguage Processing (ANLP'97), Washing-ton D.C., ACL.E.
S. Ristad 1996.
Maximum Entropy Mod-elling Toolkit.
Documentation for Version1.3 Beta, Draft,R.
Rosenfeld 1996.
A Maximum EntropyApproach to Adaptive Statistical LanguageLearning.
In Computer Speech and Language,vol.10(3), Academic Press Limited, pp.
197-228
