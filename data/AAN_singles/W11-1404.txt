Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 30?37,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsElicited Imitation for Prediction of OPI Test ScoresKevin CookBrigham Young UniversityDepartment of Computer Sciencekevincook13@gmail.comJeremiah McGhee, Deryle LonsdaleBrigham Young UniversityDepartment of Linguistics{jlmcghee,lonz}@byu.eduAbstractAutomated testing of spoken language is thesubject of much current research.
ElicitedImitation (EI), or sentence repetition, is wellsuited for automated scoring, but does not di-rectly test a broad range of speech communi-cation skills.
An Oral Proficiency Interview(OPI) tests a broad range of skills, but is not aswell suited for automated scoring.
Some havesuggested that EI can be used as a predictor ofmore general speech communication abilities.We examine EI for this purpose.
A fully au-tomated EI test is used to predict OPI scores.Experiments show strong correlation betweenpredicted and actual OPI scores.
Effective-ness of OPI score prediction depends upon atleast two important design decisions.
One ofthese decisions is to base prediction primar-ily on acoustic measures, rather than on tran-scription.
The other of these decisions is thechoice of sentences, or EI test items, to be re-peated.
It is shown that both of these designdecisions can greatly impact performance.
Itis also shown that the effectiveness of individ-ual test items can be predicted.1 Introduction1.1 BackgroundLearning to speak a second language is an impor-tant objective for many people.
Assessing progressin oral proficiency is often expensive and time-consuming.
The development of automated systemspromises to significantly lower costs and increaseaccessibility.Elicited imitation (EI) has been used for nearlyhalf a century to measure abnormal language devel-opment (Fujiki and Brinton, 1987) and the perfor-mance of second language learners (Chaudron et al,2005; Vinther, 2002).
As a method for assessing oralproficiency it consists of a person listening to a testitem, typically a full sentence, and then doing theirbest to repeat it back correctly.
This method is alsoreferred to as sentence repetition, or more simply asrepeats.
One motivation for using EI, as opposed tosome other form of test, is that it is relatively inex-pensive to administer.
An EI test can be effectivelyscored by non-experts in a relatively short amountof time.
It is also well suited for automated scoring(Graham et al, 2008), since correct responses arepredictable.1.2 MotivationThe language skills directly measured by an EI testare those involved in repeating back what one hasjust heard.
In order to directly measure a broader setof language skills, other tests must be used.
One ofthese is the Oral Proficiency Interview (OPI).The OPI is face-to-face interview conducted to as-sess language proficiency.
The interview tests dif-ferent types of relevant skills and lasts for about 30minutes.
Additionally, a validated OPI requires asecond review of a recording created during the ini-tial interview with arbitration if necessary.
This pro-cess is expensive ( $150 U.S.) and time-consumingwith a turn-around of several weeks before finalizedresults are received.A fully automated OPI test does not seem to bepractical.
This is especially the case when the in-30terpersonal aspects of a face-to-face interview areconsidered.
There have been several efforts to au-tomatically score the type of speech which might bespoken by an OPI test-taker, spontaneous non-nativespeech (Zechner and Xi, 2008).
It has been shownthat current automatic speech recognition (ASR)systems, used to transcribe such speech, have errorrates which make it challenging to use transcripts fortesting purposes.The argument has been made that although EIdoes not directly measure communicative skills,such as the ability to converse with another person,it can be used to infer such skills (Henning, 1983).Part of the theory behind EI is that people typicallyare not able to memorize the sounds of an utterancethe length of a full sentence.
Rather, people builda mental model of the meaning of an utterance, andare then able to remember the model.
People whocannot understand the utterance are not able to builda mental model, and are therefore unable to remem-ber or repeat the utterance.
If it is true that EI canbe used to infer more general speech communica-tion abilities, even if only to a limited extent, thenEI may be useful for predicting test scores which aredesigned to directly measure that ability.Bernstein et al (2000) describe a system whichelicits short predictable responses, such as readings,repeats (EI), opposites, and short answers, for auto-mated testing.
A similar system is discussed laterin Bernstein et al (2010).
It is evident that EI isused in these systems, as part of a greater whole.The argument is made that although the skills di-rectly tested are limited, the scores produced may beuseful for inferring more general language abilities.It is shown that automated scores correlate well withscores from conventional tests, such as the OPI.
Oneaspect which may not be as clear is the role that EIplays as compared to other methods used in the au-tomated test.We are interested in the use of a fully automatedEI test as a means to predict more general abilityin spoken language communication.
Since the OPItest is specifically designed to measure such generalability we use it as a gold standard, in spite of thefact that we do not expect it to be a perfect measure.We are interested in learning the extent to which OPIscores can be predicted using an EI test.
We are alsointerested in learning how to design an automatedsystem such that prediction of OPI scores is mosteffective.
We evaluate system performance based onhow highly correlated OPI score predictions are withactual OPI scores.Several design decisions must be made in the de-velopment of such a system.
One, is which methodto use for converting spoken responses to OPI scorepredictions.
Another, is the choice of sentences, orEI test items, to be repeated.
We address both ofthese issues.There are at least two approaches to scoring spo-ken responses.
One, is to score based on tran-scriptions, generated by a speech recognizer.
An-other, is to score based on acoustic measures alone,such as pronunciation and fluency (Cincarek et al,2009).
The primary difference between these twoapproaches is what is assumed about the textual con-tent of a spoken response.
Acoustic measures arebased on the assumption that the textual content ofeach spoken response is known.
Speech recognitionis based on the assumption that the content is notknown.
We explore the effect of this assumption onOPI prediction.The selection of effective EI test items has beenthe subject of some research.
Tomita et al (2009)outline principles for creating effective EI test items.Christensen et al (2010) present a tool for test itemcreation.
We explore the use of OPI scores as ameans to evaluate the effectiveness of individual testitems.2 Related WorkThe system described by Bernstein et al (2010) usesEI as part of the automated test.
Sentences rangein length from two to twenty or more syllables.
Iffewer than 90% of natives can repeat the sentenceverbatim, then the item is not used.
An augmentedASR system is used which has been optimized fornon-native speech.
The ASR system is used to tran-scribe test-taker responses.
Transcriptions are com-pared to the word string recited in the prompt.
Worderrors are counted and used to calculate a score.
Flu-ency and pronunciation of spoken responses are alsoscored.Graham et al (2008) report on a system whichuses EI for automated assessment.
Results show thatautomated scores are strongly correlated with man-31ual EI scores.
ASR grammars are specific to eachtest item.
Our work is based on this system.Mu?ller et al (2009) compare the effectiveness ofreading and repeating (EI) tasks for automated test-ing.
Automated scores are compared with manualscores for the same task.
It is found that repeatingtasks provide a better means of automatic assess-ment than reading tasks.3 ExperimentsIn this section we describe experiments, includingboth an OPI test and an automated EI test.
We de-tail the manner of automated scoring of the EI test,together with the method used to predict OPI scores.3.1 SetupWe administer an ACTFL-OPI (see www.actfl.org)and an automated EI test to each of 85 English asa Foreign Language learners of varying proficiencylevels.
This group of speakers (test-takers) is ran-domly divided into a 70%/30% training/testing split,with 60 speakers forming the training set and the re-maining 25 forming the test set.
Training data con-sists of OPI scores and EI responses for each speakerin the training set.
Test data consists of OPI scoresand EI responses for each speaker in the test set.An OPI is a face-to-face interview conducted bya skilled, certified human evaluator.
(We do not ex-pect that this interview results in an ideal evaluationof oral proficiency.
We use the OPI because it is de-signed to directly test speech communication skillswhich are not directly tested by EI.)
OPI proficiencylevels range across a 10-tiered nominal scale fromNovice Low to Superior.
We convert these levels toan integer score from 1 to 10 (NoviceLow = 1,Superior = 10).The EI test consists of 59 items, each an Englishsentence.
An automated system plays a recording ofeach sentence and then records the speaker?s attemptto repeat the sentence verbatim.
A fixed amount oftime is allotted for the speaker to repeat the sentence.After that fixed time, the next item is presented, untilall items are presented and all responses recorded.The choice of which items to include in the test issomewhat arbitrary; we select those items which webelieve might work well, given past experimentationwith EI.
We expect that improvement could be madein both the manner of administration of the test, andin the selection of test items.Responses are scored using a Sphinx 4 (Walkeret al, 2004) ASR system, version 1.0 beta 4, to-gether with the supplied 30-6800HZ WSJ acousticmodel.
ASR performance is affected by various sys-tem parameters.
For our experiments, we generallyuse default parameters found in configuration filesfor Sphinx demos.
The ASR system has not beenadapted for non-native speech.3.2 Language ModelsWe vary the language model component of the ASRsystem in order to evaluate the merit of assum-ing that the content of spoken responses is known.Speech recognizers use both an acoustic model anda language model, to transcribe text.
The acousticmodel is used to estimate a probability correspond-ing to how well input speech sounds like output text.The language model is used to estimate a probabil-ity corresponding to how well output text looks likea target language, such as English.
Output text isdetermined based on a joint probability, using boththe acoustic and the language models.
We vary thedegree to which it is assumed that the content of spo-ken responses is known.
This is done by varying thedegree to which the language model is constrainedto the text of the expected response.When the language model is fully constrained, theassumption is made that the content of each spokenresponse is known.
The language model assigns allprobability to the text of the expected response.
Allother output text has zero probability.
The acousticmodel estimates a probability for this word sequenceaccording to how well the test item is pronounced.
Ifthe joint probability of the word sequence is belowa certain rejection threshold, then there is no out-put from the speech recognizer.
Otherwise, the textof the test item is the output of the speech recog-nizer.
With this fully constrained language model,the speech recognizer is essentially a binary indica-tor of pronunciation quality.When the language model is fully unconstrained,there is no relationship between the language modeland test items, except that test items belong to theEnglish language.
In this case, the speech recognizerfunctions normally, as a means to transcribe spokenresponses.
Output text is the best guess of the ASR32system as to what was said.A partially constrained language model is one thatis based on test items, but also allows variation inoutput text.We perform experiments using the following fivelanguage models:1.
WSJ20K The 20K word Wall Street Journallanguage model, supplied with Sphinx.2.
WSJ5K The 5K word Wall Street Journal lan-guage model, supplied with Sphinx.3.
EI Items A custom language model createdfrom the corpus of all test items.4.
Item Selection A custom language model con-straining output to any one of the test items.5.
Forced Alignment A custom language modelconstraining output to only the current testitem.The first two language models, WSJ20K andWSJ5K, are supplied with Sphinx and have no spe-cial relationship to the test items.
The training cor-pus used to build these models is drawn from issuesof the Wall Street Journal.
These models are fullyunconstrained.The third model, EI Items, is a conventional lan-guage model with the exception that the training cor-pus is very limited.
The training corpus consists ofall test items; no other text is included in the train-ing corpus.
The fourth model, Item Selection, is nota conventional language model.
It assigns a set prob-ability to each test item as a whole.
That probabilityis equal to one divided by the total number of testitems.
Such a simple language model is sometimesreferred to as a grammar (Walker et al, 2004; Gra-ham et al, 2008).
Both the EI Items and Item Selec-tion models are partially constrained.
The Item Se-lection model is much more highly constrained thanthe EI Items model.The last model, Forced Alignment, is fully con-strained.
It assigns all probability to item text.
Thesefive language models are chosen for the purpose ofevaluating the effectiveness of constraining the lan-guage model to the text of the expected response.i ItemI Number of itemss Speaker (test-taker)S Number of speakersxis Score for item i, speaker sy s Predicted OPI score for speaker sos Actual OPI score for speaker sMSE i Mean squared error for item iFigure 1: Notation used in this paper.3.3 ScoringEach response is scored using a two-step process.First, the spoken response is transcribed by the ASRsystem.
Second, word error rate (WER) is calcu-lated by comparing the transcription to the item text.WER is converted to an item score xis for item i andspeaker s in the range of 0 to 1 using the followingformula:xis ={1?
WER100 if WER < 100%0 otherwise(1)A list of notation used in this paper is shown inFigure 1.3.4 PredictionIn order to avoid over-fitting, a simple linear modelis trained (Witten and Frank, 2005) to predict an OPIscore ys, given items scores xis together with modelparameters a and b.
The mean of item scores forspeaker s is multiplied by parameter a.
This productplus parameter b is the OPI score prediction: (I isthe total number of items.
)ys =1I?ixis ?
a + b (2)Correlation is calculated between predicted andactual OPI scores for all speakers in the test set.4 ResultsCorrelation for each of the language models usingall 59 test items is shown in Figure 2.
Correlation forboth of the unconstrained language models was rel-atively poor.
Performance improved significantly asthe language model was constrained to the expectedresponse.
These results suggest that it is effective33to assume that the content of spoken responses isknown.Fully constraining the language model to the textof the expected response results in an item scorewhich is a binary indicator (because, in this case,WER is either 100% or 0%) of how well the spokenresponse sounds like the expected response.
In thiscase, prediction is based on the output of the acous-tic model of the speech recognizer, an acoustic mea-sure.
Prediction is not based on transcription, sincea specific transcription is assumed prior to process-ing the spoken response.
When the language modelis fully unconstrained, an item score is an indica-tor of how well ASR transcription matches the textof the expected response.
In this case, prediction isbased on transcription, the speech recognizer?s bestguess of which words were spoken.
Results indicatethat correlation between predicted and actual OPIscores improves as prediction is based on acousticmeasures, rather than on transcription.Language Model Constrained Corr.WSJ20K Not 0.633WSJ5K Not 0.600EI Items Partial 0.737Item Selection Partial 0.805Forced Alignment Full 0.799Figure 2: Correlation with OPI scores, for all 5 languagemodels, using all 59 test items.
Language models areunconstrained, partially constrained, or fully constrainedto the text of the expected response.4.1 Item MSEThe effectiveness of individual test items is exploredby defining a measure of item quality.
If each itemscore xis were ideally linearly correlated with theactual OPI score os for speaker s then the equalityshown below would hold: (os is an integer from 1 to10.
xis is a real number from 0 to 1.
)IDEAL =?
os = xis ?
9 + 1 (3)We calculate the difference between this ideal andthe actual OPI score:(xis ?
9 + 1)?
os (4)This difference can be seen as a measure of howuseful the item is as a predictor OPI scores.
For bet-ter items, this difference is closer to zero.
The meanof the squares of these differences for a particularitem, over all S speakers in the training set, is a mea-sure of item quality MSEi:MSEi =1S?s((xis ?
9 + 1)?
os)2 (5)Because we expect improved results by assumingthat the content of expected responses is known, weuse the Forced Alignment language model to cal-culate an MSE score for each test item.
A sampleof items and their associated MSE are listed in Fig-ure 3.MSE Item text9.28 He should have walked away beforethe fight started.10.48 We should have eaten breakfast bynow.. .
.14.53 She dove into the pool gracefully, andwith perfect form.14.68 If her heart were to stop beating, wemight not be able to help her.. .
.25.78 She ought to learn Spanish.26.09 Sometimes they go to town.Figure 3: Sample EI items with corresponding MSEscores.Item MSE scores are used to define various sub-sets of test items, better items, worse items, and soon.
Better items have lower MSE scores.
These sub-sets are used to compute a series of correlations foreach of the five language models.
First, correlationis computed using only one test item.
That item isthe item with the lowest (best) MSE score.
Then,correlation is computed again using only two testitems, the two items with the lowest MSE scores.This process is repeated until correlation is com-puted using all test items.
Results are shown in Fig-ure 4.
These results show even more convincinglythat OPI prediction improves by assuming that thecontent of spoken responses is known.4.2 OPI PredictionFigure 4 also gives an idea of how effectively EI canbe used to predict OPI scores.
Correlation over 0.80is achieved using the Forced Alignment language34Figure 4: Correlation with OPI scores, for all 5 languagemodels, using varying numbers of test items.Figure 5: Plot of predicted OPI scores as a function ofactual OPI scores, using the Forced Alignment languagemodel and the best 24 test items.model for all but 7 of the 59 subsets of test items.Correlation is over 0.84 for 11 of the subsets (best20 - best 31).
Correlation is above 0.85 for 3 subsets(best 23 - best 25).
Predicted OPI scores correlatestrongly with actual OPI scores.Figure 5 shows a plot of predicted OPI scores asa function of actual OPI scores, using the ForcedAlignment language model and only the best 24 testitems.
Correlation is 0.856.
Interestingly, two of theoutliers (OPI=5, predicted OPI=2.3) and (OPI=4,predicted OPI=2.3) were for speakers whose re-sponses contained only silence, indicating those par-ticipants may have experienced technical difficultiesor may have been uncooperative during their testsession.
The inferred model used to calculate OPIpredictions for Figure 5 is shown below:ys =1I?ixis ?
6.8 + 2.3 (6)(Given this particular model, the lowest possiblepredicted OPI score is 2.3, and the highest possiblepredicted score is 9.1.
The ability to predict OPIscores 1 and 10 is lost, but the objective is to improveoverall correlation.
)4.3 Item SelectionTo see more clearly the effect that the choice of testitems has on OPI prediction, we compute a seriesof correlations similar to before, except that the or-der of test items is reversed: First, correlation iscomputed using only the test item with the high-est (worst) MSE score.
Then, correlation is com-puted again using only the two worst items, and soon.
This series of correlations is computed for theForced Alignment language model only.
It is showntogether with the original ordering for the ForcedAlignment language model from Figure 4.These two series are shown in Figure 6.
The se-ries with generally high correlation is computed us-ing best items first.
The series with generally lowcorrelation is computed using worst items first.
Atthe end of both series all items are used, and corre-lation is the same.
As mentioned earlier, correlationusing only the best 24 items is 0.856.
By contrast,correlation using only the worst 24 items is 0.679.The choice of test items can have a significant im-pact on OPI score prediction.Figure 6 also shows that the effectiveness of in-dividual test items can be predicted.
MSE scoreswere calculated using only training data.
Correla-tions were calculated for test data.4.4 Rejection ThresholdSince the Forced Alignment language model isfound to be so effective, we experiment further tolearn more about its behavior.
Using this languagemodel, item scores are either zero or one, dependingupon whether ASR output text is the same as itemtext, or there is no output text.
If joint probability,for a spoken response, is below a certain rejectionthreshold, no text is output.
We perform experiments35Figure 6: Correlation with OPI scores, showing the dif-ference between best and worst items, using the ForcedAlignment language model.Figure 7: Correlation with OPI scores versus rejectionthreshold.to see how sensitive OPI predictions are to the set-ting of this threshold.Any ASR system parameter which affects prob-ability estimates of word sequences can affect therejection threshold.
We make the arbitrary deci-sion to vary the Sphinx relativeBeamWidth pa-rameter.
For all previous experiments, the valueof this parameter was fixed at 1E ?
90.
ThewordInsertionProbability parameter, which alsoaffects the rejection threshold, was fixed at 1E?36.Correlation is computed for various values ofthe relativeBeamWidth parameter.
Results areshown in Figure 7.
Good results are obtained overa wide range of rejection thresholds.
Correlationpeaks at 1E?
80.
OPI prediction does not appear tobe overly sensitive to the setting of this threshold.5 DiscussionWe conclude that a fully-automated EI test can beused to effectively predict more general languageability than those abilities which are directly testedby EI.
Such an EI test is used to predict the OPIscores of 25 test-takers.
Correlation between pre-dicted and actual OPI scores is strong.Effectiveness of OPI score prediction dependsupon at least two important design decisions.
Oneof these decisions is to base prediction primarily onacoustic measures, rather than on transcription.
Theother of these decisions is the choice of sentences,or EI test items, to be repeated.
It is shown that bothof these design decisions can greatly impact perfor-mance.
It is also shown that the effectiveness of in-dividual test items can be predicted.We quantify the effectiveness of individual testitems using item MSE.
It may be possible to useitem MSE to learn more about the characteristicsof effective EI test items.
Developing more effec-tive test items may lead to improved prediction ofOPI test scores.
In this paper, we do not attemptto address how linguistic factors (such as sentencelength, syntactic complexity, lexical difficulty, andmorphology) affect test item effectiveness for OPIprediction.
However, others have discussed simi-lar questions (Tomita et al, 2009; Christensen et al,2010).It may be possible that a test-taker could learnstrategies for doing well on an EI test, without de-veloping more general speech communication skills.If test-takers were able to learn such strategies, itmay affect the usefulness of EI tests.
Bernstein et al(2010) suggest that, as yet, no conclusive evidencehas been presented on this issue, and that automatedtest providers welcome such research.It is possible that other automated systems arefound to be more effective as a means for testingspeech communication skills, or as a means for pre-dicting OPI scores.
We expect this to be the case.The purpose of this research is not to design the bestpossible system.
Rather, it is to improve understand-ing of how such a system might be designed.
It isshown that an EI test can be used as a key compo-nent of such a system.
Strong correlation betweenactual and predicted OPI scores is achieved withoutusing any other language testing method.36AcknowledgmentsWe would like to thank the Brigham Young Uni-versity English Language Center for their support.We also appreciate assistance from the PedagogicalSoftware and Speech Technology research group,Casey Kennington, and Dr. C. Ray Graham.ReferencesJared Bernstein, John De Jong, David Pisoni, and BrentTownshend.
2000.
Two experiments on automaticscoring of spoken language proficiency.
In P. Del-cloque, editor, Proceedings of InSTIL2000 (Integrat-ing Speech Technology in Learning), pages 57?61.Jared Bernstein, Alistair Van Moere, and Jian Cheng.2010.
Validating automated speaking tests.
LanguageTesting, 27(3):355?377.Craig Chaudron, Matthew Prior, and Ulrich Kozok.2005.
Elicited imitation as an oral proficiency mea-sure.
Paper presented at the 14th World Congress ofApplied Linguistics, Madison, WI.Carl Christensen, Ross Hendrickson, and Deryle Lons-dale.
2010.
Principled construction of elicited imita-tion tests.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh Conferenceon International Language Resources and Evaluation(LREC?10), Valletta, Malta.
European Language Re-sources Association (ELRA).Tobias Cincarek, Rainer Gruhn, Christian Hacker, ElmarNth, and Satoshi Nakamura.
2009.
Automatic pro-nunciation scoring of words and sentences indepen-dent from the non-native?s first language.
ComputerSpeech and Language, 23(1):65 ?
88.Martin Fujiki and Bonnie Brinton.
1987.
Elicited imi-tation revisited: A comparison with spontaneous lan-guage production.
Language, Speech, and HearingServices in the Schools, 18(4):301?311.C.
Ray Graham, Deryle Lonsdale, Casey Kennington,Aaron Johnson, and Jeremiah McGhee.
2008.
ElicitedImitation as an Oral Proficiency Measure with ASRScoring.
In Proceedings of the 6th International Con-ference on Language Resources and Evaluation, pages1604?1610, Paris, France.
European Language Re-sources Association.Grant Henning.
1983.
Oral proficiency testing: compar-ative validities of interview, imitation, and completionmethods.
Language Learning, 33(3):315?332.Pieter Mu?ller, Febe de Wet, Christa van der Walt, andThomas Niesler.
2009.
Automatically assessing theoral proficiency of proficient L2 speakers.
In Proceed-ings of the ISCA Workshop on Speech and LanguageTechnology in Education (SLaTE), Warwickshire, UK.Yasuyo Tomita, Watuaru Suzuki, and Lorena Jessop.2009.
Elicited imitation: Toward valid procedures tomeasure implicit second language grammatical knowl-edge.
TESOL Quarterly, 43(2):345?349.Thora Vinther.
2002.
Elicited imitation: a briefoverview.
International Journal of Applied Linguis-tics, 12(1):54?73.Willie Walker, Paul Lamere, Philip Kwok, Bhiksha Raj,Rita Singh, Evandro Gouvea, Peter Wolf, and JoeWoelfel.
2004.
Sphinx-4: A flexible open sourceframework for speech recognition.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques.
Mor-gan Kaufmann, San Francisco.Klaus Zechner and Xiaoming Xi.
2008.
Towards auto-matic scoring of a test of spoken language with het-erogeneous task types.
In Proceedings of the ThirdWorkshop on Innovative Use of NLP for Building Ed-ucational Applications, pages 98?106.
Association forComputational Linguistics.37
