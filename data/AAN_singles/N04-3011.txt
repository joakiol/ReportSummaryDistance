Use and Acquisition of Semantic Language ModelKuansan Wang Ye-Yi Wang Alex AceroSpeech Technology Group, Microsoft ResearchOne Microsoft Way, Redmond, WA 98052, USAhttp://research.microsoft.com/srgAbstractSemantic language model is a technique thatutilizes the semantic structure of an utteranceto better rank the likelihood of words compos-ing the sentence.
When used in a conversa-tional system, one can dynamically integratethe dialog state and domain semantics into thesemantic language model to better guide thespeech recognizer executing the decodingprocess.
We describe one such application thatemploys semantic language model to copewith spontaneous speech in a robust manner.The semantic language model, though can bemanually crafted without data, can benefitsignificantly from data driven machine learn-ing techniques.
An example based approach isalso described here to demonstrate a viableapproach.1 IntroductionAny spoken language understanding system must dealwith two critical issues: how to accurately infer user?sintention from speech, and how to do it robustly amidstthe prevalent spontaneous speech effects where userswould inevitably stutter, hesitate, and self correct them-selves on a regular basis.
To address these issues, it hasbeen proposed (Miller et al, 1994; Wang, 2000; Esteveet al, 2003) that one can extend the statistical patternrecognition framework commonly used for automaticspeech recognition (ASR) to the spoken language un-derstanding (SLU) problem.
The ?pattern?
to be recog-nized for ASR is a string of word, and for SLU, a treeof semantic objects that represent the domain entitiesand tasks that describe the user?s intention.
As is thecase for ASR where a language model plays the pivotalrole in guiding the recognizer to compose plausiblestring hypotheses, a pattern recognition based SLU re-lies on what is often called the semantic language model(SLM) to detect semantic objects and construct a parsetree from the user?s utterance.
Because the end outcomeis a parse tree, SLM is usually realized using the struc-tured language model techniques so that the semanticstructure of the utterance can be included in modelingthe language (Wang, 2000; Erdogan et al, 2002).In this article, we describe an application of SLM inthe semantic synchronous understanding (SSU) frame-work for multimodal conversational systems.
A key ideaof SSU is to immediately recognize and parse user?sutterance, accepting only speech segments conformingto the prediction of SLM while the user is still speaking.Since the SLM can be updated in real-time during thecourse of interaction, irrelevant expressions, includingthe spontaneous speech, can be gracefully rejectedbased on what makes sense to the dialog context.
In Sec.2, we describe a study on the efficacy of SSU for a mo-bile personal information management (PIM) applica-tion called MiPad (Huang et al, 2000).
The SLM usedthere was manually derived with combined CFG and N-gram (Microsoft, 1999; Wang, 2002) by consulting thestructure of the PIM back end without any user data.Obviously, the linguistic coverage of the SLM can befurther improved with modern data-driven learningtechniques.
In Sec.
3, we describe one such learningtechnique that can utilize the manually crafted model asa bootstrapping template to enrich the SLM when suit-able amount of training data become available.2 SSU MiPad?MiPad is a Web based PIM application that facilitatesmultimodal access to personal email, calendar, and con-tact information.
MiPad users can combine speechcommands with pen gestures to query PIM database,compose or modify email messages or appointments.We recently implemented a version of MiPad in HTMLand SALT, taking the native support of SSU in SALT?
A video demonstration of SSU MiPad is  available fordownload at http://research.microsoft.com/srg/videos/Mi-PadDemo_2Mbit.wmv(Wang, 2002).
Whenever a semantic object is detected,the PIM logic based on the current semantic parse isexecuted and the screen updated accordingly.
The na-ture of SSU insures that the user receives immediatefeedback on the process of SLU, and therefore can re-phrase rejected and correct misrecognized speech seg-ments.
Studies (Wang, 2003) that contrast SSU withconventional turn taking based system show that, be-cause SSU copes with spontaneous speech better, it elic-its longer user utterances and hence fewer sentences areneeded to complete a task.
The highly interactive natureof SSU lends itself to more effective dynamic visualprompting, leading lower out of domain utterances.
SSUalso simplifies the confirmation strategy as every se-mantic object can be implicitly confirmed.
Users haveno trouble dealing with this strategy.
In fact, users natu-rally correct and rephrase based on the immediate feed-back, making their speech even more spontaneous.
Allthese results are statistically significant.
Finally andmost intriguingly, users feel they accomplish tasksfaster in the SSU system even though the through putsfrom both systems are statistically tied.3 SLM LearningSLU utilizes SLM to infer user?s intention from speech.Before sufficient data make it practical to use machinelearning techniques, SLM often has to be developedmanually.
The manual development process is labor-intensive, requires expertise in linguistics and speechunderstanding, and often lacks good coverage because itis hard for a developer to anticipate all possible lan-guage constructions that different users may choose toexpress their minds.
The manually developed model istherefore not robust to extra-grammaticality commonlyfound in spontaneous speech.
An approach to addressthis problem is to employ a robust parser to loosen theconstraints specified in the SLM, which sometimes re-sults in unpredictable system behavior (Wang, 2001).The robust parser approach also mandates a separateunderstanding pass from speech recognition.
The resultstend to be suboptimal since the first pass, optimizingASR word accuracy, does not necessarily lead to ahigher overall SLU accuracy (Wang and Acero, 2003b).We have developed example-based grammar leaningalgorithms to acquire SLM for speech understanding.
Itis shown (Wang and Acero, 2002) that a grammar learn-ing algorithm may result in a semantic context freegrammar that has better coverage than manually au-thored grammar.
It is demonstrated (Wang and Acero,2003a) that a statistical model can also be obtained bythe learning algorithm, and the model itself is robust toextra-grammaticality in spontaneous speech.
Therefore,a robust parser is no longer necessary.
Most impor-tantly, such a statistical SLM can be incorporated di-rectly into the search algorithm for ASR, making asingle pass, joint speech recognition and understandingprocess such as SSU possible.
Because of that, themodel can be trained directly to optimize the under-standing accuracy.
It is shown (Wang and Acero,2003b) that the single pass approach achieved a 17%understanding accuracy improvement even though thereis a signification word error rate increase, suggestingthat optimizing ASR and SLU accuracy may indeed betwo very different businesses after all.ReferencesErdogan H. Sarikaya R. Gao Y. Picheny M. 2002.
Se-mantic structured language models.
Proc.
ICSLP-2002, Denver, CO.Esteve Y. Raymond C. Bechet F. De Mori R. 2003.Conceptual decoding for spoken dialog systems.Proc.
EuroSpeech-2003, Geneva, Switzerland.Huang X. et al MiPad: A next generation PDA proto-type.
Proc.
ICSLP-2000, Beijing, China.Microsoft Corporation.
1999.
Speech Application Pro-gram Interface (SAPI), Version 5.Miller S. Bobrow R. Ingria R. and Schwartz R.  1994.Hidden understanding models of natural language.Proc.
32nd Annual Meeting of ACL, Las Cruces, NM.Wang K.  2000.
A plan-based dialog system with prob-abilistic inferences.
Proc.
ICSLP-2000,  Beijing,China.Wang K. 2002.
SALT: A spoken language understand-ing interface for Web-based multimodal dialog sys-tems.
Proc.
ICSLP-2002, Denver, CO.Wang K. 2003.
Semantic synchronous understandingfor robust spoken language applications.
Proc.ASRU-2003, St. Thomas, Virgin Islands.Wang Y.
2001.
Robust language understanding in Mi-Pad.
Proc.
EuroSpeech-2001.
Aalborg, Demark.Wang Y.  Acero A.
2002.
Evaluation of spoken lan-guage grammar learning in the ATIS domain.
Proc.ICASSP-2002, Orlando, FL.Wang Y.  Acero A.
2003a.
Combination of CFG and N-gram modeling in semantic grammar learning.
Proc.EuroSpeech-2003.
Geneva, Switzerland.Wang Y.  Acero A.
2003b.
Is word error rate a goodindicator for spoken language understanding accu-racy?
Proc.
ASRU-2003,  St. Thomas, Virgin Is-lands.
