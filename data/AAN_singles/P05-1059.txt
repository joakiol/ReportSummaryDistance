Proceedings of the 43rd Annual Meeting of the ACL, pages 475?482,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsStochastic Lexicalized Inversion Transduction Grammar for AlignmentHao Zhang and Daniel GildeaComputer Science DepartmentUniversity of RochesterRochester, NY 14627AbstractWe present a version of Inversion Trans-duction Grammar where rule probabili-ties are lexicalized throughout the syn-chronous parse tree, along with pruningtechniques for efficient training.
Align-ment results improve over unlexicalizedITG on short sentences for which full EMis feasible, but pruning seems to have anegative impact on longer sentences.1 IntroductionThe Inversion Transduction Grammar (ITG) of Wu(1997) is a syntactically motivated algorithm forproducing word-level alignments of pairs of transla-tionally equivalent sentences in two languages.
Thealgorithm builds a synchronous parse tree for bothsentences, and assumes that the trees have the sameunderlying structure but that the ordering of con-stituents may differ in the two languages.This probabilistic, syntax-based approach has in-spired much subsequent reasearch.
Alshawi etal.
(2000) use hierarchical finite-state transducers.In the tree-to-string model of Yamada and Knight(2001), a parse tree for one sentence of a transla-tion pair is projected onto the other string.
Melamed(2003) presents algorithms for synchronous parsingwith more complex grammars, discussing how toparse grammars with greater than binary branchingand lexicalization of synchronous grammars.Despite being one of the earliest probabilisticsyntax-based translation models, ITG remains state-of-the art.
Zens and Ney (2003) found that the con-straints of ITG were a better match to the decod-ing task than the heuristics used in the IBM decoderof Berger et al (1996).
Zhang and Gildea (2004)found ITG to outperform the tree-to-string model forword-level alignment, as measured against humangold-standard alignments.
One explanation for thisresult is that, while a tree representation is helpfulfor modeling translation, the trees assigned by thetraditional monolingual parsers (and the treebankson which they are trained) may not be optimal fortranslation of a specific language pair.
ITG has theadvantage of being entirely data-driven ?
the treesare derived from an expectation maximization pro-cedure given only the original strings as input.In this paper, we extend ITG to condition thegrammar production probabilities on lexical infor-mation throughout the tree.
This model is reminis-cent of lexicalization as used in modern statisticalparsers, in that a unique head word is chosen foreach constituent in the tree.
It differs in that thehead words are chosen through EM rather than de-terministic rules.
This approach is designed to retainthe purely data-driven character of ITG, while givingthe model more information to work with.
By condi-tioning on lexical information, we expect the modelto be able capture the same systematic differences inlanguages?
grammars that motive the tree-to-stringmodel, for example, SVO vs. SOV word order orprepositions vs. postpositions, but to be able to doso in a more fine-grained manner.
The interactionbetween lexical information and word order also ex-plains the higher performance of IBM model 4 overIBM model 3 for alignment.We begin by presenting the probability model inthe following section, detailing how we address is-sues of pruning and smoothing that lexicalization in-troduces.
We present alignment results on a parallelChinese-English corpus in Section 3.4752 Lexicalization of Inversion TransductionGrammarsAn Inversion Transduction Grammar can generatepairs of sentences in two languages by recursivelyapplying context-free bilingual production rules.Most work on ITG has focused on the 2-normalform, which consists of unary production rules thatare responsible for generating word pairs:X ?
e/fand binary production rules in two forms that areresponsible for generating syntactic subtree pairs:X ?
[Y Z]andX ?
?Y Z?The rules with square brackets enclosing the righthand side expand the left hand side symbol into thetwo symbols on the right hand side in the same orderin the two languages, whereas the rules with pointedbrackets expand the left hand side symbol into thetwo right hand side symbols in reverse order in thetwo languages.One special case of ITG is the bracketing ITG thathas only one nonterminal that instantiates exactlyone straight rule and one inverted rule.
The ITG weapply in our experiments has more structural labelsthan the primitive bracketing grammar: it has a startsymbol S, a single preterminal C, and two interme-diate nonterminals A and B used to ensure that onlyone parse can generate any given word-level align-ment, as discussed by Wu (1997) and Zens and Ney(2003).As an example, Figure 1 shows the alignment andthe corresponding parse tree for the sentence pair Jeles vois / I see them using the unambiguous bracket-ing ITG.A stochastic ITG can be thought of as a stochasticCFG extended to the space of bitext.
The indepen-dence assumptions typifying S-CFGs are also validfor S-ITGs.
Therefore, the probability of an S-ITGparse is calculated as the product of the probabili-ties of all the instances of rules in the parse tree.
Forinstance, the probability of the parse in Figure 1 is:P (S ?
A) ?
P (A ?
[CB])?
P (B ?
?CC?)
?
P (C ?
I/Je)?
P (C ?
see/vois) ?
P (C ?
them/les)It is important to note that besides the bottom-level word-pairing rules, the other rules are all non-lexical, which means the structural alignment com-ponent of the model is not sensitive to the lexicalcontents of subtrees.
Although the ITG model caneffectively restrict the space of alignment to makepolynomial time parsing algorithms possible, thepreference for inverted or straight rules only pas-sively reflect the need of bottom level word align-ment.
We are interested in investigating how muchhelp it would be if we strengthen the structural align-ment component by making the orientation choicesdependent on the real lexical pairs that are passed upfrom the bottom.The first step of lexicalization is to associate a lex-ical pair with each nonterminal.
The head word pairgeneration rules are designed for this purpose:X ?
X(e/f)The word pair e/f is representative of the lexicalcontent of X in the two languages.For binary rules, the mechanism of head selectionis introduced.
Now there are 4 forms of binary rules:X(e/f) ?
[Y (e/f)Z]X(e/f) ?
[Y Z(e/f)]X(e/f) ?
?Y (e/f)Z?X(e/f) ?
?Y Z(e/f)?determined by the four possible combinations ofhead selections (Y or Z) and orientation selections(straight or inverted).The rules for generating lexical pairs at the leavesof the tree are now predetermined:X(e/f) ?
e/fPutting them all together, we are able to derive alexicalized bilingual parse tree for a given sentencepair.
In Figure 2, the example in Figure 1 is revisited.The probability of the lexicalized parse is:P (S ?
S(see/vois))?
P (S(see/vois) ?
A(see/vois))?
P (A(see/vois) ?
[CB(see/vois)])?
P (C ?
C(I/Je))476IseethemJe les voisCBCAsee/vois them/lesI/JeSCFigure 1: ITG ExampleIseethemJe les voisS(see/vois)C(see/vois)C(I/Je)CSC(them/les)CB(see/vois)A(see/vois)Figure 2: Lexicalized ITG Example.
see/vois is the headword of both the 2x2 cell and the entire alignment.?
P (B(see/vois) ?
?C(see/vois)C?)?
P (C ?
C(them/les))The factors of the product are ordered to showthe generative process of the most probable parse.Starting from the start symbol S, we first choosethe head word pair for S, which is see/vois in theexample.
Then, we recursively expand the lexical-ized head constituents using the lexicalized struc-tural rules.
Since we are only lexicalizing rather thanbilexicalizing the rules, the non-head constituentsneed to be lexicalized using head generation rulesso that the top-down generation process can proceedin all branches.
By doing so, word pairs can appearat all levels of the final parse tree in contrast with theunlexicalized parse tree in which the word pairs aregenerated only at the bottom.The binary rules are lexicalized rather than bilexi-calized.1 This is a trade-off between complexity andexpressiveness.
After our lexicalization, the numberof lexical rules, thus the number of parameters in thestatistical model, is still at the order of O(|V ||T |),where |V | and |T | are the vocabulary sizes of the1In a sense our rules are bilexicalized in that they conditionon words from both languages; however they do not capturehead-modifier relations within a language.two languages.2.1 ParsingGiven a bilingual sentence pair, a synchronous parsecan be built using a two-dimensional extension ofchart parsing, where chart items are indexed by theirnonterminal X , head word pair e/f if specified, be-ginning and ending positions l,m in the source lan-guage string, and beginning and ending positions i, jin the target language string.
For Expectation Max-imization training, we compute lexicalized insideprobabilities ?
(X(e/f), l,m, i, j), as well as un-lexicalized inside probabilities ?
(X, l,m, i, j), fromthe bottom up as outlined in Algorithm 1.The algorithm has a complexity of O(N4sN4t ),where Ns and Nt are the lengths of source and tar-get sentences respectively.
The complexity of pars-ing for an unlexicalized ITG is O(N3sN3t ).
Lexical-ization introduces an additional factor of O(NsNt),caused by the choice of headwords e and f in thepseudocode.Assuming that the lengths of the source and targetsentences are proportional, the algorithm has a com-plexity of O(n8), where n is the average length ofthe source and target sentences.477Algorithm 1 LexicalizedITG(s, t)for all l,m such that 0 ?
l ?
m ?
Ns dofor all i, j such that 0 ?
i ?
j ?
Nt dofor all e ?
{el+1 .
.
.
em} dofor all f ?
{fi+1 .
.
.
fj} dofor all n such that l ?
n ?
m dofor all k such that i ?
k ?
j dofor all rules X ?
Y Z ?
G do?
(X(e/f), l,m, i, j) += straight rule, where Y is headP ([Y (e/f)Z] | X(e/f)) ??
(Y (e/f), l, n, i, k) ?
?
(Z, n,m, k, j) inverted rule, where Y is head+ P (?Y (e/f)Z?
| X(e/f)) ??
(Y (e/f), n,m, i, k) ?
?
(Z, l, n, k, j) straight rule, where Z is head+ P ([Y Z(e/f)] | X(e/f)) ??
(Y, l, n, i, k) ?
?
(Z(e/f), n,m, k, j) inverted rule, where Z is head+ P (?Y Z(e/f)?
| X(e/f)) ??
(Y, n,m, i, k) ?
?
(Z(e/f), l, n, k, j)end forend forend for word pair generation rule?
(X, l,m, i, j) += P (X(e/f) | X) ??
(X(e/f), l,m, i, j)end forend forend forend for2.2 PruningWe need to further restrict the space of alignmentsspanned by the source and target strings to make thealgorithm feasible.
Our technique involves comput-ing an estimate of how likely each of the n4 cells inthe chart is before considering all ways of buildingthe cell by combining smaller subcells.
Our figureof merit for a cell involves an estimate of both theinside probability of the cell (how likely the wordswithin the box in both dimensions are to align) andthe outside probability (how likely the words out-side the box in both dimensions are to align).
Inincluding an estimate of the outside probability, ourtechnique is related to A* methods for monolingualparsing (Klein and Manning, 2003), although ourestimate is not guaranteed to be lower than com-plete outside probabity assigned by ITG.
Figure 3(a)displays the tic-tac-toe pattern for the inside andoutside components of a particular cell.
We useIBM Model 1 as our estimate of both the inside andoutside probabilities.
In the Model 1 estimate ofthe outside probability, source and target words canalign using any combination of points from the fouroutside corners of the tic-tac-toe pattern.
Thus inFigure 3(a), there is one solid cell (correspondingto the Model 1 Viterbi alignment) in each column,falling either in the upper or lower outside shadedcorner.
This can be also be thought of as squeezingtogether the four outside corners, creating a new cellwhose probability is estimated using IBM Model1.
Mathematically, our figure of merit for the cell(l,m, i, j) is a product of the inside Model 1 proba-bility and the outside Model 1 probability:P (f (i,j) | e(l,m)) ?
P (f(i,j) | e(l,m)) (1)= ?|(l,m)|,|(i,j)|?t?(i,j)?s?
{0,(l,m)}t(ft | es)?
?|(l,m)|,|(i,j)|?t?(i,j)?s?
{0,(l,m)}t(ft | es)478lmi j i jlmi j(a) (b) (c)Figure 3: The tic-tac-toe figure of merit used for pruning bitext cells.
The shaded regions in (a) showalignments included in the figure of merit for bitext cell (l,m, i, j) (Equation 1); solid black cells show theModel 1 Viterbi alignment within the shaded area.
(b) shows how to compute the inside probability of aunit-width cell by combining basic cells (Equation 2), and (c) shows how to compute the inside probabilityof any cell by combining unit-width cells (Equation 3).where (l,m) and (i, j) represent the complementaryspans in the two languages.
?L1,L2 is the probabilityof any word alignment template for a pair of L1-word source string and L2-word target string, whichwe model as a uniform distribution of word-for-word alignment patterns after a Poisson distributionof target string?s possible lengths, following Brownet al (1993).
As an alternative, the ?
operator canbe replaced by the max operator as the inside opera-tor over the translation probabilities above, meaningthat we use the Model 1 Viterbi probability as ourestimate, rather than the total Model 1 probability.2A na?
?ve implementation would take O(n6) stepsof computation, because there are O(n4) cells, eachof which takes O(n2) steps to compute its Model 1probability.
Fortunately, we can exploit the recur-sive nature of the cells.
Let INS(l,m, i, j) denotethe major factor of our Model 1 estimate of a cell?sinside probability,?t?(i,j)?s?
{0,(l,m)} t(ft | es).
Itturns out that one can compute cells of width one(i = j) in constant time from a cell of equal widthand lower height:INS(l,m, j, j) =?t?(j,j)?s?
{0,(l,m)}t(ft | es)=?s?
{0,(l,m)}t(fj | es)= INS(l,m?
1, j, j)+ t(fj | em) (2)Similarly, one can compute cells of width greaterthan one by combining a cell of one smaller width2The experimental difference of the two alternatives wassmall.
For our results, we used the max version.with a cell of width one:INS(l,m, i, j) =?t?(i,j)?s?
{0,(l,m)}t(ft | es)=?t?
(i,j)INS(l,m, t, t)= INS(l,m, i, j ?
1)?
INS(l,m, j, j) (3)Figure 3(b) and (c) illustrate the inductive compu-tation indicated by the two equations.
Each of theO(n4) inductive steps takes one additive or mul-tiplicative computation.
A similar dynammic pro-graming technique can be used to efficiently com-pute the outside component of the figure of merit.Hence, the algorithm takes just O(n4) steps to com-pute the figure of merit for all cells in the chart.Once the cells have been scored, there can bemany ways of pruning.
In our experiments, we ap-plied beam ratio pruning to each individual bucket ofcells sharing a common source substring.
We prunecells whose probability is lower than a fixed ratio be-low the best cell for the same source substring.
As aresult, at least one cell will be kept for each sourcesubstring.
We safely pruned more than 70% of cellsusing 10?5 as the beam ratio for sentences up to 25words.
Note that this pruning technique is applica-ble to both the lexicalized ITG and the conventionalITG.In addition to pruning based on the figure of meritdescribed above, we use top-k pruning to limit thenumber of hypotheses retained for each cell.
Thisis necessary for lexicalized ITG because the numberof distinct hypotheses in the two-dimensional ITG479chart has increased to O(N3sN3t ) from O(N2sN2t )due to the choice one of O(Ns) source languagewords and one of O(Nt) target language words asthe head.
We keep only the top-k lexicalized itemsfor a given chart cell of a certain nonterminal Y con-tained in the cell l,m, i, j.
Thus the additional com-plexity of O(NsNt) will be replaced by a constantfactor.The two pruning techniques can work for both thecomputation of expected counts during the trainingprocess and for the Viterbi-style algorithm for ex-tracting the most probable parse after training.
How-ever, if we initialize EM from a uniform distribution,all probabilties are equal on the first iteration, givingus no basis to make pruning decisions.
So, in ourexperiments, we initialize the head generation prob-abilities of the form P (X(e/f) | X) to be the sameas P (e/f | C) from the result of the unlexicalizedITG training.2.3 SmoothingEven though we have controlled the number of pa-rameters of the model to be at the magnitude ofO(|V ||T |), the problem of data sparseness still ren-ders a smoothing method necessary.
We use back-ing off smoothing as the solution.
The probabilitiesof the unary head generation rules are in the form ofP (X(e/f) | X).
We simply back them off to theuniform distribution.
The probabilities of the binaryrules, which are conditioned on lexicalized nonter-minals, however, need to be backed off to the prob-abilities of generalized rules in the following forms:P ([Y (?
)Z] | X(?
))P ([Y Z(?)]
| X(?
))P (?Y (?)Z?
| X(?
))P (?Y Z(?)?
| X(?
))where ?
stands for any lexical pair.
For instance,P ([Y (e/f)Z] | X(e/f)) =(1 ?
?
)PEM ([Y (e/f)Z] | X(e/f))+ ?P ([Y (?
)Z] | X(?))where?
= 1/(1 + Expected Counts(X(e/f)))The more often X(e/f) occurred, the more reli-able are the estimated conditional probabilities withthe condition part being X(e/f).3 ExperimentsWe trained both the unlexicalized and the lexical-ized ITGs on a parallel corpus of Chinese-Englishnewswire text.
The Chinese data were automati-cally segmented into tokens, and English capitaliza-tion was retained.
We replaced words occurring onlyonce with an unknown word token, resulting in aChinese vocabulary of 23,783 words and an Englishvocabulary of 27,075 words.In the first experiment, we restricted ourselves tosentences of no more than 15 words in either lan-guage, resulting in a training corpus of 6,984 sen-tence pairs with a total of 66,681 Chinese words and74,651 English words.
In this experiment, we didn?tapply the pruning techniques for the lexicalized ITG.In the second experiment, we enabled the pruningtechniques for the LITG with the beam ratio for thetic-tac-toe pruning as 10?5 and the number k for thetop-k pruning as 25.
We ran the experiments on sen-tences up to 25 words long in both languages.
Theresulting training corpus had 18,773 sentence pairswith a total of 276,113 Chinese words and 315,415English words.We evaluate our translation models in terms ofagreement with human-annotated word-level align-ments between the sentence pairs.
For scoring theViterbi alignments of each system against gold-standard annotated alignments, we use the alignmenterror rate (AER) of Och and Ney (2000), which mea-sures agreement at the level of pairs of words:AER = 1 ?
|A ?GP | + |A ?GS ||A| + |GS |where A is the set of word pairs aligned by theautomatic system, GS is the set marked in thegold standard as ?sure?, and GP is the set markedas ?possible?
(including the ?sure?
pairs).
In ourChinese-English data, only one type of alignmentwas marked, meaning that GP = GS .In our hand-aligned data, 20 sentence pairs areless than or equal to 15 words in both languages,and were used as the test set for the first experiment,and 47 sentence pairs are no longer than 25 words ineither language and were used to evaluate the pruned480AlignmentPrecision Recall Error RateIBM Model 1 .59 .37 .54IBM Model 4 .63 .43 .49ITG .62 .47 .46Lexicalized ITG .66 .50 .43Table 1: Alignment results on Chinese-English corpus (?
15 words on both sides).
Full ITG vs. Full LITGAlignmentPrecision Recall Error RateIBM Model 1 .56 .42 .52IBM Model 4 .67 .43 .47ITG .68 .52 .40Lexicalized ITG .69 .51 .41Table 2: Alignment results on Chinese-English corpus (?
25 words on both sides).
Full ITG vs. PrunedLITGLITG against the unlexicalized ITG.A separate development set of hand-aligned sen-tence pairs was used to control overfitting.
The sub-set of up to 15 words in both languages was used forcross-validating in the first experiment.
The subsetof up to 25 words in both languages was used for thesame purpose in the second experiment.Table 1 compares results using the full (unpruned)model of unlexicalized ITG with the full model oflexicalized ITG.The two models were initialized from uniformdistributions for all rules and were trained until AERbegan to rise on our held-out cross-validation data,which turned out to be 4 iterations for ITG and 3iterations for LITG.The results from the second experiment are shownin Table 2.
The performance of the full model of un-lexicalized ITG is compared with the pruned modelof lexicalized ITG using more training data and eval-uation data.Under the same check condition, we trained ITGfor 3 iterations and the pruned LITG for 1 iteration.For comparison, we also included the results fromIBM Model 1 and Model 4.
The numbers of itera-tions for the training of the IBM models were cho-sen to be the turning points of AER changing on thecross-validation data.4 DiscussionAs shown by the numbers in Table 1, the full lexical-ized model produced promising alignment results onsentence pairs that have no more than 15 words onboth sides.
However, due to its prohibitive O(n8)computational complexity, our C++ implementationof the unpruned lexicalized model took more than500 CPU hours, which were distributed over multi-ple machines, to finish one iteration of training.
Thenumber of CPU hours would increase to a point thatis unacceptable if we doubled the average sentencelength.
Some type of pruning is a must-have.
Ourpruned version of LITG controlled the running timefor one iteration to be less than 1200 CPU hours, de-spite the fact that both the number of sentences andthe average length of sentences were more than dou-bled.
To verify the safety of the tic-tac-toe pruningtechnique, we applied it to the unlexicalized ITG us-ing the same beam ratio (10?5) and found that theAER on the test data was not changed.
However,whether or not the top-k lexical head pruning tech-nique is equally safe remains a question.
One no-ticeable implication of this technique for training isthe reliance on initial probabilities of lexical pairsthat are discriminative enough.
The comparison ofresults for ITG and LITG in Table 2 and the fact thatAER began to rise after only one iteration of train-ing seem to indicate that keeping few distinct lex-ical heads caused convergence on a suboptimal set481of parameters, leading to a form of overfitting.
Incontrast, overfitting did not seem to be a problem forLITG in the unpruned experiment of Table 1, despitethe much larger number of parameters for LITG thanfor ITG and the smaller training set.We also want to point out that for a pair of longsentences, it would be hard to reflect the inherentbilingual syntactic structure using the lexicalized bi-nary bracketing parse tree.
In Figure 2, A(see/vois)echoes IP (see/vois) and B(see/vois) echoesV P (see/vois) so that it means IP (see/vois) is notinverted from English to French but its right childV P (see/vois) is inverted.
However, for longer sen-tences with more than 5 levels of bracketing and thesame lexicalized nonterminal repeatedly appearingat different levels, the correspondences would be-come less linguistically plausible.
We think the lim-itations of the bracketing grammar are another rea-son for not being able to improve the AER of longersentence pairs after lexicalization.The space of alignments that is to be consideredby LITG is exactly the space considered by ITGsince the structural rules shared by them define thealignment space.
The lexicalized ITG is designedto be more sensitive to the lexical influence on thechoices of inversions so that it can find better align-ments.
Wu (1997) demonstrated that for pairs ofsentences that are less than 16 words, the ITG align-ment space has a good coverage over all possibili-ties.
Hence, it?s reasonable to see a better chanceof improving the alignment result for sentences lessthan 16 words.5 ConclusionWe presented the formal description of a StochasticLexicalized Inversion Transduction Grammar withits EM training procedure, and proposed speciallydesigned pruning and smoothing techniques.
Theexperiments on a parallel corpus of Chinese and En-glish showed that lexicalization helped for aligningsentences of up to 15 words on both sides.
The prun-ing and the limitations of the bracketing grammarmay be the reasons that the result on sentences of upto 25 words on both sides is not better than that ofthe unlexicalized ITG.Acknowledgments We are very grateful to Re-becca Hwa for assistance with the Chinese-Englishdata, to Kevin Knight and Daniel Marcu for theirfeedback, and to the authors of GIZA.
This workwas partially supported by NSF ITR IIS-09325646and NSF ITR IIS-0428020.ReferencesHiyan Alshawi, Srinivas Bangalore, and Shona Douglas.2000.
Learning dependency translation models as col-lections of finite state head transducers.
Computa-tional Linguistics, 26(1):45?60.Adam Berger, Peter Brown, Stephen Della Pietra, Vin-cent Della Pietra, J. R. Fillett, Andrew Kehler, andRobert Mercer.
1996.
Language translation apparatusand method of using context-based tanslation models.United States patent 5,510,981.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Dan Klein and Christopher D. Manning.
2003.
A* pars-ing: Fast exact viterbi parse selection.
In Proceed-ings of the 2003 Meeting of the North American chap-ter of the Association for Computational Linguistics(NAACL-03).I.
Dan Melamed.
2003.
Multitext grammars and syn-chronous parsers.
In Proceedings of the 2003 Meetingof the North American chapter of the Association forComputational Linguistics (NAACL-03), Edmonton.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of the38th Annual Conference of the Association for Compu-tational Linguistics (ACL-00), pages 440?447, HongKong, October.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of the39th Annual Conference of the Association for Com-putational Linguistics (ACL-01), Toulouse, France.Richard Zens and Hermann Ney.
2003.
A comparativestudy on reordering constraints in statistical machinetranslation.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics,Sapporo, Japan.Hao Zhang and Daniel Gildea.
2004.
Syntax-basedalignment: Supervised or unsupervised?
In Proceed-ings of the 20th International Conference on Compu-tational Linguistics (COLING-04), Geneva, Switzer-land, August.482
