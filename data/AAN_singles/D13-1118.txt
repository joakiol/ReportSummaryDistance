Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1180?1190,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsImprovements to the Bayesian Topic N -gram ModelsHiroshi Noji?
?noji@nii.ac.jpDaichi Mochihashi?
?daichi@ism.ac.jp?Graduate University for Advanced Studies?National Institute of Informatics, Tokyo, Japan?The Institute of Statistical Mathematics, Tokyo, JapanYusuke Miyao?
?yusuke@nii.ac.jpAbstractOne of the language phenomena that n-gramlanguage model fails to capture is the topic in-formation of a given situation.
We advance theprevious study of the Bayesian topic languagemodel by Wallach (2006) in two directions:one, investigating new priors to alleviate thesparseness problem caused by dividing all n-grams into exclusive topics, and two, develop-ing a novel Gibbs sampler that enables movingmultiple n-grams across different documentsto another topic.
Our blocked sampler canefficiently search for higher probability spaceeven with higher order n-grams.
In terms ofmodeling assumption, we found it is effectiveto assign a topic to only some parts of a docu-ment.1 IntroductionN -gram language model is still ubiquitous in NLP,but due to its simplicity it fails to capture some im-portant aspects of language, such as difference ofword usage in different situations, sentence levelsyntactic correctness, and so on.
Toward languagemodel that can consider such a more global con-text, many extensions have been proposed fromlexical pattern adaptation, e.g., adding cache (Je-linek et al 1991) or topic information (Gildea andHofmann, 1999; Wallach, 2006), to grammaticalityaware models (Pauls and Klein, 2012).Topic language models are important for use ine.g., unsupervised language model adaptation: wewant a language model that can adapt to the do-main or topic of the current situation (e.g., a doc-ument in SMT or a conversation in ASR) automat-ically and select the appropriate words using bothtopic and syntactic context.
Wallach (2006) is onesuch model, which generate each word based on lo-cal context and global topic information to capturethe difference of lexical usage among different top-ics.However, Wallach?s experiments were limited tobigrams, a toy setting for language models, and ex-periments with higher-order n-grams have not yetbeen sufficiently studied, which we investigate inthis paper.
In particular, we point out the two funda-mental problems caused when extending Wallach?smodel to a higher-order: sparseness caused by di-viding all n-grams into exclusive topics, and localminima caused by the deep hierarchy of the model.On resolving these problems, we make several con-tributions to both computational linguistics and ma-chine learning.To address the first problem, we investigate incor-porating a global language model for ease of sparse-ness, along with some priors on a suffix tree to cap-ture the difference of topicality for each context,which include an unsupervised extension of the dou-bly hierarchical Pitman-Yor language model (Woodand Teh, 2009), a Bayesian generative model for su-pervised language model adaptation.
For the sec-ond inference problem, we develop a novel blockedGibbs sampler.
When the number of topics is Kand vocabulary size is V , n-gram topic model hasO(KV n) parameters, which grow exponentially ton, making the local minima problem even more se-vere.
Our sampler resolves this problem by movingmany customers in the hierarchical Chinese restau-rant process at a time.We evaluate various models by incremental cal-culation of test document perplexity on 3 types ofcorpora having different size and diversity.
By com-bining the proposed prior and the sampling method,our Bayesian model achieve much higher accura-cies than the naive extension of Wallach (2006) andshows results competitive with the unigram rescal-ing (Gildea and Hofmann, 1999), which require1180huge computational cost at prediction, with muchfaster prediction time.2 Basic ModelsAll models presented in this paper are based on theBayesian n-gram language model, the hierarchicalPitman-Yor process language model (HPYLM).
Inthe following, we first introduce the HPYLM, andthen discuss the topic model extension of Wallach(2006) with HPYLM.2.1 HPYLMLet us first define some notations.
W is a vocabularyset, V = |W | is the size of that set, and u, v, w ?Wrepresent the word type.The HPYLM is a Bayesian treatment of the n-gram language model.
The generative story startswith the unigram word distribution G?, which isa V -dimensional multinomial where G?
(w) repre-sents the probability of word w. The model firstgenerates this distribution from the PYP as G?
?PYP(a, b,G0), where G0 is a V -dimensional uni-form distribution (G0(u) = 1V ;?u ?
W ) andacts as a prior for G?
and a, b are hyperparameterscalled discount and concentration, respectively.
Itthen generates all bigram distributions {Gu}u?W asGu ?
PYP(a, b,G?).
Given this distributions, itsuccessively generates 3-gram distributions Guv ?PYP(a, b,Gu) for all (u, v) ?
W 2 pairs, whichencode a natural assumption that contexts havingcommon suffix have similar word distributions.
Forexample, two contexts ?he is?
and ?she is?, whichshare the suffix ?is?, are generated from the same(bigram) distribution Gis, so they would have simi-lar word distributions.
This process continues untilthe context length reaches n ?
1 where n is a pre-specified n-gram order (if n = 3, the above exampleis a complete process).
We often generalize this pro-cess using two contexts h and h?
asGh ?
PYP(a, b,Gh?
), (1)where h = ah?, in which a is a leftmost word of h.We are interested in the posterior word distribu-tion following a context h. Our training corpus wis a collection of n-grams, from which we can cal-culate the posterior p(w|h,w), which is often ex-plained with the Chinese restaurant process (CRP):p(w|h,w) = chw ?
athwch?
+ b+ath?
+ bch?
+ bp(w|h?,w),(2)where chw is an observed count of n-gram hw calledcustomers, while thw is a hidden variable called ta-bles.
ch?
and th?
represents marginal counts: ch?
=?w chw and th?
=?w thw.
This form is verysimilar to the well-known Kneser-Ney smoothing,and actually the Kneser-Ney can be understood as aheuristic approximation of the HPYLM.
This char-acteristic enables us to build the state-of-the-art lan-guage model into a more complex generative model.2.2 Wallach (2006) with HPYLMWallach (2006) is a generative model for a docu-ment collection that combines the topic model witha Bayesian n-gram language model.
The latentDirichlet alcation (LDA) (Blei et al 2003) is themost basic topic model, which generates each wordin a document based on a unigram word distributiondefined by a topic allocated to that word.
The bi-gram topic model of Wallach (2006) simply replacesthis unigram word distribution (a multinomial) foreach topic with a bigram word distribution 1.
Inother words, ordinary LDA generates word condi-tioning only on the latent topic, whereas the bigramtopic model generates conditioning on both the la-tent topic and the previous word, as in the bigramlanguage model.
Extending this model with a higherorder n-gram is trivial; all we have to do is to replacethe bigram language model for each topic with an n-gram language model.The formal description of the generative story ofthis n-gram topic model is as follows.
First, foreach topic k ?
1, ?
?
?
,K, where K is the num-ber of topics, the model generates an n-gram lan-guage model Gkh.2 These n-gram models are gen-erated by the PYP, so Gkh ?
PYP(a, b,Gkh?)
holds.The model then generate a document collection.
Foreach document j ?
1, ?
?
?
, D, it generates a K-1This is the model called prior 2 in Wallach (2006); it con-sistently outperformed the other prior.
Wallach used the Dirich-let language model as each topic, but we only explore the modelwith HPYLM because its superiority to the Dirichlet languagemodel has been well studied (Teh, 2006b).2We sometimes denote Gkh to represent a language model oftopic k, not a specific multinomial for some context h, depend-ing on the context.1181dimensional topic distribution ?j by a Dirichlet dis-tribution Dir(?)
where ?
= (?1, ?2, ?
?
?
, ?K) is aprior.
Finally, for each word position i ?
1, ?
?
?
, Njwhere Nj is the number of words in document j, i-th word?s topic assignment zji is chosen accordingto ?j , then a word type wji is generated from Gzjihjiwhere hji is the last n?
1 words preceding wji.
Wecan summarize this process as follows:1.
Generate topics:For each h ?
?, {W}, ?
?
?
, {W}n?1:For each k ?
1, ?
?
?
,K:Gkh ?
PYP(a, b,Gkh?)2.
Generate corpora:For each document j ?
1, ?
?
?D:?j ?
Dir(?
)For each word position i ?
1, ?
?
?
, Nj :zji ?
?jwji ?
Gzjihji3 Extended ModelsOne serious drawback of the n-gram topic modelpresented in the previous section is sparseness.
Atinference, as in LDA, we assign each n-gram a topic,resulting in an exclusive clustering of n-grams inthe corpora.
Roughly speaking, when the numberof topics is K and the number of all n-grams in thetraining corpus is N , a language model of topic k,Gkh is learned using only about O(N/K) instancesof the n-grams assigned the topic k, making eachGkh much sparser and unreliable distribution.One way to alleviate this problem is to place an-other n-gram model, say G0h, which is shared withall topic-specific n-gram models {Gkh}Kk=1.
How-ever, what is the best way to use this special distribu-tion?
We explore two different approaches to incor-porate this distribution in the model presented in theprevious section.
In one model, the HIERARCHICALmodel, G0h is used as a prior for all other n-grammodels, where G0h exploits global statistics acrossall topics {Gkh}.
In the other model, the SWITCH-ING model, no statistics are shared across G0h and{Gkh}, but some words are directly generated fromG0h regardless of the topic distribution.3.1 HIERARCHICAL ModelInformally, what we want to do is to establish hier-archies among the global G0h and other topics {Gkh}.In Bayesian formalism, we can explain this using an?????????????????
?Figure 1: Variable dependencies of the HIERARCHICALmodel.
{u, v} are word types, k is a topic and each Gkhis a multinomial word distribution.
For example, G2uvrepresents a word distribution following the context uvin topic 2.abstract distribution F as Gkh ?
F(G0h).
The prob-lem here is making the appropriate choice for thedistribution F .
Each topic word distribution alreadyhas hierarchies among n?
1-gram and n-gram con-texts as Gkh ?
PYP(a, b,Gkh?).
A natural solutionto this problem is the doubly hierarchical Pitman-Yor process (DHPYP) proposed in Wood and Teh(2009).
Using this distribution, the new generativeprocess of Gkh isGkh ?
PYP(a, b, ?Gkh?
+ (1?
?
)G0h), (3)where ?
is a new hyperparameter that determinesmixture weight.
The dependencies among G0h and{Gkh} are shown in Figure 1.
Note that the genera-tive process of G0h is the same as the HPYLM (1).Let us clarify the DHPYP usage differences be-tween our model and the previous work of Wood andTeh (2009).
A key difference is the problem setting:Wood and Teh (2009) is aimed at the supervisedadaptation of a language model for a specific do-main, whereas our goal is unsupervised adaptation.In Wood and Teh (2009), each Gkh for k ?
1, 2, ?
?
?corresponds to a language model of a specific do-main and the training corpus for each k is pre-specified and fixed.
For ease of data sparseness ofdomain-specific corpora, latent model G0h exploitsshared statistics amongGkh for k = 1, 2, ?
?
?
.
In con-trast, with our model, each Gkh is a topic, so it mustperform the clustering of n-grams in addition to ex-1182ploiting the latent G0h.
This makes inference harderand requires more careful design of ?.Modeling of ?
We can better understand the roleof ?
in (3) by considering the posterior predictiveform corresponds to (2), which is written asp(w|h, k,w) = ckhw ?
atkhwckh?
+ b+atkh?
+ bckh?
+ bq(w|h, k,w),(4)q(w|h, k,w) = ?p(w|h?, k,w) + (1?
?
)p(w|h, 0,w),where c, t with superscript k corresponds to thecount existing in topic k. This shows us that ?
de-termines the back-off behavior: which probabilitywe should take into account: the shorter context ofthe same topic Gkh?
or the full context of the globalmodel G0h.
Wood and Teh (2009) shares this vari-able across all contexts of the same length, for eachk, but this assumption may not be the best.
For ex-ample, after the context ?in order?, we can predictthe word ?to?
or ?that?, and this tendency is unaf-fected by the topic.
We call this property of contextthe topicality and say that ?in order?
has weak topi-cality.
Therefore, we place ?
as a distinct value foreach context h, which we share across all topics.
Wedesignate this ?
determined by h ?h in the follow-ing.
Moreover, similar contexts may have similarvalues of ?h.
For example, the two contexts ?of the?and ?in the?, which share the suffix ?the?, both havea strong topicality3.
We encode this assumption byplacing hierarchical Beta distributions on the suffixtree across all topics:?h ?
Beta(??h?
, ?(1?
?h?))
= DP(?, ?h?
), (5)where DP is the hierarchical Dirichlet process (Tehet al 2006), which has only two atoms in {0,1} and?
is a concentration parameter.
As in HPYLM, weplace a uniform prior ?0 = 1/2 on the base distribu-tion of the top node (??
?
DP(?, ?0)).Having generated the topic component of themodel, the corpus generating process is the same asthe previous model because we only change the gen-erating process of Gkh for k = 1, ?
?
?
,K.3These words can be used very differently depending on thecontext.
For example, in a teen story, ?in the room?
or ?in theschool?
seems more dominant than ?in the corpora?
or ?in thetopic?, which is likely to appear in this paper.3.2 SWITCHING ModelOur second extension also exploits the globalG0h, al-beit differently than the HIERARCHICAL model.
Inthis model, the relationship of G0h to the other {Gkh}is flat, not hierarchical: G0h is a special topic that cangenerate a word.
The model first generates each lan-guage model of k = 0, 1, 2, ?
?
?
,K independentlyas Gkh ?
PYP(a, b,Gkh?).
When generating a word,it first determines whether to use global model G0hor topic model {Gkh}Kk=1.
Here, we use the ?h in-troduced above in a similar way: the probability ofselecting k = 0 for the next word is determined bythe previous context.
This assumption seems natu-ral; we expect theG0h to mainly generate common n-grams, and the topicality of each context determineshow common that n-gram might be.
The completegenerative process of this model is written as fol-lows:1.
Generate topics:For each h ?
?, {V }, ?
?
?
, {V }n?1:?h ?
DP(?, ?
?h)For each k ?
0, ?
?
?
,K:Gkh ?
PYP(a, b,Gkh?)2.
Generate corpora:For each document j ?
1, ?
?
?D:?j ?
Dir(?
)For each word position i ?
1, ?
?
?
, Nj :lji ?
Bern(?hji)If lji = 0: zji = 0If lji = 1: zji ?
?jwji ?
GzjihjiThe difference between the two models is theirusage of the global model G0h.
For a better under-standing of this, we provide a comparison of theirgraphical models in Figure 2.4 InferenceFor posterior inference, we use the collapsed Gibbssampler.
In our models, all the latent variables are{Gkh, ?h, ?j , z,?
}, where z is the set of topic assign-ments and ?
= {a, b, ?,?}
are hyperparameters,which are treated later.
We collapse all multinomialsin the model, i.e., {Gkh, ?h, ?j}, in which Gkh and ?hare replaced with the Chinese restaurant process ofPYP and DP respectively.
Given the training corpusw, the target posterior distribution is p(z,S|w,?
),where S is the set of seating arrangements of allrestaurants.
To distinguish the two types of restau-rant, in the following, we refer the restaurant to indi-1183(a) HIERARCHICAL (b) SWITCHINGFigure 2: Graphical model representations of our two models in the case of a 3-gram model.
Edges that only exist inone model are colored.cate the collapsed state of Gkh (PYP), while we referthe restaurant of ?h to indicates the collapsed stateof ?h (DP).
We present two different types of sam-pler: a token-based sampler and a table-based sam-pler.
For both samplers, we first explain in the caseof our basic model (Section 2.2), and later discusssome notes on our extended models.4.1 Token-based SamplerThe token-based sampler is almost identical tothe collapsed sampler of the LDA (Griffiths andSteyvers, 2004).
At each iteration, we consider thefollowing conditional distribution of zji given allother topic assignments z?ji and S?ji, which is theset of seating arrangements with a customer corre-sponds to wji removed, asp(zji|z?ji,S?ji) ?
p(zji|z?ji)p(wji|zji, hji,S?ji),(6)where p(wji|zji, hji,S?ji) =ckhw ?
atkhwckh?
+ b+atkh?
+ bckh?
+ bp(wji|zji, hji,S?ji) (7)is a predictive word probability under the topic zji,andp(zji|z?ji) =n?jijk + ?kNj ?
1 +?k?
?k?, (8)where n?jijk is the number of words that is assignedtopic k in document j excluding wji, which is thesame as the LDA.
Given the sampled topic zji, weupdate the language model of topic zji, by addingcustomer wji to the restaurant specified by zji andcontext hji.
See Teh (2006a) for details of these cus-tomer operations.HIERARCHICAL Adding customer operation isslightly changed: When a new table is added to arestaurant, we must track the label l ?
{0, 1} indi-cating the parent restaurant of that table, and add thecustomer corresponding to l to the restaurant of ?h.See Wood and Teh (2009) for details of this opera-tion.SWITCHING We replace p(zji|z?ji) withp(zji|z?ji) =??
?p(lji = 0|hji) (zji = 0)p(lji = 1|hji) ?n?jijk +?k?k 6=0 n?jijk +?k?
?k?
(zji 6= 0),(9)where p(lji|hji) is a predictive of lji given by theCRP of ?hji .
We need not assign a label to a newtable, but rather we always add a customer to therestaurant of ?h according to whether the sampledtopic is 0 or not.4.2 Table-based SamplerOne problem with the token-based sampler is thatthe seating arrangement of the internal restaurantwould never be changed unless a new table is cre-ated (or an old table is removed) in its child restau-rant.
This probability is very low, particularly inthe restaurants of shallow depth (e.g., unigram or1184vConstruct a blockMove the block to the sampled topic: customer: tableFigure 3: Transition of the state of restaurants in thetable-based sampler when the number of topics is 2.
{u, v, w} are word types.
Each box represents a restau-rant where the type in the upper-right corner indicates thecontext.
In this case, we can change the topic of the three3-grams (vvw, vvw, uvw) in some documents from 1 to2 at the same time.bigram restaurants) because these restaurants havea larger number of customers and tables than thoseof deep depth, leading to get stack in undesirablelocal minima.
For example, imagine a table inthe restaurant of context ?hidden?
(depth is 2) andsome topic, served ?unit?.
This table is connectedto tables in its child restaurants corresponding tosome 3-grams (e.g., ?of hidden unit?
or ?train hid-den unit?
), whereas similar n-grams, such as thoseof ?of hidden units?
or ?train hidden units?
mightbe gathered in another topic, but collecting these n-grams into the same topic might be difficult underthe token-based sampler.
The table-based samplermoves those different n-grams having common suf-fixes jointly into another topic.Figure 3 shows a transition of state by the table-based sampler and Algorithm 4.2 depicts a high-level description of one iteration.
First, we selecta table in a restaurant, which is shown with a dottedline in the figure.
Next, we descend the tree to col-lect the tables connected to the selected table, whichare pointed by arrows.
Because this connection can-not be preserved in common data structures for arestaurant described in Teh (2006a) or Blunsom etal.
(2009), we select the child tables randomly.
Thisis correct because customers in CRP are exchange-Algorithm 1 Table-based samplerfor all table in all restaurants doRemove a customer from the parent restaurant.Construct a block of seating arrangement S by de-scending the tree recursively.Sample topic assignment zS ?
p(zS |S,S?S , z?S).Move S to sampled topic, and add a customer to theparent restaurant of the first selected table.end forable, so we can restore the parent-child relations ar-bitrarily.
We continue this process recursively untilreaching the leaf nodes, obtaining a block of seat-ing arrangement S. After calculating the conditionaldistribution, we sample new topic assignment forthis block.
Finally, we move this block to the sam-pled topic, which potentially changes the topic ofmany words across different documents, which areconnected to customers in a block at leaf nodes (thisconnection is also arbitrary).Conditional distribution Let zS be the block oftopic assignments connected to S and zS be a vari-able indicating the topic assignment.
Thanks to theexchangeability of all customers and tables in onerestaurant (Teh, 2006a), we can imagine that cus-tomers and tables in S have been added to the restau-rants last.
We are interested in the following condi-tional distribution: (conditioning ?
is omitted)p(zS = k?|S,S?S , z?S) ?
p(S|S?S , k?
)p(zS = k?|z?S),where p(S|S?S , k?)
is a product of customers?
ac-tions moving to another topic, which can be decom-posed as:p(S|S?S , k?)
= p(w|k?, h)?s?Sp(s|k?)
(10)p(s|k?)
=?ts?1i=0 (b+a(tk?(?s)hsw+i))?csij=1(j?a)(b+ck?(?s)hsw?)cs?(11)?
?ts?1i=0 (b+a(tk?(?s)hsw+i))(b+ck?(?s)hsw?)cs?.
(12)Let us define some notations used above.
Eachs ?
S is a part of seating arrangements in a restau-rant, there being ts tables, i-th of which with csicustomers, with hs as the corresponding context.
Arestaurant of context h and topic k has tkhw tablesserved dish w, i-th of which with ckhwi customers.Superscripts ?s indicate excluding the contribution1185of customers in s, and xn = x(x+1) ?
?
?
(x+n?1)is the ascending factorial.
In (10) p(w|k?, h) is theparent distribution of the first selected table, andthe other p(s|k?)
is the seating arrangement of cus-tomers.
The likelihood for changing topic assign-ments across documents must also be considered,which is p(zS = k?|z?S) and decomposed as:p(zS = k?|z?S) =?j(n?Sjk?+?k?
)nj(S)(N?Sj +?k ?k)nj(S), (13)where nj(S) is the number of word tokens con-nected with S in document j.HIERARCHICAL We skip tables on restaurants ofk = 0, because these tables are all from other topicsand we cannot construct a block.
The effects of ?can be ignored because these are shared by all topics.SWITCHING In the SWITCHING, p(zS = k?|z?S)cannot be calculated in a closed form becausep(lji|hji) in (9) would be changed dynamicallywhen adding customers.
This problem is the sameone addressed by Blunsom and Cohn (2011), and wefollow the same approximation in which, when wecalculate the probability, we fractionally add tablesand customers recursively.4.3 Inference of HyperparametersWe also place a prior on each hyperparameter andsample value from the posterior distribution for ev-ery iteration.
As in Teh (2006a), we set differentvalues of a and b for each depth of PYP, but shareacross all topics and sample values with an auxiliaryvariable method.
We also set different value of ?
foreach depth, on which we place Gamma(1, 1).
Wemake the topic prior ?
asymmetric: ?
= ??0;?
?Gamma(1, 1),?0 ?
Dir(1).5 Related WorkHMM-LDA (Griffiths et al 2005) is a compositemodel of HMM and LDA that assumes the wordsin a document are generated by HMM, where onlyone state has a document-specific topic distribution.Our SWITCHING model can be understood as a lex-ical extension of HMM-LDA.
It models the topical-ity by context-specific binary random variables, notby hidden states.
Other n-gram topic models havefocused mainly on information retrieval.
Wang etmin.
training set test setCorpus appear # types # docs # tokens # docs # tokensBrown 4 19,759 470 1,157,225 30 70,795NIPS 4 22,705 1500 5,088,786 50 167,730BNC 10 33,071 6,162 12,783,130 100 202,994Table 1: Corpus statistics after the pre-processing: Wereplace words appearing less than min.appear times intraining + test documents, or appearing only in a test setwith an unknown token.
All numbers are replaced with#, while punctuations are remained.al.
(2007) is a topic model on automatically seg-mented chunks.
Lindsey et al(2012) extended thismodel with the hierarchical Pitman-Yor prior.
Theyalso used switching variables, but for a different pur-pose: to determine the segmenting points.
They treatthese variables completely independently, while ourmodel employs a hierarchical prior to share statisti-cal strength among similar contexts.Our primary interest is language model adapta-tion, which has been studied mainly in the area ofspeech processing.
Conventionally, this adaptationhas relied on a heuristic combination of two sep-arately trained models: an n-gram model p(w|h)and a topic model p(w|d).
The unigram rescal-ing, which is a product model of these two mod-els, perform better than more simpler models suchas linear interpolation (Gildea and Hofmann, 1999).There are also some extensions to this method (Tamand Schultz, 2009; Huang and Renals, 2008), butthese methods have one major drawback: at predic-tion, the rescaling-based method requires normaliza-tion across vocabulary at each word, which prohibitsuse on applications requiring dynamic (incremental)adaptation, e.g., settings where we have to updatethe topic distribution as new inputs come in.
Tamand Schultz (2005) studied on this incremental set-tings, but they employ an interpolation.
The practi-cal interest here is whether our Bayesian models canrival the rescaling-based method in terms of predic-tion power.
We evaluate this in the next section.6 Experiments6.1 SettingsWe test the effectiveness of presented models andthe blocked sampling method on unsupervised lan-guage model adaptation settings.
Specifically we11860 2 4 6 8time (hr.
)7.3e+067.5e+067.7e+067.9e+068.1e+06negativelog-likelihood(a) Brown0 8 16 24 32time (hr.
)2.9e+073.1e+073.3e+073.5e+073.7e+07negativelog-likelihood(b) NIPS0 15 30 45 60time (hr.
)8.0e+078.3e+078.6e+078.9e+079.2e+07negativelog-likelihood 3-gram Hpytmtoken3-gram Hpytm4-gram Hpytmtoken4-gram Hpytm(c) BNC10 50 100# topics205210215220225230235240245testperplexity(d) Brown10 50 100# topics100105110115120125testperplexity(e) NIPS10 50 100# topics130140150160170180190testperplexityHpylmHpytmtokenHpytmRescalingSwitchingHierarchical(f) BNCFigure 4: (a)?
(c): Comparison of negative log-likelihoods at training of HPYTM (K = 50).
Lower is better.
HPYTMis trained on both token- and table-based samplers, while HPYTMtoken is trained only on the token-based sampler.(d)?
(f): Test perplexity of various 3-gram models as a function of number of topics on each corpus.concentrate on the dynamic adaptation: We updatethe posterior of language model given previously ob-served contexts, which might be decoded transcriptsat that point in ASR or MT.We use three corpora: the Brown, BNC and NIPS.The Brown and BNC are balanced corpora that con-sist of documents of several genres from news toromance.
The Brown corpus comprises 15 cate-gories.
We selected two documents from each cate-gory for the test set, and use other 470 documents forthe training set.
For the NIPS, we randomly select1,500 papers for training and 50 papers for testing.For BNC, we first randomly selected 400 documentsfrom a written corpus and then split each documentinto smaller documents every 100 sentences, leadingto 6,262 documents, from which we randomly se-lected 100 documents for testing, and other are usedfor training.
See Table 1 for the pre-processing ofunknown types and the resulting corpus statistics.For comparison, besides our proposed HIERAR-CHICAL and SWITCHING models, we prepare vari-ous models for baseline.
HPYLM is a n-gram lan-guage model without any topics.
We call the modelwithout the global G0h introduced in Section 2.2HPYTM.
To see the effect of the table-based sam-pler, we also prepare HPYTMtoken, which is trainedonly on the token-based sampler.
RESCALING isthe unigram rescaling.
This is a product model ofan n-gram model p(w|h) and a topic model p(w|d),where we learn each model separately and then com-bine them by:p(w|h, d) ?(p(w|d)p(w))?p(w|h).
(14)We set ?
in (14) to 0.7, which we tuned with theBrown corpus.6.2 Effects of Table-based SamplerWe first evaluate the effects of our blocked sam-pler at training.
For simplicity, we concentrate onthe HPYTM with K = 50.
Table 4(a)?
(c) showsnegative likelihoods of the model during training.On all corpora, the model with the table-based sam-pler reached the higher probability space with muchfaster speed on both 3-gram and 4-gram models.11876.3 Perplexity ResultsTraining For burn-in, we ran the sampler as fol-lows: For HPYLM, we ran 100 Gibbs iterations.
ForRESCALING, we ran 900 iterations on LDA and 100iterations on HPYLM.
For all other models, we ran500 iterations of the Gibbs; HPYTMtoken is trainedonly on the token-based sampler, while for othermodels, the table-based sampler is performed afterthe token-based sampler.Evaluation We have to adapt to the topic dis-tribution of unseen documents incrementally.
Al-though previous works have employed incrementalEM (Gildea and Hofmann, 1999; Tam and Schultz,2005) because their inference is EM/VB-based, weuse the left-to-right method (Wallach et al 2009),which is a kind of particle filter updating the poste-rior topic distribution of a test document.
We set thenumber of particles to 10 and resampled each parti-cle every 10 words for all experiments.
To get thefinal perplexity, after burn-in, we sampled 10 sam-ples every 10 iterations of Gibbs, calculated a testperplexity for each sample, and averaged the results.Comparison of 3-grams Figure 4(d)?
(f) showsperplexities when varying the number of top-ics.
Generally, compared to the HPYTMtoken, theHPYTM got much perplexity gains, which againconfirm the effectiveness of our blocked sampler.Both our proposed models, the HIERARCHICAL andthe SWITCHING, got better performances than theHPYTM, which does not place the global modelG0h.
Our SWITCHING model consistently performedthe best.
The HIERARCHICAL performed somewhatworse than the RESCALING when K become large,but the SWITCHING outperformed that.Comparison of 4-grams and beyond We sum-marize the results with higher order n-grams in Ta-ble 2, where we also show the time for prediction.We fixed the number of topics K = 100 becausewe saw that all models but HPYTMtoken performedbest at K = 100 when n = 3.
Generally, theresults are consistent with those of n = 3.
Themodels with n = ?
indicate a model extensionusing the Bayesian variable-order language model(Mochihashi and Sumita, 2008), which can naturallybe integrated with our generative models.
By thisextension, we can prune unnecessary nodes stochas-NIPS BNCModel n PPL time PPL timeHPYLM 4 117.2 59 169.2 74HPYLM ?
117.9 61 173.1 59RESCALING 4 101.4 19009 130.3 36323HPYTM 4 107.0 1004 133.1 980HPYTM ?
107.2 1346 133.6 1232HIERARCHICAL 4 106.3 1038 129.0 993HIERARCHICAL ?
105.7 1337 129.3 1001SWITCHING 4 100.0 1059 125.5 991SWITCHING ?
100.4 1369 125.7 1006Table 2: Comparison of perplexity and the time requirefor prediction (in seconds).
The number of topics is fixedto 100 on all topic-based models.tically during training.
We can see that this ?-gram did not hurt performances, but the sampledmodel get much more compact; in BNC, the numberof nodes of the SWITCHING with 4-gram is about7.9M, while the one with ?-gram is about 3.9M.Note that our models require no explicit normaliza-tion, thereby drastically reducing the time for pre-diction compared to the RESCALING.
This differ-ence is especially remarkable when the vocabularysize becomes large.We can see that our SWITCHING performed con-sistently better than the HIERARCHICAL.
One rea-son for this result might be the mismatch of pre-diction of the topic distribution in the HIERARCHI-CAL.
The HIERARCHICAL must allocate some (notglobal) topics to every word in a document, so eventhe words to which the SWITCHING might allocatethe global topic (mainly function words; see below)must be allocated to some other topics, causing amismatch of allocations of topic.6.4 Qualitative ResultsTo observe the behavior in which the SWITCHINGallocates some words to the global topic, in Figure5, we show the posterior of allocating the topic 0or not at each word in a part of the NIPS trainingcorpus.
We can see that the model elegantly identi-fied content and function words, learning the topicdistribution appropriately using only semantic con-texts.
These same results in the HIERARCHICAL arepresented in Table 3, where we show some relationsbetween ?h and context h. Contexts that might belikely to precede nouns have a higher value of ?h,1188there has been much recent work on measuring image statisticsand on learning probability distributions on images .
we observethat the mapping from images to statistics is many-to-one andshow it can be quantified by a phase space factor .Figure 5: The posterior for assigning topic 0 or not inNIPS by the ?-gram SWITCHING.
Darker words indi-cate a higher probability of not being assigned topic 0.?h h0.0?0.1 in spite, were unable, a sort, on behalf, .
regardless0.5?0.6 assumed it, rand mines, plans was, other excersises0.9?1.0 that the, the existing, the new, their own, and spatialTable 3: Some contexts h for various values of ?h in-duced by the 3-gram HIERARCHICAL in BNC.while prefixes of idioms have a lower value.
The?-gram extension gives us the posterior of n-gram or-der p(n|h), which can be used to calculate the proba-bility of a word ordering composing a phrase in topick as p(w, n|k, h) ?
p(n|h)p(w|k, n, h).
In Table4, we show some higher probability topic-specificphrases from the model trained on the NIPS.7 ConclusionWe have presented modeling and algorithmic con-tributions to the existing Bayesian n-gram topicmodel.
We explored two different priors to incor-porate a global model, and found the effectivenessof the flat structured model.
We developed a novelblocked Gibbs move for these types of models to ac-celerate inference.
We believe that this Gibbs op-eration can be incorporated with other models hav-ing a similar hierarchical structure.
Empirically, wedemonstrate that by a careful model design and effi-cient inference, a well-defined Bayesian model canrival the conventional heuristics.ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
The Journal of Ma-chine Learning Research, 3:993?1022.Phil Blunsom and Trevor Cohn.
2011.
A hierarchi-cal pitman-yor process hmm for unsupervised part ofspeech induction.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 865?874,Portland, Oregon, USA, June.
Association for Compu-tational Linguistics.0 46according to ?
support vectors?
( # ) in high dimentional?
section # as decision functiontechniques such as set of # observations?
( b ) original data set83 89the hierarchical mixtures ?
linear discriminantthe rbf units images per classthe gating networks multi-class classificationgrown hme ?
decision boundariesthe modular architecture references per classTable 4: Topical phrases from NIPS induced by the ?-gram SWITCHING model.
?
is a symbol for the beginningof a sentence and # represents a number.Phil Blunsom, Trevor Cohn, Sharon Goldwater, and MarkJohnson.
2009.
A note on the implementation of hi-erarchical dirichlet processes.
In Proceedings of theACL-IJCNLP 2009 Conference Short Papers, pages337?340, Suntec, Singapore, August.
Association forComputational Linguistics.Daniel Gildea and Thomas Hofmann.
1999.
Topic-basedlanguage models using em.
In In Proceedings of EU-ROSPEECH, pages 2167?2170.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences of the United States of America,101(Suppl 1):5228?5235.Thomas L. Griffiths, Mark Steyvers, David M. Blei, andJoshua B. Tenenbaum.
2005.
Integrating topics andsyntax.
In In Advances in Neural Information Pro-cessing Systems 17, pages 537?544.
MIT Press.Songfang Huang and Steve Renals.
2008.
Unsupervisedlanguage model adaptation based on topic and role in-formation in multiparty meetings.
In in Proc.
Inter-speech08, pages 833?836.F.
Jelinek, B. Merialdo, S. Roukos, and M. Strauss.
1991.A dynamic language model for speech recognition.
InProceedings of the workshop on Speech and NaturalLanguage, HLT ?91, pages 293?295, Stroudsburg, PA,USA.
Association for Computational Linguistics.Robert Lindsey, William Headden, and Michael Stipice-vic.
2012.
A phrase-discovering topic model using hi-erarchical pitman-yor processes.
In Proceedings of the2012 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, pages 214?222, Jeju Island, Ko-rea, July.
Association for Computational Linguistics.Daichi Mochihashi and Eiichiro Sumita.
2008.
The infi-nite markov model.
In J.C. Platt, D. Koller, Y. Singer,and S. Roweis, editors, Advances in Neural Informa-tion Processing Systems 20, pages 1017?1024.
MITPress, Cambridge, MA.1189Adam Pauls and Dan Klein.
2012.
Large-scale syntac-tic language modeling with treelets.
In Proceedings ofthe 50th Annual Meeting of the Association for Com-putational Linguistics: Long Papers - Volume 1, pages959?968.
Association for Computational Linguistics.Yik-Cheung Tam and Tanja Schultz.
2005.
Dynamic lan-guage model adaptation using variational bayes infer-ence.
In INTERSPEECH, pages 5?8.Yik-Cheung Tam and Tanja Schultz.
2009.
Correlatedbigram lsa for unsupervised language model adapta-tion.
In D. Koller, D. Schuurmans, Y. Bengio, andL.
Bottou, editors, Advances in Neural InformationProcessing Systems 21, pages 1633?1640.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.Yee Whye Teh.
2006a.
A Bayesian Interpretation ofInterpolated Kneser-Ney.
NUS School of ComputingTechnical Report TRA2/06.Yee Whye Teh.
2006b.
A hierarchical bayesian languagemodel based on pitman-yor processes.
In Proceed-ings of the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of the As-sociation for Computational Linguistics, pages 985?992, Sydney, Australia, July.
Association for Compu-tational Linguistics.Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,and David Mimno.
2009.
Evaluation methods fortopic models.
In Proceedings of the 26th Annual In-ternational Conference on Machine Learning, ICML?09, pages 1105?1112, New York, NY, USA.
ACM.Hanna M. Wallach.
2006.
Topic modeling: beyond bag-of-words.
In Proceedings of the 23rd internationalconference on Machine learning, ICML ?06, pages977?984.Xuerui Wang, Andrew McCallum, and Xing Wei.
2007.Topical n-grams: Phrase and topic discovery, with anapplication to information retrieval.
In Proceedingsof the 2007 Seventh IEEE International Conference onData Mining, ICDM ?07, pages 697?702, Washington,DC, USA.
IEEE Computer Society.Frank Wood and Yee Whye Teh.
2009.
A hierarchi-cal nonparametric Bayesian approach to statistical lan-guage model domain adaptation.
In Proceedings of theInternational Conference on Artificial Intelligence andStatistics, volume 12.1190
