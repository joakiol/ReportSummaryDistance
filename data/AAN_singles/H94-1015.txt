SPEECH RECOGNITION USING A STOCHASTIC LANGUAGEMODEL INTEGRATING LOCAL AND GLOBAL CONSTRAINTSRyosuke Isotani, Shoichi MatsunagaATR Interpreting Telecommunications Research LaboratoriesSeika-cho, Soraku-gun, Kyoto 619-02, JapanABSTRACTIn this paper, we propose a new stochastic language model thatintegrates local and global constraints effectively and describe aspeechrecognition system basedon it.
Theproposedlanguagemodeluses the dependencies within adjacent words as local constraints inthe same way as conventional word N-gram models.
To capturethe global constraints between on-contiguous words, we take intoaccount he sequence of the function words and that of the contentwords which are expected to represent, respectively, the syntactic andsemantic relationships between words.
Furthermore, we show thatassuming an independence b tween local- and global constraints, thenumber of parameters to be estimated and stored is greatly reduced.The proposed language model is incorporated into a speech recog-nizer based on the time-synchronous Viterbi decoding algorithm, andcompared with the word bigram model and trigram model.
The pro-posed model gives a better ecognition rate than the bigram model,though slightly worse than the trigram model, with only twice asmany parameters as the bigram model.1.
INTRODUCTIONAt  present, word N-gram models \[1\], especially bigram (N = 2).ortrigram (N = 3) models, are recognized as effective and are widelyused as language models for speech recognition.
Such models, how-ever, represent only local constraints within a few successive wordsand lack the ability to capture global or long distance dependenciesbetween words.
They might represent global constraints if N wereset at a larger value, but it is not only computationally impracticalbut also inefficient because dependencies between on-contiguouswords are often independent of the contents and length of the wordstring between them.
In addition, estimating so many parametersfrom a finite number of text corpora would result in sparseness ofdata.Recently some papers treat long distance factors.
In the long dis-tance bigrams by Huang et al \[2\] a linear combination of distance-dbigrams is used.
All the preceding words in a window of fixedlength are considered, and bigram probabilities are estimated foreach distance d between words respectively.
The extended bigrammodel by Wright et al \[3\] uses a single word selected for each wordaccording to a statistical measure as its "parent."
The extendedbigrams are insensitive to the distance between the word and itsparent, but this model does not utilize multiple information.
Thetrigger pairs described in \[4, 5\] also represent relationships betweennon-contiguous words.
They are also extracted automatically andinsensitive to the distance.
The way of combining the evidence fromtrigger pairs with local constraints ("the static model" in their term)is also given.
But this approach as the disadvantage that it is com-putationaUy expensive.
Another approach is a tree-based model \[6\],which automatically generates a binary decision tree from trainingdata.
Although it could also extract similar dependencies by settingbinary questions appropriately, it has the same disadvantage asthetrigger-based model.We therefore proposed a new language model based on functionword N-grams 1and content word N-grams \[7\].
Global constraintsare captured effectively without significantly increasing computa-tional cost nor number of parameters by utilizing simple linguistictags.
Function wordN-grams are mainly intended for syntactic on-straints, while content word N-grams are for semantic ones.
Wealready showed their effectiveness for Japanese speech recognitionby applying them to sentence candidate selection from phrase lat-tices obtained by a phrase speech recognizer.
We also gave a methodto combine these global constraints with local constraints imilarto conventional bigrams, and demonstrated that it improves perfor-mance.In this paper, we extend and modify this model so that it can beincorporated irectly into the search process in continuous peechrecognition based on the time-synchronous Viterbi decoding algo-rithm.
The new model uses the conventional word N-grams forlocal constraints with N being a small value, and uses function- andcontent word N-grams as global constraints, where N can again besmall.
These constraints are treated statistically in a unified manner.A similar approach is found in \[8\], where, to compute aword proba-bility, the headwords of the two phrases immediately preceding theword are used as well as the last two words.
Our model is differ-ent from this method in that the former also takes function wordsinto consideration, and treats function words and content words sepa-rately in computing the probability to extract more effective syntacticand semantic information, respectively.In the following sections, we first explain the proposed languagemodel, where we also show that the number of parameters can bereduced by assuming an independence between local- and globalconstraints.
Then we describe how i t  is incorporated into the time-synchronous Viterbi decoding algorithm.
Finally, results of speaker-dependent sentence recognition experiments are presented, whereour model is compared with the word bigram and trigram models inthe viewpoints of number of parameters, perplexity, and recognitionrate.2.
LANGUAGE MODELINGLinguistic onstraints between words in a sentence include syntacticones and semantic ones.
The syntactic onstraints are often specifiedby the relationships between the cases of the words or phrases.
Con-1 Previously referred to as "particle N-grams.
"88(a)(b)Kaigi -wa I futsuka -kara I itsuka -made I Kyoto -de Ithe conference (CM) the 2nd from the 5th to Kyoto in(The conference will be held in Kyoto from the 2nd to the 5th.
)Soredewa / tourokuyoushi -o / ookuri -itashi -masu.then the registration form (CM) send (aux.
v.) (aux.
v.)(Then I will send you the registration form.
)Figure 1: Examples of Japanese sentences(CM: case marker, aux.
v.: auxiliary verb)kaisaisare -masu.be held (aux.
v.)sequently, they are expected to be reflected in the sequence of thecases of the words or phrases.
Taking notice that case information ismainly conveyed by function words in Japanese, we consider func-tion word sequences to capture syntactic onstraints while ignoringcontent words in the sentences.
On the contrary, semantic infor-mation is mostly contained in the content words.
Accordingly theidea of content word sequences i  also introduced to extract semanticconstraints.After briefly explaining the roles of the function words and contentwords in Japanese sentences, we will propose a new model, modelI, as an extension of the conventionalN-gram model.
In this model,the relationships between function words and between content wordsare taken into consideration only implicitly.
Then by making someassumptions, model II will be derived as an approximation fmodel I.Model II uses the probabilities of function word N-grams and contentword N-grams directly and may be easier to grasp intuitively.2.1.
Function Words and Content Words inJ apaneseA common Japanese sentence consists of phrases ("bunsetsu"), eachof which typically has one content word and optional function words.Figure 1 shows examples of Japanese sentences.
In the figure, " f 'represents a phrase separator.
Words after " - "  in a phrase are func-tion words and all others are content words 2.
The correspondingEnglish words are given in the figure.
Content words include nouns,verbs, adjectives, adverbs, etc.
Function words are particles andauxiliary verbs.
Japanese particles include case markers such as"ga" (subjective case marker), "o" (objective case marker) as well aswords such as "kara (from)" or "de (in)."
Every word in a sentenceis classified either as a content word or as a function word.Paying attention only to function words and ignoring content wordsin sentences, "kara (from)" often comes before "made (to)" while"ga"s (subjective case markers) rarely appear in succession i a sen-tence.
Thus, a sequence of function words is expected to reflect hesyntactic onstraints of a sentence.
If we consider the content wordsequence instead, such words as "sanka (participate)" or "happyou(give a presentation)" appear more frequently than words such as"okuru (send)" after "kaigi (conference)."
On the other hand, after"youshi (form)," "okuru (send)" comes more frequently.
Like theseexamples, a sequence of content words in a sentence is expected tobe constrained by semantic relationships between words.These kinds of constraints can be described statistically.
To acquirethese global constraints, the proposed language model makes use of2 These marks are for explanation only and never appearin actual Japanesetext.the N-gram probabilities of both function words and content words.2.2.
Proposed Language Model ISuppose a sentence S consists of a word siring wl, w2,... ,  wn, anddenote asubstring Wl, w2,.
.
.
,  wi as w\[.
Then the probability of thesentence S is written asP(S) = P(w1,w2,.. .
,wn) (1)= I Ie (w i lw{-b .
(2)i=lIn conventional word N-gram models, each term of the right handside of expression (2) is approximated as the probability given fora single word based on the final N - 1 words preceding it.
In thebigram model, for example, the foll6wing approximation is adopted:e(wi \[w{ -t) ~ e(wi lwi-1).
(3)The proposed model is an extension of the N-gram model and utilizesthe global constraints represented by function- and content word N-grams as well.
For simplicity, only a single preceding word istaken into account, both for global and local relationships.
Let f iand ci denote the last function word and the last content word inthe substring w{, respectively.
The probability of a word wi givenw{ -1 is, takingfi_l and ci-i into consideration as well as wi-l,represented approximately asfollows:P(wi \[ w1-1) -~ P(wi \[ wi-1, ci-i , f  i-l).
(4)As wi-1 is identical to ci-i or f i _ l ,  it is rewritten ase(wi \[ Wi-1, Ci-1 , f  i-1 )P(wi \ ]wi- l , f  i-1), wi-l: contentword= P(wi \[ wi-1, ci-1), wi-l: function word.
(5)We refer to the model based on equation (5) as "proposed model I.
"Figure 2 shows how the word dependencies are taken into account in"7"',-.c f c f f c c f c: content word.
.~ J  r~ J / 7 f: function wordFigure 2: Word dependency in model I89this model.
The probability of each word in a sentence is determinedby the pleceding content- and function-word pair.
If content wordsand function words appear alternately, this model reduces to thetrigram model.
But when, for example, a function word is precededby more than one content word, the most recent function word isused to predict it instead of the last word but two (wi-2).2.3.
Proposed Model H- -  R, eduction of the Number of ParametersThe following two assumptions are introduced as an approximationto reduce the number of parameters:1.
Mutual information between wi and wi-1 is independent off i-1 if wi-1 is a content word, and independent of ci-i ff wi-1is a function word, i.e., the following approximations hold;l(Wi, Wi-1 If i-1 ) = l(Wi, Wi-1 ) (6)if Wi-1 is a content word, andI(wi, wi-1 \[ Ci - - l )  = l(Wi, Wi-1 ) (7)if Wi-1 is a function word.2.
The appearance of a content word and that of a functionword are mutually independent when they are located non-contiguously in a sentence, i.e.,P(wi If i-l) = e(wi) (8)if wi-1 and wi are content words, andP(wi \[ Ci-l) = P(wi) (9)if wi-~ and Wi are function words.From these approximations, expression (5) is rewritten asP(wi \[ wi-1, ci-i , f  i-1)PL(Wi \[Wi-l)" PG(fi \[f i-OPc(\]'i)wi-1: content word, wi: function word (=fi)= PG(Ci I Ci-1 ) (10)eL(wi \[ Wi-i)" PG(Ci)wi-l: function word, wi: content word (= ci)eL(wi \[ Wi-1) otherwise,where PL and PC represent the probabilities of local and globalconstraints between words.
To be more exact, Pc(f  i) is the prob-ability that the i-th word is f l  knowing that it is a function word,and PG(fi \[f~-l) is the probability that he i-th word isfi given thatthe most recent function word isfi-1 and also knowing that the i-thword is a function word.
Pc(ci) and PG(CilCi-l) are explained inthe same way.
In other words, Pc(') denotes a probability in thefunction (or content) word sequences obtained by extracting onlyfunction (or content) words from sentences.
Notice should be takenthat PG(') is used only when two function (or content) words appearnon-contiguously.
We refer to the model based on equation (10) as"proposed model II.
"This approximate equation shows that the probabilities of words ina sentence are expressed as the product of word bigram probabilitiesand function word (or content word) bigram probabilities, which de-scribe local and global inguistic onstraints, respectively.
The termword bigram probabilities (local constraints)c f  c f f  c c ffunction word bigram / content word bigramprobabilities (global constraints)c: content wordf: function wordFigure 3: Word dependency in model IIPG~i) and Pc(ci) in the denominators can be intuitively interpretedas the compensation forthe probability of word wi being multipliedtwice.Figure 3 shows how the word dependencies are taken into accountin this model.
The probability of each word is determined from theword immediately before it, and also from the preceding word of thesame category (function word or content word) ff the category ofthe word immediately before it is different from that of the currentword.
The first corresponds to the word bigram probability and thelatter corresponds to the function word (or content word) bigramprobability, which are computed independently.
It is easy to extendthis model so as to use a word trigram model or a function word(content word) trigram model.The decomposition f probabilities greatly reduces the number of pa-rameters to be estimated.
The number of parameters in each modelis summarized in Table 1, where V, Vc, Vf is the vocabulary size,the number of content words, and the number of function words, re-spectively (V = Vc + Vf ).
The word trigram model and the proposedmodel I has O(V 3) parameters, while the proposed model II has onlyO(I/2) parameters, which is comparable tothe word bigram model.3.
APPLICATION TO SPEECHRECOGNITIONSince, like N-gram models, the proposed language models areMarkov models, they can easily be incorporated into a speech recog-nition system based on the time-synchronous Viterbi decoding algo-rithm.
They could also be used in rescoring for N-best hypotheses,but it would bring some loss of information.Figure 4 shows the network representation f the language model.Symbols ci, c i, c k represent content words, andf  t, f m, f n repre-sent function words.
Each node of the network is a Markov stateLanguage ModelBigramTrigramProposed (I)Proposed (II)Number of ParametersV2V 32v~vlv++ .V: vocabulary size (= Vc + Vf )Vc: number of content wordsVf : number of function wordsTable 1: Number of parameters of each model90Figure 4: Network representation f the proposed language modelTaskVocabulary SizeSpeakerTest DataInternational conference r gistration1,500 words1 male speaker261 sentences(7.0 words/sentence, on average)Table 2: Experimental conditions for speech recognitionThe proposed model was compared with the word bigram and tri-gram models in their perplexities for test sentences and in sentencerecognition rates.
As for the proposed model I, only perplexitywas calculated.
The ratios of the numbers of parameters were alsocalculated based on Table 1.corresponding to a word pair of either (Ci-l,fi-1) or (fi-1, Ci-1),and each arc is a transition corresponding to a word wl.
In the caseof the trigram model, each node would correspond to a word pair(wi-2, wi-1 ).
Each arc is assigned with a probability value accord-ing to equation (5) (model I) or (10) (model H).
The number of nodesis 2VcVf and the total number of arcs is 2VcVf V for both model Iand model H. In the case of the trigam model, they would be V z andV 3, respectively.Ordinary time-synchronous Viterbi decoding controlled by this net-work is possible.
As the numbers of nodes and arcs are still hugealthough reduced compared with the trigram model, a beam searchis necessary in the decoding process.4.
EXPERIMENTS4.1.
Estimation of Language ModelParametersA 11,000-sentence text database of Japanese conversations concern-ing conference registration was used to train the language models.This database is manually labeled with part of speech tags.
Eachword was classified as a function word or a content word accordingto its part of speech.
The size of the vocabulary is 5,389 words(5,041 content words and 348 function words), where words havingthe same spelling but different pronunciation, or were different partsof speech, were counted as different words.The probability values in the language models were estimated by themaximum likelihood method.
These values were then smoothed us-ing the deleted interpolation method \[9\].
To cope with the unknownword problem, 'zero-gram' probabilities (uniform distribution) werealso used in the interpolation.
In the model II, this interpolation wasapplied to probabilities of local constraints (PL) and those of globalconstraints (Pc), respectively.In the calculation of perplexity for model II, use of the values obtainedby equation (10) does not give the correct perplexity because~ P(wi I wi-l, ci-i , f  i-1) = 1 (11)wldoes not hold due to the approximation.
Therefore the values ofP(wi \] Wi-l, ci-1 ,fi- i  ) were normalized in order to satisfy this equa-tion.
This normalization was done by simply multiplying aconstantvalue found for each combination of (wi-l, ci-1 ,fi-1 ).
It was omit-ted in the recognition experiment for computational reasons.Beam width for recognition was fixed at 6,000 in all cases.
Weightingvalues for the acoustic score and linguistic score were determined bypreliminary experiments.
Common weighting values were used forall models.4.3.
ResultsThe results are shown in Table 4.
The proposed models give lowerperplexities than the bigram model, although not so low as the trigrammodel, which is reflected in the speech recognition accuracy.
Theperplexity of model II is higher than that of model I, which we thinkis caused by the approximation used to derive model II, but thesmallness of the increase supports the validity of the assumptionsdescribed in 2.3.Although the perplexity and recognition rate are improved comparedwith the bigram model, the gain is modest.
This may be due to a lackof training data or to a mismatch between the training and test data,especially since the difference in performance is also small betweenthe bigram model and the trigram model.However, the fact that the performance obtained by the proposedmodel II lies almost halfway between the bigram and trigram, showsthat the proposed model has the capability to capture linguistic on-4.2.
Experimental ConditionsSpeaker-dependent continuous speech recognition experiments werecarried out under the conditions hown in Table 2.
The domain ofthe recognition task is the same as that of the training data, butthe text of the test speech data was not included in the trainingdata.
Context-independent continuous mixture HMMs were used asacoustic models.
The details of the acoustic models are shown inTable 3.Number of PhonemesTopologyOutput ProbabilitiesNumber of MixturesTraining Data384-state 3-1oop, left-to-right modelGaussian mixturesmax 14 (variable)2620 word utteranceTable 3: HMM used as the acoustic models91Language ModelBigramTrigramProposed (I)Proposed (II)Sentence Ratio ofPerplexity Recognition Rate Number of Parameters41.2 51.3% 1.036.3 54.0% 5.4 x 10336.9 - -  6.5 x 10238.1 52.5% 1.9Table 4: Test set perplexity and sentence r cognition ratestraints effectively with a comparatively small number of parameters.Its perfolanance ould be improved by extending it to use trigramprobabilities for local or global constraints,5.
DISCUSSIONSIn an attempt to capture the global constraints, we took note ofthe role of function words as case markers and used their N-gramprobabilities to extract he syntactic onstraints.
We also used theN-gram probabilities of the content words to extract he semanticconstraints.One of its advantages is that it does not need expensive computa-tional cost compared with previous works \[3, 4, 5, 6\].
Furthermore,as the syntactic constraints are considered to be less dependent onthe domain than the semantic ones, function word N-grams couldbe trained with a task-independent large database and combinedwith content word N-grams trained with a task-dependent smallerdatabase.One of the disadvantages of our approach is that he labels indicatingwhether a word is a function word or a content word are necessaryin the training data.
We think it would not be so difficult o auto-matically label if we only have to classify the words into these twocategories, because the category of function words can be regardedas a closed class.Another problem is its generality, especially its applicability ootherlanguages.
English, for example, has different structure of sentencesand different way of specifying the cases, although relationshipsbetween the content words are expected to exist.
We think sim-ilar approach could be also useful for other languages, but somemodification may be needed.6.
CONCLUSIONSIn this paper, a speech recognition system using a new stochasticlanguage model that integrates local and global linguistic onstraintswas proposed.
Function word bigrams and content word bigramswere introduced tocapture global syntactic and semantic constraints,and combined with a conventional word bigram model.
The num-ber of parameters was reduced by decomposing local and globaldependency.Continuous peech recognition based on the time-synchronousViterbi decoding algorithm with the proposed language model incor-porated into it was presented, and speaker-dependent speech recog-nition experiments were conducted.
Although the improvements inperformance over the conventional bigram model are rather modest,results how that the proposed model has the capability to captureLinguistic onstraints effectively.g2The assumptions made to reduce parameters donot degrade perplex-ity, but their validity needs to be verified from the linguistic point ofview.
The number of parameters is reduced in the proposed model,but the size of database we used is still not large enough to estimatethe statistics in the model.
More data would be necessary toevaluatethe effectiveness of the proposed model.
The use of part of speechor word equivalence lasses generated automatically (for example,\[10\]) could help to increase the robustness of the estimates obtainedfrom the limited size of the corpora.In the future, we plan to further investigate he effective utilizationof linguistic knowledge as well as statistical pproaches toex/ractmore useful global constraints.AcknowledgmentsWe would like to thank Mr. Sagayama, NTT Interface Laboratories,for his valuable advice.
We are also grateful to Dr. Sagisaka nd themembers of Department 1 for their useful comments and help.References1.
Bahl, L. R., Jelinek, F., and Mercer, R. L., "A maximumlikelihood approach to continuous speech recognition," IEEETransaction on Pattern Analysis and Machine Intelligence,vol.
PAMI-5, 1983, pp.
179-190.2.
Huang, X., Alleva, F., Hon, H-W., Hwang, M-Y., Lee, K:F., andRosenfeld, R., "The SPHINX-II speech recognition system:an overview," Computer Speech and Language, vol.
7, 1993,pp.
137-148.3.
Wright, J. H., Jones, G. J. E, and Lloyd-Thomas, H., "A con-solidated language model for speech recognition," Proc.
Eu-rospeech 93, 1993, pp.
977-980.4.
Lau, R., Rosenfeld, R., and Roukos, S., "Adaptive languagemodeling using the maximum entropy principle," Proc.
ARPAHuman Language Technology Workshop, 1993.5.
Lau, R., Rosenfeld, R., and Roukos, S., "Trigger-based lan-guage models: a maximum entropy approach," Proc.
ICASSP93, 1993, pp.
I/-45-II-48.6.
Bahl, L. R., Brown, P. F., de Souza, P. V., and Mercer, R. L.,"A tree-based statistical language model for natural languagespeech recognition," IEEE Transaction on Acoustics, Speech,and Signal Processing, vol.
37, 1989, pp.
1001-1008.7.
Isotani, R. and Sagayama, S., "Speech recognition using par-ticle N-grams and content-word N-grams," Proc.
Eurospeech93, 1993, pp.
1955-1958.8.
Jelinek, F., "Self-organized language modeling for speechrecognition," IBM research report, 1985.
Also available inReadings in Speech Recognition, Waibel, A. and Lee, K-F.,eds., 1990.9.
Jelinek, F. and Mercer, R. L., "Interpolated estimation ofMarkov source parameters from sparse data," in Pattern Recog-nition in Practice, Gelsema, E. S. and Kanal, L. N., eds., North-Holland Publishing Company, 1980.10.
Kneser, R. and Ney, H., "Improved clustering techniques forclass-based statistical language modelling," Proc.
Eurospeech93, 1993, pp.
973-976.93
