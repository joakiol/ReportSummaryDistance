Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58?67,Dublin, Ireland, August 23 2014.A Report on the DSL Shared Task 2014Marcos Zampieri1, Liling Tan2, Nikola Ljube?si?c3, J?org Tiedemann4Saarland University, Germany1,2University of Zagreb, Croatia3Uppsala University, Sweden4marcos.zampieri@uni-saarland.de, liling.tan@uni-saarland.dejorg.tiedemann@lingfil.uu.se, nljubesi@ffzg.hrAbstractThis paper summarizes the methods, results and findings of the Discriminating between SimilarLanguages (DSL) shared task 2014.
The shared task provided data from 13 different languagesand varieties divided into 6 groups.
Participants were required to train their systems to discrimi-nate between languages on a training and development set containing 20,000 sentences from eachlanguage (closed submission) and/or any other dataset (open submission).
One month later, a testset containing 1,000 unidentified instances per language was released for evaluation.
The DSLshared task received 22 inscriptions and 8 final submissions.
The best system obtained 95.7%average accuracy.1 IntroductionDiscriminating between similar languages is one of the bottlenecks of state-of-the-art language iden-tification systems.
Although in recent years systems have been trained to discriminate between morelanguages1, they still struggle to discriminate between similar languages such as Croatian and Serbian orMalay and Indonesian.From an NLP point of view, the difficulty systems face when discriminating between closely relatedlanguages is similar to the problem of discriminating between standard national language varieties (e.g.American English and British English or Brazilian Portuguese and European Portuguese), henceforthvarieties.
Recent studies show that language varieties can be discriminated automatically using words orcharacters as features (Zampieri and Gebre, 2012; Lui and Cook, 2013) .
However, due to performancelimitations, state-of-the-art general-purpose language identification systems do not distinguish texts fromdifferent national varieties, modelling pluricentric languages as unique classes.To evaluate how state-of-the-art systems perform in identifying similar languages and varieties, wedecided to organize the Discriminating between Similar Languages (DSL)2shared task.
This shared taskwas organized within the scope of the workshop on Applying NLP Tools to Similar Languages, Varietiesand Dialects (VarDial) in the 2014 edition of COLING.The motivation behind the DSL shared task is two-fold.
Firstly, we have observed an increase ofinterest in the topic.
This is reflected by a number of papers that have been published about this task inrecent years starting with Ranaivo-Malanc?on (2006) for Malay and Indonesian and Ljube?si?c et al.
(2007)for South Slavic languages.
In the DSL shared task we tried to include (depending on the availability ofdata) languages that have been studied in previous experiments, such as Croatian, English, Indonesian,Malay, Portuguese and Spanish.The second aspect that motivated us to organize this shared task is that, to our knowledge, no sharedtask focusing on the discrimination of similar languages has been organized previously.
The most sim-ilar shared tasks to DSL are the DEFT 2010 shared task (Grouin et al., 2010), in which systems wererequired to classify French journalistic texts with respect to their geographical location as well as theThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1Brown (2013) reports results on a system trained to recognize more than 1,100 languages2http://corporavm.uni-koeln.de/vardial/sharedtask.html58decade in which they were published.
Other related shared tasks include the ALTW 2010 multilinguallanguage identification shared task, a general-purpose language identification task containing data from74 languages (Baldwin and Lui, 2010) and finally the Native Language Identification (NLI) shared task(Tetreault et al., 2013) where participants were provided English essays written by foreign students of 11different mother tongues (Blanchard et al., 2013).
Participants had to train their systems to identify thenative language of the writer of each text.2 Related WorkAmong the first studies to investigate the question of discriminating between similar languages is thestudy published by Ranaivo-Malanc?on (2006).
The author presents a semi-supervised model to dis-tinguish between Indonesian and Malay, two closely related languages from the Austronesian familyrepresented in the DSL shared task.
The study uses the frequency and rank of character trigrams derivedfrom the most frequent words in each language, lists of exclusive words, and the format of numbers(Malay uses decimal points whereas Indonesian uses commas).
The author compares the performanceof this method with the performance obtained by TextCat (Cavnar and Trenkle, 1994).Ljube?si?c et al.
(2007) proposed a computational model for the identification of Croatian texts incomparison to Slovene and Serbian, reporting 99% recall and precision in three processing stages.
Theapproach includes a ?black list?, which increases the performance of the algorithm.
Tiedemann andLjube?si?c (2012) improved this method and applied it to Bosnian, Croatian and Serbian texts.
The studyreports significantly higher performance compared to general purpose language identification methods.The methods applied to discriminate between texts from different language varieties and dialects aresimilar to those applied to similar languages3.
One of the methods proposed to identify language varietiesis by Huang and Lee (2008).
This study presented a bag-of-words approach to classify Chinese texts fromthe mainland and Taiwan with results of up to 92% accuracy.Another study that focused on language varieties is the one published by Zampieri and Gebre (2012).In this study, the authors proposed a log-likelihood estimation method along with Laplace smoothing toidentify two varieties of Portuguese (Brazilian and European).
Their approach was trained and tested ina binary setting using journalistic texts with accuracy results above 99.5% for character n-grams.
Thealgorithm was later adapted to classify Spanish texts using not only the classical word and charactern-grams but also POS distribution (Zampieri et al., 2013).The aforementioned study by Lui and Cook (2013) investigates computational methods to discriminatebetween texts from three different English varieties (Canadian, Australian and British) across differentdomains.
The authors state that the results obtained suggest that each variety contains characteristics thatare consistent across multiple domains, which enables algorithms to distinguish them regardless of thedata source.Zaidan and Callison-Burch (2013) propose computational methods for the identification of Arabiclanguage varieties4using character and word n-grams.
The authors built their own dataset using crowd-sourcing and investigated annotators?
behaviour, agreement and performance when manually tagginginstances with the correct label (variety).3 MethodsIn the following subsections we will describe the methodology adopted for the DSL shared task.
Due tothe lack of comparable resources, the first decision we had to take was to create a dataset that could beused in the shared task and also redistributed to be used in other experiments.
We opted for the creationof a corpus collection based on existing datasets as discussed in 3.1 (Tan et al., 2014).Groups interested in participating in the DSL shared task had to register themselves in the sharedtask website to receive the training and test data.
Each group could participate in one or two types of3In the DSL shared task and in this paper we did not distinguish between language varieties and similar languages.
Moreon this discussion can be found in Clyne (1992) and Chamber and Trudgill (1998).4Zaidan and Callison-Burch (2013) use the terms ?varieties?
and ?dialects?
interchangeably whereas Lui and Cook (2013)use the term ?national dialect?
to refer to what previous work describes as ?national variety?.59submission as follows:?
Closed Submission: Using only the DSL corpus collection for training.?
Open Submission: Using any other dataset including or not the DSL collection for training.In the open submission we did not make any distinction between systems using the DSL corpus col-lection and those that did not.
This is different from the types of submissions for the NLI shared task2013.
The NLI shared task offered proposed two types of open submissions: open submission 1 - anydataset including the aforementioned TOEFL11 dataset (Blanchard et al., 2013) and open submission 2- any dataset excluding TOEFL11.For each of these submission types, participants were allowed to submit up to three runs, resulting ina maximum of six runs in total (three closed submissions and three open submissions).3.1 DataAs previously mentioned, we decided to compile our own dataset for the shared task.
The dataset wasentitled DSL corpus collection and its compilation was motivated by the absence of a resource thatallowed us to evaluate systems on discriminating similar languages.
The methods behind the compilationof this collection and the preliminary baseline experiments are described in Tan et al.
(2014).The DSL corpus collection consists of 18,000 randomly sampled training sentences, 2,000 develop-ment sentences and 1,000 test sentences for each language (or variety) containing at least 20 tokens5each.The languages are presented in table 1 with their ISO 639-1 language codes6.
For language varieties thecountry code is appended to the ISO code (e.g.
en-GB refers to the British variety of English).Group Language/Variety CodeBosnian bsA Croatian hrSerbian srIndonesian idB Malay myCzech czC Slovak skBrazilian Portuguese pt-BRD European Portuguese pt-PTArgentine Spanish es-ARE Castilian Spanish es-ESBritish English en-GBF American English en-USTable 1: Language Groups - DSL 2014 Shared TaskFor this collection, randomly sampled sentences from journalistic corpora (and corpora collections) wereselected for each of the 13 classes.
Journalistic corpora were preferred because they represent standardlanguage, which is an important factor to be considered when working with language varieties.
Other datasources (e.g.
Wikipedia) do not make any distinction between language varieties and they are thereforenot suitable for the purpose of the shared task.
A number of studies mentioned in the related work sectionuse journalistic texts for similar reasons (Huang and Lee, 2008; Grouin et al., 2010; Zampieri and Gebre,2012)Given what has been said in this section, we consider the collection to be a suitable comparable corporafrom this task, which was compiled to avoid bias in classification towards source, register and topics.
The5We considered a token as orthographic units delimited by white spaces.6http://www.loc.gov/standards/iso639-2/php/English_list.php60DSL corpus collection was distributed in tab delimited format; the first column contains a sentence inthe language/variety, the second column states its group and the last column refers to its language code.73.1.1 Problems with Group FThere are no major problems to report regarding the organization of the shared task nor with the com-pilation of the DSL corpus collection apart from some issues in the Group F data.
The organizers anda couple of teams participating in the shared task observed very poor performance when distinguishinginstances from group F (British English - American British).
For example, the baseline experimentsdescribed in Tan et al.
(2014) report a very low 0.59 F-measure for Group F (the lowest score) and 0.84for Group E (the second lowest score).
Some of the teams asked human annotators to try to distinguishthe sentences manually and they concluded that some instances were probably misclassified.We decided to look more carefully at the data and noticed that the instances were originally taggedbased on the websites (newspapers) that they were retrieved from and not the country of the originalpublication.
There are, however, many cases of cross citation and republication of texts that the originaldata sources did not take into account (e.g.
British texts that were later republished by an Americanwebsite).
As the DSL is a corpus collection and manually checking all 20,000 training and developmentinstances per language was not feasible, we assumed that the original sources8from which the texts wereretrieved provided the correct country of origin.
The assumption was correct for all language groups butEnglish.To illustrate the issues above we present next some misclassified examples.
Two particular cases raisedby the UMich team are the following:(1) I think they can afford to give North another innings and some time in Shield cricket and takeanother middle order batsman.
(en-US)(2) ATHENS, Ohio (AP) Albuquerque will continue its four-game series in Nashville Thursday nightwhen it takes on the Sounds behind starter Les Walrond (3-4, 4.50) against Gary Glover, who ismaking his first Triple-A start after coming down from Milwaukee.
(en-GB)Example number one was tagged as American English because it was retrieved from the online editionof The New York Times but it was in fact first published in Australia.
The second example is a textpublished by Associated Press describing an event that took place in Ohio, United States, but it wastagged as British English because it was retrieved by the UK Yahoo!
sports section.Our solution was to exclude the language group F from the final scores and perform a manual check inall its 1,000 test instances9, thus giving the chance to participants to train their algorithms on other datasources (open submission).3.2 ScheduleThe DSL shared task spanned from March 20thwhen the training set was released, to June 6thwhenparticipants could submit a paper (up to 10 pages) describing their system.
We provided one monthbetween the release of the training and the test set.
The schedule of the DSL shared task 2014 can beseen below.Event DateTraining Set Release March 20th, 2014Test Set Release April 21st, 2014Submissions Due April 23rd, 2014Results Announced April 30th, 2014Paper Submission June 6th, 2014Table 2: DSL 2014 Shared Task Schedule7To obtain the data please visit: https://bitbucket.org/alvations/dslsharedtask20148See Tan et al.
(2014) for a complete description of the data sources of the DSL corpus collection.9Our manual check suggests that about 25% of the instances in the English dataset was likely to have been misclassified.614 ResultsThis section summarises the results obtained by all participants of the shared task who submitted finalresults.10The DSL shared task included 22 enrolled teams from different countries (e.g.
Australia,Estonia, Holland, Germany, United Kingdom and United States).
From the 22 enrolled teams, eight ofthem submitted their final results.
Most of the groups opted to exclusively use the DSL corpus collectionand therefore participated solely in the closed submission track.
Two of them compiled comparabledatasets and also participated in the open submission.Given that the dataset contained misclassified instances, group F (English) was not taken into accountto compute the final shared task scores.
In the next subsections we report results in terms of macro-average F-measure and accuracy.4.1 Closed SubmissionTable 3 presents the best F-measure and Accuracy results obtained by the eight teams that submitted theirresults for the closed submission track ordered by accuracy.Team Macro-avg F-score Overall AccuracyNRC-CNRC 0.957 0.957RAE 0.947 0.947UMich 0.932 0.932UniMelb-NLP 0.918 0.918QMUL 0.925 0.906LIRA 0.721 0.766UDE 0.657 0.681CLCG 0.405 0.453Table 3: Open Submission - ResultsIn the closed submissions, we observed a group of five teams whose systems (best runs) obtained re-sults over 90% accuracy.
This is comparable to what is described in the state-of-the-art literature fordiscriminating similar languages and language varieties (Tiedemann and Ljube?si?c, 2012; Lui and Cook,2013).
These five teams submitted system descriptions that allowed us to look in more detail at successfulapproaches for this task.
System descriptions will be discussed in section 5.Three of the eight teams obtained substantially lower scores, from 45.33% to 76.64% accuracy.
Thesethree groups unfortunately did not submit system description papers.
From our point of view, this wouldcreate an interesting opportunity to look more carefully at the weaknesses of approaches that did notobtain good results in this task.4.2 Open SubmissionOnly two systems submitted results for the open submission track and their F-measure and Accuracyresults are presented in table 3.Team Macro-avg F-score Overall AccuracyUniMelb-NLP 0.878 0.880UMich 0.858 0.859Table 4: Closed Submission - ResultsThe UniMelb-NLP (Lui et al., 2014) group used data from different corpora such as the BNC, EU-ROPARL and Open Subtitles whereas UMich (King et al., 2014) compiled journalistic corpora from dif-ferent sizes for each language ranging from 695,597 tokens for Malay to 20,288,294 tokens for BritishEnglish.10Visit https://bitbucket.org/alvations/dslsharedtask2014/downloads/dsl-results.htmlfor more detail on the shared task results or at the aforementioned DSL shared task website.62Comparing the results of the closed to the open submissions, we observed that the UniMelb-NLP sub-mission was outperformed by UMich system by about 1.5% accuracy in the closed submission, but inthe open submission they scored 2.1% better than UMich.
This difference can be explained by investi-gating these two factors: 1) the quality and amount of the collected training data; 2) the robustness ofthe method to obtain correct predictions across different datasets and domains as previously discussedby Lui and Cook (2013) for English varieties.4.3 Accuracy per Language GroupIn this subsection we look more carefully at the performance of systems in discriminating each classwithin groups A to E. Table 5 presents the accuracy scores obtained per language group for each teamsorted alphabetically.
The best score per group is displayed in bold.CLCG LIRA NRC-CNRC QMUL RAE UDE UMich UniMelb-NLPA 0.338 0.333 0.936 0.879 0.919 0.785 0.919 0.915B 0.503 0.982 0.996 0.935 0.994 0.892 0.992 0.972C 0.500 1.000 1.000 0.962 1.000 0.493 0.999 1.000D 0.496 0.892 0.956 0.905 0.948 0.493 0.926 0.896E 0.503 0.843 0.910 0.865 0.888 0.694 0.876 0.807Table 5: Language Groups A to E - Accuracy ResultsThe top 5 systems plus the LIRA team obtained very good results for groups B (Malay and Indonesian)and C (Czech and Slovak).
Four out of eight systems obtained perfect performance when discriminatingCzech and Slovak texts.
Perfect performance was not achieved by any of the systems when distinguishingMalay from Indonesian texts, but even so, results were fairly high and the best result was 99.6% accuracyobtained by the NRC-CNRC group.
The perfect results obtained by four groups when distinguishingtexts from group C suggest that Czech and Slovak texts are not as similar as we assumed before theshared task, and that they therefore possess strong systemic and/or orthographic differences that allowwell-trained classifiers to perform perfectly.
Figure 1 presents the accuracy results of the top 5 groups.Figure 1: Language Groups A to E Accuracy - Top 5 SystemsDistinguishing between languages from group A (Bosnian, Croatian and Serbian), the only group con-taining 3 languages, proved to be a challenging task as discussed in previous research (Ljube?si?c et al.,2007; Tiedemann and Ljube?si?c, 2012).
The best result was again obtained by the NRC-CNRC groupwith 93.5% accuracy.
The groups containing texts written in different language varieties, namely D (Por-tuguese) and E (Spanish) were the most difficult to discriminate, particularly the Spanish varieties.
Theseresults also corroborate the findings of previous studies (Zampieri et al., 2013).63The QMUL system that was the 5thbest system in the closed submission track did not outperform anyof the other top 5 systems in groups A, B or C. However, the system did better when distinguishing textsfrom the two most difficult language groups (D and E), outperforming the UniMelb-NLP submission ontwo occasions.
The simplicity of the approach proposed by the QMUL, which the author describes as?a simple baseline?
(Purver, 2014) may be an explanation for the regular performance across differentlanguage groups.4.4 Results Group FTo document the problems in the group F (British and American English) dataset we included the resultsof both the open and closed submissions for this language group in table 6.
As previously mentioned,submitting group F results was optional and we did not include these results in the final shared taskresults.
Six out of eight systems decided to submit their predictions as closed submissions and the twogroups participating in the open submission track also submitted their group F results.Team F-score Accuracy TypeUMich 0.639 0.639 OpenUniMelb- NLP 0.581 0.583 OpenNRC-CNRC 0.522 0.524 ClosedLIRA 0.450 0.493 ClosedRAE 0.451 0.481 ClosedUMich 0.463 0.464 ClosedUDE 0.451 0.451 ClosedUniMelb-NLP 0.435 0.435 ClosedTable 6: Group F - Accuracy ResultsThe results confirm the problems in the DSL dataset discussed in section 3.1.1.
After a careful manualcheck of the 1,000 test instances, open submissions scores were still substantially lower than the othergroups: 69.9% and 58.3% accuracy.
Closed submissions proved to be impossible and only one of the sixsystems scored slightly above the 50% baseline.It should be investigated more carefully in future research whether the poor results for group F reflectonly the problems in the dataset or also the actual difficulty in discriminating between these two varietiesof English.
Moderate differences in orthography (e.g.
neighbour (UK) and neighbor (US)) as wellas lexical choices (e.g.
rubbish (UK) and garbage (US) or trousers (UK) and pants (US)) are presentin texts from these two varieties and these can be informative features for algorithms to discriminatebetween them.
Discriminating between other English varieties already proved to be a challenging yetfeasible task in previous research (Lui and Cook, 2013).5 System DescriptionsAll eight systems that submitted their final results to the shared task were invited to submit papers de-scribing their systems and the top 5 systems in the closed track submitted their papers, namely: NRC-CNRC, RAE, UMich, UniMelb-NLP and QMUL.The best scores were obtained by the NRC-CNRC (Goutte et al., 2014) team which proposed a two-step approach to predict first the language group than the language of each instance.
The language groupwas predicted in a 6-way classification using a probabilistic model similar to a Naive Bayes classifier,and later the method applied SVM classifiers to discriminate within each group: binary for groups B-Fand one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian).An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called?white lists?
inspired by the ?blacklist?
classifier (Tiedemann and Ljube?si?c, 2012).
These lists are wordlists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc?on (2006)proposed to discriminate between Malay and Indonesian.64Two groups used Information Gain (IG) to select the best features for classification, namely UMich(King et al., 2014) and UniMelb-NLP (Lui et al., 2014).
These teams were also the only ones to submitopen submissions.
The UniMelb-NLP team tried different classification methods and features (includingdelexicalized models) in each run.
The best results were obtained by their own method, the off-the-shelfgeneral-purpose language identification software langid.py (Lui and Baldwin, 2012).
This method hasbeen widely used for general-purpose language identification and its performance is regarded superiorto similar general-purpose methods such as TextCat.
In the shared task, the system was modelled hier-archically firstly identifying the language group that a sentence belongs to and subsequently the specificlanguage, achieving performance comparable to the state-of-the-art, but still slightly below the otherthree systems.The QMUL team (Purver, 2014) proposed a linear SVM classifier using words and characters as fea-tures.
The author investigated the influence of the cost parameter c (from 1.0 to 100.0), in the classifiers?performance.
The cost parameter c is responsible for the trade-off between maximum margin and clas-sification errors.
According to the system description the optimal parameter for this task lies between30.0 and 50.0.
Purver (2014) also notes that the linear SVM classifier performs well with word uni-gramlanguage models in comparison to methods using character n-grams.
This observation corroborates thefindings of previous experiments that rely on words as important features to distinguish similar languagesand varieties (Huang and Lee, 2008; Zampieri, 2013)The features and algorithms presented so far, as well as the system paper descriptions, are summarisedin table7.11Team Algorithm Features System PaperNRC-CNRC Prob.
Class.
and Linear SVM Words 1-2, Char.
2-6 (Goutte et al., 2014)RAE MaxEnt Words 1-2, Char.
1-5, ?Whitelist?
(Porta and Sancho, 2014)UMich Naive Bayes Words 1-2, Char.
2-6 (IG Feat.
Selection) (King et al., 2014)UniMelb-NLP langid.py Words, Char., POS (IG Feat.
Selection) (Lui et al., 2014)QMUL Linear SVM Words 1, Char.
1-3 (Purver, 2014)Table 7: Top 5 Systems - Features and Algorithms at a Glance6 ConclusionShared tasks are an interesting way of comparing algorithms, computational methods and features usingthe same dataset.
Given what has been presented in this paper, we believe that the DSL shared task filledan important gap in language identification and will allow other researchers to look in more detail at theproblem of discriminating similar languages.
Accurate methods for discriminating similar languages canhelp to improve performance not only in language identification but also in a number of NLP tasks andapplications such as part-of-speech-tagging, spell checking and machine translation.The best system obtained 95.71% accuracy and F-measure for a set of 11 languages and varietiesdivided into 5 groups (A to E), using only the DSL corpus collection.
Systems that performed bestmodelled their algorithms to perform two-step predictions: first the language group, then the actual classand used characters and words as features.
As we regard the corpus to be a balanced sample of thenews domain, the results obtained confirm the assumption that similar languages and varieties possesssystemic characteristics that can be modelled by algorithms in order to distinguish languages from othersimilar languages or varieties using lexical or orthographical features.Another lesson learned from this shared task is regarding the compilation of group F (English) data.Researchers, including us, often rely on previously annotated meta-data which sometimes may containinaccurate information and errors.
Corpus collection for this purpose should be thoroughly checked(manually if possible).
The issues with the group F might have discouraged some of the participants tocontinue in the shared task (particularly those who were interested only in the discrimination of Englishvarieties).11UniMelb-NLP experimented different methods in their 6 runs.
In this report we commented on the algorithm that achievedthe best performance.656.1 Future PerspectivesThe shared task was a very fruitful and positive experience for the organizers.
We would like to organizea second edition of the shared task containing, for example, new language groups for which we couldnot find suitable corpora before the 2014 edition.
This includes, most notably, the cases of Dutch andFlemish or the varieties of French and German which could not be included in the DSL shared task dueto the lack of available data.The DSL corpus collection is freely available and can be used as a gold standard for language iden-tification or to train algorithms for other NLP tasks involving similar languages.
We would like to usethe dataset to investigate, for example, lexical variation between similar languages and varieties as pro-posed by Piersman et al.
(2010) and Soares da Silva (2010) or syntactic variation using annotated dataas discussed in Anstein (2013).At present, we are investigating the influence of the length of texts in the discrimination of similarlanguages.
It is a well known fact that the longer texts are, the more likely they are to contain featuresthat allow algorithms to identify their language.
However, this variable was not explored within thescope of the DSL shared task and we are using the DSL dataset and the results for this purpose.
Anotherdirection that our work may take is the linguistic analysis of the most informative features in classificationas was done recently by Diwersy et al.
(2014).AcknowledgementsThe authors would like to thank all participants of the DSL shared task for their comments and sugges-tions throughout the organization of this shared task.
We would also like to thank Joel Tetreault andBinyam Gebrekidan Gebre for their valuable feedback on this report.ReferencesStefanie Anstein.
2013.
Computational approaches to the comparison of regional variety corpora : prototyping asemi-automatic system for German.
Ph.D. thesis, University of Stuttgart.Timothy Baldwin and Marco Lui.
2010.
Multilingual language identification: ALTW 2010 shared task data.
InProceedings of Australasian Language Technology Association Workshop, pages 4?7.Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife Cahill, and Martin Chodorow.
2013.
TOEFL11: ACorpus of Non-Native English.
Technical report, Educational Testing Service.Ralf Brown.
2013.
Selecting and weighting n-grams to identify 1100 languages.
In Proceedings of the 16th In-ternational Conference on Text Speech and Dialogue (TSD2013), Lecture Notes in Artificial Intelligence (LNAI8082), pages 519?526, Pilsen, Czech Republic.
Springer.William Cavnar and John Trenkle.
1994.
N-gram-based text catogorization.
3rd Symposium on Document Analy-sis and Information Retrieval (SDAIR-94).Jack Chambers and Peter Trudgill.
1998.
Dialectology (2nd Edition).
Cambridge University Press.Michael Clyne.
1992.
Pluricentric Languages: Different Norms in Different Nations.
CRC Press.Sascha Diwersy, Stefan Evert, and Stella Neumann.
2014.
A semi-supervised multivariate approach to the studyof language variation.
Linguistic Variation in Text and Speech, within and across Languages.Cyril Goutte, Serge L?eger, and Marine Carpuat.
2014.
The NRC system for discriminating similar languages.
InProceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial),Dublin, Ireland.Cyril Grouin, Dominic Forest, Lyne Da Sylva, Patrick Paroubek, and Pierre Zweigenbaum.
2010.
Pr?esentation etr?esultats du d?efi fouille de texte DEFT2010 o`u et quand un article de presse a-t-il ?et?e ?ecrit?
Actes du sixi`emeD?Efi Fouille de Textes.Chu-ren Huang and Lung-hao Lee.
2008.
Contrastive approach towards text source classification based on top-bag-of-word similarity.
In Proceedings of PACLIC 2008, pages 404?410.66Ben King, Dragomir Radev, and Steven Abney.
2014.
Experiments in sentence language identification withgroups of similar languages.
In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages,Varieties and Dialects (VarDial), Dublin, Ireland.Nikola Ljube?si?c, Nives Mikelic, and Damir Boras.
2007.
Language identification: How to distinguish similarlanguages?
In Proceedings of the 29th International Conference on Information Technology Interfaces.Marco Lui and Timothy Baldwin.
2012. langid.py: An off-the-shelf language identification tool.
In Proceedingsof the 50th Meeting of the ACL.Marco Lui and Paul Cook.
2013.
Classifying English documents by national dialect.
In Proceedings of Aus-tralasian Language Tchnology Workshop, pages 5?15.Marco Lui, Ned Letcher, Oliver Adams, Long Duong, Paul Cook, and Timothy Baldwin.
2014.
Exploring methodsand resources for discriminating similar languages.
In Proceedings of the 1st Workshop on Applying NLP Toolsto Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.Yves Piersman, Dirk Geeraerts, and Dirk Speelman.
2010.
The automatic identification of lexical variationbetween language varieties.
Natural Language Engineering, 16:469?491.Jordi Porta and Jos?e-Luis Sancho.
2014.
Using maximum entropy models to discriminate between similar lan-guages and varieties.
In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varietiesand Dialects (VarDial), Dublin, Ireland.Matthew Purver.
2014.
A simple baseline for discriminating similar language.
In Proceedings of the 1st Workshopon Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.Bali Ranaivo-Malanc?on.
2006.
Automatic identification of close languages - case study: Malay and Indonesian.ECTI Transactions on Computer and Information Technology, 2:126?134.Augusto Soares da Silva.
2010.
Measuring and parameterizing lexical convergence and divergence betweenEuropean and Brazilian Portuguese: endo/exogeneousness and foreign and normative influence.
Advances inCognitive Sociolinguistics.Liling Tan, Marcos Zampieri, Nikola Ljube?si?c, and J?org Tiedemann.
2014.
Merging comparable data sourcesfor the discrimination of similar languages: The DSL corpus collection.
In Proceedings of The Workshop onBuilding and Using Comparable Corpora (BUCC), Reykjavik, Iceland.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.
A report on the first native language identification sharedtask.
In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,Atlanta, GA, USA, June.
Association for Computational Linguistics.J?org Tiedemann and Nikola Ljube?si?c.
2012.
Efficient discrimination between closely related languages.
InProceedings of COLING 2012, pages 2619?2634, Mumbai, India.Omar F Zaidan and Chris Callison-Burch.
2013.
Arabic dialect identification.
Computational Linguistics.Marcos Zampieri and Binyam Gebrekidan Gebre.
2012.
Automatic identification of language varieties: The caseof Portuguese.
In Proceedings of KONVENS2012, pages 233?237, Vienna, Austria.Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy.
2013.
N-gram language models and POSdistribution for the identification of Spanish varieties.
In Proceedings of TALN2013, pages 580?587, Sabled?Olonne, France.Marcos Zampieri.
2013.
Using bag-of-words to distinguish similar languages: How efficient are they?In Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics(CINTI2013), pages 37?41, Budapest, Hungary.67
