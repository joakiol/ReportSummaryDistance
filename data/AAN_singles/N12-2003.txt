Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 11?16,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsBeauty Before Age?Applying Subjectivity to Automatic English Adjective OrderingFelix HillDept.
of Theoretical & Applied Linguisticsand Computer LaboratoryUniversity of CambridgeCambridge CB3 9DA, UKfh295@cam.ac.ukAbstractThe preferred order of pre-nominal adjectivesin English is determined primarily by seman-tics.
Nevertheless, Adjective Ordering (AO)systems do not generally exploit semantic fea-tures.
This paper describes a system that or-ders adjectives with significantly above-chance accuracy (73.0%) solely on the basisof semantic features pertaining to the cogni-tive-semantic dimension of subjectivity.
Theresults indicate that combining such semanticapproaches with current methods could resultin more accurate and robust AO systems.1 IntroductionAs a significant body of linguistic research hasobserved (see e.g.
Quirk et al (1985)), English pre-nominal adjective strings exhibit subtle order re-strictions.
Although example (2), below, does notrepresent a clear-cut violation of establishedgrammatical principles, it would sound distinctlyunnatural to native speakers in the majority of con-texts, in contrast to the entirely unproblematic (1).
(1)    He poked it with a long metal fork(2) ?
He poked it with a metal long forkThe problem of determining the principles that go-vern Adjective Ordering (henceforth, AO) in Eng-lish has been studied from a range of academicperspectives, including philosophy, linguistics,psychology and neuroscience.
AO is also of inter-est in the field of Natural Language Processing(NLP), since a method that consistently selectsfelicitous orders would serve to improve the outputof language modeling and generation systems.Previous NLP approaches to AO infer theordering of adjective combinations from instancesof the same, or superficially similar, combinationsin training corpora (Shaw & Hatzivassiloglou,1999) (Malouf, 2000), or from distributional ten-dencies of the adjectives in multiple-modifierstrings (Mitchell, 2009) (Dunlop, Mitchell, &Roark, 2010).
Such methods are susceptible todata sparseness, since the combinations fromwhich they learn are rare in everyday language.By contrast, the approach taken here deter-mines AO based on semantic features of adjec-tives, guided by the theoretical observation that thecognitive notion of subjectivity governs ordering inthe general case (Adamson, 2000).
The semanticfeatures developed are each highly significant pre-dictors of AO, and they combine to classify com-binations with 73.0% accuracy.
These preliminaryresults indicate that semantic AO systems can per-form comparably to existing systems, and thatclassifiers exploiting semantic and direct evidencemight surpass the current best-performing systems.2 Previous researchThe subtle nature of human ordering preferencesmakes AO a particularly challenging NLP task.
Inperhaps the first specific attempt to address theproblem, Shaw and Hatzivassiloglou (1999) applya direct evidence method.
For a given adjectivecombination in the test data, their system searchesa training corpus and selects the most frequent or-dering of that combination.
Because there is nobasis to determine the order of adjective combina-tions that are not in the training data, Shaw andHatzivassiloglou extend the domain of the classifi-11er by assuming transitivity in the order relation,increasing the coverage with only a small reduc-tion in accuracy.
Nevertheless, the system remainshighly dependent on the domain and quantity oftraining data.
For example, accuracy is 92% whentraining and test data are both within the medicaldomain but only 54% in cross-domain contexts.Malouf (2000) combines a direct evidenceapproach with an alternative method for extendingthe domain of his classifier.
His system infers theorder of unseen combinations from ?similar?
seencombinations, where similarity is defined purely interms of morphological form.
The method worksby exploiting a degree of correlation between formand order (e.g.
capital letters indicate nominalmodifiers, which typically occur to the right).Mitchell (2009) applies a less ?direct?
ap-proach, clustering adjectives based on their posi-tion in multiple-modifier strings.
AlthoughMitchell?s classifier requires no direct evidence,data sparseness is still an issue because the stringsfrom which the system learns are relatively infre-quent in everyday language.
Dunlop et al (2010)apply Multiple Sequence Alignment (MSA), a sta-tistical technique for automatic sequence ordering,which, as with Malouf?s system, quantifies word-similarity based solely on morphological features.Despite the greater sophistication of these morerecent approaches, Mitchell et al (2011) showedthat a simple n-gram (direct evidence) classifiertrained on 170 million words of New York Timesand Wall Street Journal text and tested on theBrown Corpus  (82.3% accuracy) outperforms boththe clustering (69.0%) and MSA (81.8%) methods.Wulff (2003) uses Linear DiscriminantAnalysis (LDA) to quantify the effects of variouspotential AO correlates, and confirms that seman-tic features are better predictors than morphologi-cal and syntactic features.
The features, extractedfrom the 10-million word Spoken British NationalCorpus (BNC) and weighted by LDA, combine topredict unseen adjective orders with 72% accuracy.Wulff?s study is unique in applying seman-tics to the problem, although her focus is theoreti-cal and several features are implemented manually.The next section describes the theoretical basis fora fully-automated semantic approach to AO thatcould help to resolve the issues of data sparsity anddomain dependence associated with the direct evi-dence methods described above.2.1 The subjectivity hypothesisAlthough phonetic, morphological and syntacticfactors influence AO in specific contexts, there isconsensus in the theoretical literature that seman-tics is the determining factor in the general case(see Quirk et al (1985) for further discussion).Several semantic theories of AO make use of thecognitive linguistic notion of subjectivity (Quirk etal.
1985; Hetzron, 1978; Adamson 2000).
Subjec-tivity in this context refers to the degree to whichan utterance can or cannot be interpreted indepen-dently of the speaker?s perspective (Langacker,1991).
For example, the deictic utterance (3) ismore subjective than (4) since its truth depends onthe speaker?s location at the time of utterance.
(3) James is sitting across the table(4) James is sitting opposite SamIn relation to AO, Quirk et al Hetzron and Adam-son each support some form of the subjectivity hy-pothesis: that more subjective modifiers generallyoccur to the left of less subjective modifiers in pre-nominal strings.
For example, in (5) the adjectivebig tells us about the relation between the car andthe speaker?s idea of typical car size.
This ascrip-tion is less objectively verifiable than that of carcolor, so big occurs further from the head noun.The position of oncoming in (6) reflects the highinherent subjectivity of deictic modifiers.
(5)  A big red Italian car        (BNC)(6)  An oncoming small black car (BNC)Figure 1:  Diachronic variation of preferred AOTo illustrate a process of changing AO preferencesthat can be explained in a compelling way by thesubjectivity hypothesis, the 1 trillion-word Googlen-Gram Viewer was queried (Figure 1).
The twoFrequency(% corpus)Year?gay young man?
?young gay man?12lines indicate the frequency of the strings ?gayyoung man?
and ?young gay man?
in the Corpusfrom 1950 to 2000, as the pre-eminent meaning ofgay evolved from the subjective merry to the cate-gorical, well-defined homosexual.
As the graphshows, this reduction in subjectivity has been ac-companied by a marked increase in the tendency ofgay to appear closer to the noun in such strings.3 System designThe AO system described below applies the theo-retical findings presented above by extracting fromtraining data various subjectivity features of adjec-tives and applying this information to classify in-put orderings as correct or incorrect.1  Systemoperation and evaluation consisted of 5 stages.Extracting feature profiles:  The 200 highest-frequency adjectives in the BNC were extracted.Following Wulff (2003, p. 6), three items, other,only and very were removed from this list becausethey occur in right-branching structures.
For theremaining adjectives, a ?profile?
of feature values(c.f.
Table 1, below), was extracted from 24 mil-lion words (Sections A-C) of the written BNC.Generating gold-standard orderings:  From the197 adjectives, 19,306 unordered pairs  , were generated.
The bigram frequencies of thestrings  , 	 and  , 	 were then extractedfrom the 1 billion-word Google n-gram Corpus.From this data, the 12,000 pairs  ,  with thelargest proportional difference in frequency be-tween  , 	 and  , 	 were selected.Defining test and training sets: A set of 12,000ordered triples ,  , ,	 was generated,where ,	 is an indicator function taking thevalue 1 if  , 	 is the preferred ordering in theGoogle corpus and 0 if  , 	 is preferred.Some of the triples were re-ordered at random toleave an equal number of preferred and dispre-ferred orderings in the data.
These triples werepopulated with feature profiles, to create vectors, ?
, ,  , ?
 , ,	 1 The system operates on adjectival and nominal modifiers butnot on articles, determiners, degree modifiers and other non-adjectival pre-modifiers.where  is the value of the  feature of the ad-jective , and n is the total number of features.The set of vectors was then randomly partitioned inthe ratio 80:20 for training and testing respectively.Training the classifier:  A logistic regression wasapplied to the set of training vectors, in which thefirst 2n elements of the vectors were independentvariables and the final element was the dependentvariable.
Logistic regression has been shown tobe preferable to alternatives such as Ordinary LeastSquares and LDA for binary outcome classificationif, as in this case, the independent variables are notnormally distributed (Press & Wilson, 1978).Evaluation: Performance was determined by thenumber of pairs in the test data correctly orderedby the classifier.
Steps 3-5 were repeated 4 times(5-fold cross-validation), with the scores averaged.3.1 The FeaturesOf the features included in the model,COMPARABILITY and POLARITY are shown tocorrelate with human subjectivity judgments byWiebe and colleagues (see e.g.
Hatzivassiloglou &Wiebe, 2000).
The remainder are motivated byobservations in the theoretical literature.MODIFIABILITY:  Gradable adjectives, such ashot or happy, tend to be more subjective than pro-totypically categorical adjectives, such as squareor black (Hetzron, 1978).
Unlike categorical ad-jectives they admit modification by intensifiers(Paradis, 1997).
Therefore, the featureMODIFIABILITY is defined as the conditionalprobability that an adjective occurs immediatelyfollowing an intensifier given that it occurs at all.2 !
"#   =   ?
&'("), 	#*?,&'("# = '-&'' )'&.    /, !
.
?'
-&) ?1& / 1' !
1& !
?COMPARABILITY:  Gradable adjectives alsohave comparative and superlative forms, whereasprototypically categorical adjectives do not.
Giventhe association between gradability and subjectivi-ty, the feature COMPARABILITY is defined as theprobability of an adjective occurring in compara-tive or superlative form given it occurs at all.2 The set of intensifiers is taken from (Paradis, 1997).13new good old different localMODIF 0.0010 0.0529 0.0208 0.0887 0.0004COM 0.0079 0.4881 0.2805 0.0011 0.0045PRED 0.0100 0.1018 0.0289 0.0806 0.0069POL 0.0000 1.0000 0.0000 0.0000 0.0000ADV 0.0220 0.0008 0.0000 0.0318 0.0478NOM 0.2900 0.0999 0.0113 0.0000 0.0212Table 1:  Example feature profiles2)3& !
"#   =   &'("4# +  &'("6#&'("# + &'("4# +  &'("6#4 = 7)3& 8' &)         6 = .93'& 8' &)  PREDICATIVITY: Adjectives can be applied inboth attributive (?the red car?
), and predicative(?the car is red?)
constructions.
Bolinger (1967)suggests that predicative constructions are concep-tualized more dynamically or temporarily than at-tributive constructions.
Since dynamic propertiesare generally ascribed more subjectively than per-manent properties (Langacker, 1991), Bolinger?sintuition implies an association between subjectivi-ty and predicative constructions.
Indeed, manyobjective modifiers sit uncomfortably in predica-tive contexts, as shown by (7) and (8).
(7)    I live in a brick house(8) ?
The house I live in is brickThe feature PREDICATIVITY is therefore definedas the probability that an adjective occurs in a pre-dicative construction given that it occurs at all.The measure is implemented by counting the num-ber of times the adjective immediately followssome form of an English copula verb.3:&'7 8!
"# =   ?
&'("7, 	#;?<&'("#2 = .'
 =>-.?
739 8'&.
>  >'7 ' &).POLARITY: An adjective is said to be polar if ittypically attributes a positive (kind, healthy,strong) or negative (poor, selfish, rotten) characte-ristic.
Semi-supervised methods for automaticallydetecting adjective polarity have been developed(Hatzivassiloglou & McKeown, 1997), and appliedto subjectivity analysis by Wiebe (2000).POLARITY is implemented as a binary feature,whose value depends on whether or not the adjec-tive appears in a list of 1,300 polar adjectives ex-tracted by Hatzivassiloglou & Mackeown.
:& !
"# =  ?
1     ?
A?C0      ?
A?C FA =  G'7 8'. '
' .
3.
8'         C = G'7 8'. '
' .
>'- 8'3 The copula verbs list was compiled manually by the author.ADVERBIABILITY:  Quirk (1985, p 1339) notesthat evaluative adjectives tend to develop derivedadverbial forms, whereas more objective adjectivesdo not.
For example, nice, beautiful and, carefulcorrespond to the adverbs nicely, beautifully, andcarefully, whereas no such derived forms exist forthe more objective adjectives male, English andbrown.
The ADVERBIABILITY of an adjective isdefined as the ratio of derived adverbial forms tototal base and adverbial forms in the corpus.8'& !
"#   =   &'("?#&'("# + &'("?#?
= 8'& &) '&8' &) NOMINALITY:  Wullf (2003) reports statisticalevidence that more ?noun-like?
modifiers appearcloser to the head in modifying strings.
Combina-tions such as ?bread knife?
or ?police car?, oftenanalyzed as noun-noun compounds rather thanmodifier/noun combinations, represent the clearestsuch examples.
Amongst more prototypical adjec-tives, some, such as green, or male have nominalsenses (?village green?, ?unidentified male?
), whe-reas others do not.
Separately, Hatzivassiloglouand Wiebe (2000) report a statistical correlationbetween the number of adjectives in a text andhuman judgments of subjectivity.
These observa-tions suggest that adjectives are inherently moresubjective than nouns, and further that noun-like?behavior?
might indicate relative objectivity with-in the class of adjectives.
Consequently, the fea-ture NOMINALITY is defined, following Wulff, asthe probability that an adjective is tagged as a noungiven that it is tagged as either an adjective or anoun.
It is the only feature that is expected to exhi-bit an inverse correlation with subjectivity.I)> !
"#   =   &'(" >#&'("# + &'(" ># = G'7 8'   --' .
>9>        J = G'7 8'   --' .
G'7 8'14ObservedPredictedTraining DataIncorrect Correct % CorrectIncorrect 2773 1637 62.9Correct 1120 4101 78.5Overall%     71.4Test DataIncorrect Correct % CorrectIncorrect 696 370 65.3Correct 270 1031 79.2Overall%     73.0Table 2:  Overall results of model cross-validationFea-tureRegressionCoefficientPredictorSignifi-cancePerform-ance inIsolationCompari-son ofA1 / A2MeansMODIF 5.205 .000 62.9% 0.000COM .177 .381 58.7% 0.000PRED 3.630 .000 68.6% 0.000POL .339 .000 60.4% 0.000ADV 1.503 .000 62.8% 0.000NOM -.405 .000 58.4% 0.000Table 3:  Influence of individual features4 ResultsThe performance of the classifier is promising withrespect to the intuition that semantic features canbe usefully applied to AO systems.
A chi-squaretest reveals the features collectively to be highlysignificant predictors of AO (K = 2257.25, 3 <0.001???#.
Once trained, the system orders unseencombinations in the test data with accuracy of73.0%, as detailed in Table 2.
This figure is notdirectly comparable with previous work because ofdifferences in the evaluation framework.It is notable that the accuracy of the classifi-er rises to 86.2% when the test data is hand-pickedas the 3000 pairs for which the strength of orderingpreference is highest.4  This suggests that the ap-proach could be particularly effective at detectinghighly unnatural combinations.
Moreover, the per-formance when tested on the 3000 (unseen) pairswith the lowest ordering preference is 70.1%, indi-cating the potential to cope well with marginal cas-es and rare combinations.As Table 3 shows, all features apart fromCOMPARABILITY are statistically significant pre-dictors in the model "3 < 0.001???#.
In addition,the mean value of each feature over adjectives infirst position  differs significantly from the meanover adjectives in second position  (  ?
28.07in each case,  = 11,283).
Whilst relatively theweakest predictor, COMPARABILITY in isolationdoes predict AO at above-chancecy "58.7%, 3 < 0.001??
?#4 The 3000 pairs for which the proportional preference for oneordering over another in the Google n-Gram corpus is highestand for which the total frequency of the pair exceeds 500.. Its low significance in the overall model re-flects its high level of interaction with other fea-tures; in particular, MODIFIABILITY (PearsonCorrelation: .367, 3 < 0.001???).
The relativemagnitude of the model coefficients is not infor-mative, since the  measurement scale is not com-mon to all features.
Nevertheless, the negativeregression coefficient of NOMINALITY confirmsthat this feature correlates inversely with distancefrom the noun.To test the influence of the training corpus size onsystem performance, features were extracted fromBNC Section A (7 million words) rather than Sec-tions A-C (24 million words) in a separate experi-ment.
This adjustment resulted in a reduction inclassifier accuracy from 73.0% to 71.4%, indicat-ing that performance could be significantly im-proved by training on the full BNC or even largercorpora.
Further improvements could be achievedthrough the combination of semantic and ?direct?features.
To illustrate this, the featureLEFTTENDENCY, a measure of the likelihood thatan adjective occurs immediately to the left ofanother adjective in the training data, was added.This adjustment raised the classifier accuracy from73.0% to 76.3%.
It should also be noted that manyof the features in the current system are extractedvia measures that approximate syntactic dependen-cy with bigram context.
It is an empirical questionwhether the additional complexity associated withmore precise measures (for example, applying de-pendency parsing) would be justified by perfor-mance improvements.155 ConclusionThis paper has tested the efficacy of applying au-tomatic subjectivity quantification to the problemof AO.
The reported results highlight the utility ofsuch semantically oriented approaches.
Althoughdirect comparison with existing systems wasbeyond the scope of this study, exploratory analys-es suggested that a refined version of this systemmight compare favorably with reported bench-marks, if trained on a corpus of comparable size.Nevertheless, the comparatively weak per-formance of the present system on previously seenexamples (?underfitting?, see Table 2) is strongevidence that six features alone are insufficient tocapture the complexity of ordering patterns.Therefore, beyond the adjustments discussedabove, the next stage in this research will evaluatethe effects of combining semantic features withdirect evidence in a single system.
Other futurework might apply subjectivity features to clusteradjectives into classes pertinent to AO, perhaps incombination with independent distributional meas-ures of semantic similarity.
Finally, the approachpresented here for English AO could have applica-tions across languages, and may also be applicableto related tasks, such as ordering binomials5, pars-ing noun phrases (?wild animal hunt?
vs. ?wildbirthday party?)
and selecting thematically appro-priate modifiers for a given head noun.Some interesting theoretical insights alsoemerge as a corollary to the results of this study.The supposition that gradability, polarity, adver-biability, predicativity and ?nouniness?
can be as-sociated, either positively or negatively, withsubjectivity, was confirmed.
Moreover, the per-formance of the classifier lends support to the sta-tus of subjectivity as a determining principle ofAO, and an important dimension of adjective se-mantics in general.
As such, the reason we saybeautiful English rose, (c.240,000 direct matcheson Google) and not English beautiful rose(c.2,730) is because beauty is in the eye of the be-holder, whereas nationality, evidently, is not.5 Binomials are noun or adjective combinations separated bycoordinating conjunctions, such as tired and emotional andsalt and pepper.
Quirk et al (1985, p. 1342) observe connec-tions between binomial ordering and AO.AcknowledgmentsThanks to Anna Korhonen, Paula Buttery and Syl-via Adamson for helpful guidance and comments.ReferencesAdamson, S.  2000.
Word Order Options and CategoryShift in the Premodifying String.
In O. Fischer,Pathways of Change: Grammaticalization in English(pp.
39-66).
Amsterdam: John Benjamins.Bolinger, D.  1967.
Adjectives in English: Attributionand Predication.
Lingua 18 ,  1-34.Dunlop, A., Mitchell, M. & Roark, B.
2010.Prenominal Modifier Ordering via Multiple SequenceAlignment.
2010 Annual Conference of the .orthAmerican Chapter of the ACL (HLT-.AACL 2010).Hatzivassiloglou, V. & McKeown, K.  1997.
Predictingthe Semantic Orientation of Adjectives.
AnnualMeeting Assoc.
Comp.Ling.
ACL '97, 174-181.Hatzivassiloglou, V. & Wiebe, J.
2000.
Effects ofAdjective Orientation and Gradability on SentenceSubjectivity.
International Conference onComputational Linguistics, COLI.G- '00.Hetzron, R.  1978.
On the Relative Order of Adjectives.In I. H.
(Ed.
),  Language Universals.
T?bingen: Narr.Langacker, R. 1991.
Foundations of CognitiveGrammar.
Stanford, CA: Stanford University Press.Malouf, R.  2000.
The Order of Prenominal Adjectivesin Natural Language Generation.
Proc.
38th AnnualMeeting, Assoc.
Comp.
Linguistics, ACL ?00, 85?92.Mitchell, M.  2009.
Class-based Ordering ofPrenominal Modifiers.
Proc.12th EuropeanWorkshop, .at.Lang.
Generation, E.LG '09, 50?57.Mitchell, M. Dunlop, A.
& Roark, B.
2011.
Semi-Supervised Modeling for Prenominal ModifierOrdering.
Proc.
49th Annual Meeting of the Assoc.Comp.
Ling., ACL '11,  236?241.Paradis, C.  1997.
Degree Modifiers of Adjectives inSpoken British English.
Lund: Lund University Press.Press, S. J.
& Wilson, S.  1978.
Choosing BetweenLogistic Regression and Discriminant Analysis.Journal of American Statistical Association,  699-705.Quirk, R. Greenbaum, A. Leech, G. & Svartvik, J.1985.
A Comprehensive Grammar of the EnglishLanguage.
London: Longmans.Shaw, J.
& Hatzivassiloglou, V. 1999.
Ordering AmongPremodifiers.
Proc.
37th Annual Meeting, Associationof Computational Linguistics, ACL ?99 , 135?143.Wiebe, J.
2000.
Learning Subjective Adjectives fromCorpora.
Proc.
17th .ational Conference on ArtificialIntelligence (AAAI-2000).Wulff, S.  2003.
A Multifactorial Analysis of AdjectiveOrder in English.
International Journal of CorpusLinguistics, 245?282.16
