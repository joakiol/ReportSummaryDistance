Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 281?288,Sydney, July 2006. c?2006 Association for Computational LinguisticsEnhancing electronic dictionaries with an index based on associationsOlivier FerretCEA ?LIST/LIC2M18 Route du PanoramaF-92265 Fontenay-aux-Rosesferreto@zoe.cea.frMichael Zock1LIF-CNRS163 Avenue de LuminyF-13288 Marseille Cedex 9michael.zock@lif.univ-mrs.frAbstractA good dictionary contains not onlymany entries and a lot of informationconcerning each one of them, but alsoadequate means to reveal the stored in-formation.
Information access dependscrucially on the quality of the index.
Wewill present here some ideas of how adictionary could be enhanced to support aspeaker/writer to find the word s/he islooking for.
To this end we suggest toadd to an existing electronic resource anindex based on the notion of association.We will also present preliminary work ofhow a subset of such associations, for ex-ample, topical associations, can be ac-quired by filtering a network of lexicalco-occurrences extracted from a corpus.1 IntroductionA dictionary user typically pursues one of twogoals (Humble, 2001): as a decoder (reading,listening), he may look for the definition or thetranslation of a specific target word, while as anencoder (speaker, writer) he may want to find aword that expresses well not only a given con-cept, but is also appropriate in a given context.Obviously, readers and writers come to thedictionary with different mindsets, informationand expectations concerning input and output.While the decoder can provide the word he wantsadditional information for, the encoder (languageproducer) provides the meaning of a word forwhich he lacks the corresponding form.
In sum,users with different goals need access to differentindexes, one that is based on form (decoding),1 In alphabetical orderthe other being based on meaning or meaningrelations (encoding).Our concern here is more with the encoder, i.e.lexical access in language production, a featurelargely neglected in lexicographical work.
Yet, agood dictionary contains not only many entriesand a lot of information concerning each one ofthem, but also efficient means to reveal thestored information.
Because, what is a huge dic-tionary good for, if one cannot access the infor-mation it contains?2 Lexical access on the basis of what:concepts (i.e.
meanings) or words?Broadly speaking, there are two views concern-ing lexicalization: the process is conceptually-driven (meaning, or parts of it are the startingpoint) or lexically-driven2 : the target word isaccessed via a source word.
This is typically thecase when we are looking for a synonym, anto-nym, hypernym (paradigmatic associations), orany of its syntagmatic associates (red-rose, cof-fee-black), the kind of association we will beconcerned with here.Yet, besides conceptual knowledge, peopleseem also to know a lot of things concerning thelexical form (Brown and Mc Neill, 1966): num-ber of syllables, beginning/ending of the targetword, part of speech (noun, verb, adjective, etc.
),origin (Greek or Latin), gender (Vigliocco et al,2 Of course, the input can also be hybrid, that is, it can becomposed of a conceptual and a linguistic component.
Forexample, in order to express the notion of intensity, MAGN inMel?Auk?s theory (Mel?Auk et al, 1995), a speaker or writerhas to use different words (very, seriously, high) dependingon the form of the argument (ill, wounded, price), as he saysvery ill, seriously wounded, high price.
In each case he ex-presses the very same notion, but by using a different word.While he could use the adverb very for qualifying the stateof somebody?s health (he is ill), he cannot do so when quali-fying the words injury or price.
Likewise, he cannot use thisspecific adverb to qualify the noun illness.2811997).
While in principle, all this informationcould be used to constrain the search space, wewill deal here only with one aspect, the words?relations to other concepts or words (associativeknowledge).Suppose, you were looking for a word expressingthe following ideas: domesticated animal, pro-ducing milk suitable for making cheese.
Supposefurther that you knew that the target word wasneither cow, buffalo nor sheep.
While none ofthis information is sufficient to guarantee theaccess of the intended word goat, the informationat hand (part of the definition) could certainly beused3.
Besides this type of information, peopleoften have other kinds of knowledge concerningthe target word.
In particular, they know how thelatter relates to other words.
For example, theyknow that goats and sheep are somehow con-nected, sharing a great number of features, thatboth are animals (hypernym), that sheep are ap-preciated for their wool and meat, that they tendto follow each other blindly, etc., while goatsmanage to survive, while hardly eating anything,etc.
In sum, people have in their mind a hugelexico-conceptual network, with words 4 , con-cepts or ideas being highly interconnected.Hence, any one of them can evoke the other.
Thelikelihood for this to happen depends on suchfactors as frequency (associative strength), sali-ency and distance (direct vs. indirect access).
Asone can see, associations are a very general andpowerful mechanism.
No matter what we hear,read or say, anything is likely to remind us ofsomething else.
This being so, we should makeuse of it.3 For some concrete proposals going in this direction, seedictionaries offering reverse lookup: http://www.ultralingua.net/ ,http://www.onelook.com/reverse-dictionary.shtml.4 Of course, one can question the very fact that people storewords in their mind.
Rather than considering the humanmind as a wordstore one might consider it as a wordfactory.Indeed, by looking at some of the work done by psycholo-gists who try to emulate the mental lexicon (Levelt et al,1999) one gets the impression that words are synthesizedrather than located and call up.
In this case one might con-clude that rather than having words in our mind we have aset of highly distributed, more or less abstract information.By propagating energy rather than data ?
(as there is nomessage passing, transformation or cumulation of informa-tion, there is only activation spreading, that is, changes ofenergy levels, call it weights, electronic impulses, or what-ever),?
that we propagate signals, activating ultimatelycertain peripherical organs (larynx, tongue, mouth, lips,hands) in such a way as to produce movements or sounds,that, not knowing better, we call words.3 Accessing the target word by navigat-ing in a huge associative networkIf one agrees with what we have just said, onecould view the mental lexicon as a huge semanticnetwork composed of nodes (words and con-cepts) and links (associations), with either beingable to activate the other5.
Finding a word in-volves entering the network and following thelinks leading from the source node (the firstword that comes to your mind) to the target word(the one you are looking for).
Suppose youwanted to find the word nurse (target word), yetthe only token coming to your mind is hospital.In this case the system would generate internallya graph with the source word at the center and allthe associated words at the periphery.
Put differ-ently, the system would build internally a seman-tic network with hospital in the center and all itsassociated words as satellites (see Figure 1, nextpage).Obviously, the greater the number of associa-tions, the more complex the graph.
Given thediversity of situations in which a given objectmay occur we are likely to build many associa-tions.
In other words, lexical graphs tend to be-come complex, too complex to be a good repre-sentation to support navigation.
Readability ishampered by at least two factors: high connec-tivity (the great number of links or associationsemanating from each word), and distribution:conceptually related nodes, that is, nodes acti-vated by the same kind of association are scat-tered around, that is, they do not necessarily oc-cur next to each other, which is quite confusingfor the user.
In order to solve this problem, wesuggest to display by category (chunks) all thewords linked by the same kind of association tothe source word (see Figure 2).
Hence, ratherthan displaying all the connected words as a flatlist, we suggest to present them in chunks to al-low for categorial search.
Having chosen a cate-gory, the user will be presented a list of words orcategories from which he must choose.
If thetarget word is in the category chosen by the user(suppose he looked for a hypernym, hence hechecked the ISA-bag), search stops, otherwise itcontinues.
The user could choose either anothercategory (e.g.
AKO or TIORA), or a word in thecurrent list, which would then become the newstarting point.5 While the links in our brain may only be weighted, theyneed to be labelled to become interpretable for human be-ings using them for navigational purposes in a lexicon.282DENTISTassistantnear-synonymGYNECOLOGISTPHYSICIANHEALTHINSTITUTIONCLINICDOCTORSANATORIUMPSYCHIATRIC HOSPITALMILITARY HOSPITALASYLUMtreatA OKtake care oftreatHOSPITALPATIENTINMATETIORAsynonymISA A OK A OKA OKA OKISAISA ISAISAISATIORATIORAnurseInternal RepresentationFigure 1: Search based on navigating in a network (internal representation)AKO: a kind of; ISA: subtype; TIORA: Typically Involved Object, Relation or Actor.list of potential target words (LOPTW)source wordlinklinklinklinklinkLOPTWLOPTWlist of potential target words (LOPTW)...Abstract representation of the search graphhospitalTIORAISAAKO clinic, sanatorium, ...military hospital, psychiatric hospitalinmateSYNONYMnursedoctor, ...patient...A concrete exampleFigure 2: Proposed candidates, grouped by fam-ily, i.e.
according to the nature of the linkAs one can see, the fact that the links are labeledhas some very important consequences:(a) While maintaining the power of a highlyconnected graph (possible cyclic navigation),it has at the interface level the simplicity of atree: each node points only to data of thesame type, i.e.
to the same kind of associa-tion.
(b) With words being presented in clusters,navigation can be accomplished by clickingon the appropriate category.The assumption being that the user generallyknows to which category the target word belongs(or at least, he can recognize within which of thelisted categories it falls), and that categoricalsearch is in principle faster than search in a hugelist of unordered (or, alphabetically ordered)words6.Obviously, in order to allow for this kind ofaccess, the resource has to be built accordingly.This requires at least two things: (a) indexingwords by the associations they evoke, (b) identi-6 Even though very important, at this stage we shall notworry too much for the names given to the links.
Indeed,one might question nearly all of them.
What is important isthe underlying rational: help users to navigate on the basisof symbolically qualified links.
In reality a whole set ofwords (synonyms, of course, but not only) could amount toa link, i.e.
be its conceptual equivalent.283fying and labelling the most frequent/useful as-sociations.
This is precisely our goal.
Actually,we propose to build an associative network byenriching an existing electronic dictionary (es-sentially) with (syntagmatic) associations comingfrom a corpus, representing the average citizen?sshared, basic knowledge of the world (encyclo-paedia).
While some associations are too com-plex to be extracted automatically by machine,others are clearly within reach.
We will illustratein the next section how this can be achieved.4 Automatic extraction of topical rela-tions4.1 Definition of the problemWe have argued in the previous sections that dic-tionaries must contain many kinds of relations onthe syntagmatic and paradigmatic axis to allowfor natural and flexible access of words.
Synon-ymy, hypernymy or meronymy fall clearly in thislatter category, and well known resources likeWordNet (Miller, 1995), EuroWordNet (Vossen,1998) or MindNet (Richardson et al, 1998) con-tain them.
However, as various researchers havepointed out (Harabagiu et al, 1999), these net-works lack information, in particular with regardto syntagmatic associations, which are generallyunsystematic.
These latter, called TIORA (Zockand Bilac, 2004) or topical relations (Ferret,2002) account for the fact that two words refer tothe same topic, or take part in the same situationor scenario.
Word-pairs like doctor?hospital,burglar?policeman or plane?airport, are exam-ples in case.
The lack of such topical relations inresources like WordNet has been dubbed as thetennis problem (Roger Chaffin, cited in Fell-baum, 1998).
Some of these links have been in-troduced more recently in WordNet via the do-main relation.
Yet their number remains still verysmall.
For instance, WordNet 2.1 does not con-tain any of the three associations mentioned hereabove, despite their high frequency.The lack of systematicity of these topical rela-tions makes their extraction and typing very dif-ficult on a large scale.
This is why some re-searchers have proposed to use automatic learn-ing techniques to extend lexical networks likeWordNet.
In (Harabagiu & Moldovan, 1998),this was done by extracting topical relations fromthe glosses associated to the synsets.
Other re-searchers used external sources: Mandala et al(1999) integrated co-occurrences and a thesaurusto WordNet for query expansion; Agirre et al(2001) built topic signatures from texts in rela-tion to synsets; Magnini and Cavagli?
(2000)annotated the synsets with Subject Field Codes.This last idea has been taken up and extended by(Avancini et al, 2003) who expanded the do-mains built from this annotation.Despite the improvements, all these ap-proaches are limited by the fact that they rely tooheavily on WordNet and some of its more so-phisticated features (such as the definitions asso-ciated with the synsets).
While often being ex-ploited by acquisition methods, these features aregenerally lacking in similar lexico-semantic net-works.
Moreover, these methods attempt to learntopical knowledge from a lexical network ratherthan topical relations.
Since our goal is different,we have chosen not to rely on any significantresource, all the more as we would like ourmethod to be applicable to a wide array of lan-guages.
In consequence, we took an incrementalapproach (Ferret, 2006): starting from a networkof lexical co-occurrences7 collected from a largecorpus, we used these latter to select potentialtopical relations by using a topical analyzer.4.2 From a network of co-occurrences to aset of Topical UnitsWe start by extracting lexical co-occurrencesfrom a corpus to build a network.
To this end wefollow the method introduced by (Church andHanks, 1990), i.e.
by sliding a window of a givensize over some texts.
The parameters of this ex-traction were set in such a way as to catch themost obvious topical relations: the window wasfairly large (20-words wide), and while it tooktext boundaries into account, it ignored the orderof the co-occurrences.
Like (Church and Hanks,1990), we used mutual information to measurethe cohesion between two words.
The finite sizeof the corpus allows us to normalize this measurein line with the maximal mutual informationrelative to the corpus.This network is used by TOPICOLL (Ferret,2002), a topic analyzer, which performs simulta-neously three tasks, relevant for this goal: it segments texts into topically homogene-ous segments; it selects in each segment the most repre-sentative words of its topic;7 Such a network is only another view of a set of co-occurrences: its nodes are the co-occurrent words and itsedges are the co-occurrence relations.284 it proposes a restricted set of words fromthe co-occurrence network to expand theselected words of the segment.These three tasks rely on a common mecha-nism: a window is moved over the text to be ana-lyzed in order to limit the focus space of theanalysis.
This latter contains a lemmatized ver-sion of the text?s plain words.
For each positionof this window, we select only words of the co-occurrence network that are linked to at leastthree other words of the window (see Figure 3).This leads to select both words that are in thewindow (first order co-occurrents) and wordscoming from the network (second order co-occurrents).
The number of links between theselected words of the network, called expansionwords, and those of the window is a good indica-tor of the topical coherence of the window?s con-tent.
Hence, when their number is small, a seg-ment boundary can be assumed.
This is the basicprinciple underlying our topic analyzer.0.140.21 0.100.18 0.130.17w5w4w3w2w10.48 = pw3x0.18+pw4x0.13+pw5x0.17selected word from the co-occurrence network (with its weight)1.0word from text (with p its weight in the window, equal to0.21 link in the co-occurrence network (with its cohesion value)1.0 1.0 1.0 1.0 1.0wi,n1 n21.0 for all words of the window in this example)0.48Figure 3: Selection and weighting of wordsfrom the co-occurrence networkThe words selected for each position of thewindow are summed, to keep only those occur-ring in 75% of the positions of the segment.
Thisallows reducing the number of words selectedfrom non-topical co-occurrences.
Once a corpushas been processed by TOPICOLL, we obtain aset of segments and a set of expansion words foreach one of them.
The association of the selectedwords of a segment and its expansion words iscalled a Topical Unit.
Since both sets of wordsare selected for reasons of topical homogeneity,their co-occurrence is more likely to be a topicalrelation than in our initial network.4.3 Filtering of Topical UnitsBefore recording the co-occurrences in the Topi-cal Units built in this way, the units are filteredtwice.
The first filter aims at discarding hetero-geneous Topical Units, which can arise as a sideeffect of a document whose topics are so inter-mingled that it is impossible to get a reliable lin-ear segmentation of the text.
We consider thatthis occurs when for a given text segment, noword can be selected as a representative of thetopic of the segment.
Moreover, we only keepthe Topical Units that contain at least two wordsfrom their original segment.
A topic is definedhere as a configuration of words.
Note that theidentification of such a configuration cannot bebased solely on a single word.Text words Expansion wordssurveillance(watch)police_judiciaire(judiciary police)t?l?phonique(telephone)?crouer(to imprison)juge(judge)garde_?_vue(police custody)policier(policeman)?coute_t?l?phonique(phone tapping)brigade(squad)juge_d?instruction(examining judge)enqu?te(investigation)contr?le_judiciaire(judicial review)placer(to put)Table 1: Content of a filtered Topical UnitThe second filter is applied to the expansionwords of each Topical Unit to increase their topi-cal homogeneity.
The principle of the filtering ofthese words is the same as the principle of theirselection described in Section 4.2: an expansionword is kept if it is linked in the co-occurrencenetwork to at least three text words of the Topi-cal Unit.
Moreover, a selective threshold is ap-plied to the frequency and the cohesion of the co-occurrences supporting these links: only co-occurrences whose frequency and cohesion arerespectively higher or equal to 15 and 0.15 areused.
For instance in Table 1, which shows anexample of a Topical Unit after its filtering,?crouer (to imprison) is selected, because it islinked in the co-occurrence network to the fol-lowing words of the text:juge (judge): 52 (frequency) ?
0.17 (cohesion)policier (policeman): 56 ?
0.17enqu?te (investigation): 42 ?
0.16285word freq.
word freq.
word freq.
word freq.sc?ne(stage) 884th?
?tral(dramatic) 62cynique(cynical) 26sc?nique(theatrical) 14th?
?tre(theater) 679sc?nariste(scriptwriter) 51miss(miss) 20Chabol(Chabol) 13r?alisateur(director) 220comique(comic) 51parti_pris(bias) 16Tchekov(Tchekov) 13cin?aste(film-marker) 135oscar(oscar) 40monologue(monolog) 15allocataire(beneficiary) 13com?die(comedy) 104film_am?ricain(american film) 38revisiter(to revisit) 14satirique(satirical) 13costumer(to dress up) 63hollywoodien(Hollywood) 30gros_plan(close-up) 14Table 2: Co-occurrents of the word acteur (actor) with a cohesion of 0.16(the co-occurrents removed by our filtering method are underlined)4.4 From Topical Units to a network oftopical relationsAfter the filtering, a Topical Unit gathers a set ofwords supposed to be strongly coherent from thetopical point of view.
Next, we record the co-occurrences between these words for all theTopical Units remaining after filtering.
Hence,we get a large set of topical co-occurrences, de-spite the fact that a significant number of non-topical co-occurrences remains, the filtering ofTopical Units being an unsupervised process.The frequency of a co-occurrence in this case isgiven by the number of Topical Units containingboth words simultaneously.
No distinction con-cerning the origin of the words of the TopicalUnits is made.The network of topical co-occurrences builtfrom Topical Units is a subset of the initial net-work.
However, it also contains co-occurrencesthat are not part of it, i.e.
co-occurrences thatwere not extracted from the corpus used for set-ting the initial network or co-occurrences whosefrequency in this corpus was too low.
Only someof these ?new?
co-occurrences are topical.
Sinceit is difficult to estimate globally which ones areinteresting, we have decided to focus our atten-tion only on the co-occurrences of the topicalnetwork already present in the initial network.Thus, we only use the network of topical co-occurrences as a filter for the initial co-occurrence network.
Before doing so, we filterthe topical network in order to discard co-occurrences whose frequency is too low, that is,co-occurrences that are unstable and not repre-sentative.
From the use of the final network byTOPICOLL (see Section 4.5), we set the thresh-old experimentally to 5.
Finally, the initial net-work is filtered by keeping only co-occurrencespresent in the topical network.
Their frequencyand cohesion are taken from the initial network.While the frequencies given by the topical net-work are potentially interesting for their topicalsignificance, we do not use them because theresults of the filtering of Topical Units are toohard to evaluate.4.5 Results and evaluationWe applied the method described here to an ini-tial co-occurrence network extracted from a cor-pus of 24 months of Le Monde, a major Frenchnewspaper.
The size of the corpus was around 39million words.
The initial network contained18,958 words and 341,549 relations.
The first runproduced 382,208 Topical Units.
After filtering,we kept 59% of them.
The network built fromthese Topical Units was made of 11,674 wordsand 2,864,473 co-occurrences.
70% of these co-occurrences were new with regard to the initialnetwork and were discarded.
Finally, we got afiltered network of 7,160 words and 183,074 re-lations, which represents a cut of 46% of the ini-tial network.
A qualitative study showed thatmost of the discarded relations are non-topical.This is illustrated by Table 2, which gives the co-occurrents of the word acteur (actor) that arefiltered by our method among its co-occurrentswith a high cohesion (equal to 0.16).
For in-stance, the words cynique (cynical) or allocataire(beneficiary) are cohesive co-occurrents of the286word actor, even though they are not topicallylinked to it.
These words are filtered out, whilewe keep words like gros_plan (close-up) or sc?-nique (theatrical), which topically cohere withacteur (actor) despite their lower frequency thanthe discarded words.Recall8 Precision F1-measureError(Pk)9initial (I) 0.85 0.79 0.82 0.20topicalfiltering(T)0.85 0.79 0.82 0.21frequencyfiltering(F)0.83 0.71 0.77 0.25Table 3: TOPICOLL?s resultswith different networksIn order to evaluate more objectively ourwork, we compared the quantitative results ofTOPICOLL with the initial network and its fil-tered version.
The evaluation showed that theperformance of the segmenter remains stable,even if we use a topically filtered network (seeTable 3).
Moreover, it became obvious that anetwork filtered only by frequency and cohesionperforms significantly less well, even with acomparable size.
For testing the statistical sig-nificance of these results, we applied to the Pkvalues a one-side t-test with a null hypothesis ofequal means.
Levels lower or equal to 0.05 areconsidered as statistically significant:pval (I-T): 0.08pval (I-F): 0.02pval (T-F): 0.05These values confirm that the difference be-tween the initial network (I) and the topicallyfiltered one (T) is actually not significant,whereas the filtering based on co-occurrence fre-quencies leads to significantly lower results, bothcompared to the initial network and the topicallyfiltered one.
Hence, one may conclude that our8 Precision is given by Nt / Nb and recall by Nt / D, with Dbeing the number of document breaks, Nb the number ofboundaries found by TOPICOLL and Nt the number ofboundaries that are document breaks (the boundary shouldnot be farther than 9 plain words from the document break).9 Pk (Beeferman et al, 1999) evaluates the probability that arandomly chosen pair of words, separated by k words, iswrongly classified, i.e.
they are found in the same segmentby TOPICOLL, while they are actually in different ones (missof a document break), or they are found in different seg-ments, while they are actually in the same one (false alarm).method is an effective way of selecting topicalrelations by preference.5 Discussion and conclusionWe have raised and partially answered the ques-tion of how a dictionary should be indexed inorder to support word access, a question initiallyaddressed in (Zock, 2002) and (Zock and Bilac,2004).
We were particularly concerned with thelanguage producer, as his needs (and knowledgeat the onset) are quite different from the ones ofthe language receiver (listener/reader).
It seemsthat, in order to achieve our goal, we need to dotwo things: add to an existing electronic diction-ary information that people tend to associate witha word, that is, build and enrich a semantic net-work, and provide a tool to navigate in it.
To thisend we have suggested to label the links, as thiswould reduce the graph complexity and allow fortype-based navigation.
Actually our basic pro-posal is to extend a resource like WordNet byadding certain links, in particular on the syntag-matic axis.
These links are associations, and theirrole consists in helping the encoder to find ideas(concepts/words) related to a given stimulus(brainstorming), or to find the word he is think-ing of (word access).One problem that we are confronted with is toidentify possible associations.
Ideally we wouldneed a complete list, but unfortunately, this doesnot exist.
Yet, there is a lot of highly relevantinformation out there.
For example, Mel?cuk?slexical functions (Mel?cuk, 1995), Fillmore?sFRAMENET10, work on ontologies (CYC), thesau-rus (Roget), WordNets (the original version fromPrinceton, various Euro-WordNets, BalkaNet),HowNet11, the work done by MICRA, the FACTO-TUM project 12 , or the Wordsmyth diction-ary/thesaurus13.Since words are linked via associations, it isimportant to reveal these links.
Once this is done,words can be accessed by following these links.We have presented here some preliminary workfor extracting an important subset of such linksfrom texts, topical associations, which are gener-ally absent from dictionaries or resources likeWordNet.
An evaluation of the topic segmenta-tion has shown that the relations extracted aresound from the topical point of view, and thatthey can be extracted automatically.
However,10 http://www.icsi.berkeley.edu/~framenet/11 http://www.keenage.com/html/e_index.html12 http://humanities.uchicago.edu/homes/MICRA/13 http://www.wordsmyth.com/287they still contain too much noise to be directlyexploitable by an end user for accessing a wordin a dictionary.
One way of reducing the noise ofthe extracted relations would be to build fromeach text a representation of its topics and to re-cord the co-occurrences in these representationsrather than in the segments delimited by a topicsegmenter.
This is a hypothesis we are currentlyexploring.
While we have focused here only onword access on the basis of (other) words, oneshould not forget that most of the time speakersor writers start from meanings.
Hence, we shallconsider this point more carefully in our futurework, by taking a serious look at the proposalsmade by Bilac et al (2004); Durgar and Oflazer(2004), or Dutoit and Nugues (2002).ReferencesEneko Agirre, Olatz Ansa, David Martinez and Edu-ard Hovy.
2001.
Enriching WordNet concepts withtopic signatures.
In NAACL?01 Workshop onWordNet and Other Lexical Resources: Applica-tions, Extensions and Customizations.Henri Avancini, Alberto Lavelli, Bernardo Magnini,Fabrizio Sebastiani and Roberto Zanoli.
2003.
Ex-panding Domain-Specific Lexicons by Term Cate-gorization.
In 18th ACM Symposium on AppliedComputing (SAC-03).Doug Beeferman, Adam Berger and Lafferty.
1999.Statistical Models for Text Segmentation.
MachineLearning, 34(1): 177-210.Slaven Bilac, Wataru Watanabe, Taiichi Hashimoto,Takenobu Tokunaga and Hozumi Tanaka.
2004.Dictionary search based on the target word descrip-tion.
In Tenth Annual Meeting of The Associationfor Natural Language Processing (NLP2004),pages 556-559.Roger Brown and David McNeill.
1996.
The tip of thetongue phenomenon.
Journal of Verbal Learningand Verbal Behaviour, 5: 325-337.Kenneth Church and Patrick Hanks.
1990.
Word As-sociation Norms, Mutual Information, And Lexi-cography.
Computational Linguistics, 16(1): 177-210.Ilknur Durgar El-Kahlout and Kemal Oflazer.
2004.Use of Wordnet for Retrieving Words from TheirMeanings, In 2nd Global WordNet Conference,BrnoDominique Dutoit and Pierre Nugues.
2002.
A lexicalnetwork and an algorithm to find words from defi-nitions.
In 15th European Conference on ArtificialIntelligence (ECAI 2002), Lyon, pages 450-454,IOS Press.Christiane Fellbaum.
1998.
WordNet - An ElectronicLexical Database, MIT Press.Olivier Ferret.
2006.
Building a network of topicalrelations from a corpus.
In LREC 2006.Olivier Ferret.
2002.
Using collocations for topicsegmentation and link detection.
In COLING 2002,pages 260-266.Sanda M. Harabagiu, George A. Miller and Dan I.Moldovan.
1999.
WordNet 2 - A Morphologicallyand Semantically Enhanced Resource.
In ACL-SIGLEX99: Standardizing Lexical Resources,pages 1-8.Sanda M. Harabagiu and Dan I. Moldovan.
1998.Knowledge Processing on an Extended WordNet.In WordNet - An Electronic Lexical Database,pages 379-405.Philip Humble.
2001.
Dictionaries and LanguageLearners, Haag and Herchen.William Levelt, Ardi Roelofs and Antje Meyer.
1999.A theory of lexical access in speech production,Behavioral and Brain Sciences, 22: 1-75.Bernardo Magnini and Gabriela Cavagli?.
2000.
Inte-grating Subject Field Codes into WordNet.
InLREC 2000.Rila Mandala, Takenobu Tokunaga and HozumiTanaka.
1999.
Complementing WordNet with Ro-get?s and Corpus-based Thesauri for InformationRetrieval.
In EACL 99.Igor Mel?Auk, Arno Clas and Alain Polgu?re.
1995.Introduction ?
la lexicologie explicative et combi-natoire, Louvain, Duculot.George A. Miller.
1995.
WordNet: A lexical Data-base, Communications of the ACM.
38(11): 39-41.Stephen D. Richardson, William B. Dolan and LucyVanderwende.
1998.
MindNet: Acquiring andStructuring Semantic Information from Text.
InACL-COLING?98, pages 1098-1102.Piek Vossen.
1998.
EuroWordNet: A MultilingualDatabase with Lexical Semantic Networks.
KluwerAcademic Publisher.Gabriella Vigliocco, Antonini, T., and Merryl Garrett.1997.
Grammatical gender is on the tip of Italiantongues.
Psychological Science, 8: 314-317.Michael Zock.
2002.
Sorry, what was your nameagain, or how to overcome the tip-of-the tongueproblem with the help of a computer?
In SemaNetworkshop, COLING 2002, Taipei.http://acl.ldc.upenn.edu /W/W02/W02-1118.pdfMichael Zock and Slaven Bilac.
2004.
Word lookupon the basis of associations: from an idea to aroadmap.
In COLING 2004 workshop: Enhancingand using dictionaries, Geneva.http://acl.ldc.upenn.edu/ coling2004/W10/pdf/5.pdf288
