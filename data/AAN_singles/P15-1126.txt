Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1301?1310,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsOrthogonality of Syntax and Semantics within Distributional SpacesJeff MitchellSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKjeff.mitchell@ed.ac.ukMark SteedmanSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKsteedman@inf.ed.ac.ukAbstractA recent distributional approach to word-analogy problems (Mikolov et al, 2013b)exploits interesting regularities in thestructure of the space of representations.Investigating further, we find that per-formance on this task can be related toorthogonality within the space.
Explic-itly designing such structure into a neu-ral network model results in represen-tations that decompose into orthogonalsemantic and syntactic subspaces.
Wedemonstrate that learning from word-orderand morphological structure within En-glish Wikipedia text to enable this de-composition can produce substantial im-provements on semantic-similarity, pos-induction and word-analogy tasks.1 IntroductionDistributional methods have become widely usedacross computational linguistics.
Recent applica-tions include predicate clustering for question an-swering (Lewis and Steedman, 2013), bilingualembeddings for machine translation (Zou et al,2013) and enhancing the coverage of POS tag-ging (Huang et al, 2013).
The popularity of thesemethods, stemming from their conceptual simplic-ity and wide applicability, motivates a deeper anal-ysis of the structure of the representations theyproduce.Commonly, these representations are made in asingle vector space with similarity being the mainstructure of interest.
However, recent work byMikolov et al (2013b) on a word-analogy tasksuggests that such spaces may have further use-ful internal regularities.
They found that seman-tic differences, such as between big and small,and also syntactic differences, as between big andbigger, were encoded consistently across theirspace.
In particular, they solved the word-analogyproblems by exploiting the fact that equivalent re-lations tended to correspond to parallel vector-differences.In this paper, we investigate orthogonality be-tween relations rather than parallelism.
While par-allelism serves to ensure that the same relationis encoded consistently, our hypothesis is that or-thogonality serves to ensure that distinct relationsare clearly differentiable.
We focus specificallyon semantic and syntactic relations as these areprobably the most distinct classes of properties en-coded in distributional spaces.Empirically, we demonstrate that orthogonal-ity predicts performance on the word-analogy taskfor three existing approaches to constructing wordvectors.
We also attempt to enhance the weak-est of these three models by imposing an orthog-onal structure in its construction.
In these exten-sions, word representations decompose into or-thogonal semantic and syntactic spaces, and weuse word-order and morphology to drive this sep-aration.
This decomposition also allows us to de-fine a novel approach to solving the word-analogyproblems and our extended models become com-petitive with the other two original models.
Inaddition, we show that the separate semantic andsyntactic sub-spaces gain improved performanceon semantic-similarity and POS-induction tasksrespectively.Our experiments here are based on models thatconstruct vector-representations within a modelthat predicts the occurence of words in context.
Inparticular we focus on the CBOW and Skip-grammodels of Mikolov etal.
(2013b) and Penningtonet al?s (2014) GloVe model.
These models sharethe property of producing a single general repre-sentation for each word, which can be utilized ina variety of tasks, from POS tagging to semanticrole labelling.
In contrast, here we attempt to de-compose the representations into separate seman-1301llllsmallsmallerbigbiggerFigure 1: Geometric relationships between small,smaller, big and bigger.tic and syntactic components.To motivate this decomposition, consider theanalogical reasoning task that Mikolov et al(2013b) apply neural embeddings to.
In this task,given vectors for the words big, bigger and small,we try to predict the vector for smaller.
Theyfind that in practice smaller ?
small+ bigger?big produces an estimate that is frequently closerto the actual representation of smaller than anyother word vector.
We can think of the vectorbigger ?
big as representing the syntactic rela-tion that holds between an adjective and its com-parative.
Adding this syntactic structure to smallthus ends up at, or near, the relevant comparative,smaller.
Alternatively, we could think of the vec-tor small?big as representing the semantic differ-ence between small and big, and adding this rela-tion to bigger produces a semantic transformationto smaller.Mikolov et al (2013b) represent these sort ofrelations in terms of a diagram similar to Figure 1.The image places the four words in a 2D space andrepresents the relations between them in terms ofarrows.
The solid black arrows represent the syn-tactic relations smaller?small and bigger?big,while the gray dashed arrows represent the seman-tic differences smaller?bigger and small?big.Their solution to the analogy problem exploits thefact that these pairs of relations are approximatelyparallel to each other, i.e.
that we can approx-imate smaller ?
small with bigger ?
big, orsmaller ?
bigger with small ?
big.
However,knowing that opposite sides of the square in Fig-ure 1 are parallel to each other still leaves openthe question of what happens at the corners.
Inother words, what is the relationship between thesemantic differences, e.g.
smaller ?
bigger, andthe syntactic differences, e.g.
smaller ?
small?In this paper we explore the idea that such se-mantic and syntactic relations ought to be orthogo-nal to each other.
This hypothesis arises both fromthe intuition that such distinct types of informa-tion ought to be represented distinctly within ourspace and also from the observation that solvingthe word-analogy task requires that words can beuniquely identified by combining these vector dif-ferences and so small ?
big ought to be easilydifferentiable from bigger?
big as these relationspoint to different end results starting from big.
Es-sentially, orthogonality will make better use of thevolume within the space, spreading words withdifferent semantic or syntactic characteristics fur-ther from each other.In terms of predicting smaller from big, biggerand small, orthogonality of the relationship be-tween smaller ?
bigger and smaller ?
smallcan be expressed in terms of their dot product:(smaller?bigger)?
(smaller?small) = 0 (1)If all semantic relations were genuinely orthog-onal to all syntactic relations, then their spacewould be decomposable into two orthogonal sub-spaces: one semantic, the other syntactic.
Anyword representation, v, would then be the combi-nation of a unique semantic vector, b, within thesemantic subspace and a unique syntactic vector,s, within the syntactic subspace.
If b were given arepresentation in terms of e components, and s interms of f components, then v would have a repre-sentation in terms of d = e+f components whichwould just be the concatenation of the two sets ofcomponents, which we will represent in terms ofthe operator ?.v = b?
s (2)Achieving this differentiation within the repre-sentations requires that the model have a meansof differentiating semantic and syntactic informa-tion in the raw text.
We consider two very simpleapproaches for this purpose, based on morpholog-ical and word order features.
Both these types offeatures have been previously employed in simpleword co-occurrence models (e.g., McDonald andLowe, 1998; Clark, 2003), with bag-of-words and1302b?2wt?2b?1wt?1b1wt+1b2wt+2AAAAK3QQQQQQkbcontext6wtFigure 2: CBOW model predicting wtfrom of abag-of-words representation, bcontext, of a 4-wordwindow around it.lemmatization being good for semantic applica-tions, while sequential order and suffixes is moreuseful for syntax.
More recently, Mitchell (2013)demonstrated that word order could be used to sep-arate syntactic from semantic structure, but onlywithin a simple bigram language model, ratherthan a neural network model, and without exploit-ing morphology.Our enhanced models are based on Mikolovet al?s (2013a) CBOW architecture, which is de-scribed in Section 2.
The novel extensions toit, employing a semantic-syntactic decomposition,are proposed in Section 3.
We then describe ourevaluation tasks and provide their results in Sec-tions 5 and 6 respectively.
These evaluations arebased on the word-analogy dataset of Mikolov etal.
(2013b), a noun-verb similarity task (Mitchell,2013) and a POS clustering task.2 Continuous Bag-of-Words Model(CBOW)In the original CBOW model, the probability of acentral target word, wt, is predicted from a bag-of-words representation of the context it occurs in,as illustrated in Figure 2.
This context representa-tion, bcontext, is a simple sum of the CBOW vec-tors, bi, that represent each item,wt+i, in a k-wordwindow either side of the target.bcontext=k?i=?k,i6=0bi(3)For speed, the output layer uses a hierarchi-cal softmax function (Morin and Bengio, 2005).Each word is given a Huffman code correspond-ing to a path through a binary tree, and the outputpredicts the binary choices on nodes of the treeas independent variables.
In comparison to thecomputational cost of doing the full softmax overthe whole vocabulary, this hierarchical approach ismuch more efficient.Each node is associated with a vector, n, andthe output at that node, given a context vector,bcontext, is:p = logistic(n ?
bcontext) (4)Here, p is the probability of choosing 1 over 0at this node of the tree, or equivalently finding a 1in the Huffman code of wtat the relevant position.The objective function is the negative log-likelihood of the data given the model.O =??
log(p) (5)Where the sum is over tokens in the training cor-pus and the relevant nodes in the tree.
Training isthen based on stochastic gradient descent, with adecreasing learning rate.3 Extensions3.1 Continuous Sequence of Words (CSOW)A major feature of the CBOW model is its use ofa bag-of-words representation of the context andthis is achieved by summing over the vectors rep-resenting words in the input.
Although the modeldoes seem to produce representations that are ef-fective on both semantic and syntactic tasks, wewant to be able to exploit word order informationto separate these two characteristics.
We thereforeneed to consider models which do not reduce thecontext to a structureless bag-of-words.
Modify-ing the original model to retain the sequential in-formation in the input is relatively straightforward.Instead of summing the input representations, wesimply leave them as an ordered sequence of vec-tors, si.Then in the output layer, we require a vector forevery input position, i, on every node.
In this way,the output of the network depends on which con-text word is in which position, rather than just theset of words, irrespective of position in the input.The network still learns a single representationfor each word independently of position, but theoutput function has more parameters.1303p = logistic(k?i=?k,i6=0ni?
si) (6)Here each node of the tree is associated with onevector, ni, for each position, i, in the input context,giving 2k vectors in total at each node.3.2 Continuous Bag and Sequence of Words(CBSOW)Having introduced a sequential version of theCBOW model, what is really desired is a modelthat combines both bag and sequence components.Each word will have both an e-dimensional bag-vector b and an f -dimensional sequence-vector s.The full representation of a word, v, is then theconcatenation of the components of b and s.Given this structure, the representation of a con-text of 2k words will be made up of the sum,bcontext, of their bag vectors, bi, as in the CBOWmodel given by Equation 3, along with the orderedsequence vectors, si, as in the CSOW model.
Eachnode in the tree then requires both a bag vector, nb,to handle the bag context, and 2k sequence vec-tors, nsi, to handle the sequence context vectors,with probabilities given by:p = logistic(nb?
bcontext+k?i=?k,i6=0nsi?
si) (7)3.3 Continuous Bag of Morphemes (CBOM)A second source of information which might beused to differentiate semantic from syntactic rep-resentations is morphology.
Specifically, Englishhas the useful characteristic that the written wordsthemselves can often be broken into a semanticstem on the left and a syntactic ending on the right.For example, dancing = dance + ing and swim-mer = swim + er.
In fact, stemming or lemma-tization is commonly used in constructing distri-butional vectors precisely because throwing awaythe syntactic information helps to enhance their se-mantic content.
Here, we want to use both the leftand right halves separately to enhance both the se-mantic and syntactic components of the represen-tations.Our starting point is to break each word intoa left-hand stem and a right-hand ending usingCELEX (Baayen et al, 1995), as explained inmore detail in Section 4.1.The simplest model is then to represent each ofthese with its own vector, liand rirespectively,and sum these vectors to form context representa-tions of words in the input.lcontext=k?i=?k,i6=0li(8)rcontext=k?i=?k,i6=0ri(9)The output function takes much the same formas the original model but now each node needsboth a left and a right vector, corresponding to thetwo context representations.p = logistic(nl?
lcontext+ nr?
rcontext) (10)3.4 Continuous Bag and Sequence of Wordsand Morphemes (CBSOWM)Finally, we want to incorporate all these elementsin a single model, with the morphological andword order elements of the model working in har-mony.
In particular, we want the sequential partof the model to be guided by morphological infor-mation without being constrained to give all wordswith same ending the same representation.
Oursolution is to add a constraint term to the objec-tive function, which penalizes sequence vectorsthat stray far from the relevant morphological rep-resentation.
The bag vectors, in contrast, are de-termined directly by the left hand stems, with allwords having the same stem then sharing the samebag vector, b = l.The main structure of the model remains as inthe CBSOW model, with the context being rep-resented by the sum of bag vectors alongside theordered sequence vectors.
Output probabilities areas given by Equation 7, and we add a morpholog-ical penalty, m, to the objective function.m =k?i=?k,i6=012?|si?
ri|2(11)The morphological representations r enter intothe model only through the penalty term, and theyadapt during training solely in terms of this in-teraction with the sequence vectors.
Gradient de-scent results in the r vectors moving towards thecentre of the corresponding s vectors, and the svectors in turn being drawn towards that centre.1304The result is to elastically connect all the s vec-tors corresponding to a single morphological ele-ment through their r vectors, so that they are drawntogether, but can still develop idiosyncratically ifthere is sufficient evidence in the data.3.5 Application to the Word-Analogy TaskDecomposition of representations into separate se-mantic and syntactic spaces enables us to utilise anew approach to solving the word-analogy prob-lems.
Rather than using vector differences topredict a vector, we can instead construct it bycopying the relevant bag and sequence vectors.So, since small and smaller share very similarsemantic content, we can use the bag vector ofsmall as the bag vector of smaller, since that iswhere the semantic content is mainly represented:bsmaller?
bsmall.
Similarly, we can use the se-quence vector of bigger as the sequence vector forsmaller, since these words share common syntac-tic behaviour: ssmaller?
sbigger.The predicted representation of smaller is thengiven by the concatenation of the components.vsmaller?
bsmall?
sbigger(12)We find that this gives the best performance onthe models that use word-order features (CBSOWand CBSOWM).4 TrainingOur experiments are based on the publicly avail-able word2vec1and GloVe2packages.
We mod-ified the original CBOW code to incorporate theCBSOW, CBOM and CBSOWM extensions de-scribed above, and trained models on three En-glish Wikipedia corpora of varying sizes, includ-ing the enwik8 and enwik9 files3suggested in theword2vec documentation, containing the first 108and 109characters of a 2006 download, and alsoa full download from 2009.
On the smallest 17Mword corpus we explored a range of vector dimen-sionalities from 10 to 1000.
On the larger 120Mand 1.6B word corpus, we trained extended mod-els with a 200-dimensional semantic componentand a 100-dimensional syntactic component com-paring to 300-dimensional CBOW, Skip-gram andGloVe models.
The parameter, ?, in Equation 11was set to 0.1 and the recommended window sizes1https://code.google.com/p/word2vec/2http://nlp.stanford.edu/projects/glove/3http://mattmahoney.net/dc/text.htmlof 5, 10 and 15 words either side of the centralword were used as context for the CBOW, Skip-gram and GloVe models respectively.4.1 CELEXWe attempted to split all the words in the trainingdata into a left hand and a right hand using CELEX(Baayen et al, 1995), an electronic dictionary con-taining morphological structure.
In the cases ofwords that were not found in the dictionary andalso those that were found but had no morpholog-ical substructure, the left hand was just the wholeword and the right hand was a ?NULL?
token.For the remaining words, we treated short suf-fixes as being syntactic inflections and stripped allthese off to leave a left hand ?semantic?
compo-nent.
The ?syntactic?
component was then right-most of these suffixes, with any additional suffixesbeing ignored.5 EvaluationThe hypothesis that orthogonality is useful to wordvector representations is investigated empiricallyin two ways.
Firstly, we attempt to quantify theorthogonality that is already implicitly present inthe original CBOW, Skip-gram and GloVe repre-sentations and relate that to their success in theword-analogy task.
Secondly, the extensions de-scribed above are evaluated on a number of tasksin order to evaluate the benefits of their explicitorthogonality between components.5.1 Orthogonality within the Original ModelsEquation 1 relates orthogonality of vector differ-ences to their dot product being zero, which cor-responds to the fact the cosine of 90?is zero.Thus, we can use the cosine as a quantifica-tion of how close to orthogonal the vector dif-ferences are and then relate that to performanceon the word-analogy dataset distributed with theword2vec toolkit.That task involves predicting a word vectorgiven vectors for other related words.
So, for ex-ample, given vectors for big, bigger and small,we would try to predict a vector for smaller.
Wethen judge the success of this prediction in termsof whether the predicted vector is in fact closer tosmaller?s actual word vector than any other wordvector.
The dataset contains 19,544 items, bro-ken down into 14 subtasks (e.g.
capitals of com-mon countries or adjective to adverb conversion).1305llllllllll llllll llll0.0 0.2 0.4 0.60.00.20.40.60.81.0CosineProportion Correctl CBOWSkip?GramGLOVEFigure 3: Proportion Correct against Average Co-sine.For each item, we measure the cosine of the an-gle between the vector differences for the wordwe are trying to predict (e.g.
smaller ?
smalland smaller ?
bigger) and analyze these valuesin terms of the success of the model?s prediction,with smaller cosine values corresponding to anglesthat are closer to orthogonal.5.2 CBOW ExtensionsWe evaluate the extensions on three tasks.
Along-side the word-analogy problems, we also evalu-ate the separate semantic and syntactic sub-spaceson their own individual tasks.
The semantic taskcorrelates predicted semantic similarities with thenoun-verb similarity ratings gathered by Mitchell(2013), and the remaining task clusters the syntac-tic representations and evaluates these clusters inrelation to the POS classes found in the Penn Tree-bank.On the word-analogy problem we compare tothe original CBOW, Skip-gram and GloVe mod-els.
In the case of these original models andalso the CBOM model, we follow Mikolov etal.
?s (2013b) method for making the word-analogypredictions in terms of addition and subtraction:smaller ?
bigger ?
big + small.
However, inthe case of the CBSOW and CBSOWM models,we use the novel approach described in Section3.5: vsmaller?
bsmall?
sbigger.
Similarity is thenbased on the cosine measure for all types of repre-sentation.The noun-verb similarity task is based on cor-relating the model?s predicted semantic similarityfor words with human ratings gathered in an on-l l l llllllll llllll l l0.0 0.2 0.4 0.60500100015002000CosineFrequencyl CBOWSkip?GramGLOVEFigure 4: Frequency against Cosine.line experiment.
Such evaluations have been com-monly used to evaluate distributional representa-tions, with higher correlations indicating a modelwhich is more effective at forming vectors whoserelations to each other mirror human notions of se-mantic similarity.
Mitchell (2013) argued that pre-dicting semantic similarity relations across syntac-tic categories provided a measure of the extent towhich word representations succeed in separatingsemantic from syntactic content, and gathered adataset of similarities for noun-verb pairs.
Eachrated item consists of a noun paired with a verb,and the pairs are constucted to range from high se-mantic similarity, e.g.
disappearance - vanish, tolow, e.g.
transmitter - grieve.
The dataset containsratings for 108 different pairs, each of which wasrated by 20 participants.
For the CBOW model,we predict similarities in terms of the cosine mea-sure for the two word vectors.
For the other mod-els, we predict similarities from cosine applied tojust the bag or left-hand vectors.The syntactic component of the representationsis evaluated by clustering the vectors and thencomparing the induced classes to the POS classesfound in the Penn Treebank.
We use the many-to-one measure (Christodoulopoulos et al, 2010;Yatbaz et al, 2012) to determine the extent towhich the clusters agree with the POS classes.Each cluster is mapped to its most frequent goldtag and the reported score is the proportion ofword tokens correctly tagged using this mapping.The clustering itself is a form of k-means cluster-ing, where similarity is measured in terms of thecosine measure.
Each vector is assigned to a clus-1306lll l l l l10 20 50 100 200 500 10000.00.10.20.30.40.50.6DimensionsAverageCorrelationllll lll CBOWCBSOWCBOMCBSOWMFigure 5: Average Correlation on Noun-VerbEvaluation Task against Size of Representations.ter based on which cluster centroid it is most sim-ilar to and then the cluster centroids are updatedgiven the new cluster assignments and the processrepeats.
This clustering was applied to either thesequence or right-hand vectors in the case of theCBSOW, CBOM and CBSOWM models, and tothe whole vectors in the case of CBOW.
We ran-domly initialized 45 clusters and then evaluated af-ter 100 iterations of the k-means algorithm.6 Results6.1 Original ModelsFigure 3 is a plot of the proportion of correct pre-dictions made by 100-dimensional CBOW, Skip-Gram and GloVe models on the word-analogy taskagainst cosine of the angle between the vector dif-ferences.
The range of the cosine distribution wasbroken into twenty intervals and the plotted valueswere derived by calculating the proportion correctand average cosine value within each interval.
Itis clear from the resulting curves that cosine is afairly strong predictor for all models of whetherthe model gets a word-analogy item correct, withhigher rates of success for smaller cosine values- i.e.
angles closer to orthogonality.
This is con-firmed by a significant (p < 0.001) result froma logistic regression of correctness against cosinevalue.
Similar results are found for both the se-mantic subtasks (e.g.
capitals of common coun-tries) and syntactic subtasks (e.g.
adjective to ad-verb conversion) considered separately.The actual distribution of cosine values for eachtype of model is given in Figure 4.
This analy-l ll l l l l10 20 50 100 200 500 10000.00.10.20.30.40.50.60.7DimensionsAverageMTOlll l l llll CBOWCBSOWCBOMCBSOWMFigure 6: Average Many-To-One Evaluationagainst Size of Representations.sis reveals that while the Skip-Gram and GloVemodels have fairly similar cosine distributions, theCBOW model?s distribution is shifted to the right,with more angles further from othogonality.
Thisbegs the question of what the effect on perfor-mance would be if we managed to push more ofthe CBOW distribution towards zero, and in thenext section we examine the extensions that im-plement this idea.6.2 CBOW ExtensionsWe first consider the models trained on the smaller17M word corpus, and the evaluations of thesemodels on the noun-verb similarity and POS clus-tering tasks are presented in Figures 5 and 6 re-spectively.
These graphs depict the performanceas the representations grow in size.
For the CBOWmodel, this is just the dimension of the inducedvectors.
For the other models, we consider mod-els with equal sizes of semantic and syntactic sub-spaces and report performance against the total di-mensionality of the combined representation.
Forboth these tasks, the results were averaged over tenrepetitions of training with random initializations.On the noun-verb similarity task, morphol-ogy produces the largest performance gains, withthe CBOM model substantially outperforming theCBOW model.
Word order structure has no clearimpact.On the syntactic task, in contrast, it is word or-der that produces reliable gains, with the CBSOWmodel clearly improving on the CBOW model.The simplistic use of morphology in the CBOMmodel results in a degradation of performance in1307lllll l l10 20 50 100 200 500 10000.00.10.20.30.40.50.6DimensionsProportion Correctl llll l lll CBOWCBSOWCBOMCBSOWMFigure 7: Proportion Correct on the Analogy Taskagainst Size of Representations.comparison to the CBOW model, but the CB-SOWM model?s performance is comparable tothat of the CBSOW for larger representations.Thus for these two tasks, the CBSOWM re-sults appear to show a reasonable integration ofmorphology and word order information givinggood performance on both semantic and syntac-tic tasks.
This conclusion is borne out the resultsof the word-analogy tasks in Figure 7, where theCBSOWM model outperforms all the other mod-els.
Here, morphology gives the greatest benefiton its own, as evidenced in the differences be-tween the CBOW and CBOM models.
Nonethe-less, word order still produces noticeable improve-ments, with the CBSOW result beating the CBOWresults, and the CBSOWM beating the CBOM atlarger dimensions.
There is considerable variationin the effects on performance among the variousanalogy subtasks, but even a task such as capi-tals of common countries (e.g.
predicting Iraq ashaving Baghdad as its capital, given that Greecehas Athens) appears to benefit from decomposi-tion of representations, despite not obviously in-volving syntactic structure.Table 1 compares 300-dimensional modelsacross different sizes of training data.
In thecase of the CBSOW, CBOM and CBSOWM mod-els we use representations with 200 semantic and100 syntactic dimensions and compare these toCBOW, Skip-gram and GloVe models of the sametotal size.
It is clear for all quantities of train-ing data that all the extensions outperform the ba-sic CBOW model, with morphology giving greaterTraining WordsModel 17M 120M 1.6BGloVe 29.53% 58.18% 72.54%Skip-Gram 30.03% 52.67% 62.34%CBOW 18.47% 38.48% 54.17%CBSOW 20.83% 42.00% 59.41%CBOM 44.29% 53.60% 61.87%CBSOWM 48.92% 63.19% 68.32%Table 1: Performance of 300-Dimensional Modelson the Word-Analogy Taskgains than word order, and the combined CB-SOWM model outperforming both.
This perfor-mance advantage of the CBOM over CBSOW ap-pears to weaken as the training data grows, whichis probably the effect of both the lack of morpho-logical information for rare words encountered inthe larger datasets and also the diminishing returnson that information as more data provides bettersupervision of the training process.
The sequen-tial information, in contrast, is internal to the train-ing data and seems to provide the same, or greater,performance boost as the training set grows.Comparing the results of our extended modelsto the Skip-gram and GloVe models, we can seethat on the two smaller corpora CBSOWM outper-forms both these models, while on the largest cor-pus, it only beats the Skip-gram results and GloVeachieves the best performance.
Of course, nei-ther the Skip-gram nor GloVe models has access tothe morphological information that the CBSOWMmodel uses, but the results demonstrate that theperformance of the CBOW model can be sub-stantially boosted by exploiting a representationalstructure that decomposes into semantic and syn-tactic sub-spaces.
Similar methods could in prin-ciple be applied to most word embedding models,including Skip-gram and GloVe.We can also examine the distribution of cosinevalues for the new models.
Figure 8 comparesthe distribution of cosine values for CBOW, CB-SOW, CBOM and CBSOWM models.
Although,in comparison to the original CBOW model, eachof the extended models shifts the distribution to-wards zero, i.e.
towards orthogonality, this shiftfor the CBSOW model is marginal.
In contrast,the CBOM model has a large number of instanceswhere the cosine is exactly zero, correspondingto cases where all of the relevant morphologicalinformation is found in CELEX.
The remainder1308lll l ll lllll llll l l l l ll llllllllllllllllll l lllll l l lllll0.0 0.2 0.4 0.6 0.801000200030004000CosineFrequencyl CBOWCBOMCBSOWCBSOWMFigure 8: Frequency against Cosine.of the data, however, seems to be less orthogo-nal than the original CBOW distribution, suggest-ing that words without a morphological analysisneed a more sophisticated treatment.
The shift inthe CBSOWM distribution, in comparison, is lessradically bimodal, with more continuity betweenthose words with and without morphology.
Thisreflects the difference in these models handling ofsuffixes, with the CBSOWM model?s greater flex-ibility resulting in gains over the CBOM model onthe POS induction and word analogy tasks.7 ConclusionsOur experiments demonstrate the utility of orthog-onality within vector-space representations in anumber of ways.
In terms of existing models,we find that the cosines of vector-differences isa strong predictor of the performance of CBOW,Skip-gram and GloVe representations on the wordanalogy task, with smaller cosine values - corre-sponding to angles closer to orthogonality - beingassociated with a greater proportion of correct pre-dictions.
With regard to developing new models,this orthogonality of relationships inspired threemodels which used word-order and morphologyto separate semantic and syntactic representations.These separate sub-spaces were shown to haveenhanced performance in semantic similarity andPOS-induction tasks and the combined representa-tions showed enhanced performance on the word-analogy task, using a novel approach to solvingthis problem that exploits the decomposable struc-ture of the representations.Both Botha and Blunsom (2014) and Luong etal.
(2013) take a more sophisticated approach tomorphology4, constructing a word?s embeddingby recursively combining representations of allits morphemes, though only within a single non-decomposed space.
Future work ought to pursuemodels in which all morphemes contribute bothsemantic and syntactic content to the word repre-sentations.It would also be desirable to explore more prac-tical applications of these representations than thelimited evaluations presented here.
It seems fea-sible that our decomposition of representationscould benefit tasks that need to differentiate theirtreatment of semantic and syntactic content.
Inparticular, applications of word embeddings thatmainly involve syntax, such as POS tagging (e.g.,Tsuboi, 2014) or supertagging for parsing (e.g.,Lewis and Steedman, 2014), may be a reasonablestarting point.AcknowledgementsWe would like to thank Stella Frank, Sharon Gold-water and other colleagues along with our review-ers for criticism, advice and discussion.
Thiswork was supported by ERC Advanced Fellow-ship 249520 GRAMPLUS and EU Cognitive Sys-tems project FP7-ICT-270273 Xperience.ReferencesHarald Baayen, Richard Piepenbrock, and Hedderikvan Rijn.
1995.
CELEX2 LDC96L14.
Web Down-load.
Philadelphia: Linguistic Data Consortium.Jan A. Botha and Phil Blunsom.
2014.
CompositionalMorphology for Word Representations and Lan-guage Modelling.
In Proceedings of the 31st Inter-national Conference on Machine Learning (ICML),Beijing, China.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsuper-vised POS induction: How far have we come?
InProceedings of EMNLP, pages 575?584.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of the tenth Annual Meetingof the European Association for Computational Lin-guistics (EACL), pages 59?66.4Though not neccessarily better performing.
Luong etal.
?s published 50-dimensional embeddings trained on 986Mwords scored only 13.57% on the word-analogy task, wellbehind 40-dimensional CBOM (34.68%) and CBSOWM(36.71%) models trained on 17M words.1309Fei Huang, Arun Ahuja, Doug Downey, Yi Yang,Yuhong Guo, and Alexander Yates.
2013.
Learn-ing Representations for Weakly Supervised NaturalLanguage Processing Tasks.
Computational Lin-guistics, 40:85?120.Mike Lewis and Mark Steedman.
2013.
Combineddistributional and logical semantics.
Transactionsof the Association for Computational Linguistics,1:179?192.Mike Lewis and Mark Steedman.
2014.
A* CCGparsing with a supertag-factored model.
In Proceed-ings of the 2014 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages990?1000, Doha, Qatar, October.
Association forComputational Linguistics.Thang Luong, Richard Socher, and Christopher Man-ning.
2013.
Better word representations with recur-sive neural networks for morphology.
In Proceed-ings of the Seventeenth Conference on Computa-tional Natural Language Learning, pages 104?113,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Scott McDonald and Will Lowe.
1998.
Modellingfunctional priming and the associative boost.
In Pro-ceedings of the 20th Annual Meeting of the Cogni-tive Science Society, pages 675?680.
Erlbaum.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
In Proceedings of Workshopat ICLR.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 746?751, Atlanta,Georgia, June.
Association for Computational Lin-guistics.Jeff Mitchell.
2013.
Learning semantic representa-tions in a bigram language model.
In Proceedingsof the 10th International Conference on Computa-tional Semantics (IWCS 2013) ?
Short Papers, pages362?368, Potsdam, Germany, March.
Associationfor Computational Linguistics.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InAISTATS05, pages 246?252.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
Glove: Global vectors for wordrepresentation.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 1532?1543, Doha,Qatar, October.
Association for Computational Lin-guistics.Yuta Tsuboi.
2014.
Neural networks leverage corpus-wide information for part-of-speech tagging.
In Pro-ceedings of the 2014 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP),pages 938?950, Doha, Qatar, October.
Associationfor Computational Linguistics.Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret.
2012.Learning syntactic categories using paradigmaticrepresentations of word context.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 940?951, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Will Y. Zou, Richard Socher, Daniel M. Cer, andChristopher D. Manning.
2013.
Bilingual word em-beddings for phrase-based machine translation.
InEMNLP, pages 1393?1398.
ACL.1310
