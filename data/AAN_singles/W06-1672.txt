Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 612?617,Sydney, July 2006. c?2006 Association for Computational LinguisticsDiscriminative Methods for TransliterationDmitry ZelenkoSRA International4300 Fair Lakes Ct.Fairfax VA 22033dmitry_zelenko@sra.comChinatsu AoneSRA International4300 Fair Lakes Ct.Fairfax VA 22033chinatsu_aone@sra.comAbstractWe present two discriminative methodsfor name transliteration.
The methodscorrespond to local and global modelingapproaches in modeling structured outputspaces.
Both methods do not requirealignment of names in different lan-guages ?
their features are computed di-rectly from the names themselves.
Weperform an experimental evaluation ofthe methods for name transliteration fromthree languages (Arabic, Korean, andRussian) into English, and compare themethods experimentally to a state-of-the-art joint probabilistic modeling approach.We find that the discriminative methodsoutperform probabilistic modeling, withthe global discriminative modeling ap-proach achieving the best performance inall languages.1 IntroductionName transliteration is an important task of tran-scribing a name from alphabet to another.
Forexample, an Arabic ??????
?, Korean ????
?, andRussian ????????
all correspond to English?William?.
We address the problem of translit-eration in the general setting: it involves trying torecover original English names from their tran-scription in a foreign language, as well as findingan acceptable spelling of a foreign name in Eng-lish.We apply name transliteration in the contextof cross-lingual information extraction.
Nameextractors are currently available in multiple lan-guages.
Our goal is to make the extracted namesunderstandable to monolingual English speakersby transliterating the names into English.The extraction context of the transliterationapplication imposes additional complexity con-straints on the task.
In particular, we aim for thetransliteration speed to be comparable to that ofextraction speed.
Since most current extractionsystems are fairly fast (>1 Gb of text per hour),the complexity requirement reduces the range oftechniques applicable to the transliteration.
Moreprecisely, we cannot use WWW and the webcount information to hone in on the right translit-eration candidate.
Instead, all relevant translitera-tion information has to be represented within acompact and self-contained transliteration model.We present two methods for creating and ap-plying transliteration models.
In contrast to mostprevious transliteration approaches, our modelsare discriminative.
Using an existing translitera-tion dictionary D (a set of name pairs {(f,e)}), welearn a function that directly maps a name f fromone language into a name e in another language.We do not estimate either direct conditionalp(e|f) or reverse conditional p(f|e) or joint p(e,f)probability models.
Furthermore, we do awaywith the notion of alignment: our transliterationmodel does not require and is not defined of interms of aligned e and f. Instead, all featuresused by the model are computed directly fromthe names f and e without any need for theiralignment.The two discriminative methods that we pre-sent correspond to local and global modelingparadigms for solving complex learning prob-lems with structured output spaces.
In the localsetting, we learn linear classifiers that predict aletter ei from the previously predicted letterse1?ei-1 and the original name f. In the global set-ting, we learn a function W mapping a pair (f,e)into a score W(f,e)?
R. The function W is linearin features computed from the pair (f,e).
We de-scribe the pertinent feature spaces as well as pre-612sent both training and decoding algorithms forthe local and global settings.We perform an experimental evaluation forthree language pairs (transliteration from Arabic,Korean, and Russian into English) comparingour methods to a joint probabilistic modelingapproach to transliteration, which was shown todeliver superior performance.
We show experi-mentally that both discriminative methods out-perform the probabilistic approach, with globaldiscriminative modeling achieving the best per-formance in all languages.2 PreliminariesLet E and F be two finite alphabets.
We will uselowercase latin letters e, f to denote letters e?E,f?F, and we use bold letters e?E*, f?F* to de-note strings in the corresponding alphabets.
Thesubscripted ei, fj denote ith and jth symbols of thestrings e and f, respectively.
We use e[i,j] to rep-resent a substring ei?ej of e. If j<i, then e[i,j] isan empty string ?.A transliteration model is a function mapping astring f to a string e. We seek to learn a translit-eration model from a transliteration dictionaryD={(f,e)}.
We apply the model in conjunctionwith a decoding algorithm that produces a stringe from a string f.3 Local Transliteration ModelingIn local transliteration modeling, we represent atransliteration model as a sequence of local pre-diction problems.
For each local prediction, weuse the history h representing the context of mak-ing a single transliteration prediction.
That is, wepredict each letter ei based on the pair h=(e[1,i-1], f) ?
H.Formally, we map H?E into a d-dimensionalfeature space ?
: H?E ?
Rd, where each?k(h,e)(k?
{1,..,d}) corresponds to a conditiondefined in terms of the history h and the cur-rently predicted letter e.In order to model string termination, we aug-ment E with a sentinel symbol $, and we append$ to each e from D.Given a transliteration dictionary D, we trans-form the dictionary in a set of |E| binary learningproblems.
Each learning problem Le correspondsto predicting a letter e?E.
More precisely, for apair (f[1,m],e[1,n]) ?
D and i ?
{1,?,n}, wegenerate a positive example ?
((e[1,i-1], f),ei) forthe learning problem Le, where e=ei, and a nega-tive example ?
((e[1,i-1], f),e) for each Le, wheree?ei.Each of the learning problems is a binary clas-sification problem and we can use our favoritebinary classifier learning algorithm to induce acollection of binary classifiers {ce : e?E}.
Frommost classifiers we can also obtain an estimate ofconditional probability p(e|h) of a letter e given ahistory h.For decoding, in our experiments we use thebeam search to find the sequence of letters (ap-proximately) maximizing p(e|h).3.1 Local FeaturesThe features used in local transliteration model-ing correspond to pairs of substrings of e and f.We limit the length of substrings as well as theirrelative location with respect to each other.?
For ?
((e[1,i-1], f),e), generate a featurefor every pair of substrings (e[i-w,i-1],f[j-v,j]), where 1?w<W(E) and  0?v<W(F)and |i-j| ?
d(E,F).
Here, W(?)
is the upperbound on the length of strings in the corre-sponding alphabet, and d(E,F) is the upperbound on the relative distance betweensubstrings.?
For ?
((e[1,i-1], f[1,m]),e), generate thelength difference feature ?len=i-m.
In ex-periments, we discretize ?len to obtain 9binary features: ?len=l (l?
[-3,3]), ?len ?
-4,4 ?
?len.?
For ?
((e[1,i-1], f[1,m]),e), generate alanguage modeling feature p(e| e[1,i-1]).?
For ?
((e[1,i-1], f),e) and i=1, generate?start?
features: (^f1,^e), (^f1f2,^e).?
For ?
((e[1,i-1], f),e) and i=2, generate?start?
features: (^f1,^e1e2), (^f1f2,^e1e2).?
For ?
((e[1,i-1], f),e) and e=$, generate?end?
features: (fm$,e$), (fm-1fm$,e$).The parameters W(E), W(F), and d(E,F) are, ingeneral, language-specific, and we will show, inthe experiments, that different values of the pa-rameters are appropriate for different languages.4 Global Transliteration ModelingIn global transliteration modeling, we directlymodel the agreement function between f and e.We follow (Collins 2002) and consider theglobal feature representation ?
: F*?E*  ?
Rd.613Each global feature corresponds to a conditionon the pair of strings.
The value of a feature isthe number of times the condition holds true fora given pair of strings.
In particular, for everylocal feature ?k((e[1,i-1], f),ei) we can define thecorresponding global feature:)),],1,1[((),( ?
?=?iikk ei feef ?
(1)We seek a transliteration model that is linearin the global features.
Such a transliterationmodel is represented by d-dimensional weightvector W?
Rd.
Given a string f, model applica-tion corresponds to finding a string e such that?
?=kkkW ),(maxarge'e'fe             (2)As with the case of local modeling, due tocomputational constraints, we use beam searchfor decoding in global transliteration modeling.
(Collins 2002) showed how to use the VotedPerceptron algorithm for learning W, and we useit for learning the global transliteration model.We use beam search for decoding within theVoted Perceptron training as well.4.1 Global FeaturesThe global features used in local transliterationmodeling directly correspond to local featuresdescribed in Section 3.1.?
For e[1,n] and f[1,m], generate a featurefor every pair of substrings (e[i-w,i],f[j-v,j]), where 1?w<W(E) and  0?v<W(F)and |i-j| ?
d(E,F).?
For e[1,n] and f[1,m], generate thelength difference feature ?len=n-m.
In ex-periments, we discretize ?len to obtain 9binary features: ?len=l (l?
[-3,3]), ?len ?
-4,4 ?
?len.?
For e[1,n], generate a language model-ing feature (p(e))1/n.?
For e[1,n] and f[1,m],, generate ?start?features: (^f1,^e1), (^f1f2,^e1), (^f1,^e1e2),(^f1f2,^e1e2).?
For e[1,n] and f[1,m], generate ?end?features: (fm$,en$), (fm-1fm$,en).5 Joint Probabilistic ModelingWe compare the discriminative approaches to ajoint probabilistic approach to transliteration in-troduced in recent years.In the joint probabilistic modeling approach,we estimate a probability distribution p(e,f).
Wealso postulate hidden random variables a repre-senting the alignment of e and f. An alignment aof e and f is a sequence a1,a2,?aL, where al =(e[il-wl,il],f[jl-vl,jl]), il-1+1=il-wl, and jl-1+1=jl-vl.Note that we allow for at most one member of apair al to be an empty string.Given an alignment a, we define the jointprobability p(e,f|a):]),[],,[()|,( lllllll jvjiwipp ?
?
?= feafeWe learn the probabilities p(e[il-wl,il],f[jl-vl,jl])using a version of EM algorithm.
In our experi-ments, we use the Viterbi version of the EM al-gorithm: starting from random alignments of allstring pairs in D, we use maximum likelihoodestimates of the above probabilities, which arethen employed to induce the most probablealignments in terms of the probability estimates.The process is repeated until the probability es-timates converge.During the decoding process, given a string f,we seek both a string e and an alignment a suchthat p(e,f|a) is maximized.
In our experiments,we used beam search for decoding.Note that with joint probabilistic modeling useof a language model p(e) is not strictly neces-sary.
Yet we found out experimentally that anadaptive combination of the language model withthe joint probabilistic model improves the trans-literation performance.
We thus combine thejoint log-likelihood log(p(e,f|a)) with log(p(e)):score(e|f) = log(p(e,f|a))+ ?log(p(e))          (3)We estimate the parameter ?
on a held-out setby generating, for each f, the set of top K=10candidates with respect to log(p(e,f|a)), then us-ing (3) for re-ranking the candidates, and picking?
to minimize the number of transliteration er-rors among re-ranked candidates.6 ExperimentsWe present transliteration experiments for threelanguage pairs.
We consider transliteration fromArabic, Korean, and Russian into English.
For alllanguage pairs, we apply the same training anddecoding algorithms.6.1 DataThe training and testing transliteration datasetsizes are shown in Table 1.
For Arabic and Rus-sian, we created the dataset manually by keyingin and translating Arabic, Russian, and Englishnames.
For Korean, we obtained a dataset oftransliterated names from a Korean governmentwebsite.
The dataset contained mostly foreign614names transliterated into Korean.
All datasetswere randomly split into training and (blind) test-ing parts.Training TestingArabic 935 233Korean 11973 1363Russian 545 121Table 1.
Transliteration Data.Prior to transliteration, the Korean words ofthe Korean transliteration data were convertedfrom their Hangul (syllabic) representation toJamo (letter-based) representation to effectivelyreduce the alphabet size for Korean.
The conver-sion process is completely automatic (see Uni-code Standard 3.0 for details).6.2 Algorithm DetailsFor language modeling, we used the list of100,000 most frequent names downloaded fromthe US Census website.
Our language model is a5-gram model with interpolated Good-Turingsmoothing (Gale and Sampson 1995).We used the learning-to-classify version ofVoted Perceptron for training local models(Freund and Schapire 1999).
We used Platt?smethod for converting scores produced bylearned linear classifiers into probabilities (Platt1999).
We ran both local and global Voted Per-ceptrons for 10 iterations during training.6.3 Transliteration ResultsOur discriminative transliteration modelshave a number of parameters reflecting thelength of strings chosen in either language aswell as the relative distance between strings.While we found that choice of W(E)=W(F) = 2always produces the best results for all of ourlanguages, the distance d(E,F) may have differ-ent optimal values for different languages.Table 2 presents the transliteration results forall languages for different values of d. Note thatthe joint probabilistic model does not depend ond.
The results reflect the accuracy of translitera-tion, that is, the proportion of times when the topEnglish candidate produced by a transliterationmodel agreed with the correct English translitera-tion.
We note that such an exact comparison maybe too inflexible, for many foreign names mayhave more than one legitimate English spelling.In future experiments, we plan to relax the re-quirement and consider alternative variants oftransliteration scoring (e.g., edit distance, top-Ncandidate scoring).Local Global ProbArabic (d=1) 31.33 32.61Arabic (d=2) 30.04 30.04Arabic (d=3) 26.61 27.0325.75Korean (d=1) 26.93 30.44Korean (d=2) 28.84 34.26Korean (d=3) 30.96 35.2826.93Russian (d=1) 44.62 46.28Russian (d=2) 38.84 41.32Russian (d=3) 38.01 38.0139.67Table 2.
Transliteration Results for DifferentValues of Relative Distance (d).Table 2 shows that, for all three languages, thediscriminative methods convincingly outperformthe joint probabilistic approach.
The global dis-criminative approach achieves the best perform-ance in all languages.
It is interesting that differ-ent values of relative distance are optimal fordifferent languages.
For example, in Korean, theHangul-Jamo decomposition leads to fairly re-dundant strings of Korean characters therebymaking transliterated characters to be relativelyfar from each other.
Therefore, Korean requires alarger relative distance bound.
In Arabic andRussian, on the other hand, transliterated charac-ters are relatively close to each other, so the dis-tance d of 1 suffices.
While for Russian such asmall distance is to be expected, we are surprisedby such a small relative distance for Arabic.
Ourintuition was that omitting short vowels in spell-ing names in Arabic will increase d.We have the following explanation of the lowvalue of d for Arabic from the machine learningperspective: incrementing d implies adding a lotof extraneous features to examples, that is, in-creasing attribute noise.
Increased attribute noiserequires a corresponding increase in the numberof training examples to achieve adequate per-formance.
While for Korean the number of train-ing examples is sufficient to cope with the attrib-ute noise, the relatively small Arabic trainingsample is not.
We hypothesize that with increas-ing the number of training examples for Arabic,the optimal value of d will also increase.7 Related WorkMost work on name transliteration adopted asource-channel approach (Knight and Grael1998; Al-Onaizan and Knight 2002a; Virga andKhudanpur 2003; Oh and Choi 2000) incorporat-615ing phonetics as an intermediate representation.
(Al-Onaizan and Knight 2002) showed that useof outside linguistic resources such as WWWcounts of transliteration candidates can greatlyboost transliteration accuracy.
(Li et al 2004)introduced the joint transliteration model whosevariant augmented with adaptive re-ranking weused in our experiments.Among direct (non-source-channel) models,we note the work of (Gao et al 2004) on apply-ing Maximum Entropy to English-Chinese trans-literation, and the English-Korean transliterationmodel of (Kang and Choi 2000) based on deci-sion trees.All of the above models require alignment be-tween names.
We follow the recent work of(Klementiev and Roth 2006) who addressed theproblem of discovery of transliterated namedentities from comparable corpora and suggestedthat alignment may not be necessary for translit-eration.Finally, our modeling approaches follow therecent  work on both local classifier-based mod-eling of complex learning problems (McCallumet al 2000; Punyakanok and Roth 2001), as wellas global discriminative approaches based onCRFs (Lafferty et al 2001), SVM (Taskar et al2005), and the Perceptron algorithm (Collins2002) that we used in our experiments.8 ConclusionsWe presented two novel discriminative ap-proaches to name transliteration that do not em-ploy the notion of alignment.
We showed ex-perimentally that the approaches lead to superiorexperimental results in all languages, with theglobal discriminative modeling approach achiev-ing the best performance.The results are somewhat surprising, for thenotion of alignment seems very intuitive and use-ful for transliteration.
We will investigatewhether similar alignment-free methodology canbe extended to full-text translation.
It will also beinteresting to study the relationship between ourdiscriminative alignment-free methods and re-cently proposed discriminative alignment-basedmethods for transliteration and translation(Taskar et al 2005a; Moore 2005).We also showed that for name transliteration,global discriminative modeling is superior tolocal classifier-based discriminative modeling.This may have resulted from poor calibration ofscores and probabilities produced by individualclassifiers.
We plan to further investigate the re-lationship between the local and global ap-proaches to complex learning problems in naturallanguage.ReferencesY.
Al-Onaizan and K. Knight.
2002.
TranslatingNamed Entities Using Monolingual and BilingualResources.
Proceedings of ACL.Y.
Al-Onaizan and K. Knight.
2002a.
Machine Trans-literation of Names in Arabic Text.
Proceedings ofACL Workshop on Computational Approaches toSemitic Languages.M.
Collins.
2002.
Discriminative Training for HiddenMarkov Models: Theory and Experiments withPerceptron Algorithms.
In Proceedings of EMNLP.Y.
Freund and R. Shapire.
1999.
Large margin clas-sification using the perceptron algorithm.
MachineLearning, 37, 277?296.W.
Gale and G. Sampson.
1995.
Good-Turing fre-quency estimation without tears.
Journal of Quan-titative Linguistics 2:217-235.Gao Wei, Kam-Fai Wong, and Wai Lam.
2004.
Pho-neme-based transliteration of foreign names forOOV problem.
Proceedings of the First Interna-tional Joint Conference on Natural LanguageProcessing.B.J.
Kang and Key-Sun Choi, 2000.
Automatic Trans-literation and Back-transliteration by Decision TreeLearning, Proceedings of the 2nd InternationalConference on Language Resources and Evalua-tion.A.
Klementiev and D. Roth.
2006.
Named EntityTransliteration and Discovery from MultilingualComparable Corpora.
Proceedings of ACL.K.
Knight and J. Graehl.
1998.
Machine Translitera-tion, Computational Linguistics, 24(4).J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional Random Fields: Probabilistic Models forSegmenting and Labeling Sequence Data.
Proceed-ings of the Eighteenth International Conference onMachine Learning.Li Haizhou, Zhang Min, and Su Jian.
2004.
A JointSource-channel Model for Machine Transliteration.Proceedings of ACL 2004.A.
McCallum, D. Freitag, and F. Pereira.
2000.
Maxi-mum entropy Markov models for information ex-traction and segmentation.
Proceedings of ICML.R.
Moore.
2005.
A Discriminative Framework forBilingual Word Alignment.
Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing.616Jong-Hoon Oh and Key-Sun Choi.
2000.
An English-Korean Transliteration Model Using Pronunciationand Contextual Rules.
Proceedings of COLING.J.
Platt.
1999.
Probabilistic outputs for support vectormachines and comparison to regularized likelihoodmethods.
In Advances in Large Margin Classi?ers.V.
Punyakanok and D. Roth.
2001.
The Use of Classi-fiers in Sequential Inference.
Proceedings of theConference on Advances in Neural InformationProcessing Systems.B.
Taskar, V. Chatalbashev, D. Koller and C. Gues-trin.
2005.
Learning Structured Prediction Models:A Large Margin Approach.
Proceedings of TwentySecond International Conference on MachineLearning.B.
Taskar, S. Lacoste-Julien, and D. Klein.
2005a.
ADiscriminative Matching Approach to Word Align-ment.
Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.P.
Virga and S. Khudanpur.
2003.
Transliteration ofProper Names in Cross-lingual Information Re-trieval.
Proceedings of ACL 2003 workshopMLNER.617
