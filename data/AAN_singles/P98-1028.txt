Beyond N-Grams: Can Linguistic Sophistication ImproveLanguage Modeling?Eric Brill, Radu Florian, John C. Henderson, Lidia ManguDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, Md.
21218 USA{bri l l ,rf lorian,jhndrsn,l idia} @ cs.
jhu.eduAbstractIt seems obvious that a successful model ofnatural anguage would incorporate a greatdeal of both linguistic and world knowledge.Interestingly, state of the art languagemodels for speech recognition are based ona very crude linguistic model, namelyconditioning the probability of a word on asmall fixed number of preceding words.Despite many attempts to incorporate moresophisticated information into the models,the n-gram model remains the state of theart, used in virtually all speech recognitionsystems.
In this paper we address thequestion of whether there is hope inimproving language modeling byincorporating more sophisticated linguisticand world knowledge, or whether the n-grams are already capturing the majority ofthe information that can be employed.IntroductionN-gram language models are very crudelinguistic models that attempt o capture theconstraints of language by simply conditioningthe probability of a word on a small fixednumber of predecessors.
It is rather frustratingto language ngineers that the n-gram model isthe workhorse of virtually every speechrecognition system.
Over the years, there havebeen many attempts to improve language modelsby utilizing linguistic information, but thesemethods have not been able to achievesignificant improvements over the n-gram.The insufficiency of Markov models hasbeen known for many years (see Chomsky(1956)).
It is easy to construct examples where atrigram model fails and a more sophisticatedmodel could succeed.
For instance, in thesentence : The dog on the hill barked, the wordbarked would be assigned a low probability bya trigram model.
However, a linguistic modelcould determine that dog is the head of the nounphrase preceding barked and therefore assignbarked a high probability, since P(barkedldog)is high.Using different sources of rich linguisticinformation will help speech recognition if thephenomena they capture are prevalent and theyinvolve instances where the recognizer makeserrors.
~ In this paper we first give a briefoverview of some recent attempts atincorporating linguistic information intolanguage models.
Then we discuss experimentswhich give some insight into what aspects oflanguage hold most promise for improving theaccuracy of speech recognizers.1 Linguistically-Based ModelsThere is a continuing push among members ofthe speech recognition community to remedy theweaknesses of linguistically impoverished n-gram language models.
It is widely believedthat incorporating linguistic oncepts can lead tomore accurate language models and moreaccurate speech recongizers.One of the In'st attempts atlinguistically-based modelling used probabilisticcontext-free grammars (PCFGs) directly toI This is one of the problems with perplexity as ameasure of language model quality: if the bettermodel simply assigns higher probability to theelements the recognizer already gets correct, themodel will look better in terms of perplexity, but willdo nothing to improve recognizer accuracy.186compute language modeling probabilities(Jelinek(1992)).
Another approach retrieved n-gram statistics from a handwritten PCFG andcombined those statistics with traditional n-grams elicited from a corpus (Jurafsky(1995)).Research has been carded out in adaptivelymodifying language models using knowledge ofthe subject matter being discussed(Seymore(1997)).
This research depends on theprevalence of jargon and domain-specificlanguage.Linguistically motivated languagemodels were investigated for two consecutiveyears at the Summer Speech RecognitionWorkshop, held at Johns Hopkins University.
In1995 experiments were run adding part-of-speech (POS) tags to the language models(Brill(1996)).
In the 1996 Summer SpeechRecognition Workshop, recognizerimprovements were attempted by exploiting thelong-distance dependencies provided by adependency parse (Chelba(1997)).
The goal wasto exploit the predictive power of predicate-argument structures found in parse trees.
InDella Pietra(1994) and Fong(1995), linkgrammars were used, again in an attempt oimprove the language model by providing itwith long-distance dependencies not captured inthe n-gram statistics.
2Although much work has been doneexploring how to create linguistically-basedlanguage models, improvement in speechrecognizer accuracy has been elusive.2 Experimental FrameworkIn an attempt to gain insight into what linguisticknowledge we should be exploring to improvelanguage models for speech recognition, we ranexperiments where people tried to improve theoutput of speech recognition systems and thenrecorded what types of knowledge they used indoing so.
We hoped to both assess how muchgain might be expected from very sophisticatedmodels and to determine just what informationsources could contribute to this gain.People were given the ordered list of theten most likely hypotheses for an utteranceaccording to the recognizer.
They were then2 For a more comprehensive r view of the historicalinvolvement of natural language parsing in languagemodelling, see Stolcke(1997).187asked to choose from the ten-best list thehypothesis that they thought would have thelowest word error rate, in other words, to try todetermine which hypothesis is closest to thetruth.
Often, the truth is not present in the 10-best list.
An example 5-best list from the WallStreet Journal corpus is shown in Figure 1.
Foursubjects were used in this experiment, and eachsubject was presented with 75 10-best lists fromthree different speech recognition systems (225instances total per subject).
From thisexperiment, we hoped to gauge what the upperbound is on how much we could improve uponstate of the art by using very rich models)For our experiments, we used threedifferent speech recognizers, trained respectivelyon Switchboard (spontaneous speech), BroadcastNews (recorded news broadcasts) and WallStreet Journal data.
4The word error rates of therecognizers for each corpus are shown in thefirst line of Table 1.The human subjects were presented withthe ten-best lists.
Sentences within each ten-bestlist were aligned to make it easier to comparethem.
In addition to choosing the mostappropriate selection from the 10-best list,subjects were also allowed to posit a string notin the list by editing any of the strings in the 10-best list in any way they chose.
For eachsample, subjects were asked to determine whattypes of information were used in deciding.This was done by presenting the subjects with aset of check boxes, and asking them to check allthat applied.
A list of the options presented tothe human can be found in Figure 2.
Subjectswere provided with a detailed explanation, aswell as examples, for each of these options .52 Net Human ImprovementThe first question to ask is whether people areable to improve upon the speech recognizer'soutput by postprocessing the n-best lists.
For3 Note that what we are really measuring is an upperbound on improvement under the paradigm of n-bestpostprocessing.
This is a common technique inspeech recognition, but it results in the postprocessornot having access to the entire set of hypotheses, ortofull acoustic information.4 HTK software was used to build all recognizers.s This program is available athttp:llwww.cs.jhu.edullabslnlpeach corpus, we have four measures: (1) therecognizer's word error rate, (2) the oracle errorrate, (3) human error rate when choosing amongthe 10-best (human selection) and (4) humanerror rate when allowed to posit any wordsequence (human edit).The oracle error rate is the upper bound onhow well anybody could do when restricted tochoosing between the 10 best hypotheses: theoracle always chooses the string with the lowestword error rate.
Note that if the human alwayspicked the highest-ranking hypothesis, then heraccuracy would be equivalent o that of therecognizer.
Below we show the results for eachcorpus, averaged across the subjects:RecognizerOracleSwitchboard Broadcast Wall StreetNews Journal43.9% 27.2% 13.2%32.7% 22.6% 7.9%Human 42.0% 25.9% 10.1%SelectionHuman Edit 41.0% 25.2% 9.2%Table 1 Word Error Rate: Recognizer,Oracle and HumanIn the following table, we show theresults as a function of what percentage of thedifference between recognizer and oracle thehumans are able to attain.
In other words, whenthe human is not restricted to the 10-best list, heis able to advance 75.5% of the way betweenrecognizer and oracle word error rate on theWall Street Journal.Switchboard Broadcast Wall StreetNews JournalHuman 17.0% 28.3% 58.5%SelectionHuman Edit 25.9% 43.5% 75.5%Table 2 Human Gain Relative to Recognizerand OracleThere are a number of interesting thingsto note about these results.
First, they are quiteencouraging, in that people are able to improvethe output on all corpora.
As the accuracy of therecognizer improves, the relative humanimprovement increases.
While people can attainover three-quarters of the possible word errorrate reduction over the recognizer on Wall StreetJournal, they are only able to attain 25.9% of thepossible reduction in Switchboard.
This isprobably attributable to two causes.
The more188varied the language is in the corpus, the harder itis for a person to predict what was said.
Also,the higher the recognizer word error rate, theless reliable the contextual cues will be whichthe human uses to choose a lower error ratestring.
In Switchboard, over 40% of the wordsin the highest ranked hypothesis are wrong.Therefore, the human is basing her judgementon much less reliable contexts in Switchboardthan in the much lower word error rate WallStreet Journal, resulting in less net improvement.For all three corpora, allowing theperson to edit the output, as opposed to beinglimited to pick one of the ten highest rankedhypotheses, resulted in significant gains: over50% for Switchboard and Broadcast News, and30% for Wall Street Journal.
This indicates thatwithin the paradigm of n-best listpostprocessing, one should strongly considermethods for editing, rather than simplychoosing.In examining the relative gain over therecognizer the human was able to achieve as afunction of sentence length, for the threedifferent corpora, we observed that the generaltrend is that the longer the sentence is, thegreater the net gain is.
This is because a longersentence provides more cues, both syntactic andsemantic, that can be used in choosing thehighest quality word sequence.
We alsoobserved that, other than the case of very loworacle error rate, the more difficult the task is thelower the net human gain.
So both acrosscorpora and corpus-internal, we find thisrelationship between quality of recognizeroutput and ability of a human to improve uponrecognizer output.3 Usefulness of Linguistic InformationIn discussions with the participants afterthey ran the experiment, it was determined thatall participants essentially used the samestrategy.
When all hypotheses appeared to beequally bad, the highest-ranking hypothesis waschosen.
This is a conservative strategy that willensure that the person does no worse than therecognizer on these difficult cases.
In othercases, people tried to use linguistic knowledge topick a hypothesis they felt was better than thehighest ranked hypothesis.In Figure 2, we show the distribution ofproficiencies that were used by the subjects.
Weshow for each of the three corpora, thepercentage of 10-best instances for which theperson used each type of knowledge (along withthe ranking of these percentages), as well as thenet gain over the recognizer accuracy that peoplewere able to achieve by using this informationsource.
For all three corpora, the most common(and most useful) proficiency was that of closedclass word choice, for example confusing thewords in and and, or confusing than and that.
Itis encouraging that although world knowledgewas used frequently, there were many linguisticproficiencies that the person used as well.
Ifonly world knowledge accounted for theperson's ability to improve upon therecognizer's output, then we might be faced withan AI-complete problem: speech recognizerimprovements are possible, but we would haveto essentially solve AI before the benefit couldbe realized.One might conclude that althoughpeople were able to make significantimprovements over the recognizer, we may stillhave to solve linguistics before theseimprovements could actually be realized by anyactual computer system.
However, we areencouraged that algorithms could be created thatcan do quite well at mimicking a number ofproficiencies that contributed to the human'sperformance improvement.
For instance,determiner choice was a factor in roughly 25%of the examples for the Wall Street Journal.There already exist algorithms for choosing theproper determiner with fairly high accuracy(Knight(1994)).
Many of the cases involvedconfusion between a relatively small set ofchoices: closed class word choice, determinerchoice, and preposition choice.
Methods alreadyexist for choosing the proper word from a fixedset of possibilities based upon the context inwhich the word appears (e.g.
Golding(1996)).ConclusionIn this paper, we have shown that humans, bypostprocessing speech recognizer output, canmake significant improvements in accuracy overthe recognizer.
The improvements increase withthe recognizer's accuracy, both within aparticular corpus and across corpora.
Thisdemonstrates that there is still a great deal togain without changing the recognizer's internalmodels, and simply operating on therecognizer's output.
This is encouraging news,as it is typically a much simpler matter to dopostprocessing than to attempt o integrate aknowledge source into the recognizer itself.We have presented a description of theproficiencies people used to make theseimprovements and how much each contributedto the person's uccess in improving over therecognizer accuracy.
Many of the gainsinvolved linguistic proficiencies that appear tobe solvable (to a degree) using methods thathave been recently developed in naturallanguage processing.
We hope that by honing inon the specific high-yield proficiencies that areamenable to being solved using currenttechnology, we will finally advance beyond n-grams.There are four primary foci of futurework.
First, we want to expand our study toinclude more people.
Second, now that we havesome picture as to the proficiencies used, wewould like to do a more refined study at a lowerlevel of granularity by expanding the repertoireof proficiencies the person can choose from indescribing her decision process.
Third, we wantto move from what to how: we now have someidea what proficiencies were used and we wouldnext like to establish to the extent we can howthe human used them.
Finally, eventually wecan only prove the validity of our claims byactually using what we have learned to improvespeech recognition, which is our ultimate goal.ReferencesBrill E, Harris D, Lowe S, Luo X, Rao P, Ristad Eand Roukos S. (1996).
A hidden tag model forlanguage.
In "Research Notes", Center forLanguage and speech processing.
The JohnsHopkins University.
Chapter 2.Chelba C, Eagle D, Jelinek F, Jimenez V, KhudanpurS, Mangu L, Printz H, Ristad E, Rosenfeld R,Stolcke A and Wu D. (1997) Structure andPerformance of a Dependency Language Model.
InEurospeech '97.
Rhodes, Greece.Chomsky N. (1956) Three models for the descriptionof language.
IRE Trans.
On Inform.
Theory.
IT-2,113-124.Della Pietra S, Della Pietra V, Gillett J, Lafferty J,Printz H and tires L. (1994) Inference andEstimation of a Long-Range Trigram Model.
InProceedings of the Second International189Colloquium on Grammatical Inference.
Alicante,Spain.Fong E and Wu D. (1995) Learning restrictedprobabilistic link grammars.
I JCAI Workshop onNew Approaches to Learning for Natural LanguageProcessing, Montreal.Golding A and Roth D. (1996) Applying Winnow toContext-Sensitive Spelling Correction.
InProceedings of ICML '96.Jelinek F, Lafferty J.D.
and Mercer R.L.
(1992) BasicMethods of Probabilistic Context-Free Grammars.In "Speech Recognition and Understanding.
RecentAdvances, Trends, and Applications", VolumeF75,345-360.
Berlin:Springer Verlag.Jurafsky D., Wooters C, Segal J, Stolcke A, Fosler E,Tajchman G and Morgan N. (1995) Using astochastic context-free grammar as a languagemodel for speech recognition.
In ICASSP '95.Knight K and Chandler I.
(1994).
AutomatedPostediting of Documents.
Proceedings, TwelfthNational Conference on Artificial Intelligence.Seymore K. and Rosenfeld R. (1997) Using StoryTopics for Language Model Adaptation.
InEurospeech '97.
Rhodes, Greece.Stolcke A.
(1997) Linguistic Knowledge andEmpirical Methods in Speech Recognition.
In AIMagazine, Volume 18, 25-31, No.4.
(1) people consider what they want but we won't comment he said(2) people to say what they want but we won't comment he said(3) people can say what they want but we won't comment he said(4) people consider what they want them we won't comment he said(5) people to say what they want them we won't comment he saidFigure 1 A sample 5-best list from the WSJ corpus.
The third hypothesis is the correct one.Switchboard Broadcast News Wall Street Journal% of time Absolute % of time Absolute % of time Absoluteclicked WER clicked WER clicked WERreduction reduction reductionusing this using this using thisArgument Structure 1.3 (14) 0.18 (10) 2.0(12) 0.10(11) 5.3 (12) 0.40(8)Closed Class Word Choice 25.7 (1) 1.62 (1) 40.2 (1) 1.14 (1) 46.4 (1) 2.40 (1)Complete Sent.
Vs. Not 16.5 (2) 1.03 (2) 11.0 (6) 0.32 (8) 29.1 (2) 1.52 (2)Determiner Choice 1.7 (12) 0.06 (13) 17.6 (3) 0.41 (5) 24.8 (3) 0.93 (5)IdiomsdCommonPhrases 3.5 (6) 0.19 (9) 6.6 (8) 0.35 (6) 8.6 (8) 0.57 (7)Modal Structure 2.6 (8) 0.13 (11) 3.0 (11) 0.09 (12) 2.3 (15) 0.04 (14)Number Agreement 4.4 (5) 0.32 (8) 3.7 (10) 0.22 (9) 4.0 (14) 0.08 (13)Open Class Word Choice 8.3 (3) 0.71 (3) 19.3 (2) 0.60 (2) 9.6 (7) 0.40 (8)Parallel Structure 0.9 (15) 0.39 (6) 0.7 (15) 0.04 (15) 5.6 (10) 0.25 (11)Part of Speech Confusion 2.2 (9) 0.06 (13) 2.0 (12) 0.07 (13) 7.6 (9) 0.04 (15)Pred-Argument/Semantic 2.2 (9) 0.13 (11) 2.0 (12) 0.06 (14) 5.6 (10) 0.34 (10)AgreementPreposition Choice 3.5 (6) 0.58 (5) 17.3 (4) 0.44 (4) 15.9 (5) 0.82 (6)Tense Agreement 1.7 (12) 0.06 (13) 4.0 (9) 0.16 (10) 5.3 (12) 0.13 (12)Topic 2.2 (9) 0.39 (6) 9.3 (7) 0.34 (7) 15.2 (6) 1.03 (4)World Knowledge 6.1 (4) 0.65 (4) 12.3 (5) 0.57 (3) 19.5 (4) 1.35 (3)Figure 2 Analysis of Proficiencies Used and their Effectiveness190
