Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1108?1118,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsUnderstanding Negation in Positive Terms Using Syntactic DependenciesZahra Sarabi and Eduardo BlancoHuman Intelligence and Language Technologies LabUniversity of North TexasDenton, TX, 76203zahrasarabi@my.unt.edu, eduardo.blanco@unt.eduAbstractThis paper presents a two-step procedure toextract positive meaning from verbal negation.We first generate potential positive interpre-tations manipulating syntactic dependencies.Then, we score them according to their like-lihood.
Manual annotations show that posi-tive interpretations are ubiquitous and intuitiveto humans.
Experimental results show thatdependencies are better suited than semanticroles for this task, and automation is possible.1 IntroductionNegation is a complex phenomenon present in allhuman languages, allowing for the uniquely humancapacities of denial, contradiction, misrepresenta-tion, lying, and irony (Horn and Wansing, 2015).Despite negation always being marked?in the ab-sence of a negation cue, statements are positive?acquiring and understanding sentences that containnegation is more challenging than those that do not.Children acquire negation after learning to commu-nicate (Nordmeyer and Frank, 2013), and adults takelonger to process negated statements than positiveones (Clark and Chase, 1972).In any given language, humans communicate inpositive terms most of the time, and use negation toexpress something unusual or an exception (Horn,1989).
Albeit most sentences are affirmative, nega-tion is ubiquitous (Morante and Sporleder, 2012):In scientific papers, 13.76% of statements containa negation (Szarvas et al, 2008); in product reviews,19% (Councill et al, 2010); and in Conan Doyle sto-ries, 22.23% (Morante and Daelemans, 2012).
InOntoNotes (Hovy et al, 2006), 10.15% of state-ments contain a verb negated with not, n?t or never.From a theoretical point of view, it is accepted thatnegation conveys positive meaning (Rooth, 1992;Huddleston and Pullum, 2002).
For example, whenreading (1) John didn?t order the right parts, hu-mans intuitively understand that (1a) John orderedsomething, or more specifically, (1b) John orderedthe wrong parts.
Interpretation (1a) can be obtainedafter determining that n?t does not negate verb order,but its THEME, i.e., the right parts.
Interpretation(1b) can be obtained after determining that n?t is ac-tually negating right, an adjective modifying parts.Determining which words are intended to benegated?identifying the foci of negation, therebyrevealing positive interpretations?is challenging.First, as exemplified in (1a, 1b), there is a granu-larity continuum yielding interpretations that entaileach other, e.g., (1b) entails (1a).
Second, a singlenegation often yields several positive interpretations,e.g., from (2) John doesn?t eat meat, we can extractthat (2a) John eats something other than meat and(2b) Some people eat meat, but not John.This paper presents a methodology to extract pos-itive interpretations from verbal negation.
The maincontributions are: (1) deterministic procedure togenerate potential interpretations by manipulatingsyntactic dependencies; (2) analysis showing thatdependencies yield finer-grained interpretations andbetter results than previous work using semanticroles; (3) a corpus of negations and their positiveinterpretations;1 and (4) experimental results withgold-standard and predicted linguistic information.1Available at http://www.cse.unt.edu/?blanco/11082 Terminology, Scope and FocusNegation is well-understood in grammars, which de-tail the valid ways to form a negation (Quirk et al,2000; van der Wouden, 1997).
Negation can be ex-pressed by verbs (e.g., avoid running), nouns (e.g.,the absence of evidence), adjectives (e.g., it is point-less), adverbs (e.g., I never tried Persian food be-fore), prepositions (e.g., you can exchange it with-out a problem), determiners (e.g., the new law hasno direct implications), pronouns (e.g., nobody willkeep election promises), and others.
In this paper,we focus on verbal negation, i.e., when the negationmark?usually an adverb such as never and not?isgrammatically associated with a verb.Positive Interpretations.
In philosophy and lin-guistics, it is generally accepted that negation con-veys positive meaning (Horn, 1989).
This positivemeaning ranges from implicatures, i.e., what is sug-gested in an utterance even though neither expressednor strictly implied (Blackburn, 2008), to entail-ments.
Other terms used in the literature include im-plied meanings (Mitkov, 2005), implied alternatives(Rooth, 1985) and semantically similars (Agirre etal., 2013).
We do not strictly fit into any of this ter-minology, we reveal positive interpretations as intu-itively done by humans when reading text.2.1 Scope and FocusFrom a theoretical perspective, it is accepted thatnegation has scope and focus, and that the focus?not just the scope?yields positive interpretations(Horn, 1989; Rooth, 1992; Taglicht, 1984).
Scopeis ?the part of the meaning that is negated?
and fo-cus ?the part of the scope that is most prominently orexplicitly negated?
(Huddleston and Pullum, 2002).Consider the following statement in the contextof the recent refugee crisis: (2) Mr. Haile wasnot looking for heaven in Europe.
By definition,scope refers to ?all elements whose individual falsitywould make the negated statement strictly true?, andfocus is ?the element of the scope that is intended tobe interpreted as false to make the overall negativetrue?
(Huddleston and Pullum, 2002).
The falsity ofany of the truth conditions below makes statement(2) true, thus the scope of the negation is (2a?2d):2a.
Somebody was looking for something some-where.
[verb looking]2b.
Mr. Haile was looking for something some-where.
[AGENT of looking, Mr. Haile]2c.
Somebody was looking for heaven somewhere.
[THEME of looking, heaven]2d.
Somebody was looking for something in Eu-rope.
[LOCATION of looking, in Europe]Determining the focus is almost always morechallenging than the scope.
The challenge relies ondetermining which of the truth conditions (2a?2d)is intended to be interpreted as false to make thenegated statement true: all of them qualify, but someare more likely.
A natural reading of statement (2)suggests that Mr. Haile was looking for something(a regular life, a job, etc.)
in Europe, but not heaven.Determining that the focus is heaven, i.e., that every-thing in statement (2) is positive except the THEMEof looking, is the key to reveal the intended positiveinterpretation.
Note that scope on its own does notidentify positive interpretations, and other foci yieldunlikely positive interpretations, e.g., Mr. Haile waslooking for heaven somewhere, but not in Europe.It is worth noting that while scope is definedfrom a logical standpoint, in most negations thereare several possible foci and corresponding posi-tive interpretations.
For example, given (3) Mostjobs now don?t last for decades, the following arevalid positive interpretations: (3a) Few jobs now lastfor decades, (3b) Most jobs in the past lasted fordecades, and (3c) Most jobs now last for a few years.Granularity of Focus.
The definition of focus doesnot provide guidelines about identifying the elementof the scope that is the focus.
The larger the focus,the more generic the corresponding positive inter-pretation; and the smaller the focus, the more spe-cific the corresponding positive interpretation.
Letus consider statement (3) again.
A possible focus isMost jobs, yielding the positive interpretation Some-thing now lasts for decades, but not most jobs.
An-other possible focus is Most, yielding the interpreta-tion Few (not most) jobs now last for decades.
Weargue that the latter is preferable, as it yields a morespecific interpretation and it entails the former: ifsome jobs last for decades, then something lasts fordecades, but not the other way around.We use the term coarse-grained focus to refer tofoci that include all tokens belonging to an argumentof a verb (e.g., Most Jobs above), and fine-grainedfocus to refer to foci that do not (e.g., Most above).11093 Previous WorkWithin computational linguistics, approaches to pro-cess negation are shallow, or target scope and focusdetection.
Popular semantic representations such assemantic roles (Palmer et al, 2005; Baker et al,1998) or AMR (Banarescu et al, 2013) do not revealthe positive interpretations we target in this paper.Shallow approaches are usually application-specific.In sentiment and opinion analysis, negation has beenreduced to marking as negated all words between anegation cue and the first punctuation mark (Pang etal., 2002), or within a five-word window of a nega-tion cue (Hu and Liu, 2004).
The examples through-out this paper show that these techniques are insuffi-cient to reveal implicit positive interpretations.3.1 Scope Annotations and DetectionScope of negation detection has received a lot ofattention, mostly using two corpora: BioScope inthe medical domain (Szarvas et al, 2008) and CD-SCO (Morante and Daelemans, 2012).
BioScopeannotates negation cues and linguistic scopes exclu-sively in biomedical texts.
CD-SCO annotates nega-tion cues, scopes, and negated events or propertiesin selected Conan Doyle stories.There have been several supervised proposals todetect the scope of negation using BioScope andCD-SCO (O?zgu?r and Radev, 2009; ?vrelid et al,2010).
Automatic approaches are mature (Abu-Jbara and Radev, 2012): F-scores are 0.96 for nega-tion cue detection, and 0.89 for negation cue andscope detection (Velldal et al, 2012; Li et al, 2010).Fancellu et al (2016) present the best results to dateusing CD-SCO, and analyze the main sources of er-rors.
Outside BioScope and CD-SCO, Reitan et al(2015) present a negation scope detector for tweets,and show that it improves sentiment analysis.
Asshown in Section 2, scope detection is insufficient toreveal positive interpretations from negation.3.2 Focus Annotation and DetectionWhile focus of negation has been studied fordecades in philosophy and linguistics (Section 2),corpora and automated tools are scarce.
Blanco andMoldovan (2011) annotate focus of negation in the3,993 negations marked with ARGM-NEG semanticrole in PropBank (Palmer et al, 2005).
Their an-notations, PB-FOC, were used in the *SEM-2012Shared Task (Morante and Blanco, 2012).
Theirguidelines require annotators to choose as focus thesemantic role that ?is most prominently negated?
orthe verb.
If several roles may be the focus, theyprioritize ?the one that yields the most meaningfulimplicit [positive] information?, but do not specifywhat most meaningful means.
Their approach has2 limitations.
First, because they select one focusper negation, they only extract one positive inter-pretation per negation.
Second, because they selectas focus a semantic role, they only consider coarse-grained foci.
Consider again statement (3) from Sec-tion 2.1.
By design, their approach is limited to ex-tract a single interpretation even though interpreta-tions (3a?3c) are valid.
Similarly, their approach islimited to select as focus Most jobs?all tokens be-longing to a semantic role?although Most yields a?more meaningful?
interpretation: Something nowlasts for decades (generic, worse) vs. Few jobs nowlast for decades (specific, better).Blanco and Sarabi (2016) present a complimen-tary approach to extract and score several posi-tive interpretations from a single verbal negation.Their methodology is grounded on semantic rolesand does not consider fine-grained foci.
In this pa-per, we improve upon their work: we extract bothcoarse- and fine-grained interpretations, and also ex-tract several interpretations from one negation.Anand and Martell (2012) reannotate PB-FOCand argue that positive interpretations arisingfrom scalar implicatures and neg-raising predicatesshould be separated from those arising from focusdetection.
They argue that 27.4% of negations witha focus annotated in PB-FOC do not have one.
Inthis paper, we are not concerned about annotatingfoci per se, but about extracting positive interpreta-tions from negation, as intuitively done by humans.Automatic systems to detect the focus of negationyield modest results.
Blanco and Moldovan (2011)obtain an accuracy of 65.5 using supervised learn-ing and features derived from gold-standard linguis-tic information.
With predicted linguistic informa-tion, Rosenberg and Bergler (2012) report an F-measure of 58.4 using 4 linguistically sound heuris-tics, and Zou et al (2014) an F-measure of 65.62using contextual discourse information.
Blanco andSarabi (2016) obtain Pearson correlation of 0.6421110ranking coarse-grained interpretations.
Unlike thework presented here, none of these systems extractfine-grained interpretations from a single negation.4 Corpus CreationOur goal is to create a corpus of negations and theirpositive interpretations.
We put a strong emphasison automation and simplicity.
First, we determin-istically generate potential positive interpretationsfrom verbal negations by manipulating syntactic de-pendencies (Section 4.1).
Second, we ask annota-tors to score potential positive interpretations (Sec-tion 4.2).
Positive interpretations and their scoresare later used to learn models to rank potential inter-pretations automatically (Section 6).
Generating po-tential interpretations deterministically prior to scor-ing them proved very beneficial.
After pilot experi-ments, it became clear that asking annotators to pro-pose positive interpretations complicates the annota-tion effort (lower agreements) as well as learning.We decided to work on top of OntoNotes (Hovyet al, 2006)2 instead of plain text or other cor-pora for several reasons.
First, OntoNotes includesgold linguistic annotations such as part-of-speechtags, parse trees and semantic roles.
Second, un-like BioScope, CD-SCO and PB-FOC (Section 3.2),OntoNotes includes sentences from several genres,e.g., newswire, broadcast news and conversations,magazines, the web.
We transformed the parsetrees in OntoNotes into syntactic dependencies us-ing Stanford CoreNLP (Manning et al, 2014).4.1 Manipulating Syntactic Dependencies toGenerate Potential Positive InterpretationsOntoNotes contains 63,918 sentences.
Annotatingall positive interpretations from all negations is out-side the scope of this paper.
Instead, we target se-lected representative negations.Selecting Negations.
We first select all verbal nega-tions by retrieving all tokens whose syntactic head isa verb and dependency type neg.3 Then, we discardnegations from sentences that contain two negations,conditionals, commas or questions.
Finally, we dis-2We use the CoNLL-2011 Shared Task distribution (Pradhanet al, 2011), http://conll.cemantix.org/2011/3The Stanford manual describes and exemplifies all syntac-tic dependencies (de Marneffe and Manning, 2008).card negations if the negated verb is to be or it doesnot have a subject (dependency nsubj or nsubjpass).Converting Negated Statements into their posi-tive counterparts.
We apply 3 steps inspired afterthe grammatical rules to form negation detailed byHuddleston and Pullum (2002, Ch.
9):1.
Remove the negation mark by deleting the to-ken with syntactic dependency neg.2.
Remove auxiliaries, expand contractions, andfix third-person singular and past tense.
For ex-ample (before: after), doesn?t go: goes, didn?tgo: went, won?t go: will go.
We loop throughthe tokens whose head is the negated verb withdependency aux, and use a list of irregularverbs and grammar rules to convert to third-person singular and past tense.3.
Rewrite negatively-oriented polarity-sensitiveitems.
For example (before: after), any-one: someone, any longer: still, yet: al-ready.
at all: somewhat.
We use the cor-respondences between negatively-oriented andpositively-oriented polarity-sensitive items by(Huddleston and Pullum, 2002, pp.
831).Selecting Relevant tokens.
Verbal negation oftenoccurs in multi-clause sentences.
In order to iden-tify the relevant (syntactically negated) eventuality,we simplify the original statement by including onlythe negated verb and all tokens that are dependentsof the verb, i.e., tokens reachable from the negatedverb traversing dependencies.
For example, fromIndividuals familiar with the Justice Department?spolicy said that Justice officials hadn?t any knowl-edge of the IRS?s actions in the last week, after get-ting the positive counterpart and selecting relevanttokens, we obtain Justice officials had some knowl-edge of the IRS?s actions in the last week.Generating Interpretations.
Given the simplifiedpositive counterpart, generating all combinations oftokens as potential foci would result in 2t poten-tial positive interpretations for t tokens.
To avoida brute-force approach that generates many nonsen-sical potential interpretations, we define a proceduregrounded on syntactic dependencies.The main idea is to run a modified breadth-firsttraversal of the dependency tree to select subtreesthat are potential foci.
We start the traversal fromthe negated verb and stop it at depth 3, selecting aspotential foci the subtrees rooted at all tokens except1111The report claims that underclass youth do n?t have those opportunities .det nsubjmarkamodnsubjaux negccompdetdobjpunctNegated statement: The report claims that underclass youth don?t have those opportunities.PositivecounterpartStep 1 The report claims that underclass youth do have those opportunities.Step 2 The report claims that underclass youth have those opportunities.Step 3 The report claims that underclass youth have those opportunities.
(idem)Relevant tokens Underclass youth have those opportunities.Potentialpositiveinterpretationsnone coarse Underclass youth [some verb] those opportunities, but not have.nsubj coarse [Some people] have those opportunities, but not Underclass youth.amod fine [Some adjective] youth have those opportunities, but not Underclass youth.nsubj fine Underclass [some people] have those opportunities, but not Underclass youth.dobj coarse Underclass youth have [something], but not those opportunities.det fine Underclass youth have [some] opportunities, but not those opportunities.dobj fine Underclass youth have those [something], but not those opportunities.Table 1: Negated statement and syntactic dependencies (top), and automatically generated positive counterpart and potential posi-tive interpretations (bottom).
For potential interpretations, we include the dependency from the focus to the rest of the interpretation.those whose syntactic dependency is aux, auxpassor punct (auxiliary, passive auxiliary and punctua-tion).
Additionally, we discard potential foci thatconsist only of (1) the determiners the, a and an,or (2) a single token with part-of-speech tag TO,CC, UH, POS, XX, IN, WP or dependency relationprt.
These rules were defined after manually observ-ing several examples and concluding that the cor-responding positive interpretation was useless.
Forexample, from the negated statement And our creditstandards haven?t changed one iota, we avoid gener-ating the useless potential interpretation Our creditstandards X changed one iota, but not have changed.
(focus would be have, with dependency aux).
Sim-ilarly, from It is not supported by the text or his-tory of the Constitution, we avoid generating poten-tial interpretation It is supported by X text or his-tory of the Constitution, but not by the text or his-tory of the Constitution (focus would be the); andfrom You don?t want to get yourself too upset aboutthese things, potential interpretation You want X getyourself too upset about these things, but not to get(focus would be to, with part-of-speech tag TO).Once potential foci are selected, we generate pos-itive interpretations by rewriting each focus with?someone/some people/something/etc.?
and ap-pending ?but not text of focus?
at the end.
Addi-tionally, if the first token of the focus is a preposi-tion, we include it to improve readability, e.g., didn?tleave [by noon]: left by sometime, but not by noon.Note that potential interpretations obtained fromfoci that are direct syntactic dependents of thenegated verb are coarse-grained interpretations, andthe rest are fine-grained interpretations.
Table 1 ex-emplifies the procedure step by step.4.2 Scoring Potential Positive InterpretationsAfter generating potential positive interpretationsautomatically, we asked annotators to score them.Annotators had access to the original negated sen-tence, the previous and next sentence as context,and one potential positive interpretation at a time.The interface asked Given the three sentences [pre-vious sentence, negated sentence and next sentence]above, do you think the statement [positive interpre-tation] below is true?
Annotators were forced to an-swer with a score from 0 to 5, where 0 means ab-solutely disagree and 5 means absolutely agree.
Wedid not provide descriptions for intermediate scoresor use categorical labels.
This simple guidelineswere sufficient to reliably score plausible positive in-terpretations automatically generated (Section 5).5 Corpus AnalysisThe procedure described in Section 4.1 generates9729 potential positive interpretations (5865 coarse-grained and 3864 fine-grained) from 1671 verbalnegations.
Out of all these potential positive inter-pretations, we annotate 1700 (1008 coarse- and 6921112Negated statement, context if relevant to determining scores, and all positive interpretations Score1Context, previous statement:You?re not giving me enough benefits.Negated Statement: You?re not paying me for my overtime work.Context, next statement: Well I think the Walton family does take it personally.- Int.
1.1 [coarse, root]: You?re [some verb] me for my overtime work, but not paying.
4- Int.
1.2 [coarse, nsubj]: [Some people]?re paying me for my overtime work, but not you.
0- Int.
1.3 [coarse, dobj]: You?re paying [somebody] for my overtime work, but not me.
1- Int.
1.4 [coarse, prep]: You?re paying me for [something], but not for my overtime work.
5- Int.
1.5 [fine, poss]: You?re paying me for [somebody?s] overtime work, but not for my overtime work.
0- Int.
1.6 [fine, nn]: You?re paying me for my [some adjective] work, but not for my overtime work.
5- Int.
1.7 [fine, pobj]: You?re paying me for my overtime [something] but not for my overtime work.
02Negated Statement: Those concerns aren?t expressed in public.- Int.
2.1 [coarse, root]: Those concerns are [some verb] in public, but not expressed.
5- Int.
2.2 [coarse, nsubjpass]: [Some things] are expressed in public, but not Those concerns.
5- Int.
2.3 [fine, nsubjpass]: Those [some noun] are expressed in public, but not Those concerns.
2- Int.
2.4 [fine, det]: [Some] concerns are expressed in public but, not Those concerns.
4- Int.
2.5 [coarse, prep]: Those concerns are expressed in [somewhere], but not in public.
5Table 3: Negated statements, all potential positive interpretations automatically generated and their manually assigned scores.Dependency # % Mean SDnsubj 358 21.13% 3.36 1.47dobj 237 14.05% 3.73 1.57pobj 178 10.51% 3.48 1.59ccomp 125 7.29% 3.29 1.77advmod 108 6.39% 3.33 1.59xcomp 90 5.28% 3.92 1.50amod 67 3.96% 4.08 1.29conj 40 2.38% 2.80 1.60advcl 40 2.32% 2.84 1.80nsubjpass 35 2.17% 3.63 1.51other 209 13.34% 2.9 1.7verb 213 12.5% 2.01 1.46All 1,700 100.00% 3.20 1.66Table 2: Basic corpus analysis.
For each dependency, we showthe number of potential interpretations generated (#) and per-centage (%), mean score and standard deviation.fine-grained).
Overall, the mean score is 3.20, andthe standard deviation is 1.66.
Table 2 shows basicstatistics for potential foci, where dependency in-dicates the dependency from the potential focus toa token outside the potential focus.
Most foci arensubj, dobj and pobj, and the mean scores and stan-dard deviation are similar for most dependencies.Annotation Quality.
In order to ensure annotationquality, we calculated Pearson correlation.
Kappaand other measures designed for categorical labelsare ill-suited for our annotations, since not all dis-agreements between numeric scores are the same,e.g., 4 vs. 5 should be counted as higher agreement,than 1 vs. 5.
Overall Pearson correlation was 0.75.5.1 Annotation ExamplesTable 3 presents 2 statements that contain verbalnegation, the list of positive interpretations automat-ically generated and the annotated scores.Example (1) is a simple negated clause, yet wegenerate 7 potential positive interpretations and 3 ofthem receive high scores (4 or 5).
Given You?re notpaying me for my overtime work and the previousstatement, it is reasonable to believe that the authoris in an employee-employer relationship, and theemployer is not fair to the employee.
Interpretations1.1, 1.4 and 1.6 are implicit positive interpretationsintuitively understood by humans when reading theoriginal negated statement.
Namely, Interpretation1.1: You (the employer) are nickel-and-diming mefor my overtime work (focus is paying), Interpreta-tion 1.4: You (the employer) are paying me for some-thing (focus is my overtime work), and Interpretation1.6: You (the employer) are paying me for my regu-lar work (focus is overtime).
These interpretationsshow the benefits of fine-grained interpretations: In-terpretation 1.6 is a refinement of Interpretation 1.4,and the former is more desirable than the latter asit reveals more specific positive knowledge.
The re-maining interpretations are legible, but do not makesense given the negated statement, e.g., interpreta-tion 1.2: Somebody (but not the employer) pays mefor my overtime (focus is You).Example (2) is also a simple negated clause, and4 out of 5 interpretations receive high scores, captur-ing valid positive meaning.
Specifically, Interpreta-1113Type Name DescriptionBasicneg mark word form of negation markverb word form and part-of-speech tag of verbcoarse or fine flag indicating whether interpretation is coarse- or fine-grainedPathsyn path dep syntactic path from focus to verb (concatenation of dependencies)syn path pos syntactic path from focus to verb (concatenation of part-of-speech tags)syn path last dep last syntactic dependency in syn path dep (direct dependent of verb)syn path last pos last part-of-speech tag in syn path pos (direct dependent of verb)Focusfocus length number of words in subgraph chosen as focusfocus first word word form and part-of-speech tag of first word in focusfocus last word word form and part-of-speech tag of last word in focusfocus direction flag indicating whether focus occurs before or after verbfocus head word word form of head of focusfocus head pos part-of-speech tag of head of focusfocus head rel syntactic dependency of head of focusTable 4: Features used to score potential positive interpretations automatically generated.tion 2.1: Those concerns are avoided in public (fo-cus is expressed), Interpretation 2.2: Something isexpressed in public (focus is Those concerns), Inter-pretation 2.4: Some concerns (but not problematicor secret concerns) are expressed in public (focus isThose), and Interpretation 2.5: Those concerns areexpressed in private (focus is in public).5.2 Syntactic Dependencies vs. Semantic RolesThe procedure presented in Section 4.1 is not thefirst to generate potential positive interpretationsfrom negation (Section 3.2).
Our approach has 2 ad-vantages with respect to those grounded on seman-tic roles (Blanco and Sarabi, 2016): (1) it generatesboth coarse- and fine-grained interpretations, and (2)learning to score interpretations is easier becausestate-of-the-art tools extract dependencies more re-liably than semantic roles.To support claim (1), we compare the interpre-tations generated with our procedure and previouswork using semantic roles.
96.12% of interpreta-tions generated using roles are also generated us-ing syntactic dependencies.
Also, using dependen-cies allow us to generate 67.9% of additional (fine-grained) interpretations not obtainable with roles.To support claim (2), we compare interpretationsgenerated with gold and predicted linguistic infor-mation (roles or dependencies).
The overlap with se-mantic roles is 70.1%, and with syntactic dependen-cies, 92.8%.
Syntactic dependencies are thus betterin a realistic scenario because they allow us to auto-matically generate (and score) most interpretations.6 Supervised Learning to Score PotentialPositive InterpretationsWe follow a standard supervised machine learningapproach.
The 1,700 potential positive interpreta-tions along with their scores become instances, andwe divide them into training (80%) and test splits(20%) making sure that all interpretations generatedfrom a sentence are assigned to either the trainingor test splits.
Note that splitting instances randomlywould not be sound: training with some interpreta-tions generated from a negation, and testing with therest of interpretations generated from the same nega-tion would be an unfair evaluation.We train a Support Vector Machine for regressionwith RBF kernel using scikit-learn (Pedregosa et al,2011), which in turn uses LIBSVM (Chang and Lin,2011).
SVM parameters (C and ?)
were tuned using10-fold cross-validation with the training set, and re-sults are calculated using the test set.6.1 Feature SelectionTable 4 presents the full feature set.
Features are rel-atively simple and characterize the verbal negationfrom which a potential interpretation was generated,as well as the interpretation per se, i.e., the depen-dency subgraph chosen as potential focus.Basic features account for the negation mark, thenegation verb (word form and part-of-speech tag)and a binary flag indicating whether we are scoringa coarse- or fine-grained interpretation.Path features are derived from the syntactic path1114Features Gold Predictedneg mark -0.109 -0.077basic 0.033 0.026basic + path 0.474 0.482basic + path + focus 0.530 0.560Table 5: Pearson correlations obtained with the test split.
Re-sults are provided using gold-standard and predicted linguisticinformation (part-of-speech tags and syntactic dependencies).between the subgraph selected as focus and the verb.We include the actual path (concatenation of de-pendencies and up/down symbols), and the modi-fied path using part-of-speech tags.
Additionally, wealso include the last dependency and part-of-speechtag, i.e., the ones closest to the verb in the path.Focus features characterize the dependency sub-graph chosen as focus to generate the potential inter-pretation.
Specifically, we include the number of to-kens, word form and part-of-speech tags of the firstand last tokens, and whether the focus occurs beforeor after the verb.
We also include features derivedform the head of the focus, which we define as thetoken whose syntactic head is outside the focus.
Weinclude the word form and part-of-speech of the fo-cus head, as well as its the dependency.7 Experiments and ResultsWe report results obtained with several combina-tions of features in Table 5.
We detail results ob-tained with features extracted from gold-standardand predicted linguistic annotations (part-of-speechtags and syntactic dependencies) as annotated in thegold and auto files from the CoNLL-2011 SharedTask release of OntoNotes (Pradhan et al, 2011).All models are trained with gold-standard linguis-tic annotations, and tested with either gold-standardor predicted linguistic annotations.Testing with gold-standard POS tags and syn-tactic dependencies.
Training with the word formof the negation mark is virtually useless, it yieldsa Pearson correlation of ?0.109.
Basic features(negation mark, verb and flag indicating coarse-or fine-grained interpretation) are also ineffectiveto score potential interpretations (Pearson: 0.033).Including features derived from the syntactic pathyields higher correlation, 0.474, even though thesefeatures only capture the syntactic relationship be-tween the focus from which the interpretation wasgenerated and the verb.
Finally, adding focusfeatures yields the best results (Pearson: 0.53,+11.8%).Testing with predicted POS tags and syntactic de-pendencies.
We selected 20% of positive interpre-tations in our corpus as test instances, totalling 379interpretations (Section 6).
When executing the pro-cedure to generate potential interpretations (Section4.1) with predicted linguistic information, however,we are unable to generate all of them due to incorrectand missing syntactic dependencies.
Specifically,352 of the 379 interpretations are generated (92.8%).While we do not generate 7.2% of instances, thispercentage is substantially lower than previous workgrounded on semantic roles (Section 5.2).Pearson correlations with predicted linguistic in-formation are calculated using the 352 instances thatwere also generated with gold dependencies (andthus assigned a score during the manual annota-tions).
Correlations are slightly higher and followa similar trend than the correlations obtained withgold-standard linguistic information.
These resultsshould be taken with a grain of salt: the test in-stances are not exactly the same, and the 352 testinstances in this scenario are presumably easier toscore than the remainder 27, as dependencies werepredicted correctly.8 ConclusionsHumans intuitively extract positive meaning fromnegation when reading text.
This paper presentsan automated procedure to generate potential posi-tive interpretations from verbal negation, and scorethem according to their likelihood.
Our procedure isgrounded on syntactic dependencies, allowing us toextract fine-grained interpretations beyond semanticroles (67.9% additional interpretations).
Addition-ally, because dependencies are extracted automati-cally more reliably than semantic roles, we gener-ate 92.8% of all potential interpretations when us-ing predicted linguistic information, as opposed to70.1% with semantic roles.On average, we generate 6.4 potential interpreta-tions per verbal negation (coarse-grained: 3.8, fine-grained: 2.6).
Manual annotations show that po-tential interpretations are deemed likely.
The mean1115score is 3.20 (out of 5.0), thus we extract a substan-tial amount of positive meaning.The work presented in this paper is not tied toany existing semantic representation.
While we relyheavily on syntactic dependencies, positive interpre-tations are generated in plain text, and they couldbe processed, along with the original negated state-ment, with any NLP pipeline.ReferencesAmjad Abu-Jbara and Dragomir Radev.
2012.
Umichi-gan: A conditional random field model for resolvingthe scope of negation.
In Proceedings of the First JointConference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference andthe shared task, and Volume 2: Proceedings of theSixth International Workshop on Semantic Evaluation,pages 328?334.
Association for Computational Lin-guistics.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*sem 2013 sharedtask: Semantic textual similarity.
In Second JointConference on Lexical and Computational Semantics(*SEM), Volume 1: Proceedings of the Main Confer-ence and the Shared Task: Semantic Textual Similarity,pages 32?43, Atlanta, Georgia, USA, June.
Associa-tion for Computational Linguistics.Pranav Anand and Craig Martell.
2012.
Annotatingthe focus of negation in terms of questions under dis-cussion.
In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in ComputationalLinguistics, ExProM ?12, pages 65?69, Stroudsburg,PA, USA.
Association for Computational Linguistics.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-ings of the 17th international conference on Computa-tional Linguistics, Montreal, Canada.Laura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
2013.
Abstract meaning representationfor sembanking.
In Proceedings of the 7th LinguisticAnnotation Workshop and Interoperability with Dis-course, pages 178?186, Sofia, Bulgaria, August.
As-sociation for Computational Linguistics.Simon Blackburn.
2008.
The Oxford Dictionary of Phi-losophy.
Oxford University Press.Eduardo Blanco and Dan Moldovan.
2011.
Semanticrepresentation of negation using focus detection.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 581?589,Portland, Oregon, USA, June.
Association for Compu-tational Linguistics.Eduardo Blanco and Zahra Sarabi.
2016.
Automaticgeneration and scoring of positive interpretations fromnegated statements.
In Proceedings of the 2016 Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 1431?1441, San Diego, Califor-nia, June.
Association for Computational Linguistics.Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm:A library for support vector machines.
ACM Trans.Intell.
Syst.
Technol., 2(3):27:1?27:27, May.H.
H. Clark and W. G. Chase.
1972.
On the process ofcomparing sentences against pictures.
Cognitive Psy-chology, 3(3):472?517, July.Isaac Councill, Ryan McDonald, and Leonid Velikovich.2010.
What?s great and what?s not: learning to clas-sify the scope of negation for improved sentiment anal-ysis.
In Proceedings of the Workshop on Negation andSpeculation in Natural Language Processing, pages51?59, Uppsala, Sweden, July.
University of Antwerp.Marie-Catherine de Marneffe and Christopher D Man-ning.
2008.
Stanford typed dependencies manual.Technical report, Technical report, Stanford Univer-sity.Federico Fancellu, Adam Lopez, and Bonnie Webber.2016.
Neural networks for negation scope detection.In Proceedings of the 54th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 495?504, Berlin, Germany, Au-gust.
Association for Computational Linguistics.Laurence R. Horn and Heinrich Wansing.
2015.
Nega-tion.
In Edward N. Zalta, editor, The Stanford Ency-clopedia of Philosophy.
Summer 2015 edition.Laurence R. Horn.
1989.
A natural history of negation.Chicago University Press, Chicago.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:the 90% Solution.
In NAACL ?06: Proceedings ofthe Human Language Technology Conference of theNAACL, Companion Volume: Short Papers on XX,pages 57?60, Morristown, NJ, USA.
Association forComputational Linguistics.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In KDD ?04: Proceedingsof the tenth ACM SIGKDD international conferenceon Knowledge discovery and data mining, pages 168?177, New York, NY, USA.
ACM.Rodney D. Huddleston and Geoffrey K. Pullum.
2002.The Cambridge Grammar of the English Language.Cambridge University Press, April.Junhui Li, Guodong Zhou, Hongling Wang, and Qiaom-ing Zhu.
2010.
Learning the Scope of Negation via1116Shallow Semantic Parsing.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics (Coling 2010), pages 671?679, Beijing, China,August.
Coling 2010 Organizing Committee.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Association for Computational Lin-guistics (ACL) System Demonstrations, pages 55?60.Ruslan Mitkov.
2005.
The Oxford handbook of compu-tational linguistics.
Oxford University Press.Roser Morante and Eduardo Blanco.
2012.
*SEM 2012Shared Task: Resolving the Scope and Focus of Nega-tion.
In Proceedings of the First Joint Conference onLexical and Computational Semantics (*SEM 2012),pages 265?274, Montre?al, Canada, June.Roser Morante and Walter Daelemans.
2012.Conandoyle-neg: Annotation of negation in conandoyle stories.
In Proceedings of the Eighth Interna-tional Conference on Language Resources and Evalu-ation, Istanbul.Roser Morante and Caroline Sporleder.
2012.
Modal-ity and negation: An introduction to the special issue.Comput.
Linguist., 38(2):223?260, June.Ann E Nordmeyer and Michael C Frank.
2013.
Measur-ing the comprehension of negation in 2-to 4-year-oldchildren.
Proceedings of the 35th Annual Conferenceof the Cognitive Science Society.
Austin, TX: CognitiveScience Society.Lilja ?vrelid, Erik Velldal, and Stephan Oepen.
2010.Syntactic Scope Resolution in Uncertainty Analysis.In Proceedings of the 23rd International Conferenceon Computational Linguistics (Coling 2010), pages1379?1387, Beijing, China, August.
Coling 2010 Or-ganizing Committee.Arzucan O?zgu?r and Dragomir R. Radev.
2009.
Detect-ing Speculations and their Scopes in Scientific Text.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1398?1407, Singapore, August.
Association for Com-putational Linguistics.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?106.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification using ma-chine learning techniques.
In Proceedings of the 2002Conference on Empirical Methods in Natural Lan-guage Processing, pages 79?86.
Association for Com-putational Linguistics, July.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
Conll-2011 shared task: Modeling unrestrictedcoreference in ontonotes.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 1?27, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.Randolph Quirk, Sidney Greenbaum, and GeoffreyLeech.
2000.
A comprehensive grammar of the En-glish language.
Longman, London.Johan Reitan, J?rgen Faret, Bjo?rn Gamba?ck, and LarsBungum.
2015.
Negation scope detection for twittersentiment analysis.
In Proceedings of the 6th Work-shop on Computational Approaches to Subjectivity,Sentiment and Social Media Analysis, pages 99?108,Lisboa, Portugal, September.
Association for Compu-tational Linguistics.Mats Rooth.
1985.
Association with focus.Mats Rooth.
1992.
A theory of focus interpretation.
Nat-ural language semantics, 1(1):75?116.Sabine Rosenberg and Sabine Bergler.
2012.
Ucon-cordia: Clac negation focus detection at *sem 2012.In *SEM 2012: The First Joint Conference on Lexi-cal and Computational Semantics ?
Volume 1: Pro-ceedings of the main conference and the shared task,and Volume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation (SemEval 2012),pages 294?300, Montre?al, Canada, 7-8 June.
Associa-tion for Computational Linguistics.Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, andJa?nos Csirik.
2008.
The BioScope corpus: annotationfor negation, uncertainty and their scopein biomedicaltexts.
In Proceedings of BioNLP 2008, pages 38?45,Columbus, Ohio, USA.
ACL.Josef Taglicht.
1984.
Message and emphasis: On fo-cus and scope in English, volume 15.
Addison-WesleyLongman Limited.Ton van der Wouden.
1997.
Negative contexts: colloca-tion, polarity, and multiple negation.
Routledge, Lon-don.Erik Velldal, Lilja Ovrelid, Jonathon Read, and StephanOepen.
2012.
Speculation and negation: Rules,rankers, and the role of syntax.
Comput.
Linguist.,38(2):369?410, June.Bowei Zou, Guodong Zhou, and Qiaoming Zhu.
2014.Negation focus identification with contextual dis-course information.
In Proceedings of the 52nd An-nual Meeting of the Association for Computational1117Linguistics (Volume 1: Long Papers), pages 522?530,Baltimore, Maryland, June.
Association for Computa-tional Linguistics.1118
