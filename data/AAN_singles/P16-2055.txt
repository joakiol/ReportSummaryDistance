Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 337?343,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsText Simplification as Tree LabelingJoachim BingelCentre for Language TechnologyUniversity of Copenhagenbingel@hum.ku.dkAnders S?gaardCentre for Language TechnologyUniversity of Copenhagensoegaard@hum.ku.dkAbstractWe present a new, structured approach totext simplification using conditional ran-dom fields over top-down traversals ofdependency graphs that jointly predictspossible compressions and paraphrases.Our model reaches readability scores com-parable to word-based compression ap-proaches across a range of metrics and hu-man judgements while maintaining moreof the important information.1 IntroductionSentence-level text simplification is the problemof automatically modifying sentences so that theybecome easier to read, while maintaining most ofthe relevant information in them.
This can benefitapplications as pre-processing for machine trans-lation (Bernth, 1998) and assisting technologiesfor readers with reduced literacy (Carroll et al,1999; Watanabe et al, 2009; Rello et al, 2013).Sentence-level text simplification ignores sen-tence splitting and reordering, and typically fo-cuses on compression (deletion of words) andparaphrasing or lexical substitution (Cohn andLapata, 2008).
We include paraphrasing and lexi-cal substitution here, while previous work in sen-tence simplification has often focused exclusivelyon deletion.
Approaches that address compres-sion and paraphrasing (or more tasks) integrallyinclude (Zhu et al, 2010; Narayan and Gardent,2014; Mandya et al, 2014).Simplification beyond deletion is motivated byPitler?s (2010) observation that abstractive sen-tence summaries written by humans often ?includeparaphrases or synonyms (?said?
versus ?stated?
)and use alternative syntactic constructions (?gaveJohn the book?
versus ?gave the book to John?).
?Such lexical or syntactic alternations may con-tribute strongly to the readability of a sentence ifthey replace difficult words with shorter or morefamiliar ones, in particular for low-literacy readers(Rello et al, 2013).
Our joint approach to deletionand paraphrasing works against the limitation thatabstractive simplifications ?are not capable of be-ing generated by [...] most sentence compressionalgorithms?
(Pitler, 2010).Furthermore, a central concern in text simplifi-cation is to ensure the grammaticality of the out-put, especially with low-proficiency readers as thetarget audience.
Our approach to this problem is toremove or paraphrase entire syntactic units in theoriginal sentence, thus avoiding to remove phraseheads without removing their arguments or mod-ifiers.
Like Filippova and Strube (2008), we relyon dependency structures rather than constituentstructures, which promises more robust syntacticanalysis and allows us to operate on discontinuoussyntactic units.Contributions We present a sentence simplifi-cation model which is, to the best of our knowl-edge, the first model that uses structured predic-tion over dependency trees and models compres-sion and paraphrasing jointly.
Our model usesViterbi decoding rather than scoring of all can-didates and outputs probabilities reflecting modelconfidence.2 DataWe use the publicly available Google compres-sion data set,1which consists of 10,000 Englishsentence triples with (1) the original sentence aspresent in the body of an online news article, (2)a headline based on the original sentence, and (3)a compression that is automatically derived fromthe original such that it only contains word forms1http://storage.googleapis.com/sentencecomp/compressiondata.json337Figure 1: An example simplification treepresent in the original, preserving their order.
Thefollowing sentence triple exemplifies these differ-ent versions:(1) In official documents released earlier thismonth it appears the Queen of Englandused the wrong name for the Republic ofIreland when writing to president PatrickHillery.
(2) Queen elizabeth ii used wrong name forRepublic(3) The Queen of England used the wrongname for the Republic of Ireland.The data is pre-processed with the StanfordCoreNLP tools (Manning et al, 2014), retrievinglemmas, parts-of-speech, named entities and de-pendency trees.
We reserve the first 200 sentencesfrom the data set for evaluation, the next 200 fortuning parameters (including the used PPDB ver-sions, see next paragraph), and use the remaining9,600 sentences for training our model.Deletion and paraphrase targets As our ap-proach operates on dependency trees, aiming toprune or paraphrase subtrees from the dependencytree of a sentence, we identify deleted or para-phrased subtrees, marking their heads with a cor-responding label.
A subtree receives a Delete labelif none of the words subsumed by this subtree oc-cur in the compressed version of the sentence.We identify paraphrased subsequences in anoriginal sentence by looking up the subsequencestring in the Paraphrase Database (PPDB) (Gan-itkevitch et al, 2013) and testing if one of its pos-sible paraphrases occurs in the headline version ofthe sentence in question.
The Paraphrase Database1.0 is a set of phrasal and lexical pairs that wereautomatically acquired from bilingual parallel cor-pora, and thus contain a portion of flawed para-phrase pairs.
The database comes in a numberof different sizes, where small editions are re-stricted to high-precision paraphrases with rela-tively high paraphrase probabilities.
As the twosmallest editions of PPDB only yield a very lownumber of paraphrase targets (less than 100 in theentire Google compression data set), we opt to em-ploy a medium-sized version of the resource (size?L?)
and find a total of 510 phrasal and lexicalparaphrases in the corpus.3 MethodWe assume that text simplification is a genera-tive process on syntactic dependency graphs witha paraphrase dictionary.
A dependency graphG = (V,A) is a labeled directed graph inthe standard graph-theoretic sense and consistsof nodes, V , and arcs, A, such that for sen-tence S = w0w1.
.
.
wnand label set R, V ?
{w0, w1, .
.
.
, wn}, and A ?
V ?
R ?
V hold,and if (wi, r, wj) ?
A then (wi, r?, wj) 6= A forall r?6= r. We restrict the dependency graphsto the class of trees, i.e., for (wi, r, wj) ?
A, if(wk, r, wj) ?
A then k = i.The generative process traverses the tree in atop-down fashion, deleting or paraphrasing sub-trees (see Figure 1).
Note that elements in sub-trees dominated by a deleted node are automati-cally deleted (analogously for paraphrases).For each dependency tree G = (V,A) in atraining set of T sentences, we derive an in-put sequence of K-dimensional feature vectorsx = x1, .
.
.
, xnand an output sequence ofy = y1, .
.
.
, yn.
Our tree-to-string simplificationmodel is a second-order linear-chain conditionalrandom field (CRF)p(y|x) =1Z(x)n?i=1exp{K?k=1?kfk(yt, yt?1, xt)}with yi= Delete if and only if xirepresents theleast upper bound in G covering a deleted spanin the training data, and yi= Paraphrase if andonly if xirepresents the least upper bound in Gcovering a paraphrased span in the training data.For example, if the entire sentence is deleted, and(w0, r, wi) ?
A, then yi= Delete (but yj= Leavefor j 6= i).This encoding means that theoretically we canpredict to paraphrase a subtree that is dominatedby a node which is in turn predicted to be deleted338(or vice versa).
However, once an operation is car-ried out on a subtree, none of its dominated nodesare considered in the remainder of the top-downsimplification process.
Giving preference to op-erations at higher-level syntactic environments inthis manner serves as a mechanism to resolve am-biguities in the decision process by taking a widercontext into account.Furthermore, predicting a node to get para-phrased at the right corner of a deleted subtreecan potentially influence labeling decisions out-side this subtree as a consequence of the dynamic-program Viterbi decoding.
We acknowledge thatthis is a theoretical drawback of the presented ap-proach, but given that we do not observe any suchdependency graphs in our data, we do not expectthis to be a serious problem in most cases.Whenever our model predicts that a subtree beparaphrased, we look up the respective token se-quence in PPDB and replace it with the candi-date paraphrase (if available) that maximises theproduct of frequency and translation probabilityaccording to PPDB.Features for CRF model We train a second-order CRF model using MarMoT (Mueller et al,2013), an efficient higher-order CRF implementa-tion.
The model computes its observational prob-abilities from features based on properties of thesubtree root token (incl.
POS, language modelprobability, NE mention, word difficulty), of theinternal structure of the subtree (incl.
number ofchildren, depth, length of sequence), and of theexternal grammatical structure (incl.
dependencyrelation, parent POS, distance from parent, posi-tion in sentence).4 EvaluationBaselines In the following experiments, wecompare our approach to state-of-the-art ap-proaches to sentence compression and joint com-pression/paraphrasing.
For the first of these twocategories, we consider the LSTM system de-scribed in Filippova et al (2015) as well as theresults reported therein for the MIRA system (Mc-Donald, 2006).
As a joint approach, we considerReluctant Trimmer (RT), a simplification systemthat employs synchronous dependency grammars(Mandya et al, 2014).
Since the LSTM system re-quires great amounts of training data, which werenot available to us, we cannot reproduce its out-Recall Precision F1Reluctant TrimmertokensDelete 54.60 20.23 29.52Paraphrase 01.67 66.67 03.27Leave 52.27 78.54 54.60Tree LabelingsubtreesDelete 43.31 67.54 52.77Paraphrase 23.85 50.89 32.48Leave 94.29 84.82 89.30tokensDelete 49.67 77.16 60.44Paraphrase 21.16 51.52 30.00Leave 80.33 50.91 62.32Table 1: Performance on joint deletion and para-phrasing detection for our tree labeling system(evaluating both on entire subtrees and token level)as well as for the RT baseline (tokens only).Note that RT is trained on the (Simple) EnglishWikipedia, not on the Google compressions, andtherefore the results may not be directly compara-ble.put and therefore limit our comparison of humanrankings to the eleven output examples providedin the paper.F-Scores We first evaluate our tree labelingmodel (TL) on its ability to predict subtree dele-tion and paraphrasing (i.e.
whether a subtreeshould be paraphrased, independent of the actualreplacement).
The results for this evaluation setup,as well as word-level performance, are listed inTable 1 and compared to RT.
Note that for dele-tion and paraphrasing, our model consistently hashigher precision than recall, thus generating moreconfident simplifications and less ungrammaticaloutput.Automated Readability Scores Table 2 reportsthe compression ratio (CR, percentage of retainedwords) as well as automated readability scores thatour model achieves on the test set and compares itto the output of the RT baseline.
Our system man-ages to compress the original texts by more thanone third, but the gold simplifications (headlinesand compressions) are still considerably shorter.Our approach improves readability as mea-sured by the Flesch Reading Ease score2(Flesch,2The negative value that the headlines receive for this met-339Data version CR?Flesch?Dale-C.?Original ?
49.15 9.55Headlines 0.32* -80.77* 17.61*Compressions 0.40* 70.80* 9.56TL output 0.62* 56.25* 9.30*RT output 0.86* 60.65* 9.27*Table 2: Compression ratios and automatic read-ability scores for the Google compression data set,compared to the system output.
Readability is in-dicated by a high Flesh Reading Ease score and alow Dale-Chall score.
* indicates differences com-pared to the original sentences that are significantat p < 10?3.System Readability InformativenessMIRA 4.31 3.55LSTM 4.51 3.78TL 4.14 4.01RT (11) 3.09 4.12LSTM (11) 4.23 3.42TL (11) 4.21 4.15Table 3: Mean readability and informativeness rat-ings for the first 200 sentences in the Google data(upper) and for the 11 sample sentences listed inFilippova et al (2015) (lower).1948) and the Dale-Chall formula (Dale and Chall,1948).
The former score measures textual diffi-culty as a function of sentence length and the num-ber of syllables per word, while the latter aims toestimate a US school grade level at which a textcan be well understood, based on a vocabulary list.Both metrics deem the output of our system eas-ier to read than the original texts, while the Dale-Chall formula also rates our system better than thegold simplifications.Human Readability Ratings Following Filip-pova et al (2015) in their evaluation setup forthe sake of comparability, we ask raters to as-sign scores on a one-to-five Likert scale to the first200 sentences from the Google compression datapaired with the output of our system.
Each pairis rated by three native or near-native speakers ofEnglish.The raters are asked to evaluate the sentenceric is due to an over-representation of longer words in head-lines.pairs for readability and informativeness.
Theformer, following Filippova et al (2015), ?cov-ers the grammatical correctness, comprehensibil-ity and fluency of the output.?
The latter metricpertains to the relation between the original sen-tence and the system output as it ?measures theamount of important content preserved in the com-pression.
?Table 3 compares the performance of our modelto the figures reported in Filippova et al (2015) fortheir LSTM model and McDonald?s (2006) system(MIRA).
For a comparison with the same judges,we repeat the evaluation with the 11 sample out-put compressions listed in Filippova et al (2015)aswell as the respective output from Reluctant Trim-mer; see the lower part of Table 3.
The resultssuggest that, compared to the compression-onlyLSTMs, our approach yields comparable perfor-mance in terms of readability, while maintainingmore of the central information in the original sen-tences.
Compared to RT, our system does con-siderably better in terms of readability and retainsslightly more of the important information.5 Related WorkSeveral approaches to sentence compression havebeen presented in the last decade.
Knight andMarcu (2002) and Turner and Charniak (2005) ap-ply noisy channel models, using language modelsto control for grammaticality.
McDonald (2006)introduces a different approach, discriminativelytraining a scoring function, informed by syntac-tic features, to score all possible subtrees of asentence.
His work was inspired by Riezler etal.
(2003) scoring substrings generated from LFGparses.
A third approach to sentence compressionis sequence labeling, which has been explored byElming et al (2013) using linear-chain CRFs withsyntactic features, and more recently by Filippovaet al (2015) and Klerke et al (2016) using recur-rent neural networks with LSTM cells.Most recent approaches to sentence compres-sion make use of syntactic analysis, either byoperating directly on trees (Riezler et al, 2003;Nomoto, 2007; Filippova and Strube, 2008; Cohnand Lapata, 2008; Cohn and Lapata, 2009) or byincorporating syntactic information in their model(McDonald, 2006; Clarke and Lapata, 2008).Recently, however, Filippova et al (2015) pre-sented an approach to sentence compression using340Original Sentence & SimplificationsO OG&E is warning customers about a prepaid debit card scam that is targeting utility customersacross the county.C OG&E is warning customers about a scam.R OG&E is warning customers about a debit card scam that is targeting utility customers acrossthe country.T OG&E is warning customers regarding a prepaid debit card scam.O The husband of murdered Melbourne woman Jill Meagher will return to Ireland later thismonth ?to clear his head?
while fighting for parole board changes.C The husband of murdered woman Jill Meagher will return to Ireland.R The husband of Melbourne woman Jill Meagher will return to Ireland this month to clear hishead fighting for parole board changes.T The husband of murdered Melbourne woman Jill Meagher will return to Ireland.O A research project has found that taxi drivers often don?t know what the speed limit is.C Taxi drivers don?t know the speed limit is.R A research project has found that drivers often do not know what the speed limit is.T A project has found taxi drivers don?t know what the speed limit is.Table 4: Example output for original sentences (O) as generated by the Reluctant Trimmer baseline (R)and our tree labeling system (T), as well as the headline-generated Google compressions (C).LSTMs with word embeddings, with no syntacticfeatures.
We return to working directly on trees,presenting a tree-to-string model of sentence sim-plification.
Our model has interesting similaritiesto (Riezler et al, 2003), but uses Viterbi decod-ing rather than scoring of all candidates.
Also,it follows Cohn and Lapata (2008) in going be-yond most of these models, modeling compressionand paraphrasing.For lexical simplification, most systems typi-cally use pre-compiled dictionaries (Devlin, 1999;Inui et al, 2003) and select the synonym candidatewith the highest frequency.
More recently, Baeza-Yates et al (2015) introduced an algorithm for lex-ical simplification in Spanish that selects the bestsynonym candidate in a context-sensitive fashion.Cohn and Lapata (2008), Woodsend and Lap-ata (2011) and Mandya et al (2014) present jointapproaches to compression and paraphrasing thatare based on (quasi-) synchronous grammars, andsimilarly Zhu et al (2010) take a syntax-basedapproach, but employ a probabilistic model ofvarious simplification operations.
Napoles et al(2011) do not use syntactic information, but in-stead employ a character-based metric to compressand paraphrase.6 ConclusionWe presented a new approach to sentence sim-plification that uses linear-chain conditional ran-dom fields over dependency graphs to jointly pre-dict compression and paraphrasing of entire syn-tactic units.
The objective of our model is todelete or paraphrase entire subtrees in dependencygraphs as a strategy to avoid ungrammatical out-put.
Our approach makes innovative use of athree-fold parallel monolingual corpus that fea-tures headlines and compressions to learn para-phrases and deletions, respectively.
Human eval-uation shows that our approach leads to readabil-ity figures that are comparable to previous state-of-the-art approaches to the more basic sentencecompression task, and better than previous workon joint compression and paraphrasing.
Whileour model does rely on syntactic analysis, it onlyneeds a tiny fraction (less than 0.5%) of the train-ing data used by Filippova et al (2015).AcknowledgmentsThis research was partially funded by the ERCStarting Grant LOWLANDS No.
313695, as wellas by Trygfonden.ReferencesRicardo Baeza-Yates, Luz Rello, and Julia Dembowski.2015.
Cassa: A context-aware synonym simplifica-tion algorithm.
Proceedings of the 2015 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology, pages 1380?1385.341Arendse Bernth.
1998.
EasyEnglish: Preprocessingfor MT.
In Proceedings of the Second Interna-tional Workshop on Controlled Language Applica-tions, pages 30?41.John Carroll, Guido Minnen, Darren Pearce, YvonneCanning, Siobhan Devlin, and John Tait.
1999.Simplifying text for language-impaired readers.
InProceedings of EACL, volume 99, pages 269?270.James Clarke and Mirella Lapata.
2008.
Global in-ference for sentence compression: An integer linearprogramming approach.
Journal of Artificial Intelli-gence Research, pages 399?429.Trevor Cohn and Mirella Lapata.
2008.
Sentencecompression beyond word deletion.
In Proceedingsof the 22nd International Conference on Computa-tional Linguistics-Volume 1, pages 137?144.
Asso-ciation for Computational Linguistics.Trevor Cohn and Mirella Lapata.
2009.
Sentence com-pression as tree transduction.
Journal of ArtificialIntelligence Research, pages 637?674.Edgar Dale and Jeanne S Chall.
1948.
A formula forpredicting readability: Instructions.
Educational re-search bulletin, pages 37?54.Siobhan Lucy Devlin.
1999.
Simplifying natural lan-guage for aphasic readers.
Ph.D. thesis, Universityof Sunderland.Jakob Elming, Anders Johannsen, Sigrid Klerke,Emanuele Lapponi, Hector Martinez Alonso, andAnders S?gaard.
2013.
Down-stream effects oftree-to-dependency conversions.
In HLT-NAACL,pages 617?626.Katja Filippova and Michael Strube.
2008.
Depen-dency tree based sentence compression.
In Proceed-ings of the Fifth International Natural LanguageGeneration Conference, pages 25?32.
Associationfor Computational Linguistics.Katja Filippova, Enrique Alfonseca, Carlos A Col-menares, Lukasz Kaiser, and Oriol Vinyals.
2015.Sentence compression by deletion with lstms.
InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages360?368.Rudolph Flesch.
1948.
A new readability yardstick.Journal of applied psychology, 32(3):221.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proceedings of NAACL-HLT, pages758?764, Atlanta, Georgia, June.
Association forComputational Linguistics.Kentaro Inui, Atsushi Fujita, Tetsuro Takahashi, RyuIida, and Tomoya Iwakura.
2003.
Text simplifica-tion for reading assistance: a project note.
In Pro-ceedings of the second international workshop onParaphrasing-Volume 16, pages 9?16.
Associationfor Computational Linguistics.Sigrid Klerke, Yoav Goldberg, and Anders S?gaard.2016.
Improving sentence compression by learningto predict gaze.
In Proceedings of ACL 2016 (short).Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: A probabilistic ap-proach to sentence compression.
Artificial Intelli-gence, 139(1):91?107.Angrosh A. Mandya, Tadashi Nomoto, and AdvaithSiddharthan.
2014.
Lexico-syntactic text simplifi-cation and compression with typed dependencies.
InCOLING, pages 1996?2006.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In Proceedings of 52ndAnnual Meeting of the Association for Computa-tional Linguistics: System Demonstrations, pages55?60.Ryan T McDonald.
2006.
Discriminative sentencecompression with soft syntactic evidence.
In EACL.Thomas Mueller, Helmut Schmid, and HinrichSch?utze.
2013.
Efficient higher-order CRFs formorphological tagging.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, pages 322?332, Seattle, Wash-ington, USA, October.
Association for Computa-tional Linguistics.Courtney Napoles, Chris Callison-Burch, Juri Ganitke-vitch, and Benjamin Van Durme.
2011.
Paraphras-tic sentence compression with a character-basedmetric: Tightening without deletion.
In Proceed-ings of the Workshop on Monolingual Text-To-TextGeneration, pages 84?90.
Association for Computa-tional Linguistics.Shashi Narayan and Claire Gardent.
2014.
Hybridsimplification using deep semantics and machinetranslation.
In the 52nd Annual Meeting of the As-sociation for Computational Linguistics, pages 435?445.Tadashi Nomoto.
2007.
Discriminative sentence com-pression with conditional random fields.
Informa-tion Processing and Management: an InternationalJournal, 43(6):1571?1587.Emily Pitler.
2010.
Methods for sentence compres-sion.
Technical report, Department of Computer andInformation Science, University of Pennsylvania.Luz Rello, Ricardo Baeza-Yates, Laura Dempere-Marco, and Horacio Saggion.
2013.
Frequent wordsimprove readability and short words improve under-standability for people with dyslexia.
In Human-Computer Interaction?INTERACT 2013, pages 203?219.
Springer.Stefan Riezler, Tracy H King, Richard Crouch, and An-nie Zaenen.
2003.
Statistical sentence condensa-tion using ambiguity packing and stochastic disam-biguation methods for lexical-functional grammar.342In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 118?125.
Association for Compu-tational Linguistics.Jenine Turner and Eugene Charniak.
2005.
Super-vised and unsupervised learning for sentence com-pression.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 290?297.
Association for Computational Lin-guistics.Willian Massami Watanabe, Arnaldo Candido Junior,Vin?
?cius Rodriguez Uz?eda, Renata Pontin de Mat-tos Fortes, Thiago Alexandre Salgueiro Pardo, andSandra Maria Alu??sio.
2009.
Facilita: reading as-sistance for low-literacy readers.
In Proceedings ofthe 27th ACM international conference on Design ofcommunication, pages 29?36.
ACM.Kristian Woodsend and Mirella Lapata.
2011.
Learn-ing to simplify sentences with quasi-synchronousgrammar and integer programming.
In Proceedingsof the conference on empirical methods in naturallanguage processing, pages 409?420.
Associationfor Computational Linguistics.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proceedings of the23rd international conference on computational lin-guistics, pages 1353?1361.
Association for Compu-tational Linguistics.343
