Bayesian Nets in Syntactic Categorization of Novel WordsLeonid PeshkinDept.
of Computer ScienceHarvard UniversityCambridge, MApesha@eecs.harvard.eduAvi PfefferDept.
of Computer ScienceHarvard UniversityCambridge, MAavi@eecs.harvard.eduVirginia SavovaDept.
of Cognitive ScienceJohns Hopkins UniversityCambridge, MAsavova@jhu.eduAbstractThis paper presents an application of aDynamic Bayesian Network (DBN) to thetask of assigning Part-of-Speech (PoS)tags to novel text.
This task is particularlychallenging for non-standard corpora,such as Internet lingo, where a large pro-portion of words are unknown.
Previouswork reveals that PoS tags depend on avariety of morphological and contextualfeatures.
Representing these dependenciesin a DBN results into an elegant and  ef-fective PoS tagger.1 IntroductionUncovering the syntactic structure of texts is anecessary step towards extracting their meaning.
Inorder to obtain an accurate parse for an unseentext, we need to assign Part-of-Speech (PoS) tagsto a string of words.
This paper covers one aspectof our work of PoS tagging with Dynamic Bayes-ian Networks (DBNs), which demonstrates theirsuccess at tagging unknown (OoV) words.
Pleaserefer to the companion paper [Peshkin, 2003] forsubstantial discussion of our method and other de-tails.
Although currently existing algorithms ex-hibit high word-level accuracy, PoS tagging is nota solved problem.
First, even a small percentage oferrors may derail subsequent processing steps.Second, the results of tagging are not robust if alarge proportion of words are unknown, or if thetesting corpus differs in style from the trainingcorpus.
At the same time, diverse training corporaare lacking and most taggers are trained on a largeannotated corpus extracted from the Wall StreetJournal (WSJ).
These factors significantly hamperthe use PoS tagging to extract information fromnon-standard corpora, such as email messages andwebsites.
Our work on Information Extractionfrom an email corpus left us searching for a PoStagger that would perform well on Internet textsand integrate easily into a large probabilistic rea-soning system by producing a distribution overtags rather than   deterministic answer.
Internetsources exhibit a set of idiosyncratic characteristicsnot present in the training corpora available to tag-gers to date.
They are often written in telegraphicstyle, omitting closed-class words, which leads to ahigher percentage of ambiguous items.
Most im-portantly, as a consequence of the rapidly evolvingNetlingo, Internet texts are full of new words, mis-spelled words and one-time expressions.
Thesecharacteristics are bound to lower the accuracy ofexisting taggers.
A look at the literature confirmsthat error rates for unknown words are quite high.According to several recent publications [Tou-tanova 2002, Lafferty et al2002] OoV tagging pre-sents a serious challenge to the field.
Thetransformation-based Brill tagger, achieves 96.5%accuracy for the WSJ, but a mere 85% on unknownwords.
Existing probabilistic taggers also don?tfare well on unknown words.
Reported results onOoV rarely exceed Brill?s performance by a tinyfraction.
They are mostly based on (Hidden)Markov Models [Brants 2000, Kupiec, 1992].
Amodel based on Conditional Random Fields[Lafferty et al] outperforms the HMM tagger onunknown words yielding 24% error rate.
The bestresult known to us is achieved by Toutanova[2002]by enriching the feature representation of theMaxEnt approach [Ratnaparkhi, 1996].2 A DBN for PoS TaggingUnlike Toutanova[2002], we deliberately baseour model on the original feature set of Ratna-parkhi?s MaxEnt.
Our Bayesian network includes aset of binary features (1-3, below) and a set of vo-cabulary features (4-6, below).
The binary featuresindicate the presence or absence of a particularcharacter in the token: 1. does the token contain acapital letter; 2. does the token contain a hyphen;3. does the token contain a number.
We used Rat-naparkhi?s vocabulary lists to encode the values of6458 frequent Words, 3602 Prefixes and 2925Suffixes up to 4 letters long.A Dynamic Bayesian network (DBN) is a Bayes-ian network unwrapped in time, such that it canrepresent dependencies between variables at adja-cent positions (see figure).
For a good overview ofDBNs, see Murphy [2002].
The set of observablevariables in our network consists of the binary andvocabulary features mentioned above.
In addition,there are two hidden variables: PoS and Memorywhich reflects contextual information about pastPoS tags.
Unlike Ratnaparkhi we do not directlyconsider any information about preceding wordseven the previous one [Toutanova 2002].
However,a special value of Memory indicates whether weare at the beginning of the sentence.Learning in our model is equivalent to collect-ing statistics over co-occurrences of feature valuesand tags.
This is implemented in GAWK scriptsand takes minutes on the WSJ training corpus.Compare this to laborious Improved Iterative Scal-ing for MaxEnt.
Tagging is carried out by the stan-dard Forward-Backward algorithm (see  e.g.Murphy[2002]).
We do not need to use specializedsearch algorithms such as Ratnaparkhi?s  "beamsearch".
In addition, our method does not require a"Development" stage.Following established data split we use sections(0-22) of WSJ for training and the rest (23-24) as atest set.
The test sections contain 4792 sentencesout of about 55600 total sentences in WSJ corpus.The average length of a sentence is 23 tokens.
Inaddition, we created  two specialized testing cor-pora (available upon request for comparison pur-poses).
A small Email corpus was prepared fromexcerpts from the MUC seminar announcementcorpus.
?The Jabberwocky?
is a poem by LouisCarol where the majority of words are made-up,but their PoS tags are apparent to speakers of Eng-lish.
We use ?The Jabberwocky?
to illustrate per-formance on unknown words.
Both the Emailcorpus and the Jabberwocky were pre-tagged bythe Brill tagger then manually corrected.We began our experiments by using the originalset of features and vocabulary lists of Ratnaparkhifor the variables Word, Prefix and Suffix.
Thisproduced a reasonable performance.
While investi-gating the relative contribution of each feature inthis setting, we discovered that the removal of thethree binary features from the feature set does notsignificantly alter performance.
Upon close exami-nation, the vocabularies we used turned out to con-tain a lot of redundant information that is otherwisehandled by these features.
For example, Prefix listcontained 84 hyphens (e.g.
both ?co-?
and ?co?
),530 numbers and 150 capitalised words, includingcapital letters.
We proceed, using reduced vocabu-laries obtained by removing redundant informationfrom the original lists.
The results are presented inTable 1 for various testing conditions.
Since Tou-tanova[2002] report that Prefix information wors-ens performance, we conducted the second set ofexperiments with a network that contained no in-formation about prefix.
We found no significantchange in performance.Our overall performance is comparable to thebest result known on this benchmark (e.g.
Tou-tanova[2002].
At the same time, our performanceon OoV words is significantly better (9.4% versus13.3%).
We attribute this difference to the purerrepresentation of morphologically relevant suffixesin our factored vocabulary, which excludes redun-dant and therefore potentially confusing informa-tion.
Another reason may be that our method puts agreater emphasis on the syntactically relevantfacts, such as morphology and tag sequence infor-mation by refraining to use word-specific cues.Despite our good performance on the WSJ corpus,we failed to improve Brill?s tagging on our twospecialized corpora.
Both Brill and our methodachieved 89% on the Jabberwocky poem.
Note,however, that Brill uses much more sophisticatedmechanisms to obtain this result.
It was particu-larly disappointing for us to find out that we didnot succeed in labeling the Email corpus accurately(16.3% versus 14.9% of Brill).
However, the rea-son for this poor performance appears to be partlyrelated to a labeling convention of the Penn Tree-bank, which essentially causes most capitalizedwords to be categorized as NNPs.
In our view,there is a significant difference between the gram-matical status of a proper name  ?VirginiaSavova?, where words can?t be said to modify oneanother, and a name of an institution such as ?De-partment of Chemical Engineering?, where?chemical?
clearly modifies ?engineering?.
Whilea rule-based system profits from this simplisticconvention, our method is harmed by it.3 ConclusionOur approach shows promise as it is both prob-abilistic and outperforms existing statistical taggerson unknown words.
We are especially encouragedby our performance on the WSJ and take this asevidence that our method has the potential to sig-nificantly improve PoS tagging of non-standardtexts.
In addition, our method has the advantage ofbeing conceptually simple, fast, and flexible withrespect to feature representation.
We are currentlyinvestigating the performance of other DBN to-pologies on PoS tagging.ReferencesBrants, T. 2000.
TnT -- a statistical part-of-speechtagger.
In Proceedings of the 6th ANLP.Brill.
E. 1995.
Transformation-based error-drivenlearning and natural language processing.
Computa-tional Linguistics, 21(4):543--565.Charniak, E., C. Hendrickson, N. Jacobson, and M.Perkowitz.
1993.
Equations for part-of-speech tag-ging.
In Proceedings of 11th AAAI, 1993Jelinek.
F. 1985.
Markov source modeling of text gen-eration.
In J. K. Skwirzinski, ed., Impact of Process-ing Techniques on Communication, DordechtKupiec.
J. M. 1992, Robust part-of-speech tagging us-ing a hidden Markov model.
Computer Speech andLanguage, 6:225-242.Lafferty J., McCallum A., Pereira F., Conditional Ran-dom Fields: Probabilistic Models for Segmentingand Labeling Sequence Data, Proc.
18th ICML, 2002Marcus M., G. Kim, M. Marcinkiewicz, R. MacIntyre,A.
Bies, M. Ferguson, K. Katz, and B. Schasberger.1994.
The Penn Treebank: Annotating predicate ar-gument structure.
In ARPA Human Language Tech-nology Workshop.Manning C. and H. Schutze.
1999.
Foundations of Sta-tistical Natural Language Processing.
The MIT Press.Cambridge, Massachusetts.Murphy.
K. Dynamic Bayesian Networks: Representa-tion, Inference and Learning.
PhD thesis.
UC Berke-ley.
2002.Peshkin.
L., 2003.
Part-of-Speech Tagging with Dy-namical Bayesian Network.
manuscriptPeshkin.
L., Pfeffer, A.
2003.
Bayesian InformationExtraction Network, manuscript, 2003Ratnaparkhi.
A  A maximum entropy model for part-of-speech tagging.
In Proceedings of EMNLP, 1996.Samuelsson.
C. Morphological Tagging Based Entirelyon Bayesian Inference.
In 9th Nordic Conference onComputational Linguistics, Stockholm University,Stockholm, Sweden.
1993.Toutanova K and Manning, C. Enriching the Knowl-edge Sources Used in a Maximum Entropy PoS Tag-ger.
2002.Description Average %% OoV WordsSentenceOriginal featureset of Ratnaparkhi 6.8 13.2 69.4Email corpus 16.3 12.2 79.0Jabberwocky 11.0 23.0 65.0Trained on WSJtested on Brown 13.1 26.5 73.2Factored featureSet on random WSJ 3.6 9.8 52.7Factored featureset on WSJ 23-24 3.6 9.4 51.7
