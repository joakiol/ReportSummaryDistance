Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 38?45,Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational LinguisticsFast and simple semantic class assignment for biomedical textK.
Bretonnel CohenComputational Bioscience ProgramU.
Colorado School of MedicineandDepartment of LinguisticsU.
of Colorado at Boulderkevin.cohen@gmail.comTom ChristiansenComput.
Bioscience Prog.U.
Colorado Sch.
of Medicinetchrist@perl.comWilliam A. Baumgartner Jr.Computational Bioscience ProgramU.
Colorado School of Medicinewilliam.baumgartner@ucdenver.eduKarin VerspoorComputational Bioscience ProgramU.
Colorado School of Medicinekarin.verspoor@ucdenver.eduLawrence E. HunterComputational Bioscience ProgramU.
Colorado School of Medicinelarry.hunter@ucdenver.eduAbstractA simple and accurate method for assigningbroad semantic classes to text strings is pre-sented.
The method is to map text stringsto terms in ontologies based on a pipeline ofexact matches, normalized strings, headwordmatching, and stemming headwords.
Theresults of three experiments evaluating thetechnique are given.
Five semantic classesare evaluated against the CRAFT corpus offull-text journal articles.
Twenty semanticclasses are evaluated against the correspond-ing full ontologies, i.e.
by reflexive match-ing.
One semantic class is evaluated againsta structured test suite.
Precision, recall,and F-measure on the corpus when evaluat-ing against only the ontologies in the cor-pus is micro-averaged 67.06/78.49/72.32 andmacro-averaged 69.84/83.12/75.31.
Accuracyon the corpus when evaluating against alltwenty semantic classes ranges from 77.12%to 95.73%.
Reflexive matching is generallysuccessful, but reveals a small number of er-rors in the implementation.
Evaluation withthe structured test suite reveals a number ofcharacteristics of the performance of the ap-proach.1 IntroductionBroad semantic class assignment is useful for anumber of language processing tasks, includingcoreference resolution (Hobbs, 1978), documentclassification (Caporaso et al, 2005), and informa-tion extraction (Baumgartner Jr. et al, 2008).
Alimited number of semantic classes have been stud-ied extensively, such as assigning text strings to thecategory gene or protein (Yeh et al, 2005;Smith et al, 2008), or the PERSON, ORGANI-ZATION, and LOCATION categories introduced inthe Message Understanding Conferences (Chinchor,1998).
A larger number of semantic classes have re-ceived smaller amounts of attention, e.g.
the classesin the GENIA ontology (Kim et al, 2004), vari-ous event types derived from the Gene Ontology(Kim et al, 2009), and diseases (Leaman and Gon-zalez, 2008).
However, many semantic types havenot been studied at all.
In addition, where ontolo-gies are concerned, although there has been workon finding mentions or evidence of specific terms intext (Blaschke et al, 2005; Stoica and Hearst, 2006;Davis et al, 2006; Shah et al, 2009), there has beenno work specifically addressing assigning multiplevery broad semantic classes with potential overlap.In particular, this paper examines the problem of tak-ing a set of ontologies and a text string (typically,but not necessarily, a noun phrase) as input and de-termining which ontology defines the semantic classthat that text string refers to.
We make an equiva-lence here between the notion of belonging to thedomain of an ontology and belonging to a specificsemantic class.
For example, if a string in text refersto something in the domain of the Gene Ontology,we take it as belonging to a Gene Ontology seman-tic class (using the name of the ontology only forconvenience); if a string in text refers to somethingbelonging to the domain of the Sequence Ontology,we take it as belonging to a Sequence Ontology se-mantic class.
We focus especially on rapid, simplemethods for making such a determination.The problem is most closely related to multi-class38classification, where in the case of this study we areincluding an unusually large number of categories,with possible overlap between them.
A text stringmight refer to something that legitimately belongsto the domain of more than one ontology.
For exam-ple, it might belong to the semantic classes of boththe Gene Ontology and the Gene Regulation Ontol-ogy; regulation is an important and frequent conceptin the Gene Ontology.
This fact has consequencesfor defining the notion of a false positive class as-signment; we return to this issue in the Results sec-tion.2 Methods2.1 Target semantic classesThe following ontologies were used to define se-mantic classes:?
Gene Ontology?
Sequence Ontology?
Foundational Model of Anatomy?
NCBI Taxonomy?
Chemical Entities of Biological Interest?
Phenotypic Quality?
BRENDA Tissue/Enzyme Source?
Cell Type Ontology?
Gene Regulation Ontology?
Homology Ontology?
Human Disease Ontology?
Human Phenotype Ontology?
Mammalian Phenotype Ontology?
Molecule Role Ontology?
Mouse Adult Gross Anatomy Ontology?
Mouse Pathology Ontology?
Protein Modification Ontology?
Protein-Protein Interaction Ontology?
Sample Processing and Separation TechniquesOntology?
Suggested Ontology for Pharmacogenomics2.2 Methodology for assigning semantic classWe applied four simple techniques for attempting tomatch a text string to an ontology.
They are arrangedin order of decreasing stringency.
That is, each sub-sequent method has looser requirements for a match.This both allows us to evaluate the contribution ofeach component more easily and, at run time, allowsthe user to set a stringency level, if the default is notdesired.2.2.1 Exact matchThe first and most stringent technique is exactmatch.
(This is essentially the only technique usedby the NCBO (National Center for Biomedical On-tology) Annotator (Jonquet et al, 2009), althoughit can also do substring matching.)
We normalizeterms in the ontology and text strings in the inputfor case and look for a match.2.2.2 StrippingAll non-alphanumeric characters, includingwhitespace, are deleted from the terms in theontology and from text strings in the input (e.g.cadmium-binding and cadmium binding bothbecome cadmiumbinding) and look for a match.2.2.3 Head nounsThis method involves a lightweight linguisticanalysis.
We traversed each ontology and deter-mined the head noun (see method below) of eachterm and synonym in the ontology.
We then pre-pared a dictionary mapping from head nouns to listsof ontologies in which those head nouns were found.Head nouns were determined by two simpleheuristics (cf.
(Collins, 1999)).
For terms fitting thepattern X of... (where of represents any preposi-tion) the term X was taken as the head noun.
Forall other terms, the rightmost word was taken as thehead noun.
These two heuristics were applied in se-quence when applicable, so that for example positiveregulation of growth (GO:0045927) becomes posi-tive regulation by application of the first heuristicand regulation by application of the second heuris-tic.
In the case of some ontologies, very limited pre-39processing was necessary?for example, it was nec-essary to delete double quotes that appeared aroundsynonyms, and in some ontologies we had to deletestrings like [EXACT SYNONYM] from some termsbefore extracting the head noun.2.2.4 Stemming head nounsIn this technique, the headwords obtained by theprevious step were stemmed with the Porter stem-mer.2.3 Corpus and other materialsWe made use of three sources in our evaluation.One is the CRAFT (Colorado Richly Annotated FullText) corpus (Verspoor et al, 2009; Cohen et al,2010a).
This is a collection of 97 full-text journalarticles, comprising about 597,000 words, each ofwhich has been used as evidence for at least one an-notation by the Mouse Genome Informatics group.It has been annotated with a number of ontologiesand database identifiers, including:?
Gene Ontology?
Sequence Ontology?
Cell Type Ontology?
NCBI Taxonomy?
Chemical Entities of Biological Interest(ChEBI)In total, there are over 119,783 annotations.
(Forthe breakdown across semantic categories, see Ta-ble 1.)
All of these annotations were done by biolog-ical scientists and have been double-annotated withinter-annotator agreement in the nineties for mostcategories.The second source is the full sets of terms fromthe twenty ontologies listed in the Introduction.
Allof the twenty ontologies that we used were obtainedfrom the OBO portal.
Version numbers are omittedhere due to space limitations, but are available fromthe authors on request.The third source is a structured test suite based onthe Gene Ontology (Cohen et al, 2010b).
Structuredtest suites are developed to test the performanceof a system on specific categories of input types.This test set was especially designed to test diffi-cult cases that do not correspond to exact matchesof Gene Ontology terms, as well as the full range oftypes of terms.
The test suite includes 300 conceptsfrom GO, as well as a number of transformations oftheir terms, such as cells migrated derived from theterm cell migration and migration of cells derivedfrom cell migration, classified according to a num-ber of linguistic attributes, such as length, whetheror not punctuation is included in the term, whetheror not it includes function (stop) words, etc.
Thistest suite determines at least one semantic categorythat should be returned for each term.
Unlike usingthe entire ontologies, this evaluation method madedetailed error analysis possible.
This test suite hasbeen used by other groups for broad characteriza-tions of successes and failures of concept recogniz-ers, and to tune the parameters of concept recogni-tion systems.2.4 EvaluationWe did three separate evaluations.
In one, we com-pared the output of our system against manually-generated gold-standard annotations in the CRAFTcorpus (op.
cit.).
This was possible only for the on-tologies that have been annotated in CRAFT, whichare listed above.In the second evaluation, we used the entire on-tologies themselves as inputs.
In this method, allresponses should be the same?for example, everyterm from the Gene Ontology should be classifiedas belonging to the GO semantic class.In the third, we utilized the structured test suitedescribed above.2.4.1 BaselinesTwo baselines are possible, but neither is optimal.The first would be to use MetaMap (Aronson, 2001),the industry standard for semantic category assign-ment.
(Note that MetaMap assigns specific cate-gories, not broad ones.)
However, MetaMap out-puts only semantic classes that are elements of theUMLS, which of the ontologies that we looked at,includes only the Gene Ontology.
The other is theNCBO Annotator.
The NCBO Annotator detectsonly exact matches (or substring matches) to ontol-ogy terms, so it is not clear that it is a strong enoughbaseline to allow for a stringent analysis of our ap-40proach.3 ResultsWe present our results in three sections:?
For the CRAFT corpus?
For the ontologies themselves?
For the Gene Ontology test suite3.1 Corpus resultsTable 1 (see next page) shows the results on theCRAFT corpus if only the five ontologies that wereactually annotated in CRAFT are used as inputs.The results are given for stemmed heads.
Perfor-mance on the four techniques that make up the ap-proach is cumulative, and results for stemmed headsreflects the application of all four techniques.
In thiscase, where we evaluate against the corpus, it is pos-sible to determine false positives, so we can giveprecision, recall, and F-measures for each semanticclass, as well as for the corpus as a whole.
Micro-averaged results were 67.06 precision, 78.49 recall,and 72.32 F-measure.
Macro-averaged results were69.84 precision, 83.12 recall, and 75.31 F-measure.Table 2 (see next page) shows the results forthe CRAFT corpus when all twenty ontologies arematched against the corpus data, including the manyontologies that are not annotated in the data.
Wegive results for just the five annotated ontologiesbelow.
Rather than calculating precision, recall,and F-measure, we calculate only accuracy.
Thisis because when classes other than the gold stan-dard class is returned, we have no way of know-ing if they are incorrect without manually examin-ing them?that is, we have no way to identify falsepositives.
If the set of classes returned included thegold standard class, a correct answer was counted.
Ifthe classifier returned zero or more classes and noneof them was the gold standard, an incorrect answerwas counted.
Results are given separately for eachof the four techniques.
This allows us to evaluatethe contribution of each technique to the overall re-sults; the value in each column is cumulative, so thevalue for Stemmed head includes the contribution ofall four of the techniques that make up the generalapproach.
Accuracies of 77.12% to 95.73% wereachieved, depending on the ontology.
We see thatthe linguistic technique of locating the head nounmakes a contribution to all categories, but makes anespecially strong contribution to the Gene Ontologyand Cell Type Ontology classes.
Stemming of head-words is also effective for all five categories.
We seethat exact match is effective only for those semanticclasses for which terminology is relatively fixed, i.e.the NCBI taxonomy and chemical names.
In someof the others, matching natural language text is verydifficult by any technique.
For example, of the 8,665Sequence Ontology false negatives in the data re-flected in the P/R/F values in Table 1, a full 2,050are due to the single character +, which does notappear in any of the twenty ontologies that we ex-amined and that was marked by the annotators as aSequence Ontology term, wild type (SO:0000817).3.2 Ontology resultsAs the second form of evaluation, we used theterms from the ontologies themselves as the inputsto which we attempted to assign a semantic class.
Inthis case, no annotation is required, and it is straight-forwardly the case that each term in a given ontologyshould be assigned the semantic class of that ontol-ogy.
We used only the head noun technique.
We didnot use the exact match or stripping heuristics, sincethey are guaranteed to return the correct answer, nordid we use stemming.
Thus, this section of the eval-uation gives us a good indication of the performanceof the head noun approach.As might be expected, almost all twenty on-tologies returned results in the 97-100% correctrate.
However, we noted much lower performancein two ontologies, the Sequence Ontology and theMolecule Role Ontology.
This lower performancereflects a number of preprocessing errors or omis-sions.
The fact that we were able to detect these low-performing ontologies indicates that our evaluationtechnique in this experiment?trying to match termsfrom an ontology against that ontology itself?is arobust evaluation technique and should be used insimilar studies.3.2.1 Structured test suite resultsThe third approach to evaluation involved use ofthe structured test suite.
The structured test suite re-vealed a number of trends in the performance of thesystem.41Ontology Annotations Precision Recall F-measureGene Ontology 39,626 66.31 73.06 69.52Sequence Ontology 40,692 63.00 72.21 67.29Cell Type Ontology 8,383 53.58 87.27 66.40NCBI Taxonomy 11,775 96.24 92.51 94.34ChEBI 19,307 70.07 90.53 79.00Total (micro-averaged) 119,783 67.06 78.49 72.32Total (macro-averaged) 69.84 83.12 75.31Table 1: Results on the CRAFT corpus when only the CRAFT ontologies are used as input.
Results are for stemmedheads.
Precision, recall, and F-measure are given for each semantic category in the corpus.
Totals are micro-averaged(over all tokens) and macro-averaged (over all categories), respectively.
P/R/F are cumulative, so that the results forstemmed heads reflect the application of all four techniques.Ontology Exact Stripped Head noun Stemmed headGene Ontology 24.26 24.68 59.18 77.12Sequence Ontology 44.28 47.63 56.63 73.33Cell Type Ontology 25.26 25.80 70.09 88.38NCBI Taxonomy 84.67 84.71 90.97 95.73ChEBI 86.93 87.44 92.43 95.49Table 2: Results on the CRAFT corpus when all twenty ontologies are used as input.
Accuracy is given for eachtechnique.
Accuracy is cumulative, so that accuracy in the final column reflects the application of all four techniques.?
The headword technique works very well forrecognizing syntactic variants.
For example, ifthe GO term induction of apoptosis is writtenas apoptosis induction, the headword techniqueallows it to be picked up.?
The headword technique works in situationswhere text has been inserted into a term.
Forexample, if the GO term ensheathment of neu-rons appears as ensheathment of some neu-rons, the headword technique will allow it to bepicked up.
If the GO term regulation of growthshows up as regulation of vascular growth, theheadword technique will allow it to be pickedup.?
The headword stemming technique allows us topick up many verb phrases, which is importantfor event detection and event coreference.
Forexample, if the GO term cell migration appearsin text as cells migrate, the technique will de-tect it.
The test suite also showed that failuresto recognize verb phrases still occur when themorphological relationship between the nomi-nal term and the verb are irregular, as for exam-ple between the GO term growth and the verbgrows.?
The technique?s ability to handle coordinationis very dependent on the type of coordination.For example, simple coordination (e.g.
cell mi-gration and proliferation) is handled well, butcomplex coordination (e.g.
cell migration, pro-liferation and adhesion) is handled poorly.?
Stemming is necessary for recognition of plu-rals, regardless of the length of the term inwords.?
The approach currently fails on irregular plu-rals, due to failure of the Porter stemmer to han-dle plurals like nuclei and nucleoli well.?
The approach handles classification of termsthat others have characterized as ?ungram-matical,?
such as transposition, DNA-mediated(GO:0006313).
This is important, because ex-act matches will always fail on these terms.424 Discussion4.1 Related workWe are not aware of similar work that tries to assigna large set of broad semantic categories to individ-ual text strings.
There is a body of work on selectinga single ontology for a domain or text.
(Mart?
?nez-Romero et al, 2010) proposes a method for selectingan ontology given a list of terms, all of which mustappear in the ontology.
(Jonquet et al, 2009) de-scribes an ontology recommender that first annotatesterms in a text with the Open Biomedical Annotatorservice, then uses the sum of the scores of the indi-vidual annotations to recommend a single ontologyfor the domain as a whole.4.2 Possible alternate approachesThree possible alternative approaches exist, all ofwhich would have as their goal the returning of a sin-gle best semantic class for every input.
However, forthe use cases that we have identified?coreferenceresolution, document classification, information ex-traction, and curator assistance?we are more inter-ested in wide coverage of a broad range of semanticclasses, so these approaches are not evaluated here.However, we describe them for completeness andfor the use of researchers who might be interestedin pursuing single-class assignment.4.2.1 Frequent wordsOne alternative approach would be to use simpleword frequencies.
For example, for each ontology,one could determine the N most frequent words, fil-tering out stop words.
At run time, check the wordsin each noun phrase in the text against the lists of fre-quent words.
For every word from the text that ap-peared in the list of frequent words from some ontol-ogy, assign a score to each ontology in which it wasfound, weighting it according to its position in thelist of frequent words.
In theory, this could accom-modate for the non-uniqueness of word-to-ontologymappings, i.e.
the fact that a single word might ap-pear in the lists for multiple ontologies.
However,we found the technique to perform very poorly fordifferentiating between ontologies and do not rec-ommend it.4.2.2 Measuring informativenessIf the system is desired to return only one sin-gle semantic class per text string, then one approachwould be to determine the informativeness of eachword in each ontology.
That is, we want to find themaximal probability of an ontology given a wordfrom that ontology.
This approach is very difficultto normalize for the wide variability in size of themany ontologies that we wanted to be able to dealwith.4.2.3 Combining scoresFinally, one could conceivably combine scores formatches obtained by the different strategies, weight-ing them according to their stringency, i.e.
exactmatch receiving a higher weight than head nounmatch, which in turn would receive a higher weightthan stemmed head noun match.
This weightingmight also include informativeness, as describedabove.4.3 Why the linguistic method worksAs pointed out above, the lightweight linguisticmethod makes a large contribution to the perfor-mance of the approach for some ontologies, partic-ularly those for which the exact match and strippingtechniques do not perform well.
It works for tworeasons, one related to the approach itself and onerelated to the nature of the OBO ontologies.
Froma methodological perspective, the approach is effec-tive because headwords are a good reflection of thesemantic content of the noun phrase and they arerelatively easy to access via simple heuristics.
Ofcourse simple heuristics will fail, as we can observemost obviously in the cases where we failed to iden-tify members of the ontologies in the second eval-uation step.
However, overall the approach workswell enough to constitute a viable tool for coref-erence systems and other applications that benefitfrom the ability to assign broad semantic classes totext strings.The approach is also able to succeed because ofthe nature of the OBO ontologies.
OBO ontologiesare meant to be orthogonal (Smith et al, 2007).
Adistributional analysis of the distribution of termsand words between the ontologies (data not shownhere, although some of it is discussed below), as wellas the false positives found in the corpus study, sug-43gests that orthogonality between the OBO ontolo-gies is by no means complete.
However, it holdsoften enough for the headword method to be effec-tive.4.4 Additional error analysisIn the section on the results for the structured testsuite, we give a number of observations on contribu-tions to errors, primarily related either to the char-acteristics of individual words or to particular syn-tactic instantiations of terms.
Here, we discuss someaspects of the distribution of lexical items and of thecorpus that contributed to errors.?
The ten most common headwords appear infrom 6-16 of the twenty ontologies.
However,they typically appear in one ontology at a fre-quency many orders of magnitude greater thantheir frequency in the other ontologies.
Takingthis frequency data into account for just theseten headwords would likely decrease false pos-itives quite significantly.?
More than 50% of Gene Ontology terms shareone of only ten headwords.
Many of our GeneOntology false negatives on the corpus are be-cause the annotated text string does not containa word such as process or complex that is thehead word of the canonical term.4.5 Future workThe heuristics that we implemented for extractingheadwords from OBO terms were very simple, inkeeping with our initial goal of developing an easy,fast method for semantic class assignment.
How-ever, it is clear that we could achieve substantial per-formance improvements from improving the heuris-tics.
We may pursue this track, if it becomes clearthat coreference performance would benefit fromthis when we incorporate the semantic classificationapproach into a coreference system.On acceptance of the paper, we will make Perl andJava versions of the semantic class assigner publiclyavailable on SourceForge.4.6 ConclusionThe goal of this paper was to develop a simple ap-proach to assigning text strings to an unprecedent-edly large range of semantic classes, where mem-bership in a semantic class is equated with belongingto the semantic domain of a specific ontology.
Theapproach described in this paper is able to do thatat a micro-averaged F-measure of 72.32 and macro-averaged F-measure of 75.31 as evaluated on a man-ually annotated corpus where false positives can bedetermined, and with an accuracy of 77.12-95.73%when only true positives and false negatives can bedetermined.ReferencesA.
Aronson.
2001.
Effective mapping of biomedical textto the UMLS Metathesaurus: The MetaMap program.In Proc AMIA 2001, pages 17?21.William A. Baumgartner Jr., Zhiyong Lu, Helen L. John-son, J. Gregory Caporaso, Jesse Paquette, Anna Linde-mann, Elizabeth K. White, Olga Medvedeva, K. Bre-tonnel Cohen, and Lawrence Hunter.
2008.
Conceptrecognition for extracting protein interaction relationsfrom biomedical text.
Genome Biology, 9.Christian Blaschke, Eduardo A. Leon, Martin Krallinger,and Alfonso Valencia.
2005.
Evaluation of BioCre-ative assessment of task 2.
BMC Bioinformatics, 6Suppl 1.J.
Gregory Caporaso, William A. Baumgartner Jr..,K. Bretonnel Cohen, Helen L. Johnson, Jesse Paque-tte, and Lawrence Hunter.
2005.
Concept recognitionand the TREC Genomics tasks.
In The Fourteenth TextREtrieval Conference (TREC 2005) Proceedings.Nancy A. Chinchor.
1998.
Overview of MUC-7/MET-2.K.
Bretonnel Cohen, Helen L. Johnson, Karin Verspoor,Christophe Roeder, and Lawrence E. Hunter.
2010a.The structural and content aspects of abstracts versusbodies of full text journal articles are different.
BMCBioinformatics, 11(492).K.
Bretonnel Cohen, Christophe Roeder, WilliamA.
Baumgartner Jr., Lawrence Hunter, and Karin Ver-spoor.
2010b.
Test suite design for biomedical ontol-ogy concept recognition systems.
In Proceedings ofthe Language Resources and Evaluation Conference.Michael Collins.
1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Universityof Pennsylvania.N.
Davis, H. Harkema, R. Gaizauskas, Y. K. Guo,M.
Ghanem, T. Barnwell, Y. Guo, and J. Ratcliffe.2006.
Three approaches to GO-tagging biomedicalabstracts.
In Proceedings of the Second InternationalSymposium on Semantic Mining in Biomedicine, pages21?28, Jena, Germany.Jerry R. Hobbs.
1978.
Resolving pronoun references.Lingua, 44:311?338.44C.
Jonquet, N.H. Shah, and M.A.
Musen.
2009.
Pro-totyping a biomedical ontology recommender ser-vice.
In Bio-Ontologies: Knowledge in Biology,ISMB/ECCB SIG.Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,Yuka Tateisi, and Nigel Collier.
2004.
Introductionto the bio-entity recognition task at JNLPBA.
In Pro-ceedings of the international joint workshop on natu-ral language processing in biomedicine and its appli-cations, pages 70?75.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 shared task on event extraction.
InBioNLP 2009 Companion Volume: Shared Task on En-tity Extraction, pages 1?9.Robert Leaman and Graciela Gonzalez.
2008.
BAN-NER: An executable survey of advances in biomedicalnamed entity recognition.
In Pac Symp Biocomput.Marcos Mart?
?nez-Romero, Jose?
Va?zquez-Naya, Cris-tian R. Munteanu, Javier Pereira, and Alejandro Pazos.2010.
An approach for the automatic recommendationof ontologies using collaborative knowledge.
In KES2010, Part II, LNAI 6277, pages 74?81.Nigam H. Shah, Nipun Bhatia, Clement Jonquet, DanielRubin, Annie P. Chiang, and Mark A. Musen.
2009.Comparison of concept recognizers for building theOpen Biomedical Annotator.
BMC Bioinformatics,10.Barry Smith, Michael Ashburner, Cornelius Rosse,Jonathan Bard, William Bug, Werner Ceusters,Louis J. Goldberg, Karen Eilbeck, Amelia Ireland,Christopher J. Mungall, The OBI Consortium, Neo-cles Leontis, Philippe Rocca-Serra, Alan Ruttenberg,Susanna-Assunta Sansone, Richard H. Scheuermann,Nigam Shah, Patricia L. Whetzel, and Suzanna Lewis.2007.
The OBO Foundry: coordinated evolution ofontologies to support biomedical data integration.
Na-ture Biotechnology, 25:1251?1255.Larry Smith, Lorraine Tanabe, Rie Johnson nee Ando,Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-Shi Lin, Roman Klinger, Christof Friedrich, KuzmanGanchev, Manabu Torii, Hongfang Liu, Barry Had-dow, Craig Struble, Richard Povinelli, Andreas Vla-chos, William Baumgartner, Jr., Lawrence Hunter,Bob Carpenter, Richard Tzong-Han Tsai, Hong-Jie Dai, Feng Liu, Yifei Chen, Chengjie Sun,Sophia Katrenko, Pieter Adriaans, Christian Blaschke,Rafael Torres Perez, Mariana Neves, Preslav Nakov,Anna Divoli, Manuel Mana, Jacinto Mata-Vazquez,and W. John Wilbur.
2008.
Overview of BioCreativeII gene mention recognition.
Genome Biology.E.
Stoica and M. Hearst.
2006.
Predicting gene functionsfrom text using a cross-species approach.
In Proceed-ings of the 11th Pacific Symposium on Biocomputing.Karin Verspoor, K. Bretonnel Cohen, and LawrenceHunter.
2009.
The textual characteristics of traditionaland Open Access scientific journals are similar.
BMCBioinformatics, 10.A.
Yeh, A. Morgan, M. Colosimo, and L. Hirschman.2005.
BioCreatve task 1A: gene mention finding eval-uation.
BMC Bioinformatics, 6(Suppl.
1).45
