Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 757?766,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsGeneration of landmark-based navigation instructionsfrom open-source dataMarkus Dra?gerDept.
of Computational LinguisticsSaarland Universitymdraeger@coli.uni-saarland.deAlexander KollerDept.
of LinguisticsUniversity of Potsdamkoller@ling.uni-potsdam.deAbstractWe present a system for the real-time gen-eration of car navigation instructions withlandmarks.
Our system relies exclusivelyon freely available map data from Open-StreetMap, organizes its output to fit intothe available time until the next driving ma-neuver, and reacts in real time to driving er-rors.
We show that female users spend sig-nificantly less time looking away from theroad when using our system compared to abaseline system.1 IntroductionSystems that generate route instructions are be-coming an increasingly interesting applicationarea for natural language generation (NLG) sys-tems.
Car navigation systems are ubiquitousalready, and with the increased availability ofpowerful mobile devices, the wide-spread use ofpedestrian navigation systems is on the horizon.One area in which NLG systems could improveexisting navigation systems is in the use of land-marks, which would enable them to generate in-structions such as ?turn right after the church?
in-stead of ?after 300 meters?.
It has been shown inhuman-human studies that landmark-based routeinstructions are easier to understand (Lovelaceet al 1999) than distance-based ones and re-duce driver distraction in in-car settings (Bur-nett, 2000), which is crucial for improved trafficsafety (Stutts et al 2001).
From an NLG per-spective, navigation systems are an obvious ap-plication area for situated generation, for whichthere has recently been increasing interest (seee.g.
(Lessmann et al 2006; Koller et al 2010;Striegnitz and Majda, 2009)).Current commercial navigation systems useonly trivial NLG technology, and in particular arelimited to distance-based route instructions.
Evenin academic research, there has been remarkablylittle work on NLG for landmark-based naviga-tion systems.
Some of these systems rely on mapresources that have been hand-crafted for a par-ticular city (Malaka et al 2004), or on a com-bination of multiple complex resources (Raubaland Winter, 2002), which effectively limits theircoverage.
Others, such as Dale et al(2003), fo-cus on non-interactive one-shot instruction dis-courses.
However, commercially successful carnavigation systems continuously monitor whetherthe driver is following the instructions and pro-vide modified instructions in real time when nec-essary.
That is, two key problems in designingNLG systems for car navigation instructions arethe availability of suitable map resources and theability of the NLG system to generate instructionsand react to driving errors in real time.In this paper, we explore solutions to both ofthese points.
We present the Virtual Co-Pilot,a system which generates route instructions forcar navigation using landmarks that are extractedfrom the open-source OpenStreetMap resource.1The system computes a route plan and splits itinto episodes that end in driving maneuvers.
Itthen selects landmarks that describe the locationsof these driving maneuvers, and aggregates in-structions such that they can be presented (viaa TTS system) in the time available within theepisode.
The system monitors the user?s positionand computes new, corrective instructions whenthe user leaves the intended path.
We evaluateour system using a driving simulator, and com-pare it to a baseline that is designed to replicatea typical commercial navigation system.
The Vir-tual Co-Pilot performs comparably to the baseline1http://www.openstreetmap.org/757on the number of driving errors and on user sat-isfaction, and outperforms it significantly on thetime female users spend looking away from theroad.
To our knowledge, this is the first time thatthe generation of landmarks has been shown tosignificantly improve the instructions of a wide-coverage navigation system.Plan of the paper.
We start by reviewing ear-lier literature on landmarks, route instructions,and the use of NLG for route instructions in Sec-tion 2.
We then present the way in which weextract information on potential landmarks fromOpenStreetMap in Section 3.
Section 4 showshow we generate route instructions, and Section 5presents the evaluation.
Section 6 concludes.2 Related WorkWhat makes an object in the environment a goodlandmark has been the topic of research in vari-ous disciplines, including cognitive science, com-puter science, and urban planning.
Lynch (1960)defines landmarks as physical entities that serveas external points of reference that stand out fromtheir surroundings.
Kaplan (1976) specified alandmark as ?a known place for which the in-dividual has a well-formed representation?.
Al-though there are different definitions of land-marks, a common theme is that objects are con-sidered landmarks if they have some kind of cog-nitive salience (both in terms of visual distinctive-ness and frequeny of interaction).The usefulness of landmarks in route instruc-tions has been shown in a number of differenthuman-human studies.
Experimental results fromLovelace et al(1999) show that people not onlyuse landmarks intuitively when giving directions,but they also perceive instructions that are given tothem to be of higher quality when those instruc-tions contain landmark information.
Similar find-ings have also been reported by Michon and Denis(2001) and Tom and Denis (2003).Regarding car navigation systems specifically,Burnett (2000) reports on a road-based user studywhich compared a landmark-based navigationsystem to a conventional car navigation system.Here the provision of landmark information inroute directions led to a decrease of navigationalerrors.
Furthermore, glances at the navigationdisplay were shorter and fewer, which indicatesless driver distraction in this particular experi-mental condition.
Minimizing driver distractionis a crucial goal of improved navigation systems,as driver inattention of various kinds is a lead-ing cause of traffic accidents (25% of all police-reported car crashes in the US in 2000, accordingto Stutts et al(2001)).
Another road-based studyconducted by May and Ross (2006) yielded simi-lar results.One recurring finding in studies on landmarksin navigation is that some user groups are ableto benefit more from their inclusion than oth-ers.
This is particularly the case for female users.While men tend to outperform women in wayfind-ing tasks, completing them faster and with fewernavigation errors (c.f.
Allen (2000)), women arelikely to show improved wayfinding performancewhen landmark information is given (e.g.
Saucieret al(2002)).Despite all of this evidence from human-humanstudies, there has been remarkably little researchon implemented navigation systems that use land-marks.
Commercial systems make virtually nouse of landmark information when giving direc-tions, relying on metric representations instead(e.g.
?Turn right in one hundred meters?).
In aca-demic research, there have only been a handful ofrelevant systems.
A notable example is the DEEPMAP system, which was created in the SmartKomproject as a mobile tourist information system forthe city of Heidelberg (Malaka and Zipf, 2000;Malaka et al 2004).
DEEP MAP uses landmarksas waypoints for the planning of touristic routesfor car drivers and pedestrians, while also makinguse of landmark information in the generation ofroute directions.
Raubal and Winter (2002) com-bine data from digital city maps, facade images,cultural heritage information, and other sourcesto compute landmark descriptions that could beused in a pedestrian navigation system for the cityof Vienna.The key to the richness of these systems is aset of extensive, manually curated geographic andlandmark databases.
However, creation and main-tenance of such databases is expensive, whichmakes it impractical to use these systems outsideof the limited environments for which they werecreated.
There have been a number of suggestionsfor automatically acquiring landmark data fromexisting electronic databases, for instance cadas-tral data (Elias, 2003) and airborne laser scans(Brenner and Elias, 2003).
But the raw data forthese approaches is still hard to obtain; informa-758tion about landmarks is mostly limited to geomet-ric data and does not specify the semantic typeof a landmark (such as ?church?
); and updatingthe landmark database frequently when the realworld changes (e.g., a shop closes down) remainsan open issue.The closest system in the literature to the re-search we present here is the CORAL system(Dale et al 2003).
CORAL generates a text ofdriving instructions with landmarks out of the out-put of a commercial web-based route planner.
Un-like CORAL, our system relies purely on open-source map data.
Also, our system generates driv-ing instructions in real time (as opposed to a sin-gle discourse before the user starts driving) andreacts in real time to driving errors.
Finally, weevaluate our system thoroughly for driving errors,user satisfaction, and driver distraction on an ac-tual driving task, and find a significant improve-ment over the baseline.3 OpenStreetMapA system that generates landmark-based route di-rections requires two kinds of data.
First, it mustplan routes between points in space, and thereforeneeds data on the road network, i.e.
the road seg-ments that make up streets along with their con-nections.
Second, the system needs informationabout the landmarks that are present in the envi-ronment.
This includes geographic informationsuch as position, but also semantic informationsuch as the landmark type.We have argued above that the availability ofsuch data has been a major bottleneck in thedevelopment of landmark-based navigation sys-tems.
In the Virtual Co-Pilot system, whichwe present below, we solve this problem by us-ing data from OpenStreetMap, an on-line mapresource that provides both types of informa-tion mentioned above, in a unified data struc-ture.
The OpenStreetMap project is to maps whatWikipedia is to encyclopedias: It is a map ofthe entire world which can be edited by anyonewishing to participate.
New map data is usuallyadded by volunteers who measure streets usingGPS devices and annotate them via a Web inter-face.
The decentralized nature of the data entryprocess means that when the world changes, themap will be updated quickly.
Existing map datacan be viewed as a zoomable map on the Open-StreetMap website, or it can be downloaded in anFigure 1: A graphical representation of some nodesand ways in OpenStreetMap.Landmark TypeStreet Furniture stop signtraffic lightspedestrian crossingVisual Landmarks churchcertain video storescertain supermarketsgas stationpubs and barsFigure 2: Landmarks used by the Virtual Co-Pilot.XML format for offline use.Geographical data in OpenStreetMap is repre-sented in terms of nodes and ways.
Nodes rep-resent points in space, defined by their latitudeand longitude.
Ways consist of sequences ofedges between adjacent nodes; we call the in-dividual edges segments below.
They are usedto represent streets (with curved streets consist-ing of multiple straight segments approximatingtheir shape), but also a variety of other real-worldentities: buildings, rivers, trees, etc.
Nodes andways can both be enriched with further infor-mation by attaching tags.
Tags encode a widerange of additional information using a predefinedtype ontology.
Among other things, they specifythe types of buildings (church, cafe, supermarket,etc.
); where a shop or restaurant has a name, it toois specified in a tag.
Fig.
1 is a graphical represen-tation of some OpenStreetMap data, consisting ofnodes and ways for two streets (with two and fivesegments) and a building which has been taggedas a gas station.For the Virtual Co-Pilot system, we have cho-sen a set of concrete landmark types that we con-sider useful (Fig.
2).
We operationalize the crite-ria for good landmarks sketched in Section 2 byrequiring that a landmark should be easily visible,and that it should be generic in that it is appli-759cable not just for one particular city, but for anyplace for which OpenStreetMap data is available.We end up with two classes of landmark types:street furniture and visual landmarks.
Street fur-niture is a generic term for objects that are in-stalled on streets.
In this subset, we include stopsigns, traffic lights, and pedestrian crossings.
Ourassumption is that these objects inherently pos-sess a high salience, since they already requireparticular attention from the driver.
?Visual land-marks?
encompass roadside buildings that are notdirectly connected to the road infrastructure, butdraw the driver?s attention due to visual salience.Churches are an obvious member of this group; inaddition, we include gas stations, pubs, and bars,as well as certain supermarket and video storechains (selected for wide distribution over differ-ent cities and recognizable, colorful signs).Given a certain location at which the VirtualCo-Pilot is to be used, we automatically extractsuitable landmarks along with their types and lo-cations from OpenStreetMap.
We also gatherthe road network information that is requiredfor route planning, and collect informations onstreets, such as their names, from the tags.
Wethen transform this information into a directedstreet graph.
The nodes of this graph are theOpenStreetMap nodes that are part of streets; twoadjacent nodes are connected by a single directededge for segments of one-way streets and a di-rected edge in each direction for ordinary streetsegments.
Each edge is weighted with the Eu-clidean distance between the two nodes.4 Generation of route directionsWe will now describe how the Virtual Co-Pilotgenerates route directions from OpenStreetMapdata.
The system generates three types of mes-sages (see Fig.
3).
First, at every decision point,i.e.
at the intersection where a driving maneu-ver such as turning left or right is required, theuser is told to turn immediately in the given di-rection (?now turn right?).
Second, if the driverhas followed an instruction correctly, we gener-ate a confirmation message after the driver hasmade the turn, letting them know they are stillon the right track.
Finally, we generate previewmessages on the street leading up to the decisionpoint.
These preview messages describe the loca-tion of the next driving maneuver.Of the three types, preview messages are theFigure 3: Schematic representation of an episode(dashed red line), with sample trigger positions of pre-view, turn instruction, and confirmation messages.most interesting.
Our system avoids the genera-tion of metric distance indicators, as in ?turn leftin 100 meters?.
Instead, it tries to find landmarksthat describe the position of the decision point:?Prepare to turn left after the church.?
When nolandmark is available, the system tries to use streetintersections as secondary landmarks, as in ?Turnright at the next/second/third intersection.?
Metricdistances are only used when both of these strate-gies fail.In-car NLG takes place in a heavily real-timesetting, in which an utterance becomes uninter-pretable or even misleading if it is given too late.This problem is exacerbated for NLG of speechbecause simply speaking the utterance takes timeas well.
One consequence that our system ad-dresses is the problem of planning preview mes-sages in such a way that they can be spoken be-fore the decision point without overlapping eachother.
We handle this problem in the sentenceplanner, which may aggregate utterances to fitinto the available time.
A second problem is thatthe user?s reactions to the generated utterances areunpredictable; if the driver takes a wrong turn, thesystem must generate updated instructions in realtime.Below, we describe the individual componentsof the system.
We mostly follow a standard NLGpipeline (Reiter and Dale, 2000), with a focus onthe sentence planner and an extension to interac-tive real-time NLG.760Segment123From: Node1To: Node2On: ?Main Street?Segment124From: Node2To: Node3On: ?Main Street?Segment125From: Node3To: Node4On: ?Park Street?Segment126From: Node4To: Node5On: ?Park Street?Figure 4: A simple example of a route plan consistingof four street segments.4.1 Content determination and text planningThe first step in our system is to obtain a plan forreaching the destination.
To this end, we com-pute a shortest path on the directed street graphdescribed in Section 3.
The result is an orderedlist of street segments that need to be traversed inthe given order to successfully reach the destina-tion; see Fig.
4 for an example.To be suitable as the input for an NLG system,this flat list of OpenStreetMap nodes needs to besubdivided into smaller message chunks.
In turn-by-turn navigation, the general delimiter betweensuch chunks are the driving maneuvers that thedriver must execute at each decision point.
Wecall each span between two decision points anepisode.
Episodes are not explicitly representedin the original route plan: although every segmenthas a street name associated with it, the name ofa street sometimes changes as we go along, andbecause chains of segments are used to modelcurved streets in OpenStreetMap, even segmentsthat are joined at an angle may be parts of thesame street.
Thus, in Fig.
4 it is not apparentwhich segment traversals require any navigationalmaneuvers.We identify episode boundaries with the fol-lowing heuristic.
We first assume that episodeboundaries occur when the street name changesfrom one segment to the next.
However, stay-ing on the road may involve a driving maneu-ver (and therefore a decision point) as well, e.g.when the road makes a sharp turn where a minorstreet forks off.
To handle this case, we introducedecision points at nodes with multiple adjacentsegments if the angle between the incoming andoutgoing segment of the street exceeds a certainthreshold.
Conversely, our heuristic will some-times end an episode where no driving maneuveris necessary, e.g.
when an ongoing street changesits name.
This is unproblematic in practice; thesystem will simply generate an instruction to keepdriving straight ahead.
Fig.
3 shows a graphicalrepresentation of an episode, with the street seg-ments belonging to it drawn as red dashed lines.4.2 AggregationBecause we generate spoken instructions that aregiven to the user while they are driving, the timingof the instructions becomes a crucial issue, espe-cially because a driver moves faster than the userof a pedestrian navigation system.
It is undesir-able for a second instruction to interrupt an ear-lier one.
On the other hand, the second instruc-tion cannot be delayed because this might makethe user miss a turn or interpret the instruction in-correctly.We must therefore control at which points in-structions are given and make sure that they donot overlap.
We do this by always presenting pre-view messages at trigger positions at certain fixeddistances from the decision point.
The sentenceplanner calculates where these trigger positionsare located for each episode.
In this way, we cre-ate time frames during which there is enough timefor instructions to be presented.However, some episodes are too short to ac-commodate the three trigger positions for the con-firmation message and the two preview messages.In such episodes, we aggregate different mes-sages.
We remove the trigger positions for the twopreview messages from the episode, and insteadadd the first preview message to the turn instruc-tion message of the previous episode.
This allowsour system to generate instructions like ?Now turnright, and then turn left after the church.
?4.3 Generation of landmark descriptionsThe Virtual Co-Pilot computes referring expres-sions to decision points by selecting appropriatelandmarks.
To this end, it first looks up landmarkcandidates within a given range of the decisionpoint from the database created in Section 3.
This761yields an initial list of landmark candidates.Some of these landmark candidates may be un-suitable for the given situation because of lack ofuniqueness.
If there are several visual landmarksof the same type along the course of an episode,all of these landmark candidates are removed.
Forepisodes which contain multiple street furniturelandmarks of the same type, the first three in eachepisode are retained; a referring expression for thedecision point might then be ?at the second traf-fic light?.
If the decision point is no more thanthree intersections away, we also add a landmarkdescription of the form ?at the third intersection?.Furthermore, a landmark must be visible from thelast segment of the current episode; we only retaina candidate if it is either adjacent to a segment ofthe current episode or if it is close to the end pointof the very last segment of the episode.
Amongthe landmarks that are left over, the system prefersvisual landmarks over street furniture, and streetfurniture over intersections.
If no landmark candi-dates are left over, the system falls back to metricdistances.Second, the Virtual Co-Pilot determines thespatial relationship between the landmark and thedecision point so that an appropriate prepositioncan be used in the referring expression.
If the de-cision point occurs before the landmark along thecourse of the episode, we use the preposition ?infront of?, otherwise, we use ?after?.
Intersectionsare always used with ?at?
and metric distanceswith ?in?.Finally, the system decides how to refer to thelandmark objects themselves.
Although it has ac-cess to the names of all objects from the Open-StreetMap data, the user may not know thesenames.
We therefore refer to churches, gas sta-tions, and any street furniture simply as ?thechurch?, ?the gas station?, etc.
For supermar-kets and bars, we assume that these buildings aremore saliently referred to by their names, whichare used in everyday language, and therefore usethe names to refer to them.The result of the sentence planning stage isa list of semantic representations, specifying theindividual instructions that are to be uttered ineach episode; an example is shown in Fig.
5.For each type of instruction, we then use a sen-tence template to generate linguistic surface formsby inserting the information contained in thoseplans into the slots provided by the templates (e.g.Preview message p1:Trigger position: Node3 ?
50mTurn direction: rightLandmark: churchPreposition: afterPreview message p2 = p1, except:Trigger position: Node3 ?
100mTurn instruction t1:Trigger position: Node3Turn direction: rightConfirmation message c1:Trigger position: Node3 + 50mFigure 5: Semantic representations of the differenttypes of instructions in one episode.
?Turn direction preposition landmark?
).4.4 Interactive generationAs a final point, the NLG process of a car naviga-tion system takes place in an interactive setting:as the system generates and utters instructions, theuser may either follow them correctly, or they maymiss a turn or turn incorrectly because they mis-understood the instruction or were forced to disre-gard it by the traffic situation.
The system must beable to detect such problems, recover from them,and generate new instructions in real time.Our system receives a continuous stream of in-formation about the position and direction of theuser.
It performs execution monitoring to checkwhether the user is still following the intendedroute.
If a trigger position is reached, we presentthe instruction that we have generated for this po-sition.
If the user has left the route, the systemreacts by planning a new route starting from theuser?s current position and generating a new set ofinstructions.
We check whether the user is follow-ing the intended route in the following way.
Thesystem keeps track of the current episode of theroute plan, and monitors the distance of the carto the final node of the episode.
While the useris following the route correctly, the distance be-tween the car and the final node should decreaseor at least stay the same between two measure-ments.
To accommodate for occasional deviationsfrom the middle of the road, we allow five subse-quent measurements to increase the distance; thesixth increase of the distance triggers a recompu-tation of the route plan and a freshly generatedinstruction.
On the other hand, when the distance762of the car to the final node falls below a certainthreshold, we assume that the end of the episodehas been reached, and activate the next episode.By monitoring whether the user is now approach-ing the final node of this new episode, we can inparticular detect wrong turns at intersections.Because each instruction carries the risk that itmay not be followed correctly, there is a questionas to whether it is worth planning out all remain-ing instructions for the complete route plan.
Afterall, if the user does not follow the first instruc-tion, the computation of all remaining instructionswas a waste of time.
We decided to compute allfuture instructions anyway because the aggrega-tion procedure described above requires them.
Inpractice, the NLG process is so efficient that allinstructions can be done in real time, but this de-cision would have to be revisited for a slower sys-tem.5 EvaluationWe will now report on an experiment in which weevaluated the performance of the Virtual Co-Pilot.5.1 Experimental Method5.1.1 SubjectsIn total, 12 participants were recruited throughprinted ads and mailing lists.
All of them wereuniversity students aged between 21 and 27 years.Our experiment was balanced for gender, hencewe recruited 6 male and 6 female participants.
Allparticipants were compensated for their effort.5.1.2 DesignThe driving simulator used in the experimentreplicates a real-world city center using a 3Dmodel that contains buildings and streets as theycan be perceived in reality.
The street layout 3Dmodel used by the driving simulator is based onOpenStreetMap data, and buildings were added tothe virtual environment based on cadastral data.To increase the perceived realism of the model,some buildings were manually enhanced withphotographic images of their real-world counter-parts (see Fig.
7).Figure 6 shows the set-up of the evaluation ex-periment.
The virtual driving simulator environ-ment (main picture in Fig.
7) was presented to theparticipants on a 20?
computer screen (A).
In ad-dition, graphical navigation instructions (shownin the lower right of Fig.
7) were displayed onFigure 6: Experiment setup.
A) Main screen B) Navi-gation screen C) steering wheel D) eye trackera separate 7?
monitor (B).
The driving simula-tor was controlled by means of a steering wheel(C), along with a pair of brake and accelerationpedals.
We recorded user eye movements usinga Tobii IS-Z1 table-mounted eye tracker (D).
Thegenerated instructions were converted to speechusing MARY, an open-source text-to-speech sys-tem (Schro?der and Trouvain, 2003), and playedback on loudspeakers.The task of the user was to drive the car inthe virtual environment towards a given destina-tion; spoken instructions were presented to themas they were driving, in real time.
Using thesteering wheel and the pedals, users had full con-trol over steering angles, acceleration and brak-ing.
The driving speed was limited to 30 km/h, butthere were no restrictions otherwise.
The drivingsimulator sent the NLG system a message with thecurrent position of the car (as GPS coordinates)once per second.Each user was asked to drive three short routesin the driving simulator.
Each route took aboutfour minutes to complete, and the travelled dis-tance was about 1 km.
The number of episodesper route ranged from three to five.
Landmarkcandidates were sufficiently dense that the VirtualCo-Pilot used landmarks to refer to all decisionpoints and never had to fall back to the metric dis-tance strategy.There were three experimental conditions,which differed with respect to the spoken routeinstructions and the use of the navigation screen.In the baseline condition, designed to replicate thebehavior of an off-the-shelf commercial car nav-763All Users Males FemalesB VCP B VCP B VCPTotal Fixation Duration (seconds) 4.9 3.5 2.7 4.1 7.0 2.9*Total Fixation Count (N) 21.8 15.4 13.5 16.5 30.0 14.3*?The system provided the right amountof information at any time?3.9 2.9 4.2* 3.3 3.5 2.5?I was insecure at times about still be-ing on the right track.
?2.3 3.2 1.9* 2.8 2.6 3.5?It was important to have a visual rep-resentation of route directions?4.3 4.0 4.2 4.2 4.3 3.7?I could trust the navigation system?
3.6 3.7 4.1 3.7 3.0 3.7Figure 8: Mean values for gaze behavior and subjective evaluation, separated by user group and condition (B =baseline, VCP = our system).
Significant differences are indicated by *; better values are printed in boldface.Figure 7: Screenshot of a scene in the driving simula-tor.
Lower right corner: matching screenshot of navi-gation display.igation system, participants were provided withspoken metric distance-to-turn navigation instruc-tions.
The navigation screen showed arrows de-picting the direction of the next turn, along withthe distance to the decision point (cf.
Fig.
7).
Thesecond condition replaced the spoken route in-structions by those generated by the Virtual Co-Pilot.
In a third condition, the output of the nav-igation screen was further changed to display anicon for the next landmark along with the arrowand distance indicator.
The three routes were pre-sented to the users in different orders, and com-bined with the conditions in a Latin Squares de-sign.
In this paper, we focus on the first and sec-ond condition, in order to contrast the two stylesof spoken instruction.Participants were asked to answer two ques-tionnaires after each trial run.
The first was theDALI questionnaire (Pauzie?, 2008), which askssubjects to report how they perceived differentaspects of their cognitive workload (general, vi-sual, auditive and temporal workload, as well asperceived stress level).
In the second question-naire, participants were state to rate their agree-ment with a number of statements about their sub-jective impression of the system on a 5-point un-labelled Likert scale, e.g.
whether they had re-ceived instructions at the right time or whetherthey trusted the navigation system to give themthe right instructions during trials.5.2 ResultsThere were no significant differences between theVirtual Co-Pilot and the baseline system on taskcompletion time, rate of driving errors, or any ofthe questions of the DALI questionnaire.
Driv-ing errors in particular were very rare: there wereonly four driving errors in total, two of whichwere due to problems with left/right coordination.We then analyzed the gaze data collected by thetable-mounted eye tracker, which we set up suchthat it recognized glances at the navigation screen.In particular, we looked at the total fixation dura-tion (TFD), i.e.
the total amount of time that a userspent looking at the navigation screen during agiven trial run.
We also looked at the total fixationcount (TFC), i.e.
the total number of times that auser looked at the navigation screen in each run.Mean values for both metrics are given in Fig.
8,averaged over all subjects and only male and fe-male subjects, respectively; the ?VCP?
column isfor the Virtual Co-Pilot, whereas ?B?
stands forthe baseline.
We found that male users tendedto look more at the navigation screen in the VCPcondition than in B, although the difference is notstatistically significant.
However, female userslooked at the navigation screen significantly fewer764times (t(5) = 3.2, p < 0.05, t-test for dependentsamples) and for significantly shorter amounts oftime (t(5) = 3.2, p < 0.05) in the VCP conditionthan in B.On the subjective questionnaire, most questionsyielded no significant differences (and are not re-ported here).
However, we found that femaleusers tended to rate the Virtual Co-Pilot more pos-itively than the baseline on questions concerningtrust in the system and the need for the navigationscreen (but not significantly).
Male users foundthat the baseline significantly outperformed theVirtual Co-Pilot on presenting instructions at theright time (t(5) = 2.7, p < 0.05) and on givingthem a sense of security in still being on the righttrack (t(5) = ?2.7, p < 0.05).5.3 DiscussionThe most striking result of the evaluation is thatthere was a significant reduction of looks to thenavigation display, even if only for one groupof users.
Female users looked at the navigationscreen less and more rarely with the Virtual Co-Pilot compared to the baseline system.
In a realcar navigation system, this translates into a driverwho spends less time looking away from the road,i.e.
a reduction in driver distraction and an in-crease in traffic safety.
This suggests that femaleusers learned to trust the landmark-based instruc-tions, an interpretation that is further supportedby the trends we found in the subjective question-naire.We did not find these differences in the maleuser group.
Part of the reason may be the knowngender differences in landmark use we mentionedin Section 2.
But interestingly, the two signifi-cantly worse ratings by male users concerned thecorrect timing of instructions and the feedback fordriving errors, i.e.
issues regarding the system?sreal-time capabilities.
Although our system doesnot yet perform ideally on these measures, thisconfirms our initial hypothesis that the NLG sys-tem must track the user?s behavior and scheduleits utterances appropriately.
This means that ear-lier systems such as CORAL, which only com-pute a one-shot discourse of route instructionswithout regard to the timing of the presentation,miss a crucial part of the problem.Apart from the exceptions we just discussed,the landmark-based system tended to score com-parably or a bit worse than the baseline on theother subjective questions.
This may partly be dueto the fact that the subjects were familiar with ex-isting commercial car navigation systems and notused to landmark-based instructions.
On the otherhand, this finding is also consistent with resultsof other evaluations of NLG systems, in whichan improvement in the objective task usefulnessof the system does not necessarily correlate withimproved scores from subjective questionnaires(Gatt et al 2009).6 ConclusionIn this paper, we have described a system for gen-erating real-time car navigation instructions withlandmarks.
Our system is distinguished from ear-lier work in its reliance on open-source map datafrom OpenStreetMap, from which we extract boththe street graph and the potential landmarks.
Thisdemonstrates that open resources are now infor-mative enough for use in wide-coverage naviga-tion NLG systems.
The system then chooses ap-propriate landmarks at decision points, and con-tinuously monitors the driver?s behavior to pro-vide modified instructions in real time when driv-ing errors occur.We evaluated our system using a driving simu-lator with respect to driving errors, user satisfac-tion, and driver distraction.
To our knowledge,we have shown for the first time that a landmark-based car navigation system outperforms a base-line significantly; namely, in the amount of timefemale users spend looking away from the road.In many ways, the Virtual Co-Pilot is a verysimple system, which we see primarily as a start-ing point for future research.
The evaluationconfirmed the importance of interactive real-timeNLG for navigation, and we therefore see this asa key direction of future work.
On the other hand,it would be desirable to generate more complexreferring expressions (?the tall church?).
Thiswould require more informative map data, as wellas a formal model of visual salience (Kelleher andvan Genabith, 2004; Raubal and Winter, 2002).Acknowledgments.
We would like to thank theDFKI CARMINA group for providing the driv-ing simulator, as well as their support.
We wouldfurthermore like to thank the DFKI Agents andSimulated Reality group for providing the 3D citymodel.765ReferencesG.
L. Allen.
2000.
Principles and practices for com-municating route knowledge.
Applied CognitivePsychology, 14(4):333?359.C.
Brenner and B. Elias.
2003.
Extracting land-marks for car navigation systems using existinggis databases and laser scanning.
Internationalarchives of photogrammetry remote sensing andspatial information sciences, 34(3/W8):131?138.G.
Burnett.
2000.
?Turn right at the Traffic Lights?
:The Requirement for Landmarks in Vehicle Nav-igation Systems.
The Journal of Navigation,53(03):499?510.R.
Dale, S. Geldof, and J. P. Prost.
2003.
Using naturallanguage generation for navigational assistance.
InACSC, pages 35?44.B.
Elias.
2003.
Extracting landmarks with data min-ing methods.
Spatial information theory, pages375?389.A.
Gatt, F. Portet, E. Reiter, J.
Hunter, S. Mahamood,W.
Moncur, and S. Sripada.
2009.
From data to textin the neonatal intensive care unit: Using NLG tech-nology for decision support and information man-agement.
AI Communications, 22:153?186.S.
Kaplan.
1976.
Adaption, structure and knowledge.In G. Moore and R. Golledge, editors, Environmen-tal knowing: Theories, research and methods, pages32?45.
Dowden, Hutchinson and Ross.J.
D. Kelleher and J. van Genabith.
2004.
Visualsalience and reference resolution in simulated 3-Denvironments.
Artificial Intelligence Review, 21(3).A.
Koller, K. Striegnitz, D. Byron, J. Cassell, R. Dale,J.
Moore, and J. Oberlander.
2010.
The First Chal-lenge on Generating Instructions in Virtual Environ-ments.
In E. Krahmer and M. Theune, editors, Em-pirical Methods in Natural Language Generation.Springer.N.
Lessmann, S. Kopp, and I. Wachsmuth.
2006.
Sit-uated interaction with a virtual human ?
percep-tion, action, and cognition.
In G. Rickheit andI.
Wachsmuth, editors, Situated Communication,pages 287?323.
Mouton de Gruyter.K.
Lovelace, M. Hegarty, and D. Montello.
1999.
El-ements of good route directions in familiar and un-familiar environments.
Spatial information theory.Cognitive and computational foundations of geo-graphic information science, pages 751?751.K.
Lynch.
1960.
The image of the city.
MIT Press.R.
Malaka and A. Zipf.
2000.
DEEP MAP ?
Chal-lenging IT research in the framework of a tourist in-formation system.
Information and communicationtechnologies in tourism, 7:15?27.R.
Malaka, J. Haeussler, and H. Aras.
2004.SmartKom mobile: intelligent ubiquitous user in-teraction.
In Proceedings of the 9th InternationalConference on Intelligent User Interfaces.A.
J.
May and T. Ross.
2006.
Presence and qualityof navigational landmarks: effect on driver perfor-mance and implications for design.
Human Fac-tors: The Journal of the Human Factors and Er-gonomics Society, 48(2):346.P.
E. Michon and M. Denis.
2001.
When and why arevisual landmarks used in giving directions?
Spatialinformation theory, pages 292?305.A.
Pauzie?.
2008.
Evaluating driver mental workloadusing the driving activity load index (DALI).
InProc.
of European Conference on Human InterfaceDesign for Intelligent Transport Systems, pages 67?77.M.
Raubal and S. Winter.
2002.
Enriching wayfind-ing instructions with local landmarks.
Geographicinformation science, pages 243?259.E.
Reiter and R. Dale.
2000.
Building natural lan-guage generation systems.
Studies in natural lan-guage processing.
Cambridge University Press.D.
M. Saucier, S. M. Green, J. Leason, A. MacFadden,S.
Bell, and L. J. Elias.
2002.
Are sex differences innavigation caused by sexually dimorphic strategiesor by differences in the ability to use the strategies?.Behavioral Neuroscience, 116(3):403.M.
Schro?der and J. Trouvain.
2003.
The Germantext-to-speech synthesis system MARY: A tool forresearch, development and teaching.
InternationalJournal of Speech Technology, 6(4):365?377.K.
Striegnitz and F. Majda.
2009.
Landmarks innavigation instructions for a virtual environment.Online Proceedings of the First NLG Challengeon Generating Instructions in Virtual Environments(GIVE-1).J.
C. Stutts, D. W. Reinfurt, L. Staplin, and E. A. Rodg-man.
2001.
The role of driver distraction in traf-fic crashes.
Washington, DC: AAA Foundation forTraffic Safety.A.
Tom and M. Denis.
2003.
Referring to landmarkor street information in route directions: What dif-ference does it make?
Spatial information theory,pages 362?374.766
