Chinese Text Summarization Based on Thematic Area DetectionPo HuDepartment of ComputerScienceCentral China NormalUniversityWuhan, China, 430079geminihupo@163.comTingting HeDepartment of ComputerScienceCentral China NormalUniversityWuhan, China, 430079hett@163.netDonghong JiInstitute for InfocommResearchHeng Mui Keng Terrace,Singapore, 119613dhji@i2r.a-star.edu.sgAbstractAutomatic summarization is an active researcharea in natural language processing.
This paper hasproposed a special method that produces textsummary by detecting thematic areas in Chinesedocument.
The specificity of the method is that theproduced summary can both cover many differentthemes and reduce its redundancy obviously at thesame time.
In this method, the detection of latentthematic areas is realized by adopting K-medoidsclustering method as well as a novel clusteringanalysis method, which can be used to determineautomatically K, the number of clusters.. Inaddition, a novel parameter, which is known asrepresentation entropy, is used for summarizationredundancy evaluation.
Experimental resultsindicate a clear superiority of the proposed methodover the traditional non-thematic-area-detectionmethod under the proposed evaluation schemewhen dealing with different genres of textdocuments with free style and flexible themedistribution.1 IntroductionWith the approaching information explosion,people begin to feel at a loss about the mass ofinformation.
Because the effectiveness  of theexisting information retrieval technology is stillunsatisfactory, it becomes a problem to efficientlyfind the information mostly related to the needs ofcustomers retrieval results so that customers caneasily accept or reject the retrieved informationwithout needing to look at the original retrievalresults.
This paper has proposed a newsummarization method, where K-medoidclustering method is applied to detect all possiblepartitions of thematic areas, and a novel clusteringanalysis method, which is based on a self-definedobjective function, is applied to automaticallydetermine K, the number of latent thematic areas in adocumentThis method consists of three main stages: 1) Findout the thematic areas in the document by adopting theK-medoid clustering method (Kaufmann andRousseeuw, 1987as well as a novel clustering analysismethod.
2) From each thematic area, find a sentencewhich has the maximum semantic similarity valuewith this area as the representation.
3) Output theselected sentences to form the final summaryaccording to their pos itions in the original document.To validate the effectiveness of the proposedmethod, use this method as well as the traditionalnon-thematic -areas-detection method on ourexperimental samples to generate two groups ofsummaries.
Next, make a comparison between them.The final results show a clear superiority of ourmethod over the traditional one in the scores of theevaluation parameters.The remainder of this paper is organized asfollows.
In the next section, we review relatedmethods that are commonly discussed in theautomatic summarization literature.
Section 3describes our method in detail.
The evaluationmethodology and experimental results are presentedin Section 4.
Finally, we conclude with a discussionand future work.2 Related WorkThe research of automatic summarization begins withH.P.Luhn?s work.
By far, a large number of scholarshave taken part in the research and had manyachievements.
Most of the researchers haveconcentrated on the sentence-extractionsummarization method (the so-called shallowerapproach) (Wang et al, 2003; Nomoto andMatsumoto, 2001; Gong and Liu, 2001), but not thesentence-generation method (the so-called deeperapproach)(Yang and Zhong., 1998).
On the one hand,it is caused by the high complexity and the severelimitation of practical fields of rational naturallanguage processing technology and knowledgeengineering technology.
On the other hand, it isclosely associated with the great achievements inmany fields of natural language processing bystatistical research methods, machine learningmethods and pattern recognition methods in recentyears (Mani, 2001).The summarization method of sentence-extraction can roughly be divided into two kinds:supervised and unsupervised (Nomoto andMatsumoto, 2001).
Generally, the realization of theformer relies on plenty of manual summaries, thatis so-called ?Gold Standards?
which helpdetermining the relevant parameters of thestatistical model for summarization.
However, notall people believe that manual summaries arereliable, so the researchers have begun toinvestigate the general unsupervised method,which can avoid the requirement of support ofmanual summaries.
Nevertheless it is soondiscovered that the summaries produced by thismethod can?t cover all the themes and have greatredundancy at the same time.
Usually, it can onlycover those intensively distributed themes whileneglects others.
So researchers in NanjingUniversity proposed a summarization methodbased on the analysis of the discourse structure toovercome these problems (Wang et al, 2003).
Bymaking statistics of the reduplicated words in theadjacent paragraphs of the document, the semanticdistances among them can be worked out.
Thenanalyse the thematic structure of the document andextract sentences from each theme to form asummary.
It is ideal to employ this method whiledealing with those documents with standarddiscourse structure, because it can effectivelyavoid the problems caused by the summarizationmethod without discourse structure analysis.
Yetwhen the writing style of a document is rather freeand the distribution of the themes is variable, thatis the same theme can be distributed in severalparagraphs not adjacent to each other, then the useof this method can?t be equally effective.To deal with a lot of Chinese documents whichhave free style of writing and flexible themes, asentence-extraction summarization method createdby detecting thematic areas is tried following suchwork as (Nomoto and Matsumoto, 2001; Salton etal., 1996; Salton et al, 1997; Carbonell andGoldstein, 1998; Lin and Hovy, 2000).
Thethematic areas detection in a document is obtainedthrough the adaptive clustering of paragraphs (cf.Moens et al 1999), so it can overcome in a certaindegree the defects of the above methods in dealingwith the documents with rather flexible themedistribution.3 The AlgorithmIn this section, the proposed method will beintroduced in detail.
The method consists of thefollowing three main stages:Stage 1: Find the different thematic areas in thedocument through paragraph clustering andclustering analysis.Stage 2: Select the most suitable sentence fromeach thematic area as the representativeone.Stage 3: Make the representative sentences formthe final summary according to certainrequirements.3.1 Stage 1: Thematic Area DetectionThe process of thematic area detection is displayed inFigure 1.The each step of Figure 1 is explained in thefollowing subsections.Figure 1: The process of thematic area detection (4steps in all)3.1.1 Step 1: Term ExtractionDifferent from the general word segmentationoperation adopted in the traditional Chineseautomatic summarization research, we do not take thegeneral operation when pre-processing the originalOriginal documentTerm extractionVector representation of paragraphWeight calculation of paragraphParagraph clusteringClustering analysisThematic area detection1234document, but make use of the method introducedby (Liu et al, 2003) to extract termsfrom the document and then express its content bysuch metadata elements as terms.The greatest advantage of term extractiontechnology is that it needs no support of fixedthesaurus, only through the continuous updatingand making statistics of a real corpus.
We candynamically establish and update a term bank andimprove the extraction quality through continuouscorrecting of the parameters for extraction.
Thus itis of wide practical prospects for natural languageprocessing.
In addition, the terms can represent arelative specific meaning, because most of themare phrases, which consist of multi-characters.3.1.2 Step 2: Vector Representation and WeightCalculation of ParagraphThe advantage of the vector space model (VSM) isthat it successfully makes the unstructureddocuments structured which makes it possible tohandle the massive real documents by adopting theexisting mathematical instruments.
All the termsextracted from the document are considered as thefeatures of a vector, while the values of thefeatures are statistics of the terms.
According tothis, we can set up the VSM of paragraphs, that iseach paragraph Pi (i:1~M,M is the number of allparagraphs in a document) is represented as thevector of weights of terms, VPi, VPi =( WPi1,WPi2,?,WPiN)Where N is the total number of terms, WPijdenotes the weight of the j-th term in the i-thparagraph.
There are many methods of calculatingWPij, such as tf, tf*idf, mutual information (PatrickPantel and Lin, 2002), etc.
The method adoptedhere (Gong and Liu, 2001) is shown as follows:WPi j= log(1+TF(Ti j))*log(M/Mj ) (1)Where TF(Tij) denotes the number of occurrenceof the j-th term in the i-th paragraph, M/Mj denotesthe inverse paragraph frequency of term j, and Mjdenotes the number of paragraphs in which term joccurs.
In accordance, on the basis of definingWPij, we can further define the weight ofparagraph P i, W(P i), by the follwing formula:(2)In formula (2), n represents the total number ofdifferent terms occurring in the i-th paragraph.3.1.3 Step 3: Paragraph Clustering andClustering Analysis1) Paragraph clusteringThe existing clustering algorithms can becategorized as hierarchical (e.g.
agglomerative etc)and partitional (e.g.
K-means, K-medoids, etc)(Pantel and Lin, 2002).The complexity of the hierarchical clusteringalgorithm is O(n2Log(n)) , where n is the numberof elements to be clustered, which is usuallygreater than that of the partitional method.
Forexample, the complexity of K-means is linear in n.So in order to achieve high efficiency of algorithm,we choose the latter to cluster paragraphs.K-means clustering algorithm is a fine choice inmany circumstances, because it is simple andeffective.
But in the process of clustering by meansof K-means, the quality of clustering is greatlyaffected by the elements that marginally belong tothe cluster, and the centroid can?t represent the realelement in the cluster, So while choosing theparagraphs clustering algorithm, we adopt K-medoids (Kaufmann and Rousseeuw, 1987; Moenset al 1999) which is less sensitive to the effect ofmarginal elements than K-means.Suppose that every sample point in the N-dimensional sample space respectively represent aparagraph vector, and the clustering of paragraphscan be visualized as that of the M sample points inthe sample space.
Here N is the number of terms inthe document and M is the number of paragraphs.Table 1 shows the formal description of theparagraph clustering process based on K-medoidsmethod.2) Clustering analysisA classical problem when adopting K-medoidclustering method and many other clusteringmethods is the determination of K, the number ofclusters.
In traditional K-medoid method, K mustbe offered by the user in advance.
In many cases,it?s impractical.
As to clustering of paragraphs,customers can?t predict the latent thematic numberin the document, so it?s impossible to offer Kcorrectly.In view of the problem, the authors put forward anew clustering analysis method to automaticallydetermine the value of K according to thedistribution of values of the self-defined objectivefunction.
The basic idea is that if K, the number ofclusters,  is  determined  with  each  value of K, andInput: <a, b>, they respectively denote theparagraph matrix composed by all theparagraph vectors in the document andthe number of clusters, k (the range of kis set to 2~M).Step 1: randomly select k paragraph vectors asthe initial medoids of the clusters (here,the medoids denote the representativeparagraphs of k clusters).Step 2: assign each paragraph vector to a clusteraccording to the medoid X closest to it.Step 3: calculate the Euclidean distance betweenall the paragraph vectors and their closestmedoids.Step  4: randomly select a paragraph vector Y.Step 5: to all the X, if it can reduce theEuclidean distance between all theparagraph vectors and their closestmedoids by interchanging X and Y, thenchange their positions, otherwise keep asthe original.Step 6: repeat from step 2 to 5 until no changestake place.Output: <A, B, C>, they respectively denote thecluster id, the representative paragraphvector and all the paragraph vectors ofeach cluster under the k clusters.Table 1: Paragraph clustering process based onK-medoid methodsuitably, then the corresponding clusteringresults can well distinguish the different themesin the document, and correspondingly theaverage of the sum of the weight of therepresentative paragraph under each theme willtend to maximize.
We call this the maximumproperty of the objective function.Correspondingly, we define the followingobjective function Objf(K) to reflect clusteringquality and determine the number of clusters, K.1( )( )KjjW PO b j f KK==?
(3)Where W(Pj) denotes the weight of theselected representative paragraph in the j-thcluster, here the selected representativeparagraph Pj can be regarded as the medoid inthe j-th cluster which is determined by the finaloutput of the presented K-medoid paragraphclustering process, and the weight of Pj iscalculated by formula (2).
Put the objectivefunction in K clustering results correspondingthen make good use of the maximum property ofthe objective function to adaptively determine thefinal number of clusters, K.Figure 2 shows the concrete distribution of thevalues of objective function obtained in theexample document ?On the Situation and MeasuresThat Face Fishing in the Sea in Da Lian City?when adopting the proposed clustering analysismethod.
According to the maximum property ofobjective function, that is take the value of K whenthe values of the objective function take maximumas the final number of clusters.
From the results inFigure 2, we can know that K equals to six, that iswe find six latent thematic areas from nineparagraphs in the document with this method.11.051 .
11.151 .
21.251 .
3k=2 k=3 k=4 k=5 k=6 k=7 k=8 k=9k?Figure 2: The distribution of the values of theobjective function when K takes different valuesFigure 3 displays the paragraph clustering resultswhen K equals six in the process of adopting K-medoid clustering method on the exampledocument.Component 1Component2-10 -5 0 5-505These two components explain 54.44 % of the point variability.Figure 3: The paragraphs clustering result when Kequals to six3.1.4 Step 4: Thematic Area DetectionOutput the complete information table of eachthematic area in the form of the representativeparagraph and all the paragraphs and sentencescovered by the thematic area.3.2 Stage 2: Selection of the ThematicRepresentative SentencesTo select a most suitable representative sentencefrom each thematic area, the author proposes thefollowing method.
This is in contrast  with amethod proposed by Radev (Radev et al, 2000 ),where the centroid of a cluster is selected as therepresentative one.Method: select the sentence which is mostsimilar to the thematic area semantically asrepresentative one.Before carrying out the method in detail, thereare two problems to be solved:1) The vector representation of sentence andthematic areaThe vector representation of sentence andthematic area is similar to that of paragraphintroduced before.
We only need to change theweight calculation field of the terms from theinterior of paragraph to the interior of sentenceor thematic area.
Accordingly, we can describethe sentence vector and thematic area vector asfollowsVSj= ( WSj1,WSj2,?,WSjN)VAk= ( WAk1,WAk2,?,WAkN)2) The semantic similarity calculation betweensentence and thematic areaThe calculation of semantic similarity ofsentence and thematic area can be achieved bycalculating the vector distance between sentencevector and thematic area vector.
Here we adoptthe traditional cosine method for vector distancecalculation.
Correspondingly, the distancebetween the sentence vector VSj and the thematicarea vector VAk is calculated by the followingformula:( ) ,2 21 1 1( , )N N Nj k ji ki ji kii i iWS WA WS WAVS VACos= = =?
?
?
?= ?
??
?
?
??
?
?
??
?
?
(4)Principles of evaluating summarization redundancyAt the premise of the same number ofsummarization sentences selected out bydifferent summarization methods:The higher the value of RE calculated by thecovariance matrix of the summarization sentencevectors.The lower the summarization redundancy.Table 2: The evaluation principles of thesummarization redundancy based on RE3.3 Stage 3: The Creation of the SummaryOuput the selected representative sentences fromeach thematic area according to their postions in theoriginal document to form the final summary.4 Experimental Results and PerformanceEvaluation4.1 Evaluation MethodologyIt is challenging to objectively evaluate the qua lity ofdifferent automatic summarization methods.
Methodsfor evaluation can be broadly classified into twocategories: intrinsic and extrinsic (Mani, 2001).
Weadopt the former to evaluate the quality ofsummarization by defining the following parametersfor evaluation.1) Theme coverage (TC)The definition of TC is the percentage of thethematic contents covered by the selectedsummarization sentences.
The value of theparameter can be got by means of the works ofsome experts.2) Representation entropy (RE)In order to effectively and objectively evaluate theredundancy of the produced summary, we refer tothe parameter which was initially proposed by(Mitra et al, 2002) for evaluating the featureredundancy in the process of feature selection andtransform it into the novel parameter to evaluatethe summarization redundancy.According to this, some important notations aredefined as follows:N Number of terms in the originaldocument ;Nz Number of sentences in theproduced summary ;LzNz-by-N matrix composed byall the sentence vectors in theproduced summary ;?zNz-by-Nz covariance matrixcomposed by all the sentencevectors in the producedsummary ;l i Eigenvalues of ?z i:1~Nz ;?
i ?
i= l i /1Nzi =l  ?
i ;Theme coverage (TC) Representationentropy (RE)Genre Sample ID NumberofcharactersNumberofparagraphsNumberofdetectedthematicareasMethod1 Method2 Method1 Method2d10000801 1461 11 5 0.6 0.56 1.44 1.25d10000901 1192 7 5 0.64 0.6 1.36 1.35d10100101 1936 14 9 0.66 0.64 2.14 2.06d10100201 1778 12 6 0.8 0.5 1.62 1.54d10100301 2472 4 3 0.64 0.4 0.81 1.05d10100601 1553 11 7 0.9 0.64 1.79 1.83d29600501 2400 6 4 0.7 0.56 1.33 1.01d29800101 670 4 3 0.64 0.6 1.06 1.01d40000301 2026 8 5 0.56 0.52 1.45 1.54Economyd40100101 1529 7 4 0.6 0.58 1.19 1.31e10000101 907 4 2 0.72 0.56 0.64 0.24e10000201 845 5 3 0.9 0.6 1.06 0.89e29600201 2035 5 4 0.72 0.5 1.36 1.21Arte29800201 1831 7 2 0.56 0.52 0.67 0.57f20000101 2354 12 7 0.58 0.5 1.92 1.79 Prosef20000201 1769 9 6 0.64 0.52 1.72 1.50g00000201 1163 5 4 0.84 0.56 1.34 1.21g00000501 790 6 4 0.64 0.54 1.31 1.26g00001201 425 5 5 0.92 0.62 1.45 1.49g00100101 1629 10 3 0.84 0.6 0.93 0.82g00100301 817 6 4 0.76 0.7 1.32 1.26g00100501 1355 4 4 0.84 0.5 1.31 1.12g09600901 2179 7 6 0.72 0.62 1.75 1.73Militaryg09601601 1271 5 3 0.7 0.52 1.03 0.98h00000401 1224 6 6 0.72 0.54 1.75 1.60h00000601 1331 15 7 0.6 0.5 1.88 1.80h00000901 1507 7 3 0.64 0.68 1.05 0.83h00001801 1604 8 6 0.68 0.64 1.73 1.66h00100301 960 6 3 0.9 0.4 1.04 1.05Lifeh00100601 1228 6 3 0.8 0.6 1.06 0.89Table 3: Experimental dataMean of themecoverage ( TC )Mean of representationentropy ( R E )Ratio ofinformation andnoise (F)Genre Numberof samplesMethod1Method2Method1Method2Method1Method2Economy 10 0.68 0.56 1.42 1.40 2.81 2.27Art 4 0.72 0.54 0.93 0.73 1.82 1.12Prose 2 0.62 0.52 1.82 1.65 3.83 2.71Military  8 0.78 0.58 1.31 1.23 2.89 1.98Life 6 0.72 0.56 1.42 1.31 2.98 2.08Table 4: Evaluation results of parametersThe value of RE (Mitra et al, 2002) is calculatedas follows:RE= -1N zi =?
?
?
i * il o g  (5)The evaluation principles of the summarizationredundancy based on RE  are demonstrated inTable 2.3) Ratio of information and noise (F)F=TC/e ?RE (6)The novel evaluation parameter proposed by uscan objectively evaluate the quality of the producedsummary by effectively combining the above twoparameters.
The more the value of F, the better thequality of the produced summary.4.2 Experimental ResultsWe randomly extract 200 documents of differentgenres from the Modern Chinese Corpus of StateLanguage Commission to form the experimentalcorpus.
Because summarizing short documentsdoesn?t make much sense in real applications (Gongand Liu, 2001), we select 30 documents of morethan 400 characters from the corpus as the sampleswhich are summarized by the proposedsummarization method (method 1 for abbreviation)and the traditional non-thematic -area-detectionmethod (method 2 for abbreviation), that is themethod of determining the weights of sentences in adocument, sorting them in a decreasing order, andselecting the top sentences in the end.
The specificexperimental data and evaluation results ofparameters are given in table 3 and table 4.The synthetic evaluation of the 30 samples provesthat our method under the above evaluationparameters is superior to the traditional non-thematic-area-detection  summarization methodwhen dealing with different genres of textdocuments with free style and flexible themedistribution, and the results we have achieved areencouraging.5 ConclusionsIn this paper, we have proposed a newsummarization method based on thematic areasdetection.
By adopting a novel clustering analysismethod, it can adaptively detect the differentthematic areas in the document, and automaticallydetermine K, the number of thematic areas.
So theproduced summary can both cover as many asdifferent themes and reduce its redundancyobviously at the same time.For our experiment, we used three differentparameters to evaluate the quality of the producedsummaries in theme coverage and summarizationredundancy.
We achieved a better performance thanthe traditional non-thematic -areas-detection methodin the proposed evaluation scheme.
As a futurework , we need the additional research for testingthe proposed method on la rger-scale real corpora ,and have the further comparison with earlier similarworks such as MMR, etc.
In addition, we?llimprove our summarization system by consideringthe structure of thematic areas and user?srequirement.ReferencesJaime Carbonell and Jade Goldstein.
1998.
The useof MMR, diversity-based reranking for reorderingdocuments and producing summaries.
InProceedings of the 21th Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval.
ACM,New York.Yihong Gong, Xin Liu.
2001.
Generic textsummarization using relevance measure andlatent semantic analysis.
In Proceedings of ACMSIGIR?01, pages 19-25, ACM, New York.L.
Kaufmann and P.J.
Rousseeuw.
1987.
Clusteringby means of medoids.
In Statistical Data AnalysisBased on the L1 Norm,Y.Dodge,Ed,Amsterdam,405-416.Chin-Yew Lin and Eduard Hovy.
2000.
Theautomatic acquisition of topic signatures for textsummarization.
In Proceedings of the 18thInternational Conference of ComputationalLinguistics (COLING 2000).Jian-Zhou Liu, Ting-Ting He, and Dong-Hong Ji.2003.
Extracting Chinese term based on opencorpus.
In Proceedings of the 20th InternationalConference on Computer Processing of OrientalLanguages,pages 43-49.
ACM, New York.Inderjeet Mani.
2001.
Summarization evaluation: anoverview.
In Proceedings of the NTCIRWorkshop 2 Meeting on Evaluation of Chineseand Japanese Text Retrieval and TextSummarization.Inderjeet Mani.
2001.
Recent developments in textsummarization.
In Proceedings of CIKM?01, 529-531.Pabitra Mitra, C.A.
Murthy, Sankar and K.Pal.2002.
Unsupervised feature selection usingfeature similarity.
IEEE Transactions of PatternAnalysis and Machine Intelligence: 1-13.Marie-Francine Moens, Caroline Uyttendaele andJos Dumortier.
1999.
Abstracting of legal cases:The potential of clustering based on the selectionof representative objects.
Journal of the AmericanSociety for Information Science, 50 (2): 151-161.Tadashi Nomoto, Yuji Matsumoto.
2001.
A newapproach to unsupervised text summarization.
InProceedings of ACM SIGIR?01, pages 26-34.ACM, New York.Patrick Pantel and Dekang Lin.
2002.
Documentclustering with committees.
In Proceedings ofACM SIGIR?02, pages 199-206.
ACM, NewYork.Dragomir R. Radev, Hongyan Jing, and MalgorzataBudzikowska.
2000.
Centroid-basedsummarization of multiple documents: sentenceextraction, utility-based evaluation, and userstudies.
In ANLP/NAACL Workshop onSummarization.Gerard Salton, Amit Singhal, Chris Buckley andMandar Mitra.
1996.
Automatic textdecomposition using text segments and textthemes.
Hypertext 1996: 53-65.Gerard Salton, Amit Singhal, Mandar Mitra andChris Buckley.
1997.
Automatic text structuringand summarization.
In Information Processingand Management, 33(2):193-208.Ji-Cheng Wang, Gang-Shan Wu, Yuan-Yuan Zhou,Fu-Yan Zhang.
2003.
Research on automaticsummarization of web document guided bydiscourse.
Journal of Computer Research andDevelopment, 40(3):398-405.Xiao-Lan Yang and Yi-Xin Zhong.
1998.
Study andrealization for text interpretation and automaticabstracting.
Acta Electronica Sinica, 26(7):155-158.
