Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 414?423,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsEmploying Personal/Impersonal Views in Supervised andSemi-supervised Sentiment ClassificationShoushan Li??
Chu-Ren Huang?
Guodong Zhou?
Sophia Yat Mei Lee?
?Department of Chinese and BilingualStudiesThe Hong Kong Polytechnic University{shoushan.li,churenhuang,sophiaym}@gmail.com?Natural Language Processing LabSchool of Computer Science and TechnologySoochow University, Chinagdzhou@suda.edu.cnAbstractIn this paper, we adopt two views, personaland impersonal views, and systematicallyemploy them in both supervised andsemi-supervised sentiment classification.
Here,personal views consist of those sentenceswhich directly express speaker?s feeling andpreference towards a target object whileimpersonal views focus on statements towardsa target object for evaluation.
To obtain them,an unsupervised mining approach is proposed.On this basis, an ensemble method and aco-training algorithm are explored to employthe two views in supervised andsemi-supervised sentiment classificationrespectively.
Experimental results across eightdomains demonstrate the effectiveness of ourproposed approach.1 IntroductionAs a special task of text classification, sentimentclassification aims to classify a text according tothe expressed sentimental polarities of opinionssuch as ?thumb up?
or ?thumb down?
on themovies (Pang et al, 2002).
This task has recentlyreceived considerable interests in the NaturalLanguage Processing (NLP) community due to itswide applications.In general, the objective of sentimentclassification can be represented as a kind ofbinary relation R, defined as an ordered triple (X,Y, G), where X is an object set including differentkinds of people (e.g.
writers, reviewers, or users),Y is another object set including the targetobjects (e.g.
products, events, or even somepeople), and G is a subset of the Cartesianproduct X Y?
.
The concerned relation insentiment classification is X ?s evaluation on Y,such as ?thumb up?, ?thumb down?, ?favorable?,and ?unfavorable?.
Such relation is usuallyexpressed in text by stating the informationinvolving either a person (one element in X ) or atarget object itself (one element in Y ).
The firsttype of statement called personal view, e.g.
?I amso happy with this book?, contains X ?s?subjective?
feeling and preference towards atarget object, which directly expressessentimental evaluation.
This kind of informationis normally domain-independent and serves ashighly relevant clues to sentiment classification.The latter type of statement called impersonalview, e.g.
?it is too small?, contains Y ?s?objective?
(i.e.
or at least criteria-based)evaluation of the target object.
This kind ofinformation tends to contain muchdomain-specific classification knowledge.Although such information is sometimes not asexplicit as personal views in classifying thesentiment of a text, speaker?s sentiment isusually implied by the evaluation result.It is well-known that sentiment classificationis very domain-specific (Blitzer et al, 2007), soit is critical to eliminate its dependence on alarge-scale labeled data for its wide applications.Since the unlabeled data is ample and easy tocollect, a successful semi-supervised sentimentclassification system would significantlyminimize the involvement of labor and time.Therefore, given the two different viewsmentioned above, one promising application is toadopt them in co-training algorithms, which hasbeen proven to be an effective semi-supervisedlearning strategy of incorporating unlabeled datato further improve the classification performance(Zhu, 2005).
In addition, we would show thatpersonal/impersonal views are linguisticallymarked and mining them in text can be easilyperformed without special annotation.414In this paper, we systematically employpersonal/impersonal views in supervised andsemi-supervised sentiment classification.
First,an unsupervised bootstrapping method is adoptedto automatically separate one document intopersonal and impersonal views.
Then, both viewsare employed in supervised sentimentclassification via an ensemble of individualclassifiers generated by each view.
Finally, aco-training algorithm is proposed to incorporateunlabeled data for semi-supervised sentimentclassification.The remainder of this paper is organized asfollows.
Section 2 introduces the related work ofsentiment classification.
Section 3 presents ourunsupervised approach for mining personal andimpersonal views.
Section 4 and Section 5propose our supervised and semi-supervisedmethods on sentiment classification respectively.Experimental results are presented and analyzedin Section 6.
Section 7 discusses on thedifferences between personal/impersonal andsubjective/objective.
Finally, Section 8 draws ourconclusions and outlines the future work.2 Related WorkRecently, a variety of studies have been reportedon sentiment classification at different levels:word level (Esuli and Sebastiani, 2005), phraselevel (Wilson et al, 2009), sentence level (Kimand Hovy, 2004; Liu et al, 2005), and documentlevel (Turney, 2002; Pang et al, 2002).
Thispaper focuses on the document-level sentimentclassification.
Generally, document-levelsentiment classification methods can becategorized into three types: unsupervised,supervised, and semi-supervised.Unsupervised methods involve deriving asentiment classifier without any labeleddocuments.
Most of previous work use a set oflabeled sentiment words called seed words toperform unsupervised classification.
Turney(2002) determines the sentiment orientation of adocument by calculating point-wise mutualinformation between the words in the documentand the seed words of ?excellent?
and ?poor?.Kennedy and Inkpen (2006) use a term-countingmethod with a set of seed words to determine thesentiment.
Zagibalov and Carroll (2008) firstpropose a seed word selection approach and thenapply the same term-counting method for Chinesesentiment classifications.
These unsupervisedapproaches are believed to bedomain-independent for sentiment classification.Supervised methods consider sentimentclassification as a standard classification problemin which labeled data in a domain are used totrain a domain-specific classifier.
Pang et al(2002) are the first to apply supervised machinelearning methods to sentiment classification.Subsequently, many other studies make efforts toimprove the performance of machinelearning-based classifiers by various means, suchas using subjectivity summarization (Pang andLee, 2004), seeking new superior textual features(Riloff et al, 2006), and employing documentsubcomponent information (McDonald et al,2007).
As far as the challenge ofdomain-dependency is concerned, Blitzer et al(2007) present a domain adaptation approach forsentiment classification.Semi-supervised methods combine unlabeleddata with labeled training data (oftensmall-scaled) to improve the models.
Comparedto the supervised and unsupervised methods,semi-supervised methods for sentimentclassification are relatively new and have muchless related studies.
Dasgupta and Ng (2009)integrate various methods in semi-supervisedsentiment classification including spectralclustering, active learning, transductive learning,and ensemble learning.
They achieve a veryimpressive improvement across five domains.Wan (2009) applies a co-training method tosemi-supervised learning with labeled Englishcorpus and unlabeled Chinese corpus for Chinesesentiment classification.3 Unsupervised Mining of Personal andImpersonal ViewsAs mentioned in Section 1, the objective ofsentiment classification is to classify a specificbinary relation: X ?s evaluation on Y, where X isan object set including different kinds of personsand Y is another object set including the targetobjects to be evaluated.
First of all, we focus onan analysis on sentences in product reviewsregarding the two views: personal andimpersonal views.The personal view consists of personalsentences (i.e.
X ?s sentences) exemplifiedbelow:I.
Personal preference:E1: I love this breadmaker!E2: I disliked it from the beginning.II.
Personal emotion description:E3: Very disappointed!E4: I am happy with the product.III.
Personal actions:415E5: Do not waste your money.E6: I have recommended this machine to all myfriends.The impersonal view consists of impersonalsentences (i.e.Y ?s sentences) exemplified below:I. Impersonal feature description:E7: They are too thin to start with.E8: This product is extremely quiet.II.
Impersonal evaluation:E9: It's great.E10: The product is a waste of time and money.III.
Impersonal actions:E11: This product not even worth a penny.E12: It broke down again and again.We find that the subject of a sentence presentsimportant cues for personal/impersonal views,even though a formal and computable definitionof this contrast cannot be found.
Here, subjectrefers to one of the two main constituents in thetraditional English grammar (the otherconstituent being the predicate) (Crystal, 2003)1.For example, the subjects in the above examplesof E1, E7 and E11 are ?I?, ?they?, and ?thisproduct?
respectively.
For automatic mining thetwo views, personal/impersonal sentences can bedefined according to their subjects:Personal sentence: the sentence whosesubject is (or represents) a person.Impersonal sentence: the sentence whosesubject is not (does not represent) a person.In this study, we mainly focus on productreview classification where the target object inthe set Y  is not a person.
The definitions needto be adjusted when the evaluation target itself isa person, e.g.
the political sentimentclassification by Durant and Smith (2007).Our unsupervised mining approach for miningpersonal and impersonal sentences consists oftwo main steps.
First, we extract an initial set ofpersonal and impersonal sentences with someheuristic rules: If the first word of one sentenceis (or implies) a personal pronoun including ?I?,?we?, and ?do?, then the sentence is extracted as apersonal sentence; If the first word of onesentence is an impersonal pronoun including 'it','they', 'this', and 'these', then the sentence isextracted as an impersonal sentence.
Second, weapply the classifier which is trained with theinitial set of personal and impersonal sentencesto classify the remaining sentences.
This stepaims to classify the sentences without pronouns1The subject has the grammatical function in a sentence ofrelating its constituent (a noun phrase) by means of the verb to anyother elements present in the sentence, i.e.
objects, complements,and adverbials.(e.g.
E3).
Figure 1 shows the unsupervisedmining algorithm.Input:The training data DOutput:All personal and impersonal sentences, i.e.sentence sets personalS  and impersonalS .Procedure:(1).
Segment all documents in D to sentencesS using punctuations (such as periods andinterrogation marks)(2).
Apply the heuristic rules to classify thesentences S  with proper pronouns into, 1pSand  1iS(3).
Train a binary classifier p if ?
with  1pS  and1iS(4).
Use  p if ?
to classify the remaining sentencesinto  2pS  and  2iS(5).
1 2personal p pS S S= ?
,  1 2impersonal i iS S S= ?Figure 1: The algorithm for unsupervised miningpersonal and impersonal sentences from a trainingdata4 Employing Personal/ImpersonalViews in Supervised SentimentClassificationAfter unsupervised mining of personal andimpersonal sentences, the training data is dividedinto two views: the personal view, whichcontains personal sentences, and the impersonalview, which contains impersonal sentences.Obviously, these two views can be used to traintwo different classifiers, 1f  and 2f , forsentiment classification respectively.Since our mining approach is unsupervised,there inevitably exist some noises.
In addition,the sentences of different views may share thesame information for sentiment classification.For example, consider the following twosentences: ?It is a waste of money.?
and ?Do notwaste your money.?
Apparently, the first onebelongs to the impersonal view while the secondone belongs to personal view, according to ourheuristic rules.
However, these two sentencesshare the same word, ?waste?, which conveysstrong negative sentiment information.
Thissuggests that training a single-view classifier 3fwith all sentences should help.
Therefore, threebase classifiers, 1f , 2f , and 3f , are eventuallyderived from the personal view, the impersonal416view and the single view, respectively.
Each baseclassifier provides not only the class labeloutputs but also some kinds of confidencemeasurements, e.g.
posterior probabilities of thetesting sample belonging to each class.Formally, each base classifier  ( 1,2,3)lf l =assigns a test sample (denoted as lx ) a posteriorprobability vector ( )lP x:1 2( ) ( | ), ( | ) tl l lP x p c x p c x= < >where 1( | )lp c x  denotes the probability that the-thl base classifier considers the samplebelonging to 1c .In the ensemble learning literature, variousmethods have been presented for combining baseclassifiers.
The combining methods arecategorized into two groups (Duin, 2002): fixedrules such as voting rule, product rule, and sumrule (Kittler et al, 1998), and trained rules suchas weighted sum rule (Fumera and Roli, 2005)and meta-learning approaches (Vilalta and Drissi,2002).
In this study, we choose a fixed rule and atrained rule to combine the three base classifiers1f , 2f , and 3f .The chosen fixed rule is product rule whichcombine base classifiers by multiplying theposterior possibilities and using the multipliedpossibility for decision, i.e.31arg max ( | )ji li lassign y cwhere j p c x=?= ?The chosen trained rule is stacking (Vilalta andDrissi, 2002; D?eroski and ?enko, 2004) where ameta-classifier is trained with the output of thebase classifiers as the input.
Formally, let 'xdenote a feature vector of a sample from thedevelopment data.
The output of the -thl baseclassifier lf on this sample is the probabilitydistribution over the category set 1 2{ , }c c , i.e.1 2( ' ) ( | ' ), ( | ' )l l l lP x p c x p c x=< >Then, a meta-classifier is trained using thedevelopment data with the meta-level featurevector 2 3metax R ?
?1 2 3( ' ), ( ' ), ( ' )meta l l lx P x P x P x= = ==< >  In our experiments, we perform stacking with4-fold cross validation to generate meta-trainingdata where each fold is used as the developmentdata and the other three folds are used to train thebase classifiers in the training phase.5 Employing Personal/ImpersonalViews in Semi-Supervised SentimentClassificationSemi-supervised learning is a strategy whichcombines unlabeled data with labeled trainingdata to improve the models.
Given the two-viewclassifiers 1f  and 2f  along with the single-viewclassifier 3f , we perform a co-training algorithmfor semi-supervised sentiment classification.
Theco-training algorithm is a specificsemi-supervised learning approach which startswith a set of labeled data and increases theamount of labeled data using the unlabeled databy bootstrapping (Blum and Mitchell, 1998).Figure 2 shows the co-training algorithm in oursemi-supervised sentiment classification.Input:The labeled data Lcontaining personalsentence set L personalS ?
and impersonal sentence setL impersonalS ?The unlabeled data U  containing personalsentence setU personalS ?
and impersonal sentence setU impersonalS ?Output:New labeled data LProcedure:Loop for N iterations untilU ?=(1).
Learn the first classifier 1f  with L personalS ?(2).
Use 1f  to label samples from U withU personalS ?(3).
Choose 1n  positive and 1n negative mostconfidently predicted samples 1A(4).
Learn the second classifier 2f  with L impersonalS ?(5).
Use 2f to label samples from U withU impersonalS ?(6).
Choose 2n  positive and 2n negative mostconfidently predicted samples 2A(7).
Learn the third classifier 3f  with L(8).
Use 3f  to label samples from U(9).
Choose 3n  positive and 3n  negative mostconfidently predicted samples 3A(10).
Add samples 1 2 3A A A?
?
with thecorresponding labels into L(11).
Update L personalS ?
and L impersonalS ?Figure 2: Our co-training algorithm forsemi-supervised sentiment classification417After obtaining the new labeled data, we caneither adopt one classifier (i.e.
3f ) or acombined classifier (i.e.
1 2 3f f f+ + ) in furthertraining and testing.
In our experimentation, weexplore both of them with the former referred toas co-training and single classifier and the latterreferred to as co-training and combinedclassifier.6 Experimental StudiesWe have systematically explored our method onproduct reviews from eight domains: book, DVD,electronic appliances, kitchen appliances, health,network, pet and software.6.1 Experimental SettingThe product reviews on the first four domains(book, DVD, electronic, and kitchen appliances)come from the multi-domain sentimentclassification corpus, collected fromhttp://www.amazon.com/ by Blitzer et al (2007)2.Besides, we also collect the product views fromhttp://www.amazon.com/ on other four domains(health, network, pet and software)3.
Each of theeight domains contains 1000 positive and 1000negative reviews.
Figure 3 gives the distributionof personal and impersonal sentences in thetraining data (75% labeled data of all data).
Itshows that there are more impersonal sentencesthan personal ones in each domain, in particularin the DVD domain, where the number ofimpersonal sentences is at least twice as many asthat of personal sentences.
This unusualphenomenon is mainly attributed to the fact thatmany objective descriptions, e.g.
the movie plotintroductions, are expressed in the DVD domainwhich makes the extracted personal andimpersonal sentences rather unbalanced.We apply both support vector machine (SVM)and Maximum Entropy (ME) algorithms with thehelp of the SVM-light4 and Mallet5 tools.
Allparameters are set to their default values.
Wefind that ME performs slightly better than SVMon the average.
Furthermore, ME offers posteriorprobability information which is required for2http://www.seas.upenn.edu/~mdredze/datasets/sentiment/3Note that the second version of multi-domain sentimentclassification corpus does contain data from many other domains.However, we find that the reviews in the other domains containmany duplicated samples.
Therefore, we re-collect the reviews fromhttp://www.amazon.com/ and filter those duplicated ones.
The newcollection is here:http://llt.cbs.polyu.edu.hk/~lss/ACL2010_Data_SSLi.zip4http://svmlight.joachims.org/5http://mallet.cs.umass.edu/combination methods.
Thus we apply the MEclassification algorithm for further combinationand co-training.
In particular, we only employBoolean features, representing the presence orabsence of a word in a document.
Finally, weperform t-test to evaluate the significance of theperformance difference between two systemswith different methods (Yang and Liu, 1999).Sentence Number in the Training Data161348477 8337 88431309729290148521441412691 119411381814265 16441147531557327714010000200003000040000Book DVDElectronicKitchenHealthNetwork PetSoftwareNumberNumber of personal sentencesNumber of impersonal sentencesFigure 3: Distribution of personal and impersonalsentences in the training data of each domain6.2 Experimental Results on SupervisedSentiment Classification4-fold cross validation is performed forsupervised sentiment classification.
Forcomparison, we generate two random views byrandomly splitting the whole feature space intotwo parts.
Each part is seen as a view and used totrain a classifier.
The combination (two randomview classifiers along with the single-viewclassifier 3f ) results are shown in the last columnof Table 1.
The comparison between random twoviews and our proposed two views will clarifywhether the performance gain comes truly fromour proposed two-view mining, or simply fromusing the classifier combination strategy.Table 1 shows the performances of differentclassifiers, where the single-view classifier 3fwhich uses all sentences for training and testing,is considered as our baseline.
Note that thebaseline performances of the first four domainsare worse than the ones reported in Blitzer et al(2007).
But their experiment is performed withonly one split on the data with 80% as thetraining data and 20% as the testing data, whichmeans the size of their training data is larger thanours.
Also, we find that our performances aresimilar to the ones (described as fully supervisedresults) reported in Dasgupta and Ng (2009)where the same data in the four domains are usedand 10-fold cross validation is performed.418Domain PersonalViewClassifier1fImpersonalViewClassifier2fSingle ViewClassifier(baseline)3fCombination(Stacking)1 2 3f f f+ +Combination(Product rule)1 2 3f f f+ +Combinationwith tworandom views(Product rule)Book 0.7004 0.7474 0.7654 0.7919 0.7949 0.7546DVD 0.6931 0.7663 0.7884 0.8079 0.8165 0.8054Electronic 0.7414 0.7844 0.8074 0.8304 0.8364 0.8210Kitchen 0.7430 0.8030 0.8290 0.8555 0.8565 0.8152Health 0.7000 0.7370 0.7559 0.7780 0.7815 0.7548Network 0.7655 0.7710 0.8265 0.8360 0.8435 0.8312Pet 0.6940 0.7145 0.7390 0.7565 0.7665 0.7423Software 0.7035 0.7205 0.7470 0.7730 0.7715 0.7615AVERAGE 0.7176 0.7555 0.7823 0.8037 0.8084 0.7858Table 1: Performance of supervised sentiment classificationFrom Table 1, we can see that impersonal viewclassifier 1f  consistently performs better thanpersonal view classifier 2f .
Similar to thesentence distributions, the difference in theclassification performances between these twoviews in the DVD domain is the largest (0.6931vs.
0.7663).Both the combination methods (stacking andproduct rule) significantly outperform thebaseline in each domain (p-value<0.01) with adecent average performance improvement of2.61%.
Although the performance differencebetween the product rule and stacking is notsignificant, the product rule is obviously a betterchoice as it involves much easier implementation.Therefore, in the semi-supervised learningprocess, we only use the product rule to combinethe individual classifiers.
Finally, it shows thatrandom generation of two views with thecombination method of the product rule onlyslightly outperforms the baseline on the average(0.7858 vs. 0.7823) but performs much worsethan our unsupervised mining of personal andimpersonal views.6.3 Experimental Results onSemi-supervised SentimentClassificationWe systematically evaluate and compare ourtwo-view learning method with varioussemi-supervised ones as follows:Self-training, which uses the unlabeled datain a bootstrapping way like co-training yet limitsthe number of classifiers and the number ofviews to one.
Only the baseline classifier 3f  isused to select most confident unlabeled samplesin each iteration.Transductive SVM, which seeks the largestseparation between labeled and unlabeled datathrough regularization (Joachims, 1999).
Weimplement it with the help of the SVM-light tool.Co-training with random two-viewgeneration (briefly called co-training withrandom views), where two views are generatedby randomly splitting the whole feature spaceinto two parts.In semi-supervised sentiment classification,the data are randomly partitioned into labeledtraining data, unlabeled data, and testing datawith the proportion of 10%, 70% and 20%respectively.
Figure 4 reports the classificationaccuracies in all iterations, where baselineindicates the supervised classifier 3f  trained onthe 10% data; both co-training and singleclassifier and co-training and combinedclassifier refer to co-training using our proposedpersonal and impersonal views.
But the formermerely applies the baseline classifier 3f  trainedthe new labeled data to test on the testing datawhile the latter applies the combined classifier1 2 3f f f+ + .
In each iteration, two top-confidentsamples in each category are chosen, i.e.1 2 3 2n n n= = = .
For clarity, results of othermethods (e.g.
self-training, transductive SVM)are not shown in Figure 4 but will be reported inFigure 5 later.Figure 4 shows that co-training andcombined classifier always outperformsco-training and single classifier.
This againjustifies the effectiveness of our two-viewlearning on supervised sentiment classification.41925 50 75 100 1250.620.640.660.680.70.720.740.76Domain: BookIteration NumberAccuracy25 50 75 100 1250.580.60.620.640.660.680.7Domain: DVDIteration NumberAccuracy25 50 75 100 1250.70.720.740.760.780.8Domain: ElectronicIteration NumberAccuracy25 50 75 100 1250.720.740.760.780.80.82Domain: KitchenIteration NumberAccuracy25 50 75 100 1250.540.560.580.60.620.640.66Domain: HealthIteration NumberAccuracy25 50 75 100 1250.720.740.760.780.80.820.840.86Domain: NetworkIteration NumberAccuracyBaselineCo-traning and single classifierCo-traning and combined classifier25 50 75 100 1250.580.60.620.640.660.68Domain: PetIteration NumberAccuracy25 50 75 100 1250.620.640.660.680.70.72Domain: SoftwareIteration NumberAccuracyFigure 4: Classification performance vs. iteration numbers (using 10% labeled data as training data)One open question is whether the unlabeleddata improve the performance.
Let us set asidethe influence of the combination strategy andfocus on the effectiveness of semi-supervisedlearning by comparing the baseline andco-training and single classifier.
Figure 4shows different results on different domains.Semi-supervised learning fails on the DVDdomain while on the three domains of book,electronic, and software, semi-supervisedlearning benefits slightly (p-value>0.05).
Incontrast, semi-supervised learning benefits muchon the other four domains (health, kitchen,network, and pet) from using unlabeled data andthe performance improvements are statisticallysignificant (p-value<0.01).
Overall speaking, wethink that the unlabeled data are very helpful asthey lead to about 4% accuracy improvement onthe average except for the DVD domain.
Alongwith the supervised combination strategy, ourapproach can significantly improve theperformance more than 7% on the averagecompared to the baseline.Figure 5 shows the classification results ofdifferent methods with different sizes of thelabeled data: 5%, 10%, and 15% of all data,where the testing data are kept the same (20% ofall data).
Specifically, the results of othermethods including self-training, transductiveSVM, and random views are presented when10% labeled data are used in training.
It showsthat self-training performs much worse than ourapproach and fails to improve the performance offive of the eight domains.
Transductive SVMperforms even worse and can only improve theperformance of the ?software?
domain.
Althoughco-training with random views outperforms thebaseline on four of the eight domains, it performsworse than co-training and single classifier.This suggests that the impressive improvementsare mainly due to our unsupervised two-viewmining rather than the combination strategy.420Using 10% labeled data as training data0.50.550.60.650.70.750.80.85Book DVD Electronic Kitchen Health Network Pet SoftwareAccuracyBaseline Transductive SVM Self-trainingCo-training with random views Co-training and single classifier Co-training and combined classifierUsing 5% labeled data as training data0.690.7470.5840.5250.67 0.6530.6260.550.5640.6830.4950.6150.86750.78550.70.6010.450.550.650.750.85Book DVDElectronicKitchenHealthNetwork PetSoftwareAccuracyUsing 15% labeled data as training data0.7630.69250.7650.59250.6790.5640.6770.73750.66250.7350.6550.6150.86250.83250.7820.7160.450.550.650.750.85Book DVDElectronicKitchenHealthNetwork PetSoftwareAccuracyFigure 5: Performance of semi-supervised sentiment classification when 5%, 10%, and 15% labeled data are usedFigure 5 also shows that our approach is ratherrobust and achieves excellent performances indifferent training data sizes, although ourapproach fails on two domains, i.e.
book andDVD, when only 5% of the labeled data are used.This failure may be due to that some of thesamples in these two domains are too ambiguousand hard to classify.
Manual checking shows thatquite a lot of samples on these two domains areeven too difficult for professionals to give ahigh-confident label.
Another possible reason isthat there exist too many objective descriptionsin these two domains, thus introducing too muchnoisy information for semi-supervised learning.The effectiveness of different sizes of chosensamples in each iteration is also evaluated like1 2 3 6n n n= = = and 1 2 33, 6n n n= = = (Thisassignment is considered because the personalview classifier performs worse than the other twoclassifiers).
Our experimental results are stillunsuccessful in the DVD domain and do notshow much difference on other domains.
We alsotest the co-training approach without thesingle-view classifier 3f .
Experimental resultsshow that the inclusion of the single-viewclassifier 3f  slightly helps the co-trainingapproach.
The detailed discussion of the resultsis omitted due to space limit.6.4 Why our approach is effective?One main reason for the effectiveness of ourapproach on supervised learning is the way howpersonal and impersonal views are dealt with.
Aspersonal and impersonal views have differentways of expressing opinions, splitting them intotwo separations can filter some classificationnoises.
For example, in the sentence of ?I haveseen amazing dancing, and good dancing.
Thiswas TERRIBLE dancing!?.
The first sentence isclassified as a personal sentence and the secondone is an impersonal sentence.
Although thewords ?amazing?
and ?good?
convey strongpositive sentiment information, the whole text isnegative.
If we get the bag-of-words from thewhole text, the classification result will be wrong.Rather, splitting the text into two parts based ondifferent views allows correct classification asthe personal view rarely contains impersonalwords such as ?amazing?
and ?good?.
Theclassification result will thus be influenced bythe impersonal view.In addition, a document may contain bothpersonal and impersonal sentences, and each ofthem, to a certain extent, , provides classificationevidence.
In fact, we randomly select 50documents in the domain of kitchen appliancesand find that 80% of the documents take bothpersonal and impersonal sentences in which bothof them express explicit opinions.
That is to say,the two views provide different, complementaryinformation for classification.
This qualifies thesuccess requirement of co-training algorithm tosome extend.
This might be the reason for theeffectiveness of our approach on semi-supervisedlearning.4217 Discussion on Personal/Impersonal vs.Subjective/ObjectiveAs mentioned in Section 1, personal viewcontains X ?s ?subjective?
feeling, andimpersonal view containsY ?s ?objective?
(i.e.
orat least criteria-based) evaluation of the targetobject.
However, our technically-definedconcepts of personal/impersonal are definitelydifferent from subjective/objective: Personalview can certainly contain many objectiveexpressions, e.g.
?I bought this electric kettle?
andimpersonal view can contain many subjectiveexpressions, e.g.
?It is disappointing?.Our technically-defined personal/impersonalviews are two different ways to describeopinions.
Personal sentences are often used toexpress opinions in a direct way and their targetobject should be one of X. Impersonal ones areoften used to express opinions in an indirect wayand their target object should be one of Y. Theideal definition of personal (or impersonal) viewgiven in Section 1 is believed to be a subset ofour technical definition of personal (orimpersonal) view.
Thus impersonal view maycontain both Y ?s objective evaluation (morelikely to be domain independent) and subjectiveY?s description.In addition, simply splitting text intosubjective/objective views is not particularlyhelpful.
Since a piece of objective text providesrather limited implicit classification information,the classification abilities of the two views arevery unbalanced.
This makes the co-trainingprocess unfeasible.
Therefore, we believe thatour technically-defined personal/impersonalviews are more suitable for two-view learningcompared to subjective/objective views.8 Conclusion and Future WorkIn this paper, we propose a robust and effectivetwo-view model for sentiment classificationbased on personal/impersonal views.
Here, thepersonal view consists of subjective sentenceswhose subject is a person, whereas theimpersonal view consists of objective sentenceswhose subject is not a person.
Such views arelexically cued and can be obtained withoutpre-labeled data and thus we explore anunsupervised learning approach to mine them.Combination methods and a co-trainingalgorithm are proposed to deal with supervisedand semi-supervised sentiment classificationrespectively.
Evaluation on product reviews fromeight domains shows that our approachsignificantly improves the performance across alleight domains on supervised sentimentclassification and greatly outperforms thebaseline with more than 7% accuracyimprovement on the average across seven ofeight domains (except the DVD domain) onsemi-supervised sentiment classification.In the future work, we will integrate thesubjectivity summarization strategy (Pang andLee, 2004) to help discard noisy objectivesentences.
Moreover, we need to consider thecases when both X and Y appear in a sentence.For example, the sentence ?I think they're poor?should be an impersonal view but wronglyclassified as a personal one according to ourtechnical rules.
We believe that these will helpimprove our approach and hopefully areapplicable to the DVD domain.
Anotherinteresting and practical idea is to integrateactive learning (Settles, 2009), another popularbut principally different kind of semi-supervisedlearning approach, with our two-view learningapproach to build high-performance systemswith the least labeled data.AcknowledgmentsThe research work described in this paper hasbeen partially supported by Start-up Grant forNewly Appointed Professors, No.
1-BBZM inthe Hong Kong Polytechnic University and twoNSFC grants, No.
60873150 and No.
90920004.We also thank the three anonymous reviewersfor their invaluable comments.ReferencesBlitzer J., M. Dredze, and F. Pereira.
2007.Biographies, Bollywood, Boom-boxes andBlenders: Domain Adaptation for SentimentClassification.
In Proceedings of ACL-07.Blum A. and T. Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
InProceedings of COLT-98.Crystal D. 2003.
The Cambridge Encyclopedia of theEnglish Language.
Cambridge University Press.Dasgupta S. and V. Ng.
2009.
Mine the Easy andClassify the Hard: Experiments with AutomaticSentiment Classification.
In Proceedings ofACL-IJCNLP-09.Duin R. 2002.
The Combining Classifier: To Train OrNot To Train?
In Proceedings of 16th InternationalConference on Pattern Recognition (ICPR-02).Durant K. and M. Smith.
2007.
Predicting thePolitical Sentiment of Web Log Posts using422Supervised Machine Learning Techniques Coupledwith Feature Selection.
In Processing of Advancesin Web Mining and Web Usage Analysis.D?eroski S. and B.
?enko.
2004.
Is CombiningClassifiers with Stacking Better than Selecting theBest One?
Machine Learning, vol.54(3),pp.255-273, 2004.Esuli A. and F. Sebastiani.
2005.
Determining theSemantic Orientation of Terms through GlossClassification.
In Proceedings of CIKM-05.Fumera G. and F. Roli.
2005.
A Theoretical andExperimental Analysis of Linear Combiners forMultiple Classifier Systems.
IEEE Trans.
PAMI,vol.27, pp.942?956, 2005Joachims, T. 1999.
Transductive Inference for TextClassification using Support Vector Machines.ICML1999.Kennedy A. and D. Inkpen.
2006.
SentimentClassification of Movie Reviews using ContextualValence Shifters.
Computational Intelligence,vol.22(2), pp.110-125, 2006.Kim S. and E. Hovy.
2004.
Determining theSentiment of Opinions.
In Proceedings ofCOLING-04.Kittler J., M. Hatef, R. Duin, and J. Matas.
1998.
OnCombining Classifiers.
IEEE Trans.
PAMI, vol.20,pp.226-239, 1998Liu B., M. Hu, and J. Cheng.
2005.
Opinion Observer:Analyzing and Comparing Opinions on the Web.In Proceedings of WWW-05.McDonald R., K. Hannan, T. Neylon, M. Wells, and J.Reynar.
2007.
Structured Models forFine-to-coarse Sentiment Analysis.
In Proceedingsof ACL-07.Pang B. and L. Lee.
2004.
A Sentimental Education:Sentiment Analysis using SubjectivitySummarization based on Minimum Cuts.
InProceedings of ACL-04.Pang B., L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment Classification using MachineLearning Techniques.
In Proceedings ofEMNLP-02.Riloff E., S. Patwardhan, and J. Wiebe.
2006.
FeatureSubsumption for Opinion Analysis.
In Proceedingsof EMNLP-06.Settles B.
2009.
Active Learning Literature Survey.Technical Report 1648, Department of ComputerSciences, University of Wisconsin at Madison,Wisconsin.Turney P. 2002.
Thumbs Up or Thumbs Down?Semantic Orientation Applied to UnsupervisedClassification of Reviews.
In Proceedings ofACL-02.Vilalta R. and Y. Drissi.
2002.
A Perspective Viewand Survey of Meta-learning.
Artificial IntelligenceReview, 18(2): 77?95.Wan X.
2009.
Co-Training for Cross-LingualSentiment Classification.
In Proceedings ofACL-IJCNLP-09.Wilson T., J. Wiebe, and P. Hoffmann.
2009.Recognizing Contextual Polarity: An Explorationof Features for Phrase-Level Sentiment Analysis.Computational Linguistics, vol.35(3), pp.399-433,2009.Yang Y. and X. Liu.
1999.
A Re-Examination of TextCategorization methods.
In Proceedings ofSIGIR-99.Zagibalov T. and J. Carroll.
2008.
Automatic SeedWord Selection for Unsupervised SentimentClassification of Chinese Test.
In Proceedings ofCOLING-08.Zhu X.
2005.
Semi-supervised Learning LiteratureSurvey.
Technical Report Computer Sciences 1530,University of Wisconsin ?
Madison.423
