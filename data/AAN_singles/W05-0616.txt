Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 120?127, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsAn Analogical Learner for Morphological AnalysisNicolas Stroppa & Franc?ois YvonGET/ENST & LTCI, UMR 514146 rue Barrault, 75013 Paris, France{stroppa,yvon}@enst.frAbstractAnalogical learning is based on a two-step inference process: (i) computationof a structural mapping between a newand a memorized situation; (ii) transferof knowledge from the known to the un-known situation.
This approach requiresthe ability to search for and exploit suchmappings, hence the need to properly de-fine analogical relationships, and to effi-ciently implement their computation.In this paper, we propose a unified defini-tion for the notion of (formal) analogicalproportion, which applies to a wide rangeof algebraic structures.
We show that thisdefinition is suitable for learning in do-mains involving large databases of struc-tured data, as is especially the case in Nat-ural Language Processing (NLP).
We thenpresent experimental results obtained ontwo morphological analysis tasks whichdemonstrate the flexibility and accuracy ofthis approach.1 IntroductionAnalogical learning (Gentner et al, 2001) is basedon a two-step inductive process.
The first step con-sists in the construction of a structural mapping be-tween a new instance of a problem and solved in-stances of the same problem.
Once this mappingis established, solutions for the new instance can beinduced, based on one or several analogs.
The im-plementation of this kind of inference process re-quires techniques for searching for, and reasoningwith, structural mappings, hence the need to prop-erly define the notion of analogical relationships andto efficiently implement their computation.In Natural Language Processing (NLP), the typ-ical dimensionality of databases, which are madeup of hundreds of thousands of instances, makesthe search for complex structural mappings a verychallenging task.
It is however possible to take ad-vantage of the specific nature of linguistic data towork around this problem.
Formal (surface) analog-ical relationships between linguistic representationsare often a good sign of deeper analogies: a surfacesimilarity between the word strings write and writerdenotes a deeper (semantic) similarity between therelated concepts.
Surface similarities can of coursebe misleading.
In order to minimize such confu-sions, one can take advantage of other specificitiesof linguistic data: (i) their systemic organization in(pseudo)-paradigms, and (ii) their high level of re-dundancy.
In a large lexicon, we can indeed expectto find many instances of pairs like write-writer: forinstance read -reader, review-reviewer...Complementing surface analogies with statisticalinformation thus has the potential to make the searchproblem tractable, while still providing with manygood analogs.
Various attempts have been made touse surface analogies in various contexts: automaticword pronunciation (Yvon, 1999), morphologicalanalysis (Lepage, 1999a; Pirrelli and Yvon, 1999)and syntactical analysis (Lepage, 1999b).
These ex-periments have mainly focused on linear represen-120tations of linguistic data, taking the form of finitesequences of symbols, using a restrictive and some-times ad-hoc definition of the notion of an analogy.The first contribution of this paper is to propose ageneral definition of formal analogical proportionsfor algebraic structures commonly used in NLP:attribute-value vectors, words on finite alphabets andlabeled trees.
The second contribution is to showhow these formal definitions can be used within aninstance-based learning framework to learn morpho-logical regularities.This paper is organized as follows.
In Section 2,our interpretation of analogical learning is intro-duced and related to other models of analogicallearning and reasoning.
Section 3 presents a generalalgebraic framework for defining analogical propor-tions as well as its instantiation to the case of wordsand labeled trees.
This section also discusses thealgorithmic complexity of the inference procedure.Section 4 reports the results of experiments aimedat demonstrating the flexibility of this model and atassessing its generalization performance.
We con-clude by discussing current limitations of this modeland by suggesting possible extensions.2 Principles of analogical learning2.1 Analogical reasoningThe ability to identify analogical relationships be-tween what looks like unrelated situations, and touse these relationships to solve complex problems,lies at the core of human cognition (Gentner et al,2001).
A number of models of this ability havebeen proposed, based on symbolic (e.g.
(Falken-heimer and Gentner, 1986; Thagard et al, 1990;Hofstadter and the Fluid Analogies Research group,1995)) or subsymbolic (e.g.
(Plate, 2000; Holyoakand Hummel, 2001)) approaches.
The main focusof these models is the dynamic process of analogymaking, which involves the identification of a struc-tural mappings between a memorized and a new sit-uation.
Structural mapping relates situations which,while being apparently very different, share a set ofcommon high-level relationships.
The building ofa structural mapping between two situations utilizesseveral subparts of their descriptions and the rela-tionships between them.Analogy-making seems to play a central role inour reasoning ability; it is also invoked to explainsome human skills which do not involve any sort ofconscious reasoning.
This is the case for many tasksrelated to the perception and production of language:lexical access, morphological parsing, word pronun-ciation, etc.
In this context, analogical models havebeen proposed as a viable alternative to rule-basedmodels, and many implementation of these low-level analogical processes have been proposed suchas decision trees, neural networks or instance-basedlearning methods (see e.g.
(Skousen, 1989; Daele-mans et al, 1999)).
These models share an accepta-tion of analogy which mainly relies on surface simi-larities between instances.Our learner tries to bridge the gap between theseapproaches and attempts to remain faithful to theidea of structural analogies, which prevails in theAI literature, while also exploiting the intuitions oflarge-scale, instance-based learning models.2.2 Analogical learningWe consider the following supervised learning task:a learner is given a set S of training instances{X1, .
.
.
, Xn} independently drawn from some un-known distribution.
Each instance Xi is a vectorcontaining m features: ?Xi1, .
.
.
, Xim?.
Given S,the task is to predict the missing features of partiallyinformed new instances.
Put in more standard terms,the set of known (resp.
unknown) features for a newvalue X forms the input space (resp.
output space):the projections of X onto the input (resp.
output)space will be denoted I(X) (resp.
O(X)).
This set-ting is more general than the simpler classificationtask, in which only one feature (the class label) isunknown, and covers many other interesting tasks.The inference procedure can be sketched as fol-lows: training examples are simply stored for fu-ture use; no generalization (abstraction) of the datais performed, which is characteristic of lazy learning(Aha, 1997).
Given a new instance X , we identifyformal analogical proportions involving X in the in-put space; known objects involved in these propor-tions are then used to infer the missing features.An analogical proportion is a relation involv-ing four objects A, B, C and D, denoted byA : B :: C : D and which reads A is to B as C isto D. The definition and computation of these pro-portions are studied in Section 3.
For the moment,121we contend that it is possible to construct analogicalproportions between (possibly partially informed)objects in S. Let I(X) be a partially described ob-ject not seen during training.
The analogical infer-ence process is formalized as:1.
Construct the set T (X) ?
S3 defined as:T (X) = {(A,B,C) ?
S3 |I(A) : I(B) :: I(C) : I(X)}2.
For each (A,B,C) ?
T (X), compute hy-potheses O?
(X) by solving the equation:O?
(X) = O(A) : O(B) :: O(C) :?This inference procedure shows lots of similari-ties with the k-nearest neighbors classifier (k-NN)which, given a new instance, (i) searches the trainingset for close neighbors, (ii) compute the unknownclass label according to the neighbors?
labels.
Ourmodel, however, does not use any metric betweenobjects: we only rely on the definition of analogicalproportions, which reveal systemic, rather than su-perficial, similarities.
Moreover, inputs and outputsare regarded in a symmetrical way: outputs are notrestricted to a set of labels, and can also be structuredobjects such as sequences.
The implementation ofthe model still has to address two specific issues.?
When exploring S3, an exhaustive search eval-uates |S|3 triples, which can prove to be in-tractable.
Moreover, objects in S may beunequally relevant, and we might expect thesearch procedure to treat them accordingly.?
Whenever several competing hypotheses areproposed for O?
(X), a ranking must be per-formed.
In our current implementation, hy-potheses are ranked based on frequency counts.These issues are well-known problems for k-NNclassifiers.
The second one does not appear to becritical and is usually solved based on a majorityrule.
In contrast, a considerable amount of effort hasbeen devoted to reduce and optimize the search pro-cess, via editing and condensing methods, as stud-ied e.g.
in (Dasarathy, 1990; Wilson and Martinez,2000).
Proposals for solving this problem are dis-cussed in Section 3.4.3 An algebraic framework for analogicalproportionsOur inductive model requires the availability of a de-vice for computing analogical proportions on featurevectors.
We consider that an analogical proportionholds between four feature vectors when the propor-tion holds for all components.
In this section, wepropose a unified algebraic framework for defininganalogical proportions between individual features.After giving the general definition, we present its in-stantiation for two types of features: words over afinite alphabet and sets of labelled trees.3.1 Analogical proportionsOur starting point will be analogical proportions ina set U , which we define as follows: ?x, y, z, t ?U, x : y :: z : t if and only if either x = y and z = tor x = z and y = t. In the sequel, we assume thatU is additionally provided with an associative inter-nal composition law?, which makes (U,?)
a semi-group.
The generalization of proportions to semi-groups involves two key ideas: the decomposition ofobjects into smaller parts, subject to alternation con-straints.
To formalize the idea of decomposition, wedefine the factorization of an element u in U as:Definition 1 (Factorization)A factorization of u ?
U is a sequence u1 .
.
.
un,with ?i, ui ?
U , such that: u1 ?
.
.
.
?
un = u.Each term ui is a factor of u.The alternation constraint expresses the fact thatanalogically related objects should be made of alter-nating factors: for x : y :: z : t to hold, each factorin x should be found alternatively in y and in z. Thisyields a first definition of analogical proportions:Definition 2 (Analogical proportion)(x, y, z, t) ?
U form an analogical proportion, de-noted by x : y :: z : t if and only if there exists somefactorizations x1?
.
.
.
?xd = x, y1?
.
.
.
?yd = y,z1 ?
.
.
.
?
zd = z, t1 ?
.
.
.
?
td = t such that?i, (yi, zi) ?
{(xi, ti), (ti, xi)}.
The smallest d forwhich such factorizations exist is termed the degreeof the analogical proportion.This definition is valid for any semigroup, and afortiori for any richer algebraic structure.
Thus, itreadily applies to the case of groups, vector spaces,free monoids, sets and attribute-value structures.1223.2 Words over Finite Alphabets3.2.1 Analogical Proportions between WordsLet ?
be a finite alphabet.
??
denotes the set offinite sequences of elements of ?, called words over?.
?
?, provided with the concatenation operation .is a free monoid whose identity element is the emptyword ?.
For w ?
?
?, w(i) denotes the ith symbol inw.
In this context, definition (2) can be re-stated as:Definition 3 (Analogical proportion in (??,.
))(x, y, z, t) ?
??
form an analogical proportion, de-noted by x : y :: z : t if and only if there exists someinteger d and some factorizations x1 .
.
.
xd = x,y1 .
.
.
yd = y, z1 .
.
.
zd = z, t1 .
.
.
td = t such that?i, (yi, zi) ?
{(xi, ti), (ti, xi)}.An example of analogy between words is:viewing : reviewer :: searching : researcherwith x1 = ?, x2 = view, x3 = ing and t1 = re,t2 = search, t3 = er.
This definition generalizesthe proposal of (Lepage, 1998).
It does not ensurethe existence of a solution to an analogical equation,nor its uniqueness when it exists.
(Lepage, 1998)gives a set of necessary conditions for a solution toexist.
These conditions also apply here.
In particu-lar, if t is a solution of x : y :: z :?, then t contains,in the same relative order, all the symbols in y and zthat are not in x.
As a consequence, all solutions ofan equation have the same length.3.2.2 A Finite-state SolverDefinition (3) yields an efficient procedure forsolving analogical equations, based on finite-statetransducers.
The main steps of the procedure aresketched here.
A full description can be found in(Yvon, 2003).
To start with, let us introduce the no-tions of complementary set and shuffle product.Complementary set If v is a subword of w, thecomplementary set of v with respect to w, denotedby w\v is the set of subwords of w obtained by re-moving from w, in a left-to-right fashion, the sym-bols in v. For example, eea is a complementary sub-word of xmplr with respect to exemplar.
When v isnot a subword of w, w\v is empty.
This notion canbe generalized to any regular language.The complementary set of v with respect to w isa regular set: it is the output language of the finite-state transducer Tw (see Figure 1) for the input v.0 1 kw(1) : ??
: w(1)w(k) : ??
: w(k)Figure 1: The transducer Tw computing comple-mentary sets wrt w.Shuffle The shuffle u ?
v of two words u and v isintroduced e.g.
in (Sakarovitch, 2003) as follows:u ?
v = {u1v1u2v2 .
.
.
unvn, st. ui, vi ?
?
?,u1 .
.
.
un = u, v1 .
.
.
vn = v}The shuffle of two words u and v contains all thewords w which can be composed using all the sym-bols in u and v, subject to the condition that if aprecedes b in u (or in v), then it precedes b in w.Taking, for instance, u = abc and v = def , thewords abcdef , abdefc, adbecf are in u ?
v; thisis not the case with abefcd.
This operation gen-eralizes straightforwardly to languages.
The shuf-fle of two regular languages is regular (Sakarovitch,2003); the automaton A, computing K?L, is derivedfrom the automata AK = (?, QK , q0K , FK , ?K) andAL = (?, QL, q0L, FL, ?L) recognizing respectivelyK and L as the product automata A = (?, QK ?QL, (q0K , q0L), FK ?
FL, ?
), where ?
is defined as:?
((qK , qL), a) = (rK , rL) if and only if either?K(qK , a) = rK and qL = rL or ?L(qL, a) = rLand qK = rK .The notions of complementary set and shuffle arerelated through the following property, which is adirect consequence of the definitions.w ?
u ?
v ?
u ?
w\vSolving analogical equations The notions ofshuffle and complementary sets yield anothercharacterization of analogical proportion betweenwords, based on the following proposition:Proposition 1.?x, y, z, t ?
?
?, x : y :: z : t?
x ?
t ?
y ?
z 6= ?An analogical proportion is thus established if thesymbols in x and t are also found in y and z, and ap-pear in the same relative order.
A corollary follows:123Proposition 2.t is a solution of x : y :: z :??
t ?
(y ?
z)\xThe set of solutions of an analogical equationx : y :: z :?
is a regular set, which can be computedwith a finite-state transducer.
It can also be shownthat this analogical solver generalizes the approachbased on edit distance proposed in (Lepage, 1998).3.3 TreesLabelled trees are very common structures in NLPtasks: they can represent syntactic structures, orterms in a logical representation of a sentence.
Toexpress the definition of analogical proportion be-tween trees, we introduce the notion of substitution.Definition 4 (Substitution)A (single) substitution is a pair (variable ?
tree).The application of the substitution (v ?
t?)
to a treet consists in replacing each leaf of t labelled by v bythe tree t?.
The result of this operation is denoted:t(v ?
t?).
For each variable v, we define the binaryoperator /v as t /v t?
= t (v ?
t?
).Definition 2 can then be extended as:Definition 5 (Analogical proportion (trees))(x, y, z, t) ?
U form an analogical propor-tion, denoted by x : y :: z : t iff there exists somevariables (v1, .
.
.
, vn?1) and some factorizationsx1 /v1 .
.
.
/vn?1 xn = x, y1 /v1 .
.
.
/vn?1 yn = y,z1 /v1 .
.
.
/vn?1 zn = z, t1 /v1 .
.
.
/vn?1 tn = t suchthat ?i, (yi, zi) ?
{(xi, ti), (ti, xi)}.An example of such a proportion is illustrated onFigure 2 with syntactic parse trees.This definition yields an effective algorithmcomputing analogical proportions between trees(Stroppa and Yvon, 2005).
We consider here a sim-pler heuristic approach, consisting in (i) linearizinglabelled trees into parenthesized sequences of sym-bols and (ii) using the analogical solver for wordsintroduced above.
This approach yields a faster, al-beit approximative algorithm, which makes analogi-cal inference tractable even for large tree databases.3.4 Algorithmic issuesWe have seen how to compute analogical relation-ships for features whose values are words and trees.SNPthe policeVPhave VPimpoundedNPhis car:SNPhis carVPAUXhaveVPbeen VPimpoundedPPby NPthe police::SNPthe mouseVPhas VPeatenNPthe cat:SNPthe catVPAUXhasVPbeen VPeatenPPby NPthe mouseFigure 2: Analogical proportion between trees.If we use, for trees, the solver based on tree lin-earizations, the resolution of an equation amounts,in both cases, to solving analogies on words.The learning algorithm introduced in Section 2.2is a two-step procedure: a search step and a trans-fer step.
The latter step only involves the resolu-tion of (a restricted number of) analogical equations.When x, y and z are known, solving x : y :: z :?amounts to computing the output language of thetransducer representing (y ?
z)\x: the automatonfor this language has a number of states bounded by|x |?
|y |?
|z |.
Given the typical length of words inour experiments, and given that the worst-case ex-ponential bound for determinizing this automaton ishardly met, the solving procedure is quite efficient.The problem faced during the search procedureis more challenging: given x, we need to retrieveall possible triples (y, z, t) in a finite set L suchthat x : y :: z : t. An exhaustive search requiresthe computation of the intersection of the finite-state automaton representing the output language of(L ?
L)\x with the automaton for L. Given the sizeof L in our experiments (several hundreds of thou-sands of words), a complete search is intractable andwe resort to the following heuristic approach.L is first split into K bins {L1, ..., LK}, with |Li |small with respect to |L |.
We then randomly selectk bins and compute, for each bin Li, the output lan-guage of (Li ?Li)\x, which is then intersected withL: we thus only consider triples containing at least124two words from the same bin.
It has to be noted thatthe bins are not randomly constructed: training ex-amples are grouped into inflectional or derivationalfamilies.
To further speed up the search, we also im-pose an upper bound on the degree of proportions.All triples retrieved during these k partial searchesare then merged and considered for the transfer step.The computation of analogical relationships hasbeen implemented in a generic analogical solver;this solver is based on Vaucanson, an automata ma-nipulation library using high performance genericprogramming (Lombardy et al, 2003).4 Experiments4.1 MethodologyThe main purpose of these experiments is to demon-strate the flexibility of the analogical learner.
Weconsidered two different supervised learning tasks,both aimed at performing the lexical analysis of iso-lated word forms.
Each of these tasks represents apossible instantiation of the learning procedure in-troduced in Section 2.2.The first experiment consists in computing oneor several vector(s) of morphosyntactic features tobe associated with a form.
Each vector comprisesthe lemma, the part-of-speech, and, based on thepart-of-speech, additional features such as number,gender, case, tense, mood, etc.
An (English) in-put/output pair for this tasks thus looks like: in-put=replying; output={reply; V-pp--}, where theplaceholder ?-?
denotes irrelevant features.
Lexi-cal analysis is useful for many applications: a POStagger, for instance, needs to ?guess?
the possi-ble part(s)-of-speech of unknown words (Mikheev,1997).
For this task, we use the definition of analog-ical proportions for ?flat?
feature vectors (see sec-tion 3.1) and for word strings (section 3.2).
Thetraining data is a list of fully informed lexical en-tries; the test data is a list of isolated word formsnot represented in the lexicon.
Bins are constructedbased on inflectional families.The second experiment consists in computing amorphological parse of unknown lemmas: for eachinput lemma, the output of the system is one or sev-eral parse trees representing a possible hierarchicaldecomposition of the input into (morphologicallycategorized) morphemes (see Figure 3).
This kindof analysis makes it possible to reconstruct the seriesof morphological operations deriving a lemma, tocompute its root, its part-of-speech, and to identifymorpheme boundaries.
This information is required,for instance, to compute the pronunciation of an un-known word; or to infer the compositional meaningof a complex (derived or compound) lemma.
Binsgather entries sharing a common root.input=acrobatically; output =BHHHA HHNacrobatA|N.icB|A.allyFigure 3: Input/output pair for task 2.
Bound mor-phemes have a compositional type: B|A.
denotes asuffix that turns adjectives into adverbs.These experiments use the English, German, andDutch morphological tables of the CELEX database(Burnage, 1990).
For task 1, these tables containrespectively 89 000, 342 000 and 324 000 differentword forms, and the number of features to predict isrespectively 6, 12, and 10.
For task 2, which wasonly conducted with English lemma, the total num-ber of different entries is 48 407.For each experiment, we perform 10 runs, using1 000 randomly selected entries for testing1.
Gen-eralization performance is measured as follows: thesystem?s output is compared with the reference val-ues (due to lexical ambiguity, a form may be asso-ciated in the database with several feature vectorsor parse trees).
Per instance precision is computedas the relative number of correct hypotheses, i.e.hypotheses which exactly match the reference: fortask 1, all features have to be correct; for task 2, theparse tree has to be identical to the reference tree.Per instance recall is the relative number of refer-ence values that were actually hypothesized.
Preci-sion and recall are averaged over the test set; num-bers reported below are averaged over the 10 runs.Various parameters affect the performance: k, thenumber of randomly selected bins considered duringthe search step (see Section 3.4) and d, the upper1Due to lexical ambiguity, the number of tested instances isusually greater than 1 000.125bound of the degree of extracted proportions.4.2 Experimental resultsExperimental results for task 1 are given in Tables 1,2 and 3.
For each main category, two recall and pre-cision scores are computed: one for the sole lemmaand POS attributes (left column); and one for thelemma and all the morpho-syntactic features (on theright).
In these experiments, parameters are set asfollows: k = 150 and d = 3.
As k grows, both recalland precision increase (up to a limit); k = 150 ap-pears to be a reasonable trade-off between efficiencyand accuracy.
A further increase of d does not sig-nificantly improve accuracy: taking d = 3 or d = 4yields very comparable results.Lemma + POS Lemma + FeaturesRec.
Prec.
Rec.
Prec.Nouns 76.66 94.64 75.26 95.37Verbs 94.83 97.14 94.79 97.37Adjectives 26.68 72.24 27.89 87.67Table 1: Results on task 1 for EnglishLemma + POS Lemma + FeaturesRec.
Prec.
Rec.
Prec.Nouns 71.39 92.17 54.59 74.75Verbs 96.75 97.85 93.26 94.36Adjectives 91.59 96.09 90.02 95.33Table 2: Results on task 1 for DutchLemma + POS Lemma + FeaturesRec.
Prec.
Rec.
Prec.Nouns 93.51 98.28 77.32 81.70Verbs 99.55 99.69 90.50 90.63Adjectives 99.14 99.28 99.01 99.15Table 3: Results on task 1 for GermanAs a general comment, one can note that highgeneralization performance is achieved for lan-guages and categories involving rich inflectionalparadigms: this is exemplified by the performanceon all German categories.
English adjectives, atthe other end of this spectrum, are very difficult toanalyze.
A simple and effective workaround forthis problem consists in increasing the size the sub-lexicons (Li in Section 3.4) so as to incorporate in agiven bin all the members of the same derivational(rather than inflectional) family.
For Dutch, theseresults are comparable with the results reported in(van den Bosch and Daelemans, 1999), who reportan accuracy of about 92% on the task of predictingthe main syntactic category.Rec.
Prec.Morphologically Complex 46.71 70.92Others 17.00 46.86Table 4: Results on task 2 for EnglishThe second task is more challenging since the ex-act parse tree of a lemma must be computed.
Formorphologically complex lemmas (involving affixa-tion or compounding), it is nevertheless possible toobtain acceptable results (see Table 4, showing thatsome derivational phenomena have been captured.Further analysis is required to assess more preciselythe potential of this method.From a theoretical perspective, it is important torealize that our model does not commit us to amorpheme-based approach of morphological pro-cesses.
This is obvious in task 1; and even iftask 2 aims at predicting a morphematic parse of in-put lemmas, this goal is achieved without segment-ing the input lemma into smaller units.
For in-stance, our learner parses the lemma enigmaticallyas: [[[.N enigma][.A|N ical]]B|A.
ly], that is with-out trying to decide to which morph the orthographict should belong.
In this model, input and outputspaces are treated symmetrically and correspond todistinct levels of representation.5 Discussion and future workIn this paper, we have presented a generic analog-ical inference procedure, which applies to a widerange of actual learning tasks, and we have detailedits instantiation for common feature types.
Prelimi-nary experiments have been conducted on two mor-phological analysis tasks and have shown promisinggeneralization performance.These results suggest that our main hypothesesare valid: (i) searching for triples is tractable evenwith databases containing several hundred of thou-sands instances; (ii) formal analogical proportionsare a reliable sign of deeper analogies between lin-126guistic entities; they can thus be used to devise flex-ible and effective learners for NLP tasks.This work is currently being developed in variousdirections: first, we are gathering additional experi-mental results on several NLP tasks, to get a deeperunderstanding of the generalization capabilities ofour analogical learner.
One interesting issue, notaddressed in this paper, is the integration of vari-ous forms of linguistic knowledge in the definitionof analogical proportions, or in the specification ofthe search procedure.
We are also considering al-ternative heuristic search procedures, which couldimprove or complement the approaches presented inthis paper.
A possible extension would be to defineand take advantage of non-uniform distributions oftraining instances, which could be used both duringthe searching and ranking steps.
We finally believethat this approach might also prove useful in otherapplication domains involving structured data andare willing to experiment with other kinds of data.ReferencesDavid W. Aha.
1997.
Editorial.
Artificial IntelligenceReview, 11(1-5):7?10.
Special Issue on Lazy Learn-ing.Gavin Burnage.
1990.
CELEX: a guide for users.
Tech-nical report, University of Nijmegen, Center for Lexi-cal Information, Nijmegen.Walter Daelemans, Antal Van Den Bosch, and Jakub Za-vrel.
1999.
Forgetting exceptions is harmful in lan-guage learning.
Machine Learning, 34(1?3):11?41.B.V.
Dasarathy, editor.
1990.
Nearest neighbor (NN)Norms: NN Pattern Classification Techniques.
IEEEComputer Society Press, Los Alamitos, CA.Brian Falkenheimer and Dedre Gentner.
1986.
Thestructure-mapping engine.
In Proceedings of the meet-ing of the American Association for Artificial Intelli-gence (AAAI), pages 272?277.Dedre Gentner, Keith J. Holyoak, and Boicho N.Konikov, editors.
2001.
The Analogical Mind.
TheMIT Press, Cambridge, MA.Douglas Hofstadter and the Fluid Analogies Researchgroup, editors.
1995.
Fluid Concepts and CreativeAnalogies.
Basic Books.Keith J. Holyoak and John E. Hummel.
2001.
Under-standing analogy within a biological symbol system.In Dedre Gentner, Keith J. Holyoak, and Boicho N.Konikov, editors, The analogical mind, pages 161?195.
The MIT Press, Cambridge, MA.Yves Lepage.
1998.
Solving analogies on words: Analgorithm.
In Proceedings of COLING-ACL ?98, vol-ume 2, pages 728?735, Montre?al, Canada.Yves Lepage.
1999a.
Analogy+tables=conjugation.In G. Friedl and H.G.
Mayr, editors, Proceedings ofNLDB ?99, pages 197?201, Klagenfurt, Germany.Yves Lepage.
1999b.
Open set experiments with directanalysis by analogy.
In Proceedings of NLPRS ?99,volume 2, pages 363?368, Beijing, China.Sylvain Lombardy, Raphae?l Poss, Yann Re?gis-Gianas,and Jacques Sakarovitch.
2003.
Introducing Vaucan-son.
In Proceedings of CIAA 2003, pages 96?107.Andrei Mikheev.
1997.
Automatic rule induction forunknown word guessing.
Computational Linguistics,23(3):405?423.Vito Pirrelli and Franc?ois Yvon.
1999.
Analogy in thelexicon: a probe into analogy-based machine learningof language.
In Proceedings of the 6th InternationalSymposium on Human Communication, Santiago deCuba, Cuba.Tony A.
Plate.
2000.
Analogy retrieval and processingwith distributed vector representations.
Expert sys-tems, 17(1):29?40.Jacques Sakarovitch.
2003.
Ele?ments de the?orie des au-tomates.
Vuibert, Paris.Royal Skousen.
1989.
Analogical Modeling of Lan-guage.
Kluwer, Dordrecht.Nicolas Stroppa and Franc?ois Yvon.
2005.
Formalmodels of analogical relationships.
Technical report,ENST, Paris, France.Paul Thagard, Keith J. Holoyak, Greg Nelson, and DavidGochfeld.
1990.
Analog retrieval by constraint satis-faction.
Artificial Intelligence, 46(3):259?310.Antal van den Bosch and Walter Daelemans.
1999.Memory-based morphological processing.
In Pro-ceedings of ACL, pages 285?292, Maryland.D.
Randall Wilson and Tony R. Martinez.
2000.
Reduc-tion techniques for instance-based learning algorithms.Machine Learning, 38(3):257?286.Franc?ois Yvon.
1999.
Pronouncing unknown words us-ing multi-dimensional analogies.
In Proc.
Eurospeech,volume 1, pages 199?202, Budapest, Hungary.Franc?ois Yvon.
2003.
Finite-state machines solvinganalogies on words.
Technical report, ENST.127
