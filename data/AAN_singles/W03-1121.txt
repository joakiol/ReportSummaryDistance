ABSTRACTNamed entity recognition is important in sophisticated informationservice system such as Question Answering and Text Mining sincemost of the answer type and text mining unit depend on the namedentity type.
Therefore we focus on named entity recognition model inKorean.
Korean named entity recognition is difficult since each wordof named entity has not specific features such as the capitalizingfeature of English.
It has high dependence on the large amounts ofhand-labeled data and the named entity dictionary, even thoughthese are tedious and expensive to create.
In this paper, we deviseHMM based named entity recognizer to consider various contextmodels.
Furthermore, we consider weakly supervised learningtechnique, CoTraining, to combine labeled data and unlabeled data.Keywords : Korean Named Entity, HMM, Co-Training1.
IntroductionNamed entity(NE) recognition is important for recentsophisticated information service such as question answeringand text mining since it recognizes the words to present thecore information in text.
In particular, the NE recognizer isimportant module in the well-known question answeringsystems such as FALCON, IBM[3][10].
NE recognizer iswell suited for the recognition of answer type which can beequal to the NE type or not.
Although an answer is notexactly matched to the NE, these two types can be mappingto each other by using WordNet[3].NE recognition can be explained with two steps, NEdetection and NE classification.
Whereas NE detection is tocatch the named entities in the text , NE classification is toclassify NE into person, organization or location.
In Korean,NE detection is difficult since each word of name entity hasnot specific features such as the capitalizing feature ofEnglish.
It has high dependence on the large amounts ofhand-labeled data and the NE dictionary, even though theseare tedious and expensive to create.In the case of NE classification, NE can be classified withthe clues such as inner word and context word.
Althoughthese clue words present the feature of NE type, it can beused in detecting the NE since the contained word andcontext word can be used in determining the boundary of NE.However, the clue words can provoke ambiguity todetermine the NE type since various NEs  can share  the sameclue word.
Therefore, we devise the statistical model basedNE recognizer which can unify the detection andclassification.Furthermore, we consider unlabeled data based statis ticallearning to extend the initial seed data.
The weaklysupervised learning technique is Co-Training method.
In thispaper, we describe the HMM based Korean NE recognitionand Co-Training method for HMM based boosting.2.
The ProblemNE dictionary is not enough to cover all of the NEs sincethere are a few types of NEs besides single word.
We classifyKorean NE into three types.
The first is single word type, thesecond is compound noun type, and the third is noun phrasetype.
The single word type is usually single noun.
Thesecond type, compound noun, is composed of a few wordsand affix.
The third type can have grammatical morphemesbesides nouns.
The example is described in Figure 1.
Itdescribes NEs with PERSON(PER), LOCATION (LOC),and ORGANIZATION(ORG).
In each type, secondsentence is the result of morphological analysis.The example shows the diversity of the Korean NE type.In the single word type, if the dictionary has the word, thenthe NE could be detected easily.
However, compound nountype and noun phrase type require a tremendous number ofentries in the dictionary.
Moreover, in Korean these kinds ofNE types are used differently in each people .
Therefore, itKorean Named Entity Recognition using HMM and CoTraining ModelEuisok Chung,  Yi-Gyu Hwang,  Myung-Gil JangElectronics and Telecommunications Research Institute161, Kajong-Dong, Yusong-Gu, Daejon, 305-350, KOREA{eschung, yghwang, mgjang}@etri.re.krshould use the clue word to recognize NE which is shown asthe compound noun or noun phrase type.
This clue word,which is context or inner word of NE, can be used in NEdetection and classification, but we found that the clue wordcan provoke another problem which is the ambiguity; thusdifferent types of NEs  can share the same clue word.
Whichmeans that the clue word dictionary cannot be the uniquesolution.Single word type??????
<ORG> ??
?<ORG> <PER> ??
?<PER> ??
?<ORG> ?
?/nc ?/sn<ORG> <PER> ?/nc ?
?/nc<PER> ?
?/nc ?/jc<ORG>jeonnam dae<ORG> <PER>kim - yeong  yong<PER> nunCompound noun type- 11?
<LOC> ??
???
???
????
?<LOC> ?
?11/nn ?/nb <LOC>?
?/nc ??
?/nc ??
?/nc ????
?/nc<LOC> ?
?/jc11 il  <LOC>seoul  hannamdong  hayat  grandvolum<LOC> -eseoNoun phrase type??
<LOC> ???
????
?
?<LOC> ???
?/nc <LOC>?
?/nc ?/jm ?/nc ?
?/pv ?/em ?
?/nc<LOC> ?
?/jccafe <LOC> syagal - eui nun - naeri - neun  maeul <LOC> -eseoFigure 1.
Named entity example1Consequently, we suggest three approaches for NErecognition.
The first is feature dictionary based approachwhich classify the clue words and generate feature types ofthe context or inner word of NE.
The second is statisticalapproach which needs named entity tagged corpus andcontext  model to recognize NE.
The third is unlabeled databased boosting approach since statistical approach, whichneeds hand-labeled NE tagged corpus, cannot avoid datasparseness.3.
Related WorksStatistical approach in NE recognition can be classifiedinto supervised learning and weakly supervised learning.Supervised learning is based on labeled data.
On the otherhand, weakly supervised method is the learning approach tocombine labeled data and unlabeled data.
From thesupervised learning point of view, the most representativeresearch is HMM based NE recognition.
It builds various1 nc(noun), pv(verb), pa(adjective), px(auxiliary verb),co(copula), mag(general adverb), mj(conjunctiveadverb), m(adnoun), xs (noun-derivational suffix),xsv(verb-derivational suffix), xsm (adjective-derivational suffix ), jc(case particle), j(auxiliaryparticle), jj(conjunctive particle), jm(adnom nal CaseParticle), ep(Prefinal Ending)bigram models of NEs and predicts next NE type with theprevious history, lexical item and NE type.
Using simpleword feature Bikel shows F-measure 90% in English [4].Zhou?s HMM-based chunk tagger approach adopt moredetailed feature than Bikel?s, and show F-measure 94.3%[5].In this paper, when we designed feature model, we followedthe HMM-based chunk tagger approach considering theproperty of Korean NE.Recently there have been many researches in weaklysupervised learning technique to combine labeled data andunlabeled data.
Co-Training method Blum is most famousapproach to boost the initial learning data with unlabeleddata[2].
Blum showed that using a large unlabeled data toboost performance of a learning algorithm could be used toclassify web pages when only a small set of labeledexamples is available [2].
Nigam demonstrated that whenlearning from labeled and unlabeled data, algorithmsexplicitly leveraging a natural independent split of thefeatures outperform algorithms that do not[7][8].
Collins andSinger showed that the use of unlabeled data can reduce therequirements for supervision to just 7 simple ?seed?
rules [9].In addition to a heuristic based on decision list learning, theyalso presented a boosting-like framework that builds on ideasfrom Blum[2].4.
Named entity recognizerIn this paper, we propose the Korean NE recognizingmethodology.
It is based on the feature model, HMM basedstatistical model and Co-Training based boosting model.4.1 Feature modelNE recognition depends on various clues to distinguisheach type.
The inside and outside properties of NE could bethe clues which can be composed of a few clue class.
Theclass consists of ?character feature?, ?named entity dictionary?,?inner word?, and ?context word?.
It is described in Table 1.
(1) ?character feature?
shows the digit or Chinese feature inKorean NE.
Digit feature is used to recognize MONEY,TIME, and others.
(2) ?named entity dictionary?
means thatwe should build NE dictionaries.
The dictionaries arecomposed of DATE, PERSON, LOCATION, andORGANIZATION.
(3) ?inner word?
consists of suffix wordand constituent word of NE.
(4) ?context word?
is the wordset adjacent to NE.
These feature values are constructedmanually  and are used with history in annotating NE type.As stated above, all the feature classes have ambiguity sinceone feature word can be another feature type.
Therefore, wecannot recognize NE with only feature dictionaries.Type Feature valueDIGIT  OneDigitNumber,  TwoDigitNumber,FourDigitNumDIGIT&LETTER  ContainsDigitAndPeriod, ContainsDigitAndLetterCHINESE  OneChinese, ThreeChinese, ContainsOneChineseAndLetterALPHA BET  ContainsAlphaAndLetter, AllCapitalizationcharacterfeatureLETTER  TreeLettersDATE  DicDATEPERSON  DicPERSONLOCATION  DicLOCnamedentitydictionaryORGANIZATION  DicORGPERCENT  SuffixPERCENTMONEY  SuffixMONEY,  SuffixCURRENCYTIME  SuffixTIME,  PeriodTIMEDATESuffixDATE,  WeekDATE,SeasonDATE,  PeriodDATE,YearDATE,  OthersDATE.LOCATION  DistrictSuffixLOC,  SuffixLOCinnerwordORGANIZATION  SuffixORGPERSON  PositionPERSON,  RelationPERSON,JobPERSONLOCATION  ClueLOCDATE  ClueDATETIME  ClueTIMEPHONE  CluePHONEORGANIZATION  ClueORGPERCENT  CluePERCENTMONEY  ClueMONEYADDRESS  ClueADDRESScontextwordQUANTITY  ClueQUANTITYTable 1.
Feature type4.2 HMM based statistical modelNE recognizer should adopt statistical model since it isimpossible to construct the dictionary having all of the NEentries and moreover, clue word dictionary can provokeambiguity.
For instance, proper noun such as person namecreates everyday.
It is unknown word problem.
In the featuremodel, some feature  classes can share the same clue words.Simply, both DATE and TIME can have the DIGITcharacter feature.
Therefore, we adopt statistical model forNE recognition.
For the statistical approach, we construct NEtagged documents, design NE context model, and suggestforward-backward view based boosting approach.We build 300 named entity tagged documents in thenewspaper article domains such as economy, performanceand travel.
The labeled data is  tagged by using tagging tools.We attached only NE tags to the text  and do not considermorphological information because Korean morphologicaltag is various to the analyzer.
Furthermore, we buildstatistical information extractor to learn the NE distributionalinformation.
The labeled data is used in constructing NEstatistical data.To build NE context model for statistical NE recognizer,we analyze 201 NEs in labeled documents.
From the threepoints of view, word feature, inner word feature, and contextword feature, we analyze NE examples.
From the analysis ,word feature can be classified into single word  andcompound word.
Inner feature has property of full string andinner word.
From the context word feature, we recognizedthree kinds of features such as root, adjacent morpheme andno context.
The result of analysis is described in Table 2.Finally, we can build four types of NE context model such aslexical model, feature model, POS(part of speech) model androot model.Single Noun 129 64.2 % Wordfeature Compound Noun 72 35.8 %full string 118 58.7 % InnerWordFeature Inner word 83 41.3 %Left root of a word 14 7.0 %Right root of a word 64 31.8 %Morpheme left adjacent toname entity 38 18.9 %Morpheme right adjacent toname entity 112 55.7 %ContextWordFeatureno context 35 17.4 %Table 2.
Named entity analysisWhereas the rule-based approach needs enormous hand-crafted rules, statistical model has the advantages ofsimplicity, expansity and robustness in the named entitymodel.
The most representative approach of statistical modelis HMM based approach.
To adopt HMM based model, wedefine HMM state and build NE context model which cancover various NE contextual information.For HMM based approach, HMM state should bedefined.
HMM state is the type of NE constituent.
Thus,S_LOC is the state wh ich can be the first lexical item ofLOC typed named entity.
C_LOC is the middle state of theNE.
E_LOC is the final morpheme.
In the case of U_LOC,single word is the NE word.
For example, location name?Inchon International Airport?
can be tagged with?Inchon/S_LOC International /C_LOC Airport/ E_LOC?and another named entity ?Seoul?
can be labeled with?Seoul/U_LOC?.
The HMM state is described in Table 5.The NE context model is composed of four types such asmorp(morphology), root, POS and feature .
Through this  NEcontext model, the statistical data is learned from the taggedcorpus, and is used in computing probability of predictingHMM state to lexical item.
In the case of feature type,?Indiana?
has ?DicLOC?
since it is discovered in LOCdictionary and ?professor?
is allocated with ?Position-PERSON?
since it is used with position clue word.
Thecontext model and example is presented in table 3.We designed the NE context model with the divided viewtypes such as forward/backward views which means left-right NE context view.
In each view type, the probability iscomputed with the product of state transitional probabilityand lexical probability.
In forward view, state transitionalprobability is Pr(Si|Si-1, mi-1) which predict ith HMM state Siwith i-1th state Si-1 and morpheme mi-1, and lexical probabilityis Pr(mi|Si, mi-1) which predict ith morpheme mi with ith state Siand morpheme mi-1.In table 4, the state of morpheme mi-1 ??
eui?, the stateSi, is ?NE_U?
and next Si-1 is ?-?.
Which means that statetransitional probability can be computed by count(-, NE_U,?
eui)/count(-, ?
eui).
In the case of lexical probability, itis computed by count(NE_U,?
eui, ?????stiglitz)/count(NE_U,?
eui).
The difference of forwardview and backward view can be explained with statetransitional probability.
Whereas forward view computescurrent state probability with pre-state, backward viewcomputes current state probability with next-state.
Thusforward view considers left contextual information.
On theother hand, backward view considers right context.For the statistical approach, we build labeled data forsupervised learning and propose four typed NE contextmodel which considers left -right contextual information.Furthermore, we adopt smoothing model based on themodified Kneser-Ney smoothing technique[6] since HMMbased approach needs smoothing technique for better result.After allocating the lexical probability and state transitionalprobability to the HMM state which is coupled with themorpheme of sentence, the recognition is processed byViterbi algorithm.
Then, NE is recognized in the inputsentence.4.3 CoTraining based boosting modelCo-Training method is most famous approach to boost theinitial learning data with unlabeled data.
In this paper, wepropose the method to apply Co-Training method to theHMM based statistical approach.
The main idea is to divideview type of the context model into forward view andbackward view.
Simply the forward view?s output, which isthe result of NE recognition to the unlabeled data, is used forinput data in the backward view and vise verse.
From theiteration of the Co-Training procedure, both views  couldboost each other, which means that the both statistical datacould increase by using unlabeled data.
HMM basedCoTraining approach is described in Figure 2.????(Indiana)??(Gary)??(Birth)?
< i-1>eui?????
< i >(Stigritz)??
< i+1>(Professor)?nun TypeNE_S NE_E - - NE_U  - -POS  Nc nc nc jm nc nc jxMORP  ????
??
??
?
?????
??
?ROOT  Root Root Root - Root Root -FEATURE  DicLOC - - - - PositionPERSON -Table 3.
Context model typeType View type Statistical Model ExampleForward Pr(si|si-1,mi-1) x Pr(mi|si,mi-1)c(-, NE_U, ?
eui) / c(-, ?
eui)x c( NE_U, ?
eui, ?????
Stigritz) / c( NE_U, ?
eui)MORPBackward Pr(si|si+1,mi+1) x Pr(mi|si,mi+1)c( NE_U, -, ??
professor) / c(-, ??
professor)x c( NE_U, ?????
Stigritz, ??
professor) / c(NE_U, ??
professor)Table 4.
Forward/Backward model for CoTrainingNE type HMM statePERSON  S_PER, C_PER, E_PER, U_PERLOCATION  S_LOC, C_LOC, E_LOC, U_LOCORGANIZATION  S_ORG, C_ORG, E_ORG, U_ORGDATE  S_DATE, C_DATE, E_DATE, U_DATETIME  S_TIME, C_TIME, E_TIME, U_TIMEPERCENT  S_PERCENT, C_PERCENT, E_PERCENT, U_PERCENTMONEY  S_MONEY, C_MONEY, E_MONEY, U_MONEYQUANTITY  S_QUANT, C_QUANT, E_QUANT, U_QUANTTable 5.
HMM state for NE context modelFigure 2.
HMM based CoTraining approach(1) CurrentPath is ForwardView, k-th rounds(2) Unlabeled text random sampling(3) if CurrentPath = ForwardViewthen(3-1) ForwardView HMM based NE taggingelse(3-2) BackwardView HMM based NE taggingendif(4) extract n-best tagging result(5) if CurrentPath = ForwardViewthen(5-1) extract BackwardView based statistical datafrom n-best taggin result(5-2) boost Backward data(5-3) CurrentPath = BackwardViewelse(5-4) extract ForwardView based statistical datafrom n-best taggin result(5-5) boost Forward data(5-6) CurrentPath = ForwardViewendif(6) if this round is k-th rounds ?then CoTraining exitelse goto (2)Figure 3.
CoTraining ProcedureThe CoTraining algorithm which boosts HMM based NEstatistical model is described in Figure 3.
Here the procedureof CoTraining is shown for boosting between dividedstatistical models.
In first step, the current view type and thenumber of times of learning round are determined(1) sinceCoTraining approach, which is based on feature redundantprinciple, divides the learning model and reflects onelearning result to the other learning model.
The next step israndom sampling of unlabeled text data(2).After random sampling, the NE statistical data should beextracted from the sampling data.
At this time the next stepdepends on the current view type which can be Forward-View or BackwardView.
If current learning view is Forward-View, the next step is forward model based NE taggingtask(3-1), otherwise, the current learning view is Backward-View, the next step is backward view based task(3-2).
Afterthat, from the result of NE tagging, the n-th best taggingresults are selected(4) and added to the learning data.From the first step till now, NE tagging using unlabeleddata is processed, and the tagging result prepares for boostingother view type.
If CurrentPath is ForwardView(5),backward view data is extracted from the tagging result(5-1)and boost backward view data(5-2), and then CurrentPathchange to BackwardView(5-3).
Otherwise, CurrentPath isBackwardView(5), forward view data is extracted from thetagging result(5-4) and boost forward view data(5-5), andthen CurrentPath change to ForwardView(5-6).
Finally, theround is checked whether it is over the pre -defined iterationtime or not(6).
If it pass the time, the procedure ends,otherwise the random sampling step repeats.5.
ExperimentsWe evaluate named entity recognition with two kinds ofexperiments.
One is the performance of named entityrecognition which unified morp model and feature model tolearn statistical information.
The other experiment is aboutCoTraining performance.5.1 Name d Entity Recognition TestWe evaluate NE recognition with morp model and featuremodel since POS statistical data, which is extracted fromlabeled corpus, has some POS tagging error, and root modelcannot be implemented due to the difficult to determine whatthe root is.
Therefore, in this paper we suggest the evaluationof the morp model, feature model and morp/feature modelconsidering forward/backward view: (1) morp model basedforward view [M/F], (2) morp model based backward view[M/B], (3) morp/feature model based forward view [MF/F],(4) morp/feature model based backward view [MF/B], (5)morp/feature model based forward/backward view [MF /FB],which combination of forward/backward view is based onforward-backward algorithm.
We give 0.93 weight to morpmodel and give 0.07 weight to feature model.
It is optimizedfrom many experiments.With 300 NE tagged documents, we train the recognizerwith 270 documents which is compose of 90 economy, 90HMM ModelForward ViewBackward ViewStatisticalDataStatisticalDataforward Viewoutputbackward ViewoutputUnlabeled dataLabeleddataperformance, and 90 travel articles.
Other 30 documents,which is not used in training, are used as test data.
Test datahas three types such as untrained 10 economical documents(N10), untrained 30 documents(N30), and trained 270documents(T270).The result of the experiment is described in Table 6.
Fromthe result, we find that the best result of economy 10 test datais morp/feature based backward view(MF/B) type with F-measure 0.67 considering all NE types.
If we consideredonly PLO(Person/Location/Organization) type, MF/FB typeis the best with F-measure 0.77.
The first reason that PLOrecognition is better than other types is that the PLO traineddata is more abundant.
Figure 4 shows that the PLO type is alarge number in 300 labeled documents.
The second reasonis that the statistical model is not appropriate to some kinds ofNE types such as DATE, QUANTITY.
These type is moreappropriate when the pattern based approach is adopted.
Inthe future, we test the NE recognizer with balanced traineddata in each NE types.PERLOC ORG DATE PCTTIMEQUATMONEY0.005.0010.0015.0020.0025.0030.0035.0040.00Figure 4.
Distribution of NE types in 300 NElabeled documents5.2 CoTraining TestWe evaluate Co-Training for the NE recognition using anunlabeled economy domain newspaper articles(39,480articles).
For the training data, we train the NE recognizerwith 270 labeled articles  and use 10 evaluate articles as testdata in each Co-Training iteration.
In each training step, weincrease the number of the NE labels from the high rankedNE tagging results in proportion to the training data.
We testCoTraining with morp model which is divided into forwardview and backward view.
After 145 iteration, backward viewF-measure decreases from 0.615 to 0.6, but forward view F-measure increases from 0.558 to 0.57.0.520.530.540.550.560.570.580.590.60.610.620.631 15 29 43 57 71 85 99 113 127 141Morp Model based BackwardMorp Model based ForwardFigure 5.
CoTraining result6.
ConclusionIn this paper, we suggest HMM based NE recognition andNAME ENTITY TYPE TOTALNumofTestDoc.
PERSON LOCACTION ORGANIZATION DATE PERCENT TIME QUANTITY MONEY Precision Recall F-measureT270 0.97F 0.82F 0.87F 0.79F 0.97F 0.74F 0.71F 0.91F 0.80 0.87 0.84N30 0.33F 0.47F 0.33F 0.63F 0.71F 0.51F 0.55F 0.57F 0.40 0.65 0.49M/FN10 0.56F 0.75F 0.50F 0.51F 0.21F 0.0F 0.0F 0.0F 0.46 0.70 0.55T270 0.93F 0.87F 0.85F 0.73F 0.96F 0.68F 0.71F 0.77F 0.80 0.87 0.83N30 0.30F 0.47F 0.31F 0.55F 0.71F 0.32F 0.50F 0.42F 0.35 0.63 0.45M/BN10 0.46F 0.83F 0.47F 0.53F 0.74F 0.0F 0.0F 0.33F 0.51 0.75 0.61T270 0.78F 0.70F 0.68F 0.43F 0.28F 0.54F 0.33F 0.31F 0.54 0.74 0.62N30 0.67F 0.62F 0.57F 0.33F 0.0F 0.44F 0.44F 0.28F 0.47 0.64 0.54MF/FN10 0.80F 0.85F 0.64F 0.24F 0.0F 0.0F 0.0F 0.0F 0.54 0.76 0.63T270 0.79F 0.68F 0.73F 0.61F 0.94F 0.65F 0.53F 0.68F 0.67 0.70 0.69N30 0.71F 0.55F 0.54F 0.53F 0.97F 0.39F 0.45F 0.37F 0.53 0.58 0.55MF/BN10 0.73F 0.71F 0.68F 0.50F 1.0F 0.0F 0.0F 0.46F 0.69 0.65 0.67N30 0.68F 0.55F 0.62F 0.39F 0.0F 0.48F 0.27F 0.06F 0.45 0.58 0.51 MF/FBN10 0.81F 0.79F 0.73F 0.38F 0.0F 0.0F 0.0F 0.0F 0.60 0.73 0.66Table 6.
Named entity recognition evaluationboosting technique.
Through this research,  we met sometechnical problems such as NE context model unification,unbalanced labeled corpus, and boosting degradation.
(1) Weconsider HMM based NE recognition with four types NEcontext models which are derived from the analysis of NElabeled data.
However, we cannot unify all of the models inunique way since the weighted integration of the models donot guarantee the good performance.
(2) In using the labeleddata, we meet the problem that the recognition of NE typesdepends highly on the size of learning data.
(3) In HMMbased CoTraining, the test result shows that high-performance model enhance low-performance model buthigh-performance model decrease step by step.Finally, we conclude that (1) unification issue of variouscontext models can be resolved with Maximum Entropymodel which can combine diverse forms of contextualinformation in a principled manner, (2) unbalanced labeledcorpus issue may be resolved by gathering contextualinformation independently from the labeled corpus.
It makesit possible to balance learning data in each type, and (3)degradation of the boosting approach is not difficult problemsince the boosting step in each round can be controlled withpre-test.References[1] A. Borthwick.
A Japanese named entiry recognizerconstructed by a non-speaker of Japanese.
In Proceedings ofthe IREX Workshop, pages 187-193, 1999.
[2] A. Blum and T. Mitchell.
Combining labeled andunlabeled data with cotraining.
In Proceedings of the 11thAnnual Conference on Computational Learning Theory,pages 92-100, 1998.
[3] A. Ittycheriah, M. Franz, W. Zhu, and A. Ratnaparkhi,IBM?s Statistical Question Answering System, InProceedings of the Text Retrieval Conference TRECT-9,2000.
[4] D. M. Bikel, S. Miller, R. Schwartz, R. Weishedel,Nymble : a high-performance learning named-finder, InProceedings of the Fifth Conference on Applied NaturalLanguage Processing, 1997[5] G.. Zhou, J. Su, Named Entity Recognition using anHMM-based Chunk Tagger, In 40th Annual Meeting of theAssociation for Computational Linguistics, 2002.
[6] F. James, Modified Kneser-Ney Smoothing of n-gramModels.
Technical Report TR00-07, RIACS, USRA, 2000.
[7] K. Nigam and R. Ghani.
Analyzing the effectiveness andapplicability of co-training.
In Proceedings of the NinthInternational Conference on Information and KnowledgeManagement, 2000.
[8] K. Nigam and R. Ghani.
Understanding the Behavior ofCo-training.
In Proceedings of KDD-2000 Workshop onText Mining, 2000.
[9] M. Collins and Y.
Singer.
Unsupervised models fornamed entity classification.
In Empirical Methods in NaturalLanguage Processing and Very Large Corpora, 1999.
[10] S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M.Surdeanu, R. Bunescu, R. Girju, V. Rus and P. Morarescu,FALCON: Boosting Knowledge for Answer Engines, InProceedings of the Text Retrieval Conference TRECT-9,2000.
