Towards a Road Map on Human Language Technology:Natural Language ProcessingEditors: Andreas Eisele, Dorothea Ziegler-EiseleVersion 2 (March 2002)AbstractThis document summarizes contributions and discussions from two workshops that took placein November 2000 and July 2001.
It presents some visions of NLP-related applications thatmay become reality within ten years from now.
It investigates the technological requirementsthat must be met in order to make these visions realistic and sketches milestones that mayhelp to measure our progress towards these goals.1.
IntroductionScope of this DocumentOne of the items on ELSNET's agenda for the period 2000-2002 is to develop views on andvisions of the longer-term future of the field of language and speech technologies andneighboring areas, also called ELSNET's Road Map for Human Language Technologies.
As afirst step in this process, ELSNET's Research Task group is organizing a series ofbrainstorming workshop with a number of prominent researchers and developers from ourcommunity.
The first one of these workshops took place in November 2000 under the generalmotto ?How will language and speech technology be used in the information world of 2010?Research challenges and infrastructure needs for the next ten years".
The second one was co-organized in July 2001 by ELSNET and MITRE as part of ACL-2001 and had the somewhatmore specific orientation on ?Human Language Technology and Knowledge Management(HLT-KM)?.
This workshop brought together more than 40 researchers from industry andacademia and covered a considerable range of topics related to KM and HLT in general.This paper aims at summarizing and organizing material from both workshops, butconcentrates on applications and technologies that involve NLP, i.e.
the processing of writtennatural language, as speech-related technologies and new models of interactivity have alreadybeen covered in documents presented around the first workshop.
In the discussion of questionanswering and summarization, vision papers and roadmaps compiled by researchers in the USand published by NIST have been taken as an additional source of inspiration.The Growing Need for Human Language TechnologyNatural language is the prime vehicle in which information is encoded, by which it isaccessed and through which it is disseminated.
With the explosion in the quantity of on-linetext and multimedia information in recent years there is a pressing demand for technologiesthat facilitate the access to and exploitation of the knowledge contained in these documents.Advances in human language technology will offer nearly universal access to on-lineinformation and services for more and more people, with or without skills to use computers.These technologies will play a key role in the age of information and are cited as keycapabilities for competitive advantage in global enterprises.Extraction of knowledge from multiple sources and languages (books, periodicals, newscasts,satellite images, etc.)
and the fusion into a single, coherent textual representation requires notonly an understanding of the informational content of each of these documents, the removalof redundancies and resolution of contradictions.
Also, models of the user are required, theprior knowledge that can be assumed, the level of abstraction and the style that is appropriateto produce output that is suitable for a given purpose.More advanced knowledge management (KM) applications will be able to draw inferencesand to present the conclusions to the user in condensed form, but let the user ask forexplanations of the internal reasoning.
In order to find solutions for problems beyond a staticpool of knowledge, we need systems that are able to identify experts, who have solved similarproblems.
Again, advanced NLP capabilities will be required to appraise the aptitude ofcandidates from documents authored by them or describing prior performance.But also outside of KM, sophisticated applications of NLP will emerge over the next yearsand decades and find their way into our daily lives.
The range of possibilities is almostunlimited.
An important group of applications is related to electronic commerce, i.e.
newmethods to establish and maintain contact between companies and their customers.
Viamobile phones, e-mail, animated web-based interfaces, or innovative multi-channel interfaces,people will want to make use of all kinds of services related to buying and selling goods,home-banking, booking of journeys, and the like.
Also in the area of electronic learning aconsiderable growth is expected within the coming years.MultilingualityWhereas English is still the predominant language on the WWW, the fraction of non-EnglishWeb pages and sites is steadily increasing.
Contrasting earlier apprehensions, the future willprobably present ample opportunities for giving value to different languages and cultures.However, the possibility to collect information from disparate, multilingual sources alsoprovides considerable challenges for the human user of these sources and for any kind of NLPtechnology that will be employed.One of the major challenges is lexical complexity.
There will be about 200 differentlanguages on the web and thus about 40.000 potential language pairs for translation.
Clearly,it will not be possible to build bilingual dictionaries that are comprehensive both in thenumber of language pairs and in the coverage of application domains.
Instead, multilingualvocabularies need to provide mappings into language independent knowledge organizationstructures, i.e.
common systems of concepts linked by semantic relations.
However, thedefinition of such an ?interlingua?
will be difficult in cases in which languages makedistinctions of different granularity.Research Trends and ChallengesThe field of human language technology covers a broad range of activities with the goal ofenabling people to communicate with machines using natural communication skills.Although NLP can help to facilitate knowledge management, it requires a large amount ofspecialized knowledge by itself.
This knowledge may be encoded in complex systems oflinguistic rules and descriptions, such as grammars and lexicons, which are written indedicated grammar formalisms and typically require many person-years of developmenteffort.
The rules and entries in such descriptions interact in complex ways, and adaptation ofsuch a sophisticated system to a new text style or application domain is a task that requires aconsiderable amount of specialized manpower.One way to cope with the difficulties in the acquisition of linguistic knowledge was to restrictattention to shallower tasks, such as looking for syntactic ?chunks?
instead of a full syntacticanalysis.
Whereas this has proven rather successful for some applications, it obviouslyseverely limits the depth to which the meaning of a document or utterance is taken intoaccount.Another approach was to shift attention towards models of linguistic performance (whatoccurs in practice, instead of what is principally possible) and to use statistical or machinelearning methods to acquire the necessary parameters from corpora of annotated examples.These data-driven approaches offer the possibility to express and exploit gradual distinctions,which is quite important in practice.
They are not only easier to scale and adapt to newdomains, their algorithms are also inherently robust, i.e.
they can deal, to a certain extent,gracefully with errors in the input.Statistical parsers, trained on suitable tree banks, now achieve more than 90% precision andrecall in the recognition of syntactic constituents in unseen sentences from English financialnewspaper text.However, a lot of work remains to be done, and it is not obvious how the success of corpus-driven approaches can be enlarged along many dimensions simultaneously.
One challenge isthat analysis methods need to work for many languages, application domains and text types,whereas the manual annotation of large corpora of all relevant types will not be economicallyfeasible.
Another challenge is that, other than syntax, many additional levels of analysis willbe required, such as the identification of word sense, the reference of expressions, structure ofargumentation and of documents, and the pragmatic role of utterances.
Often, the theoreticalfoundation that is required before the annotation of corpora can begin is still lacking.One could say that for corpus-driven approaches the issue of scalability of the requiredresources shows up again, albeit in a somewhat different disguise.
Hence, research in NLPwill have to address this issue seriously, and find answers to the question how better tools andlearning methods can reduce the effort of manual annotation, how annotated corpora of aslightly different type could best be re-used, how data-driven acquisition processes canexploit and extend existing lexicons and grammars, and finally how analysis levels for whichthe theoretical basis is still under development could be advanced in a data-driven way.Structure of this DocumentThe remainder of this document is structured as follows.
In Chapter 2 we describe a numberof prototypical applications and scenarios in which NLP will play a crucial role.
Whereaseach of these scenarios is discussed mainly from a user?s perspective, we also giveindications, which technological requirements must be met to make various levels ofsophistication of these applications possible.
In Chapter 3, the technologies that have beenmentioned earlier are discussed in more detail, and we try to indicate which levels offunctionality may be expected within the timeframe of this study.
These building blocks arethen put into a tentative chronological order, which is displayed in Chapter 4.
Finally,Chapter 5 gives some general recommendations about beneficial measures concerning theinfrastructure for the relevant research.2.
Applications of NLPRecent developments in natural language processing have made it clear that formerlyindependent technologies can be harnessed together to an increasing degree in order to formsophisticated and powerful information delivery vehicles.
Information retrieval engines, textsummarizers, question answering and other dialog systems, and language translators providecomplementary functionalities which can be combined to serve a variety of users, rangingfrom the casual user asking questions of the web to a sophisticated, professional knowledgeworker.Though one cannot strictly separate the following applications from each other, because onecan act as a part of another, we try to dissect the large field of existing and future applicationsin the hope of making the field as a whole more transparent.Information Retrieval (IR)What is called information retrieval today is actually but a foretaste of what it should be.Current systems neither understand the information need of the user, nor the content of thedocuments in their repositories.
Instead of meaningful replies, they just return a ranked, andoften very long list of documents that are somehow related to the given query, which istypically very short.
A better name for this restricted functionality would be text retrieval.Information retrieval systems must understand a query, retrieve relevant information, andpresent the results.
Retrieved information may consist of a long document, multipledocuments of the same topic, etc and good systems should present the most importantmaterial in a clear and coherent manner.Current information retrieval techniques either rely on an encoding process using a certainperspective or classification scheme to describe a given item, or perform a superficial full-textanalysis, searching for user-specific words.
Neither case guarantees content matching.The ability to leverage advances in input processing (especially natural language queryprocessing) together with advances in content-based access to multimedia artifacts (e.g., text,audio, imagery, video) promises to enhance the richness and breadth of accessible materialwhile at the same time improving retrieval precision and recall and thus reducing the searchtime.
Dealing with noisy, large scale, and multimedia data from sources as diverse as radio,television, documents, web pages, and human conversations (e.g., chat sessions and speechtranscriptions) will offer challenges.One important part of IR would be multi-document summarization that can turn a large set ofinput documents into several different short summaries, which can then be sorted by topics orotherwise put into a coherent order.SummarizationSummarization will enable knowledge workers access to larger amounts of material with lessrequired reading time.
The goal of automatic text summarization is to take a partiallystructured source text, extract information content from it and present the most importantcontent in a condensed form in a manner sensitive to the needs of the user and task.Scalability to large collections and the generation of user-tailored or purpose-tailoredsummaries are active areas of research.The summarization can either be an extract consisting entirely of material copied from theinput, or an abstract containing material not present in the input, such as subject categories,paraphrases of content, etc.For extraction shallower approaches are possible, as frequently the sentences may beextracted out of context.
The transformation here involves selecting salient units andsynthesizing them with the necessary smoothing (adjusting references, rearranging thetext?).
Training by using large corpora is possible.Abstracts need a deeper level of analysis, the synthesis involves natural language generationand some coding for a domain is required.Depending on their function, three types of abstracts can be distinguished: An indicativeabstract provides a reference function for selecting documents for more in-depth reading.
Aninformative abstract covers all the salient information in the source at some level of detail andevaluative abstracts express the abstractor?s views on the quality of the work of the author.Characteristics for the summarization are the reduction of the information content(compression rate), the fidelity to the source, the relevance to the user?s interest, and the well-formedness regarding both to syntactic and discourse level.
Extracts need to avoid gaps,dangling anaphora, ravaged tables and lists, abstracts need to produce grammatical, plausibleoutput.Some current applications of summarization are:1.
Multimedia news summaries: watch the news and tell what happened while I wasaway2.
Physicians?
aids: summarize and compare the recommended treatments for this patient3.
Meeting summarization: find out what happened at that teleconference I missed4.
Search engine hits: summarize the information in hit lists retrieved by search engines5.
Intelligence gathering: create a 500-word biography of Osama bin Laden6.
Hand-held devices: create a screen-sized summary of a book7.
Aids for the Handicapped: compact the text and read it out for a blind personThough there are already promising approaches towards mastering all types of summaries,there are still obstacles to overcome such as the need for robust methods for the recognition ofsemantic relations, speech acts, and rhetorical structure.Question Answering  (QA)The straightest way to get access to the gigantic volume of knowledge around us is probablyasking questions by communicating with other persons, computers or machines.An important new class of systems will move us from our current form of search on the web(type in keywords to retrieve documents) to a more direct form of asking questions in naturallanguage, which are then directly responded to with an extracted or generated answer.Currently it is rather straightforward to get an answer to ?what questions?
(what is the capitalof China, what are the opening hours of the hermitage etc.
), whereas ?why questions?
(whydid the new market fail) are normally not answered by an information retrieval query, unlessthe answer happens to be present in the information database, or can be inferred afterwards bythe user from the answers she gets.In the next decade time has come to find answers to why questions from information systemsby letting the systems make the appropriate inferences.
This requires very sophisticatedautomatic reasoning methods, based on systematic extraction of information from texts,storing the information in a systematized way, which lends itself to reasoning and inferencerules that will be able to draw the proper conclusions from the knowledge stored in theinformation database.We can subdivide the long-term goal of building powerful, multipurpose informationmanagement systems for QA in simpler subtasks that can be attacked in parallel at varyinglevels of sophistication, over shorter time frames.Clearly there is not a single, archetypical user of a Q&A system.
In fact there is a fullspectrum of questions, starting with simple factual questions, which could be answered in asingle short phrase found in a single document (e.g.
?Where is the Taj Mahal??).
Next,questions like ?What do we know about Company xyz?
?, where the answer cannot be foundin a single document but will require retrieving multiple documents, locating portions ofanswers in them and combining them into a single response.
This kind of question might beaddressed by decomposing it into a series of single focus questions.Finally there are very complex questions, with broad scope, using judgment terms andneeding deep knowledge of the user?s context to be answered.
Imagine someone is watching atelevision newscast, becomes interested in a person, who appears to be acting as an advisor tothe country?s Prime Minister.
And now the person wants to know things like: ?Who is thisindividual.
What is his background?
What do we know about the political relationship of thisperson and the Prime Minister and/or the ruling party??.
The future systems that can deal withthis type of questions must manage the search in multiple sources in multiplemedia/languages, the fusion of information, resolution of conflicting data, multiplealternatives, adding interpretation, drawing conclusions.In order to realize this goal, research must deal with question analysis, response discovery andgeneration from heterogeneous sources, which may include structured and unstructuredlanguage data of all media types, multiple languages, multiple styles, formats and also imagedata i.e.
document images, photography and video.To the extent to which NLP research will learn to master the challenges of source selection,source segmentation, extraction, and semantic integration across heterogeneous sources ofunstructured and semi-structured data, NLP technology will help us to reduce the time,memory, and attention required to sift through many returned web pages from a traditionalsearch by providing direct answers to questions.Semantic WebThe standardization committee for the WWW (called W3C) expects around a billion webusers by 2002 and an even higher number of available documents.
However, this success andexponential grow makes it increasingly difficult to find, to access, to present, and to maintainthe information of use to a wide variety of users.The semantic web will bring structure to the meaningful content of Web pages, creating anenvironment where software agents roaming from page to page can readily carry outsophisticated tasks for users.The semantic web is not a separate web but an extension of the current one, in whichinformation is given well-defined meaning better enabling computers and people to work incooperation.
With the help of ontologies large amounts of text can be semantically annotatedand classified.Currently pages on the web use representations rooted in format languages such as HTML orSGML.
The information content, however, is mainly presented by natural language.
Thus,there is a wide gap between the information available for tools that try to address theproblems above and the information kept in human readable form.The semantic web will provide intelligent access to heterogeneous and distributed informationenabling software agents to mediate between the user needs and the available informationsources.The first steps in weaving the semantic web into the structure of the existing web are alreadyunder way.
In the near future, these developments will usher in significant new functionalityas machines become much better able to process and ?understand?
the data that they merelydisplay at present.What is required: creation of a machine understandable semantics for some or all of theinformation presented in the WWW i.e.?
Developing languages for expressing machine understandable meta-information fordocuments, in the line of RDF, DAML, and similar proposals.?
Developing terminologies (i.e., name spaces or ontologies) using these languages andmaking them available on the web.?
Integrating and translating different terminologies?
Developing tools that use such languages and terminologies to provide support infinding, accessing, presenting and maintaining information sources.Developing such languages, ontologies and tools is a wide-ranging problem that touches onthe research areas of a broad variety of research communities.Creation of the relevant tools will require a better knowledge of what the users want to knowfrom websites, i.e.
these developments need to be based on a user-centered process view.Another crucial issue will be: ?Who is going to populate the semantic web??
The semanticmarkup that is required by automated software agents needs to be very easy to create andsupporting tools need to be provided, otherwise this wonderful idea will not have significantimpact for a long time.
Advanced NLP technology that can ?guess?
the correct semanticannotation and propose suitable markup semi-automatically will enable conformance to theneeds of software agents with minimal manual effort.Dialogue SystemsNo matter if people want to buy something, find or use a service or just need information,dialog systems promise user-friendly and effective ways to achieve these goals, even for firsttime users.Despite the apparent resemblance to QA systems, there are several specific problems to besolved concerning dialogue modality and structure.
Input to a dialog system might be viakeypad, voice, pointing device, combinations thereof, or other channels, so all errors andincompleteness of spontaneous natural language will show up.
In contrast to QA systems,there will be mixed initiatives of speaker and system and the scope is much wider if we takeinto account that the focus during natural dialogue may often change.
Also, the utterancemade during a dialog can only be correctly interpreted based on the dialog context and themutual knowledge that has been accumulated before it was made.In future we require systems that can support natural, mixed initiative human computerinteraction that deals robustly with context shift, interruptions, feedback and shift of locus orcontrol.Open research challenges include the ability to tailor flow and control of interactions andfacilitate interactions including error detection and correction tailored to individual physical,perceptual and cognitive differences.Motivational and engaging life-like agents offer promising opportunities for innovation.Agent/user modeling: Computers can construct models of user beliefs, goals and plans as wellas models of users?
individual and collective skills by processing materials such as documentsor user interactions/conversations.
While raising important privacy issues, modeling users orgroups of users unobtrusively from public materials or conversations can enable a range ofimportant knowledge management capabilitiestracking of user characteristic skills and goals enhances interaction as well as discovery ofexperts by other users or agentsA central problem for the development of dialogue systems is the fact that contemporarylinguistics is still struggling to achieve a genuine integration of semantics and pragmatics.
Asatisfactory analysis of dialogue requires in general both semantic representation i.e.representation of the content of what the different participants are saying and pragmaticinformation, i.e.
what kinds of speech acts they are performing (are they asking a question,making a proposal?
)Analysis of a dialog needs to explain the purpose behind the utterances it consists of.Determining the semantic representation of an utterance and its pragmatic features must ingeneral proceed in tandem.
A dialogue system identifying the relevant semantic andpragmatic information will thus have to be based on a theory in which semantics andpragmatics are both developed with the formal precision that is a prerequisite forimplementation and suitably attuned to each other and intertwined.Applications in Electronic CommerceNew technological possibilities can quickly impact the interaction between companies andtheir customers.
One example are dialog systems that allow customers to obtain personaladvises or services.
For reasons indicated above, these systems are difficult to build, but oncethis investment has been done, they can be operated at low cost for the company.Another example, which may be even sooner to come, is the creation of systems that supportprocessing of emails sent by customers.
According to business analyses, e-mail has alreadynow become one of the most common forms of customer communication.
For numerousbusinesses that are not well-prepared, this has transformed e-mail into a severe pain point,giving rise to the pressing need to adopt e-mail response management systems.Obviously, NLP technologies that are able to extract the salient facts from email messagescan constitute a central part of these systems.
Due to the potential complexity of the queriesand additional problems like ungrammatical input and spelling errors, the correctinterpretation of arbitrary messages is far from easy.
However, there are several factors thatalleviate the situation: Messages that are too difficult for automatic processing can be routedto human agents.
In cases in which doubts about the correctness of generated responsespersist, these responses can always be checked by manual inspection.
Historical data aboutemail exchange with customers can be used to bootstrap the models that are required for thesystem.
Depending on the business, a significant fraction of the emails may be amenable toNLP, including requests for information material, business reports, certificates, statements ofaccount, scheduling requests, conference registrations etc.e-LearningUsing modern technology to facilitate learning is one of the most promising applicationdomains of NLP.
Good QA systems that are able to give answers to the point, orsummarization systems that can adapt to the user?s prior knowledge and present importantadditions in a way that is easy to understand could immediately take the place of a goodteacher, which an unlimited supply of time and patience.
One technology is ripe to buildthese tools, using them for e-learning will one of the biggest opportunities to our knowledgesociety.However, as the European society evolves more and more into multilingualism, it is natural toask how NLP can help to make language learning easier and more effective.
We can imaginesystems to help train children to write and to speak a foreign language.
There will becombinations of multi-modal aids for the handicapped.
A child will write a sentence and thesystem will correct it and tutor him about the problems.
A child will read a text aloud and thesystem will monitor which words are not right and why and will analyze where thepronunciation problems are.
Later the system would suggest some pronunciation exercises inthe particular problem.Systems that are able to guess the intention of a speaker from the speaker?s utterances in aflexible and intelligent way will offer a plethora of possibilities for e-learning.
As similarcapabilities are required for dialog systems in general, there will be significant synergy effectsbetween these fields of research.TranslationThe idea of machine translation (MT) has been one of the driving forces in the early days ofNLP.
However, even after more than 50 years of effort, current systems still produce outputof limited quality, which is suitable for assimilation of foreign-language documents, but notfor the production of publishable material.
But even if the old dreams did not come true, MTwill play an increasing role in the multilingual world.Last year, for the first time, English constituted less than half the material on the web.
Somepredict that Chinese will be the primary language of the web by 2007.
Given that informationon the web will increasingly appear in foreign languages and not all users will be fluent inthose languages, there will be a need to gist or skim content for relevance assessment and/orprovide high quality translation for deeper understanding.
Some forms of translation forinformation access is already today available in the web at no cost.
The increasing demand forthese services will give a push to improve their quality and the providers will find ways toincrease vocabularies and translation quality semi-automatically from terminologicalresources, bilingual corpora and similar sources.
Also the need for interactive systems thatcan give rough translations of chat sessions in real time will create interesting challenges.Clearly, any systematic collection of lexical and terminological information in the form ofdomain-specific ontologies will help to build better MT systems for these domains.Conversely, the construction of ontologies can be facilitated by automatic alignment ofexisting translations, as this will naturally lead to a clustering of the vocabulary along therelevant semantic distinctions.These developments will also have an impact on improved systems for high-qualitytranslation for the dissemination of documents.
Chances are that hybrid combinations ofsymbolic and stochastic translation engines, able to learn relevant terminology fromtranslation memories will eventually achieve a level of performance that will make themuseful for the professional translator.
Combined with multi-modal workbenches where voiceinput, keyboard and mouse interaction will make the composition of the target text asconvenient as possible, these new technologies may help at least in some easier domains,where so far the effort of the human translator is dominated by low-level activities such asentering the text, adjusting the formatting, copying names and numbers, which are clearlyamenable to partial automation.3.
Technologies for NLPThis chapter contains a more detailed discussion of some of the technologies that are requiredfor the applications mentioned in the last chapter.
Most of the material is organized alongtraditional fields of research in NLP, describing technologies that already exist, but must befurther developed to achieve the ambitious goals.
Some technologies cannot be assigned toone specific level, because they serve a more generic purpose, such as the extraction ofrelevant knowledge from text corpora.Low-Level ProcessingMost systems that analyse natural language text typically start by segmenting the text intomeaningful tokens.
Sometimes, the exact spelling of these tokens needs to be brought into acanonical form, so that it can match with a lexical entry.
Both processes can be based onmatching the input against regular expressions, for which efficient algorithms exist.
Whereasthis task looks straightforward from the distance, there are actually some subtle details thatneed to be considered.
Quite often, a decision whether a word should be split at a specialcharacter or whether a dot ends a sentence or is part of the preceding word depends on thevocabulary of the domain and on layout conventions used in this document, so that generalrules cannot be defined.
Documents that need to be analyzed may contain markup from textprocessors, which needs to be stripped or interpreted in a suitable way.
The knowledgerequired in these preliminary stages of processing can already be quite specific, so that amanual creation of suitable rule systems is not economically feasible.Current research on the automatic tokenization and normalization of texts thereforeconcentrates on the question how the knowledge required by these methods can automaticallybe derived from examples, using techniques statistical or machine learning approaches.Another difficulty is the treatment of noise in the input.
Output of speech recognition systemsoften contains recognition errors at rather high rates.
Utterances entered interactively orprinted documents that have undergone OCR have similar problems.
Unfortunately, thedistortion of even a single character can mess up the linguistic analysis of the complete input.But of course, we expect NLP systems to deal gracefully and intelligently with smalldistortions and errors in the input.To make systems more robust against noisy input, probabilistic techniques for the restorationof distorted signals,  which have shown to be quite effective in speech recognition, need to beadapted and generalized to new applications.
However, training simple-minded statisticalmodels on massive amounts of data will often not be feasible.
By now, statistical languagemodels that incorporate grammatical knowledge are able to give slight improvements over n-gram approaches, and it seems plausible to expect that future improvements of these will beeasier to use in specific situation where training data is scarce.
Large vocabularies, manytypes of distortions, and the need to use fine-grained contextual knowledge for improvedpredictive models constitute significant research challenges.
Most likely, there will be somesynergy between language models used in speech and similar models that will be developedfor low-level processing and correction of written ill-formed input.Once the segmentation into basic units has been performed, the next step is to identifysuitable lexical entries for each token and, in cases where more than one entry applies, todetermine which one is most appropriate in the given context.
This process is called part-of-speech disambiguation or POS tagging and is usually done with statistical models or machine-learning approaches trained on manually tagged data.
Current technology achieves ratherhigh accuracy on newspaper text, but again, performance suffers significantly when a modeltrained on a certain set of data is applied to text from a different domain.
As the output of thePOS tagger is typically used as input to subsequent modules, tagging errors may hamper thecorrect analysis of much more than the affected word.
Research on high-quality POS taggingwill face problems that are similar to those of language modelling: It requires detailedinformation about a large number of rare words that may be quite specific to the given domainand application, which is difficult to construct, no matter which road to lexical acquisition istaken.
Any effort that will support the construction, distribution, sharing and re-use of large,domain-specific lexical resources will doubtlessly also help to improve the accuracy of POStagging on text from these domains.The next step in the analysis of text is to identify groups of words that belong together andrefer to one semantic entity.
Often, these phrases contain names, and for many practicalapplications, it is important to classify these expressions according to the type of entity theydenote (Person, City, Company, etc.).
Depending on the application, the classification maybe more or less fine-grained.
Again, it is obvious that improved lexical knowledge will help toimprove the performance of named entity recognition.
But we cannot in all cases rely on alexical resource to cover the relevant entities.
A text may discuss the opening of a newcompany, which will therefore not be contained in the lexicon.
To handle such casesintelligently, we need mechanisms that can exploit contextual clues for the correctclassification of unknown entities and we need effective mechanisms that propagateinformation about new entities into the lexical repositories, so that the system as a wholelearns from the texts it sees, similar to the way a human reader would do.Syntactic AnalysisThe goal of syntactic analysis is to break down given textual units, typically sentences, intosmaller constituents, to assign categorical labels to them, and to identify the grammaticalrelations that hold between the various parts.In most applications of language technology the encoded linguistic knowledge, i.e.
thegrammar, is separated from the processing components.
The grammar consists of a lexicon,and rules that syntactically and semantically combine words and phrases into larger phrasesand sentences.Several language technology products on the market today employ annotated phrase-structuregrammars, grammars with several hundreds or thousands of rules describing different phrasetypes.
Each of these rules is annotated by features and sometimes also by expressions in aprogramming language.The resulting systems might be sufficiently efficient for some applications but they lack thespeed of processing needed for interactive systems, such as applications involving spokeninput, or systems that have to process large volumes of texts, as in machine translation.In current research, a certain polarization has taken place.
Very simple grammar models areemployed, e.g.
different kinds of finite-state grammars that support highly efficientprocessing.
Some approaches do away with grammars altogether and use statistical methodsto find basic linguistic patterns.
Other than speed, these shallow and statistically trainedapproaches have advantages in terms of robustness, and they also implicitly performdisambiguation, i.e.
when more than one analysis is possible, they make a decision for onereading (which of course may be the wrong one).On the other end of the scale, we find a variety of powerful linguistically sophisticatedrepresentation formalisms that facilitate grammar engineering.
These systems are typically setup in a way that all logically possible readings are computed, which increases the clarity (nomagic heuristics hidden in procedures), but also slows down the processing.
Despite theirnice theoretical properties it has so far been difficult to adapt these systems to the needs ofreal-world applications, where speed, robustness, and partial correctness in typical cases aremore urgent than theoretical faithfulness and depth of analysis.How will this situation evolve?
The two approaches will continue to compete for potentialapplications, and the current advantage for shallow approaches will diminish as moreambitious applications get within reach, and as languages are used that require richer analysis.This will give incentives for shallow approaches to struggle for higher accuracy and moredetailed analyses, whereas the deep processing will be forced to find workable solutions forthe problems with speed and robustness.
In the ideal case, more fine-grained forms ofintegration will be found, i.e.
hybrid systems that will keep the advantages of both worlds asfar as possible.The simplest integration will just use shallow analysis as a fallback mechanism when deepanalysis fails.
In this case, results from both approaches need to be translated into onecommon representation, and the development of such a ?common denominator?
will be asignificant challenge.
To achieve an even more fine-grained cooperation between bothapproaches, deep analysis may be equipped with the ability to locally fall back to moresuperficial processing, driven by the need to deal with a specific problem in the input.
Viceversa, the results of shallow analysis might be combined into a more detailed structureincrementally, based on rules from a deep grammar.
Also analyses of corpus data obtainedwith shallow tools can be mined for linguistic knowledge that is then fed into resources usedby a deep parser, and vice versa.Research challenges will be how to find syntactic parsers that are at the same time fast,robust, deliver a detailed analysis that is correct with high probability and that are easily toadapt to special domains.Semantic AnalysisThe goal of semantic analysis is to assign meanings to utterances, which is an essentialprecondition for most applications of NLP.
However, what level of abstraction is required inthis phase depends on the difficulty of the task.
Extraction of answers to simple factualquestions from a given text will require less depth in analysis than the summarization of alengthy treatise in few paragraphs.We can dissect the task of semantic analysis into several subtasks, depending on the linguisticlevel where it takes place.
Most important are the semantic tagging of ambiguous words andphrases, and the resolution of referring expressions.The disambiguation of word senses needs to identify the meaning that should be assigned to agiven word.
The hardest part of this task is to define the set of meanings that should beconsidered in this task, i.e.
to select the appropriate granularity for the conceptualization.
Theemergence of standardized, large-scale ontological resources will help to solve this part of thetask, as the concepts that appear in such ontologies are a natural choice for the meanings ofsingle words or simple phrases.
Additionally, multilingual corpora that are aligned on thelevel of words and phrases can serve as an approximation to sense-tagged corpora, so draftontologies and models for sense disambiguation can be extracted from these.Considerable efforts in defining useful evaluation metrics for sense disambiguation arepursued in the ongoing SENSEVAL activities.
So far, the methods used by the participants ofSENSEVAL are mostly based on simple statistical classification using features extracted fromthe context of word occurrences.
To the extent to which robust, high quality systems forsyntactic analysis will appear, this will also help to obtain improved accuracy in the semanticdisambiguation.The resolution of referring expression such as pronouns or definite noun phrases is the abilityto identify their target, which may be expressions that appear prior in the text, abstractions ofmaterial that appeared earlier, or entities that exist independently from the text in existingbackground knowledge.
Seen in a more general way, the task is to cull out objects and eventsfrom multimedia sources (text, audio, video).
An example challenge includes extractingentities within media and correlating those across media.
For example this might includeextracting names or locations from written/spoken sources and correlating those withassociated images.
Whereas commercial products exist to extract named entities from textwith precision and recall in the ninetieth percentile, domain independent event extractorswork at best in the fiftieth percentile and performance degrades further with noisy, corrupted,or idiosyncratic data.Therefore work on the resolution of referring expression and the identification of entities intext and multimedia documents remains important fields of activity for the future.Discourse and DialogueExtracting the knowledge contained in documents and understanding and generating naturaldialog behavior requires more than the resolution of local semantic ambiguities.
Intelligentanalysis needs to consider the global argumentative structure of documents and discourse, anddialogs need to be analyzed for pragmatic content.Computational work in discourse has focused on two different types of discourse: extendedtexts and dialogues, both spoken and written, yet there is a clear overlap between these two:dialogues contain text-like sequences spoken by a single individual and texts may containdialogues.
But application opportunities and needs are different.
Work on text is of directrelevance to document analysis and retrieval applications, whereas work on dialogue is ofimport for human-computer interfaces regardless of the modality of interaction.
Both aredivisible into segments (discourse segments and phrases) with the meaning of the segmentsbeing more than the meaning of the individual parts.The main focus of the research is the interpretation beyond sentence boundaries, theintentional and informational approach.According to the informational approaches, the coherence of discourse follows from semanticrelationships between the information conveyed by successive utterances.
As a result, themajor computational tools used here are inference and abduction on representations of thepropositional content of utterances.According to the intentional approaches the coherence of discourse derives from theintentions of speakers and writers and understanding depends on recognition of thoseintentions.One difficulty is to build models of human-machine-dialog when initially only examples ofhuman-human interaction exist, which may not be relevant.
Bootstrapping suitable modelswill therefore require Wizard-of-Oz studies with simulated systems.Natural Language GenerationIn many of the applications mentioned above, systems need to produce high-quality naturallanguage text from computer-internal representations of information.
Natural languagegeneration can be decomposed into the tasks of text planning, sentence planning and surfacerealization.
Text planners select from a knowledge pool which information to include in theoutput and out of this create a text structure to ensure coherence.
On a more local scale,sentence planners organize the content of each sentence, massaging and ordering its parts.Surface realizers convert sentence-sized chunks of representation into grammatically correctsentences.Generator processes can be classified into points on a range of sophistication and expressivepower, starting with inflexible canned methods and ending with maximally flexible featurecombination methods.
It is safe to say that at the present time one can fairly easily build asingle-purpose generator for any specific application, or with some difficulty adapt anexisting sentence generator to the application, with acceptable results.
However, one cannotyet build a general-purpose sentence generator or a non-toy text planner.
Several significantproblems remain without sufficiently general solutions:?
Lexical selection is one of the most difficult problems in generation.
At its simplestthis question involves selecting the most appropriate single word for a given unit ofinput.
However as soon as the semantic model approaches a realistic size and as soonas the lexicon is large enough to permit alternative locutions, the problem becomesvery complex.
The decision depends on what has already been said, what isreferentially available from context, what is most salient, what stylistic effect thespeaker wishes to produce and so on.
What is required: development of theories aboutand implementations of lexical selection algorithms, for reference to objects, eventsstates, etc., and tested with large lexical.?
Discourse structure (see also there) So far, no text planner exists that can reliably plantexts of several paragraphs in general.
What is required: Theories of the structuralnature of discourse, of the development of theme and focus in discourse, and ofcoherence and cohesion; libraries of discourse relations, communicative goals and textplans: implemented representational paradigms for characterizing stereotypical textssuch as reports and business letters; implemented text planners that are tested inrealistic non-toy domains.?
Sentence planning: Even assuming the text planning problem is solved, a number oftasks remain before well-structured multi-sentence text can be generated: These tasks,required for planning the structure and content of each sentence, include: pronounspecification, theme signaling, focus signaling, content aggregation to removeunnecessary redundancies, the ordering of prepositional phrases, adjectives, etc.
Whatis required: Theories of pronoun use, theme and focus selection and signaling, andcontent aggregation; implemented sentence planners with rules that perform theseoperations; testing in realistic domains.?
Domain modeling: a significant shortcoming in generation research is the lack oflarge, well-motivated application domain models, or even the absence of clearprinciples by which to build such models.
A traditional problem with generators is thatthe inputs are frequently hand-crafted, or are built by some other system that usesrepresentation elements from a fairly small hand-crafted domain model, making thegenerator?s inputs already highly oriented toward the final language desired?.What isrequired: Implemented large-size (over 10.000 concepts) domain models that areuseful both for some non-linguistic application and for generation; criteria forevaluating the internal consistency of such models; theories on and practicalexperience in the linking of generators to such models: lexicon of commensurate size.Probably the problem least addressed in generator systems today is the one that will take thelongest to solve.
This is the problem of guiding the generation process through its choiceswhen multiple options exist to handle any given input.The generator user has to specify not only the semantic content of the desired text, but also itspragmatic ?
interpersonal and situational ?
effects.
Very little research has been performed onthis question beyond a handful of small-scale pilot studies.
What is required: Classificationsof the types of reader characteristics and goals, the types of author goals, and the interpersonaland situational aspects that affect the form and content of language; theories of how theseaspects affect the generation process; implemented rules and/or planning systems that guidegenerator systems?
choices; criteria for evaluating appropriateness of general text in specifiedcommunicative situations.Effective presentations require the appropriate selection of content, allocation to media, andfine grained coordination and realization in time and space.
Discovery and presentation ofknowledge may require mixed media (e.g., text, graphics, video, speech and non-speechaudio) and mixed mode (e.g., linguistic, visual, auditory) displays tailored to the user andcontext.
This might include tailoring content and form to the specific physical, perceptual, orcognitive characteristics of the user.
It might lead to new visualization and browsingparadigms for massive multimedia and multilingual repositories that reduce cognitive load ortask time, increase analytic depth and breadth, or simply increase user satisfaction.
A grandchallenge is the automated generation of coordinated speech, natural language, gesture,animation, non-speech audio, generation, possibly delivered via interactive, animated lifelikeagents.
Preliminary experiments suggest that, independent of task performance, agents maysimply be more engaging/motivating to younger and/or less experienced users.OntologiesLarge-scale ontologies are becoming an essential component of many applications includingstandard search (such as Yahoo and Lycos), e-commerce (such as Amazon and eBay),configuration (such as Dell and PC-Order), and government intelligence (such as DARPA?sHigh Performance Knowledge Base program).
As discussed in the preceding paragraphs,ontologies will constitute a major source of knowledge needed for several levels of NLP.Ontologies are increasingly seen as an important vehicle for describing the semantic contentof web-based information sources and they are becoming so large that it is not uncommon fordistributed teams of people to be in charge of the ontology development, design, population,and maintenance.Ontologies define a vocabulary for researchers who need to share common understanding ofthe structure of information in a domain.
It includes machine-interpretable definitions of basicconcepts in the domain and relations among them.
The principal reasons to use an ontology inmachine translation (MT) and other language technologies are to enable source languageanalyzers and target language generators to share knowledge, to store semantic constraintsand to resolve semantic ambiguities by making inferences using the concept network of theontology.
An ontology contains only language independent information and many othersemantic relations as well as taxonomic relations.Though the utility of domain ontologies is now widely acknowledged in the IT (InformationTechnology) community, several barriers must be overcome before ontologies becomepractical and useful tools.
One important achievement would be to reduce the time and cost ofidentifying and manually entering several thousand concept descriptions by developingautomatic ontology construction.
Another important task is to find arrangements that makedevelopment and sharing of ontologies commercially attractive.Some challenges for ontology research:Work on ontologies needs to provide generally applicable top-ontologies that cover mostimportant core concepts that will be needed for many domains.
Extensions to new domainscould then start by enriching these top-ontologies in a specific direction, reducing the initialeffort for creating new ontologies, for merging independently developed extensions, and forrapid customisation of existing ontologies.This requires that ontology-creators are willing to share parts of their work and find suitableprocesses to organize cooperation.
It also requires the development of standards for thelanguages in which ontologies are specified and can be interchanged (e.g.
along the lines ofthe OIL proposal).
Here, the challenge is to find suitable compromises between expressivepower and depth on one hand and ease of use on the other hand.
Ideally, one specificationlanguage should be able to cover the whole spectrum up to advanced knowledgerepresentation as used in the CYC project.Incremental improvement of ontologies needs to be facilitated by specialized tools for easyvisualization and modification.
These tools (and the representations they work on) need to bedomain-independent and suited even for casual users, and their design needs to be based on auser-centred process view.It must be easy to plug in ontologies into various NLP-based tools such as tools forinformation extraction, organization and annotation of document collections (semantic Web),environments for terminology management and controlled language.
This will permit to auditthe contained knowledge in manifold ways, and will allow for rapid quality improvement.What is required: tools that support broad ranges of users in (1) merging of ontological termsfrom varied sources, (2) diagnosis of coverage and correctness of ontologies, and (3)maintaining ontologies over time.LexiconsLexical knowledge ?
knowledge about individual words in the language ?
is essential for alltypes of natural language processing.
Developers of machine translation systems, which fromthe beginning have involved large vocabularies, have long recognized the lexicon as a critical(and perhaps the critical) system resource.
As researchers and developers in other areas ofnatural language processing move from toy systems to systems which process real texts overbroad subject domains, larger and richer lexicons will be needed and the task of lexicondesign and development will become a more central aspect of any project.A basic lexicon will typically include information about morphology and on the syntacticlevel, the complement structures of each word or word sense.
A more complex lexicon mayalso include semantic information, such as a classification hierarchy and selectional patternsor case frames stated in terms of this hierarchy.
For machine translation, the lexicon will alsohave to record correspondences between lexical items in the source and target language; forspeech understanding and generation, it will have to include information about thepronunciation of individual words.
For this purpose the overall lexicon architecture and therepresentation formalism used to encode the data are important issues.No matter if we want to build an ontology or a lexicon, in general for this kind of high-qualitysemantic knowledge base, manual processing is indispensable.
Traditionally computerlexicons have been built by hand specifically for the purpose of language analysis andgeneration.
However, the needs for larger lexicons are now leading to efforts for thedevelopment of common lexical representations and co-operative lexicon development.The area is ripe ?
at least for some levels of linguistic description ?
for reaching in the shortterm a consensus on common lexical specifications.
We must expand the experiences with thesorts of semantic knowledge that could be effectively used by multiple systems.
We must alsorecognize the importance of the rapidly growing stock of machine-readable text as a resourcefor lexical research.
The major areas of potential results in the immediate future seem to lie inthe combination of lexicon and corpus work.
There?s a growing interest from many groups intopics such as sense tagging or sense disambiguation on very large text corpora, where lexicaltools and data provide a first input to the systems and are in turn enhanced with theinformation acquired and extracted from corpus analysis.Machine LearningAs mentioned above, the acquisition of knowledge continues to impose on of the biggestdifficulties to the application of NLP technologies.
This holds both for linguistic knowledge(grammars lexicons) and for world knowledge (ontologies, facts).
In order to makeextensions of NLP to new domains possible, the acquisition process needs to be supported byalgorithms that can exploit existing textual material and extract knowledge of various typesfrom it.Approaches to these methods can be found in various fields of research, such as statisticallanguage models, bilingual alignment, grammar induction, statistical parsing, statisticalclassification technology, Bayesian networks and other ML methods used in artificialintelligence research, data mining techniques etc.Due to the specific nature of lexical information, it is important to pick or develop methodsthat scale to large vocabularies and large sets of features and that can exploit multiple sourcesof evidence in a good way.
Also, the methods need to be able to use a rich set of existingbackground knowledge, so that no effort is wasted in re-discovering what was already known.It is important to have methods that can use richly annotated training data, but do not requirethat large datasets have to be annotated in this way.
Instead, methods should be able to drawa maximum of advantage from raw data without annotation using unsupervised learningapproaches.
Also, it will be important to guide the effort of human annotation so that time isspent in the most efficient way, using active learning methods.
Tools and processes formanaging annotation projects (including assessment of quality levels) need to be developedand shared on a broad basis.Whenever possible, one should try to use models that contain explicit linguisticrepresentations (ideally organized along different strata) so that partial reuse of models andrapid adaptation to slightly different is facilitated.4.
MilestonesSome relevant items not included in Bernsen 2000.Basic technologiesShort term- accurate syntactic analysis for well-formed input from specific domains- simple methods for minimizing annotation effort during domain adaptation- ML algorithms that combine active and unsupervised learning for optimal exploitationof data- generally applicable annotation schemes for semantic markup of text- standards for encoding and exchange of ontological resources emerge- top-level ontologies generally available- tools for semi-automatic construction and population of ontologies from text- tools for simple semantic enrichment of Web pages- approaches to markup of discourse structure and pragmaticsMedium term- improved methods for minimizing annotation effort during domain adaptation- tools for adaptation of syntactic analysis to specific application with minimal humaneffort- accurate syntactic analysis for slightly ill-formed input for restricted domains- improved syntactic analysis of input with uncertainties (word lattices)- machine learning methods that exploit and extend existing knowledge sources- sufficiently accurate semantic analysis of free text from restricted domains- generic schemes for the annotation of pragmatic content- schemes for annotation of discourse and document structure- generally usable ontologies exist for many domains- NL generation verbalizes information extracted/deduced from multiple sources for QA- Agent/user models for dialogs of moderate complexityLong term- accurate syntactic analysis for ill-formed input from multiple domains- sufficiently accurate semantic analysis of free text from multiple domains- recognition of pragmatic content in text and dialog- NL generation produces stylistically adequate and well-structured textSystemsShort term- QA systems are able to answer simple factual questions- Summarization system produce well-formed extracts from short documents- automated e-mail response systems deliver high-quality replies in easy cases- MT for information assimilationMedium term- QA systems that deduce answers from information in multiple sources- Summarization systems are able to merge multiple documents- Summarization systems are able to deliver different types of summaries- Integration of translation memories with MT enables fast domain-adaptation- Mixed-initiative dialogue systems for services and e-commerceLong term- Translator?s workbenches based on TM, MT, and multi-modal input facilities- QA systems that are able to explain their reasoning5.
Recommendations for NLP research in Europe1.
Build and make publicly available at low cost large-scale multilingual lexicalresources, with broad coverage, generic enough to be reusable in different applicationframeworks2.
To turn special attention to the development of better ontologies which are reusableacross domains in order to encode static world knowledge3.
Creation of large common accessible multilingual corpora of syntactical andsemantically annotated data annotated also beyond sentence boundaries4.
Encourage development of statistical and machine-learning methods that facilitatebootstrapping of linguistic resources5.
Common standards will improve the effectiveness of people?s cooperation, theidentification of the requirements for the system specification, the inter-operabilityamong systems and the possibility of re-using and sharing system components.6.
Integration of language processing into the rest of cognitive science, artificialintelligence and computer science e.g.
some ambitious projects centered on NL butcombining various techniques and different areas of AI.
New type of projects: Verydifferent for scale, ambition and timeframe7.
Establishment of centers of excellence as focus points for projects for a period of fiveto ten years.8.
Encourage systematic evaluations (but how ?)6.
References?
Berners-Lee, T. (2001) The Semantic Web, Scientific American (5/2001)?
Bernsen , N.O.
(2000) Speech-Related Technologies.
Where will the field go in 10years?
roadmap workshop, Katwijk?
Burger, J. e.a.
(2000) Issues, Tasks and Program Structures to Roadmap Research inQuestion & Answering, Memo National Institute of Standards and Technology,Gaithersburg?
Carbonell, J. e.a.
(2000) Vision Statement to Guide Research in Q&A and TextSummarization, Memo National Institute of Standards and Technology, Gaithersburg?
Cole,R.A.
(Ed.).
(1997) Survey of the State of the Art in Human LanguageTechnology Cambridge University Press, Cambridge?
Declerck, Th., Wittenburg, P., Cunningham, H. (2001) The Automatic Generation ofFormal Annotations in a Multimedia Indexing and Searching Environment, ACLWorkshop, Toulouse?
Delannoy, J.-F. (2001) What are the points?
What are the stances?
Decanting forquestion-driven retrieval and executive summarization, ACL Meeting, Toulouse?
Fensel, D. Hendler, J., Lieberman, H. ,Wahlster, W. (2000) Dagstuhl-Seminar:Semantics for the WWW, Dagstuhl, Germany?
Grishman, R. and Calzolari, N. Lexicons in Survey of the State of the Art in HumanLanguage Technology, Cambridge University Press, Cambridge?
Grosz, B.
(1997) Discourse and Dialogue in Survey of the State of the Art in HumanLanguage Technology, Cambridge University Press, Cambridge?
Heisterkamp, P., (2000) Speech Technology in the year 2010, roadmap workshop,Katwijk?
Hirschman, L. and Thompson, H.S.
(1997) Evaluation in Survey of the State of theArt in Human Language Technology, Cambridge University Press, Cambridge?
Hovy, E., (1997) Language Generation in Survey of the State of the Art in HumanLanguage Technology, Cambridge University Press, Cambridge?
Kang, S.-J.
and Lee, J.-H. (2001) Semi-Automatic Practical Ontology Construction byusing Thesaurus, Computational Dictionaries, and Large Corpora, ACL workshopToulouse?
Kay, M. (1997) Machine Translation: The Disappointing Past and Present.
In: Surveyof the State of the Art in Human Language Technology, Cambridge University Press,Cambridge?
Kay, M. (1997) Multilinguality.
In: Survey of the State of the Art in Human LanguageTechnology, Cambridge University Press, Cambridge?
Knight, K. (2001) Language Modeling for Good Generation, Workshop on LanguageModeling and Information Retrieval, Pittsburgh?
Krauwer, St., (2000) Going from ?what?
to ?why?
across language barriers in theunified distributed information space.
Roadmap workshop, Katwijk?
Maybury, M.T.
and Mani, I., (2001) Automatic Summarization, ACL MeetingToulouse?
Maybury, M.T., (2001) Human Language Technologies for Knowledge Management:Challenges and Opportunities, ACL Meeting, Toulouse?
Pardo, J.M., (2000) How will language and speech technology be used in theinformation world of 2010?
Research challenges & Infrastructure needs for the nextten years.
Report on the Roadmap Workshop, Katwijk aan Zee?
Staab, St., (2001) Knowledge Portals, ACL Meeting, Toulouse?
Stock, O.
(2000) Processing Natural Language from 2000 to 2010, roadmapworkshop, Katwijk?
Velardi, P. and Missikoff, M. and Basili, R. (2001) Identification of relevant terms tosupport the construction of Domain Ontologies, ACL workshop Toulouse?
Uszkoreit; H.  (2001) Crosslingual Language Technologies for Knowledge Creationand Knowledge Sharing, Toulouse?
Zaenen, A. and Uszkoreit, H. (1997) Language Analysis and Understanding.
In:Survey of the State of the Art in Human Language Technology, Cambridge UniversityPress, Cambridge
