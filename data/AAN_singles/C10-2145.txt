Coling 2010: Poster Volume, pages 1265?1273,Beijing, August 2010Citation Author Topic Model in Expert SearchYuancheng Tu, Nikhil Johri, Dan Roth, Julia HockenmaierUniversity of Illinois at Urbana-Champaign{ytu,njohri2,danr,juliahmr}@illinois.eduAbstractThis paper proposes a novel topic model,Citation-Author-Topic (CAT) model thataddresses a semantic search task we defineas expert search ?
given a research area asa query, it returns names of experts in thisarea.
For example, Michael Collins wouldbe one of the top names retrieved given thequery Syntactic Parsing.Our contribution in this paper is two-fold.First, we model the cited author informa-tion together with words and paper au-thors.
Such extra contextual informationdirectly models linkage among authorsand enhances the author-topic association,thus produces more coherent author-topicdistribution.
Second, we provide a prelim-inary solution to the task of expert searchwhen the learning repository contains ex-clusively research related documents au-thored by the experts.
When comparedwith a previous proposed model (Johriet al, 2010), the proposed model pro-duces high quality author topic linkageand achieves over 33% error reductionevaluated by the standard MAP measure-ment.1 IntroductionThis paper addresses the problem of searching forpeople with similar interests and expertise, giventheir field of expertise as the query.
Many existingpeople search engines need people?s names to do a?keyword?
style search, using a person?s name asa query.
However, in many situations, such infor-mation is insufficient or impossible to know be-forehand.
Imagine a scenario where the statisticsdepartment of a university invited a world-wideknown expert in Bayesian statistics and machinelearning to give a keynote speech; how can theorganizer notify all the people on campus whoare interested without spamming those who arenot?
Our paper proposes a solution to the afore-mentioned scenario by providing a search enginewhich goes beyond ?keyword?
search and can re-trieve such information semantically.
The orga-nizer would only need to input the research do-main of the keynote speaker, i.e.
Bayesian statis-tics, machine learning, and all professors and stu-dents who are interested in this topic will be re-trieved and an email agent will send out the infor-mation automatically.Specifically, we propose a Citation-Author-Topic (CAT) model which extracts academic re-search topics and discovers different researchcommunities by clustering experts with similar in-terests and expertise.
CAT assumes three steps ofa hierarchical generative process when producinga document: first, an author is generated, then thatauthor generates topics which ultimately generatethe words and cited authors.
This model linksauthors to observed words and cited authors vialatent topics and captures the intuition that whenwriting a paper, authors always first have topicsin their mind, based on which, they choose wordsand cite related works.Corpus linguists or forensic linguists usually1265identify authorship of disputed texts based onstylistic features, such as vocabulary size, sen-tence length, word usage that characterize a spe-cific author and the general semantic content isusually ignored (Diederich et al, 2003).
On theother hand, graph-based and network based mod-els ignore the content information of documentsand only focus on network connectivity (Zhanget al, 2007; Jurczyk and Agichtein, 2007).
Incontrast, the model we propose in this paper fullyutilizes the content words of the documents andcombines them with the stylistic flavor contex-tual information to link authors and documents to-gether to not only identify the authorship, but alsoto be used in many other applications such as pa-per reviewer recommendation, research commu-nity identification as well as academic social net-work search.The novelty of the work presented in this pa-per lies in the proposal of jointly modeling thecited author information and using a discrimi-native multinomial distribution to model the co-author information instead of an artificial uni-form distribution.
In addition, we apply and eval-uate our model in a semantic search scenario.While current search engines cannot support in-teractive and exploratory search effectively, ourmodel supports search that can answer a range ofexploratory queries.
This is done by semanticallylinking the interests of authors to the topics of thecollection, and ultimately to the distribution of thewords in the documents.In the rest of this paper, we first present somerelated work on author topic modeling and expertsearch in Sec.
2.
Then our model is described inSec.
3.
Sec.
4 introduces our expert search systemand Sec.
5 presents our experiments and the evalu-ation.
We conclude this paper in Sec.
6 with somediscussion and several further developments.2 Related WorkAuthor topic modeling, originally proposedin (Steyvers et al, 2004; Rosen-Zvi et al, 2004),is an extension of Latent Dirichlet Allocation(LDA) (Blei et al, 2003), a probabilistic genera-tive model that can be used to estimate the proper-ties of multinomial observations via unsupervisedlearning.
LDA represents each document as amixture of probabilistic topics and each topic asa multinomial distribution over words.
The Au-thor topic model adds an author layer over LDAand assumes that the topic proportion of a givendocument is generated by the chosen author.Author topic analysis has attracted much atten-tion recently due to its broad applications in ma-chine learning, text mining and information re-trieval.
For example, it has been used to pre-dict authors for new documents (Steyvers et al,2004), to recommend paper reviewers (Rosen-Zviet al, 2004), to model message data (Mccallum etal., 2004), to conduct temporal author topic anal-ysis (Mei and Zhai, 2006), to disambiguate propernames (Song et al, 2007), to search academic so-cial networks (Tang et al, 2008) and to generatemeeting status analyses for group decision mak-ing (Broniatowski, 2009).In addition, there are many related works onexpert search at the TREC enterprise track from2005 to 2007, which focus on enterprise scalesearch and discovering relationships between enti-ties.
In that setting, the task is to find the experts,given a web domain, a list of candidate expertsand a set of topics 1.
The task defined in our paperis different in the sense that our topics are hid-den and our document repositories are more ho-mogeneous since our documents are all researchpapers authored by the experts.
Within this set-ting, we can explore in depth the influence of thehidden topics and contents to the ranking of ourexperts.
Similar to (Johri et al, 2010), in this pa-per we apply CAT in a semantic retrieval scenario,where searching people is associated with a set ofhidden semantically meaningful topics instead oftheir personal names.In recent literature, there are three main lines ofwork that extend author topic analyses.
One lineof work is to relax the model?s ?bag-of-words?assumption by automatically discovering multi-word phrases and adding them into the originalmodel (Johri et al, 2010).
Similar work has alsobeen proposed for other topic models such asNgram topic models (Wallach, 2006; Wang andMcCallum, 2005; Wang et al, 2007; Griffiths etal., 2007).1http://trec.nist.gov/pubs.html1266Another line of work models authors informa-tion as a general contextual information (Mei andZhai, 2006) or associates documents with networkstructure analysis (Mei et al, 2008; Serdyukov etal., 2008; Sun et al, 2009).
This line of workaims to propose a general framework to deal withcollections of texts with an associated networksstructure.
However, it is based on a different topicmodel than ours; for example, Mei?s works (Meiand Zhai, 2006; Mei et al, 2008) extend proba-bilistic latent semantic analysis (PLSA), and donot have cited author information explicitly.Our proposal follows the last line of workwhich extends author topic modeling with spe-cific contextual information and directly capturesthe association between authors and topics to-gether with this contextual information (Tang etal., 2008; Mccallum et al, 2004).
For exam-ple, in (Tang et al, 2008), publication venue isadded as one extra piece of contextual informa-tion and in (Mccallum et al, 2004), email recip-ients, which are treated as extra contextual infor-mation, are paired with email authors to model anemail message corpus.
In our proposed method,the extra contextual information consists of thecited authors in each documents.
Such contextualinformation directly captures linkage among au-thors and cited authors, enhances author-topic as-sociations, and therefore produces more coherentauthor-topic distributions.3 The Citation-Author-Topic (CAT)ModelCAT extends previously proposed author topicmodels by explicitly modelling the cited authorinformation during the generative process.
Com-pared with these models (Rosen-Zvi et al, 2004;Johri et al, 2010), whose plate notation is shownin Fig.
1, CAT (shown in Fig.
2) adds cited au-thor information and generates authors accordingto the observed author distribution.Four plates in Fig.
1 represent topic (T ), au-thor (A), document (D) and words in each doc-ument (Nd) respectively.
CAT (Fig.
2) has onemore plate, cited-author topic plate, in which eachtopic is represented as a multinomial distributionover all cited authors (?c).Within CAT, each author is associated with a DAN dFigure 1: Plate notation of the previously pro-posed author topic models (Rosen-Zvi et al,2004; Johri et al, 2010).DAN dFigure 2: Plate notation of our current model:CAT generates words W and cited authors C in-dependently given the topic.multinomial distribution over all topics, ~?a, andeach topic is a multinomial distribution over allwords, ~?t, as well as a multinomial distributionover all cited authors ~?c.
Three symmetric Dirich-let conjugate priors, ?, ?
and ?, are defined foreach of these three multinomial distributions inCAT as shown in Fig.
2.The generative process of CAT is formally de-fined in Algorithm 1.
The model first samplesthe word-topic, cited author-topic and the author-topic distributions according to the three Dirich-let hyperparameters.
Then for each word in eachdocument, first the author k is drawn from theobserved multinomial distribution and that authorchooses the topic zi, based on which word wi andcited author ci are generated independently.CAT differs from previously proposed MAT(Multiword-enhanced Author Topic) model (Johriet al, 2010) in two aspects.
First of all, CAT uses1267Algorithm 1: CAT: A, T ,D,N are fourplates as shown in Fig.
2.
The generative pro-cess of CAT modeling.Data: A, T ,D,Nfor each topic t ?
T dodraw a distribution over words:~?t ?
DirN (?)
;draw a distribution over cited authors:~?c ?
DirC(?)
;for each author a ?
A dodraw a distribution over topics:~?a ?
DirT (?)
;for each document d ?
D and k authors ?
ddofor each word w ?
d dochoose an authork ?
Multinomial(Ad) ;assign a topic i given the author:zk,i|k ?
Multinomial(?a) ;draw a word from the chosen topic:wd,k,i|zk,i ?
Multinomial(?zk,i) ;draw a cited author from the topic:cd,k,i|zk,i ?
Multinomial(?zk,i)cited author information to enhance the modeland assumes independence between generatingthe words and cited authors given the topic.
Sec-ondly, instead of an artificial uniform distributionover all authors and co-authors, CAT uses the ob-served discriminative multinomial distribution togenerate authors.3.1 Parameter EstimationCAT includes three sets of parameters.
The Ttopic distribution over words, ?t which is similarto that in LDA.
The author-topic distribution ?a aswell as the cited author-topic distribution ?c.
Al-though CAT is a relatively simple model, findingits posterior distribution over these hidden vari-ables is still intractable due to their high dimen-sionality.
Many efficient approximate inferencealgorithms have been used to solve this problemincluding Gibbs sampling (Griffiths and Steyvers,2004; Steyvers and Griffiths, 2007; Griffiths et al,2007) and mean-field variational methods (Blei etal., 2003).
Gibbs sampling is a special case ofMarkov-Chain Monte Carlo (MCMC) samplingand often yields relatively simple algorithms forapproximate inference in high dimensional mod-els.In our CAT modeling, we use a collapsed Gibbssampler for our parameter estimation.
In thisGibbs sampler, we integrated out the hidden vari-ables ?, ?
and ?
using the Dirichlet delta func-tion (Heinrich, 2009).
The Dirichlet delta func-tion with an M dimensional symmetric Dirichletprior ?
is defined as:?M (?)
=?(?M)?
(M?
)Based on the independence assumptions de-fined in Fig.
2, the joint distribution of topics,words and cited authors given all hyperparame-ters which originally represented by integrals canbe transformed into the delta function format andformally derived in Equation 1.P (~z, ~w,~c|?, ?, ?)
(1)= P (~z|?, ?, ?
)P (~w,~c|~z, ?, ?, ?
)= P (~z)P (~w|~z)P (~c|~z)=A?a=1?(nA+?)?(?)T?z=1?(nZw+?)?(?)T?z=1?(nZc+?)?(?
)The updating equation from which the Gibbssampler draws the hidden variable for the currentstate j, i.e., the conditional probability of drawingthe kth author Kkj , the ith topic Zij , and the cthcited author Ccj tuple, given all the hyperparame-ters and all the observed documents and authors,cited authors except the current assignment (theexception is denoted by the symbol ?
?j), is de-fined in Equation 2.P (Zij ,Kkj , Ccj |Wwj ,?
?j, Ad, ?, ?, ?)
(2)?
?(nZ+?)?(nZ,?j+?)?(nK+?)?(nK,?j+?)?(nC+?)?(nC,?j+?
)= nwi,?j+?wVPw=1nwi,?j+V ?wnik,?j+?iTPi=1nik,?j+T?inci,?j+?cCPc=1nci,?j+C?cThe parameter sets ?
and ?, ?
can be interpretedas sufficient statistics on the state variables ofthe Markov Chain due to the Dirichlet conjugatepriors we used for the multinomial distributions.1268These three sets of parameters are estimated basedon Equations 3 , 4 and 5 respectively, in which nwiis defined as the number of times the word w isgenerated by topic i; nik is defined as the numberof times that topic i is generated by author k andnic is defined as the number of times that the citedauthor c is generated by topic i.
The vocabularysize is V , the number of topics is T and the cited-author size is C.?w,i =nwi + ?wV?w=1nwi + V ?w(3)?k,i =nik + ?iT?i=1nik + T?i(4)?c,i =nci + ?cC?c=1nci + C?c(5)The Gibbs sampler used in our experiments isadapted from the Matlab Topic Modeling Tool-box 2.4 Expert SearchIn this section, we describe a preliminary re-trieval system that supports expert search, whichis intended to identify groups of research expertswith similar research interests and expertise by in-putting only general domain key words.
For ex-ample, we can retrieve Michael Collins via searchfor natural language parsing.Our setting is different from the standard TRECexpert search in that we do not have a pre-definedlist of experts and topics, and our documents areall research papers authored by experts.
Withinthis setting, we do not need to identify the status ofour experts, i.e., a real expert or a communicator,as in TREC expert search.
All of our authors andcited authors are experts and the task amounts toranking the experts according to different topicsgiven samples of their research papers.The ranking function of this retrieval model isderived through the CAT parameters.
The search2http://psiexp.ss.uci.edu/research/programs data/aims to link research topics with authors to by-pass the proper names of these authors.
Our re-trieval function ranks the joint probability of thequery words (W ) and the target author (a), i.e.,P (W,a).
This probability is marginalized over alltopics, and the probability that an author is citedgiven the topic is used as an extra weight in ourranking function.
The intuition is that an authorwho is cited frequently should be more prominentand ranked higher.
Formally, we define the rank-ing function of our retrieval system in Equation 6.ca denotes when the author is one of the cited au-thors in our corpus.
CAT assumes that words andauthors, and cited authors are conditionally inde-pendent given the topic, i.e., wi ?
a ?
ca.P (W,a) =?wi?i?tP (wi, a|t, ca)P (t, ca)=?wi?i?tP (wi|t)P (a|t)P (ca|t)P (t)(6)W is the input query, which may contain one ormore words.
If a multiword is detected within thequery, it is added into the query.
The final scoreis the sum of all words in this query weighted bytheir inverse document frequency ?i.In our experiments, we chose ten queries whichcover several popular research areas in computa-tional linguistics and natural language processingand run the retrieval system based on three mod-els: the original author topic model (Rosen-Zviet al, 2004), the MAT model (Johri et al, 2010)and the CAT model.
In the original author topicmodel, query words are treated token by token.Both MAT and CAT expand the query terms withmultiwords if they are detected inside the originalquery.
For each query, top 10 authors are returnedfrom the system.
We manually label the relevanceof these 10 authors based on the papers collectedin our corpus.Two standard evaluation metrics are used tomeasure the retrieving results.
First we evaluatethe precision at a given cut-off rank, namely pre-cision at rank k with k ranging from 1 to 10.
Wethen calculate the average precision (AP) for eachquery and the mean average precision (MAP) for1269the queries.
Unlike precision at k, MAP is sensi-tive to the ranking and captures recall informationsince it assumes the precision of the non-retrieveddocuments to be zero.
It is formally defined asthe average of precisions computed at the point ofeach of the relevant documents in the ranked listas shown in Equation 7.AP =?nr=1(Precision(r)?
rel(r))| relevant documents | (7)To evaluate the recall of our system, we col-lected a pool of authors for six of our queries re-turned from an academic search engine, Arnet-Miner (Tang et al, 2008)3 as our reference authorpool and evaluate our recall based on the numberof authors we retrieved from that pool.5 Experiments and AnalysisIn this section, we describe the empirical evalua-tion of our model qualitatively and quantitativelyby applying our model to the expert search we de-fined in Sec.
4.
We compare the retrieving resultswith two other models: Multiword- enhanced Au-thor Topic (MAT) model (Johri et al, 2010) andthe original author topic model (Rosen-Zvi et al,2004).5.1 Data set and Pre-processingWe crawled the ACL anthology website and col-lected papers from ACL, EMNLP and CONLLover a period of seven years.
The ACL anthol-ogy website explicitly lists each paper togetherwith its title and author information.
Therefore,the author information of each paper can be ob-tained accurately without extracting it from theoriginal paper.
However, many author names arenot represented consistently.
For example, thesame author may have his/her middle name listedin some papers, but not in others.
We thereforenormalized all author names by eliminating mid-dle names from all authors.Cited authors of each paper are extracted fromthe reference section and automatically identifiedby a named entity recognizer tuned for citation ex-traction (Ratinov and Roth, 2009).
Similar to reg-ular authors, all cited authors are also normalized3http://www.arnetminer.orgConf.
Year Paper Author uni.
Vocab.ACL 03-09 1,326 2,084 34,012 205,260EMNLP 93-09 912 1,453 40,785 219,496CONLL 97-09 495 833 27,312 123,176Total 93-09 2,733 2,911 62,958 366,565Table 1: Statistics about our data set.
Uni.
denotesunigram words and Vocab.
denotes all unigramsand multiword phrases discovered in the data set.with their first name initial and their full last name.We extracted about 20,000 cited authors from ourcorpus.
However, for the sake of efficiency, weonly keep those cited authors whose occurrencefrequency in our corpus is above a certain thresh-old.
We experimented with thresholds of 5, 10 and20 and retained the total number of 2,996, 1,771and 956 cited authors respectively.We applied the same strategy to extract mul-tiwords from our corpus and added them intoour vocabulary to implement the model describedin (Johri et al, 2010).
Some basic statistics aboutour data set are summarized in Table 1 4.5.2 Qualitative Coherence AnalysisAs shown by other previous works (Wallach,2006; Griffiths et al, 2007; Johri et al, 2010),our model also demonstrates that embedding mul-tiword tokens into the model can achieve more co-hesive and better interpretable topics.
We list thetop 10 words from two topics of CAT and comparethem with those from the unigram model in Ta-ble 2.
Unigram topics contain more general wordswhich can occur in every topic and are usually lessdiscriminative among topics.Our experiments also show that CAT achievesbetter retrieval quality by modeling cited authorsjointly with authors and words.
The rank of anauthor is boosted if that author is cited more fre-quently.
We present in Table 3 the ranking of oneof our ten query terms to demostrate the high qual-ity of our proposed model.
When compared to themodel without cited author information, CAT notonly retrieves more comprehensive expert list, itsranking is also more reasonable than the modelwithout cited author information.Another observation in our experiments is that4Download the data and the software package at:http://L2R.cs.uiuc.edu/?cogcomp/software.php.1270Query term: parsingProposed CAT Model Model without cited authorsRank Author Prob.
Author Prob.1 J. Nivre 0.125229 J. Nivre 0.0332002 C. Manning 0.111252 R. Barzilay 0.0238633 M. Johnson 0.101342 M. Johnson 0.0237814 J. Eisner 0.063528 D. Klein 0.0189375 M. Collins 0.047347 R. McDonald 0.0173536 G. Satta 0.042081 L. Marquez 0.0160037 R. McDonald 0.041372 A. Moschitti 0.0157818 D. Klein 0.041149 N. Smith 0.0147929 K. Toutanova 0.024946 C. Manning 0.01404010 E. Charniak 0.020843 K. Sagae 0.013384Table 3: Ranking for the query term: parsing.
CAT achieves more comprehensive and reasonable ranklist than the model without cited author information.CAT Uni.
AT ModelTOPIC 49 Topic 27pronoun resolution anaphorantecedent antecedentscoreference resolution anaphoricitynetwork anphoricresolution isanaphor anaphorapronouns soonanaphor antecedent determinationsemantic knowledge pronominalproper names salienceTOPIC 14 Topic 95translation quality hypernymtranslation systems seedssource sentence taxonomyword alignments factsparaphrases hyponymdecoder walkparallel corpora hypernymstranslation system pageparallel corpus logstranslation models extractionsTable 2: CAT with embedded multiword com-ponents achieves more interpretable topics com-pared with the unigram Author Topic (AT) model.some experts who published many papers, but onheterogeneous topics, may not be ranked at thevery top by models without cited author infor-mation.
However, with cited author information,those authors are ranked higher.
Intuitively thismakes sense since many of these authors are alsothe most cited ones.5.3 Quantitative retrieval resultsOne annotator labeled the relevance of the re-trieval results from our expert search system.
Theannotator was also given all the paper titles of eachPrecision@KK CAT Model Model w/o Cited Authors1 0.80 0.802 0.80 0.703 0.73 0.604 0.70 0.505 0.68 0.486 0.70 0.477 0.69 0.408 0.68 0.459 0.73 0.4410 0.70 0.44Table 4: Precision at K evaluation of our proposedmodel and the model without cited author infor-mation.corresponding retrieved author to help make thisbinary judgment.
We experiment with ten queriesand retrieve the top ten authors for each query.We first used the precision at k for evaluation.We calculate the precision at k for both our pro-posed CAT model and the MAT model, whichdoes not have the cited author information.
Theresults are listed in Table 4.
It can be observedthat at every rank position, our CAT model worksbetter.
In order to focus more on relevant retrievalresults, we also calculated the mean average pre-cision (MAP) for both models.
For the given tenqueries, the MAP score for the CAT model is 0.78,while the MAT model without cited author infor-mation has a MAP score of 0.67.
The CAT modelwith cited author information achieves about 33%error reduction in this experiment.1271Query ID Query Term1 parsing2 machine translation3 dependency parsing4 transliteration5 semantic role labeling6 coreference resolution7 language model8 Unsupervised Learning9 Supervised Learning10 Hidden Markov ModelTable 5: Queries and their corresponding ids weused in our experiments.Recall for each queryQuery ID CAT Model Model w/o Cite1 0.53 0.202 0.13 0.203 0.27 0.134 0.13 0.25 0.27 0.206 0.13 0.26Average 0.24 0.20Table 6: Recall comparison between our proposedmodel and the model without cited author infor-mation.Since we do not have a gold standard expertspool for our queries, to evaluate recall, we col-lected a pool of authors returned from an aca-demic search engine, ArnetMiner (Tang et al,2008) as our reference author pool and evaluatedour recall based on the number of authors we re-trieved from that pool.
Specifically, we get thetop 15 returned persons from that website for eachquery and treat them as the whole set of relevantexperts for that query and our preliminary recallresults are shown in Table 6.In most cases, the CAT recall is better than thatof the compared model, and the average recall isbetter as well.
All the queries we used in our ex-periments are listed in Table 5.
And the averagerecall value is based on six of the queries whichhave at least one overlap author with those in ourreference recall pool.6 Conclusion and Further DevelopmentThis paper proposed a novel author topic model,CAT, which extends the existing author topicmodel with additional cited author information.We applied it to the domain of expert retrievaland demonstrated the effectiveness of our modelin improving coherence in topic clustering and au-thor topic association.
The proposed model alsoprovides an effective solution to the problem ofcommunity mining as shown by the promising re-trieval results derived in our expert search system.One immediate improvement would result fromextending our corpus.
For example, we can ap-ply our model to the ACL ARC corpus (Bird etal., 2008) to check the model?s robustness and en-hance the ranking by learning from more data.
Wecan also apply our model to data sets with richlinkage structure, such as the TREC benchmarkdata set or ACL Anthology Network (Radev et al,2009) and try to enhance our model with the ap-propriate network analysis.AcknowledgmentsThe authors would like to thank Lev Ratinov forhis help with the use of the NER package and thethree anonymous reviewers for their helpful com-ments and suggestions.
The research in this pa-per was supported by the Multimodal InformationAccess & Synthesis Center at UIUC, part of CCI-CADA, a DHS Science and Technology Center ofExcellence.ReferencesBird, S., R. Dale, B. Dorr, B. Gibson, M. Joseph,M.
Kan, D. Lee, B Powley, D. Radev, and Y. Tan.2008.
The acl anthology reference corpus: A refer-ence dataset for bibliographic research in computa-tional linguistics.
In Proceedings of LREC?08.Blei, D., A. Ng, and M. Jordan.
2003.
Latent dirichletallocation.
Journal of Machine Learning Research.Broniatowski, D. 2009.
Generating status hierar-chies from meeting transcripts using the author-topic model.
In In Proceedings of the Workshop:Applications for Topic Models: Text and Beyond.Diederich, J., J. Kindermann, E. Leopold, andG.
Paass.
2003.
Authorship attribution with supportvector machines.
Applied Intelligence, 19:109?123.1272Griffiths, T. and M. Steyvers.
2004.
Finding scientifictopic.
In Proceedings of the National Academy ofScience.Griffiths, T., M. Steyvers, and J. Tenenbaum.
2007.Topics in semantic representation.
PsychologicalReview.Heinrich, G. 2009.
Parameter estimation for text anal-ysis.
Technical report, Fraunhofer IGD.Johri, N., D. Roth, and Y. Tu.
2010.
Experts?
retrievalwith multiword-enhanced author topic model.
InProceedings of NAACL-10 Semantic Search Work-shop.Jurczyk, P. and E. Agichtein.
2007.
Discovering au-thorities in question answer communities by usinglink analysis.
In Proceedings of CIKM?07.Mccallum, A., A. Corrada-emmanuel, and X. Wang.2004.
The author-recipient-topic model for topicand role discovery in social networks: Experimentswith enron and academic email.
Technical report,University of Massachusetts Amherst.Mei, Q. and C. Zhai.
2006.
A mixture model for con-textual text mining.
In Proceedings of KDD-2006,pages 649?655.Mei, Q., D. Cai, D. Zhang, and C. Zhai.
2008.
Topicmodeling with network regularization.
In Proceed-ing of WWW-08:, pages 101?110.Radev, D., M. Joseph, B. Gibson, and P. Muthukrish-nan.
2009.
A Bibliometric and Network Analysisof the field of Computational Linguistics.
Journalof the American Society for Information Science andTechnology.Ratinov, L. and D. Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProc.
of the Annual Conference on ComputationalNatural Language Learning (CoNLL).Rosen-Zvi, M., T. Griffiths, M. Steyvers, and P. Smyth.2004.
the author-topic model for authors and docu-ments.
In Proceedings of UAI.Serdyukov, P., H. Rode, and D. Hiemstra.
2008.
Mod-eling multi-step relevance propagation for expertfinding.
In Proceedings of CIKM?08.Song, Y., J. Huang, and I. Councill.
2007.
Efficienttopic-based unsupervised name disambiguation.
InProceedings of JCDL-2007, pages 342?351.Steyvers, M. and T. Griffiths.
2007.
Probabilistic topicmodels.
In Handbook of Latent Semantic Analysis.Lawrence Erlbaum Associates.Steyvers, M., P. Smyth, and T. Griffiths.
2004.
Proba-bilistic author-topic models for information discov-ery.
In Proceedings of KDD.Sun, Y., J. Han, J. Gao, and Y. Yu.
2009. itopicmodel:Information network-integrated topic modeling.
InProceedings of ICDM-2009.Tang, J., J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su.2008.
Arnetminer: Extraction and mining of aca-demic social networks.
In Proceedings of KDD-2008, pages 990?998.Wallach, H. 2006.
Topic modeling; beyond bag ofwords.
In International Conference on MachineLearning.Wang, X. and A. McCallum.
2005.
A note on topi-cal n-grams.
Technical report, University of Mas-sachusetts.Wang, X., A. McCallum, and X. Wei.
2007.
Topicaln-grams: Phrase and topic discoery with an appli-cation to information retrieval.
In Proceedings ofICDM.Zhang, J., M. Ackerman, and L. Adamic.
2007.
Ex-pertise networks in online communities: Structureand algorithms.
In Proceedings of WWW 2007.1273
