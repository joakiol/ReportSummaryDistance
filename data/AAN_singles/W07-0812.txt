Proceedings of the 5th Workshop on Important Unresolved Matters, pages 89?96,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsImproved Arabic Base Phrase Chunking with a new enriched POS tag setMona T. DiabCenter for Computational Learning SystemsColumbia Universitymdiab@cs.columbia.eduAbstractBase Phrase Chunking (BPC) or shallowsyntactic parsing is proving to be a task ofinterest to many natural language processingapplications.
In this paper, A BPC systemis introduced that improves over state of theart performance in BPC using a new part ofspeech tag (POS) set.
The new POS tag set,ERTS, reflects some of the morphologicalfeatures specific to Modern Standard Ara-bic.
ERTS explicitly encodes definiteness,number and gender information increasingthe number of tags from 25 in the standardLDC reduced tag set to 75 tags.
For the BPCtask, we introduce a more language specificset of definitions for the base phrase anno-tations.
We employ a support vector ma-chine approach for both the POS tagging andthe BPC processes.
The POS tagging per-formance using this enriched tag set, ERTS,is at 96.13% accuracy.
In the BPC exper-iments, we vary the feature set alng twofactors: the POS tag set and a set of explic-itly encoded morphological features.
Us-ing the ERTS POS tagset, BPC achieves thehighest overall F?=1 of 96.33% on 10 dif-ferent chunk types outperforming the use ofthe standard POS tag set even when explicitmorphological features are present.1 IntroductionBase Phrase Chunking (BPC), also known as shal-low syntactic parsing, is the process by which ad-jacent words are grouped together to form non-recursive chunks in a sentence.
The base chunksform phrases such as verb phrases, noun phrasesand adjective phrases, etc.
An English example ofbase phrases is [I]NP [would eat]V P [red lusciousapples]NP [on Sundays]PP .
The BPC task is prov-ing to be an enabling step that is useful to many nat-ural language processing (NLP) applications suchas information extraction and semantic role label-ing (Hacioglu & Ward, 2003).
In English, these ap-plications have shown robust relative performancewhen exploiting BPC when compared to using fullsyntactic parses.
In general, BPC is appealing asan enabling technology since state of the art perfor-mance for BPC is higher (F?=1 95.48%) than thatfor full syntactic parsing (F?=1 90.02% ) in English(Collins, 2000; Kudo & Matsumato, 2000).
More-over, since BPC had been cast as a classificationproblem by Ramshaw and Marcus (1995), the taskis performed with greater efficiency and is easilyportable to new languages in a supervised manner(Diab et al, 2004; Diab et al, 2007).
For Arabic,BPC is especially interesting as it constitutes a vi-able alternative to full syntactic parsing due to thelow performance in Arabic full syntactic parsing (la-beled F?=1 = ?80% compared to F?=1=91.44% forBPC) (Bikel, 2004; Diab et al, 2007; Kulick et al,2006).In this paper, we present a support vector machine(SVM) based supervised method for Arabic BPC.The new BPC system achieves an F?=1 of 96.33%across 10 base phrase chunk types.
We vary thefeature sets along two different factors: the usageof explicit morphological features, and the use ofdifferent part of speech (POS) tag sets.
We intro-duce a new enriched POS set for Arabic which com-prises 75 POS tags.
The new POS tag set enrichesthe standard reduced POS tag set (RTS), distributed89with the LDC Arabic Treebank (ATB) (Maamouriet al, 2004), by adding definiteness, gender andnumber information to the basic RTS.
We devisean automatic POS tagger based on the enriched tagset, ERTS.
We use the same unified discriminativemodel approach to both POS tagging and BPC.
ThePOS tagging results using ERTS are comparable tostate of the art POS tagging using RTS with an ac-curacy of 96.13% for ERTS and 96.15% for RTS.However, using the ERTS as a feature in the BPCtask, we note an overall significant increase of 1%absolute F?=1 improvement over the usage of RTSalone.
Moreover, we experiment with different ex-plicit morphological features together with the dif-ferent POS tag sets with no significant improvement.The paper is laid out as follows: Section 2 dis-cusses state of the art related work in the field ofBPC in both English and Arabic; Section 3 illus-trates our approach for both POS and BPC in de-tail; Section 4 presents the experimental setup, re-sults and discussions.2 Related WorkEnglish BPC has made a lot of head way in recentyears.
It was first cast as a classification problemby Ramshaw and Marcus (1995), as a problem ofNP chunking.
Then it was extended to include othertypes of base phrases by Sang and Buchholz (2000)in the CoNLL 2000 shared task.
Most successful ap-proaches are based on machine learning techniquesand sequence modeling of the different labels as-sociated with the chunks.
Both generative algo-rithms such as HMMs and multilevel Markov mod-els, as well as, discriminative methods such as Sup-port Vector Machines (SVM) and conditional ran-dom fields have been used for the BPC task.
Theclosest relevant approach to the current investigationis the work of Kudo and Matsumato (2000) (KM00)on using SVMs and a sequence model for chunking.A la Ramshaw and Marcus (1995), they representthe words as a sequence of labeled words with IOBannotations, where the B marks a word at the begin-ning of a chunk, I marks a word inside a chunk, andO marks those words (and punctuation) that are out-side chunks.
The IOB annotation scheme for the En-glish example described earlier is illustrated in Table1.word IOB labelI B-NPwould B-VPeat I-VPred B-NPluscious I-NPapples I-NPon B-PPSundays I-PP.
OTable 1: IOB annotation exampleKM00 develop a sequence model, YAMCHA, overthe labeled sequences of words.1 YAMCHA is basedon the TinySVM algorithm (Joachims, 1998).
Theyuse a degree 2 polynomial kernel.
In their work onthe task of English BPC, YAMCHA achieves an over-all F?=1 of 93.48% on five chunk types.
They reportimproved results of 93.91% using a combined votingscheme (Kudo & Matsumato, 2000).As far as Arabic BPC, Diab et al (2004 & 2007)adopt the KM00 model for Arabic using YAMCHA.They cast the Arabic data from the ATB in the IOBannotation scheme.
They use the reduced standardLDC tag set, RTS, as their only feature.
Their sys-tem achieves an overall F?=1 of 91.44% on 9 chunktypes.3 Current ApproachThis paper is a significant extension to the Diab etal.
(2007) work.
Similar to other researchers in thearea of BPC, we adopt a discriminative approach.A la Ramshaw and Marcus (1995), and Kudo andMatsumato (2000), we use the IOB tagging style formodeling and classification.Various machine learning approaches have beenapplied to POS and BPC tagging, by casting them asclassification tasks.
Given a set of features extractedfrom the linguistic context, a classifier predicts thePOS or BPC class of a token.
SVMs (Vapnik, 1995)are one such supervised machine learning algorithm,with the advantages of: discriminative training, ro-bustness and a capability to handle a large number of(overlapping) features with good generalization per-1http://www.chasen.org/ taku/software/yamcha/90formance.
Consequently, SVMs have been appliedin many NLP tasks with great success (Joachims,1998; Hacioglu & Ward, 2003).We adopt a unified tagging perspective for boththe POS tagging and the BPC tasks.
We addressthem using the same SVM experimental setup whichcomprises a standard SVM as a multi-class classifier(Allwein et al, 2000).A la KM00, we use the YAMCHA sequence modelon the SVMs to take advantage of the context of theitems being compared in a vertical manner in addi-tion to the encoded features in the horizontal inputof the vectors.
Accordingly, in our different tasks,we define the notion of context to be a window offixed size around the segment in focus for learningand tagging.3.1 POS TaggingModern Standard Arabic is a rich morphologicallanguage, where words are explicitly marked forcase, gender, number, definiteness, mood, person,voice, tense and other features.
These morpholog-ical features are explicitly encoded in the full tagset provided in the ATB.
These full morphologicaltags amount to over 2000 tag types (FULL).
As ex-pected, such morphological tags are not very use-ful for automatic statistical syntactic parsing sincethey are extremely sparse.2 Hence, the LDC intro-duced the reduced tag set (RTS) of 25 tags.
RTSmasks case, mood, gender, person, definiteness forall categories.
It maintains voice and tense for verbs,and some number information for nouns, namely,marking plural vs. singular for nouns and propernouns.
Therefore, in the process it masks duality fornouns and number for all adjectives.
It should benoted, however, that it is extremely useful to havethese morphological tags in order to induce features.There exists a system that produces the full morpho-logical POS tag set, MADA, with very high accu-racy, 96% (Habash & Rambow, 2005).In this work, we introduce a new tag set that ex-plicitly marks gender, number, and definiteness fornominals (namely, nouns, proper nouns, adjectivesand pronouns).
Verbs, particles, as well as, the per-son feature on pronouns, are not affected by this en-richment process, since neither person nor mood are2Dan Bikel, personal communication.explicitly encoded.
Morphological case is also notexplicitly encoded in ERTS.
The new tag set, ERTS,is derived from the FULL tag set where there is anexplicit specification in the tag itself for the differentfeatures to be encoded.
We restricted ERTS to thisset of features as they tend to be explicitly markedin the surface form of unvowelized Arabic text.
InERTS, definiteness is encoded with a present (D)or an absent one.
Gender is encoded with an F oran M, corresponding to Fem and Masc, respectively.Number is encoded with (Du) for dual or an (S) forplurals or the absence of any marking for singular.For example, Table 2 illustrates some words withthe FULL morphological tag and their correspond-ing RTS and ERTS definitions.3Our approach: ERTS comprises 75 tags.
For thecurrent system, only 57 tags are instantiated.
Wedevelop a POS tagger based on this new set.
Weadopt the YAMCHA sequence model based on theTinySVM classifier.
The tagger trained for ERTStag set uses lexical features of +/-4 character n-grams from the beginning and end of a word infocus.
The context for YAMCHA is defined as +/-2 words around the focus word.
The words be-fore the focus word are considered with their ERTStags.
The kernel is a polynomial degree 2 kernel.We adopt the one-vs-all approach for classification,where the tagged examples for one class are con-sidered positive training examples and instances forother classes are considered negative examples.
Wepresent results and a brief discussion of the POS tag-ging performance in Section 4.3.2 Base Phrase ChunkingIn this task, we use a setup similar to that of Kudo& Matsumato (2000) and Diab et al (2004 & 2007),with the IOB annotation representation: Inside I aphrase, Outside O a phrase, and Beginning B of aphrase.
However, we designate 10 types of chunkedphrases.
The chunk phrases identified for Arabicare ADJP, ADVP, CONJP, INTJP, NP, PP, PREDP,PRTP, SBARP, VP.
Thus the task is a one of 21 clas-sification task (since there are I and B tags for eachchunk phrase type, and a single O tag).
The 21IOB tags are listed: {O, I-ADJP, B-ADJP, I-ADVP,3All the romanized Arabic is presented in the Buckwaltertransliteration scheme (Buckwalter, 2002).91Gloss FULL RTS ERTS  HSylp ?outcome?
NOUN+NSUFF FEM SG+CASE IND NOM NN NNF    nhA}yp ?final?
ADJ+NSUFF FEM SG+CASE IND NOM JJ JJF  HAdv ?accident?
NOUN+CASE DEF ACC NN NNMfffffiffifl AlnAr ?the-fire?
DET+NOUN+CASE DEF GEN NN DNNM! "$# % &fl AljmAEy ?group?
DET+ADJ+CASE DEF GEN JJ DJJM')( # * +$xSyn ?two persons?
NOUN+NSUFF MASC DU GEN NN NNMDuTable 2: Examples of POS tag sets RTS, ERTS and FULLB-ADVP, I-CONJP, B-CONJP, I-INTJP, B-INTJP, I-NP, B-NP, I-PP, B-PP, I-PREDP, B-PREDP, I-PRTP,B-PRTP, I-SBARP, B-SBARP, I-VP, B-VP}.The training data is derived from the ATB usingthe ChunkLink software.4 ChunkLink flattensthe tree to a sequence of base (non-recursive) phrasechunks with their IOB labels.
For example, a to-ken occurring at the beginning of a noun phrase islabeled as B-NP.
The following Table 3 Arabic ex-ample illustrates the IOB annotation scheme:Tags B-VP B-NP I-NP OArabic , -/.
0 132 4"$# % &fl.Translit wqE msA?
AljmEp .Gloss happened night the-Friday .Table 3: An Arabic IOB annotation exampleChunklink, however, is tailored for Englishsyntactic structures.
Hence, in order to train on rea-sonable Arabic chunks, we modify Chunklink?soutput using linguistic knowledge of Arabic syntac-tic structures.
Some of the rules used are describedbelow (we illustrate using ERTS for ease of exposi-tion).?
IDAFA: This syntactic structure marks posses-sion in Arabic.
It is syntactically the case wherean indefinite noun is followed by a definite one.The Chunklink output is modified to ensure thatthey form a single NP.
Example:  4 "$# % & fl50  132 msA?AljmEp ?night of Friday?
is IOB annotated as [msA?DNN B-NP, AljmEp DNNM I-NP].?
NOUN-ADJ: Nouns followed by adjectives andthey agree in their morphological features of gender,number and definiteness form a single NP chunk.4http://ilk.uvt.nl/ sabine/chunklinkExample: 6 !7 8+ ff $    ffHSylp nhA}yp rsmyp?final official outcome?
is IOB annotated as [HSylpNNF B-NP, nhA}yp JJF I-NP, rsmyp JJF I-NP]?
Pronouns: This is an artifact of the ATB styleof clitic tokenization.
All pronouns, except in nom-inative position in the sentence such as hw and hy,are chunk internal.?
Interjections: If an interjection is followed bya noun, the noun is marked as internal to the inter-jective phrase.?
Prepositional Phrases: Nouns followingprepositions are considered internal to the preposi-tional phrase and are IOB annotated, I-PP.Phrase Types: The different phrase types are de-scribed as follows.?
ADJP: This is an adjectival phrase.
The ad-jectival phrase could comprise a single adjective ifmentioned in isolation such as fl:9; !
% jydA ?well?, ormultiple words such as fl:9 <%=  %?>  - qrybA jdA ?verysoon?.
The latter is IOB annotated [qrybA B-ADJP]and [jdA I-ADJP], respectively.?
ADVP: This is an adverbial phrase.
It may com-prise a single adverb following a verb, such as  4:>ffi@ +sryEA ?quickly?, or multiple words such as  A 'CB filkn hA ?but she?.5 The latter is IOB annotated [lknB-ADVP] and [hA I-ADVP], respectively.?
CONJP: This chunk marks conjunctivephrases.
We see single word CONJP when the con-junction appears before a verb phrase or a preposi-tional phrase such the conjunction .
w ?and?.
Butwe also have multiword conjunctive phrases whenthe conjunction is followed by a noun.
For instance,DFE G/ :H1IfiffiflF.
w AlflsTynywn ?and the Palestinians?5This is a result of the ATB clitic tokenization style.92is IOB annotated [w B-CONJP] and [AlflsTynywn I-CONJP].?
INTJP: This is an interjective phrase.
The in-terjective phrase could comprise a single interjectionif mentioned in isolation such as   4 nEm ?yes?.
Orwith multiple words such as   	 fl  yA Axt ?Oh sis-ter?, where it is IOB annotated [yA B-INTJP] and[Axt I-INTJP].?
NP: This is a noun phrase.
It may comprise asingle noun or multiple nouns or a noun and one ormore adjectives.
In this phrase type, we see typi-cal noun adjective constructions as in     "$# % & fl   - > fiffiflAlzfAf AljmAEy ?the group wedding?, where the ini-tial noun is marked as [AlzfAf B-NP], and the folow-ing adjective is IOB annotated [AljmAEy I-NP].
Wealso encounter idafa constructions.
For example,Dfl 2 mlk AlArdn ?king of Jordan?, where mlk?king?
is an indefinite noun, and AlArdn ?Jordan?
isa definite one.
This phrase is IOB annotated [mlkB-NP] and [AlArdn I-NP].?
PP: This is a prepositional phrase.
The phrasestarts with a preposition followed by a pronoun,noun or proper noun.
If the noun or proper noun is it-self the beginning of an NP, the whole NP is internalto the PP.
For example,  !
 "$# % & fl   - > fiffifl IxlAl Hfl AlzfAf AljmAEy ?during the group weddingparty?, xlAl is the preposition and is IOB annotated[xlAl B-PP], [Hfl I-PP] (if Hfl were not preceded bya preposition it would have been annotated [Hfl B-NP]), then for the noun [AlzfAf I-PP], and finally theadjective [AljmAEy I-PP].?
PREDP: This is a predicative phrase.
Ittypically begins with a particle D flAn ?
[is]?,followed by a noun phrase.
For example,'9%7  fl " 29;fiffiflflDflAn AlASlAH Al-dyny mhmp Almjddyn ?religious improvement is thereformers?
task?.
In our data, since we do not markrecursive structures in this BPC level, only the pred-icative particle is IOB annotated with B-PREDP.
If itwere followed by a possessive pronoun, the pronounis annotated I-PREDP.?
PRTP: This phrase type marks particles such asnegative particles that precede both nouns and verbs.A particle could be single word or a complex particlephrase.
An example of a simple word particle is  fi lm?not?, and a complex one is  "   lA sy?mA ?notas long?.
In the latter case, it is IOB annotated [lAB-PRTP] and [sy?mA I-PRTP].?
SBARP: This phrase structure marks the sub-junctive constructions.
SBARP phrases typically be-gin with a particle meaning ?that?
such as Dfl An or "$2 mmA or ff  9 fiffifl Al*y followed by a verb phrase.?
VP: This is a verb phrase.
VP phrases are typ-ically headed by a verb.
All object pronouns pro-ceeding a verb are IOB annotated I-VP.
Moreover,we observe cases where a VP is headed by nominals(nouns and adjectives, in particular).
The majorityof these nominals are the active participle.
The ac-tive participle in the ATB is tagged as an adjective.Active participles in Arabic are equivalent to pred-icative nominals in English (gerunds).
Hence, someVPs are headed by JJs.
An example active partici-ple heading a verb phrase in our data is  "$  fi 2 mthmA?accusing?.Our Approach: We vary two factors in our fea-ture sets: the POS tag set, and the presence or ab-sence of explicit morphological features.
We havethree possible tag sets: RTS, ERTS and the full mor-phological tag set (FULL).
We define a set of 6morphological features (and their possible values):CASE (ACC, GEN, NOM, NULL), MOOD (Indica-tive, Jussive, Subjunctive, NULL), DEF (DEF, IN-DEF, NULL), NUM (Sing, Dual, Plural, NULL),GEN (Fem, Masc, NULL), PER (1, 2, 3, NULL).From the intersection of the two factors, we devise10 different experimental conditions.
The condi-tions always have one of the POS tag sets and eitherno explicit features (noFeat), all explicit features(allFeat), or some selective features of: case moodand person (CASE MOOD PER), or definitenessgender and number (DEF GEN NUM).
Therefore,the experimental conditions are as follows: RTS-noFeat, RTS-allFeat, RTS-CASE MOOD PER,RTS-DEF GEN NUM, ERTS-noFeat, ERTS-allFeat, ERTS-CASE MOOD PER, ERTS-DEF GEN NUM, FULL-noFeat, FULL-allFeat.The BPC context is defined as a window of +/?2tokens centered around the focus word where all thefeatures for the specific condition are used and thetags for the previous two tokens before the focus to-ken are also considered.934 Experiments and Results4.1 DataThe dev, test and training data are obtained fromATB1v3, ATB2v2 and ATB3v2 (Maamouri etal., 2004).
We adopt the same data splits intro-duced by Chiang et.
al (2006).
The corpora are allnews genre.
The total development data comprises2304 sentences and 70188 tokens, the total trainingdata comprises 18970 sentences and 594683 tokens,and the total test data comprises 2337 sentences and69665 tokens.We use the unvocalized Buckwalter transliteratedversion of the ATB.
For both POS tagging and BPC,we use the gold annotations of the training and testdata for preprocessing.
Hence, for POS tagging, thetraining and test data are both gold tokenized in theATB clitic tokenization style.
And for BPC, the POStags, the morphological features, and, the tokeniza-tion is all gold.
We derive the gold ERTS determin-istically from the FULL set for the BPC results re-ported here.The IOB annotations on the training and goldevaluation data are derived using Chunklink fol-lowed by our linguistic fixes described in Section 3.4.2 SVM SetupWe use the default values for YAMCHA with the Cparameter set to 0.5.
It has a degree 2 polynomialkernel.
YAMCHA adopts a one-vs-all binarizationmethod.4.3 Evaluation MetricStandard metrics of Accuracy (Acc.
), Precision, Re-call, and F-measure F?=1, on the test data are uti-lized.
For both POS tagging and BPC, we use theCoNLL shared task evaluation tools.64.4 Results4.4.1 ERTS POS Tagging ResultsTable 4 shows the results obtained with the YAM-CHA based POS tagger, POS-TAG, and the resultsobtained with a simple baseline, BASELINE.
BASE-LINE is a supervised baseline, where the most fre-quent POS tag associated with a token from thetraining data is assigned to it in the test set, regard-less of context.
If the token does not occur in the6http://cnts.uia.ac.be/conll2003/ner/bin/conllevaltraining data, the token is assigned the NN tag as adefault tag.POS tagset Acc.%RTS 96.15ERTS 96.13BASELINE 86.5Table 4: Results of POS-TAG on two different tagsets RTS and ERTSPOS-TAG clearly outperforms the most frequentbaseline.
Looking closely at the data, the worst ob-tained results are for the NO FUNC category, as itis randomly confusable with almost all POS tags.Then, the imperative verbs are mostly confused withpassive verbs 50% of the time, however the test dataonly comprises 8 imperative verbs.
VBN, passiveverbs, yields an accuracy of 68% only.
It is worthnoting that the most frequent baseline for VBN is21%.
VBN is a most difficult category to discernin the absence of the passivization diacritic which isnaturally absent in unvowelized text (our experimen-tal setup).
The overall performance on the nouns andadjectives is relatively high.
However, confusingthese two categories is almost always present dueto the inherent ambiguity.
In fact, almost all Arabicadjectives could be used as nouns in Arabic.74.4.2 Base Phrase Chunking (BPC)Table 5 illustrates the overall obtained results byour BPC system over the different experimental con-ditions.The overall results for all the conditions signif-icantly outperform state of the art published resultson Arabic BPC of F?=1=91.44% in Diab et al (2004& 2007).
This is mainly attributed to the betterquality annotations associated with tailoring of theChunklink IOB annotations to the Arabic lan-guage characteristics.All the F?=1 results yielded by ERTS POS tagset outperform their counterparts using the RTS POStagset.
In fact, ERTS-noFeat condition outperformsall other conditions in our experiments.We note that adding morphological features tothe RTS POS tag set helps the performance slightly7This inherent ambiguity leads to inconsistency in the ATBgold annotations.94Condition F?=1 Condition F?=1RTS-noFeat 95.41 ERTS-noFeat 96.33RTS-CASE MOOD PER 95.73 ERTS-CASE MOOD PER 96.32RTS-DEF GEN NUM 95.8 ERTS-DEF GEN NUM 96.33RTS-allFeat 95.97 ERTS-allFeat 96.25FULL-noFeat 96.29 FULL-allFeat 96.22Table 5: Overall F?=1 % results yielded for the different BPC experimental conditionsas we see a sequence of small jumps in perfor-mance from RTS-noFeat (95.41%) to RTS-allFeat(95.97%).
However adding these features to theERTS and FULL conditions does not help.
In fact, inboth the allFeat conditionsfor both ERTS and FULL,we note a slight decrease.
The ERTS condition per-formance goes down from 96.33% (ERTS-noFeat)to 96.25% (ERTS-allFeat), and the FULL condi-tion performance goes down from 96.29% (FULL-noFeat) to 96.22% (FULL-allFeat).
This suggeststhat the features are not adding much informationover and above what is already encoded in the POStag set, and, in fact adding the explicit morphologi-cal features might be adding noise.There is no significant difference between usingERTS and FULL in the overall results.
However, wenote that ERTS conditions slightly outperform theFULL conditions.
This may be attributed the consis-tency introduced by ERTS over FULL, i.e., if FULLis not consistent in assigning CASE or MOOD orPER, for instance, ERTS, being insensitive to thesefeatures masks these inconsistencies present in theFULL tag set.RTS-DEF GEN NUM may be viewed as an ex-plicit encoding of the features in ERTS-noFeat, how-ever, ERTS-noFeat outperforms it.
Explicitly encod-ing the CASE MOOD and PER features does nothelp ERTS, in fact we see a slight drop in overallperformance.
However, upon closer inspection ofthe results per phrase type, we note slight relativeimprovement on PRTP and VP chunk types perfor-mance when the CASE, MOOD and PER are ex-plicitly encoded.
In ERTS-CASE MOOD PER, VPyields an F?=1 of 99.3% and PRTP yields 97.2%,corresponding to ERTS-noFeat where VP yields anF?=1 of 99.2% and PRTP an F?=1 of 96.8%.To better assess the quality of the performanceand impact of the new POS tag set, we examineclosely in Table 6 the phrase types directly affectedby the added information whether encoded in thePOS tag set or explicitly used as independent fea-tures.
These phrase types are the ADJP, INTJP, NPand PP.
The PP scored highly across the board withF?=1 over 99% for all conditions, hence, it is notincluded in Table 6.Condition ADJP INTJP NPRTS-noFeat 68.42 55.17 92.98RTS-CASE MOOD PER 69.47 59.26 93.72RTS-DEF GEN NUM 71.57 64.29 93.71RTS-allFeat 72.22 57.14 94.15ERTS-noFeat 72.35 57.14 94.92ERTS-CASE MOOD PER 73.16 61.54 94.86ERTS-DEF GEN NUM 72.6 64.29 94.92ERTS-allFeat 72.91 59.26 94.78FULL-noFeat 71.84 51.85 94.81FULL-allFeat 72.52 57.14 94.67Table 6: F?=1 Results for ADJP, INTJP, and NP,across the different experimental conditionsAs illustrated in Table 6, ERTS outperforms RTSand FULL in all corresponding conditions wherethey have similar corresponding morphological fea-ture settings.
ERTS-noFeat yields better results thanRTS-noFeat and FULL-noFeat for the three differ-ent phrase types.
The INTJP phrase is the worstperforming of the three phrase types, however itmarks the most significant change in performancedepending on the experimental condition.
We notethat adding explicit morphological features to thebase condition RTS yields consistently better re-sults for the three phrase types.
The highest per-formance for NP is yielded by ERTS-noFeat andERTS-DEF GEN NUM with an F?=1 of 94.92%.95The highest scores yielded for ADJP (73.16) and IN-TJP (64.29) are in an ERTS experimental condition.We also observe a slight drop in performance in NPfor the ERTS conditions when the CASE MOODand PER features are added.
This might be due tothe inconsistent or confusable assignment of thesedifferent features in the ATB.5 Conclusions and Future WorkIn this paper, we address the problem of Arabic basephrase chunking, BPC.
In the process, we introducea new enriched POS tag set, ERTS, that adds def-initeness, gender and number information to nomi-nals.
We present an SVM approach to both the POStagging with ERTS and the BPC tasks.
The POStagger yields 96.13% accuracy which is comparableto the results obtained on the standard reduced tagset RTS.
On the BPC front, the results obtained forall conditions are significantly better than state ofthe art published results.
This indicates that betterlinguistic tailoring of the IOB chunks creates moreconsistent data.
Overall, we show that using the en-riched POS tag set, ERTS, yields the best BPC per-formance.
Even using ERTS with no explicit mor-phological features yields better results than usingRTS in all conditions with or without explicit mor-phological features.
These results are confirmed byclosely observing specific phrases that are directlyaffected by the change in POS tag set namely, ADJP,INTJP and NP.
Our results strongly suggests thatchoosing the POS tag set carefully has a significantimpact on higher level syntactic processing.6 AcknowledgementsThis work was funded by DARPA Contract No.
HR0011-06-C-0023.
Any opinions, findings and conclusions or recommenda-tions expressed in this material are those of the author and donot necessarily reflect the views of DARPA.ReferencesErin L. Allwein, Robert E. Schapire, and Yoram Singer.2000.
Reducing multiclass to binary: A unifying ap-proach for margin classifiers.
Journal of MachineLearning Research, 1:113-141.Daniel Bikel.
2004.
Intricacies of Collins Parser.
Com-putational Linguistics.Tim Buckwalter.
2002.
Buckwalter Arabic Morphologi-cal Analyzer Version 1.0.
Linguistic Data Consortium,University of Pennsylvania, 2002.
LDC Catalog No.
:LDC2002L49David Chiang, Mona Diab, Nizar Habash, Owen Ram-bow, and Safiullah Shareef.
2006.
Parsing Arabic Di-alects.
Proceedings of the European Chapter of ACL(EACL).Michael Collins.
2000.
Discriminative Reranking forNatural Language Parsing.
Proceedings of the 17thInternational Conference on Machine Learning.Mona Diab, Kadri Hacioglu and Daniel Jurafsky.
2004.Automatic Tagging of Arabic Text: From Raw Text toBase Phrase Chunks.
Proceedings of North AmericanAssociation for Computational Linguistics.Mona Diab, Kadri Hacioglu and Daniel Jurafsky.
2007.Automated Methods for Processing Arabic Text: FromTokenization to Base Phrase Chunking.
Book Chapter.In Arabic Computational Morphology: Knowledge-based and Empirical Methods.
Editors Antal van denBosch and Abdelhadi Soudi.
Kluwer/Springer Publi-cations.Nizar Habash and Owen Rambow.
2005.
ArabicTokenization, Morphological Analysis, and Part-of-Speech Tagging in One Fell Swoop.
Proceedings ofthe Conference of American Association for Compu-tational Linguistics (ACL).Kadri Hacioglu and Wayne Ward.
2003.
Target wordDetection and semantic role chunking using supportvector machines.
HLT-NAACL.Thorsten Joachims.
1998.
Text Categorization with Sup-port Vector Machines: Learning with Many RelevantFeatures.
Proc.
of ECML-98, 10th European Conf.
onMachine Learning.Seth Kulick, Ryan Gabbard, and Mitch Marcus.
2006.Parsing the Arabic Treebank: Analysis and Improve-ments.
in Treebanks and Linguistic Theories.Taku Kudo and Yuji Matsumato.
2000.
Use of supportvector learning for chunk identification.
Proc.
of the4th Conf.
on Very Large Corpora, pages 142-144.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004.
The Penn Arabic Treebank: Building a Large-Scale Annota ted Arabic Corpus.NEMLAR Conference on Arabic Language Resourcesand Tools.
pp.
102-109.Lance A. Ramshaw and Mitchell P. Marcus.
1995.Text Chunking using transformational based learning.Proc.
of the 3rd ACL workshop on Very Large CorporaErik Tjong, Kim Sang, and Sabine Buchholz.
2000.
In-troduction to the CoNLL-2000 shared task: Chunking.Proc.
of the 4th Conf.
on Computational Natural Lan-guage Learning (CoNLL), Lisbon, Portugal, 2000, pp.127-132.Vladamir Vapnik.
1995.
The Nature of Statistical Learn-ing Theory.
Springer Verlag, New York, USA.96
