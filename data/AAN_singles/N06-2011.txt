Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 41?44,New York, June 2006. c?2006 Association for Computational LinguisticsSpectral Clustering for Example Based Machine TranslationRashmi GangadharaiahLTICarnegie Mellon UniversityPittsburgh P.A.
15213rgangadh@andrew.cmu.eduRalf BrownLTICarnegie Mellon UniversityPittsburgh P.A.
15213ralf@cs.cmu.eduJaime CarbonellLTICarnegie Mellon UniversityPittsburgh P.A.
15213jgc@cs.cmu.eduAbstractPrior work has shown that generaliza-tion of data in an Example Based Ma-chine Translation (EBMT) system, re-duces the amount of pre-translated text re-quired to achieve a certain level of accu-racy (Brown, 2000).
Several word clus-tering algorithms have been suggested toperform these generalizations, such as k-Means clustering or Group Average Clus-tering.
The hypothesis is that better con-textual clustering can lead to better trans-lation accuracy with limited training data.In this paper, we use a form of spectralclustering to cluster words, and this isshown to result in as much as 29.08% im-provement over the baseline EBMT sys-tem.1 IntroductionIn EBMT, the source sentence to be translatedis matched against the source language sentencespresent in a corpus of source-target sentence pairs.When a partial match is found, the correspondingtarget translations are obtained through subsenten-tial alignment.
These partial matches are put to-gether to obtain the final translation by optimizingtranslation and alignment scores and using a statisti-cal target language model in the decoding process.Prior work has shown that EBMT requires largeamounts of data (in the order of two to three mil-lion words) (Brown, 2000) of pre-translated text, tofunction reasonably well.
Thus, some modificationof the basic EBMT method is required to make it ef-fective when less data is available.
In order to usethe available text efficiently, systems such as, (Vealeand Way, 1997) and (Brown, 1999), convert the ex-amples in the corpus into templates against whichthe new text can be matched.
Thus, source-targetsentence pairs are converted to source-target gener-alized template pairs.
An example of such a pair isshown below:The session opened at 2p.mLa se?ance est ouverte a?
2 heuresThe <event> <verb-past-tense> at <time>La <event> <verb-past-tense> a <time>This single template can be used to translate differ-ent source sentences, including for example,The session adjourned at 6p.mThe seminar opened at 8a.mif ?session?
and ?seminar?
are both generalized to?<event>?, ?opened?
and ?adjourned?
are both gen-eralized to ?<verb-past-tense>?
and finally ?6p.m?and ?8a.m?
are both generalized to ?<time>?.The system used by (Brown, 1999) performsits generalization using both equivalence classes ofwords and a production rule grammar.
This paperdescribes the use of spectral clustering (Ng.
et.
al.,2001; Zelnik-Manor and Perona, 2004), for auto-mated extraction of equivalence classes.
Spectralclustering is seen to be superior to Group AverageClustering (GAC) (Brown, 2000) both in terms ofsemantic similarity of words falling in a single clus-ter, and overall BLEU score (Papineni.
et.
al., 2002)in a large scale EBMT system.The next section explains the term vectors ex-tracted for each word, which are then used to clusterwords into equivalence classes and provides an out-line of the Standard GAC algorithm.
Section 3 de-scribes the spectral clustering algorithm used.
Sec-41tion 4 lists results obtained in a full evaluation of thealgorithm.
Section 5 concludes and discusses direc-tions for future work.2 Term vectors for clusteringUsing a bilingual dictionary, usually created usingstatistical methods such as those of (Brown et.
al.,1990) or (Brown, 1997), and the parallel text, arough mapping between source and target words canbe created.
This word pair is then treated as an in-divisible token for future processing.
For each suchword pair we then accumulate counts for each to-ken in the surrounding context of its occurrences(N words, currently 3, immediately prior to andN words immediately following).
The counts areweighted with respect to distance from occurrence,with a linear decay (from 1 to 1/N) to give great-est importance to the words immediately adjacent tothe word pair being examined.
These counts form apseudo-document for each pair, which are then con-verted into term vectors for clustering.In this paper, we compare our algorithm againstthe incremental GAC algorithm(Brown, 2000).
Thismethod examines each word pair in turn, comput-ing a similarity measure to every existing cluster.If the best similarity measure is above a predeter-mined threshold, the new word is placed in the cor-responding cluster, otherwise a new cluster is cre-ated if the maximum number of clusters has not yetbeen reached.3 Spectral clusteringSpectral clustering is a general term used to de-scribe a group of algorithms that cluster points usingthe eigenvalues of ?distance matrices?
obtained fromdata.
In our case, the algorithm described in (Ng.et.
al., 2001) was performed with certain variationsthat were proposed by (Zelnik-Manor and Perona,2004) to compute the scaling factors automaticallyand for the k-Means orthogonal treatment (Vermaand Meila, 2003) during the initialization.
Thesescaling factors help in self-tuning distances betweenpoints according to the local statistics of the neigh-borhoods of the points.
The algorithm is briefly de-scribed below.1.
Let S =s1, s2, ....sn, denote the term vectors tobe clustered into k classes.2.
Form the affinity matrix A defined byAij = exp(?d2(si, sj)/?i?j) for i 6= jAii = 1Where, d(si, sj) = 1/(sim(si, sj) + )sim(si, sj) is the Cosine similarity between siand sj ,  is used to prevent the ratio from be-coming infinity?i is the set of local scaling parameters for si.
?i = d(si, sT ) where, sT is the T th neighbor ofpoint si for some fixed T (7 for this paper).3.
Define D to be the diagonal matrix given by,Dii = ?jAij4.
Compute L = D?1/2AD?1/25.
Select k eigenvectors corresponding to klargest eigenvalues (k is presently an externallyset parameter).
The eigenvectors are normal-ized to have unit length.
Form matrix U bystacking all the eigenvectors in columns.6.
Form the matrix Y by normalizing U?s rows,Yij = Uij/?(?jU2ij)7.
Perform k-Means clustering treating each rowof Y as a point in k dimensions.
The k-Meansalgorithm is initialized either with random cen-ters or with orthogonal vectors.8.
After clustering, assign the point si to cluster cif the corresponding row i of the matrix Y wasassigned to cluster c.9.
Sum the distances between the members andthe centroid of each cluster to obtain the classi-fication cost.10.
Goto step 7, iterate for a fixed number of it-erations.
In this paper, 20 iterations were per-formed with orthogonal k-Means initializationand 5 iterations with random k-Means initial-ization.11.
The clusters obtained from the iteration withleast classification cost are selected as the kclusters.4 Preliminary ResultsThe clusters obtained from the spectral clusteringmethod are seen by inspection to correspond to morenatural and intuitive word classes than those ob-tained by GAC.
Even though this is subjective andnot guaranteed to lead to improve translation perfor-mance, it shows that maybe the increased power ofspectral clustering to represent non-convex classes42(non-convex in the term vector domain) could beuseful in a real translation experiment.
Some ex-ample classes are shown in Table 1.
The firstclass in an intuitive sense corresponds to measure-ment units.
We see that in the <units> case,GAC misses some of the members which are ac-tually distributed among many different classes andhence these are not well generalized.
In the secondclass <months>, spectral clustering has primarilythe months in a single class whereas GAC adds anumber of seemingly unrelated words to the clus-ter.
The classes were all obtained by finding 80clusters in a 20,000-sentence pair subset of the IBMHansard Corpus (Linguistic Data Consortium, 1997)for spectral clustering.
80 was chosen as the numberof clusters since it gave the highest BLEU score inthe evaluation.
For GAC, 300 clusters were used asthis gave the best performance.To show the effectiveness of the clustering meth-ods in an actual evaluation, we set up the followingexperiment for an English to French translation taskon the Hansard corpus.
The training data consists ofthree sets of size 10,000 (set1), 20,000 (set2) and30,000 (set3) sentence pairs chosen from the firstsix files of the Hansard Corpus.
Only sentences oflength 5 to 21 words were taken.
Only words withfrequency of occurrence greater than 9 were chosenfor clustering because more contextual informationwould be available when the word occurs frequentlyand this would help in obtaining better clusters.
Thetest data was chosen to be a set of 500 sentences ob-tained from files 20, 40, 60 and 80 of the Hansardcorpus with 125 sentences from each file.
Each ofthe methods was run with different number of clus-ters and results are reported only for the optimalnumber of clusters in each case.The results in Table 2 show that spectral clus-tering requires moderate amounts of data to get alarge improvement.
For small amounts of data it isslightly worse than GAC, but neither gives much im-provement over the baseline.
For larger amounts ofdata, again both methods are very similar, thoughspectral clustering is better.
Finally, for moderateamounts of data, when generalization is the mostuseful, spectral clustering gives a significant im-provement over the baseline as well as over GAC.By looking at the clusters obtained with varyingamounts of data, it can be concluded that high pu-Table 1: Clusters for <units> and <months>Spectral clustering GAC?adjourned?
?hre?
?adjourned?
?hre??cent?
?%??days?
?jours??families?
?familles?
?families?
?familles??hours?
?heures??million?
?millions?
?million?
?millions??minutes?
?minutes??o?clock?
?heures?
?o?clock?
?heures??p.m.?
?heures?
?p.m.?
?heures??p.m.?
?hre??people?
?personnes?
?people?
?personnes??per?
?%?
?per?
?%??times?
?fois?
?times?
?fois??years?
?ans??august?
?aou?t?
?august?
?aou?t??december?
?de?cembre?
?december?
?de?cembre??february?
?fe?vrier?
?february?
?fe?vrier??january?
?janvier?
?january?
?janvier??march?
?mars?
?march?
?mars??may?
?mai?
?may?
?mai??november?
?novembre?
?november?
?novembre??october?
?octobre?
?october?
?octobre??only?
?seulement?
?only?
?seulement??june?
?juin?
?june?
?juin??july?
?juillet?
?july?
?juillet??april?
?avril?
?april?
?avril??september?
?septembre?
?september?
?septembre??page?
?page??per?
?$??recognize?
?parole??recognized?
?parole??recorded?
?page??section?
?article??since?
?depuis?
?since?
?depuis??took?
?se?ance??under?
?loi?43Table 2: % Relative improvement over baseline EBMT# clus is the number of clusters for best performanceGAC Spectral% Rel imp #clus % Rel imp #clus10k 3.33 50 1.37 2020k 22.47 300 29.08 8030k 2.88 300 3.88 200rity clusters can be obtained with even just moderateamounts of data.5 Conclusions and future workFrom the experimental results we see that spectralclustering leads to relatively purer and more intu-itive clusters.
These clusters result in an improvedBLEU score in comparison with the clusters ob-tained through GAC.
GAC can only collect clustersin convex regions in the term vector space, whilespectral clustering is not limited in this regard.
Theability of spectral clustering to represent non-convexshapes arises due to the projection onto the eigen-vectors as described in (Ng.
et.
al., 2001).As future work, we would like to analyze thevariation in performance as the amount of data in-creases.
It is widely known that increasing theamount of training data in a generalized EBMT sys-tem eventually leads to saturation of performance,where all clustering methods perform about as wellas baseline.
Thus, all methods have an operating re-gion where they are the most useful.
We would liketo locate and extend this region for spectral cluster-ing.Also, it would be interesting to compare the clus-ters obtained with spectral clustering and the Part ofSpeech tags of the words in the same cluster, espe-cially for languages such as English where good tag-gers are available.Finally, an important direction of research is inautomatically selecting the number of clusters forthe clustering algorithm.
To do this, we could useinformation from the eigenvalues or the distributionof points in the clusters.AcknowledgmentThis work was funded by National Business Centeraward NBCHC050082.ReferencesAndrew Ng, Michael Jordan, and Yair Weiss 2001.
OnSpectral Clustering: Analysis and an algorithm.
In Ad-vances in Neural Information Processing Systems 14:Proceeding of the 2001 Conference, pages 849-856,Vancouver, British Columbia, Canada, December.Deepak Verma and Marina Meila.
2003.
Comparison ofSpectral Clustering Algorithms.
http://www.ms.washington.edu/?spectral/.Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu.
2002.
BLEU: a method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics (ACL 2002), pages 311-318,Philadelphia, PA, July.
http://acl.ldc.upenn.edu/P/P02Linguistic Data Consortium.
1997.
Hansard Corpus ofParallel English and French.
Linguistic Data Con-sortium, December.
http://www.ldc.upenn.edu/L.
Zelnik-Manor and P. Perona 2004 Self-Tuning Spec-tral Clustering.
In Advances in Neural InformationProcessing Systems 17: Proceeding of the 2004 Con-ference.Peter Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F.Jelinek, J. Lafferty, R. Mercer and P. Roossin.
1990.A Statistical Approach to Machine Translation.
Com-putational Linguistics, 16:79-85.Ralf D. Brown.
1997.
Automated Dictionary Extrac-tion for ?Knowledge-Free?
Example-Based Transla-tion.
In Proceedings of the Seventh International Con-ference on Theoretical and Methodological Issues inMachine Translation (TMI-97), pages 111-118, SantaFe, New Mexico, July.
http://www.cs.cmu.edu/?ralf/papers.htmlRalf D. Brown.
1999.
Adding Linguistic Knowledgeto a Lexical Example-Based Translation System.
InProceedings of the Eighth International Conferenceon Theoretical and Methodological Issues in MachineTranslation(TMI-99), pages 22-32, August.
http://www.cs.cmu.edu/?ralf/papers.htmlRalf.
D. Brown.
2000.
Automated Generalization ofTranslation Examples.
In Proceedings of EighteenthInternational Conference on Computational Linguis-tics (COLING-2000), pages 125-131, Saarbru?cken,Germany.Tony Veale and Andy Way.
1997.
Gaijin: A Template-Driven Bootstrapping Approach to Example-BasedMachine Translation.
In Proceedings of NeMNLP97,New Methods in Natural Language Processing, Sofia,Bulgaria, September.
http://www.compapp.dcu.ie/?tonyv/papers/gaijin.html.44
