COMPUTATIONAL TECHNIQUES FOR IMPROVED NAME SEARCHBeatrice T. OshikaSPARTA, Inc.2560 Ninth StreetSuite 315BBerkeley, CA 94710Bruce EvansTRWMS 02/1761One Space ParkRedondo Beach, CA 90278Filip MachiDepartment of MathematicsUniversity of California, BerkeleyBerkeley, CA 94720Janet TomSanta Monica Research CenterUnisys2400 Colorado AvenueSanta Monica, CA 94710ABSTRACTThis paper describes enhancements made totechniques currently used to search large databases ofproper names.
Improvements included use of aHidden Markov Model (HMM) statistical classifierto identify the likely linguistic provenance of asurname, and application of language-specific rulesto generate plausible spelling variations of names.These two components were incorporated into aprototype front-end system driving existing namesearch procedures.
HMM models and sets of linguisticrules were constructed for Farsi, Spanish andVietnamese surnames and tested on a database of over11,000 entries.
Preliminary evaluation indicatesimproved retrieval of 20-30% as measured by numberof correct items retrieved.1.0 INTRODUCTIONThis paper describes enhancements made tocurrent name search techniques used to access largedatabases of proper names.
The work focused onimproving name search algorithms to yield bettermatching and retrieval performance on data-basescontaining large numbers of non-European 'foreign'names.
Because the linguistic mix of names in largecomputer-supported databases has changed ue torecent immigration and other demographic factors,current name search procedures do not provide theaccurate retrieval required by insurance companies,state motor vehicle bureaus, law enforcement agenciesand other institutions.
As the potential consequencesof incorrect retrieval are so severe (e.g., loss ofbenefits, false arrest), it is necessary that namename search techniques be improved to handle thelinguistic variability reflected in current databases.Our specific approach decomposed the namesearch problem into two main components:?
Language classification techniques to identifythe source language for a given query name, andName association techniques, once a sourcelanguage for a name is known, to exploitlanguage-specific rules to generate variants of aname due to spelling variation, bad transcrip-tions, nicknames, and other name conventions.A statistical classification technique basedon the use of Hidden Markov Models (HMM) wasused as a language discriminator.
The test databasecontained about 11,000 names, including about 2,000each from three target languages, Vietnamese, Farsiand Spanish, and 5,000 termed 'other' to broadlyrepresent general European names.
The decisionprocedures assumed a closed-world situation inwhich a name must be assigned to one of the fourclasses.Language-specific rules in the form ofcontext-sensitive, string rewrite rules were used togenerate name variants.
These were based onlinguistic analysis of naming convent ions,pronunciations and common misspellings for eachtarget language.These two components were incorporatedinto a front-end system driving existing name searchprocedures.
The front-end system was implemented inthe C language and runs on a VAX-11/780 and Sun 3workstations under Unix 4.2.
Preliminary tests203indicate improved retrieval (number of correct itemsretrieved) by as much as 20-30% over standardSOUNDEX and NYSIIS (Taft 1970) techniques.2.0 CURRENT NAME SEARCH PROCEDURESIn current name search procedures, a searchrequest is reduced to a canonical form which is thenmatched against a database of names also reduced totheir canonical equivalents.
All names having thesame canonical form as the query name will beretrieved.
The intent is that similar names (e.g.,Cole, Kohl, Koll) will have identical canonicalforms and dissimilar names (e.g., Cole, Smith, Jones)will have different canonical forms.
Retrievalshould then be insensitive to simple transformationssuch as spelling variants.
Techniques of this typehave been reviewed by Moore et al (1977).However, because of spelling variation inproper names, the canonical reduction algorithm maynot always have the desired characteristics.Sometimes imilar names are mapped to differentcanonical forms and dissimilar names mapped to thesame forms.
This is especially true when 'foreign' ornon-European names are included in the database,because the canonical reduction techniques uch asSOUNDEX and NYSIIS are very language-specificand based largely on Western European ames.
Forexample, one of the SOUNDEX reduction rulesassumes that the characteristic shape of a name isembodied in its consonants and therefore the ruledeletes most of the vowels.
Although reasonable forEnglish and certain other languages, this rule is lessapplicable to Chinese surnames which may bedistinguished only by vowel (e.g., Li, Lee, Lu).In large databases with diverse sources of names,other name conventions may also need to be handled,such as the use of both matronymic and patronymic nSpanish (e.g., Maria Hernandez Garcia) or theinverted order of Chinese names (e.g., Li-Fang-Kuei,where Li is the surname).3.0 LANGUAGE CLAS SIF ICATIONAs mentioned in section 1.0, the approachtaken to improve xisting name search techniques wasto first classify the query name as to language sourceand then use language-specific rewrite rules togenerate plausible name variants.
A statisticalclassifier based on Hidden Markov Models (HMM)was developed for several reasons.
Similar modelshave been used successfully in language identificationbased on phonetic strings (House and Neuburg 1977, Liand Edwards 1980) and text strings (Ferguson 1980).Also, HMMs have a relatively simple structure thatmake them tractable, both analytically andcomputationally, and effective procedures alreadyexist for deriving HMMs from a purely statisticalanalysis of representative t xt.HMMs are useful in language classificationbecause they provide a means of assigning aprobability distribution to words or names in aspecific language.
In particular, given an HMM, theprobability that a given word would be generated bythat model can be computed.
Therefore, the decisionprocedure used in this project is to compute thatprobability for a given name against each of thelanguage models, and to select as the source languagethat language whose model is most likely to generatethe name.3.1 EXAMPLE OF HMM MODELING TEXTThe following example illustrates howHMMs can be used to capture important informationabout language data.
Table 1 contains training datarepresenting sample text strings in a language corpus.Three different HMMs of two, four and six states,were built from these data and are shown in Tables 2-4, respectively.
(The symbol CR in the tablescorresponds to the blank space between words and isused as a word delimiter.
)These HMMs can also be representedgraphically, as shown in Figures 1-3.
The numberedcircles correspond to states; the arrows represent statetransitions with non-zero probability and are labeledwith the transition probability.
The boxes containthe probability distribution of the output symbolsproduced when the model is in the state to which thebox is connected.
The process of generating the outputsequence of a model can then be seen as a randomtraversal of the graph according to the probabilityweights on the arrows, with an output symbolgenerated randomly each time a state is visited,according to the output distribution associated withthat state.For example, in the two-state model shownin Table 2 (and graphically in Figure 1), letter (non-delimiter) symbols can be produced only in state two,and the output probability distribution for this stateis simply the relative frequency with which eachletter appears in the training data.
That is, in thetraining data in Table 1 there are 15 letter symbols:204Table 1.
Sample Training DataTrai~ing_Dat a for ExampleaababeabedabedeTable 2.
Two State HMM Based on Sample DataI , .
,Final Hidden Markov Model Parameters'Two Sta!e, State Output ModelOutput ProbabilitiesSymbolCRabcdeState TransitionState1 21 00 0.3330 0.2670 0.20 0.1330 0.0667ProbabilitiesTo Froml102 0,333210.667(.333.667.5Table 3.
Four State HMM Based on Sample DataFinal Hidden Markov Model Pa'ram'eters IFour Stat% State Ou.
!put Model I Output ProbabilitiesState Symbol1 2 3 4CR 1 0 0 0--a 0 1 0 0b 0 0 0 Ic 0 0 0.5 0d 0 0 0.333 0e 0 0 0.167 0State Transition ProbabilJtiesFroml234To1 2 30 1 00.2 0 00.5 0 0.50.25 0 0.75400.80 l0.2.5 I 1.8'.25.75Figure 1.
Graphic Representation f Two StateHMM for Sample DataFigure 2.
Graphic Representation f Four StateHMM for Sample Data205Table 4.
Six State HMM Based on Sample DataHidden Mazkov Model ParametersSix State, State Output Model, ,  , ,Output ProbabilitiesSymbolCR.a.bcdeState1 2 3 4 5 61 0 0 0 0 00 0 1 0 0 00 0 0 0 1 00 0 0 I 0 00 1 0 0 0 00 0 0 0 0 lState Transition ProbabilitiesFrom11 02 0.53 0.24 0.3335 0.256 1To2 3 4 5 60 l 0 0 00 0 0 0 0.50 0 0 0.8 00.667 0 0 0 00 0 0.75 0 00 0 0 0 0\ /.81/  /,, / .25.333.75Table 5.
Output from Two, Four and Six StateHMM for Sample DataOutputs from Illdden Markov ModelsTwo States Four States Six Statesaadcc ab abcdebe ab abeabcacaa abcc abcddcace abd abcd~aaedb abd ac ab abcd?caea abe abcc ab abedcbc ab abec ab abeb abe abcbbcbcaebd abed abea a aca ab abedb abe abedcb abccdcc abeode abcc abbccbabebd ab abebc ab abeddd ab abeddca abe abcd?ad abed ac nb abodec abe abedba ab abbaea abe abb abe abba a abcdecabbd ab ab ab aac abe ab.5.667.5Figure 3.
Graphic Representation of Six State HMM for Sample Data206five "a", four "b", three "c", etc., and the modelassigns a probability of 5/15 = 0.333 to "a", 4/15 =0.267 to "o", and so on.
Similarly, the state transitionprobabilities for state two reflect the relativefrequency with which letters follow letters and worddelimiters follow letters.
These parameters arederived strictly from an iterative automaticprocedure and do not reflect human analysis of thedata.In the four state model shown in Table 3 (andFigure 2), it is possible to model the training datawith more detail, and the iterations converge to amodel with the two most frequently occuring symbols,"a" and "b", assigned to unique states (states two andfour, respectively) and the remaining lettersaggregated in state three.
State one contains theword delimiter and transitions from state one occuronly to state two, reflecting the fact that "a" isalways word-initial in the training data.In the six state model shown in Table 4 (andFigure 3), the training data is modeled exactly.
Eachstate corresponds to exactly one output symbol (aletter or word delimiter).
For each state, transitionsoccur only to the state corresponding to the nextallowable letter or to the word delimiter.The outputs generated by these three modelsare shown in Table 5.
The six state model can be usedto model the training data exactly, and in general,the faithfulness with which the training data arerepresented increases with the number of states.3.2 HMM MODEL OF SPANISH NAMESThe simple example in the preceding sectionillustrates the connection between model parametersand training data.
It is more difficult to interpretmodels derived from more complex data such asnatural anguage text, but it is possible to provideintuitive interpretations to the states in such models.Table 6 shows an eight state HMM derivedfrom Spanish surnames.
State transitionprobabilities are shown at the bottom of the table,and it can be seen that the transition probabilityfrom state eight to state one (word delimiter) isgreater than .95.
That is, state eight can beconsidered to represent a "word final" state.
The toppart of the table shows that the highest outputprobabilities for state eight are assigned to theletters "a,o,s,z", correctly reflecting the fact thatthese letters commonly occur word final in SpanishGarcia, Murillo, Fuentes, Diaz.
This HMM also"discovers" linguistic categories, uch as the class ofnon-word-final vowels represented by state sevenwith the highest output probabilities assigned to thevowels "a,e,i,o,u".3.3 LANGUAGE CLASSIF ICATIONIn order to use HMMs for languageclassification, it was first necessary to construct amodel for each language category based on arepresentative sample.
A maximum likelihood (ML)estimation technique was used because it leads to arelatively simple method for iteratively generatinga sequence of successively better models for a given setof words.
HMMs of four, six and eight states weregenerated for each of the language categories, and aneight state HMM was selected for the finalconfiguration of the classifier.
Higher dimensionalmodels were not evaluated because the eight statemodel performed well enough for the application.With combined training and test data, languageclassification accuracy was 98% for Vietnamese, 96%for Farsi, 91% for Spanish, and 88% for Other.
Withtraining data separate from test data, languageclassification accuracy was 96% for Vietnamese, 90%for Farsi, 89% for Spanish, and 87% for Other.
Thelanguage classification results are shown in Tables 7and 8.4.0 L INGUISTIC RULE COMPONENTFor each of the three language groups,Vietnamese, Farsi and Spanish, a set of linguisticrules could be applied using a general rule interpreter.The rules were developed after studying namingconventions and common transcription variations andalso after performing protocol analyses to see hownative English speakers (mis)spelled namespronounced by native Vietnamese (and Farsi andSpanish) speakers and (mis)pronounced by otherEnglish speakers.
Naming conventions included wordorder (e.g., surnames coming first, or parents' urnamesboth used); common transcription variations includedRomanization issues (e.g., Farsi character that iswritten as either 'v' or 'w').The general form of the rules islhs --> rhs / leftContext rightContextwhere the left-hand-side (lhs) is a character stringand the right-hand-side is a string with a possible207Table 6.
Eight State HMM for SpanishHidden Markov Model ParametersEight State, State Output Model for SpanishOutput ProbabilitiesSymbol State1 2 3 6 7 8CR- 0 0a 0 0.0479b 00.004270.01330 0.00208C 0 0.0193 0d 0 0.0755 0.02070.0753 0.3240.0427 00.0864 00.0408000.219e 0 0.567 0.032 0.00368 0.196 0.0268f 0 0 0 0.0612 0 00 0.0207 0 0.052 0 0.00161h 0 0 0 0.0825 0.0109 00.001930.002950.0495 0.164 0.00432 00.01040.004424 50 00 00 0.00420.0681 0.001580.127 0.002220.0601 0.2290.00169 0.004770.00875 00.174 00 00 0.0130.0233 00 00.066 0.O6260.118 0.004480.0697 0.05930 00.0132 0.01380.0149 0.01990.0794 0.2730.00992 0.008990.0726 0.1550 00.0884 00.00103 00 00.0031 0.004650 0.140 0.00252 0 0.00123 0 01 0 0.0048 0.189 0.0565 0.00559 0.0118m 0 0.00484 0 0.0917 0 0n 0 0.0743 0.262 0 0 0.0252o 0 0.00784 0.00968 0.0122 0.186 0.1890 0.0121 0.00825 0.122 0 00 0 0 0.00551 0 00.05280.0393Pqrs0 0.1410.08720.0028800.346 0.01290.10.04420.004760.03390.001620.002790.1230.01310.00671v 0 0.015 0 0.0177 0 0w 0 0 0 0.00213 0 0x 0 0 0 0 0 0.00183y 0 0.00198 0.013 0.00149 0 0.00534z 0 0.00175 0.00287 0.00727 0 0.368Smte Transiuon PmbabilidesTo4 7 8 Froml2I00.00968 0.075 0.005610.339 0.003230.08690.6020.002120.05480.00665 0.8140.104 3 0.0615 0.269 0.0353 0.259 0.235 0.0097 0.02534" 0 0.0101 0.0132 0 0.00503 0.0245 0.929 0.01825 0.0117 0.228 0.00477 0.00466 0.0537 0.00145 0.542 0.1546 0 0 0.0587 0.0341 0 - 0.0564 0.85 07 0.0165 0.13 0.506 0.162 0.0627 0.00977 0.0207 0.09158 0.954 0 0.00169 0 0.00723 0.00216 0.00858 0.0256208Table 7, Language Classification Performance for Training DataLanguage Classification Accuracy Statistics for Training Data., Eight Stat% State Transi, fion Output ModelLanguageFarsiFarsi 95.5?
Spanish 2,0Vietnamese 0.3Other 5.4Table 8.
LanguagePercent Classified as:Spanish Vi.e.mamese1.4 0.191.1 0.i1.0 97.85.8 0.4Other ' Error Rate3.0 4.56.9 8.90.9 2.288.4 11.6Classification Performance for Test DataLanguage Classification Accuracy Statistics for Non.Training DataEight Star% State Transition Output ModelPercent Classified as:Language Error Rate Va si_ IFarsi 90.1Spanish 2.6Vietnamese 0.6Other 6.3Spanish Vietnamese2.7 1.088.8 0.1=,1.6 96.06.1 0.4Other7.18.51.8"87.39.911.94.012.4mTable 9.
Examples of Linguistic RulesRulePH -> FC->K/  AEnglish ParaphrasePH goes to F.... (ever'/where)C goes to K whenit precedes A.J goes to J, H or Gwhen it is word initial.
J -> J IH IG/#_Y goes to Y or I Y ->Y I I / .
when it is not word initial.F goes to F or V F ->F IV \ [ _ .
when it is not word final.C -> C I S /_\[El\]H ->H I J/\['CS\]_\[AEIOU\]T -> T I D \[ #_F.^RIIE -> IE I I I Y / _#O -> O I E I U / S_N#C goes to C or S whenit precedes E or I.H goes to H or Jwhen it follows a letterother than C or S, andprecedes A,E,I,O, or U.T goes to T or D'when it is word initialand precedes a letterother than R.IE goes to IE, Ior Y when it is word finalO goes to O, Eor U when it follows Sand precedes final N.inputPhredStephenCathyJknenezBryanSherryFilipStefanCespedesGarciaTruhiUoTaoTuyetVinnieAndersonExamplesoutputFredS te fenN/A.
.
oKathy ColinJimenez, Himenez, BorjasGimenezYonkers Bryan, BrianSherr 7, SherriFilip, VilipStefan, StevranCespedes, SespedesGarcia, GarsiaTruhilloTrujilloTao, DaoTuyet, DuyetVinnie, Vinni,VinnyAnderson,Andersen,AndersunJosefCarrilloChaconSherriTranKietPiersonMierAndersonsAnderzon209weight, so that the rules could be associated with aplausibility factor.Rules may include a specific context; if aspecific environment is not described, the rule appliesin all cases.
Table 9 shows sample rules andexamples of output strings generated by applying therules.
The 'N/A'  column gives examples of namestrings for which a rule does not apply because thespecified context is absent.
An example withplausibility weights is also shown.5.0 PERFORMANCEAlthough the statistical model building iscomputationally intensive and time-consuming(several hours), the actual classification procedure isvery efficient.
The average cpu time to classify aquery name was under 200 msec on a VAX-11/780.
Therule component that generates spelling variants canprocess 100 query names in about 2-6 cpu seconds, thedifference in time depending on average length ofna l -ne .As for retrieval performance, in a test of 160query names (including names known to be in thedatabase and spelling variants not known to be in thedatabase), there were 111 hits (69%) using NYSIISprocedures alone and 141 hits (88%) using the front-end language classifier and linguistic rules andsending the expanded query set to NYSIIS.In recent work, this technique has beenextended to include modeling a database of Slavicsurnames.
Language classification accuracy based ona combined atabase of 13000 surnames representingSpanish, Farsi, Vietnamese, Slavic and 'other'names, with combined training data (1000 names fromeach language group to build each language model)and test data (remaining 8000 names), is 96.8% forVietnamese, 87.7% for Farsi, 86.9% for Spanish,86.5% for Slavic, and 82.9% for 'other'.an Utterance, |ournal of the Acoustical Society ofAmerica, 62 (3):708-713.Li, K. P. and Edwards, Thomas J.
1980 StatisticalModels for Automatic Language Identification, Proc.IEEE International Conference on Acoustics, Speechand Signal Processing, Denver, Colorado, 884-887.Moore, Gwendolyn B.; Kuhns, John L.; Trefftzs,Jeffrey L.; and Montgomery, Christine A.
1977Accessing Individual Records from Personal DataFiles Using Non-Unique Identifiers.
ComputerScience and Technology, National Bureau ofStandards Special Publication 500-2, Washington,D.C.Taft, Robert L. 1970 Name Search Techniques.
NewYork State Identification and Intelligence System,Special Report No.
1, Albany, New York.6.0 REFERENCESFerguson, John D., Ed.
1980 Symposium on theApplication of Hidden Markov Models to Text andSpeech ,  Inst i tute for Defense Analyses,Communications Research Division, Princeton, NewJersey.House, Arthur H. and Neuburg, Edward P. 1977Toward Automatic Identification of the Language of210
