The Language Demographics of Amazon Mechanical TurkEllie Pavlick1 Matt Post2 Ann Irvine2 Dmitry Kachaev2 Chris Callison-Burch1,21Computer and Information Science Department, University of Pennsylvania2Human Language Technology Center of Excellence, Johns Hopkins UniversityAbstractWe present a large scale study of the languagesspoken by bilingual workers on MechanicalTurk (MTurk).
We establish a methodologyfor determining the language skills of anony-mous crowd workers that is more robust thansimple surveying.
We validate workers?
self-reported language skill claims by measuringtheir ability to correctly translate words, andby geolocating workers to see if they reside incountries where the languages are likely to bespoken.
Rather than posting a one-off survey,we posted paid tasks consisting of 1,000 as-signments to translate a total of 10,000 wordsin each of 100 languages.
Our study ranfor several months, and was highly visible onthe MTurk crowdsourcing platform, increas-ing the chances that bilingual workers wouldcomplete it.
Our study was useful both to cre-ate bilingual dictionaries and to act as cen-sus of the bilingual speakers on MTurk.
Weuse this data to recommend languages with thelargest speaker populations as good candidatesfor other researchers who want to developcrowdsourced, multilingual technologies.
Tofurther demonstrate the value of creating datavia crowdsourcing, we hire workers to createbilingual parallel corpora in six Indian lan-guages, and use them to train statistical ma-chine translation systems.1 OverviewCrowdsourcing is a promising new mechanism forcollecting data for natural language processing re-search.
Access to a fast, cheap, and flexible work-force allows us to collect new types of data, poten-tially enabling new language technologies.
Becausecrowdsourcing platforms like Amazon MechanicalTurk (MTurk) give researchers access to a world-wide workforce, one obvious application of crowd-sourcing is the creation of multilingual technologies.With an increasing number of active crowd workerslocated outside of the United States, there is even thepotential to reach fluent speakers of lower resourcelanguages.
In this paper, we investigate the feasi-bility of hiring language informants on MTurk byconducting the first large-scale demographic studyof the languages spoken by workers on the platform.There are several complicating factors when try-ing to take a census of workers on MTurk.
Theworkers?
identities are anonymized, and Amazonprovides no information about their countries of ori-gin or their language abilities.
Posting a simple sur-vey to have workers report this information may beinadequate, since (a) many workers may never seethe survey, (b) many opt not to do one-off surveyssince potential payment is low, and (c) validating theanswers of respondents is not straightforward.Our study establishes a methodology for deter-mining the language demographics of anonymouscrowd workers that is more robust than simple sur-veying.
We ask workers what languages they speakand what country they live in, and validate theirclaims by measuring their ability to correctly trans-late words and by recording their geolocation.
Toincrease the visibility and the desirability of ourtasks, we post 1,000 assignments in each of 100 lan-guages.
These tasks each consist of translating 10foreign words into English.
Two of the 10 wordshave known translations, allowing us to validate thatthe workers?
translations are accurate.
We constructbilingual dictionaries with up to 10,000 entries, withthe majority of entries being new.Surveying thousands of workers allows us to ana-lyze current speaker populations for 100 languages.79Transactions of the Association for Computational Linguistics, 2 (2014) 79?92.
Action Editor: Mirella Lapata.Submitted 12/2013; Published 2/2014.
c?2014 Association for Computational Linguistics.11/26/13 turkermap.htmlfile:///Users/ellie/Documents/Research/turker-demographics/code/src/20130905/paper-rewrite/turkermap.html 1/11 1,998Figure 1: The number of workers per country.
This map was generated based on geolocating the IP addressof 4,983 workers in our study.
Omitted are 60 workers who were located in more than one country duringthe study, and 238 workers who could not be geolocated.
The size of the circles represents the numberof workers from each country.
The two largest are India (1,998 workers) and the United States (866).
Tocalibrate the sizes: the Philippines has 142 workers, Egypt has 25, Russia has 10, and Sri Lanka has 4.The data also allows us to answer questions like:How quickly is work completed in a given language?Are crowdsourced translations reliably good?
Howoften do workers misrepresent their language abili-ties to obtain financial rewards?2 Background and Related WorkAmazon?s Mechanical Turk (MTurk) is an on-line marketplace for work that gives employersand researchers access to a large, low-cost work-force.
MTurk allows employers to provide micro-payments in return for workers completing micro-tasks.
The basic units of work on MTurk are called?Human Intelligence Tasks?
(HITs).
MTurk was de-signed to accommodate tasks that are difficult forcomputers, but simple for people.
This facilitatesresearch into human computation, where people canbe treated as a function call (von Ahn, 2005; Little etal., 2009; Quinn and Bederson, 2011).
It has appli-cation to research areas like human-computer inter-action (Bigham et al., 2010; Bernstein et al., 2010),computer vision (Sorokin and Forsyth, 2008; Denget al., 2010; Rashtchian et al., 2010), speech pro-cessing (Marge et al., 2010; Lane et al., 2010; Parentand Eskenazi, 2011; Eskenazi et al., 2013), and natu-ral language processing (Snow et al., 2008; Callison-Burch and Dredze, 2010; Laws et al., 2011).On MTurk, researchers who need work completedare called ?Requesters?, and workers are often re-ferred to as ?Turkers?.
MTurk is a true market, mean-ing that Turkers are free to choose to complete theHITs which interest them, and Requesters can pricetheir tasks competitively to try to attract workers andhave their tasks done quickly (Faridani et al., 2011;Singer and Mittal, 2011).
Turkers remain anony-mous to Requesters, and all payment occurs throughAmazon.
Requesters are able to accept submittedwork or reject work that does not meet their stan-dards.
Turkers are only paid if a Requester acceptstheir work.Several reports examine Mechanical Turk as aneconomic market (Ipeirotis, 2010a; Lehdonvirta andErnkvist, 2011).
When Amazon introduced MTurk,it first offered payment only in Amazon credits, andlater offered direct payment in US dollars.
More re-cently, it has expanded to include one foreign cur-rency, the Indian rupee.
Despite its payments be-ing limited to two currencies or Amazon credits,MTurk claims over half a million workers from 190countries (Amazon, 2013).
This suggests that itsworker population should represent a diverse set oflanguages.80A demographic study by Ipeirotis (2010b) fo-cused on age, gender, martial status, income lev-els, motivation for working on MTurk, and whetherworkers used it as a primary or supplemental formof income.
The study contrasted Indian and USworkers.
Ross et al.
(2010) completed a longitudi-nal follow-on study.
A number of other studies haveinformally investigated Turkers?
language abilities.Munro and Tily (2011) compiled survey responsesof 2,000 Turkers, revealing that four of the six mostrepresented languages come from India (the top sixbeing Hindi, Malayalam, Tamil, Spanish, French,and Telugu).
Irvine and Klementiev (2010) hadTurkers evaluate the accuracy of translations thathad been automatically inducted from monolingualtexts.
They examined translations of 100 words in42 low-resource languages, and reported geolocatedcountries for their workers (India, the US, Romania,Pakistan, Macedonia, Latvia, Bangladesh and thePhilippines).
Irvine and Klementiev discussed thedifficulty of quality control and assessing the plausi-bility of workers?
language skills for rare languages,which we address in this paper.Several researchers have investigated usingMTurk to build bilingual parallel corpora for ma-chine translation, a task which stands to benefitlow cost, high volume translation on demand (Ger-mann, 2001).
Ambati et al.
(2010) conducted a pilotstudy by posting 25 sentences to MTurk for Span-ish, Chinese, Hindi, Telugu, Urdu, and Haitian Cre-ole.
In a study of 2000 Urdu sentences, Zaidanand Callison-Burch (2011) presented methods forachieving professional-level translation quality fromTurkers by soliciting multiple English translationsof each foreign sentence.
Zbib et al.
(2012) usedcrowdsourcing to construct a 1.5 million word par-allel corpus of dialect Arabic and English, train-ing a statistical machine translation system that pro-duced higher quality translations of dialect Arabicthan a system a trained on 100 times more Mod-ern Standard Arabic-English parallel data.
Zbib etal.
(2013) conducted a systematic study that showedthat training an MT system on crowdsourced trans-lations resulted in the same performance as trainingon professional translations, at 15 the cost.
Hu etal.
(2010; Hu et al.
(2011) performed crowdsourcedtranslation by having monolingual speakers collab-orate and iteratively improve MT output.English 689 Tamil 253 Malayalam 219Hindi 149 Spanish 131 Telugu 87Chinese 86 Romanian 85 Portuguese 82Arabic 74 Kannada 72 German 66French 63 Polish 61 Urdu 56Tagalog 54 Marathi 48 Russian 44Italian 43 Bengali 41 Gujarati 39Hebrew 38 Dutch 37 Turkish 35Vietnamese 34 Macedonian 31 Cebuano 29Swedish 26 Bulgarian 25 Swahili 23Hungarian 23 Catalan 22 Thai 22Lithuanian 21 Punjabi 21 Others ?
20Table 1: Self-reported native language of 3,216bilingual Turkers.
Not shown are 49 languages with?20 speakers.
We omit 1,801 Turkers who did notreport their native language, 243 who reported 2 na-tive languages, and 83 with ?3 native languages.Several researchers have examined cost optimiza-tion using active learning techniques to select themost useful sentences or fragments to translate (Am-bati and Vogel, 2010; Bloodgood and Callison-Burch, 2010; Ambati, 2012).To contrast our research with previous work, themain contributions of this paper are: (1) a robustmethodology for assessing the bilingual skills ofanonymous workers, (2) the largest-scale census todate of language skills of workers on MTurk, and (3)a detailed analysis of the data gathered in our study.3 Experimental DesignThe central task in this study was to investigate Me-chanical Turk?s bilingual population.
We accom-plished this through self-reported surveys combinedwith a HIT to translate individual words for 100languages.
We evaluate the accuracy of the work-ers?
translations against known translations.
In caseswhere these were not exact matches, we used a sec-ond pass monolingual HIT, which asked Englishspeakers to evaluate if a worker-provided translationwas a synonym of the known translation.Demographic questionnaire At the start of eachHIT, Turkers were asked to complete a brief surveyabout their language abilities.
The survey asked thefollowing questions:?
Is [language] your native language??
How many years have you spoken [language]?81?
Is English your native language??
How many years have you spoken English??
What country do you live in?We automatically collected each worker?s current lo-cation by geolocating their IP address.
A total of5,281 unique workers completed our HITs.
Of these,3,625 provided answers to our survey questions, andwe were able to geolocate 5,043.
Figure 1 plotsthe location of workers across 106 countries.
Table1 gives the most common self-reported native lan-guages.Selection of languages We drew our data from thedifferent language versions of Wikipedia.
We se-lected the 100 languages with the largest number ofarticles 1 (Table 2).
For each language, we chosethe 1,000 most viewed articles over a 1 year period,2and extracted the 10,000 most frequent words fromthem.
The resulting vocabularies served as the inputto our translation HIT.Translation HIT For the translation task, weasked Turkers to translate individual words.
Weshowed each word in the context of three sentencesthat were drawn from Wikipedia.
Turkers were al-lowed to mark that they were unable to translate aword.
Each task contained 10 words, 8 of whichwere words with unknown translations, and 2 ofwhich were quality control words with known trans-lations.
We gave special instruction for translat-ing names of people and places, giving examplesof how to handle ?Barack Obama?
and ?Australia?using their interlanguage links.
For languages withnon-Latin alphabets, names were transliterated.The task paid $0.15 for the translation of 10words.
Each set of 10 words was independentlytranslated by three separate workers.
5,281 workerscompleted 256,604 translation assignments, totalingmore than 3 million words, over a period of threeand a half months.Gold standard translations A set of gold stan-dard translations were automatically harvested from1http://meta.wikimedia.org/wiki/List_of_Wikipedias2http://dumps.wikimedia.org/other/pagecounts-raw/500K+ ARTICLES: German (de), English (en), Spanish (es), French(fr), Italian (it), Japanese (ja), Dutch (nl), Polish (pl), Portuguese(pt), Russian (ru)100K-500K ARTICLES: Arabic (ar), Bulgarian (bg), Catalan (ca),Czech (cs), Danish (da), Esperanto (eo), Basque (eu), Persian (fa),Finnish (fi), Hebrew (he), Hindi (hi), Croatian (hr), Hungarian (hu),Indonesian (id), Korean (ko), Lithuanian (lt), Malay (ms), Norwe-gian (Bokmal) (no), Romanian (ro), Slovak (sk), Slovenian (sl), Ser-bian (sr), Swedish (sv), Turkish (tr), UKrainian (UK), Vietnamese(vi), Waray-Waray (war), Chinese (zh)10K-100K ARTICLES: Afrikaans (af) Amharic (am) Asturian (ast)Azerbaijani (az) Belarusian (be) Bengali (bn) Bishnupriya Manipuri(bpy) Breton (br) Bosnian (bs) Cebuano (ceb) Welsh (cy) Zazaki(diq) Greek (el) West Frisian (fy) Irish (ga) Galician (gl) Gujarati(gu) Haitian (ht) Armenian (hy) Icelandic (is) Javanese (jv) Geor-gian (ka) Kannada (kn) Kurdish (ku) Luxembourgish (lb) Latvian(lv) Malagasy (mg) Macedonian (mk) Malayalam (ml) Marathi(mr) Neapolitan (nap) Low Saxon (nds) Nepali (ne) Newar / NepalBhasa (new) Norwegian (Nynorsk) (nn) Piedmontese (pms) Sicil-ian (scn) Serbo-Croatian (sh) Albanian (sq) Sundanese (su) Swahili(sw) Tamil (ta) Telugu (te) Thai (th) Tagalog (tl) Urdu (ur) Yoruba(yo)<10K ARTICLES: Central Bicolano (bcl) Tibetan (bo) Ilokano (ilo)Punjabi (pa) Kapampangan (pam) Pashto (ps) Sindhi (sd) Somali(so) Uzbek (uz) Wolof (wo)Table 2: A list of the languages that were used in ourstudy, grouped by the number of Wikipedia articlesin the language.
Each language?s code is given inparentheses.
These language codes are used in otherfigures throughout this paper.Wikipedia for every language to use as embeddedcontrols.
We used Wikipedia?s inter-language linksto pair titles of English articles with their corre-sponding foreign article?s title.
To get a more trans-latable set of pairs, we excluded any pairs where: (1)the English word was not present in the WordNetontology (Miller, 1995), (2) either article title waslonger than a single word, (3) the English Wikipediapage was a subcategory of person or place, or (4)the English and the foreign titles were identical or asubstring of the other.Manual evaluation of non-identical translationsWe counted all translations that exactly matchedthe gold standard translation as correct.
For non-exact matches we created a second-pass quality as-surance HIT.
Turkers were shown a pair of En-glish words, one of which was a Turker?s transla-tion of the foreign word used for quality control,and the other of which was the gold-standard trans-lation of the foreign word.
Evaluators were askedwhether the two words had the same meaning, andchose between three answers: ?Yes?, ?No?, or ?Re-82Figure 2: Days to complete the translation HITs for40 of the languages.
Tick marks represent the com-pletion of individual assignments.lated but not synonymous.?
Examples of mean-ing equivalent pairs include: <petroglyphs, rockpaintings>, <demo, show> and <loam, loam: soilrich in decaying matter>.
Non-meaning equiva-lents included: <assorted, minutes>, and <major,URL of image>.
Related items were things like<sky, clouds>.
Misspellings like <lactation, lac-tiation > were judged to have same meaning, andwere marked as misspelled.
Three separate Turkersjudged each pair, allowing majority votes for diffi-cult cases.We checked Turkers who were working on thistask by embedding pairs of words which were ei-??
?$ %???
( ??
+ ?$ ???
%?.?
??
??
????
5 ??
????
?9 ???:?
??
?
?<?In retribution pakistan also did six nuclear tests on 28 may 1998.On 28 May Pakistan also conducted six nuclear tests as an actof redressal.Retaliating on this ?Pakistan?
conducted Six(6) Nuclear Testson 28 May, 1998.pakistan also did 6 nuclear test in retribution on 28 may, 1998Figure 3: An example of the Turkers?
translations ofa Hindi sentence.
The translations are unedited andcontain fixable spelling, capitalization and grammat-ical errors.ther known to be synonyms (drawn from Word-Net) or unrelated (randomly chosen from a corpus).Automating approval/rejections for the second-passevaluation allowed the whole pipeline to be run au-tomatically.
Caching judgments meant that we ulti-mately needed only 20,952 synonym tasks to judgeall of the submitted translations (a total of 74,572non-matching word pairs).
These were completedby an additional 1,005 workers.
Each of these as-signments included 10 word pairs and paid $0.10.Full sentence translations To demonstrate thefeasibility of using crowdsourcing to create multi-lingual technologies, we hire Turkers to constructbilingual parallel corpora from scratch for six In-dian languages.
Germann (2001) attempted to builda Tamil-English translation system from scratch byhiring professional translators, but found the costprohibitive.
We created parallel corpora by trans-lating the 100 most viewed Wikipedia pages in Ben-gali, Malyalam, Hindi, Tamil, Telugu, and Urdu intoEnglish.
We collected four translations from differ-ent Turkers for each source sentence.Workers were paid $0.70 per HIT to translate10 sentences.
We accepted or rejected translationsbased on a manual review of each worker?s submis-sions, which included a comparison of the transla-tions to a monotonic gloss (produced with a dic-tionary), and metadata such as the amount of timethe worker took to complete the HIT and their geo-graphic location.Figure 3 shows an example of the translations weobtained.
The lack of a professionally translatedreference sentences prevented us from doing a sys-tematic comparison between the quality of profes-83pt bs sh tl it sr ro es ms de af te hr id da nl tr gu sk fi he ml fr ja pa bg mk no gl ht ga sv cy lv hu kn az be lt ko ne eo ar pl mr ca cs sw ta hi bn nn ka so zh jv el ceb vi bcl is su uz lb bpy scn new ur sd br ps ru am wo bo0.00.20.40.60.81.0Figure 4: Translation quality for languages with at least 50 Turkers.
The dark blue bars indicate the pro-portion of translations which exactly matched gold standard translations, and light blue indicate translationswhich were judged to be correct synonyms.
Error bars show the 95% confidence intervals for each language.sion and non-professional translations as Zaidan andCallison-Burch (2011) did.
Instead we evaluate thequality of the data by using it to train SMT systems.We present results in section 5.4 Measuring Translation QualityFor single word translations, we calculate the qual-ity of translations on the level of individual assign-ments and aggregated over workers and languages.We define an assignment?s quality as the proportionof controls that are correct in a given assignment,where correct means exactly correct or judged to besynonymous.Quality(ai) = 1kiki?j=1?
(trij ?
syns[gj]) (1)where ai is the ith assignment, ki is the number ofcontrols in ai, trij is the Turker?s provided transla-tion of control word j in assignment i, gj is the goldstandard translation of control word j, syns[gj] isthe set of words judged to be synonymous with gjand includes gj , and ?
(x) is Kronecker?s delta andtakes value 1 when x is true.
Most assignments hadtwo known words embedded, so most assignmentshad scores of either 0, 0.5, or 1.Since computing overall quality for a language asthe average assignment quality score is biased to-wards a small number of highly active Turkers, weinstead report language quality scores as the aver-age per-Turker quality, where a Turker?s quality isthe average quality of all the assignments that shecompleted:Quality(ti) =?aj?assigns[i] Quality(aj)| assigns[i] | (2)where assigns[i] is the assignments completedby Turker i, and Quality(a) is as above.Quality for a language is then given byQuality(li) =?tj?turkers[i] Quality(tj)| turkers[i] | (3)When a Turker completed assignments in more thanone language, their quality was computed separatelyfor each language.
Figure 4 shows the transla-tion quality for languages with contributions fromat least 50 workers.Cheating using machine translation One obvi-ous way for workers to cheat is to use availableonline translation tools.
Although we followedbest practices to deter copying-and-pasting into on-line MT systems by rendering words and sentences84as images (Zaidan and Callison-Burch, 2011), thisstrategy does not prevent workers from typing thewords into an MT system if they are able to type inthe language?s script.To identify and remove workers who appeared tobe cheating by using Google Translate, we calcu-lated each worker?s overlap with the Google transla-tions.
We used Google to translate all 10,000 wordsfor the 51 foreign languages that Google Trans-late covered at the time of the study.
We mea-sured the percent of workers?
translations that ex-actly matched the translation returned from Google.Figure 5a shows overlap between Turkers?s trans-lations and Google Translate.
When overlap is high,it seems likely that those Turkers are cheating.
It isalso reasonable to assume that honest workers willoverlap with Google some amount of the time asGoogle?s translations are usually accurate.
We di-vide the workers into three groups: those with veryhigh overlap with Google (likely cheating by usingGoogle to translate words), those with reasonableoverlap, and those with no overlap (likely cheatingby other means, for instance, by submitting randomtext).Our gold-standard controls are designed to iden-tify workers that fall into the third group (those whoare spamming or providing useless translations), butthey will not effectively flag workers who are cheat-ing with Google Translate.
We therefore remove the500 Turkers with the highest overlap with Google.This equates to removing all workers with greaterthan 70% overlap.
Figure 5b shows that removingworkers at or above the 70% threshold retains 90%of the collected translations and over 90% of theworkers.Quality scores reported throughout the paper re-flect only translations from Turkers whose overlapwith Google falls below this 70% threshold.5 Data AnalysisWe performed an analysis of our data to address thefollowing questions:?
Do workers accurately represent their languageabilities?
Should we constrain tasks by region??
How quickly can we expect work to be com-pleted in a particular language?
(a) Individual workers?
overlap with Google Translate.We removed the 500 workers with the highest overlap(shaded region on the left) from our analyses, as it is rea-sonable to assume these workers are cheating by submit-ting translations from Google.
Workers with no overlap(shaded region on the right) are also likely to be cheating,e.g.
by submitting random text.
(b) Cumulative distribution of overlap with Google trans-late for workers and translations.
We see that eliminatingall workers with >70% overlap with google translate stillpreserves 90% of translations and >90% of workers.Figure 5?
Can Turkers?
translations be used to train MTsystems??
Do our dictionaries improve MT quality?Language skills and location We measured theaverage quality of workers who were in countriesthat plausibly speak a language, versus workers fromcountries that did not have large speaker populationsof that language.
We used the Ethnologue (Lewis85Avg.
Turker quality (# Ts) Primary locations Primary locationsIn region Out of region of Turkers in region of Turkers out of regionHindi 0.63 (296) 0.69 (7) India (284) UAE (5) UK (3) Saudi Arabia (2) Russia (1) Oman (1)Tamil 0.65 (273) ** 0.25 (2) India (266) US (3) Canada (2) Tunisia (1) Egypt (1)Malayalam 0.76 (234) 0.83 (2) India (223) UAE (6) US (3) Saudi Arabia (1) Maldives (1)Spanish 0.81 (191) 0.84 (18) US (122) Mexico (16) Spain (14) India (15) New Zealand (1) Brazil (1)French 0.75 (170) 0.82 (11) India (62) US (45) France (23) Greece (2) Netherlands (1) Japan (1)Chinese 0.60 (116) 0.55 (21) US (75) Singapore (13) China (9) Hong Kong (6) Australia (3) Germany (2)German 0.82 (91) 0.77 (41) Germany (48) US (25) Austria (7) India (34) Netherlands (1) Greece (1)Italian 0.86 (90) * 0.80 (42) Italy (42) US (29) Romania (7) India (33) Ireland (2) Spain (2)Amharic 0.14 (16) ** 0.01 (99) US (14) Ethiopia (2) India (70) Georgia (9) Macedonia (5)Kannada 0.70 (105) NA (0) India (105)Arabic 0.74 (60) ** 0.60 (45) Egypt (19) Jordan (16) Morocco (9) US (19) India (11) Canada (3)Sindhi 0.19 (96) 0.06 (9) India (58) Pakistan (37) US (1) Macedonia (4) Georgia (2) Indonesia (2)Portuguese 0.87 (101) 0.96 (3) Brazil (44) Portugal (31) US (15) Romania (1) Japan (1) Israel (1)Turkish 0.76 (76) 0.80 (27) Turkey (38) US (18) Macedonia (8) India (19) Pakistan (4) Taiwan (1)Telugu 0.80 (102) 0.50 (1) India (98) US (3) UAE (1) Saudi Arabia (1)Irish 0.74 (54) 0.71 (47) US (39) Ireland (13) UK (2) India (36) Romania (5) Macedonia (2)Swedish 0.73 (54) 0.71 (45) US (25) Sweden (22) Finland (3) India (23) Macedonia (6) Croatia (2)Czech 0.71 (45) * 0.61 (50) US (17) Czech Republic (14) Serbia (5) Macedonia (22) India (10) UK (5)Russian 0.15 (67) * 0.12 (27) US (36) Moldova (7) Russia (6) India (14) Macedonia (4) UK (3)Breton 0.17 (3) 0.18 (89) US (3) India (83) Macedonia (2) China (1)Table 3: Translation quality when partitioning the translations into two groups, one containing translationssubmitted by Turkers whose location is within regions that plausibly speak the foreign language, and theother containing translations from Turkers outside those regions.
In general, in-region Turkers providehigher quality translations.
(**) indicates differences significant at p=0.05, (*) at p=0.10.et al., 2013) to compile the list of countries whereeach language is spoken.
Table 3 compares the av-erage translation quality of assignments completedwithin the region of each language, and compares itto the quality of assignments completed outside thatregion.Our workers reported speaking 95 languages na-tively.
US workers alone reported 61 native lan-guages.
Overall, 4,297 workers were located in aregion likely to speak the language from which theywere translating, and 2,778 workers were locatedin countries considered out of region (meaning thatabout a third of our 5,281 Turkers completed HITsin multiple languages).Table 3 shows the differences in translation qual-ity when computed using in-region versus out-of-region Turkers, for the languages with the greatestnumber of workers.
Within region workers typi-cally produced higher quality translations.
Given thenumber of Indian workers on Mechanical Turk, itis unsurprising that they represent majority of out-of-region workers.
For the languages that had morethan 75 out of region workers (Malay, Amharic, Ice-landic, Sicilian, Wolof, and Breton), Indian workersrepresented at least 70% of the out of region workersin each language.A few languages stand out for having suspiciouslystrong performance by out of region workers, no-tably Irish and Swedish, for which out of regionworkers account for a near equivalent volume andquality of translations to the in region workers.
Thisis admittedly implausible, considering the relativelysmall number of Irish speakers worldwide, and thevery low number living in the countries in which ourTurkers were based (primarily India).
Such resultshighlight the fact that cheating using online transla-tion resources is a real problem, and despite our bestefforts to remove workers using Google Translate,some cheating is still evident.
Restricting to withinregion workers is an effective way to reduce theprevalence of cheating.
We discuss the languageswhich are best supported by true native speakers insection 6.Speed of translation Figure 2 gives the comple-tion times for 40 languages.
The 10 languages tofinish in the shortest amount of time were: Tamil,Malayalam, Telugu, Hindi, Macedonian, Spanish,Serbian, Romanian, Gujarati, and Marathi.
Seven ofthe ten fastest languages are from India, which is un-86320 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30800,0000100,000200,000300,000400,000500,000600,000700,000MalayalamTamilTeluguHindiUrduBengaliFigure 6: The total volume of translations (measuredin English words) as a function of elapsed days.sentence English + dictionarylanguage pairs foreign words entriesBengali 22k 732k 22kHindi 40k 1,488k 22kMalayalam 32k 863k 23kTamil 38k 916k 25kTelugu 46k 1,097k 21kUrdu 35k 1,356k 20kTable 4: Size of parallel corpora and bilingual dic-tionaries collected for each language.surprising given the geographic distribution of work-ers.
Some languages follow the pattern of having asmattering of assignments completed early, with therate picking up later.Figure 6 gives the throughput of the full-sentencetranslation task for the six Indian languages.
Thefastest language was Malayalam, for which we col-lected half a million words of translations in just un-der a week.
Table 4 gives the size of the data set thatwe created for each of these languages.Training SMT systems We trained statisticaltranslation models from the parallel corpora that wecreated for the six Indian languages using the Joshuamachine translation system (Post et al., 2012).
Table5 shows the translation performance when trainedon the bitexts alone, and when incorporating thebilingual dictionaries created in our earlier HIT.
Thescores reflect the performance when tested on heldout sentences from the training data.
Adding the dic-trained on bitext + BLEUlanguage bitexts alone dictionaries ?Bengali 12.03 17.29 5.26Hindi 16.19 18.10 1.91Malayalam 6.65 9.72 3.07Tamil 8.08 9.66 1.58Telugu 11.94 13.70 1.76Urdu 19.22 21.98 2.76Table 5: BLEU scores for translating into Englishusing bilingual parallel corpora by themselves, andwith the addition of single-word dictionaries.
Scoresare calculated using four reference translations andrepresent the mean of three MERT runs.tionaries to the training set produces consistent per-formance gains, ranging from 1 to 5 BLEU points.This represents a substantial improvement.
It isworth noting, however, that while the source doc-uments for the full sentences used for testing werekept disjoint from those used for training, there isoverlap between the source materials for the dictio-naries and those from the test set, since both the dic-tionaries and the bitext source sentences were drawnfrom Wikipedia.6 DiscussionCrowdsourcing platforms like Mechanical Turk giveresearchers instant access to a diverse set of bilin-gual workers.
This opens up exciting new avenuesfor researchers to develop new multilingual systems.The demographics reported in this study are likely toshift over time.
Amazon may expand its payments tonew currencies.
Posting long-running HITs in otherlanguages may recruit more speakers of those lan-guages.
New crowdsourcing platforms may emerge.The data presented here provides a valuable snap-shot of the current state of MTurk, and the methodsused can be applied generally in future research.Based on our study, we can confidently recom-mend 13 languages as good candidates for researchnow: Dutch, French, German, Gujarati, Italian, Kan-nada, Malayalam, Portuguese, Romanian, Serbian,Spanish, Tagalog, and Telugu.
These languageshave large Turker populations who complete tasksquickly and accurately.
Table 6 summarizes thestrengths and weaknesses of all 100 languages cov-ered in our study.
Several other languages are viable87workers quality speedmany high fast Dutch, French, German, Gu-jarati, Italian, Kannada, Malay-alam, Portuguese, Romanian,Serbian, Spanish, Tagalog, Tel-uguslow Arabic, Hebrew, Irish, Punjabi,Swedish, Turkishlow fast Hindi, Marathi, Tamil, Urduormediumslow Bengali, Bishnupriya Ma-nipuri, Cebuano, Chinese,Nepali, Newar, Polish, Russian,Sindhi, Tibetanfew high fast Bosnia, Croatian, Macedonian,Malay, Serbo-Croatianslow Afrikaans, Albanian,Aragonese, Asturian, Basque,Belarusian, Bulgarian, CentralBicolano, Czech, Danish,Finnish, Galacian, Greek,Haitian, Hungarian, Icelandic,Ilokano, Indonesian, Japanese,Javanese, Kapampangan,Kazakh, Korean, Lithuanian,Low Saxon, Malagasy, Nor-wegian (Bokmal), Sicilian,Slovak, Slovenian, Thai, UKra-nian, Uzbek, Waray-Waray,West Frisian, Yorubalow fast ?ormediumslow Amharic, Armenian, Azer-baijani, Breton, Catalan,Georgian, Latvian, Luxembour-gish, Neapolitian, Norwegian(Nynorsk), Pashto, Pied-montese, Somali, Sudanese,Swahili, Tatar, Vietnamese,Walloon, Welshnone low ormediumslow Esperanto, Ido, Kurdish, Per-sian, Quechua, Wolof, ZazakiTable 6: The green box shows the best languages totarget on MTurk.
These languages have many work-ers who generate high quality results quickly.
Wedefined many workers as 50 or more active in-regionworkers, high quality as?70% accuracy on the goldstandard controls, and fast if all of the 10,000 wordswere completed within two weeks.candidates provided adequate quality control mech-anisms are used to select good workers.Since Mechanical Turk provides financial incen-tives for participation, many workers attempt tocomplete tasks even if they do not have the lan-guage skills necessary to do so.
Since MTurk doesnot provide any information about workers demo-graphics, including their language competencies, itcan be hard to exclude such workers.
As a resultnaive data collection on MTurk may result in noisydata.
A variety of techniques should be incorporatedinto crowdsourcing pipelines to ensure high qualitydata.
As a best practice, we suggest: (1) restrictingworkers to countries that plausibly speak the foreignlanguage of interest, (2) embedding gold standardcontrols or administering language pretests, ratherthan relying solely on self-reported language skills,and (3) excluding workers whose translations havehigh overlap with online machine translation sys-tems like Google translate.
If cheating using exter-nal resources is likely, then also consider (4) record-ing information like time spent on a HIT (cumulativeand on individual items), patterns in keystroke logs,tab/window focus, etc.Although our study targeted bilingual workers onMechanical Turk, and neglected monolingual work-ers, we believe our results reliably represent the cur-rent speaker populations, since the vast majority ofthe work available on the crowdsourced platformis currently English-only.
We therefore assume thenumber of non-English speakers is small.
In the fu-ture, it may be desirable to recruit monolingual for-eign workers.
In such cases, we recommend othertests to validate their language abilities in place ofour translation test.
These could include perform-ing narrative cloze, or listening to audio files con-taining speech in different language and identifyingtheir language.7 Data releaseWith the publication of this paper, we are releasingall data and code used in this study.
Our data releaseincludes the raw data, along with bilingual dictionar-ies that are filtered to be high quality.
It will include256,604 translation assignments from 5,281 Turkersand 20,952 synonym assignments from 1,005 Turk-ers, along with meta information like geolocation88and time submitted, plus external dictionaries usedfor validation.
The dictionaries will contain 1.5Mtotal translated words in 100 languages, along withcode to filter the dictionaries based on different cri-teria.
The data also includes parallel corpora for sixIndian languages, ranging in size between 700,000to 1.5 million words.8 AcknowledgementsThis material is based on research sponsored bya DARPA Computer Science Study Panel phase 3award entitled ?Crowdsourcing Translation?
(con-tract D12PC00368).
The views and conclusionscontained in this publication are those of the authorsand should not be interpreted as representing offi-cial policies or endorsements by DARPA or the U.S.Government.
This research was supported by theJohns Hopkins University Human Language Tech-nology Center of Excellence and through gifts fromMicrosoft and Google.The authors would like to thank the anonymousreviewers for their thoughtful comments, which sub-stantially improved this paper.ReferencesAmazon.
2013.
Service summary tour for re-questers on Amazon Mechanical Turk.
https://requester.mturk.com/tour.Vamshi Ambati and Stephan Vogel.
2010.
Can crowdsbuild parallel corpora for machine translation systems?In Proceedings of the NAACL HLT 2010 Workshop onCreating Speech and Language Data with Amazon?sMechanical Turk.
Association for Computational Lin-guistics.Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.2010.
Active learning and crowd-sourcing for ma-chine translation.
In Proceedings of the 7th Interna-tional Conference on Language Resources and Evalu-ation (LREC).Vamshi Ambati.
2012.
Active Learning and Crowd-sourcing for Machine Translation in Low ResourceScenarios.
Ph.D. thesis, Language Technologies In-stitute, School of Computer Science, Carnegie MellonUniversity, Pittsburgh, PA.Michael S. Bernstein, Greg Little, Robert C. Miller,Bjrn Hartmann, Mark S. Ackerman, David R. Karger,David Crowell, and Katrina Panovich.
2010.
Soylent:a word processor with a crowd inside.
In Proceed-ings of the ACM Symposium on User Interface Soft-ware and Technology (UIST).Jeffrey P. Bigham, Chandrika Jayant, Hanjie Ji, Greg Lit-tle, Andrew Miller, Robert C. Miller, Robin Miller,Aubrey Tatarowicz, Brandyn White, Samual White,and Tom Yeh.
2010.
VizWiz: nearly real-time an-swers to visual questions.
In Proceedings of the ACMSymposium on User Interface Software and Technol-ogy (UIST).Michael Bloodgood and Chris Callison-Burch.
2010.Large-scale cost-focused active learning for statisti-cal machine translation.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics.Chris Callison-Burch and Mark Dredze.
2010.
Creatingspeech and language data with Amazon?s MechanicalTurk.
In Proceedings of the NAACL HLT 2010 Work-shop on Creating Speech and Language Data withAmazon?s Mechanical Turk, pages 1?12, Los Angeles,June.
Association for Computational Linguistics.Jia Deng, Alexander Berg, Kai Li, and Li Fei-Fei.
2010.What does classifying more than 10,000 image cate-gories tell us?
In Proceedings of the 12th EuropeanConference of Computer Vision (ECCV, pages 71?84.Maxine Eskenazi, Gina-Anne Levow, Helen Meng,Gabriel Parent, and David Suendermann.
2013.Crowdsourcing for Speech Processing, Applications toData Collection, Transcription and Assessment.
Wi-ley.Siamak Faridani, Bjo?rn Hartmann, and Panagiotis G.Ipeirotis.
2011.
What?s the right price?
pricing tasksfor finishing on time.
In Third AAAI Human Compu-tation Workshop (HCOMP?11).Ulrich Germann.
2001.
Building a statistical machinetranslation system from scratch: How much bang forthe buck can we expect?
In ACL 2001 Workshop onData-Driven Machine Translation, Toulouse, France.Chang Hu, Benjamin B. Bederson, and Philip Resnik.2010.
Translation by iterative collaboration betweenmonolingual users.
In Proceedings of ACM SIGKDDWorkshop on Human Computation (HCOMP).Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Ei-delman, Olivia Buzek, and Benjamin B. Bederson.2011.
The value of monolingual crowdsourcing ina real-world translation scenario: Simulation usinghaitian creole emergency sms messages.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 399?404, Edinburgh, Scot-land, July.
Association for Computational Linguistics.Panagiotis G. Ipeirotis.
2010a.
Analyzing the mechani-cal turk marketplace.
In ACM XRDS, December.Panagiotis G. Ipeirotis.
2010b.
Demographics ofMechanical Turk.
Technical Report Working paper89CeDER-10-01, New York University, Stern School ofBusiness.Ann Irvine and Alexandre Klementiev.
2010.
Using Me-chanical Turk to annotate lexicons for less commonlyused languages.
In Workshop on Creating Speech andLanguage Data with MTurk.Ian Lane, Matthias Eck, Kay Rottmann, and AlexWaibel.
2010.
Tools for collecting speech corporavia mechanical-turk.
In Proceedings of the NAACLHLT 2010 Workshop on Creating Speech and Lan-guage Data with Amazon?s Mechanical Turk, Los An-geles.Florian Laws, Christian Scheible, and Hinrich Schu?tze.2011.
Active learning with amazon mechanical turk.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, Edinburgh,Scotland.Matthew Lease, Jessica Hullman, Jeffrey P. Bigham,Juho Kim Michael S. Bernstein and, Walter Lasecki,Saeideh Bakhshi, Tanushree Mitra, and Robert C.Miller.
2013.
Mechanical Turk is not anony-mous.
http://dx.doi.org/10.2139/ssrn.2228728.Vili Lehdonvirta and Mirko Ernkvist.
2011.
Knowl-edge map of the virtual economy: Convertingthe virtual economy into development potential.http://www.infodev.org/en/Document.1056.pdf, April.
An InfoDev Publication.M.
Paul Lewis, Gary F. Simons, and Charles D.
Fennig(eds.).
2013.
Ethnologue: Languages of the world,seventeenth edition.
http://www.ethnologue.com.Greg Little, Lydia B. Chilton, Rob Miller, and Max Gold-man.
2009.
Turkit: Tools for iterative tasks on me-chanical turk.
In Proceedings of the Workshop onHuman Computation at the International Conferenceon Knowledge Discovery and Data Mining (KDD-HCOMP ?09), Paris.Matthew Marge, Satanjeev Banerjee, and AlexanderRudnicky.
2010.
Using the Amazon Mechanical Turkto transcribe and annotate meeting speech for extrac-tive summarization.
In Workshop on Creating Speechand Language Data with MTurk.George A. Miller.
1995.
WordNet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41.Robert Munro and Hal Tily.
2011.
The start of theart: Introduction to the workshop on crowdsourcingtechnologies for language and cognition studies.
InCrowdsourcing Technologies for Language and Cog-nition Studies, Boulder.Scott Novotney and Chris Callison-Burch.
2010.
Cheap,fast and good enough: Automatic speech recognitionwith non-expert transcription.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 207?215.
Association forComputational Linguistics.Gabriel Parent and Maxine Eskenazi.
2011.
Speakingto the crowd: looking at past achievements in usingcrowdsourcing for speech and predicting future chal-lenges.
In Proceedings Interspeech 2011, Special Ses-sion on Crowdsourcing.Matt Post, Chris Callison-Burch, and Miles Osborne.2012.
Constructing parallel corpora for six indianlanguages via crowdsourcing.
In Proceedings of theSeventh Workshop on Statistical Machine Translation,pages 401?409, Montre?al, Canada, June.
Associationfor Computational Linguistics.Alexander J. Quinn and Benjamin B. Bederson.
2011.Human computation: A survey and taxonomy of agrowing field.
In Computer Human Interaction (CHI).Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-lia Hockenmaier.
2010.
Collecting image annotationsusing Amazon?s Mechanical Turk.
In Workshop onCreating Speech and Language Data with MTurk.Joel Ross, Lilly Irani, M. Six Silberman, Andrew Zal-divar, and Bill Tomlinson.
2010. Who are the crowd-workers?
: Shifting demographics in Amazon Mechan-ical Turk.
In alt.CHI session of CHI 2010 extendedabstracts on human factors in computing systems, At-lanta, Georgia.Yaron Singer and Manas Mittal.
2011.
Pricing mecha-nisms for online labor markets.
In Third AAAI HumanComputation Workshop (HCOMP?11).Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast - but is itgood?
Evaluating non-expert annotations for naturallanguage tasks.
In Proceedings of EMNLP.Alexander Sorokin and David Forsyth.
2008.
Utilitydata annotation with amazon mechanical turk.
In FirstIEEE Workshop on Internet Vision at CVPR.Luis von Ahn.
2005.
Human Computation.
Ph.D. thesis,School of Computer Science, Carnegie Mellon Uni-versity, Pittsburgh, PA.Omar F. Zaidan and Chris Callison-Burch.
2011.
Crowd-sourcing translation: Professional quality from non-professionals.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies, pages 1220?1229.
Association for Computational Linguistics.Rabih Zbib, Erika Malchiodi, Jacob Devlin, DavidStallard, Spyros Matsoukas, Richard Schwartz, JohnMakhoul, Omar F. Zaidan, and Chris Callison-Burch.2012.
Machine translation of Arabic dialects.
In The2012 Conference of the North American Chapter ofthe Association for Computational Linguistics.
Asso-ciation for Computational Linguistics.90Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,Richard Schwartz, and John Makhoul.
2013.
Sys-tematic comparison of professional and crowdsourcedreference translations for machine translation.
In Pro-ceedings of the 2013 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Atlanta,Georgia.9192
