Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 320?327,New York, June 2006. c?2006 Association for Computational LinguisticsPrototype-Driven Learning for Sequence ModelsAria HaghighiComputer Science DivisionUniversity of California Berkeleyaria42@cs.berkeley.eduDan KleinComputer Science DivisionUniversity of California Berkeleyklein@cs.berkeley.eduAbstractWe investigate prototype-driven learning for pri-marily unsupervised sequence modeling.
Priorknowledge is specified declaratively, by provid-ing a few canonical examples of each target an-notation label.
This sparse prototype informationis then propagated across a corpus using distri-butional similarity features in a log-linear gener-ative model.
On part-of-speech induction in En-glish and Chinese, as well as an information extrac-tion task, prototype features provide substantial er-ror rate reductions over competitive baselines andoutperform previous work.
For example, we canachieve an English part-of-speech tagging accuracyof 80.5% using only three examples of each tagand no dictionary constraints.
We also compare tosemi-supervised learning and discuss the system?serror trends.1 IntroductionLearning, broadly taken, involves choosing a goodmodel from a large space of possible models.
In su-pervised learning, model behavior is primarily de-termined by labeled examples, whose productionrequires a certain kind of expertise and, typically,a substantial commitment of resources.
In unsu-pervised learning, model behavior is largely deter-mined by the structure of the model.
Designingmodels to exhibit a certain target behavior requiresanother, rare kind of expertise and effort.
Unsuper-vised learning, while minimizing the usage of la-beled data, does not necessarily minimize total ef-fort.
We therefore consider here how to learn mod-els with the least effort.
In particular, we argue for acertain kind of semi-supervised learning, which wecall prototype-driven learning.In prototype-driven learning, we specify prototyp-ical examples for each target label or label configu-ration, but do not necessarily label any documents orsentences.
For example, when learning a model forPenn treebank-style part-of-speech tagging in En-glish, we may list the 45 target tags and a few exam-ples of each tag (see figure 4 for a concrete prototypelist for this task).
This manner of specifying priorknowledge about the task has several advantages.First, is it certainly compact (though it remains tobe proven that it is effective).
Second, it is more orless the minimum one would have to provide to ahuman annotator in order to specify a new annota-tion task and policy (compare, for example, with thelist in figure 2, which suggests an entirely differenttask).
Indeed, prototype lists have been used ped-agogically to summarize tagsets to students (Man-ning and Schu?tze, 1999).
Finally, natural languagedoes exhibit proform and prototype effects (Radford,1988), which suggests that learning by analogy toprototypes may be effective for language tasks.In this paper, we consider three sequence mod-eling tasks: part-of-speech tagging in English andChinese and a classified ads information extractiontask.
Our general approach is to use distributionalsimilarity to link any given word to similar pro-totypes.
For example, the word reported may belinked to said, which is in turn a prototype for thepart-of-speech VBD.
We then encode these pro-totype links as features in a log-linear generativemodel, which is trained to fit unlabeled data (seesection 4.1).
Distributional prototype features pro-vide substantial error rate reductions on all threetasks.
For example, on English part-of-speech tag-ging with three prototypes per tag, adding prototypefeatures to the baseline raises per-position accuracyfrom 41.3% to 80.5%.2 Tasks and Related Work: TaggingFor our part-of-speech tagging experiments, we useddata from the English and Chinese Penn treebanks(Marcus et al, 1994; Ircs, 2002).
Example sentences320(a) DT VBN NNS RB MD VB NNS TO VB NNS IN NNS RBR CC RBR RB .The proposed changes also would allow executives to report exercises of options later and less often .
(b) NR AD VV AS PU NN VV DER VV PU PN AD VV DER VV PU DEC NN VV PU! "
# $ % & ?
( ) * + , - .
/ 0 * + , 1 2 3 4 5 6 7(c) FEAT FEAT FEAT FEAT NBRHD NBRHD NBRHD NBRHD NBRHD SIZE SIZE SIZE SIZEVine covered cottage , near Contra Costa Hills .
2 bedroom house ,FEAT FEAT FEAT FEAT FEAT RESTR RESTR RESTR RESTR RENT RENT RENT RENTmodern kitchen and dishwasher .
No pets allowed .
1050 / month$Figure 1: Sequence tasks: (a) English POS, (b) Chinese POS, and (c) Classified ad segmentationare shown in figure 1(a) and (b).
A great deal of re-search has investigated the unsupervised and semi-supervised induction of part-of-speech models, es-pecially in English, and there is unfortunately onlyspace to mention some highly related work here.One approach to unsupervised learning of part-of-speech models is to induce HMMs from un-labeled data in a maximum-likelihood framework.For example, Merialdo (1991) presents experimentslearning HMMs using EM.
Merialdo?s results mostfamously show that re-estimation degrades accu-racy unless almost no examples are labeled.
Lessfamously, his results also demonstrate that re-estimation can improve tagging accuracies to somedegree in the fully unsupervised case.One recent and much more successful approachto part-of-speech learning is contrastive estimation,presented in Smith and Eisner (2005).
They utilizetask-specific comparison neighborhoods for part-of-speech tagging to alter their objective function.Both of these works require specification of thelegal tags for each word.
Such dictionaries are largeand embody a great deal of lexical knowledge.
Aprototype list, in contrast, is extremely compact.3 Tasks and Related Work: ExtractionGrenager et al (2005) presents an unsupervisedapproach to an information extraction task, calledCLASSIFIEDS here, which involves segmenting clas-sified advertisements into topical sections (see fig-ure 1(c)).
Labels in this domain tend to be ?sticky?in that the correct annotation tends to consist ofmulti-element fields of the same label.
The over-all approach of Grenager et al (2005) typifies theprocess involved in fully unsupervised learning onnew domain: they first alter the structure of theirHMM so that diagonal transitions are preferred, thenmodify the transition structure to explicitly modelboundary tokens, and so on.
Given enough refine-Label PrototypesROOMATES roommate respectful dramaRESTRICTIONS pets smoking dogUTILITIES utilities pays electricityAVAILABLE immediately begin cheaperSIZE 2 br sqPHOTOS pictures image linkRENT $ month *number*15*1CONTACT *phone* call *time*FEATURES kitchen laundry parkingNEIGHBORHOOD close near shoppingADDRESS address carlmont *ordinal*5BOUNDARY ; .
!Figure 2: Prototype list derived from the develop-ment set of the CLASSIFIEDS data.
The BOUND-ARY field is not present in the original annotation,but added to model boundaries (see Section 5.3).The starred tokens are the results of collapsing ofbasic entities during pre-processing as is done in(Grenager et al, 2005)ments the model learns to segment with a reasonablematch to the target structure.In section 5.3, we discuss an approach to thistask which does not require customization of modelstructure, but rather centers on feature engineering.4 ApproachIn the present work, we consider the problem oflearning sequence models over text.
For each doc-ument x = [xi], we would like to predict a sequenceof labels y = [yi], where xi ?
X and yi ?
Y .
Weconstruct a generative model, p(x, y|?
), where ?
arethe model?s parameters, and choose parameters tomaximize the log-likelihood of our observed dataD:L(?
;D) =?x?Dlog p(x|?
)=?x?Dlog?yp(x, y|?
)321yi?1?DT,NN?yi?NN,VBD?xireportedxi?1witnessf(xi, yi) =????????
?word = reportedsuffix-2 = edproto = saidproto = had?????????
?VBDf(yi?1, yi) = DT ?NN ?VBDFigure 3: Graphical model representation of trigramtagger for English POS domain.4.1 Markov Random FieldsWe take our model family to be chain-structuredMarkov random fields (MRFs), the undirectedequivalent of HMMs.
Our joint probability modelover (x, y) is given byp(x, y|?)
= 1Z(?)n?i=1?
(xi, yi)?
(yi?1, yi)where ?
(c) is a potential over a clique c, taking theform exp{?T f(c)}, and f(c) is the vector of fea-tures active over c. In our sequence models, thecliques are over the edges/transitions (yi?1, yi) andnodes/emissions (xi, yi).
See figure 3 for an exam-ple from the English POS tagging domain.Note that the only way an MRF differs froma conditional random field (CRF) (Lafferty et al,2001) is that the partition function is no longer ob-servation dependent; we are modeling the joint prob-ability of x and y instead of y given x.
As a result,learning an MRF is slightly harder than learning aCRF; we discuss this issue in section 4.4.4.2 Prototype-Driven LearningWe assume prior knowledge about the target struc-ture via a prototype list, which specifies the set oftarget labels Y and, for each label y ?
Y , a set ofprototypes words, py ?
Py.
See figures 2 and 4 forexamples of prototype lists.11Note that this setting differs from the standard semi-supervised learning setup, where a small number of fully la-beled examples are given and used in conjunction with a largeramount of unlabeled data.
In our prototype-driven approach, wenever provide a single fully labeled example sequence.
See sec-tion 5.3 for further comparison of this setting to semi-supervisedlearning.Broadly, we would like to learn sequence modelswhich both explain the observed data and meet ourprior expectations about target structure.
A straight-forward way to implement this is to constrain eachprototype word to take only its given label(s) attraining time.
As we show in section 5, this doesnot work well in practice because this constraint onthe model is very sparse.In providing a prototype, however, we generallymean something stronger than a constraint on thatword.
In particular, we may intend that words whichare in some sense similar to a prototype generally begiven the same label(s) as that prototype.4.3 Distributional SimilarityIn syntactic distributional clustering, words aregrouped on the basis of the vectors of their pre-ceeding and following words (Schu?tze, 1995; Clark,2001).
The underlying linguistic idea is that replac-ing a word with another word of the same syntacticcategory should preserve syntactic well-formedness(Radford, 1988).
We present more details in sec-tion 5, but for now assume that a similarity functionover word types is given.Suppose further that for each non-prototype wordtype w, we have a subset of prototypes, Sw, whichare known to be distributionally similar to w (abovesome threshold).
We would like our model to relatethe tags of w to those of Sw.One approach to enforcing the distributional as-sumption in a sequence model is by supplementingthe training objective (here, data likelihood) with apenalty term that encourages parameters for whicheach w?s posterior distribution over tags is compati-ble with it?s prototypes Sw. For example, we mightmaximize,?x?Dlog p(x|?)
?
?w?z?SwKL( t|z || t|w)where t|w is the model?s distribution of tags forword w. The disadvantage of a penalty-based ap-proach is that it is difficult to construct the penaltyterm in a way which produces exactly the desiredbehavior.Instead, we introduce distributional prototypesinto the learning process as features in our log-linearmodel.
Concretely, for each prototype z, we intro-duce a predicate PROTO = z which becomes active322at each w for which z ?
Sw (see figure 3).
One ad-vantage of this approach is that it allows the strengthof the distributional constraint to be calibrated alongwith any other features; it was also more successfulin our experiments.4.4 Parameter EstimationSo far we have ignored the issue of how we learnmodel parameters ?
which maximizeL(?;D).
If ourmodel family were HMMs, we could use the EM al-gorithm to perform a local search.
Since we havea log-linear formulation, we instead use a gradient-based search.
In particular, we use L-BFGS (Liuand Nocedal, 1989), a standard numerical optimiza-tion technique, which requires the ability to evaluateL(?
;D) and its gradient at a given ?.The density p(x|?)
is easily calculated up to theglobal constant Z(?)
using the forward-backwardalgorithm (Rabiner, 1989).
The partition functionis given byZ(?)
=?x?yn?i=1?
(xi, yi)?
(yi?1, yi)=?x?yscore(x, y)Z(?)
can be computed exactly under certain as-sumptions about the clique potentials, but can in allcases be bounded byZ?(?)
=K?`=1Z?`(?)
=K?`=1?x:|x|=`score(x, y)WhereK is a suitably chosen large constant.
We canefficiently compute Z?`(?)
for fixed ` using a gener-alization of the forward-backward algorithm to thelattice of all observations x of length ` (see Smithand Eisner (2005) for an exposition).Similar to supervised maximum entropy prob-lems, the partial derivative of L(?
;D) with respectto each parameter ?j (associated with feature fj) isgiven by a difference in feature expectations:?L(?;D)?
?j=?x?D(Ey|x,?fj ?
Ex,y|?fj)The first expectation is the expected count of the fea-ture under the model?s p(y|x, ?)
and is again eas-ily computed with the forward-backward algorithm,Num TokensSetting 48K 193KBASE 42.2 41.3PROTO 61.9 68.8PROTO+SIM 79.1 80.5Table 1: English POS results measured by per-position accuracyjust as for CRFs or HMMs.
The second expectationis the expectation of the feature under the model?sjoint distribution over all x, y pairs, and is harder tocalculate.
Again assuming that sentences beyond acertain length have negligible mass, we calculate theexpectation of the feature for each fixed length ` andtake a (truncated) weighted sum:Ex,y|?fj =K?`=1p(|x| = `)Ex,y|`,?fjFor fixed `, we can calculate Ex,y|`,?fj using the lat-tice of all inputs of length `.
The quantity p(|x| = `)is simply Z?`(?)/Z?(?
).As regularization, we use a diagonal Gaussianprior with variance ?2 = 0.5, which gave relativelygood performance on all tasks.5 ExperimentsWe experimented with prototype-driven learning inthree domains: English and Chinese part-of-speechtagging and classified advertisement field segmenta-tion.
At inference time, we used maximum poste-rior decoding,2 which we found to be uniformly butslightly superior to Viterbi decoding.5.1 English POS TaggingFor our English part-of-speech tagging experiments,we used the WSJ portion of the English Penn tree-bank (Marcus et al, 1994).
We took our data to beeither the first 48K tokens (2000 sentences) or 193Ktokens (8000 sentences) starting from section 2.
Weused a trigram tagger of the model form outlined insection 4.1 with the same set of spelling features re-ported in Smith and Eisner (2005): exact word type,2At each position choosing the label which has the highestposterior probability, obtained from the forward-backward al-gorithm.323Label Prototype Label PrototypeNN % company year NNS years shares companiesJJ new other last VBG including being accordingMD will would could -LRB- -LRB- -LCB-VBP are ?re ?ve DT the a TheRB n?t also not WP$ whose-RRB- -RRB- -RCB- FW bono del kanjiWRB when how where RP Up ONIN of in for VBD said was hadSYM c b f $ $ US$ C$CD million billion two # #TO to To na : ?
: ;VBN been based compared NNPS Philippines Angels RightsRBR Earlier duller ?
?
?
non-?VBZ is has says VB be take provideJJS least largest biggest RBS WorstNNP Mr. U.S. Corp. , ,POS ?S CC and or ButPRP$ its their his JJR smaller greater largerPDT Quite WP who what WhatWDT which Whatever whatever .
.
?
!EX There PRP it he they?
?
UH Oh Well YeahFigure 4: English POS prototype listCorrect Tag Predicted Tag % of ErrorsCD DT 6.2NN JJ 5.3JJ NN 5.2VBD VBN 3.3NNS NN 3.2Figure 5: Most common English POS confusions forPROTO+SIM on 193K tokenscharacter suffixes of length up to 3, initial-capital,contains-hyphen, and contains-digit.
Our only edgefeatures were tag trigrams.With just these features (our baseline BASE) theproblem is symmetric in the 45 model labels.
Inorder to break initial symmetry we initialized ourpotentials to be near one, with some random noise.To evaluate in this setting, model labels must bemapped to target labels.
We followed the commonapproach in the literature, greedily mapping eachmodel label to a target label in order to maximizeper-position accuracy on the dataset.
The results ofBASE, reported in table 1, depend upon random ini-tialization; averaging over 10 runs gave an averageper-position accuracy of 41.3% on the larger trainingset.We automatically extracted the prototype list bytaking our data and selecting for each annotated la-bel the top three occurring word types which werenot given another label more often.
This resultedin 116 prototypes for the 193K token setting.3 Forcomparison, there are 18,423 word types occurringin this data.Incorporating the prototype list in the simplestpossible way, we fixed prototype occurrences in thedata to their respective annotation labels.
In thiscase, the model is no longer symmetric, and weno longer require random initialization or post-hocmapping of labels.
Adding prototypes in this waygave an accuracy of 68.8% on all tokens, but only47.7% on non-prototype occurrences, which is onlya marginal improvement over BASE.
It appears asthough the prototype information is not spreading tonon-prototype words.In order to remedy this, we incorporated distri-butional similarity features.
Similar to (Schu?tze,1995), we collect for each word type a context vectorof the counts of the most frequent 500 words, con-joined with a direction and distance (e.g +1,-2).
Wethen performed an SVD on the matrix to obtain a re-duced rank approximation.
We used the dot productbetween left singular vectors as a measure of distri-butional similarity.
For each word w, we find the setof prototype words with similarity exceeding a fixedthreshold of 0.35.
For each of these prototypes z,we add a predicate PROTO = z to each occurrence ofw.
For example, we might add PROTO = said to eachtoken of reported (as in figure 3).4Each prototype word is also its own prototype(since a word has maximum similarity to itself), sowhen we lock the prototype to a label, we are alsopushing all the words distributionally similar to thatprototype towards that label.53To be clear: this method of constructing a prototype listrequired statistics from the labeled data.
However, we believeit to be a fair and necessary approach for several reasons.
First,we wanted our results to be repeatable.
Second, we did not wantto overly tune this list, though experiments below suggest thattuning could greatly reduce the error rate.
Finally, it allowed usto run on Chinese, where the authors have no expertise.4Details of distributional similarity features: To extract con-text vectors, we used a window of size 2 in either direction anduse the first 250 singular vectors.
We collected counts fromall the WSJ portion of the Penn Treebank as well as the entireBLIPP corpus.
We limited each word to have similarity featuresfor its top 5 most similar prototypes.5Note that the presence of a prototype feature does not en-sure every instance of that word type will be given its proto-type?s label; pressure from ?edge?
features or other prototypefeatures can cause occurrences of a word type to be given differ-ent labels.
However, rare words with a single prototype featureare almost always given that prototype?s label.324This setting, PROTO+SIM, brings the all-tokensaccuracy up to 80.5%, which is a 37.5% error re-duction over PROTO.
For non-prototypes, the accu-racy increases to 67.8%, an error reduction of 38.4%over PROTO.
The overall error reduction from BASEto PROTO+SIM on all-token accuracy is 66.7%.Table 5 lists the most common confusions forPROTO+SIM.
The second, third, and fourth mostcommon confusions are characteristic of fully super-vised taggers (though greater in number here) andare difficult.
For instance, both JJs and NNs tend tooccur after determiners and before nouns.
The CDand DT confusion is a result of our prototype list notcontaining a contains-digit prototype for CD, so thepredicate fails to be linked to CDs.
Of course in arealistic, iterative design setting, we could have al-tered the prototype list to include a contains-digitprototype for CD and corrected this confusion.Figure 6 shows the marginal posterior distribu-tion over label pairs (roughly, the bigram transi-tion matrix) according to the treebank labels and thePROTO+SIM model run over the training set (usinga collapsed tag set for space).
Note that the broadstructure is recovered to a reasonable degree.It is difficult to compare our results to other sys-tems which utilize a full or partial tagging dictio-nary, since the amount of provided knowledge issubstantially different.
The best comparison is toSmith and Eisner (2005) who use a partial taggingdictionary.
In order to compare with their results,we projected the tagset to the coarser set of 17 thatthey used in their experiments.
On 24K tokens, ourPROTO+SIM model scored 82.2%.
When Smith andEisner (2005) limit their tagging dictionary to wordswhich occur at least twice, their best performingneighborhood model achieves 79.5%.
While thesenumbers seem close, for comparison, their taggingdictionary contained information about the allow-able tags for 2,125 word types (out of 5,406 types)and the their system must only choose, on average,between 4.4 tags for a word.
Our prototype list,however, contains information about only 116 wordtypes and our tagger must on average choose be-tween 16.9 tags, a much harder task.
When Smithand Eisner (2005) include tagging dictionary entriesfor all words in the first half of their 24K tokens, giv-ing tagging knowledge for 3,362 word types, they doachieve a higher accuracy of 88.1%.Setting AccuracyBASE 46.4PROTO 53.7PROTO+SIM 71.5PROTO+SIM+BOUND 74.1Figure 7: Results on test set for ads data in(Grenager et al, 2005).5.2 Chinese POS TaggingWe also tested our POS induction system on the Chi-nese POS data in the Chinese Treebank (Ircs, 2002).The model is wholly unmodified from the Englishversion except that the suffix features are removedsince, in Chinese, suffixes are not a reliable indi-cator of part-of-speech as in English (Tseng et al,2005).
Since we did not have access to a large aux-iliary unlabeled corpus that was segmented, our dis-tributional model was built only from the treebanktext, and the distributional similarities are presum-ably degraded relative to the English.
On 60K wordtokens, BASE gave an accuracy of 34.4, PROTO gave39.0, and PROTO+SIM gave 57.4, similar in order ifnot magnitude to the English case.We believe the performance for Chinese POS tag-ging is not as high as English for two reasons: thegeneral difficulty of Chinese POS tagging (Tseng etal., 2005) and the lack of a larger segmented corpusfrom which to build distributional models.
Nonethe-less, the addition of distributional similarity featuresdoes reduce the error rate by 35% from BASE.5.3 Information Field SegmentationWe tested our framework on the CLASSIFIEDS datadescribed in Grenager et al (2005) under conditionssimilar to POS tagging.
An important characteristicof this domain (see figure 1(a)) is that the hidden la-bels tend to be ?sticky,?
in that fields tend to consistof runs of the same label, as in figure 1(c), in con-trast with part-of-speech tagging, where we rarelysee adjacent tokens given the same label.
Grenageret al (2005) report that in order to learn this ?sticky?structure, they had to alter the structure of theirHMM so that a fixed mass is placed on each diag-onal transition.
In this work, we learned this struc-ture automatically though prototype similarity fea-tures without manually constraining the model (see325INPUNCPRTTOVBNLPUNCWDETADVVPOSENDPUNCVBGPREPADJRPUNCNCONJINPUNCPRTTO VBNLPUNCW DETADVV POSENDPUNCVBGPREPADJRPUNCN CONJINPUNCPRTTOVBNLPUNCWDETADVVPOSENDPUNCVBGPREPADJRPUNCNCONJINPUNCPRTTO VBNLPUNCW DETADVV POSENDPUNCVBGPREPADJRPUNCN CONJ(a) (b)Figure 6: English coarse POS tag structure: a) corresponds to ?correct?
transition structure from labeleddata, b) corresponds to PROTO+SIM on 24K tokensROOMATESUTILITIESRESTRICTIONSAVAILABLESIZEPHOTOSRENTFEATURESCONTACTNEIGHBORHOODADDRESSROOMATESUTILITIESRESTRICTIONSAVAILABLESIZEPHOTOSRENTFEATURESCONTACTNEIGHBORHOODADDRESSROOMATESUTILITIESRESTRICTIONSAVAILABLESIZEPHOTOSRENTFEATURESCONTACTNEIGHBORHOODADDRESS(a) (b) (c)Figure 8: Field segmentation observed transition structure: (a) labeled data, (b) BASE(c)BASE+PROTO+SIM+BOUND (after post-processing)figure 8), though we did change the similarity func-tion (see below).On the test set of (Grenager et al, 2005),BASE scored an accuracy of 46.4%, comparable toGrenager et al (2005)?s unsupervised HMM base-line.
Adding the prototype list (see figure 2) withoutdistributional features yielded a slightly improvedaccuracy of 53.7%.
For this domain, we utilizeda slightly different notion of distributional similar-ity: we are not interested in the syntactic behaviorof a word type, but its topical content.
Therefore,when we collect context vectors for word types inthis domain, we make no distinction by directionor distance and collect counts from a wider win-dow.
This notion of distributional similarity is moresimilar to latent semantic indexing (Deerwester etal., 1990).
A natural consequence of this definitionof distributional similarity is that many neighboringwords will share the same prototypes.
Thereforedistributional prototype features will encourage la-bels to persist, naturally giving the ?sticky?
effectof the domain.
Adding distributional similarity fea-tures to our model (PROTO+SIM) improves accuracysubstantially, yielding 71.5%, a 38.4% error reduc-tion over BASE.6Another feature of this domain that Grenager etal.
(2005) take advantage of is that end of sen-tence punctuation tends to indicate the end of afield and the beginning of a new one.
Grenager etal.
(2005) experiment with manually adding bound-ary states and biasing transitions from these statesto not self-loop.
We capture this ?boundary?
ef-fect by simply adding a line to our protoype-list,adding a new BOUNDARY state (see figure 2) witha few (hand-chosen) prototypes.
Since we uti-lize a trigram tagger, we are able to naturally cap-ture the effect that the BOUNDARY tokens typicallyindicate transitions between the fields before andafter the boundary token.
As a post-processingstep, when a token is tagged as a BOUNDARY6Distributional similarity details: We collect for each worda context vector consisting of the counts for words occurringwithin three token occurrences of a word.
We perform a SVDonto the first 50 singular vectors.326Correct Tag Predicted Tag % of ErrorsFEATURES SIZE 11.2FEATURES NBRHD 9.0SIZE FEATURES 7.7NBRHD FEATURES 6.4ADDRESS NBRHD 5.3UTILITIES FEATURES 5.3Figure 9: Most common classified ads confusionstoken it is given the same label as the previousnon-BOUNDARY token, which reflects the annota-tional convention that boundary tokens are given thesame label as the field they terminate.
Adding theBOUNDARY label yields significant improvements,as indicated by the PROTO+SIM+BOUND setting inTable 5.3, surpassing the best unsupervised resultof Grenager et al (2005) which is 72.4%.
Further-more, our PROTO+SIM+BOUND model comes closeto the supervised HMM accuracy of 74.4% reportedin Grenager et al (2005).We also compared our method to the most ba-sic semi-supervised setting, where fully labeled doc-uments are provided along with unlabeled ones.Roughly 25% of the data had to be labeledin order to achieve an accuracy equal to ourPROTO+SIM+BOUND model, suggesting that the useof prior knowledge in the prototype system is partic-ularly efficient.In table 5.3, we provide the top confusions madeby our PROTO+SIM+BOUND model.
As can be seen,many of our confusions involve the FEATURE field,which serves as a general purpose background state,which often differs subtly from other fields such asSIZE.
For instance, the parenthical comment: ( mas-ter has walk - in closet with vanity ) is labeled asa SIZE field in the data, but our model proposedit as a FEATURE field.
NEIGHBORHOOD and AD-DRESS is another natural confusion resulting fromthe fact that the two fields share much of the samevocabulary (e.g [ADDRESS 2525 Telegraph Ave.] vs.[NBRHD near Telegraph]).Acknowledgments We would like to thank theanonymous reviewers for their comments.
Thiswork is supported by aMicrosoft / CITRIS grant andby an equipment donation from Intel.6 ConclusionsWe have shown that distributional prototype featurescan allow one to specify a target labeling schemein a compact and declarative way.
These featuresgive substantial error reduction on several inductiontasks by allowing one to link words to prototypes ac-cording to distributional similarity.
Another positiveproperty of this approach is that it tries to reconcilethe success of sequence-free distributional methodsin unsupervised word clustering with the success ofsequence models in supervised settings: the similar-ity guides the learning of the sequence model.ReferencesAlexander Clark.
2001.
The unsupervised induction of stochas-tic context-free grammars using distributional clustering.
InCoNLL.Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer,George W. Furnas, and Richard A. Harshman.
1990.
In-dexing by latent semantic analysis.
Journal of the AmericanSociety of Information Science, 41(6):391?407.Trond Grenager, Dan Klein, and Christopher Manning.
2005.Unsupervised learning of field segmentation models for in-formation extraction.
In Proceedings of the 43rd Meeting ofthe ACL.Nianwen Xue Ircs.
2002.
Building a large-scale annotated chi-nese corpus.John Lafferty, Andrew McCallum, and Fernando Pereira.
2001.Conditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In International Con-ference on Machine Learning (ICML).Dong C. Liu and Jorge Nocedal.
1989.
On the limited mem-ory bfgs method for large scale optimization.
MathematicalProgramming.Christopher D. Manning and Hinrich Schu?tze.
1999.
Founda-tions of Statistical Natural Language Processing.
The MITPress.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated corpusof english: The penn treebank.
Computational Linguistics,19(2):313?330.Bernard Merialdo.
1991.
Tagging english text with a proba-bilistic model.
In ICASSP, pages 809?812.L.R Rabiner.
1989.
A tutorial on hidden markov models andselected applications in speech recognition.
In IEEE.Andrew Radford.
1988.
Transformational Grammar.
Cam-bridge University Press, Cambridge.Hinrich Schu?tze.
1995.
Distributional part-of-speech tagging.In EACL.Noah Smith and Jason Eisner.
2005.
Contrastive estimation:Training log-linear models on unlabeled data.
In Proceed-ings of the 43rd Meeting of the ACL.Huihsin Tseng, Daniel Jurafsky, and Christopher Manning.2005.
Morphological features help pos tagging of unknownwords across language varieties.
In Proceedings of theFourth SIGHAN Workshop on Chinese Language Process-ing.327
