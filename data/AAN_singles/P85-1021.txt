PARSING HEAD-DRIVEN PHRASE STRUCTURE GRAMMARDerek Proudlan and Carl PollardHewlett-Packard Laboratories1501 Page Mill RoadPalo Alto, CA.
94303, USAAbstractThe Head-driven Phrase Structure Grammar  project(HPSG)  is an English language database query systemunder development at Hewlett-Packard Laboratories.Unlike other product-oriented efforts in the natural lan-guage understanding field, the HPSG system was de-signed and implemented by linguists on the basis ofrecent theoretical developments.
But, unlike other im-plementations of linguistic theories, this system is not atoy, as it deals with a variety of practical problems notcovered in the theoretical literature.
We believe thatthis makes the HPSG system ,nique in its combinationof linguistic theory and practical application.The HPSG system differs from its predecessorGPSG,  reported on at the 1982 ACL  meeting (Gawronet al 119821), in four significant respects: syntax, lexi-cal representation, parsing, and semantics.
The paperfocuses on parsing issues, but also gives a synopsis ofthe underlying syntactic formalism.1 SyntaxHPSG is a lexically based theory of phrase struc-ture, so called because of the central role played bygrammlttical heads and their associated complements.
'Roughly speaking, heads are linguistic forms (wordsand phrases) tl, at exert syntactic and semantic restric-tions on the phrases, called complements, that charac-teristically combine with them to form larger phrases.Verbs are the heads of verb phrm~es (apd sentences),nouns are the heads of noun phra~es, and so forth.As in most current syntactic theories, categoriesare represented as complexes of feature specifications.But the \[IPSG treatment of lcxical subcategorizationobviates the need in the theory of categories for the no-tion of bar-level (in the sense of X-bar theory, prevalentin much current linguistic research}.
\[n addition, theaugmentation of the system of categories with stack-valued features - features whose values ~re sequencesof categories - unilies the theory of lexical subcatego-riz~tion with the theory of bi,~ding phenomena.
Bybinding pimnomena we meaa essentially noJL-clause-bounded delmndencies, ,'such a.~ th~rse involving dislo-cated constituents, relative ~Lnd interrogative pronouns,and reflexive and reciprocal pronouns \[12 I.
* i I PSG ul a relinwlJ~i?
~ld  ?zt.,~nsioll ,,f th~ clu~dy rel~tteu Gt~lmr~dilmd Ph?.
'tmeStructu lm Gran|n |ar  lTI.
The  detaaJs uf l i ly tllt~J/-y of HPSG ar~ Nt  forth in I i | \ [ .More precisely, the subcategor izat ion of a head isencoded as the value of a stack-valued feature called~SUBCAT".
For example, the SUBCAT value of theverb persuade is the sequence of three categories IVP,NP, NP I, corresponding to the grammatical relations(GR's):  controlled complement, direct object, and sub-ject respectively.
We are adopting a modified version ofDowty's \[19821 terminology for GR's, where subject "LSlast, direct object second-to-last, etc.
For semantic rea-sons we call the GR following a controlled complementthe controller.One of the key differences between HPSG and itspredecesor GPSG is the massive relocation of linguisticinformation from phrase structure rules into the lexi-con \[5\].
This wholesale lexicalization of linguistic infor-mation in HPSG results in a drastic reduction in thenumber of phrase structure rules.
Since rules no longerhandle subcategorization, their sole remaining functionis to encode a small number of language-specific prin-ciples for projecting from \[exical entries h, surface con-stituent order.The schematic nature of the grammar  rules allowsthe system to parse a large fragment of English withonly a small number of rules (the system currently usessixteen), since each r1,le can be used in many differentsituations.
The constituents of each rule are sparselyannotated with features, but are fleshed out when takentogether with constituents looked for and constituentsfound.For example the sentence The manager works canbe parsed using the single rule RI below.
The ruleis applied to build the noun phrase The manager byidentifying the head H with the \[exical element man-aqer and tile complement CI with the lexical elementthe.
The entire sentence is built by ideutifying the Hwith works and the C1 with the noun phrase describedabove.
Thus the single rule RI functions as both the S-* NP VP, and NP ~ Det N rules of familiar contextfRe grammars.R1.
x -> ci hi(CONTROL INTRANS)\] a*Figure I.
A Grammar Rule.167\]Feature Pass ingThe theory of HPSG embodies a number of sub-stantive hypotheses about universal granunatical prin-ciples, Such principles as the Head Feature Princi-ple, the Binding Inheritance Principle, and the Con-trol Agreement Principle, require that certain syntac-tic features specified on daughters in syntactic trees areinherited by the mothers.
Highly abstract phrase struc-ture rules thus give rise to fully specified grammaticalstructures in a recursive process driven by syntactic in-formation encoded on lexical heads.
Thus HPSG,  un-like similar ~unification-based" syntactic theories, em-bodies a strong hypothesis about the flow of relevantinformation in the derivation of complex structures.UnificationAnother important difference between HPSG andother unification based syntactic theories concerns theform of the expressions which are actually unified.In HPSG, the structures which get unified are (withlimited exceptions to be discussed below) not generalgraph structures as in Lexical Functional Qrammar \[1 I,or Functional Unification Granmaar IlOI, but rather fiatatomic valued feature matrices, such as those ~hownbelow.\[(CONTROL 0 INTRANS) (MAJ N A)(AGR 3RDSG) (PRD MINUS) (TOP MINUS)\]\[(CONTROL O) (MAJ H V) (INV PLUS)\]Figure 2.
Two feature matrices.In the implementation f \[\[PSG we have been ableto use this restrictiou on the form of feature tnatricesto good advantage.
Since for any given version of thesystem the range of atomic features and feature valuesis fixed, we are able to represent fiat feature matrices,such as the ores above, as vectors of intcKers, whereeach cell in the vector represents a feature, and ~he in-teger in each cell represents a disjunctioa of tile possiblevalues for that feature.CON MAJ AGR PRD INV TOP .... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.i tTt to i2  I t  13 I t II t \ [  t21 7 13 I t I 3 IFigure 3: Two ~ransduced feature matrices.For example, if the possible values of the MAJ  fea-ture are N, V, A, and P then we can uuiquely representany combination of these features with an integer inthe raalge 0..15.
This is accomplished simply by a~ign-ing each possible value an index which is an integralpower of 2 in this range and then adding up the indicesso derived for each disjunction of values encountered.Unification in such cases is thus reduced to the "logicaland" of the integers in each cell of the vector represent-ing the feature matrix.
In this way unification of theseflat structures can be done in constant time, and since=logical and" is generally a single machine instructionthe overhead is very low.N V A P\[ t \[ 0 \[ t \[ 0 \[ = tO = (MAJ N A)I t  I t  10 io l= t2= (MAJ gV)I l m l l m l l m m l l l l l l l ~ l  Un?ficationI I I 0 I 0 I 0 I = 8 = (MAJ H)Figure 4: Closeup of the MAJ feature.There are, however, certain cases when the valuesof features are not atomic, but are instead themselvesfeature matrices.
The unification of such structurescould, in theory, involve arbitrary recursion on the gen-eral unification algorithm, and it would seem that wehad not progressed very far from the problem of uni-fying general graph structures.
Happily, the featuresfor which this property of embedding holds, constitutea small finite set (basically tlte so called "binding fea-tures").
Thus we are able to segregate such featuresfrom the rest, and recurse only when such a "categoryvalued ~ feature is present.
\[n practice, therefore, thetime performance of the general uailication algorithmis very good, essentially the sanze a.s that of the lintstructure unification algorithm described above.2 Pars ingAs in the earlier GPSG system, the primary jobof the parser in the HPSG system is to produce a se-mantics for the input sentence.
This is done composi-tionally as the phrase structure is built, and uses onlylocally available information.
Thus every constituentwhich is built syntactically has a corresponding seman-tics built for it at the same time, using only informationavailable in the phrasal subtree which it immediatelydominates.
This locality constraint in computing thesemantics for constituents is an essential characteristicof HPSG.
For a more complete description of the se-mantic treatment used in the HPSG system see Crearyand Pollard \[2\].Head-dr iven Active Chart  ParserA crucial dilference between the HPSG system andits predecessor GPSG is the importance placed on thehead constituent in HPSG.
\[n HPSG it is the head con-stituent of a rule which carries the subcategorizationinformation needed to build the other constituents of168the rule.
Thus parsing proceeds head first through thephrase structure of a sentence, rather than left to rightthrough the sentence string.The parser itself is a variation of an active chartparser \[4,9,8,13\], modified to permit he construction ofconstituents head first, instead of in left-to-right order.In order to successfully parse "head first", an edge*must be augmented to include information about itsspan (i.e.
its position in the string).
This is necessarybecause heaA can appear as a middle constituent ofa rule with other constituents (e.g.
complements oradjuncts) on either side.
Thus it is not possible torecord all the requisite boundary information simplyby moving a dot through the rule (as in Earley), or bykeeping track of just those constituents which remainto be built (as in Winograd).
An example should makethis clear.Suppose as before we are confronted with the taskof parsing the sentence The manager works, and againwe have available the grammar ule R1.
Since we areparsing in a ~head first" manner we must match theH constituent against some substring of the sentence.But which substring?
In more conventional chart pars-ing algorithms which proceed left to right this is nota serious problem, since we are always guaranteed tohave an anchor to the left.
We simply try building the\[eftmost constituent of the rule starting at the \[eftmostposition of the string, and if this succeeds we try tobuild the next \[eftmost constituent s arting at one po-sition to the right of wherever the previous constituentended.
However in our case we cannot ausume any suchanchoring to the left, since as the example illustrates.the H is not always leftmost.The solution we have adopted in the HPSG systemis to annotate ach edge with information about thespan of substring which it covers.
In the example be-low the inactive dge E1 is matched against he head ofrule R1, and since they unify the new active edge E2 iscreated with its head constituent instantiated with thefeature specifications which resulted from the unifica-tion.
This new edge E2 is annotated with the span ofthe inactive edge El.
Some time later the inactive edgeI,:3 is matched against he "np" constituent of our ac-tive edge E2, resulting in the new active edge E.I.
Thespan of E4 is obtained by combining the starting posi-tion of E3 {i.e.
t) with the finishing postion of E2 (i.e.3).
The point is that edges ~Lre constructed from thehead out, so that at any given tame in Lhe life cycle ofan edge the spanning informatiun on the edge recordsthe span of contiguous substring which it covers.Note that in the transition from rule il l to edge1~2 we have relabeled the constituent markers z, cl,~nd h with the symbols ~, np, ~utd VP respectively.This is done merely a.s ~t mnemouic device to reflectthe fact that once the head of the edge is found, thesubcategorization information on that head (i.e.
thevalues of the "SUHCAT" feature of the verb work.s) isAn edi\[e is, Iooe~y spea&ing, ,-tn inlCantiation of a nile witll ~nnle of tile\ [e~urml  on  conlltituentll m~de ntore spm:ifl?.propagated to the other elements of the edge, therebyrestricting the types of constituents with which theycan be satisfied.
Writing a constituent marker in uppercase indicates that an inactive edge has been foundto instantiate it, while a lower case (not yet found)constituent in bold face indicates that this is the nextconstituent which will try to be instantiated.El.
V<3.3>RI.
x -> ci h a*g2.
s<3.3> -> np VP a*E3.
NP<I,2>"E2.
s<3,3> -> np VP a*E4.
s<1.3> -> ~P VP R*'Figure 5: Combining edges and rules.Us ing Semantics RestrictionsParsing ~head first" offers both practical and theo-retical advantages.
As mentioned above, the categoriesof the grammatical relations subcategorized for by aparticular head are encoded as the SUBCAT value ofthe head.
Now GR's are of two distinct types: thosewhich are ~saturated" (i.e.
do not subcategorize foranything themselves), such as subject and objects, andthose which subcategorize for a subject (i.e.
controlledcomplements).
One of the language-universal gram-matical principles (the Control Agreement Principle)requires that the semantic controller of a controlledcomplement always be the next grammatical relation(in the order specified by the value of the SUBCATfeature of the head) after the controlled complementto combine with the head.
But since the HPSG parseralways finds the head of a clause first, the grammati-cal order of its complements, as well as their semanticroles, are always specified before the complements arefound.
As a consequence, semantic processing ~f con-stituents can be done on the fly as the constituentsare found, rather than waiting until an edge has beencompleted.
Thus semantic processing can be do.e ex-tremely locally (constituent-to-constituent in the edge,rather than merely node-to-node in the parse tree as inMontague semantics), and therefore a parse path ,anbe abandoned on semantic grounds (e.g.
sortal iltcon-sistency) in the rniddle of constructing an edge.
la thisway semantics, as well as syntax, can be used to controlthe parsing process.Anaphora  ill HPSGAnother example of how parsing ~head first" paysoil is illustrated by the elegant technique this strat-egy makes possible for the binding of intr~ententiala~taphors.
This method allows us to assimilate cases ofbound anaphora to the same general binding methodused iu the HPSG system to handle other non-lexically-governed ependencies ~uch a.s ~ap.~, ~,ttt~ro~,t.ive pro-nouns, and relative pronouns.
Roughly, the unbounddependencies of each type on every constituent are en- ?coded as values of a,n appropriate stack-valued feature169("binding feature").
In particular, unbound anaphorsaxe kept track of by two binding features, REFL  (forreflexive pronouns) and BPRO \['or personal pronounsavailable to serve as bound anaphors.
According tothe Binding Inheritance Principle, all categories onbinding-feature stacks which do not get bound under aparticular node are inherited onto that node.
Just howbinding is effected depends on the type of dependency.In the case of bound anaphora, this is accomplishedby merging the relevant agreement information (storedin the REFL  or BPRO stack of the constituent contain-ing the anaphor) with one of the later GR's subcatego-rized for by the head which governs that constituent.This has the effect of forcing the node that ultimatelyunifies with that GR (if any) to be the sought-afterantecedent.
The difference between reflexives and per-sonal pronouns is this.
The binding feature REFLis not allowed to inherit onto nodes of certain types(those with CONTROL value \[N'rRANS}, thus forc-ing the reflexive pronoun to become locally bound.
Inthe case of non-reflexive pronouns, the class of possibleantecedents is determined by n,,difying the subcatego-rization information on the hel,,l governing the pronounso that all the subcategorized-fl~r GR's later in gram-matical order than the pronoun are "contra-indexed"with the pronoun (and thereby prohibited front beingits antecedent).
Binding then takes place precisely aswith reflexives, but somewhere higher in the tree.We illustrate this d~ttttction v, ti, kh I.~O examples.\[n sentence S I below told subcategorizes for three con-stituents: the subject NP  Pullum, the direct objectGazdar, and the oblique object PP about himself.
'Thus either PuUum or f;uzdur are po~ible antecedentsof himself, but not Wasow.SI.
Wasow was convinced that Pullum toldGazdar about himself.$2.
Wasow persuaded Pullum to shave him.\[n sentence 52 shave subcategorizes for tile directobject NP him and an NP subject eventue.tly tilled bythe constituent Pullum via control.
Since the subjectposition is contra-indexed with tile pronoun, PuUum isblocked from serving a~ the a,tecedent.
The pro,munis eventually bound by the NP WanouJ higher up in thetree.Heuristics to Opt iudze .
"Joareh'\['he liPS(; system, based as it is upon a care-fully developed hngui~tic theory, has broad expressivepower.
In practice, how-ver, much of this power is oftennot necessary.
To exploit this fact the IiPSC, systemu.~cs heuristics to help r,,duve the search space implic-itly defined by the grammar.
These heuristics allowthe parser to produce an optimally ordered agenda ofedges to try ba.sed on words used in tile sentence, andon constituents it has found so far.?
The  pNpOl l t iO l l  t l  t re l t t~|  eel~lttt~tllF :is a c:~se tam'k ing .One type of heuristic involves additional syntacticinformation which can be attached to rules to deter-mine their likelihood.
Such a heuristic is based on thecurrently intended use for the rule to which it is at-tached, and on the edges already available in the chart.An example of this type of heuristic is sketched below.RI.
x -> cl h a*Heuristic-l: Are the features of cl +QUE?Figure 6: A rule with an attached heuristic.Heuristic-I encodes the fact that rule RI, whenused in its incarnation as the S -- NP  VP  rule, is pri-marily intended to handle declarative sentences ratherthan questions.
Thus if the answer to Heuristic-1 is"no" then this edge is given a higher ranking than ifthe answer is "yes".
This heuristic, taken together withothers, determines the rank of the edge instantiatedfrom this rule, which in turn determines the order inwhich edges will be tried.
The result in this case is thatfor a sentence such as 53 below, the system will pre-fer the reading for which an appropriate answer is ".acharacter in a play by Shakespeare", over the readingwhich has as a felicitous answer "Richard Burton".S3.
Who is Hamlet?It should be empha~sized, however, that heuristicsare not an essential part of the system, as are the fea-ture passing principles, but rather are used only forreasons of efficiency.
In theory all possible constituentspermitted by the grammar will be found eventuallywith or without heuristics.
The heuristics siu,ply helpa linguist tell the parser which readings are most likely,and which parsing strategies are usually most fruitful,thereby allowing the parser to construct the most likelyreading first.
We believe that this clearly diifereuti-ares \[IPSG from "ad hoc" systems which do not makesharp the distinction between theoretical principle andheuristic guideline, and that this distinction is an izn-portant one if the natural language understanding pro-grams of today are to be of any use to the naturallanguage programs and theories of the future.ACKO WLED(4EME1NTSWe would like to acknowledge the valuable assi-tance of Thomas Wasow ~md Ivan Sag ht tile writingof this paper.
We would also like to thank Martin Kayand Stuart Shi.e~:r lot tlke~r tte\[p\[u\[ cut tut teut ,  a ly  on  anearlier draft.170REFERENCESIll Bresnan, J.
(ed).
(1982)The Mental Representation of Grammatical Rela-tions, The MIT Press, Cambridge, Mass.\[z) Creary, L. and C. Pol lard (1985)~A Computational Semantics for Natural Lan-guage", Proceedings of the gSrd Annual Meetingof the Association for Computational Linguistics.\[31 Dowry, D.R.
(1982)"Grammatical Relations and Montague Grammar",In P. Jacobson and G.K. Pullum (eds.
), The Natureof Syntactic Representation D. Reidel PublishingCo., Dordrecht, Holland.L41 Earley, J.
(1970)"An efficient context-free parsing algorithm",CACM 6:8, 1970.\[5t Fliekinger, D., C. Pollard.
T. Wasow (1985)"Structure-Sharing in Lexical Representation",Proceedings of the 23rd Annual Meetin!l of theAssociation for Computational Linguistics.i61 Gawron,  3. et al (1982)"Processing English with a Generalized PhraseStructure Grammar", ACL Proceedings 20.it!
Gazdar ,  G. et al (in pres,s)Generalized Phrase Structure (;rammar,Blackwell and Harvard University Press.Is!
Kaplan,  R. ( 197:.
!~A General Syl!tlxt:l, ic Processor", la Rustin (ed.
)Natural Langua~te Proeessiny.
Algorithmics Press,N.Y.Kay, M. \[t973)"The MIND System", lu Rusl, in (ed.)
NaturalLanguage Processiur.i.
Algorithmics Press, N.Y.it01 Kay, M. (forthcoming)"Parsing in Functiotml Uailicatiou Grammar".iLll Pollard.
C. (198,1)Generalized Context-Free (;rammur~, Ile, ad (:r.m-mar.s,  and Natural L,mtlU.Ve, I'tn.D.
Dissertation,Stanford.Pollard, C. (forthcotnitlg)"A Semantic Approlu:h to Ilhuling in ;t Movms-trata\[ Theory", To appe,~r in Lin!luistic~ andPhilosophy.131 Winograd, T. (tg~O)Language as a Uo~nitive l'rocess.Addi~on-W~lcy, lteadiag, Marts.171
