Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 277?281,Prague, June 2007. c?2007 Association for Computational LinguisticsPUTOP: Turning Predominant Senses into a Topic Model for Word SenseDisambiguationJordan Boyd-GraberComputer SciencePrinceton UniversityPrinceton, NJ 08540jbg@princeton.eduDavid BleiComputer SciencePrinceton UniversityPrinceton, NJ 08540blei@cs.princeton.eduAbstractWe extend on McCarthy et al?s predom-inant sense method to create an unsuper-vised method of word sense disambiguationthat uses automatically derived topics us-ing Latent Dirichlet alocation.
Using topic-specific synset similarity measures, we cre-ate predictions for each word in each doc-ument using only word frequency informa-tion.
It is hoped that this procedure can im-prove upon the method for larger numbersof topics by providing more relevant train-ing corpora for the individual topics.
Thismethod is evaluated on SemEval-2007 Task1 and Task 17.1 Generative Model of WSDWord Sense Disambiguation (WSD) is the problemof labeling text with the appropriate semantic labelsautomatically.
Although WSD is claimed to be anessential step in information retrieval and machinetranslation, it has not seen effective practical appli-cation because the dearth of labeled data has pre-vented the use of established supervised statisticalmethods that have been successfully applied to othernatural language problems.Unsupervised methods have been developed forWSD, but despite modest success have not al-ways been well understood statistically (Abney,2004).
Unsupervised methods are particularly ap-pealing because they do not require expensive sense-annotated data and can use the ever-increasingamount of raw text freely available.
This paper ex-pands on an effective unsupervised method for WSDand embeds it into a topic model, thus allowing analgorithm trained on a single, monolithic corpora toinstead hand-pick relevant documents in choosinga disambiguation.
After developing this generativestatistical model, we present its performance on anumber of tasks.1.1 The Intersection of Syntactic and SemanticSimilarityMcCarthy et al (2004) outlined a method for learn-ing a word?s most-used sense given an untagged cor-pus that ranks each sense wsi using a distributionalsyntactic similarity ?
and a WORDNET-derived se-mantic similarity ?.
This process for a word w usesits distributional neighbors Nw, the possible sensesof not only the word in question, Sw, and also thoseof the distributionally similar words, Snj .
Thus,P (wsi) =?nj?Nw?
(w, nj)wnss(wsi, nj)?wsj?Swwnss(wsj , nj), (1)where wnss(s, c) =maxa?Sc?
(a, s).
(2)One can view finding the appropriate sense as asearch in two types of space.
In determining howgood a particular synset wsi is, ?
guides the searchin the semantic space and ?
drives the search in thesyntactic space.
We consider all of the words usedin syntactically similar contexts, which we call ?cor-roborators,?
and for each of them we find the closestmeaning to wsi using a measure of semantic sim-ilarity ?, for instance a WORDNET-based similar-ity measure such as Jiang-Conrath (1997).
Each ofthe neighboring words?
contributions is weighted bythe syntactic probability, as provided by Lin?s distri-butional similarity measure (1998), which rates twowords to be similar if they enter into similar syntac-tic constructions.277Vw csFigure 1: A reinterpretation of McCarthy et al?s pre-dominant sense method as a generative model.
Notethat this model has no notion of context; a synset isassigned in an identical manner for all of the wordsin a vocabulary.One can think of this process as a generativemodel, even though it was not originally posed insuch a manner.
For each word w in the vocabulary,we generate one of the neighbor corroborators ac-cording to the Lin similarity, ?
(c, w), between thetwo words.
We then generate a synset s for thatword proportional to the maximum semantic sim-ilarity between s and any synset that contains thecorroborator c (see Figure 1).Our aim in this paper is to extend the method ofMcCarthy et al using topic models.
It is hoped thatallowing the method to in effect ?choose?
the con-texts that it uses will improve its ability to disam-biguate sentences.1.2 Using Topic Models to Partition aDocument?s WordsTopic models like Latent Dirichlet alocation(LDA) (Blei et al, 2003) assume a model of textgeneration where each document has a multinomialdistribution over topics and each word comes fromone of these topics.
In LDA, each topic is a multino-mial distribution, and each document has a multino-mial distribution over topics drawn from a Dirichletprior that selects the topic for each word in a docu-ment.
Previous work has shown that such a modelimproves WSD over using a single corpus (Boyd-Graber et al, 2007), and we use this insight to de-velop an extension of McCarthy?s method for multi-ple topics.Although describing the statistical backgroundand motivations behind topic models are beyond thescope of this paper, it suffices to note that the topicsinduced from a corpus provide a statistical group-ing of words that often occur together and a proba-bilistic assignment of each word in a corpus to top-ics.
Thus, one topic might have terms like ?gov-ernment,?
?president,?
?govern,?
and ?regal,?
whileanother topic might have terms like ?finance,?
?high-yield,?
?investor,?
and ?market.?
This paper assumesthat the machinery for learning these distributionscan, given a corpus and a specified number of top-ics, return the topic distributions most likely to havegenerated the corpus.1.3 Defining the ModelWhile the original predominant senses method usedLin?s thesaurus similarity method alone in generat-ing the corroborator, we will also use the probabilityof that word being part of the same topic as the wordto be disambiguated.
Thus the process of choosingthe ?corroborator?
is no longer identical for eachword; it is affected by its topic, which changes forevery document.
This new generative process canbe thought of as a modified LDA system that, afterselecting the word generated by the topic, continueson by generating a corroborator and a sense for theoriginal word:For each document d ?
{1 .
.
.D}:1.
Select a topic distribution ?d ?
Dir(?)2.
For each word in the document n ?
{1 .
.
.
N}:(a) Select a topic zn ?
Mult(1, ?d)(b) Select a word from that topic wn ?
Mult(1, ?z)(c) Select a ?corroborator?
cn also proportional to howimportant it is to the topic and its similarity to w(d) Now, select a synset sn for that word based on adistribution p(sn|wn, cn, zn)The conditional dependencies for generating asynset are shown in Figure 2.
Our goal, like Mc-Carthy et al?s, is to determine the most likely sensefor each word.
This amounts to posterior inference,which we address by marginalizing over the unob-served variables (the topics and the corroborators),where p(wsi) =p(s|w) =??
?z?cp(s|w, c, z)p(c|z, w)p(z|w, ?).
(3)In order to fully specify this, we must determine thedistribution from which the corroborator is drawnand the distribution from which the synset is drawn.Ideally, we would want a distribution that for asingle topic would be identical to McCarthy et al?s278KDN?
z?w csFigure 2: Our generative model assumes that doc-uments are divided into topics and that these topicsgenerate both the observed word and a ?corrobora-tor,?
a term similar in usage to the word.
Next, asense that minimizes the semantic distance betweenthe corroborator and the word is generated.method but would, as more topics are added, favorcorroborators in the same topic as the number of top-ics increases.
In McCarthy et al?s method, the prob-ability of the corroborator given a word w is pro-portional to the Lin similarity ?
(w, c) between theword and the corroborator.
Here, the probability ofa corroborator c isp(c|z, w) ??z,c?0c?
(w, c), (4)where ?z,c is the multinomial probability of word cin the zth topic, and ?0c is the multinomial probabil-ity of the word with a single topic (i.e.
backgroundword probability).Before, the corroborator was weighted simplybased on its syntactic similarity to the word w, nowwe also weight that contribution by how important(or unimportant) that word is to the topic that w hasbeen assigned to.
This has the effect of increasingthe probability of words pertinent to the topic thatalso have high syntactic similarity.
Thus, wheneverthe syntactic similarity captures polysemous usage,we hope to be able to separate the different usages.Note, however, that since for a single topic the ?term cancels out and the procedure is equivalent toMcCarthy et alWe adapt the semantic similarity in much thesame way to make it topic specific.
Because theJiang-Conrath similarity measure uses an underly-ing term frequency to generate a similarity score, weuse the topic term frequency instead of the undividedterm frequency.
Thus, the probability of a sense isproportional to semantic similarity between it andthe closest sense among the senses of a corroboratorwith respect to this topic-specific similarity (c.f.
theglobal similarity in Equation 2).
The probability ofselecting a synset s given the corroborator c and atopic z then becomesp(s|w, c, z) ?
maxs?
?S(c)?z(s, s?).
(5)This new dependence on the topic happens be-cause we recompute the information content used byJiang-Conrath with the distribution over words im-plied by each topic.
We then use the similarity im-plied by that similarity for ?z .
Following the lead ofMcCarthy, for notational ease, this becomes definedas wnss in Equation 8.1.4 Choosing a SynsetThe problem of choosing a synset then is reduced tofinding the synset with the highest probability underthis model.
The model is also designed so that thetask of learning the assignment of topics to wordsand documents is not affected by this new machin-ery for corroborators and senses that we?ve addedonto the model.
Thus, we can use the variational in-ference method described in (Blei et al, 2003) as afoundation for the problem of synset inference.Taking p(z|w) as a given (i.e.
determined by run-ning LDA on the corpus), the probability for a synsets given a word w then becomesp(s|w, z) =?z?cp(s|w, c, z)p(c|z)p(z|w), (6)whose terms have been described in the previoussection.
With all of the normalization terms, we nowsee that p(s|w, z) becomes?z?c?z,c?0c?
(w, c)?c??z,c?0c?
(w, c?
)wnss(s, c, z)?s?
?Sw wnss(s?, c, z).
(7)and wnss(s, c, z) now becomes, for the zth topic,maxa?S(c)?z(a, s).
(8)Thus, we?ve now assigned a probability to each ofthe possible senses a word can take in a document.2791.5 IntuitionFor example, consider the word ?fly,?
which has twoother words that have high syntactic similarity (inour formulation, ?)
with the terms ?fly ball?
and ?in-sect.?
Both of these words would, given the seman-tic similarity provided by WORDNET, point to a sin-gle sense of ?fly;?
one of them would give a highervalue, however, and thus all senses of the word ?fly?would be assigned that sense.
By separately weight-ing these words by the topic frequencies, we wouldhope to choose the sports sense in topics that havea higher probability of the terms like ?foul ball,?
?pop fly,?
and ?grounder?
and the other sense in thecontexts where insect has a higher probability in thetopic.2 EvaluationsThis section describes three experiments to deter-mine the effectiveness of this unsupervised system.The first was used to help understand the system,and the second two were part of the SemEval 2007competition.2.1 SemCorAs an initial evaluation, we learned LDA topics onthe British National corpus with paragraphs as theunderlying ?document?
(this allowed for a more uni-form document length).
These documents were thenused to infer topic probabilities for each of the wordsin SemCor (Miller et al, 1993), and the model de-scribed in the previous section was run to determinethe most likely synset.
The results of this procedureare shown in Table 1.
Accuracy is determined as thepercentage of words for which the most likely sensewas the one tagged in the corpus.While the method does roughly recreate Mc-Carthy et al?s result for a single topic, it only of-fers a one percent improvement over McCarthy etal.
on five topics and then falls below McCarthy forall greater numbers of topics tried.
Thus, for allsubsequent experiments we used a five topic modeltrained on the BNC.2.2 SemEval-2007 Task 1: CLIRUsing IR metrics, this disambiguation scheme wasevaluated against another competing platform andan algorithm provided by the Task 1 (Agirre et al,Topics All Nouns1 .393 .4675 .397 .47825 .387 .456200 .359 .420Table 1: Accuracy on disambiguating words in Sem-CorTask PUTOPTopic Expansion 0.30Document Expansion 0.15English Translation 0.17SensEval 2 0.39SensEval 3 0.33Table 2: Performance results on Task 12007) organizers.
Our system had the best results ofany expansion scheme considered (0.30) , althoughnone of the expansion schemes did better than us-ing no expansion (0.36).
Although our techniquealso yielded a better score than the other competingplatform for cross-language queries (0.17), it did notsurpass the first sense-heuristic (0.26), but this is notsurprising given that our algorithm does not assumethe existence of such information.
For an overviewof Task 1 results, see Table 2.2.3 SemEval-2007 Task 17: All-WordsTask 17 (Pradhan et al, 2007) asked participantsto submit results as probability distributions oversenses.
Because this is also the output of this algo-rithm, we submitted the probabilities to the contestbefore realizing that the distributions are very closeto uniform over all senses and thus yielded a pre-cision of 0.12, very close to the random baseline.Placing a point distribution on the argmax with ouroriginal submission to the task, however, (consistentwith our methodology for evaluation on SemCor),gives a precision of 0.39.3 ConclusionWhile the small improvement over the single topicsuggests that topic techniques might have tractionin determining the best sense, the addition is not ap-preciable.
In a way the failure of the technique is en-280couraging in that it affirms the original methodologyof McCarthy et al in finding a single predominantsense for each word.
While the syntactic similaritymeasure indeed usually offers high values of similar-ity for words related to a single sense of a word, thesimilarity for words related to other senses, whichwe had hoped to strengthen by using topic features,are on par with words observed because of noise.Thus, for a word like ?bank,?
words like?firm,?
?commercial bank,?
?company,?
and ?finan-cial institution?
are the closest in terms of the syn-tactic similarity, and this allows the financial sensesto be selected without any difficulty.
Even if we hadcorroborating words for another sense in some topic,these words are absent from the syntactically simi-lar words.
If we want the meaning similar to that of?riverbank,?
the word with the most similar mean-ing, ?side,?
had a syntactic similarity on par with theunrelated words ?individual?
and ?group.?
Thus, in-terpretations other than the dominant sense as deter-mined by the baseline method of McCarthy et al arehard to find.Because one topic is equivalent to McCarthy etal.
?s method, this means that we do no worse ondisambiguation.
However, contrary to our hope, in-creasing the number of topics does not lead to sig-nificantly better sense predictions.
This work has notinvestigated using a topic-based procedure for deter-mining the syntactic similarity, but we feel that thisextension could provide real improvement to the un-supervised techniques that can make use of the co-pious amounts of available unlabeled data.ReferencesSteven Abney.
2004.
Understanding the yarowsky algo-rithm.
Comput.
Linguist., 30(3):365?395.Eneko Agirre, Oier Lopez de Lacalle, Arantxa Otegi,German Rigau, and Piek Vossen.
2007.
The Senseval-2007 Task 1: Evaluating WSD on cross-language in-formation retrieval.
In Proceedings of SemEval-2007.Association for Computational Linguistics.David Blei, Andrew Ng, and Michael Jordan.
2003.
La-tent Dirichlet alocation.
Journal of Machine LearningResearch, 3:993?1022, January.Jordan L. Boyd-Graber, David M. Blei, and Jerry Zhu.2007.
Probabalistic walks in semantic hierarchies as atopic model for WSD.
In Proc.
EMNLP 2007.Jay J. Jiang and David W. Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexical taxon-omy.
In Proceedings on International Conference onResearch in Computational Linguistics, Taiwan.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proc.
15th International Conf.
on Ma-chine Learning, pages 296?304.
Morgan Kaufmann,San Francisco, CA.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word senses inuntagged text.
In In 42nd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 280?287.George Miller, Claudia Leacock, Randee Tengi, and RossBunker.
1993.
A semantic concordance.
In 3rdDARPA Workshop on Human Language Technology,pages 303?308.Sameer Pradhan, Martha Palmer, and Edward Loper.2007.
The Senseval-2007 Task 17: English fine-grained all-words.
In Proceedings of SemEval-2007.Association for Computational Linguistics.281
