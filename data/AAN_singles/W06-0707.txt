Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 48?55,Sydney, July 2006. c?2006 Association for Computational LinguisticsDUC 2005: Evaluation of Question-Focused Summarization SystemsHoa Trang DangInformation Access DivisionNational Institute of Standards and Technology100 Bureau DriveGaithersburg, MD, 20899hoa.dang@nist.govAbstractThe Document Understanding Conference(DUC) 2005 evaluation had a single user-oriented, question-focused summarizationtask, which was to synthesize from a setof 25-50 documents a well-organized, flu-ent answer to a complex question.
Theevaluation shows that the best summariza-tion systems have difficulty extracting rel-evant sentences in response to complexquestions (as opposed to representativesentences that might be appropriate to ageneric summary).
The relatively gener-ous allowance of 250 words for each an-swer also reveals how difficult it is forcurrent summarization systems to producefluent text from multiple documents.1 IntroductionThe Document Understanding Conference (DUC)is a series of evaluations of automatic text sum-marization systems.
It is organized by the Na-tional Institute of Standards of Technology withthe goals of furthering progress in automatic sum-marization and enabling researchers to participatein large-scale experiments.In DUC 2001-2004 a growing number ofresearch groups participated in the evaluationof generic and focused summaries of Englishnewspaper and newswire data.
Various targetsizes were used (10-400 words) and both single-document summaries and summaries of multipledocuments were evaluated (around 10 documentsper set).
Summaries were manually judged forboth content and readability.
To evaluate content,each peer (human or automatic) summary wascompared against a single model (human) sum-mary using SEE (http://www.isi.edu/ cyl/SEE/)to estimate the percentage of information in themodel that was covered in the peer.
Addition-ally, automatic evaluation of content coverage us-ing ROUGE (Lin, 2004) was explored in 2004.Human summaries vary in both writing styleand content.
For example, (Harman and Over,2004) noted that a human summary can vary in itslevel of granularity, whether the summary has avery high-level analysis or primarily contains de-tails.
They analyzed the effects of human vari-aion in the DUC evaluations and concluded thatdespite large variation in model summaries, therankings of the systems when compared against asingle model for each document set remained sta-ble when averaged over a large number of docu-ment sets and human assessors.
The use of a largetest set to smooth over natural human variation isnot a new technique; it is the approach that hasbeen taken in TREC (Text Retrieval Conference)for many years (Voorhees and Buckley, 2002).While evaluators can achieve stable overall sys-tem rankings by averaging scores over a largenumber of document sets, system builders are stillfaced with the challenge of producing a summaryfor a given document set that is most likely tosatisfy any human user (since they cannot knowahead of time which human will be using or judg-ing the summary).
Thus, system developers desirean evaluation methodology that takes into accounthuman variation in summaries for any given doc-ument set.DUC 2005 marked a major change in direc-tion from previous years.
The road mapping com-mittee had strongly recommended that new tasksbe undertaken that were strongly tied to a clearuser application.
At the same time, the programcommittee wanted to work on new evaluationmethodologies and metrics that would take into48account variation of content in human-authoredsummaries.Therefore, DUC 2005 had a single user-orientedsystem task that allowed the community to putsome time and effort into helping with a new eval-uation framework.
The system task modeled real-world complex question answering (Amigo et al,2004).
Systems were to synthesize from a set of25-50 documents a brief, well-organized, fluentanswer to a need for information that could notbe met by just stating a name, date, quantity, etc.Summaries were evaluated for both content andreadability.The task design attempted to constrain twoparameters that could produce summaries withwidely different content: focus and granularity.Having a question to focus the summary was in-tended to improve agreement in content betweenthe model summaries.
Additionally, the assessorwho developed each topic specified the desiredgranularity (level of generalization) of the sum-mary.
Granularity was a way to express one typeof user preference; one user might want a generalbackground or overview summary, while anotheruser might want specific details that would allowhim to answer questions about specific events orsituations.Because it is both impossible and unnatural toeliminate all human variation, our assessors cre-ated as many manual summaries as feasible foreach topic, to provide examples of the range ofnormal human variability in the summarizationtask.
These multiple models would provide morerepresentative training data to system developers,while enabling additional experiments to investi-gate the effect of human variability on the evalua-tion of summarization systems.As in past DUCs, assessors manually evalu-ated each summary for readability using a setof linguistic quality questions.
Summary con-tent was manually evaluated using the pseudo-extrinsic measure of responsiveness, which doesnot attempt pairwise comparison of peers againsta model summary but gives a coarse ranking ofall the summaries based on responsiveness ofthe summary to the topic.
In parallel, ISI andColumbia University led the summarization re-search community in two exploratory efforts at in-trinsic evaluation of summary content; these eval-uations compared peer summaries against multiplereference summaries, using Basic Elements at ISIand Pyramids at Columbia University.This paper describes the DUC 2005 task and theresults of our evaluations of summary content andreadability.
(Hovy et al, 2005) and (Passonneauet al, 2005) provide additional details and resultsof the evaluations of summary content using BasicElements and Pyramids.2 Task DescriptionThe DUC 2005 task was a complex question-focused summarization task that required summa-rizers to piece together information from multipledocuments to answer a question or set of questionsas posed in a topic.Assessors developed a total of 50 topics to beused as test data.
For each topic, the assessor se-lected 25-50 related documents from the Los An-geles Times and Financial Times of London andformulated a topic statement, which was a requestfor information that could be answered using theselected documents.
The topic statement could bein the form of a question or set of related questionsand could include background information that theassessor thought would help clarify his/her infor-mation need.The assessor also indicated the ?granularity?
ofthe desired response for each topic.
That is, theyindicated whether they wanted the answer to theirquestion(s) to name specific events, people, places,etc., or whether they wanted a general, high-levelanswer.
Only one value of granularity was givenfor each topic, since the goal was not to measurethe effect of different granularities on system per-formance for a given topic, but to provide addi-tional information about the user?s preferences toboth human and automatic summarizers.An example DUC topic follows:num: D345title: American Tobacco Companies Over-seasnarr: In the early 1990?s, American to-bacco companies tried to expand their busi-ness overseas.
What did these companies door try to do and where?
How did their parentcompanies fare?granularity: specificThe summarization task was the same for bothhuman and automatic summarizers: Given a DUCtopic with granularity specification and a set ofdocuments relevant to the topic, the summariza-tion task was to create from the documents a brief,49well-organized, fluent summary that answers theneed for information expressed in the topic, atthe specified level of granularity.
The summarycould be no longer than 250 words (whitespace-delimited tokens).
Summaries over the size limitwere truncated, and no bonus was given for cre-ating a shorter summary.
No specific format-ting other than linear was allowed.
The summaryshould include (in some form or other) all theinformation in the documents that contributed tomeeting the information need.Ten assessors produced a total of 9 human sum-maries for each of 20 topics, and 4 human sum-maries for each of the remaining 30 topics.
Thesummarization task was a relatively difficult task,requiring about 5 hours to manually create eachsummary.
Thus, there would be a real benefit tousers if the task could be performed automatically.3 ParticipantsThere was much interest in the longer, question-focused summaries required in the DUC 2005task.
31 participants submitted runs to the evalua-tion; they are identified by numeric Run IDs (2-32)in the remainder of this paper.
We also developeda simple baseline system that returned the first 250words of the most recent document for each topic(Run ID = 1).
In addition to the automatic peers,there were 10 human peers, assigned alphabeticRun IDs, A-J.Most system developers treated the summariza-tion task as a passage retrieval task.
Sentenceswere ranked according to relevance to the topic.The most relevant sentences were then selected forinclusion in the summary while minimizing redun-dancy within the summary, up to the maximum250-word allowance.
A significant minority ofsystems first decomposed the topic narrative intoa set of simpler questions, and then extracted sen-tences to answer each subquestion.
Systems dif-fered in the approach taken to compute relevanceand redundancy, using similarity metrics rangingfrom simple term frequency to semantic graphmatching.
In order to include more relevant infor-mation in the summary, systems attempted within-sentence compression by removing phrases suchas parentheticals and relative clauses.Many systems simply ignored the granularityspecification.
The systems that addressed gran-ularity did so by preferring to extract sentencesthat contained proper names for topics with a ?spe-cific?
granularity but not for topics with ?general?granularity.Cross-sentence dependencies had to be handled,including anaphora.
Strategies for dealing withpronouns that occurred in relevant sentences in-cluded co-reference resolution, including the pre-vious sentence for additional context, or simplyexcluding all sentences containing any pronouns.Most systems made no attempt to reword the ex-tracted sentences to improve the readability of thefinal summary.
Although some systems groupedrelated sentences together to improve cohesion,the most common heuristic to improve readabil-ity was simply to order the extracted sentences bydocument date and position in the document.
Sys-tem 12 achieved high readability scores by choos-ing a single representative document and extract-ing sentences in the order of appearance in thatdocument.
This approach is similar to the base-line summarizer and produces summaries that aremore fluent than those constructed from multipledocuments.4 Evaluation ResultsSummaries were manually evaluated by 10 asses-sors.
All summaries for a given topic were judgedby a single assessor (who was usually the same asthe topic developer).
In all cases, the assessor wasone of the summarizers for the topic.
All sum-maries for the topic (including the one written bythe assessor) were anonymously presented to theassessor, in a random order, and the ssessor judgedeach summary for readability and responsivenessto the topic, giving separate scores for responsive-ness and each of 5 linguistic qualities.
This al-lowed participants who could not work on opti-mizing all 6 manual scores, to focus on only theelements that they were interested in or had the re-sources to address.No single score was reported that reflected acombination of readability and content.
In pre-vious years, responsiveness considered both thecontent and readability of the summary.
While ittracked SEE coverage, responsiveness could notbe seen as a direct measure of content due to pos-sible effects of readability on the score.
Becausewe needed an inexpensive manual measure of cov-erage, we revised the definition of responsivenessin 2005 so that it considered only the informationcontent and not the readability of the summary, tothe extent possible.504.1 Evaluation of ReadabilityThe readability of the summaries was assessed us-ing five linguistic quality questions which mea-sured qualities of the summary that do not in-volve comparison with a reference summary orDUC topic.
The linguistic qualities measuredwere Grammaticality, Non-redundancy, Referen-tial clarity, Focus, and Structure and coherence.Q1: Grammaticality The summary shouldhave no datelines, system-internal formatting, cap-italization errors or obviously ungrammatical sen-tences (e.g., fragments, missing components) thatmake the text difficult to read.Q2: Non-redundancy There should be no un-necessary repetition in the summary.
Unnecessaryrepetition might take the form of whole sentencesthat are repeated, or repeated facts, or the repeateduse of a noun or noun phrase (e.g., ?Bill Clinton?
)when a pronoun (?he?)
would suffice.Q3: Referential clarity It should be easy toidentify who or what the pronouns and nounphrases in the summary are referring to.
If a per-son or other entity is mentioned, it should be clearwhat their role in the story is.
So, a referencewould be unclear if an entity is referenced but itsidentity or relation to the story remains unclear.Q4: Focus The summary should have a focus;sentences should only contain information that isrelated to the rest of the summary.Q5: Structure and Coherence The summaryshould be well-structured and well-organized.
Thesummary should not just be a heap of related infor-mation, but should build from sentence to sentenceto a coherent body of information about a topic.Each linguistic quality question was assessed ona five-point scale:1.
Very Poor2.
Poor3.
Barely Acceptable4.
Good5.
Very GoodTable 1 shows the distribution of the scoresacross all the summaries, broken down by the typeof summarizer (Human, Baseline, or Participants).All summarizers generally performed well on thefirst two linguistic qualities.
The high scores onnon-redundancy show that most participants haveHumansQ1Frequency1 2 3 4 5050100150200250BaselineQ1Frequency1 2 3 4 505101520ParticipantsQ1Frequency1 2 3 4 50100300500Q1: GrammaticalityHumansQ2Frequency1 2 3 4 5050100150200250BaselineQ2Frequency1 2 3 4 5010203040ParticipantsQ2Frequency1 2 3 4 502004006008001000Q2: Non-redundancyHumansQ3Frequency1 2 3 4 5050100150200250300BaselineQ3Frequency1 2 3 4 50102030ParticipantsQ3Frequency1 2 3 4 50100200300400Q3: Referential ClarityHumansQ4Frequency1 2 3 4 5050100150200250BaselineQ4Frequency1 2 3 4 50102030ParticipantsQ4Frequency1 2 3 4 50100200300400500Q4: FocusHumansQ5Frequency1 2 3 4 5050100150200250BaselineQ5Frequency1 2 3 4 505101520ParticipantsQ5Frequency1 2 3 4 50100300500Q5: Structure and CoherenceTable 1: Frequency of scores for each linguisticquality, broken down by source of summary (Hu-mans, Baseline, Participants).51successfully achieved this capability.
Humans andthe baseline system also scored well on the last3 linguistic qualities.
The multi-document sum-marization systems submitted by participants, onthe other hand, still struggle with referential clar-ity and focus, and perform very poorly on structureand coherence.4.1.1 Comparison by systemFor each linguistic quality question, we per-formed a multiple comparison test between thescores of all peers using Tukey?s honestly signif-icant difference criterion.
A multiple comparisontest between all human and automatic peers wasperformed using the Kruskall-Wallis test, to seehow the individual automatic peers performed rel-ative to human peers.
For grammaticality, the besthuman summarizer is significantly better than 28of the 32 systems; the worst human summarizeris better than 8 systems.
For non-redundancy, thetwo best humans are significantly better than 6 sys-tems, and the two worst humans are not signifi-cantly different from any system.
For referentialclarity, all humans are significantly better than allbut 2 automatic peers (baseline and System 12).For focus, the best human is significantly betterthan all automatic peers except the baseline; allother humans are significantly better than all au-tomatic peers except the baseline and System 12.For structure and coherence, the two best humansare significantly better than 31 systems (all auto-matic peers except the baseline); all humans arebetter than 30 of the automatic peers (all automaticpeers except baseline and System 12).4.2 Evaluation of ContentWe performed manual pseudo-extrinsic evaluationof peer summaries in the form of assessment ofresponsiveness.
Responsiveness is different fromSEE coverage in that it does not compare a peersummary against a single reference; however, re-sponsiveness tracked SEE coverage in DUC 2003and 2004, and was used to provide a coarse-grained measure of content in 2005.
We also com-puted ROUGE scores as was done in DUC 2004.4.2.1 ResponsivenessAssessors assigned a raw responsiveness scoreto each summary.
The score provides a coarseranking of the summaries for each topic, accordingto the amount of information in the summary thathelps to satisfy the information need expressed inthe topic statement, at the level of granularity re-quested in the user profile.
The score was an inte-ger between 1 and 5, with 1 being least respon-sive and 5 being most responsive.
For a giventopic, some summary was required to receive eachof the five possible scores, but no distribution wasspecified for how many summaries had to receiveeach score.
The number of human summariesscored per topic also varied.
Therefore, raw re-sponsiveness scores should not be directly addedand compared across topics.
Assigning respon-siveness scores can be seen as a clustering task inwhich peers are partitioned into exactly 5 clusters,where members of a cluster are more similar toeach other in quality.RunID10 A5 A4 A B15 A B C29 A B C D11 A B C D17 A B C D8 A B C D7 A B C D E14 A B C D E6 A B C D E28 A B C D E F21 A B C D E F19 A B C D E F24 A B C D E F9 A B C D E F16 A B C D E F32 A B C D E F12 A B C D E F25 A B C D E F18 A B C D E F27 A B C D E F20 A B C D E F3 A B C D E F2 B C D E F13 C D E F30 D E F22 E F1 E F26 F31 F G23 GTable 2: Multiple comparison of systems based onFriedman?s test on responsivenessFor each topic, we computed the scaled respon-siveness score for each summary, such that thesum of the scaled responsiveness score is propor-tional to the number of summaries for the topic.The scaled responsiveness is the rank of the sum-mary based on the raw responsiveness score.
Wecomputed the average scaled responsiveness scoreof each summarizer across all topics.
Since the52number of human summaries varied across topics,we also computed the average scaled responsive-ness score of only the automatic summaries (ig-noring the human summaries in scaling respon-siveness).Table 2 shows the results of a multiple com-parison of scaled responsiveness of the automaticpeers using Tukey?s honestly significant criterionand Friedman?s test, with the best peers on top;peers not sharing a common letter are significantlydifferent at the 95.5% confidence level.
None ofthe automatic peers performed significantly bet-ter than the majority of the remaining peers, andonly eight of the automatic peers performed signif-icantly better than the simple baseline.
In multiplecomparison of all peers using the Kruskal-Wallistest, all human peers were significantly better thanall the automatic peers.4.2.2 ROUGEWe computed two ROUGE scores: ROUGE-2and ROUGE-SU4 recall, both with stemming andimplementing jackknifing for each [peer, topic]pair so that human and automatic peers could becompared.
Since the number of ROUGE evalu-ations per topic varied depending on the numberof reference summaries, we computed a macro-average of each score for each peer, where themacro-average score is the mean over all topics ofthe mean per-topic score for the peer.Unlike responsiveness and linguistic qualityscores, which are ordinal data and are best suitedfor non-parametric analyses, ROUGE scores, canbe measured on an interval scale and are suit-able for parametric analysis.
Analysis of varianceshowed significant effects from peer and topic(p = 0 for each factor) for both ROUGE-2 andROUGE-SU4 recall.
To see which peers weredifferent, a multiple comparison of populationmarginal means (PMM) was performed for eachtype of ROUGE score.
The population marginalmeans remove any effect of an unbalanced design(since not all human peers created summaries forall topics) by fixing the values of the ?peer?
factor,and averaging out the effects of the ?topic?
factoras if each factor combination occurred the samenumber of times.Table 3 shows multiple comparison of all peersbased on ANOVA of ROUGE-2 recall (ROUGE-SU4 shows similar results).
ROUGE-2 andROUGE-SU4 both distinguish human peers fromautomatic ones.
The difference in the ROUGE-25 10 15 20 25 30 35101520253035Average scaled responsiveness (primary)Averagescaledresponsiveness(secondary)Figure 1: Primary vs. secondary average scaledresponsivenessscore of the best system and worst human is notconsidered significant (possibly due to the veryconservative nature of the multiple comparisontest) but is still relatively large.
On the otherhand, ANOVA of ROUGE-2 found more signifi-cant differences between the automatic peers thandid Friedman?s test of responsiveness.4.3 CorrelationA metric must produce stable rankings of systemsin the face of human variation.
Intrinsic measureslike ROUGE rely on multiple model summaries totake into account human variation (although Pyra-mids add another level of human variation in themanual pyramid and peer annotation).
For a met-ric like responsiveness, which does not depend oncomparison of peer summaries against a model orset of model summaries, it is appropriate to con-sider the stability of the measure across differentassessors.A secondary assessment was done on respon-siveness for the 20 topics that had 9 summarieseach.
The secondary assessor had written a sum-mary for the topic but was generally not the sameperson who developed the topic.
As seen in Figure1, average scaled responsiveness scores from thetwo sets of assessments (averaged over the 20 top-ics) track each other very well.
The human sum-maries are clustered on the upper right side of thegraph, while the automatic summaries form a sec-ond cluster on the lower left side.The actual responsiveness scores for each sys-tem and each topic do vary between assessors, butthis variation in human judgment is smoothed outby averaging over multiple topics.
Table 4 showsthat the correlation between the primary and sec-53RunID PMM of R2C 0.1172 AA 0.1156 A BI 0.1023 A B CB 0.1014 A B CJ 0.1012 A B CE 0.1009 A B CD 0.0986 A B CG 0.0970 B CF 0.0947 CH 0.0897 C D15 0.0725 D E17 0.0717 E10 0.0698 E F8 0.0696 E F4 0.0686 E F G5 0.0675 E F G11 0.0643 E F G H14 0.0635 E F G H I16 0.0633 E F G H I19 0.0632 E F G H I7 0.0628 E F G H I J9 0.0625 E F G H I J29 0.0609 E F G H I J K25 0.0609 E F G H I J K6 0.0609 E F G H I J K24 0.0597 E F G H I J K28 0.0594 E F G H I J K3 0.0594 E F G H I J K21 0.0573 E F G H I J K12 0.0563 F G H I J K18 0.0553 F G H I J K L26 0.0547 F G H I J K L27 0.0546 F G H I J K L32 0.0534 G H I J K L20 0.0515 H I J K L13 0.0497 H I J K L30 0.0496 H I J K L31 0.0487 I J K L2 0.0478 J K L22 0.0462 K L1 0.0403 L M23 0.0256 MTable 3: Multiple comparison of all peers based on ANOVA of ROUGE-2 recall54Spearman PearsonAll peers 0.900 0.976 [0.960, 1.000]Auto peers 0.775 0.822 [0.695, 1.000]Table 4: Correlation between primary and sec-ondary average scaled responsiveness (20 topics),with 95% confidence intervals for Pearson?s r.ondary average scaled responsiveness scores is re-spectable despite the low number of topics.
Thecorrelation suggests that responsiveness wouldgive a stable ranking of the systems when aver-aged over the entire set of 50 topics.Table 5 shows that there is high correlationbetween macro-average ROUGE scores (intrin-sic measures) and average scaled responsiveness(a pseudo-extrinisic measure).
The correlation ishigh even when the human summaries are ignored.Metric Spearman PearsonROUGE-2 (all) 0.951 0.972 [0.953, 1.000]ROUGE-SU4 (all) 0.942 0.958 [0.930, 1.000]ROUGE-2 (auto) 0.901 0.928 [0.872, 1.000]ROUGE-SU4 (auto) 0.872 0.919 [0.855, 1.000]Table 5: Correlation between average scaled re-sponsiveness and macro-average ROUGE recallover all topics and either all peers or only auto-matic peers.5 ConclusionThe DUC 2005 task was to summarize the answerto a complex question, as found in a set of docu-ments.
The evaluation showed that only the topsystems are able to extract sentences whose in-formation content is more responsive to the ques-tion than a simple baseline.
Additionally, systemsrequire much additional work to produce coher-ent, well-structured text, which is apparent in thelonger summary sizes of DUC 2005.
On the otherhand, systems do well on non-redundancy, sincetext summarization has historically been formu-lated as a text compression task.
Since DUC 2005is the first time question-focused summarizationhas been evaluated on a large-scale, we have re-peated the task in 2006, with some modifications.We eliminated the ?granularity?
specification inDUC 2006.
Assessors had appreciated the theorybehind the granularity specification, but found thatthe size limit for the summaries was a much big-ger factor in determining what information to in-clude; some ?specific?
summaries ended up beingvery general given the large amount of informa-tion and limited space allowed.
From a humanperspective, the actual granularity of the resultingsummary mostly fell out naturally from the topicquestion and the content that was available in thesource documents.The definition of responsiveness scores wasmeant to yield a coarse ranking of the peer sum-maries into 5 ordered clusters.
However, asses-sors found it difficult to form these 5 clusters be-cause of the large number (36+) of summaries thatneeded to be compared with one another, and theimpression that many sets of human and automaticsummaries could not be separated into as manyas 5 groups.
We therefore changed the scoringof responsiveness in 2006 so that it is based onthe same scale as the linguistic quality questions;this may reduce the discriminative power of theresponsiveness measure but should produce scoresthat more accurately reflect the true differences be-tween summaries.ReferencesEnrique Amigo, Julio Gonzalo, Victor Peinado,Anselmo Penas, and Felisa Verdejo.
2004.
An em-pirical study of information synthesis tasks.
In Pro-ceedings of the 42nd Annual Meeting of the Associa-tion for Computational Linguistics, pages 207?214,Barcelona, Spain.Donna Harman and Paul Over.
2004.
The effects ofhuman variation in duc summarization evaluation.In Proceedings of the ACL-04 Workshop: Text Sum-marization Branches Out, pages 10?17, Barcelona,Spain.Eduard Hovy, Chin-Yew Lin, and Liang Zhou.
2005.Evaluating duc 2005 using basic elements.
In Pro-ceedings of the Fifth Document Understanding Con-ference (DUC), Vancouver, Canada.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Proceedings of theACL-04 Workshop: Text Summarization BranchesOut, pages 74?81, Barcelona, Spain.Rebecca J. Passonneau, Ani Nenkova, Kathleen McK-eown, and Sergey Sigelman.
2005.
Applying thepyramid method in duc 2005.
In Proceedings of theFifth Document Understanding Conference (DUC),Vancouver, Canada.Ellen M. Voorhees and Chris Buckley.
2002.
Theeffect of topic set size on retrieval experiment er-ror.
In Proceedings of the 25th Annual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval, pages 316?323, Tam-pere, Finland, August.55
