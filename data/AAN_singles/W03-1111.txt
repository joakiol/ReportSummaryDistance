Very Low-Dimensional Latent Semantic Indexing for Local Query RegionsYinghui Xu Kyoji UmemuraToyohashi Unversity of Technology Dept.
of Information and Computer Sciences1-1, Hibarigaoka, Toyohashi, Aichi,Japanxyh@ss.ics.tut.ac.jp umemura@tutics.tut.ac.jpAbstractIn this paper, we focus on performingLSI on very low SVD dimensions.
Theresults show that there is a nearly linearsurface in the local query region.
Usinglow-dimensional LSI on local query re-gion we can capture such a linear surface,obtain much better performance thanVSM and come comparably to globalLSI.
The surprisingly small requirementsof the SVD dimension resolve the com-putation restrictions.
Moreover, on thecondition that several relevant sampledocuments are available, application oflow-dimensional LSI to these documentsyielded comparable IR performance tolocal RF but in a different manner.1 IntroductionThe increasing size of searchable text collectionposes a great challenge to performing the informa-tion retrieval (IR) task.
Latent Semantic Index-ing (LSI) is an enhancement of the familiar VectorModel of IR.
It satisfies the IR task through discov-ering corpus-wide word relationship based on co-occurrence analysis of a whole collection.
LSI hasbeen successfully applied to various document col-lections and has achieved favorable results, some-times outperforming VSM (Dumais, 1996).
How-ever, the principal challenges to applying LSI tolarge data collections are the cost of computing andstoring SVD.Local analysis of the information in a set of top-ranked documents for the query is one promisingway to solve the computationally demanding IR taskfor a large collection.
To solve the computationalcomplexity of LSI, David Hull introduced one inter-esting method, local LSI, for routing problems(Hull,1994).
The basic idea is: apply the SVD to a set ofdocuments known to be relevant to the query; thenall the documents in the collection can be folded intothe reduced space of those relevant documents.
Byconcentrating on the local space around the query re-sults, we may be able to compute using flexible andefficient LSI algorithms.In this paper we put much emphasis on localdimensionality analysis of the local query regionsfilled with relevant documents.
In ideal experimen-tal cases, local LSI involves only the documentsknown to be relevant to the query.
To our surprise,in most of our experiments, local LSI obtains its bestIR performance using just one or two SVD dimen-sions.
These interesting results moved us to try per-forming local LSI with one or two SVD dimensionson the top return sets of VSM in ad-hoc IR experi-ments.
We found that this worked surprisingly well.In a practical setting, local LSI may be regarded as avariation of pseudo relevance feedback (RF).
There-fore, the comparative results with local RF are pro-vided in this paper as well.
The experiments showthat local LSI with one or two SVD dimensions cancontribute to expanding the query information in amanner different from traditional local RF.This paper is organized as follows: Section 2 re-views existing related techniques.
Section 3 de-scribes the implementation architecture of the ex-periments and gives the experiment results.
Section4 explains the result and points out characteristic ofthe local LSI.
Section 5 draws the conclusions.2 Related works2.1 Latent Semantic IndexingLatent semantic indexing (Berry et al, 1999) is onekind of vector-based query-expansion methods thatuse neither terms nor documents as the orthogo-nal basis of a semantic space.
Instead, it computesthe most significant orthogonal dimensions in theterm-document matrix of the corpus, via SVD, andprojects documents into the low rank subspace thusfound.
LSI then computes semantic similarity basedon the proximity among projected vectors.LSI uses SVD to factor the term-documenttraining matrix A into three factors: A =U?V T = Udiag(?1, ?2, ?
?
?
, ?n)V T WhereU = (u1, u2, ?
?
?
, um) ?
<m?m and V =(v1, v2, ?
?
?
, vn) ?
<n?n are unitary matrices (i.e.UTU = I, V TV = I), whose columns are theleft and the right singular vectors of A respectively,?
?
<m?n is a diagonal matrix whose diagonal ele-ments are non-negative and arranged in descendingorder (?1 ?
?2 ?
?
?
?
?
?k), and p = min(m,n).The values ?1, ?2, ?
?
?
, ?p are known as the singularvalues of A, and are the square roots of the eigen-values of AAT and ATA.Suppose the rank of A isr, then r ?
p and only ?1 ?
?2 ?
?
?
?
?
?r arepositive, while the remaining (p-r), if r<p, singularvalues are zero.
In LSI retrieval, researchers are onlyconcerned with the first r singular values of A. LSIuses the structure from SVD to obtain the reduced-dimension form of the training matrix A as its ?la-tent semantic space.?
Notation for k ?
r, defines thereduced-dimension form of A to be A = U?V T =Udiag(?1, ?2, ?
?
?
, ?k, 0, ?
?
?
, 0)V T .
That is, Ak isobtained by discarding the r-k least significant sin-gular values and the corresponding left and rightsingular vectors of A (since they are now mul-tiplied by zeros).
Then, the first k columns ofU that correspond to the k largest singular valuesof A together constitute the projection matrix forLSI: Sim(~d, ~q) = (ATk ~d) ?
(ATk ~q).
Analogous toVSM, the vector representation of a document isthe weighted sum of the vector representation ofits constituent terms.
For document vector di andquery vector qi, ATk ~d and ATK~q are now the LSI vec-tor representations of that document and query, re-spectively, in the reduced-dimension vector space.This process is known as ?folding in?
documents (orqueries) into the training space.
Actually, LSI as-sumes that the semantic associations among termscan be found through this one-step analysis of theirstatistical usage in the collection, and they are im-plicitly stored in the singular vectors computed bySVD.2.2 Relevance FeedbackA feedback query creation algorithm developed byRocchio (Rocchio, 1971) in the mid-1960s has, overthe years, proven to be one of the most successfulprofile learning algorithms.
The algorithm is basedupon the fact that if the relevance for a query isknown, an optimal query vector will maximize theaverage query-document similarity for the relevantdocuments, and will simultaneously minimize theaverage query-document similarity for non-relevantdocuments.
Rocchio shows that an optimal queryvector is the difference vector among the centroidvectors for the relevant and non-relevant documents.~Qo = 1R?D?Rel.
~D ?
1N?R?D/?Rel.
~D where R isthe number of relevant documents, and N is the to-tal number of documents in the collection.
Also, allnegative components of the resulting optimal queryare assigned a zero weight.
To maintain focus of thequery, researchers have found that it is useful to in-clude the original user-query in the feedback querycreation process.
Also, coefficients have been intro-duced in Rocchio?s formulation, which control thecontribution of the original query, the relevant docu-ments, and the non-relevant documents to the feed-back query.
These modifications yield the follow-ing query reformulation function: ~Qn = ?
?
~Qo +?
?
1R?D?rel~D ?
?
?
1N?R?D/?rel~D In this paper,the experiment results based on the local RF wereperformed for comparing with the results of LocalLSI.
The terms in the query are reweighted usingthe Rocchio formula with ?
: ?
: ?
= 1 : 1 : 0.As for the local information relevant to the query,they were obtained by extracting several top-rankeddocuments through the VSM retrieving process inthe experiments.
Jiang has ever used the similarexperiments (VSM+LSI) for Local LSI in his pa-per ?Approximate Dimension Reduction at NTCIR?
(Fan and Littmen, 2000).document set (corpus)topic set (query)term (m) X doc (n) matrix1.
<docid - document vector>2.
<termid -term vector>   Qids (          )query vector(   )preprocess1.
stopword removal.2.
porter's stemming.3.
smart "ltc" twdoci, docj, ... ,dockdoc1   vector:       :docn  vectorcreating local query regionthrough the identified documentsSVDreduced feature space (          )organized by singular vectorsLQSXWGRFYHFWRULQSXWTXHU\YHFWRU\HVquery vector setQid1  (t1, t2, ... tr):Qidk  (t1, t2, ... tr)output the score-list for Qidsand continuefor next queryscore tableQid  Docid  scoreQRrelevant sets of Qidslocal LSIlocalKAnkji ??
,,,1 "ks ?
?1sqGKs ?
( ) ( ) ( )local localT Ts k kscore qid q A d A= GG <Figure 1: Implementation architecture3 Experiment Set-up and Results3.1 Implementation ArchitectureFigure 1 shows overall the architecture of the exper-iment.
The procedure is described as follows:1.
Indexing the document collection and querysets2.
Given a query, retrieving some document bythe relevant sets.
In some cases the relevant setsare derived from the known relevant documentsand in other cases we regarded the top returneddocuments as the relevant sets.3.
performing the singular value decompositionon documents identified in 2.4.
Only a few dimensions for the LSI are retained.5.
Projecting the document vectors and the queryvector into the user-cared feature space, andthen using the standard Cosine measure to getthe final score for this query.6.
Back to the step 2 and continue the analysis onthe next query in the same way.Step 1 is pre-processing procedure for IR system.Only the tag removal, upper case characters trans-verse, stoplist removal and Porter?s stemming wereadopted in this phase (Frakes and Baeza-Yates,1992).
Next, the smart ?ltc?
term weighting scheme(Salton and McGill, 1983) was used to compute theentries of the term document matrix for the collec-tion and entries of the query vector.
The second stepcan be regarded as filter container.
In this paper,the three kinds of routine schemes were performed.In the first case, the local space for each query wasrepresented simply by all document vectors, whichhave already been judged to be relevant (appearingin the relevant judgment file).
We note that althoughit is an ideal case, it may form a useful upper boundon performance.
In the second case, we assume thecondition that the user provides a reasonable num-ber of relevant documents.
In the third case, the lo-cal space for each query was built on the top returnsets of VSM.
The use of the top returned items fromVSM is similar to blind feedback or pseudo RF.3.2 Characteristic of test collectionThere are three test collections in our experiments.Two of them, Cranfield and Medlars, are small.
Thethird one is a large-scale test collection, NACSIS.The Cranfield corpus consists of 1,400 documentson aerodynamics and 225 queries, while Medlarsconsists of 1,033 medical abstracts and 30 queries.Although these two collections are very small, theywere used extensively in the past by IR researchers.As for the NACSIS test collection for the IR 1 & 2(NTCIR 1&NTCIR 2) (Kando, 2001), these docu-ments are abstracts of academic papers presented atmeetings hosted by 65 Japanese scientists and lin-guists.
In our experiments, the English MonolingualIR was performed.
This collection consists of ap-proximately 320,000 English documents in NTCIR-1 and NTCIR-2.3.3 Local Routine Experiments (Ideal Case)We first present the experimental results on the idealcondition.
The document vectors already judged tobe relevant to the query were used.
SVD calcula-tion are performed on the local region organized byTable 1: Results on the Cran., Med.
and NTCIR are shown in terms of ave. precision, precision at documentcutoff of 10.
Results of the local LSI experiment based on three different SVD dimensions were provided.Cranfield Medlars NTCIR (E-E) (D run)K Avr.
P-R R-p K Avr.
P-R R-p K Avr.
P-R R-pVSM - 0.4148 0.3885 - 0.5306 0.5359 - 0.212 0.2277+0% +0% +0% +0% +0% +0%G.
200 0.4543 0.4180 80 0.6680 0.6648 - - -LSI +9% +0% +8% +0% +26% +0% +25% +0% - - -1 0.8833 0.8243 1 0.8946 0.8139 1 0.6997 0.6508+113% +95% +112% +97% +69% +34% +52% +22% +230% +186%L.
2 0.8607 0.8185 2 0.8769 0.8035 2 0.7062 0.6314LSI +108% +90% +108% +96% +67% +32% +52% +20% +233% +177%3 0.8585 0.8102 3 0.8726 0.8019 3 0.6934 0.6293+107% +90% +108% +96% +68% +30% +51% +20% +228% +176%these relevant documents with respect to its query.The IR performance of VSM and global LSI wereregarded as the baseline for comparison.
As for theNTCIR collection, English-English Monolingual IRwas performed and we only extracted the ?D?
(De-scription) field of the topic as the query.
Due toits large size, only the result of VSM is the base-line.
Additionally, to observe the influences of SVDfactors on the IR performance for local LSI exper-iments, results based on LSI dimension from 1 to3 were also provided for comparison.
As we ex-pected, the majority of experimental studies are di-rected towards obtaining better solutions for the lo-cal routine LSI method.
In table 1, K representsthe SVD dimension for LSI analysis.
As for the kvalue of global LSI, it is the parameter by whichLSI yields the best IR performance.
The improve-ment in the average precision of local routine LSIis 113, 69 and 233 percent better than that of VSMon Cranfield, Medlars and NTCIR test collectionsrespectively.
The improvement in average precisionof local routine LSI is 95 and 34 percent better thanthat of global LSI on Cranfield with 200 SVD di-mensions and Medlars with 80 SVD dimensions re-spectively.
Moreover, in the case of SVD factorsequal to 1, we obtain the best IR performance amongall cases on the Cranfield and Medlars.
While theNTCIR collection obtained its best IR performancewith 2 SVD dimensions, there is only a slight dif-ference between the case with 1 singular vector andthe case with 2.
Such small numbers caught our at-tention, since they indicate that there is a nearly lin-ear surface in the local region and that the dominantSVD dimensions can capture such surface and yielda good IR performance for local LSI analysis.To clarify how the local LSI space influences IRperformance, we projected the document vectorsonto the extracted local routine LSI space and fig-ured out the distribution in figure 2.
The data ofplots are based on one query from the Medlars col-lection.
Only the largest singular vector was usedfor the left plot, and the two largest were used for theright.
Based on the plots, we find that these dimen-sions do not vary significantly for the non-relevantdocuments, Thus, they tend to cluster around theorigin.
On the other hand, the relevant documentspace illustrates that local SVD factors are designedto capture their structure.Since the pre-judged set of documents is generallynot available for the ad-hoc query, In this paper, toinvestigate the efficiency of local LSI using very lowdimensions, we continue to do some experiments us-ing different numbers of relevant documents, whichwere selected from the relevant judgment file.
Thecomparative results based on four cases in which theSVD factors equal 1, 2 and 3, respectively, wereshown in Table 2.
The second column is the condi-tion, which means that the number of relevant doc-uments belonging to the analyzing object (query)should exceed the value in table.
Column 3 ?#qry?0 200 400 600 800 1000 12000.00.10.20.30.40.50.6Doc.
IDinner productvaluethe distribution of inner product of document vectorwith the largest singular vectornon-relevant documentqueryrelevant document0.0 0.1 0.2 0.3 0.4 0.5 0.6-0.6-0.4-0.20.00.20.4secondfactorlargest factornon-relevant documentqueryrelevant documentFigure 2: Medlars: document collection distribution after represented by the Local region singular vectors.For the left figure the X-axis is doc.ID and Y are the inner products of the doc vector with the largest singularvectors.
X and Y coordinates on the right are the inner product of the document vectors with the first andsecond largest singular vector, respectively.0.0 0.2 0.4 0.6 0.8 1.00.00.10.20.30.40.50.60.70.80.91.0MedlarsAve.precision-recalllocal LSI (s=20,k=1)global LSI (k=80)VSM0.0 0.2 0.4 0.6 0.8 1.00.10.20.30.40.50.60.70.80.9CranfieldAve.precision-recalllocal LSI (s=3,k=2)global LSI (k=200)VSM0.0 0.2 0.4 0.6 0.8 1.00.00.10.20.30.40.50.60.7NTCIRAve.precision-recallLocal LSI (s=3,k=2)VSMFigure 3: the Ave. precision-recall comparison plots between the best run of local LSI with the baselineVSM and global LSI.indicates the numbers of queries in the test collec-tion which satisfy the condition appearing in the sec-ond column.
The fourth column gives the parameterindicating the number of relevant documents to beused for creating the local space of the correspon-dent query.
As we expected, local LSI using one ortwo SVD dimensions built from the first two singu-lar vectors resulted in the best IR performance in thepartially ideal experiments.
The comparison of theresults was shown in the Table 2.We know that the most important step in LSI isthe phase of SVD.
It requires O(k ?
nz2) to findthe k leading eigenvectors.
The parameter nz is thenon-zero entries of the term-by-document matrix.These requirements are unacceptably high for doc-ument data sets as the non-zero entries number tensof thousands.
According to the LSI analyzing proce-dure, it includes the SVD phase and the subsequentprojecting treatment.
For global LSI, the compu-tation complexity can be evaluated by: O(nz2k +#qry ?
k2 ?
nz2 ?
qnz2) k = (100 ?
300)While our approach can be estimated by: O(#qry?
[(nz2lockloc) + (k2loc ?
nz2 ?
qnz2)]) k = 1 or 2In the above equation, ?nzloc?
represents the non-zero entries of the local query region.
?qnz?
arenon-zero entities in the query vector.
The value of?nzloc?
varies with the number of known relevantdocuments.
Note that the difference between thesetwo equations shows clearly that local LSI on smallSVD dimensions is much easier to compute thanglobal LSI.
According to our observation, it is par-ticularly fast when computing only the largest sin-gular value.Based on the above experiments, the interestingresults and the power of the two largest singular vec-tors prompted us to try putting the local LSI with oneor two singular dimensions into the practical experi-ments.
In this paper, we used the simplest and mostefficient VSM method as the initial retrieval step forextracting the relevant information around the query.We assume that the top-ranked documents obtainedby VSM are relevant documents.
The details are in-troduced in section 3.4.3.4 Ad-hoc local LSI experimentIn this experiment, we note that using the top re-turned items from VSM is sometimes called blindfeedback or pseudo RF.
Hence, we borrow the ideaof local RF.
The expanded query representation wasobtained by combining the original query vectorwith its projecting result on the local SVD dimen-sions.
The equation for expanding the scheme is asfollows: ~qnew = ~qori+Alock (Alock )T ~qori In the equa-tion:Alock = U lock ?lock (V lock )TSim(~d, ~qnew) = ~d?
(~qori + U lock (?lock )2(U lock )T ~qori)As for the parameter k, representing the SVD dimen-sionality of the local region, we set its value equal to1 or 2 in this experiment.
At first, to show that localLSI on small dimensions works well is a practicalcase clearly, we gave the comparable plots betweenlocal LSI with the baseline VSM and global LSI.The 11ppt.
average precision recall plots of localLSI were figured out for the three test collections infigure 3.
The symbol s in the figure represents thesample size and k represents the SVD dimension.To our satisfactions, local LSI based query expan-sion method does much better than VSM and moreclosely approaches the global LSI.Next, to investigate the effectiveness of low di-mensional LSI on local query region in restructur-ing the user cared information space, local RF withRocchio?s weights ?
: ?
: ?
= 1 : 1 : 0, as inXu and Croft (Xu and Croft, 2000), was used forcomparison.
Both of them were used on the samesample documents.
The difference between them isa twofold one.
First, the standard RF formula shownin section 2.2 make use of weighting parameters forquery expansion, while our approach does not.
Sec-ondly, different combination object was used.
Thelocal RF experiment performed in this paper makesuse of the centroid of the top s returned documentvectors.
In our approach, we combine the originalquery vector with its projecting results on the lowlocal SVD space.Table 3 shows these results in terms of varyingfeedback size with one or two SVD dimensions.
Thefirst column ?sample size?
in the table is the value ofs according to which we would select the top rankdocuments .
We see that local LSI outperforms lo-cal RF for most combinations of sample size andone or two SVD dimensions in the experiment onMedlars.
The best run on Medlars using local LSIis 8.4% better than the best in local RF.
As for thebest run on Cranfield and on NTCIR, local LSI gotcomparable results with the local RF.
In the exper-iments, we note that with the increasing of samplesize, the precision of local LSI decreased more thanthat of local RF.
Based on our analysis, there are tworeasons for this.
First, In the VSM based local LSIexperiments, we assume that the top s documentsfrom the initial retrieval by VSM are relevant, al-though that assumption does not always hold.
In thecase where the dominant components of the top sreturn sets are non-relevant, the maintained SVD di-mensions would deviate from the orientation that wepreferred.
This will influence the following projec-tion procedure greatly.
The average precision-recallresults of VSM on Cranfield and NTCIR is 0.38 and0.21, respectively.
Neither one is ideal.
The secondfactor is the characteristic of the test collection.
Thenumber of relevant documents for query sets rangesfrom 2 to 40 and from 3 to 170 for the Cranfieldand NTCIR, respectively.
With such wide range ofquery sets, some queries don?t have enough relevantdocuments for this strategy to be feasible.
There-fore, from the experiment results, it is still reason-able for us to believe that if several relevant sampledocuments of a query are available, low-dimensionallocal LSI will be able to achieve comparable perfor-mance to local RF.4 Analysis and discussion4.1 Local dimensionsOne important variable for LSI retrieval is the num-ber of dimensions in the reduced space.
In this pa-per, we found that one or two SVD dimensions areable to represent the structure of the local regionthat corresponds to the user?s interests.
The first twolargest singular vectors will represent the two majorTable 2: Ave. precision-recall comparing resultsbased on different SVD factors.Coll.
Cond.
#qry #sel.
SVD Ave.Rel.
fact.
P-R1 0.685710 2 0.6667Cran.
>15 27 3 0.6654(#rel) 1 0.57495 2 0.56923 0.56411 0.794510 2 0.8007Med.
>15 25 3 0.7952(#rel) 1 0.71605 2 0.71423 0.71371 0.389910 2 0.3987NTCIR >15 23 3 0.3967(#rel) 1 0.29175 2 0.29133 0.2883interests.
The local SVD dimensions built on themhave the ability to absorb the interests of a query andhave no interest in the non-relevant information.
Itindicates that there is near linear surface in the localquery region.
That is why local LSI works well onsmall dimensions, especially on the condition thatthere is only one dominant interests in the query.
Ofcourse, in cases where there is much noisy informa-tion in the local region, the SVD dimension may failto satisfy the true needs of the user.
Finally, basedon the experiments in this paper, we would like topoint out that for performing SVD on a particularlocal query region, the requirement of the SVD di-mension should not be demanding.
In our opinion,2 or less is sufficient to obtain ideal IR performance.4.2 Size of local regionThe size of a local region is also one important pa-rameter for local LSI.
We did not do much analysison how to determine the best size of the local regionfor local LSI.
In the absence of any clear guidelinesnow, we merely offer some suggestions and an anal-ysis.
The local region should be large enough so thatit will contain more relevant information.
However,there are also several reasons why the local regionshould not be too large.
Adding a large number ofnon-relevant documents of marginal value will onlyincrease the number of LSI factors needed to de-scribe the local region without improving their qual-ity, and this will only degrade the IR performance.Therefore, as for the size of the local region, it is atradeoff.
According to the experimental results andthe analysis in section 3.4, since the local LSI doeswell on one or two SVD dimensions, so as to avoidinfluences of non-relevant information brought bymore involved documents, it is better to restrict thesize of a local region below 30.
In the experimentson Medlars, local LSI produced its best run at 20 topreturn documents.
Of course, the threshold for thesize of local region should be collection-dependentand experiment-determined.
It may also be possibleto set the threshold by the performance of the initialretrieval method, but we have not yet analyzed this.4.3 AdvantagesFinally, we would like to point out the advantagesof low-dimensional LSI analysis for local query re-gion.
Our results compared with VSM and globalLSI show clearly that local LSI with low dimensionsperforms much better than VSM under some samplesets and achieves the comparable IR performance toglobal LSI.
Additionally, because the largest singu-lar vectors are essential for retrieval performance onthe local query regions, local LSI approaches thecomputational complexity of global LSI by usingsuch small SVD dimensions.
Despite the fact thatlocal LSI has increased the cost of separate SVDcomputation for each query, the relative modest re-quirements of SVD dimension make it feasible forlarge scale IR task.Compared with the local RF method, both the lo-cal LSI and local RF achieve better results by pro-viding high-centralized relevant information in thelocal region.
Provided that relevant sample docu-ments are used with the same number, local RF isable to make use of the combination of documentvectors and a heuristic procedure to improve IR per-formance, while local LSI makes use of SVD toextract the useful information from the informationspace.
In some sense, this SVD method is morecomprehensive than local RF.Table 3: comparative results of Local LSI and Localrelevance feedback on the local region organized bythe return sets of VSM on Med., Cran.
and NTCIR,respectively.
The SVD dimension value for the localLSI is the one from which the best IR performancewas obtained at the specific sample size.#ss.
svd 11 ppt.
Ave. P-R R-p(s) fac.
LLSI LRF LLSI LRF3 2 0.5858 0.5977 0.5760 0.58165 1 0.6417 0.6243 0.6300 0.619810 1 0.6577 0.6152 0.6431 0.609320 1 0.6764 0.6044 0.6393 0.584530 1 0.6598 0.5854 0.6246 0.569940 2 0.6514 0.5776 0.6157 0.5722#ss.
svd 11 ppt.
Ave. P-R R-p(s) fac.
LLSI LRF LLSI LRF3 2 0.4524 0.4528 0.4206 0.41865 2 0.4443 0.4403 0.4203 0.414510 2 0.4357 0.4327 0.3981 0.398820 2 0.3993 0.4269 0.3571 0.387030 2 0.3782 0.4252 0.3345 0.391540 2 0.3464 0.4232 0.3106 0.3873#ss.
svd 11 ppt.
Ave. P-R R-p(s) fac.
LLSI LRF LLSI LRF3 2 0.2367 0.2346 0.2380 0.22975 2 0.2292 0.2302 0.2341 0.234710 2 0.2119 0.2249 0.2205 0.240420 2 0.1728 0.2110 0.1800 0.220330 2 0.1458 0.2026 0.1575 0.220840 2 0.1404 0.1978 0.1470 0.21715 Conclusion and future workIn this paper, the results show that very low-dimensional LSI on the local query region performsIR task well.
Such small dimensional requirementsof local LSI make it more attractive, enabling us tobetter address the computation complexity.
We canperform the low-dimensional LSI on several knownrelevant document spaces to obtain significant im-provements in retrieval performance.
Moreover,provided that several relevant sample documents areavailable, local LSI using small dimensions obtainsresults comparable to the local RF although in a dif-ferent manner.
Our future work will:1.
Continue to study the optimal size of local re-gion for local LSI so as to automatically deter-mine it.2.
Find a more efficient initial retrieval methodfor obtaining high quality sample sets of eachquery.AcknowledgementThis work was supported in The 21st Century COEProgram ?Intelligent Human Sensing?, from theministry of Education, Culture,Sports,Science andTechnology.ReferencesM.
W. Berry, Zlatko Drmax, and Elizabeth R. Jessup.1999.
Matrix, vector space, and information retrieval(technical report).
SIAM Review, 41:335?362.S.
T. Dumais.
1996.
Using for information filtering:Trec-3 experiments.
In In Donna K. Harman, editor,The 3rd Text Retrieval Conference (TREC-3), pages282?291.
Department of Commerce, National Instituteof Standards and Technology.J.
Fan and M. L. Littmen.
2000.
Approximate dimensionequalization in vector based information retrieval.
InProceedings of the Seventeenth International Confer-ence on Machine Learning.
Morgan-Kauffman.W.
B. Frakes and R. Baeza-Yates.
1992.
Informationretrieval - Data Structure Algorithms.
Prentice Hall,Englewood Cliffs, New Jersey 07632.D.
Hull.
1994.
Improving text retrieval for the routingproblem using latent semantic indexing.
In In pro-ceedings of the 17th Annual International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, pages 282?291.
Association for com-puting Machnery.N.
Kando.
2001.
Clir syetem evaluation at ntcir work-shop.
National Information (NII) Japan.J.
Rocchio.
1971.
Relevance feedback in informationretrieval.
In The Smart Retrieval System-Experimentsin Automatic Document Processing, pages 313?323.Englewood Cliffs, NJ, 1971, Prentice-Hall, Inc.G.
Salton and M. J. McGill.
1983.
Introduction to Mod-ern Information Retrieval.
McGraw-Hill, New York,NY.J.
Xu and W. B. Croft.
2000.
Improving the effec-tiveness of informational retrieval with local contextanalysis.
ACM Transactions on Information Systems(TOIS), 18(1).
