Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 12?22,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsA Bayesian Approach to Unsupervised Semantic Role InductionIvan Titov Alexandre KlementievSaarland UniversitySaarbru?cken, Germany{titov|aklement}@mmci.uni-saarland.deAbstractWe introduce two Bayesian models for un-supervised semantic role labeling (SRL)task.
The models treat SRL as clusteringof syntactic signatures of arguments withclusters corresponding to semantic roles.The first model induces these clusteringsindependently for each predicate, exploit-ing the Chinese Restaurant Process (CRP)as a prior.
In a more refined hierarchicalmodel, we inject the intuition that the clus-terings are similar across different predi-cates, even though they are not necessar-ily identical.
This intuition is encoded asa distance-dependent CRP with a distancebetween two syntactic signatures indicatinghow likely they are to correspond to a singlesemantic role.
These distances are automat-ically induced within the model and sharedacross predicates.
Both models achievestate-of-the-art results when evaluated onPropBank, with the coupled model consis-tently outperforming the factored counter-part in all experimental set-ups.1 IntroductionSemantic role labeling (SRL) (Gildea and Juraf-sky, 2002), a shallow semantic parsing task, hasrecently attracted a lot of attention in the com-putational linguistic community (Carreras andMa`rquez, 2005; Surdeanu et al 2008; Hajic?
etal., 2009).
The task involves prediction of predi-cate argument structure, i.e.
both identification ofarguments as well as assignment of labels accord-ing to their underlying semantic role.
For exam-ple, in the following sentences:(a) [A0 Mary] opened [A1 the door].
(b) [A0 Mary] is expected to open [A1 the door].
(c) [A1 The door] opened.
(d) [A1 The door] was opened [A0 by Mary].Mary always takes an agent role (A0) for the pred-icate open, and door is always a patient (A1).SRL representations have many potential appli-cations in natural language processing and haverecently been shown to be beneficial in questionanswering (Shen and Lapata, 2007; Kaisser andWebber, 2007), textual entailment (Sammons etal., 2009), machine translation (Wu and Fung,2009; Liu and Gildea, 2010; Wu et al 2011; Gaoand Vogel, 2011), and dialogue systems (Basili etal., 2009; van der Plas et al 2011), among others.Though syntactic representations are often predic-tive of semantic roles (Levin, 1993), the interfacebetween syntactic and semantic representations isfar from trivial.
The lack of simple determinis-tic rules for mapping syntax to shallow semanticsmotivates the use of statistical methods.Although current statistical approaches havebeen successful in predicting shallow seman-tic representations, they typically require largeamounts of annotated data to estimate model pa-rameters.
These resources are scarce and ex-pensive to create, and even the largest of themhave low coverage (Palmer and Sporleder, 2010).Moreover, these models are domain-specific, andtheir performance drops substantially when theyare used in a new domain (Pradhan et al 2008).Such domain specificity is arguably unavoidablefor a semantic analyzer, as even the definitionsof semantic roles are typically predicate specific,and different domains can have radically differentdistributions of predicates (and their senses).
Thenecessity for a large amounts of human-annotateddata for every language and domain is one of themajor obstacles to the wide-spread adoption of se-mantic role representations.These challenges motivate the need for unsu-pervised methods which, instead of relying on la-beled data, can exploit large amounts of unlabeledtexts.
In this paper, we propose simple and effi-12cient hierarchical Bayesian models for this task.It is natural to split the SRL task into twostages: the identification of arguments (the iden-tification stage) and the assignment of semanticroles (the labeling stage).
In this and in muchof the previous work on unsupervised techniques,the focus is on the labeling stage.
Identification,though an important problem, can be tackled withheuristics (Lang and Lapata, 2011a; Grenager andManning, 2006) or, potentially, by using a super-vised classifier trained on a small amount of data.We follow (Lang and Lapata, 2011a), and regardthe labeling stage as clustering of syntactic sig-natures of argument realizations for every predi-cate.
In our first model, as in most of the previouswork on unsupervised SRL, we define an indepen-dent model for each predicate.
We use the Chi-nese Restaurant Process (CRP) (Ferguson, 1973)as a prior for the clustering of syntactic signatures.The resulting model achieves state-of-the-art re-sults, substantially outperforming previous meth-ods evaluated in the same setting.In the first model, for each predicate we inde-pendently induce a linking between syntax and se-mantics, encoded as a clustering of syntactic sig-natures.
The clustering implicitly defines the setof permissible alternations, or changes in the syn-tactic realization of the argument structure of theverb.
Though different verbs admit different alter-nations, some alternations are shared across mul-tiple verbs and are very frequent (e.g., passiviza-tion, example sentences (a) vs. (d), or dativiza-tion: John gave a book to Mary vs. John gaveMary a book) (Levin, 1993).
Therefore, it is nat-ural to assume that the clusterings should be sim-ilar, though not identical, across verbs.Our second model encodes this intuition by re-placing the CRP prior for each predicate witha distance-dependent CRP (dd-CRP) prior (Bleiand Frazier, 2011) shared across predicates.
Thedistance between two syntactic signatures en-codes how likely they are to correspond to a sin-gle semantic role.
Unlike most of the previouswork exploiting distance-dependent CRPs (Bleiand Frazier, 2011; Socher et al 2011; Duan et al2007), we do not encode prior or external knowl-edge in the distance function but rather induce itautomatically within our Bayesian model.
Thecoupled dd-CRP model consistently outperformsthe factored CRP counterpart across all the experi-mental settings (with gold and predicted syntacticparses, and with gold and automatically identifiedarguments).Both models admit efficient inference: the es-timation time on the Penn Treebank WSJ corpusdoes not exceed 30 minutes on a single proces-sor and the inference algorithm is highly paral-lelizable, reducing inference time down to sev-eral minutes on multiple processors.
This sug-gests that the models scale to much larger corpora,which is an important property for a successfulunsupervised learning method, as unlabeled datais abundant.The rest of the paper is structured as follows.Section 2 begins with a definition of the seman-tic role labeling task and discuss some specificsof the unsupervised setting.
In Section 3, we de-scribe CRPs and dd-CRPs, the key componentsof our models.
In Sections 4 ?
6, we describeour factored and coupled models and the infer-ence method.
Section 7 provides both evaluationand analysis.
Finally, additional related work ispresented in Section 8.2 Task DefinitionIn this work, instead of assuming the availabil-ity of role annotated data, we rely only on auto-matically generated syntactic dependency graphs.While we cannot expect that syntactic structurecan trivially map to a semantic representation(Palmer et al 2005)1, we can use syntactic cuesto help us in both stages of unsupervised SRL.Before defining our task, let us consider the twostages separately.In the argument identification stage, we imple-ment a heuristic proposed in (Lang and Lapata,2011a) comprised of a list of 8 rules, which usenonlexicalized properties of syntactic paths be-tween a predicate and a candidate argument to it-eratively discard non-arguments from the list ofall words in a sentence.
Note that inducing theserules for a new language would require some lin-guistic expertise.
One alternative may be to an-notate a small number of arguments and train aclassifier with nonlexicalized features instead.In the argument labeling stage, semantic rolesare represented by clusters of arguments, and la-beling a particular argument corresponds to decid-ing on its role cluster.
However, instead of deal-1Although it provides a strong baseline which is diffi-cult to beat (Grenager and Manning, 2006; Lang and Lapata,2010; Lang and Lapata, 2011a).13ing with argument occurrences directly, we rep-resent them as predicate specific syntactic signa-tures, and refer to them as argument keys.
Thisrepresentation aids our models in inducing highpurity clusters (of argument keys) while reducingtheir granularity.
We follow (Lang and Lapata,2011a) and use the following syntactic features toform the argument key representation:?
Active or passive verb voice (ACT/PASS).?
Argument position relative to predicate(LEFT/RIGHT).?
Syntactic relation to its governor.?
Preposition used for argument realization.In the example sentences in Section 1, the argu-ment keys for candidate arguments Mary for sen-tences (a) and (d) would be ACT:LEFT:SBJ andPASS:RIGHT:LGS->by,2 respectively.
Whileaiming to increase the purity of argument keyclusters, this particular representation will not al-ways produce a good match: e.g.
the door insentence (c) will have the same key as Mary insentence (a).
Increasing the expressiveness of theargument key representation by flagging intransi-tive constructions would distinguish that pair ofarguments.
However, we keep this particular rep-resentation, in part to compare with the previouswork.In this work, we treat the unsupervised seman-tic role labeling task as clustering of argumentkeys.
Thus, argument occurrences in the corpuswhose keys are clustered together are assigned thesame semantic role.
Note that some adjunct-likemodifier arguments are already explicitly repre-sented in syntax and thus do not need to be clus-tered (modifiers AM-TMP, AM-MNR, AM-LOC, andAM-DIR are encoded as ?syntactic?
relations TMP,MNR, LOC, and DIR, respectively (Surdeanu et al2008)); instead we directly use the syntactic labelsas semantic roles.3 Traditional and Distance-dependentCRPsThe central components of our non-parametricBayesian models are the Chinese Restaurant Pro-cesses (CRPs) and the closely related DirichletProcesses (DPs) (Ferguson, 1973).CRPs define probability distributions over par-titions of a set of objects.
An intuitive metaphor2LGS denotes a logical subject in a passive construction(Surdeanu et al 2008).for describing CRPs is assignment of tables torestaurant customers.
Assume a restaurant with asequence of tables, and customers who walk intothe restaurant one at a time and choose a table tojoin.
The first customer to enter is assigned thefirst table.
Suppose that when a client number ienters the restaurant, i ?
1 customers are sittingat each of the k ?
(1, .
.
.
,K) tables occupied sofar.
The new customer is then either seated at oneof theK tables with probability Nki?1+?
, whereNkis the number customers already sitting at tablek, or assigned to a new table with the probability?i?1+?
.
The concentration parameter ?
encodesthe granularity of the drawn partitions: the larger?, the larger the expected number of occupied ta-bles.
Though it is convenient to describe CRP in asequential manner, the probability of a seating ar-rangement is invariant of the order of customers?arrival, i.e.
the process is exchangeable.
In ourfactored model, we use CRPs as a prior for clus-tering argument keys, as we explain in Section 4.Often CRP is used as a part of the Dirich-let Process mixture model where each subset inthe partition (each table) selects a parameter (ameal) from some base distribution over parame-ters.
This parameter is then used to generate alldata points corresponding to customers assignedto the table.
The Dirichlet processes (DP) areclosely connected to CRPs: instead of choosingmeals for customers through the described gener-ative story, one can equivalently draw a distribu-tion G over meals from DP and then draw a mealfor every customer from G. We refer the readerto Teh (2010) for details on CRPs and DPs.
Inour method, we use DPs to model distributions ofarguments for every role.In order to clarify how similarities betweencustomers can be integrated in the generative pro-cess, we start by reformulating the traditionalCRP in an equivalent form so that distance-dependent CRP (dd-CRP) can be seen as its gen-eralization.
Instead of selecting a table for eachcustomer as described above, one can equiva-lently assume that a customer i chooses one ofthe previous customers ci as a partner with prob-ability 1i?1+?
and sits at the same table, or occu-pies a new table with the probability ?i?1+?
.
Thetransitive closure of this seating-with relation de-termines the partition.A generalization of this view leads to the defini-tion of the distance-dependent CRP.
In dd-CRPs,14a customer i chooses a partner ci = j withthe probability proportional to some non-negativescore di,j (di,j = dj,i) which encodes a similaritybetween the two customers.3 More formally,p(ci = j|D,?)
?
{di,j , i 6= j?, i = j(1)where D is the entire similarity graph.
This pro-cess lacks the exchangeability property of the tra-ditional CRP but efficient approximate inferencewith dd-CRP is possible with Gibbs sampling.For more details on inference with dd-CRPs, werefer the reader to Blei and Frazier (2011).Though in previous work dd-CRP was used ei-ther to encode prior knowledge (Blei and Fra-zier, 2011) or other external information (Socheret al 2011), we treat D as a latent variabledrawn from some prior distribution over weightedgraphs.
This view provides a powerful approachfor coupling a family of distinct but similar clus-terings: the family of clusterings can be drawn byfirst choosing a similarity graph D for the entirefamily and then re-usingD to generate each of theclusterings independently of each other as definedby equation (1).
In Section 5, we explain how weuse this formalism to encode relatedness betweenargument key clusterings for different predicates.4 Factored ModelIn this section we describe the factored methodwhich models each predicate independently.
InSection 2 we defined our task as clustering of ar-gument keys, where each cluster corresponds to asemantic role.
If an argument key k is assignedto a role r (k ?
r), all of its occurrences are la-beled r.Our Bayesian model encodes two common as-sumptions about semantic roles.
First, we enforcethe selectional restriction assumption: we assumethat the distribution over potential argument fillersis sparse for every role, implying that ?peaky?
dis-tributions of arguments for each role r are pre-ferred to flat distributions.
Second, each role nor-mally appears at most once per predicate occur-rence.
Our inference will search for a clusteringwhich meets the above requirements to the maxi-mal extent.3It may be more standard to use a decay function f :R ?
R and choose a partner with the probability propor-tional to f(?di,j).
However, the two forms are equivalentand using scores di,j directly is more convenient for our in-duction purposes.Our model associates two distributions witheach predicate: one governs the selection of argu-ment fillers for each semantic role, and the othermodels (and penalizes) duplicate occurrence ofroles.
Each predicate occurrence is generated in-dependently given these distributions.
Let us de-scribe the model by first defining how the set ofmodel parameters and an argument key clusteringare drawn, and then explaining the generation ofindividual predicate and argument instances.
Thegenerative story is formally presented in Figure 1.We start by generating a partition of argumentkeys Bp with each subset r ?
Bp representinga single semantic role.
The partitions are drawnfrom CRP(?)
(see the Factored model section ofFigure 1) independently for each predicate.
Thecrucial part of the model is the set of selectionalpreference parameters ?p,r, the distributions of ar-guments x for each role r of predicate p. Werepresent arguments by their syntactic heads,4 ormore specifically, by either their lemmas or wordclusters assigned to the head by an external clus-tering algorithm, as we will discuss in more detailin Section 7.5 For the agent role A0 of the pred-icate open, for example, this distribution wouldassign most of the probability mass to argumentsdenoting sentient beings, whereas the distributionfor the patient role A1 would concentrate on ar-guments representing ?openable?
things (doors,boxes, books, etc).In order to encode the assumption about sparse-ness of the distributions ?p,r, we draw them fromthe DP prior DP (?,H(A)) with a small concen-tration parameter ?, the base probability distribu-tionH(A) is just the normalized frequencies of ar-guments in the corpus.
The geometric distribution?p,r is used to model the number of times a roler appears with a given predicate occurrence.
Thedecision whether to generate at least one role r isdrawn from the uniform Bernoulli distribution.
If0 is drawn then the semantic role is not realizedfor the given occurrence, otherwise the numberof additional roles r is drawn from the geometricdistribution Geom(?p,r).
The Beta priors over ?4For prepositional phrases, we take as head the head nounof the object noun phrase as it encodes crucial lexical infor-mation.
However, the preposition is not ignored but ratherencoded in the corresponding argument key, as explainedin Section 2.5Alternatively, the clustering of arguments could be in-duced within the model, as done in (Titov and Klementiev,2011).15Clustering of argument keys:Factored model:for each predicate p = 1, 2, .
.
.
:Bp ?
CRP (?)
[partition of arg keys]Coupled model:D ?
NonInform [similarity graph]for each predicate p = 1, 2, .
.
.
:Bp ?
dd-CRP (?,D) [partition of arg keys]Parameters:for each predicate p = 1, 2, .
.
.
:for each role r ?
Bp:?p,r ?
DP (?,H(A)) [distrib of arg fillers]?p,r ?
Beta(?0, ?1) [geom distr for dup roles]Data Generation:for each predicate p = 1, 2, .
.
.
:for each occurrence l of p:for every role r ?
Bp:if [n ?
Unif(0, 1)] = 1: [role appears at least once]GenArgument(p, r) [draw one arg]while [n ?
?p,r] = 1: [continue generation]GenArgument(p, r) [draw more args]GenArgument(p, r):kp,r ?
Unif(1, .
.
.
, |r|) [draw arg key]xp,r ?
?p,r [draw arg filler]Figure 1: Generative stories for the factored and cou-pled models.can indicate the preference towards generating atmost one argument for each role.
For example,it would express the preference that a predicateopen typically appears with a single agent and asingle patient arguments.Now, when parameters and argument key clus-terings are chosen, we can summarize the re-mainder of the generative story as follows.
Webegin by independently drawing occurrences foreach predicate.
For each predicate role we in-dependently decide on the number of role occur-rences.
Then we generate each of the arguments(see GenArgument) by generating an argumentkey kp,r uniformly from the set of argument keysassigned to the cluster r, and finally choosing itsfiller xp,r, where the filler is either a lemma or aword cluster corresponding to the syntactic headof the argument.5 Coupled ModelAs we argued in Section 1, clusterings of argu-ment keys implicitly encode the pattern of alter-nations for a predicate.
E.g., passivization can beroughly represented with the clustering of the keyACT:LEFT:SBJ with PASS:RIGHT:LGS->byand ACT:RIGHT:OBJ with PASS:LEFT:SBJ.The set of permissible alternations is predicate-specific,6 but nevertheless they arguably repre-sent a small subset of all clusterings of argu-ment keys.
Also, some alternations are morelikely to be applicable to a verb than others: forexample, passivization and dativization alterna-tions are both fairly frequent, whereas, locative-preposition-drop alternation (Mary climbed up themountain vs. Mary climbed the mountain) is lesscommon and applicable only to several classesof predicates representing motion (Levin, 1993).We represent this observation by quantifying howlikely a pair of keys is to be clustered.
Thesescores (di,j for every pair of argument keys i andj) are induced automatically within the model,and treated as latent variables shared across pred-icates.
Intuitively, if data for several predicatesstrongly suggests that two argument keys shouldbe clustered (e.g., there is a large overlap be-tween argument fillers for the two keys) then theposterior will indicate that di,j is expected to begreater for the pair {i, j} than for some other pair{i?, j?}
for which the evidence is less clear.
Con-sequently, argument keys i and j will be clusteredeven for predicates without strong evidence forsuch a clustering, whereas i?
and j?
will not.One argument against coupling predicates maystem from the fact that we are using unlabeleddata and may be able to obtain sufficient amountof learning material even for less frequent pred-icates.
This may be a valid observation, but an-other rationale for sharing this similarity structureis the hypothesis that alternations may be easierto detect for some predicates than for others.
Forexample, argument key clustering of predicateswith very restrictive selectional restrictions on ar-gument fillers is presumably easier than clusteringfor predicates with less restrictive and overlap-ping selectional restriction, as compactness of se-lectional preferences is a central assumption driv-ing unsupervised learning of semantic roles.
E.g.,predicates change and defrost belong to the sameLevin class (change-of-state verbs) and thereforeadmit similar alternations.
However, the set of po-tential patients of defrost is sufficiently restricted,6Or, at least specific to a class of predicates (Levin,1993).16whereas the selectional restrictions for the patientof change are far less specific and they overlapwith selectional restrictions for the agent role, fur-ther complicating the clustering induction task.This observation suggests that sharing clusteringpreferences across verbs is likely to help even ifthe unlabeled data is plentiful for every predicate.More formally, we generate scores di,j , orequivalently, the full labeled graph D with ver-tices corresponding to argument keys and edgesweighted with the similarity scores, from a prior.In our experiments we use a non-informative priorwhich factorizes over pairs (i.e.
edges of thegraph D), though more powerful alternatives canbe considered.
Then we use it, in a dd-CRP(?,D), to generate clusterings of argument keys forevery predicate.
The rest of the generative story isthe same as for the factored model.
The part rele-vant to this model is shown in the Coupled modelsection of Figure 1.Note that this approach does not assume thatthe frequencies of syntactic patterns correspond-ing to alternations are similar, and a large valuefor di,j does not necessarily mean that the corre-sponding syntactic frames i and j are very fre-quent in a corpus.
What it indicates is that a largenumber of different predicates undergo the corre-sponding alternation; the frequency of the alterna-tion is a different matter.
We believe that this is animportant point, as we do not make a restrictingassumption that an alternation has the same dis-tributional properties for all verbs which undergothis alternation.6 InferenceAn inference algorithm for an unsupervisedmodel should be efficient enough to handle vastamounts of unlabeled data, as it can easily be ob-tained and is likely to improve results.
We usea simple approximate inference algorithm basedon greedy MAP search.
We start by discussingMAP search for argument key clustering with thefactored model and then discuss its extension ap-plicable to the coupled model.6.1 Role InductionFor the factored model, semantic roles for everypredicate are induced independently.
Neverthe-less, search for a MAP clustering can be expen-sive, as even a move involving a single argumentkey implies some computations for all its occur-rences in the corpus.
Instead of more complexMAP search algorithms (see, e.g., (Daume III,2007)), we use a greedy procedure where we startwith each argument key assigned to an individualcluster, and then iteratively try to merge clusters.Each move involves (1) choosing an argument keyand (2) deciding on a cluster to reassign it to.
Thisis done by considering all clusters (including cre-ating a new one) and choosing the most probableone.Instead of choosing argument keys randomly atthe first stage, we order them by corpus frequency.This ordering is beneficial as getting clusteringright for frequent argument keys is more impor-tant and the corresponding decisions should bemade earlier.7 We used a single iteration in ourexperiments, as we have not noticed any benefitfrom using multiple iterations.6.2 Similarity Graph InductionIn the coupled model, clusterings for differentpredicates are statistically dependent, as the simi-larity structureD is latent and shared across pred-icates.
Consequently, a more complex inferenceprocedure is needed.
For simplicity here and inour experiments, we use the non-informative priordistribution over D which assigns the same priorprobability to every possible weight di,j for everypair {i, j}.Recall that the dd-CRP prior is defined in termsof customers choosing other customers to sit with.For the moment, let us assume that this relationamong argument keys is known, that is, every ar-gument key k for predicate p has chosen an argu-ment key cp,k to ?sit?
with.
We can compute theMAP estimate for all di,j by maximizing the ob-jective:argmaxdi,j , i 6=j?p?k?Kplogdk,cp,k?k?
?Kp dk,k?,where Kp is the set of all argument keys for thepredicate p. We slightly abuse the notation by us-ing di,i to denote the concentration parameter ?in the previous expression.
Note that we also as-sume that similarities are symmetric, di,j = dj,i.If the set of argument keysKp would be the samefor every predicate, then the optimal di,j would7This idea has been explored before for shallow semanticrepresentations (Lang and Lapata, 2011a; Titov and Klemen-tiev, 2011).17be proportional to the number of times either i se-lects j as a partner, or j chooses i as a partner.8This no longer holds if the sets are different, butthe solution can be found efficiently using a nu-meric optimization strategy; we use the gradientdescent algorithm.We do not learn the concentration parameter?, as it is used in our model to indicate the de-sired granularity of semantic roles, but insteadonly learn di,j (i 6= j).
However, just learningthe concentration parameter would not be suffi-cient as the effective concentration can be reducedor increased arbitrarily by scaling all the similar-ities di,j (i 6= j) at once, as follows from expres-sion (1).
Instead, we enforce the normalizationconstraint on the similarities di,j .
We ensure thatthe prior probability of choosing itself as a part-ner, averaged over predicates, is the same as itwould be with uniform di,j (di,j = 1 for everykey pair {i, j}, i 6= j).
This roughly says thatwe want to preserve the same granularity of clus-tering as it was with the uniform similarities.
Weaccomplish this normalization in a post-hoc fash-ion by dividing the weights after optimization by?p?k,k?
?Kp, k?
6=k dk,k?/?p |Kp|(|Kp| ?
1).If D is fixed, partners for every predicate p andevery k can be found using virtually the same al-gorithm as in Section 6.1: the only difference isthat, instead of a cluster, each argument key itera-tively chooses a partner.Though, in practice, both the choice of partnersand the similarity graphs are latent, we can use aniterative approach to obtain a joint MAP estimateof ck (for every k) and the similarity graph D byalternating the two steps.9Notice that the resulting algorithm is againhighly parallelizable: the graph induction stageis fast, and induction of the seat-with relation(i.e.
clustering argument keys) is factorizable overpredicates.One shortcoming of this approach is typicalfor generative models with multiple ?features?
:when such a model predicts a latent variable, ittends to ignore the prior class distribution and re-lies solely on features.
This behavior is due tothe over-simplifying independence assumptions.It is well known, for instance, that the poste-8Note that weights di,j are invariant under rescalingwhen the rescaling is also applied to the concentration pa-rameter ?.9In practice, two iterations were sufficient.rior with Naive Bayes tends to be overconfidentdue to violated conditional independence assump-tions (Rennie, 2001).
The same behavior is ob-served here: the shared prior does not have suf-ficient effect on frequent predicates.10 Thoughdifferent techniques have been developed to dis-count the over-confidence (Kolcz and Chowdhury,2005), we use the most basic one: we raise thelikelihood term in power 1T , where the parameterT is chosen empirically.7 Empirical Evaluation7.1 Data and EvaluationWe keep the general setup of (Lang and Lapata,2011a), to evaluate our models and compare themto the current state of the art.
We run all of ourexperiments on the standard CoNLL 2008 sharedtask (Surdeanu et al 2008) version of Penn Tree-bank WSJ and PropBank.
In addition to golddependency analyses and gold PropBank annota-tions, it has dependency structures generated au-tomatically by the MaltParser (Nivre et al 2007).We vary our experimental setup as follows:?
We evaluate our models on gold and auto-matically generated parses, and use eithergold PropBank annotations or the heuristicfrom Section 2 to identify arguments, result-ing in four experimental regimes.?
In order to reduce the sparsity of predicateargument fillers we consider replacing lem-mas of their syntactic heads with word clus-ters induced by a clustering algorithm as apreprocessing step.
In particular, we useBrown (Br) clustering (Brown et al 1992)induced over RCV1 corpus (Turian et al2010).
Although the clustering is hierarchi-cal, we only use a cluster at the lowest levelof the hierarchy for each word.We use the purity (PU) and collocation (CO) met-rics as well as their harmonic mean (F1) to mea-sure the quality of the resulting clusters.
Puritymeasures the degree to which each cluster con-tains arguments sharing the same gold role:PU =1N?imaxj|Gj ?
Ci|where if Ci is the set of arguments in the i-th in-duced cluster,Gj is the set of arguments in the jth10The coupled model without discounting still outper-forms the factored counterpart in our experiments.18gold cluster, and N is the total number of argu-ments.
Collocation evaluates the degree to whicharguments with the same gold roles are assignedto a single cluster.
It is computed as follows:CO =1N?jmaxi|Gj ?
Ci|We compute the aggregate PU, CO, and F1scores over all predicates in the same way as(Lang and Lapata, 2011a) by weighting the scoresof each predicate by the number of its argumentoccurrences.
Note that since our goal is to evalu-ate the clustering algorithms, we do not includeincorrectly identified arguments (i.e.
mistakesmade by the heuristic defined in Section 2) whencomputing these metrics.We evaluate both factored and coupled modelsproposed in this work with and without Brownword clustering of argument fillers (Factored,Coupled, Factored+Br, Coupled+Br).
Our mod-els are robust to parameter settings, they weretuned (to an order of magnitude) on the develop-ment set and were the same for all model variants:?
= 1.e-3, ?
= 1.e-3, ?0 = 1.e-3, ?1 = 1.e-10,T = 5.
Although they can be induced within themodel, we set them by hand to indicate granular-ity preferences.
We compare our results with thefollowing alternative approaches.
The syntacticfunction baseline (SyntF) simply clusters predi-cate arguments according to the dependency re-lation to their head.
Following (Lang and Lapata,2010), we allocate a cluster for each of 20 mostfrequent relations in the CoNLL dataset and onecluster for all other relations.
We also compareour performance with the Latent Logistic classifi-cation (Lang and Lapata, 2010), Split-Merge clus-tering (Lang and Lapata, 2011a), and Graph Parti-tioning (Lang and Lapata, 2011b) approaches (la-beled LLogistic, SplitMerge, and GraphPart, re-spectively) which achieve the current best unsu-pervised SRL results in this setting.7.2 Results7.2.1 Gold ArgumentsExperimental results are summarized in Ta-ble 1.
We begin by comparing our models to thethree existing clustering approaches on gold syn-tactic parses, and using gold PropBank annota-tions to identify predicate arguments.
In this set ofexperiments we measure the relative performanceof argument clustering, removing the identifica-gold parses auto parsesPU CO F1 PU CO F1LLogistic 79.5 76.5 78.0 77.9 74.4 76.2SplitMerge 88.7 73.0 80.1 86.5 69.8 77.3GraphPart 88.6 70.7 78.6 87.4 65.9 75.2Factored 88.1 77.1 82.2 85.1 71.8 77.9Coupled 89.3 76.6 82.5 86.7 71.2 78.2Factored+Br 86.8 78.8 82.6 83.8 74.1 78.6Coupled+Br 88.7 78.1 83.0 86.2 72.7 78.8SyntF 81.6 77.5 79.5 77.1 70.9 73.9Table 1: Argument clustering performance with goldargument identification.
Bold-face is used to highlightthe best F1 scores.tion stage, and minimize the noise due to auto-matic syntactic annotations.
All four variants ofthe models we propose substantially outperformother models: the coupled model with Brownclustering of argument fillers (Coupled+Br) beatsthe previous best model SplitMerge by 2.9% F1score.
As mentioned in Section 2, our approachspecifically does not cluster some of the modifierarguments.
In order to verify that this and argu-ment filler clustering were not the only aspectsof our approach contributing to performance im-provements, we also evaluated our coupled modelwithout Brown clustering and treating modifiersas regular arguments.
The model achieves 89.2%purity, 74.0% collocation, and 80.9% F1 scores,still substantially outperforming all of the alter-native approaches.
Replacing gold parses withMaltParser analyses we see a similar trend, whereCoupled+Br outperforms the best alternative ap-proach SplitMerge by 1.5%.7.2.2 Automatic ArgumentsResults are summarized in Table 2.11 Theprecision and recall of our re-implementation ofthe argument identification heuristic described inSection 2 on gold parses were 87.7% and 88.0%,respectively, and do not quite match 88.1% and87.9% reported in (Lang and Lapata, 2011a).Since we could not reproduce their argumentidentification stage exactly, we are omitting theirresults for the two regimes, instead including theresults for our two best models Factored+Br andCoupled+Br.
We see a similar trend, where thecoupled system consistently outperforms its fac-tored counterpart, achieving 85.8% and 83.9% F111Note, that the scores are computed on correctly iden-tified arguments only, and tend to be higher in these ex-periments probably because the complex arguments get dis-carded by the heuristic.19gold parses auto parsesPU CO F1 PU CO F1Factored+Br 87.8 82.9 85.3 85.8 81.1 83.4Coupled+Br 89.2 82.6 85.8 87.4 80.7 83.9SyntF 83.5 81.4 82.4 81.4 79.1 80.2Table 2: Argument clustering performance with auto-matic argument identification.for gold and MaltParser analyses, respectively.We observe that consistently through the fourregimes, sharing of alternations between predi-cates captured by the coupled model outperformsthe factored version, and that reducing the argu-ment filler sparsity with clustering also has a sub-stantial positive effect.
Due to the space con-straints we are not able to present detailed anal-ysis of the induced similarity graph D, however,argument-key pairs with the highest induced sim-ilarity encode, among other things, passivization,benefactive alternations, near-interchangeabilityof some subordinating conjunctions and preposi-tions (e.g., if and whether), as well as, restoringsome of the unnecessary splits introduced by theargument key definition (e.g., semantic roles foradverbials do not normally depend on whether theconstruction is passive or active).8 Related WorkMost of SRL research has focused on the super-vised setting (Carreras and Ma`rquez, 2005; Sur-deanu et al 2008), however, lack of annotated re-sources for most languages and insufficient cover-age provided by the existing resources motivatesthe need for using unlabeled data or other formsof weak supervision.
This work includes methodsbased on graph alignment between labeled andunlabeled data (Fu?rstenau and Lapata, 2009), us-ing unlabeled data to improve lexical generaliza-tion (Deschacht and Moens, 2009), and projectionof annotation across languages (Pado and Lapata,2009; van der Plas et al 2011).
Semi-supervisedand weakly-supervised techniques have also beenexplored for other types of semantic representa-tions but these studies have mostly focused on re-stricted domains (Kate and Mooney, 2007; Lianget al 2009; Titov and Kozhevnikov, 2010; Gold-wasser et al 2011; Liang et al 2011).Unsupervised learning has been one of the cen-tral paradigms for the closely-related area of re-lation extraction, where several techniques havebeen proposed to cluster semantically similar ver-balizations of relations (Lin and Pantel, 2001;Banko et al 2007).
Early unsupervised ap-proaches to the SRL problem include the workby Swier and Stevenson (2004), where the Verb-Net verb lexicon was used to guide unsupervisedlearning, and a generative model of Grenager andManning (2006) which exploits linguistic priorson syntactic-semantic interface.More recently, the role induction problem hasbeen studied in Lang and Lapata (2010) whereit has been reformulated as a problem of detect-ing alterations and mapping non-standard link-ings to the canonical ones.
Later, Lang and La-pata (2011a) proposed an algorithmic approachto clustering argument signatures which achieveshigher accuracy and outperforms the syntacticbaseline.
In Lang and Lapata (2011b), the roleinduction problem is formulated as a graph parti-tioning problem: each vertex in the graph corre-sponds to a predicate occurrence and edges repre-sent lexical and syntactic similarities between theoccurrences.
Unsupervised induction of seman-tics has also been studied in Poon and Domin-gos (2009) and Titov and Klementiev (2010) butthe induced representations are not entirely com-patible with the PropBank-style annotations andthey have been evaluated only on a question an-swering task for the biomedical domain.
Also, therelated task of unsupervised argument identifica-tion was considered in Abend et al(2009).9 ConclusionsIn this work we introduced two Bayesian modelsfor unsupervised role induction.
They treat thetask as a family of related clustering problems,one for each predicate.
The first factored modelinduces each clustering independently, whereasthe second model couples them by exploiting anovel technique for sharing clustering preferencesacross a family of clusterings.
Both methodsachieve state-of-the-art results with the coupledmodel outperforming the factored counterpart inall regimes.AcknowledgementsThe authors acknowledge the support of the MMCICluster of Excellence, and thank Hagen Fu?rstenau,Mikhail Kozhevnikov, Alexis Palmer, Manfred Pinkal,Caroline Sporleder and the anonymous reviewers fortheir suggestions, and Joel Lang for answering ques-tions about their methods and data.20ReferencesOmri Abend, Roi Reichart, and Ari Rappoport.
2009.Unsupervised argument identification for semanticrole labeling.
In ACL-IJCNLP.Michele Banko, Michael J Cafarella, Stephen Soder-land, Matt Broadhead, and Oren Etzioni.
2007.Open information extraction from the web.
In IJ-CAI.Roberto Basili, Diego De Cao, Danilo Croce,Bonaventura Coppola, and Alessandro Moschitti.2009.
Cross-language frame semantics transfer inbilingual corpora.
In CICLING.David M. Blei and Peter Frazier.
2011.
Distance de-pendent chinese restaurant processes.
Journal ofMachine Learning Research, 12:2461?2488.Peter F. Brown, Vincent Della Pietra, Peter V. deSouza,Jenifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models for natural language.
Compu-tational Linguistics, 18(4):467?479.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Intro-duction to the CoNLL-2005 Shared Task: SemanticRole Labeling.
In CoNLL.Hal Daume III.
2007.
Fast search for dirichlet processmixture models.
In AISTATS.Koen Deschacht and Marie-Francine Moens.
2009.Semi-supervised semantic role labeling using theLatent Words Language Model.
In EMNLP.Jason Duan, Michele Guindani, and Alan Gelfand.2007.
Generalized spatial dirichlet process models.Biometrika, 94:809?825.Thomas S. Ferguson.
1973.
A Bayesian analysisof some nonparametric problems.
The Annals ofStatistics, 1(2):209?230.Hagen Fu?rstenau and Mirella Lapata.
2009.
Graphalignment for semi-supervised semantic role label-ing.
In EMNLP.Qin Gao and Stephan Vogel.
2011.
Corpus expansionfor statistical machine translation with semantic rolelabel substitution rules.
In ACL:HLT.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabelling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Dan Goldwasser, Roi Reichart, James Clarke, and DanRoth.
2011.
Confidence driven unsupervised se-mantic parsing.
In ACL.Trond Grenager and Christoph Manning.
2006.
Unsu-pervised discovery of a statistical verb lexicon.
InEMNLP.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic depen-dencies in multiple languages.
In Proceedingsof the 13th Conference on Computational NaturalLanguage Learning (CoNLL-2009), June 4-5.Michael Kaisser and Bonnie Webber.
2007.
Questionanswering based on semantic roles.
In ACL Work-shop on Deep Linguistic Processing.Rohit J. Kate and Raymond J. Mooney.
2007.
Learn-ing language semantics from ambigous supervision.In AAAI.Aleksander Kolcz and Abdur Chowdhury.
2005.
Dis-counting over-confidence of naive bayes in high-recall text classification.
In ECML.Joel Lang and Mirella Lapata.
2010.
Unsupervisedinduction of semantic roles.
In ACL.Joel Lang and Mirella Lapata.
2011a.
Unsupervisedsemantic role induction via split-merge clustering.In ACL.Joel Lang and Mirella Lapata.
2011b.
Unsupervisedsemantic role induction with graph partitioning.
InEMNLP.Beth Levin.
1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
University ofChicago Press.Percy Liang, Michael I. Jordan, and Dan Klein.
2009.Learning semantic correspondences with less super-vision.
In ACL-IJCNLP.Percy Liang, Michael Jordan, and Dan Klein.
2011.Learning dependency-based compositional seman-tics.
In ACL: HLT.Dekang Lin and Patrick Pantel.
2001.
DIRT ?
discov-ery of inference rules from text.
In KDD.Ding Liu and Daniel Gildea.
2010.
Semantic role fea-tures for machine translation.
In Coling.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007shared task on dependency parsing.
In EMNLP-CoNLL.Sebastian Pado and Mirella Lapata.
2009.
Cross-lingual annotation projection for semantic roles.Journal of Artificial Intelligence Research, 36:307?340.Alexis Palmer and Caroline Sporleder.
2010.
Evalu-ating FrameNet-style semantic parsing: the role ofcoverage gaps in FrameNet.
In COLING.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In EMNLP.Sameer Pradhan, Wayne Ward, and James H. Martin.2008.
Towards robust semantic role labeling.
Com-putational Linguistics, 34:289?310.Jason Rennie.
2001.
Improving multi-class textclassification with Naive bayes.
Technical ReportAITR-2001-004, MIT.M.
Sammons, V. Vydiswaran, T. Vieira, N. Johri,M.
Chang, D. Goldwasser, V. Srikumar, G. Kundu,Y.
Tu, K. Small, J.
Rule, Q.
Do, and D. Roth.
2009.Relation alignment for textual entailment recogni-tion.
In Text Analysis Conference (TAC).21Dan Shen and Mirella Lapata.
2007.
Using semanticroles to improve question answering.
In EMNLP.Richard Socher, Andrew Maas, and Christopher Man-ning.
2011.
Spectral chinese restaurant processes:Nonparametric clustering based on similarities.
InAISTATS.Mihai Surdeanu, Adam Meyers Richard Johansson,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syn-tactic and semantic dependencies.
In CoNLL 2008:Shared Task.Richard Swier and Suzanne Stevenson.
2004.
Unsu-pervised semantic role labelling.
In EMNLP.Yee Whye Teh.
2010.
Dirichlet processes.
In Ency-clopedia of Machine Learning.
Springer.Ivan Titov and Alexandre Klementiev.
2011.
ABayesian model for unsupervised semantic parsing.In ACL.Ivan Titov and Mikhail Kozhevnikov.
2010.Bootstrapping semantic analyzers from non-contradictory texts.
In ACL.Joseph Turian, Lev Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In ACL.Lonneke van der Plas, Paola Merlo, and James Hen-derson.
2011.
Scaling up automatic cross-lingualsemantic role annotation.
In ACL.Dekai Wu and Pascale Fung.
2009.
Semantic roles forSMT: A hybrid two-pass model.
In NAACL.Dekai Wu, Marianna Apidianaki, Marine Carpuat, andLucia Specia, editors.
2011.
Proc.
of Fifth Work-shop on Syntax, Semantics and Structure in Statisti-cal Translation.
ACL.22
