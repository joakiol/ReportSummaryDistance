Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1049?1056,Sydney, July 2006. c?2006 Association for Computational LinguisticsLearning to Predict Case Markers in Japanese                Hisami Suzuki    Kristina Toutanova1Microsoft ResearchOne Microsoft Way, Redmond WA 98052 USA{hisamis,kristout}@microsoft.comAbstractJapanese case markers, which indicate the gram-matical relation of the complement NP to thepredicate, often pose challenges to the generationof Japanese text, be it done by a foreign languagelearner, or by a machine translation (MT) system.In this paper, we describe the task of predictingJapanese case markers and propose machinelearning methods for solving it in two settings: (i)monolingual, when given information only fromthe Japanese sentence; and (ii) bilingual, whenalso given information from a corresponding Eng-lish source sentence in an MT context.
We formu-late the task after the well-studied task of Englishsemantic role labelling, and explore features froma syntactic dependency structure of the sentence.For the monolingual task, we evaluated our modelson the Kyoto Corpus and achieved over 84% ac-curacy in assigning correct case markers for eachphrase.
For the bilingual task, we achieved an ac-curacy of 92% per phrase using a bilingual datasetfrom a technical domain.
We show that in bothsettings, features that exploit dependency informa-tion, whether derived from gold-standard annota-tions or automatically assigned, contribute signifi-cantly to the prediction of case markers.11 Introduction: why predict case?Generation of grammatical elements such as inflec-tional endings and case markers has become an impor-tant component technology, particularly in the contextof machine translation (MT).
In an English-to-JapaneseMT system, for example, Japanese case markers,which indicate grammatical relations (e.g., subject,object, location) of the complement noun phrase to thepredicate, are among the most difficult to generateappropriately.
This is because the case markers oftendo not correspond to any word in the source languageas many grammatical relations are expressed via wordorder in English.
It is also difficult because the map-ping between the case markers and the grammatical1Author names arranged alphabeticallyrelations they express is very complex.
For the samereasons, generation of case markers is challenging toforeign language learners.
This difficulty in generation,however, does not mean the choice of case markers isinsignificant: when a generated sentence contains mis-takes in grammatical elements, they often lead to se-vere unintelligibility, sometimes resulting in a differentsemantic interpretation from the intended one.
There-fore, having a model that makes reasonable predictionsabout which case marker to generate given the contentwords of a sentence, is expected to help MT and gen-eration in general, particularly when the source (ornative) and the target languages are morphologicallydivergent.But how reliably can we predict case markers inJapanese using the information that exists only in thesentence?
Consider the example in Figure 1.
This sen-tence contains two case markers, kara 'from' and ni, thelatter not corresponding to any word in English.
If wewere to predict the case markers in this sentence, thereare multiple valid answers for each decision, many ofwhich correspond to different semantic relations.
Forexample, for the first case marker slot in Figure 1 filledby kara, wa (topic marker), ni 'in' or no case marker atall are all reasonable choices, while other markers suchas wo (object marker), de 'at', made 'until', etc.
are notconsidered reasonable.
For the second slot filled by ni,ga (subject marker) is also a grammatically reasonablechoice, making Einstein the subject of idolize, thuschanging the meaning of the sentence.
As is obvious inthis example, the choice among the correct answers isdetermined by the speaker's intent in uttering the sen-tence, and is therefore impossible to recover from thecontent words or the sentence structure alone.
At thesame time, many impossible or unlikely case markingdecisions can be eliminated by a case prediction model.Combined with an external component (for example anMT component) that can resolve semantic and inten-tional ambiguity, a case prediction model can be quiteuseful in sentence generation.This paper discusses the task of case marker as-signment in two distinct but related settings.
Afterdefining the task in Section 2 and describing our mod-els in Section 3, we first discuss the monolingual taskin Sections 4, whose goal is to predict the case markers1049using Japanese sentences and their dependency struc-ture alone.
We formulated this task after thewell-studied task of semantic role labeling in English(e.g., Gildea and Jurafsky, 2002; Carreras and M?rques,2005), whose goal is to assign one of 20 semantic rolelabels to each phrase in a sentence with respect to agiven predicate, based on the annotations provided byPropBank (Palmer et al, 2005).
Though the task ofcase marker prediction is more ambiguous and subjectto uncertainty than the semantic role labeling task, weobtained some encouraging results which we present inSection 4.
Next, in Section 5, we describe the bilingualtask, in which information about case assignment canbe extracted from a corresponding source languagesentence.
Though the process of MT introduces uncer-tainties in generating the features we use, we show thatthe benefit of using dependency structure in our mod-els is far greater than not using it even when the as-signed structure is not perfect.2 The task of case predictionIn this section, we define the task of case prediction.We start with the description of the case markers weused in this study.2.1 Nominal particles in JapaneseTraditionally, Japanese nominal postpositions are clas-sified into the following three categories (e.g., Tera-mura, 1991; Masuoka and Takubo, 1992):Case particles (or case markers).
They indicategrammatical relations of the complement NP to thepredicate.
As they are jointly determined by the NPand the predicate, case markers often do not allow asimple mapping to a word in another language, whichmakes their generation more difficult.
The relationshipbetween the case marker and the grammatical relationit indicates is not straightforward either: a case markercan (and often does) indicate multiple grammaticalrelations as in Ainshutain-ni akogareru "idolize Ein-stein" where ni marks the Object relation, and in To-kyo-ni sumu "live in Tokyo" where ni indicates Loca-tion.
Conversely, the same grammatical relation maybe indicated by different case markers: both ni and dein Tokyo-ni sumu "live in Tokyo" and Tokyo-de au"meet in Tokyo" indicate the Location relation.
Weincluded 10 case markers as the primary target of pre-diction, as shown in the first 10 lines of Table 1.Conjunctive particles.
These particles are used toconjoin words and phrases, corresponding to English"and" and "or".
As their occurrence is not predictablefrom the sentence structure alone, we did not includethem in the current prediction task.Focus particles.
These particles add focus to a phraseagainst a given background or contextual knowledge,for example shika and mo in pasuta-shika tabenakatta"ate only pasta" and pasuta-mo tabeta "also ate pasta",corresponding to only and also respectively.
Note thatthey often replace case markers: in the above examples,the object marker wo is no longer present when shikaor mo is used.
As they add information to the predi-cate-argument structure and are in principle not pre-dictable given the sentence structure alone, we did notconsider them as the target of our task.
One exceptionis the topic marker wa, which we included as a targetof prediction for the following reasons: Some linguists recognize wa as a topic marker,separately from other focus particles (e.g.
Masuokaand Takubo, 1992).
The main function of wa is tointroduce a topic in the sentence, which is to a someextent predictable from the structure of the sentence. wa is extremely frequent in Japanese text.
For ex-ample, it accounts for 13.2% of all postpositions inKyoto University Text Corpus (henceforth KyotoCorpus, Kurohashi and Nagao, 1997), making it thethird most frequent postposition after no (20.57%)and wo (13.5%).
Generating wa appropriately thusgreatly enhances the readability of the text. Unlike other focus particles such as shika and mo,wa does not translate into any word in English,which makes it difficult to generate by using the in-formation from the source language.Therefore, in addition to the 10 true case markers, wealso included wa as a case marker in our study.2 Fur-thermore, we also included the combination of caseparticles plus wa as a secondary target of prediction.The case markers that can appear followed by wa areindicated by a check mark in the column "+wa" inTable 1.
Thus there are seven secondary targets: niwa,karawa, towa, dewa, ewa, madewa, yoriwa.
Therefore,we have in total 18 case particles to assign to phrases.2.2 Task definitionThe case prediction task we are solving is as follows.We are given a sentence as a list of bunsetsu together2This set comprises the majority (92.5%) of the nominal parti-cles, while conjunctive and focus particles account for only7.5% of the nominal particles in Kyoto Corpus.Figure 1.
Example of case markers in Japanese (takenfrom the Kyoto Corpus).
Square brackets indicate bun-setsu (phrase) boundaries, to be discussed below.
Ar-rows between phrases indicate dependency relations.1050with a dependency structure.
For our monolingualexperiments, we used the dependency structure annota-tion in the Kyoto Corpus; for our bilingual experiments,we used automatically derived dependency structure(Quirk et al, 2005).
Each bunsetsu (or simply phrasein this paper) is defined as consisting of one contentword (or n-content words in the case of compoundswith n-components) plus any number of functionwords (including particles, auxiliaries and affixes).Case markers are classified as function words, andthere is at most one case marker per phrase.3 In testing,the case marker for each phrase is hidden; the task is toassign to each phrase one of the 18 case markers de-fined above or NONE; NONE indicates that the phrasedoes not have a case marker.2.3 Related workThough the task of case marker prediction as formu-lated in this paper is novel, similar tasks have beendefined in the past.
The semantic role labeling taskmentioned in Section 1 is one example; the task offunction tag assignment in English (e.g., Blaheta andCharniak, 2000) is another.
These tasks are similar tothe case prediction task in that they try to assign se-mantic or function tags to a parsed structure.
However,there is one major difference between these tasks andthe current task: semantic role labels and function tagscan for the most part be uniquely determined given thesentence and its parse structure; decisions about casemarkers, on the other hand, are highly ambiguousgiven the sentence structure alone, as mentioned inSection 1.
This makes our task more ambiguous thanthe related tasks.
As a concrete comparison, the twomost frequent semantic role labels (ARG0 and ARG1)account for 60% of the labeled arguments in PropBank3One exception is that no can appear after certain case markers;in such cases, we considered no to be the case for the phrase.4no is typically not considered as a case marker but rather as aconjunctive particle indicating adnominal relation; however, asno can also be used to indicate the subject in a relative clause,we included it in our study.
(Carreras and M?rquez, 2005), whereas our 2 mostfrequent case markers (no and wo) account for only43% of the case-marked phrases.
We should also notethat semantic role labels and function tags have beenartificially defined in accordance with theoretical deci-sions about what annotations should be useful fornatural language understanding tasks; in contrast, thecase markers are part of the surface sentence string anddo not reflect any theoretical decisions.The task of case prediction in Japanese has previ-ously focused on recovering implicit case relations,which result when noun phrases are relativized ortopicalized (e.g., Baldwin, 2000; Kawahara et al,2004; Murata and Isahara, 2005).
Their goal is differ-ent form ours, as we aim to generate surface forms ofcase markers rather than recover deeper case relationsfor which surface case marker are often used as aproxy.In the context of sentence generation, Gamon et al(2002) used a decision tree to classify nouns into oneof the four cases in German, as part of their sentencerealization from a semantic representation, achievinghigh accuracy (87% to 93.5%).
Again, this is a sub-stantially easier task than ours, because there are onlyfour classes and one of them (nominative) accounts for70% of all cases.
Uchimoto et al (2002), which is thework most related to ours, propose a model of generat-ing function words (not limited to case markers) from"keywords" or headwords of phrases in Japanese.
Thecomponents of their model are based on n-gram lan-guage models using the surface word strings and bun-setsu dependency information, and the results theyreport are not comparable to ours, as they limit theirtest sentences to the ones consisting only of two orthree content words.
We will see in the next sectionthat our models are also quite different from theirs aswe employ a much richer set of features.3 Classifiers for case predictionWe implemented two types of models for the task ofcase prediction: local models, which choose the casemarker of each phrase independently of the case mark-ers of other phrases, and joint models, which incorpo-rate dependencies among the case markers of depend-ents of the same head phrase.
We describe the twotypes of models in turn.3.1 Local classifiersFollowing the standard practice in semantic role label-ing, we divided the case prediction task into the tasksof identification and classification (Gildea and Juraf-sky, 2002; Pradhan et al, 2004).
In the identificationtask, we assign to each phrase one of two labels: HAS-CASE, meaning that the phrase has a case marker, orNONE, meaning that it does not have a case.
In thecase markers grammatical functions (e.g.)
+wa ga subject; objectwo object; path4no genitive; subjectni dative object, location kara source to quotative, reciprocal, as de location, instrument, cause e goal, direction made goal (up to, until) yori source, object of comparison wa topicTable 1.
Case markers included in this study1051classification task, we assign one of the 18 case mark-ers to each phrase that has been labeled with HASCASEby the identification model.We train a binary classifier for identification and amulti-class classifier (with 18 classes) for classification.We obtain a classifier for the complete task by chain-ing the two classifiers.
Let PID(c|b) and  PCLS(c|b)denote the probability of class c for bunsetsu b accord-ing to the identification and classification models, re-spectively.
We define the probability distribution overclasses of the complete model for case assignment asfollows:PCaseAssign(NONE |b) = PID(NONE |b)PCaseAssign(l|b) = PID(HASCASE |b)* PCLS(l|b)Here, l denotes one of the 18 case markers.We employ this decomposition mainly for effi-ciency in training: that is, the decomposition allows usto train the classification models on a subset of trainingexamples consisting only of those phrases that have acase marker, following Toutanova et al (2005).Among various machine learning methods that can beused to train the classifiers, we chose log-linear modelsfor both identification and classification tasks, as theyproduce probability distributions which allows chain-ing of  the two component models and easy integra-tion into an MT system.3.2 Joint classifiersToutanova et al (2005) report a substantial improve-ment in performance on the semantic role labeling taskby building a joint classifier, which takes the labels ofother phrases into account when classifying a givenphrase.
This is motivated by the fact that the argumentstructure is a joint structure, with strong dependenciesamong arguments.
Since the case markers also reflectthe argument structure to some extent, we implementeda joint classifier for the case prediction task as well.We applied the joint classifiers in the framework ofN-best reranking (Collins, 2000), following Toutanovaet al (2005).
That is, we produced N-best (N=5 in ourexperiments) case assignment sequence candidates fora set of sister phrases using the local models, andtrained a joint classifier that learns to choose the bestcandidate from the set of sisters.
The oracle accuracyof the 5-best candidate list was 95.9% per phrase.4 Monolingual case prediction taskIn this section we describe our models trained andevaluated using the gold-standard dependency annota-tions provided by the Kyoto Corpus.
These annotationsallow us to define a rich set of features exploring thesyntactic structure.4.1 FeaturesThe basic local model features we used for the identi-fication and classification models are listed in Table 2.They consist of features for a phrase, for its parentphrase and for their relations.
Only one feature(GrandparentNounSubPos) currently refers to thegrandparent of the phrase; all other features are be-tween the phrase, its parent and its sibling nodes, andare a superset of the dependency-based features usedby Hacioglu (2004) for the semantic labeling task.
Inaddition to these basic features, we added 20 combinedfeatures, some of which are shown at the bottom ofTable 2.For the joint model, we implemented only twotypes of features: sequence of non-NONE case markersfor a set of sister phrases, and repetition of non-NONEcase markers.
These features are intended to captureregularities in the sequence of case markers of phrasesthat modify the same head phrase.All of these features are represented as binary fea-tures: that is, when the value of a feature is not binary,we have treated the combination of the feature nameplus the value as a unique feature.
With a count cut-offof 2 (i.e., features must occur at least twice to be in themodel), we have 724,264 features in the identificationBasic features for phrases (self, parent)HeadPOS, PrevHeadPOS, NextHeadPOSPrevPOS,Prev2POS,NextPOS,Next2POSHeadNounSubPos: time, formal nouns, adverbialHeadLemmaHeadWord, PrevHeadWord, NextHeadWordPrevWord, Prev2Word, NextWord, Next2WordLastWordLemma (excluding case markers)LastWordInfl (excluding case markers)IsFiniteClauseIsDateExpressionIsNumberExpressionHasPredicateNominalHasNominalizerHasPunctuation: comma, periodHasFiniteClausalModifierRelativePosition: sole, first, mid, lastNSiblings (number of siblings)Position (absolute position among siblings)Voice: pass, caus, passcausNegationBasic features for phrase relations (parent-child pair)DependencyType: D,P,A,IDistance: linear distance in bunsetsu, 1, 2-5, >6Subcat: POS tag of parent + POS tag of all children +indication for currentCombined features (selected)HeadPOS + HeadLemmaParentLemma + HeadLemmaPosition + NSiblingsIsFiniteClause + GrandparentNounSubPosTable 2: Basic and combined features for local classifiers1052model, and 3,963,096 features in the classificationmodel.
The number of joint features in the joint modelis 3,808.
All models are trained using a Gaussian prior.4.2 Data and baselinesWe divided the Kyoto Corpus (version 3.0) into thefollowing three sections: Training: contains news articles of January 1, 3-11and editorial articles of January-August; 24,263sentences, 234,474 phrases. Devtest: contains news articles of January 12-13 andeditorial article of September.
4,833 sentences,47,580 phrases. Test: contains news articles of January 14-17 andeditorial articles of October-December.
9,287 sen-tences, 89,982 phrases.The devtest set was used only for tuning model pa-rameters and for performing error analysis.As no previous work exists on the task of predictingcase markers on the Kyoto Corpus, it is important toestablish a good baseline.
The simplest baseline ofalways selecting the most frequent label (NONE) givesus an accuracy of 47.5% on the test set.
Out of thenon-NONE case markers, the most frequent is no,which occurs in 26.6% of all case-marked phrases.A more reasonable baseline is to use a languagemodel to predict case.
We trained and tested two lan-guage models: the first model, called KCLM, is trainedon the same data as our log-linear models (24,263 sen-tences); the second model, called BigCLM, is trainedon much more data from the same domain (826,373sentences), taking advantage of the fact that languagemodels do not require dependency annotation fortraining.
The language models were trained using theCMU language modeling toolkit with default parame-ter settings (Clarkson and Rosenfeld, 1997).We tested the language model baselines using thesame task set-up as for our classifier: for each phrase,each of the 18 possible case markers and NONE isevaluated.
The position for insertion of a case markerin each phrase is given according to our task set-up, i.e.,at the end of a phrase preceding any punctuation.
Wechoose the case assignment of the sequence of phrasesin the sentence that maximizes the language modelprobability of the resulting sentence.
We computed themost likely case assignment sequence using a dynamicprogramming algorithm.4.3 Results and discussionThe results of running our models on case marker pre-diction are shown in Table 3.
The first three rows cor-respond to the components of the local model: theidentification task (Id, for all phrases), the classifica-tion task (Cls, only for case-marked phrases) and thecomplete task (Both, for all phrases).
The accuracy onthe complete task using the local model is 83.9%; thejoint model improves it to 84.3%.The improvement due to the joint model is small inabsolute percentage points (0.4%), but is statisticallysignificant according to a test for the difference ofproportions (p< 0.05).
The use of a joint classifier didnot lead to as large an improvement over the localclassifier as for the semantic role labeling task.
Thereare several reasons for that we can think of.
First, wehave only used a limited set of features for the jointmodel, i.e., case sequence and repetition features.
Amore extensive use of global features might lead to alarger improvement.
Secondly, unlike the task of se-mantic role labeling, where there are about 20 phrasesthat need to be labeled with respect to a predicate,about 50% of all phrases in the Kyoto Corpus do nothave sister nodes.
This means that these phrases cannottake advantage of the joint classifier using the currentmodel formulation.
Finally, case markers are muchshallower than semantic role labels in the level of lin-guistic analysis, and so are inherently subject to morevariations, including missing arguments (so called zeropronouns) and repeated case markers corresponding todifferent semantic roles.From Table 3, it is clear that our models outperformthe baseline model significantly.
The language modeltrained on the same data has much lower performance(67.0% vs. 84.3%), which shows that our system isexploiting the training data much more efficiently bylooking at the dependency and other syntactic features.An inspection of the 500 most highly weighted featuresalso indicates that phrase dependency-based featuresare very useful for both identification and classification.Given much more data, though, the language modelimproves significantly to 78%, but our classifier stillachieves a 29% error reduction over it.
The differencesbetween the language models and the log-linear modelsare statistically significant at level p < 0.01 accordingto a test for the difference of proportions.Figure 2 plots the recall and precision for the fre-quently occurring (>500) cases.
We achieve good re-sults on NONE and no, which are the least ambiguousdecisions.
Cases such as ni, wa, ga, and de are highlyconfusable with other markers as they indicate multiplegrammatical relations, and the performance of ourModels Task Training  Testlog-linear Id 99.8 96.9log-linear Cls 96.6 74.3log-linear (local) Both 98.0 83.9log-linear( joint) Both 97.8 84.3baseline (frequency) Both 48.2 47.5baseline (KCLM) Both 93.9 67.0baseline (BigCLM) Both ?
78.0Table 3: Accuracy of case prediction models (%)1053models on them is therefore limited.
As expected, per-formance (especially recall) on secondary targets(dewa, niwa) suffers greatly due to the ambiguity withtheir primary targets.5 Bilingual case prediction task: simulatingcase prediction in MTIncorporating a case prediction model into MT requirestaking additional factors into consideration, comparedto the monolingual task described above.
On the onehand, we need to extend our model to handle the addi-tional knowledge source, i.e., the source sentence.
Thiscan potentially provide very useful features to ourmodel, which are not available in the monolingual task.On the other hand, since gold-standard dependencyannotation is not available in the MT context, we mustdeal with the imperfections in structural annotations.In this section, we describe our case predictionmodels in the context of English-to-Japanese MT.
Inthis setting, dependency information for the targetlanguage (Japanese) is available only through projec-tion of a dependency structure from the source lan-guage (English) in a tree-to-string-based statistical MTsystem (Quirk et al, 2005).
We conducted experimentsusing the English source sentences and the referencetranslations in Japanese: that is, our task is to predictthe case markers of the Japanese reference translationscorrectly using all other words in the reference sen-tence, information from the source sentence throughword alignment, and the Japanese dependency struc-ture projected via an MT component.
Ultimately, ourgoal is to improve the case marker assignment of acandidate translation using a case prediction model; theexperiments described in this section on referencetranslations serve as an important preliminary steptoward achieving that final goal.
We will show in thissection that even the automatically derived syntacticinformation is very useful in assigning case markers inthe target language, and that utilizing the informationfrom the source language also greatly contributes toreducing case marking errors.5.1 Data and task set-upThe dataset we used is a collection of parallel Eng-lish-Japanese sentences from a technical (computer)domain.
We used 15,000 sentence pairs for training,5,000 for development, and 4,241 for testing.The parallel sentences were word-aligned usingGIZA++ (Och and Ney, 2000), and submitted to atree-to-string-based MT system (Quirk et al, 2005)which utilizes the dependency structure of the sourcelanguage and projects dependency structure to thetarget language.
Figure 3 shows an example of analigned sentence pair: on the source (English) side,part-of-speech (POS) tags and word dependencystructure are assigned (solid arcs).
The alignmentsbetween English and Japanese words are indicated bythe dotted lines.
In order to create phrase-level de-pendency structures like the ones utilized in the KyotoCorpus monolingual task, we derived some additionalinformation for the Japanese sentence in the followingmanner.Figure 3.
Aligned English-Japanese sentence pairFirst, we tagged the sentence using an automatictagger with a set of 19 POS tags.
We used these POStags to parse the words into phrases (bunsetsu): eachbunsetsu consists of one content word plus any numberof function words, where content and function wordsare defined via POS.
We then constructed aphrase-level dependency structure using a breadth-firsttraversal of the word dependency structure projectedfrom English.
These phrase dependencies are indicatedby bold arcs in Figure 3.
The case markers to be pre-dicted (wa and de in this case) are underlined.The task of case marker prediction is the same asdescribed in Section 2: to assign one of the 18 casemarkers described in Section 2 or NONE to each phrase.5.2 Baseline modelsWe implemented the baseline models discussed inSection 4.2 for this domain as well.
The most frequent0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1niwa (523)dewa (548)kara (868)de (2582)to (3664)ga (5797)wa (5937)ni (6457)wo (7782)no (12570)NONE (42756)precisionrecallFigure 2: Precision and recall per case marker (frequencyin parentheses)1054case assignment is again NONE, which accounts for62.0% of the test set.
The frequency of NONE is higherin this task than in the Kyoto Corpus, because ourbunsetsu-parsing algorithm prefers to err on the side ofmaking too many rather than too few phrases.
This isbecause our final goal is to generate all case markers,and if we mistakenly joined two bunsetsu into one, ourcase assigner would be able to propose only one casemarker for the resulting bunsetsu, which would benecessarily wrong if both bunsetsu had case markers.The most frequent case marker is again no, which oc-curs in 29.4% of all case-marked phrases.
As in themonolingual task, we trained two trigram languagemodels: one was trained on the training set of our caseprediction models (15,000 sentences); another wastrained on a much larger set of 450,000 sentences fromthe same domain.
The results of these baselines arediscussed in Section 5.4.5.3 Log-linear modelsThe models we built for this task are log-linear modelsas described in Section 3.
In order to isolate the impactof information from the source language available forthe case prediction task, we built two kinds of models:monolingual models, which do not use any informationfrom the source English sentences, and bilingual mod-els, which use information from the source.
Both mod-els are local models in the sense discussed in Section 3.Table 4 shows the features used in the monolingualand bilingual models, along with the examples (thevalue of the feature for the phrase [saabisu wa] in Fig-ure 3); in addition to these, we also provided somefeature combinations for both monolingual and bilin-gual models.
Many of the monolingual features (i.e.,first 11 lines in Table 4) are also present in Table 2.Note that lexically based features are of greater impor-tance for this task, as the dependency informationavailable in this context is of much poorer quality thanthat provided by the Kyoto Corpus.
In addition to thefeatures in Table 2, we added a Direction feature (withvalues left and right), and an Alternative Parent feature.Alternative parents are all words which are the parentsof any word in the phrase, according to the word-baseddependency tree, with the constraint that case markerscannot be alternative parents.
This feature captures theinformation that is potentially lost in the process ofbuilding a phrase dependency structure from worddependency information in the target language.The bottom half of Table 4 shows bilingual features.The features of the source sentence are obtainedthrough word alignments.
We create features from thesource words aligned to the head of the phrase, to thehead of the parent phrase, or to any alterative parents.If any word in the phrase is aligned to a preposition inthe source language, our model can use the informationas well.
In addition to word- and POS-features foraligned source words, we also refer to the correspond-ing dependency between the phrase and its parentphrase in the English source.
If the head of the Japa-nese phrase is aligned to a single source word s1, andthe head of its parent phrase is aligned to a singlesource word s2, we extract the relationship between s1and s2, and define subcategorization, direction, distance,and number of siblings features, in order to capture thegrammatical relation in the source, which is more reli-able than in the projected target dependency structure.5.4 Results and discussionTable 5 summarizes the results on the complete caseassignment task in the MT context.
Compared to thelanguage model trained on the same data (15kLM), ourMonolingual featuresFeature ExampleHeadWord /HeadPOS saabisu/NNPrevWord/PrevPOS kono/ANDPrev2Word/Prev2WordPOS none/noneNextWord/NextPOS seefu/NNNext2Word/Net2POS moodo/NNPrevHeadWord/PrevHeadPOS kono/ANDNextHeadWord/NextHeadPOS seefu/NNParentHeadWord/ParentHeadPOS kaishi/VNSubcat: POS tags of all sisters and parent NN-c,NN,VN-hNSiblings (including self) 2Distance 1Direction leftAlternative Parent Word /POS saabisu/NNBilingual featuresFeature ExampleWord/POS of source words aligned to thehead of the phraseservice/NNWord/POS of all source words aligned toany word in the phraseservice/NNWord/POS of all source words aligned tothe head word of the parent phrasestarted/VERBWord/POS of all source words aligned toalternative parent words of the phraseservice/NN,started/VERBAll source preposition words inWord/POS of parent of source word alignedto any word in the phrasestarted/VERBAligned Subcat          NN-c,VERB,VERB,VERB-h,PREPAligned NSiblings 4Aligned Distance 2Aligned Direction leftTable 4: Monolingual and bilingual featuresModel Test databaseline (frequency) 62.0baseline (15kLM) 79.0baseline (450kLM) 83.6log-linear monolingual 85.3log-linear bilingual 92.3Table 5: Accuracy of bilingual case prediction (%)1055monolingual model performs significantly better,achieving a 30% error reduction (85.3% vs. 79.0%).Our monolingual model outperforms even the languagemodel trained on 30 times more data (85.3% vs.83.6%), with an error reduction of 10%.
The differenceis statistically significant at level p < 0.01 according toa test for the difference of proportions.
This means thateven though the projected dependency information isnot perfect, it is still useful for the case prediction task.When we add the bilingual features, the error rate ofour model is cut almost in half: the bilingual modelachieves an error reduction of 48% over the monolin-gual model (92.3% vs. 85.3%, statistically significantat level p < 0.01).
This result is very encouraging: itindicates that information from the source sentence canbe exploited very effectively to improve the accuracyof case assignment.
The usefulness of the source lan-guage information is also obvious when we inspectwhich case markers had the largest gains in accuracydue to this information: the top three cases were kara(0.28 to 0.65, a 57% gain), dewa (0.44 to 0.65, a 32%gain) and to (0.64 to 0.85, a 24% gain), all of whichhave translations as English prepositions.
Markers suchas ga (subject marker, 0.68 to 0.74, a 8% gain) and wo(object marker, 0.83 to 0.86, a 3.5% gain), on the otherhand, showed only a limited gain.6 Conclusion and future directionsThis paper described the task of predicting case mark-ers in Japanese, and reported results in a monolingualand a bilingual settings.
The results show that the mod-els we proposed, which explore syntax-based featuresand features from the source language in the bilingualtask, can effectively predict case markers.There are a number of extensions and next steps wecan think of at this point, the most immediate and im-portant one of which is to incorporate the proposedmodel in an end-to-end MT system to make improve-ments in the output of MT.
We would also like to per-form a more extensive analysis of features and featureablation experiments.
Finally, we would also like toextend the proposed model to include languages withinflectional morphology and the prediction of gram-matical elements in general.AcknowledgementsWe would like to thank the anonymous reviewers fortheir comments, and Bob Moore, Arul Menezes, ChrisQuirk, and Lucy Vanderwende for helpful discussions.ReferencesBaldwin, T. 2004.
Making Sense of Japanese RelativeClause Constructions, In Proceedings of the 2ndWorkshop on Text Meaning and Interpretation.Blaheta, D. and E. Charniak.
2000.
Assigning functiontags to parsed text.
In Proceedings of NAACL,pp.234-240.Carreras, X. and L. M?rquez.
2005.
Introduction to theCoNLL-2005 Shared Task: Semantic Role Labeling.
InProceedings of CoNLL-2005.Clarkson, P.R.
and R. Rosenfeld.
1997.
Statistical Lan-guage Modeling Using the CMU-Cambridge Toolkit.In Proceedings of ESCA Eurospeech, pp.
2007-2010.Collins, M. 2000.
Discriminative reranking for naturallanguage parsing.
In Proceedings of ICML.Gamon, M., E. Ringger, S. Corston-Oliver and R. Moore.2002.
Machine-learned Context for Linguistic Opera-tions in German Sentence Realization.
In Proceedingof ACL.Gildea, D. and D. Jurafsky.
2002.
Automatic Labeling ofSemantic Roles.
In Computational Linguistics 28(3):245-288.Hacioglu, K. 2004.
Semantic Role Labeling using De-pendency Trees.
In Proceedings of COLING 2004.Kawahara, D., N. Kaji and S. Kurohashi.
2000.
JapaneseCase Structure Analysis by Unsupervised Constructionof a Case Frame Dictionary.
In Proceedings of COL-ING, pp.
432-438.Kurohashi, S. and M.Nagao.
1997.
Kyoto University TextCorpus Project.
In Proceedings of ANLP, pp.115-118.Masuoka, T. and Y. Takubo.
1992.
Kiso Nihongo Bunpou(Fundamental Japanese grammar), revised version.Kuroshio Shuppan, Tokyo.Murata, M., and H. Isahara.
2005.
Japanese Case AnalysisBased on Machine Learning Method that Uses Bor-rowed Supervised Data.
In Proceedings of IEEENLP-KE-2005, pp.774-779.Och, F.J. and H. Ney.
2000.
Improved statistical align-ment models.
In Proceedings of ACL: pp.440-447.Palmer, M., D. Gildea and P. Kingsbury.
2005.
TheProposition Bank: An Annotated Corpus of SemanticRoles.
In Computational Linguistics 31(1).Pradhan, S., W. Ward, K. Hacioglu, L. Martin, D. Juraf-sky.
2004.
Shallow Semantic Parsing Using SupportVector Machines.
In Proceedings of HLT/NAACL.Quirk, C., A. Menezes and C. Cherry.
2005.
DependencyTree Translation: Syntactically Informed Phrasal SMT.
InProceedings of ACL.Teramura, H. 1991.
Nihongo-no shintakusu-to imi (Japa-nese syntax and meaning).
Volume III.
KuroshioShuppan, Tokyo.Toutanova, K., A. Haghighi and C. D. Manning.
2005.Joint Learning Improves Semantic Role Labeling.
InProceeding of ACL, pp.589-596.Uchimoto, K., S. Sekine and H. Isahara.
2002.
Text Gen-eration from Keywords.
In Proceedings of COLING2002, pp.1037-1043.1056
