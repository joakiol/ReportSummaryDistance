Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1098?1108,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLearning From Collective Human Behavior toIntroduce Diversity in Lexical ChoiceVahed QazvinianDepartment of EECSUniversity of MichiganAnn Arbor, MIvahed@umich.eduDragomir R. RadevSchool of InformationDepartment of EECSUniversity of MichiganAnn Arbor, MIradev@umich.eduAbstractWe analyze collective discourse, a collectivehuman behavior in content generation, andshow that it exhibits diversity, a property ofgeneral collective systems.
Using extensiveanalysis, we propose a novel paradigm for de-signing summary generation systems that re-flect the diversity of perspectives seen in real-life collective summarization.
We analyze 50sets of summaries written by human about thesame story or artifact and investigate the diver-sity of perspectives across these summaries.We show how different summaries use vari-ous phrasal information units (i.e., nuggets) toexpress the same atomic semantic units, calledfactoids.
Finally, we present a ranker that em-ploys distributional similarities to build a net-work of words, and captures the diversity ofperspectives by detecting communities in thisnetwork.
Our experiments show how our sys-tem outperforms a wide range of other docu-ment ranking systems that leverage diversity.1 IntroductionIn sociology, the term collective behavior is used todenote mass activities that are not centrally coordi-nated (Blumer, 1951).
Collective behavior is dif-ferent from group behavior in the following ways:(a) it involves limited social interaction, (b) mem-bership is fluid, and (c) it generates weak and un-conventional norms (Smelser, 1963).
In this paper,we focus on the computational analysis of collectivediscourse, a collective behavior seen in interactivecontent contribution and text summarization in on-line social media.
In collective discourse each in-dividual?s behavior is largely independent of that ofother individuals.In social media, discourse (Grosz and Sidner,1986) is often a collective reaction to an event.
Onescenario leading to collective reaction to a well-defined subject is when an event occurs (a movie isreleased, a story occurs, a paper is published) andpeople independently write about it (movie reviews,news headlines, citation sentences).
This process ofcontent generation happens over time, and each per-son chooses the aspects to cover.
Each event hasan onset and a time of death after which nothing iswritten about it.
Tracing the generation of contentover many instances will reveal temporal patternsthat will allow us to make sense of the text gener-ated around a particular event.To understand collective discourse, we are inter-ested in behavior that happens over a short periodof time.
We focus on topics that are relatively well-defined in scope such as a particular event or a singlenews event that does not evolve over time.
This caneventually be extended to events and issues that areevolving either in time or scope such as elections,wars, or the economy.In social sciences and the study of complex sys-tems a lot of work has been done to study such col-lective systems, and their properties such as self-organization (Page, 2007) and diversity (Hong andPage, 2009; Fisher, 2009).
However, there is littlework that studies a collective system in which mem-bers individually write summaries.In most of this paper, we will be concerned withdeveloping a complex systems view of the set of col-lectively written summaries, and give evidence of1098the diversity of perspectives and its cause.
We be-lieve that out experiments will give insight into newmodels of text generation, which is aimed at model-ing the process of producing natural language texts,and is best characterized as the process of mak-ing choices between alternate linguistic realizations,also known as lexical choice (Elhadad, 1995; Barzi-lay and Lee, 2002; Stede, 1995).2 Prior WorkIn summarization, a number of previous methodshave focused on diversity.
(Mei et al, 2010) in-troduce a diversity-focused ranking methodologybased on reinforced random walks in informationnetworks.
Their random walk model introduces therich-gets-richer mechanism to PageRank with rein-forcements on transition probabilities between ver-tices.
A similar ranking model is the Grasshopperranking model (Zhu et al, 2007), which leveragesan absorbing random walk.
This model starts witha regular time-homogeneous random walk, and ineach step the node with the highest weight is setas an absorbing state.
The multi-view point sum-marization of opinionated text is discussed in (Paulet al, 2010).
Paul et al introduce Compar-ative LexRank, based on the LexRank rankingmodel (Erkan and Radev, 2004).
Their random walkformulation is to score sentences and pairs of sen-tences from opposite viewpoints (clusters) based onboth their representativeness of the collection as wellas their contrastiveness with each other.
Once a lex-ical similarity graph is built, they modify the graphbased on cluster information and perform LexRankon the modified cosine similarity graph.The most well-known paper that address diver-sity in summarization is (Carbonell and Goldstein,1998), which introduces Maximal Marginal Rele-vance (MMR).
This method is based on a greedyalgorithm that picks sentences in each step that arethe least similar to the summary so far.
There area few other diversity-focused summarization sys-tems like C-LexRank (Qazvinian and Radev, 2008),which employs document clustering.
These paperstry to increase diversity in summarizing documents,but do not explain the type of the diversity in their in-puts.
In this paper, we give an insightful discussionon the nature of the diversity seen in collective dis-course, and will explain why some of the mentionedmethods may not work under such environments.In prior work on evaluating independent contri-butions in content generation, Voorhees (Voorhees,1998) studied IR systems and showed that rele-vance judgments differ significantly between hu-mans but relative rankings show high degrees of sta-bility across annotators.
However, perhaps the clos-est work to this paper is (van Halteren and Teufel,2004) in which 40 Dutch students and 10 NLP re-searchers were asked to summarize a BBC news re-port, resulting in 50 different summaries.
Teufeland van Halteren also used 6 DUC1-provided sum-maries, and annotations from 10 student participantsand 4 additional researchers, to create 20 summariesfor another news article in the DUC datasets.
Theycalculated the Kappa statistic (Carletta, 1996; Krip-pendorff, 1980) and observed high agreement, indi-cating that the task of atomic semantic unit (factoid)extraction can be robustly performed in naturally oc-curring text, without any copy-editing.The diversity of perspectives and the unprece-dented growth of the factoid inventory also affectsevaluation in text summarization.
Evaluation meth-ods are either extrinsic, in which the summaries areevaluated based on their quality in performing a spe-cific task (Spa?rck-Jones, 1999) or intrinsic where thequality of the summary itself is evaluated, regardlessof any applied task (van Halteren and Teufel, 2003;Nenkova and Passonneau, 2004).
These evaluationmethods assess the information content in the sum-maries that are generated automatically.Finally, recent research on analyzing online so-cial media shown a growing interest in mining newsstories and headlines because of its broad appli-cations ranging from ?meme?
tracking and spikedetection (Leskovec et al, 2009) to text summa-rization (Barzilay and McKeown, 2005).
In sim-ilar work on blogs, it is shown that detecting top-ics (Kumar et al, 2003; Adar et al, 2007) and sen-timent (Pang and Lee, 2004) in the blogosphere canhelp identify influential bloggers (Adar et al, 2004;Java et al, 2006) and mine opinions about prod-ucts (Mishne and Glance, 2006).1Document Understanding Conference10993 Data AnnotationThe datasets used in our experiments represent twocompletely different categories: news headlines, andscientific citation sentences.
The headlines datasetsconsist of 25 clusters of news headlines collectedfrom Google News2, and the citations datasets have25 clusters of citations to specific scientific papersfrom the ACL Anthology Network (AAN)3.
Eachcluster consists of a number of unique summaries(headlines or citations) about the same artifact (non-evolving news story or scientific paper) written bydifferent people.
Table 1 lists some of the clusterswith the number of summaries in them.ID type Name Story/Title #1 hdl miss Miss Venezuela wins miss universe?09 1252 hdl typhoon Second typhoon hit philippines 1003 hdl russian Accident at Russian hydro-plant 1014 hdl redsox Boston Red Sox win world series 995 hdl gervais ?Invention of Lying?
movie reviewed 97?
?
?
?
?
?
?
?
?25 hdl yale Yale lab tech in court 1026 cit N03-1017 Statistical Phrase-Based Translation 17227 cit P02-1006 Learning Surface Text Patterns ... 7228 cit P05-1012 On-line Large-Margin Training ... 7129 cit C96-1058 Three New Probabilistic Models ... 6630 cit P05-1033 A Hierarchical Phrase-Based Model ... 65?
?
?
?
?
?
?
?
?50 cit H05-1047 A Semantic Approach to Recognizing ... 7Table 1: Some of the annotated datasets and the numberof summaries in each of them (hdl = headlines; cit = cita-tions)3.1 Nuggets vs. FactoidsWe define an annotation task that requires explicitdefinitions that distinguish between phrases that rep-resent the same or different information units.
Un-fortunately, there is little consensus in the literatureon such definitions.
Therefore, we follow (van Hal-teren and Teufel, 2003) and make the following dis-tinction.
We define a nugget to be a phrasal infor-mation unit.
Different nuggets may all representthe same atomic semantic unit, which we call as afactoid.
In the following headlines, which are ran-domly extracted from the redsox dataset, nuggetsare manually underlined.red sox win 2007 world seriesboston red sox blank rockies to clinch world series2news.google.com3http://clair.si.umich.edu/clair/anthology/boston fans celebrate world series win; 37 arrests re-portedThese 3 headlines contain 9 nuggets, which rep-resent 5 factoids or classes of equivalent nuggets.f1 : {red sox, boston, boston red sox}f2 : {2007 world series, world series win, world series}f3 : {rockies}f4 : {37 arrests}f5 : {fans celebrate}This example suggests that different headlines onthe same story written independently of one an-other use different phrases (nuggets) to refer to thesame semantic unit (e.g., ?red sox?
vs. ?boston?
vs.?boston red sox?)
or to semantic units correspondingto different aspects of the story (e.g., ?37 arrests?
vs.?rockies?).
In the former case different nuggets areused to represent the same factoid, while in the lattercase different nuggets are used to express differentfactoids.
This analogy is similar to the definition offactoids in (van Halteren and Teufel, 2004).The following citation sentences to Koehn?s worksuggest that a similar phenomenon also happens incitations.We also compared our model with pharaoh (Koehn et al2003).Koehn et al(2003) find thatphrases longer than three words improve per-formance little.Koehn et al(2003) suggest limiting phrase lengthto three words or less.For further information on these parameter settings,confer (koehn et al 2003).where the first author mentions ?pharaoh?
as acontribution of Koehn et al but the second and thirduse different nuggets to represent the same contribu-tion: use of trigrams.
However, as the last citationshows, a citation sentence, unlike news headlines,may cover no information about the target paper.The use of phrasal information as nuggets is an es-sential element to our experiments, since some head-line writers often try to use uncommon terms to re-fer to a factoid.
For instance, two headlines from theredsox cluster are:Short wait for bossox this timeSoxcess started upstairs1100Following these examples, we asked two anno-tators to annotate all 1, 390 headlines, and 926 ci-tations.
The annotators were asked to follow pre-cise guidelines in nugget extraction.
Our guidelinesinstructed annotators to extract non-overlappingphrases from each headline as nuggets.
Therefore,each nugget should be a substring of the headlinethat represents a semantic unit4.Previously (Lin and Hovy, 2002) had shown thatinformation overlap judgment is a difficult task forhuman annotators.
To avoid such a difficulty, weenforced our annotators to extract non-overlappingnuggets from a summary to make sure that they aremutually independent and that information overlapbetween them is minimized.Finding agreement between annotated well-defined nuggets is straightforward and can be cal-culated in terms of Kappa.
However, when nuggetsthemselves are to be extracted by annotators, thetask becomes less obvious.
To calculate the agree-ment, we annotated 10 randomly selected head-line clusters twice and designed a simple evalua-tion scheme based on Kappa5.
For each n-gram,w, in a given headline, we look if w is part of anynugget in either human annotations.
If w occursin both or neither, then the two annotators agreeon it, and otherwise they do not.
Based on thisagreement setup, we can formalize the ?
statisticas ?
= Pr(a)?Pr(e)1?Pr(e) where Pr(a) is the relative ob-served agreement among annotators, and Pr(e) isthe probability that annotators agree by chance ifeach annotator is randomly assigning categories.Table 2 shows the unigram, bigram, and trigram-based average ?
between the two human annotators(Human1, Human2).
These results suggest thathuman annotators can reach substantial agreementwhen bigram and trigram nuggets are examined, andhas reasonable agreement for unigram nuggets.4 DiversityWe study the diversity of ways with which humansummarizers talk about the same story or event andexplain why such a diversity exists.4Before the annotations, we lower-cased all summaries andremoved duplicates5Previously (Qazvinian and Radev, 2010) have shown highagreement in human judgments in a similar task on citation an-notationAverage ?unigram bigram trigramHuman1 vs. Human20.76?
0.4 0.80?
0.4 0.89?
0.3Table 2: Agreement between different annotators in termsof average Kappa in 25 headline clusters.100 101 10210?210?1100Pr(X ?c)cheadlinesPr(X ?
c)100 101 10210?210?1100Pr(X ?c)ccitationsPr(X ?
c)Figure 1: The cumulative probability distribution for thefrequency of factoids (i.e., the probability that a factoidwill be mentioned in c different summaries) across ineach category.4.1 Skewed DistributionsOur first experiment is to analyze the popularity ofdifferent factoids.
For each factoid in the annotatedclusters, we extract its count, X , which is equal tothe number of summaries it has been mentioned in,and then we look at the distribution of X .
Fig-ure 1 shows the cumulative probability distributionfor these counts (i.e., the probability that a factoidwill be mentioned in at least c different summaries)in both categories.These highly skewed distributions indicate that alarge number of factoids (more than 28%) are onlymentioned once across different clusters (e.g., ?poorpitching of colorado?
in the redsox cluster), andthat a few factoids are mentioned in a large numberof headlines (likely using different nuggets).
Thelarge number of factoids that are only mentioned inone headline indicates that different summarizers in-crease diversity by focusing on different aspects ofa story or a paper.
The set of nuggets also exhibitsimilar skewed distributions.
If we look at individ-ual nuggets, the redsox set shows that about 63(or 80%) of the nuggets get mentioned in only oneheadline, resulting in a right-skewed distribution.The factoid analysis of the datasets reveals twomain causes for the content diversity seen in head-lines: (1) writers focus on different aspects of thestory and therefore write about different factoids1101(e.g., ?celebrations?
vs. ?poor pitching of col-orado?).
(2) writer use different nuggets to representthe same factoid (e.g., ?redsox?
vs.
?bosox?).
In thefollowing sections we analyze the extent at whicheach scenario happens.100 101 102 10302004006008001000number of summariesInventory sizeheadlinesNuggetsFactoids100 101 102 103050100150200250300350number of summariesInventory sizecitationsNuggetsFactoidsFigure 2: The number of unique factoids and nuggets ob-served by reading n random summaries in all the clustersof each category4.2 Factoid InventoryThe emergence of diversity in covering different fac-toids suggests that looking at more summaries willcapture a larger number of factoids.
In order to ana-lyze the growth of the factoid inventory, we performa simple experiment.
We shuffle the set of sum-maries from all 25 clusters in each category, and thenlook at the number of unique factoids and nuggetsseen after reading nth summary.
This number showsthe amount of information that a randomly selectedsubset of n writers represent.
This is important tostudy in order to find out whether we need a largenumber of summaries to capture all aspects of astory and build a complete factoid inventory.
Theplot in Figure 4.1 shows, at each n, the number ofunique factoids and nuggets observed by reading nrandom summaries from the 25 clusters in each cat-egory.
These curves are plotted on a semi-log scaleto emphasize the difference between the growth pat-terns of the nugget inventories and the factoid inven-tories6.This finding numerically confirms a similar ob-servation on human summary annotations discussedin (van Halteren and Teufel, 2003; van Halterenand Teufel, 2004).
In their work, van Halteren andTeufel indicated that more than 10-20 human sum-maries are needed for a full factoid inventory.
How-ever, our experiments with nuggets of nearly 2, 400independent human summaries suggest that neitherthe nugget inventory nor the number of factoids willbe likely to show asymptotic behavior.
However,these plots show that the nugget inventory grows ata much faster rate than factoids.
This means that alot of the diversity seen in human summarization isa result of the so called different lexical choices thatrepresent the same semantic units or factoids.4.3 Summary QualityIn previous sections we gave evidence for the diver-sity seen in human summaries.
However, a moreimportant question to answer is whether these sum-maries all cover important aspects of the story.
Here,we examine the quality of these summaries, studythe distribution of information coverage in them,and investigate the number of summaries requiredto build a complete factoid inventory.The information covered in each summary can bedetermined by the set of factoids (and not nuggets)and their frequencies across the datasets.
For exam-ple, in the redsox dataset, ?red sox?, ?boston?, and?boston red sox?
are nuggets that all represent thesame piece of information: the red sox team.
There-fore, different summaries that use these nuggets torefer to the red sox team should not be seen as verydifferent.We use the Pyramid model (Nenkova and Pas-sonneau, 2004) to value different summary factoids.Intuitively, factoids that are mentioned more fre-quently are more salient aspects of the story.
There-fore, our pyramid model uses the normalized fre-quency at which a factoid is mentioned across adataset as its weight.
In the pyramid model, the in-dividual factoids fall in tiers.
If a factoid appears inmore summaries, it falls in a higher tier.
In princi-ple, if the term wi appears |wi| times in the set of6Similar experiment using individual clusters exhibit similarbehavior1102headlines it is assigned to the tier T|wi|.
The pyra-mid score that we use is computed as follows.
Sup-pose the pyramid has n tiers, Ti, where tier Tn isthe top tier and T1 is the bottom.
The weight ofthe factoids in tier Ti will be i (i.e.
they appearedin i summaries).
If |Ti| denotes the number of fac-toids in tier Ti, and Di is the number of factoids inthe summary that appear in Ti, then the total factoidweight for the summary is D =?ni=1 i ?
Di.
Ad-ditionally, the optimal pyramid score for a summaryis Max =?ni=1 i?
|Ti|.
Finally, the pyramid scorefor a summary can be calculated asP =DMaxBased on this scoring scheme, we can use the an-notated datasets to determine the quality of individ-ual headlines.
First, for each set we look at the vari-ation in pyramid scores that individual summariesobtain in their set.
Figure 3 shows, for each clus-ter, the variation in the pyramid scores (25th to 75thpercentile range) of individual summaries evaluatedagainst the factoids of that cluster.
This figure in-dicates that the pyramid score of almost all sum-maries obtain values with high variations in most ofthe clusters For instance, individual headlines fromredsox obtain pyramid scores as low as 0.00 andas high as 0.93.
This high variation confirms the pre-vious observations on diversity of information cov-erage in different summaries.Additionally, this figure shows that headlines gen-erally obtain higher values than citations when con-sidered as summaries.
One reason, as explained be-fore, is that a citation may not cover any importantcontribution of the paper it is citing, when headlinesgenerally tend to cover some aspects of the story.High variation in quality means that in order tocapture a larger information content we need to reada greater number of summaries.
But how manyheadlines should one read to capture a desired levelof information content?
To answer this question,we perform an experiment based on drawing randomsummaries from the pool of all the clusters in eachcategory.
We perform a Monte Carlo simulation, inwhich for each n, we draw n random summaries,and look at the pyramid score achieved by readingthese headlines.
The pyramid score is calculated us-ing the factoids from all 25 clusters in each cate-gory7.
Each experiment is repeated 1, 000 times tofind the statistical significance of the experiment andthe variation from the average pyramid scores.Figure 4.3 shows the average pyramid scores overdifferent n values in each category on a log-logscale.
This figure shows how pyramid score growsand approaches 1.00 rapidly as more randomly se-lected summaries are seen.100 101 102 10310?210?1100number of summariesPyramid ScoreheadlinescitationsFigure 4: Average pyramid score obtained by reading nrandom summaries shows rapid asymptotic behavior.5 Diversity-based RankingIn previous sections we showed that the diversityseen in human summaries could be according to dif-ferent nuggets or phrases that represent the same fac-toid.
Ideally, a summarizer that seeks to increase di-versity should capture this phenomenon and avoidcovering redundant nuggets.
In this section, we usedifferent state of the art summarization systems torank the set of summaries in each cluster with re-spect to information content and diversity.
To evalu-ate each system, we cut the ranked list at a constantlength (in terms of the number of words) and calcu-late the pyramid score of the remaining text.5.1 Distributional SimilarityWe have designed a summary ranker that will pro-duce a ranked list of documents with respect to thediversity of their contents.
Our model works basedon ranking individual words and using the rankedlist of words to rank documents that contain them.In order to capture the nuggets of equivalent se-mantic classes, we use a distributional similarity of7Similar experiment using individual clusters exhibit similarresults110300.20.40.60.81abortionamazonbabiesburgercolombiaenglandgervaisgoogleirelandmainemercurymissmonkeymozartnobel priestps3slimradiationredsoxrussianscientistsoupyswedentyphoonyaleA00_1023A00_1043A00_2024C00_1072C96_1058D03_1017D04_9907H05_1047H05_1079J04_4002N03_1017N04_1033P02_1006P03_1001P05_1012P05_1013P05_1014P05_1033P97_1003P99_1065W00_0403W00_0603W03_0301W03_0510W05_1203Pyramid ScoreheadlinescitationsFigure 3: The 25th to 75th percentile pyramid score range in individual clusterswords that is inspired by (Lee, 1999).
We representeach word by its context in the cluster and find thesimilarity of such contexts.
Particularly, each wordwi is represented by a bag of words, `i, that have asurface distance of 3 or smaller to wi anywhere inthe cluster.
In other words, `i contains any word thatco-occurs with wi in a 4-gram in the cluster.
Thisbag of words representation of words enables us tofind the word-pair similarities.sim(wi, wj) =~`i ?
~`j?|~`i|| ~`j |(1)We use the pair-wise similarities of words in eachcluster, and build a network of words and their simi-larities.
Intuitively, words that appear in similar con-texts are more similar to each other and will have astronger edge between them in the network.
There-fore, similar words, or words that appear in similarcontexts, will form communities in this graph.
Ide-ally, each community in the word similarity networkwould represent a factoid.
To find the communitiesin the word network we use (Clauset et al, 2004), ahierarchical agglomeration algorithm which worksby greedily optimizing the modularity in a linearrunning time for sparse graphs.The community detection algorithm will assignto each word wi, a community label Ci.
For eachcommunity, we use LexRank to rank the words us-ing the similarities in Equation 1, and assign a scoreto each word wi as S(wi) =Ri|Ci|, where Ri is therank of wi in its community, and |Ci| is the numberof words that belong to Ci.
Figure 5.1 shows partpolicesecondsoxcelebrations red jumpbaseballunhappysweepspitchinghittingarrestvictorytitle dynastyfan poorer2ndpoorgloryPajekFigure 5: Part of the word similarity graph in the redsoxclusterof the word similarity graph in the redsox cluster,in which each node is color-coded with its commu-nity.
This figure illustrates how words that are se-mantically related to the same aspects of the storyfall in the same communities (e.g., ?police?
and ?ar-rest?).
Finally, to rank sentences, we define the scoreof each document Dj as the sum of the scores of itswords.pds(Dj) =?wi?DjS(wi)Intuitively, sentences that contain higher rankedwords in highly populated communities will have asmaller score.
To rank the sentences, we sort themin an ascending order, and cut the list when its sizeis greater than the length limit.5.2 Other Methods5.2.1 RandomFor each cluster in each category (citations andheadlines), this method simply gets a random per-1104mutations of the summaries.
In the headlinesdatasets, where most of the headlines cover somefactoids about the story, we expect this method toperform reasonably well since randomization willincrease the chances of covering headlines that fo-cus on different factoids.
However, in the citationsdataset, where a citing sentence may cover no infor-mation about the cited paper, randomization has thedrawback of selecting citations that have no valuableinformation in them.5.2.2 LexRankLexRank (Erkan and Radev, 2004) works by firstbuilding a graph of all the documents (Di) in acluster.
The edges between corresponding nodes(di) represent the cosine similarity between them isabove a threshold (0.10 following (Erkan and Radev,2004)).
Once the network is built, the system findsthe most central sentences by performing a randomwalk on the graph.p(dj) = (1?
?
)1|D|+ ?
?dip(di)P (di ?
dj) (2)5.2.3 MMRMaximal Marginal Relevance (MMR) (Carbonelland Goldstein, 1998) uses the pairwise cosine simi-larity matrix and greedily chooses sentences that arethe least similar to those already in the summary.
Inparticular,MMR = argminDi?D?A[maxDj?A Sim(Di, Dj)]where A is the set of documents in the summary,initialized to A = ?.5.2.4 DivRankUnlike other time-homogeneous random walks(e.g., PageRank), DivRank does not assume thatthe transition probabilities remain constant overtime.
DivRank uses a vertex-reinforced randomwalk model to rank graph nodes based on a diversitybased centrality.
The basic assumption in DivRankis that the transition probability from a node to otheris reinforced by the number of previous visits to thetarget node (Mei et al, 2010).
Particularly, let?s as-sume pT (u, v) is the transition probability from anynode u to node v at time T .
Then,pT (di, dj) = (1?
?).p?
(dj) + ?.p0(di, dj).NT (dj)DT (di)(3)whereNT (dj) is the number of times the walk hasvisited dj up to time T andDT (di) =?dj?Vp0(di, dj)NT (dj) (4)Here, p?
(dj) is the prior distribution that deter-mines the preference of visiting vertex dj .
We trytwo variants of this algorithm: DivRank, in whichp?
(dj) is uniform, and DivRank with priors inwhich p?
(dj) ?
l(Dj)??
, where l(Dj) is the num-ber of the words in the document Dj and ?
is a pa-rameter (?
= 0.8).5.2.5 C-LexRankC-LexRank is a clustering-based model in whichthe cosine similarities of document pairs are used tobuild a network of documents.
Then the the networkis split into communities, and the most salient doc-uments in each community are selected (Qazvinianand Radev, 2008).
C-LexRank focuses on findingcommunities of documents using their cosine simi-larity.
The intuition is that documents that are moresimilar to each other contain similar factoids.
We ex-pect C-LexRank to be a strong ranker, but incapableof capturing the diversity caused by using differentphrases to express the same meaning.
The reason isthat different nuggets that represent the same factoidoften have no words in common (e.g., ?victory?
and?glory?)
and won?t be captured by a lexical measurelike cosine similarity.5.3 ExperimentsWe use each of the systems explained above to rankthe summaries in each cluster.
Each ranked list isthen cut at a certain length (50 words for headlines,and 150 for citations) and the information contentin the remaining text is examined using the pyramidscore.Table 3 shows the average pyramid score achievedby different methods in each category.
The methodbased on the distributional similarities of words out-performs other methods in the citations category.
Allmethods show similar results in the headlines cate-gory, where most headlines cover at least 1 factoidabout the story and a random ranker performs rea-sonably well.
Table 4 shows top 3 headlines from3 rankers: word distributional similarity (WDS), C-LexRank, and MMR.
In this example, the first 31105Methodheadlines citations Meanpyramid 95% C.I.
pyramid 95% C.I.R 0.928 [0.896, 0.959] 0.716 [0.625, 0.807] 0.822MMR 0.930 [0.902, 0.960] 0.766 [0.684, 0.847] 0.848LR 0.918 [0.891, 0.945] 0.728 [0.635, 0.822] 0.823DR 0.927 [0.900, 0.955] 0.736 [0.667, 0.804] 0.832DR(p) 0.916 [0.884, 0.949] 0.764 [0.697, 0.831] 0.840C-LR 0.942 [0.919, 0.965] 0.781 [0.710, 0.852] 0.862WDS 0.931 [0.905, 0.958] 0.813 [0.738, 0.887] 0.872R=Random; LR=LexRank; DR=DivRank; DR(p)=DivRank with Priors; C-LR=C-LexRank; WDS=Word Distributional Similarity; C.I.=Confidence In-tervalTable 3: Comparison of different ranking systemsMethod Top 3 headlinesWDS1: how sweep it is2: fans celebrate red sox win3: red sox take titleC-LR1: world series: red sox sweep rockies2: red sox take world series3: red sox win world seriesMMR1:red sox scale the rockies2: boston sweep colorado to win world series3: rookies respond in first crack at the big timeC-LR=C-LexRank; WDS=Word Distributional SimilarityTable 4: Top 3 ranked summaries of the redsox clusterusing different methodsheadlines produced by WDS cover two importantfactoids: ?red sox winning the title?
and ?fans cel-ebrating?.
However, the second factoid is absent inthe other two.6 Conclusion and Future WorkOur experiments on two different categories ofhuman-written summaries (headlines and citations)showed that a lot of the diversity seen in humansummarization comes from different nuggets thatmay actually represent the same semantic informa-tion (i.e., factoids).
We showed that the factoids ex-hibit a skewed distribution model, and that the sizeof the nugget inventory asymptotic behavior evenwith a large number of summaries.
We also showedhigh variation in summary quality across differentsummaries in terms of pyramid score, and that theinformation covered by reading n summaries has arapidly growing asymptotic behavior as n increases.Finally, we proposed a ranking system that employsword distributional similarities to identify semanti-cally equivalent words, and compared it with a widerange of summarization systems that leverage diver-sity.In the future, we plan to move to content fromother collective systems on Web.
In order to gen-eralize our findings, we plan to examine blog com-ments, online reviews, and tweets (that discuss thesame URL).
We also plan to build a generation sys-tem that employs the Yule model (Yule, 1925) to de-termine the importance of each aspect (e.g.
who,when, where, etc.)
in order to produce summariesthat include diverse aspects of a story.Our work has resulted in a publicly availabledataset 8 of 25 annotated news clusters with nearly1, 400 headlines, and 25 clusters of citation sen-tences with more than 900 citations.
We believe thatthis dataset can open new dimensions in studying di-versity and other aspects of automatic text genera-tion.7 AcknowledgmentsThis work is supported by the National ScienceFoundation grant number IIS-0705832 and grantnumber IIS-0968489.
Any opinions, findings, andconclusions or recommendations expressed in thispaper are those of the authors and do not necessarilyreflect the views of the supporters.ReferencesEytan Adar, Li Zhang, Lada A. Adamic, and Rajan M.Lukose.
2004.
Implicit structure and the dynamics of8http://www-personal.umich.edu/?vahed/data.html1106Blogspace.
In WWW?04, Workshop on the WebloggingEcosystem.Eytan Adar, Daniel S. Weld, Brian N. Bershad, andSteven S. Gribble.
2007.
Why we search: visualiz-ing and predicting user behavior.
In WWW?07, pages161?170, New York, NY, USA.Regina Barzilay and Lillian Lee.
2002.
Bootstrappinglexical choice via multiple-sequence alignment.
InProceedings of the ACL-02 conference on Empiricalmethods in natural language processing - Volume 10,EMNLP ?02, pages 164?171.Regina Barzilay and Kathleen R. McKeown.
2005.
Sen-tence fusion for multidocument news summarization.Comput.
Linguist., 31(3):297?328.Herbert Blumer.
1951.
Collective behavior.
In Lee, Al-fred McClung, Ed., Principles of Sociology.Jaime G. Carbonell and Jade Goldstein.
1998.
The use ofMMR, diversity-based reranking for reordering docu-ments and producing summaries.
In SIGIR?98, pages335?336.Jean Carletta.
1996.
Assessing agreement on classifi-cation tasks: the kappa statistic.
Comput.
Linguist.,22(2):249?254.Aaron Clauset, Mark E. J. Newman, and CristopherMoore.
2004.
Finding community structure in verylarge networks.
Phys.
Rev.
E, 70(6).Michael Elhadad.
1995.
Using argumentation in textgeneration.
Journal of Pragmatics, 24:189?220.Gu?nes?
Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based centrality as salience in text summa-rization.
Journal of Artificial Intelligence Research(JAIR).Len Fisher.
2009.
The Perfect Swarm: The Science ofComplexity in Everyday Life.
Basic Books.Barbara J. Grosz and Candace L. Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.
Com-put.
Linguist., 12:175?204, July.Lu Hong and Scott Page.
2009.
Interpreted andgenerated signals.
Journal of Economic Theory,144(5):2174?2196.Akshay Java, Pranam Kolari, Tim Finin, and Tim Oates.2006.
Modeling the spread of influence on the blogo-sphere.
In WWW?06.Klaus Krippendorff.
1980.
Content Analysis: An Intro-duction to its Methodology.
Beverly Hills: Sage Pub-lications.Ravi Kumar, Jasmine Novak, Prabhakar Raghavan, andAndrew Tomkins.
2003.
On the bursty evolution ofblogspace.
In WWW?03, pages 568?576, New York,NY, USA.Lillian Lee.
1999.
Measures of distributional similar-ity.
In Proceedings of the 37th annual meeting of theAssociation for Computational Linguistics on Compu-tational Linguistics, pages 25?32.Jure Leskovec, Lars Backstrom, and Jon Kleinberg.2009.
Meme-tracking and the dynamics of the newscycle.
In KDD ?09: Proceedings of the 15th ACMSIGKDD international conference on Knowledge dis-covery and data mining, pages 497?506.Chin-Yew Lin and Eduard Hovy.
2002.
Manual and au-tomatic evaluation of summaries.
In ACL-Workshopon Automatic Summarization.Qiaozhu Mei, Jian Guo, and Dragomir Radev.
2010.
Di-vrank: the interplay of prestige and diversity in infor-mation networks.
In Proceedings of the 16th ACMSIGKDD international conference on Knowledge dis-covery and data mining, pages 1009?1018.Gilad Mishne and Natalie Glance.
2006.
Predictingmovie sales from blogger sentiment.
In AAAI 2006Spring Symposium on Computational Approaches toAnalysing Weblogs (AAAI-CAAW 2006).Ani Nenkova and Rebecca Passonneau.
2004.
Evaluat-ing content selection in summarization: The pyramidmethod.
Proceedings of the HLT-NAACL conference.Scott E. Page.
2007.
The Difference: How the Power ofDiversity Creates Better Groups, Firms, Schools, andSocieties.
Princeton University Press.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: sentiment analysis using subjectivity summariza-tion based on minimum cuts.
In ACL?04, Morristown,NJ, USA.Michael Paul, ChengXiang Zhai, and Roxana Girju.2010.
Summarizing contrastive viewpoints in opin-ionated text.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Process-ing, pages 66?76.Vahed Qazvinian and Dragomir R. Radev.
2008.
Scien-tific paper summarization using citation summary net-works.
In COLING 2008, Manchester, UK.Vahed Qazvinian and Dragomir R. Radev.
2010.
Identi-fying non-explicit citing sentences for citation-basedsummarization.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 555?564, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Neil J. Smelser.
1963.
Theory of Collective Behavior.Free Press.Karen Spa?rck-Jones.
1999.
Automatic summarizing:factors and directions.
In Inderjeet Mani and Mark T.Maybury, editors, Advances in automatic text summa-rization, chapter 1, pages 1 ?
12.
The MIT Press.Manfred Stede.
1995.
Lexicalization in natural languagegeneration: a survey.
Artificial Intelligence Review,(8):309?336.Hans van Halteren and Simone Teufel.
2003.
Examin-ing the consensus between human summaries: initialexperiments with factoid analysis.
In Proceedings of1107the HLT-NAACL 03 on Text summarization workshop,pages 57?64, Morristown, NJ, USA.
Association forComputational Linguistics.Hans van Halteren and Simone Teufel.
2004.
Evaluatinginformation content by factoid analysis: human anno-tation and stability.
In EMNLP?04, Barcelona.Ellen M. Voorhees.
1998.
Variations in relevance judg-ments and the measurement of retrieval effectiveness.In SIGIR ?98: Proceedings of the 21st annual interna-tional ACM SIGIR conference on Research and devel-opment in information retrieval, pages 315?323.G.
Udny Yule.
1925.
A mathematical theory of evo-lution, based on the conclusions of dr. j. c. willis,f.r.s.
Philosophical Transactions of the Royal Societyof London.
Series B, Containing Papers of a BiologicalCharacter, 213:21?87.Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, andDavid Andrzejewski.
2007.
Improving diversity inranking using absorbing random walks.
In HumanLanguage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics; Proceedings of the Main Con-ference, pages 97?104.1108
