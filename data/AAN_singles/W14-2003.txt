Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 19?27,Baltimore, Maryland USA, June 26 2014.c?2014 Association for Computational LinguisticsSentence Processing in a Vectorial Model of Working MemoryWilliam SchulerDepartment of LinguisticsThe Ohio State Universityschuler@ling.osu.eduAbstractThis paper presents a vectorial incremen-tal parsing model defined using indepen-dently posited operations over activation-based working memory and weight-basedepisodic memory.
This model has the at-tractive property that it hypothesizes onlyone unary preterminal rule application andonly one binary branching rule applica-tion per time step, which allows it to besmoothly integrated into a vector-basedrecurrence that propagates structural am-biguity from one time step to the next.Predictions of this model are calculatedon a center-embedded sentence processingtask and shown to exhibit decreased pro-cessing accuracy in center-embedded con-structions.1 IntroductionCurrent models of memory (Marr, 1971; Ander-son et al., 1977; Murdock, 1982; McClelland etal., 1995; Howard and Kahana, 2002) involve acontinuous activation-based (or ?working?)
mem-ory, typically modeled as a vector representing thecurrent firing pattern of neurons or neural clus-ters in the cortex.
This activation-based memoryis then supported by a durable but rapidly mu-table weight-based (or ?episodic?)
memory, typi-cally modeled as one or more matrices formed bysummed outer-products of cue and target vectorsand cued by simple matrix multiplication, repre-senting variable synaptic connection strengths be-tween neurons or neural clusters.The lack of discrete memory units in such mod-els makes it difficult to imagine a neural imple-mentation of a typical e.g.
chart-based computa-tional account of sentence processing.
On theother hand, superposition in vectorial models sug-gests a natural representation of a parallel incre-mental processing model.
This paper exploreshow such an austere model of memory not onlymight be used to encode a simple probabilistic in-cremental parser, but also lends itself to naturallyimplement a vectorial interpreter and coreferenceresolver.
This model is based on the left-cornerparser formulation of van Schijndel et al.
(2013a),which has the attractive property of generating ex-actly one binary-branching rule application afterprocessing each word.
This property greatly sim-plifies a vectorial implementation because it al-lows these single grammar rule applications to besuperposed in cases of attachment ambiguity.Predictions of the vectorial model described inthis paper are then calculated on a simple center-embedded sentence processing task, producing alower completion accuracy for center-embeddedsentences than for right-branching sentences withthe same number of words.
As noted by Levy andGibson (2013), this kind of memory effect is noteasily explained by existing information-theoreticmodels of frequency effects (Hale, 2001; Levy,2008).The model described in this paper also providesan explanation for the apparent reality of linguisticobjects like categories, grammar rules, discoursereferents and dependency relations, as cognitivestates in activation-based memory (in the case ofcategories and discourse referents), or cued asso-ciations in weight-based memory (in the case ofgrammar rules, and dependency relations), with-out having to posit complex machinery specificto language processing.
In this sense, unlike ex-isting chart-based parsers or connectionist modelsbased on recurrent neural networks, this model in-tegrates familiar notions of grammar and seman-tic relations with current ideas of activation-basedand weight-based memory.
It is also anticipatedthat this interface to both linguistic and neurosci-entific theories will make the model useful as abasis for more nuanced understanding of linguisticphenomena such as ambiguity resolution, seman-19tic representation, and language acquisition.2 Related WorkThe model described in this paper is based on theleft-corner parser formulation of van Schijndel etal.
(2013a), which is an implementation of a fullyparallel incremental parser.
This parser differsfrom chart-based fully parallel incremental parsersused by Hale (2001), Levy (2008) and others inthat it enforces a cognitively-motivated bound oncenter-embedding depth.
This bound allows theparser to represent a tractable set of incrementalhypotheses in an explicitly enumerated list as afactored hidden Markov model, without necessi-tating the use of a parser chart.
This model has theattractive property that, in any context, it hypoth-esizes exactly one binary-branching rule applica-tion at each time step.The model described in this paper extends thevan Schijndel et al.
(2013a) parser by maintain-ing possible store configurations as superposedsequence states in a finite-dimensional state vec-tor.
The model then exploits the uniformity ofits parsing operations to integrate probabilisticallyweighted grammar rule applications into this su-perposed state vector.
These superposed statesare then used to cue more superordinate sequen-tial states as ?continuations?
whenever subordinatestates conclude.
Interference in this cueing pro-cess is then observed to produce a natural center-embedding limit.This model is defined as a recurrence over anactivation vector, similar to the simple recurrentnetwork of Elman (1991) and others, but unlike anSRN, which does not encode anything in weight-based memory during processing, this model en-codes updates to a processing hierarchy in weight-based memory at every time step.
The model isalso similar to the ACT-R parser of Lewis and Va-sishth (2005) in that it maintains a single statewhich is updated based on content-based cuedassociation, but unlike the ACT-R parser, whichcues category tokens on category types and there-fore models memory limits as interference amonggrammar rules, this model cues category tokenson other category tokens, and therefore predictsmemory limits even in cases where grammar rulesdo not involve similar category types.
Also unlikeLewis and Vasishth (2005), this model is definedpurely in terms of state vectors and outer-productassociative memory and therefore has the capacitySVPNPNNNNmudDtheVshookNPNwindDthe????????????????????????
?S/N}N/NFigure 1: Example incomplete category duringprocessing of the sentence The wind shook themud room door.to maintain parallel states in superposition.3 Background: Non-vectorialIncremental ParsingThe model defined in this paper is based on theleft-corner parser formulation of van Schijndel etal.
(2013a).
This parser maintains a set of incom-plete categories a/b at each time step, each con-sisting of an active category a lacking an awaitedcategory b yet to come.
For example, Figure 1shows an incomplete category S/N consisting ofa sentence lacking a common noun yet to come,which non-immediately dominates another incom-plete category N/N consisting of a common nounlacking another common noun yet to come.Processing in this model is defined to alternatebetween two phases:1. a ?fork?
phase in which a word is either usedto complete an existing incomplete category,or forked into a new complete category; and2.
a ?join?
phase in which one of these completecategories is used as a left child of a grammarrule application and then either joined ontoa superordinate incomplete category or keptdisjoint.In any case, only one grammar rule is applied af-ter each word.
These fork and join operations areshown graphically and as natural deduction rulesin Figure 2.An example derivation of the sentence, Thewind shook the mud room door, using the pro-ductions in Figure 2 is shown in Figure 3, withcorresponding partial parse trees shown in Fig-ure 4.
Van Schijndel et al.
(2013a) show that a20?F:abxt+F:aba?xta/b xtab?
xt(?F)a/b xta/b a?b+?
a?...
; a??
xt(+F)+J:aba??b???J:aba?a??b?
?a/b a??a/b??b?
a??b??
(+J)a/b a?
?a/b a?/b??b+?
a?...
; a??
a??b??
(?J)Figure 2: Fork and join operations from the vanSchijndel et al.
(2013a) left-corner parser formu-lation.
During the fork phase, word x either com-pletes an existing incomplete category a, or forksinto a new complete category a?.
During the joinphase, complete category a?
?becomes a left childof a grammar rule application, then either joinsonto a superordinate incomplete category a/b orremains disjoint.probabilistic version of this incremental parser canreproduce the results of a state-of-the-art chart-based parser (Petrov and Klein, 2007).4 Vectorial ParsingThis left corner parser can be implemented in avectorial model of working memory using vec-tors as activation-based memory and matrices asweight-based memory.
Following Anderson et al.
(1977) and others, vectors v in activation-basedmemory are cued from other vectors u throughweight-based memory matrices M using ordinaryT/T theT/T, D+FT/T, NP/N?JwindT/T, NP?FT/T, S/VP?JshookT/T, S/VP, V+FT/T, S/NP+JtheT/T, S/NP, D+FT/T, S/N+JmudT/T, S/N, N+FT/T, S/N, N/N?JroomT/T, S/N, N?FT/T, S/N+JdoorT/T, S?FT/T+JFigure 3: Processing steps in parsing the sentenceThe wind shook the mud room door.matrix multiplication:1v = M u (1)This representation has been used to model the in-fluence of activation in antecedent neurons on ac-tivation in consequent neurons (Marr, 1971; An-derson et al., 1977).Unless they are cued from some other source,all vectors in this model are initially randomlygenerated by sampling from an exponential distri-bution, denoted here simply by:v ?
Exp (2)Also following Anderson et al.
(1977), weight-based memory matrices M are themselves definedand updated by simply adding outer products ofdesired cue u and target v vectors:2Mt= Mt?1+ v ?
u (3)This representation has been used to model rapidsynaptic sensitization in the hippocampus (Marr,1971; McClelland et al., 1995), in which synapsesof activated antecedent neurons that impinge onactivated consequent neurons are strengthened.1That is, multiplication of an associative memory ma-trix M by a state vector v yields:(M v)[i]def=?Jj=1M[i, j]?
v[ j](1?
)2An outer product v ?
u defines a matrix by multiplyingeach combination of scalars in vectors v and u:(v ?
u)[ j,i]def= v[ j]?
u[i](2?
)21a) NPNDtheb) SVPNPNwindDthec) SVPNPVshookNPNwindDthed) SVPNPNDtheVshookNPNwindDthee) SVPNPNNNNmudDtheVshookNPNwindDthef) SVPNPNNNNroomNmudDtheVshookNPNwindDtheFigure 4: Processing steps in parsing the sentence The wind shook the mud room door.Finally, cued associations can be combined us-ing pointwise or diagonal products:3w = diag(u) v (4)Unlike a symbolic statistical model, a vectorialmodel must explicitly distinguish token represen-tations from types in order to define structural rela-tions that would be implicit in the positions of datastructure elements in a symbolic model.
Thus, theactive or awaited distinction is applied to categorytokens rather than types, but grammar rule appli-cations are defined over category types rather thantokens.The vectorial left-corner parser described in thispaper is therefore defined on a single category to-ken vector btwhich encodes the awaited categorytoken of the most subordinate incomplete categoryat the current time step t. A hierarchy of nested in-complete category tokens is then encoded in two?continuation?
matrices:?
At, which cues the active category token aof the same incomplete category as a givenawaited token b; and3A diagonal product diag(v) u defines a vector by multi-plying corresponding scalars in vectors v and u:(diag(v) u)[i]def= v[i]?
u[i](3?)?
Bt, which cues the awaited category to-ken b of the incomplete category that non-immediately dominates any active categorytoken a.Together, the cued associations in these continua-tion matrices trace a path up from the most sub-ordinate awaited category token b to the most su-perordinate category token currently hypothesizedas the root of the syntactic tree.
Vectors for cate-gory types c can then be cued from any categorytoken a or b through an associative matrix Ct. Allthree of these matrices may be updated from timestep to time step by associating cue and target vec-tors through outer product addition, as describedabove.The model also defines vectors for binary-branching grammar rules g, which are associ-ated with parent, left child, or right child cat-egory types via ?accessor?
matrices G, G?, orG?
?.4These accessor matrices are populatedfrom binary-branching rules in a probabilisticcontext-free grammar (PCFG) in Chomsky Nor-mal Form (CNF).
For example, the PCFG ruleP(S ?
NP VP) = 0.8 may be encoded using a4This use of reification and accessor matrices for gram-mar rules emulates a tensor model (Smolensky, 1990; beimGraben et al., 2008) in that in the worst case grammar rules(composed of multiple categories) would require a spacepolynomially larger than that of category types, but since thisspace is sparsely inhabited in the expected case, this reifiedrepresentation is computationally more tractable.22grammar rule vector gS?
NP VPand category vec-tors cS, cVP, cNPwith the following outer-productassociations:Gdef= gS?
NP VP?
cS?
0.8G?def= gS?
NP VP?
cNPG?
?def= gS?
NP VP?
cVPGrammars with additional rules can then be en-coded as a sum of outer products of rule and cat-egory vectors.
Grammar rules can then be cuedfrom category types by matrix multiplication, e.g.:gS?
NP VP= G?cNPand category types can be cued from grammarrules using transposed versions of accessor matri-ces:cNP= G?>gS?
NP VPThe model also defines:?
vectors xtfor observation types (i.e.
words),?
a matrix P cueing category types from obser-vation types, populated from unary rules in aCNF PCFG, and?
a matrix D = DKof leftmost descendant cate-gories cued from ancestor categories, derivedfrom accessor matrices G and G?by K itera-tions of the following recurrence:5D?0def= diag(1) (5)D0def= diag(0) (6)D?kdef= G?>G D?k?1(7)Dkdef= Dk?1+ D?k(8)where each D?kcues a probabilistically-weighted descendant at distance k from itscue, and Dkis the superposition of all suchdescendant associations from length 1 tolength K. This produces a superposed set ofcategory types that may occur as leftmost de-scendants of a (possibly superposed) ancestorcategory type.In order to exclude active category types Ctatthat are not compatible with awaited categorytypes Ctbtin the same incomplete category, themodel also defines:5Here 1 and 0 denote vectors of ones and zeros, respec-tively.?
a matrix E = EKof rightmost descendantcategories cued from ancestor categories, de-rived in the same manner as D, except us-ing G?
?in place of G?.The parser proceeds in two phases, generating acomplete category token vector a?
?tfrom bt?1dur-ing the F phase, then generating an awaited cat-egory token vector btof an incomplete categoryduring the J phase.
Since the parser proceeds intwo phases, this paper will distinguish variablesupdated in each phase using a subscript for timestep t?
.5 at the end of the first phase and t at theend of the second phase.The vectorial parser implements the F phase ofthe left-corner parser (the ?fork/no-fork?
decision)by first defining two new category tokens for thepossibly forked or unforked complete category:at?.5, a?t?.5?
ExpThe parser then obtains:?
the category type of the most subordinateawaited category token at the previous timestep: Ct?1bt?1(which involves no fork), and?
a superposed set of non-immediate descen-dants of the category type of this most sub-ordinate awaited category token: D Ct?1bt?1(which involves a fork),These fork and no-fork categories are then diag-onally multiplied (intersected) with a superposedset of preterminal categories for the current obser-vation (P xt):c?t= diag(P xt) Ct?1bt?1c+t= diag(P xt) D Ct?1bt?1The B and C continuation and category matricesare then updated with a superordinate awaited cat-egory token and category type for a and a?
:at?1= At?1bt?1Bt?.5= Bt?1+ bt?1?
a?t?.5+ Bt?1at?1?
at?.5Ct?.5= Ct?1+ c+t?
a?t?.5+ diag(Ct?1at?1) E>c?t?
at?.5where the updated category for at?.5results froman intersection (diagonal product) of the currentcategory at at?1with the set of categories that canoccur with c?tas a rightmost child, as defined by E.The intersected fork and no-fork category typesare then used to weight superposed hypotheses for23?F:at?1(= a?
?t)bt?1xt+F:at?1bt?1a?t?.5(= a?
?t)xtBFigure 5: Updates to continuation matrices duringthe ?fork?
phase of a left-corner parser.the complete category token a?
?tthat will resultfrom this phase of processing, and the b vector isupdated to encode the category token:6a?
?t=at?1||c?t|| + a?t?.5||c+t||||at?1||c?t|| + a?t?.5||c+t||||bt?.5= Bt?.5a?
?tThese updates can be represented graphically asshown in Figure 5.The vectorial parser then similarly implementsthe J phase (the ?join/no-join?
decision) of the left-corner parser by first defining a new category to-ken a?for a possible new active category of themost subordinate incomplete category, and b?
?fora new awaited category token:a?t, b??t?
ExpThe parser then obtains:?
a superposed set of grammar rules with par-ent category matching the category of themost subordinate awaited category token atthe previous time step: G Ct?.5bt?.5(which as-sumes a join), and?
a superposed set of grammar rules withparent category non-immediately descendedfrom the category of this most subordinateawaited category token: G D Ct?.5bt?.5(whichassumes no join)These join and no-join grammar rule vectorsare then diagonally multiplied (intersected) with6This uses the two norm ||v||, which is the magnitude ofvector v, defined as the square root of the sum of the squaresof its scalar values:||v||def=??i(v[i])2(4?
)Dividing a vector by its two norm has the effect of normaliz-ing it to unit length.+J:at?.5bt?.5a??tb??tA?J:at?.5bt?.5a?ta??tb?
?tBAFigure 6: Updates to continuation matrices duringthe ?join?
phase of a left-corner parser.the superposed set of grammar rules whose leftchild category type matches the category typeof the most subordinate complete category to-ken (G?Ct?.5a?
?t):g+t= diag(G?Ct?.5a?
?t) G Ct?.5bt?.5g?t= diag(G?Ct?.5a?
?t) G D Ct?.5bt?.5These intersected join and no-join grammar rulevectors are then used to weight superposed hy-potheses for the incomplete category that will re-sult from this phase of processing in updates to thecontinuation and category matrices A, B, and C:At= At?1+At?1bt?.5||g+t|| + a?t||g?t||||At?1bt?.5||g+t|| + a?t||g?t||||?
b?
?tBt= Bt?.5+ bt?.5?
a?tCt= Ct?.5+ G>g?t?
a?t+G?
?>g+t+ G??>g?t||G?
?>g+t+ G??>g?t||?
b?
?tThese updates can be represented graphically asshown in Figure 6.
Finally the the most subordi-nate awaited category token is updated for the nextword:bt= b?
?t5 PredictionsIn order to assess the cognitive plausibility of thememory modeling assumptions in this vectorialparser, predictions of the implementation definedin Section 4 were calculated on center-embeddingand right-branching sentences, exemplified by:(1) If either Kim stays or Kim leaves then Patleaves.
(center-embedded condition)(2) If Kim stays then if Kim leaves then Patleaves.
(right-branching condition)24P(T?
S T) = 1.0P(S?
NP VP) = 0.5P(S?
IF S THEN S) = 0.25P(S?
EITHER S OR S) = 0.25P(IF?
if) = 1.0P(THEN?
then) = 1.0P(EITHER?
either) = 1.0P(OR?
or) = 1.0P(NP?
kim) = 0.5P(NP?
pat) = 0.5P(VP?
leaves) = 0.5P(VP?
stays) = 0.5Figure 7: ?If .
.
.
then .
.
.
?
grammar used in sen-tence processing experiment.
Branches with aritygreater than two are decomposed into equivalentright-branching sequences of binary branches.both of which contain the same number of words.These sentences were processed using the gram-mar shown in Figure 7, which assigns the sameprobability to both center-embedding and right-branching sentences.
The if .
.
.
then .
.
.
and ei-ther .
.
.
or .
.
.
constructions used in these ex-amples are taken from the original Chomsky andMiller (1963) paper introducing center-embeddingeffects, and are interesting because they do not in-volve the same grammar rule (as is the case withfamiliar nested object relative constructions), anddo not involve filler-gap constructions, which mayintroduce overhead processing costs as a possibleconfound.This assessment consisted of 500 trials for eachsentence type.
Sentences were input to an imple-mentation of this model using the Numpy packagein Python, which consists of the equations shownin Section 4 enclosed in a loop over the words ineach sentence.
Each trial initially sampled a, b,c, and g vectors from random exponential distri-butions of dimension 100, and the parser initial-ized b0with category type T as shown in Figure 3,with the active category token at A0b0also asso-ciated with category type T.Accuracy for this assessment was calculated byfinding the category type with the maximum co-sine similarity for the awaited category bTat theend of the sentence.
If this category type was Tsentence correct incorrectcenter-embedded 231 269right-branching 297 203Table 1: Accuracy of vectorial parser on each sen-tence type.
(as it is in Figure 3), the parser was awarded apoint of accuracy; otherwise it was not.
The re-sults of this assessment are shown in Table 1.
Theparser processes sentences with right-branchingstructure substantially more accurately than sen-tences with center-embedded structure.
These re-sults are strongly significant (p < .001) using a ?2test.These predictions seem to be consistent withobservations by Chomsky and Miller (1963) thatcenter-embedded structures are more difficult toparse than right-branching structures, but it is alsoimportant to note how the model arrives at thesepredictions.
The decreased accuracy of center-embedded sentences is not a result of an ex-plicit decay factor, as in ACT-R and other models(Lewis and Vasishth, 2005), or distance measuresas in DLT (Gibson, 2000), nor is it attributableto cue interference (as modeled by Lewis andVasishth for nested object relative constructions),since the inner and outer embeddings in thesesentences use different grammar rules.
The de-creased accuracy for center-embedding is also notattributable to frequency effects of grammar rules(as modeled by Hale, 2001), since the rules inthis grammar are relatively common and equallyweighted.Instead, the decrease for center-embeddedstructures emerges from this model as a necessaryresult of drift due to repeated superposition of tar-gets encoded in continuation matrices A and B.This produces a natural decay over time as se-quences of subordinate category token vectors btintroduce noise in updates to Atand Bt.
Whenthese matrices are cued in concert, as happenswhen cueing across incomplete categories, the dis-tortion is magnified.
This decay is therefore aconsequence of encoding hierarchic structural in-formation using cued associations.
In contrast,right-branching parses are not similarly as badlydegraded over time because the flat treatment ofleft- and right- branching structures in a left-cornerparser does not cue as often across incomplete cat-egories using matrix B.256 ExtensionsThis model is also interesting because it allows se-mantic relations to be constructed using the sameouter product associations used to define contin-uation and category matrices in Section 4.
First,discourse referent instances and numbered relationtypes are defined as vectors i and n, respectively.Then relation tokens are reified as vectors r, simi-lar to the reification of grammar rules described inSection 4, and connected to relation type vectors nby cued association R and to source and target dis-course referents i by cued associations R?and R?
?.Semantic relation types can then be cued fromgrammar rules g using associative matrix N, al-lowing relations of various types to be constructedin cases of superposed grammar rules.
In futurework, it would be interesting to see whether thisrepresentation is consistent with observations oflocal syntactic coherence (Tabor et al., 2004).This model can also constrain relations to dis-course referents introduced in a previous sentenceor earlier in the same sentence using a vector oftemporal features (Howard and Kahana, 2002).This is a vector of features zt, that has a randomlychosen selection of features randomly resampledat each time step, exponentially decreasing the co-sine similarity of the current version of the ztvec-tor to earlier versions zt?.
If discourse referents iare cued from the current temporal features ztinan outer product associative matrix Z, it will cuerelatively recently mentioned discourse referentsmore strongly than less recently mentioned refer-ents.
If discourse referents for eventualities andpropositions j are connected to explicit predicatetype referents k (say, cued by a relation of type?0?
), and if temporal cues are combined in a diag-onal product with cues by semantic relations froma common predicate type, the search for a consis-tent discourse referent can be further constrainedto match the gender of a pronoun or other rela-tions from a definite reference.
In future work, itwould be interesting to compare the predictions ofthis kind of model to human coreference resolu-tion, particularly in the case of parsing conjunc-tions with reflexive pronouns, which has been usedto argue for fully connected incremental parsing(Sturt and Lombardo, 2005).7 ConclusionThis paper has presented a vectorial left-corner parsing model defined using independentlyposited operations over activation-based workingmemory and weight-based episodic memory.
Thismodel has the attractive property that it hypoth-esizes only one unary branching rule applicationand only one binary branching rule application pertime step, which allows it to be smoothly inte-grated into a vector-based recurrence that propa-gates structural ambiguity from one time step tothe next.
Predictions of this model were calcu-lated on a center-embedded sentence processingtask and the model was shown to exhibit decreasedprocessing accuracy in center-embedded construc-tions, as observed by Chomsky and Miller (1963),even in the absence of repeated grammar rules orpotential confounding overhead costs that may beassociated with filler-gap constructions.This model is particularly interesting because,unlike other vectorial or connectionist parsers,it directly implements a recursive probabilisticgrammar with explicit categories of syntactic con-text.
This explicit implementation of a probabilis-tic grammar allows variations of this processingmodel to be evaluated without having to also posita human-like model of acquisition.
For example,the model can simply be defined with a PCFG de-rived from a syntactically annotated corpus.The model is also interesting because it servesas an existence proof that recursive grammar is notincompatible with current models of human mem-ory.Finally, the fact that this model predicts mem-ory effects at boundaries between incomplete cat-egories, in line with predictions of fully paral-lel left-corner parsers (van Schijndel and Schuler,2013; van Schijndel et al., 2013b), suggeststhat measures based on incomplete categories (orbased on connected components of other kinds ofsyntactic or semantic structure) are not simply ar-bitrary but rather may naturally emerge from theuse of associative memory during sentence pro-cessing.Although the model may not scale to broad-coverage parsing evaluations in its present form,future work will explore hybridization of some ofthese methods into a parser with an explicit beamof parallel hypotheses.
It is anticipated that analgorithmic-level comprehension model such asthis will allow a more nuanced understanding ofhuman semantic representation and grammar ac-quisition.26ReferencesJames A. Anderson, Jack W. Silverstein, Stephen A.Ritz, and Randall S. Jones.
1977.
Distinctive fea-tures, categorical perception and probability learn-ing: Some applications of a neural model.
Psycho-logical Review, 84:413?451.Peter beim Graben, Sabrina Gerth, and Shravan Va-sishth.
2008.
Towards dynamical system modelsof language-related brain potentials.
Cognitive Neu-rodynamics, 2(3):229?255.Noam Chomsky and George A. Miller.
1963.
Intro-duction to the formal analysis of natural languages.In Handbook of Mathematical Psychology, pages269?321.
Wiley, New York, NY.Jeffrey L. Elman.
1991.
Distributed representations,simple recurrent networks, and grammatical struc-ture.
Machine Learning, 7:195?225.Edward Gibson.
2000.
The dependency locality the-ory: A distance-based theory of linguistic complex-ity.
In Image, language, brain: Papers from the firstmind articulation project symposium, pages 95?126,Cambridge, MA.
MIT Press.John Hale.
2001.
A probabilistic earley parser as apsycholinguistic model.
In Proceedings of the sec-ond meeting of the North American chapter of theAssociation for Computational Linguistics, pages159?166, Pittsburgh, PA.Marc W. Howard and Michael J. Kahana.
2002.
A dis-tributed representation of temporal context.
Journalof Mathematical Psychology, 45:269?299.Roger Levy and Edward Gibson.
2013.
Surprisal, thepdc, and the primary locus of processing difficultyin relative clauses.
Frontiers in Psychology, 4(229).Roger Levy.
2008.
Expectation-based syntactic com-prehension.
Cognition, 106(3):1126?1177.Richard L. Lewis and Shravan Vasishth.
2005.An activation-based model of sentence processingas skilled memory retrieval.
Cognitive Science,29(3):375?419.David Marr.
1971.
Simple memory: A theoryfor archicortex.
Philosophical Transactions of theRoyal Society (London) B, 262:23?81.J.
L. McClelland, B. L. McNaughton, and R. C.O?Reilly.
1995.
Why there are complementarylearning systems in the hippocampus and neocortex:Insights from the successes and failures of connec-tionist models of learning and memory.
Psychologi-cal Review, 102:419?457.B.B.
Murdock.
1982.
A theory for the storage andretrieval of item and associative information.
Psy-chological Review, 89:609?626.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACLHLT 2007, pages 404?411, Rochester, New York,April.
Association for Computational Linguistics.Paul Smolensky.
1990.
Tensor product variable bind-ing and the representation of symbolic structures inconnectionist systems.
Artificial intelligence, 46(1-2):159?216.Patrick Sturt and Vincent Lombardo.
2005.
Processingcoordinate structures: Incrementality and connect-edness.
Cognitive Science, 29:291?305.W.
Tabor, B. Galantucci, and D Richardson.
2004.Effects of merely local syntactic coherence on sen-tence processing.
Journal of Memory and Lan-guage, 50(4):355?370.Marten van Schijndel and William Schuler.
2013.
Ananalysis of frequency- and recency-based processingcosts.
In Proceedings of NAACL-HLT 2013.
Associ-ation for Computational Linguistics.Marten van Schijndel, Andy Exley, and WilliamSchuler.
2013a.
A model of language processingas hierarchic sequential prediction.
Topics in Cogni-tive Science, 5(3):522?540.Marten van Schijndel, Luan Nguyen, and WilliamSchuler.
2013b.
An analysis of memory-based pro-cessing costs using incremental deep syntactic de-pendency parsing.
In Proceedings of CMCL 2013.Association for Computational Linguistics.27
