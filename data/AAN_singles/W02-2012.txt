GraSp:  Grammar learningfrom unlabelled speech corporaPeter Juel HenrichsenCMOLCenter for Computational Modelling of Languagec/o Dept.
of Computational LinguisticsCopenhagen Business SchoolFrederiksberg, Denmarkpjuel@id.cbs.dkAbstractThis paper presents the ongoing projectComputational Models of First LanguageAcquisition, together with its currentproduct, the learning algorithm GraSp.GraSp is designed specifically forinducing grammars from large, unlabelledcorpora of spontaneous (i.e.
unscripted)speech.
The learning algorithm does notassume a predefined grammaticaltaxonomy; rather the determination ofcategories and their relations is consideredas part of the learning task.
While GraSplearning can be used for a range ofpractical tasks, the long-term goal of theproject is to contribute to the debate ofinnate linguistic knowledge ?
under thehypothesis that there is no such.IntroductionMost current models of grammar learningassume a set of primitive linguistic categoriesand constraints, the learning process beingmodelled as category filling and ruleinstantiation ?
rather than category formationand rule creation.
Arguably, distributinglinguistic data over predefined categories andtemplates does not qualify as grammar 'learning'in the strictest sense, but is better described as'adjustment' or  'adaptation'.
Indeed, Chomsky,the prime advocate of the hypothesis of innatelinguistic principles, has claimed that "in certainfundamental respects we do not really learnlanguage" (Chomsky 1980: 134).
As Chomskypoints out, the complexity of the learning task isgreatly reduced given a structure of primitivelinguistic constraints ("a highly restrictiveschematism", ibid.).
It has however been veryhard to establish independently thepsychological reality of such a structure, and thequestion of innateness is still far from settled.While a decisive experiment may never beconceived, the issue could be addressedindirectly, e.g.
by asking: Are innate principlesand parameters necessary preconditions forgrammar acquisition?
Or rephrased in the spiritof constructive logic: Can a learning algorithmbe devised that learns what the infant learnswithout incorporating specific linguistic axioms?The presentation of such an algorithm wouldcertainly undermine arguments referring to the'poverty of the stimulus', showing the innatenesshypothesis to be dispensable.This paper presents our first try.1 The essential algorithm1.1 Psycho-linguistic preconditionsTypical spontaneous speech is anything butsyntactically 'well-formed' in the Chomskyansense of the word.right well let's er --= let's look at the applications- erm - let me just ask initially this -- I discussedit with er Reith er but we'll = have to go into it abit further - is it is it within our erm er = are wefree er to er draw up a rather = exiguous list - ofpeople to interview(sample from the London-Lund corpus)Yet informal speech is not perceived as beingdisorderly (certainly not by the languagelearning infant), suggesting that its organizingprinciples differ from those of the writtenlanguage.
So, arguably, a speech grammarinducing algorithm should avoid referring to theusual categories of text based linguistics ?
'sentence', 'determiner phrase', etc.1Instead we allow a large, indefinite numberof (indistinguishable) basic categories ?
and thenleave it to the learner to shape them, fill themup, and combine them.
For this task, the learnerneeds a built-in concept of constituency.
Thiskind of innateness is not in conflict with ourmain hypothesis, we believe, since constituencyas such is not specific to linguistic structure.1.2 Logical preliminariesFor the reasons explained, we want the learningalgorithm to be strictly data-driven.
This putsspecial demands on our parser which must berobust enough to accept input strings with littleor no hints of syntactic structure (for the earlystages of a learning session), while at the sametime retaining the discriminating powers of astandard context free parser (for the later stages).Our solution is a sequent calculus, a variantof the Gentzen-Lambek categorial grammarformalism (L) enhanced with non-classical rulesfor isolating a residue of uninterpretable sequentelements.
The classical part is identical to L(except that antecedents may be empty).Classical part???????
link?
?
?
?B ?
B      ?1  A  ?2 ?
C?????????????????????
/L?1  A/B  ?B  ?2 ?
C?0  B  ?
A??????????
/R?0 ?
A/B?B ?
B      ?1  A  ?2 ?
C?????????????????????
\L?1  ?B  B\A  ?2 ?
CB  ?0 ?
A??????????
\R?0 ?
B\A?1  A  B  ?2 ?
C???????????????
*L?1  A*B  ?2 ?
C?1 ?
A     ?2 ?
B???????????????
*R?1  ?2 ?
A*BA, B, C are categories; ?x are (possibly empty)strings of categories.1Hoekstra (2000) and Nivre (2001) discuss theannotation of spoken corpora with traditional tags.These seven rules capture the input parts thatcan be interpreted as syntactic constituents(examples below).
For the remaining parts, weinclude two non-classical rules (?L and ?R).2Non-classical part?+         ?1  ?2 ?
C??????????????????
?L?1  ?
?2  ?
C????????
?R?
??
is a basic category.
?x are (possibly empty)strings of categories.
Superscripts + ?
denotepolarity of residual elements.By way of an example, consider the input stringright well let's er let's look at the applicationsas analyzed in an early stage of a learningsession.
Since no lexical structure has developedyet, the input is mapped onto a sequent of basic(dummy) categories:3c29  c22  c81  c5  c81  c215  c10  c1  c891 ?
c0Using ?L recursively, each category of theantecedent (the part to the left of ?)
is removedfrom the main sequent.
As the procedure is fairlysimple, we just show a fragment of the proof.Notice that proofs read most easily bottom-up.c0??????
?Rc81+c10+c1+c891+  ?
c0????????????????????????
?L...????????????????????
?Lc215+c81  c10  c1  c891 ?
c0?????????????????????????
?Lc5+c81  c215  c10  c1  c891 ?
c0???????????????????????????????
?L...  c5  c81  c215  c10  c1  c891 ?
c0In this proof there are no links, meaning that nogrammatical structure was found.
Later, whenthe lexicon has developed, the parser may2The calculus presented here is slightly simplified.Two rules are missing, and so is the reservedcategory T ('noise') used e.g.
for consequents (inplace of c0 of the example).
Cf.
Henrichsen (2000).3By convention the indexing of category namesreflects the frequency distribution: If word W hasrank n in the training corpus, it is initialized as W:cn .recognize more structure in the same input:??????
?l ???????
?lc10?c10  c891?c891?????????????????
*Rc10 c891 ?
c10*c891   c81 c215 ?
c0?????
?l ????????????????????????????
?/Lc1 ?
c1 c81  c215/(c10*c891)  c10  c891 ?
c0?????????????????????????????????????\L...
c81 c215/(c10*c891) c10  c1 c1\c891 ?
c0... let's        look  at the  applicationsThis proof tree has three links, meaning that thedisorder of the input string (wrt.
the newlexicon) has dropped by three degrees.
More ondisorder shortly.1.3 The algorithm in outlineHaving presented the sequent parser, we nowshow its embedding in the learning algorithmGraSp (Grammar of Speech).For reasons mentioned earlier, the commoninventory of categories (S, NP, CN, etc) isavoided.
Instead each lexeme initially inhabitsits own proto-category.
If a training corpus has,say, 12,345 word types the initial lexicon mapsthem onto as many different categories.
Alearning session, then, is a sequence of lexicalchanges, introducing, removing, andmanipulating the operators /, \, and * as guidedby a well-defined measure of structural disorder.We prefer formal terms without a linguisticbias ("no innate linguistic constraints").Suggestive linguistic interpretations areprovided in square brackets.A-F summarize the learning algorithm.A) There are categories.
Complex categories arebuilt from basic categories using /, \, and *:Basic categoriesc1,  c2,  c3, ... ,  c12345 , ...Complex categoriesc1\c12345,  c2/c3,  c4*c5,  c2/(c3\(c4*c5))B) A lexicon is a mapping of lexemes [wordtypes represented in phonetic or enriched-orthographic encoding] onto categories.C) An input segment is an instance of a lexeme[an input word].
A solo is a string of segments[an utterance delimited by e.g.
turntakes andpauses].
A corpus is a bag of soli [a transcript ofa conversation].D) Applying an update L:C1?C2 in lexicon Lexmeans changing the mapping of L in Lex fromC1 to C2.
Valid changes are minimal, i.e.
C2 isconstrued from C1 by adding or removing 1basic category (using \, /, or *).E) The learning process is guided by a measureof disorder.
The disorder function Dis takes asequent ?
[the lexical mapping of an utterance]returning the number of uninterpretable atoms in?, i.e.
?+s and ?
?s in a (maximally linked) proof.Dis(?
)=0 iff ?
is Lambek valid.
Examples:Dis(  ca/cb  cb  ?
ca  ) =  0Dis(  ca/cb  cb  ?
cc  ) =  2Dis(  cb  ca/cb  ?
cc  ) =  4Dis(  ca/cb  cc  cb  ?
ca  ) =  1Dis(  ca/cc  cb  ca\cc  ?
ca  ) =  2DIS(Lex,K) is the total amount of disorder intraining corpus K wrt.
lexicon Lex, i.e.
the sumof Dis-values for all soli in K as mapped by Lex.F) A learning session is an iterative process.
Ineach iteration i a suitable update Ui is applied inthe lexicon Lexi?1 producing Lexi .
Quantifyingover all possible updates, Ui is picked so as tomaximize the drop in disorder (DisDrop):DisDrop  =  DIS(Lexi?1,K) ?
DIS(Lexi,K)The session terminates when no suitable updateremains.It is possible to GraSp efficiently and yetpreserve logical completeness.
See Henrichsen(2000) for discussion and demonstrations.1.4 A staged learning sessionGiven this tiny corpus of four soli ('utterances')if you must you canif you must you must and if we must we mustif you must you can and if you can you mustif we must you must and if you must you must, GraSp produces the lexicon below.Lexeme InitialCategoryFinalCategory4TextbookCategorymust c1 c2\c1 NP\Syou c2 c2 NPif c3 (c3/c1)/c1 (S/S)/Sand c4 (c3\c4)/c3 (S\S)/Scan c5 c2\c1 NP\Swe c6 c2 NPAs shown, training corpora can be manufacturedso as to produce lexical structure fairly similar towhat is found in CG textbooks.
Such closesimilarity is however not typical of 'naturalistic'learning sessions ?
as will be clear in section 2.1.5  Why categorial grammar?In CG, all structural information is located in thelexicon.
Grammar rules (e.g.
VP ?
Vt N) andparts of speech (e.g.
'transitive verb', 'commonnoun') are treated as variants of the same formalkind.
This reduces the dimensionality of thelogical learning space, since a CG-based learnerneeds to induce just a single kind of structure.Besides its formal elegance, the CG basisaccomodates a particular kind of cognitivemodels, viz.
those that reject the idea of separatemental modules for lexical and grammaticalprocessing (e.g.
Bates 1997).
As we see it, ourformal approach allows us the luxury of nottaking sides in the heated debate of modularity.52 Learning from spoken languageThe current GraSp implementation completes alearning session in about one hour when fedwith our main corpus.6 Such a session spans2500-4000 iterations and delivers a lexicon rich4For perspicuity, two of the GraSped categories ?viz.
'can':(c2\c5)*(c5\c1)  and  'we':(c2/c6)*c6 ?
arereplaced in the table by functional equivalents.5A caveat: Even if we do share some tools with otherCG-based NL learning programmes, our goals aredistinct, and our results do not compare easily withe.g.
Kanazawa (1994), Watkinson (2000).
In terms ofphilosophy, GraSp seems closer to connectionistapproaches to NLL.6The Danish corpus BySoc (person interviews).
Size:1.0 mio.
words.
Duration: 100 hours.
Style: Labovianinterviews.
Transcription: Enriched orthography.Tagging: none.
Ref.
: http://www.cphling.dk/BySocin microparadigms and microstructure.
Lexicalstructure develops mainly around content wordswhile most function words retain their initialcategory.
The structure grown is almost fractalin character with lots of inter-connectedcategories, while the traditional large openclasses ?
nouns, verbs, prepositions, etc.
?
areabsent as such.
The following sections presentsome samples from the main corpus session(Henrichsen 2000 has a detailed description).2.1 Microparadigms{ "Den Franske", "Nyboder","S?lvgades", "Krebses" }These four lexemes ?
or rather lexeme clusters ?chose to co-categorize.
The collection does notresemble a traditional syntactic paradigm, yetthe connection is quite clear: all four itemsappeared in the training corpus as names ofprimary schools.Lexeme InitialCategoryFinalCategoryDen c882 c882Franske c1588 ((c882\c97)/c1588)*c1588Nyboder c97 c97S?lvgades c5351 (c97/c5351)*c5351Krebses c3865 (c3865/c288)*c97Skole c288 c97\c288The final categories are superficially different,but are easily seen to be functionally equivalent.The same session delivered several othermicroparadigms: a collection of family members(in English translation: brother, grandfather,younger-brother, stepfather, sister-in-law, etc.
),a class of negative polarity items, a class of massterms, a class of disjunctive operators, etc.
(Henrichsen 2000 6.4.2).GraSp-paradigms are usually small andalmost always intuitively 'natural' (not unlike thesmall categories of L1 learners reported by e.g.Lucariello 1985).2.2 MicrogrammarsGraSp'ed grammar rules are generally not of thekind studied within traditional phrase structuregrammar.
Still PSG-like 'islands' do occur, in theform of isolated networks of connected lexemes.Lexeme InitialCategoryFinalCategoryCon-nectionSankt c620 c620Sct.
c4713 (c620/c4713)*c4713Skt.
c3301 (c620/c3301)*c3301c620+Ann?
c3074 c620\(c22\c3074)Josef c2921 c620\c2921Joseph c3564 c620\c3564Knuds c6122 c620\c6122Pauls c1218 c620\c1218Paulsgade c2927 c620\c2927Pouls c2180 c620\c2180Poulsgade c4707 c620\c4707c620?Pauls c1218 c620\c1218 c1218+Gade c3849 c1218\(c9\c3849)Plads c1263 c1218\(c22\c1263)c1218?Centred around lexeme 'Pauls', a microgrammar(of street names) has evolved almost directlytranslatable into rewrite rules:7PP ?
'i'  N1  'Gade'PP ?
'p?'
N1  'Plads'PP ?
'p?'
N2N1 ?
X  'Pauls'N2 ?
X  'Ann?
'Nx ?
X  YX ?
'Sankt' | 'Skt.'
| 'Sct.
'Y ?
'Pauls' | 'Josef' | 'Joseph' | 'Knuds' | ...2.3 Idioms and locutionsConsider the five utterances of the main corpuscontaining the word 'rafle' (cast-diceINF):8det g?r den der er ikke noget at rafle om derder er ikke s?
meget at rafle omder er ikke noget og rafle oms?tte sig ned og rafle lidt med fyrene derat rafle om derOn most of its occurrences, 'rafle' takes part inthe idiom "der er ikke noget/meget og/at rafleom", often followed by a resumptive 'der'(literally: there is not anything/much and/to7Lexemes 'Sankt', 'Sct.
', and 'Skt.'
have in effectcocategorized, since it holds that (x/y)*y ?
x. Thiscocategorization is quite neat considering that GraSpis blind to the interior of lexemes.
c9 and c22 are thecategories of 'i' (in) and 'p?'
(on).8In writing, only two out of five would probablyqualify as syntactically well-formed sentences.cast-diceINF about (there), meaning: this is not asubject of negotiations).
Lexeme 'ikke' (categoryc8) occurs in the left context of 'rafle' more oftenthan not, and this fact is reflected in the finalcategory of 'rafle':rafle: ((c12\(c8\(c5\(c7\c5808))))/c7)/c42Similarly for the lexemes 'der' (c7), 'er' (c5), 'at'(c12), and 'om' (c42) which are also present in theargument structure of the category, while the topfunctor is the initial 'rafle' category (c5808).The minimal context motivating the fullrafle category is:... der ... er ... ikke ... at ... rafle ... om ... der ...("..." means that any amount and kind ofmaterial may intervene).
This template is a quiteaccurate description of an acknowledged Danishidiom.Such idioms have a specific categorialsignature in the GraSped lexicon: a rich, but flatargument structure (i.e.
analyzed solely by ?R)centered around a single low-frequency functor(analyzed by ?L).
Further examples with thesame signature:... det ... kan ... man ... ikke ... fort?nke ... i ...... det ... vil ... bl?se ... p?
...... ikke ... en ... kinamands ... chance ...?
all well-known Danish locutions.9There are of course plenty of simpler andfaster algorithms available for extracting idioms.Most such algorithms however include specificknowledge about idioms (topological andmorphological patterns, concepts of mutualinformation, heuristic and statistical rules, etc.
).Our algorithm has no such inclination: it doesnot search for idioms, but merely finds them.Observe also that GraSp may induce idiomtemplates like the ones shown even from corporawithout a single verbatim occurrence.9For entry rafle, Danish-Danish dictionary Politikenhas this paradigmatic example: "Der er ikke noget atrafle om".
Also fort?nke, bl?se, kinamands haveexamples near-identical with the learned templates.3 Learning from exotic corporaIn order to test GraSp as a general purposelearner we have used the algorithm on a range ofnon-verbal data.
We have had GraSp studymelodic patterns in musical scores and prosodicpatterns in spontaneous speech (and even dna-structure of the banana fly).
Results are not yetconclusive, but encouraging (Henrichsen 2002).When fed with HTML-formatted text,GraSp delivers a lexical patchwork of linguisticstructure and HTML-structure.
GraSp'suncritical appetite for context-free structuremakes it a candidate for intelligent web-crawling.
We are preparing an experiment with alarge number of cloned learners to be let loose inthe internet, reporting back on the structure ofthe documents they see.
Since GraSp producesformatting definitions as output (rather thanrequiring it as input), the algorithm could savethe www-programmer the troubles of preparinghis web-crawler for this-and-that format.Of course such experiments are side-issues.However, as discussed in the next section,learning from non-verbal sources may serve asan inspiration in the L1 learning domain also.4 Towards a model of L1 acquisition4.1 Artificial language learningTraining infants in language tasks withinartificial (i.e.
semantically empty) languages isan established psycho-linguistic method.
Infantshave been shown able to extract structuralinformation ?
e.g.
rules of phonemicsegmentation, prosodic contour, and evenabstract grammar (Cutler 1994, Gomez 1999,Ellefson 2000) ?
from streams of carefullydesigned nonsense.
Such results are an importantsource of inspiration for us, since theexperimental conditions are relatively easy tosimulate.
We are conducting a series of 'retakes'with the GraSp learner in the subject's role.Below we present an example.In an often-quoted experiment, psychologistJenny Saffran and her team had eight-months-old infants listening to continuous streams ofnonsense syllables: ti, do, pa, bu, la, go, etc.Some streams were organized in three-syllable'words' like padoti and golabu (repeated inrandom order) while others consisted of thesame syllables in random order.
After just twominutes of listening, the subjects were able todistinguish the two kinds of streams.Conclusion: Infants can learn to identifycompound words on the basis of structural cluesalone, in a semantic vacuum.Presented with similar streams of syllables,the GraSp learner too discovers word-hood.Lexeme InitialCategoryFinalCategory10pa c2 c2do c1 (c2\c1)/c3ti c3 c3go c5 c5la c6 c6bu c4 c6\(c5\c4)... ...
...It may be objected that such streams ofpresegmented syllables do not represent theexperimental conditions faithfully, leaping overthe difficult task of segmentation.
While we donot yet have a definitive answer to thisobjection, we observe that replacing "pa do ti gola bu (..)" by "p a d o t i g o l a b u (..)" has theGraSp learner discover syllable-hood and word-hood on a par.114.2 Naturalistic language learningEven if human learners can demonstrably learnstructural rules without access to semantic andpragmatic cues, this is certainly not the typicalL1 acquisition scenario.
Our current learningmodel fails to reflect the natural conditions in anumber of ways, being a purely syntacticcalculus working on symbolic input organized inwell-delimited strings.
Natural learning, incontrast, draws on far richer input sources:?
continuous (unsegmented) input streams?
suprasegmental (prosodic) information?
sensory data?
background knowledge10As seen, padoti has selected do for its functionalhead, and golabu, bu.
These choices are arbitrary.11The very influential Eimas (1971) showed one-month-old infants to be able to distinguish /p/ and /b/.Many follow-ups have established that phonemicsegmentation develops very early and may be innate.Any model of first language acquisition must beprepared to integrate such information sources.Among these, the extra-linguistic sources areperhaps the most challenging, since theyintroduce a syntactic-semantic interface in themodel.
As it seems, the formal simplicity of one-dimensional learning (cf.
sect.
1.5) is at stake.If, however, semantic information (such assensory data) could be 'syntactified' and includedin the lexical structure in a principled way,single stratum learning could be regained.
Weare currently working on a formal upgrading ofthe calculus using a framework of constructivetype theory (Coquant 1988, Ranta 1994).
InCTT, the radical lexicalism of categorialgrammar is taken even a step further,representing semantic information in the samedata structure as grammatical and lexicalinformation.
This formal upgrading takes asubstantial refinement of the Dis function (cf.sect.
1.3 E) as the determination of 'structuraldisorder' must now include contextual reasoning(cf.
Henrichsen 1998).
We are pursuing a designwith ?+ and ??
as instructions to respectivelyinsert and search for information in a CTT-stylecontext.These formal considerations are reflectionsof our cognitive hypotheses.
Our aim is to studylearning as a radically data-driven processdrawing on linguistic and extra-linguisticinformation sources on a par ?
and we shouldlike our formal system to fit like a glove.5 Concluding remarksAs far as we know, GraSp is the first publishedalgorithm for extracting grammatical taxonomyout of untagged corpora of spoken language.12This in an uneasy situation, since if our findingsare not comparable to those of other approachesto grammar learning, how could our results bejudged ?
or falsified?
Important issues wideopen to discussion are: validation of results,psycho-linguistic relevance of the experimentalsetup, principled ways of surpassing the context-free limitations of Lambek grammar (inheritedin GraSp), just to mention a few.On the other hand, already the spin-offs ofour project (the collection of non-linguisticlearners) do inspire confidence in our tenets, we12The learning experiment sketched in Moortgat(2001) shares some of GraSp's features.think ?
even if the big issue of psychologicalrealism has so far only just been touched.The GraSp implementation referred to in thispaper is available for test runs athttp://www.id.cbs.dk/~pjuel/GraSpReferencesBates, E.; J.C. Goodman (1997) On the Inseparabilityof Grammar and the Lexicon: Evidence FromAcquisition, Aphasia, and Real-time Processing;Language and Cognitive Processes 12, 507-584Chomsky, N. (1980) Rules and Representations;Columbia Univ.
PressCoquant, T.; G. Huet (1988) The Calculus ofConstructions; Info.
& Computation 76, 95-120Cutler, A.
(1994) Segmentation Problems, RhythmicSolutions; Lingua 92, 81-104Eimas, P.D.
; E.D.
Siqueland; P.W.
Jusczyk (1971)Speech Perception in Infants; Science 171 303-306Ellefson, M.R.
; M.H.Christiansen (2000) SubjacencyConstraints Without Universal Grammar:Evidence from Artificial Language Learning andConnectionist Modelling; 22nd Ann.
Conference ofthe Cognitive Science Society, Erlbaum, 645-650Gomez, R.L.
; L.A. Gerken (1999) Artificial Gram-mar Learning by 1-year-olds Leads to Specific andAbstract Knowledge; Cognition 70 109-135Henrichsen, P.J.
(1998) Does the Sentence Exist?
DoWe Need It?
; in K. Korta et al (eds) Discourse,Interaction, and Communication; Kluwer Acad.Henrichsen, P.J.
(2000) Learning Within Grasp ?Interactive Investigations into the Grammar ofSpeech; Ph.D., http://www.id.cbs.dk/~pjuel/GraSpHenrichsen, P.J.
(2002) GraSp: Grammar LearningWith a Healthy Appetite (in prep.
)Hoekstra, H. et al (2000) Syntactic Annotation forthe Spoken Dutch Corpus Project; CLIN2000Kanazawa (1994) Learnable Classes of CG; Ph.D.Moortgat, M. (2001) Structural Equations inLanguage Learning; 4th LACL2001 1-16Nivre, J.; L. Gr?nqvist (2001) Tagging a Corpus ofSpoken Swedish: Int.
Jn.
of Corpus Ling.
6:1 47-78Ranta, A.
(1994) Type-Theoretical Grammar; OxfordSaffran, J.R. et al (1996) Statistical Learning By 8-Months-Old Infants; Science 274 1926-1928Watkinson S.; S. Manandhar (2000) UnsupervisedLexical Learning with CG; in Cussens J. et al (eds)Learning Language in Logic; Springer
