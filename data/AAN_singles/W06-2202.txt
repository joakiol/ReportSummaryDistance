Simple Information Extraction (SIE):A Portable and Effective IE SystemClaudio Giuliano and Alberto Lavelli and Lorenza RomanoITC-irstVia Sommarive, 1838050, Povo (TN)Italy{giuliano,lavelli,romano}@itc.itAbstractThis paper describes SIE (Simple Infor-mation Extraction), a modular informationextraction system designed with the goalof being easily and quickly portable acrosstasks and domains.
SIE is composed bya general purpose machine learning algo-rithm (SVM) combined with several cus-tomizable modules.
A crucial role in thearchitecture is played by Instance Filter-ing, which allows to increase efficiencywithout reducing effectiveness.
The re-sults obtained by SIE on several standarddata sets, representative of different tasksand domains, are reported.
The experi-ments show that SIE achieves performanceclose to the best systems in all tasks, with-out using domain-specific knowledge.1 IntroductionIn designing Information Extraction (IE) systemsbased on supervised machine learning techniques,there is usually a tradeoff between carefully tun-ing the system to specific tasks and domains andhaving a ?generic?
IE system able to obtain good(even if not the topmost) performance when ap-plied to different tasks and domains (requiring avery reduced porting time).
Usually, the formeralternative is chosen and system performance isoften shown only for a very limited number oftasks (sometimes even only for a single task), af-ter a careful tuning.
For example, in the Bio-entityRecognition Shared Task at JNLPBA 2004 (Kimet al, 2004) the best performing system obtaineda considerable performance improvement adopt-ing domain specific hacks.A second important issue in designing IE sys-tems concerns the fact that usually IE data sets arehighly unbalanced (i.e., the number of positive ex-amples constitutes only a small fraction with re-spect to the number of negative examples).
Thisfact has important consequences.
In some ma-chine learning algorithms the unbalanced distri-bution of examples can yield a significant loss inclassification accuracy.
Moreover, very large datasets can be problematic to process due to the com-plexity of many supervised learning techniques.For example, using kernel methods, such as wordsequence and tree kernels, can become prohibitivedue to the difficulty of kernel based algorithms,such as Support Vector Machines (SVM) (Cortesand Vapnik, 1995), to scale to large data sets.
Asa consequence, reducing the number of instanceswithout degrading the prediction accuracy is a cru-cial issue for applying advanced machine learningtechniques in IE, especially in the case of highlyunbalanced data sets.In this paper, we present SIE (Simple Informa-tion Extraction), an information extraction systembased on a supervised machine learning approachfor extracting domain-specific entities from docu-ments.
In particular, IE is cast as a classificationproblem by applying SVM to train a set of classi-fiers, based on a simple and general-purpose fea-ture representation, for detecting the boundaries ofthe entities to be extracted.SIE was designed with the goal of being easilyand quickly portable across tasks and domains.
Tosupport this claim, we conducted a set of exper-iments on several tasks in different domains andlanguages.
The results show that SIE is competi-tive with the state-of-the-art systems, and it oftenoutperforms systems customized to a specific do-main.SIE resembles the ?Level One?
of the ELIEalgorithm (Finn and Kushmerick, 2004).
How-9ever, a key difference between the two algorithmsis the capability of SIE to drastically reduce thecomputation time by exploiting Instance Filtering(Gliozzo et al, 2005a).
This characteristic allowsscaling from toy problems to real-world data setsmaking SIE attractive in applicative fields, such asbioinformatics, where very large amounts of datahave to be analyzed.2 A Simple IE systemSIE has a modular system architecture.
It is com-posed by a general purpose machine learning algo-rithm combined with several customizable com-ponents.
The system components are combinedin a pipeline, where each module constrains thedata structures provided by the previous ones.This modular specification brings significant ad-vantages.
Firstly, a modular architecture is sim-pler to implement.
Secondly, it allows to easilyintegrate different machine learning algorithms.Finally, it allows, if necessary, a fine tuning toa specific task by simply specializing few mod-ules.
Furthermore, it is worth noting that we testedSIE across different domains using the same basicconfiguration without exploiting any domain spe-cific knowledge, such as gazetteers, and ad-hocpre/post-processing.InstanceFilteringFeatureExtractionLearningAlgorithmTagMatcherClassificationAlgorithmInstanceFilteringFeatureExtractionLexiconTraining Corpus New DocumentsData ModelTaggedDocumentsFilter ModelExtractionScriptExtractionScriptFigure 1: The SIE Architecture.The architecture of the system is shown in Fig-ure 1.
The information extraction task is per-formed in two phases.
SIE learns off-line a set ofdata models from a specified labeled corpus, thenthe models are applied to tag new documents.In both phases, the Instance Filtering module(Section 3) removes certain tokens from the dataset in order to speed-up the whole process, whileFeature Extraction module (Section 4) is used toextract a pre-defined set of features from the to-kens.
In the training phase, the Learning Mod-ule (Section 5) learns two distinct models for eachentity, one for the beginning boundary and an-other for the end boundary (Ciravegna, 2000; Fre-itag and Kushmerick, 2000).
In the recognitionphase, as a consequence, the Classification mod-ule (Section 5) identifies the entity boundaries asdistinct token classifications.
A Tag Matcher mod-ule (Section 6) is used to match the boundary pre-dictions made by the Classification module.
Taskswith multiple entities are considered as multipleindependent single-entity extraction tasks (i.e.
SIEonly extracts one entity at a time).3 Instance FilteringThe purpose of the Instance Filtering (IF) mod-ule is to reduce the data set size and skewnessby discarding harmful and superfluous instanceswithout degrading the prediction accuracy.
Thisis a generic module that can be exploited by anysupervised system that casts IE as a classificationproblem.Instance Filtering (Gliozzo et al, 2005a) isbased on the assumption that uninformative wordsare not likely to belong to entities to recognize,being their information content very low.
A naiveimplementation of this assumption consists in fil-tering out very frequent words in corpora becausethey are less likely to be relevant than rare words.However, in IE relevant entities can be composedby more than one token and in some domains a fewof such tokens can be very frequent in the corpus.For example, in the field of bioinformatics, proteinnames often contain parentheses, whose frequencyin the corpus is very high.To deal with this problem, we exploit a set of In-stance Filters (called Stop Word Filters), includedin a Java tool called jInFil1.
These filters per-form a ?shallow?
supervision to identify frequentwords that are often marked as positive examples.The resulting filtering algorithm consists of twostages.
First, the set of uninformative tokens isidentified by training the term filtering algorithmon the training corpus.
Second, instances describ-ing ?uninformative?
tokens are removed from boththe training and the test sets.
Note that instancesare not really removed from the data set, but just1http://tcc.itc.it/research/textec/tools-resources/jinfil/10marked as uninformative.
In this way the learningalgorithm will not learn from these instances, butthey will still appear in the feature description ofthe remaining instances.A Stop Word Filter is fully specified by a list ofstop words.
To identify such a list, different fea-ture selection methods taken from the text catego-rization literature can be exploited.
In text catego-rization, feature selection is used to remove non-informative terms from representations of texts.
Inthis sense, IF is closely related to feature selection:in the former non-informative words are removedfrom the instance set, while in the latter they areremoved from the feature set.
Below, we describethe different metrics used to collect a stop wordlist from the training corpora.Information Content (IC) The most commonlyused feature selection metric in text categoriza-tion is based on document frequency (i.e, the num-ber of documents in which a term occurs).
Thebasic assumption is that very frequent terms arenon-informative for document indexing.
The fre-quency of a term in the corpus is a good indica-tor of its generality, rather than of its informationcontent.
From this point of view, IF consists ofremoving all tokens with a very low informationcontent2.Correlation Coefficient (CC) In text catego-rization the ?2 statistic is used to measure the lackof independence between a term and a category(Yang and Pedersen, 1997).
The correlation coef-ficient CC2 = ?2 of a term with the negative classcan be used to find those terms that are less likelyto express relevant information in texts.Odds Ratio (OR) Odds ratio measures the ra-tio between the odds of a term occurring in thepositive class, and the odds of a term occurring inthe negative class.
In text categorization the ideais that the distribution of the features on the rel-evant documents is different from the distributionon non-relevant documents (Raskutti and Kowal-czyk, 2004).
Following this assumption, a termis non-informative when its probability of being anegative example is sensibly higher than its prob-ability of being a positive example (Gliozzo et al,2005b).2The information content of a word w can be measuredby estimating its probability from a corpus by the equationI(w) = ?p(w) log p(w).An Instance Filter is evaluated by using twometrics: the Filtering Rate (?
), the total percent-age of filtered tokens in the data set, and the Pos-itive Filtering Rate (?+), the percentage of pos-itive tokens (wrongly) removed.
A filter is opti-mized by maximizing ?
and minimizing ?+; thisallows us to reduce as much as possible the dataset size preserving most of the positive instances.We fixed the accepted level of tolerance () on ?+and found the maximum ?
by performing 5-foldcross-validation on the training set.4 Feature ExtractionThe Feature Extraction module is used to extractfor each input token a pre-defined set of features.As said above, we consider each token an instanceto be classified as a specific entity boundary ornot.
To perform Feature Extraction an applica-tion called jFex3 was implemented.
jFex gener-ates the features specified by a feature extractionscript, indexes them, and returns the example set,as well as the mapping between the features andtheir indices (lexicon).
If specified, it only ex-tracts features for the instances not marked as ?un-informative?
by instance filtering.
jFex is stronglyinspired by FEX (Cumby and Yih, 2003), but itintroduces several improvements.
First of all, itprovides an enriched feature extraction language.Secondly, it makes possible to further extend thislanguage through a Java API, providing a flexi-ble tool to define task specific features.
Finally,jFex can output the example set in formats di-rectly usable by LIBSVM (Chang and Lin, 2001),SVMlight (Joachims, 1998) and SNoW (Carlsonet al, 1999).4.1 Corpus FormatThe corpus must be prepared in IOBE notation, aextension of the IOB notation.
Both notations donot allow nested and overlapping entities.
Tokensoutside entities are tagged with O, while the firsttoken of an entity is tagged with B-entity-type, thelast token is tagged E-entity-type, and all the to-kens inside the entity boundaries are tagged withI-entity-type, where entity-type is the type of themarked entity (e.g.
protein, person).Beside the tokens and their types, the nota-tion allows to represent general purpose and task-specific annotations defining new columns.
Blank3http://tcc.itc.it/research/textec/tools-resources/jfex.html.11lines can be used to specify sentence or documentboundaries.
Table 1 shows an example of a pre-pared corpus.
The columns are: the entity-type,the PoS tag, the actual token, the token index, andthe output of the instance filter (the ?uninforma-tive?
tokens are marked with 0) respectively.O TO To 2.12 0O VB investigate 2.13 0O IN whether 2.14 0O DT the 2.15 0B-cell type NN tumor 2.16 1O NN expression 2.17 1O IN of 2.18 0B-protein NN Beta-2-Microglobulin 2.19 1O ( ( 2.20 1B-protein NN Beta 2.21 1I-protein NN 2 2.22 1I-protein NN - 2.22 1E-protein NN M 2.22 1O ) ) 2.23 1Table 1: A corpus fragment represented in IOBEnotation.4.2 Extraction LanguageAs input to the begin and end classifiers, we usea bit-vector representation.
Each instance is rep-resented encoding all the following basic featuresfor the actual token and for all the tokens in a con-text window of fixed size (in the reported experi-ments, 3 words before and 3 words after the actualtoken):Token The actual token.POS The Part of Speech (PoS) of the token.Token Shapes This feature maps each token intoequivalence classes that encode attributessuch as capitalization, numerals, single char-acter, and so on.Bigrams of tokens and PoS tags.The Feature Extraction language allows toformally encode the above problem descriptionthrough a script.
Table 2 provides the extractionscript used in all the tasks4.
More details about theExtraction Language are provided in (Cumby andYih, 2003; Giuliano et al, 2005).4In JNLPBA shared task we added some orthographic fea-tures borrowed from the bioinformatics literature.-1 inc loc: w [-3, 3]-1 inc loc: coloc(w,w) [-3, 3]-1 inc loc: t [-3, 3]-1 inc loc: coloc(t,t) [-3, 3]-1 inc loc: sh [-3, 3]Table 2: The extraction script used in all tasks.5 Learning and Classification ModulesAs already said, we approach IE as a classifica-tion problem, assigning an appropriate classifica-tion label to each token in the data set except forthe tokens marked as irrelevant by the instance fil-ter.
As learning algorithm we use SVM-light5.
Inparticular, we identify the boundaries that indi-cate the beginning and the end of each entity astwo distinct classification tasks, following the ap-proach adopted in (Ciravegna, 2000; Freitag andKushmerick, 2000).
All tokens that begin(end) anentity are considered positive instances for the be-gin(end) classifier, while all the remaining tokensare negative instances.
In this way, two distinctmodels are learned, one for the beginning bound-ary and another for the end boundary.
All the pre-dictions produced by the begin and end classifiersare then paired by the Tag Matcher module.When we have to deal with more than one en-tity (i.e., with a multi-class problem) we train 2nbinary classifiers (where n is the number of entity-types for the task).
Again, all the predictions arepaired by the Tag Matcher module.6 Tag MatcherAll the positive predictions produced by the beginand end classifiers are paired by the Tag Matchermodule.
If nested or overlapping entities occur,even if they are of different types, the entity withthe highest score is selected.
The score of eachentity is proportional to the entity length probabil-ity (i.e., the probability that an entity has a certainlength) and the scores assigned by the classifiers tothe boundary predictions.
Normalizing the scoresmakes it possible to consider the score function asa probability distribution.
The entity length distri-bution is estimated from the training set.For example, in the corpus fragment of Table 3the begin and end classifiers have identified fourpossible entity boundaries for the speaker of aseminar.
In the table, the left column shows the5http://svmlight.joachims.org/12Table 3: A corpus fragment with multiple predic-tions.O TheO speakerO willO beB-speaker Mr. B-speaker (0.23)I-speaker John B-speaker (0.1), E-speaker (0.12)E-speaker Smith E-speaker (0.34)O .Table 4: The length distribution for the entityspeaker.entity len 1 2 3 4 5 ...P(entity len) 0.10 0.33 0.28 0.02 0.01 ...actual label, while the right column shows the pre-dictions and their normalized scores.
The match-ing algorithm has to choose among three mutu-ally exclusive candidates: ?Mr.
John?, ?Mr.
JohnSmith?
and ?John Smith?, with scores 0.23 ?0.12 ?
0.33 = 0.009108, 0.23 ?
0.34 ?
0.28 =0.021896 and 0.1 ?
0.34 ?
0.33 = 0.01122, re-spectively.
The length distribution for the entityspeaker is shown in Table 4.
In this example, thematcher, choosing the candidate that maximizesthe score function, namely the second one, extractsthe actual entity.7 EvaluationIn order to demonstrate that SIE is domain andlanguage independent we tested it on several tasksusing exactly the same configuration.
The tasksand the experimental settings are described in Sec-tion 7.1.
The results (Section 7.2) show that theadopted filtering technique decreases drasticallythe computation time while preserving (and some-times improving) the overall accuracy of the sys-tem.7.1 The TasksSIE was tested on the following IE benchmarks:JNLPBA Shared Task This shared task (Kimet al, 2004) is an open challenge task proposedat the ?International Joint Workshop on NaturalLanguage Processing in Biomedicine and its Ap-plications?6.
The data set consists of 2, 404 MED-LINE abstracts from the GENIA project (Kim et6http://research.nii.ac.jp/?collier/workshops/JNLPBA04st.htm.al., 2003), annotated with five entity types: DNA,RNA, protein, cell-line, and cell-type.
The GE-NIA corpus is split into two partitions: training(492,551 tokens), and test (101,039 tokens).
Thefraction of positive examples with respect to thetotal number of tokens in the training set variesfrom 0.2% to 6%.CoNLL 2002 & 2003 Shared Tasks Theseshared tasks (Tjong Kim Sang, 2002; TjongKim Sang and De Meulder, 2003)7 concernlanguage-independent named entity recognition.Four types of named entities are considered:persons (PER), locations (LOC), organizations(ORG) and names of miscellaneous (MISC) en-tities that do not belong to the previous threegroups.
SIE was applied to the Dutch and Englishdata sets.
The Dutch corpus is divided into threepartitions: training and validation (on the whole258, 214 tokens), and test (73, 866 tokens).
Thefraction of positive examples with respect to thetotal number of tokens in the training set variesfrom 1.1% to 2%.
The English corpus is dividedinto three partitions: training and validation (onthe whole 274, 585 tokens), and test (50, 425 to-kens).
The fraction of positive examples with re-spect to the total number of tokens in the trainingset varies from 1.6% to 3.3%.TERN 2004 The TERN (Time ExpressionRecognition and Normalization) 2004 Evaluation8requires systems to detect and normalize temporalexpressions occurring in English text (SIE did notaddress the normalization part of the task).
TheTERN corpus is divided into two partitions: train-ing (249,295 tokens) and test (72,667 tokens).
Thefraction of positive examples with respect to thetotal number of tokens in the training set is about2.1%.Seminar Announcements The Seminar An-nouncements (SA) collection (Freitag, 1998) con-sists of 485 electronic bulletin board postings.
Thepurpose of each document in the collection is toannounce or relate details of an upcoming talk orseminar.
The documents were annotated for fourentities: speaker, location, stime, and etime.
Thecorpus is composed by 156, 540 tokens.
The frac-tion of positive examples varies from about 1% to7http://www.cnts.ua.ac.be/conll2002/ner/, http://www.cnts.ua.ac.be/conll2003/ner/.8http://timex2.mitre.org/tern.html.13Metric  ?train/test R P F1 T0 66.4 67.0 66.7 615CC 1 64.1/62.3 67.5 67.3 67.4 4202.5 80.1/78.0 66.6 69.1 67.8 2265 88.9/86.4 64.8 68.1 66.4 109OR 1 70.7/68.9 68.3 67.3 67.8 3082.5 81.0/79.1 67.5 68.3 67.9 1935 87.8/85.6 65.4 68.2 66.8 114IC 1 37.3/36.9 58.5 65.7 61.9 5702.5 38.4/38.0 56.9 65.4 60.9 5585 39.5/38.9 55.6 65.5 60.1 552Zhou and Su (2004) 76.0 69.4 72.6baseline 52.6 43.6 47.7Table 5: Filtering Rate, Micro-averaged Recall,Precision, F1 and Time for JNLPBA.Metric  ?train/test R P F1 T0 73.6 78.7 76.1 134CC 1 64.4/64.4 71.6 79.9 75.5 702.5 75.1/73.3 72.8 80.3 76.4 505 88.6/84.2 66.6 64.7 65.6 24OR 1 71.5/71.6 72.0 78.3 75.0 612.5 82.1/80.7 73.6 78.9 76.2 395 90.5/86.1 66.8 64.5 65.6 19IC 1 47.3/47.5 67.0 79.2 72.6 1012.5 51.3/51.5 65.9 79.3 72.0 955 55.7/56.0 63.8 78.9 70.5 89Carreras et al (2002) 76.3 77.8 77.1baseline 45.4 81.3 58.3Table 6: Filtering Rate, Micro-averaged Recall,Precision, F1 and total computation time forCoNLL-2002 (Dutch).about 2%.
The entire document collection is ran-domly partitioned five times into two sets of equalsize, training and test (Lavelli et al, 2004).
Foreach partition, learning is performed on the train-ing set and performance is measured on the corre-sponding test set.
The resulting figures are aver-aged over the five test partitions.7.2 ResultsThe experimental results in terms of filtering rate,recall, precision, F1, and computation time forJNLPBA, CoNLL-2002, CoNLL-2003, TERN andSA are given in Tables 5, 6, 7, 8 and 9 respectively.To show the differences among filtering strategiesfor JNLPBA, CoNLL-2002, TERN 2004 we usedCC, OR and IC filters, while the results for SAand CoNLL-2003 are reported only for OR filter(which usually produces the best performance).For all filters we report results obtained by set-ting four different values for parameter , the max-imum value allowed for the Filtering Rate of pos-itive examples.
 = 0 means that no filter is used.Metric  ?train/test R P F1 T0 76.7 90.5 83.1 228OR 1 70.4/83.9 78.2 88.1 82.8 742.5 83.6/95.6 76.4 62.6 68.8 335 90.5/97.2 75.3 66.5 70.7 14Florian et al (2003) 88.5 89.0 88.8baseline 50.9 71.9 59.6Table 7: Filtering Rate, Micro-averaged Recall,Precision, F1 and total computation time forCoNLL-2003 (English).Metric  ?train/test R P F1 T0 77.9 89.8 83.4 82CC 1 41.8/41.2 76.6 90.7 83.1 572.5 64.5/62.8 60.3 88.6 71.7 415 86.9/81.7 59.7 76.0 66.9 14OR 1 56.4/54.6 77.5 91.1 83.8 482.5 69.4/66.7 59.8 88.1 71.2 365 82.9/79.0 59.5 88.6 71.2 20IC 1 17.8/17.4 74.9 91.2 82.3 482.5 24.0/23.3 74.8 91.5 82.3 365 27.6/27.1 75.0 91.5 82.5 20Table 8: Filtering Rate, Micro-averaged Recall,Precision, F1 and total computation time forTERN.The results indicate that both CC and OR do ex-hibit good performance and are far better than ICin all the tasks.
For example, in the JNLPBA dataset, OR allows to remove more than 70% of the in-stances, losing less than 1% of the positive exam-ples.
These results pinpoint the importance of us-ing a supervised metric to collect stop words.
Theresults also highlight that both CC and OR are ro-bust against overfitting, because the difference be-tween the filtering rates in the training and test setsis minimal.
We also report a significant reductionof the data skewness.
Table 10 shows that all the IFtechniques reduce sensibly the skewness ratio, theratio between the number of negative and positiveexamples, on the JNLPBA data set9.
As expected,both CC and OR consistently outperform IC.The computation time10 reported includes thetime to perform the overall process of training andtesting the boundary classifiers for each entity11.The results indicate that both CC and OR are farsuperior to IC, allowing a drastic reduction of thetime.
Supervised IF techniques are then particu-9We only report results for this data set as it exhibits thehighest skewness ratios.10All the experiments have been performed using a dual1.66 GHz Power Mac G5.11Execution time for filter optimization is not reported be-cause it is negligible.14Metric  ?train/test R P F1 T0 81.3 92.5 86.6 179OR 1 53.6/86.2 81.5 92.1 86.5 912.5 69.1/90.8 81.6 90.5 85.9 445 74.7/90.8 81.0 85.0 83.0 31Table 9: Filtering Rate, Micro-averaged Recall,Precision, F1 and total computation time for SA.entity  CC OR ICprotein 0 17.1 17.1 17.11 7.5 3.8 9.62.5 3.0 2.5 9.05 1.5 1.4 8.8DNA 0 59.3 59.3 59.31 26.4 18.5 33.22.5 14.7 12.6 31.75 8.3 8.6 32.4RNA 0 596.2 596.2 596.21 250.7 253.1 288.42.5 170.4 170.1 274.55 92.4 111.1 280.7cell type 0 72.9 72.9 72.91 13.8 13.4 43.22.5 6.3 6.5 43.95 3.4 4.4 44.5cell line 0 146.4 146.4 146.41 40.4 41.6 87.72.5 24.2 25.9 87.55 13.6 14.6 89.6Table 10: Skewness ratio of each entity forJNLPBA.larly convenient when dealing with large data sets.For example, using the CC metric the time re-quired by SIE to perform the JNLPBA task is re-duced from 615 to 109 minutes (see Table 5).Both OR and CC allow to drastically reducethe computation time and maintain the predictionaccuracy12 with small values of .
Using OR,for example, with  = 2.5% on JNLPBA, F1 in-creases from 66.7% to 67.9%.
On the contrary,for CoNLL-2002 and TERN, for  > 2.5% and > 1% respectively, the performance of all thefilters rapidly declines.
The explanation for thisbehavior is that, for the last two tasks, the differ-ence between the filtering rates on the training andtest sets becomes much larger for  > 2.5% and > 1%, respectively.
That is, the data skewnesschanges significantly from the training to the testset.
It is not surprising that an extremely aggres-sive filtering step reduces too much the informa-tion available to the classifiers, leading the overall12For JNLPBA, CoNLL 2002 & 2003 and Tern 2004, re-sults are obtained using the official evaluation software madeavailable by the organizers of the tasks.performance to decrease.SIE achieves results close to the best systems inall tasks13.
It is worth noting that state-of-the-artIE systems often exploit external, domain-specificinformation (e.g.
gazetteers (Carreras et al, 2002)and lexical resources (Zhou and Su, 2004)) whileSIE adopts exactly the same feature set and doesnot use any external or task dependent knowledgesource.8 Conclusion and Future WorkThe portability, the language independence andthe efficiency of SIE suggest its applicability inpractical problems (e.g.
semantic web, infor-mation extraction from biological data) in whichhuge collections of texts have to be processed ef-ficiently.
In this perspective we are pursuing therecognition of bio-entities from several thousandsof MEDLINE abstracts.
In addition, the effective-ness of instance filtering will allow us to experi-ment with complex kernel methods.
For the fu-ture, we plan to implement more aggressive in-stance filtering schemata for Entity Recognition,by performing a deeper semantic analysis of thetexts.AcknowledgmentsSIE was developed in the context of the IST-Dot.Kom project (http://www.dot-kom.org), sponsored by the European Commission aspart of the Framework V (grant IST-2001-34038).Claudio Giuliano and Lorenza Romano have beensupported by the ONTOTEXT project, funded bythe Autonomous Province of Trento under theFUP-2004 research program.ReferencesAndrew J. Carlson, ChadM.
Cumby, Jeff L. Rosen, andDan Roth.
1999.
SNoW user?s guide.
TechnicalReport UIUCDCS-DCS-R-99-210, Department ofComputer Science, University of Illinois at Urbana-Champaign, April.Xavier Carreras, Llu?
?s Ma?rques, and Llu?
?s Padro?.2002.
Named entity extraction using adaboost.
InProceedings of CoNLL-2002, Taipei, Taiwan.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a library for support vector machines.13Note that the TERN results cannot be disclosed, so no di-rect comparison can be provided.
For the reasons mentionedin (Lavelli et al, 2004), direct comparison cannot be providedfor Seminar Announcements as well.15Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Fabio Ciravegna.
2000.
Learning to tag for infor-mation extraction.
In F. Ciravegna, R. Basili, andR.
Gaizauskas, editors, Proceedings of the ECAIworkshop on Machine Learning for Information Ex-traction, Berlin.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine Learning, 20(3):273?297.Chad Cumby and W. Yih.
2003.
FEX user guide.Technical report, Department of Computer Science,University of Illinois at Urbana-Champaign, April.Aidan Finn and Nicholas Kushmerick.
2004.
Multi-level boundary classification for information extrac-tion.
In Proceedings of the 15th European Confer-ence on Machine Learning, Pisa, Italy.Radu Florian, Abe Ittycheriah, Hongyan Jing, andTong Zhang.
2003.
Named entity recognitionthrough classifier combination.
In Walter Daele-mans and Miles Osborne, editors, Proceedings ofCoNLL-2003, pages 168?171.
Edmonton, Canada.Dayne Freitag and Nicholas Kushmerick.
2000.Boosted wrapper induction.
In Proceedings of the17th National Conference on Artificial Intelligence(AAAI 2000), pages 577?583.Dayne Freitag.
1998.
Machine Learning for Informa-tion Extraction in Informal Domains.
Ph.D. thesis,Carnegie Mellon University.Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-mano.
2005.
Simple information extraction (SIE).Technical report, ITC-irst.Alfio Massimiliano Gliozzo, Claudio Giuliano, andRaffaella Rinaldi.
2005a.
Instance filtering for en-tity recognition.
SIGKDD Explorations (special is-sue on Text Mining and Natural Language Process-ing), 7(1):11?18, June.Alfio Massimiliano Gliozzo, Claudio Giuliano, andRaffaella Rinaldi.
2005b.
Instance pruning by fil-tering uninformative words: an Information Extrac-tion case study.
In Proceedings of the Sixth Interna-tional Conference on Intelligent Text Processing andComputational Linguistics (CICLing-2005), MexicoCity, Mexico, 13-19 February.T.
Joachims.
1998.
Making large-scale supportvector machine learning practical.
In A. SmolaB.
Scho?lkopf, C. Burges, editor, Advances in Ker-nel Methods: Support Vector Machines.
MIT Press,Cambridge, MA.J.
Kim, T. Ohta, Y. Tateishi, and J. Tsujii.
2003.
Ge-nia corpus - a semantically annotated corpus for bio-textmining.
Bioinformatics, 19(Suppl.1):180?182.J.
Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-lier.
2004.
Introduction to the bio-entity recog-nition task at JNLPBA.
In N. Collier, P. Ruch,and A. Nazarenko, editors, Proceedings of the In-ternational Joint Workshop on Natural LanguageProcessing in Biomedicine and its Applications(JNLPBA-2004), pages 70?75, Geneva, Switzer-land, August 28?29.A.
Lavelli, M. Califf, F. Ciravegna, D. Freitag, C. Giu-liano, N. Kushmerick, and L. Romano.
2004.
IEevaluation: Criticisms and recommendations.
InAAAI-04 Workshop on Adaptive Text Extraction andMining (ATEM-2004), San Jose, California.Bhavani Raskutti and Adam Kowalczyk.
2004.Extreme re-balancing for SVMs: a case study.SIGKDD Explor.
Newsl., 6(1):60?69.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CoNLL-2003 shared task:Language-independent named entity recognition.
InWalter Daelemans and Miles Osborne, editors, Pro-ceedings of CoNLL-2003, pages 142?147.
Edmon-ton, Canada.Erik F. Tjong Kim Sang.
2002.
Introduction to theCoNLL-2002 shared task: Language-independentnamed entity recognition.
In Proceedings ofCoNLL-2002, pages 155?158.
Taipei, Taiwan.Yiming Yang and Jan O. Pedersen.
1997.
A compara-tive study on feature selection in text categorization.In Douglas H. Fisher, editor, Proceedings of the14th International Conference on Machine Learning(ICML-97), pages 412?420, Nashville, US.
MorganKaufmann Publishers, San Francisco, US.Guo Dong Zhou and Jian Su.
2004.
Exploring deepknowledge resources in biomedical name recogni-tion.
In Proceedings of 2004 Joint Workshop on Nat-ural Processing in Biomedicine and its Applications,Geneva, Switzerland.16
