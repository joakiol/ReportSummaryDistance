A Context-Theoretic Framework forCompositionality in Distributional SemanticsDaoud Clarke?University of HertfordshireFormalizing ?meaning as context?
mathematically leads to a new, algebraic theory of meaning,in which composition is bilinear and associative.
These properties are shared by other methodsthat have been proposed in the literature, including the tensor product, vector addition, point-wise multiplication, and matrix multiplication.Entailment can be represented by a vector lattice ordering, inspired by a strengthenedform of the distributional hypothesis, and a degree of entailment is defined in the form of aconditional probability.
Approaches to the task of recognizing textual entailment, including theuse of subsequence matching, lexical entailment probability, and latent Dirichlet alocation, canbe described within our framework.1.
IntroductionThis article presents the thesis that defining meaning as context leads naturally toa model in which meanings of strings are represented as elements of an associativealgebra over the real numbers, and entailment is described by a vector lattice ordering.This model is general enough to encompass several proposed methods of compositionin vector-based representations of meaning.In recent years, the abundance of text corpora and computing power has allowedthe development of techniques to analyze statistical properties of words.
For exampletechniques such as latent semantic analysis (Deerwester et al 1990) and its variants,and measures of distributional similarity (Lin 1998; Lee 1999), attempt to derive aspectsof the meanings of words by statistical analysis, and statistical information is oftenused when parsing to determine sentence structure (Collins 1997).
These techniqueshave proved useful in many applications within computational linguistics and naturallanguage processing (Grefenstette 1994; Schu?tze 1998; Bellegarda 2000; Choi, Wiemer-Hastings, and Moore 2001; Lin 2003; McCarthy et al 2004), arguably providing evidencethat they capture something about the nature of words that should be included inrepresentations of their meaning.
However, it is very difficult to reconcile these tech-niques with existing theories of meaning in language, which revolve around logicaland ontological representations.
The new techniques, almost without exception, can beviewed as dealing with vector-based representations of meaning, placing meaning (atleast at the word level) within the realm of mathematics and algebra; conversely theolder theories of meaning dwell in the realm of logic and ontology.
It seems there is?
Gorkana Group, Discovery House, 28?48 Banner Street, London EC1Y8QE.E-mail: daoud.clarke@gorkana.com.Submission received: 29 October 2008; revised submission received: 28 April 2011; accepted for publication:29 May 2011.?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 1no unifying theory of meaning to provide guidance to those making use of the newtechniques.The problem appears to be a fundamental one in computational linguistics becausethe whole foundation of meaning seems to be in question.
The older, logical theoriesoften subscribe to a model-theoretic philosophy of meaning (Kamp and Reyle 1993;Blackburn and Bos 2005).
According to this approach, sentences should be translated toa logical form that can be interpreted as a description of the state of the world.
The newvector-based techniques, on the other hand, are often closer in spirit to the philosophyof meaning as context, the idea that the meaning of an expression is determined by howit is used.
This is an old idea with origins in the philosophy of Wittgenstein (1953), whosaid that ?meaning just is use,?
Firth?s (1968) ?You shall know a word by the companyit keeps,?
and the distributional hypothesis of Harris (1968), that words will occur insimilar contexts if and only if they have similar meanings.
This hypothesis is justifiedby the success of techniques such as latent semantic analysis as well as experimentalevidence (Miller and Charles 1991).
Although the two philosophies are not obviouslyincompatible?especially because the former applies mainly at the sentence level andthe latter mainly at the word level?it is not clear how they relate to each other.The problem of how to compose vector representations of meanings of words hasrecently received increased attention (Clark, Coecke, and Sadrzadeh 2008; Mitchell andLapata 2008; Widdows 2008; Erk and Pado?
2009; Baroni and Zamparelli 2010; Guevara2011; Preller and Sadrzadeh 2011) although the problem has been considered in earlierwork (Smolensky 1990; Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998;Kintsch 2001).
A solution to this problem would have practical as well as philosophicalbenefits.
Current techniques such as latent semantic analysis work well at the wordlevel, but we cannot extend them much beyond this, to the phrase or sentence level,without quickly encountering the data-sparseness problem: There are not enough occur-rences of strings of words to determine what their vectors should be merely by lookingin corpora.
If we knew how such vectors should compose then we would be able toextend the benefits of the vector based techniques to the many applications that requirereasoning about the meaning of phrases and sentences.This article describes the results of our own efforts to identify a theory that can unitethese two paradigms, introduced in the author?s DPhil thesis (Clarke 2007).
In addition,we also discuss the relationship between this theory and methods of composition thathave recently been proposed in the literature, showing that many of them can beconsidered as falling within our framework.Our approach in identifying the framework is summarized in Figure 1: Inspired by the philosophy of meaning as context and vector-basedtechniques we developed a mathematical model of meaning as context,in which the meaning of a string is a vector representing contexts inwhich that string occurs in a hypothetical infinite corpus. The theory on its own is not useful when applied to real-world corporabecause of the problem of data sparseness.
Instead we examine themathematical properties of the model, and abstract them to form aframework which contains many of the properties of the model.Implementations of the framework are called context theories becausethey can be viewed as theories about the contexts in which stringsoccur.
By analogy with the term ?model-theoretic?
we use the term?context-theoretic?
for concepts relating to context theories, thus wecall our framework the context-theoretic framework.42Clarke A Context-Theoretic Framework for Distributional SemanticsFigure 1Our approach in developing the context-theoretic framework. In order to ensure that the framework was practically useful, contexttheories were developed in parallel with the framework itself.
The aimwas to be able to describe existing approaches to representing meaningwithin the framework as fully as possible.In developing the framework we were looking for specific properties; namely, wewanted it to: provide some guidelines describing in what way the representation of aphrase or sentence should relate to the representations of the individualwords as vectors; require information about the probability of a string of words to beincorporated into the representation; provide a way to measure the degree of entailment between strings basedon the particular meaning representation; be general enough to encompass logical representations of meaning; and be able to incorporate the representation of ambiguity and uncertainty,including statistical information such as the probability of a parse or theprobability that a word takes a particular sense.The framework we present is abstract, and hence does not subscribe to a particularmethod for obtaining word vectors: They may be raw frequency counts, or vectors ob-tained by a method such as latent semantic analysis.
Nor does the framework provide arecipe for how to represent meaning in natural language; instead it provides restrictionson the set of possibilities.
The advantage of the framework is in ensuring that techniquesare used in a way that is well-founded in a theory of meaning.
For example, given vectorrepresentations of words, there is not one single way of combining these to give vectorrepresentations of phrases and sentences, but in order to fit within the framework thereare certain properties of the representation that need to hold.
Any method of combining43Computational Linguistics Volume 38, Number 1these vectors in which these properties hold can be considered within the frameworkand is thus justified according to the underlying theory; in addition the frameworkinstructs us as to how to measure the degree of entailment between strings according tothat particular method.The contribution of this article is as follows: We define the context-theoretic framework and introduce the mathematicsnecessary to understand it.
The description presented here is cleaner thanthat of Clarke (2007), and in addition we provide examples that shouldprovide intuition for the concepts we describe. We relate the framework to methods of composition that have beenproposed in the literature, namely:?
vector addition (Landauer and Dumais 1997; Foltz, Kintsch,and Landauer 1998);?
the tensor product (Smolensky 1990; Clark and Pulman 2007;Widdows 2008);?
the multiplicative models of Mitchell and Lapata (2008);?
matrix multiplication (Baroni and Zamparelli 2010; Rudolphand Giesbrecht 2010); and?
the approach of Clark, Coecke, and Sadrzadeh (2008).It is important to note that the purpose of describing related work in terms of ourframework is not merely to demonstrate the generality of our framework: In doingso, we identify previously ignored features of this work such as the lattice structurewithin the vector space.
This allows any one of these approaches to be endowed with anentailment property defined by this lattice structure, based on a philosophy of meaningas context.Although the examples described here show that existing approaches can be de-scribed within the framework and show some of its potential, they cannot demonstrateits full power.
The mathematical structures we make use of are extremely general, andwe hope that in the future many interesting discoveries will be made by exploring therealm we identify here.Our approach in defining the framework may be perceived as overly abstract;however, we believe this approach has many potential benefits, because approaches tocomposition which may have been considered unrelated (such as the tensor productand vector addition) are now shown to be related.
This means that when studyingsuch constructions, work can be avoided by considering the general case, for the samereason that class inheritance aids code reuse.
For example, definitions given in termsof the framework can be applied to all instances, such as our definition of a degreeof entailment.
We also hope to motivate people to prove theorems in terms of theframework, having demonstrated its wide applicability.The remainder of the article is as follows: In Section 2 we define our framework,introducing the necessary definitions, and showing how related work fits into theframework.
In Section 3 we introduce our motivating example, showing that a simplemathematical definition of the notions of ?corpus?
and ?context?
leads to an instanceof our framework.
In Section 4, we describe specific instances of our framework inapplication to the task of recognizing textual entailment.
In Section 5 we show how the44Clarke A Context-Theoretic Framework for Distributional Semanticssophisticated approach of Clark, Coecke, and Sadrzadeh (2008) can be described withinour framework.
Finally, in Section 6 we present our conclusions and plans for furtherwork.2.
Context TheoryIn this section, we define the fundamental concept of our concern, a context theory,and discuss its properties.
The definition is an abstraction of both the more commonlyused methods of defining composition in vector-based semantics and our motivatingexample of meaning as context, described in the next section.
Because of its relation tothis motivating example, a context theory can be thought of as a hypothesis describingin what contexts all strings occur.Definition 1 (Context Theory)A context theory is a tuple ?A,A,?,V,?
?, where A is a set (the alphabet), A is a unitalalgebra over the real numbers, ?
is a function from A to A, V is an abstract Lebesguespace, and ?
is an injective linear map from A to V.We will explain each part of this definition, introducing the necessary mathematicsas we proceed.
We assume the reader is familiar with linear algebra; see Halmos (1974)for definitions that are not included here.2.1 Algebra over a FieldWe have identified an algebra over a field (or simply algebra when there is no am-biguity) as an important construction because it generalizes nearly all the methods ofvector-based composition that have been proposed.
An algebra adds a multiplicationoperation to a vector space; the vector space is intended to describe meaning, and itis this multiplication operation that defines the composition of meaning in context-theoretic semantics.Definition 2 (Algebra over a Field)An algebra over a field is a vector space A over a field K together with a binaryoperation (a, b) ?
ab on A that is bilinear,a(?b+ ?c) = ?ab+ ?ac (1)(?a+ ?b)c = ?ac+ ?bc (2)and associative, (ab)c = a(bc) for all a, b, c ?
A and all ?,?
?
K. Some authors do notplace the requirement that an algebra is associative, in which case our definition wouldrefer to an associative algebra.
An algebra is called unital if it has a distinguished unityelement 1 satisfying 1x = x1 = x for all x ?
A.
We are generally only interested in realalgebras, where K is the field of real numbers, R.Example 1The square real-valued matrices of order n form a real unital associative algebra understandard matrix multiplication.
The vector operations are defined entry-wise.
The unityelement of the algebra is the identity matrix.45Computational Linguistics Volume 38, Number 1This means that our proposal is more general than that of Rudolph and Giesbrecht(2010), who suggest using matrix multiplication as a framework for distributionalsemantic composition.
The main differences in our proposal are as follows. We allow dimensionality to be infinite, instead of restricting ourselves tofinite-dimensional matrices. Matrix algebras form a ?-algebra, whereas we do not currently impose thisrequirement. Many of the vector spaces used in computational linguistics have animplicit lattice structure; we emphasize the importance of this structureand use the associated partial ordering to define entailment.The purpose of ?
in the context theory is to associate elements of the algebra withstrings of words.
Considering only the multiplication of A (and ignoring the vectoroperations), A is a monoid, because we assumed that the multiplication on A is asso-ciative.
Then ?
induces a monoid homomorphism a ?
a?
from A?
to A.
We denote themapped value of a ?
A?
by a?
?
A, which is defined as follows:a?
= ?(a1)?
(a2) .
.
.
?
(an) (3)where a = a1a2 .
.
.
an for ai ?
A, and we define ?
= 1, where  is the empty string.
Thus,the mapping defined by ?
allows us to associate an element of the algebra with everystring of words.The algebra is what tells us how meanings compose.
A crucial part of our thesisis that meanings can be represented by elements of an algebra, and that the type ofcomposition that can be defined using an algebra is general enough to describe thecomposition of meaning in natural language.
To go some way towards justifying this,we give several examples of algebras that describe methods of composition that havebeen proposed in the literature: namely, point-wise multiplication (Mitchell and Lapata2008), vector addition (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998),and the tensor product (Smolensky 1990; Clark and Pulman 2007; Widdows 2008).Example 2 (Point-wise Multiplication)Consider the n-dimensional real vector space Rn.
We describe a vector u ?
Rn in termsof its components as (u1,u2, .
.
.
un) with each ui ?
R. We can define a multiplication ?
onthis space by(u1,u2, .
.
.
,un) ?
(v1, v2, .
.
.
, vn) = (u1v1,u2v2, .
.
.
unvn) (4)It is easy to see that this satisfies the requirements for an algebra as specified earlier.Table 1 shows a simple example of possible occurrences for three terms in three differentcontexts, d1, d2, and d3, which may, for example, represent documents.
We use this todefine the mapping ?
from terms to vectors.
Thus, in this example, we have ?
(cat) =(0, 2, 3) and ?
(big) = (1, 3, 0).
Under point-wise multiplication, we would haveb?ig cat = ?
(big) ?
?
(cat) = (1, 3, 0) ?
(0, 2, 3) = (0, 6, 0) (5)46Clarke A Context-Theoretic Framework for Distributional SemanticsTable 1Example of possible occurrences for three terms in three different contexts.d1 d2 d3cat 0 2 3animal 2 1 2big 1 3 0One commonly used operation for composing vector-based representations ofmeaning is vector addition.
As noted by Rudolph and Giesbrecht (2010), this can bedescribed using matrix multiplication, by embedding an n-dimensional vector u into amatrix of order n+ 1:????????
u1 u2 ?
?
?
un0 ?
0 ?
?
?
00 0 ?
?
?
?
0............0 0 0 ?
?
?
????????
(6)where ?
= 1.
The set of all such matrices, for all real values of ?, forms a subalgebra ofthe algebra of matrices of order n+ 1.
A subalgebra of an algebraA is a sub-vector spaceof A which is closed under the multiplication of A.
This subalgebra can be equivalentlydescribed as follows:Example 3 (Additive Algebra)For two vectors u = (?,u1,u2, .
.
.
un) and v = (?, v1, v2 .
.
.
vn) in Rn+1, we define theadditive product  byu v = (?
?, ?v1 + ?u1, ?v2 + ?u2, .
.
.
?vn + ?un) (7)To verify that this multiplication makes Rn+1 an algebra, we can directly verify thebilinear and associativity requirements, or check that it is isomorphic to the subalgebraof matrices discussed previously.Using Table 1, we define ?+ so that it maps n-dimensional context vectors to Rn+1,where the first component is 1, so ?+(big) = (1, 1, 3, 0) and ?+(cat) = (1, 0, 2, 3) andb?ig cat = ?+(big) ?+(cat) = (1, 1, 5, 3) (8)Point-wise multiplication and addition are not ideal as methods for composingmeaning in natural language because they are commutative; although it is often usefulto consider the simpler, commutative case, natural language itself is inherently non-commutative.
One obvious method of composing vectors that is not commutative is thetensor product.
This method of composition can be viewed as a product in an algebra byconsidering the tensor algebra, which is formed from direct sums of all tensor powersof a base vector space.We assume the reader is familiar with the tensor product and direct sum (seeHalmos [1974] for definitions); we recall their basic properties here.
Let Vn denote avector space of dimensionality n (note that all vector spaces of a fixed dimensionality47Computational Linguistics Volume 38, Number 1are isomorphic).
Then the tensor product space Vn ?
Vm is isomorphic to a space Vnm ofdimensionality nm; moreover, given orthonormal bases B = {b1, b2, .
.
.
, bn} for Vn andC = {c1, c2, .
.
.
, cm} for Vm there is an orthonormal basis for Vnm defined by{bi ?
cj : 1 ?
i ?
n and 1 ?
j ?
m} (9)Example 4The multiplicative models of Mitchell and Lapata (2008) correspond to the class of finitedimensional algebras.
LetA be a finite-dimensional vector space.
Then every associativebilinear product on A can be described by a linear function T from A?A to A, asrequired in Mitchell and Lapata?s model.
To see this, consider the action of the product ?on two orthonormal basis vectors a and b of A.
This is a vector in A, thus we can defineT(a?
b) = a ?
b.
By considering all basis vectors, we can define the linear function T.If the tensor product can loosely be viewed as ?multiplying?
vector spaces, then thedirect sum is like adding them; the space Vn ?
Vm has dimensionality n+m and hasbasis vectors{bi ?
0 : 1 ?
i ?
n} ?
{0 ?
cj : 1 ?
j ?
m}; (10)it is usual to write b?
0 as b and 0 ?
c as c.Example 5 (Tensor Algebra)If V is a vector space, then we define T(V), the free algebra of tensor algebra generatedby V, as:T(V) = R?
V ?
(V ?
V) ?
(V ?
V ?
V) ?
?
?
?
(11)where we assume that the direct sum is commutative.
We can think of it as the directsum of all tensor powers of V, with R representing the zeroth power.
In order to makethis space an algebra, we define the product on elements of these tensor powers, viewedas subspaces of the tensor algebra, as their tensor product.
This is enough to define theproduct on the whole space, because every element can be written as a sum of tensorpowers of elements of V. There is a natural embedding from V to T(V), where eachelement maps to an element in the first tensor power.
Thus for example we can think ofu, u?
v, and u?
v+ w as elements of T(V), for all u, v,w ?
V.This product defines an algebra because the tensor product is a bilinear operation.Taking V = R3 and using ?
as the natural embedding from the context vector of a stringT(V), our previous example becomesb?ig cat = ?
(big) ?
?
(cat) (12)= (1, 3, 0) ?
(0, 2, 3) (13)?= (1(0, 2, 3), 3(0, 2, 3), 0(0, 2, 3)) (14)?= (0, 2, 3, 0, 6, 9, 0, 0, 0) (15)where the last two lines demonstrate how a vector in R3 ?
R3 can be described in theisomorphic space R9.48Clarke A Context-Theoretic Framework for Distributional Semantics2.2 Vector LatticesThe next part of the definition specifies an abstract Lebesgue space.
This is a specialkind of vector lattice, or even more generally, a partially ordered vector space.
Thislattice structure is implicit in most vector spaces used in computational linguistics, andan important part of our thesis is that the partial ordering can be interpreted as anentailment relation.Definition 3 (Partially Ordered Vector Space)A partially ordered vector spaceV is a real vector space together with a partial ordering?
such that:if x ?
y then x+ z ?
y+ zif x ?
y then ?x ?
?yfor all x, y, z ?
V, and for all ?
?
0.
Such a partial ordering is called a vector space orderon V. An element u of V satisfying u ?
0 is called a positive element; the set of allpositive elements of V is denoted V+.
If ?
defines a lattice on V then the space is calleda vector lattice or Riesz space.Example 6 (Lattice Operations on Rn)A vector lattice captures many properties that are inherent in real vector spaces whenthere is a distinguished basis.
In Rn, given a specific basis, we can write two vectors u andv as sequences of numbers: u = (u1,u2, .
.
.
un) and v = (v1, v2, .
.
.
vn).
This allows us todefine the lattice operations of meet ?
and join ?
asu ?
v = (min(u1, v1),min(u2, v2), .
.
.min(un, vn)) (16)u ?
v = (max(u1, v1),max(u2, v2), .
.
.max(un, vn)) (17)These are the component-wise minimum and maximum, respectively.
The partialordering is then given by u ?
v if and only if u ?
v = u, or equivalently un ?
vn for alln.
A graphical depiction of the meet operation is shown in Figure 2.The vector operations of addition and multiplication by scalar, which can be definedin a similar component-wise fashion, are nevertheless independent of the particularFigure 2Vector representations of the terms orange and fruit based on hypothetical occurrences in sixdocuments and their vector lattice meet (the darker shaded area).49Computational Linguistics Volume 38, Number 1basis chosen.
Conversely, the lattice operations depend on the choice of basis, so theoperations as defined herein would behave differently if the components were writtenusing a different basis.
We argue that it makes sense for us to consider these propertiesof vectors in the context of computational linguistics because we can often have a distin-guished basis: namely, the one defined by the contexts in which terms occur.
Of course itis true that techniques such as latent semantic analysis introduce a new basis which doesnot have a clear interpretation in relation to contexts; nevertheless they nearly alwaysidentify a distinguished basis which we can use to define the lattice operations.
Becauseour aim is a theory of meaning as context, we should include in our theory a descriptionof the lattice structure which arises out of consideration of these contexts.We argue that the mere association of words with vectors is not enough to constitutea theory of meaning?a theory of meaning must allow us to interpret these vectors.
Inparticular it should be able to tell us whether one meaning entails or implies another;indeed this is one meaning of the verb to mean.
Entailment is an asymmetric relation: ?xentails y?
does not have the same meaning as ?y entails x?.
Vector representations allowthe measurement of similarity or distance, through an inner product or metric; this is asymmetric relation, however, and so cannot be suitable for describing entailment.In propositional and first order logic, the entailment relation is a partial ordering;in fact it is a Boolean algebra, which is a special kind of lattice.
It seems natural toconsider whether the lattice structure that is inherent in the vector representations usedin computational linguistics can be used to model entailment.We believe our framework is suited to all vector-based representations of naturallanguage meaning, however the vectors are obtained.
Given this assumption, we canonly justify our assumption that the partial order structure of the vector space is suitableto represent the entailment relation by observing that it has the right kind of propertieswe would expect from this relation.There may be more justification for this assumption, however, based on the casewhere the vectors for terms are simply their frequencies of occurrences in n differentcontexts, so that they are vectors in Rn.
In this case, the relation ?
(x) ?
?
(y) meansthat y occurs at least as frequently as x in every context.
This means that y occurs inat least as wide a range of contexts as x, and occurs as least as frequently as x.
Thus thestatement ?x entails y if and only if ?
(x) ?
?(y)?
can be viewed as a stronger form of thedistributional hypothesis of Harris (1968).In fact, this idea can be related to the notion of distributional generality, introducedby Weeds, Weir, and McCarthy (2004) and developed by Geffet and Dagan (2005).
Aterm x is distributionally more general than another term y if x occurs in a subset of thecontexts that y occurs in.
The idea is that distributional generality may be connected tosemantic generality.
An example of this is the hypernymy or is-a relation that is used toexpress generality of concepts in ontologies; for example, the term animal is a hypernymof dog because a dog is an animal.
Weeds, Weir, and McCarthy (2004, p. 1019) explainthe connection to distributional generality as follows:Although one can obviously think of counter-examples, we would generally expect thatthe more specific term dog can only be used in contexts where animal can be used andthat the more general term animal might be used in all of the contexts where dog is usedand possibly others.
Thus, we might expect that distributional generality is correlatedwith semantic generality.
.
.Our proposal, in the case where words are represented by frequency vectors, canbe considered a stronger version of distributional generality, where the additional50Clarke A Context-Theoretic Framework for Distributional Semanticsrequirement is on the frequency of occurrences.
In practice, this assumption is unlikelyto be compatible with the ontological view of entailment.
For example the term entityis semantically more general than the term animal; however, entity is unlikely to occurmore frequently in each context, because it is a rarer word.
A more realistic foundationfor this assumption might be if we were to consider the components for a word torepresent the plausibility of observing the word in each context.
The question then,of course, is how such vectors might be obtained.
Another possibility is to attempt toweight components in such a way that entailment becomes a plausible interpretationfor the partial ordering relation.Even if we allow for such alternatives, however, in general it is unlikely that therelation will hold between any two strings, because u ?
v iff ui ?
vi for each component,ui, vi, of the two vectors.
Instead, we propose to allow for degrees of entailment.
We takea Bayesian perspective on this, and suggest that the degree of entailment should takethe form of a conditional probability.
In order to define this, however, we need someadditional structure on the vector lattice that allows it to be viewed as a description ofprobability, by requiring it to be an abstract Lebesgue space.Definition 4 (Banach Lattice)A Banach lattice V is a vector lattice together with a norm ?
?
?
such that V is completewith respect to ?
?
?.Definition 5 (Abstract Lebesgue Space)An abstract Lebesgue (or AL) space is a Banach lattice V such that?u+ v?
= ?u?+ ?v?
(18)for all u, v in V with u ?
0, v ?
0 and u ?
v = 0.Example 7 (p Spaces)Let u = (u1,u2, .
.
.)
be an infinite sequence of real numbers.
We can view ui as compo-nents of the infinite-dimensional vector u.
We call the set of all such vectors the sequencespace; it is a vector space where the operations are defined component-wise.
We definea set of norms, the p-norms, on the space of all such vectors by?u?p =(?i>0|ui|p)1/p(19)The space of all vectors u for which ?u?p is finite is called the p space.
Consideredas vector spaces, these are Banach spaces, because they are complete with respect tothe associated norm, and under the component-wise lattice operations, they are Banachlattices.
In particular, the 1 space is an abstract Lebesgue space under the 1 norm.The finite-dimensional real vector spaces Rn can be considered as special cases ofthe sequence spaces (consisting of vectors in which all but n components are zero) and,because they are finite-dimensional, we can use any of the p norms.
Thus, our previousexamples, in which ?
mapped terms to vectors in Rn, can be considered as mapping toabstract Lebesgue spaces if we adopt the 1 norm.51Computational Linguistics Volume 38, Number 12.3 Degrees of EntailmentWe propose that in vector-based semantics, a degree of entailment is more appropriatethan a black-and-white observation of whether or not entailment holds.
If we think ofthe vectors as describing ?degrees of meaning,?
it makes sense that we should then lookfor degrees of entailment.Conditional probability is closely connected to entailment: If A entails B, thenP(B|A) = 1.
Moreover, if A and B are mutually exclusive, then P(A|B) = P(B|A) = 0.
Itis thus natural to think of conditional probability as a degree of entailment.An abstract Lebesgue space has many of the properties of a probability space, wherethe set operations of a probability space are replaced by the lattice operations of thevector space.
This means that we can think of an abstract Lebesgue space as a vector-based probability space.
Here, events correspond to positive elements with the normless than or equal to 1; the probability of an event u is given by the norm (which we shallalways assume is the 1 norm), and the joint probability of two events u and v is ?u ?
v?1.Definition 6 (Degree of Entailment)We define the degree to which u entails v in the form of a conditional probability:Ent(u, v) =?u ?
v?1?u?1(20)If we are only interested in degrees of entailment (i.e., conditional probabilities) andnot probabilities, then we can drop the requirement that the norm should be less thanor equal to one, because conditional probabilities are automatically normalized.
Thisdefinition, together with the multiplication of the algebra, allows us to compute thedegree of entailment between any two strings according to the context theory.Example 8The vectors given in Table 1 give the following calculation for the degree to which catentails animal:?
(cat) = (0, 2, 3) (21)?
(animal) = (2, 1, 2) (22)?
(cat) ?
?
(animal) = (0, 1, 2) (23)Ent(?(cat),?
(animal)) = ??
(cat) ?
?(animal)?1/??
(cat)?1 = 3/5 (24)An important question is how this context-theoretic definition of the degree ofentailment relates to more familiar notions of entailment.
There are three main waysin which the term entailment is used: the model-theoretic sense of entailment in which a theory A entails atheory B if every model of A is also a model of B.
It was shown in Clarke(2007) that this type of entailment can be described using context theories,where sentences are represented as projections on a vector space. entailment between terms (as expressed for example in the WordNethierarchy), for example the hypernymy relation between the terms cat andanimal encodes the fact that a cat is an animal.
In Clarke (2007) we showed52Clarke A Context-Theoretic Framework for Distributional Semanticsthat such relations can be encoded in the partial order structure of a vectorlattice. Human common-sense judgments as to whether one sentence entails orimplies another sentence, as used in the Recognising Textual EntailmentChallenges (Dagan, Glickman, and Magnini 2005).Our context-theoretic notion of entailment is thus intended to generalize both the firsttwo senses of entailment given here.
In addition, we hope that context theories will beuseful in the practical application of recognizing textual entailment.
Capturing this typeof entailment is not our initial aim because we are interested in foundational issues,and doing well at this task poses major engineering challenges beyond the scope ofour work.
Nevertheless, we believe the ability to represent the preceding two types ofentailment as well as standard distributional methods of composition bodes well for thepossibility of using our framework for this task.
In Section 4 we describe several basicapproaches to textual entailment within the framework.Our definition is more general than the model-theoretic and hypernymy notions ofentailment, however, as it allows the measurement of a degree of entailment betweenany two strings: As an extreme example, one may measure the degree to which not aentails in the.
Although this may not be useful or philosophically meaningful, we viewit as a practical consequence of the fact that every string has a vector representation inour model, which coincides with the current practice in vector-based compositionalitytechniques (Clark, Coecke, and Sadrzadeh 2008; Widdows 2008).2.4 Lattice Ordered AlgebrasA lattice ordered algebra merges the lattice ordering of the vector space V with theproduct of A.
This structure encapsulates the ordering properties that are familiar frommultiplication in matrices and elementary arithmetic.
For this reason, many proposedmethods of composing vector-based representations of meaning can be viewed as latticeordered algebras.
The only reason we have not included it as a requirement of theframework is because our motivating example (described in the next section) is notguaranteed to have this property, although it does give us a partially ordered algebra.Definition 7 (Partially Ordered Algebra)A partially ordered algebra A is an algebra which is also a partially ordered vectorspace, which satisfies u ?
v ?
0 for all u, v ?
A+.
If the partial ordering is a lattice, thenA is called a lattice-ordered algebra.Example 9 (Lattice-Ordered Algebra of Matrices)The matrices of order n form a lattice-ordered algebra under normal matrix multi-plication, where the lattice operations are defined as the entry-wise minimum andmaximum.Example 10 (Operators on p Spaces)Matrices can be viewed as operators on finite-dimensional vector spaces; in fact this lat-tice property extends to operators on certain infinite-dimensional spaces, the p spaces,53Computational Linguistics Volume 38, Number 1by the Riesz-Kantorovich theorem (Abramovich and Aliprantis 2002).
The operationsare defined by:(S ?
T)(u) = sup{S(v)+ T(w) : v,w ?
U+ and v+ w = u} (25)(S ?
T)(u) = inf{S(v)+ T(w) : v,w ?
U+ and v+ w = u} (26)If A is a lattice-ordered algebra which is also an abstract Lebesgue space, then?A,A,?,A, 1?
is a context theory.
In this simplified situation, A plays the role of thevector lattice as well as the algebra; ?
maps from A to A as before, and 1 indicates theidentity map on A.
Many of the examples we discuss will be of this form, so we willuse the shorthand notation, ?A,A,??.
It is tempting to adopt this as the definition ofcontext theory; as we will see in the next section, however, this is not supported by ourprototypical example of a context theory as in this case the algebra is not necessarilylattice-ordered.3.
Context AlgebrasIn this section we describe the prototypical examples of a context theory, the contextalgebras.
The definition of a context algebra originates in the idea that the notion of?meaning as context?
can be extended beyond the word level to strings of arbitrarylength.
In fact, the notion of context algebra can be thought of as a generalization of thesyntactic monoid of a formal language: Instead of a set of strings defining the language,we have a fuzzy set of strings, or more generally, a real-valued function on a free monoid.We call such functions real-valued languages and they take the place of formallanguages in our theory.
We attach a real number to each string which is intended as anindication of its importance or likelihood of being observed; for example, those with avalue of zero are considered not to occur.Definition 8 (Real-Valued Language)Let A be a finite set of symbols.
A real-valued language (or simply a language whenthere is no ambiguity) L on A is a function from A?
to R. If the range of L is a subset ofR+ then L is called a positive language.
If the range of L is a subset of [0, 1] then L iscalled a fuzzy language.
If L is a positive language such that?x?A?
L(x) = 1 then L is aprobability distribution over A?, a distributional language.One possible interpretation for L when it is a distributional language is that L(x) is theprobability of observing the string x when selecting a document at random from aninfinite collection of documents.The following inclusion relation applies among these classes of language:distributional =?
fuzzy =?
positive =?
real-valued (27)Because A?
is a countable set, the set RA?of functions from A?
to R is isomorphicto the sequence space, and we shall treat them equivalently.
We denote by p(A?)
theset of functions with a finite p norm when considered as sequences.
There is anotherhierarchy of spaces given by the inclusion of the p spaces: p(A?)
?
q(A?)
if p ?
q.
Inparticular,1(A?)
?
2(A?)
?
?(A?)
?
RA?
(28)54Clarke A Context-Theoretic Framework for Distributional Semanticswhere the ?
norm gives the maximum value of the function and ?(A?)
is the space ofall bounded real-valued functions on A?.
Recall that a linear operator T from one vectorspaceU to anotherV is called bounded if there exists some ?
> 0 such that ?Tu?
?
?
?u?for all u ?
U, where the norm on the left hand side is the norm inV, and that on the righthand side is in U.Note that probability distributions are in 1(A?)
and fuzzy languages are in ?(A?
).If L ?
1(A?
)+ (the space of positive functions on A?
such that the sum of all valuesof the function is finite) then we can define a probability distribution pL over A?
bypL(x) = L(x)/?L?1.
Similarly, if L ?
?(A?
)+ (the space of bounded positive functionson A?)
then we can define a fuzzy language fL by fL(x) = L(x)/?L?
?.Example 11Given a finite set of strings C ?
A?, which we may imagine to be a corpus of documents,define L(x) = 1/|C| if x ?
C, or 0 otherwise.
Then L is a probability distribution over A?.In general, we think of a real-valued language as an abstraction of a corpus; in par-ticular, we think of a corpus as a finite sample of a distributional language representingall possible documents that could ever be written.Example 12Let L be a language such that L(x) = 0 for all but a finite subset of A?.
Then L ?
p(A?
)for all p.Example 13Let L be the language defined by L(x) = |x| where x is the length of (i.e., number ofsymbols in) string x.
Then L is a positive language which is not bounded: For any stringy there exists a z such that L(z) > L(y), for example z = ay for a ?
A.Example 14Let L be the language defined by L(x) = 1/2 for all x.
Then L is a fuzzy language butL /?
1(A?
)We will assume now that L is fixed, and consider the properties of contexts of stringswith respect to this language.
As in a syntactic monoid, we consider the context to beeverything surrounding the string, although in this case instead of a set of pairs ofstrings we have a function from pairs of strings to the real numbers.
We emphasizethe vector nature of these real-valued functions by calling them ?context vectors.?
Ourthesis is centered around these vectors, and it is their properties that form the inspirationfor the context-theoretic framework.Definition 9 (Context Vectors)Let L be a language on A.
For x ?
A?, we define the context of x as a vector x?
?
RA??A?
:x?
(y, z) = L(yxz) (29)In other words, x?
is a function from pairs of strings to the real numbers, and the valueof x?
(y, z) is the value of x in the context (y, z), which is L(yxz).The question we are addressing is: Does there exist some algebra A containingthe context vectors of strings in A?
such that x?
?
y?
= ?xy where x, y ?
A?
and ?
indicatesmultiplication in the algebra?
As a first try, consider the vector space L?(A?
?
A?)
inwhich the context vectors live.
Is it possible to define multiplication on the whole vectorspace such that the condition just specified holds?55Computational Linguistics Volume 38, Number 1Example 15Consider the language C on the alphabet A = {a, b, c, d, e, f} defined by C(abcd) =C(aecd) = C(abfd) = 13 and C(x) = 0 for all other x ?
A?.
Now if we take the shorthandnotation of writing the basis vector in L?(A?
?
A?)
corresponding to a pair of stringsas the pair of strings itself thenb?
= 13 (a, cd)+13 (a, fd) (30)c?
= 13 (ab, d)+13 (ae, d) (31)?bc = 13 (a, d) (32)It would thus seem sensible to define multiplication of contexts so that 13 (a, cd) ?13 (ab, d) =13 (a, d).
However we then finde?
?
f?
= 13 (a, cd) ?13 (ab, d) =?ef = 0 (33)showing that this definition of multiplication doesn?t provide us with what we arelooking for.
In fact, if there did exist a way to define multiplication on contexts ina satisfactory manner it would necessarily be far from intuitive, as, in this example,we would have to define (a, cd) ?
(ab, d) = 0 meaning the product b?
?
c?
would have tohave a non-zero component derived from the products of context vectors (a, fd) and(ae, d) which don?t relate at all to the contexts of bc.
This leads us to instead definemultiplication on a subspace of L?(A?
?
A?
).Definition 10 (Generated Subspace A)The subspace A of L?(A?
?
A?)
is the set defined byA = {a : a =?x?A??xx?
for some ?x ?
R} (34)In other words, it is the space of all vectors formed from linear combinations of contextvectors.Because of the way we define the subspace, there will always exist some basis B ={u?
: u ?
B} where B ?
A?, and we can define multiplication on this basis by u?
?
v?
=?uvwhere u, v ?
B.
Defining multiplication on this basis defines it for the whole vectorsubspace, because we define multiplication to be linear, making A an algebra.There are potentially many different bases we could choose, however, each corre-sponding to a different subset of A?, and each giving rise to a different definition ofmultiplication.
Remarkably, this isn?t a problem.Proposition 1 (Context Algebra)Multiplication on A is the same irrespective of the choice of basis B.ProofWe say B ?
A?
defines a basis B for A when B is a basis such that B = {x?
: x ?
B}.Assume there are two sets B1,B2 ?
A?
that define corresponding bases B1 and B2 forA.
We will show that multiplication in basis B1 is the same as in the basis B2.56Clarke A Context-Theoretic Framework for Distributional SemanticsWe represent two basis elements u?1 and u?2 of B1 in terms of basis elements of B2:u?1 =?i?iv?i and u?2 =?j?jv?j (35)for some ui ?
B1, vj ?
B2 and ?i,?j ?
R. First consider multiplication in the basis B1.Note that u?1 =?i ?iv?i means that L(xu1y) =?i ?iL(xviy) for all x, y ?
A?.
This includesthe special case where y = u2y?
soL(xu1u2y?)
=?i?iL(xviu2y?)
(36)for all x, y?
?
A?.
Similarly, we have L(xu2y) =?j ?jL(xvjy) for all x, y ?
A?
which in-cludes the special case x = x?vi, so L(x?viu2y) =?j ?jL(x?vivjy) for all x?, y ?
A?.
Insert-ing this into Equation (36) yieldsL(xu1u2y) =?i,j?i?jL(xvivjy) (37)for all x, y ?
A?
which we can rewrite asu?1 ?
u?2 = ?u1u2 =?i,j?i?j(v?i ?
v?j) =?i,j?i?j?vivj (38)Conversely, the product of u1 and u2 using the basis B2 isu?1 ?
u?2 =?i?iv?i ?
?j?jv?j =?i,j?i?j(v?i ?
v?j) (39)thus showing that multiplication is defined independently of what we choose as thebasis.
Example 16Returning to the previous example, we can see that in this case multiplication is in factdefined on L?(A?
?
A?)
because we can describe each basis vector in terms of contextvectors:(a, fd) ?
(ae, d) = 3(b??
e?)
?
3(c??
f? )
= ?3(a, d) (40)(a, cd) ?
(ae, d) = 3e?
?
3(c??
f? )
= 3(a, d) (41)(a, fd) ?
(ab, d) = 3(b??
e?)
?
3f?
= 3(a, d) (42)(a, cd) ?
(ab, d) = 3e?
?
3f?
= 0, (43)thus confirming what we predicted about the product of b?
and c?
: The value is onlycorrect because of the negative correction from (a, fd) ?
(ae, d).
This example also servesto demonstrate an important property of context algebras: They do not satisfy thepositivity condition; it is possible for positive vectors (those with all componentsgreater than or equal to zero) to have a non-positive product.
This means they are notnecessarily partially ordered algebras under the normal partial order.
Compare thisto the case of matrix multiplication, for example, where the product of two positivematrices is always positive.57Computational Linguistics Volume 38, Number 1The notion of a context theory is founded on the prototypical example given bycontext vectors.
So far we have shown that multiplication can be defined on the vectorspace A generated by context vectors of strings; we have not discussed the latticeproperties of the vector space, however.
In fact, A does not come with a natural latticeordering that makes sense for our purposes, although the original space RA??A?
does?it is isomorphic to the sequence space.
Thus ?A,A,?,RA??A?
,??
will form our contexttheory, where ?
(a) = a?
for a ?
A and ?
is the canonical map that simply maps elementsof A to themselves, but are considered as elements of RA??A?
.
There is an importantcaveat here, however: We required that the vector lattice be an abstract Lebesgue space,which means we need to be able to define a norm on it.
The 1 norm on RA??A?
is anobvious candidate, although it is not guaranteed to be finite.
This is where the nature ofthe underlying language L becomes important.We might hope that the most restrictive class of the languages we discussed, thedistributional languages, would guarantee that the norm is finite.
Unfortunately, this isnot the case, as the following example demonstrates.Example 17Let L be the language defined byL(a2n) = 1/2n+1 (44)for integer n ?
0, and zero otherwise, where by an we mean n repetitions of a, sofor example, L(a) = 12 , L(aa) =14 , L(aaa) = 0, and L(aaaa) =18 .
Then L is a probabilitydistribution over A?, because L is positive and ?L?1 = 1.
However, ?a?
?1 is infinite,because each string x for which L(x) > 0 contributes 1/2 to the value of the norm, andthere are an infinite number of such strings.The problem in the previous example is that the average string length is infinite.
Ifwe restrict ourselves to distributional languages in which the average string length isfinite, then the problem goes away.Proposition 2Let L be a probability distribution over A?
such thatL?
=?x?A?L(x)|x| (45)is finite, where |x| is the number of symbols in string x; we will call such languages finiteaverage length.
Then ?y?
?1 is finite for each y ?
A?.ProofDenote the number of occurrences of string y as a substring of string x by |x|y.
Clearly|x|y ?
|x| for all x, y ?
A?.
Moreover,?y?
?1 =?x?A?L(x)|x|y ?
?x?A?L(x)|x| (46)and so ?y?
?1 ?
L?
is finite for all y ?
A?.
If L is finite average length, then A ?
1(A?
?
A?
), and so ?A,A,?, 1(A?
?
A?),?
?is a context theory, where ?
is the canonical map from A to 1(A?
?
A?).
Thus context58Clarke A Context-Theoretic Framework for Distributional Semanticsalgebras of finite average length languages provide our prototypical examples ofcontext theories.3.1 DiscussionThe benefit of the context-theoretic framework is in providing a space of exploration formodels of meaning in language.
Our effort has been in finding principles by which todefine the boundaries of this space.
Each of the key boundaries, namely, bilinearity andassociativity of multiplication and entailment through vector lattice structure, can alsobe viewed as limitations of the model.Bilinearity is a strong requirement to place, and has wide-ranging implications forthe way meaning is represented in the model.
It can be interpreted loosely as follows:Components of meaning persist or diminish but do not spontaneously appear.
This isparticularly counterintuitive in the case of idiom and metaphor in language.
It meansthat, for example, both red and herring must contain some components relating to themeaning of red herring which only come into play when these two words are combinedin this particular order.
Any other combination would give a zero product for thesecomponents.
It is easy to see how this requirement arises from a context-theoreticperspective, nevertheless from a linguistic perspective it is arguably undesirable.One potential limitation of the model is that it does not explicitly model syntax, butrather syntactic restrictions are encoded into the vector space and product itself.
Forexample, we may assume the word square has some component of meaning in commonwith the word shape.
Then we would expect this component to be preserved in thesentencesHe drew a square andHe drew a shape.
However, in the case of the two sentencesThe box is square and *The box is shape we would expect the second to be represented bythe zero vector because it is not grammatical; square can be a noun and an adjective,whereas shape cannot.
Distributivity of meaning means that the component of meaningthat square has in common with shape must be disjoint with the adjectival component ofthe meaning of square.Associativity is also a very strong requirement to place; indeed Lambek (1961)introduced non-associativity into his calculus precisely to deal with examples that werenot satisfactorily dealt with by his associative model (Lambek 1958).Our framework provides answers to someone considering the use of algebra fornatural language semantics.
What field should be used?
The real numbers.
Need thealgebra be finite-dimensional?
No.
Should the algebra by unital?
Yes.
Some of theseanswers impose restrictions on what is possible within the framework.
The full impli-cation of these restrictions for linguistics is beyond the scope of this article, and indeedis not yet known.Although we hope that these features or boundaries are useful in their current form,it may be that with time, or for certain applications, there is a reason to expand orcontract certain of them, perhaps because of theoretical discoveries relating to the modelof meaning as context, or for practical or linguistic reasons, if, for example, the model isfound to be too restrictive to model certain linguistic phenomena.4.
Applications to Textual EntailmentIn this section we analyze approaches to the problem of recognizing textual entailment,showing how they can be related to the context-theoretic framework, and discussingpotential new approaches that are suggested by looking at them within the framework.We first discuss some simple approaches to textual entailment based on subsequence59Computational Linguistics Volume 38, Number 1matching and measuring lexical overlap.
We then look at the approach of Glickmanand Dagan (2005), showing that it can be considered as a context theory in which wordsare represented as projections on the vector space of documents.
This leads us to animplementation of our own in which we used latent Dirichlet alocation as an alternativeapproach to overcoming the problem of data sparseness.A fair amount of effort is required to describe these approaches within our frame-work.
Although there is no immediate practical benefit to be gained from this, ourmain purpose in doing this is to demonstrate the generality of the framework.
We alsohope that insight into these approaches may be gleaned by viewing them from a newperspective.4.1 Subsequence Matching and Lexical OverlapA sequence x ?
A?
is a subsequence of y ?
A?
if each element of x occurs in y in thesame order, but with the possibility of other elements occurring in between, so forexample abba is a subsequence of acabcba in {a, b, c}?.
Subsequence matching comparesthe subsequences of two sequences: The more subsequences they have in commonthe more similar they are assumed to be.
This idea has been used successfully in textclassification (Lodhi et al 2002) and also formed the basis of the author?s entry to thesecond Recognising Textual Entailment Challenge (Clarke 2006).If S is a semigroup, 1(S) is a lattice-ordered algebra under the multiplication ofconvolution:( f ?
g)(x) =?yz=xf (y)g(z) (47)where x, y, z ?
S, f, g ?
1(S).Example 18 (Subsequence Matching)Consider the algebra 1(A?)
for some alphabet A.
This has a basis consisting of elementsex for x ?
A?, where ex the function that is 1 on x and 0 elsewhere.
In particular e isa unity for the algebra.
Define ?
(a) = 12 (ea + e); then ?A, 1(A?),??
is a context theory.Under this context theory, a sequence x completely entails y if and only if it is a sub-sequence of y.
In our experiments, we have shown that this type of context theory canperform significantly better than straightforward lexical overlap (Clarke 2006).
Manyvariations on this idea are possible: for example, using more complex mappings fromA?
to 1(A?
).Example 19 (Lexical Overlap)The simplest approach to textual entailment is to measure the degree of lexical over-lap: the proportion of words in the hypothesis sentence that are contained in the textsentence (Dagan, Glickman, and Magnini 2005).
This approach can be described asa context theory in terms of a free commutative semigroup on a set A, defined byA?/ ?
where x ?
y in A?
if the symbols making up x can be reordered to make y. Thendefine ??
by ??
(a) = 12 (e[a] + e[] ) where [a] is the equivalence class of a in A?/ ?.
Then?A, 1(A?/ ?),???
is a context theory in which entailment is defined by lexical overlap.More complex definitions of x?
can be used, for example, to weight different words bytheir probabilities.60Clarke A Context-Theoretic Framework for Distributional Semantics4.2 Document ProjectionsGlickman and Dagan (2005) give a probabilistic definition of entailment in terms of?possible worlds?
which they use to justify their lexical entailment model based on oc-currences of words in Web documents.
They estimate the lexical entailment probabilityLEP(u, v) to beLEP(u, v)nu,vnv(48)where nv and nu,v denote the number of documents in which the word v occurs and inwhich the words u and v both occur, respectively.
From the context-theoretic perspec-tive, we view the set of documents in which the word occurs as its context vector.
Todescribe this situation in terms of a context theory, consider the vector space ?
(D)where D is the set of documents.
With each word u in some set A we associate anoperator Pu on this vector space byPued ={ed if u occurs in document d0 otherwise.
(49)where ed is the basis element associated with document d ?
D. Pu is a projection, that is,PuPu = Pu; it projects onto the space of documents that u occurs in.
These projections areclearly commutative (they are in fact band projections): PuPv = PvPu = Pu ?
Pv projectsonto the space of documents in which both u and v occur.In their paper, Glickman and Dagan (2005) assume that probabilities can be attachedto individual words, as we do, although they interpret these as the probability that aword is ?true?
in a possible world.
In their interpretation, a document corresponds to apossible world, and a word is true in that world if it occurs in the document.They do not, however, determine these probabilities directly; instead they makeassumptions about how the entailment probability of a sentence depends on lexicalentailment probability.
Although they do not state this, the reason for this is presumablydata sparseness: They assume that a sentence is true if all its lexical components are true;this will only happen if all the words occur in the same document.
For any sizeablesentence this is extremely unlikely, hence their alternative approach.It is nevertheless useful to consider this idea from a context-theoretic perspective.We define a context theory ?A,B(?
(D)),?, ?
(D), p?
?, where: We denote by B(U) the set of bounded operators on the vector space U;in this case we are considering the bounded operators on the vector spaceindexed by the set of documents D. Because D is finite, all operators onthis space are in fact bounded; this property will be needed when wegeneralize D to an infinite set, however. ?
: A ?
B(?
(D)) is defined by ?
(u) = Pu; it maps words to documentprojections. p?
: B(?
(D)) ?
?
(D) is a map defined by p?
(T) = Tp, where p ?
?
(D) is defined by p(d) = 1/|D| for all d ?
D. This is defined suchthat ?Pup?1 is the probability of the term u.61Computational Linguistics Volume 38, Number 1The degree to which x entails y is then given by ?Pxp ?
Pyp?1/?Px?1 =?PxPyp?1/?Px?1.
This corresponds directly to Glickman and Dagan?s (2005) entailment?confidence?
; it is simply the proportion of documents that contain all the terms of xwhich also contain all the terms of y.4.3 Latent Dirichlet ProjectionsThe formulation in the previous section suggests an alternative approach to that ofGlickman and Dagan (2005) to cope with the data sparseness problem.
We consider thefinite data available D as a sample from a distributional language D?
; the vector p thenbecomes a probability distribution over the documents in D?.
In our own experiments,we used latent Dirichlet alocation (Blei, Ng, and Jordan 2003) to build a model of thecorpus as a probabilistic language based on a subset of around 380,000 documents fromthe Gigaword corpus.
Having this model allows us to consider an infinite array ofpossible documents, and thus we can use our context-theoretic definition of entailmentbecause there is no problem of data sparseness.Latent Dirichlet alocation (LDA) follows the same vein as latent semantic analy-sis (LSA; Deerwester et al 1990) and probabilistic latent semantic analysis (PLSA;Hofmann 1999) in that it can be used to build models of corpora in which words withina document are considered to be exchangeable, so that a document is treated as a bagof words.
LSA performs a singular value decomposition on the matrix of words anddocuments which brings out hidden ?latent?
similarities in meaning between words,even though they may not occur together.In contrast, PLSA and LDA provide probabilistic models of corpora using Bayesianmethods.
LDA differs from PLSA in that, whereas the latter assumes a fixed numberof documents, LDA assumes that the data at hand are a sample from an infinite set ofdocuments, allowing new documents to be assigned probabilities in a straightforwardmanner.Figure 3 shows a graphical representation of the latent Dirichlet alocation genera-tive model, and Figure 4 shows how the model generates a document of length N. Inthis model, the probability of occurrence of a word w in a document is considered to bea multinomial variable conditioned on a k-dimensional ?topic?
variable z.
The numberof topics k is generally chosen to be much fewer than the number of possible words,so that topics provide a ?bottleneck?
through which the latent similarity in meaningbetween words becomes exposed.The topic variable is assumed to follow a multinomial distribution parameterizedby a k-dimensional variable ?, satisfyingk?i=1?i = 1 (50)and which is in turn assumed to follow a Dirichlet distribution.
The Dirichlet distribu-tion is itself parameterized by a k-dimensional vector ?.
The components of this vectorcan be viewed as determining the marginal probabilities of topics, becausep(zi) =?p(zi|?)p(?)d?
(51)=??ip(?)d?
(52)62Clarke A Context-Theoretic Framework for Distributional SemanticsFigure 3Graphical representation of the Dirichlet model.
The inner box shows the choices that arerepeated for each word in the document; the outer box shows the choice that is made for eachdocument; the parameters outside the boxes are constant for the model.This is just the expected value of ?i, which is given byp(zi) =?i?j ?j(53)The model is thus entirely specified by ?
and the conditional probabilities p(w|z)that we can assume are specified in a k?
V matrix ?
where V is the number of words inthe vocabulary.
The parameters ?
and ?
can be estimated from a corpus of documentsby a variational expectation maximization algorithm, as described by Blei, Ng, andJordan (2003).LDA was applied by Blei, Ng, and Jordan (2003) to the tasks of document model-ing, document classification, and collaborative filtering.
They compare LDA to severaltechniques including PLSA; LDA outperforms these on all of the applications.
LDA hasbeen applied to the task of word sense disambiguation (Boyd-Graber, Blei, and Zhu2007; Cai, Lee, and Teh 2007) with significant success.Consider the vector space ?(A?)
for some alphabet A, the space of all boundedfunctions on possible documents.
In this approach, we define the representation of aFigure 4Generative process assumed in the Dirichlet model.63Computational Linguistics Volume 38, Number 1string x to be a projection Px on the subspace representing the (infinite) set of documentsin which all the words in string x occur.
We define a vector q(x) for x ?
A?
where q(x) isthe probability of string x in the probabilistic language.Our context theory is then given by ?A,B(?(A?
)),?, ?(A?
), q?
?, where ?
is definedas before and q?
is defined as p?
earlier.
In this case, we are considering an infinite setof possible documents, A?, so the boundedness property becomes important.
?Pxq?1is thus the probability that a document chosen at random contains all the words thatoccur in string x.
In order to estimate this value we have to integrate over the Dirichletparameter ?
:?Pxq?1 =??(?a?xp?(a))p(?)d?
(54)where by a ?
x we mean that the word a occurs in string x, and p?
(a) is the probabilityof observing word a in a document generated by the parameter ?.
We estimate this byp?
(a)  1 ?
(1 ??zp(a|z)p(z|?
))N(55)where we have assumed a fixed document length N. This formula is an estimate of theprobability of a word occurring at least once in a document of length N, and the sumover the topic variable z is the probability that the word a occurs at any one point ina document given the parameter ?.
We approximated the integral using Monte Carlosampling to generate values of ?
according to the Dirichlet distribution.We built a latent Dirichlet alocation model using Blei, Ng, and Jordan?s (2003)implementation on documents from the British National Corpus, using 100 topics.
Weevaluated this model on the 800 entailment pairs from the first Recognizing TextualEntailment Challenge test set.1 Results were comparable to those obtained by Glickmanand Dagan (2005) (see Table 2).
In this table, Accuracy is the accuracy on the test set,consisting of 800 entailment pairs, and CWS is the confidence weighted score; seeDagan, Glickman, and Magnini (2005) for the definition.
The differences between theaccuracy values in the table are not statistically significant because of the small dataset, although all accuracies in the table are significantly better than chance at the 1%level.
The accuracy of the model is considerably lower than the state of the art, whichis around 75% (Bar-Haim et al 2006).
We experimented with various document lengthsand found very long documents (N = 106 and N = 107) to work best.It is important to note that because the LDA model is commutative, the resultingcontext algebra must also be commutative, which is clearly far from ideal in modelingnatural language.5.
The Model of Clark, Coecke, and SadrzadehOne of the most sophisticated proposals for a method of composition is that of Clark,Coecke, and Sadrzadeh (2008) and the more recent implementation of Grefenstette et al1 We have so far only used data from the first challenge, because we performed the experiment before theother challenges had taken place.64Clarke A Context-Theoretic Framework for Distributional SemanticsTable 2Results obtained with our latent Dirichlet projection model on the data from the firstRecognizing Textual Entailment Challenge for two document lengths N = 106 and N = 107using a cut-off for the degree of entailment of 0.5 at which entailment was regarded as holding.Model Accuracy CWSDirichlet (106) 0.584 0.630Dirichlet (107) 0.576 0.642Bayer (MITRE) 0.586 0.617Glickman (Bar Ilan) 0.586 0.572Jijkoun (Amsterdam) 0.552 0.559Newman (Dublin) 0.565 0.6(2011).
In this section, we will show how their model can be described as a contexttheory.The authors describe the syntactic element of their construction using pregroups(Lambek 2001), a formalism which simplifies the syntactic calculus of Lambek (1958).These can be described in terms of partially orderedmonoids, a monoidGwith a partialordering ?
satisfying x ?
y implies xz ?
yz and zx ?
zy for all x, y, z ?
G.Definition 11 (Pregroup)Let G be a partially ordered monoid.
Then G is called a pregroup if for each x ?
G thereare elements xl and xr in G such thatxlx ?
1 (56)xxr ?
1 (57)1 ?
xxl (58)1 ?
xrx (59)If x, y ?
G, we call y a reduction of x if y can be obtained from x using only Rules (56)and (57).Pregroup grammars are defined by freely generating a pregroup on a set of basicgrammatical types.
Words are then represented as elements formed from these basictypes, for example:John likes Mary?
?rsol o(60)where ?, s, and o are the basic types for first person singular, statement, and object,respectively.
It is easy to see that this sentence reduces to type s under the pregroupreductions.As Clark, Coecke, and Sadrzadeh (2008) note, their construction can be generalizedby endowing the grammatical type of a word with a vector nature, in addition to itssemantics.
We use this slightly more general construction to allow us to formulate itin the context-theoretic framework.
We define an elementary meaning space to be thetensor product space V = S?
P where S is a vector space representing meanings ofwords and P is a vector space with an orthonormal basis corresponding to the basic65Computational Linguistics Volume 38, Number 1grammatical types in a pregroup grammar and their adjoints.
We assume that meaningsof words live in the tensor algebra space T(V), defined byT(V) = R?
V ?
(V ?
V) ?
(V ?
V ?
V) ?
?
?
?For an element v in a particular tensor power of V, such that v = (s1 ?
p1) ?
(s2 ?
p2) ??
?
?
?
(sn ?
pn), where the pi are basis vectors of P, then we can recover a complex gram-matical type for v as the product ?
(v) = ?1?2 ?
?
?
?n, where ?i is the basic grammaticaltype corresponding to pi.
We will call the vectors such as this which have a singlecomplex type (i.e., they are not formed from a weighted sum of more than one type)unambiguous.We also assume that words are represented by vectors whose grammatical type isirreducible: There is no pregroup reduction possible on the type.
We define ?
(T(V)) asthe vector space generated by all such vectors.We will now define a product ?
on ?
(T(V)) that will make it an algebra.
To do this,it suffices to define the product between two elements u1,u2 which are unambiguousand whose grammatical type is basic, so that they can be viewed as elements of V.The definition of the product on the rest of the space follows from the assumption ofdistributivity.
We defineu1 ?
u2 =??
?u1 ?
u2 if ?
(u1 ?
u2) is irreducible?u1,u2?
otherwise.
(61)This product is bilinear, because for a particular pair of basis elements, only one of thesetwo conditions will apply, and both the tensor and inner products are bilinear functions.Moreover, it corresponds to composed and reduced word vectors, as defined in Clark,Coecke, and Sadrzadeh (2008).To see how this works on our example sentence, we assume we have vectors forthe meanings of the three words, which we write as vword.
We assume for the purposeof this example that the word like is represented as a product state composed of threevectors, one for each basic grammatical type.
This removes any potentially interestingsemantics, but allows us to demonstrate the product in a simple manner.
We write thisas follows:John likes Mary(vJohn ?
e?)
?
(vlikes,1 ?
e?r ) ?
(vlikes,2 ?
es) ?
(vlikes,3 ?
eol ) ?
(vMary ?
eo)(62)where e?
is the orthonormal basis vector corresponding to basic grammatical type ?.More interesting representations of like would consist of sums over similar vectors.Computing this product from left to right:(vJohn ?
e?)
?
(vlikes,1 ?
e?r )?
(vlikes,2 ?
es) ?
(vlikes,3 ?
eol ) ?
(vMary ?
eo)= ?vJohn, vlikes,1?
(vlikes,2 ?
es) ?
(vlikes,3 ?
eol ) ?
(vMary ?
eo)= ?vJohn, vlikes,1?
(vlikes,2 ?
es) ?
(vlikes,3 ?
eol ) ?
(vMary ?
eo)= ?vJohn, vlikes,1?
?vlikes,3, vMary?
(vlikes,2 ?
es)(63)66Clarke A Context-Theoretic Framework for Distributional SemanticsAs we would expect in this simplified example the product is a scalar multiple of thesecond vector for like, with the type of a statement.This construction thus allows us to represent complex grammatical types, similarto Clark, Coecke, and Sadrzadeh (2008), although it also allows us to take weightedsums of these complex types, giving us a powerful method of expressing syntactic andsemantic ambiguity.6.
Conclusions and Future WorkWe have presented a context-theoretic framework for natural language semantics.
Theframework is founded on the idea that meaning in natural language can be determinedby context, and is inspired by techniques that make use of statistical properties oflanguage by analyzing large text corpora.
Such techniques can generally be viewed asrepresenting language in terms of vectors.
These techniques are currently used in appli-cations such as textual entailment recognition, although the lack of a theory of meaningthat incorporates these techniques means that they are often used in a somewhat ad hocmanner.
The purpose behind the framework is to provide a unified theoretical founda-tion for such techniques so that they may be used in a principled manner.By formalizing the notion of ?meaning as context?
we have been able to build amathematical model that informs us about the nature of meaning under this paradigm.Specifically, it gives us a theory about how to represent words and phrases usingvectors, and tells us that the product of two meanings should be distributive andassociative.
It also gives us an interpretation of the inherent lattice structure on thesevector spaces as defining the relation of entailment.
It tells us how to measure the sizeof the vector representation of a string in such a way that the size corresponds to theprobability of the string.We have demonstrated that the framework encompasses several related approachesto compositional distributional semantics, including those based on a predefinedcomposition operation such as addition (Landauer and Dumais 1997; Foltz, Kintsch,and Landauer 1998; Mitchell and Lapata 2008) or the tensor product (Smolensky1990; Clark and Pulman 2007; Widdows 2008), matrix multiplication (Rudolphand Giesbrecht 2010), and the more sophisticated construction of Clark, Coecke, andSadrzadeh (2008).6.1 Practical InvestigationsSection 4 raises many possibilities for the design of systems to recognize textual entail-ment within the framework. Variations on substring matching: experiments with different weightingschemes for substrings, allowing partial commutativity of words orphrases, and replacing words with vectors representing their context,using tensor products of these vectors instead of concatenation. Extensions of Glickman and Dagan?s approach and our owncontext-theoretic approach using LDA, perhaps using other distributionallanguages based on n-grams or other models in which words do notcommute, or a combination of context theories based on commutativeand non-commutative models.67Computational Linguistics Volume 38, Number 1 The LDA model we used is a commutative one.
This is a considerablesimplification of what is possible within the context-theoretic framework;it would be interesting to investigate methods of incorporatingnon-commutativity into the model. Implementations based on the approach to representing uncertainty inlogical semantics similar to those described in Clarke (2007).All of these ideas could be evaluated using the data sets from the Recognising TextualEntailment Challenges.There are many approaches to textual entailment that we have not considered here;we conjecture that variations of many of them could be described within our frame-work.
We leave the task of investigating the relationship between these approaches andour framework to further work.Other areas that we are investigating, together with researchers at the Universityof Sussex, is the possibility of learning finite-dimensional algebras directly from corpusdata, along the lines of Guevara (2011) and Baroni and Zamparelli (2010).One question we have not addressed in this article is the feasibility of computingwith algebraic representations.
Although this question is highly dependent on theparticular context theory chosen, it is possible that general algorithms for computationwithin this framework could be found; this is another area that we intend to address infurther work.6.2 Theoretical InvestigationsAlthough the context-theoretic framework is an abstraction of the model of meaning ascontext, it would be good to have a complete understanding of the model and the typesof context theories that it allows.
Tying down these properties would allow us to definealgebras that could truly be called ?context theories.
?The context-theoretic framework shares a lot of properties with the study of freeprobability (Voiculescu 1997).
It would be interesting to investigate whether ideas fromfree probability would carry over to context-theoretic semantics.Although we have related our model to many techniques described in the literature,we still have to investigate its relationship with other models such as that of Song andBruza (2003) and Guevara (2011).We have not given much consideration here to the issue of multi-word expres-sions and non-compositionality.
What predictions does the context-theoretic frameworkmake about non-compositionality?
Answering this may lead us to new techniques forrecognizing and handling multi-word expressions and non-compositionality.Of course it is hard to predict the benefits that may result from what we havepresented, because we have given a way of thinking about meaning in natural languagethat in many respects is new.
This new way of thinking opens the door to the unificationof logic-based and vector-based methods in computational linguistics, and the potentialfruits of this union are many.AcknowledgmentsThe ideas presented here have benefittedenormously from the input and support ofmy DPhil supervisor, David Weir, withoutwhom this work would not exist; Rudi Lutz;and Stephen Clark, who really grokked thisand made many excellent suggestions forimprovements.
I am also grateful for theadvice and encouragement of Bill Keller,John Carroll, Peter Williams, MarkW.
Hopkins, Peter Lane, Paul Hender, andPeter Hines.
I am indebted to the anonymous68Clarke A Context-Theoretic Framework for Distributional Semanticsreviewers; their suggestions haveundoubtedly improved this article beyondmeasure; the paragraph on the three uses ofthe term entailment was derived directly fromone of their suggestions.ReferencesAbramovich, Yuri A. and Charalambos D.Aliprantis.
2002.
An Invitation to OperatorTheory.
American Mathematical Society,Providence, RI.Bar-Haim, Roy, Ido Dagan, Bill Dolan, LisaFerro, Danilo Giampiccolo, BernardoMagnini, and Idan Szpektor.
2006.The second pascal recognising textualentailment challenge.
In Proceedings of theSecond PASCAL Challenges Workshop onRecognising Textual Entailment, pages 1?9,Venice.Baroni, Marco and Roberto Zamparelli.2010.
Nouns are vectors, adjectives arematrices: Representing adjective-nounconstructions in semantic space.
InProceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMNLP 2010), pages 1183?1193, EastStroudsburg PA.Bellegarda, Jerome R. 2000.
Exploitinglatent semantic information in statisticallanguage modeling.
Proceedings of theIEEE, 88(8):1279?1296.Blackburn, Patrick and Johan Bos.2005.
Representation and Inference forNatural Language.
CSLI Publications,Stanford, CA.Blei, David M., Andrew Y. Ng, and Michael I.Jordan.
2003.
Latent Dirichlet alocation.Journal of Machine Learning Research,3:993?1022.Boyd-Graber, Jordan, David Blei, andXiaojin Zhu.
2007.
A topic model forword sense disambiguation.
In Proceedingsof the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing andComputational Natural Language Learning(EMNLP-CoNLL), pages 1024?1033,Prague.Cai, Junfu, Wee Sun Lee, and Yee WhyeTeh.
2007.
Improving word sensedisambiguation using topic features.In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural LanguageProcessing and Computational NaturalLanguage Learning (EMNLP-CoNLL),pages 1015?1023, Prague.Choi, Freddy, Peter Wiemer-Hastings,and Johanna Moore.
2001.
LatentSemantic Analysis for text segmentation.In Proceedings of the 2001 Conference onEmpirical Methods in Natural LanguageProcessing, pages 109?117, Ithaca, NY.Clark, Stephen, Bob Coecke, and MehrnooshSadrzadeh.
2008.
A compositionaldistributional model of meaning.In Proceedings of the Second QuantumInteraction Symposium (QI-2008),pages 133?140, Oxford.Clark, Stephen and Stephen Pulman.
2007.Combining symbolic and distributionalmodels of meaning.
In Proceedingsof the AAAI Spring Symposium onQuantum Interaction, pages 52?55,Stanford, CA.Clarke, Daoud.
2006.
Meaning as contextand subsequence analysis for textualentailment.
In Proceedings of the SecondPASCAL Recognising Textual EntailmentChallenge, pages 134?139, Venice.Clarke, Daoud.
2007.
Context-theoreticSemantics for Natural Language:An Algebraic Framework.
Ph.D. thesis,Department of Informatics, Universityof Sussex.Collins, Michael.
1997.
Three generative,lexicalised models for statistical parsing.In Proceedings of the 35th Annual Meetingof the Association for ComputationalLinguistics and Eighth Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 16?23,Madrid.Dagan, Ido, Oren Glickman, and BernardoMagnini.
2005.
The PASCAL recognisingtextual entailment challenge.
In Proceedingsof the PASCAL Challenges Workshop onRecognising Textual Entailment, pages 1?8,Southampton, UK.Deerwester, Scott, Susan Dumais,George Furnas, Thomas Landauer, andRichard Harshman.
1990.
Indexing bylatent semantic analysis.
Journal of theAmerican Society for Information Science,41(6):391?407.Erk, Katrin and Sebastian Pado?.
2009.Paraphrase assessment in structuredvector space: Exploring parameters anddatasets.
In Proceedings of the Workshop onGeometrical Models of Natural LanguageSemantics, pages 57?65, Athens.Firth, John R. 1968.
A synopsis of linguistictheory, 1930?1955.
In John R. Firth, editor,Selected Papers of JR Firth, 1952?59.Indiana University Press, Bloomington,pages 168?205.Foltz, Peter W., Walter Kintsch, andThomas K. Landauer.
1998.
Themeasurement of textual coherence69Computational Linguistics Volume 38, Number 1with latent semantic analysis.
DiscourseProcess, 15:285?307.Geffet, M. and I. Dagan.
2005.
Thedistributional inclusion hypothesesand lexical entailment.
In Proceedingsof the 43rd Annual Meeting on Association forComputational Linguistics, pages 107?114,Ann Arbor, MI.Glickman, O. and I. Dagan.
2005.A probabilistic setting and lexicalcooccurrence model for textualentailment.
In Proceedings of the ACLWorkshop on Empirical Modeling of SemanticEquivalence and Entailment, pages 43?48,Ann Arbor, MI.Grefenstette, Edward, MehrnooshSadrzadeh, Stephen Clark,Bob Coecke, and Stephen Pulman.2011.
Concrete sentence spaces forcompositional distributional modelsof meaning.
Proceedings of the 9thInternational Conference on ComputationalSemantics (IWCS 2011), pages 125?134,Oxford.Grefenstette, Gregory.
1994.
Explorations inAutomatic Thesaurus Discovery.
KluwerAcademic Publishers, Dordrecht,Netherlands.Guevara, Emiliano.
2011.
Computingsemantic compositionality in distributionalsemantics.
In Proceedings of the 9thInternational Conference on ComputationalSemantics (IWCS 2011), pages 135?144,Oxford.Halmos, Paul.
1974.
Finite Dimensional VectorSpaces.
Springer, Berlin.Harris, Zellig.
1968.
Mathematical Structuresof Language.
Wiley, New York.Hofmann, Thomas.
1999.
Probabilisticlatent semantic analysis.
In Proceedings ofthe 15th Conference on Uncertainty in AI,pages 289?296, Stockholm.Kamp, Hans and Uwe Reyle.
1993.
FromDiscourse to Logic: Introduction toModel-theoretic Semantics of NaturalLanguage, Formal Logic and DiscourseRepresentation Theory, volume 42 ofStudies in Linguistics and Philosophy.Kluwer, Dordrecht.Kintsch, Walter.
2001.
Predication.
CognitiveScience, 25:173?202.Lambek, Joachim.
1958.
The mathematics ofsentence structure.
American MathematicalMonthly, 65:154?169.Lambek, Joachim.
1961.
On the calculus ofsyntactic types.
In Roman Jakobson, editor,Structure of Language and Its MathematicalAspects, pages 166?178, AmericanMathematical Society, Providence, RI.Lambek, Joachim.
2001.
Type grammars aspregroups.
Grammars, 4(1):21?39.Landauer, Thomas K. and Susan T. Dumais.1997.
A solution to Plato?s problem:The latent semantic analysis theory ofacquisition, induction and representationof knowledge.
Psychological Review,104(2):211?240.Lee, Lillian.
1999.
Measures of distributionalsimilarity.
In Proceedings of the 37th AnnualMeeting of the Association for ComputationalLinguistics (ACL-1999), pages 23?32,College Park, MD.Lin, Dekang.
1998.
Automatic retrievaland clustering of similar words.
InProceedings of the 36th Annual Meetingof the Association for ComputationalLinguistics and the 17th InternationalConference on Computational Linguistics(COLING-ACL ?98), pages 768?774,Montreal.Lin, Dekang.
2003.
Dependency-basedevaluation of MINIPAR.
In Anne Abeille?,editor, Treebanks: Building and UsingParsed Corpora, pages 317?330, Kluwer,Dordrecht.Lodhi, Huma, Craig Saunders, JohnShawe-Taylor, Nello Cristianini, andChris Watkins.
2002.
Text classificationusing string kernels.
Journal of MachineLearning Research, 2:419?444.McCarthy, Diana, Rob Koeling, JulieWeeds, and John Carroll.
2004.
Findingpredominant word senses in untaggedtext.
In Proceedings of the 42nd Meeting ofthe Association for Computational Linguistics(ACL?04), pages 279?286, Barcelona.Miller, George A. and Walter G. Charles.1991.
Contextual correlates of semanticsimilarity.
Language and CognitiveProcesses, 6(1):1?28.Mitchell, Jeff and Mirella Lapata.
2008.Vector-based models of semanticcomposition.
In Proceedings of ACL-08:HLT, pages 236?244, Columbus, OH.Preller, Anne and Mehrnoosh Sadrzadeh.2011.
Bell states and negative sentencesin the distributed model of meaning.Electronic Notes in Theoretical ComputerScience, 270(2):141?153.Rudolph, Sebastian and Eugenie Giesbrecht.2010.
Compositional matrix-space modelsof language.
In Proceedings of the 48thAnnual Meeting of the Association forComputational Linguistics, pages 907?916,Uppsala.Schu?tze, Heinrich.
1998.
Automatic wordsense discrimination.
ComputationalLinguistics, 24(1):97?123.70Clarke A Context-Theoretic Framework for Distributional SemanticsSmolensky, Paul.
1990.
Tensor productvariable binding and the representationof symbolic structures in connectionistsystems.
Artificial Intelligence,46(1-2):159?216.Song, Dawei and Peter D. Bruza.
2003.Towards context-sensitive informationinference.
Journal of the American Societyfor Information Science and Technology(JASIST), 54:321?334.Voiculescu, Dan-Virgil.
1997.
Free ProbabilityTheory.
American Mathematical Society,Providence, RI.Weeds, Julie, David Weir, and DianaMcCarthy.
2004.
Characterising measuresof lexical distributional similarity.In Proceedings of CoLING 2004,pages 1015?1021, Geneva.Widdows, Dominic.
2008.
Semantic vectorproducts: Some initial investigations.In Proceedings of the Second Symposiumon Quantum Interaction, pages 1?8,Oxford.Wittgenstein, Ludwig.
1953.
PhilosophicalInvestigations.
Macmillan, New York.
G.Anscombe, translator.71
