Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 37?42,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Latent Variable Model of Synchronous Syntactic-Semantic Parsing forMultiple LanguagesAndrea GesmundoUniv GenevaDept Computer SciAndrea.Gesmundo@unige.chJames HendersonUniv GenevaDept Computer SciJames.Henderson@unige.chPaola MerloUniv GenevaDept LinguisticsPaola.Merlo@unige.chIvan Titov?Univ Illinois at U-CDept Computer Scititov@uiuc.eduAbstractMotivated by the large number of languages(seven) and the short development time (twomonths) of the 2009 CoNLL shared task, weexploited latent variables to avoid the costlyprocess of hand-crafted feature engineering,allowing the latent variables to induce featuresfrom the data.
We took a pre-existing gener-ative latent variable model of joint syntactic-semantic dependency parsing, developed forEnglish, and applied it to six new languageswith minimal adjustments.
The parser?s ro-bustness across languages indicates that thisparser has a very general feature set.
Theparser?s high performance indicates that its la-tent variables succeeded in inducing effectivefeatures.
This system was ranked third overallwith a macro averaged F1 score of 82.14%,only 0.5% worse than the best system.1 IntroductionRecent research in syntax-based statistical machinetranslation and the recent availability of syntac-tically annotated corpora for multiple languages(Nivre et al, 2007) has provided a new opportunityfor evaluating the cross-linguistic validity of statis-tical models of syntactic structure.
This opportu-nity has been significantly expanded with the 2009CoNLL shared task on syntactic and semantic pars-ing of seven languages (Hajic?
et al, 2009) belongingto several different language families.We participate in this task with a generative,history-based model proposed in the CoNLL 20080Authors in alphabetical order.shared task for English (Henderson et al, 2008) andfurther improved to tackle non-planar dependencies(Titov et al, 2009).
This model maximises the jointprobability of the syntactic and semantic dependen-cies and thereby enforces that the output structure beglobally coherent, but the use of synchronous pars-ing allows it to maintain separate structures for thesyntax and semantics.
The probabilistic model isbased on Incremental Sigmoid Belief Networks (IS-BNs), a recently proposed latent variable model forsyntactic structure prediction, which has shown verygood performance for both constituency (Titov andHenderson, 2007a) and dependency parsing (Titovand Henderson, 2007b).
The use of latent variablesenables this architecture to be extended to learninga synchronous parse of syntax and semantics with-out overly restrictive assumptions about the linkingbetween syntactic and semantic structures.In this work, we evaluate the ability of thismethod to generalise across several languages.
Wetake the model as it was developed for English, andapply it directly to all seven languages.
The onlyfine-tuning was to evaluate whether to include onefeature type which we had previously found did nothelp for English, but helped overall.
No other fea-ture engineering was done.
The use of latent vari-ables to induce features automatically from the datagives our method the adaptability necessary to per-form well across all seven languages, and demon-strates the lack of language specificity in the modelsof Henderson et al (2008) and Titov et al (2009).The main properties of this model, that differen-tiate it from other approaches, is the use of syn-chronous syntactic and semantic derivations and the37use of online planarisation of crossing semantic de-pendencies.
This system was ranked third overallwith a macro averaged F1 score of 82.14%, only0.5% worse than the best system.2 The Synchronous ModelThe use of synchronous parsing allows separatestructures for syntax and semantics, while still mod-eling their joint probability.
We use the approachto synchronous parsing proposed in Henderson et al(2008), where we start with two separate derivationsspecifying each of the two structures, then synchro-nise these derivations at each word.
The individualderivations are based on Nivre?s shift-reduce-styleparsing algorithm (Nivre et al, 2006), as discussedfurther below.
First we illustrate the high-level struc-ture of the model, discussed in more detail in Hen-derson et al (2008).Let Td be a syntactic dependency tree withderivation D1d, ..., Dmdd , and Ts be a semanticdependency graph with derivation D1s , ..., Dmss .To define derivations for the joint structureTd, Ts, we divide the two derivations into thechunks between shifting each word onto thestack, ctd = Dbtdd , ..., Detdd and cts = Dbtss , ..., Detss ,where Dbtd?1d = Dbts?1s = Shiftt?1 andDetd+1d = Dets+1s = Shiftt.
Then the actions ofthe synchronous derivations consist of quadruplesCt = (ctd, Switch, cts, Shiftt), where Switch meansswitching from syntactic to semantic mode.
Thisgives us the following joint probability model,where n is the number of words in the input.P (Td, Ts) = ?nt=1 P (Ct|C1, .
.
.
, Ct?1) (1)These synchronous derivations C1, .
.
.
, Cn only re-quire a single input queue, since the Shift actions aresynchronised, but they require two separate stacks,one for the syntactic derivation and one for the se-mantic derivation.The probability of each synchronous derivationchunk Ct is the product of four factors, related tothe syntactic level, the semantic level and the twosynchronising steps.
The probability of ctd is de-composed into one probability for each derivationaction Di, conditioned on its history using the chainrule, and likewise for cts.
These probabilities are es-timated using the method described in section 3.Syn cross Sem cross Sem tree No parseCat 0% 0% 61.4% 0%Chi 0% 28.0% 28.6% 9.5%Cze 22.4% 16.3% 6.1% 1.8%Eng 7.6% 43.9% 21.4% 3.9%Ger 28.1% 1.3% 97.4% 0.0%Jap 0.9% 38.3% 11.2% 14.4%Spa 0% 0% 57.1% 0%Table 1: For each language, percentage of training sen-tences with crossing arcs in syntax and semantics, withsemantic arcs forming a tree, and which were not parsableusing the Swap action.One of the main characteristics of our syn-chronous representation, unlike other synchronousrepresentations of syntax and semantics (Nesson etal., 2008), is that the synchronisation is done onwords, rather than on structural components.
Wetake advantage of this freedom and adopt differentmethods for handling crossing arcs for syntax andfor semantics.While both syntax and semantics are representedas dependency graphs, these graphs differ substan-tially in their properties.
Some statistics which in-dicate these differences are shown in table 1.
Forexample, English syntactic dependencies form trees,while semantic dependency structures are only trees21.4% of the time, since in general each struc-ture does not form a connected graph and somenodes may have more than one parent.
The syn-tactic dependency structures for only 7.6% of En-glish sentences contain crossing arcs, while 43.9%of the semantic dependency structures contain cross-ing arcs.
Due to variations both in language char-acteristics and annotation decisions across corpora,these differences between syntax and semantics varyacross the seven languages, but they are consis-tent enough to motivate the development of newtechniques specifically for handling semantic depen-dency structures.
In particular, we use a differentmethod for parsing crossing arcs.For parsing crossing semantic arcs (i.e.
non-planar graphs), we use the approach proposed inTitov et al (2009), which introduces an action Swapthat swaps the top two elements on the parser?sstack.
The Swap action allows the parser to reorderwords online during the parse.
This allows wordsto be processed in different orders during different38portions of the parse, so some arcs can be specifiedusing one ordering, then other arcs can be specifiedusing another ordering.
Titov et al (2009) found thatonly using the Swap action as a last resort is the beststrategy for English (compared to using it preemp-tively to address future crossing arcs) and we usethe same strategy here for all languages.Syntactic graphs do not use a Swap action.We adopt the HEAD method of Nivre and Nils-son (2005) to de-projectivise syntactic dependenciesoutside of parsing.13 Features and New DevelopmentsThe synchronous derivations described above aremodelled with a type of Bayesian Network called anIncremental Sigmoid Belief Network (ISBN) (Titovand Henderson, 2007a).
As in Henderson et al(2008), the ISBN model distinguishes two types oflatent states: syntactic states, when syntactic deci-sions are considered, and semantic states, when se-mantic decision are considered.
Latent states arevectors of binary latent variables, which are condi-tioned on variables from previous states via a patternof connecting edges determined by the previous de-cisions.
These latent-to-latent connections are usedto engineer soft biases which reflect the relevant do-mains of locality in the structure being built.
Forthese we used the set of connections proposed inTitov et al (2009), which includes latent-to-latentconnections both from syntax states to semanticsstates and vice versa.
The latent variable vectors arealso conditioned on a set of observable features ofthe derivation history.
For these features, we startwith the feature set from Titov et al (2009), whichextends the semantic features proposed in Hender-son et al (2008) to allow better handling of the non-planar structures in semantics.
Most importantly, allthe features previously included for the top of thestack were also included for the word just under thetop of the stack.
To this set we added one more typeof feature, discussed below.We made some modifications to reflect differ-ences in the task definition between the 2008 and2009 shared tasks, and experimented with onetype of features which had been previously imple-1The statistics in Table 1 suggest that, for some languages,swapping might be beneficial for syntax as well.mented.
For the former modifications, the systemwas adapted to allow the use of the PFEAT andFILLPRED fields in the data, which both resultedin improved accuracy for all the languages.
ThePFEAT data field (automatically predicted morpho-logical features) was introduced in the system intwo ways, as an atomic feature bundle that is pre-dicted when predicting the word, and split into itselementary components when conditioning on a pre-vious word, as was done in Titov and Henderson(2007b).
Because the testing data included a spec-ification of which words were annotated as predi-cates (the FILLPRED data field), we constrained theparser?s output so as to be consistent with this speci-fication.
For rare predicates, if the predicate was notin the parser?s lexicon (extracted from the trainingset), then a sense was taken from the list of sensesreported in the Lexicon and Frame Set resourcesavailable for the closed challenge.
If this informa-tion was not available, then a default sense was con-structed based on the automatically predicted lemma(PLEMMA) of the predicate.We also made use of a previously implementedtype of feature that allows the prediction of a seman-tic link between two words to be conditioned on thesyntactic dependency already predicted between thesame two words.
While this feature had previouslynot helped for English, it did result in an overall im-provement across the languages.Also, in comparison with previous experiments,the search beam used in the decoding phase was in-creased from 50 to 80, producing a small improve-ment in the overall development score.All development effort took about two person-months, mostly by someone who had no previousexperience with the system.
Most of this time wasspent on the above differences in the task definitionbetween the 2008 and 2009 shared tasks.4 Results and DiscussionWe participated in the joint task of the closed chal-lenge, as described in Hajic?
et al (2009).
Thedatasets used in this challenge are described in Taule?et al (2008) (Catalan and Spanish), Palmer and Xue(2009) (Chinese), Hajic?
et al (2006) (Czech), Sur-deanu et al (2008) (English), Burchardt et al (2006)(German), and Kawahara et al (2002) (Japanese).39Rank Average Catalan Chinese Czech English German Japanese Spanishmacro F1 3 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43syntactic acc 1 @85.77 @87.86 76.11 @80.38 88.79 87.29 92.34 @87.64semantic F1 3 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19Table 2: The three main scores for our system.
Rank is within task.Rank Ave Cze-ood Eng-ood Ger-oodmacro F1 3 75.93 @80.70 75.76 71.32syn Acc 2 78.01 @76.41 80.84 76.77sem F1 3 73.63 84.99 70.65 65.25Table 3: Results on out-of-domain for our system.
Rankis within task.The official results on the testing set are shown intables 2, 3, and 4.
The symbol ?@?
indicates thebest result across systems.
In table 5, we show ourrankings across the different datasets, amongst sys-tems submitted for the same task.The overall score used to rank systems is the un-weighted average of the syntactic labeled accuracyand the semantic labeled F1 measure, across all lan-guages (?macro F1?
in table 2).
We were rankedthird, out of 14 systems.
There was only a 0.5% dif-ference between our score and that of the best sys-tem, while there was a 1.29% difference between ourscore and the fourth ranked system.
Only consid-ering syntactic accuracy, we had the highest aver-age score of all systems, with the highest individualscore for Catalan, Czech, and Spanish.
Only con-sidering semantic F1, we were again ranked third.Our results for out-of-domain data (table 3) achieveda similar level of success, although here we wereranked second for average syntactic accuracy.
Ourprecision on semantic arcs was generally much bet-ter than our recall (shown in table 4).
However,other systems had a similar imbalance, resulting inno change in our third place ranking for semanticprecision and for semantic recall.
Only when the se-mantic precision is averaged with syntactic accuracydo we squeeze into second place (?macro Prec?
).To get a more detailed picture of the strengthsand weaknesses of our system, we computed its rankwithin each dataset, shown in table 5.
Overall, oursystem is robust across languages, with little fluc-tuation in ranking for the overall score, includingfor out-of-domain data.
The one noticeable excep-tion to this consistency is the syntactic score for En-data time (min) macro F1Czech 25% 5007 73.8450% 3699 77.5775% 4201 79.10100% 6870 80.55English 25% 1300 79.0250% 1899 81.6175% 3196 82.41100% 3191 83.27Table 6: Training times and development set accuraciesusing different percentages of the training data, for Czechand English.glish out-of-domain data.
The other ranks for En-glish out-of-domain and English in-domain scoresare also on the poor side.
These results support ourclaim that our parser has not undergone much hand-tuning, since it was originally developed for English.It is not currently clear whether this relative differ-ence reflects a English-specific weakness in our sys-tem, or that many of the other systems have beenfine-tuned for English.On the higher end of our dataset rankings, wedo relatively well on Catalan, Czech, and Span-ish.
Catalan and Spanish are unique amongst thesedatasets in that they have no crossing arcs in theirsemantic structure.
Czech seems to have semanticstructures which are relatively well handled by ourderivations with Swap.
As indicated above in ta-ble 1, only 2% of sentences are unparsable, despite16% requiring the Swap action.
However, this argu-ment does not explain why our parser did relativelypoorly on German semantic dependencies.
Regard-less, these observations would suggest that our sys-tem is still having trouble with crossing dependen-cies, despite the introduction of the Swap operation,and that our learning method could achieve betterperformance with an improved treatment of cross-ing semantic dependencies.Table 6 shows how accuracies and training timesvary with the size of the training dataset, for Czechand English.
Training times vary in part because40Rank Ave Cat Chi Cze Eng Ger Jap Spa Cze-ood Eng-ood Ger-oodsemantic Prec 3 81.60 79.08 80.93 87.45 84.92 75.60 83.75 79.44 85.90 72.89 75.19semantic Rec 3 75.56 75.87 71.73 @84.64 81.63 68.33 71.65 75.05 @84.09 68.55 57.63macro Prec 2 83.68 83.47 78.52 83.91 86.86 81.44 88.05 83.54 81.16 76.86 @75.98macro Rec 3 80.66 @81.86 73.92 @82.51 85.21 77.81 81.99 81.35 @80.25 74.70 67.20Table 4: Semantic precision and recall and macro precision and recall for our system.
Rank is within task.Rank by Ave Cat Chi Cze Eng Ger Jap Spa Ave-ood Cze-ood Eng-ood Ger-oodmacro F1 3 2 3 2 4 4 3 2 3 1 4 3syntactic Acc 1 1 4 1 3 2 2 1 2 1 7 2semantic F1 3 2 4 2 4 5 4 2 3 2 4 3Table 5: Our system?s rank within task according to the three main measures, for each dataset.
?1.4?1.2?1?0.8?0.6?0.4?0.200  10  20  30  40  50MacroF1DifferenceWords per SecondJapSpaCatGerEngCzeChiFigure 1: Difference in development set macro F1 as thesearch beam is decreased from the submitted beam (80)to 40, 20, 10, and 5, plotted against parser speed.random variations can result in different numbers oftraining cycles before convergence.
Accuracies ap-pear to be roughly log-linear with data size.Figure 1 shows how the accuracy of the parser de-grades as we speed it up by decreasing the searchbeam used in decoding, for each language.
For somelanguages, a slightly smaller search beam is actuallymore accurate,2 but for smaller beams the trade-offof accuracy versus words-per-second is roughly lin-ear.
Parsing time per word is also linear in beamwidth, with a zero intercept.5 ConclusionIn the joint task of the closed challenge of theCoNLL 2009 shared task (Hajic?
et al, 2009), we in-vestigated how well a model of syntactic-semanticdependency parsing developed for English would2This fact suggests that we could have gotten improved re-sults by tailoring the search beam to individual languages.generalise to the other six languages.
This modelprovides a single generative probability of the jointsyntactic and semantic dependency structures, butallows separate representations for these two struc-tures by parsing the two structures synchronously.Finding the statistical correlations both between andwithin these structures is facilitated through the useof latent variables, which induce features automat-ically from the data, thereby greatly reducing theneed for hand-coded feature engineering.This latent variable model proved very robustacross languages, achieving a ranking of betweensecond and fourth on each language, including forout-of-domain data.
The extent to which the parserdoes not rely on hand-crafting is underlined by thefact that its worst ranking is for English, the lan-guage for which it was developed (particularly forout-of-domain data).
The parser was ranked thirdoverall out of 14 systems, with a macro averaged F1score of 82.14%, only 0.5% worse than the best sys-tem.Both joint learning and conditioning decisionsabout semantic dependencies on latent representa-tions of syntactic parsing states were crucial to thesuccess of our model, as was previously demon-strated in Henderson et al (2008).
There, remov-ing this conditioning led to a 3.5% drop in the SRLscore.
This result seems to contradict the gen-eral trend in the CoNLL-2008 shared task, wherejoint learning had only limited success.
The lat-ter fact may be explained by recent theoretical re-sults demonstrating that pipelines can be preferableto joint learning (Roth et al, 2009) when no sharedhidden representation is learnt.
Our system (Hender-son et al, 2008) was the only one which attempted to41learn a common hidden representation for this mul-titask learning problem and also was the only onewhich achieved significant gain from joint parameterestimation.
We believe that learning shared hiddenrepresentations for related NLP problems is a verypromising direction for further research.AcknowledgementsWe thank Gabriele Musillo and Dan Roth for help andadvice.
This work was partly funded by Swiss NSFgrants 100015-122643 and PBGE22-119276, EuropeanCommunity FP7 grant 216594 (CLASSiC, www.classic-project.org), US NSF grant SoD-HCER-0613885 andDARPA (Bootstrap Learning Program).ReferencesAljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pado?, and Manfred Pinkal.
2006.The SALSA corpus: a German corpus resource forlexical semantics.
In Proceedings of the 5th Interna-tional Conference on Language Resources and Evalu-ation (LREC-2006), Genoa, Italy.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, PetrSgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, MarieMikulova?, and Zdene?k Z?abokrtsky?.
2006.
Prague De-pendency Treebank 2.0.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic depen-dencies in multiple languages.
In Proceedings ofthe 13th Conference on Computational Natural Lan-guage Learning (CoNLL-2009), June 4-5, Boulder,Colorado, USA.James Henderson, Paola Merlo, Gabriele Musillo, andIvan Titov.
2008.
A latent variable model of syn-chronous parsing for syntactic and semantic dependen-cies.
In Proceedings of CONLL 2008, pages 178?182,Manchester, UK.Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.2002.
Construction of a Japanese relevance-taggedcorpus.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation(LREC-2002), pages 2008?2013, Las Palmas, CanaryIslands.Rebecca Nesson, Giorgio Satta, and Stuart M. Shieber.2008.
Optimal k-arization of synchronous tree-adjoining grammar.
In Proceedings of ACL-08: HLT,pages 604?612, Columbus, Ohio, June.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projectivedependency parsing.
In Proc.
43rd Meeting of Asso-ciation for Computational Linguistics, pages 99?106,Ann Arbor, MI.Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,and Svetoslav Marinov.
2006.
Pseudo-projective de-pendency parsing with support vector machines.
InProc.
of the Tenth Conference on Computational Nat-ural Language Learning, pages 221?225, New York,USA.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on depen-dency parsing.
In Proceedings of the CoNLL SharedTask Session of EMNLP-CoNLL 2007, pages 915?932,Prague, Czech Republic, June.Martha Palmer and Nianwen Xue.
2009.
Adding seman-tic roles to the Chinese Treebank.
Natural LanguageEngineering, 15(1):143?172.Dan Roth, Kevin Small, and Ivan Titov.
2009.
Sequentiallearning of classifiers for structured prediction prob-lems.
In AISTATS, Clearwater, Florida, USA.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In Proceedings of the 12th Con-ference on Computational Natural Language Learning(CoNLL-2008).Mariona Taule?, Maria Anto`nia Mart?
?, and Marta Re-casens.
2008.
AnCora: Multilevel Annotated Corporafor Catalan and Spanish.
In Proceedings of the 6thInternational Conference on Language Resources andEvaluation (LREC-2008), Marrakesh, Morroco.Ivan Titov and James Henderson.
2007a.
Constituentparsing with Incremental Sigmoid Belief Networks.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 632?639,Prague, Czech Republic.Ivan Titov and James Henderson.
2007b.
Fast and ro-bust multilingual dependency parsing with a genera-tive latent variable model.
In Proc.
Joint Conf.
on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL 2007), Prague, Czech Republic.
(CoNLLShared Task).Ivan Titov, James Henderson, Paola Merlo, and GabrieleMusillo.
2009.
Online graph planarisation for syn-chronous parsing of semantic and syntactic dependen-cies.
In Proc.
Twenty-First International Joint Confer-ence on Artificial Intelligence (IJCAI-09), Pasadena,California.42
