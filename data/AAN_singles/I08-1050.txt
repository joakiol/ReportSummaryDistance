Identifying Sections in Scientific Abstracts using Conditional Random FieldsKenji Hirohata?hirohata@nii.ac.jpNaoaki Okazaki?okazaki@is.s.u-tokyo.ac.jpSophia Ananiadou?sophia.ananiadou@manchester.ac.uk?Graduate School of InformationScience and Technology,University of Tokyo7-3-1 Hongo, Bunkyo-ku,Tokyo 113-8656, JapanMitsuru Ishizuka?ishizuka@i.u-tokyo.ac.jp?School of Computer Science,University of ManchesterNational Centre for Text Mining (NaCTeM)Manchester Interdisciplinary Biocentre,131 Princess Street, Manchester M1 7DN, UKAbstractOBJECTIVE: The prior knowledge aboutthe rhetorical structure of scientific abstractsis useful for various text-mining tasks suchas information extraction, information re-trieval, and automatic summarization.
Thispaper presents a novel approach to cate-gorize sentences in scientific abstracts intofour sections, objective, methods, results,and conclusions.
METHOD: Formalizingthe categorization task as a sequential label-ing problem, we employ Conditional Ran-dom Fields (CRFs) to annotate section la-bels into abstract sentences.
The train-ing corpus is acquired automatically fromMedline abstracts.
RESULTS: The pro-posed method outperformed the previousapproaches, achieving 95.5% per-sentenceaccuracy and 68.8% per-abstract accuracy.CONCLUSION: The experimental resultsshowed that CRFs could model the rhetor-ical structure of abstracts more suitably.1 IntroductionScientific abstracts are prone to share a similarrhetorical structure.
For example, an abstract usu-ally begins with the description of background in-formation, and is followed by the target problem,solution to the problem, evaluation of the solution,and conclusion of the paper.
Previous studies ob-served the typical move of rhetorical roles in sci-entific abstracts: problem, solution, evaluation, andconclusion (Graetz, 1985; Salanger-Meyer, 1990;Swales, 1990; Ora?san, 2001).
The American Na-tional Standard Institute (ANSI) recommends au-thors and editors of abstracts to state the purpose,methods, results, and conclusions presented in thedocuments (ANSI, 1979).The prior knowledge about the rhetorical structureof abstracts is useful to improve the performance ofvarious text-mining tasks.
Marcu (1999) proposedan extraction method for summarization that cap-tured the flow of text, based on Rhetorical Struc-ture Theory (RST).
Some extraction methods makeuse of cue phrases (e.g., ?in conclusion?, ?our in-vestigation has shown that ...?
), which suggest thatthe rhetorical role of sentences is to identify im-portant sentences (Edmundson, 1969; Paice, 1981).We can survey the problems, purposes, motivations,and previous approaches of a research field by read-ing texts in background sections of scientific papers.Tbahriti (2006) improved the performance of theirinformation retrieval engine, giving more weight tosentences referring to purpose and conclusion.In this paper, we present a supervised machine-learning approach that categorizes sentences in sci-entific abstracts into four sections, objective, meth-ods, results, and conclusions.
Figure 1 illustratesthe task of this study.
Given an unstructured ab-stract without section labels indicated by boldfacetype, the proposed method annotates section labelsof each sentence.
Assuming that this task is wellformalized as a sequential labeling problem, we useConditional Random Fields (CRFs) (Lafferty et al,2001) to identify rhetorical roles in scientific ab-stracts.The proposed method outperforms previousapproaches to this problem, achieving 95.5% per-381OBJECTIVE: This study assessed the role of adrenergic signal transmission in the control of renal erythropoietin (EPO) pro-duction in humans.
METHODS: Forty-six healthy male volunteers underwent a hemorrhage of 750 ml.
After phlebotomy, theyreceived (intravenously for 6 hours in a parallel, randomized, placebo-controlled and single-blind design) either placebo (0.9%sodium chloride), or the beta 2-adrenergic receptor agonist fenoterol (1.5 microgram/min), or the beta 1-adrenergic receptor ago-nist dobutamine (5 micrograms/kg/min), or the nonselective beta-adrenergic receptor antagonist propranolol (loading dose of 0.14mg/kg over 20 minutes, followed by 0.63 micrograms/kg/min).
RESULTS: The AUCEPO(0-48 hr)fenoterol was 37% higher (p ?0.03) than AUCEPO(0-48 hr)placebo, whereas AUCEPO(0-48 hr)dobutamine and AUCEPO(0-48 hr)propranolol were comparablewith placebo.
Creatinine clearance was significantly increased during dobutamine treatment.
Urinary cyclic adenosine monophos-phate excretion was increased only by fenoterol treatment, whereas serum potassium levels were decreased.
Plasma renin activitywas significantly increased during dobutamine and fenoterol infusion.
CONCLUSIONS: This study shows in a model of con-trolled, physiologic stimulation of renal erythropoietin production that the beta 2-adrenergic receptor agonist fenoterol but not thebeta 1-adrenergic receptor agonist dobutamine is able to increase erythropoietin levels in humans.
The result can be interpreted asa hint that signals for the control of erythropoietin production may be mediated by beta 2-adrenergic receptors rather than by beta1-adrenergic receptors.
It appears to be unlikely that an increase of renin concentrations or glomerular filtration rate is causallylinked to the control of erythropoietin production in this experimental setting.Figure 1: An abstract with section labels indicated by boldface type (Gleiter et al, 1997).sentence accuracy and 68.8% per-abstract accuracy.This paper is organized as follows.
Section 2describes previous approaches to this task.
For-malizing the task as a sequential-labeling problem,Section 3 designs a sentence classifier using CRFs.Training corpora for the classifier are acquired au-tomatically from the Medline abstracts.
Section 4reports considerable improvements in the proposedmethod over the baseline method using Support Vec-tor Machine (SVM) (Cortes and Vapnik, 1995).
Weconclude this paper in Section 5.2 Related WorkThe previous studies regarded the task of identify-ing section names as a text-classification problemthat determines a label (section name) for each sen-tence.
Various classifiers for text categorization,Na?
?ve Bayesian Model (NBM) (Teufel and Moens,2002; Ruch et al, 2007), Hidden Markov Model(HMM) (Wu et al, 2006; Lin et al, 2006), and Sup-port Vector Machines (SVM) (McKnight and Arini-vasan, 2003; Shimbo et al, 2003; Ito et al, 2004;Yamamoto and Takagi, 2005) were applied.Table 1 summarizes these approaches and perfor-mances.
All studies target scientific abstracts exceptfor Teufel and Moens (2002) who target scientificfull papers.
Field classes show the set of sectionnames that each study assumes: background (B),objective/aim/purpose (O), method (M), result (R),conclusion (C), and introduction (I) that combinesthe background and objective.
Although we shouldnot compare directly the performances of these stud-ies, which use a different set of classification labelsand evaluation corpora, SVM classifiers appear toyield better results for this task.
The rest of this sec-tion elaborates on the previous studies with SVMs.Shimbo et al (2003) presented an advanced textretrieval system for Medline that can focus on aspecific section in abstracts specified by a user.The system classifies sentences in each Medline ab-stract into four sections, objective, method, results,and conclusion.
Each sentence is represented bywords, word bigrams, and contextual information ofthe sentence (e.g., class of the previous sentence,relative location of the current sentence).
Theyreported 91.9% accuracy (per-sentence basis) and51.2% accuracy (per-abstract basis1) for the clas-sification with the best feature set for quadraticSVM.
Ito et al (2004) extended the work with asemi-supervised learning technique using transduc-tive SVM (TSVM).Yamamoto and Takagi (2005) developed a sys-tem to classify abstract sentences into five sections,background, purpose, method, result, and conclu-sion.
They trained a linear-SVM classifier with fea-tures such as unigram, subject-verb, verb tense, rel-ative sentence location, and sentence score (averageTF*IDF score of constituent words).
Their methodachieved 68.9%, 63.0%, 83.6%, 87.2%, 89.8% F-scores for classifying background, purpose, method,result, and conclusion sentences respectively.
Theyalso reported the classification performance of intro-duction sentences, which combines background andpurpose sentences, with 91.3% F-score.1An abstract is considered correct if all constituent sentencesare correctly labeled.382Methods Model Classes Performance (reported in papers)Teufel and Moens (2002) NBM (7 classes) 44% precision and 65% recall for aim sentencesRuch et al (2007) NBM O M R C 85% F-score for conclusion sentencesWu et al (2006) HMM B O M R C 80.54% precisionLin et al (2006) HMM I M R C 88.5%, 84.3%, 89.8%, 89.7% F-scoresMcKnight and Srinivasan (2003) SVM I M R C 89.2%, 82.0%, 82.1%, 89.5% F-scoresShimbo et al (2003) SVM B O M R C 91.9% accuracyIto et al (2004) TSVM B O M R C 66.0%, 51.0%, 49.3%, 72.9%, 67.7% F-scoresYamamoto and Takagi (2005) SVM I (B O) M R C 91.3% (68.9%, 63.0%), 83.6%, 87.2%, 89.8% F-scoresTable 1: Approaches and performances of previous studies on section identification3 Proposed method3.1 Section identification as a sequence labelingproblemThe previous work saw the task of labeling as a textcategorization that determines the class label yi foreach sentence xi.
Even though some work includesfeatures of the surrounding sentences for xi, e.g.
?class label of xi?1 sentence,?
?class label of xi+1sentence,?
and ?unigram in xi?1 sentence,?
the clas-sifier determines the class label yi for each sentencexi independently.
It has been an assumption for textclassification tasks to decide a class label indepen-dently of other class labels.However, as described in Section 1, scientific ab-stracts have typical moves of rhetorical roles: itwould be very peculiar if result sentences appear-ing before method sentences were described in anabstract.
Moreover, we would like to model thestructure of abstract sentences rather than model-ing just the section label for each sentence.
Thus,the task is more suitably formalized as a sequencelabeling problem: given an abstract with sentencesx = (x1, ..., xn), determine the optimal sequence ofsection names y = (y1, ..., yn) of all possible se-quences.Conditional Random Fields (CRFs) have beensuccessfully applied to various NLP tasks includ-ing part-of-speech tagging (Lafferty et al, 2001) andshallow parsing (Sha and Pereira, 2003).
CRFs de-fine a conditional probability distribution p(y|x) foroutput and input sequences, y and x,p(y|x) =1Z?
(x)exp {?
?
F (y,x)} .
(1)Therein: function F (y,x) denotes a global featurevector for input sequence x and output sequence y,F (y,x) =?if(y,x, i), (2)i ranges over the input sequence, function f(y,x, i)is a feature vector for input sequence x and outputsequence y at position i (based on state features andtransition features), ?
is a vector where an element?k represents the weight of feature Fk(y,x), andZ?
(x) is a normalization factor,Z?
(x) =?yexp {?
?
F (y,x)} .
(3)The optimal output sequence y?
for an input se-quence x,y?
= argmaxyp(y|x), (4)is obtained efficiently by the Viterbi algorithm.
Theoptimal set of parameters ?
is determined efficientlyby the Generalized Iterative Scaling (GIS) (Darrochand Ratcliff, 1972) or Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) (Nocedal andWright, 1999) method.3.2 FeaturesWe design three kinds of features to represent eachabstract sentence for CRFs.
The contributions ofthese features will be evaluated later in Section 4.Content (n-gram) This feature examines the exis-tence of expressions that characterize a specific sec-tion, e.g.
?to determine ...,?
and ?aim at ...?
for stat-ing the objective of a study.
We use features for sen-tence contents represented by: i) words, ii) word bi-grams, and iii) mixture of words and word bigrams.Words are normalized into their base forms by theGENIA tagger (Tsuruoka and Tsujii, 2005), whichis a part-of-speech tagger trained for the biomedical383Rank OBJECTIVE METHOD RESULTS CONCLUSIONS1 # to be measure % ) suggest that2 be to be perform ( p may be3 to determine n = p < # these4 study be be compare ) .
should be5 this study be determine % .
these resultTable 2: Bigram features with high ?2 values (?#?
stands for a beginning of a sentence).domain.
We measure the co-occurrence strength (?2value) between each feature and section label.
If afeature appears selectively in a specific section, the?2 value is expected to be high.
Thus, we extract thetop 200,000 features2 that have high ?2 values to re-duce the total number of features.
Table 3.2 showsexamples of the top five bigrams that have high ?2values.Relative sentence location An abstract is likely tostate objective of the study at the beginning and itsconclusion at the end.
The position of a sentencemay be a good clue for determining its section la-bel.
Thus, we design five binary features to indicaterelative position of sentences in five scales.Features from previous/next w sentences Thisreproduces features from previous and following wsentences to the current sentence (w = {0, 1, 2}),so that a classifier can make use of the content ofthe surrounding sentences.
Duplicated features haveprefixes (e.g.
PREV_ and NEXT_) to distinguishtheir origins.3.3 Section labelsIt would require much effort and time to prepare alarge amount of abstracts annotated with section la-bels.
Fortunately, some Medline abstracts have sec-tion labels stated explicitly by its authors.
We ex-amined section labels in 7,811,582 abstracts in thewhole Medline3, using the regular-expression pat-tern:?
[A-Z]+([ ][A-Z]+){0,3}:[ ]A sentence is qualified to have a section name if itbegins with up to 4 uppercase token(s) followed by2We chose the number of features based on exploratory ex-periments.3The Medline database was up-to-date on March 2006.a colon ?:?.
This pattern identified 683,207 (ca.
9%)abstracts with structured sections.Table 3 shows typical moves of sections in Med-line abstracts.
The majority of sequences in thistable consists of four sections compatible with theANSI standard, purpose, methods, results, and con-clusions.
Moreover, the most frequent sequenceis ?OBJECTIVE ?
METHOD(S) ?
RESULTS?
CONCLUSION(S),?
supposing that AIM andPURPOSE are equivalent to OBJECTIVE.
Hence,this study assumes four sections, OBJECTIVE,METHOD, RESULTS, and CONCLUSIONS.Meanwhile, it is common for NP chunking tasksto represent a chunk (e.g., NP) with two labels,the begin (e.g., B-NP) and inside (e.g., I-NP) ofa chunk (Ramshaw and Marcus, 1995).
Althoughnone of the previous studies employed this repre-sentation, attaching B- and I- prefixes to section la-bels may improve a classifier by associating cluephrases (e.g., ?to determine?)
with the starts of sec-tions (e.g., B-OBJECTIVE).
We will compare clas-sification performances on two sets of label repre-sentations: namely, we will compare four sectionlabels and eight labels with BI prefixes attached tosection names.4 Evaluation4.1 ExperimentWe constructed two sets of corpora (?pure?
and ?ex-panded?
), each of which contains 51,000 abstractssampled from the abstracts with structured sections.The ?pure?
corpus consists of abstracts that have theexact four section labels.
In other words, this cor-pus does not include AIM or PURPOSE sentenceseven though they are equivalent to OBJECTIVE sen-tences.
The ?pure?
corpus is useful to compare theperformance of this study with the previous work.384Rank # abstracts (%) Section sequence1 111,617 (17.6) OBJECTIVE?METHOD(S)?
RESULT(S)?
CONCLUSION(S)2 107,124 (16.9) BACKGROUND(S)?METHOD(S)?
RESULT(S)?
CONCLUSION(S)3 40,083 (6.3) PURPOSE?METHOD(S)?
RESULT(S)?
CONCLUSION(S)4 20,519 (3.2) PURPOSE?MATERIAL AND METHOD(S)?
RESULT(S)?
CONCLUSION(S)5 16,705 (2.6) AIM(S)?METHOD(S)?
RESULT(S)?
CONCLUSION(S)6 16,400 (2.6) BACKGROUND?
OBJECTIVE?METHOD(S)?
RESULT(S)?
CONCLUSION(S)7 12,227 (1.9) OBJECTIVE?
STUDY DESIGN?
RESULT(S)?
CONCLUSION(S)8 11,483 (1.8) BACKGROUND?METHOD(S) AND RESULT(S)?
CONCLUSION(S)9 8,866 (1.4) OBJECTIVE?MATERIAL AND METHOD(S)?
RESULT(S)?
CONCLUSION(S)10 8,537 (1.3) PURPOSE?
PATIENT AND METHOD(S)?
RESULT(S)?
CONCLUSION(S).. ... ... ...Total 683,207 (100.0)Table 3: Typical sequences of sections in Medline abstractsRepresentative Equivalent section labelsOBJECTIVE AIM, AIM OF THE STUDY, AIMS, BACKGROUND/AIMS, BACKGROUND/PURPOSE, BACK-GROUND, BACKGROUND AND AIMS, BACKGROUND AND OBJECTIVE, BACKGROUND ANDOBJECTIVES, BACKGROUND AND PURPOSE, CONTEXT, INTRODUCTION, OBJECT, OBJEC-TIVE, OBJECTIVES, PROBLEM, PURPOSE, STUDY OBJECTIVE, STUDY OBJECTIVES, SUM-MARY OF BACKGROUND DATAMETHOD ANIMALS, DESIGN, DESIGN AND METHODS, DESIGN AND SETTING, EXPERIMENTAL DE-SIGN,INTERVENTION, INTERVENTION(S), INTERVENTIONS, MATERIAL AND METHODS, MA-TERIALS AND METHODS, MEASUREMENTS, METHOD, METHODOLOGY, METHODS, METH-ODS AND MATERIALS, PARTICIPANTS, PATIENT(S), PATIENTS, PATIENTS AND METHODS,PROCEDURE, RESEARCH DESIGN AND METHODS, SETTING, STUDY DESIGN, STUDY DESIGNAND METHODS, SUBJECTS, SUBJECTS AND METHODSRESULTS FINDINGS, MAIN RESULTS, RESULT, RESULT(S), RESULTSCONCLUSIONS CONCLUSION, CONCLUSION(S), CONCLUSIONS, CONCLUSIONS AND CLINICAL RELE-VANCE, DISCUSSION, IMPLICATIONS, INTERPRETATION, INTERPRETATION AND CONCLU-SIONSTable 4: Representative section names and their expanded sectionsIn contrast, the ?expanded?
corpus includes sen-tences in equivalent sections: AIM and PURPOSEsentences are mapped to the OBJECTIVE.
Table 4shows the sets of equivalent sections for representa-tive sections.
We created this mapping table man-ually by analyzing the top 100 frequent section la-bels found in the Medline.
The ?expanded?
corpusis close to the real situation in which the proposedmethod annotates unstructured abstracts.We utilized FlexCRFs4 implementation to builda classifier with linear-chain CRFs.
As a baselinemethod, we also prepared an SVM classifier5 withthe same features.4Flexible Conditional Random Field Toolkit (FlexCRFs):http://flexcrfs.sourceforge.net/5We used SVMlight implementation with the linear kernel,which achieved the best accuracy through this experiment:http://svmlight.joachims.org/203040506070801000  10000  100000Per-abstractaccuracy(%)Number of abstracts for trainingCRFSVMFigure 2: Training curve4.2 ResultsGiven the number of abstracts for training n, we ran-domly sampled n abstracts from a corpus for train-ing and 1,000 abtracts for testing.
Content (n-gram)features were generated for each trainig set.
We385Section labels With B- and I- prefixes Without B- and I- prefixesFeatures CRF SVM CRF SVMn-gram 88.7 (42.4) 81.5 (19.1) 85.7 (33.0) 83.3 (23.4)n-gram + position 93.4 (59.7) 88.2 (35.5) 92.4 (55.4) 89.6 (39.4)n-gram + surrounding (w = 1) 93.3 (60.4) 89.9 (42.2) 92.1 (52.8) 90.0 (42.0)n-gram + surrounding (w = 2) 93.7 (61.1) 91.8 (49.4) 92.8 (54.3) 91.8 (47.0)Full 94.3 (62.9) 93.3 (55.5) 93.3 (56.1) 92.9 (52.2)Table 5: Classification performance (accuracy) on ?pure?
corpus (n = 10, 000)Section labels With B- and I- prefixes Without B- and I- prefixesFeatures CRF SVM CRF SVMn-gram 87.7 (35.6) 78.5 (14.5) 81.9 (21.0) 80.0 (16.2)n-gram + position 92.6 (54.3) 87.1 (31.2) 91.4 (48.7) 88.1 (31.2)n-gram + surrounding (w = 1) 92.3 (52.0) 88.5 (37.6) 89.9 (44.0) 88.4 (37.1)n-gram + surrounding (w = 2) 92.4 (52.5) 90.1 (41.1) 91.2 (46.6) 90.4 (41.6)Full 93.0 (55.0) 92.0 (47.3) 92.5 (50.9) 91.7 (44.0)Table 6: Classification performance (accuracy) on ?expanded?
corpus (n = 10, 000)measured the classification accuracy of sentences(per-sentence accuracy) and abstracts (per-abstractaccuracy).
In per-abstract accuracy, an abstract isconsidered correct if all constituent sentences arecorrectly labeled.Trained with n = 50, 000 abstracts from ?pure?corpus, the proposed method achieved 95.5% per-sentence accuracy and 68.8% per-abstract accuracy.The F-score for each section label was 98.7% (O),95.8% (M), 95.0% (R), and 94.2% (C).
The pro-posed method performed this task better than theprevious studies by a great margin.
Figure 2 showsthe training curve for the ?pure?
corpus with all fea-tures presented in this paper.
CRF and SVM meth-ods performed better with more abstracts used fortraining.
This training curve demonstrated that, withless than half the number of training corpus, the pro-posed method could achieve the same accuracy asthe baseline method.Tables 5 and 6 report the performance of theproposed and baseline methods on ?pure?
and ?ex-panded?
corpora respectively (n = 10, 000).
Thesetables show per-sentence accuracy followed by per-abstract accuracy in parentheses with different con-figurations of features (row) and label representa-tions (column).
For example, the proposed methodobtained 94.3% per-sentence accuracy and 62.9%per-abstract accuracy with 10,000 training abstractsfrom ?pure?
corpus, all features, and BI prefixes forclass labels.The proposed method outperformed the baselinemethod in all experimental configurations.
Thissuggests that CRFs are more suitable for modelingmoves of rhetorical roles in scientific abstracts.
Itis noteworthy that the CRF classifier gained higherper-abstract accuracy than the SVM.
For example,both the CRF classifier with features from surround-ing sentences (w = 1), and SVM classifier with fullfeatures, obtained 93.3% per-sentence accuracy inTable 5.
Nevertheless, the per-abstract accuracies ofthe former and latter were 60.4% and 55.5% respec-tively: the CRF classifier had roughly 5% advantageon per-abstract accuracy over SVM.
This analysisreflects the capability of CRFs to determine the op-timal sequence of section names.Additional features such as sentence position andsurrounding sentences improved the performance byca.
5?10%.
The proposed method achieved the bestresults with all features.
Another interesting discus-sion arises with regard to the representations of sec-tion labels.
The BI representation always boostedthe per-abstract accuracy of CRF classifiers by ca.4?14%.
In contrast, the SVM classifier could notleverage the BI representation, and in some configu-rations, even degraded the accuracy.3865 ConclusionThis paper presented a novel approach to identifyingrhetorical roles in scientific abstracts using CRFs.The proposed method achieved more successful re-sults than any other previous reports.
The CRF clas-sifier had roughly 5% advantage on per-abstract ac-curacy over SVM.
The BI representation of sectionnames also boosted the classification accuracy by5%.
In total, the proposed method gained more than10% improvement on per-abstract accuracy.We have evaluated the proposed method only onmedical literatures.
In addition to improving theclassification performance, a future direction for thisstudy would be to examine the adaptability of theproposed method to include other types of texts.
Weare planning to construct a summarization systemusing the proposed method.ReferencesANSI.
1979.
American national standard for writingabstracts.
Z39.14-1979, American National StandardsInstitute (ANSI).Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine Learning, 20(3):273?297.John N. Darroch and Douglas Ratcliff.
1972.
General-ized iterative scaling for log-linear models.
The An-nals of Mathematical Statistics, 43(5):1470?1480.Harold P. Edmundson.
1969.
New methods in automaticextracting.
Journal of the Association for ComputingMachinery, 16(2):264?285.Christoph H. Gleiter, Tilmann Becker, Katharina H.Schreeb, Stefan Freudenthaler, and Ursula Gundert-Remy.
1997.
Fenoterol but not dobutamine increaseserythropoietin production in humans.
Clinical Phar-macology & Therapeutics, 61(6):669?676.Naomi Graetz.
1985.
Teaching EFL students to extractstructural information from abstracts.
In Jan M. Ulijnand Anthony K. Pugh, editors, Reading for Profes-sional Purposed: Methods and Materials in TeachingLanguages, pages 123?135.
Acco, Leuven, Belgium.Takahiko Ito, Masashi Simbo, Takahiro Yamasaki, andYuji Matsumoto.
2004.
Semi-supervised sentenceclassification for medline documents.
In IPSJ SIGTechnical Report, volume 2004-ICS-138, pages 141?146.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of the 18th International Conference on Ma-chine Learning (ICML-2001), pages 282?289.Jimmy Lin, Damianos Karakos, Dina Demner-Fushman,and Sanjeev Khudanpur.
2006.
Generative con-tent models for structural analysis of medical ab-stracts.
In Proceedings of the HLT/NAACL 2006Workshop on Biomedical Natural Language Process-ing (BioNLP?06), pages 65?72, New York City, USA.Daniel Marcu.
1999.
Discourse trees are good indicatorsof importance in text.
In Inderjeet Mani and Mark T.Maybury, editors, Advances in Automatic Text Summa-rization.
MIT Press.Larry McKnight and Padmini Arinivasan.
2003.
Cate-gorization of sentence types in medical abstracts.
InAMIA 2003 Symposium Proceedings, pages 440?444.Jorge Nocedal and Stephen J. Wright.
1999.
NumericalOptimization.
Springer-Verlag, New York, USA.Constantin Ora?san.
2001.
Patterns in scientific abstracts.In Proceedings of Corpus Linguistics 2001 Confer-ence, pages 433 ?
443, Lancaster University, Lan-caster, UK.Chris D. Paice.
1981.
The automatic generation of litera-ture abstracts: an approach based on the identificationof self-indicating phrases.
In SIGIR ?80: Proceedingsof the 3rd annual ACM conference on Research anddevelopment in information retrieval, pages 172?191,Kent, UK.
Butterworth & Co.Lance A. Ramshaw and Mitchell P. Marcus.
1995.
Textchunking using transformation-based learning.
In Pro-ceedings of the ACL 3rd Workshop on Very Large Cor-pora, pages 82?94.Patrick Ruch, Celia Boyer, Christine Chichester, ImadTbahriti, Antoine Geissbu?hler, Paul Fabry, Julien Gob-eill, Violaine Pillet, Dietrich Rebholz-Schuhmann,Christian Lovis, and Anne-Lise Veuthey.
2007.
Usingargumentation to extract key sentences from biomedi-cal abstracts.
International Journal of Medical Infor-matics, 76(2?3):195?200.Franc?oise Salanger-Meyer.
1990.
Discoursal flawsin medical english abstracts: A genre analysis perresearch- and text-type.
Text, 10(4):365?384.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In NAACL ?03: Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology, pages 134?141, Edmonton, Canada.387Masashi Shimbo, Takahiro Yamasaki, and Yuji Mat-sumoto.
2003.
Using sectioning information for textretrieval: a case study with the medline abstracts.
InProceedings of Second International Workshop on Ac-tive Mining (AM?03), pages 32?41.John M. Swales, 1990.
Genre Analysis: English in aca-demic and research settings, chapter 6.
CambridgeUniversity Press, UK.Imad Tbahriti, Christine Chichester, Fre?de?rique Lisacek,and Patrick Ruch.
2006.
Using argumentation to re-trieve articles with similar citations: An inquiry intoimproving related articles search in the medline digitallibrary.
International Journal OF Medical Informat-ics, 75(6):488?495.Simone Teufel and Marc Moens.
2002.
Summa-rizing scientific articles: experiments with relevanceand rhetorical status.
Computational Linguistics,28(4):409?445.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005.
Bidi-rectional inference with the easiest-first strategy fortagging sequence data.
In Proceedings of HumanLanguage Technology Conference and Conference onEmpirical Methods in Natural Language Processing,pages 467?474, Vancouver, British Columbia, Canada.Jien-Chen Wu, Yu-Chia Chang, Hsien-Chin Liou, andJason S. Chang.
2006.
Computational analysis ofmove structures in academic abstracts.
In Proceed-ings of the COLING/ACL on Interactive presentationsessions, pages 41?44, Sydney, Australia.Yasunori Yamamoto and Toshihisa Takagi.
2005.
A sen-tence classification system for multi-document sum-marization in the biomedical domain.
In Proceedingsof the International Workshop on Biomedical Data En-gineering (BMDE2005), pages 90?95.388
