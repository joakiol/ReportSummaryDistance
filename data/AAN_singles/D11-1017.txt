Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183?192,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsTraining a Parser for Machine Translation ReorderingJason Katz-Brown Slav Petrov Ryan McDonald Franz OchDavid Talbot Hiroshi Ichikawa Masakazu Seno Hideto KazawaGoogle{jasonkb|slav|ryanmcd|och|talbot|ichikawa|seno|kazawa}@google.comAbstractWe propose a simple training regime that canimprove the extrinsic performance of a parser,given only a corpus of sentences and a wayto automatically evaluate the extrinsic qualityof a candidate parse.
We apply our methodto train parsers that excel when used as partof a reordering component in a statistical ma-chine translation system.
We use a corpus ofweakly-labeled reference reorderings to guideparser training.
Our best parsers contributesignificant improvements in subjective trans-lation quality while their intrinsic attachmentscores typically regress.1 IntroductionThe field of syntactic parsing has received a greatdeal of attention and progress since the creation ofthe Penn Treebank (Marcus et al, 1993; Collins,1997; Charniak, 2000; McDonald et al, 2005;Petrov et al, 2006; Nivre, 2008).
A common?and valid?criticism, however, is that parsers typi-cally get evaluated only on Section 23 of the WallStreet Journal portion of the Penn Treebank.
Thisis problematic for many reasons.
As previously ob-served, this test set comes from a very narrow do-main that does not necessarily reflect parser perfor-mance on text coming from more varied domains(Gildea, 2001), especially web text (Foster, 2010).There is also evidence that after so much repeatedtesting, parsers are indirectly over-fitting to this set(Petrov and Klein, 2007).
Furthermore, parsing wasnever meant as a stand-alone task, but is rather ameans to an end, towards the goal of building sys-tems that can process natural language input.This is not to say that parsers are not used in largersystems.
All to the contrary, as parsing technologyhas become more mature, parsers have become ef-ficient and accurate enough to be useful in manynatural language processing systems, most notablyin machine translation (Yamada and Knight, 2001;Galley et al, 2004; Xu et al, 2009).
While it hasbeen repeatedly shown that using a parser can bringnet gains on downstream application quality, it is of-ten unclear how much intrinsic parsing accuracy ac-tually matters.In this paper we try to shed some light on this is-sue by comparing different parsers in the context ofmachine translation (MT).
We present experimentson translation from English to three Subject-Object-Verb (SOV) languages,1 because those require ex-tensive syntactic reordering to produce grammaticaltranslations.
We evaluate parse quality on a num-ber of extrinsic metrics, including word reorderingaccuracy, BLEU score and a human evaluation of fi-nal translation quality.
We show that while there isa good correlation between those extrinsic metrics,parsing quality as measured on the Penn Treebankis not a good indicator of the final downstream ap-plication quality.
Since the word reordering metriccan be computed efficiently offline (i.e.
without theuse of the final MT system), we then propose to tuneparsers specifically for that metric, with the goal ofimproving the performance of the overall system.To this end we propose a simple training regime1We experiment with Japanese, Korean and Turkish, butthere is nothing language specific in our approach.183which we refer to as targeted self-training (Sec-tion 2).
Similar to self-training, a baseline modelis used to produce predictions on an unlabeled dataset.
However, rather than directly training on theoutput of the baseline model, we generate a list ofhypotheses and use an external signal to select thebest candidate.
The selected parse trees are addedto the training data and the model is then retrained.The experiments in Section 5 show that this simpleprocedure noticeably improves our parsers for thetask at hand, resulting in significant improvementsin downstream translation quality, as measured in ahuman evaluation on web text.This idea is similar in vein to McClosky.
et al(2006) and Petrov et al (2010), except that we use anextrinsic quality metric instead of a second parsingmodel for making the selection.
It is also similar toBurkett and Klein (2008) and Burkett et al (2010),but again avoiding the added complexity introducedby the use of additional (bilingual) models for can-didate selection.It should be noted that our extrinsic metric is com-puted from data that has been manually annotatedwith reference word reorderings.
Details of the re-ordering metric and the annotated data we used aregiven in Sections 3 and 4.
While this annotation re-quires some effort, such annotations are much easierto obtain than full parse trees.
In our experimentsin Section 6 we show that we can obtain similarimprovements on downstream translation quality bytargeted self-training with weakly labeled data (inform of word reorderings), as with training on thefully labeled data (with full syntactic parse trees).2 Targeted Self-TrainingOur technique for retraining a baseline parser is anextension of self-training.
In standard parser self-training, one uses the baseline parsing model toparse a corpus of sentences, and then adds the 1-bestoutput of the baseline parser to the training data.
Totarget the self-training, we introduce an additionalstep, given as Algorithm 1.
Instead of taking the 1-best parse, we produce a ranked n-best list of predic-tions and select the parser which gives the best scoreaccording to an external evaluation function.
Thatis, instead of relying on the intrinsic model score,we use an extrinsic score to select the parse towardsAlgorithm 1 Select parse that maximizes an extrin-sic metric.Input: baseline parser BInput: sentence SInput: function COMPUTEEXTRINSIC(parse P )Output: a parse for the input sentencePn = {P1, .
.
.
, Pn} ?
n-best parses of S by BmaxScore = 0bestParse = ?for k = 1 to n doextrinsicScore = COMPUTEEXTRINSIC(Pk)if extrinsicScore > maxScore thenmaxScore = extrinsicScorebestParse = Pkend ifend forreturn bestParsewhich to update.
In the case of a tie, we prefer theparse ranked most highly in the n-best list.The motivation of this selection step is that goodperformance on the downstream external task, mea-sured by the extrinsic metric, should be predictiveof an intrinsically good parse.
At the very least,even if the selected parse is not syntactically cor-rect, or even if it goes against the original treebank-ing guidelines, it results in a higher extrinsic scoreand should therefore be preferred.One could imagine extending this framework byrepeatedly running self-training on successively im-proving parsers in an EM-style algorithm.
A recentwork by Hall et al (2011) on training a parser withmultiple objective functions investigates a similaridea in the context of online learning.In this paper we focus our attention on machinetranslation as the final application, but one could en-vision applying our techniques to other applicationssuch as information extraction or question answer-ing.
In particular, we explore one application oftargeted self-training, where computing the extrin-sic metric involves plugging the parse into an MTsystem?s reordering component and computing theaccuracy of the reordering compared to a referenceword order.
We now direct our attention to the de-tails of this application.1843 The MT Reordering TaskDetermining appropriate target language word or-der for a translation is a fundamental problem inMT.
When translating between languages with sig-nificantly different word order such as English andJapanese, it has been shown that metrics which ex-plicitly account for word-order are much better cor-related with human judgments of translation qual-ity than those that give more weight to word choice,like BLEU (Lavie and Denkowski, 2009; Isozaki etal., 2010a; Birch and Osborne, 2010).
This demon-strates the importance of getting reordering right.3.1 Reordering as a separately evaluablecomponentOne way to break down the problem of translat-ing between languages with different word orderis to handle reordering and translation separately:first reorder source-language sentences into target-language word order in a preprocessing step, andthen translate the reordered sentences.
It has beenshown that good results can be achieved by reorder-ing each input sentence using a series of tree trans-formations on its parse tree.
The rules for treetransformation can be manually written (Collins etal., 2005; Wang, 2007; Xu et al, 2009) or auto-matically learned (Xia and McCord, 2004; Habash,2007; Genzel, 2010).Doing reordering as a preprocessing step, sepa-rately from translation, makes it easy to evaluate re-ordering performance independently from the MTsystem.
Accordingly, Talbot et al (2011) present aframework for evaluating the quality of reorderingseparately from the lexical choice involved in trans-lation.
They propose a simple reordering metricbased on METEOR?s reordering penalty (Lavie andDenkowski, 2009).
This metric is computed solelyon the source language side.
To compute it, onetakes the candidate reordering of the input sentenceand partitions it into a set C of contiguous spanswhose content appears contiguously in the same or-der in the reference.
The reordering score is thencomputed as?
(esys, eref) = 1?|C| ?
1|e| ?
1 .This metric assigns a score between 0 and 1 where 1indicates that the candidate reordering is identical tothe reference and 0 indicates that no two words thatare contiguous in the candidate reordering are con-tiguous in the reference.
For example, if a referencereordering is A B C D E, candidate reordering AB E C D would get score 1?
(3?1)/(5?1) = 0.5.Talbot et al (2011) show that this reordering scoreis strongly correlated with human judgment of trans-lation quality.
Furthermore, they propose to evalu-ate the reordering quality of an MT system by com-puting its reordering score on a test set consistingof source language sentences and their reference re-orderings.
In this paper, we take the same approachfor evaluation, and in addition, we use corpora ofsource language sentences and their reference re-orderings for training the system, not just testingit.
We describe in more detail how the reference re-ordering data was prepared in Section 4.1.3.2 Reordering quality as predictor of parsequalityFigure 1 gives concrete examples of good and badreorderings of an English sentence into Japaneseword order.
It shows that a bad parse leads to a badreordering (lacking inversion of verb ?wear?
and ob-ject ?sunscreen?)
and a low reordering score.
Couldwe flip this causality around, and perhaps try to iden-tify a good parse tree based on its reordering score?With the experiments in this paper, we show that in-deed a high reordering score is predictive of the un-derlying parse tree that was used to generate the re-ordering being a good parse (or, at least, being goodenough for our purpose).In the case of translating English to Japanese oranother SOV language, there is a large amount ofreordering required, but with a relatively small num-ber of reordering rules one can cover a large pro-portion of reordering phenomena.
Isozaki et al(2010b), for instance, were able to get impressiveEnglish?Japanese results with only a single re-ordering rule, given a suitable definition of a head.Hence, the reordering task depends crucially on acorrect syntactic analysis and is extremely sensitiveto parser errors.1854 Experimental Setup4.1 Treebank dataIn our experiments the baseline training corpus isthe Wall Street Journal (WSJ) section of the PennTreebank (Marcus et al, 1993) using standard train-ing/development/testing splits.
We converted thetreebank to match the tokenization expected by ourMT system.
In particular, we split tokens containinghyphens into multiple tokens and, somewhat sim-plistically, gave the original token?s part-of-speechtag to all newly created tokens.
In Section 6 wemake also use of the Question Treebank (QTB)(Judge et al, 2006), as a source of syntactically an-notated out-of-domain data.
Though we experimentwith both dependency parsers and phrase structureparsers, our MT system assumes dependency parsesas input.
We use the Stanford converter (de Marneffeet al, 2006) to convert phrase structure parse trees todependency parse trees (for both treebank trees andpredicted trees).4.2 Reference reordering dataWe aim to build an MT system that can accuratelytranslate typical English text that one finds on theInternet to SOV langauges.
To this end, we ran-domly sampled 13595 English sentences from theweb and created Japanese-word-order reference re-orderings for them.
We split the sentences arbitrarilyinto a 6268-sentence Web-Train corpus and a 7327-sentence Web-Test corpus.To make the reference alignments we used thetechnique suggested by Talbot et al (2011): askannotators to translate each English sentence toJapanese extremely literally and annotate which En-glish words align to which Japanese words.
Goldenreference reorderings can be made programmati-cally from these annotations.
Creating a large setof reference reorderings is straightforward becauseannotators need little special background or train-ing, as long as they can speak both the source andtarget languages.
We chose Japanese as the targetlanguage through which to create the English refer-ence reorderings because we had access to bilingualannotators fluent in English and Japanese.Good parseReordered:15 or greater of an SPF has that sunscreen WearReordering score: 1.0 (matches reference)Bad parseReordered:15 or greater of an SPF has that Wear sunscreenReordering score: 0.78 (?Wear?
is out of place)Figure 1: Examples of good and bad parses and cor-responding reorderings for translation from English toJapanese.
The good parse correctly identifies ?Wear?
asthe main verb and moves it to the end of the sentence; thebad parse analyses ?Wear sunscreen?
as a noun phraseand does not reorder it.
This example was one of thewins in the human evaluation of Section 5.2.4.3 ParsersThe core dependency parser we use is an implemen-tation of a transition-based dependency parser usingan arc-eager transition strategy (Nivre, 2008).
Theparser is trained using the averaged perceptron algo-rithm with an early update strategy as described inZhang and Clark (2008).
The parser uses the fol-lowing features: word identity of the first two wordson the buffer, the top word on the stack and the headof the top word on the stack (if available); part-of-speech identities of the first four words on the bufferand top two words on the stack; dependency arc la-bel identities for the top word on the stack, the leftand rightmost modifier of the top word on the stack,and the leftmost modifier of the first word in thebuffer.
We also include conjunctions over all non-lexical features.We also give results for the latent variable parser(a.k.a.
BerkeleyParser) of Petrov et al (2006).
Weconvert the constituency trees output by the Berke-leyParser to labeled dependency trees using the sameprocedure that is applied to the treebanks.While the BerkeleyParser views part-of-speech(POS) tagging as an integral part of parsing, ourdependency parser requires the input to be tagged186with a separate POS tagger.
We use the TnT tag-ger (Brants, 2000) in our experiments, because ofits efficiency and ease of use.
Tagger and parser arealways trained on the same data.For all parsers, we lowercase the input at train andtest time.
We found that this improves performancein parsing web text.
In addition to general upper-case/lowercase noisiness of the web text negativelyimpacting scores, we found that the baseline case-sensitive parsers are especially bad at parsing imper-ative sentences, as discussed in Section 5.3.2.4.4 Reordering rulesIn this paper we focus on English to Japanese, Ko-rean, and Turkish translation.
We use a superset ofthe reordering rules proposed by Xu et al (2009),which flatten a dependency tree into SOV word or-der that is suitable for all three languages.
The rulesdefine a precedence order for the dependents of eachpart of speech.
For example, a slightly simplifiedversion of the precedence order of child labels fora verbal head HEADVERB is: advcl, nsubj, prep,[other children], dobj, prt, aux, neg, HEADVERB,mark, ref, compl.Alternatively, we could have used an automaticreordering-rule learning framework like that of Gen-zel (2010).
Because the reordering accuracy met-ric can be computed for any source/target languagepair, this would have made our approach languagecompletely independent and applicable to any lan-guage pair.
We chose to use manually written rulesto eliminate the variance induced by the automaticreordering-rule learning framework.4.5 MT systemWe carried out all our translation experiments on astate-of-the-art phrase-based statistical MT system.During both training and testing, the system reorderssource-language sentences in a preprocessing stepusing the above-mentioned rules.
During decoding,we used an allowed jump width of 4 words.
In ad-dition to the regular distance distortion model, weincorporate a maximum entropy based lexicalizedphrase reordering model (Zens and Ney, 2006) asa feature used in decoding.Overall for decoding, we use between 20 to30 features, whose weights are optimized usingMERT (Och, 2003).
All experiments for a given lan-guage pair use the same set of MERT weights tunedon a system using a separate parser (that is neitherthe baseline nor the experiment parser).
This po-tentially underestimates the improvements that canbe obtained, but also eliminates MERT as a pos-sible source of improvement, allowing us to traceback improvements in translation quality directly toparser changes.2For parallel training data, we use a custom collec-tion of parallel documents.
They come from vari-ous sources with a substantial portion coming fromthe web after using simple heuristics to identify po-tential document pairs.
For all language pairs, wetrained on approximately 300 million source wordseach.5 Experiments Reordering Web TextWe experimented with parsers trained in three dif-ferent ways:1.
Baseline: trained only on WSJ-Train.2.
Standard self-training: trained on WSJ-Trainand 1-best parse of the Web-Train set by base-line parser.3.
Targeted self-training: trained on WSJ-Trainand, for each sentence in Web-Train, the parsefrom the baseline parser?s 512-best list thatwhen reordered gives the highest reorderingscore.35.1 Standard self-training vs targetedself-trainingTable 1 shows that targeted self-training on Web-Train significantly improves Web-Test reorderingscore more than standard self-training for both theshift-reduce parser and for the BerkeleyParser.
Thereordering score is generally divorced from the at-tachment scores measured on the WSJ-Test tree-bank: for the shift-reduce parser, Web-Test reorder-ing score and WSJ-Test labeled attachment score2We also ran MERT on all systems and the pattern of im-provement is consistent, but sometimes the improvement is big-ger or smaller after MERT.
For instance, the BLEU delta forJapanese is +0.0030 with MERT on both sides as opposed to+0.0025 with no MERT.3We saw consistent but diminishing improvements as we in-creased the size of the n-best list.187Parser Web-Test reordering WSJ-Test LASShift-reduce WSJ baseline 0.757 85.31%+ self-training 1x 0.760 85.26%+ self-training 10x 0.756 84.14%+ targeted self-training 1x 0.770 85.19%+ targeted self-training 10x 0.777 84.48%Berkeley WSJ baseline 0.780 88.66%+ self-training 1x 0.785 89.21%+ targeted self-training 1x 0.790 89.32%Table 1: English?Japanese reordering scores on Web-Test for standard self-training and targeted self-training onWeb-Train.
Label ?10x?
indicates that the self-training data was weighted 10x relative to the WSJ training data.Bolded reordering scores are different from WSJ-only baseline with 95% confidence but are not significantly differentfrom each other within the same group.English to BLEU Human evaluation (scores range 0 to 6)WSJ-only Targeted WSJ-only Targeted Sig.
difference?Japanese 0.1777 0.1802 2.56 2.69 yes (at 95% level)Korean 0.3229 0.3259 2.61 2.70 yes (at 90% level)Turkish 0.1344 0.1370 2.10 2.20 yes (at 95% level)Table 2: BLEU scores and human evaluation results for translation between three language pairs, varying only theparser between systems.
?WSJ-only?
corresponds to the baseline WSJ-only shift-reduce parser; ?Targeted?
corre-sponds to the Web-Train targeted self-training 10x shift-reduce parser.
(LAS) are anti-correlated, but for BerkeleyParserthey are correlated.
Interestingly, weighting the self-training data more seems to have a negative effect onboth metrics.4One explanation for the drops in LAS is that someparts of the parse tree are important for downstreamreordering quality while others are not (or only toa lesser extent).
Some distinctions between labelsbecome less important; for example, arcs labeled?amod?
and ?advmod?
are transformed identicallyby the reordering rules.
Some semantic distinctionsalso become less important; for example, any saneinterpretation of ?red hot car?
would be reorderedthe same, that is, not at all.5.2 Translation quality improvementTo put the improvement of the MT system in termsof BLEU score (Papineni et al, 2002), a widely usedmetric for automatic MT evaluation, we took 5000sentences from Web-Test and had humans gener-ate reference translations into Japanese, Korean, and4We did not attempt this experiment for the BerkeleyParsersince training was too slow.Turkish.
We then trained MT systems varying onlythe parser used for reordering in training and decod-ing.
Table 2 shows that targeted self-training dataincreases BLEU score for translation into all threelanguages.In addition to BLEU increase, a side-by-side hu-man evaluation on 500 sentences (sampled fromthe 5000 used to compute BLEU scores) showeda statistically significant improvement for all threelanguages (see again Table 2).
For each sen-tence, we asked annotators to simultaneously scoreboth translations from 0 to 6, with guidelinesthat 6=?Perfect?, 4=?Most Meaning/Grammar?,2=?Some Meaning/Grammar?, 0=?Nonsense?.
Wecomputed confidence intervals for the average scoredifference using bootstrap resampling; a differenceis significant if the two-sided confidence intervaldoes not include 0.5.3 AnalysisAs the divergence between the labeled attachmentscore on the WSJ-Test data and the reordering scoreon the WSJ-Test data indicates, parsing web text188Parser Click as N Click as V Imperative ratecase-sensitive shift-reduce WSJ-only 74 0 6.3%case-sensitive shift-reduce + Web-Train targeted self-training 75 0 10.5%case-insensitive shift-reduce WSJ-only 75 0 10.3%case-insensitive shift-reduce + Web-Train targeted self-training 75 0 11.6%Berkeley WSJ-only 35 35 11.9%Berkeley + Web-Train targeted self-training 13 58 12.5%(WSJ-Train) 1 0 0.7%Table 3: Counts on Web-Test of ?click?
tagged as a noun and verb and percentage of sentences parsed imperatively.poses very different challenges compared to parsingnewswire.
We show how our method improves pars-ing performance and reordering performance on twoexamples: the trendy word ?click?
and imperativesentences.5.3.1 ClickThe word ?click?
appears only once in the train-ing portion of the WSJ (as a noun), but appears manytimes in our Web test data.
Table 3 shows the distri-bution of part-of-speech tags that different parsersassign to ?click?.
The WSJ-only parsers tag ?click?as a noun far too frequently.
The WSJ-only shift-reduce parser refuses to tag ?click?
as a verb evenwith targeted self-training, but BerkeleyParser doeslearn to tag ?click?
more often as a verb.It turns out that the shift-reduce parser?s stub-bornness is not due to a fundamental problem ofthe parser, but due to an artifact in TnT.
To in-crease speed, TnT restricts the choices of tags forknown words to previously-seen tags.
This causesthe parser?s n-best lists to never hypothesize ?click?as a verb, and self-training doesn?t click no matterhow targeted it is.
This shows that the targeted self-training approach heavily relies on the diversity ofthe baseline parser?s n-best lists.It should be noted here that it would be easy tocombine our approach with the uptraining approachof Petrov et al (2010).
The idea would be to use theBerkeleyParser to generate the n-best lists; perhapswe could call this targeted uptraining.
This way, theshift-reduce parser could benefit both from the gen-erally higher quality of the parse trees produced bythe BerkeleyParser, as well as from the informationprovided by the extrinsic scoring function.5.3.2 ImperativesAs Table 3 shows, the WSJ training set containsonly 0.7% imperative sentences.5 In contrast, ourtest sentences from the web contain approximately10% imperatives.
As a result, parsers trained exclu-sively on the WSJ underproduce imperative parses,especially a case-sensitive version of the baseline.Targeted self-training helps the parsers to predict im-perative parses more often.Targeted self-training works well for generatingtraining data with correctly-annotated imperativeconstructions because the reordering of main sub-jects and verbs in an SOV language like Japaneseis very distinct: main subjects stay at the begin-ning of the sentence, and main verbs are reorderedto the end of the sentence.
It is thus especially easyto know whether an imperative parse is correct ornot by looking at the reference reordering.
Figure 1gives an example: the bad (WSJ-only) parse doesn?tcatch on to the imperativeness and gets a low re-ordering score.6 Targeted Self-Training vs Training onTreebanks for Domain AdaptationIf task-specific annotation is cheap, then it is rea-sonable to consider whether we could use targetedself-training to adapt a parser to a new domain asa cheaper alternative to making new treebanks.
Forexample, if we want to build a parser that can reorderquestion sentences better than our baseline WSJ-only parser, we have these two options:1.
Manually construct PTB-style trees for 20005As an approximation, we count every parse that begins witha root verb as an imperative.189questions and train on the resulting treebank.2.
Create reference reorderings for 2000 questionsand then do targeted self-training.To compare these approaches, we created referencereordering data for our train (2000 sentences) andtest (1000 sentences) splits of the Question Tree-bank (Judge et al, 2006).
Table 4 shows that bothways of training on QTB-Train sentences give sim-ilarly large improvements in reordering score onQTB-Test.
Table 5 confirms that this correspondsto very large increases in English?Japanese BLEUscore and subjective translation quality.
In the hu-man side-by-side comparison, the baseline transla-tions achieved an average score of 2.12, while thetargeted self-training translations received a score of2.94, where a score of 2 corresponds to ?some mean-ing/grammar?
and ?4?
corresponds to ?most mean-ing/grammar?.But which of the two approaches is better?
Inthe shift-reduce parser, targeted self-training giveshigher reordering scores than training on the tree-bank, and in BerkeleyParser, the opposite is true.Thus both approaches produce similarly good re-sults.
From a practical perspective, the advantage oftargeted self-training depends on whether the extrin-sic metric is cheaper to calculate than treebanking.For MT reordering, making reference reorderings ischeap, so targeted self-training is relatively advanta-geous.As before, we can examine whether labeled at-tachment score measured on the test set of theQTB is predictive of reordering quality.
Table 4shows that targeted self-training raises LAS from64.78?69.17%.
But adding the treebank leadsto much larger increases, resulting in an LAS of84.75%, without giving higher reordering score.
Wecan conclude that high LAS is not necessary toachieve top reordering scores.Perhaps our reordering rules are somehow defi-cient when it comes to reordering correctly-parsedquestions, and as a result the targeted self-trainingprocess steers the parser towards producing patho-logical trees with little intrinsic meaning.
To explorethis possibility, we computed reordering scores afterreordering the QTB-Test treebank trees directly.
Ta-ble 4 shows that this gives reordering scores similarto those of our best parsers.
Therefore it is at leastpossible that the targeted self-training process couldhave resulted in a parser that achieves high reorder-ing score by producing parses that look like those inthe QuestionBank.7 Related WorkOur approach to training parsers for reordering isclosely related to self/up-training (McClosky.
et al,2006; Petrov et al, 2010).
However, unlike uptrain-ing, our method does not use only the 1-best outputof the first-stage parser, but has access to the n-bestlist.
This makes it similar to the work of McClosky.et al (2006), except that we use an extrinsic metric(MT reordering score) to select a high quality parsetree, rather than a second, reranking model that hasaccess to additional features.Targeted self-training is also similar to the re-training of Burkett et al (2010) in which theyjointly parse unannotated bilingual text using a mul-tiview learning objective, then retrain the monolin-gual parser models to include each side of the jointlyparsed bitext as monolingual training data.
Our ap-proach is different in that it doesn?t use a secondparser and bitext to guide the creation of new train-ing data, and instead relies on n-best lists and anextrinsic metric.Our method can be considered an instance ofweakly or distantly supervised structured prediction(Chang et al, 2007; Chang et al, 2010; Clarke et al,2010; Ganchev et al, 2010).
Those methods attemptto learn structure models from related external sig-nals or aggregate data statistics.
This work differsin two respects.
First, we use the external signalsnot as explicit constraints, but to compute an ora-cle score used to re-rank a set of parses.
As such,there are no requirements that it factor by the struc-ture of the parse tree and can in fact be any arbitrarymetric.
Second, our final objective is different.
Inweakly/distantly supervised learning, the objectiveis to use external knowledge to build better struc-tured predictors.
In our case this would mean usingthe reordering metric as a means to train better de-pendency parsers.
Our objective, on the other hand,is to use the extrinsic metric to train parsers that arespecifically better at the reordering task, and, as a re-sult, better suited for MT.
This makes our work morein the spirit of Liang et al (2006), who train a per-190Parser QTB-Test reordering QTB-Test LASShift-reduce WSJ baseline 0.663 64.78%+ treebank 1x 0.704 77.12%+ treebank 10x 0.768 84.75%+ targeted self-training 1x 0.746 67.84%+ targeted self-training 10x 0.779 69.17%Berkeley WSJ baseline 0.733 76.50%+ treebank 1x 0.800 87.79%+ targeted self-training 1x 0.775 80.64%(using treebank trees directly) 0.788 100%Table 4: Reordering and labeled attachment scores on QTB-Test for treebank training and targeted self-training onQTB-Train.English to QTB-Test BLEU Human evaluation (scores range 0 to 6)WSJ-only Targeted WSJ-only Targeted Sig.
difference?Japanese 0.2379 0.2615 2.12 2.94 yes (at 95% level)Table 5: BLEU scores and human evaluation results for English?Japanese translation of the QTB-Test corpus, varyingonly the parser between systems between the WSJ-only shift-reduce parser and the QTB-Train targeted self-training10x shift-reduce parser.ceptron model for an end-to-end MT system wherethe alignment parameters are updated based on se-lecting an alignment from a n-best list that leads tohighest BLEU score.
As mentioned earlier, this alsomakes our work similar to Hall et al (2011) whotrain a perceptron algorithm on multiple objectivefunctions with the goal of producing parsers that areoptimized for extrinsic metrics.It has previously been observed that parsers of-ten perform differently for downstream applications.Miyao et al (2008) compared parser quality in thebiomedical domain using a protein-protein interac-tion (PPI) identification accuracy metric.
This al-lowed them to compare the utility of extant depen-dency parsers, phrase structure parsers, and deepstructure parsers for the PPI identification task.
Onecould apply the targeted self-training technique wedescribe to optimize any of these parsers for the PPItask, similar to how we have optimized our parserfor the MT reordering task.8 ConclusionWe introduced a variant of self-training that targetsparser training towards an extrinsic evaluation met-ric.
We use this targeted self-training approach totrain parsers that improve the accuracy of the wordreordering component of a machine translation sys-tem.
This significantly improves the subjective qual-ity of the system?s translations from English intothree SOV languages.
While the new parsers giveimprovements in these external evaluations, their in-trinsic attachment scores go down overall comparedto baseline parsers trained only on treebanks.
Weconclude that when using a parser as a componentof a larger external system, it can be advantageousto incorporate an extrinsic metric into parser train-ing and evaluation, and that targeted self-training isan effective technique for incorporating an extrinsicmetric into parser training.ReferencesA.
Birch and M. Osborne.
2010.
LRscore for evaluatinglexical and reordering quality in MT.
In ACL-2010WMT.T.
Brants.
2000.
TnT ?
a statistical part-of-speech tagger.In ANLP ?00.D.
Burkett and D. Klein.
2008.
Two languages are betterthan one (for syntactic parsing).
In EMNLP ?08.D.
Burkett, S. Petrov, J. Blitzer, and D. Klein.
2010.Learning better monolingual models with unannotatedbilingual text.
In CoNLL ?10.191M.
Chang, L. Ratinov, and D. Roth.
2007.
Guiding semi-supervision with constraint-driven learning.
In ACL?07.M.
Chang, D. Goldwasser, D. Roth, and V. Srikumar.2010.
Structured output learning with indirect super-vision.
In ICML ?10.E.
Charniak.
2000.
A maximum?entropy?inspiredparser.
In NAACL ?00.J.
Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010.Driving semantic parsing from the world?s response.In CoNLL ?10.M.
Collins, P. Koehn, and I. Kuc?erova?.
2005.
Clause re-structuring for statistical machine translation.
In ACL?05.M.
Collins.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In ACL ?97.M.-C. de Marneffe, B. MacCartney, and C. Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In LREC ?06.J.
Foster.
2010.
?cba to check the spelling?
: Investigat-ing parser performance on discussion forum posts.
InNAACL ?10.M.
Galley, M. Hopkins, K. Knight, and D. Marcu.
2004.What?s in a translation rule?
In HLT-NAACL ?04.K.
Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.2010.
Posterior regularization for structured latentvariable models.
Journal of Machine Learning Re-search.D.
Genzel.
2010.
Automatically learning source-side re-ordering rules for large scale machine translation.
InCOLING ?10.D.
Gildea.
2001.
Corpus variation and parser perfor-mance.
In EMNLP ?01.N.
Habash.
2007.
Syntactic preprocessing for statisticalmachine translation.
In MTS ?07.K.
Hall, R. McDonald, J. Katz-Brown, and M. Ringgaard.2011.
Training dependency parsers by jointly optimiz-ing multiple objectives.
In EMNLP ?11.H.
Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada.2010a.
Automatic evaluation of translation quality fordistant language pairs.
In EMNLP ?10.H.
Isozaki, K. Sudoh, H. Tsukada, and K. Duh.
2010b.Head finalization: A simple reordering rule for SOVlanguages.
In ACL-2010 WMT.J.
Judge, A. Cahill, and J. v. Genabith.
2006.
Question-Bank: creating a corpus of parse-annotated questions.In ACL ?06.A.
Lavie and M. Denkowski.
2009.
The Meteor metricfor automatic evaluation of machine translation.
Ma-chine Translation, 23(2-3).P.
Liang, A.
Bouchard-Co?te?, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach to ma-chine translation.
In ACL ?06.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: ThePenn Treebank.
In Computational Linguistics.D.
McClosky., E. Charniak, and M. Johnson.
2006.
Ef-fective self-training for parsing.
In NAACL ?06.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In ACL?05.Y.
Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and J. Tsu-jii.
2008.
Task-oriented evaluation of syntactic parsersand their representations.
In ACL ?08.J.
Nivre.
2008.
Algorithms for deterministic incremen-tal dependency parsing.
Computational Linguistics,34(4).F.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In ACL ?03.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In ACL ?02.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In NAACL ?07.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In ACL ?06.S.
Petrov, P. Chang, and M. Ringgaard H. Alshawi.
2010.Uptraining for accurate deterministic question parsing.In EMNLP ?10.D.
Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,M.
Seno, and F. Och.
2011.
A lightweight evalua-tion framework for machine translation reordering.
InEMNLP-2011 WMT.C.
Wang.
2007.
Chinese syntactic reordering for statisti-cal machine translation.
In EMNLP ?07.F.
Xia and M. McCord.
2004.
Improving a statistical MTsystem with automatically learned rewrite patterns.
InColing ?04.P.
Xu, J. Kang, M. Ringgaard, and F. Och.
2009.
Using adependency parser to improve SMT for subject-object-verb languages.
In NAACL-HLT ?09.K.
Yamada and K. Knight.
2001.
A syntax-based statis-tical translation model.
In ACL ?01.R.
Zens and H. Ney.
2006.
Discriminative reorderingmodels for statistical machine translation.
In NAACL-06 WMT.Y.
Zhang and S. Clark.
2008.
A tale of two parsers: In-vestigating and combining graph-based and transition-based dependency parsing.
In EMNLP ?08.192
