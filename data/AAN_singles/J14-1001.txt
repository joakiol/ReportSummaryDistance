ObituaryIvan A. SagEmily M. BenderUniversity of WashingtonIvan Sag died on September 10, 2013, after a long illness.
He is survived by his wife,Stanford sociolinguistics professor Penny Eckert.
In a career spanning four decades, hepublished over 100 articles and 10 books, centered on the theme of developing precise,implementable, testable, psychologically plausible, and scalable models of natural lan-guage, especially syntax and semantics.Sag earned a B.A.
from the University of Rochester (1971), an M.A.
from theUniversity of Pennsylvania (1973) and a Ph.D. from MIT (1976), all in Linguistics.
Heheld teaching positions at the University of Pennsylvania (1976?1979) and StanfordUniversity (1979?2013) where he was the Sadie Dunham Patek Professor in Humanitiessince 2008.
In addition, he taught at ten LSA Linguistic Institutes (most recently as theEdward Sapir Professor at the Linguistic Institute at the University of Colorado, Boul-der, in 2011), at five ESSLLIs, as well as in summer schools or other visiting positionsat NTNU (Trondheim), Universite?
de Paris 7, Rijksuniversiteit Utrecht, the Universityof Rochester, the University of Chicago, and Harvard University.
At Stanford he wasa founding member of and active participant in the Center for the Study of Languageand Information (CSLI), which housed the HPSG and LinGO projects.
He was also akey member of the group of faculty that developed and launched the Symbolic Systemsprogram (in 1985) and was director of Symbolic Systems from 2000?2001 and 2005?2009.Among other honors, he was elected to the American Academy of Arts and Sciences in2007 and named a Fellow of the Linguistic Society of America in 2008.Sag?s advisor was Noam Chomsky; throughout his career, he saw himself as fur-thering what he understood to be the original Chomskyan enterprise.
However, in thelate 1970s, he broke with the Chomskyan mainstream because he felt it had abandonedcentral aspects of that original enterprise.
Here?s how Sag told the story to Ta!
magazinein 1993:Well, it has always been hard for me to reconcile the original Chomskyan research goalsand methods with most of the modern work that goes on.
Though Chomsky has deniedthis.
Current work in so-called Government and Binding Theory is basicallyformulating ideas in ways that are so loose that they do not add up to preciselyconstructing hypotheses about the nature of language.All too often, people throw around formalisms that have no precise interpretationand the consequences of particular proposals are absolutely impossible to assess.
In myopinion that is just not the way to do science.
I think that the original goals ofgenerative grammar do constitute a set of desiderata for the science of language thatone can try to execute with much greater success than current work in GB has achievedor is likely to, given the directions it seems to be going in.doi:10.1162/COLI a 00179?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 1The result of Sag holding to the initial goals of generative grammar, even whenmainstream syntax did not, has been an enormous boon for the field of computationallinguistics.
Whereas much mainstream work in theoretical syntax is neither explicitlyformalized nor concerned with broad coverage, the frameworks that Sag wasinstrumental in helping to create (Generalized Phrase Structure Grammar, Head-driven Phrase Structure Grammar, and Sign-Based Construction Grammar [Boas andSag 2012], but especially HPSG) are implementable and in fact implemented, anddemonstrably scalable.Sag first encountered the community working on what would come to be calledGeneralized Phrase Structure Grammar (GPSG, a term coined by Sag), and in particularGerald Gazdar and Geoff Pullum, at the 1978 LSA Linguistic Institute.
Gazdar andcolleagues set out to show that English (and other natural languages) could in fact bedescribed with context-free models, as Pullum and Gazdar (1982) had debunked allprevious arguments against that claim.
But more importantly Sag and his colleaguesdeveloping GPSG strove to be formally precise, in order to support valid scientificinvestigation.1 The GPSG book (Gazdar et al.
1985) begins by throwing down thegauntlet:This book contains a fairly complete exposition of a general theory of grammar that wehave worked out in detail over the past four years.
Unlike much theoretical linguistics,it lays considerable stress on detailed specifications both of the theory and of thedescriptions of parts of English grammar that we use to illustrate the theory.
We do notbelieve that the working out of such details can be dismissed as ?a matter of execution?,to be left to lab assistants.
In serious work, one cannot ?assume some version of theX-bar theory?
or conjecture that a ?suitable?
set of interpretive rules will do something asdesired, any more than one can evade the entire enterprise of generative grammar byannouncing: ?We assume some recursive function that assigns to each grammatical andmeaningful sentence of English an appropriate structure and interpretation.?
One mustset about constructing such a function, or one is not in the business of theoreticallinguistics.
(p. ix)The computational benefits of that precision were quickly apparent.
In 1981, Sagtaught a course on GPSG at Stanford with Gazdar and Pullum.
One of the studentsattending that course, Anne Paulson, was working at Hewlett-Packard Labs and saw thepotential for using GPSG as the basis of a question answering system (with a databaseback-end).
Paulson arranged a meeting between her boss, Egon Loebner, and Sag,Gazdar, Pullum, and Tom Wasow, which led to a nearly decade-long project implement-ing a grammar for English and processing tools to work with it.
The project included HPstaff as well as Sag, Pullum, and Wasow as consultants, and Stanford and UC Berkeleystudents, including Mark Gawron, Carl Pollard, and Dan Flickinger.The work initially set out to implement GPSG (Gawron et al.
1982), but in thecontext of developing and implementing analyses, Sag and colleagues added inno-vations to the underlying theory until its formal basis was so different it warranteda new name.
The new theory, laid out in Pollard and Sag 1987 and Pollard and Sag1994, among others, came to be called Head-driven Phrase Structure Grammar (HPSG).1 The formal precision had much more lasting impact than the attempt at a context-free model: EventuallyBresnan et al.
(1982), Culy (1985), and Shieber (1985) found more solid counterexamples to the claim thatnatural languages can be modeled with CF-PSGs.2Bender ObituaryHPSG synthesizes ideas from GPSG, Dependency Grammar (Hudson 1984), CategorialGrammar (Wood 1993), Lexical Functional Grammar (Bresnan and Kaplan 1982), andeven Government and Binding Theory (Chomsky 1981).Importantly, rather than encoding theoretical results as constraints on the formal-ism, HPSG defines a flexible formalism (typed feature structures) in which differenttheories can be defined.
This flexibility facilitates testing and synthesis of theoreticalideas developed in other frameworks.
The stability of the formalism has been critical tothe success of HPSG in computational linguistics, as it has allowed for the developmentof a variety of processing engines that interpret the formalism and thus can applygrammars to the tasks of parsing and generation (Uszkoreit et al.
1994; Carpenter andPenn 1994; Makino et al.
1998; Copestake 2002; Callmeier 2002; Penn 2004; Crysmannand Packard 2012; Slayden 2012, inter alios).HPSG-based parsing was deployed in Verbmobil (Wahlster 2000), a large-scale,multi-site machine translation project funded by the German government, for whichSag headed up the English Grammar Project (1994?2000), later redubbed LinGO (Lin-guistic Grammars Online), at CSLI.
The English grammar developed in that project(beginning actually in 1993) came to be known as the English Resource Grammar(Flickinger 2000, 2011).2 The resource grammar idea builds on insights articulated inGawron et al.
1982, namely, that detailed syntactic and semantic analysis are a crucialcomponent of natural language understanding (in addition to discourse and worldknowledge processing) and that the grammar which does that analysis can and shouldbe portable across domains.The English Resource Grammar has been refined and extended as it has beendeveloped in the context of applications ranging from machine translation of dialoguesregarding travel and appointment scheduling (Verbmobil; Wahlster 2000), automatedcustomer service response (YY Technologies), machine translation of Norwegian hikingbrochures (LOGON; Oepen et al.
2007), and grammar checking as part of a languagearts instructional application (EPGY/Redbird; Suppes et al.
2012).
The work of buildinga grammar such as this involves identifying phenomena in sentences from the domainof interest that the grammar does not yet account for, delimiting the phenomena, anddeveloping and implementing analyses.
Throughout the mid and late 1990s and intothe early 2000s, the LinGO project at CSLI featured weekly project meetings led byFlickinger, who would bring phenomena in need of analysis for discussion by the group,including Sag and Wasow as well as Ann Copestake, Rob Malouf, Stanford linguisticsgraduate students, and visitors to CSLI.In these always lively discussions, Sag could be counted on to share his encyclo-pedic knowledge of theoretical literature pertaining to the phenomenon in questionand key examples that had been identified and analyzed in that literature, to suggestanalyses, as well as to invent on the spot further examples to illustrate differences inpredictions of competing candidate analyses.
Supporting his ability to do this was anunsurpassed command of the theory and the workings of the grammar written in it.3These meetings not only fed the development of the English Resource Grammar, but2 The initial grammar was developed by Rob Malouf, then a grad student at Stanford.
Since 1994 DanFlickinger has been its lead developer.3 GPSG and HPSG were developed on the premise that precise formalization is critical to the testing oflinguistic hypotheses: Without the formalization, it is not possible to say for certain what the predictionsof a theory are.
In fact, formalization alone isn?t enough: Grammars of the scale supported by the HPSGframework are too complex for humans to reliably do those calculations without the aid of a machine.The one possible exception to this generalization was Ivan Sag.3Computational Linguistics Volume 40, Number 1also formed an important point of contact between computational and theoretical workin HPSG, such that the ?pen and paper?
theory remained responsive to computationalconcerns.Another key result of the LinGO project during the Verbmobil days was the de-velopment of Minimal Recursion Semantics (MRS) (Copestake et al.
1995, 2005).
Sagand colleagues designed MRS to meet the competing demands of expressive adequacy,grammatical compatibility, computational tractability, and underspecifiability.
In otherwords, it is a computational semantic formalism that allows grammars like the En-glish Resource Grammar to make explicit exactly as much information about semanticpredicate argument structure and quantifier and operator scope as is determined bysentence structure, leaving further ambiguity represented via underspecification ratherthan enumeration of, for example, the full set of possible quantifier scopings for eachitem.The experience of the Verbmobil project highlighted the value of collaborativework on natural language processing between grammarians and those working onthe software required to develop and deploy grammars in practical applications.
Inthe late 1990s, Sag and others, notably Dan Flickinger, Stephan Oepen, Jun?ichi Tsu-jii, and Hans Uszkoreit, began exploring ways to continue the collaboration past theend of the Verbmobil project.
The result was the DELPH-IN consortium,4 which hascontinued to facilitate further exchange of ideas and joint development of softwareand grammars.
As of 2013, DELPH-IN spans four continents and has developed open-source grammars (including some large scale ones) for many languages, all of whichare interoperable with the same set of open-source processing (analysis and generation)tools.A hallmark of Sag?s work has been his openness to collaboration across frame-works, subfields, and disciplines.
In addition to the interactions he facilitatedbetween linguistics and computational linguistics, Sag also built bridges betweenformal linguistics and psycholinguistics.
Towards the end of his career, togetherwith graduate students at Stanford, Sag turned his attention to developing a clearerunderstanding of the roles of processing and grammar in determining (un)acceptability.Specifically, he argued that the unacceptability associated with phenomena such as?island constraints?
derives not from ungrammaticality but in fact from more generalprocessing pressures (Staum and Sag 2008; Hofmeister and Sag 2010; Staum Casasanto,Hofmeister, and Sag 2010; Hofmeister et al.
2013).
More generally, Sag?s workon grammatical theory has highlighted the importance of ?performance-plausiblecompetence grammar,?
namely, models of linguistic knowledge that can in fact beembedded in models of human language processing (Sag 1992; Sag and Fodor 1994;Sag, Wasow, and Bender 2003; Sag and Wasow 2011).
A key feature of HPSG that makesit ?performance plausible?
is representation of grammatical information in a formthat allows it to be combined flexibly with other grammatical information as well asextragrammatical information relevant to processing (e.g., general world knowledge,or information about the present situation perceived by the hearer) as that informationbecomes available.
This form of grammatical description has been very influentialin computational work on incremental processing models which explicitly accountfor the generation of upcoming grammatical structure (Jurafsky 1996; Hale 2001;Levy 2008).4 Deep Linguistic Processing in HPSG Initiative; http://www.delph-in.net.4Bender ObituaryWhere many saw opposition between stochastic and symbolic methods, Sag sawopportunity for hybridization, as early as 1993.
When asked by Ta!
magazine (in thesame interview cited above) about connectionism, he replied:If you think of grammar as a transformational grammar, and you look at theconnectionist models that have evolved, then it is like looking at apples and oranges.However, if you look at a theory of typed feature structures and a connectionist model,it is more like apples and applesauce....I am not just interested in computer natural language processing.
I am interestedin understanding how in the world communication is possible at all.
To me, it isastonishing that the huge space of ambiguity and uncertain information that languagepresents somehow gives rise to accurate and efficient communication.
What we docognitively, in language processing, is nothing short of miraculous.
We bring togetherknowledge of the language, knowledge of the world, knowledge of the subject matter,knowledge of the situation, in such a way as to never even see the landscape of whatthe linguistic possibilities are....Now is not the time to stop exploring discrete methodologies, but rather to lookfor hybrid methodologies that exploit the complementary strengths of discrete andstatistical methodologies.
That is the only way that language technology will everdevelop to play the role it must in the technology of tomorrow.Finally, no appreciation of Sag?s career and its impact on the fields of computationallinguistics and especially linguistics would be complete without remarking on theenergy and verve he brought to the social aspects of doing science: He had a keenunderstanding of the importance of community and communication in the doing ofscience, and worked tirelessly to promote both.
He organized the first InternationalConference on Head-Driven Phrase Structure Grammar in Columbus, Ohio, in 1993 inorder to provide a forum for HPSG researchers scattered across the globe to exchangeideas; the 20th iteration of that conference was held in Berlin in 2013.
But his notionof community extended beyond staid academic discourse: He also started the bandDead Tongues with Geoffrey Nunberg in the early 1980s, and played keyboards inevery iteration of that band (with ever-changing, but always linguistically motivated,membership) through the roof-raising performance at Ivan Fest at CSLI in April 2013.He also organized the rental of sorority houses and the hiring of gourmet chefs forshared housing at every Linguistic Institute he attended.
These opportunities for socialengagement helped build the community, not just of HPSG researchers, but of linguistsmore generally, which in turn supports the lively exchange of ideas on which Sagthrived and to which he contributed so much.ReferencesBoas, Hans Christian and Ivan A Sag.
2012.Sign-Based Construction Grammar.
CSLIPublications, Stanford, CA.Bresnan, Joan and Ronald M. Kaplan.1982.
Lexical-Functional Grammar:A formal system for grammaticalrepresentation.
In Joan Bresnan,editor, The Mental Representation ofGrammatical Relations, pp.
29?130,MIT Press, Cambridge, MA.Bresnan, Joan, Ronald M. Kaplan, StanleyPeters, and Annie Zaenen.
1982.Cross-serial dependencies in Dutch.Linguistic Inquiry, 13(4):613?635.Callmeier, Ulrich.
2002.
Preprocessing andencoding techniques in PET.
In StephanOepen, Daniel Flickinger, J. Tsujii, and HansUszkoreit, editors, Collaborative LanguageEngineering.
A Case Study in EfficientGrammar-based Processing, pp.
127?140,CSLI Publications, Stanford, CA.Carpenter, Bob and Gerald Penn.
1994.ALE: The attribute logic engine user?sguide, version 2.0.1.
Carnegie MellonUniversity, Department of Philosophy,Paper 526.Chomsky, Noam.
1981.
Lectures onGovernment and Binding.
Foris Publications,Dordrecht, Holland.5Computational Linguistics Volume 40, Number 1Copestake, Ann.
2002.
Implementing TypedFeature Structure Grammars.
CSLIPublications, Stanford, CA.Copestake, Ann, Dan Flickinger, Rob Malouf,Susanne Riehemann, and Ivan Sag.
1995.Translation using minimal recursionsemantics.
In Proceedings of the SixthInternational Conference on Theoretical andMethodological Issues in Machine Translation,pp.
15?32, Leuven.Copestake, Ann, Dan Flickinger, CarlPollard, and Ivan A.
Sag.
2005.
Minimalrecursion semantics: An introduction.Research on Language & Computation,3(4):281?332.Crysmann, Berthold and Woodley Packard.2012.
Towards efficient HPSG generationfor German, a non-configurationallanguage.
In Proceedings of COLING 2012,pages 695?710, Mumbai.Culy, Christopher.
1985.
The complexity ofthe vocabulary of bambara.
Linguistics andPhilosophy, 8(3):345?351.Flickinger, Dan.
2000.
On building a moreefficient grammar by exploiting types.Natural Language Engineering, 6 (1)(Special Issue on Efficient Processing withHPSG):15 ?
28.Flickinger, Dan.
2011.
Accuracy v. robustnessin grammar engineering.
In Emily M.Bender and Jennifer E. Arnold, editors,Language from a Cognitive Perspective:Grammar, Usage and Processing.
CSLIPublications, Stanford, CA, pages 31?50.Gawron, Jean Mark, Jonathan King, JohnLamping, Egon Loebner, E. Anne Paulson,Geoffrey K. Pullum, Ivan A.
Sag, andThomas Wasow.
1982.
Processing Englishwith a Generalized Phrase StructureGrammar.
In Proceedings of the 20th AnnualMeeting of the Association for ComputationalLinguistics, pages 74?81, Toronto.Gazdar, Gerald, Ewan Klein, GeoffreyPullum, and Ivan Sag.
1985.
GeneralizedPhrase Structure Grammar.
HarvardUniversity Press, Cambridge, MA.Hale, John.
2001.
A probabilistic Earleyparser as a psycholinguistic model.In Proceedings of the Second Meeting of theNorth American Chapter of the Associationfor Computational Linguistics on LanguageTechnologies, pages 1?8, Pittsburgh, PA.Hofmeister, Philip, T Florian Jaeger, InbalArnon, Ivan A.
Sag, and Neal Snider.2013.
The source ambiguity problem:Distinguishing the effects of grammar andprocessing on acceptability judgments.Language and Cognitive Processes,28(1-2):48?87.Hofmeister, Philip and Ivan A.
Sag.
2010.Cognitive constraints and island effects.Language, 86(2):366?415.Hudson, Richard.
1984.
Word Grammar.Blackwell, Oxford.Jurafsky, Daniel.
1996.
A probabilistic modelof lexical and syntactic access anddisambiguation.
Cognitive Science,20:137?194.Levy, Roger.
2008.
Expectation-basedsyntactic comprehension.
Cognition,106(3):1126?1177.Makino, Takaki, Minoru Yoshida, KentaroTorisawa, and J. Tsujii.
1998.
LiLFeS ?towards a practical HPSG parser.
InProceedings of the 17th InternationalConference on Computational Linguistics andthe 36th Annual Meeting of the Association forComputational Linguistics, pages 807?811,Montreal.Oepen, Stephan, Erik Velldal, Jan ToreL?nning, Paul Meurer, Victoria Rose?n,and Dan Flickinger.
2007.
Towards hybridquality-oriented machine translation.
Onlinguistics and probabilities in MT.
In the11th International Conference on Theoreticaland Methodological Issues in MachineTranslation (TMI-07), pages 144?153,Sko?vde.Penn, Gerald.
2004.
Balancing clarity andefficiency in typed feature logic throughdelaying.
In Proceedings of the 42nd Meetingof the Association for ComputationalLinguistics (ACL?04), Main Volume,pages 239?246, Barcelona.Pollard, Carl and Ivan A.
Sag.
1987.Information-Based Syntax and Semantics.Volume 1: Fundamentals.
CSLI LectureNotes # 13.
Center for the Study ofLanguage and Information, Chicago,IL and Stanford, CA.
Distributed bythe University of Chicago Press.Pollard, Carl and Ivan A.
Sag.
1994.Head-Driven Phrase Structure Grammar.Studies in Contemporary Linguistics.The University of Chicago Press andCSLI Publications, Chicago, IL, andStanford, CA.Pullum, Geoffrey K. and Gerald Gazdar.1982.
Natural languages and context-freelanguages.
Linguistics and Philosophy,4:471?504.Sag, Ivan A.
1992.
Taking performanceseriously.
VII Congresso de LanguajesNaturales y Lenguajes Formales,pages 61?74.Sag, Ivan A.
1993.
Interview withAnne-Marie Mineur and Gerrit Rentier.Ta!, 2(2).6Bender ObituarySag, Ivan A. and Janet D. Fodor.
1994.Extraction without traces.
In West CoastConference on Formal Linguistics,volume 13, pages 365?384,Los Angeles, CA.Sag, Ivan A. and Thomas Wasow.
2011.Performance-compatible competencegrammar.
Non-transformational Syntax:Formal and Explicit Models of Grammar,pages 359?377.Sag, Ivan A., Thomas Wasow, and Emily M.Bender.
2003.
Syntactic Theory: A FormalIntroduction.
CSLI, Stanford, CA,second edition.Shieber, Stuart M. 1985.
Evidence against thecontext-freeness of natural language.Linguistics and Philosophy, 8(3):333?343.Slayden, Glenn C. 2012.
Array TFS storagefor unification grammars.
Master?s thesis,University of Washington.Staum, Laura and Ivan A.
Sag.
2008.The advantage of the ungrammatical.In Proceedings of the 30th Annual Meeting ofthe Cognitive Science Society, pages 601?606,Washington, DC.Staum Casasanto, Laura, Philip Hofmeister,and Ivan A.
Sag.
2010.
Understandingacceptability judgments: Distinguishingthe effects of grammar and processing onacceptability judgments.
In Proceedingsof the 32nd Annual Conference of theCognitive Science Society, pages 224?229,Portland, OR.Suppes, P., D. Flickinger, B. Macken, J. Cook,and T. Liang.
2012.
Description of theEPGY Stanford University online coursesfor mathematics and language arts.In International Society for Technology inEducation (ISTE) Annual 2012 Conference,pages 1?9, San Diego, CA.Uszkoreit, Hans, Rolf Backofen, StephanBusemann, Abdel Kader Diagne,Elizabeth A. Hinkelman, Walter Kasper,Bernd Kiefer, Hans-Ulrich Krieger, KlausNetter, Gu?nter Neumann, Stephan Oepen,and Stephen P. Spackman.
1994.
DISCO?an HPSG-based NLP system and itsapplication for appointment scheduling.In Proceedings of the 15th InternationalConference on Computational Linguistics,pages 436?440, Kyoto.Wahlster, Wolfgang, editor.
2000.
Verbmobil.Foundations of Speech-to-Speech Translation.Springer, Berlin, Germany.Wood, Mary McGee.
1993.
CategorialGrammars.
Routledge.7
