Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 83?88,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLanguage of Vandalism:Improving Wikipedia Vandalism Detection via Stylometric AnalysisManoj Harpalani, Michael Hart, Sandesh Singh, Rob Johnson, and Yejin ChoiDepartment of Computer ScienceStony Brook UniversityNY 11794, USA{mharpalani, mhart, sssingh, rob, ychoi}@cs.stonybrook.eduAbstractCommunity-based knowledge forums, such asWikipedia, are susceptible to vandalism, i.e.,ill-intentioned contributions that are detrimen-tal to the quality of collective intelligence.Most previous work to date relies on shallowlexico-syntactic patterns and metadata to au-tomatically detect vandalism in Wikipedia.
Inthis paper, we explore more linguistically mo-tivated approaches to vandalism detection.
Inparticular, we hypothesize that textual vandal-ism constitutes a unique genre where a groupof people share a similar linguistic behav-ior.
Experimental results suggest that (1) sta-tistical models give evidence to unique lan-guage styles in vandalism, and that (2) deepsyntactic patterns based on probabilistic con-text free grammars (PCFG) discriminate van-dalism more effectively than shallow lexico-syntactic patterns based on n-grams.1 IntroductionWikipedia, the ?free encyclopedia?
(Wikipedia,2011), ranks among the top 200 most visited web-sites worldwide (Alexa, 2011).
This editable ency-clopedia has amassed over 15 million articles acrosshundreds of languages.
The English language en-cyclopedia alone has over 3.5 million articles andreceives over 1.25 million edits (and sometimes up-wards of 3 million) daily (Wikipedia, 2010).
Butallowing anonymous edits is a double-edged sword;nearly 7% (Potthast, 2010) of edits are vandalism,i.e.
revisions to articles that undermine the qualityand veracity of the content.
As Wikipedia contin-ues to grow, it will become increasingly infeasiblefor Wikipedia users and administrators to manuallypolice articles.
This pressing issue has spawned re-cent research activities to understand and counteractvandalism (e.g., Geiger and Ribes (2010)).
Much ofprevious work relies on hand-picked rules such aslexical cues (e.g., vulgar words) and metadata (e.g.,anonymity, edit frequency) to automatically detectvandalism in Wikipedia (e.g., Potthast et al (2008),West et al (2010)).
Although some recent workhas started exploring the use of natural languageprocessing, most work to date is based on shallowlexico-syntactic patterns (e.g., Wang and McKeown(2010), Chin et al (2010), Adler et al (2011)).We explore more linguistically motivated ap-proaches to detect vandalism in this paper.
Ourhypothesis is that textual vandalism constitutes aunique genre where a group of people share simi-lar linguistic behavior.
Some obvious hallmarks ofthis style include usage of obscenities, misspellings,and slang usage, but we aim to automatically un-cover stylistic cues to effectively discriminate be-tween vandalizing and normal text.
Experimental re-sults suggest that (1) statistical models give evidenceto unique language styles in vandalism, and that (2)deep syntactic patterns based on probabilistic con-text free grammar (PCFG) discriminate vandalismmore effectively than shallow lexico-syntactic pat-terns based on n-grams.2 Stylometric FeaturesStylometric features attempt to recognize patternsof style in text.
These techniques have been tra-ditionally applied to attribute authorship (Argamonet al (2009), Stamatatos (2009)), opinion mining83(Panicheva et al, 2010), and forensic linguistics(Turell, 2010).
For our purposes, we hypothesizethat different stylistic features appear in regular andvandalizing edits.
For regular edits, honest editorswill strive to follow the stylistic guidelines set forthby Wikipedia (e.g.
objectivity, neutrality and factu-ality).
For edits that vandalize articles, these usersmay converge on common ways of vandalizing arti-cles.2.1 Language ModelsTo differentiate between the styles of normal usersand vandalizers, we employ language models to cap-ture the stylistic differences between authentic andvandalizing revisions.
We train two trigram lan-guage model (LM) with Good-Turing discountingand Katz backoff for smoothing of vandalizing ed-its (based on the text difference between the vandal-izing and previous revision) and good edits (basedon the text difference between the new and previousrevision).2.2 Probabilistic Context Free Grammar(PCFG) ModelsProbabilistic context-free grammars (PCFG) capturedeep syntactic regularities beyond shallow lexico-syntactic patterns.
Raghavan et al (2010) reportedfor the first time that PCFG models are effective inlearning stylometric signature of authorship at deepsyntactic levels.
In this work, we explore the use ofPCFG models for vandalism detection, by viewingthe task as a genre detection problem, where a groupof authors share similar linguistic behavior.
We givea concise description of the use of PCFG models be-low, referring to Raghavan et al (2010) for more de-tails.
(1) Given a training corpus D for vandalism de-tection and a generic PCFG parser Co trainedon a manually tree-banked corpus such as WSJor Brown, tree-bank each training documentdi ?
D using the generic PCFG parser Co.(2) Learn vandalism language by training a newPCFG parser Cvandal using only those tree-banked documents in D that correspond to van-dalism.
Likewise, learn regular Wikipedia lan-guage by training a new PCFG parser Cregularusing only those tree-banked documents in Dthat correspond to regular Wikipedia edits.
(3) For each test document, compare the proba-bility of the edit determined by Cvandal andCregular, where the parser with the higher scoredetermines the class of the edit.We use the PCFG implementation of Klein andManning (2003).3 System DescriptionOur system decides if an edit to an article is vandal-ism by training a classifier based on a set of featuresderived from many different aspects of the edit.
Forthis task, we use an annotated corpus (Potthast etal., 2010) of Wikipedia edits where revisions are la-beled as either vandalizing or non-vandalizing.
Thissection will describe in brief the features used byour classifier, a more exhaustive description of ournon-linguistically motivated features can be foundin Harpalani et al (2010).3.1 Features Based on MetadataOur classifier takes into account metadata generatedby the revision.
We generate features based on au-thor reputation by recording if the edit is submittedby an anonymous user or a registered user.
If the au-thor is registered, we record how long he has beenregistered, how many times he has previously van-dalized Wikipedia, and how frequent he edits arti-cles.
We also take into account the comment left byan author.
We generate features based on the charac-teristics of the articles revision history.
This includeshow many times the article has been previously van-dalized, the last time it was edited, how many timesit has been reverted and other related features.3.2 Features Based on Lexical CuesOur classifier also employs a subset of features thatrely on lexical cues.
Simple strategies such as count-ing the number of vulgarities present in the revisionare effective to capture obvious forms of vandalism.We measure the edit distance between the old andnew revision, the number of repeated patterns, slangwords, vulgarities and pronouns, the type of edit (in-sert, modification or delete) and other similar fea-tures.84Features P R F1 AUCBaseline 72.8 41.1 52.6 91.6+LM 73.3 42.1 53.5 91.7+PCFG 73.5 47.7 57.9 92.9+LM+PCFG 73.2 47.3 57.5 93.0Table 1: Results on naturally unbalanced test data3.3 Features Based on SentimentWikipedia editors strive to maintain a neutral andobjective voice in articles.
Vandals, however, in-sert subjective and polar statements into articles.
Webuild two classifiers based on the work of Pang andLee (2004) to measure the polarity and objectivityof article edits.
We train the classifier on how manypositive and negative sentences were inserted as wellas the overall change in the sentiment score from theprevious version to the new revision and the num-ber of inserted or deleted subjective sentences in therevision.3.4 Features Based on Stylometric MeasuresWe encode the output of the LM and PCFG in thefollowing manner for training our classifier.
Wetake the log-likelihood of the regular edit and van-dalizing edit LMs.
For our PCFG, we take the dif-ference between the minimum log-likelihood score(i.e.
the sentences with the minimum log-likelihood)of Cvandal and Cregular, the difference in the max-imum log-likelihood score, the difference in themean log-likelihood score, the difference in thestandard deviation of the mean log-likelihood scoreand the difference in the sum of the log-likelihoodscores.3.5 Choice of ClassifierWe use Weka?s (Hall et al, 2009) implementationof LogitBoost (Friedman et al, 2000) to perform theclassification task.
We use Decision Stumps (Ai andLangley, 1992) as the base learner and run Logit-Boost for 500 iterations.
We also discretize the train-ing data using the Multi-Level Discretization tech-nique (Perner and Trautzsch, 1998).4 Experimental ResultsData We use the 2010 PAN Wikipedia vandalismcorpus Potthast et al (2010) to quantify the ben-Feature ScoreTotal number of author contributions 0.106How long the author has been registered 0.098How frequently the author contributedin the training set 0.097If the author is registered 0.0885Difference in the maximum PCFG scores 0.0437Difference in the mean PCFG scores 0.0377How many times the article has been reverted 0.0372Total contributions of author to Wikipedia 0.0343Previous vandalism count of the article 0.0325Difference in the sum of PCFG scores 0.0320Table 2: Top 10 ranked features on the unbalanced testdata by InfoGainefit of stylometric analysis to vandalism detection.This corpus comprises of 32452 edits on 28468 ar-ticles, with 2391 of the edits identified as vandal-ism by human annotators.
The class distribution ishighly skewed, as only 7% of edits corresponds tovandalism.
Among the different types of vandalism(e.g.
deletions, template changes), we focus only onthose edits that inserted or modified text (17145 ed-its in total) since stylometric features are not relevantto deletes and template modifications.
Note that in-sertions and modifications are the main source forvandalism.We randomly separated 15000 edits for trainingof Cvandal and Cregular, and 17444 edits for testing,preserving the ratio of vandalism to non-vandalismrevisions.
We eliminated 7359 of the testing ed-its to remove revisions that were exclusively tem-plate modifications (e.g.
inserting a link) and main-tain the observed ratio of vandalism for a total of10085 edits.
For each edit in the test set, we com-pute the probability of each modified sentence forCvandal and Cregular and generate the statistics forthe features described in 3.4.
We compare the per-formance of the language models and stylometricfeatures against a baseline classifier that is trainedon metadata, lexical and sentiment features using 10fold stratified cross validation on the test set.Results Table 1 shows the experimental results.Because our dataset is highly skewed (97% corre-sponds to ?not vandalism?
), we report F-score and85One day rodrigo was in the school and he saw agirl and she love her now and they are happy to-getherSo listen Im going to attack ur family with mightypowers.He?s also the best granddaddy ever.Beatrice Rosen (born 29 November 1985 (Happybirthday)), also known as Batrice Rosen or Ba-trice Rosenblatt, is a French-born actress.
She isbest known for her role as Faith in the second sea-son of the TV series ?Cuts?.Table 3: Examples of vandalism detected by base-line+PCFG features.
Baseline features alone could notdetect these vandalism.
Notice that several stylistic fea-tures present in these sentences are unlikely to appear innormal Wikipedia articles.AUC rather than accuracy.1 The baseline system,which includes a wide range of features that areshown to be highly effective in vandalism detection,achieves F-score 52.6%, and AUC 91.6%.
The base-line features include all features introduced in Sec-tion 3.Adding language model features to the baseline(denoted as +LM in Table 1) increases the F-scoreslightly (53.5%), while the AUC score is almostthe same (91.7%).
Adding PCFG based features tothe baseline (denoted as +PCFG) brings the mostsubstantial performance improvement: it increasesrecall substantially while also improving precision,achieving 57.9% F-score and 92.9% AUC.
Combin-ing both PCFG and language model based features(denoted as +LM+PCFG) only results in a slightimprovement in AUC.
From these results, we drawthe following conclusions:?
There are indeed unique language styles in van-dalism that can be detected with stylometricanalysis.?
Rather unexpectedly, deep syntax oriented fea-tures based on PCFG bring a much more sub-stantial improvement than language modelsthat capture only shallow lexico-syntactic pat-terns.1A naive rule that always chooses the majority class (?notvandalism?)
will receive zero F-score.All those partaking in the event get absolutely?fritzeld?
and certain attendees have even beenknown to soil themselvesMarch 10,1876 Alexander Grahm Ball dscoveredth telephone when axcidently spilt battery juice onhis expeiriment.English remains the most widely spoken languageand New York is the largest city in the Englishspeaking world.
Although massive pockets inQueens and Brooklyn have 20% or less peoplewho speak English not so good.Table 4: Examples of vandalism that evaded both ourbaseline and baseline+PCFG classifier.
Dry wit, forexample, relies on context and may receive a goodscore from the parser trained on regular Wikipedia edits(Cregular).Feature Analysis Table 2 lists the informationgain ranking of our features.
Notice that several ofour PCFG features are in the top ten most informa-tive features.
Language model based features wereranked very low in the list, hence we do not includethem in the list.
This finding will be potentially ad-vantageous to many of the current anti-vandalismtools such as vulgarisms, which rely only on shal-low lexico-syntactic patterns.Examples To provide more insight to the task, Ta-ble 3 shows several instances where the additionof the PCFG derived features detected vandalismthat the baseline approach could not.
Notice thatthe first example contains a lot of conjunctions thatwould be hard to characterize using shallow lexico-syntactic features.
The second and third examplesalso show sentence structure that are more informaland vandalism-like.
The fourth example is one thatis harder to catch.
It looks almost like a benign edit,however, what makes it a vandalism is the phrase?
(Happy Birthday)?
inserted in the middle.Table 4 shows examples where all of our systemscould not detect the vandalism correctly.
Notice thatexamples in Table 4 generally manifest more a for-mal voice than those in Table 3.5 Related WorkWang and McKeown (2010) present the first ap-proach that is linguistically motivated.
Their ap-86proach was based on shallow syntactic patterns,while ours explores the use of deep syntactic pat-terns, and performs a comparative evaluation acrossdifferent stylometry analysis techniques.
It is worth-while to note that the approach of Wang and McKe-own (2010) is not as practical and scalable as ours inthat it requires crawling a substantial number (150)of webpages to detect each vandalism edit.
Fromour pilot study based on 1600 edits (50% of whichis vandalism), we found that the topic-specific lan-guage models built from web search do not producestronger result than PCFG based features.
We donot have a result directly comparable to theirs how-ever, as we could not crawl the necessary webpagesrequired to match the size of corpus.The standard approach to Wikipedia vandalismdetection is to develop a feature based on either thecontent or metadata and train a classifier to recog-nize it.
A comprehensive overview of what typesof features have been employed for this task can befound in Potthast et al (2010).
WikiTrust, a repu-tation system for Wikipedia authors, focuses on de-termining the likely quality of a contribution (Adlerand de Alfaro, 2007).6 Future Work and ConclusionThis paper presents a vandalism detection system forWikipedia that uses stylometric features to aide inclassification.
We show that deep syntactic patternsbased on PCFGs more effectively identify vandal-ism than shallow lexico-syntactic patterns based onn-grams or contextual language models.
PCFGs donot require the laborious process of performing websearches to build context language models.
Rather,PCFGs are able to detect differences in languagestyles between vandalizing edits and normal edits toWikipedia articles.
Employing stylometric featuresincreases the baseline classification rate.We are currently working to improve this tech-nique through more effective training of our PCFGparser.
We look to automate the expansion of thetraining set of vandalized revisions to include exam-ples from outside of Wikipedia that reflect similarlanguage styles.
We also are investigating how wecan better utilize the output of our PCFG parsers forclassification.7 AcknowledgmentsWe express our most sincere gratitude to Dr. TamaraBerg and Dr. Luis Ortiz for their valuable guid-ance and suggestions in applying Machine Learningand Natural Language Processing techniques to thetask of vandalism detection.
We also recognize thehard work of Megha Bassi and Thanadit Phumpraofor assisting us in building our vandalism detectionpipeline that enabled us to perform these experi-ments.ReferencesB.
Thomas Adler and Luca de Alfaro.
2007.
A content-driven reputation system for the wikipedia.
In Pro-ceedings of the 16th international conference on WorldWide Web, WWW ?07, pages 261?270, New York, NY,USA.
ACM.B.
Thomas Adler, Luca de Alfaro, Santiago M. Mola-Velasco, Paolo Rosso, and Andrew G. West.
2011.Wikipedia vandalism detection: Combining naturallanguage, metadata, and reputation features.
In CI-CLing ?11: Proceedings of the 12th International Con-ference on Intelligent Text Processing and Computa-tional Linguistics.Wayne Iba Ai and Pat Langley.
1992.
Induction of one-level decision trees.
In Proceedings of the Ninth In-ternational Conference on Machine Learning, pages233?240.
Morgan Kaufmann.Alexa.
2011.
Top 500 sites (retrieved April 2011).http://www.alexa.com/topsites.Shlomo Argamon, Moshe Koppel, James W. Pennebaker,and Jonathan Schler.
2009.
Automatically profilingthe author of an anonymous text.
Commun.
ACM,52:119?123, February.Si-Chi Chin, W. Nick Street, Padmini Srinivasan, andDavid Eichmann.
2010.
Detecting wikipedia vandal-ism with active learning and statistical language mod-els.
In WICOW ?10: Proceedings of the 4rd Workshopon Information Credibility on the Web.J.
Friedman, T. Hastie, and R. Tibshirani.
2000.
AdditiveLogistic Regression: a Statistical View of Boosting.The Annals of Statistics, 38(2).R.
Stuart Geiger and David Ribes.
2010.
The work ofsustaining order in wikipedia: the banning of a vandal.In Proceedings of the 2010 ACM conference on Com-puter supported cooperative work, CSCW ?10, pages117?126, New York, NY, USA.
ACM.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18.87Manoj Harpalani, Thanadit Phumprao, Megha Bassi,Michael Hart, and Rob Johnson.
2010.
Wikivandalysis- wikipedia vandalism analysis lab reportfor pan at clef 2010.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics, pages 423?430.
Association for Computa-tional Linguistics.Bo Pang and Lillian Lee.
2004.
A sentimental edu-cation: sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, ACL ?04, Stroudsburg, PA, USA.Association for Computational Linguistics.Polina Panicheva, John Cardiff, and Paolo Rosso.
2010.Personal sense and idiolect: Combining authorship at-tribution and opinion analysis.
In Nicoletta Calzo-lari (Conference Chair), Khalid Choukri, Bente Mae-gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,Mike Rosner, and Daniel Tapias, editors, Proceed-ings of the Seventh conference on International Lan-guage Resources and Evaluation (LREC?10), Valletta,Malta, may.
European Language Resources Associa-tion (ELRA).Petra Perner and Sascha Trautzsch.
1998.
Multi-intervaldiscretization methods for decision tree learning.
InIn: Advances in Pattern Recognition, Joint IAPR In-ternational Workshops SSPR 98 and SPR 98, pages475?482.Martin Potthast, Benno Stein, and Robert Gerling.
2008.Automatic vandalism detection in wikipedia.
InECIR?08: Proceedings of the IR research, 30th Euro-pean conference on Advances in information retrieval,pages 663?668, Berlin, Heidelberg.
Springer-Verlag.Martin Potthast, Benno Stein, and Teresa Holfeld.
2010.Overview of the 1st International Competition onWikipedia Vandalism Detection.
In Martin Braschlerand Donna Harman, editors, Notebook Papers ofCLEF 2010 LABs and Workshops, 22-23 September,Padua, Italy, September.Martin Potthast.
2010.
Crowdsourcing a wikipedia van-dalism corpus.
In SIGIR?10, pages 789?790.Sindhu Raghavan, Adriana Kovashka, and RaymondMooney.
2010.
Authorship attribution using proba-bilistic context-free grammars.
In Proceedings of theACL, pages 38?42, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Efstathios Stamatatos.
2009.
A survey of modern author-ship attribution methods.
J.
Am.
Soc.
Inf.
Sci.
Technol.,60:538?556, March.M.
Teresa Turell.
2010.
The use of textual, grammaticaland sociolinguistic evidence in forensic text compari-son:.
International Journal of Speech Language andthe Law, 17(2).William Yang Wang and Kathleen R. McKeown.2010.
?got you!?
: Automatic vandalism detec-tion in wikipedia with web-based shallow syntactic-semantic modeling.
In 23rd International Conferenceon Computational Linguistics (Coling 2010), page1146?1154.Andrew G. West, Sampath Kannan, and Insup Lee.
2010.Detecting wikipedia vandalism via spatio-temporalanalysis of revision metadata.
In EUROSEC ?10: Pro-ceedings of the Third European Workshop on SystemSecurity, pages 22?28, New York, NY, USA.
ACM.Wikipedia.
2010.
Daily edit statistics.http://stats.wikimedia.org/EN/PlotsPngDatabaseEdits.htm.Wikipedia.
2011.
Wikipedia.
http://www.wikipedia.org.88
