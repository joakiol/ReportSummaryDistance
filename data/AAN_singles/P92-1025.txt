MODEL ING NEGOTIAT ION SUBDIALOGUES 1Lynn Lamber t  and Sandra Carber ryDepar tment  of Computer  and Informat ion SciencesUnivers i ty  of DelawareNewark,  Delaware 19716, USAemail : lambert~cis, udel.
edu, carberry@cis, udel.
eduAbst ractThis paper presents a plan-based model that han-dles negotiation subdialogues by inferring both thecommunicative actions that people pursue whenspeaking and the beliefs underlying these actions.We contend that recognizing the complex dis-course actions pursued in negotiation subdialogues(e.g., expressing doubt) requires both a multi-strength belief model and a process model thatcombines different knowledge sources in a unifiedframework.
We show how our model identifies thestructure of negotiation subdialogues, includingrecognizing expressions of doubt, implicit accep-tance of communicated propositions, and negotia-tion subdialogues mbedded within other negotia-tion subdialogues.1 In t roduct ionSince negotiation is an integral part ofmulti-agent activity, a robust natural anguage un-derstanding system must be able to handle subdi-alogues in which participants negotiate what hasbeen claimed in order to try to come to someagreement about those claims.
To handle suchdialogues, the system must be able to recognizewhen a dialogue participant has initiated a nego-tiation subdialogue and why the participant beganthe negotiation (i.e., what beliefs led the partici-pant to start the negotiation).
This paper presentsa plan-based model of task-oriented interactionsthat assimilates negotiation subdialogues by in-ferring both the communicative actions that peo-ple pursue when speaking and the beliefs under-lying these actions.
We will argue that recogniz-ing the complex discourse actions pursued in ne-gotiation subdialogues (e.g., expressing doubt) re-quires both a multi-strength belief model and aprocessing strategy that combines different knowl-edge sources in a unified framework, and we willshow how our model incorporates these and rec-ognizes the structure of negotiation subdialogues.2 P rev ious  WorkSeveral researchers have built argument un-derstanding systems, but none of these has ad-dressed participants coming to an agreement ormutual belief about a particular situation, ei-ther because the arguments were only monologues1 This work is being supported by the National ScienceFoundation under Grant No.
IRI-9122026.
The Govern-ment has certain rights in this material.
(Cohen, 1987; Cohen and Young, 1991), or be-cause they assumed that dialogue participants donot change their minds (Flowers, McGuire andBirnbaum, 1982; Quilici, 1991).
Others have ex-amined more cooperative dialogues.
Clark andSchaefer (1989) contend that utterances must begrounded, or understood, by both parties, but theydo not address conflicts in belief, only lack of un-derstanding.
Walker (1991) has shown that evi-dence is often provided to ensure both understand-ing and believing an utterance, but she does notaddress recognizing lack of belief or lack of under-standing.
Reichman (1981) outlines a model forinformal debate, but does not provide a detailedcomputational mechanism for recognizing the roleof each utterance in a debate.In previous work (Lambert and Carberry,1991), we described a tripartite plan-based modelof dialogue that recognizes and differentiates threedifferent kinds of actions: domain, problem-solving, and discourse.
Domain actions relate toperforming tasks in a given domain.
We are mod-eling cooperative dialogues in which one agenthas a domain goal and is working with anotherhelpful, more expert agent to determine what do-main actions to perform in order to accomplishthis goal.
Many researchers (Allen, 1979; Car-berry, 1987; Goodman and Litman, 1992; Pol-lack, 1990; Sidner, 1985) have shown that recog-nition of domain plans and goals gives a systemthe ability to address many difficult problems inunderstanding.
Problem-solving actions relate tohow the two dialogue participants are going aboutbuilding a plan to achieve the planning agent'sdomain goal.
Ramshaw, Litman, and Wilensky(Ramshaw, 1991; Litman and Allen, 1987; Wilen-sky, 1981) have noted the need for recognizingproblem-solving actions.
Discourse actions are thecommunicative actions that people perform in say-ing something, e.g., asking a question or express-ing doubt.
Recognition of discourse actions pro-vides expectations for subsequent utterances, andexplains the purpose of an utterance and how itshould be interpreted.Our system's knowledge about how to per-form actions is contained in a library of discourse,problem-solving, and domain recipes (Pollack,1990).
Although domain recipes are not mutuallyknown by the participants (Pollack, 1990), how tocommunicate and how to solve problems are corn-193Discourse Recipe-C3:{_agent1 informs _agent~ of_prop}Action: Inform(_agentl, _agent2, _prop)Recipe-type: DecompositionApp Cond: believe(_agentl, _prop, \[C:C\])believe(_agentl, believe(_agent2, _prop, \[CN:S\]), \[0:C\])Body: Tell(_agent 1, _agent2, _prop)Address-Believability(_agent2, _agentl, _prop)Effects: believe(_agent2, want(_agentl, believe(_agent2, _prop, \[C:C\])), \[C:C\])Goal: believe(_agent2, _prop, \[C:C\])Discourse Recipe-C2:{_agent1 expresses doubt to _agent2 about _propI because _agent1 believes _prop~ to be true}Action: Express-Doubt(_agentl, _agent2, _propl, _prop2, _rule)Recipe-type: DecompositionApp Cond: believe(_agentl, _prop2, \[W:S\])believe(_agentl, believe(_agent2, _propl, \[S:C\]), \[S:C\])believe(_agentl, ((_prop2 A _rule) ::~ -,_propl), \[S:C\])believe(_agentl, _rule, \[S:C\])in-focus(_propl))Body: Convey- Uncertain- Belief(_ agent 1, _agent 2, _prop2)Address-Q-Acceptanee(_agent2, _agentl, _prop2)Effects: believe(_agent2, believe(_agentl, _propl, \[SN:W2~\]), \[S:C\])believe(_agent2, want(_agentl, Resolve-Conflict(_agent2, _agentl, _propl, _prop2)), \[S:C\])Goal: want(_agent2, Resolve-Conflict(_agent2, _agentl, _propl, _prop2))Figure 1.
Two Sample Discourse Recipesmen skills that people use in a wide variety ofcontexts, so the system can assume that knowl-edge about discourse and problem-solving recipesis shared knowledge.
Figure 1 contains two dis-course recipes.
Our representation of a recipe in-cludes a header giving the name of the recipe andthe action that it accomplishes, preconditions, ap-plicability conditions, constraints, a body, effects,and a goal.
Constraints limit the allowable instan-tiation of variables in each of the components ofa recipe (Litman and Allen, 1987).
Applicabilityconditions (Carberry, 1987) represent conditionsthat must be satisfied in order for the recipe tobe reasonable to apply in the given situation and,in the case of many of our discourse recipes, theapplicability conditions capture beliefs that the di-alogue participants must hold.
Especially in thecase of discourse recipes, the goals and effects arelikely to be different.
This allows us to differen-tiate between ilIocutionary and perlocutionary ef-fects and to capture the notion that one can, forexample, perform an inform act without the heareradopting the communicated proposition.
2As actions are inferred by our processmodel, a structure of the discourse is built which isreferred to as the Dialogue Model, or DM.
In theDM, discourse, problem-solving, and domain ac-tions are each modeled on a separate level.
Withineach of these levels, actions may contribute toother actions in the dialogue, and this is capturedwith specialization (Kautz and Allen, 1986), sub-2Consider, for example, someone saying "I in.formed youof X but you wouldn't believe me.
"action, and enablement arcs.
Thus, actions at eachlevel form a tree structure in which each node rep-resents an action that a participant is performingand the children of a node represent actions pur-sued in order to contribute to the parent action.By using a tree structure to model actions at eachlevel and by allowing the tree structures to grow atthe root as well as at the leaves, we are able to in-crementally recognize discourse, problem-solving,and domain intentions, and can recognize the re-lationship among several utterances that are allpart of the same higher-level discourse act evenwhen that act cannot be recognized from the firstutterance alone.
Other advantages of our tripar-tite model are discussed in Lambert and Carberry.
(1991).An action on one level in the DM may alsocontribute to an action on an immediately higherlevel.
For example, discourse actions may be ex-ecuted in order to obtain the information eces-sary for performing a problem-solving action andproblem-solving actions may be executed in orderto construct a domain plan.
We capture this withlinks between actions on adjacent levels of the DM.Figure 2 gives a DM built by our proto-type system whose implementation is currently be-ing expanded to include belief ascription and useof linguistic information.
It shows that a ques-tion has been asked and answered, that this ques-tion/answer pair contributes to the higher-leveldiscourse action of obtaining information aboutwhat course Dr. Smith is teaching, that this dis-course action enables the problem-solving actionOf instantiating a parameter in a Learn-Material194Domain  Leve l? "
* ' ?
?*??? '
? '
? '
? '
?
?
"????***?? '
?
;  -0-~.
= Enable Arci I .Ta~.Co~:s,.
=o,,,,=) I ,.
'~" .
.
.
.
.
.
.
.
.
.
.
.
~t .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
~ = Subact ion  ArcProb lem-So lv ln_C l  Leve l  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.~ooooo*****o****~**********ooo**~*o******o  oo  * o*?
| ?I Build-Plan(Sl, $2, Take-C0urse(S1, _course)) I |\[ ?
i t IInstamiate-Vars(Sl, 2, Learn-Matertal(S1, _course, Dr. Smith)) \[ o? '
t ' :?
T 0 ?
e?
I I 0 ,0 l* Instamiate-Single-Var(Sl, S2 _course, Learn-Material(S1, _course, Dr. Smith)) \]~ o .o to .oo********oo .o .oo .o .oo .
.
*~**oo***ooo*ooo*o~u=ooomo*moooo**oooooeoooo  -?Discourse Leve l  *!
, \] Obtain-Info-Ref(Sl, S2, course, Teaches(Dr. S..__mith, _course)) I$2, Teaches(Dr. Smith, IAnswer-Ref(S2, SI-, course, Teaches(Dr. Smith, I course.
I course), Teaches(Dr. Smith, Arch)) II RexlUest(Sl, $2, Inf0rm-Rcf(S2, ISl, _ course, Teaches(Dr. Smith, course)) II t$I Inform(S2, SI,.
Teaches(Dr. Smith, Arch))\]?\[ * Tell(S2, SI, Teaches(Dr. Smith, Arch)) J?\[ \[ Surface-WH-Quesd0n(Sl, 2, Inform-Ref I\[ \[ ($2, SI, _course, Teaches(Dr. Smith, _course)) \[ Surface-lnf0rm(S20 SI, Teaches(Dr. Smith, Arch))o ?oooo~ooooo  =ooooo  ~?oo~o~o~*o~****=* ?
*** ?
?
* * ?
?
* **  ?
o* ?
?
?
?
* ?
oooo*~ooo ?
*o*o*ooo**oo**oo ?
*** ?o*~moooo~ oo* ?
!E \[ti, iFigure 2.
Dialogue Model for two utterancesaction, and that this problem-solving action con-tributes to the problem-solving action of buildinga plan ill order to perform the domain action oftaking a course.The work described in this paper uses ourtripartite model, but addresses the recognition ofdiscourse actions and their use in the modeling ofnegotiation subdialogues.3 Discourse Actions and ImplicitAcceptanceOne of the most important aspects of as-similating dialogue is the recognition of discourseactions and the role that an utterance plays withrespect o the rest of the dialogue.
For example,in (3), if S1 believes that each course has a sin-gle instructor, then S1 is expressing doubt at theproposition conveyed in (2).
But in another con-text, (3) might simply be asking for verification.
(1) SI: What is Dr. Smith teaching?
(2) $2: Dr. Smith is teaching Architecture.
(3) SI: Isu't Dr. Browa teaching Architecture?Unless a natural language system is able to iden-tify the role that an utterance is intended to playin a dialogue, the system will not be able to gener-ate cooperative responses which address the par-ticipants' goals.In addition to recognizing discourse ac-tions, it is also necessary for a cooperative sys-tem to recognize a user's changing beliefs as thedialogue progresses.
Allen's representation of anInform speech act (Allen, 1979) assumed that alistener adopted the communicated proposition.Clearly, listeners do not adopt everything theyare told (e.g., (3) indicates that S1 does not im-mediately accept that Dr. Smith is teaching Ar-chitecture).
Perrault's persistence model of belief(Perrault, 1990) assumed that a listener adoptedthe communicated proposition unless the listenerhad conflicting beliefs.
Since Perrault's model as-sumes that people's beliefs persist, it cannot ac-count for S1 eventually accepting the propositionthat Dr. Smith is teaching Architecture.
We showin Section 6 how our model overcomes this limita-tion.Our investigation of naturally occurring di-alogues indicates that listeners are not passive par-ticipants, but instead assimilate each utteranceinto a dialogue in a multi-step acceptance phase.For statements, 3 a listener first attempts to un-derstand the utterance because if the utterance isnot understood, then nothing else about it can bedetermined.
Second, the listener determines if theutterance is consistent with the listener's beliefs;and finally, the listener determines the appropri-ateness of the utterance to the current context.Since we are assuming that people are engagedin a cooperative dialogue, a listener must indicatewhen the listener does not understand, believe, orconsider elevant a particular utterance, address-ing understandability first, then believability, thenrelevance.
We model this acceptance process byincluding acceptance actions in the body of manyof our discourse recipes.
For example, the actionsthe body of an Inform recipe (see Figure 1) are:il)n the speaker (_agentl) tells the listener (_agent2)3Questions must also be accepted and assimilated intoa dialogue, but we axe concentrating on statements here.195the proposition that the speaker wants the listenerto believe (_prop); and 2) the listener and speakeraddress believability by discussing whatever is nec-essary in order for the listener and speaker to cometo an agreement about what the speaker said.
4This second action, and the subactions executedas part of performing it, account for subdialogueswhich address the believability of the propositioncommunicated in the Inform action.
Similar ac-ceptance actions appear in other discourse recipes.The Tell action has a body containing a Surface-Inform action and an Address-Understanding ac-tion; the latter enables both participants to ensurethat the utterance has been understood.The combination of the inclusion of accep-tance actions in our discourse recipes and the or-dered manner in which people address acceptanceallows our model to recognize the implicit accep-tance of discourse actions.
For example, Figure 2presents the DM derived from utterances (1) and(2), with the current focus of attention on the dis-course level, the Tell action, marked with an aster-isk.
In attempting to assimilate (3) into this DM,the system first tries to interpret (3) as address-ing the understanding of (2) (i.e., as part of theTell action which is the current focus of attentionin Figure 2).
Since a satisfactory interpretation isnot found, the system next tries to relate (3) to theInform action in Figure 2, trying to interpret (3)as addressing the believability of (2).
The systemfinds that the best interpretation of (3) is that ofexpressing doubt at (2), thus confirming the hy-pothesis that (3) is addressing the believability of(2).
This recognition of (3) as contributing to theInform action in Figure 2 indicates that S1 hasimplicitly indicated understanding by passing upthe opportunity to address understanding in theTell action that appears in the DM and insteadmoving to a relevant higher-level discourse action,thus conveying that the Tell action has been suc-cessful.4 Recogn iz ing  Be l ie fsIn the dialogue in the preceding section, inorder for $1 to use the proposition communicatedin (3) to express doubt at the proposition conveyedin (2), $1 must believe(a) that Dr. Brown teaches Architecture;(b) that $2 believes that Dr. Smith isteaching Architecture; and(c) that Dr. Brown teaching Architecture isan indication that Dr. Smith does notteach Architecture.We capture these beliefs in the applicability condi-tions for an Express-Doubt discourse act (see Fig-ure 1).
In order for the system to recognize (3)4This  is where our  model  differs f rom Al len's and  Per-rault 's ;  we allow the l istener to adopt ,  reject, or negoti-ate the speaker 's  c laims, which might  result  in the l istenereventua l ly  adopt ing  the speakers  claims, the l istener chang-ing the mind  of the speaker,  or both  agreeing to disagree.a~s an expression of doubt, it nmst come to be-lieve that these applicability conditions are satis-fied.
The system's evidence that S1 believes (a)is provided by Sl's utterance, (3).
But (3) doesnot state that Dr. Brown teaches Architecture;instead, Sl uses a negative yes-no question to askwhether or not Dr. Brown teaches Architecture.The surface form of this utterance indicates thatS1 thinks that Dr. Brown teaches Architecturebut is not sure of it.
Thus, from the surface formof utterance (3), a listener can attribute to Sl anuncertain belief in the proposition that Dr. Brownteaches Architecture.This recognition of uncertain beliefs is animportant part of recognizing complex discourseactions such as expressing doubt.
If the systemwere limited to recognizing only lack of belief andbelief, then yes-no questions would have to be in-terpreted as conveying lack of belief about thequeried proposition, since a question in a cooper-ative consultation setting would not be felicitousif the speaker already knew the answer.
Thus itwould be impossible to attribute (a) to S1 from aquestion such as (3).
And without this belief at-tribution, it would not be possible to recognizeexpressions of doubt.
Furthermore, the systemmust be able to differentiate between expressionsof doubt and objections; since we are assumingthat people are engaged in a cooperative dialogueand communicate beliefs that they intend to berecognized, if S1 were certain of both (a) and (c),then S1 would object to (2), not simply expressdoubt at it.
In summary, the surface form of ut-terances is one way that speakers convey belief.But these surface forms convey more than just be-lief and disbelief; they convey multiple strengthsof belief, the recognition of which is necessary foridentifying whether an agent holds the requisitebeliefs for some discourse actions.We maintain a belief model for each partic-ipant which captures these multiple strengths ofbelief.
We contend that at least three strengthsof belief must be represented: certain belief (a be-lief strength of C); strong but uncertain belief, asin (3) above (a belief strength of S); and a weakbelief, as in I think that Dr. C might be an edu-cation instructor (a belief strength of W).
There-fore, our model maintains three degrees of belief,three degrees of disbelief (indicated by attachinga subscript of N, such as SN to represent strongdisbelief and WN to represent weak disbelief), andone degree indicating no belief about a proposition(a belief strength of 0).
5 Our belief model usesbelief intervals to specify the range of strengths5Others  (Walker, 1991; Gall iers, 1991) have also arguedfor mult ip le s t rengths  of belief, bas ing the s t rength  of beliefon the amount  and kind of evidence avai lable for that  be-lief.
We have not  invest igated how much evidence is neededfor an  agent  to have a part icu lar  amount  of confidence ina belief; our  work has  concentrated on recogniz ing how thes t rength  of belief is communicated  in a discourse and theimpact  that  the different belief s t rengths  have on the recog-nit ion of discourse acts.196within which an agent's beliefs are thought o fall,and our discourse recipes use belief intervals tospecify the range of strengths that an agent's be-liefs may assume.
Intervals uch as \[bi:bj\] spec-ify a strength of belief within bi and bj, inclu-sive.
For example, the goal of the Inform recipein Figure 1, (believe(..agent2, _prop, \[C:C\])),is that _agentl be certain that _prop is true; on theother hand, believe(_agentl, _prop, \[W:C\]),means that _agent I must have some belief in _prop.In order to recognize other beliefs, such as(b) and (c), it is necessary to use more informa-tion than just a speaker's utterances.
For exam-ple, $2 might attribute (c) to $1 because $2 be-lieves that most people think that only one pro-fessor teaches each course.
Our system incorpo-rates these commonly held beliefs by maintaininga model of a stereotypical user whose beliefs maybe attributed to the user during the conversationas appropriate.
People also communicate their be-liefs by their acceptance (explicit and implicit) andnon-acceptance of other people's actions.
Thus,explicit or implicit acceptance of discourse actionsprovides another mechanism for updating the be-lief model: when an action is recognized as suc-cessful, we update our model of the user's beliefswith the effects and goals of the completed ac-tion.
For example, in determining whether (3) isexpressing doubt at (2), thereby implicitly indi-cating that (2) has been understood and that theTell action has therefore been successful, the sys-tem tentatively hypothesizes that the effects andgoals of the Tell action hold, the goal being that$1 believes that $2 believes that Dr. Smith isteaching Architecture (belief (b) above).
If thesystem determines that tiffs Express-Doubt infer-ence is the most coherent interpretation f (3), itattributes the hypothesized beliefs to S1.
So, ourmodel captures many of the ways in which peopleinfer beliefs: 1) from the surface form of utter-ances; 2) from stereotype models; and 3) from ac-ceptance (explicit or implicit) or non-acceptanceof previous actions.5 Combin ing  Knowledge  SourcesGrosz and Sidner (1986) contend that mod-eling discourse requires integrating different kindsof knowledge in a unified framework in order toconstrain the possible role that an utterance mightbe serving.
We use three kinds of knowledge,1) contextual information provided by previousutterances; 2) world knowledge; and 3) the lin-guistic information contained in each utterance.Contextual knowledge in our model is captured bythe DM and the current focus of attention withinit.
The system's world knowledge contains factsabout the world, the system's beliefs (includingits beliefs about a stereotypical user's beliefs), andknowledge about how to go about performing dis-course, problem-solving, and domain actions.
Thelinguistic knowledge that we exploit includes thesurface form of the utterance, which conveys be-liefs and the strength of belief, as discussed in thepreceding section, and linguistic lue words.
Cer-tain words often suggest what type of discourseaction the speaker might be pursuing (Litman andAllen, 1987; Hinkelman, 1989).
For example, thelinguistic lue please suggests a request discourseact (Hinkelman, 1989) while the clue word but sug-gests a non-acceptance discourse act.
Our modeltakes these linguistic clues into consideration iidentifying the discourse acts performed by an ut-terance.Our investigation ofnaturally occurring di-alogues indicates that listeners use a combinationof information to determine what a speaker is try-ing to do in saying something.
For example, S2'sworld knowledge of commonly held beliefs enabled$2 to determine that $1 probably believes (c), andtherefore infer that $1 was expressing doubt at (2).However, $1 might have said (4) instead of (3).
(4) But didn't Dr. Smith win a teaching award?It is not likely that $2 would think that people typ-ically believe that Dr. Smith winning a teachingaward implies that she is not teaching Architec-ture.
However, $2 would probably still recognize(4) as an expression of doubt because the linguis-tic clue but suggests that (4) may be some sort ofnon-acceptance action, there is nothing to suggestthat S1 does not believe that Dr. Smith winning ateaching award implies that she is not teaching Ar-chitecture, and no other interpretation seems morecoherent.
Since linguistic knowledge is present,less evidence is needed from world knowledge torecognize the discourse actions being performed(Grosz and Sidner, 1986).In our model, if a new utterance contributesto a discourse action already in the DM, then theremust be an inference path from the utterance thatlinks the utterance up to the current ree structureon the discourse level.
This inference path willcontain an action that determines the relationshipof the utterance to the DM by introducing newparameters for which there are many possible in-stantiations, but which must be instantiated basedon values from the DM in order for the path to ter-minate with an action already in the DM.
We willrefer to such actions as e-actions ince we contendthat there must be evidence to support he infer-ence of these actions.
By substituting values fromthe DM that are not present in the semantic repre-sentation of the utterance for the new parametersin e-actions, we are hypothesizing a relationshipbetween the new utterance and the existing dis-course level of the DM.Express-Doubt is an example of an e-action(Figure 1).
From the speaker's conveying uncer-tain belief in the proposition _prop2, plan chain-ing suggests that the speaker might be expressingdoubt at some proposition _propl, and from thisExpress-Doubt action, further plan chaining maysuggest a sequence of actions terminating at anInform action already in the DM.
The ability of_propl to unify with the proposition that was con-veyed by the Inform action (and _rule to unify197with a rule in the system's world knowledge) isnot sufficient o justify inferring that the currentutterance contributes to an Express-Doubt actionwhich contributes to an Inform action; more evi-dence is needed.
This is further discussed in Lam-bert and Carberry (1992).Thus we need evidence for including e-actions on an inference path.
The required evi-dence for e-actions may be provided by linguisticknowledge that suggests certain discourse actions(e.g., the evidence that (4) is expressing doubt)or may be provided by world knowledge that in-dicates that the applicability conditions for a par-ticular action hold (e.g., the evidence that (3) isexpressing doubt).Our model combines these different knowl-edge sources in our plan recognition algorithm.From the semantic representation f an utterance,higher level actions are inferred using plan infer-ence rules (Allen, 1979).
If the applicability condi-tions for an inferred action are not plausible, thisaction is rejected.
If the applicability conditionsare plausible, then the beliefs contained in themare temporarily ascribed to the user (if an infer-ence line containing this action is later adopted asthe correct interpretation, these applicability con-ditions are added to the belief model of the user).The focus of attention and focusing heuristics (dis-cussed in Lambert and Carberry (1991)) orderthese sequences of inferred actions, or inferencelines, in terms of coherence.
For those inferencelines with an e-action, linguistic clues are checkedto determine if the action is suggested by linguisticknowledge, and world knowledge is checked to de-termine if there is evidence that the applicabilityconditions for the e-action hold.
If there is worldand linguistic evidence for the e-action of one ormore inference lines, the inference line that is clos-est to the focus of attention (i.e., the most contex-tually coherent) is chosen.
Otherwise, if there isworld or linguistic evidence for the e-action of oneor more inference lines, again the inference linethat is closest to the focus of attention is chosen.Otherwise, there is no evidence for the e-action inany inference line, so the inference line that is clos-est to the current focus of attention and containsno e-action is chosen.6 ExampleThe following example, an expansion of ut-terances (1), (2), and (3) from Section 3, illustrateshow our model handles 1) implicit and explicit ac-ceptance; 2) negotiation subdialogues embeddedwithin other negotiation subdialogues; 3) expres-sions of doubt at both immediately preceding andearlier utterances; and 4) multiple expressions ofdoubt at the same proposition.
We will concen-trate on how Sl 's utterances are understood andassimilated into the DM.
(5) $1: What is Dr. Smith teaching?
(6) S2: Dr. Smith is teaching Architecture.
(7) SI: Isn't Dr. Brown teaching Architecture?
(8) $2: No.
(9) Dr. Brown is on sabbatical.
(10) SI: But didn't 1see him on campusyesterday?
(11) $2: Yes.
(12) He was giving a University colloquium.
(13) SI: OK.(14) But isn't Dr. Smith a theory person?The inferencing for utterances similar to (5)and (6) is discussed in depth in Lambert and Car-berry (1992), and the resultant DM is given inFigure 2.
No clarification or justification of theRequest action or of the content of the question hasbeen addressed by either S1 or $2, and $2 has pro-vided a relevant answer, so both parties have im-plicitly indicated (Clark and Schaefer, 1989) thatthey think that S1 has made a reasonable and un-derstandable r quest in asking the question in (5).The surface form of (7) suggests that S1thinks that Dr. Brown is teaching Architecture,but isn't certain of it.
This belief is enteredinto the system's model of Sl 's beliefs.
This sur-face question is one way to Convey-Uncertain-Belief.
As discussed in Section 3, the most coher-ent interpretation of (7) based on focusing heuris-tics, addressing the understandability of (6), isrejected (because there is not evidence to sup-port this inference), so the system tries to relate(7) to the Inform action in (6); that is, the sys-tem tries to interpret (7) as addressing the believ-ability of (6).
Plan chaining determines that theConvey-Uncertain-Belief action could be part ofan Express-Doubt action which could be part ofan Address-Unacceptance action which could bean action in an Address-Believability discourse ac-tion which could in turn be an action in the In-form action of (6).
Express-Doubt is an e-actionbecause the action header introduces new argu-ments that have not appeared previously on theinference path (see Figure 1).
Since there is evi-dence from world knowledge that the applicabilityconditions hold for interpreting (7) as an expres-sion of doubt and since there is no other evidencefor any other e-action, the system infers that thisis the correct interpretation and stops.
Thus, (7)is interpreted as an Express-Doubt action.
S2's re-sponse in (8) and (9) indicates that $2 is trying toresolve $1 and S2's conflicting beliefs.
The struc-ture that the DM has built after these utterancesis contained in Figure 3, 6 above the numbers (5) -(9).The Surface-Neg-YN-Question in utterance(10) is one way to Convey-Uneerlain-Belief.
Thelinguistic clue but suggests that S1 is execut-6 For space reasons, only inferencing of discourse actionswill be discussed here, and only action names on the dis-course level are shown; the problem-solvlng and domainlevels are as shown in Figure 2.198(5) (6) Resolve-ConflictSurface-Neg YN-Question \](7)(9)Figure 3.
Discourse Level of DM|Address-UnacCeptance I\[Express-Doubt I\[YN-Question J(14)iIIt'eft/onIbguer(10) (11) (12) t"for Dialogue in Section 6ing a non-acceptance discourse action; this non-acceptance action might be addressing either (9)or (6).
Focusing heuristics uggest hat the mostlikely candidate is the Inform act attempted in(9), and plan chaining suggests that the Convey-Uncertain-Belief could be part of an Express-Doubt action which in turn could be part of anAddress-Unacceptance action which could be partof an Address-Believability action which could bepart of the Inform action in (9).
Again, there isevidence that the applicability conditions for thee-action (tile Express-Doubt action) hold: worldknowledge indicates that a typical user believesthat professors who are on sabbatical are not oncampus.
Thus, there is both linguistic and worldknowledge giving evidence for the Express-Doubtaction (and no other e-action has both linguisticand world knowledge vidence), so (10) is inter-preted as expressing doubt at (9).In (11) and (12), $2 clears up the confu-sion that S1 has expressed in (10), by telling S1that the rule that people on sabbatical are noton campus does not hold in this case.
In (13),S1 indicates explicit acceptance of the previouslycommunicated proposition, so the system is ableto determine that S1 has accepted S2's response in12).
This additional negotiation, utterances (10)-13), illustrates our model's handling of negotia-tion subdialogues embedded within other negoti-ation subdialogues.
The subtree contained withinthe dashed lines in Figure 3 shows the structureof this embedded negotiation subdialogue.The linguistic clue but in (14) then againsuggests non-acceptance.
Since (12) has been ex-plicitly accepted, (14) could be expressing non-acceptance of the information conveyed in either(9) or (6).
Focusing heuristics uggest hat (14)is most likely expressing doubt at (9).
Worldknowledge, however, provides no evidence that theapplicability conditions hold for (14) expressingdoubt at (9).
Thus, there is evidence from lin-guistic knowledge for this inference, but not fromworld knowledge.
The system's tereotype modeldoes indicate, however, that it is typically believedthat faculty only teach courses in their field andthat Architecture and Theory are different fields.So in this case, the system's world knowledge pro-vides evidence that Dr. Smith being a theoryperson is an indication that Dr. Smith does notteach Architecture.
Therefore, the system inter-prets (14) as again expressing doubt at (6) becausethere is evidence for this inference from both worldand linguistic knowledge.
The system infers there-fore that S1 has implicitly accepted the statementin (9), that Dr. Smith is on sabbatical.
Thus, thesystem is able to recognize and assimilate a secondexpression of doubt at the proposition conveyed in6).
The DM for the discourse level of the entireialogue is given in Figure 3.1997 ConclusionWe have presented a plan-based model thathandles cooperative negotiation subdialogues byinferring both the communicative actions thatpeople pursue when speaking and the beliefs un-derlying these actions.
Beliefs, and the strength ofthose beliefs, are recognized from the surface formof utterances and from the explicit and implicit ac-ceptance of previous utterances.
Our model com-bines linguistic, contextual, and world knowledgein a unified framework that enables recognitionnot only of when an agent is negotiating a con-flict between the agent's beliefs and the precedingdialogue but also which part of the dialogue theagent's beliefs conflict with.
Since negotiation isan integral part of multi-agent activity, our modeladdresses an important aspect of cooperative in-teraction and communication.Re ferencesAllen, James F. (1979).
A Plan-Based Approachto Speech Act Recognition.
PhD thesis, Uni-versity of Toronto, Toronto, Ontario, Canada.Carberry, Sandra (1987).
Pragmatic Modeling:Toward a Robust Natural Language Interface.Computational Intelligence, 3, 117-136.Clark, tlerbert and Schaefer, Edward (1989).
Con-tributing to Discourse.
Cognitive Science,259-294.Cohen, Robin (1987).
Analyzing the Structureof Argumentative Discourse.
ComputationalLinguistics, 13(1-2), 11-24.Cohen, Robin and Young, Mark A.
(1991).
Deter-mining Intended Evidence Relations in Natu-ral Language Arguments.
Computational In-telligence, 7, 110-118.Flowers, Margot, McGuire, Rod, and Birnbaum,Lawrence (1982).
Adversary Arguments andthe Logic of Personal Attack.
In W. Lehn-eft and M. Ringle (Eds.
), Strategies for Natu-ral Language Processing (pp.
275-294).
Hills-dage, New Jersey: Lawrence Erlbaum Assoc.Galliers, Julia R. (1991).
Belief Revision and aTheory of Communication.
Technical Report193, University of Cambridge, Cambridge,England.Goodman, Bradley A. and Litman, Diane J.(1992).
On the Interaction between PlanRecognition and Intelligent Interfaces.
UserModeling and User-Adapted Interaction, 2,83-115.Grosz, Barbara and Sidner, Candace (1986).
At-tention, Intention, and the Structure of Dis-course.
Computational Linguistics, le(3),175-204.Hinkelman, Elizabeth (1989).
Two Constraints onSpeech Act Ambiguity.
In Proceedings of the27th Annual Meeting of the ACL (pp.
212-219), Vancouver, Canada.Kautz, Henry and Allen, James (1986).
General-ized Plan Recognition.
In Proceedings of theFifth National Conference on Artificial Intel-li.gence (pp.
32-37), Philadelphia, Pennsylva-nia.Lambert, Lynn and Carberry, Sandra (1991).
ATripartite Plan-based Model of Dialogue.
InProceedings of the 29th Annual Meeting of theACL (pp.
47-54), Berkeley, CA.Lambert, Lynn and Carberry, Sandra (1992).
Us-ing Linguistic, World, and Contextual Knowl-edge in a Plan Recognition Model of Dia-logue.
In Proceedings of COLING-92, Nantes,France.
To appear.Litman, Diane and Allen, James (1987).
A PlanRecognition Model for Subdialogues in Con-versation.
Cognitive Science, 11, 163-200.Perrault, Raymond (1990).
An Application of De-fault Logic to Speech Act Theory.
In P. Co-hen, J. Morgan, and M. Pollack (Eds.
), Inten-tions in Communication (pp.
161-185).
Cam-bridge, Massachusetts: MIT Press.Pollack, Martha (1990).
Plans as Complex Men-tal Attitudes.
In P. R. Cohen, J. Morgan, andM.
E. Pollack (Eds.
), Intentions in Commu-nication (pp 77-104).
MIT Press.Quilici, Alexander (1991).
The Correction Ma-chine: A Computer Model of Recognizing andProducing Belief Justifications in Argumenta-tive Dialogs.
PhD thesis, Department ofCom-puter Science, University of California t LosAngeles, Los Angeles, California.Ramshaw, Lance A.
(1991).
A Three-Level Modelfor Plan Exploration.
In Proceedings of the29th Annual Meeting of the ACL (pp.
36-46),Berkeley, California.Reichman, Rachel (1981).
Modeling Informal De-bates.
In Proceedings of the 1981 Interna-tional Joint Conference on Artificial Intelli-gence (pp.
19-24), Vancouver, B.C.
IJCAI.Sidner, Candace L. (1985).
Plan Parsing for In-tended Response Recognition in Discourse.Computational Intelligence, 1, 1-10.Walker, Marilyn (1991).
Redundancy in Collabo-rative Dialogue.
Presented at The AAAI FallSymposium: Discourse Structure in NaturalLanguage Understanding and Generation (pp.124-129), Asilomar, CA.Wilensky, Robert (1981).
Meta-Planning: Rep-resenting and Using Knowledge About Plan-ning in Problem Solving and Natural Lan-guage Understanding.
Cognitive Science, 5,197-233.200
