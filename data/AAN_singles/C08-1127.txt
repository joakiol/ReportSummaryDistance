Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1009?1016Manchester, August 2008Linguistically Annotated BTG for Statistical Machine TranslationDeyi Xiong, Min Zhang, Aiti Aw and Haizhou LiHuman Language TechnologyInstitute for Infocomm Research21 Heng Mui Keng Terrace, Singapore 119613{dyxiong, mzhang, aaiti}@i2r.a-star.edu.sgAbstractBracketing Transduction Grammar (BTG)is a natural choice for effective integrationof desired linguistic knowledge into sta-tistical machine translation (SMT).
In thispaper, we propose a Linguistically Anno-tated BTG (LABTG) for SMT.
It conveyslinguistic knowledge of source-side syn-tax structures to BTG hierarchical struc-tures through linguistic annotation.
Fromthe linguistically annotated data, we learnannotated BTG rules and train linguisti-cally motivated phrase translation modeland reordering model.
We also present anannotation algorithm that captures syntac-tic information for BTG nodes.
The ex-periments show that the LABTG approachsignificantly outperforms a baseline BTG-based system and a state-of-the-art phrase-based system on the NISTMT-05 Chinese-to-English translation task.
Moreover, weempirically demonstrate that the proposedmethod achieves better translation selec-tion and phrase reordering.1 IntroductionFormal grammar used in statistical machine trans-lation (SMT), such as Bracketing TransductionGrammar (BTG) proposed by (Wu, 1997) and thesynchronous CFG presented by (Chiang, 2005),provides a natural platform for integrating lin-guistic knowledge into SMT because hierarchicalstructures produced by the formal grammar resem-ble linguistic structures.1 Chiang (2005) attemptsto integrate linguistic information into his formallyc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1We inherit the definitions of formal and linguistic from(Chiang, 2005) which makes a distinction between formallysyntax-based SMT and linguistically syntax-based SMT.syntax-based system by adding a constituent fea-ture.
Unfortunately, the linguistic feature does notshow significant improvement on the test set.
Inthis paper, we further this effort by integrating lin-guistic knowledge into BTG.We want to augment BTG?s formal structureswith linguistic structures since they are both hier-archical.
In particular, our goal is to learn a morelinguistically meaningful BTG from real-world bi-texts by projecting linguistic structures onto BTGformal structures.
In doing so, we hope to (1)maintain the strength of phrase-based approachsince phrases are still used on BTG leaf nodes; (2)obtain a tight integration of linguistic knowledge inthe translation model; (3) and finally avoid induc-ing a complicated linguistic synchronous grammarwith expensive computation.
The challenge, ofcourse, is that BTG hierarchical structures are notalways aligned with the linguistic structures in thesyntactic parse trees of source or target language.Along this line, we propose a novel approach:Linguistically Annotated BTG (LABTG) for SMT.The LABTG annotates BTG rules with linguisticelements that are learned from syntactic parse treeson the source side through an annotation algo-rithm, which is capable of labelling both syntacticand non-syntactic phrases.
The linguistic elementsextracted from parse trees capture both internallexical content and external context of phrases.With these linguistic annotations, we expect theLABTG to address two traditional issues of stan-dard phrase-based SMT (Koehn et al, 2003) in amore effective manner.
They are (1) phrase trans-lation: translating phrases according to their con-texts; (2) phrase reordering: incorporating richerlinguistic features for better reordering.The proposed LABTG displays two uniquecharacteristics when compared with BTG-basedSMT (Wu, 1996; Xiong et al, 2006).
The firstis that two linguistically-informed sub-models areintroduced for better phrase translation and re-ordering: annotated phrase translation model and1009annotated reordering model.
The second is thatour proposed annotation algorithm and scheme arecapable of conveying linguistic knowledge fromsource-side syntax structures to BTG structures.We describe the LABTG model and the annota-tion algorithm in Section 4.
To better explain theLABTG model, we establish a unified frameworkof BTG-based SMT in Section 3.
We conducta series of experiments to study the effect of theLABTG in Section 5.2 Related WorkThere have been various efforts to integrate lin-guistic knowledge into SMT systems, either fromthe target side (Marcu et al, 2006; Hassan et al,2007; Zollmann and Venugopal, 2006), the sourceside (Quirk et al, 2005; Liu et al, 2006; Huanget al, 2006) or both sides (Eisner, 2003; Ding etal., 2005; Koehn and Hoang, 2007), just to name afew.
LABTG can be considered as, but not limitedto, a new attempt that enriches translation modelwith source-side linguistic annotations.
(Huang and Knight, 2006) and (Hassan et al,2007) introduce relabeling and supertagging on thetarget side, respectively.
The former re-annotatessyntactified phrases to learn grammatical distinc-tions while the latter supertags standard plainphrases, both applied on the target side.
The differ-ence between their work and LABTG is significantbecause we annotate standard plain phrases usinglinguistic elements on the source side.
Comparedwith the target side annotation which improves flu-ency and grammaticality of translation output, lin-guistic annotation on the source side helps to im-prove translation adequacy.Recently, some researchers have extended andcreated several variations of BTG/ITG.
Zhang etal.
(2005) propose lexicalized ITG for better wordalignment.
Xiong et al (2006) demonstrate thattheir MEBTG, a BTG variation with MaxEnt-based reordering model, can improve phrase re-ordering significantly.
Similarly, Setiawan et al(2007) use an enhanced BTG variation with func-tion words for reordering.
LABTG differs fromthese BTG variations in that the latter does not useany external linguistic knowledge.Zhang et al (2007) describe a phrase reorder-ing model based on BTG-style rules which inte-grates source-side syntactic knowledge.
Our an-notated reordering model of LABTG differs fromtheir work in two key aspects.
Firstly, we al-low any phrase reorderings while they only reordersyntactic phrases.
In their model, only syntacticphrases can use linguistic knowledge from parsetrees for reordering while non-syntactic phrasesare combined monotonously with a constant re-ordering score since no syntactic knowledge canbe used at all.
Our proposed LABTG successfullyovercomes this limitation by supporting linguis-tic annotation on both syntactic and non-syntacticphrases.
Moreover, we show that excluding non-syntactic phrase from reordering does hurt theperformance.
Secondly, we use richer linguisticknowledge in reordering, including head wordsand syntactic labels of context nodes, when com-pared with their model.
Our experiments show thatthese additional information can improve reorder-ing.3 BTG Based SMTWe establish a unified framework for BTG-basedSMT in this section.
There are two kinds of rulesin BTG, lexical rules (denoted as rl) and mergingrules (denoted as rm):rl: A ?
x/yandrm: A ?
[Al, Ar]|?Al, Ar?Lexical rules translate source phrase x into targetphrase y and generate a leaf node A in BTG tree.Merging rules combine left and right neighboringphrases Aland Arinto a larger phrase A in an or-der o ?
{straight, inverted}.We define a BTG derivation D as a sequenceof independent applications of lexical and mergingrules (D = ?rl1..nl, rm1..nm?).
Given a source sen-tence, the decoding task of BTG-based SMT is tofind a best derivation, which yields the best trans-lation.Similar to (Xiong et al, 2006), we can as-sign a probability to each rule using a log-linearmodel with different features and corresponding ?weights, then multiply them to obtain P (D).
Forconvenience of notation and keeping in line withthe common understanding of standard phrase-based model, here we re-organize these featuresinto translation model (PT), reordering model(PR) and target language model (PL) as followsP (D) = PT(rl1..nl) ?
PR(rm1..nm)?R?PL(e)?L?
exp(|e|)?w (1)where exp(|e|) is the word penalty.1010The translation model is defined as:PT(rl1..nl) =nl?i=1P (rli)P (rl) = p(x|y)?1?
p(y|x)?2?
plex(x|y)?3?plex(y|x)?4?
exp(1)?5 (2)where p(?)
represent the phrase translation proba-bilities in both directions, plex(?)
denote the lexi-cal translation probabilities in both directions, andexp(1) is the phrase penalty.Similarly, the reordering model is defined on themerging rules as followsPR(rm1..nm) =nm?i=1P (rmi) (3)In the original BTGmodel (Wu, 1996), P (rm) wasactually a prior probability which can be set basedon the order preference of the language pairs.
InMEBTG (Xiong et al, 2006), however, the prob-ability is calculated more sophisticatedly using aMaxEnt-based classification model with boundarywords as its features.4 Linguistically Annotated BTG BasedSMTWe extend the original BTG into the linguisticallyannotated BTG by adding linguistic annotationsfrom source-side parse trees to both BTG lexicalrules and merging rules.
Before we elaborate howthe LABTG extends the baseline, we introduce an-notated BTG rules.In the LABTG, both lexical rules and mergingrules are annotated with linguistic elements as fol-lowsarl: Aa?
x#a/yandarm: Aa?
[Aall, Aarr]|?Aall, Aarr?The annotation a comprises three linguistic ele-ments from source-side syntactic parse tree: (1)head word hw, (2) the part-of-speech (POS) taght of head word and (3) syntactic label sl.
In an-notated lexical rules, the three elements are com-bined together and then attached to x as an anno-tation unit.
In annotated merging rules, each nodeinvolved in merging is annotated with these threeelements individually.There are various ways to learn the annotatedrules from training data.
The straight-forward wayis to first generate the best BTG tree for each sen-tence pair using the way of (Wu, 1997), then an-notate each BTG node with linguistic elementsby projecting source-side syntax tree to BTG tree,and finally extract rules from these annotated BTGtrees.
This way restricts learning space to only thebest BTG trees2, and leads to the loss of many use-ful annotated rules.Therefore, we use an alternative way to extractthe annotated rules as illustrated below.
Firstly, werun GIZA++ (Och and Ney, 2000) on the train-ing corpus in both directions and then apply theogrow-diag-finalp refinement rule (Koehn et al,2003) to obtain many-to-many word alignments.Secondly, we extract bilingual phrases from theword-aligned corpus, then annotate their sourcesides with linguistic elements to obtain the an-notated lexical rules.3 Finally, we learn reorder-ing examples (Xiong et al, 2006), annotate theirtwo neighboring sub-phrases and whole phrases,and then generalize them in the annotated mergingrules.
Although this alternative way may also missreorderings due to word alignment errors, it is stillmore flexible and robust than the straight-forwardone, and can learn more annotated BTG rules with-out constructing BTG trees explicitly.4.1 LABTG Annotation AlgorithmDuring the process of rule learning and decod-ing, we need to annotate bilingual phrases or BTGnodes generated by the decoder given a sourcesentence together with its parse tree.
Since bothphrases and BTG nodes can be projected to a spanon the source sentence, we run our annotation al-gorithm on source-side spans and then assign an-notation results to the corresponding phrases ornodes.
If the span is exactly covered by a singlesubtree in the source-side parse tree, it is calledsyntactic span, otherwise non-syntactic span.One of the challenges in this annotation algorithmis that BTG nodes (or phrases) are not always cov-ering syntactic span, in other words, are not alwaysaligned to constituent nodes in the source-side tree.To solve this problem, we use heuristic rules togenerate pseudo head word and composite labelwhich consists of syntactic labels of three relevantconstituents for the non-syntactic span.The annotation algorithm is shown in Fig.
1.For a syntactic span, the annotation is trivial.
An-notation elements directly come from the subtreethat exactly covers the span.
For a non-syntactic2Producing BTG forest for each sentence pair is very time-consuming.3This makes the number of extracted annotated lexicalrules proportional to that of bilingual phrases.10111: Annotator (span s = ?i, j?, source-side parse tree t)2: if s is a syntactic span then3: Find the subtree c in t which exactly covers s4: s.a := {c.hw, c.ht, c.sl}5: else6: Find the smallest subtree c?
subsuming s in t7: if c?.hw ?
s then8: s.a.hw := c?.hw and s.a.ht := c?.ht9: else10: Find the word w ?
s which is nearest to c?.hw11: s.a.hw := w and s.a.ht := w.t /*w.t is the POStag of w*/12: end if13: Find the left context node ln of s in c?14: Find the right context node rn of s in c?15: s.a.sl := ln.sl-c?.sl-rn.sl16: end ifFigure 1: The LABTG Annotation Algorithm.span, the process is much complicated.
Firstly,we need to locate the smallest subtree c?
subsum-ing the span (line 6).
Secondly, we try to identifythe head word/tag of the span (line 7-12) by us-ing c?
?s head word hw directly if it is within thespan.
Otherwise, the word within the span whichis nearest to hw will be assigned as the head wordof the span.
Finally, we determine the compositelabel of the span (line 13-15), which is formulatedas L-C-R. L/R refers to the syntactic label of theleft/right context node of s which is a sub-node ofc?.
There are different ways to define the contextnode of a span in the source-side parse tree.
It canbe the closest neighboring node or the boundarynode which is the highest leftmost/rightmost sub-node of c?
not overlapping the span.
If there is nosuch context node (the span s is exactly aligned tothe left/right boundary of c?
), L/R will be set toNULL.
C is the label of c?.
L, R and C togetherdefine the external syntactic context of s.Fig.
2 shows a syntactic parse tree for a Chinesesentence, with head word annotated for each inter-nal node.4 Some sample annotations are given inTable 1.
We also show different composite labelsfor non-syntactic spans with different definitionsof their context nodes.
sl1is obtained when theboundary node is defined as the context node whilesl2is obtained when the closest neighboring nodeis defined as the context node.4.2 LABTG ModelTo better model annotated rules, the LABTG con-tributes two significant modifications to formula(1).
First is the annotated phrase translation model4In this paper, we use phrase labels from the Penn ChineseTreebank (Xue et al, 2005).IP(??)?????HHHHHNP(??)??HHNP(??)NR??1TibetNP(??)?HNN??2financialNN??3workVP(??)?????HHHHHVV??4gainAS?5NP(??)??HHADJP(??)JJ??6remarkableNP(??
)NN?7?achievementFigure 2: A syntactic parse tree with head wordannotated for each internal node.
The superscriptsof leaf nodes denote their surface positions fromleft to right.span hw ht sl1(boundary node) sl2(neighboring node)?1, 2?
??
NN NULL-NP-NN NULL-NP-NN?2, 3?
??
NN NP NP?2, 4?
??
VV NP-IP-NP NP-IP-AS?3, 4?
??
VV NP-IP-NP NN-IP-ASTable 1: Annotation samples according to the treeshown in Fig.
2. hw/ht represents head word/tag,respectively.
sl means the syntactic label.with source side linguistically enhanced to replacethe standard phrase translation model, and secondis the additional MaxEnt-based reordering modelthat uses linguistic annotations as features.
TheLABTG model is formulated as followsP (D) = PTa(arl1..nl) ?
PRb(rm1..nm)?Rb?PRa(arm1..nm)?Ra?
PL(e)?L?
exp(|e|)?w (4)Here PTais the annotated phrase translationmodel, PRbis the reordering model from MEBTGusing boundary words as features and PRais theannotated reordering model using linguistic anno-tations of nodes as features.Annotated Phrase Translation Model Theannotated phrase translation model PTais sim-ilar to formula (2) except that phrase transla-tion probabilities on both directions are p(x#a|y)and p(y|x#a) respectively, instead of p(x|y) andp(y|x).
By introducing annotations into the trans-lation model, we integrate linguistic knowledgeinto the statistical selection of target equivalents.Annotated Reordering Model The annotatedreordering model PRais a MaxEnt-based classi-fication model which uses linguistic elements ofeach annotated node as its features.
The model canbe formulated asPRa(arm) = p?
(o|Aa, Aall, Aarr)1012=exp(?i?ihi(o,Aa, Aall, Aarr))?oexp(?i?ihi(o,Aa, Aall, Aarr))where the functions hi?
{0, 1} are reordering fea-tures and ?iare weights of these features.Each merging rule involves 3 nodes(Aa, Aall, Aarr) and each node has 3 linguisticelements (hw, ht, sl).
Therefore, the model has 9features in total.
Taking the left node Aallas anexample, the model could use its head word w asfeature as followshi(o,Aa, Aall, Aarr) ={1, Aall.hw = w, o = straight0, otherwise4.3 TrainingTo train the annotated translation model, firstly weextract all annotated lexical rules from source-sideparsed, word-aligned training data.
Then we es-timate the annotated phrase translation probabili-ties p(x#a|y) and p(y|x#a) using relative countsfrom all collected annotated lexical rules.
For ex-ample, p(y|x#a) can be calculated as followsp(y|x#a) =count(x#a, y)?ycount(x#a, y)One might think that linguistic annotations wouldcause serious data sparseness problem and theprobabilities should be smoothed.
However, ac-cording to our statistics (described in the next sec-tion), the differences in annotations for the samesource phrase x are not so diverse.
So we takea direct backoff strategy to map unseen annotatedlexical rules to their un-annotated versions on thefly during decoding, which is detailed in the nextsubsection.To train the annotated reordering model, wegenerate all annotated reordering examples, thenobtain features using linguistic elements of theseexamples, and finally estimate feature weightsbased on the maximum entropy principle.4.4 DecodingA CKY-style decoder with beam search is devel-oped, similar to (Xiong et al, 2006).
Each in-put source sentence is firstly parsed to obtain itssyntactic tree.
Then the CKY-style decoder triesto generate the best annotated BTG tree using thetrained annotated lexical and merging rules.
Westore all annotated lexical rules and their proba-bilities in a standard phrase table ?, where sourcephrases are augmented with annotations.
Duringthe application of annotated lexical rules, we la-bel each source phrase x with linguistic annota-tion a through the annotation algorithm given thesource-side parse tree, and retrieve x#a from ?.In the case of unseen combination x#a, we mapit to x and lookup x in the phrase table so that wecan use the un-annotated lexical rule A ?
x/y.We set p(y|x) = maxa?p(y|x#a?)
and p(x|y) =maxa?p(x#a?|y) where (x, a?, y) ?
?.
When twoneighboring nodes are merged in a specific order,the two reordering models, PRband PRa, will eval-uate this merging independently with individualscores.
The former uses boundary words as fea-tures while the latter uses the linguistic elementsas features, annotated on the BTG nodes throughthe annotation algorithm according to the source-side parse tree.5 Experiments and AnalysisIn this section we conducted a number of ex-periments to demonstrate the competitiveness ofthe proposed LABTG based SMT when comparedwith two baseline systems: Moses (Koehn et al,2007), a state-of-the-art phrase-based system andMEBTG (Xiong et al, 2006), a BTG based sys-tem.
We also investigated the impact of differ-ent annotation schemes on the LABTG model andstudied the effect of annotated phrase translationmodel and annotated reordering model on transla-tion selection and phrase reordering respectively.All experiments were carried out on the Chinese-to-English translation task of the NISTMT-05 withcase-sensitive BLEU scores reported.The systems were trained on the FBIS cor-pus.
We removed 15,250 sentences, for whichthe Chinese parser (Xiong et al, 2005) failed toproduce syntactic parse trees.
The parser wastrained on the Penn Chinese Treebank with a F1score of 79.4%.
From the remaining FBIS corpus(224, 165 sentence pairs), we obtained 4.55M stan-dard bilingual phrases (including 2.75M sourcephrases) for the baseline systems and 4.65M an-notated lexical rules (including 3.13M annotatedsource phrases augmented with linguistic anno-tations) for the LABTG system using the algo-rithm mentioned above.
These statistics revealthat there are 1.14 (3.13M/2.75M) annotations persource phrase, which means our annotation algo-rithm does not increase the number of extractedrules exponentially.We extracted 2.8M reordering examples, from1013System BLEUMoses 0.2386MEBTG 0.2498LABTG 0.2667Table 2: LABTG vs. Moses and MEBTG.which we generated 114.8K reordering features forthe reordering model PRb(shared by both MEBTGand LABTG systems) using the right boundarywords of phrases and 85K features for the anno-tated reordering model PRa(only included in theLABTG system) using linguistic annotations.
Weran the MaxEnt toolkit (Zhang, 2004) to tune re-ordering feature weights with iteration number be-ing set to 100 and Gaussian prior to 1 to avoid over-fitting.We built our four-gram language model usingXinhua section of the English Gigaword corpus(181.1M words) with the SRILM toolkit (Stolcke,2002).
For the efficiency of minimum-error-ratetraining (Och, 2003), we built our development set(580 sentences) using sentences not exceeding 50characters from the NIST MT-02 evaluation testdata.5.1 LABTG vs. phrase-based SMT andBTG-based SMTWe compared the LABTG system with two base-line systems.
The results are given in Table 2.The LABTG outperforms Moses and MEBTG by2.81 and 1.69 absolute BLEU points, respectively.These significant improvements indicate that BTGformal structures can be successfully extendedwith linguistic knowledge extracted from syntac-tic structures without losing the strength of phrase-based method.5.2 The Effect of Different AnnotationSchemesA great amount of linguistic knowledge is con-veyed through the syntactic label sl.
To obtainthis label, we tag syntactic BTG node with singlelabel C from its corresponding constituent in thesource-side parse tree while annotate non-syntacticBTG node with composite label formulated as L-C-R. We conducted experiments to study the effectof different annotation schemes on the LABTGmodel by comparing three different annotationschemes for non-syntactic BTG node: (1) usingsingle label C from its corresponding smallest sub-tree c?
(C), (2) constructing composite label usingAnnotation scheme BLEUC 0.2626N-C-N 0.2591B-C-B 0.2667Annotating syntactic nodes with com-posite label0.2464Table 3: Comparison of different annotationschemes.neighboring node as context node (N-C-N), and (3)constructing composite label using boundary nodeas context node (B-C-B).
The results are shown inTable 3.On the one hand, linguistic annotation providesadditional information for LABTG, transferringknowledge from source-side linguistic structuresto BTG formal structures.
On the other hand, how-ever, it is also a constraint on LABTG, guiding theannotated translation model and reordering modelto the selection of target alernatives and reorder-ing patterns, respectively.
A tight constraint al-ways means that annotations are too specific, al-though they incorporate rich knowledge.
Too spe-cific annotations are more sensitive to parse errors,and easier to make the model lose correct transla-tions or use wrong reordering patterns.
That is thereason why the annotation scheme ?N-C-N?
and?Annotating syntactic nodes with composite label?5 both hurt the performance.
Conversely, a looseconstraint means that annotations are too genericand have less knowledge incorporated.
The an-notation scheme ?C?
is such a scheme with looseconstraint and less knowledge.Therefore, an ideal annotation scheme shouldnot be too specific or too generic.
The annota-tion scheme ?B-C-B?
achieves a reasonable bal-ance between knowledge incorporation and con-straint, which obtains the best performance.
There-fore we choose boundary node as context node forlabel annotation of non-syntactic BTG nodes in ex-periments described later.5.3 The Effect of Annotated TranslationModelTo investigate the effect of the annotated transla-tion model on translation selection, we comparedthe standard phrase translation model PTusedin MEBTG with the annotated phrase translation5In this annotation scheme, we produce composite labelL-C-R for both syntactic and non-syntactic BTG nodes.
Forsyntactic node, sibling node is used as context node while fornon-syntactic node, boundary node is used as context node.1014Translation model BLEUPT0.2498PTa0.2581PTa(-NULL) 0.2548Table 4: The effect of annotated translation model.model PTa.
The experiment results are shown inTable 4.
The significant improvement in the BLEUscore indicates that the annotated translation modelhelps to select better translation options.Our study on translation output shows that anno-tating phrases with source-side linguistic elementscan provide at least two kinds of information fortranslation model to improve the adequacy: cate-gory and context.
The category knowledge of aphrase can be used to select its appropriate trans-lation related to its category.
For example, Chi-nese phrase ???
can be translated into ?value?
ifit is a verb or ?at/on?
if it is a proposition.
How-ever, the baseline BTG-based system always se-lects the proposition translation even if it is a verbbecause the language model probability for propo-sition translation is higher than that of verb trans-lation.
This wrong translation of content words issimilar to the incorrect omission reported in (Ochet al, 2003), which both hurt translation adequacy.The annotated translation model can avoid wrongtranslation by filtering out phrase candidates withunmatched categories.The context information (provided by contextnode) is also quite useful for translation selection.Even the ?NULL?
context, which we used in labelannotation to indicate that a phrase is located at theboundary of a constituent, provides some informa-tion, such as, transitive or intransitive attribute ofa verb phrase.
The last row of Tabel 4 shows thatif we remove ?NULL?
in label annotation, the per-formance is degraded.
(Huang and Knight, 2006)also reported similar result by using sisterhood an-notation on the target side.5.4 The Effect of Annotated ReorderingModelTo investigate the effect of the annotated reorder-ing model, we integrate PRawith various settingsin MEBTG while keeping its original phrase trans-lation model PTand reordering model PRbun-changed.
We augment PRa?s feature pool incre-mentally: firstly using only single labels 6(SL)6For non-syntactic node, we only use the single label C,without constructing composite label L-C-R.Reordering Configuration BLEUPRb0.2498PRb+ PRa(SL) 0.2588PRb+ PRa(+BNL) 0.2627PRb+ PRa(+BNL+HWT) 0.2652PRb+ PRa(SL+BNL+HWT): only al-lowed syntactic phrase reordering0.2512Table 5: The effect of annotated reordering model.as features (132 features in total), then construct-ing composite labels for non-syntactic phrases(+BNL) (6.7K features), and finally introducinghead words into the feature pool (+BNL+HWT)(85K features).
This series of experiments demon-strate the impact and degree of contribution madeby each feature for reordering.
We also conductedexperiments to investigate the effect of restrict-ing reordering to syntactic phrases using the bestreordering feature set (SL+BNL+HWT) for PRa.The experimental results are presented in Table 2,from which we have the following observations:(1) Source-side syntactic labels (SL) capture re-ordering patterns between source structures andtheir target counterparts.
Even when the base-line feature set SL with only 132 features is usedfor PRa, the BLEU score improves from 0.2498to 0.2588.
This is because most of the frequentreordering patterns between Chinese and Englishhave been captured using syntactic labels.
For ex-ample, the pre-verbal modifier PP in Chinese istranslated into post-verbal counterpart in English.This reordering can be described by a rule with aninverted order: V P ?
?PP, V P ?, and capturedby our syntactic reordering features.
(2) Context information, provided by labels ofcontext nodes (BNL) and head word/tag pairs(HWT), also improves phrase reordering.
Produc-ing composite labels for non-syntactic BTG nodes(+BNL) and integrating head word/tag pairs intoPRaas reordering features (+BNL+HWT) are botheffective, indicating that context information com-plements syntactic label for capturing reorderingpatterns.
(3) Restricting phrase reordering to syntacticphrases is harmful.
The BLEU score plummetsfrom 0.2652 to 0.2512.6 ConclusionsIn this paper, we have presented a LinguisticallyAnnotated BTG based approach to effectively in-tegrate linguistic knowledge into SMT by merging1015source-side linguistic structures with BTG hierar-chical structures.
The LABTG brings BTG-basedSMT towards linguistically syntax-based SMT andnarrows the linguistic gap between them.
Ourexperimental results show that the LABTG sig-nificantly outperforms the state-of-the-art phrase-based SMT and the baseline BTG-based SMT.
Theproposed method also offers better translation se-lection and phrase reordering by introducing theannotated phrase translation model and the anno-tated reordering model with linguistic annotations.We conclude that (1) source-side syntactic in-formation can improve translation adequacy; (2)linguistic annotations of BTG nodes well capturereordering patterns between source structures andtheir target counterparts; (3) integration of linguis-tic knowledge into SMT should be carefully con-ducted so that the incorporated knowledge couldnot have negative constraints on the model7.ReferencesDavid Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings of ACL2005.Yuan Ding and Martha Palmer.
2005.
Machine Transla-tion Using Probabilistic Synchronous Dependency Inser-tion Grammars.
In Proceedings of ACL 2005.Jason Eisner.
2003.
Learning non-isomorphic tree mappingsfor machine translation.
In Proceedings of ACL 2003.Hany Hassan, Khalil Sima?an and Andy Way.
2007.
Su-pertagged Phrase-based Statistical Machine Translation.In Proceedings of ACL 2007.Bryant Huang, Kevi Knight.
2006.
Relabeling Syntax Treesto Improve Syntax-Based Machine Translation Quality.
InProceedings of NAACL-HLT 2006.Liang Huang, Kevi Knight and Aravind Joshi.
2006.
Statisti-cal Syntax-directed Translation with Extended Domain ofLocality.
In Proceedings of AMTA 2006.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proceedings ofHLT/NAACL.Philipp Koehn, Hieu Hoang.
2007.
Factored TranslationModels.
In Proceedings of EMNLP 2007.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, BrookeCowan, Wade Shen, Christine Moran, Richard Zens, ChrisDyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst.2007.
Moses: Open Source Toolkit for Statistical MachineTranslation.
ACL 2007, demonstration session, Prague,Czech Republic, June 2007.7For example, the annotation scheme ?N-C-N?
incorpo-rates rich syntactic knowledge, but also tightens the constrainton the model, which therefore loses robustness.Yang Liu, Qun Liu, Shouxun Lin.
2006.
Tree-to-StringAlignment Template for Statistical Machine Translation.In Proceedings of ACL-COLING 2006.Daniel Marcu, Wei Wang, Abdessamad Echihabi, and KevinKnight.
2006.
SPMT: Statistical Machine Translationwith Syntactified Target Language Phraases.
In Proceed-ings of EMNLP.Franz Josef Och and Hermann Ney.
2000.
Improved statisti-cal alignment models.
In Proceedings of ACL 2000.Franz Josef Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proceedings of ACL2003.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, AnoopSarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, LibinShen, David Smith, Katherine Eng, Viren Jain, Zhen Jin,Dragomir Radev.
2003.
Final Report of Johns Hopkins2003 SummerWorkshop on Syntax for Statistical MachineTranslation.Chris Quirk, Arul Menezes and Colin Cherry.
2005.
Depen-dency Treelet Translation: Syntactically Informed PhrasalSMT.
In Proceedings of ACL 2005.Hendra Setiawan, Min-Yen Kan and Haizhou Li.
2007.
Or-dering Phrases with Function Words.
In Proceedings ofACL 2007.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of International Con-ference on Spoken Language Processing, volume 2, pages901-904.Dekai Wu.
1996.
A Polynomial-Time Algorithm for Statisti-cal Machine Translation.
In Proceedings of ACL 1996.Dekai Wu.
1997.
Stochastic Inversion Transduction Gram-mars and Bilingual Parsing of Parallel Corpora.
Computa-tional Linguistics, 23(3):377-403.Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, YueliangQian.
2005.
Parsing the Penn Chinese Treebank with Se-mantic Knowledge.
In Proceedings of IJCNLP, Jeju Is-land, Korea.Deyi Xiong, Qun Liu and Shouxun Lin.
2006.
Maximum En-tropy Based Phrase Reordering Model for Statistical Ma-chine Translation.
In Proceedings of ACL-COLING 2006.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer.2005.
The Penn Chinese TreeBank: Phrase Structure An-notation of a Large Corpus.
Natural Language Engineer-ing, 11(2):207-238.Dongdong Zhang, Mu Li, Chi-Ho Li and Ming Zhou.
2007.Phrase Reordering Model Integrating Syntactic Knowl-edge for SMT.
In Proceedings of EMNLP-CoNLL 2007.Hao Zhang and Daniel Gildea.
2005.
Stochastic LexicalizedInversion Transduction Grammar for Alignment.
In Pro-ceedings of ACL 2005.Le Zhang.
2004.
Maximum Entropy Model-ing Tooklkit for Python and C++.
Available athttp://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.Andreas Zollmann and Ashish Venugopal.
2006.
SyntaxAugmented Machine Translation via Chart Parsing.
InNAACL 2006 - Workshop on statistical machine transla-tion, New York.
June 4-9.1016
