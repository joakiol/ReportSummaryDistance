Structured Composition of Semantic VectorsStephen WuMayo Clinicwu.stephen@mayo.eduWilliam SchulerThe Ohio State Universityschuler@ling.ohio-state.eduAbstractDistributed models of semantics assume that word meanings can be discovered from ?the com-pany they keep.?
Many such approaches learn semantics from large corpora, with each documentconsidered to be unstructured bags of words, ignoring syntax and compositionality within a docu-ment.
In contrast, this paper proposes a structured vectorial semantic framework, in which semanticvectors are defined and composed in syntactic context.
As such, syntax and semantics are fullyinteractive; composition of semantic vectors necessarily produces a hypothetical syntactic parse.Evaluations show that using relationally-clustered headwords as a semantic space in this frameworkimproves on a syntax-only model in perplexity and parsing accuracy.1 IntroductionDistributed semantic representations like Latent Semantic Analysis (Deerwester et al, 1990), probabilis-tic LSA (Hofmann, 2001), Latent Dirichlet Allocation (Blei et al, 2003), or relational clustering (Taskaret al, 2001) have garnered widespread interest because of their ability to quantitatively capture ?gist?semantic content.Two modeling assumptions underlie most of these models.
First, the typical assumption is that wordsin the same document are an unstructured bag of words.
This means that word order and syntactic struc-ture are ignored in the resulting vectorial representations of meaning, and the only relevant relationshipbetween words is the ?same-document?
relationship.
Second, these semantic models are not composi-tional in and of themselves.
They require some external process to aggregate the meaning representationsof words to form phrasal or sentential meaning; at best, they can jointly represent whole strings of wordswithout the internal relationships.This paper introduces structured vectorial semantics (SVS) as a principled response to these weak-nesses of vector space models.
In this framework, the syntax?semantics interface is fully interactive:semantic vectors exist in syntactic context, and any composition of semantic vectors necessarily pro-duces a hypothetical syntactic parse.
Since semantic information is used in syntactic disambiguation(MacDonald et al, 1994), we would expect practical improvements in parsing accuracy by accountingfor the interactive interpretation process.Others have incorporated syntactic information with vector-space semantics, challenging the bag-of-words assumption.
Syntax and semantics may be jointly generated with Bayesian methods (Griffithset al, 2005); syntactic structure may be coupled to the basis elements of a semantic space (Pado?
andLapata, 2007); clustered semantics may be used as a pre-processing step (Koo et al, 2008); or, semanticsmay be learned in some defined syntactic context (Lin, 1998).
These techniques are interactive, buttheir semantic models are not syntactically compositional (Frege, 1892).
SVS is a generative model ofsentences that uses a variant of the last strategy to incorporate syntax at preterminal tree nodes, but isinherently compositional.Mitchell and Lapata (2008) provide a general framework for semantic vector composition:p = f(u,v,R,K) (1)295where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base,and p is a resulting composed vector (or tensor).
In this initial work of theirs, they leave out any notionof syntactic context, focusing on additive and multiplicative vector composition (with some variations):Add: p[i] = u[i] + v[i] Mult: p[i] = u[i] ?
v[i] (2)Since the structured vectorial semantics proposed here may be viewed within this framework, our dis-cussion will begin from their definition in Section 2.1.Erk and Pado?
?s (2008) model also fits inside Mitchell and Lapata?s framework, and like SVS, it in-cludes syntactic context.
Their semantic vectors use syntactic information as relations between multiplevectors in arriving at a final meaning representation.
The emphasis, however, is on selectional prefer-ences of individual words; resulting representations are similar to word-sense disambiguation output,and do not construct phrase-level meaning from word meaning.
Mitchell and Lapata?s more recent work(2009) combines syntactic parses with distributional semantics; but the underlying compositional modelrequires (as other existing models would) an interpolation of the vector composition results with a sepa-rate parser.
It is thus not fully interactive.Though the proposed structured vectorial semantics may be defined within Equation 1, the end outputnecessarily includes not only a semantic vector, but a full parse hypothesis.
This slightly shifts the focusfrom the semantically-centered Equation 1 to an accounting of meaning that is necessarily interactive(between syntax and semantics); vector composition and parsing are then twin lenses by which the pro-cess may be viewed.
Thus, unlike previous models, a unique phrasal vectorial semantic representationis composed during decoding.Due to the novelty of phrasal vector semantics and lack of existing evaluative measures, we havechosen to report results on the well-understood dual problem of parsing.
The structured vectorial seman-tic framework subsumes variants of several common parsing algorithms, two of which will be discussed:lexicalized parsing (Charniak, 1996; Collins, 1997, etc.)
and relational clustering (akin to latent annota-tions (Matsuzaki et al, 2005; Petrov et al, 2006; Gesmundo et al, 2009)).
Because previous work hasshown that linguistically-motivated syntactic state-splitting already improves parses (Klein andManning,2003), syntactic states are split as thoroughly as possible into subcategorization classes (e.g., transitiveand intransitive verbs).
This pessimistically isolates the contribution of semantics on parsing accuracy ?it will only show parsing gains where semantic information does not overlap with distributional syntacticinformation.
Evaluations show that interactively considering semantic information with syntax has thepredicted positive impact on parsing accuracy over syntax alone; it also lowers per-word perplexity.The remainder of this paper is organized as follows: Section 2 describes SVS as both vector compo-sition and parsing; Section 3 shows how relational-clustering SVS subsumes PCFG-LAs; and Section 4evaluates modeling assumptions and empirical performance.2 Structured Vectorial Semantics2.1 Vector CompositionWe begin with some notation.
This paper will use boldfaced uppercase letters to indicate matrices(e.g., L), boldfaced lowercase letters to indicate vectors (e.g., e), and no boldface to indicate any single-valued variable (e.g.
i).
Indices of vectors and matrices will be associated with semantic concepts(e.g., i1, i2, .
.
.
); variables over those indices are single-value (scalar) variables (e.g., i); the contentsof vectors and matrices can be accessed by index (e.g., e[i1] for a constant, e[i] for a variable).
We willalso define an operation d(?
), which lists the elements of a column vector on the diagonal of a diagonalmatrix, i.e., d(e)[i, i]=e[i].
Often, these variables will technically be functions with arguments writtenin parentheses, producing vectors or matrices (e.g., L(l) produces a matrix based on the value of l).As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populateour vectors for now.
We can rewrite their equation (Equation 1) in SVS notation by following severalconventions.
All semantic vectors have a fixed dimensionality and are denoted e; source vectors and the296a) e(lMOD )Se0(lMOD )NPe00(lMOD )DTthee01(lID )NNengineerspulled off ...b)e00 =headwordsi ui ti ptruth???????01.1??????
?e01 =headwordsi ui ti ptruth???????001??????
?aT0 = [ .1 .1 .8 ]L0?00(lMOD) =parent0i ui ti pchild 00ip it iu??????
?0 .2 .80 0 1.1 .4 .5??????
?M(lMOD?NP ?
lMOD?DT lID?NN) =parent0i ui ti pparent 0ip it iu??????
?.2 0 00 .1 00 0 .4??????
?Figure 1: a) Syntax and semantics on a tree during decoding.
Semantic vectors e are subscripted with thenode?s address.
Relations l and syntactic categories c are constants for the example.
b) Example vectorsand matrices needed for the composition of a vector at address 0 (Section 2.2.1).target vector are differentiated by subscript; instead of context variables R and K we will use M and L:e?
= f(e?,e?,M,L) (3)Syntactic context is in the form of grammar rules M that are aware of semantic concepts; semanticknowledge is in the form of labeled dependency relationships between semantic concepts, L. Both ofthese are present and explicitly modeled as matrices in SVS?s canonical form of vector composition:e?
= M ?
d(L???
?
e?)
?
d(L???
?
e?)
?
1 (4)Here, M is a diagonal matrix that encapsulates probabilistic syntactic information, where the syntacticprobabilities depend on the semantic concept being considered.
TheLmatrices are linear transformationsthat capture how semantically relevant source vectors are to the resulting vector (e.g., L???
defines thethe relevance of e?
to e?
), with the intuition that two 1D vectors are under consideration and requirea 2D matrix to relate them.
1 is a vector of ones ?
this takes a diagonal matrix and returns a columnvector corresponding to the diagonal elements.Of note in this definition of f(?)
is the presence of matrices that operate on distributed semanticvectors.
While it is widely understood that matrices can represent transformations, relatively few haveused matrices to represent the distributed, dynamic nature of meaning composition (see Rudolph andGiesbrecht (2010) for a counterexample).2.2 Syntax?Semantics InterfaceThis section aims to more thoroughly define the way in which the syntax and semantics interact duringstructured vectorial semantic composition.
SVS will specify this interface such that the composition ofsemantic vectors is probabilistically consistent and subsumes parsing under various frameworks.
Parsinghas at times added semantic annotations that unwittingly carry some semantic value: headwords (Collins,1997) are one-word concepts that subsume the words below them; latent annotations (Matsuzaki et al,2005) are clustered concepts that touch on both syntactic and semantic information at a node.
Of course,other annotations (Ge and Mooney, 2005) carry more explicit forms of semantics.
In this light, semanticconcepts (vector indices i) and relation labels (matrix arguments l) may also be seen as annotations ongrammar trees.Let us introduce notation to make the connection with parsing and syntax explicit.
This paper willdenote syntactic categories as c and string yields as x.
The location of these variables in phrase structurewill be identified using subscripts that describe the path from the root to the constituent.1 Paths consistof left and/or right branches (indicated by ?0?s and ?1?s, respectively, as in Figure 1a).
Variables ?, ?,and ?
stand for whole paths; ?
is the path of a composed vector; and  is the empty path at the root.
Theyield x?
is the observed (sub)string that eventually results from the progeny of c?
.
Multiple trees ??
canbe constructed at ?
by stringing together grammar rules that are consistent with observed text.1For simplicity, trees are assumed to be compiled into strictly binary-branching form.2972.2.1 Lexicalized ParsingTo illustrate the definitions and operations presented in this section, we start with the concrete ?semantic?space of headwords (i.e., bilexical parsing) before moving on to a formal definition.
Our example herecorresponds to the best parse of the first two words in Figure 1a.
In this example domain, assume thatthe semantic space of concept headwords is {ipulled, ithe, iunk}, abbreviated as {ip, it, iu} where the lastconcept is a constant for infrequently-observed words.
This semantic space becomes the indices ofsemantic vectors; complete vectors e at each node of Figure 1a are shown in Figure 1b.The tree in Figure 1a contains complete concept vectors e at each node, with corresponding indicesi.
Values in these vectors (see Figure 1b) are probabilities, indicating the likelihood that a particularconcept summarizes the meaning below a node.
For example, consider e00: it produces the yield belowaddress 00 (?the?)
with probability 1, and iu may also produce ?the?
with probability 0.1.Not shown on the tree are the matrices in Figure 1b.
In the parametrized matrixM(lMOD?NP ?
lMOD?DT lID?NN), each diagonal element corresponds to the hypothesized grammar rule?sprobability, given a headword.
Similarly, the matrix L0?00(lMOD) is parametrized by the semantic contextlMOD ?
here, lMOD represents a generalized ?modifier?
semantic role.
For the semantic concept ip at ad-dress 0, the left-child modifier (address 00) could be semantic concept it with probability 0.2, or conceptiu with probability 0.8.
Finally, by adding an identity matrix for L0?01(lID) (a ?head?
semantic role) tothe quantities in Figure 1b, we would have all the components to construct the vector at address 0:e0 =??????
?.2 0 00 .1 00 0 .4????????????????????????????????????????????????M?d????????
?0 .2 .80 0 1.1 .4 .5????????????????????????????????????????????????L0?01???????01.1???????dcurlye00??
?
d????????
?1 0 00 1 00 0 1?????????????????????????????????L0?01???????001???????dcurlye01??
????????111??????
?dcurly1=i ui ti ptruth???????000.036??????????????????????????????
?e0Since the vector was constructed in syntactic and semantic context, the tree structure shown (includingsemantic relationships l) is implied by the context.2.2.2 Probabilities in vectors and matricesFormally defining the probabilities in Figure 1, SVS populates vectors and matrices by means of 5probability models (models are denoted by ?
), along with the process of composition:Syntactic model M(lc??
lc?
lc?)[i?
, i?]
=P?M(lci?
?
lc?
lc?
)Semantic model L???(l?)[i?
, i?]
=P?L(i?
?
i?
, l?
)Preterminal model e?[i?]
=P?P-Vit(G)(x?
?
lci?
), for preterm ?
(5)Root const.
model aT [i] =PpiG(lci)Any const.
model aT?[i?]
=PpiG(lci?
)These probabilities are encapsulated into vectors and matrices using a convention: column indices ofvectors or matrices represent conditioned semantic variables, row indices represent modeled variables.As an example, from Figure 1b, elements of L0?00(lMOD) represent the probability P?L(i00 ?
i0, l00).Thus, the conditioned variable i00 is shown in the figure as column indices, and the modeled i0 asrow indices.
This convention applies to the M matrix as well.
Recall that M is a diagonal matrix?
its rows and columns model the same variable.
Thus, we could rewrite P?M(lci?
?
lc?
lc?)
asP?M(lci?
?
lc?
lc?, i?)
to make a consistent probabilistic interpretation.We have intentionally left out the probabilistic definition of normal (non-preterminal) nonterminalsP?Vit(G) , and the rationale for aT vectors.
These are both best understood in the dual problem of parsing.2.2.3 Vector Composition for ParsingThe vector composition of Equation 4 can be rewritten with all arguments and syntactic information as:e?
= M(lc?
?
lc?
lc?)
?
d(L???(l?)
?
e?)
?
d(L???(l?)
?
e?)
?
1 (4?
)298a compact representation that masks the underlying consistent probability operations.
This section willexpand the vector composition equation to show its equivalence to standard statistical parsing methods.Let us say that e?[i?]
= P(x?
?
lci?
), the probability of giving a particular yield given the presentdistributed semantics.
Recall that in matrix multiplication, there is a summation over the inner dimen-sions of the multiplied objects; replacing matrices and vectors with their probabilistic interpretations andsumming in the appropriate places, each element of e?
is then:e?[i?]
= P?M(lci?
?
lc?
lc?)
?
?i?P?L(i?
?
i?
, l?)
?
P?Vit(G)(x?
?
lci?)?
?i?P?L(i?
?
i?
, l?)
?
P?Vit(G)(x?
?
lci?)
(6)This can be loosely considered the multiplication of the syntax (?M term), left-child semantics (firstsum), and right-child semantics (second sum).
The only summations are between L and e, since all othermultiplications are between diagonal matrices (similar to pointwise multiplication).We can simplify this probability expression by grouping ?M and ?L into a grammar ruleP?G(lci??
lci?
lci?
)def= P?M(lci??
lc?
lc?)
?
P?L(i?
?
i?
, l?)
?
P?L(i?
?
i?
, l?
), since they deal with every-thing except the yield of the two child nodes.
The summations are then pushed to the front:e?[i?]
= ?i?,i?P?G(lci?
?
lci?
lci?)?P?Vit(G)(x?
?
lci?)
?
P?Vit(G)(x?
?
lci?)
(7)Thus, we have a standard chart-parsing probability P(x?
?
lci?)
?
with distributed semantic concepts ?in each vector element.The use of grammar rules necessarily builds a hypothetical subtree ??
.
In a typical CKY algorithm,the tree corresponding to the highest probability would be chosen; however, we have not defined how tomake this choice for vectorial semantics.We will choose the best tree with probability 1.0, so we define a deterministic Viterbi probabilityover candidate vectors (not concepts) and context variables:P?Vit(G)(x?
?
lce?
)def= J e?
= argmaxlce?
(aT?
e?
?
PpiG(lcaT? )
?
P?Vit(G)(x ?
lce?
))K (8)where J?K is an indicator function such that J?K=1 if ?
is true, 0 otherwise.
Intuitively, the process is asfollows: we construct the vector e?
at a node, according to Eqn.
4?
; we then weight this vector againstprior knowledge about the context aT?
; the best vector in context will be chosen (the argmax).
Also, thevector at a node comes with assumptions of what structure produced it.
Thus, the last two terms in theparentheses are deterministic models ensuring that the best subtree ??
is indeed the one generated.Determining the root constituent of the Viterbi tree is the same process as choosing any other Viterbiconstituent, except that prior contextual knowledge gets its own probability model in aT .
As before,the most likely tree ?
? is the tree that maximizes the probability at the root, and can be constructedrecursively from the best child trees.
Importantly, ?
? has an associated, sentential semantic vector whichmay be construed as the composed semantic information for the whole parsed sentence.
Similar phrasalsemantic vectors can be obtained anywhere on the parse chart.These equations complete the linear algebraic definition of structured vectorial semantics.3 SVS with Relational Clusters3.1 Inducing Relational ClustersUnlike many vector space models that are based on the frequencies of terms in documents, we mayconsider frequencies of terms that occur in similar semantic relations (e.g., head lID or modifier lMOD).Reducing the dimensionality of terms in a term?context matrix will result in relationally-clustered con-cepts.
From a parsing perspective, this amounts to latent annotations (Matsuzaki et al, 2005) in l-context.299Let us re-notate the headword-lexicalized version of SVS (the example in Section 2.2.1) using hfor headword semantics, and reserve i for relationally-clustered concepts.
Treebank trees can be deter-ministically annotated with headwords h and relations l by using head rules (Magerman, 1995).
The 5SVS models ?M, ?L, ?P-Vit(G), piG, and piG can thus be obtained by counting instances and normalizing.Empirical probabilities of this kind are denoted with a tilde, whereas estimated models have a hat.Concepts i in a distributed semantic representation, however, cannot be found from annotated trees(see example concepts in Figure 2).
Therefore, we use Expectation Maximization (EM) in a variant ofthe inside-outside algorithm (Baker, 1979) to learn distributed-concept behavior.
In the M-step, the data-informed result of the E-step is used to update the estimates of ?M, ?L, and ?H (where ?H is a generlizationof ?P-Vit(G) to any nonterminal).
These updated estimates are then plugged back in to the next E-step.
Thetwo steps continually alternate until convergence or a maximum number of iterations.E-step:P?(i?
, i?, i?
?
lc?
, lc?, lc?)
= P??Out(lci?
, lch?lch?)
?
P??Ins(lch?
?
lci?)P?
(lch) (9)E(lci?
,lci?,lci?)
= P?(i?
,i?,i?
?lc?
,lc?,lc?)
?P?(lc?
,lc?,lc?)M-step:P??M(lci?
 lc?, lc?)
=?i?,i?
E(lci?
, lci?, lci?)?lci?,lci?
E(lci?
, lci?, lci?)P??L(i?
?
i?
; l?)
=?lc?
,c?,lci?
E(lci?
, lci?, lci?)?lc?
,ci?,lci?
E(lci?
, lci?, lci?)(10)P??H(h?
?
lci?)
= E(lci?
,?,?)?h?
E(lci?
,?,?
)Inside probabilities can be recursively calculated on training trees from the bottom up.
These aresimply probability sums of all subsumed subtrees (Viterbi probabilities with sums instead of maxes).Outside probabilities can also be recursively calculated from training trees, here from parent proba-bilities.
For a left child (the right-child case is similar):P?
?Out(lci?, lch?lch?)
=P??Out(lci?
, lch?lch?)
?
P??M(lci?
 lc?, lc?)?
?i?
P??L(i?
?
i?
, l?)
?
P??Ins(lch?
?
lci?)
?
P??L(i?
?
i?
, l?)
(11)Since outside probabilities signify everything but what is subsumed by the node, they carry a comple-mentary set of information to inside probabilities.
Thus, inside and outside probabilities together are anatural way to produce parent and child clustered concepts.3.2 Relational Semantic Clusters in ParsingSection 2.2.2 listed the five probability models necessary for SVS.
To define SVS with relational clusters,the estimates in Equation 10 can be used for ?M and ?L.The preterminal model is based on ?H, but it also includes some backoff for words that have not beenused as headwords.
The other two models also fall out nicely from the algorithm, though they are notexplicitly estimated in EM.
The prior probability at the root is just the base case for outside probabilities:P?piG(lci)def= P?
?Out(lci, lch?lch) (12)Prior probabilities at non-root constituents are estimated from the empirically-weighted joint probability.P?piG(lci?
)def= ?lci?,lci?P?(lci?
, lci?, lci?)
(13)With these models, a relationally-clustered SVS parser is now defined.300Cluster i1?announcement?unk 0.362was 0.173reported 0.097posted 0.036earned 0.029filed 0.024were 0.022had 0.020told 0.013approved 0.013Cluster i5?change in value?rose 0.137fell 0.124unk 0.116gained 0.063dropped 0.051attributed 0.051jumped 0.046added 0.041lost 0.039advanced 0.022Cluster i7?change possession?unk 0.381had 0.065was 0.062took 0.036bought 0.027completed 0.025received 0.024were 0.023got 0.018made 0.018acquired 0.016Figure 2: Example ?H clusters from 1,000 head-words clustered into 10 referents, after 10 EM itera-tions, for transitive past-tense verbs (VBD-argNP).0 5 10 15 20 25 30 35 400100200300400500Sentence LengthAverageParsingTime(s)Non?vectorizedVectorizedFigure 3: Speed of relationally-clustered SVSparsers, with and without vectorization.4 EvaluationSections 02?21 of the Wall Street Journal (WSJ) corpus were used as training data; Section 23 wasused as test data with reported parsing results on sentences greater than length 40.
Punctuation wasleft in for all reported evaluations.
Trees were binarized, and syntactic states were thoroughly split intosubcategorization classes.
As previously discussed, unlike tests on state-of-the-art automatically state-splitting parsers, this isolates the contribution of semantics.
The baseline 83.57 F-measure is comparableto Klein and Manning (2003) before the inclusion of head annotations.Subsequently, each branch was annotated with a head relation lID or a modifier relation lMOD ac-cording to a binarized version of headword percolation rules (Magerman, 1995; Collins, 1997), and theheadword was propagated up from its head constituent.
The most frequent headwords (e.g., h1, .
.
.
, h50)were stored, and the rest were assigned a constant, ?unk?
headword category.From counts on the binary rules of these annotated trees, the ?M, ?L, ?P-Vit(G), piG, and piG proba-bilities for headword-lexicalization SVS were obtained.
Modifier relations lMOD were deterministicallyaugmented with their syntactic context; both c and l symbols appearing fewer than 10 times in the wholecorpus were assigned ?unknown?
categories.These lexicalized models served as a baseline, but the augmented trees from which they were derivedwere also inputs to the EM algorithm in Section 3.1.
Each parameter in the model or training algorithmwas examined, with ?I ?={1,5,10,15,20} clusters, random initialization from reproducible seeds, and avarying numbers of EM iterations.The implemented parser had few adjustments from a plain CKY parser other than these vectors.
Noapproximate inference was used, with no beam for candidate parses and no re-ranking.4.1 Interpretable relational clustersFigure 2 shows example clusters for one of the headword models used, where EM clustered 1,000 head-words into 10 concepts in 10 iterations.
The lists are parts of the P??H(h?
?
lci?)
model.
As such, each ofthe 10 clusters will only produce headwords in light of some syntactic constituent.
The figure shows howdistributed concepts produce headwords for transitive past-tense verbs.
Note that the probability distri-butions for different headwords are quite uneven, again confirming that some clusters are more specific,and others are more general.Each cluster has been given a heading of its approximate meaning ?
i5, for example, mostly picksverbs that are ?change in value?
events.
With 10 clusters, we might not expect such fine-grained clusters,since pLSA-related approaches typically use several hundred for such tasks.
The syntactic context oftransitive (and therefore state-split) past-tense verbs allows for much finer-grained distinctions, whichare then predominantly semantic in nature.301a)Sec.
23, length < 40 wds LR LP Fsyntax-only baseline: 83.32 83.83 83.57headword-lex.
10hw: 83.10 83.61 83.35headword-lex.
50hw: 83.09 83.40 83.24rel?n clust.
50hw10 clust: 83.67 84.13 83.90b)Sec.
23, length < 40 wds LR LP Fbaseline1 clust 83.34 83.90 83.621000 hw5 clust, avg 83.85 84.23 84.041000 hw10 clust, avg 84.04 84.40 84.211000 hw15 clust, avg 84.15 84.38 84.261000 hw20 clust, avg 84.21 84.42 84.31Table 1: a) Unsmoothed lexicalized CKY parsers versus 10 semantic clusters.
Evaluations were runwith EM trained to 10 iterations.
b) Average dependence of parsing performance on number of semanticclusters.
Averages are taken over different random seeds, with EM running 4 or 10 iterations.4.2 Engineering considerationsWe should note that relationally-clustered SVS is feasible with respect to random initialization and speed.Four relationally-clustered SVS models (with 500 headwords clustered into 5 concepts) were trained,each having a different random initialization.
We found that the parsing F-score had a mean of 83.98and a standard deviation of 0.21 across different initializations of the model.
This indicates that thoughthere are significant difference between the models, they still outperform models without SVS (see nextsection).Also, it may seem slow to consider the set of semantic concepts and relations alongside syntax, atleast with respect to normal parsing.
The definition of SVS in terms of vectors actually mitigates thiseffect on WSJ Section 23, according to Figure 3.
Since SVS is probabilistically consistent, the parsercould be defined without vectors, but this would have the ?non-vectorized?
speed curve.
The contiguousstorage and access of information in the ?vectorized?
version leads to an efficient implementation.4.3 Comparison to LexicalizationOne important comparison to draw here is between the effectiveness of semantic clusters versusheadword-lexicalization.
For fair head-to-head comparison onWSJ Section 23, both models were vector-ized and included no smoothing or backoff.
Neither relational clusters nor lexicalization were optimizedwith backoff or smoothing.Table 1a shows precision, recall, and F-score for lexicalized models and for clustered semantic mod-els.
First, note that the 10-cluster model (in bold) improves on a syntax-only parser (top line), showingthat the semantic model is contributing useful information to the parsing task.Next, compare the 50-headword, 10-cluster model (in bold) to the line above it.
It is natural tocompare this model to the headword-lexicalized model with 50 headwords, since the same informationfrom the trees is available to both models.
The relationally-clustered model outperforms the headword-lexicalized model, showing that clustering the headwords actually improves their usefulness, despite thefact that fewer referents are used in the actual vectors.It is also interesting, then, to compare this 50-headword, 10-cluster model to a headword-lexicalizedmodel with 10 headwords.
In this case, the possible size of the grammar is equal.
Again, the relationally-clustered model outperforms plain lexicalization.
This indicates that the 10 clustered referents are muchmore meaningful than 10 headword referents for the disambiguating of syntax.4.4 Effect of Number of clustersThe final experiment on relational-clustering SVS was to determine whether performance would varywith the number of clusters.
Table 1b compares average performance (over different random initializa-tions) for numbers of clusters from 1 (a syntax-equivalent case) to 20.First, it should be noted that all of the relationally clustered models improved on the baseline.
Ran-dom initializations did not vary enough for these models to do worse than syntax alone.
For each vec-tor/domain size, in fact, the gains over syntax-only are substantial.302In addition, the table shows that average performance increases with the number of clusters.
Thisloosely positive slope means that EM is still finding useful parts of the semantic space to explore andcluster, so that the clusters remain meaningful.
However, the increase in performance with number ofclusters is likely to eventually plateau.Maximum-accuracy models were also evaluated, since each model is a full-fledged parser.
The best20-referent model obtained an F score of 84.60%, beating the syntactic baseline by almost a full absolutepoint.
Thus, finding relationally-clustered semantic output also contributes to some significant parsingbenefit.4.5 PerplexityFinally, per-word perplexities were calculated for a syntactic model and for a 5-concept relationally-clustered model.
Specific to this evaluation, following Mitchell and Lapata (2009), only the top 20,000words in WSJ Sections 02-21 were kept in training or test sentences, and the rest replaced with ?unk?
;numbers were replaced with ?num.?Sec.
23, ?unk?+?num?
Perplexitysyntax only baseline 428.94rel?n clust.
1khw?005e 371.76Table 2: Model fit as measured by perplexity.Table 2 shows that adding semantic information greatly reduces perplexity.
Since as much syntacticinformation as possible (such as argument structure) has been pre-annotated onto trees, the isolatedcontribution of interactive semantics improves on a syntax-only model model.5 ConclusionThis paper has introduced a structured vectorial semantic (SVS) framework in which vector compositionand syntactic parsing are a single, interactive process.
The framework thus fully integrates distributionalsemantics with traditional syntactic models of language.Two standard parsing techniques were defined within SVS and evaluated: headword-lexicalizationSVS (bilexical parsing) and relational-clustering SVS (latent annotations).
It was found that relationally-clustered SVS outperformed the simpler lexicalized model and syntax-only models, and that additionalclusters had a mildly positive effect.
Additionally, perplexity results showed that the integration ofdistributed semantics in relationally-clustered SVS improved the model over a non-interactive baseline.It is hoped that this flexible framework will enable new generations of interactive interpretationmodels that deal with the syntax?semantics interface in a plausible manner.ReferencesBaker, J.
(1979).
Trainable grammars for speech recognition.
In D. Klatt and J. Wolf (Eds.
), SpeechCommunication Papers for the 97th Meeting of the Acoustical Society of America, pp.
547?550.Blei, D. M., A. Y. Ng, and M. I. Jordan (2003).
Latent dirichlet alocation.
Journal of Machine LearningResearch 3, 993?1022.Charniak, E. (1996).
Tree-bank grammars.
In Proceedings of the National Conference on ArtificialIntelligence, pp.
1031?1036.Collins, M. (1997).
Three generative, lexicalised models for statistical parsing.
In Proceedings of the35th Annual Meeting of the Association for Computational Linguistics (ACL ?97).303Deerwester, S., S. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman (1990).
Indexing by latentsemantic analysis.
Journal of the American Society for Information Science 41(6), 391?407.Erk, K. and S. Pado?
(2008).
A structured vector space model for word meaning in context.
In Proceedingsof EMNLP 2008.Frege, G. (1892).
Uber sinn und bedeutung.
Zeitschrift fur Philosophie und Philosophischekritik 100,25?50.Ge, R. and R. J. Mooney (2005).
A statistical semantic parser that integrates syntax and semantics.
InNinth Conference on Computational Natural Language Learning, pp.
9?16.Gesmundo, A., J. Henderson, P. Merlo, and I. Titov (2009).
A latent variable model of synchronoussyntactic-semantic parsing for multiple languages.
In Proceedings of CoNLL, pp.
37?42.
Associationfor Computational Linguistics.Griffiths, T. L., M. Steyvers, D. M. Blei, and J.
B. Tenenbaum (2005).
Integrating topics and syntax.Advances in neural information processing systems 17, 537?544.Hofmann, T. (2001).
Unsupervised learning by probabilistic latent semantic analysis.
Machine Learn-ing 42(1), 177?196.Klein, D. and C. D. Manning (2003).
Accurate unlexicalized parsing.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguistics, Sapporo, Japan, pp.
423?430.Koo, T., X. Carreras, and M. Collins (2008).
Simple semi-supervised dependency parsing.
In Proceed-ings of the 46th Annual Meeting of the ACL, Volume 8.
Citeseer.Lin, D. (1998).
An information-theoretic definition of similarity.
In Proceedings of the 15th InternationalConference on Machine Learning, Volume 296304.MacDonald, M. C., N. J. Pearlmutter, and M. S. Seidenberg (1994).
The lexical nature of syntacticambiguity resolution.
Psychological Review 101(4), 676?703.Magerman, D. (1995).
Statistical decision-tree models for parsing.
In Proceedings of the 33rd AnnualMeeting of the Association for Computational Linguistics (ACL?95), Cambridge, MA, pp.
276?283.Matsuzaki, T., Y. Miyao, and J. Tsujii (2005).
Probabilistic CFG with latent annotations.
In Proceedingsof the 43rd Annual Meeting on Association for Computational Linguistics, pp.
75?82.
Association forComputational Linguistics.Mitchell, J. and M. Lapata (2008).
Vector-based models of semantic composition.
In Proceedings ofACL-08: HLT, Columbus, OH, pp.
236?244.Mitchell, J. and M. Lapata (2009).
Language Models Based on Semantic Composition.
In Proceedingsof the 2009 Conference on Empirical Methods in Natural Language Processing, pp.
430?439.Pado?, S. and M. Lapata (2007).
Dependency-based construction of semantic space models.
Computa-tional Linguistics 33(2), 161?199.Petrov, S., L. Barrett, R. Thibaux, and D. Klein (2006).
Learning accurate, compact, and interpretabletree annotation.
In Proceedings of the 44th Annual Meeting of the Association for ComputationalLinguistics (COLING/ACL?06).Rudolph, S. and E. Giesbrecht (2010).
Compositional matrix-space models of language.
In Proceedingsof ACL 2010.
Association for Computational Linguistics.Taskar, B., E. Segal, and D. Koller (2001).
Probabilistic classification and clustering in relational data.In IJCAI?01: Proceedings of the 17th international joint conference on Artificial intelligence, SanFrancisco, CA, USA, pp.
870?876.
Morgan Kaufmann Publishers Inc.304
