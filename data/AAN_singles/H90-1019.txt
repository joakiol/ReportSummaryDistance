Session 5: Overview of the ATIS SystemDavid S. Pallett, ChairNational Institute of Standards and TechnologyBldg.
225, Rm A216Gaithersburg, MD 20899Two Sessions were devoted to introducing the DARPASLS Common Task: the Air Travel Information System(ATIS).
The first of these (in the late afternoon) providedan overview of the ATIS task and presented a summary ofresults reported to NIST in the first of a projected series ofATIS benchmark tests, while the second (immediately afterdinner) served to describe different approaches in im-plementing the common task at several sites: BBN, CMU,MIT/LCS, SRI and Unisys.
The two sessions were filledwith a sense of excitement and appreciation of theprecedent-setting nature of these tests, and revealed strongdifferences of opinion over details of implementation f thecommon task and implementation f the test, as well asfrustration with the limitations on available time and theamount of ATIS domain material at the present time.The Chairperson oted that in some sense the DARPASLS community has recently been involved in a "construc-tion project" - -  building a common task domain that is torely on a common relational (knowledge) database fordatabase queries, building a speech corpus that is to beused for system development and evaluation, and (perhapsmost significantly in the context of these sessions) buildingan evaluation methodology for DARPA spoken languagesystems.
Such a construction project is not without pres-sures of scheduling, various constraints and unknowndangers!The first paper, presented by Patti Price of SRI, outlinedsome of the issues and rationale behind selection of the airtravel domain for a DARPA SLS common task.
One im-portant function of the common task is to serve to limit thedomain-specific-sub-language for the DARPA spoken lan-guage systems.
Patti noted that choice of air travel plan-ning makes use of data derived from the on-line OfficialAirline Guide put into a relational format.
It offers theadvantages ofhaving been used by "hundreds of thousandsof people, providing a wide pool of users familiar with thedomain, and the domain is rich, interesting, and can bescaled with the technology" \[1\].Charles Hemphill, the "Wizard" at TI responsible forcollection of the ATIS Pilot Corpus, next described theprocess of corpus collection at TI.
Spontaneous speechwas collected using a Wizard-of-Oz" simulation.
Follow-ing collection of the speech, TI developed transcriptions,classified the queries, generated reference SQL expres-sions, and obtained reference answers, prior to sending thecorpus to NIST for distribution to the SLS community.Charles noted that our collective xperience with the ATISPilot Corpus has demonstrated that "objective valuation ofspoken language systems is both possible and beneficial"\[2\].Lyn Bates, of BBN, presented a paper describing thedevelopment of the evaluation methodology proposed forspoken language systems and implemented for this meeting\[3\].
Boisen et al had outlined such a procedure at theOctober 1989 DARPA Workshop \[4\], and Lyn outlinedsome of the details of the present implementation for theATIS domain.
Strengths of the methodology noted in-clude: (1) forcing agreement on the meaning of criticalterms, (2) being objective \[with some caveats\], (3) being\[largely\] automated, and (4) extendibility to other domainsand to account for context-dependency and limiteddialogue.
Weaknesses that have been noted include: (1)not distinguishing between "acceptable" answers and "verygood answers", (2) potentially crediting systems that "over-answer" for answers to specific questions (by virtue ofproviding what some have termed "the kitchen sink" inresponse to some questions - -  in the hope that the correctanswer will be found somewhere in the response), and (3)not being able to tell if the right answer was gotten for thewrong reason.Lynnette Hirschman outlined a proposal for automaticevaluation of discourse \[5\].
The proposal, termed "BeyondClass A", suggests procedures that might be used to permitevaluation of context-dependent queries.
To extend thepresent sconng methodology to evaluate context-dependentsentences, a "canonical display format" could be defined,with reference to this display as an additional input for"resynchronization" of the SLS systems by providing "thefull context".Prior to this meeting, NIST had distributed a set of 93test ATIS domain queries and received results (more-or-less in "canonical answer format") for a total of 7 systemsfrom 5 sites.
These results had been scored at NIST.
In thelast presentation before dinner, results were distributed byNIST for the preliminary scoring of these results \[6\].
BillFisher reviewed NIST's experience in administering thesetests.
Bill noted some inconsistencies in different sites'implementations of the tests, and outlined steps NIST hadtaken to make the scoring software more tolerant of format-related errors.
Several sites had opined that hree of the testqueries were ambiguous, and NIST agreed to delete these 3from the original test set of 93, leaving a set of 90 "officialtest queries".89Following the NIST presentation, a break for dinnertook place.REFERENCES\[1\] Price, P., Evaluation of Spoken Language Systems: theATIS Domain (in this Proceedings).\[2\] Hemphill, C. T., Godfrey, J. J. and Doddington, G. R.,The ATIS Spoken Language Systems Pilot Corpus (in thisProceedings).\[3\] Bates, M. and Boisen, S., Developing an EvaluationMethodology for Spoken Language Systems (in thisProceedings).\[4\] Boisen, S. et al, A Proposal for SLS Evaluation inProceedings of the Second DARPA Speech and NaturalLanguage Workshop (Cape Cod, MA) October, 1989.\[5\] Hirschman, L. et al, Beyond Class A: A Proposal forAutomatic Evaluation of Discourse (in this Proceedings).\[6\] Pallett, D. S. et al, DARPA ATIS Test Results: June1990 (in this Proceedings).90
