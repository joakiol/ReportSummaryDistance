Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 20?29,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsFull Machine Translation for Factoid Question AnsweringCristina Espan?a-Bonet and Pere R. ComasTALP Research CenterUniversitat Polite`cnica de Catalunya (UPC){cristinae,pcomas}@lsi.upc.eduAbstractIn this paper we present an SMT-based ap-proach to Question Answering (QA).
QAis the task of extracting exact answers inresponse to natural language questions.
Inour approach, the answer is a translation ofthe question obtained with an SMT system.We use the n-best translations of a givenquestion to find similar sentences in thedocument collection that contain the realanswer.
Although it is not the first time thatSMT inspires a QA system, it is the firstapproach that uses a full Machine Transla-tion system for generating answers.
Our ap-proach is validated with the datasets of theTREC QA evaluation.1 IntroductionQuestion Answering (QA) is the task of extract-ing short, relevant textual answers from a givendocument collection in response to natural lan-guage questions.
QA extends IR techniques be-cause it outputs concrete answers to a questioninstead of references to full documents which arerelevant to a query.
QA has attracted the attentionof researchers for some years, and several pub-lic evaluations have been recently carried in theTREC, CLEF, and NTCIR conferences (Dang etal., 2007; Pen?as et al, 2011; Sakai et al, 2008).All the example questions of this paper are ex-tracted from the TREC evaluations.QA systems are usually classified according towhat kind of questions they can answer; factoid,definitional, how to or why questions are treated ina distinct way.
This work focuses on factoid ques-tions, that is, those questions whose answers aresemantic entities (e.g., organisation names, per-son names, numbers, dates, objects, etc.).
For ex-ample, the question Q1545: What is a female rab-bit called?
is factoid and its answer, ?doe?, is asemantic entity (although not a named entity).Factoid questions written in natural languagecontain implicit information about the relationsbetween the concepts expressed and the expectedoutcomes of the search, and QA explicitly ex-ploits this information.
Using an IR engine tolook up a boolean query would not consider therelations therefore losing important information.Consider the question Q0677: What was the nameof the television show, starring Karl Malden, thathad San Francisco in the title?
and the candi-date answer A.
In this question, two types ofconstraints are expressed over the candidate an-swers.
One is that the expected type of A is akind of ?television show.?
The rest of the ques-tion indicates that ?Karl Malden?
is related to Aas being ?starred?
by, and that ?San Francisco?is a substring of A.
Many factoid questions ex-plicitly express an hyponymy relation about theanswer type, and also several other relations de-scribing its context (i.e.
spatial, temporal, etc.
).The QA problem can be approached from sev-eral points of view, ranging from simple surfacepattern matching (Ravichandran and Hovy, 2002),to automated reasoning (Moldovan et al, 2007)or supercomputing (Ferrucci et al, 2010).
Inthis work, we propose to use Statistical MachineTranslation (SMT) for the task of factoid QA.
Un-der this perspective, the answer is a translation ofthe question.
It is not the first time that SMT isused for QA tasks, several works have been us-ing translation models to determine the answers(Berger et al, 2000; Cui et al, 2005; Surdeanuet al, 2011).
But to our knowledge this is the first20approach that uses a full Machine Translation sys-tem for generating answers.The paper is organised as follows: Section 2reviews the previous usages of SMT in QA, Sec-tion 3 reports our theoretical approach to the task,Section 4 describes our QA system, Section 5presents the experimental setting, Section 6 anal-yses the results and Section 7 draws conclusions.2 Translation Models in QAThe use of machine translation in IR is not new.Berger and Lafferty (1999) firstly propose a prob-abilistic approach to IR based on methods ofSMT.
Under their perspective, the human user hasan information need that is satisfied by an ?ideal?theoretical document d from which the user drawsimportant query words q.
This process can bemirrored by a translation model: given the queryq, they find the documents in the collection withwords a most likely to translate to q.
The keyingredient is the set of translation probabilitiesp(q|a) from IBM model 1 (Brown et al, 1993).In a posterior work, Berger et al also intro-duce the formulation of the QA problem in termsof SMT (Berger et al, 2000).
They estimate thelikelihood that a given answer containing a wordai corresponds to a question containing wordqj .
This estimation relies on an IBM model 1.The method is tested with a collection of closed-domain Usenet and call-center questions, whereeach question must be paired with one of therecorded answers.
Soricut and Brill (2004) im-plement a similar strategy but with a richer for-mulation and targeted to open-domain QA.
Givena question Q, a web-search engine is used toretrieve 3-sentence-long answer texts from FAQpages.
These texts are later ranked with the like-lihood of containing the answer to Q, and thislikelihood is estimated via a noisy-channel archi-tecture.
The work of Murdock and Croft (2005)applies the same strategy to TREC data.
Theyevaluate the TREC 2003 passage retrieval task.In this task, the system must output a single sen-tence containing the answer to a factoid ques-tion.
Murdock and Croft tackle the length dis-parity in question-answer pairs and show that thisMT-based approach outperforms traditional querylikelihood techniques.Riezler et al (2007) define the problem of an-swer retrieval from FAQ and social Q/A websitesas a query expansion problem.
SMT is used totranslate the original query terms to the languageof the answers, thus obtaining an expanded list ofterms usable in standard IR techniques.
They alsouse SMT to perform question paraphrasing.
In thesame context, Lee et al (2008) study methods forimproving the translation quality removing noisefrom the parallel corpus.SMT can be also applied to sentence represen-tations different than words.
Cui et al (2005)approach the task of passage retrieval for QAwith translations of dependency parsing relations.They extract the sequences of relations that linkeach pair of words in the question and, using theIBM translation model 1, score their similarityto the relations extracted from the candidate pas-sage.
Thus, an approximate relation matchingscore is obtained.
Surdeanu et al (2011) extendthe scope of this approach by combining togetherthe translation probabilities of words, dependencyrelations, and semantic roles in the context of an-swer searching in FAQ collections.The works we have described so far usearchives of question-answer pairs as informationsources.
They are really doing document re-trieval and sentence retrieval rather than questionanswering, because every document/sentence isknown to be the answer of a question written inthe form of an answer, and no further informationextraction is necessary, they just select the bestanswer from a given pool of answers.
The dif-ference with a standard IR task is that these sys-tems are not searching for relevant documents butfor answer documents.
In contrast, Echihabi andMarcu (2003) introduce an SMT-based methodfor extracting the concrete answer in factoid QA.First, they use a standard IR engine to retrievecandidate sentences and process them with a con-stituent parser.
Then, an elaborated process sim-plifies these parse trees converting them into se-quences of relevant words and/or syntactic tags.This process reduces the length disparity betweenquestions and answers.
For the answer extraction,a special tag marking the position of the answeris sequentially added to all suitable positions inthe sentence, thus yielding several candidate an-swers for each sentence.
Finally, each answer israted according to its likelihood of being a trans-lation of the question, according to an IBM model4 trained on a corpus of TREC and web-basedquestion-answer pairs.With the exception of the query expansion ap-21proaches (Riezler et al, 2007), all works dis-cussed here use some form of noisy-channelmodel (translation model and target languagemodel) but do not perform the decoding part ofthe SMT process to generate translations, nor usethe rich set of features of a full SMT.
In fact, theformulation of the noisy-channel in these workshas very few differences with pure language mod-elling approaches to QA like the one of Heie et al(2011), where two different models for retrievaland filtering are learnt from a corpus of question-answer pairs.3 Question-to-Answer TranslationThe core of our QA system is an SMT system forthe Question-to-Answer language pair.
In SMT,the best translation for a given source sentence isthe most probable one, and the probability of eachtranslation is given by the Bayes theorem.
In ourcase, the source sentence corresponds to the ques-tion Q and the target or translation is the sentencecontaining the answer A.
With this correspon-dence, the fundamental equation of SMT can bewritten as:A(Q) = A?
= argmaxA P (A|Q)= argmaxA P (Q|A)P (A), (1)where P (Q|A) is the translation model and P (A)is the language model, and each of them can beunderstood as the sum of the probabilities for eachof the segments or phrases that conform the sen-tence.
The translation model quantifies the appro-priateness of each segment of Q being answeredby A; the language model is a measure of the flu-ency of the answer sentence and does not take intoaccount which is the question.
Since we are in-terested in identifying the concrete string that an-swers the question and not a full sentence, thisprobability is not as important as it is in the trans-lation problem.The log-linear model (Och and Ney, 2002), ageneralisation of the original noisy-channel ap-proach (Eq.
1), estimates the final probability asthe logarithmic sum of several terms that dependon both the question Q and the answer sentenceA.
Using just two of the features, the model re-produces the noisy-channel approach but writtenin this way one can include as many features asdesired at the cost of introducing the same numberof free parameters.
The model in its traditionalform includes 8 terms:A(Q) = A?
= argmaxA logP (A|Q) =+ ?lm logP (A) + ?d logPd(A,Q)+ ?lg log lex(Q|A) + ?ld log lex(A|Q)+ ?g logPt(Q|A) + ?d logPt(A|Q)+ ?ph log ph(A) + ?w logw(A) , (2)where P (A) is the language model probabil-ity, lex(Q|A) and lex(A|Q) are the generativeand discriminative lexical translation probabilitiesrespectively, Pt(Q|A) the generative translationmodel, Pt(A|Q) the discriminative one, Pd(A,Q)the distortion model, and ph(A) and w(A) corre-spond to the phrase and word penalty models.
Westart by using this form for the answer probabil-ity and analyse the importance and validity of theterms in the experiments Section.
The ?
weights,which account for the relative importance of eachfeature in the log-linear probabilistic model, arecommonly estimated by optimising the translationperformance on a development set.
For this opti-misation one may use Minimum Error Rate Train-ing (MERT) (Och, 2003) where BLEU (Papineniet al, 2002) is the reference evaluation.Once the weights are determined and the prob-abilities estimated from a corpus of question-answer pairs (a parallel corpus in this task), a de-coder uses Eq.
2 to score the possible outputs andto find the best answer sentence given a questionor, in general, an n-best list of answers.This formulation, although possible from anabstract point of view, is not feasible in prac-tice.
The corpus from which probabilities are es-timated is finite, and therefore new questions maynot be represented.
There is no chance that SMTcan generate ex nihilo the knowledge necessary toanswer questions such as Q1201: What planet hasthe strongest magnetic field of all the planets?.So, rather than generating answers via translation,we use translations as indicators of the sentencecontext where an answer can be found.
Contexthere has not only the meaning of near words butalso a context at a higher level of abstraction.To achieve this, we use two different represen-tations of the question-answer pairs and two dif-ferent SMT models in our QA system.
We callLevel1 representation the original strings of textof the question-answer pairs.
The Level2 repre-sentation, that aims at being more abstract, moregeneral and more useful in SMT, is constructed22applying this sequence of transformations: 1)Quoted expressions in the question are identified,paired with their counterpart in the answer (incase any exists) and substituted by a special tagQUOTED.
2) Each named entity is substitutedby its entity class (e.g., ?Karl Malone?
by PER-SON).
3) Each noun and verb is substituted bytheir WordNet supersense1 (e.g.
?nickname?
byCOMMUNICATION).
4) Any remaining word,such as adjectives, adverbs and stop words, is leftas is.
Additionally, in the answer sentence string,the correct answer entity is substituted by a spe-cial tag ANSWER.
An example of this annotationis given in Figure 1.An SMT system trained with Level1 exampleswill translate Q to answer sentences with vocab-ulary and structure similar to the learning exam-ples.
The Level2 system will translate to a mix ofnamed entities, WordNet supersenses, bare words,and ANSWER markers that represent the abstractstructure of the answer sentence.
We call patternsto the Level2 translations.
The rationale of thisprocess is that the SMT model can learn the con-text where answers appear depending of the struc-ture of the question.
The obtained translationsfrom both levels can be searched in the documentcollection to find sentences that are very similar.Note that in Level2, the vocabulary size ofthe question-answer pairs is dramatically reducedwith respect to the original Level1 sentences, asseen in Table 2.
Thus, the sparseness is reduced,and the translation model gains in coverage; pat-terns are also easier to find than Level1 sentences,and give flexibility and generality to the transla-tion.
And the most important feature, patternscapture the context of the answer, pinpointing itwith accuracy.These Level1 and Level2 translations are thecore of our QA system that is presented in the fol-lowing Section.4 The Question Answering SystemOur QA system is a pipeline of three modules.In the first one, the question is analysed and an-notated with several linguistic processors.
Thisinformation is used by the rest of the modules.In the second one, relevant documents are ob-1WordNet noun synsets are organised in 26 semantic cat-egories based on logical groupings, e.g., ARTIFACT, ANI-MAL, BODY, COMMUNICATION.
.
.
The verbs are organ-ised in 15 categories.
(Fellbaum, 1998)Level1 Q: What is Karl Malone?s nickname ?Level1 A: Malone , whose overall consistency has earnedhim the nickname ANSWER , missed both of them with nineseconds remaining .Level2 Q: What STATIVE B-PERSON ?s COMMUNICA-TION ?Level2 A: B-PERSON , whose overall ATTRIBUTE POS-SESSION POSSESSION him the COMMUNICATIONANSWER , PERCEPTION both of them with B-NUM TIMECHANGE .Figure 1: Example of the two annotation levels used.tained from the document collection with straight-forward IR techniques and a list of candidate an-swers is generated.
Finally, these candidate an-swers are filtered and ranked to obtain a final listof proposed answers.
This pipeline is a commonarchitecture for a simple QA system.4.1 Question AnalysisQuestions are processed with a tokeniser, a POStagger, a chunker, and a NERC.
Besides, eachword is tagged with its most frequent sense inWordNet.
Then, a maximum-entropy classi-fier determines the most probable expected an-swer types for the question (EAT).
This classi-fier is built following the approach of Li and Roth(2005), it can classify questions into 53 differentanswer types and belongs to our in-house QA sys-tem.
Finally, a weighted list of relevant keywordsis extracted from the question.
Their saliences areheuristically determined: the most salient tokensare the quoted expressions, followed by namedentities, then sequences of nouns and adjectives,then nouns, and finally verbs and any remainingnon-stop word.
This list is used in the candidateanswer generation module.4.2 Candidate Answer GenerationThe candidate answer generation comprises twosteps.
First a set of passages is retrieved from thedocument collection, and then the candidate an-swers are extracted from the text.For the retrieval, we have used the passageretrieval module of our in-house QA system.The passage retrieval algorithm initially createsa boolean query with all nouns and more salientwords, and sets a threshold t to 50.
It uses theLucene IR engine2 to fetch the documents match-2http://lucene.apache.org23ing the current query and a subsequent passageconstruction module extracts passages as docu-ment segments where two consecutive keywordoccurrences are separated by at most t words.If too few or too many passages are obtainedthis way, a relaxation procedure is applied.
Theprocess iteratively adjusts the salience level ofthe keywords used in the query by dropping lowsalient words when too few are obtained or addingthem when too many, and it also adjusts theirproximity threshold until the quality of the recov-ered information is satisfactory (see ?)
for furtherdetails).When the passages have been gathered, theyare split into sentences and processed with POStagging, chunking and a NERC.
The candidate an-swer list is composed of all named entities andall phrases containing a noun.
Each candidate isassociated to the sentence it has been extractedfrom.4.3 Answer RankingThis module selects the best answers from thecandidates previously generated.
It employs threefamilies of scores to rank them.Context scores B and R: The n-best list ofLevel2 question translations is generated.
In thisstep retrieved sentences are also transformed tothe Level2 representation.
Then, each candidateanswer is replaced by the special ANSWER tag inthe associated sentence, thus, each sentence has aunique ANSWER tag, as in the training examples.Finally, each candidate is evaluated assessing thesimilarity of the source sentence with the n-besttranslations.For this assessment we use two different met-rics.
One of them is a lexical metric commonlyused in machine translation, BLEU (Papineni etal., 2002).
A smoothed version is used to evalu-ate the pairs at sentence level yielding the score B.The other metric is ROUGE (Lin and Och, 2004),here named R. We use the skip-bigram overlap-ping measure with a maximum skip distance of4 unigrams (ROUGE-S4).
Contrary to BLEU,ROUGE-S does not require consecutive matchesbut is still sensitive to word order.Both BLEU and ROUGE are well-known met-rics that are useful for finding partial matchings inlong strings of words.
Therefore it is an easy wayof implementing an approximated pattern match-ing algorithm with off-the-shelf components.Although these scores can determine if a sen-tence is a candidate for asserting a certain prop-erty of a certain object, they do not have the powerto discriminate if these objects are the actually re-quired by the question.
Level2 representation isvery coarse and, for example, treats all named en-tities of the same categories as the same word.Thus, it is prone to introduce noise in the formof totally irrelevant answers.
For example, con-sider the questions Q1760: Where was C.S.
Lewisborn?
and Q1519: Where was Hans ChristianAnderson born?.
Both questions have the sameLevel2 representation: Where STATIVE PERSONSTATIVE?, and the same n-best list of transla-tions.
Any sentence stating the birthplace (or evendeathplace) of any person is equally likely to bethe correct answer of both questions because thelexicalisation of Lewis and Anderson is lost.On the other hand, B and R also show anotherlimitation.
Since they are based on n-gram match-ing, they cannot be discriminative enough whenthere is only one different token between options,and that happens when a same sentence has differ-ent candidates for the answer.
In this case the sys-tem would be able to distinguish among answersentences but then all the variations with the an-swer in a different position would have too muchsimilar scores.
In order to mitigate these draw-backs, we consider two other scores.Language scores Lb, Lr, Lf : To alleviate thediscriminative problem of the context matchingmetrics, we calculate the same B and R scoresbut with Level1 translations and the original lexi-calised question.
These are the Lb and Lr scores.Additionally, we introduce a new score Lf thatdoes not take into account the n-gram structureof the sentences: after the n-best list of Level1question translations is generated, the frequencyof each word present in the translations is com-puted.
Then, the words in the candidate answersentence are scored according to their normalisedfrequency in the translations list and added up to-gether.
This score lies in the [0, 1] range.Expected answer type score E: This scorechecks if the type of the answer we are evalu-ating matches the expected types we have deter-mined in the question analysis.
For this task, theexpected answer types are mapped to named enti-ties and/or supersenses (e.g., type ENTY:product24is mapped to ARTIFACT).
If the candidate answeris a named entity of the expected type, or con-tains a noun of the expected supersense, then thiscandidate receives a score E equal to the confi-dence of the question classification (the scores ofthe ME classifier have been previously normalisedto probabilities).These three families of scores can be combinedin several ways in order to produce a ranked listof answers.
In Section 6 the combination methodsare discussed.5 Experiments5.1 Training and Test CorporaWe have used the datasets from the QuestionAnswering Track of the TREC evaluation cam-paigns3 ranging from TREC-9 to TREC-16 in ourexperiments.
These datasets provide both a robusttestbed for evaluation, and a source of question-answer pairs to use as a parallel corpus for train-ing our SMT system.
Each TREC evaluationprovides a collection of documents composed ofnewspaper texts (three different collections havebeen used over the years), a set of new ques-tions, and an answer key providing both the an-swer string and the source document.
Descrip-tion of these collections can be found in the TRECoverviews (Voorhees, 2002; Dang et al, 2007).We use the TREC-11 questions for test pur-poses, the remaining sets are used for training un-less some parts of TREC-9, TREC-10 and TREC-12 that are kept for fitting the weights of our SMTsystem.
To gather the SMT corpus, we select allthe factoid questions whose answer can be foundin the documents and extract the full sentence thatcontains the answer.
With this methodology, aparallel corpus with 12,335 question-answer pairsis obtained.
We have divided it into two subsets:the pairs with only a single answer found in thedocuments are used for the development set, andthe remaining pairs (i.e.
having multiple occur-rences of the correct answer) are used for train-ing.
The test set are the 500 TREC-11 questions,452 out of them have a correct answer in the doc-uments.
The numbers are summarised in Table 1.In order to obtain the Level2 representation ofthese corpora, the documents and the test setsmust be annotated.
For the annotation pipeline3http://trec.nist.gov/data/qamain.htmlQ A TRECsTrain 2264 12116 9,10,12,13,14,15,16Dev 219 219 9,10,12Test 500 2551 11Table 1: Number of Questions and Answers in our datasets.
The number of TREC evaluation from which areobtained is indicated.Tokens VocabularyQ A Q ATrainL1 97028 393978 3232 32013TrainL2 91567 373008 540 9130Table 2: Statistics for the 12,116 Q-A pairs in the train-ing corpus according to the annotation level.we use the TnT POS tagger (Brants, 2000),WordNet (Fellbaum, 1998), the YamCha chun-ker (Kudo and Matsumoto, 2003), the StanfordNERC (Finkel et al, 2005), and an in-house tem-poral expressions recogniser.Table 2 shows some statistics for the parallelcorpus and the two different levels of annotation.From the SMT point of view the corpus is smallin order to estimate the translation probabilities ina reliable way but, as stated before, Level2 repre-sentation diminishes the vocabulary considerablyand alleviates the problem.5.2 SMT systemThe statistical system is a state-of-the-art phrase-based SMT system trained on the previouslyintroduced corpus.
Its development has beendone using standard freely available software.The language model is estimated using interpo-lated Kneser-Ney discounting with SRILM (Stol-cke, 2002).
Word alignment is done withGIZA++ (Och and Ney, 2003) and both phraseextraction and decoding are done with the Mosespackage (Koehn et al, 2007).
The model weightsare optimised with Moses?
script of MERTagainst the BLEU evaluation metric.For the full model, we consider the languagemodel, direct and inverse phrase probabilities, di-rect and inverse lexical probabilities, phrase andword penalties, and a non-lexicalised reordering.5.3 QA systemThe question answering system has three differ-ent modules as explained in Section 4.
For the25T1 T50 MRRQA 0.006 (4) 0.206 (14) 0.024 (4)SR 0.066 (8) 0.538 (9) 0.142 (8)Upper bound 0.677 0.677 0.677Table 3: Mean and standard deviation for 1000 real-isations of the random baseline for QA and SR. Theupper bound is also shown.first module, questions are annotated using thesame tools introduced in the corpora Section.
Thesecond module generates 2,866,098 candidate an-swers (373,323 different sentences), that is to say,a mean of 5,700 answers per question (750 sen-tences per question).
These candidates are madeavailable to the third module resulting in the ex-periments that will be discussed in Section 6.The global QA system performance is evalu-ated with three measures.
T1 is a measure ofthe system?s precision and gives the percentageof correct answers in the first position; T50 givesthe number of correct answers in the first 50 po-sitions, in some cases that corresponds to all can-didate answers; finally the Mean Reciprocal Rank(MRR) is a measure of the ranking capability ofthe system and is estimated as the mean of the in-verse ranking of the first correct answer for everyquestion: MRR= Q?1?i rank?1i .6 Results AnalysisGiven the set of answers retrieved by the candi-date answer generation module, a na?
?ve baselinesystem is estimated by selecting randomly 50 an-swers for each of the questions.
Table 3 showsthe mean of the three measures after applying thisrandom process 1000 times.
The upper boundof this task is the oracle that selects always thecorrect answer/sentence if it is present in the re-trieved passages.
An answer is considered correctif it perfectly matches the official TREC?s answerkey and a sentence is correct if it contains a cor-rect answer.
The random baseline has a precisionof 0.6%.We also evaluate a second task, sentence re-trieval for QA (SR).
In this task, the system hasto provide a sentence that contains the answer, butnot to extract it.
Within our SMT approach, bothtasks are done simultaneously, because the answeris extracted according to its context sentence.
Arandom baseline for this second task, where onlyQA SRMetric T1 T50 MRR T1 T50 MRRB 0.018 0.292 0.049 0.084 0.540 0.164R 0.018 0.283 0.045 0.119 0.608 0.209B+R 0.022 0.294 0.053 0.097 0.573 0.180BR 0.027 0.294 0.057 0.137 0.591 0.211Table 4: System performance using an SMT that gen-erates a 100-best list, uses a 5-gram LM and all thefeatures of the TM.1st best: The B-ORGANIZATION B-LOCATION ,B-DATE ( B-ORGANIZATION ) - B-PERSON , whoseCOMMUNICATION STATIVE ?
ANSWER .
?50th best: The ANSWER ANSWER , B-DATE ( B-ORGA-NIZATION ) - B-PERSON , the PERSON of ANSWER, the most popular ARTIFACT , serenely COGNITIONCOMMUNICATION .100th best: The B-LOCATION , B-DATE ( B-ORGANIZA-TION ) - B-PERSON , the PERSON of ANSWER , COM-MUNICATION B-LOCATION ?s COMMUNICATION .Figure 2: Example of patterns found in an n-best list.full sentences without marked answers are takeninto account, can also be read in Table 3.We begin this analysis studying the perfor-mance of the SMT-based parts alone.
Table 4shows the results when using an SMT decoderthat generates a 100-best list, uses a 5-gram lan-guage model and all the features of the transla-tion model.
An example of the generated patternsin Level2 representation can be found in Figure 2for the question of Figure 1, Q1565: What is KarlMalone?s nickname?.Candidate answer sentences are ranked accord-ing to the similarity with the patterns generated bytranslation as measured by BLEU (B), ROUGE-S4 (R) or combinations of them.
To calcu-late these metrics the n-best list with patterns isconsidered to be a list of reference translations(Fig.
2) to every candidate (Fig.
1).
In general,a combination of both metrics is more powerfulthan any of them alone and the product outper-forms the sum given that in most cases BLEU islarger than ROUGE and smooths its effect.
Theinclusion of the SMT patterns improves the base-line but it does not imply a quantum leap.
T1 isat least three times better than the baseline?s onebut still the system answers less than a 3% of thequestions.
In the first 50 positions the answer is26SMT Features T1 T50 MRRLex, LM5, 100-best 0.027 0.294 0.057noLex, LM5, 100-best 0.015 0.281 0.045Lex, LM3, 100-best 0.015 0.257 0.041Lex, LM7, 100-best 0.033 0.288 0.050Lex, LM5, 10-best 0.024 0.310 0.056Lex, LM5, 1000-best 0.027 0.301 0.061Lex, LM5, 10000-best 0.011 0.290 0.045Table 5: System performance with different combina-tions of the SMT features used in decoding.
BR is themetric used to score the answers.found a 30% of the times.
In the sentence re-trieval task, results grow up to 14% and 59% re-spectively.
Its difference between tasks shows oneof the limitations of these metrics commented be-fore, they are not discriminative enough when theonly difference among options is the position ofthe ANSWER tag inside the sentence.
This is theempirical indication of the need for a score likeE.
On the other hand, each question has a meanof 5,732 candidate answers, and although T50 isnot a significant measure, its good results indicatethat the context scores metrics are doing their job.The highest T50, 0.608, is reached by R and it isvery close to the upper bound 0.667.Taking BR as a reference measure, we investi-gate the impact of three features of the SMT inTable 5.
Regarding the length of the languagemodel used in the statistical translation, there isa trend to improve the accuracy with longer lan-guage models (T1 is 0.015 for a LM3, 0.027 forLM5 and 0.033 for LM7 with the product of met-rics) but recall is not very much affected and thebest values are obtained for LM5.Second, the number of features in the trans-lation model indicates that the best scores arereached when one reproduces the same numberof features as a standard translation system.
Thatis, all of the measures when the lexical trans-lation probabilities are ignored are significantlylower than when the eight features are used.
Ina counterintuitive way, the token to token transla-tion probability helps to improve the final systemalthough word alignments here can be meaning-less or nonexistent given the difference in lengthand structure between question and answer.Finally, the length of the n-best list is not a de-cisive factor to take into account.
Since the ele-QA SRMetric T1 T50 MRR T1 T50 MRRLf 0.016 0.286 0.046 0.137 0.605 0.236Lb 0.022 0.304 0.054 0.100 0.581 0.192Lr 0.018 0.326 0.060 0.131 0.627 0.225Lbrf 0.038 0.330 0.079 0.147 0.622 0.238E 0.044 0.373 0.096 0.058 0.579 0.142ELbrf 0.018 0.293 0.048 0.118 0.623 0.214BLbrf 0.051 0.337 0.091 0.184 0.616 0.271RLbrf 0.033 0.346 0.069 0.191 0.618 0.279BRLbrf 0.042 0.350 0.082 0.182 0.616 0.273(B+R)Lbrf 0.044 0.346 0.085 0.187 0.618 0.273BE 0.035 0.384 0.084 0.086 0.579 0.179RE 0.035 0.377 0.086 0.131 0.630 0.228BRE 0.049 0.377 0.098 0.135 0.608 0.220(B+R)E 0.040 0.386 0.091 0.102 0.596 0.196BELbrf 0.093 0.379 0.137 0.200 0.621 0.283RELbrf 0.071 0.377 0.123 0.208 0.619 0.294BRELbrf 0.091 0.379 0.132 0.200 0.622 0.287(B+R)ELbrf 0.100 0.377 0.141 0.204 0.621 0.286Table 6: System performance according to three dif-ferent ranking strategies: context score (B and R), thelanguage scores (Lx) and EAT type checking (E).ments in a n-best list usually differ very little, andthis is even more important for a system with areduced vocabulary, increasing the size of the listdoes not enrich in a substantial way the variety ofthe generated answers and results show no signif-icant variances.
Given these observations, we fixan SMT system with a 5-gram language model,the full set of translation model features and thegeneration of a 100-best list for obtaining B andR scores.Each score approaches different problems ofthe task and therefore, complement each otherrather than overlapping.
Table 6 introduces theresults of a selected group of score combinations,where Lbrf =LbLrLf .The scores Lbrf and E alone are not very usefulbecause Lbrf gives the same score to all candi-dates in the same sentence and E gives the samescore to all candidates of the same type.
Exper-imental results confirm that, as expected, Lbrf ismore appropriate for the SR task and E for theQA task, although the figures are very low.
Whenjoining E and the Ls together, no improvement isobtained, and the results for the QA task are worsethan Lbrf alone, thus demonstrating that Level1translations are not good enough for the QA task.27A better system combines all the metrics together.The best results are achieved when adding Band R scores to the combination.
All of thesecombinations (i.e.
B, R, BR and B+R) are bet-ter when are multiplied by both E and Lbrf thanby only one of them alone.
Otherwise, combina-tions of only E and Lbrf yield very poor results.Thus, the Level2 representation boosts T1 scoresfrom 0.018 (ELbrf ) to 0.100 ((B+R)ELbrf ) in QAand almost doubles it in SR. As a general trend,we see that combinations involving R but not Bare better in the SR task than in the QA task.
Infact the best results for SR are obtained with theRELbrf combination.
The best MRR scores areachieved also with the best T1 scores.7 Discussion and ConclusionsThe results here presented are our approach toconsider question answering a translation prob-lem.
Questions in an abstract representation(Level2) are translated into an abstract represen-tation of the answer, and these generated answersare matched against all the candidates obtainedwith the retrieval module.
The candidates are thenranked according to their similarity with the n-best list of translations as measured by three fam-ilies of metrics that include R, B, E and L.The best combination of metrics is able toanswer a 10.0% of the questions in first place(T1).
This result is in the lowest part of the ta-ble reported by the official TREC-11 overview(Voorhees, 2002).
The approach of Echihabiand Marcu (2003) that uses translation proba-bilities to rank the answers achieves higher re-sults on the same data set (an MRR of 0.325versus our 0.141).
Although both works useSMT techniques, the approach is quite different.In fact, our system is more similar in spirit tothat of Ravichandran and Hovy (2002), whichlearns regular expressions to find answer contextsand shows significant improvements for out-of-domain test sets, that is web data.
Besides the factthat Echihabi and Marcu use translation modelsinstead of a full translation system, they explicitlytreat the problem of the difference of length be-tween the question and the answer.
In our work,this is not further considered than by the word andphrase penalty features of the translation model.Future work will address this difficulty.The results of sentence ranking of our systemare similar to those obtained by Murdock andCroft (2005), however, since test sets are differentthey are not directly comparable.
This is notablebecause we tackle QA, and sentence retrieval isobtained as collateral information.Possible lines of future research include thestudy abstraction levels different from Level2.The linguistic processors provide us with interme-diate information such as POS that is not currentlyused as it is WordNet and named entities.
Sev-eral other levels combining this information canbe also tested in order to find the most appropri-ate degree of abstraction for each kind of word.The development part of the SMT system is adelicate issue.
MERT is currently optimising to-wards BLEU, but the final score for ranking theanswers is a combination of a smoothed BLEU,ROUGE, L and E. It has been shown that opti-mising towards the same metric used to evaluatethe system is beneficial for translation, but alsothat BLEU is one of the most robust metrics tobe used (Cer et al, 2010), so the issue has tobe investigated for the QA problem.
Also, refin-ing BLEU and ROUGE for this specific problemcan be useful.
A first approximation could be anadaptation of the n-gram counting of BLEU andROUGE so that it is weighted by its distance tothe answer; this way sentences that differ only be-cause of the candidate answer string would be bet-ter differentiated.Related to this, the generation of the candidateanswer strings is exhaustive; the suppression ofthe less frequent candidates could help to elimi-nate noise in the form of irrelevant answer sen-tences.
Besides, the system correlates these an-swer strings with the expected answer type ofthe question (coincidence measured with E).
Thisstep should be replaced by an SMT-based mech-anism to build a full system only based on SMT.Furthermore, we plan to include the Level1 trans-lations into the candidate answer generation mod-ule in order to do query expansion in the style ofRiezler et al (2007).AcknowledgementsThis work has been partially funded by the Eu-ropean Community?s Seventh Framework Pro-gramme (MOLTO project, FP7-ICT-2009-4-247914) and the Spanish Ministry of Scienceand Innovation projects (OpenMT-2, TIN2009-14675-C03-01 and KNOW-2, TIN2009-14715-C04-04).28ReferencesA.
Berger and J. Lafferty.
1999.
Information retrievalas statistical translation.
In Proceedings of ACM SI-GIR Conference.A.
Berger, R. Caruana, D. Cohn, D. Freitag, andV.
Mittal.
2000.
Bridging the lexical chasm: statis-tical approaches to answer-finding.
In Proceedingsof the ACM SIGIR Conference.T.
Brants.
2000.
TnT ?
a statistical part-of-speechtagger.
In Proceedings ANLP Conference.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: parameter estimation.
Compu-tational Linguistics, 19(2).D.
Cer, C. D. Manning, and D. Jurafsky.
2010.
Thebest lexical metric for phrase-based statistical MTsystem optimization.
In Proceeding of the HLTConference.H.
Cui, R. Sun, K. Li, M.Y.
Kan, and T.S.
Chua.
2005.Question answering passage retrieval using depen-dency relations.
In Proceedings of the ACM SIGIRConference.H.T.
Dang, D. Kelly, and J. Lin.
2007.
Overview ofthe TREC 2007 question answering track.
In Pro-ceedings of the Text REtrieval Conference, TREC.A.
Echihabi and D. Marcu.
2003.
A noisy-channelapproach to question answering.
In Proceedings ofthe ACL Conference.
ACL.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
Cambridge, MA: MIT Press.D.
Ferrucci, E. Brown, J. Chu-Carroll, J. Fan,D.
Gondek, A. Kalyanpur, A. Lally, J. Murdock,E.
Nyberg, J. Prager, N. Schlaefer, and C. Welty.2010.
Building Watson: An overview of theDeepQA project.
AI Magazine, 31(3):59?79.J.
R. Finkel, T. Grenager, and C. D. Manning.
2005.Incorporating non-local information into informa-tion extraction systems by gibbs sampling.
In ACL.M.H.
Heie, E.W.D.
Whittaker, and S. Furui.
2011.Question answering using statistical language mod-elling.
Computer Speech & Language.P.
Koehn, H. Hoang, A. Mayne, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Con-stantin, and E. Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In AnnualMeeting of the ACL, Demonstration Session.T.
Kudo and Y. Matsumoto.
2003.
Fast methods forkernelbased text analysis.
In Proceedings of ACLConference.J.T.
Lee, S.B.
Kim, Y.I.
Song, and H.C. Rim.
2008.Bridging lexical gaps between queries and ques-tions on large online Q&A collections with compacttranslation models.
In Proceedings of the EMNLPConference.
ACL.X.
Li and D. Roth.
2005.
Learning question classi-fiers: The role of semantic information.
Journal ofNatural Language Engineering.C-Y.
Lin and F. Och.
2004.
Automatic Evaluation ofMachine Translation Quality Using Longest Com-mon Subsequence and Skip-Bigram Statics.
In Pro-ceedings of the ACL Conference.D.
Moldovan, C. Clark, S. Harabagiu, and D. Hodges.2007.
Cogex: A semantically and contextually en-riched logic prover for question answering.
Journalof Applied Logic, 5(1).V.
Murdock and W.B.
Croft.
2005.
Simple translationmodels for sentence retrieval in factoid question an-swering.
In Procedings of the ACM SIGIR Confer-ence.F.
Och and H. Ney.
2002.
Discriminative Training andMaximum Entropy Models for Statistical MachineTranslation.
In Proceedings of the ACL Conference.F.
Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1).F.
Och.
2003.
Minimum error rate training in statisti-cal machine translation.
In Proceedings of the ACLConference.K.
Papineni, S. Roukos, T. Ward, and W-J.
Zhu.
2002.BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
In Proceedings of the ACL Con-ference.A.
Pen?as, E. Hovy, P. Forner, A?.
Rodrigo, R. Sutcliffe,C.
Forascu, and C. Sporleder.
2011.
Overviewof QA4MRE at CLEF 2011: Question answeringfor machine reading evaluation.
Working Notes ofCLEF.D.
Ravichandran and E. Hovy.
2002.
Learning surfacetext patterns for a question answering system.
InProceedings of the ACL Conference.S.
Riezler, A. Vasserman, I. Tsochantaridis, V. Mit-tal, and Y. Liu.
2007.
Statistical machine trans-lation for query expansion in answer retrieval.
InProceedings of the ACL Conference.T.
Sakai, N. Kando, C.J.
Lin, T. Mitamura, H. Shima,D.
Ji, K.H.
Chen, and E. Nyberg.
2008.
Overviewof the NTCIR-7 ACLIA IR4QA task.
In Proceed-ings of NTCIR Conference.R.
Soricut and E. Brill.
2004.
Automatic questionanswering: Beyond the factoid.
In Proceedings ofHLT-NAACL Conference.A.
Stolcke.
2002.
SRILM ?
An extensible languagemodeling toolkit.
In Proc.
Intl.
Conf.
on SpokenLanguage Processing.M.
Surdeanu, M. Ciaramita, and H. Zaragoza.
2011.Learning to rank answers to non-factoid questionsfrom web collections.
Computational Linguistics,37(2).E.M.
Voorhees.
2002.
Overview of the TREC 2002Question Answering track.
In In Proceedings of theText REtrieval Conference, TREC.29
