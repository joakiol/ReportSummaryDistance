Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 183?192,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsPolyglot: Distributed Word Representations for Multilingual NLPRami Al-Rfou?
Bryan PerozziComputer Science Dept.
Stony Brook University Stony Brook, NY 11794{ralrfou, bperozzi, skiena}@cs.stonybrook.eduSteven SkienaAbstractDistributed word representations (wordembeddings) have recently contributedto competitive performance in languagemodeling and several NLP tasks.
Inthis work, we train word embeddings formore than 100 languages using their cor-responding Wikipedias.
We quantitativelydemonstrate the utility of our word em-beddings by using them as the sole fea-tures for training a part of speech taggerfor a subset of these languages.
We findtheir performance to be competitive withnear state-of-art methods in English, Dan-ish and Swedish.
Moreover, we inves-tigate the semantic features captured bythese embeddings through the proximityof word groupings.
We will release theseembeddings publicly to help researchers inthe development and enhancement of mul-tilingual applications.1 IntroductionBuilding multilingual processing systems is achallenging task.
Every NLP task involves dif-ferent stages of preprocessing and calculating in-termediate representations that will serve as fea-tures for later stages.
These stages vary in com-plexity and requirements for each individual lan-guage.
Despite recent momentum towards devel-oping multilingual tools (Nivre et al 2007; Hajic?et al 2009; Pradhan et al 2012), most of NLPresearch still focuses on rich resource languages.Common NLP systems and tools rely heavily onEnglish specific features and they are infrequentlytested on multiple datasets.
This makes them hardto port to new languages and tasks (Blitzer et al2006).A serious bottleneck in the current approachfor developing multilingual systems is the require-ment of familiarity with each language under con-sideration.
These systems are typically carefullytuned with hand-manufactured features designedby experts in a particular language.
This approachcan yield good performance, but tends to createcomplicated systems which have limited portabil-ity to new languages, in addition to being hard toenhance and maintain.Recent advancements in unsupervised featurelearning present an intriguing alternative.
In-stead of relying on expert knowledge, these ap-proaches employ automatically generated task-independent features (or word embeddings) givenlarge amounts of plain text.
Recent developmentshave led to state-of-art performance in severalNLP tasks such as language modeling (Bengioet al 2006; Mikolov et al 2010), and syntactictasks such as sequence tagging (Collobert et al2011).
These embeddings are generated as a resultof training ?deep?
architectures, and it has beenshown that such representations are well suited fordomain adaptation tasks (Glorot et al 2011; Chenet al 2012).We believe two problems have held back theresearch community?s adoption of these methods.The first is that learning representations of wordsinvolves huge computational costs.
The processusually involves processing billions of words overweeks.
The second is that so far, these systemshave been built and tested mainly on English.In this work we seek to remove these barriersto entry by generating word embeddings for overa hundred languages using state-of-the-art tech-niques.
Specifically, our contributions include:?
Word embeddings - We will release wordembeddings for the hundred and seventeenlanguages that have more than 10,000 ar-ticles on Wikipedia.
Each language?s vo-cabulary will contain up to 100,000 words.The embeddings will be publicly available at183(www.cs.stonybrook.edu/?dsl), forthe research community to study their charac-teristics and build systems for new languages.We believe our embeddings represent a valu-able resource because they contain a minimalamount of normalization.
For example, wedo not lower case words for European lan-guages as other studies have done for En-glish.
This preserves features of the under-lying language.?
Quantitative analysis - We investigatethe embedding?s performance on a part-of-speech (PoS) tagging task, and conduct qual-itative investigation of the syntactic and se-mantic features they capture.
Our experi-ments represent a valuable chance to evalu-ate distributed word representations for NLPas the experiments are conducted in a consis-tent manner and a large number of languagesare covered.
As the embeddings capture in-teresting linguistic features, we believe themultilingual resource we are providing givesresearchers a chance to create multilingualcomparative experiments.?
Efficient implementation - Training thesemodels was made possible by our contri-butions to Theano (machine learning library(Bergstra et al 2010)).
These optimizationsempower researchers to produce word em-beddings under different settings or for dif-ferent corpora than Wikipedia.The rest of this paper is as follows.
In Section2, we give an overview of semi-supervised learn-ing and learning representations related work.
Wethen describe, in Section 3, the network used togenerate the word embeddings and its characteris-tics.
Section 4 discusses the details of the corpuscollection and preparation steps we performed.Next, in Section 5, we discuss our experimentalsetup and the training progress over time.
In Sec-tion 6 we discuss the semantic features capturedby the embeddings by showing examples of theword groupings in multiple languages.
Finally,in Section 7 we demonstrate the quality of ourlearned features by training a PoS tagger on sev-eral languages and then conclude.2 Related WorkThere is a large body of work regarding semi-supervised techniques which integrate unsuper-vised feature learning with discriminative learningmethods to improve the performance of NLP ap-plications.
Word clustering has been used to learnclasses of words that have similar semantic fea-tures to improve language modeling (Brown et al1992) and knowledge transfer across languages(Ta?ckstro?m et al 2012).
Dependency parsingand other NLP tasks have been shown to bene-fit from such a large unannotated corpus (Koo etal., 2008), and a variety of unsupervised featurelearning methods have been shown to unilaterallyimprove the performance of supervised learningtasks (Turian et al 2010).
(Klementiev et al2012) induce distributed representations for a pairof languages jointly, where a learner can be trainedon annotations present in one language and ap-plied to test data in another.Learning distributed word representations is away to learn effective and meaningful informationabout words and their usages.
They are usuallygenerated as a side effect of training parametriclanguage models as probabilistic neural networks.Training these models is slow and takes a signif-icant amount of computational resources (Bengioet al 2006; Dean et al 2012).
Several sugges-tions have been proposed to speed up the trainingprocedure, either by changing the model architec-ture to exploit an algorithmic speedup (Mnih andHinton, 2009; Morin and Bengio, 2005) or by esti-mating the error by sampling (Bengio and Senecal,2008).
(Collobert and Weston, 2008) shows that wordembeddings can almost substitute NLP commonfeatures on several tasks.
The system they built,SENNA, offers part of speech tagging, chunking,named entity recognition, semantic role labelingand dependency parsing (Collobert, 2011).
Thesystem is built on top of word embeddings and per-forms competitively compared to state of art sys-tems.
In addition to pure performance, the systemhas a faster execution speed than comparable NLPpipelines (Al-Rfou?
and Skiena, 2012).To speed up the embedding generation process,SENNA embeddings are generated through a pro-cedure that is different from language modeling.The representations are acquired through a modelthat distinguishes between phrases and corruptedversions of them.
In doing this, the model avoidsthe need to normalize the scores across the vocab-ulary to infer probabilities.
(Chen et al 2013)shows that the embeddings generated by SENNA184Apple apple Bush bush corpora dangerousDell tomato Kennedy jungle notations costlyParamount bean Roosevelt lobster digraphs chaoticMac onion Nixon sponge usages bizarreFlex potato Fisher mud derivations destructiveTable 1: Words nearest neighbors as they appear in the English embeddings.perform well in a variety of term-based evaluationtasks.
Given the training speed and prior perfor-mance on NLP tasks in English, we generate ourmultilingual embeddings using a similar networkarchitecture to the one SENNA used.However, our work differs from SENNA in thefollowing ways.
First, we do not limit our mod-els to English, we train embeddings for a hundredand seventeen languages.
Next, we preserve lin-guistic features by avoiding excessive normaliza-tion to the text.
For example, our English modelplaces ?Apple?
closer to IT companies and ?ap-ple?
to fruits.
More examples of linguistic fea-tures preserved by our model are shown in Table1.
This gives us the chance to evaluate the embed-dings performance over PoS tagging without theneed for manufactured features.
Finally, we re-lease the embeddings and the resources necessaryto generate them to the community to eliminateany barriers.Despite the progress made in creating dis-tributed representations, combining them to pro-duce meaning is still a challenging task.
Sev-eral approaches have been proposed to addressfeature compositionality for semantic problemssuch as paraphrase detection (Socher et al 2011),and sentiment analysis (Socher et al 2012) usingword embeddings.3 Distributed Word RepresentationDistributed word representations (word embed-dings) map the index of a word in a dictionary to afeature vector in high-dimension space.
Every di-mension contributes to multiple concepts, and ev-ery concept is expressed by a combination of sub-set of dimensions.
Such mapping is learned byback-propagating the error of a task through themodel to update random initialized embeddings.The task is usually chosen such that examples canbe automatically generated from unlabeled data(i.e so it is unsupervised).
In case of languagemodeling, the task is to predict the last word ofa phrase that consists of n words.In our work, we start from the example con-struction method outlined in (Bengio et al 2009).They train a model by requiring it to distinguishbetween the original phrase and a corrupted ver-sion of the phrase.
If it does not score theoriginal one higher than the corrupted one (bya margin), the model will be penalized.
Moreprecisely, for a given sequence of words S =[wi?n .
.
.
wi .
.
.
wi+n] observed in the corpus T ,we will construct another corrupted sequence S?by replacing the word in the middle wi with a wordwj chosen randomly from the vocabulary.
Theneural network represents a function score thatscores each phrase, the model is penalized throughthe hinge loss function J(T ) as shown in 1.J(T ) = 1|T |?i?T|1?score(S?
)+score(S)|+ (1)Figure 1 shows a neural network that takes a se-quence of words with size 2n + 1 to compute ascore.
First, each word is mapped through a vo-cabulary dictionary with the size |V | to an indexthat is used to index a shared matrix C with thesize |V |?M where M is the size of the vector rep-resenting the word.
Once the vectors are retrieved,they are concatenated into one vector called pro-jection layer P with size (2n + 1) ?M .
The pro-jection layer plays the role of an input to a hiddenlayer with size |H|, the activations A of which arecalculated according to equation 3, where W1, b1are the weights and bias of the hidden layer.A = tanh(W1P + b1) (2)To calculate the phrase score, a linear combina-tion of the hidden layer activations A is computedusing W2 and b2.score(P ) = W2A+ b2 (3)Therefore, the five parameters that have to belearned are W1, W2, b1, b2, C with a total numberof parameters (2n+ 1) ?M ?H +H +H + 1+|V | ?M ?M ?
(nH + |V |) .185CImaginationCisCgreaterCthanCdetailScoreHidden LayerHCM|V|ProjectionLayerFigure 1: Neural network architecture.
Words areretrieved from embeddings matrix C and concate-nated at the projection layer as an input to com-puter the hidden layer activation.
The score isthe linear combination of the activation values ofthe hidden layer.
The scores of two phrases areranked according to hinge loss to distinguish thecorrupted phrase from the original one.4 Corpus PreparationWe have chosen to generate our word embeddingsfrom Wikipedia.
In addition to size, there are otherdesirable properties that we wish for the source ofour language model to have:?
Size and variety of languages - As of thiswriting (April, 2013), 42 languages had morethan 100,000 article pages, and 117 lan-guages had more than 10,000 article pages.?
Well studied - Wikipedia is a prolific re-source in the literature, and has been usedfor a variety of problems.
Particularly,Wikipedia is well suited for multilingual ap-plications (Navigli and Ponzetto, 2010).?
Quality - Wikipedians strive to write arti-cles that are readable, accurate, and consistof good grammar.?
Openly accessible - Wikipedia is a resourceavailable for free use by researchers?
Growing - As technology becomes more ac-cessible, the size and scope of the multilin-gual Wikipedia effort continues to expand.To process Wikipedia markup, we first extractthe text using a modified version of the Bliki en-gine1.
Next we must tokenize the text.
We relyon an OpenNLP probabilistic tokenizer wheneverpossible, and default to the Unicode text segmen-tation2 algorithm offered by Lucene when we haveno such OpenNLP model.
After tokenization, wenormalize the tokens to reduce their sparsity.
Wehave two main normalization rules.
The first re-places digits with the symbol #, so ?1999?
be-comes ####.
In the second, we remove hyphensand brackets that appear in the middle of a token.As an additional rule for English, we map non-Latin characters to their unicode block groups.In order to capture the syntactic and semanticfeatures of words, we must observe each word sev-eral times in each of its valid contexts.
This re-quirement, when combined with the Zipfian dis-tribution of words in natural language, implies thatlearning a meaningful representation of a languagerequires a huge amount of unstructured text.
Inpractice we deal with this limitation by restrictingourselves to considering the most frequently oc-curring tokens in each language.Table 2 shows the size of each language corpusin terms of tokens, number of word types and cov-erage of text achieved by building a vocabulary outof the most frequent 100,000 tokens, |V |.
Out ofvocabulary (OOV) words are replaced with a spe-cial token ?UNK?.While Wikipedia has 284 language specific en-cyclopedias, only five of them have more than amillion articles.
The size drops dramatically, suchthat the 42nd largest Wikipedia, Hindi, has slightlyabove 100,000 articles and the 100th, Tatar, hasslightly over 16,000 articles3.Significant Wikipedias in size have a word cov-erage over 92% except for German, Russian, Ara-bic and Czech which shows the effect of heavy us-age of morphological forms in these languages onthe word usage distribution.The highest word coverage we achieve is unsur-prisingly for Chinese.
This is expected given thelimited size vocabulary of the language - the num-ber of entries in the Contemporary Chinese Dictio-nary are estimated to be 65 thousand words (Shux-iang, 2004).1Java Wikipedia API (Bliki engine) - http://code.google.com/p/gwtwiki/2http://www.unicode.org/reports/tr29/3http://meta.wikimedia.org/w/index.php?title=List_of_Wikipedias&oldid=5248228186Language Tokens Words Coverage?106 ?103English 1,888 12,125 96.30%German 687 9,474 91.78%French 473 4,675 95.78%Spanish 399 3,978 96.07%Russian 328 5,959 90.43%Italian 322 3,642 95.52%Portuguese 197 2,870 95.68%Dutch 197 3,712 93.81%Chinese 196 423 99.67%Swedish 101 2,707 92.36%Czech 80 2,081 91.84%Arabic 52 1,834 91.78%Danish 44 1,414 93.68%Bulgarian 39 1,114 94.35%Slovene 30 920 94.42%Hindi 23 702 96.25%Table 2: Statistics of a subset of the languages pro-cessed.
The second column reports the number oftokens found in the corpus in millions while thethird column reports the word types found in thou-sands.
The coverage indicates the percentage ofthe corpus that will be matching words in a vocab-ulary consists of the most frequent 100 thousandwords.5 TrainingFor our experiments, we build a model as the onedescribed in Section 3 using Theano (Bergstra etal., 2010).
We choose the following parameters,context window size 2n + 1 = 5, vocabulary|V | = 100, 000, word embedding size M = 64,and hidden layer size H = 32.
The intuition, here,is to maximize the relative size of the embeddingscompared to the rest of the network.
This mightforce the model to store the necessary informationin the embeddings matrix instead of the hiddenlayer.
Another benefit is that we will avoid over-fitting on the smaller Wikipedias.
Increasing thewindow size or the embedding size slows downthe training speed, making it harder to convergewithin a reasonable time.The examples are generated by sweeping a win-dow over sentences.
For each sentence in the cor-pus, all unknown words are replaced with a specialtoken ?UNK?
and sentences are padded with ?S?,?/S?
tokens.
In case the window exceeds the edgesof a sentence, the missing slots are filled with ourpadding token, ?PAD?.Figure 2: Training and test errors of the Frenchmodel after 23 days of training.
We did not noticeany overfitting while training the model.
The errorcurves are smoother the larger the language corpusis.To train the model, we consider the data in mini-batches of size 16.
Every 16 examples, we es-timate the gradient using stochastic gradient de-scent (Bottou, 1991), and update the parameterswhich contributed to the error using backpropaga-tion (Rumelhart et al 2002).
Calculating an exactgradient is prohibitive given that the dataset size isin millions of examples.
We calculate the devel-opment error by sampling randomly 10000 mini-batches from the development dataset.For each language, we set the batch size to 16examples, and the learning rate to be 0.1.
Follow-ing, (Collobert et al 2011)?s advice, we divideeach layer by the fan in of that layer, and we con-sider the embeddings layer to have a fan in of 1.We divide the corpus to three sets, training, devel-opment and testing with the following percentages90, 5, 5 respectively.One disadvantage of the approach used by (Col-lobert et al 2011) is that there is no clear stop-ping criteria for the model training process.
Wehave noticed that after a few weeks of training,the model?s performance reaches the point wherethere is no significant decrease in the average lossover the development set, and when this occurs wemanually stop the training.
An interesting prop-erty of this model is that we did not notice anysign of overfitting for large Wikipedias.
This couldbe explained by the infinite amount of exampleswe can generate by randomly choosing the re-187Word Translation Word Translation Word WordFrenchrouge redSpanishdentista dentistEnglishMumbai Bombayjuane yellow peluquero barber Chennai Madrasrose pink gineco?log gynecologist Bangalore Shanghaiblanc white camionero truck driver Kolkata Calulttaorange orange oftalmo?logo ophthalmologist Cairo Bangkokbleu blue telegrafista telegraphist Hyderabad HyderabadArabicArabicGermanJkr  thanks ??d ?
two boys Eisenbahnbetrieb rail operations?Jkr  and thanks  nA?
two sons Fahrbetrieb drivingyA?
greetings ??d??
two boys Reisezugverkehr passenger trainsJkr ?
thanks + diacritic Vf??
two children Fa?hrverkehr ferries?Jkr ?
and thanks + diacritic  ny?
two sons Handelsverkehr Trade?rbA hello  ntA?
two daughters Schu?lerverkehr students TransportRussianChineseTransliterationItalian?????
Putin dongzhi Winter Solstice papa Pope????????
Yanukovych chunfen Vernal Equinox Papa Pope???????
Trotsky xiazhi Summer solstice pontefice pontiff??????
Hitler qiufen Autumnal Equinox basileus basileus??????
Stalin ziye Midnight canridnale cardinal????????
Medvedev chuxi New Year?s Eve frate friarTable 3: Examples of the nearest five neighbors of every word in several languages.
Translation isretrieved from http://translate.google.com.placement word in the corrupted phrase.
Figure2 shows a typical learning curve of the training.As the number of examples have been seen so farincreased both the training error and the develop-ment error go down.6 Qualitative AnalysisIn order to understand how the embeddings spaceis organized, we examine the subtle informationcaptured by the embeddings through investigatingthe proximity of word groups.
This informationhas the potential to help researchers develop ap-plications that use such semantic and syntactic in-formation.
The embeddings not only capture syn-tactic features, as we will demonstrate in Section4, but also demonstrate the ability to capture in-teresting semantic information.
Table 3 shows dif-ferent words in several languages.
For each wordon top of each list, we rank the vocabulary accord-ing to their Euclidean distance from that word andshow the closest five neighboring words.?
French & Spanish - Expected groupings ofcolors and professions is clearly observed.?
English - The example shows how the em-bedding space is aware of the name changethat happened to a group of Indian cities.?Mumbai?
used to be called ?Bombay?,?Chennai?
used to be called ?Madras and?Kolkata?
used to be called ?Calcutta?.
Onthe other hand, ?Hyderabad?
stayed at a sim-ilar distance from both names as they point tothe same conceptual meaning.?
Arabic - The first example shows the word?Thanks?.
Despite not removing the diacrit-ics from the text, the model learned that thetwo surface forms of the word mean similarthings and, therefore, grouped them together.In Arabic, conjunction words do not get sepa-rated from the following word.
Usually, ?andthanks?
serves as a letter signature as ?sin-cerely?
is used in English.
The model learnedthat both words {?and thanks?, ?thanks?
}are similar, regardless their different forms.The second example illustrates a specific syn-tactic morphological feature of Arabic, whereenumeration of couples has its own form.?
German - The example demonstrates that thecompositional semantics of multi-unit wordsare still preserved.?
Russian - The model learned to group Rus-sian/Soviet leaders and other figures relatedto the Soviet history together.?
Chinese - The list contains three solar termsthat are part of the traditional East Asian lu-nisolar calendars.
The remaining two termscorrespond to traditional holidays that occurat the same dates of these solar terms.?
Italian - The model learned that the lowerand upper cases of the word has similarmeaning.7 Sequence TaggingHere we analyze the quality of the models we havegenerated.
To test the quantitative performance ofthe embeddings, we use them as the sole featuresfor a well studied NLP task, part of speech tag-ging.To demonstrate the capability of the learned dis-188Language Source Test TnTUnknown Known AllGerman Tiger?
(Brants et al 2002) 89.17% 98.60% 97.85% 98.10%Bulgarian BTB?
(Simov et al 2002) 75.74% 98.33% 96.33% 97.50%Czech PDT 2.5 (Bejc?ek et al 2012) 71.98% 99.15% 97.13% 99.10%Danish DDT?
(Kromann, 2003) 73.03% 98.07% 96.45% 96.40%Dutch Alpino?
(Van der Beek et al 2002) 73.47% 95.85% 93.86% 95.00%English PennTreebank (Marcus et al 1993) 75.97% 97.74% 97.18% 96.80%Portuguese Sint(c)tica?
(Afonso et al 2002) 75.36% 97.71% 95.95% 96.80%Slovene SDT?
(Dz?eroski et al 2006) 68.82% 95.17% 93.46% 94.60%Swedish Talbanken05?
(Nivre et al 2006) 83.54% 95.77% 94.68% 94.70%Table 4: Results of our model against several PoS datasets.
The performance is measured using accuracyover the test datasets.
Third column represents the total accuracy of the tagger the former two columnsreports the accuracy over known words and OOV words (unknown).
The results are compared to theTnT tagger results reported by (Petrov et al 2012).
?CoNLL 2006 datasettributed representations in extracting useful wordfeatures, we train a PoS tagger over the subset oflanguages that we were able to acquire free anno-tated resources for.
We choose our tagger for thistask to be a neural network because it has a fastconvergence rate based on our initial experiments.The part of speech tagger has similar architec-ture to the one used for training the embeddings.However we have changed some of the networkparameters, specifically, we use a hidden layer ofsize 300 and learning rate of 0.3.
The network istrained by minimizing the negative of the log like-lihood of the labeled data.
To tag a specific wordwi we consider a window with size 2n where nin our experiment is equal to 2.
Equation 4 showshow we construct a feature vector F by concate-nating (?)
the embeddings of the words occurredin the window, where C is the matrix that containsthe embeddings of the language vocabulary.F =i+2?j=i?2C[wj ] (4)The feature vector will be fed to the network andthe error will back propagated back to the embed-dings.The results of this experiment are presented inTable 4.
We train and test our models on the uni-versal tagset proposed by (Petrov et al 2012).This universal tagset maps each original tag in atreebank to one out of twelve general PoS tags.This simplifies the comparison of classifiers per-formance across languages.
We compare our re-sults to a similar experiment conducted in theirwork, where they trained a TnT tagger (Brants,2000) on several treebanks.
The TnT tagger isbased on Markov models and depends on trigramcounts observed in the labeled data.
It was cho-sen for its fast speed and (near to) state-of-the-artaccuracy, without language specific tuning.The performance of embeddings is competitivein general.
Surprisingly, it is doing better than theTnT tagger in English and Danish.
Moreover, ourperformance is so close in the case of Swedish.This task is hard for our tagger for two reasons.The first is that we do not add OOV words seenduring training of the tagger to our vocabulary.The second is that all OOV words are substitutedwith one representation, ?UNK?
and there is nocharacter level information used to inform the tag-ger about the characteristic of the OOV words.On the other hand, the performance on theknown words is strong and consistent showing thevalue of the features learned about these wordsfrom the unsupervised stage.
Although the wordcoverage of German and Czech are low in the orig-inal Wikipedia corpora (See Table 2), the featureslearned are achieving great accuracy on the knownwords.
They both achieve above 98.5% accuracy.It is noticeable that the Slovene model performsthe worst, under both known and unknown wordscategories.
It achieves only 93.46% accuracy onthe test dataset.
Given that the Slovene embed-dings were trained on the least amount of dataamong all other embeddings we test here, we ex-pect the quality to go lower for the other smallerWikipedias not tested here.189In Table 5, we present how well the vocabularyof each language?s embeddings covered the part ofspeech datasets.
The datasets come from a differ-ent domain than Wikipedia, and this is reflected inthe results.In Table 6, we present the results of training thesame neural network part of speech tagger with-out using our embeddings as initializations.
Wefound that the embeddings benefited all the lan-guages we considered, and observed the greatestbenefit in languages which had a small number oftraining examples.
We believe that these resultsillustrate the performanceLanguage % Token % WordCoverage CoverageBulgarian 94.58 77.70Czech 95.37 65.61Danish 95.41 80.03German 94.04 60.68English 98.06 79.73Dutch 96.25 77.76Portuguese 94.09 72.66Slovene 95.33 83.67Swedish 95.87 73.92Table 5: Coverage statistics of the embedding?svocabulary on the part of speech datasets after nor-malization.
Token coverage is the raw percentageof words which were known, while the Word cov-erage ignores repeated words.8 ConclusionDistributed word representations represent a valu-able resource for any language, but particularly forresource-scarce languages.
We have demonstratedhow word embeddings can be used as off-the-shelfsolution to reach near to state-of-art performanceover a fundamental NLP task, and we believe thatour embeddings will help researchers to developtools in languages with which they have no exper-tise.Moreover, we showed several examples of in-teresting semantic relations expressed in the em-beddings space that we believe will lead to inter-esting applications and improve tasks as semanticcompositionality.While we have only considered the properties ofword embeddings as features in this work, it hasbeen shown that using word embeddings in con-junction with traditional NLP features can signifi-Language # Training AccuracyExamples DropBulgarian 200,049 -2.01%Czech 1,239,687 -0.86%Danish 96,581 -1.77%German 735,826 -0.89%English 950,561 -0.25%Dutch 208,418 -1.37%Portuguese 212,749 -0.91%Slovene 27,284 -2.68%Swedish 199,509 -0.82%Table 6: Accuracy of randomly initialized tag-ger compared to our results.
Using the embed-dings was generally helpful, especially in lan-guages where we did not have many training ex-amples.
The scores presented are the best wefound for each language (languages with more re-sources could afford to train longer before overfit-ting).cantly improve results on NLP tasks (Turian et al2010; Collobert et al 2011).
With this in mind,we believe that the entire research community canbenefit from our release of word embeddings forover 100 languages.We hope that these resources will advance thestudy of possible pair-wise mappings between em-beddings of several languages and their relations.Our future work in this area includes improvingthe models by increasing the size of the contextwindow and their domain adaptivity through in-corporating other sources of data.
We will beinvestigating better strategies for modeling OOVwords.
We see improvements to OOV word han-dling as essential to ensure robust performance ofthe embeddings on real-world tasks.AcknowledgmentsThis research was partially supported by NSFGrants DBI-1060572 and IIS-1017181, with ad-ditional support from TexelTek.ReferencesSusana Afonso, Eckhard Bick, Renato Haber, and Di-ana Santos.
2002.
Floresta sinta?
(c) tica?
: a treebankfor portuguese.
In Proc.
of the Third Intern.
Conf.
onLanguage Resources and Evaluation (LREC), pages1698?1703.Rami Al-Rfou?
and Steven Skiena.
2012.
Speedread:A fast named entity recognition pipeline.
In Pro-190ceedings of the 24th International Conference onComputational Linguistics (Coling 2012), pages 53?61, Mumbai, India, December.
Coling 2012 Orga-nizing Committee.Eduard Bejc?ek, Jarmila Panevova?, Jan Popelka, PavelStran?a?k, Magda S?evc??
?kova?, Jan S?te?pa?nek, andZdene?k Z?abokrtsky?.
2012.
Prague DependencyTreebank 2.5 ?
a revisited version of PDT 2.0.In Proceedings of COLING 2012, pages 231?246,Mumbai, India, December.
The COLING 2012 Or-ganizing Committee.Yoshua Bengio and J-S Senecal.
2008.
Adaptive im-portance sampling to accelerate training of a neu-ral probabilistic language model.
Neural Networks,IEEE Transactions on, 19(4):713?722.Y.
Bengio, H. Schwenk, J.S.
Sene?cal, F. Morin, and J.L.Gauvain.
2006.
Neural probabilistic language mod-els.
Innovations in Machine Learning, pages 137?186.Y.
Bengio, J. Louradour, R. Collobert, and J. Weston.2009.
Curriculum learning.
In International Con-ference on Machine Learning, ICML.James Bergstra, Olivier Breuleux, Fre?de?ric Bastien,Pascal Lamblin, Razvan Pascanu, Guillaume Des-jardins, Joseph Turian, David Warde-Farley, andYoshua Bengio.
2010.
Theano: a CPU andGPU math expression compiler.
In Proceedingsof the Python for Scientific Computing Conference(SciPy), June.
Oral Presentation.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Conference on Empirical Meth-ods in Natural Language Processing, Sydney, Aus-tralia.Le?on Bottou.
1991.
Stochastic gradient learning inneural networks.
In Proceedings of Neuro-N?
?mes91, Nimes, France.
EC2.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The tigertreebank.
In IN PROCEEDINGS OF THE WORK-SHOP ON TREEBANKS AND LINGUISTIC THEO-RIES, pages 24?41.Thorsten Brants.
2000.
Tnt: a statistical part-of-speech tagger.
In Proceedings of the sixth confer-ence on Applied natural language processing, pages224?231.
Association for Computational Linguis-tics.Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n-gram models of natural language.Computational linguistics, 18(4):467?479.Minmin Chen, Zhixiang Xu, Kilian Weinberger, andFei Sha.
2012.
Marginalized denoising autoen-coders for domain adaptation.
In John Langford andJoelle Pineau, editors, Proceedings of the 29th Inter-national Conference on Machine Learning (ICML-12), ICML ?12, pages 767?774.
ACM, New York,NY, USA, July.Yanqing Chen, Bryan Perozzi, Rami Al-Rfou?, andSteven Skiena.
2013.
The expressive power of wordembeddings.
CoRR, abs/1301.3226.R.
Collobert and J. Weston.
2008.
A unified architec-ture for natural language processing: Deep neuralnetworks with multitask learning.
In InternationalConference on Machine Learning, ICML.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
J. Mach.
Learn.
Res., 12:2493?2537,November.Ronan Collobert.
2011.
Deep learning for efficientdiscriminative parsing.
In AISTATS.Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,Matthieu Devin, Quoc Le, Mark Mao, Marc?AurelioRanzato, Andrew Senior, Paul Tucker, Ke Yang, andAndrew Ng.
2012.
Large scale distributed deep net-works.
In P. Bartlett, F.C.N.
Pereira, C.J.C.
Burges,L.
Bottou, and K.Q.
Weinberger, editors, Advancesin Neural Information Processing Systems 25, pages1232?1240.Sas?o Dz?eroski, Tomaz?
Erjavec, Nina Ledinek, Petr Pa-jas, Zdenek Z?abokrtsky, and Andreja Z?ele.
2006.Towards a slovene dependency treebank.
In Proc.
ofthe Fifth Intern.
Conf.
on Language Resources andEvaluation (LREC).Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In Pro-ceedings of the Twenty-eight International Confer-ence on Machine Learning (ICML?11), volume 27,pages 97?110, June.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic depen-dencies in multiple languages.
In Proceedings ofthe 13th Conference on Computational Natural Lan-guage Learning (CoNLL-2009), June 4-5, Boulder,Colorado, USA.Alexandre Klementiev, Ivan Titov, and Binod Bhat-tarai.
2012.
Inducing crosslingual distributed rep-resentations of words.
In Proceedings of COLING2012, pages 1459?1474, Mumbai, India, December.The COLING 2012 Organizing Committee.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In In Proc.
ACL/HLT.191Matthias Trautner Kromann.
2003.
The danish depen-dency treebank and the dtag treebank tool.
In Pro-ceedings of the Second Workshop on Treebanks andLinguistic Theories (TLT), page 217.Mitchell P Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Compu-tational linguistics, 19(2):313?330.T.
Mikolov, M. Karafia?t, L. Burget, J. Cernocky, andS.
Khudanpur.
2010.
Recurrent neural networkbased language model.
Proceedings of Interspeech.Andriy Mnih and Geoffrey E Hinton.
2009.
A scalablehierarchical distributed language model.
Advancesin neural information processing systems, 21:1081?1088.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InProceedings of the international workshop on artifi-cial intelligence and statistics, pages 246?252.Roberto Navigli and Simone Paolo Ponzetto.
2010.Babelnet: Building a very large multilingual seman-tic network.
In Proceedings of the 48th annual meet-ing of the association for computational linguistics,pages 216?225.
Association for Computational Lin-guistics.Joakim Nivre, Jens Nilsson, and Johan Hall.
2006.Talbanken05: A swedish treebank with phrase struc-ture and dependency annotation.
In Proceedings ofthe fifth International Conference on Language Re-sources and Evaluation (LREC), pages 1392?1395.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages915?932, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Nicoletta Cal-zolari (Conference Chair), Khalid Choukri, ThierryDeclerck, Mehmet Ug?ur Dog?an, Bente Maegaard,Joseph Mariani, Jan Odijk, and Stelios Piperidis, ed-itors, Proceedings of the Eight International Con-ference on Language Resources and Evaluation(LREC?12), Istanbul, Turkey, may.
European Lan-guage Resources Association (ELRA).Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 shared task: Modeling multilingual unre-stricted coreference in OntoNotes.
In Proceedingsof the Sixteenth Conference on Computational Natu-ral Language Learning (CoNLL 2012), Jeju, Korea.David E Rumelhart, Geoffrey E Hinton, and Ronald JWilliams.
2002.
Learning representations by back-propagating errors.
Cognitive modeling, 1:213.Lu Shuxiang.
2004.
The Contemporary Chinese Dic-tionary (Xiandai Hanyu Cidian).
Commercial Press.Kiril Simov, Petya Osenova, Milena Slavcheva,Sia Kolkovska, Elisaveta Balabanova, DimitarDoikoff, Krassimira Ivanova, Er Simov, and MilenKouylekov.
2002.
Building a linguistically inter-preted corpus of bulgarian: the bultreebank.
In In:Proceedings of LREC 2002, Canary Islands.Richard Socher, Eric H. Huang, Jeffrey Pennington,Andrew Y. Ng, and Christopher D. Manning.
2011.Dynamic pooling and unfolding recursive autoen-coders for paraphrase detection.
In Advances inNeural Information Processing Systems 24.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic com-positionality through recursive matrix-vector spaces.In Proceedings of the 2012 Conference on Em-pirical Methods in Natural Language Processing(EMNLP).Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszko-reit.
2012.
Cross-lingual word clusters for directtransfer of linguistic structure.
In Proceedings of the2012 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 477?487.
Asso-ciation for Computational Linguistics.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: a simple and general method for semi-supervised learning.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 384?394.
Association for Com-putational Linguistics.Leonoor Van der Beek, Gosse Bouma, Rob Malouf,and Gertjan Van Noord.
2002.
The alpino depen-dency treebank.
Language and Computers, 45(1):8?22.192
