Coordination and context-dependence in the generation of embodiedconversationJustine Cassell**Media LaboratoryMITE15-31520 Ames, Cambridge MA{justine, yanhao}@media.mit, eduMatthew Stone t Hao Yan*tDepartment of Computer Science &Center for Cognitive ScienceRutgers University110 Frelinghuysen, Piscataway NJ 08854-8019mdstone@cs, rutgers, eduAbstractWe describe the generation of communicative ac-tions in an implemented embodied conversationalagent.
Our agent plans each utterance so that mul-tiple communicative goals may be realized oppor-tunistically by a composite action including not onlyspeech but also coverbal gesture that fits the con-text and the ongoing speech in ways representativeof natural human conversation.
We accomplish thisby reasoning from a grammar which describes ges-ture declaratively in terms of its discourse function,semantics and synchrony with speech.1 IntroductionWhen we are face-to-face with another human, nomatter what our language, cultural background, orage, we virtually all use our faces and hands as an in-tegral part of our dialogue with others.
Research onembod ied  conversat iona l  agents  aims to imbue in-teractive dialogue systems with the same nonverbalskills and behaviors (Cassell, 2000a).There is good reason to think that nonverbal be-havior will play an important role in evoking fromusers the kinds of communicative dialogue behav-iors they use with other humans, and thus allowthem to use the computer with the same kind of ef-ficiency and smoothness that characterizes their di-alogues with other people.
For example, (Casselland Th6risson, 1999) show that humans are morelikely to consider computers lifelike, and to rate theirlanguage skills more highly, when those computersdisplay not only speech but appropriate nonverbalcommunicative behavior.
This argument akes onparticular importance given that users repeat hem-selves needlessly, mistake when it is their turn tospeak, and so forth when interacting with voice di-alogue systems (Oviatt, 1995): tn -life; noisy situa-tions like these provoke the non-verbal modalities tocome into play (Rogers, 1978).In this paper, we describe the generation of com-municative actions in an implemented embodiedconversational gent.
Our generation frameworkadopts a goal-directed view of generation and castsknowledge about communicative action in the formof a grammar that specifies how forms combine,what interpretive ffects they impart and in whatcontexts they are appropriate (Appelt, 1985; Moore,1994; Dale, 1992; Stone and Doran, 1997).
We ex-pand this framework to take into account findings,by ourselves and others, on the relationship betweenspontaneous coverbal hand gestures and speech.
Inparticular, our agent plans each utterance so thatmultiple communicative goals may be realized op-portunistically by a composite action including notonly speech but also coverbal gesture.
By describinggesture declaratively in terms of its discourse func-tion, semantics and synchrony with speech, we en-sure that coverbal gesture fits the context and the on-going speech in ways representative of natural hu-man conversation.
The result is a streamlined imple-mentation that instantiates important theoretical in-sights into the relationship between speech and ges-ture in human-human conversation.2 Explor ing the relat ionship between-speech and gestureTo generate mbodied communicative action re-quires an architecture for embodied conversation;ours is provided by the agent REA ("Real EstateAgent"), a computer-generated humanoid that hasan articulated graphical body, can sense the userpassively through cameras and audio input, andsupports communicative actions realized in speechwith intonation, facial display, and animated ges-ture.
REA currently offers the reasoning and dis-play capabilities to act as a real estate agent showing..... ~users'the--features"o~ vm i-o-wsmodels"of howsesthat ~appear on-screen behind her.
We use existing fea-tures of kEA here as a resem'ch platform for imple-171menting models of the relationship between speechand spontaneous hand gestures during conversation.For more details about the functionality of REA see(Cassell, 2000a).Evidence from many sources uggests that this re-.lationship is aclose one..About three,quarters of al!clauses in narrative discourse are accompanied bygestures of one kind or another (McNeill, 1992), andwithin those clauses, the most effortful part of ges-tures tends to co-occur with or just before the phono-logically most prominent syllable of the accompany-ing speech (Kendon, 1974).Of course, communication is still possible with-out gesture.
But it has been shown that when speechis ambiguous (Thompson and Massaro, 1986) or ina speech situation with some noise (Rogers, 1978),listeners do rely on gestural cues (and, the higher thenoise-to-signal ratio, the more facilitation by ges-ture).
Similarly, Cassell et al (1999) established thatlisteners rely on information conveyed only in ges-ture as they try to comprehend a story.Most interesting in terms of building interactivedialogue systems i the semantic and pragmatic rela-tionship between gesture and speech.
The two chan-nels do not always manifest he same information,but what they convey is virtually always compati-ble.
Semantically, speech and gesture give a con-sistent view of an overall situation.
For example,gesture may depict he way in which an action wascarried out when this aspect of meaning is not de-picted in speech.
Pragmatically, speech and ges-ture mark information about his meaning as advanc-ing the purposes of the conversation i a consistentway.
Indeed, gesture often emphasizes informationthat is also focused pragmatically by mechanismslike prosody in speech (Cassell, 2000b).
The seman-tic and pragmatic ompatibility seen in the gesture-speech relationship recalls the interaction of wordsand graphics in multimodal presentations (Feinerand McKeown, 1991; Green et al, 1998; Wahlsteret al, 1991 ).
In fact, some suggest (McNeill, 1992),that gesture and speech arise together f om an under-lying representation that has both visual and linguis-tic aspects, and so the relationship between gestureand speech is essential to the production of meaningand to its comprehension.This theoretical perspective on speech and gestureinvolves two key claims with computational import:that gesture and speech ref lectacommon concep-tual source; and that the content and form of a ges-ture is tuned to the communicative context and the172actor's communicative intentions.
We believe thatthese characteristics of the use of gesture are uni-versal, and see the key contribution of this work asproviding ageneral framework for building dialoguesystems in accord with them.
However, a concrete!mplementationrequires " more thanJustgeneralitiesbehind its operation; we also need an understandingof the precise ways gesture and speech are used to-gether in a particular task and setting.To this end, we collected a sample of real-estatedescriptions in line with what REA might be askedto provide.
To elicit each description, we asked onesubject o study a video and floor plan of a partic-ular house, and then to describe the house to a sec-ond subject (who did not know the house and had notseen the video).
During the conversation, the videoand floor plan were not available to either subject;the listener was free to interrupt and ask questions.The collected conversations were transcribed,yielding 328 utterances and 134 referential gestures,and coded to describe the general communicativegoals of the speaker and the kinds of semantic fea-tures realized in speech and gesture.Analysis of the data revealed that for roughly50% of the gesture-accompanied utterances, gestu-ral content was redundant with speech; for the other50% gesture contributed content hat was different,but complementary, to that contributed by speech.In addition, the relationship between content of ges-ture, content of speech and general communicativefunctions in house descriptions could be captured bya small number or rules; these rules are informed byand accord with our two key claims about speechand gesture.
For example, one rule describes di-alogue contributions whose general function waswhat we call presentation, to advance the descrip-tion of the house by introducing a single new ob-ject..
These contributions tended to be made up ofa sentence that asserted the existence of an objectof some type, accompanied by a non-redundant ges-?
ture that elaborated theshape or location of  the ob-ject.
Our approach casts this extended escription ofa new entity, mediated by two compatible modali-ties, as the speaker's expression of one overall func-tion of presentation.
( I ) is a representative example.
(1) It has \[a nice garden\].
(right hand, held flat,traces a circle, indicating location of thegarden sunounding the house)Six rules account for 60% of the gestures in theFigure 1" Interacting with REAtranscriptions (recall) and apply with an accuracy of96% (precision).
These patterns provide a concretespecification for the main communicative strategiesand communicative r sources required for REA.
mfull discussion of the experimental methods andanalysis, and the resulting rules, can be found in(Yan, 2000).3 F raming  the generat ion problemIn REA, requests for the generation of speech andgesture are formulated within the dialogue manage-ment module.
REA'S utterances reflect a coordina-tion of multiple kinds of processing in the dialoguemanager- the system recognizes that it has the floor,derives the appropriate communicative context fora response and an appropriate set of communicativegoals, triggers the generation process, and realizesthe resulting speech and gesture.
The dialogue man-ager is only one component in a multithreaded ar-chitecture that carries out hardwired reactions to in-put as well as deliberative processing.
The diver-sity is required in order to exhibit appropriate inter-actional and propositional conversational behaviorsat a range of time scales, from tracking the user'smovements with gaze and providing nods and otherfeedback as the user speaks, to participating in rou-tine exchanges and generating principled responsesto user's queries.
See (Cassell, 2000a) for descrip-tion and motivation of the architecture, aswell as theconversational functions and behaviors it supports.REA'S design and capabilities reflect our researchfocus on allying conversational content with conver-sation management, and allying nonverbal modali-ties with speech: how can anembodiedagent use'allits communicative modalities to contribute new con-tent when needed (propositional function), to signalthe state of the dialogue, and to regulate the over-all process of conversation (interactional function)?Within this focus, REA's talk is firmly delimited.REA'S utterances take a question-answer fo mat, inwhich the user asks about (and REA describes) asingle house .at.a.
time.
REA'S .sentences ,ate short;generally, they contribute just a few new semanticfeatures about particular ooms or features of thehouse (in speech and gesture), and flesh this contri-bution out with a handful of meaningful e ements (inspeech and gesture) that ground the contribution ishared context of the conversation.Despite the apparent simplicity, the dialoguemanager must contribute a wealth of informationabout he domain and the conversation torepresentthe communicative context.
This detail is needed forREA tO achieve atheoretically-motivated realizationof the common patterns of speech and gesture we ob-served in human conversation.
For example, a vari-ety of changing features determine whether markedforms in speech and gesture are appropriate in thecontext.
REA'S dialogue manager t acks the chang-ing status of such features as:e Attentionalprominence, r presented (as usualin natural language generation) by setting up acontext set for each entity (Dale, 1992).
Ourmodel of prominence is a simple local one sim-ilar to (Strube, 1998).o Cognitive status, including whether an entity ishearer-old or hearer-new (Prince, 1992), andwhether an entity is in-focus or not (Gundelet al, 1993).
We can assume that houses andtheir rooms are hearer-new until REA describesthem; and that just those entities mentioned inthe prior sentence are in-focus.Information structure, including the openpropositions or, following (Steedman, 1991 ),themes, which describe the salient questionscurrently at issue in the discourse (Prince,1986).
In REA'S dialogue, open questions arealways general questions about some entityraised by a recent urn; although in principlesuch an open question ought o be formalizedas theme(XP.Pe), REA can use the simplertheme(e).In fact, both speech and gesture depend on the same? "
kinds of'feamresi;-andaccessthem in the same way; "this specification of the dialogue state crosscuts dis-tinctions of communicative modality.173Another component of context is provided by adomain knowledge base, consisting of facts explic-itly labeled with the kind of information they repre-sent.
This defines the common ground in the con-versation in terms of sources of information thatlation of goals and tightly fits the context specifiedby the dialogue manager.4 Generation and linguistic representationspeaker and hearer share.
Modeling the discourse as We model REA'S communicative actions as com-a shared source of information means that new ~e "'-':''~'lmsed~?f:a~c~rHeeti?n'?f'at?mie'etementsiqnclndiiagboth lexical items in speech and clusters of seman- mantic features REA imparts are added to the com-mon ground as the dialogue proceeds.
Following re-sults from (Kelly et al, 1999) which show that infor-mation from both speech and gesture is used to pro-vide context for ongoing talk, our common groundmay be updated by both speech and gesture.The structured omain knowledge also providesa resource for specifying communicative strategies.Recall that REA'S communicative strategies are for-mulated in terms of functions which are commonin naturally-occurring dialogues (such as "presenta-tion") and which lead to distinctive bundles of con-tent in gesture and speech.
The knowledge base'skinds of information provide a mechanism for spec-ifying and reasoning about such functions.
Theknowledge base is structured to describe the rela-tionship between the system's private informationand the questions of interest hat that informationcan be used to settle.
Once the user's words havebeen interpreted, a layer of production rules con-structs obligations for response (Traum and Allen,1994); then, a second layer plans to meet hese obli-gations by deciding to present a specified kind ofinformation about a specified object.
This deter-mines some concrete communicative goals--factsof this kind that a contribution to dialogue couldmake.
Both speech and gesture can access thewhole structured database inrealizing these concretecommunicative goals.
For example, a variety offacts that bear on where a residence is--which city,which neighborhood or, if appropriate, where in abuilding--all provide the same kind of information,and would therefore fit the obligation to specify thelocation of a residence.
Or, to implement the rulefor presentation described in connection with ( 1 ), wecan associate an obligation of presentation with acluster of facts describing an object's type, its loca-tion in a house, and its size, shape or quality.The communicative context and concrete com-municative goals provide a common source for gen-erating speech and gesture in REA.
The utterancegeneration problem ,in REa,.then, is to construct a-complex communicative action, made up of speechand coverbal gesture, that achieves a given constel-tic features expressed as gestures; ince we assumethat any such item usually conveys a specific pieceof content, we refer to these elements generally aslexicalized escriptors.
The generation task in REAthus involves selecting a number of such lexical-ized descriptors and organizing them into a gram-matical whole that manifests the right semantic andpragmatic coordination between speech and gesture.The information conveyed must be enough that thehearer can identify the entity in each domain ref-erence from among its context set.
Moreover, thedescriptors must provide a source which allows thehearer to recover any needed new domain proposi-tion, either explicitly or by inference.We use the SPUD generator ("Sentence PlanningUsing Description") introduced in (Stone and Do-ran, 1997) to carry out this task for REA.
SPUDbuilds the utterance lement-by-element; at eachstage of construction, SPUD'S representation f thecurrent, incomplete utterance specifies its syntax,semantics, interpretation a d fit to context.
This rep-resentation both allows SPUD to determine whichlexicalized escriptors are available at each stage toextend the utterance, and to assess the progress to-wards its communicative goals which each exten-sion would bring about.
At each stage, then, SPUDselects the available option that offers the best im-mediate advance toward completing the utterancesuccessfully.
(We have developed a suite of guide-lines for the design of syntactic structures, eman-tic and pragmatic representations, and the interfacebetween them so that SPUD'S greedy search, whichis necessary for real-time performance, succeeds infinding concise and effective Utterances describedby the grammar (Stone et al, 2000).
)As part of the development of REA, we have con-structed a new inventory of lexicalized escriptors.REA'S descriptors consist of entries that contributeto coverbal gestures, as well as revised entries forspoken words that allow for their coordination withgesture under appropriate discourse conditions.
The:-organization f'these ntries assures'that--rasing thesame mechanism as with speech--REA'S gesturesdraw on the single available conceptual representa-174tion and that both REA'S  gesture and the relation-ship between gesture and speech-vary as a functionof pragmatic context in the same way as natural ges-tures and speech do.
More abstractly, these entriesenable SPUD to realize the concrete goals tied tocommon communicative functions with same dis-tribution of speech and gestiire bbse~ed:iffn/lttl'ral-conversations.To explain how these entries work, we need toconsider SPUD's representation of lexicalized de-scriptors in more detail.
Each entry is specifiedin three parts.
The first part--the syntax of theelemenv--sets out what words or other actions theelement contributes to its utterance.
The syn-tax is a hierarchical structure, formalized usingFeature-Based Lexicalized Tree Adjoining Gram-mar (LTAG) (Joshi et al, 1975; Schabes, 1990).Syntactic structures are also associated with referen-tial indices that specify the entities in the discoursethat the entry refers to.
For the entry to apply at aparticular stage, its syntactic structure must combineby LTAG operations with the syntax of the ongoingutterance.REA'S syntactic entries combine typical phrase-structure analyses of linguistic constructions withannotations that describe the occurrence of gesturesin coordination with linguistic phrases.
Our devicefor this is a construction SYNC which pairs a descrip-tion of a gesture G with the syntactic structure of aspoken constituent c:SYNC(2) G CThe temporal interpretation of (2) mirrors the rulesfor surface synchrony between speech and gesturepresented in (Cassell et al, 1994).
That is, thepreparatory phase of gesture G is set to begin beforethe time constituent c begins; the stroke of gestureG (the most effortful part) co-occurs with the mostphonologically prominent syllable in c; and, exceptin cases of coarticulation between successive ges-tures, by the time the constituent c is complete, thespeaker must be relaxing and bringing the hands outof gesture space (while the generator specifies yn-chrony as described, in practice the synchronizationof synthesized speech with graphics is an ongoingchallenge in the REA~projeet).-Jn.
sum; 'the produc-tion of gesture G is synchronized with the produc-tion of speech c. (Our representation f synchrony175in a single tree conveniently allows modules dowrL-stream to describe mbodied communicative actionsas marked-up text.
)The syntactic description of the gesture itself in-dicates the choices the generator must make to pro-duce a gesture, but does not analyze a ,gesture lit-er~i|y--~is '~/ hier~chy :i~f ~+p~a~e  m~=~fi~s~-'~f~?
:"= ....stead, these choices specify independent semanticfeatures which we can associate with aspects of agesture (such as handshape and trajectory throughspace).
Our current grammar does not undertake thefinal step of associating semantic features to choiceof particular handshapes and movements, orgesturemorphology; we reserve this problem for later inthe research program.
We allow gesture to accom-pany alternative constituents by introducing alterna-tive syntactic entries; these entries take on differentpragmatic requirements (as described below) to cap-ture their respective discourse functions.So much for syntax.
The second part--the seman-tics of the element--is a formula that specifies thecontent hat the element carries.
Before the entrycan be used, SPUD must establish that the semanticsholds of the entities the entry describes.
If the se-mantics already follows from the common ground,SPUD assumes that the hearer can use it to help iden-tify the entities described.
If the semantics i merelypart of the system's private knowledge, SPUD treatsit as new information for the hearer.Finally, the third part--the pragmatics of theelement--is also a formula that SPUD looks to provebefore using the entry.
Unlike the semantics, how-ever, the pragmatics does not achieve specific com-municative goals like identifying referents.
Instead,the pragmatics establishes a general fit between theentry and the context.The entry schematized in (3) illustrates these threecomponents; the entry also suggests how these com-ponents can define coordinated actions of speechand gesture that respond coherently to the context.
(3) a syntax: sNP VPNP:o V SYNC/have/  G:x I NP:xb semantics: have(o,x)c 'pragmaties:"heardr-n-ew(x) A'theme{O) .
.
.
.
.
(3) describes the use of have to introduce a new fea-ture of (a house) o.
The feature, indicated through-out the entry by the variable x,.is realized as the ob-ject NP of the verb have, but x can also form the ba-sis of a gesture G coordinated with the noun phrase(as indicated by the SYNC constituent).
The entry as-serts that o has x.
(3) is a presentational Construction; in otherwords, it coordinates non-redundant paired speechand gesture in the same way as demonstrated by ourhouse description data.
To represent this constrainton its use, the entry carries two pragmatic require-ments: first, x must be new to the hearer; moreover,o must link up with the open question in the dis-course that the sentence responds to.The pragmatic onditions of (3) help supportour theory of the discourse function of gesture andspeech.
A similar kind of sentence could be usedto address other open questions in the discourse--for example, to answer which house has a garden?This would not be a presentational function, and(3) would be infelicitous here.
In that case, gesturewould naturally coordinate with and elaborate on theanswering information--in this case the house.
Sothe different information structure would activate adifferent entry, where the gesture would coordinatewith the subject and describe o.Meanwhile, alternative ntries like (4a) and(4b)---two entries that both convey (4c) and thatboth could combine with (3) by LTAG operations--underlie our claim that our implementation allowsgesture and speech to draw on a single conceptualsource and fulfill similar communicative intentions.
(4) a syntax: G:xcircular-trajectory RS:x lb syntax: NPNP.
:x VPV NP:p jIsurroundingc semantics: urround(x.p)(4a) provides astructure that could substitute for theG node in (3) to produce semantically and pragmat-ically coordinated speech and gesture.
(4a) speci-fies a right hand gestnre:in.wlhieh.~the hand.
tracesout a circular trajectory; a further decision must de-termine the correct handshape (node RS, as a func-176tion of the entity x that the gesture describes).
Wepair (4a) with the semantics in (4c), and therebymodel that the gesture indicates that one object, x,surrounds another, p. Since p cannot be further de-scribed, p must be identified by an additional pre-supposition ofthe gesture which.picks up~a referenceframe from the sliared context.Similarly, (4b) describes how we could modifythe vP introduced by (3) (using the LTAG operationof adjunction), to produce an utterance such as Ithas a garden surrounding it.
By pairing (4b) withthe same semantics (4c), we ensure that SPUD willtreat he communicative contribution of the alterna-tive constructions of (4) in a parallel fashion.
Bothare triggered by accessing background knowledgeand both are recognized as directly communicatingspecified facts.5 Solving the generat ion problemWe now sketch how entries uch as these combinetogether to account for REA'S utterances.
Our exam-ple is the dialogue in (5):(5) a User: Tell me more about he house.b REA: It has \[a nice garden\].
(right hand, heldfiat, traces a circle)REA's response indicates both that the house has anice garden and that it surrounds the house.As we have seen, (5b) represents a common pat-tern of description; this particular example is moti-vated by an exchange two human subjects had in ourstudy, cf.
(1).
(5b) represents a solution to a gen-eration problem that arises as follows within REA'Soverall architecture.
The user's directive is inter-preted and classified as a directive requiring adelib-erative response.
The dialogue manager recognizesan obligation to respond to the directive, and con-cludes that to fulfill the function of presenting thegarden would discharge this obligation.
The presen-tational function grounds out in the communicativegoal to convey a collection of facts about he garden(type, quality, location relative to the house).
Alongwith these goals, the dialogue manager supplies itscommunicative context, which represents he cen-trality of the house in attentional prominence, cog-nitive status and information structure.In producing (5b) in response to this NLG prob-lem, SPUD both calculates the applicability of and.
~determines a preference,for-theqexiOatized descrip-tors involved.
Initially, (3) is applicable; the systemknows the house has the garden, and represents hegarden as new and the house as questioned.
The en-try can be selected over potential alternatives basedon its interpretation--it achieves a communicativegoal, refers to a prominent entity, and makes a rel-atively specific connection to facts in the context.and what its role might be.
Likewise, we need amodel of the communicative effects of spontaneouscoverbal gesture--one that allows us to reason at-urally about he multiple goals speakers have in pro-ducing each utterance._Similarly, in the .second, stage, SPUD evaluates .andselects (4a) because it Communicates a needed fact 7in a way that helps flesh out a concise, balancedcommunicative act by supplying a gesture that byusing (3) SPUD has already realized belongs here.Choices of remaining elements--the words gardenand nice, the semantic features to represent the gar-den in the gesture--proceed similarly.
Thus SPUDarrives at the response in (5b) just by reasoning fromthe declarative specification ofthe meaning and con-text of communicative actions.6 Re la ted  WorkThe interpretation of speech and gesture has beeninvestigated since the pioneering work of (Bolt,1980) on deictic gesture; recent work includes(Koons et al, 1993; Bolt and Herranz, 1992).
Sys-tems have also attempted generation of gesture inconjunction with speech.
Lester et al (1998) gener-ate deictic gestures and choose referring expressionsas a function of the potential ambiguity of objects re-ferred to, and their proximity to the animated agent.Rickel and Johnson (1999)'s pedagogical gent pro-duces a deictic gesture at the beginning of explana-tions about objects in the virtual world.
Andr6 etal.
(1999) generate pointing estures as a sub-actionof the rhetorical action of labeling, in turn a sub-action of elaborating.Missing from these prior systems, however, is arepresentation f communicative action that treatsthe different modalities on a par.
Such representa-tions have been explored in research on combininglinguistic and graphical interaction.
For example,multimodal managers have been described to allo-cate an underlying content representation for gen-eration of text and graphics (Wahlster et al, 1991;Green et al, 1998).
Meanwhile, (Johnston et al,1997; Johnston, 1998) describe a formalism fortightly-coupled interpretation which uses a gram-mar and semantic onstraints oanalyze input fromspeech and pen.
While many insights from theseformalisms are relevant in embodied conversation,spontaneous gesture requires adistinct analysis withdifferent emphasis:For example;-we need some no-tion of discourse pragmatics that would allow us topredict where gesture occurs with respect to speech,Conclusion .
.
.
.
.Research on the robustness of human conversationsuggests that a dialogue agent capable of actingas a conversational partner would provide for effi-cient and natural collaborative dialogue.
But humanconversational partners display gestures that derivefrom the same underlying conceptual source as theirspeech, and which relate appropriately to their com-municative intent.
In this paper, we have summa-rized the evidence for this view of human conver-sation, and shown how it informs the generationof communicative action in our artificial embodiedconversational agent, REA.
REA has a working im-plementation, which includes the modules describedin this paper, and can engage in a variety of interac-tions including that in (5).
Experiments are under-way to investigate the extent o which REA 'S conver-sational capacities share the strengths of the humancapacities they are modeled on.AcknowledgmentsThe research reported here was supported by NSF (awardIIS-9618939), Deutsche Telekom, AT&T, and the othergenerous sponsors of the MIT Media Lab, and a postdoc-toral fellowship from RUCCS.
Hannes Vilhj~ilmsson as-sisted with the implementation of REA'S discourse man-ager.
We thank Nancy Green.
James Lester, Jeff Rickel,Candy Sidner, and anonymous reviewers for commentson this and earlier drafts.ReferencesElisabeth Andr6, Thomas Rist, and Jochen Mailer.
1999.Employing AI methods to control the behavior of ani-mated interface agents.
AppliedArtificial Intelligence,13:415-448.Douglas Appelt.
1985.
Planning English Sentences.Cambridge University Press, Canlbridge England.R.
A. Bolt and E. Herranz.
1992.
Two-handed gesture inmulti-modal natural dialog.
In UIST92: Fifth AmmalSymposium on User Interface Software and Technol-ogy.R.
A. Bolt.
1980.
Put-that-there: voice and gesture at thegraphics interface.
Computer Graphics.
14(3):262-270.J.
Cassell and K. Th6risson.
1999.
The power of a nodand a glance: Envelope vs. emotional feedback in an-imatefd conversational agents.
AppliedArt(ficial Intel-ligence, 13(3).177Justine Casseil, Catherine Pelachaud, Norm Badler,Mark Steedman, Brett Achorn, Tripp Becket, BrettDouville, Scott Prevost, and Matthew Stone.
1994.Animated conversation: Rule-based generation of fa-cial expression, gesture and spoken intonation for mul-tiple conversational gents.
In SIGGRAPH, pagesPatrick FitzGerald.
1998.
Deictic and emotive com-munication i  animated pedagogical gents.
In Work-shop on Embodied Conversational Characters.David McNeill.
1992.
Hand and Mind: What GesturesReveal about Thought.
University of Chicago Press,Chicago.413-420.J.
Cassell, D. McNeill, and K. E. McCullough.
1999.Speech-gesture mismatches: evidence for one under-lying representation f linguistic and nonlinguistic n-formation.
Pragmatics and Cognition, 6(2).Justine Cassell.
2000a.
Embodied conversational inter-face agents.
Communications of the ACM, 43(4):70-78.Justine Cassell.
2000b.
Nudge nudge wink wink: Ele-ments of face-to-face conversation for embodied con-versational gents.
In J. Cassell, J. Sullivan, S. Pre-vost, and E. Churchill, editors, Embodied Conversa-tional Agents, pages 1-28.
MIT Press, Cambridge,MA.Robert Dale.
1992.
Generating Referring Expressions:Constructing Descriptions ina Domain of Objects andProcesses.
MIT Press, Cambridge MA.S.
Feiner and K. McKeown.
1991.
Automating the gen-eration of coordinated multimedia explanations.
IEEEComputer, 24(10): 33-41.Nancy Green, Giuseppe Carenini, Stephan Kerpedjiev,Steven Roth, and Johanna Moore.
1998.
A media-independent content language for integrated text andgraphics generation.
In CVIR '98- Workshop on Con-tent Visualization and Intermedia Representations.Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski.1993.
Cognitive status and the form of referring ex-pressions in discourse.
Language, 69(2):274--307.M.
Johnston, P. R. Cohen, D. McGee, J. Pittman, S. L.Oviatt, and I. Smith.
1997.
Unification-based multi-modal integration.
In ACL/EACL 97: Proceedings ofthe Annual Meeting of the Assocation for Computa-tional Linguistics.Michael Johnston.
1998.
Unification-based naultimodalparsing.
In COLING/ACL.Aravind K. Joshi, L. Levy, and M. Takahashi.
1975.
Treeadjunct grammars.
Journal of the Computer and Sys-tem Sciences, 10:136--- 163.S.
D. Kelly, J. D. Barr, R. B.
Church, and K. Lynch.
1999.Offering a hand to pragmatic understanding: The roleof speech and gesture in comprehension a d memory.Journal of Memory and Language, 40:577-592.A.
Kendon.
1974.
Movement coordination i social in-teraction: somem examples described.
In S. Weitz, ed-itor, Nonverbal Communication.
Oxford, New York.D.
B. Koons.
C. J. Sparrell, and K. R. Th6risson.
1993.Integrating simultaneous input from speech, gaze andhand gestures.
In M. T. Maybtiry,'-editor~-lntel/igentMulti-media Interfaces.
MIT Press, Cambridge.James Lester, Stuart Towns.
Charles Calloway, and.
.
.
.
.
.
.
.
- ~ .....  :.~,, ~Joharma:Moore; ~,~994,,P, articipating in Explanatory Di-alogues.
MIT Press, Cambridge MA.S.
L. Oviatt.
1995.
Predicting spoken language disflu-encies during human-computer interaction.
ComputerSpeech and Language, 9( l ): 19-35.Ellen Prince.
1986.
On the syntactic marking of pre-supposed open propositions.
In Proceedings of the22nd Annual Meeting of the Chicago Linguistic Soci-ety, pages 208-222, Chicago.
CLS.Ellen F. Prince.
1992.
The ZPG letter: Subjects, definite-ness and information status.
In William C. Mann andSandra A. Thompson, editors, Discourse Description:Diverse Analyses of a Fund-raising Text, pages 295-325.
John Benjamins, Philadelphia.Jeff Rickel and W. Lewis Johnson.
1999.
Animatedagents for procedural training in virtual reality: Per-ception, cognition and motor control.
Applied Artifi-cial Intelligence, 13:343-382.W.
T. Rogers.
1978.
The contribution of kinesic illus-trators towards the comprehension f verbal behaviorwithin utterances.
Human Communication Research,5:54--62.Yves Schabes.
1990.
Mathematical nd ComputationalAspects of Lexicalized Grammars.
Ph.D. thesis, Com-puter Science Department, University of Pennsylva-nia.Mark Steedman.
1991.
Structure and intonation.
Lan-guage, 67:260-296.Matthew Stone and Christine Doran.
1997.
Sentenceplanning as description using tree-adjoining grammar.In Proceedings of ACL, pages 198-205.Matthew Stone, Tonia Bleam, Christine Doran, andMartha Palmer.
2000.
Lexicalized grammar and thedescription of motion events.
In TAG+: Workshop onTree-Adjoining Grammar and Related Forntalisms.Michael Strube.
1998.
Never look back: An alternativeto centering.
In Proceedings of COLING-ACL.L.
A. Thompson and D. W. Massaro.
1986.
Evaluationand integration of speech and pointing gestures dur-ing referential understanding.
Journal of Experimen-tal Child Psychology, 42:144-168.David R. Traum and James F. Allen.
1994.
Discourseobligations in dialogue processing.
In ACL, pages 1-8.W.
Wahlster, E. Andr6, W. Graf, and T. Rist.
1991.Designing illustrated texts.
In Proceedings of EACL,pages 8-14.Hao Yan.~ 20,00.
Paired speech~and gesture generation iembodied conversational agents~ Master's thesis, Me-dia Lab, MIT.178
