Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 487?494,New York, June 2006. c?2006 Association for Computational LinguisticsUnlimited vocabulary speech recognition for agglutinative languagesMikko Kurimo1, Antti Puurula1, Ebru Arisoy2, Vesa Siivola1,Teemu Hirsim?ki1, Janne Pylkk?nen1, Tanel Alum?e3, Murat Saraclar21 Adaptive Informatics Research Centre, Helsinki University of TechnologyP.O.Box 5400, FIN-02015 HUT, Finland{Mikko.Kurimo,Antti.Puurula,Vesa.Siivola}@tkk.fi2 Bogazici University, Electrical and Electronics Eng.
Dept.34342 Bebek, Istanbul, Turkey{arisoyeb,murat.saraclar}@boun.edu.tr3 Laboratory of Phonetics and Speech Technology,Institute of Cybernetics, Tallinn Technical University, Estoniatanel.alumae@phon.ioc.eeAbstractIt is practically impossible to build aword-based lexicon for speech recogni-tion in agglutinative languages that wouldcover all the relevant words.
The prob-lem is that words are generally built byconcatenating several prefixes and suffixesto the word roots.
Together with com-pounding and inflections this leads to mil-lions of different, but still frequent wordforms.
Due to inflections, ambiguity andother phenomena, it is also not trivial toautomatically split the words into mean-ingful parts.
Rule-based morphologicalanalyzers can perform this splitting, butdue to the handcrafted rules, they also suf-fer from an out-of-vocabulary problem.
Inthis paper we apply a recently proposedfully automatic and rather language andvocabulary independent way to build sub-word lexica for three different agglutina-tive languages.
We demonstrate the lan-guage portability as well by building asuccessful large vocabulary speech recog-nizer for each language and show superiorrecognition performance compared to thecorresponding word-based reference sys-tems.1 IntroductionSpeech recognition for dictation or prepared radioand television broadcasts has had huge advancesduring the last decades.
For example, broadcastnews (BN) in English can now be recognized withabout ten percent word error rate (WER) (NIST,2000) which results in mostly quite understandabletext.
Some rare and new words may be missing butthe result has proven to be sufficient for many im-portant applications, such as browsing and retrievalof recorded speech and information retrieval fromthe speech (Garofolo et al, 2000).
However, besidesthe development of powerful computers and new al-gorithms, a crucial factor in this development is thevast amount of transcribed speech and suitable textdata that has been collected for training the mod-els.
The problem faced in porting the BN recogni-tion systems to conversational speech or to other lan-guages is that almost as much new speech and textdata have to be collected again for the new task.The reason for the need for a vast amount of train-ing texts is that the state-of-the-art statistical lan-guage models contain a huge amount of parametersto be estimated in order to provide a proper probabil-ity for any possible word sequence.
The main reasonfor the huge model size is that for an acceptable cov-erage in an English BN task, the vocabulary mustbe very large, at least 50,000 words, or more.
Forlanguages with a higher degree of word inflectionsthan English, even larger vocabularies are required.This paper focuses on the agglutinative languages inwhich words are frequently formed by concatenat-ing one or more stems, prefixes, and suffixes.
Forthese languages in which the words are often highlyinflected as well as formed from several morphemes,even a vocabulary of 100,000 most common wordswould not give sufficient coverage (Kneissler and487Klakow, 2001; Hirsim?ki et al, 2005).
Thus, thesolution to the language modeling clearly has to in-volve splitting of words into smaller modeling unitsthat could then be adequately modeled.This paper focuses on solving the vocabularyproblem for several languages in which the speechand text database resources are much smaller thanfor the world?s main languages.
A common fea-ture for the agglutinative languages, such as Finnish,Estonian, Hungarian and Turkish is that the largevocabulary continuous speech recognition (LVCSR)attempts so far have not resulted comparable perfor-mance to the English systems.
The reason for thisis not only the language modeling difficulties, but,of course, the lack of suitable speech and text train-ing data resources.
In (Geutner et al, 1998; Sii-vola et al, 2001) the systems aim at reducing theactive vocabulary and language models to a feasi-ble size by clustering and focusing.
In (Szarvas andFurui, 2003; Alum?e, 2005; Hacioglu et al, 2003)the words are split into morphemes by language-dependent hand-crafted morphological rules.
In(Kneissler and Klakow, 2001; Arisoy and Arslan,2005) different combinations of words, grammati-cal morphemes and endings are utilized to decreasethe OOV rate and optimize the speech recognitionaccuracy.
However, constant large improvementsover the conventional word-based language modelsin LVCSR have been rare.The approach presented in this paper relies on adata-driven algorithm called Morfessor (Creutz andLagus, 2002; Creutz and Lagus, 2005) which is alanguage independent unsupervised machine learn-ing method to find morpheme-like units (called sta-tistical morphs) from a large text corpus.
Thismethod has several advantages over the rule-basedgrammatical morphemes, e.g.
that no hand-craftedrules are needed and all words can be processed,even the foreign ones.
Even if good grammaticalmorphemes are available, the language modeling re-sults by the statistical morphs seem to be at least asgood, if not better (Hirsim?ki et al, 2005).
In thispaper we evaluate the statistical morphs for threeagglutinative languages and describe three differentspeech recognition systems that successfully utilizethe n-gram language models trained for these unitsin the corresponding LVCSR tasks.2 Building the lexicon and languagemodels2.1 Unsupervised discovery of morph unitsNaturally, there are many ways to split the wordsinto smaller units to reduce a lexicon to a tractablesize.
However, for a subword lexicon suitablefor language modeling applications such as speechrecognition, several properties are desirable:1.
The size of the lexicon should be small enoughthat the n-gram modeling becomes more feasi-ble than the conventional word based modeling.2.
The coverage of the target language by wordsthat can be built by concatenating the unitsshould be high enough to avoid the out-of-vocabulary problem.3.
The units should be somehow meaningful, sothat the previously observed units can help inpredicting the next one.4.
In speech recognition one should be able to de-termine the pronunciation for each unit.A common approach to find the subword unitsis to program the language-dependent grammaticalrules into a morphological analyzer and utilize thatto then split the text corpus into morphemes as ine.g.
(Hirsim?ki et al, 2005; Alum?e, 2005; Ha-cioglu et al, 2003).
There are some problems re-lated to ambiguous splits and pronunciations of veryshort inflection-type units, but also the coverage in,e.g., news texts may be poor because of many namesand foreign words.In this paper we have adopted a similar approachas (Hirsim?ki et al, 2005).
We use unsupervisedlearning to find the best units according to some costfunction.
In the Morfessor algorithm the minimizedcost is the coding length of the lexicon and the wordsin the corpus represented by the units of the lexicon.This minimum description length based cost func-tion is especially appealing, because it tends to giveunits that are both as frequent and as long as possi-ble to suit well for both training the language modelsand also decoding of the speech.
Full coverage ofthe language is also guaranteed by splitting the rarewords into very short units, even to single phonemesif necessary.
For language models utilized in speech488recognition, the lexicon of the statistical morphs canbe further reduced by omitting the rare words fromthe input of the Morfessor algorithm.
This operationdoes not reduce the coverage of the lexicon, becauseit just splits the rare words then into smaller units,but the smaller lexicon may offer a remarkable speedup of the recognition.The pronunciation of, especially, the short unitsmay be ambiguous and may cause severe problemsin languages like English, in which the pronuncia-tions can not be adequately determined from the or-thography.
In most agglutinative languages, such asFinnish, Estonian and Turkish, rather simple letter-to-phoneme rules are, however, sufficient for mostcases.2.2 Building the lexicon for open vocabularyThe whole training text corpus is first passed througha word splitting transformation as in Figure 1.
Basedon the learned subword unit lexicon, the best splitfor each word is determined by performing a Viterbisearch with the unigram probabilities of the units.
Atthis point the word break symbols are added betweeneach word in order to incorporate that information inthe statistical language models, as well.
Then the n-gram models are trained similarly as if the languageunits were words including word and sentence breaksymbols as additional units.2.3 Building the n-gram model over morphsEven though the required morph lexicon is muchsmaller than the lexicon for the corresponding wordn-gram estimation, the data sparsity problem is stillimportant.
Interpolated Kneser-Ney smoothing isutilized to tune the language model probabilities inthe same way as found best for the word n-grams.The n-grams that are not very useful for modelingthe language can be discarded from the model inorder to keep the model size down.
For Turkish,we used the entropy based pruning (Stolcke, 1998),where the n-grams, that change the model entropyless than a given treshold, are discarded from themodel.
For Finnish and Estonian, we used n-gramgrowing (Siivola and Pellom, 2005).
The n-gramsthat increase the training set likelihood enough withrespect to the corresponding increase in the modelsize are accepted into the model (as in the minimumdescription length principle).
After the growing pro-Morph lexicon+ probabilitiesword formsDistinctText with wordssegmented intomorphsmodelLanguageText corpussegmentationViterbisegmentationMorphExtractvocabularyTrainn?gramsFigure 1: The steps in the process of estimating alanguage model based on statistical morphs from atext corpus (Hirsim?ki et al, 2005).cess the model is further pruned with entropy basedpruning.
The method allows us to train models withhigher order n-grams, since the memory consump-tion is lower and also gives somewhat better mod-els.
Both methods can also be viewed as choosingthe correct model complexity for the training data toavoid over-learning.3 Statistical properties of Finnish,Estonian and TurkishBefore presenting the speech recognition results,some statistical properties are presented for the threeagglutinative languages studied.
If we considerchoosing a vocabulary of the 50k-70k most commonwords, as usual in English broadcast news LVCSRsystems, the out-of-vocabulary (OOV) rate in En-glish is typically smaller than 1%.
Using the lan-guage model training data the following OOV ratescan be found for a vocabulary including only themost common words: 15% OOV for 69k in Finnish(Hirsim?ki et al, 2005), 10% for 60k in Estonianand 9% for 50k in Turkish.
As shown in (Hacioglu etal., 2003) this does not only mean the same amountof extra speech recognition errors, but even more,because the recognizer tends to lose track when un-known words get mapped to those that are in the vo-cabulary.
Even doubling the vocabulary is not a suf-4890 1 2 3x 10602468 x 105Number of sentencesNumber of distinctunits0 1 2 3x 1062.62.833.23.43.6 x 104Number of sentencesNumber of distinctmorphsMorphsWords MorphsFigure 2: Vocabulary growth of words and morphsfor Turkish languageficient solution, because a vocabulary twice as large(120k) would only reduce the OOV rate to 6% inEstonian and 5% in Turkish.
In Finnish even a 400kvocabulary of the most common words still gives 5%OOV in the language model training material.Figure 2 illustrates the vocabulary explosion en-countered when using words and how using morphsavoids this problem for Turkish.
The figure on theleft shows the vocabulary growth for both words andmorphs.
The figure on the right shows the graphfor morphs in more detail.
As seen in the figure,the number of new words encountered continues toincrease as the corpus size gets larger whereas thenumber of new morphs encountered levels off.4 Speech recognition experiments4.1 About selection of the recognition tasksIn this work the morph-based language models havebeen applied in speech recognition for three differ-ent agglutinative languages, Finnish, Estonian andTurkish.
The recognition tasks are speaker depen-dent and independent fluent dictation of sentencestaken from newspapers and books, which typicallyrequire very large vocabulary language models.4.2 FinnishFinnish is a highly inflected language, in whichwords are formed mainly by agglutination and com-pounding.
Finnish is also the language for which thealgorithm for the unsupervised morpheme discovery(Creutz and Lagus, 2002) was originally developed.The units of the morph lexicon for the experimentsin this paper were learned from a joint corpus con-taining newspapers, books and newswire stories oftotally about 150 million words (CSC, 2001).
Weobtained a lexicon of 25k morphs by feeding thelearning algorithm with the word list containing the160k most common words.
For language modeltraining we used the same text corpus and the re-cently developed growing n-gram training algorithm(Siivola and Pellom, 2005).
The amount of resultedn-grams are listed in Table 4.
The average lengthof a morph is such that a word corresponds to 2.52morphs including a word break symbol.The speech recognition task consisted of a bookread aloud by one female speaker as in (Hirsim?ki etal., 2005).
Speaker dependent cross-word triphonemodels were trained using the first 12 hours of dataand evaluated by the last 27 minutes.
The modelsincluded tied state hidden Markov models (HMMs)of totally 1500 different states, 8 Gaussian mixtures(GMMs) per state, short-time mel-cepstral features(MFCCs), maximum likelihood linear transforma-tion (MLLT) and explicit phone duration models(Pylkk?nen and Kurimo, 2004).
The real-time fac-tor of recognition speed was less than 10 xRT witha 2.2 GHz CPU.
However, with the efficient LVCSRdecoder utilized (Pylkk?nen, 2005) it seems that bymaking an even smaller morph lexicon, such as 10k,the decoding speed could be optimized to only a fewtimes real-time without an excessive trade-off withrecognition performance.4.3 EstonianEstonian is closely related to Finnish and a similarlanguage modeling approach was directly appliedto the Estonian recognition task.
The text corpusused to learn the morph units and train the statis-tical language model consisted of newspapers andbooks, altogether about 55 million words (Segakor-pus, 2005).
At first, 45k morph units were obtainedas the best subword unit set from the list of the 470kmost common words in the corpora.
For speed-ing up the recognition, the morph lexicon was after-wards reduced to 37k by splitting the rarest morphs(occurring in only one or two words) further intosmaller ones.
Corresponding growing n-gram lan-guage models as in Finnish were trained from theEstonian corpora resulting the n-grams in Table 4.The speech recognition task in Estonian consistedof long sentences read by 50 randomly picked held-out test speakers, 7 sentences each (a part of (Meister490et al, 2002)).
Unlike the Finnish and Turkish micro-phone data, this data was recorded from telephone,i.e.
8 kHz sampling rate and narrow band data in-stead of 16 kHz and normal (full) bandwidth.
Thephoneme models were trained for speaker indepen-dent recognition using windowed cepstral mean sub-traction and significantly more data (over 200 hoursand 1300 speakers) than for the Finnish task.
Thespeaker independence, together with the telephonequality and occasional background noises, made thistask still a considerably more difficult one.
Other-wise the acoustic models were similar cross-wordtriphone GMM-HMMs with MFCC features, MLLTtransformation and the explicit phone duration mod-eling, except larger: 5100 different states and 16GMMs per state.
Thus, the recognition speed isalso slower than in Finnish, about 20 xRT (2.2GHzCPU).4.4 TurkishTurkish is another a highly-inflected and agglutina-tive language with relatively free word order.
Thesame Morfessor tool (Creutz and Lagus, 2005) as inFinnish and Estonian was applied to Turkish textsas well.
Using the 360k most common words fromthe training corpus, 34k morph units were obtained.The training corpus consists of approximately 27Mwords taken from literature, law, politics, socialsciences, popular science, information technology,medicine, newspapers, magazines and sports news.N-gram language models for different orders withinterpolated Kneser-Ney smoothing as well as en-tropy based pruning were built for this morph lexi-con using the SRILM toolkit (Stolcke, 2002).
Thenumber of n-grams for the highest order we tried (6-grams without entropy-based pruning) are reportedin Table 4.
In average, there are 2.37 morphs perword including the word break symbol.The recognition task in Turkish consisted of ap-proximately one hour of newspaper sentences readby one female speaker.
We used decision-tree stateclustered cross-word triphone models with approx-imately 5000 HMM states.
Instead of using letterto phoneme rules, the acoustic models were baseddirectly on letters.
Each state of the speaker inde-pendent HMMs had a GMM with 6 mixture compo-nents.
The HTK frontend (Young et al, 2002) wasused to get the MFCC based acoustic features.
Theexplicit phone duration models were not applied.The training data contained 17 hours of speech fromover 250 speakers.
Instead of the LVCSR decoderused in Finnish and Estonian (Pylkk?nen, 2005), theTurkish evaluation was performed using another de-coder (AT&T, 2003), Using a 3.6GHz CPU, the real-time factor was around one.5 ResultsThe recognition results for the three different tasks:Finnish, Estonian and Turkish, are provided in Ta-bles 1 ?
3.
In each task the word error rate (WER)and letter error rate (LER) statistics for the morph-based system is compared to a corresponding word-based system.
The resulting morpheme strings areglued to words according to the word break symbolsincluded in the language model (see Section 2.2) andthe WER is computed as the sum of substituted, in-serted and deleted words divided by the correct num-ber of words.
LER is included here as well, becausealthough WER is a more common measure, it is notcomparable between languages.
For example, in ag-glutinative languages the words are long and containa variable amount of morphemes.
Thus, any incor-rect prefix or suffix would make the whole word in-correct.
The n-gram language model statistics aregiven in Table 4.Finnish lexicon WER LERWords 400k 8.5 1.20Morphs 25k 7.0 0.95Table 1: The LVCSR performance for the speaker-dependent Finnish task consisting of book-reading(see Section 4.2).
For a reference (word-based) lan-guage model a 400k lexicon was chosen.Estonian lexicon WER LERWords 60k 56.3 22.4Morphs 37k 47.6 18.9Table 2: The LVCSR performance for the speaker-independent Estonian task consisting of read sen-tences recorded via telephone (see Section 4.3).
Fora reference (word-based) language model a 60k lex-icon was used here.491Turkish lexicon WER LERWords3-gram 50k 38.8 15.2Morphs3-gram 34k 39.2 14.84-gram 34k 35.0 13.15-gram 34k 33.9 12.4Morphs, rescored by morph 6-gram3-gram 34k 33.8 12.44-gram 34k 33.2 12.35-gram 34k 33.3 12.2Table 3: The LVCSR performance for the speaker-independent Turkish task consisting of read news-paper sentences (see Section 4.4).
For the refer-ence 50k (word-based) language model the accuracygiven by 4 and 5-grams did not improve from that of3-grams.In the Turkish recognizer the memory constraintsduring network optimization (Allauzen et al, 2004)allowed the use of language models only up to 5-grams.
The language model pruning thresholds wereoptimized over a range of values and the best re-sults are shown in Table 3.
We also tried the sameexperiments with two-pass recognition.
In the firstpass, instead of the best path, lattice output was gen-erated with the same language models with prun-ing.
Then these lattices were rescored using the non-pruned 6-gram language models (see Table 4) andthe best path was taken as the recognition output.For the word-based reference model, the two-passrecognition gave no improvements.
It is likely thatthe language model training corpus was too small totrain proper 6-gram word models.
However, for themorph-based model, we obtained a slight improve-ment (0.7 % absolute) by two-pass recognition.6 DiscussionThe key result of this paper is that we can success-fully apply the unsupervised statistical morphs inlarge vocabulary language models in all the three ex-perimented agglutinative languages.
Furthermore,the results show that in all the different LVCSRtasks, the morph-based language models performvery well and constantly dominate the reference lan-guage model based on words.
The way that the lexi-# morph-based modelsngrams Finnish Estonian Turkish1grams 24,833 37,061 34,3322grams 2,188,476 1,050,127 655,6213grams 17,064,072 7,133,902 1,936,2634grams 25,200,308 8,201,543 3,824,3625grams 7,167,021 3,298,429 4,857,1256grams 624,832 691,899 5,523,9227grams 23,851 55,363 -8grams 0 1045 -Sum 52,293,393 20,469,369 16,831,625Table 4: The amount of different n-grams in eachlanguage model based on statistical morphs.
Notethat the Turkish language model was not preparedby the growing n-gram algorithm as the others andthe model was limited to 6-grams.con is built from the word fragments allows the con-struction of statistical language models, in practice,for almost an unlimited vocabulary by a lexicon thatstill has a convenient size.The recognition was here restricted to agglutina-tive languages and tasks in which the language usedis both rather general and matches fairly well withthe available training texts.
Significant performancevariation in different languages can be observedhere, because of the different tasks and the fact thatcomparable recognition conditions and training re-sources have not been possible to arrange.
However,we believe that the tasks are still both difficult andrealistic enough to illustrate the difference of per-formance when using language models based on alexicon of morphs vs. words in each task.
There areno directly comparable previous LVCSR results onthe same tasks and data, but the closest ones whichcan be found are slightly over 20% WER for theFinnish task (Hirsim?ki et al, 2005), slightly over40 % WER for the Estonian task (Alum?e, 2005)and slightly over 30 % WER for the Turkish task(Erdogan et al, 2005).Naturally, it is also possible to prepare a huge lex-icon and still succeed in recognition fairly well (Sar-aclar et al, 2002; McTait and Adda-Decker, 2003;Hirsim?ki et al, 2005), but this is not a very con-venient approach because of the resulting huge lan-guage models or the heavy pruning required to keep492them still tractable.
The word-based language mod-els that were constructed in this paper as referencemodels were trained as much as possible in the sameway as the corresponding morph language models.For Finnish and Estonian the growing n-grams (Sii-vola and Pellom, 2005) were used including the op-tion of constructing the OOV words from phonemesas in (Hirsim?ki et al, 2005).
For Turkish a con-ventional n-gram was built by SRILM similarly asfor the morphs.
The recognition approach taken forTurkish involves a static decoding network construc-tion and optimization resulting in near real time de-coding.
However, the memory requirements of net-work optimization becomes prohibitive for large lex-icon and language models as presented in this paper.In this paper the recognition speed was not a ma-jor concern, but from the application point of viewthat is a very important factor to be taken into a ac-count in the comparison.
It seems that the major fac-tors that make the recognition slower are short lexi-cal units, large lexicon and language models and theamount of Gaussian mixtures in the acoustic model.7 ConclusionsThis work presents statistical language modelstrained on different agglutinative languages utilizinga lexicon based on the recently proposed unsuper-vised statistical morphs.
To our knowledge this isthe first work in which similarly developed subwordunit lexica are developed and successfully evaluatedin three different LVCSR systems in different lan-guages.
In each case the morph-based approach con-stantly shows a significant improvement over a con-ventional word-based LVCSR language models.
Fu-ture work will be the further development of alsothe grammatical morph-based language models andcomparison of that to the current approach, as wellas extending this evaluation work to new languages.8 AcknowledgmentsWe thank the Finnish Federation of the Visually Im-paired for providing the Finnish speech data and theFinnish news agency (STT) and the Finnish IT cen-ter for science (CSC) for the text data.
Our work wassupported by the Academy of Finland in the projectsNew information processing principles, Adaptive In-formatics and New adaptive and learning methods inspeech recognition.
This work was supported in partby the IST Programme of the European Community,under the PASCAL Network of Excellence, IST-2002-506778.
The authors would like to thank Sa-banci and ODTU universities for the Turkish acous-tic and text data and AT&T Labs ?
Research forthe software.
This research is partially supportedby SIMILAR Network of Excellence and TUBITAKBDP (Unified Doctorate Program of the Scientificand Technological Research Council of Turkey).ReferencesCyril Allauzen, Mehryar Mohri, Michael Riley, and BrianRoark.
2004.
A generalized construction of integratedspeech recognition transducers.
In Proceedings of theIEEE International Conference on Acoustics, Speechand Signal Processing (ICASSP), Montreal, Canada.Tanel Alum?e.
2005.
Phonological and morphologi-cal modeling in large vocabulary continuous Estonianspeech recognition system.
In Proceedings of SecondBaltic Conference on Human Language Technologies,pages 89?94.Mehryar Mohri and Michael D. Riley.
DCD Library ?Speech Recognition Decoder Library.
AT&T Labs ?Research.
http://www.research.att.com/sw/tools/dcd/.Ebru Arisoy and Levent Arslan.
2005.
Turkish dictationsystem for broadcast news applications.
In 13th Euro-pean Signal Processing Conference - EUSIPCO 2005,Antalya, Turkey, September.Mathias Creutz and Krista Lagus.
2002.
Unsuperviseddiscovery of morphemes.
In Proceedings of the Work-shop on Morphological and Phonological Learning ofACL-02, pages 21?30.Mathias Creutz and Krista Lagus.
2005.
Unsuper-vised morpheme segmentation and morphology in-duction from text corpora using Morfessor.
Techni-cal Report A81, Publications in Computer and Infor-mation Science, Helsinki University of Technology.URL: http://www.cis.hut.fi/projects/morpho/.J.
Garofolo, G. Auzanne, and E. Voorhees.
2000.
TheTREC spoken document retrieval track: A successstory.
In Proceedings of Content Based MultimediaInformation Access Conference, April 12-14.P.
Geutner, M. Finke, and P. Scheytt.
1998.
Adap-tive vocabularies for transcribing multilingual broad-cast news.
In Proceedings of the IEEE InternationalConference on Acoustics, Speech and Signal Process-ing (ICASSP), Seattle, WA, USA, May.493H.
Erdogan, O. Buyuk, K. Oflazer.
2005.
Incorporatinglanguage constraints in sub-word based speech recog-nition.
IEEE Automatic Speech Recognition and Un-derstanding Workshop, Cancun, Mexico.Kadri Hacioglu, Brian Pellom, Tolga Ciloglu, Ozlem Oz-turk, Mikko Kurimo, and Mathias Creutz.
2003.
Onlexicon creation for Turkish LVCSR.
In Proceedingsof 8th European Conference on Speech Communica-tion and Technology, pages 1165?1168.Teemu Hirsim?ki, Mathias Creutz, Vesa Siivola, MikkoKurimo, Sami Virpioja, and Janne Pylkk?nen.
2005.Unlimited vocabulary speech recognition with morphlanguage models applied to Finnish.
Computer Speechand Language.
(accepted for publication).Jan Kneissler and Dietrich Klakow.
2001.
Speech recog-nition for huge vocabularies by using optimized sub-word units.
In Proceedings of the 7th European Con-ference on Speech Communication and Technology(Eurospeech), pages 69?72, Aalborg, Denmark.CSC Tieteellinen laskenta Oy.
2001.
Finnish Lan-guage Text Bank: Corpora Books, Newspapers,Magazines and Other.
http://www.csc.fi/kielipankki/.Kevin McTait and Martine Adda-Decker.
2003.
The300k LIMSI German Broadcast News TranscriptionSystem.
In Proceedings of 8th European Conferenceon Speech Communication and Technology.Einar Meister, J?rgen Lasn, and Lya Meister.
2002.
Esto-nian SpeechDat: a project in progress.
In Proceedingsof the Fonetiikan P?iv?t ?
Phonetics Symposium 2002in Finland, pages 21?26.NIST.
2000.
Proceedings of DARPA workshop on Auto-matic Transcription of Broadcast News.
NIST, Wash-ington DC, May.Janne Pylkk?nen.
2005.
New pruning criteria for effi-cient decoding.
In Proceedings of 9th European Con-ference on Speech Communication and Technology.Janne Pylkk?nen and Mikko Kurimo.
2004.
Durationmodeling techniques for continuous speech recogni-tion.
In Proceedings of the International Conferenceon Spoken Language Processing.Murat Saraclar, Michael Riley, Enrico Bocchieri, andVincent Goffin.
2002.
Towards automatic closed cap-tioning: Low latency real time broadcast news tran-scription.
In Proceedings of the International Confer-ence on Spoken Language Processing (ICSLP), Den-ver, CO, USA.Segakorpus ?
Mixed Corpus of Estonian.
Tartu Uni-versity.
http://test.cl.ut.ee/korpused/segakorpus/.Vesa Siivola and Bryan Pellom.
2005.
Growing an n-gram language model.
In Proceedings of 9th EuropeanConference on Speech Communication and Technol-ogy.Vesa Siivola, Mikko Kurimo, and Krista Lagus.
2001.Large vocabulary statistical language modeling forcontinuous speech recognition.
In Proceedings of 7thEuropean Conference on Speech Communication andTechnology, pages 737?747, Aalborg, Copenhagen.Andreas Stolcke.
1998.
Entropy-based pruning of back-off language models.
In Proc.
DARPA Broadcast NewsTranscription and Understanding Workshop, pages270?274.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,pages 901?904.Mate Szarvas and Sadaoki Furui.
2003.
Evaluation of thestochastic morphosyntactic language model on a onemillion word Hungarian task.
In Proceedings of the8th European Conference on Speech Communicationand Technology (Eurospeech), pages 2297?2300.S.
Young, D. Ollason, V. Valtchev, and P. Woodland.2002.
The HTK book (for HTK version 3.2.
), March.494
