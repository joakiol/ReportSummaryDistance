Towards Free-text Semantic Parsing: A Unified Framework Based onFrameNet, VerbNet and PropBankAna-Maria Giuglea and Alessandro MoschittiUniversity of Rome ?Tor Vergata?,Rome, Italyana-maria.giuglea@topex.romoschitti@info.uniroma2.itAbstractThis article describes a robust semanticparser that uses a broad knowledge basecreated by interconnecting three majorresources: FrameNet, VerbNet andPropBank.
The FrameNet corpus con-tains the examples annotated with se-mantic roles whereas the VerbNet lexi-con provides the knowledge about thesyntactic behavior of the verbs.
Weconnect VerbNet and FrameNet bymapping the FrameNet frames to theVerbNet Intersective Levin classes.
ThePropBank corpus, which is tightly con-nected to the VerbNet lexicon, is used toincrease the verb coverage and also totest the effectiveness of our approach.The results indicate that our model is aninteresting step towards the design offree-text semantic parsers.1 IntroductionDuring the last years a noticeable effort has beendevoted to the design of lexical resources thatcan provide the training ground for automaticsemantic role labelers.
Unfortunately, most of thesystems developed until now are confined to thescope of the resource that they use during thelearning stage.
A very recent example in thissense was provided by the CONLL 2005 SharedTask on PropBank (Kingsbury and Palmer,2002) role labeling (Carreras and M?rquez,2005).
While the best F-measure recorded on atest set selected from the training corpus (WSJ)was 80%, on the Brown corpus, the F-measuredropped below 70%.
The most significant causesfor this performance decay were highly ambigu-ous and unseen predicates (i.e.
predicates that donot have training examples, unseen in the train-ing set).On the FrameNet (Johnson et al, 2003) rolelabeling task, the Senseval-3 competition (Lit-kowski, 2004) registered similar results (~80%)by using the gold frame information as a givenfeature.
No tests were performed outside Frame-Net.
In this paper, we show that when the framefeature is not used, the performance decay ondifferent corpora reaches 30 points.
Thus, thecontext knowledge provided by the frame is veryimportant and a free-text semantic parser usingFrameNet roles depends on the accurate auto-matic detection of this information.In order to test the feasibility of such a task,we have trained an SVM (Support Vector Ma-chine) Tree Kernel model for the automatic ac-quisition of the frame information.
Although Fra-meNet contains three types of predicates (nouns,adjectives and verbs), we concentrated on theverb predicates and the roles associated withthem.
Therefore, we considered only the framesthat have at least one verb lexical unit.
Ourexperiments show that given a FrameNetpredicate-argument structure, the task of identi-fying the originating frame can be performedwith very good results when the verb predicateshave enough training examples, but becomesvery challenging otherwise.
The predicates notyet included in FrameNet and the predicates be-longing to new application domains (that requirenew frames) are especially problematic as forthem there is no available training data.We have thus studied new means of captur-ing the semantic context, other than the frame,which can be easily annotated on FrameNet andare available on a larger scale (i.e.
have a bettercoverage).
A very good candidate seems to bethe Intersective Levin classes (Dang et al, 1998)that can be found as well in other predicate re-sources like PropBank and VerbNet (Kipper etal., 2000).
Thus, we have designed a semi-automatic algorithm for assigning an IntersectiveLevin class to each FrameNet verb predicate.78The algorithm creates a mapping between Fra-meNet frames and the Intersective Levin classes.By doing that we could connect FrameNet toVerbNet and PropBank and obtain an increasedtraining set for the Intersective Levin class.
Thisleads to better verb coverage and a more robustsemantic parser.
The newly created knowledgebase allows us to surpass the shortcomings thatarise when FrameNet, VerbNet and PropBankare used separately while, at the same time, webenefit from the extensive research involvingeach of them (Pradhan et al, 2004; Gildea andJurafsky, 2002; Moschitti, 2004).We mention that there are 3,672 distinctverb senses1 in PropBank and 2,351 distinct verbsenses in FrameNet.
Only 501 verb senses are incommon between the two corpora which mean13.64% of PropBank and 21.31% of FrameNet.Thus, by training an Intersective Levin classclassifier on both PropBank and FrameNet weextend the number of available verb senses to5,522.In the remainder of this paper, Section 2summarizes previous work done on FrameNetautomatic role detection.
It also explains in moredetail why models based exclusively on this cor-pus are not suitable for free-text parsing.
Section3 focuses on VerbNet and PropBank and howthey can enhance the robustness of our semanticparser.
Section 4 describes the mapping betweenframes and Intersective Levin classes whereasSection 5 presents the experiments that supportour thesis.
Finally, Section 6 summarizes theconclusions.2 Automatic semantic role detection onFrameNetOne of the goals of the FrameNet project is todesign a linguistic ontology that can be used forautomatic processing of semantic information.This hierarchy contains an extensive semanticanalysis of verbs, nouns, adjectives and situa-tions in which they are used, called frames.
Thebasic assumption on which the frames are built isthat each word evokes a particular situation withspecific participants (Fillmore, 1968).
The situa-tions can be fairly simple depicting the entitiesinvolved and the roles they play or can be verycomplex and in this case they are called scenar-ios.
The word that evokes a particular frame iscalled target word or predicate and can be an1 A verb sense is an Intersective Levin class in whichthe verb is listed.adjective, noun or verb.
The participant entitiesare defined using semantic roles and they arecalled frame elements.Several models have been developed for theautomatic detection of the frame elements basedon the FrameNet corpus (Gildea and Jurafsky,2002; Thompson et al, 2003; Litkowski, 2004).While the algorithms used vary, almost all theprevious studies divide the task into 1) the identi-fication of the verb arguments to be labeled and2) the tagging of each argument with a role.Also, most of the models agree on the core fea-tures as being: Predicate, Headword, PhraseType, Governing Category, Position, Voice andPath.
These are the initial features adopted byGildea and Jurafsky (2002) (henceforth G&J) forboth frame element identification and role classi-fication.A difference among the previous machine-learning models is whether the frame informationwas used as gold feature.
Of particular interestfor us is the impact of the frame over unseenpredicates and unseen words in general.
Theresults obtained by G&J are relevant in thissense; especially, the experiment that uses theframe to generalize from predicates seen in thetraining data to other predicates (i.e.
when nodata is available for a target word, G&J use datafrom the corresponding frame).
The overall per-formance induced by the frame usage increased.Other studies suggest that the frame is cru-cial when trying to eliminate the major sourcesof errors.
In their error analysis, (Thompson etal., 2003) pinpoints that the verb arguments withheadwords that are ?rare?
in a particular framebut not rare over the whole corpus are especiallyhard to classify.
For these cases the frame is veryimportant because it provides the context infor-mation needed to distinguish between differentword senses.Overall, the experiments presented in G&J?sstudy correlated with the results obtained in theSenseval-3 competition show that the frame fea-ture increases the performance and decreases theamount of annotated examples needed in training(i.e.
frame usage improves the generalizationability of the learning algorithm).
On the otherhand the results obtained without the frame in-formation are very poor.This behavior suggests that predicates in thesame frame behave similarly in terms of theirargument structure and that they differ with re-spect to other frames.
From this perspective, hav-ing a broader verb knowledge base becomes ofmajor importance for free-text semantic parsing.79Unfortunately, the 321 frames that contain atleast one verb predicate cover only a small frac-tion of the English verb lexicon and of possibledomains.
Also from these 321 frames only 100were considered to have enough training dataand were used in Senseval-3 (see Litkowski,2004 for more details).Our approach for solving such problems in-volves the usage of a frame-like feature, namelythe Intersective Levin class.
We show that theLevin class is similar in many aspects to theframe and can replace it with almost no loss inperformance.
At the same time, Levin class pro-vides better coverage as it can be learned alsofrom other corpora (i.e.
PropBank).
We annotateFrameNet with Intersective Levin classes by us-ing a mapping algorithm that exploits currenttheories of linking.
Our extensive experimenta-tion shows the validity of our technique and itseffectiveness on corpora different from Frame-Net.
The next section provides the theoreticalsupport for the unified usage of FrameNet,VerbNet and PropBank, explaining why and howis possible to link them.3 Linking FrameNet to VerbNet andPropBankIn general, predicates belonging to the sameFrameNet frame have a coherent syntactic be-havior that is also different from predicates per-taining to other frames (G&J).
This finding isconsistent with theories of linking that claim thatthe syntactic behavior of a verb can be predictedfrom its semantics (Levin 1993, Levin and Rap-paport Hovav, 1996).
This insight determined usto study the impact of using a feature based onIntersective Levin classes instead of the framefeature when classifying FrameNet semanticroles.
The main advantage of using Levin classescomes from the fact that other resources likePropBank and the VerbNet lexicon contain thiskind of information.
Thus, we can train a Levinclass classifier also on the PropBank corpus,considerably increasing the verb knowledge baseat our disposal.
Another advantage derives fromthe syntactic criteria that were applied in defin-ing the Levin clusters.
As shown later in this ar-ticle, the syntactic nature of these classes makesthem easier to classify than frames, when usingonly syntactic and lexical features.More precisely, the Levin clusters areformed according to diathesis alternation criteriawhich are variations in the way verbal argumentsare grammatically expressed when a specific se-mantic phenomenon arises.
For example, twodifferent types of diathesis alternations are thefollowing:(a) Middle Alternation[Subject, Agent The butcher] cuts [Direct Object, Patient the meat].
[Subject, Patient The meat] cuts easily.
(b) Causative/inchoative Alternation[Subject, Agent Janet] broke [Direct Object, Patient the cup].
[Subject, Patient The cup] broke.In both cases, what is alternating is thegrammatical function that the Patient role takeswhen changing from the transitive use of theverb to the intransitive one.
The semantic phe-nomenon accompanying these types of alterna-tions is the change of focus from the entity per-forming the action to the theme of the event.Levin documented 79 alternations whichconstitute the building blocks for the verbclasses.
Although alternations are chosen as theprimary means for identifying the classes, addi-tional properties related to subcategorization,morphology and extended meanings of verbs aretaken into account as well.
Thus, from a syntacticpoint of view, the verbs in one Levin class have aregular behavior, different from the verbs per-taining to other classes.
Also, the classes are se-mantically coherent and all verbs belonging toone class share the same participant roles.This constraint of having the same semanticroles is further ensured inside the VerbNet lexi-con that is constructed based on a more refinedversion of the Levin classification called Inter-sective Levin classes (Dang et al, 1998).
Thelexicon provides a regular association betweenthe syntactic and semantic properties of each ofthe described classes.
It also provides informa-tion about the syntactic frames (alternations) inwhich the verbs participate and the set of possi-ble semantic roles.One corpus associated with the VerbNetlexicon is PropBank.
The annotation scheme ofPropBank ensures that the verbs belonging to thesame Levin class share similarly labeled argu-ments.
Inside one Intersective Levin class, to oneargument corresponds one semantic role num-bered sequentially from Arg0 to Arg5.
Highernumbered argument labels are less consistent andassigned per-verb basis.The Levin classes were constructed based onregularities exhibited at grammatical level andthe resulting clusters were shown to be semanti-cally coherent.
As opposed, the FrameNet frameswere build on semantic bases, by putting togetherverbs, nouns and adjectives that evoke the samesituations.
Although different in conception, the80FrameNet verb clusters and VerbNet verb clus-ters have common properties2:(1) Coherent syntactic behavior of verbs inside onecluster,(2) Different syntactic properties between any twodistinct verb clusters,(3) Shared set of possible semantic roles for all verbspertaining to the same cluster.Having these insights, we have assigned a corre-spondent VerbNet class not to each verb predi-cate but rather to each frame.
In doing this wehave applied the simplifying assumption that aframe has a unique corresponding Levin class.Thus, we have created a one-to-many mappingbetween the Intersective Levin classes and theframes.
In order to create a pair ?FrameNetframe, VerbNet class?, our mapping algorithmchecks both the syntactic and semantic consis-tency by comparing the role frequency distribu-tions on different syntactic positions for the twocandidates.
The algorithm is described in detailin the next section.4 Mapping FrameNet frames toVerbNet classesThe mapping algorithm consists of three steps:(a) we link the frames and Intersective Levinverb classes that have the largest number ofverbs in common and we create a set of pairs?FrameNet frame, VerbNet class?
(see Figure 1);(b) we refine the pairs obtained in the previousstep based on diathesis alternation criteria, i.e.the verbs pertaining to the FrameNet frame haveto undergo the same diathesis alternation thatcharacterize the corresponding VerbNet class(see Figure 2) and (c) we manually check andcorrect the resulting mapping.
In the next sec-tions we will explain in more detail each step ofthe mapping algorithm.4.1 Linking frames and Intersective Levinclasses based on common verbsDuring the first phase of the algorithm, given aframe, we compute its intersection with eachVerbNet class.
We choose as candidate for themapping the Intersective Levin class that has thelargest number of verbs in common with thegiven frame (Figure 1, line (I)).
If the size of theintersection between the FrameNet frame and thecandidate VerbNet class is bigger than or equal2 For FrameNet, properties 1 and 2 are true for mostof the frames but not for all.
See section 4.4 for moredetails.to 3 elements then we form a pair ?FrameNetframe, VerbNet class?
that qualifies for thesecond step of the algorithm.Only the frames that have more than threeverb lexical units are candidates for this step(frames with less than 3 members cannot passcondition (II)).
This excludes a number of 60frames that will subsequently be mappedmanually.Figure 1.
Linking FrameNet frames and VerbNetclasses4.2 Refining the mapping based on verbalternationsIn order to assign a VerbNet class to a frame, wehave to check that the verbs belonging to thatframe respect the diathesis alternation criteriaused to define the VerbNet class.
Thus, the pairs?FrameNet frame, VerbNet class?
formed in step(I) of the mapping algorithm have to undergo avalidation step that verifies the similarity be-tween the enclosed FrameNet frame and VerbNetclass.
This validation process has several sub-steps.First, we make use of the property (3) of theLevin classes and FrameNet frames presented inthe previous section.
According to this property,all verbs pertaining to one frame or Levin classhave the same participant roles.
Thus, a first testof compatibility between a frame and a Levinclass is that they share the same participant roles.As FrameNet is annotated with frame-specificsemantic roles we manually mapped these rolesinto the VerbNet set of thematic roles.
Given aframe, we assigned thematic roles to all frameelements that are associated with verbal predi-cates.
For example the roles Speaker, Addressee,Message and Topic from the Telling frame wererespectively mapped into Agent, Recipient,Theme and Topic.
)({ }( )***,3)(maxarg)(:,|,}|{}|{}|{}|{CFPairsPairsthenCFifIICFCcomputeIFNFeachforPairsLet:PAIRSCOMPUTECtomappedisFVNCFNFCFPairsOUTPUTFofverbaisvvFFrameFNframeFrameNetaisFFFNCofverbaisvvCClassVNclassVerbNetaisCCVNINPUTVNC?=???=??=?
?=====?81)(||||||||31||||||||32),,(#),,..,(),,(#),,..,(),,(#),,..,(),,(#),,..,(,}:{,1111CFCFCFCFCFiinCiinCiinFiinFthiiDSTDSTDSTDSTADJADJADJADJScorepositionCowhereooDSTpositionCowhereooADJpositionFowhereooDSTpositionFowhereooADJPairsCFeachfora role setrbNet thete of theVe theta rolis the iTR??+??=============?=??distantadjacentdistantadjacent?????
?Second, we build a frequency distribution ofVerbNet thematic roles on different syntacticposition.
Based on our observation and previousstudies (Merlo and Stevenson, 2001), we assumethat each Levin class has a distinct frequencydistribution of roles on different grammaticalslots.
As we do not have matching grammaticalfunction in FrameNet and VerbNet, we approxi-mate that subjects and direct objects are morelikely to appear on positions adjacent to thepredicate, while indirect objects appear on moredistant positions.
The same intuition is used suc-cessfully by G&J in the design of the Positionfeature.We will acquire from the corpus, for eachthematic role ?i, the frequencies with which itappears on an adjacent (ADJ) or distant (DST)position in a given frame or VerbNet class (i.e.#(?i, class, position)).
Therefore, for each frameand class, we obtain two vectors with thematicrole frequencies corresponding respectively tothe adjacent and distant positions (see Figure 2).We compute a score for each pair ?FrameNetframe, VerbNet class?
using the normalized sca-lar product.
We give a bigger weight to the adja-cent dot product multiplying its score by 2/3 withrespect to the distant dot product that is multi-plied by 1/3.
We do this to minimize the impactthat adjunct roles like Temporal and Location(that appear mostly on the distant positions)could have on the final outcome.Figure 2.
Mapping algorithm ?
refining stepThe above frequency vectors are computedfor FrameNet directly from the corpus of predi-cate-argument structure examples associatedwith each frame.
The examples associated withthe VerbNet lexicon are extracted from thePropBank corpus.
In order to do this we apply apreprocessing step in which each label ARG0..Nis replaced with its corresponding thematic rolegiven the Intersective Levin class of the predi-cate.
We assign the same roles to the adjuncts allover PropBank as they are general for all verbclasses.
The only exception is ARGM-DIR thatcan correspond to Source, Goal or Path.
We as-sign different roles to this adjunct based on theprepositions.
We ignore some adjuncts likeARGM-ADV or ARGM-DIS because they can-not bear a thematic role.4.3 Mapping ResultsWe found that only 133 VerbNet classes havecorrespondents among FrameNet frames.
Also,from the frames mapped with an automatic scoresmaller than 0.5 points almost a half did notmatch any of the existing VerbNet classes3.
Asummary of the results is depicted in Table 1.The first column contains the automatic scoreprovided by the mapping algorithm when com-paring frames with Intersective Levin classes.The second column contains the number offrames for each score interval.
The third columncontains the percentage of frames, per each scoreinterval, that did not have a correspondingVerbNet class and finally the forth column con-tains the accuracy of the mapping algorithm.Score No.
of FramesNotmapped CorrectOverallCorrect[0,0.5] 118 48.3% 82.5%(0.5,0.75] 69 0 84%(0.75,1] 72 0 100%89.6%Table 1.
Results of the mapping algorithm4.4 DiscussionIn the literature, other studies compared theLevin classes to the FrameNet frames (Baker andRuppenhofer, 2002).
Their findings suggest thatalthough the two set of clusters are roughlyequivalent  there are also several types ofmistmaches: 1) Levin classes that are narrowerthan  the corresponding frames, 2) Levin classesthat are broader that the corresponding framesand 3) overlapping groupings.
For our task, point2 does not pose a problem.
Points 1 and 3however suggest that there are cases in which toone FrameNet frame corresponds more than oneLevin class.
By investigating such cases wenoted that the mapping algorithm consistentlyassigns scores below 75% to cases that matchproblem 1 (two Levin classes inside one frame)and below 50% to cases that match problem 3(more than two Levin classes inside one frame).Thus, in order to increase the accuracy of ourresults a first step should be to assign an3 The automatic mapping  can be improved by manu-ally assigning the FrameNet frames of the pairs thatreceive a score lower than 0.5.82Intersective Levin class to each of the verbspertaining to frames with score lower than 0.75.Nevertheless the current results are encouragingas they show that the algorithm is achiving itspurpose by successfully detecting syntacticincoherencies that can be subsequently correctedmanually.
Also, in the next section we will showthat our current mapping achieves very goodresults, giving evidence for  the effectivenes ofthe Levin class feature.5 ExperimentsIn the previous section we have presented thealgorithm for annotating the verb predicates ofFrameNet with Intersective Levin classes.
In or-der to show the effectiveness of this annotationand of the Intersective Levin class in general wehave performed several experiments.First, we trained (1) an ILC multiclassifierfrom FrameNet, (2) an ILC multiclassifier fromPropBank and (3) a frame multiclassifier fromFrameNet.
We compared the results obtainedwhen trying to classify the VerbNet class withthe results obtained when classifying frame.
Weshow that Intersective Levin classes are easier todetect than FrameNet frames.Our second set of experiments regards theautomatic labeling of FrameNet semantic roleson FrameNet corpus when using as features: goldframe, gold Intersective Levin class, automati-cally detected frame and automatically detectedIntersective Levin class.
We show that in allsituations in which the VerbNet class feature isused, the accuracy loss, compared to the usage ofthe frame feature, is negligible.
We thus showthat the Intersective Levin class can successfullyreplace the frame feature for the task of semanticrole labeling.Another set of experiments regards the gen-eralization property of the Intersective Levinclass.
We show the impact of this feature whenvery few training data is available and its evolu-tion when adding more and more training exam-ples.
We again perform the experiments for: goldframe, gold Intersective Levin class, automati-cally detected frame and automatically detectedIntersective Levin class.Finally, we simulate the difficulty of freetext by annotating PropBank with FrameNet se-mantic roles.
We use PropBank because it is dif-ferent from FrameNet from a domain point ofview.
This characteristic makes PropBank a dif-ficult test bed for semantic role models trainedon FrameNet.In the following section we present the re-sults obtained for each of the experiments men-tioned above.5.1 Experimental setupThe corpora available for the experiments werePropBank and FrameNet.
PropBank containsabout 54,900 sentences and gold parse trees.
Weused sections from 02 to 22 (52,172 sentences) totrain the Intersective Levin class classifiers andsection 23 (2,742 sentences) for testing purposes.For the experiments on FrameNet corpus weextracted 58,384 sentences from the 319 framesthat contain at least one verb annotation.
Thereare 128,339 argument instances of 454 semanticroles.
Only verbs are selected to be predicates inour evaluations.
Moreover, as there is no fixedsplit between training and testing, we randomlyselected 20% of sentences for testing and 80%for training.
The sentences were processed usingCharniak?s parser (Charniak, 2000) to generateparse trees automatically.For classification, we used the SVM-light-TK software available at http://ai-nlp.info.uniroma2.it/moschitti which en-codes tree kernels in the SVM-light software(Joachims, 1999).
The classification performancewas evaluated using the F1 measure for the sin-gle-argument classifiers and the accuracy for themulticlassifiers.5.2 Automatic VerbNet vs. automatic Fra-meNet frame detectionIn these experiments we classify IntersectiveLevin classes (ILC) on PropBank (PB) andFrameNet (FN) and frame on FrameNet.
For thetraining stage we use SVMs with Tree Kernels.The main idea of tree kernels is the modelingof a KT(T1,T2) function which computes thenumber of common substructures between twotrees T1 and T2.
Thus, we can train SVMs withstructures drawn directly from the syntactic parsetree of the sentence.The kernel that we employed in ourexperiments is based on the SCF structuredevised in (Moschitti, 2004).
We slightlymodified SCF by adding the headwords of thearguments, useful for representing the selectionalpreferences.For frame detection on FrameNet, we trainedour classifier on 46,734 training instances andtested on 11,650 testing instances, obtaining anaccuracy of 91.11%.
For ILC detection theresults are depicted in Table  2.
The first sixcolumns report the F1 measure of some verb83class classifiers whereas the last column showsthe global multiclassifier accuracy.We note that ILC detection is performed betterthan frame detection on both FrameNet andPropBank.
Also, the results obtained on ILC onPropBank are similar with the ones obtained onILC on FrameNet.
This suggests that the trainingcorpus does not have a major influence.
Also, theSCF-based tree kernel seems to be robust in whatconcerns the quality of the parse trees.
Theperformance decay is very small on FrameNetthat uses automatic parse trees with respect toPropBank that contains gold parse trees.
Theseproperties suggest that ILC are very suitable forfree text.Table 2 .
F1 and accuracy of the argument classifiers and the overall multiclassifier for Intersective Levin class5.3 Automatic semantic role labeling onFrameNetIn the experiments involving semantic rolelabelling, we used a SVM with a polynomialkernel.
We adopted the standard featuresdeveloped for semantic role detection by Gildeaand Jurafsky (see Section 2).
Also, weconsidered some of the features designed by(Pradhan et al, 2004): First and Last Word/POSin Constituent, Subcategorization, Head Word ofPrepositional Phrases and the Syntactic Framefeature from (Xue and Palmer, 2004).
For therest of the paper we will refer to these features asbeing literature features (LF).
The resultsobtained when using the literature features aloneor in conjunction with the gold frame feature,gold ILC, automatically detected frame featureand automatically detected ILC are depicted inTable 3.
The first four columns report the F1measure of some role classifiers whereas the lastcolumn shows the global multiclassifieraccuracy.
The first row contains the number oftraining and testing instances and each of theother rows contains the performance obtained fordifferent feature combinations.
The results arereported for the labeling task as the argument-boundary detection task is not affected by theframe-like features (G&J).We note that automatic frame results arevery similar to automatic ILC results suggestingthat ILC feature is a very good candidate forreplacing the frame feature.
Also, both automaticfeatures are very effective, decreasing the errorrate of 20%.Body_part Crime Degree Agent MulticlassifierFN #Train InstancesFN #Test Instances1,5113563957651876,4411,643102,72425,615LF+Gold Frame 90.91 88.89 70.51 93.87 90.8LF+Gold ILC 90.80 88.89 71.52 92.01 88.23LF+Automatic Frame 84.87 88.89 70.10 87.73 85.64LF+Automatic ILC 85.08 88.89 69.62 87.74 84.45LF 79.76 75.00 64.17 80.82 80.99Table 3.
F1 and accuracy of the argument classifiers and the overall multiclassifier forFrameNet semantic roles5.4 Semantic role learning curve when us-ing Intersective Levin classesThe next set of experiments show the impact ofthe ILC feature on semantic role labelling whenfew training data is available (Figure 3).
As canbe noted, the automatic ILC features (i.e.
derivedwith classifers trained on FrameNet or PB)produce accuracy almost as good as the gold ILCone.
Another observation is that the SRLclassifiers are not saturated and more trainingexamples would improve their accuracy.run-51.3.2cooking-45.3characterize-29.2other_cos-45.4say-37.7correspond-36.1 MulticlassifierPB #Train InstancesPB #Test Instances2625652,9451342,2071499,7076082592052,1722,742PB Results 75 33.33 96.3 97.24 100 88.89 92.96FN #Train InstancesFN #Test Instances5,3811,34313835765407211841,8601,34355711146,73411,650FN Results 96.36 72.73 95.73 92.43 94.43 78.23 92.63843040506070809010 20 30 40 50 60 70 80 90 100% Training DataAccuracy--LF+ILCLFLF+Automatic ILC Trained on PBLF+Automatic ILC Trained on FNFigure 3.
Semantic Role learning curve5.5 Annotating PropBank with FrameNetsemantic rolesTo show that our approach can be suitable forsemantic role free-text annotation, we haveautomatically classified PropBank sentences withthe FrameNet semantic-role classifiers.
In orderto measure the quality of the annotation, we ran-domly selected 100 sentences and manually veri-fied them.
We measured the performance ob-tained with and without the automatic ILC fea-ture.
The sentences contained 189 argumentsfrom which 35 were incorrect when ILC wasused compared to 72 incorrect in the absence ofthis feature.
This corresponds to an accuracy of81% with Intersective Levin class versus 62%without it.6 ConclusionsIn this paper we have shown that the IntersectiveLevin class feature can successfully replace theFrameNet frame feature.
By doing that we couldinterconnect FrameNet to VerbNet and Prop-Bank obtaining better verb coverage and a morerobust semantic parser.
Our good results showthat we have defined an effective frameworkwhich is a promising step toward the design offree-text semantic parsers.In the future, we intend to measure the effective-ness of our system by testing on larger, morecomprehensive corpora and without relying onany manual annotation.ReferenceCollin Baker and Josef Ruppenhofer.
2002.
Frame-Net?s frames vs. Levin?s verb classes.
28th AnnualMeeting of the Berkeley Linguistics Society.Xavier Carreras and Llu?s M?rquez.
2005.
Introduc-tion to the CoNLL-2005 Shared Task: SemanticRole Labeling.
CONLL?05.Eugene Charniak.
2000.
A Maximum-Entropy-Inspired Parser.
ANLP?00Hoa Trang Dang, Karin Kipper, Martha Palmer andJoseph Rosenzweig.
1998.
Investigating regularsense extensions based on Intersective Levinclasses.
Coling-ACL?98.Charles Fillmore.
1968.
The case for case.
Universalsin Linguistic Theory.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
CL Journal.Christopher Johnson, Miriam Petruck, Collin Baker,Michael Ellsworth, Josef Ruppenhofer, and CharlesFillmore.
2003.
FrameNet: Theory and Practice.Berkeley, California.Paul Kingsbury, Martha Palmer.
2002.
From Tree-Bank to PropBank.
LREC?02.Karin Kipper, Hoa Trang Dang and Martha Palmer.2000.
Class-based construction of a verb lexicon.AAAI?00.Beth Levin.
1993.
English Verb Classes and Alterna-tions A Preliminary Investigation.
Chicago: Uni-versity of Chicago Press.Kenneth Litkowski.
2004.
Senseval-3 task automaticlabeling of semantic roles.
Senseval-3.Paola Merlo and Suzanne Stevenson.
2001.
Auto-matic verb classification based on statistical distri-bution of argument structure.
CL Journal.Alessandro Moschitti.
2004.
A study on convolutionkernel for shallow semantic parsing.
ACL?04.Sameer Pradhan, Kadri Hacioglu, Valeri Krugler,Wayne Ward, James H. Martin, and Daniel Juraf-sky.
2004.
Support vector learning for semantic ar-gument classification.
Machine Learning Journal.Cynthia A. Thompson, Roger Levy, and ChristopherManning.
2003.
A Generative Model for FrameNetSemantic Role Labeling.
ECML?03.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.. Advances in Kernel Methods -Support Vector Learning.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
EMNLP?04.85
