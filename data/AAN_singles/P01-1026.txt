Organizing Encyclopedic Knowledge based on the Web and itsApplication to Question AnsweringAtsushi FujiiUniversity of Library andInformation Science1-2 Kasuga, Tsukuba305-8550, JapanCREST, Japan Science andTechnology Corporationfujii@ulis.ac.jpTetsuya IshikawaUniversity of Library andInformation Science1-2 Kasuga, Tsukuba305-8550, Japanishikawa@ulis.ac.jpAbstractWe propose a method to generate large-scaleencyclopedic knowledge, which is valuablefor much NLP research, based on the Web.We first search the Web for pages contain-ing a term in question.
Then we use lin-guistic patterns and HTML structures to ex-tract text fragments describing the term.
Fi-nally, we organize extracted term descrip-tions based on word senses and domains.
Inaddition, we apply an automatically gener-ated encyclopedia to a question answeringsystem targeting the Japanese Information-Technology Engineers Examination.1 IntroductionReflecting the growth in utilization of the World WideWeb, a number of Web-based language processingmethods have been proposed within the natural lan-guage processing (NLP), information retrieval (IR)and artificial intelligence (AI) communities.
A sam-ple of these includes methods to extract linguisticresources (Fujii and Ishikawa, 2000; Resnik, 1999;Soderland, 1997), retrieve useful information in re-sponse to user queries (Etzioni, 1997; McCallum etal., 1999) and mine/discover knowledge latent in theWeb (Inokuchi et al, 1999).In this paper, mainly from an NLP point of view,we explore a method to produce linguistic resources.Specifically, we enhance the method proposed by Fu-jii and Ishikawa (2000), which extracts encyclopedicknowledge (i.e., term descriptions) from the Web.In brief, their method searches the Web for pagescontaining a term in question, and uses linguistic ex-pressions and HTML layouts to extract fragments de-scribing the term.
They also use a language model todiscard non-linguistic fragments.
In addition, a clus-tering method is used to divide descriptions into a spe-cific number of groups.On the one hand, their method is expected to en-hance existing encyclopedias, where vocabulary sizeis relatively limited, and therefore the quantity prob-lems has been resolved.On the other hand, encyclopedias extracted from theWeb are not comparable with existing ones in terms ofquality.
In hand-crafted encyclopedias, term descrip-tions are carefully organized based on domains andword senses, which are especially effective for humanusage.
However, the output of Fujii?s method is simplya set of unorganized term descriptions.
Although clus-tering is optionally performed, resultant clusters arenot necessarily related to explicit criteria, such as wordsenses and domains.To sum up, our belief is that by combining extrac-tion and organization methods, we can enhance bothquantity and quality of Web-based encyclopedias.Motivated by this background, we introduce an or-ganization model to Fujii?s method and reformalizethe whole framework.
In other words, our proposedmethod is not only extraction but generation of ency-clopedic knowledge.Section 2 explains the overall design of our ency-clopedia generation system, and Section 3 elaborateson our organization model.
Section 4 then exploresa method for applying our resultant encyclopedia toNLP research, specifically, question answering.
Sec-tion 5 performs a number of experiments to evaluateour methods.2 System Design2.1 OverviewFigure 1 depicts the overall design of our system,which generates an encyclopedia for input terms.Our system, which is currently implemented forJapanese, consists of three modules: ?retrieval,?
?ex-traction?
and ?organization,?
among which the orga-nization module is newly introduced in this paper.
Inprinciple, the remaining two modules (?retrieval?
and?extraction?)
are the same as proposed by Fujii andIshikawa (2000).In Figure 1, terms can be submitted either on-line oroff-line.
A reasonable method is that while the systemperiodically updates the encyclopedia off-line, termsunindexed in the encyclopedia are dynamically pro-cessed in real-time usage.
In either case, our systemprocesses input terms one by one.We briefly explain each module in the followingthree sections, respectively.domainmodelWebextractionrulesorganizationencyclopediaretrievalextractionterm(s)descriptionmodelFigure 1: The overall design of our Web-based ency-clopedia generation system.2.2 RetrievalThe retrieval module searches the Web for pages con-taining an input term, for which existing Web searchengines can be used, and those with broad coverageare desirable.However, search engines performing query expan-sion are not always desirable, because they usually re-trieve a number of pages which do not contain an in-put keyword.
Since the extraction module (see Sec-tion 2.3) analyzes the usage of the input term in re-trieved pages, pages not containing the term are of nouse for our purpose.Thus, we use as the retrieval module ?Google,?which is one of the major search engines and does notconduct query expansion1.2.3 ExtractionIn the extraction module, given Web pages containingan input term, newline codes, redundant white spacesand HTML tags that are not used in the following pro-cesses are discarded to standardize the page format.Second, we approximately identify a region describ-ing the term in the page, for which two rules are used.1http://www.google.com/The first rule is based on Japanese linguistic patternstypically used for term descriptions, such as ?X tohaY dearu (X is Y).?
Following the method proposedby Fujii and Ishikawa (2000), we semi-automaticallyproduced 20 patterns based on the Japanese CD-ROMWorld Encyclopedia (Heibonsha, 1998), which in-cludes approximately 80,000 entries related to variousfields.
It is expected that a region including the sen-tence that matched with one of those patterns can be aterm description.The second rule is based on HTML layout.
In a typ-ical case, a term in question is highlighted as a headingwith tags such as <DT>, <B> and <Hx> (?x?
denotesa digit), followed by its description.
In some cases,terms are marked with the anchor <A> tag, providinghyperlinks to pages where they are described.Finally, based on the region briefly identified by theabove method, we extract a page fragment as a termdescription.
Since term descriptions usually consist ofa logical segment (such as a paragraph) rather than asingle sentence, we extract a fragment that matchedwith one of the following patterns, which are sortedaccording to preference in descending order:1. description tagged with <DD> in the case wherethe term is tagged with <DT>2,2.
paragraph tagged with <P>,3.
itemization tagged with <UL>,4.
N sentences, where we empirically set N = 3.2.4 OrganizationAs discussed in Section 1, organizing information ex-tracted from the Web is crucial in our framework.
Forthis purpose, we classify extracted term descriptionsbased on word senses and domains.Although a number of methods have been proposedto generate word senses (for example, one based on thevector space model (Schu?tze, 1998)), it is still difficultto accurately identify word senses without explicit dic-tionaries that define sense candidates.In addition, since word senses are often associatedwith domains (Yarowsky, 1995), word senses can beconsequently distinguished by way of determining thedomain of each description.
For example, differentsenses for ?pipeline (processing method/transportationpipe)?
are associated with the computer and construc-tion domains (fields), respectively.To sum up, the organization module classifies termdescriptions based on domains, for which we use do-main and description models.
In Section 3, we elabo-rate on our organization model.2<DT> and <DD> are inherently provided to describeterms in HTML.3 Statistical Organization Model3.1 OverviewGiven one or more (in most cases more than one)descriptions for a single input term, the organizationmodule selects appropriate description(s) for each do-main related to the term.We do not need all the extracted descriptions as fi-nal outputs, because they are usually similar to oneanother, and thus are redundant.For the moment, we assume that we know a prioriwhich domains are related to the input term.From the viewpoint of probability theory, our taskhere is to select descriptions with greater probabilityfor given domains.
The probability for description dgiven domain c, P (d|c), is commonly transformed asin Equation (1), through use of the Bayesian theorem.P (d|c) = P (c|d) ?
P (d)P (c) (1)In practice, P (c) can be omitted because this factor isa constant, and thus does not affect the relative proba-bility for different descriptions.In Equation (1), P (c|d) models a probability that dcorresponds to domain c. P (d) models a probabilitythat d can be a description for the term in question,disregarding the domain.
We shall call them domainand description models, respectively.To sum up, in principle we select d?s that arestrongly associated with a specific domain, and arelikely to be descriptions themselves.Extracted descriptions are not linguistically under-standable in the case where the extraction process isunsuccessful and retrieved pages inherently containnon-linguistic information (such as special charactersand e-mail addresses).To resolve this problem, Fujii and Ishikawa (2000)used a language model to filter out descriptions withlow perplexity.
However, in this paper we integrateda description model, which is practically the same asa language model, with an organization model.
Thenew framework is more understandable with respectto probability theory.In practice, we first use Equation (1) to computeP (d|c) for all the c?s predefined in the domain model.Then we discard such c?s whose P (d|c) is below a spe-cific threshold.
As a result, for the input term, relateddomains and descriptions are simultaneously selected.Thus, we do not have to know a priori which domainsare related to each term.In the following two sections, we explain methodsto realize the domain and description models, respec-tively.3.2 Domain ModelThe domain model quantifies the extent to which de-scription d is associated with domain c, which is fun-damentally a categorization task.
Among a numberof existing categorization methods, we experimentallyused one proposed by Iwayama and Tokunaga (1994),which formulates P (c|d) as in Equation (2).P (c|d) = P (c) ?
?tP (t|c) ?
P (t|d)P (t) (2)Here, P (t|d), P (t|c) and P (t) denote probabilitiesthat word t appears in d, c and all the domains, respec-tively.
We regard P (c) as a constant.
While P (t|d) issimply a relative frequency of t in d, we need prede-fined domains to compute P (t|c) and P (t).
For thispurpose, the use of large-scale corpora annotated withdomains is desirable.However, since those resources are prohibitivelyexpensive, we used the ?Nova?
dictionary forJapanese/English machine translation systems3, whichincludes approximately one million entries related to19 technical fields as listed below:aeronautics, biotechnology, business, chem-istry, computers, construction, defense,ecology, electricity, energy, finance, law,mathematics, mechanics, medicine, metals,oceanography, plants, trade.We extracted words from dictionary entries to esti-mate P (t|c) and P (t), which are relative frequenciesof t in c and all the domains, respectively.
We usedthe ChaSen morphological analyzer (Matsumoto et al,1997) to extract words from Japanese entries.
We alsoused English entries because Japanese descriptions of-ten contain English words.It may be argued that statistics extracted from dic-tionaries are unreliable, because word frequencies inreal word usage are missing.
However, words that arerepresentative for a domain tend to be frequently usedin compound word entries associated with the domain,and thus our method is a practical approximation.3.3 Description ModelThe description model quantifies the extent to which agiven page fragment is feasible as a description for theinput term.
In principle, we decompose the descriptionmodel into language and quality properties, as shownin Equation (3).P (d) = PL(d) ?
PQ(d) (3)Here, PL(d) and PQ(d) denote language and qualitymodels, respectively.3Produced by NOVA, Inc.It is expected that the quality model discards in-correct or misleading information contained in Webpages.
For this purpose, a number of quality ratingmethods for Web pages (Amento et al, 2000; Zhu andGauch, 2000) can be used.However, since Google (i.e., the search engine usedin our system) rates the quality of pages based onhyperlink information, and selectively retrieves thosewith higher quality (Brin and Page, 1998), we tenta-tively regarded PQ(d) as a constant.
Thus, in practicethe description model is approximated solely with thelanguage model as in Equation (4).P (d) ?
PL(d) (4)Statistical approaches to language modeling havebeen used in much NLP research, such as machinetranslation (Brown et al, 1993) and speech recogni-tion (Bahl et al, 1983).
Our model is almost the sameas existing models, but is different in two respects.First, while general language models quantify theextent to which a given word sequence is linguisti-cally acceptable, our model also quantifies the extentto which the input is acceptable as a term description.Thus, we trained the model based on an existing ma-chine readable encyclopedia.We used the ChaSen morphological analyzer tosegment the Japanese CD-ROM World Encyclope-dia (Heibonsha, 1998) into words (we replaced head-words with a common symbol), and then used theCMU-Cambridge toolkit (Clarkson and Rosenfeld,1997) to model a word-based trigram.Consequently, descriptions in which word se-quences are more similar to those in the World En-cyclopedia are assigned greater probability scoresthrough our language model.Second, P (d), which is a product of probabilitiesfor N -grams in d, is quite sensitive to the length of d.In the cases of machine translation and speech recog-nition, this problem is less crucial because multiplecandidates compared based on the language model arealmost equivalent in terms of length.However, since in our case length of descriptions aresignificantly different, shorter descriptions are morelikely to be selected, regardless of the quality.
To avoidthis problem, we normalize P (d) by the number ofwords contained in d.4 Application4.1 OverviewEncyclopedias generated through our Web-basedmethod can be used in a number of applications, in-cluding human usage, thesaurus production (Hearst,1992; Nakamura and Nagao, 1988) and natural lan-guage understanding in general.Among the above applications, natural language un-derstanding (NLU) is the most challenging from a sci-entific point of view.
Current practical NLU researchincludes dialogue, information extraction and questionanswering, among which we focus solely on questionanswering (QA) in this paper.A straightforward application is to answer inter-rogative questions like ?What is X??
in which a QAsystem searches the encyclopedia database for one ormore descriptions related to X (this application is alsoeffective for dialog systems).In general, the performance of QA systems are eval-uated based on coverage and accuracy.
Coverage isthe ratio between the number of questions answered(disregarding their correctness) and the total numberof questions.
Accuracy is the ratio between the num-ber of correct answers and the total number of answersmade by the system.While coverage can be estimated objectively andsystematically, estimating accuracy relies on humansubjects (because there is no absolute description forterm X), and thus is expensive.In view of this problem, we targeted InformationTechnology Engineers Examinations4, which are bian-nual (spring and autumn) examinations necessary forcandidates to qualify to be IT engineers in Japan.Among a number of classes, we focused on the?Class II?
examination, which requires fundamentaland general knowledge related to information technol-ogy.
Approximately half of questions are associatedwith IT technical terms.Since past examinations and answers are open to thepublic, we can evaluate the performance of our QAsystem with minimal cost.4.2 Analyzing IT Engineers ExaminationsThe Class II examination consists of quadruple-choicequestions, among which technical term questions canbe subdivided into two types.In the first type of question, examinees choosethe most appropriate description for a given technicalterm, such as ?memory interleave?
and ?router.
?In the second type of question, examinees choosethe most appropriate term for a given question, forwhich we show examples collected from the exami-nation in the autumn of 1999 (translated into Englishby one of the authors) as follows:1.
Which data structure is most appropriate forFIFO (First-In First-Out)?a) binary trees, b) queues, c) stacks, d) heaps2.
Choose the LAN access method in which mul-tiple terminals transmit data simultaneously and4Japan Information-Technology Engineers ExaminationCenter.
http://www.jitec.jipdec.or.jp/thus they potentially collide.a) ATM, b) CSM/CD, c) FDDI, d) token ringIn the autumn of 1999, out of 80 questions, the num-ber of the first and second types were 22 and 18, re-spectively.4.3 Implementing a QA systemFor the first type of question, human examinees wouldsearch their knowledge base (i.e., memory) for the de-scription of a given term, and compare that descriptionwith four candidates.
Then they would choose the can-didate that is most similar to the description.For the second type of question, human examineeswould search their knowledge base for the descriptionof each of four candidate terms.
Then they wouldchoose the candidate term whose description is mostsimilar to the question description.The mechanism of our QA system is analogous tothe above human methods.
However, unlike humanexaminees, our system uses an encyclopedia generatedfrom the Web as a knowledge base.In addition, our system selectively uses term de-scriptions categorized into domains related to infor-mation technology.
In other words, the descriptionof ?pipeline (transportation pipe)?
is irrelevant or mis-leading to answer questions associated with ?pipeline(processing method).
?To compute the similarity between two descriptions,we used techniques developed in IR research, in whichthe similarity between a user query and each documentin a collection is usually quantified based on word fre-quencies.
In our case, a question and four possibleanswers correspond to query and document collection,respectively.
We used a probabilistic method (Robert-son and Walker, 1994), which is one of the major IRmethods.To sum up, given a question, its type and fourchoices, our QA system chooses one of four candi-dates as the answer, in which the resolution algorithmvaries depending on the question type.4.4 Related WorkMotivated partially by the TREC-8 QA collec-tion (Voorhees and Tice, 2000), question answeringhas of late become one of the major topics within theNLP/IR communities.In fact, a number of QA systems targetingthe TREC QA collection have recently been pro-posed (Harabagiu et al, 2000; Moldovan andHarabagiu, 2000; Prager et al, 2000).
Those sys-tems are commonly termed ?open-domain?
systems,because questions expressed in natural language arenot necessarily limited to explicit axes, including who,what, when, where, how and why.However, Moldovan and Harabagiu (2000) foundthat each of the TREC questions can be recast as ei-ther a single axis or a combination of axes.
They alsofound that out of the 200 TREC questions, 64 ques-tions (approximately one third) were associated withthe what axis, for which the Web-based encyclopediais expected to improve the quality of answers.Although Harabagiu et al (2000) proposed aknowledge-based QA system, most existing systemsrely on conventional IR and shallow NLP methods.The use of encyclopedic knowledge for QA systems,as we demonstrated, needs to be further explored.5 Experimentation5.1 MethodologyWe conducted a number of experiments to investigatethe effectiveness of our methods.First, we generated an encyclopedia by way of ourWeb-based method (see Sections 2 and 3), and evalu-ated the quality of the encyclopedia itself.Second, we applied the generated encyclopedia toour QA system (see Section 4), and evaluated its per-formance.
The second experiment can be seen as atask-oriented evaluation for our encyclopedia genera-tion method.In the first experiment, we collected 96 terms fromtechnical term questions in the Class II examination(the autumn of 1999).
We used as test inputs those 96terms and generated an encyclopedia, which was usedin the second experiment.For all the 96 test terms, Google (see Section 2.2)retrieved a positive number of pages, and the averagenumber of pages for one term was 196,503.
SinceGoogle practically outputs contents of the top 1,000pages, the remaining pages were not used in our ex-periments.In the following two sections, we explain the firstand second experiments, respectively.5.2 Evaluating Encyclopedia GenerationFor each test term, our method first computed P (d|c)using Equation (1) and discarded domains whoseP (d|c) was below 0.05.
Then, for each remaining do-main, descriptions with higher P (d|c) were selected asthe final outputs.We selected the top three (not one) descriptions foreach domain, because reading a couple of descriptions,which are short paragraphs, is not laborious for humanusers in real-world usage.
As a result, at least one de-scription was generated for 85 test terms, disregardingthe correctness.
The number of resultant descriptionswas 326 (3.8 per term).
We analyzed those descrip-tions from different perspectives.First, we analyzed the distribution of the Googleranks for the Web pages from which the top three de-scriptions were eventually retained.
Figure 2 showsthe result, where we have combined the pages ingroups of 50, so that the leftmost bar, for example, de-notes the number of used pages whose original Googleranks ranged from 1 to 50.Although the first group includes the largest numberof pages, other groups are also related to a relativelylarge number of pages.
In other words, our methodexploited a number of low ranking pages, which arenot browsed or utilized by most Web users.0102030405060700 100 200 300 400 500 600 700 800 900 1000#ofpagesrankingFigure 2: Distribution of rankings for original pages inGoogle.Second, we analyzed the distribution of domainsassigned to the 326 resultant descriptions.
Figure 3shows the result, in which, as expected, most descrip-tions were associated with the computer domain.However, the law domain was unexpectedly asso-ciated with a relatively great number of descriptions.We manually analyzed the resultant descriptions andfound that descriptions for which appropriate domainsare not defined in our domain model, such as sports,tended to be categorized into the law domain.computers (200), law (41), electricity (28),plants (15), medicine (10), finance (8),mathematics (8), mechanics (5), biotechnology (4),construction (2), ecology (2), chemistry (1),energy (1), oceanography (1)Figure 3: Distribution of domains related to the 326resultant descriptions.Third, we evaluated the accuracy of our method,that is, the quality of an encyclopedia our method gen-erated.
For this purpose, each of the resultant descrip-tions was judged as to whether or not it is a correct de-scription for a term in question.
Each domain assignedto descriptions was also judged correct or incorrect.We analyzed the result on a description-by-description basis, that is, all the generated descriptionswere considered independent of one another.
The ratioof correct descriptions, disregarding the domain cor-rectness, was 58.0% (189/326), and the ratio of cor-rect descriptions categorized into the correct domainwas 47.9% (156/326).However, since all the test terms are inherently re-lated to the IT field, we focused solely on descriptionscategorized into the computer domain.
In this case,the ratio of correct descriptions, disregarding the do-main correctness, was 62.0% (124/200), and the ratioof correct descriptions categorized into the correct do-main was 61.5% (123/200).In addition, we analyzed the result on a term-by-term basis, because reading only a couple of descrip-tions is not crucial.
In other words, we evaluatedeach term (not description), and in the case where atleast one correct description categorized into the cor-rect domain was generated for a term in question, wejudged it correct.
The ratio of correct terms was 89.4%(76/85), and in the case where we focused solely on thecomputer domain, the ratio was 84.8% (67/79).In other words, by reading a couple of descriptions(3.8 descriptions per term), human users can obtainknowledge of approximately 90% of input terms.Finally, we compared the resultant descriptions withan existing dictionary.
For this purpose, we used the?Nichigai?
computer dictionary (Nichigai Associates,1996), which lists approximately 30,000 Japanesetechnical terms related to the computer field, and con-tains descriptions for 13,588 terms.
In the Nichigaidictionary, 42 out of the 96 test terms were described.Our method, which generated correct descriptions as-sociated with the computer domain for 67 input terms,enhanced the Nichigai dictionary in terms of quantity.These results indicate that our method for generat-ing encyclopedias is of operational quality.5.3 Evaluating Question AnsweringWe used as test inputs 40 questions, which are relatedto technical terms collected from the Class II exami-nation in the autumn of 1999.The objective here is not only to evaluate the perfor-mance of our QA system itself, but also to evaluate thequality of the encyclopedia generated by our method.Thus, as performed in the first experiment (Sec-tion 5.2), we used the Nichigai computer dictionary asa baseline encyclopedia.
We compared the followingthree different resources as a knowledge base:?
the Nichigai dictionary (?Nichigai?),?
the descriptions generated in the first experiment(?Web?),?
combination of both resources (?Nichigai +Web?
).Table 1 shows the result of our comparative exper-iment, in which ?C?
and ?A?
denote coverage and ac-curacy, respectively, for variations of our QA system.Since all the questions we used are quadruple-choice, in case the system cannot answer the question,random choice can be performed to improve the cov-erage to 100%.
Thus, for each knowledge resource wecompared cases without/with random choice, whichare denoted ?w/o Random?
and ?w/ Random?
in Ta-ble 1, respectively.Table 1: Coverage and accuracy (%) for different ques-tion answering methods.w/o Random w/ RandomResource C A C ANichigai 50.0 65.0 100 45.0Web 92.5 48.6 100 46.9Nichigai + Web 95.0 63.2 100 61.3In the case where random choice was not per-formed, the Web-based encyclopedia noticeably im-proved the coverage for the Nichigai dictionary, butdecreased the accuracy.
However, by combining bothresources, the accuracy was noticeably improved, andthe coverage was comparable with that for the Nichi-gai dictionary.On the other hand, in the case where random choicewas performed, the Nichigai dictionary and the Web-based encyclopedia were comparable in terms of boththe coverage and accuracy.
Additionally, by combin-ing both resources, the accuracy was further improved.We also investigated the performance of our QAsystem where descriptions related to the computer do-main are solely used.
However, coverage/accuracy didnot significantly change, because as shown in Figure 3,most of the descriptions were inherently related to thecomputer domain.6 ConclusionThe World Wide Web has been an unprecedentedlyenormous information source, from which a numberof language processing methods have been exploredto extract, retrieve and discover various types of infor-mation.In this paper, we aimed at generating encyclopedicknowledge, which is valuable for many applicationsincluding human usage and natural language under-standing.
For this purpose, we reformalized an exist-ing Web-based extraction method, and proposed a newstatistical organization model to improve the quality ofextracted data.Given a term for which encyclopedic knowledge(i.e., descriptions) is to be generated, our method se-quentially performs a) retrieval of Web pages contain-ing the term, b) extraction of page fragments describ-ing the term, and c) organizing extracted descriptionsbased on domains (and consequently word senses).In addition, we proposed a question answering sys-tem, which answers interrogative questions associatedwith what, by using a Web-based encyclopedia as aknowledge base.
For the purpose of evaluation, weused as test inputs technical terms collected from theClass II IT engineers examination, and found that theencyclopedia generated through our method was ofoperational quality and quantity.We also used test questions from the Class II exam-ination, and evaluated the Web-based encyclopedia interms of question answering.
We found that our Web-based encyclopedia improved the system coverage ob-tained solely with an existing dictionary.
In addition,when we used both resources, the performance wasfurther improved.Future work would include generating informationassociated with more complex interrogations, such asones related to how and why, so as to enhance Web-based natural language understanding.AcknowledgmentsThe authors would like to thank NOVA, Inc. for theirsupport with the Nova dictionary and Katunobu Itou(The National Institute of Advanced Industrial Scienceand Technology, Japan) for his insightful comments onthis paper.ReferencesBrian Amento, Loren Terveen, and Will Hill.
2000.Does ?authority?
mean quality?
predicting expertquality ratings of Web documents.
In Proceedingsof the 23rd Annual International ACM SIGIR Con-ference on Research and Development in Informa-tion Retrieval, pages 296?303.Lalit.
R. Bahl, Frederick Jelinek, and Robert L. Mer-cer.
1983.
A maximum linklihood approach tocontinuous speech recognition.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,5(2):179?190.Sergey Brin and Lawrence Page.
1998.
The anatomyof a large-scale hypertextual Web search engine.Computer Networks, 30(1?7):107?117.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?311.Philip Clarkson and Ronald Rosenfeld.
1997.
Statisti-cal language modeling using the CMU-Cambridgetoolkit.
In Proceedings of EuroSpeech?97, pages2707?2710.Oren Etzioni.
1997.
Moving up the information foodchain.
AI Magazine, 18(2):11?18.Atsushi Fujii and Tetsuya Ishikawa.
2000.
Utilizingthe World Wide Web as an encyclopedia: Extract-ing term descriptions from semi-structured texts.In Proceedings of the 38th Annual Meeting of theAssociation for Computational Linguistics, pages488?495.Sanda M. Harabagiu, Marius A. Pas?ca, and Steven J.Maiorano.
2000.
Experiments with open-domaintextual question answering.
In Proceedings of the18th International Conference on ComputationalLinguistics, pages 292?298.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedingsof the 14th International Conference on Computa-tional Linguistics, pages 539?545.Hitachi Digital Heibonsha.
1998.
CD-ROM WorldEncyclopedia.
(In Japanese).Akihiro Inokuchi, Takashi Washio, Hiroshi Motoda,Kouhei Kumasawa, and Naohide Arai.
1999.
Bas-ket analysis for graph structured data.
In Proceed-ings of the 3rd Pacific-Asia Conference on Knowl-edge Discovery and Data Mining, pages 420?431.Makoto Iwayama and Takenobu Tokunaga.
1994.
Aprobabilistic model for text categorization: Basedon a single random variable with multiple values.
InProceedings of the 4th Conference on Applied Nat-ural Language Processing, pages 162?167.Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,Yoshitaka Hirano, Osamu Imaichi, and TomoakiImamura.
1997.
Japanese morphological analysissystem ChaSen manual.
Technical Report NAIST-IS-TR97007, NAIST.
(In Japanese).Andrew McCallum, Kamal Nigam, Jason Rennie, andKristie Seymore.
1999.
A machine learning ap-proach to building domain-specific search engines.In Proceedings of the 16th International Joint Con-ference on Artificial Intelligence, pages 662?667.Dan Moldovan and Sanda Harabagiu.
2000.
Thestructure and performance of an open-domain ques-tion answering system.
In Proceedings of the 38thAnnual Meeting of the Association for Computa-tional Linguistics, pages 563?570.Jun?ichi Nakamura and Makoto Nagao.
1988.
Extrac-tion of semantic information from an ordinary En-glish dictionary and its evaluation.
In Proceedingsof the 10th International Conference on Computa-tional Linguistics, pages 459?464.Nichigai Associates.
1996.
English-Japanese com-puter terminology dictionary.
(In Japanese).John Prager, Eric Brown, and Anni Coden.
2000.Question-answering by predictive annotation.
InProceedings of the 23rd Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval, pages 184?191.Philip Resnik.
1999.
Mining the Web for bilingualtexts.
In Proceedings of the 37th Annual Meetingof the Association for Computational Linguistics,pages 527?534.S.
E. Robertson and S. Walker.
1994.
Some simpleeffective approximations to the 2-poisson model forprobabilistic weighted retrieval.
In Proceedings ofthe 17th Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 232?241.Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?123.Stephen Soderland.
1997.
Learning to extract text-based information from the World Wide Web.
InProceedings of 3rd International Conference onKnowledge Discovery and Data Mining.Ellen M. Voorhees and Dawn M. Tice.
2000.
Buildinga question answering test collection.
In Proceed-ings of the 23rd Annual International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, pages 200?207.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Pro-ceedings of the 33rd Annual Meeting of the Associa-tion for Computational Linguistics, pages 189?196.Xiaolan Zhu and Susan Gauch.
2000.
Incorporatingquality metrics in centralized/distributed informa-tion retrieval on the World Wide Web.
In Proceed-ings of the 23rd Annual International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, pages 288?295.
