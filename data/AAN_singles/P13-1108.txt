Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1094?1104,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLearning to Extract International Relations from Political ContextBrendan O?ConnorSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAbrenocon@cs.cmu.eduBrandon M. StewartDepartment of GovernmentHarvard UniversityCambridge, MA 02139, USAbstewart@fas.harvard.eduNoah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAnasmith@cs.cmu.eduAbstractWe describe a new probabilistic modelfor extracting events between major polit-ical actors from news corpora.
Our un-supervised model brings together famil-iar components in natural language pro-cessing (like parsers and topic models)with contextual political information?temporal and dyad dependence?to in-fer latent event classes.
We quantita-tively evaluate the model?s performanceon political science benchmarks: recover-ing expert-assigned event class valences,and detecting real-world conflict.
We alsoconduct a small case study based on ourmodel?s inferences.A supplementary appendix, and replica-tion software/data are available online, at:http://brenocon.com/irevents1 IntroductionThe digitization of large news corpora has pro-vided an unparalleled opportunity for the system-atic study of international relations.
Since the mid-1960s political scientists have used political eventsdata, records of public micro-level interactions be-tween major political actors of the form ?someonedoes something to someone else?
as reported inthe open press (Schrodt, 2012), to study the pat-terns of interactions between political actors andhow they evolve over time.
Scaling this data effortto modern corpora presents an information extrac-tion challenge: can a structured collection of ac-curate, politically relevant events between majorpolitical actors be extracted automatically and ef-ficiently?
And can they be grouped into meaning-ful event types with a low-dimensional structureuseful for further analysis?We present an unsupervised approach to eventextraction, in which political structure and linguis-tic evidence are combined.
A political contextmodel of the relationship between a pair of polit-ical actors imposes a prior distribution over typesof linguistic events.
Our probabilistic model in-fers latent frames, each a distribution over textualexpressions of a kind of event, as well as a repre-sentation of the relationship between each politicalactor pair at each point in time.
We use syntacticpreprocessing and a logistic normal topic model,including latent temporal smoothing on the politi-cal context prior.We apply the model in a series of compar-isons to benchmark datasets in political science.First, we compare the automatically learned verbclasses to a pre-existing ontology and hand-craftedverb patterns from TABARI,1 an open-source andwidely used rule-based event extraction system forthis domain.
Second, we demonstrate correlationto a database of real-world international conflictevents, the Militarized Interstate Dispute (MID)dataset (Jones et al, 1996).
Third, we qualitativelyexamine a prominent case not included in the MIDdataset, Israeli-Palestinian relations, and comparethe recovered trends to the historical record.We outline the data used for event discovery(?2), describe our model (?3), inference (?4), eval-uation (?5), and comment on related work (?6).2 DataThe model we describe in ?3 is learned from acorpus of 6.5 million newswire articles from theEnglish Gigaword 4th edition (1994?2008, Parkeret al, 2009).
We also supplement it with a sam-ple of data from the New York Times AnnotatedCorpus (1987?2007, Sandhaus, 2008).2 The Stan-1Available from the Penn State Event Data Project:http://eventdata.psu.edu/2For arbitrary reasons this portion of the data is muchsmaller (we only parse the first five sentences of each arti-cle, while Gigaword has all sentences parsed), resulting inless than 2% as many tuples as from the Gigaword data.1094ford CoreNLP system,3 under default settings, wasused to POS-tag and parse the articles, to eventu-ally produce event tuples of the form?s, r, t, wpredpath?where s and r denote ?source?
and ?receiver?
ar-guments, which are political actor entities in a pre-defined set E , t is a timestep (i.e., a 7-day pe-riod) derived from the article?s published date, andwpredpath is a textual predicate expressed as a de-pendency path that typically includes a verb (weuse the terms ?predicate-path?
and ?verb-path?
in-terchangeably).
For example, on January 1, 2000,the AP reported ?Pakistan promptly accused In-dia,?
from which our preprocessing extracts the tu-ple ?PAK, IND, 678, accuse dobj???
?.
(The path ex-cludes the first source-side arc.)
Entities and verbpaths are identified through the following sets ofrules.Named entity recognition and resolution is donedeterministically by finding instances of countrynames from the CountryInfo.txt dictionary fromTABARI,4 which contains proper noun and adjec-tival forms for countries and administrative units.We supplement these with a few entries for in-ternational organizations from another dictionaryprovided by the same project, and clean up a fewambiguous names, resulting in a final actor dictio-nary of 235 entities and 2,500 names.Whenever a name is found, we identify its en-tity?s mention as the minimal noun phrase thatcontains it; if the name is an adjectival or noun-noun compound modifier, we traverse any suchamod and nn dependencies to the noun phrasehead.
Thus NATO bombing, British view, andPalestinian militant resolve to the entity codes IG-ONAT, GBR, and PSE respectively.We are interested in identifying actions initi-ated by agents of one country targeted towards an-other, and hence concentrate on verbs, analyzingthe ?CCprocessed?
version of the Stanford Depen-dencies (de Marneffe and Manning, 2008).
Verbpaths are identified by looking at the shortest de-pendency path between two mentions in a sen-tence.
If one of the mentions is immediately dom-inated by a nsubj or agent relation, we considerthat the Source actor, and the other mention is theReceiver.
The most common cases are simple di-rect objects and prepositional arguments like talk3http://nlp.stanford.edu/software/corenlp.shtml4http://eventdata.psu.edu/software.dir/dictionaries.html.prep with?????
and fight prep alongside???????
(?talk with R,?
?fightalongside R?)
but many interesting multiword con-structions also result, such as reject dobj???
allegationposs???
(?rejected R?s allegation?)
or verb chains asin offer xcomp???
help dobj???
(?offer to help R?
).We wish to focus on instances of directly re-ported events, so attempt to remove factively com-plicated cases such as indirect reporting and hy-potheticals by discarding all predicate paths forwhich any verb on the path has an off-path gov-erning verb with a non-conj relation.
(For exam-ple, the verb at the root of a sentence always sur-vives this filter.)
Without this filter, the ?s, r, w?tuple ?USA, CUB, want xcomp???
seize dobj???
?
is ex-tracted from the sentence ?Parliament Speaker Ri-cardo Alarcon said the United States wants to seizeCuba and take over its lands?
; the filter removesit since wants is dominated by an off-path verbthrough say ccomp???
wants.
The filter was iterativelydeveloped by inspecting dozens of output exam-ples and their labelings under successive changesto the rules.Finally, only paths length 4 or less are allowed,the final dependency relation for the receiver maynot be nsubj or agent, and the path may not containany of the dependency relations conj, parataxis,det, or dep.
We use lemmatized word forms indefining the paths.Several document filters are applied before tu-ple extraction.
Deduplication removes 8.5% of ar-ticles.5 For topic filtering, we apply a series ofkeyword filters to remove sports and finance news,and also apply a text classifier for diplomatic andmilitary news, trained on several hundred man-ually labeled news articles (using `1-regularizedlogistic regression with unigram and bigram fea-tures).
Other filters remove non-textual junk andnon-standard punctuation likely to cause parse er-rors.For experiments we remove tuples where thesource and receiver entities are the same, and re-strict to tuples with dyads that occur at least 500times, and predicate paths that occur at least 10times.
This yields 365,623 event tuples from235,830 documents, for 421 dyads and 10,457unique predicate paths.
We define timestepsto be 7-day periods, resulting in 1,149 discrete5We use a simple form of shingling (ch.
3, Rajaraman andUllman, 2011): represent a document signature as its J = 5lowercased bigrams with the lowest hash values, and reject adocument with a signature that has been seen before withinthe same month.
J was manually tuned, as it affects the pre-cision/recall tradeoff.1095ik?k,s,r,t?s,r,tz wpredpathrLanguageModelP(Text|EventType)ContextModelP(EventType|Context)b 2kss "Source"entityr "Receiver"entityt  Timestepi  Event tuplek  Framekk k,s,r,t 1  k,s,r,t?2?kFigure 1: Directed probabilistic diagram of the model for one(s, r, t) dyad-time context, for the smoothed model.timesteps (1987 through 2008, though the vast ma-jority of data starts in 1994).3 ModelWe design two models to learn linguistic eventclasses over predicate paths by conditioning onreal-world contextual information about interna-tional politics, p(wpredpath | s, r, t), leveraging thefact there tends to be dyadic and temporal coher-ence in international relations: the types of actionsthat are likely to occur between nations tend to besimilar within the same dyad, and usually their dis-tribution changes smoothly over time.Our model decomposes into two submodels:a Context submodel, which encodes how politi-cal context affects the probability distribution overevent types, and a Language submodel, for howthose events are manifested as textual predicatepaths (Figure 1).
The overall generative process isas follows.
We color global parameters for a frameblue, and local context parameters red, and usethe term ?frame?
as a synonym for ?event type.
?The fixed hyperparameter K denotes the numberof frames.?
The context model generates a frame prior ?s,r,tfor every context (s, r, t).?
Language model:?
Draw lexical sparsity parameter b from a dif-fuse prior (see ?4).?
For each frame k, draw a multinomial distri-bution of dependency paths, ?k ?
Dir(b/V )(where V is the number of dependency pathtypes).?
For each (s, r, t), for every event tuple i inthat context,?
Sample its frame z(i) ?
Mult(?s,r,t).?
Sample its predicate realizationw(i)predpath ?
Mult(?z(i)).Thus the language model is very similar to a topicmodel?s generation of token topics and wordtypes.We use structured logistic normal distributionsto represent contextual effects.
The simplest is thevanilla (V) context model,?
For each frame k, draw global parameters fromdiffuse priors: prevalence ?k and variability ?2k.?
For each (s, r, t),?
Draw ?k,s,r,t ?
N(?k, ?2k) for each frame k.?
Apply a softmax transform,?k,s,r,t =exp ?k,s,r,t?Kk?=1 exp ?k?,s,r,tThus the vector ?
?,s,r,t encodes the relative log-odds of the different frames for events appearingin the context (s, r, t).
This simple logistic nor-mal prior is, in terms of topic models, analogousto the asymmetric Dirichlet prior version of LDAin Wallach et al (2009), since the ?k parametercan learn that some frames tend to be more likelythan others.
The variance parameters ?2k controladmixture sparsity, and are analogous to a Dirich-let?s concentration parameter.Smoothing Frames Across TimeThe vanilla model is capable of inducing framesthrough dependency path co-occurences, whenmultiple events occur in a given context.
How-ever, many dyad-time slices are very sparse; forexample, most dyads (all but 18) have events infewer than half the time slices in the dataset.
Onesolution is to increase the bucket size (e.g., tomonths); however, previous work in political sci-ence has demonstrated that answering questionsof interest about reciprocity dynamics requires re-covering the events at weekly or even daily gran-ularity (Shellman, 2004), and in any case widebuckets help only so much for dyads with fewerevents or less media attention.
Therefore we pro-pose a smoothed frames (SF) model, in which the1096frame distribution for a given dyad comes from alatent parameter ?
?,s,r,t that smoothly varies overtime.
For each (s, r), draw the first timestep?s val-ues as ?k,s,r,1 ?
N(0, 100), and for each context(s, r, t > 1),?
Draw ?k,s,r,t ?
N(?k,s,r,t?1, ?2)?
Draw ?k,s,r,t ?
N(?k + ?k,s,r,t, ?2k)Other parameters (?k, ?2k) are same as the vanillamodel.
This model assumes a random walk pro-cess on ?, a variable which exists even for contextsthat contain no events.
Thus inferences about ?will be smoothed according to event data at nearbytimesteps.
This is an instance of a linear Gaussianstate-space model (also known as a linear dynami-cal system or dynamic linear model), and is a con-venient formulation because it has well-known ex-act inference algorithms.
Dynamic linear modelshave been used elsewhere in machine learning andpolitical science to allow latent topic frequencies(Blei and Lafferty, 2006; Quinn et al, 2010) andideological positions (Martin and Quinn, 2002) tosmoothly change over time, and thus share statis-tical strength between timesteps.4 InferenceAfter randomly initializing all ?k,s,r,t, inference isperformed by a blocked Gibbs sampler, alternat-ing resamplings for three major groups of vari-ables: the language model (z,?
), context model(?, ?, ?, p), and the ?, ?
variables, which bottle-neck between the submodels.The language model sampler sequentially up-dates every z(i) (and implicitly ?
via collapsing)in the manner of Griffiths and Steyvers (2004):p(z(i)|?, w(i), b) ?
?s,r,t,z(nw,z + b/V )/(nz + b),where counts n are for all event tuples besides i.For the context model, ?
is conjugate resam-pled as a normal mean.
The random walk vari-ables ?
are sampled with the forward-filtering-backward-sampling algorithm (FFBS; Harrisonand West, 1997; Carter and Kohn, 1994); there isone slight modification of the standard dynamiclinear model that the zero-count weeks have no ?observation; the Kalman filter implementation isappropriately modified to handle this.The ?
update step is challenging since it is anonconjugate prior to the z counts.
Logistic nor-mal distributions were introduced to text mod-eling by Blei and Lafferty (2007), who devel-oped a variational approximation; however, wefind that experimenting with different models iseasier in the Gibbs sampling framework.
WhileGibbs sampling for logistic normal priors is pos-sible using auxiliary variable methods (Mimnoet al, 2008; Holmes and Held, 2006; Polson et al,2012), it can be slow to converge.
We opt forthe more computationally efficient approach ofZeger and Karim (1991) and Hoff (2003), usinga Laplace approximation to p(?
| ?
?,?, z), whichis a mode-centered Gaussian having inverse co-variance equal to the unnormalized log-posterior?snegative Hessian (?8.4 in Murphy, 2012).
We findthe mode with the linear-time Newton algorithmfrom Eisenstein et al (2011), and sample in lineartime by only using the Hessian?s diagonal as theinverse covariance (i.e., an axis-aligned normal),since a full multivariate normal sample requiresa cubic-time-to-compute Cholesky root of the co-variance matrix.
This ??
sample is a proposal for aMetropolis-within-Gibbs step, which is moved toaccording to the standard Metropolis-Hastings ac-ceptance rule.
Acceptance rates differ by K, rang-ing approximately from 30% (K = 100) to nearly100% (small K).Finally, we use diffuse priors on all global pa-rameters, conjugate resampling variances ?2, ?konce per iteration, and slice sampling (Neal, 2003)the Dirichlet concentration b every 100 iterations.Automatically learning these was extremely con-venient for model-fitting; the only hyperparameterwe set manually wasK.
It also allowed us to mon-itor the convergence of dispersion parameters tohelp debug and assess MCMC mixing.
For othermodeling and implementation details, see the on-line appendix and software.5 ExperimentsWe fit the two models on the dataset described in?2, varying the number of frames K, with 8 ormore separate runs for each setting.
Posteriors aresaved and averaged from 11 Gibbs samples (every100 iterations from 9,000 to 10,000) for analysis.We present intrinsic (?5.1) and extrinsic (?5.2)quantitative evaluations, and a qualitative casestudy (?5.4).5.1 Lexical Scale ImpurityIn the international relations literature, much ofthe analysis of text-based events data makes use ofa unidimensional conflict to cooperation scale.
Apopular event ontology in this domain, CAMEO,consists of around 300 different event types, each1097given an expert-assigned scale in the range from?10 to +10 (Gerner et al, 2002), derived froma judgement collection experiment in Goldstein(1992).
The TABARI pattern-based event extrac-tion program comes with a list of almost 16,000manually engineered verb patterns, each assignedto one CAMEO event type.It is interesting to consider the extent to whichour unsupervised model is able to recover theexpert-designed ontology.
Given that many ofthe categories are very fine-grained (e.g.
?Expressintent to de-escalate military engagement?
), weelect to measure model quality as lexical scale pu-rity: whether all the predicate paths within oneautomatically learned frame tend to have similargold-standard scale scores.
(This measures clus-ter cohesiveness against a one-dimensional con-tinuous scale, instead of measuring cluster cohe-siveness against a gold-standard clustering as inVI, Rand index, or purity.)
To calculate this, weconstruct a mapping between our corpus-derivedverb path vocabulary and the TABARI verb pat-terns, many of which contain one to several wordstems that are intended to be matched in surfaceorder.
Many of our dependency paths, when tra-versed from the source to receiver direction, alsofollow surface order, due to English?s SVO wordorder.6 Therefore we convert each path to aword sequence and match against the TABARIlexicon?plus a few modifications for differencesin infinitives and stemming?and find 528 depen-dency path matches.
We assign each path w agold-standard scale g(w) by resolving through itsmatching pattern?s CAMEO code.We formalize lexical scale impurity as the av-erage absolute difference of scale values betweentwo predicate paths under the same frame.
Specif-ically, we want a token-level posterior expectationE(|g(wi)?
g(wj)| | zi = zj , wi 6= wj) (1)which is taken over pairs of path instances (i, j)where both paths wi, wj are in M , the set of verbpaths that were matched between the lexicons.This can be reformulated at the type level as:71N?k?w,v?Mw 6=vnw,k nv,k |g(w)?
g(v)| (2)6There are plenty of exceptions where a Source-to-Receiver path traversal can have a right-to-left move, suchas dependency edges for posessives.
This approach can notmatch them.7Derivation in supplementary appendix.where n refers to the averaged Gibbs samples?counts of event tuples having frame k and a par-ticular verb path,8 and N is the number of to-ken comparisons (i.e.
the same sum, but with a1 replacing the distance).
The worst possible im-purity is upper bounded at 20 (= max(g(w)) ?min(g(w))) and the best possible is 0.
We alsocompute a randomized null hypothesis to see howlow impurity can be by chance: each of ?1000simulations randomly assigns each path in M toone of K frames (all its instances are exclusivelyassigned to that frame), and computes the impu-rity.
On average the impurity is same at all K,but variance increases with K (since small clus-ters might by chance get a highly similar paths inthem), necessitating this null hypothesis analysis.We report the 5th percentile over simulations.5.2 Conflict DetectionPolitical events data has shown considerablepromise as a tool for crisis early warning systems(O?Brien, 2010; Brandt et al, 2011).
While con-flict forecasting is a potential application of ourmodel, we conduct a simpler prediction task tovalidate whether the model is learning somethinguseful: based on news text, tell whether or not anarmed conflict is currently happening.
For a goldstandard, we use the Militarized Interstate Dispute(MID) dataset (Jones et al, 1996; Ghosn et al,2004), which documents historical internationaldisputes.
While not without critics, the MID datais the most prominent dataset in the field of in-ternational relations.
We use the Dyadic MIDs,each of which ranks hostility levels between pairsof actors on a five point scale over a date inter-val; we define conflict to be the top two categories?Use of Force?
(4) and ?War?
(5).
We convertthe data into a variable ys,r,t, the highest hostilitylevel reached by actor s directed towards receiverr in the dispute that overlaps with our 7-day in-terval t, and want to predict the binary indicator1{ys,r,t ?
4}.
For the illustrative examples (USAto Iraq, and the Israel-Palestine example below)we use results from a smaller but more internallycomparable dataset consisting of the 2 million As-sociated Press articles within the Gigaword cor-pus.For an example of the MID data, see Figure 2,which depicts three disputes between the US and8Results are nearly identical whether we use counts av-eraged across samples (thus giving posterior marginals),or simply use counts from a single sample (i.e., iteration10,000).1098kill, fire at,seal, invade,enteraccuse,criticize, warn,reject, urgeaccuse,reject, blame,kill, takecriticize, call,ask, condemn,denounceUSA to Iraq (Vanilla Model)0.00.40.81995 1996 1997 1998 1999 2000 2001 2002USA to Iraq (Smoothed Frames)0.00.40.81995 1996 1997 1998 1999 2000 2001 2002Figure 2: The USA?Iraq directed dyad, analyzed bysmoothed (above) and vanilla (below) models, showing (1)gold-standard MID values (red intervals along top), (2) weekswith non-zero event counts (vertical lines along x-axis), (3)posterior E[?k,USA,IRQ,t] inferences for two frames chosenfrom two different K = 5 models, and (4) most commonverb paths for each frame (right).
Frames corresponding tomaterial and verbal conflict were chosen for display.
Verticalline indicates Operation Desert Fox (see ?5.2).Iraq in this time period.
The MID labels aremarked in red.The first dispute is a ?display of force?
(level3), cataloguing the U.S. response to a series oftroop movements along the border with Kuwait.The third dispute (10/7/1997 to 10/10/2001) be-gins with increasing Iraqi violations of the no-fly zone, resulting in U.S. and U.K. retaliation,reaching a high intensity with Operation DesertFox, a four-day bombing campaign from Decem-ber 16 to 19, 1998?which is not shown in MID.These cases highlight MID?s limitations?while itis well regarded in the political science literature,its coarse level of aggregation can fail to capturevariation in conflict intensity.Figure 2 also shows model inferences.
Oursmoothed model captures some of these phenom-ena here, showing clear trends for two relevantframes, including a dramatic change in Decem-ber 1998.
The vanilla model has a harder time,since it cannot combine evidence between differ-ent timesteps.The MID dataset overlaps with our data for 470weeks, from 1993 through 2001.
After excludingdyads with actors that the MID data does not in-tend to include?Kosovo, Tibet, Palestine, and in-ternational organizations?we have 267 directeddyads for evaluation, 117 of which have at leastone dispute in the MID data.
(Dyads with no dis-pute in the MID data, such as Germany-France,are assumed to have y = 0 throughout the timeperiod.)
About 7% of the dyad-time contexts havea dispute under these definitions.We split the dataset by time, training on the firsthalf of the data and testing on the second half, andmeasure area under the receiver operating charac-teristic curve (AUC).9 For each model, we train an`1-regularized logistic regression10 with the K el-ements of ?
?,s,r,t as input features, tuning the reg-ularization parameter within the training set (bysplitting it in half again) to optimize held-out like-lihood.
We weight instances to balance positiveand negative examples.
Training is on all individ-ual ?
samples at once (thus accounting for pos-terior uncertainty in learning), and final predictedprobabilities are averaged from individual proba-bilities from each ?
test set sample, thus propa-gating posterior uncertainty into the predictions.We also create a baseline `1-regularized logisticregression that uses normalized dependency pathcounts as the features (10,457 features).
For boththe baseline and vanilla model, contexts with noevents are given a feature vector of all zeros.11(We also explored an alternative evaluation setup,to hold out by dyad; however, the performancevariance is quite high between different randomdyad splits.
)5.3 ResultsResults are shown in Figure 3.12The verb-path logistic regression performsstrongly at AUC 0.62; it outperforms all ofthe vanilla frame models.
This is an exam-ple of individual lexical features outperforming atopic model for predictive task, because the topicmodel?s dimension reduction obscures importantindicators from individual words.
Similarly, Ger-rish and Blei (2011) found that word-based regres-sion outperformed a customized topic model whenpredicting Congressional bill passage, and Eisen-9AUC can be interpreted as follows: given a positive andnegative example, what is the probability that the classifier?sconfidences order them correctly?
Random noise or predict-ing all the same class both give AUC 0.5.10Using the R glmnet package (Friedman et al, 2010).11For the vanilla model, this performed better than linearinterpolation (about 0.03 AUC), and with less variance be-tween runs.12Due to an implementation bug, the model put the vastmajority of the probability mass only on K ?
1 frames,so these settings might be better thought of as K =1, 2, 3, 4, 9, .
.
.
; see the appendix for details.1099l l0.40.50.60.72 3 4 5 10 20 50 100Number of frames (K)Conflict predictionAUC(higheris better)modell Log.
RegVanillaSmoothedl l l lllll1.52.53.54.55.52 3 4 5 10 20 50 100Number of frames (K)Scale impurity(lowerisbetter)modell NullVanillaSmoothedFigure 3: Evaluation results.
Each point indicates one modelrun.
Lines show the average per K, with vertical lines indi-cating the 95% bootstrapped interval.
Top: Conflict detectionAUC for different models (?5.2).
Green line is the verb-pathlogistic regression baseline.
Bottom: Lexical scale impurity(?5.1).
Top green line indicates the simple random baselineE(|g(wi) ?
g(wj)|) = 5.33; the second green line is fromthe random assignment baseline.stein et al (2010) found word-based regressionoutperformed Supervised LDA for geolocation,13and we have noticed this phenomenon for othertext-based prediction problems.However, adding smoothing to the model sub-stantially increases performance, and in fact out-performs the verb-path regression at K = 100.It is unclear why the vanilla model fails to in-crease performance in K. Note also, the vanillamodel exhibits very little variability in predictionperformance between model runs, in comparisonto the smoothed model which is much more vari-able (presumably due to the higher number of pa-rameters in the model); at small values of K, thesmoothed model can perform poorly.
It would alsobe interesting to analyze the smoothed model withhigher values of K and find where it peaks.We view the conflict detection task only as oneof several validations, and thus turn to lexical eval-uation of the induced frames.
For lexical scalepurity (bottom of Figure 3), the models performabout the same, with the smoothed model a lit-tle bit worse at some values of K (though some-times with better stability of the fits?opposite ofthe conflict detection task).
This suggests that se-mantic coherence does not benefit from the longer-13In the latter, a problem-specific topic model did best.range temporal dependencies.In general, performance improves with higherK, but not beyond K = 50.
This suggests themodel reaches a limit for how fine-grained of se-mantics it can learn.5.4 Case studyHere we qualitatively examine the narrative storybetween the dyad with the highest frequency ofevents in our dataset, the Israeli-Palestinian rela-tionship, finding qualitative agreement with othercase studies of this conflict (Brandt et al, 2012;Goldstein et al, 2001; Schrodt and Gerner, 2004).
(The MID dataset does not include this conflict be-cause the Palestinians are not considered a stateactor.)
Using the Associated Press subset, we plotthe highest incidence frames from one run of theK = 20 smoothed frame models, for the two di-rected dyads, and highlight some of the interestingrelationships.Figure 4(a) shows that tradeoffs in the use ofmilitary vs. police action by Israel towards thePalestinians tracks with major historical events.The first period in the data where police actions(?impose, seal, capture, seize, arrest?)
exceed mil-itary actions (?kill, fire, enter, attack, raid?)
iswith the signing of the ?Interim Agreement on theWest Bank and the Gaza Strip,?
also known as theOslo II agreement.
This balance persists until theabrupt breakdown in relations that followed theunsuccessful Camp David Summit in July of 2000,which generally marks the starting point of thewave of violence known as the Second Intifada.In Figure 4(b) we show that our model producesa frame which captures the legal aftermath of par-ticular events (?accuse, criticize,?
but also ?detain,release, extradite, charge?).
Each of the majorspikes in the data coincides with a particular eventwhich either involves the investigation of a par-ticular attack or series of attacks (as in A,B,E) ora discussion about prisoner swaps or mass arrests(as in events D, F, J).Our model also picks up positive diplomaticevents, as seen in Figure 4(c), a frame describ-ing Israeli diplomatic actions towards Palestine(?meet with, sign with, praise, say with, arrivein?).
Not only do the spikes coincide with majorpeace treaties and negotiations, but the model cor-rectly characterizes the relative lack of positivelyvalenced action from the beginning of the SecondIntifada until its end around 2005?2006.In Figure 4(d) we show the relevant frames de-1100a.0.00.40.8Israeli Use of Force Tradeoff1994 1997 2000 2002 2005 2007Second Intifada BeginsOslo II Signedb.0.00.40.81.2Police Actions and Crime ResponseABCDEFGH I J1994 1997 2000 2002 2005 2007A: Series of Suicide Attacksin JerusalemB: Island of Peace MassacreC: Arrests over ProtestsD: Tensions over Treatmentof Pal.
PrisonersE: Passover MassacreF: 400-Person Prisoner SwapG: Gaza Street Bus BombingH: Stage Club BombingI: House to House Sweep for 7militant leadersJ: Major Prisoner Releasec.0.00.40.8Israeli?Palestinian DiplomacyA B C D E F1994 1997 2000 2002 2005 2007C: U.S. Calls for West BankWithdrawalD: Deadlines for Wye River PeaceAccordE: Negotiations in MeccaF: Annapolis ConferenceA: Israel-Jordan PeaceTreatyB: Hebron Protocold.0.00.40.8Palestinian Use of Force1994 1997 2000 2002 2005 2007Figure 4: For Israel-Palestinian directed dyads, plots ofE[?]
(proportion of weekly events in a frame) over time, annotated withhistorical events.
(a): Words are ?kill, fire at, enter, kill, attack, raid, strike, move, pound, bomb?
and ?impose, seal, capture,seize, arrest, ease, close, deport, close, release?
(b): ?accuse, criticize, reject, tell, hand to, warn, ask, detain, release, order?
(c):?meet with, sign with, praise, say with, arrive in, host, tell, welcome, join, thank?
(d): again the same ?kill, fire at?
frame in (a),plus the erroneous frame (see text) ?include, join, fly to, have relation with, protest to, call, include bomber appos????
informerfor?.
Figures (b) and (c) use linear interpolation for zero-count weeks (thus relying exclusively on the model for smoothing);(a) and (d) apply a lowess smoother.
(a-c) are for the ISR?PSE direction; (d) is PSE?ISR.picting use of force from the Palestinians towardsthe Israelis (brown trend line).
At first, the dropin the use of force frame immediately followingthe start of the Second Intifada seems inconsis-tent with the historical record.
However, there is aconcucrrent rise in a different frame driven by theword ?include?, which actually appears here dueto an NLP error compounded with an artifact ofthe data source.
A casualties report article, con-taining variants of the text ?The Palestinian fig-ure includes... 13 Israeli Arabs...?, is repeated 27times over two years.
?Palestinian figure?
is er-roneously identified as the PSE entity, and severalnoun phrases in a list are identified as separate re-ceivers.
This issue causes 39 of all 86 PSE?ISRevents during this period to use the word ?in-clude?, accounting for the rise in that frame.
(Thishighlights how better natural language processingcould help the model, and the dangers of falsepositives for this type of data analysis, especiallyin small-sample drilldowns.)
Discounting this er-roneous inference, the results are consistent withheightened violence during this period.We conclude the frame extractions for theIsraeli-Palestinian case are consistent with the his-torical record over the period of study.6 Related Work6.1 Events Data in Political ScienceProjects using hand-collected events data repre-sent some of the earliest efforts in the statisti-cal study of international relations, dating back tothe 1960s (Rummel, 1968; Azar and Sloan, 1975;McClelland, 1970).
Beginning in the mid-1980s,political scientists began experimenting with au-tomated rule-based extraction systems (Schrodtand Gerner, 1994).
These efforts culminated inthe open-source program, TABARI, which usespattern matching from extensive hand-developedphrase dictionaries, combined with basic part ofspeech tagging (Schrodt, 2001); a rough analoguein the information extraction literature might bethe rule-based, finite-state FASTUS system forMUC IE (Hobbs et al, 1997), though TABARI isrestricted to single sentence analysis.
Later pro-prietary work has apparently incorporated moreextensive NLP (e.g., sentence parsing) thoughfew details are available (King and Lowe, 2003).The most recent published work we know of, byBoschee et al (2013), uses a proprietary parsingand coreference system (BBN SERIF, Ramshawet al, 2011), and directly compares to TABARI,finding significantly higher accuracy.
The origi-1101nal TABARI system is still actively being devel-oped, including just-released work on a new 200million event dataset, GDELT (Schrodt and Lee-taru, 2013).14 All these systems crucially rely onhand-built pattern dictionaries.It is extremely labor intensive to develop thesedictionaries.
Schrodt (2006) estimates 4,000trained person-hours were required to create dic-tionaries of political actors in the Middle East, andthe phrase dictionary took dramatically longer; thecomments in TABARI?s phrase dictionary indicatesome of its 15,789 entries were created as early as1991.
Ideally, any new events data solution wouldincorporate the extensive work already completedby political scientists in this area while minimiz-ing the need for further dictionary development.
Inthis work we use the actor dictionaries, and hopeto incorporate the verb patterns in future work.6.2 Events in Natural Language ProcessingPolitical event extraction from news has also re-ceived considerable attention within natural lan-guage processing in part due to government-funded challenges such as MUC-3 and MUC-4(Lehnert, 1994), which focused on the extractionof terrorist events, as well as the more recentACE program.
The work in this paper is inspiredby unsupervised approaches that seek to discovertypes of relations and events, instead of assumingthem to be pre-specified; this includes research un-der various headings such as template/frame/eventlearning (Cheung et al, 2013; Modi et al, 2012;Chambers and Jurafsky, 2011; Li et al, 2010; Be-jan, 2008), script learning (Regneri et al, 2010;Chambers and Jurafsky, 2009), relation learning(Yao et al, 2011), open information extraction(Banko et al, 2007; Carlson et al, 2010), verbcaseframe learning (Rooth et al, 1999; Gildea,2002; Grenager and Manning, 2006; Lang and La-pata, 2010; O?
Se?aghdha, 2010; Titov and Klemen-tiev, 2012), and a version of frame learning called?unsupervised semantic parsing?
(Titov and Kle-mentiev, 2011; Poon and Domingos, 2009).
Un-like much of the previous literature, we do notlearn latent roles/slots.
Event extraction is alsoa large literature, including supervised systemstargeting problems similar to MUC and politicalevents (Piskorski and Atkinson, 2011; Piskorskiet al, 2011; Sanfilippo et al, 2008).One can also see this work as a relational ex-14http://eventdata.psu.edu/data.dir/GDELT.htmltension of co-occurence-based methods such asGerrish (2013; ch.
4), Diesner and Carley (2005),Chang et al (2009), or Newman et al (2006),which perform bag-of-words-style analysis of textfragments containing co-occurring entities.
(Ger-rish also analyzed the international relations do-main, using supervised bag-of-words regressionto assess the expressed valence between a pairof actors in a news paragraph, using the predic-tions as observations in a latent temporal model,and compared to MID.)
We instead use parsing toget a much more focused and interpretable repre-sentation of the relationship between textually co-occurring entities; namely, that they are the sourceand target of an action event.
This is more in linewith work in relation extraction on biomedical sci-entific articles (Friedman et al, 2001; Rzhetskyet al, 2004) which uses parsing to extracting a net-work of how different entities, like drugs or pro-teins, interact.7 ConclusionLarge-scale information extraction can dramati-cally enhance the study of political behavior.
Herewe present a novel unsupervised approach to animportant data collection effort in the social sci-ences.
We see international relations as a richand practically useful domain for the developmentof text analysis methods that jointly infer events,relations, and sociopolitical context.
There arenumerous areas for future work, such as: usingverb dictionaries as semi-supervised seeds or pri-ors; interactive learning between political scienceresearchers and unsupervised algorithms; build-ing low-dimensional scaling, or hierarchical struc-ture, into the model; and learning the actor liststo handle changing real-world situations and newdomains.
In particular, adding more supervisionto the model will be crucial to improve semanticquality and make it useful for researchers.AcknowledgmentsThanks to Justin Betteridge for providing the parsed Giga-word corpus, Erin Baggott for help in developing the doc-ument filter, and the anonymous reviewers for helpful com-ments.
This research was supported in part by NSF grant IIS-1211277, and was made possible through the use of comput-ing resources made available by the Pittsburgh Supercomput-ing Center.
Brandon Stewart gratefully acknowledges fund-ing from an NSF Graduate Research Fellowship.ReferencesAzar, E. E. and Sloan, T. (1975).
Dimensions of interactions.Technical report, University Center of International Stud-ies, University of Pittsburgh, Pittsburgh.1102Banko, M., Cafarella, M. J., Soderland, S., Broadhead, M.,and Etzioni, O.
(2007).
Open Information Extraction fromthe Web.
IJCAI.Bejan, C. A.
(2008).
Unsupervised discovery of event sce-narios from texts.
In Proceedings of the 21st Florida Arti-ficial Intelligence Research Society International Confer-ence (FLAIRS), Coconut Grove, FL, USA.Blei, D. M. and Lafferty, J. D. (2006).
Dynamic topic models.In Proceedings of ICML.Blei, D. M. and Lafferty, J. D. (2007).
A correlated topicmodel of science.
Annals of Applied Statistics, 1(1), 17?35.Boschee, E., Natarajan, P., and Weischedel, R. (2013).
Au-tomatic extraction of events from open source text forpredictive forecasting.
Handbook of Computational Ap-proaches to Counterterrorism, page 51.Brandt, P. T., Freeman, J. R., and Schrodt, P. A.
(2011).
Realtime, time series forecasting of inter-and intra-state po-litical conflict.
Conflict Management and Peace Science,28(1), 41?64.Brandt, P. T., Freeman, J. R., Lin, T.-m., and Schrodt, P.
A.(2012).
A Bayesian time series approach to the compari-son of conflict dynamics.
In APSA 2012 Annual MeetingPaper.Carlson, A., Betteridge, J., Kisiel, B., Settles, B., Hruschka,E.
R., and Mitchell, T. M. (2010).
Toward an architecturefor never-ending language learning.
In Proceedings of theConference on Artificial Intelligence (AAAI), pages 1306?1313.Carter, C. K. and Kohn, R. (1994).
On Gibbs sampling forstate space models.
Biometrika, 81(3), 541?553.Chambers, N. and Jurafsky, D. (2009).
Unsupervised learn-ing of narrative schemas and their participants.
In Pro-ceedings of ACL-IJCNLP.
Association for ComputationalLinguistics.Chambers, N. and Jurafsky, D. (2011).
Template-based infor-mation extraction without the templates.
In Proceedingsof ACL.Chang, J., Boyd-Graber, J., and Blei, D. M. (2009).
Con-nections between the lines: augmenting social networkswith text.
In Proceedings of the 15th ACM SIGKDD in-ternational conference on Knowledge discovery and datamining, pages 169?178.
ACM.Cheung, J. C. K., Poon, H., and Vanderwende, L. (2013).Probabilistic frame induction.
In Proceedings of NAACL.arXiv preprint arXiv:1302.4813.de Marneffe, M.-C. and Manning, C. D. (2008).
Stanfordtyped dependencies manual.
Technical report, StanfordUniversity.Diesner, J. and Carley, K. M. (2005).
Revealing socialstructure from texts: meta-matrix text analysis as a novelmethod for network text analysis.
In Causal mapping forinformation systems and technology research, pages 81?108.
Harrisburg, PA: Idea Group Publishing.Eisenstein, J., O?Connor, B., Smith, N. A., and Xing, E. P.(2010).
A latent variable model for geographic lexicalvariation.
In Proceedings of the 2010 Conference on Em-pirical Methods in Natural Language Processing, pages1277?1287.Eisenstein, J., Ahmed, A., and Xing, E. (2011).
Sparse ad-ditive generative models of text.
In Proceedings of ICML,pages 1041?1048.Friedman, C., Kra, P., Yu, H., Krauthammer, M., and Rzhet-sky, A.
(2001).
GENIES: a natural-language process-ing system for the extraction of molecular pathways fromjournal articles.
Bioinformatics, 17(suppl 1), S74?S82.Friedman, J., Hastie, T., and Tibshirani, R. (2010).
Regular-ization paths for generalized linear models via coordinatedescent.
Journal of Statistical Software, 33(1).Gerner, D. J., Schrodt, P. A., Yilmaz, O., and Abu-Jabr, R.(2002).
The Creation of CAMEO (Conflict and Media-tion Event Observations): An Event Data Framework fora Post Cold War World.
Annual Meeting of the AmericanPolitical Science Association.Gerrish, S. M. (2013).
Applications of Latent Variable Mod-els in Modeling Influence and Decision Making.
Ph.D.thesis, Princeton University.Gerrish, S. M. and Blei, D. M. (2011).
Predicting legislativeroll calls from text.
In Proceedings of ICML.Ghosn, F., Palmer, G., and Bremer, S. A.
(2004).
The MID3data set, 1993?2001: Procedures, coding rules, and de-scription.
Conflict Management and Peace Science, 21(2),133?154.Gildea, D. (2002).
Probabilistic models of verb-argumentstructure.
In Proceedings of COLING.Goldstein, J. S. (1992).
A conflict-cooperation scale forWEIS events data.
Journal of Conflict Resolution, 36,369?385.Goldstein, J. S., Pevehouse, J. C., Gerner, D. J., and Telhami,S.
(2001).
Reciprocity, triangularity, and cooperation inthe middle east, 1979-97.
Journal of Conflict Resolution,45(5), 594?620.Grenager, T. and Manning, C. D. (2006).
Unsupervised dis-covery of a statistical verb lexicon.
In Proceedings of the2006 Conference on Empirical Methods in Natural Lan-guage Processing, page 18.Griffiths, T. L. and Steyvers, M. (2004).
Finding scientifictopics.
PNAS, 101(suppl.
1), 5228?5235.Harrison, J. and West, M. (1997).
Bayesian forecasting anddynamic models.
Springer Verlag, New York.Hobbs, J. R., Appelt, D., Bear, J., Israel, D., Kameyama,M., Stickel, M., and Tyson, M. (1997).
FASTUS: Acascaded finite-state transducer for extracting informationfrom natural-language text.
Finite-State Language Pro-cessing, page 383.Hoff, P. D. (2003).
Nonparametric modeling of hierarchi-cally exchangeable data.
University of Washington Statis-tics Department, Technical Report, 421.Holmes, C. C. and Held, L. (2006).
Bayesian auxiliaryvariable models for binary and multinomial regression.Bayesian Analysis, 1(1), 145?168.Jones, D., Bremer, S., and Singer, J.
(1996).
Militarized in-terstate disputes, 1816?1992: Rationale, coding rules, andempirical patterns.
Conflict Management and Peace Sci-ence, 15(2), 163?213.King, G. and Lowe, W. (2003).
An automated informationextraction tool for international conflict data with perfor-mance as good as human coders: A rare events evaluationdesign.
International Organization, 57(3), 617?642.Lang, J. and Lapata, M. (2010).
Unsupervised induction ofsemantic roles.
In Human Language Technologies: The2010 Annual Conference of the North American Chapterof the Association for Computational Linguistics, pages939?947.
Association for Computational Linguistics.Lehnert, W. G. (1994).
Cognition, computers, and car bombs:How Yale prepared me for the 1990s.
In Beliefs, Reason-ing, and Decision-Making.
Psycho-Logic in Honor of BobAbelson, pages 143?173, Hillsdale, NJ, Hove, UK.
Erl-baum.
http://ciir.cs.umass.edu/pubfiles/cognition3.pdf.Li, H., Li, X., Ji, H., and Marton, Y.
(2010).
Domain-independent novel event discovery and semi-automatic1103event annotation.
In Proceedings of the 24th Pacific AsiaConference on Language, Information and Computation,Sendai, Japan, November.Martin, A. D. and Quinn, K. M. (2002).
Dynamic ideal pointestimation via Markov chain Monte Carlo for the U.S.Supreme Court, 1953?1999.
Political Analysis, 10(2),134?153.McClelland, C. (1970).
Some effects on theory from the in-ternational event analysis movement.
Mimeo, Universityof Southern California.Mimno, D., Wallach, H., and McCallum, A.
(2008).
Gibbssampling for logistic normal topic models with graph-based priors.
In NIPS Workshop on Analyzing Graphs.Modi, A., Titov, I., and Klementiev, A.
(2012).
Unsuper-vised induction of frame-semantic representations.
In Pro-ceedings of the NAACL-HLT Workshop on the Induction ofLinguistic Structure, pages 1?7.
Association for Computa-tional Linguistics.Murphy, K. P. (2012).
Machine Learning: a ProbabilisticPerspective.
MIT Press.Neal, R. M. (2003).
Slice sampling.
Annals of Statistics,pages 705?741.Newman, D., Chemudugunta, C., and Smyth, P. (2006).
Sta-tistical entity-topic models.
In Proceedings of the 12thACM SIGKDD international conference on Knowledgediscovery and data mining, pages 680?686.
ACM.O?
Se?aghdha, D. (2010).
Latent variable models of selectionalpreference.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics, pages435?444.
Association for Computational Linguistics.O?Brien, S. P. (2010).
Crisis early warning and decision sup-port: Contemporary approaches and thoughts on future re-search.
International Studies Review, 12(1), 87?104.Parker, R., Graff, D., Kong, J., Chen, K., and Maeda, K.(2009).
English Gigaword Fourth Edition.
Linguistic DataConsortium.
LDC2009T13.Piskorski, J. and Atkinson, M. (2011).
Frontex real-timenews event extraction framework.
In Proceedings of the17th ACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 749?752.
ACM.Piskorski, J., Tanev, H., Atkinson, M., van der Goot, E., andZavarella, V. (2011).
Online news event extraction forglobal crisis surveillance.
Transactions on computationalcollective intelligence V , pages 182?212.Polson, N. G., Scott, J. G., and Windle, J.
(2012).
Bayesianinference for logistic models using Polya-Gamma latentvariables.
arXiv preprint arXiv:1205.0310.Poon, H. and Domingos, P. (2009).
Unsupervised semanticparsing.
In Proceedings of EMNLP, pages 1?10.
Associa-tion for Computational Linguistics.Quinn, K. M., Monroe, B. L., Colaresi, M., Crespin, M. H.,and Radev, D. R. (2010).
How to analyze political atten-tion with minimal assumptions and costs.
American Jour-nal of Political Science, 54(1), 209228.Rajaraman, A. and Ullman, J. D. (2011).
Mining of mas-sive datasets.
Cambridge University Press; http://infolab.stanford.edu/?ullman/mmds.html.Ramshaw, L., Boschee, E., Freedman, M., MacBride, J.,Weischedel, R., , and Zamanian, A.
(2011).
SERIF lan-guage processing effective trainable language understand-ing.
Handbook of Natural Language Processing and Ma-chine Translation, pages 636?644.Regneri, M., Koller, A., and Pinkal, M. (2010).
Learningscript knowledge with web experiments.
In Proceedingsof the 48th Annual Meeting of the Association for Compu-tational Linguistics, ACL ?10, pages 979?988.Rooth, M., Riezler, S., Prescher, D., Carroll, G., and Beil,F.
(1999).
Inducing a semantically annotated lexicon viaEM-based clustering.
In Proceedings of the 37th annualmeeting of the Association for Computational Linguisticson Computational Linguistics, page 104111.Rummel, R. (1968).
The Dimensionality of Nations project.Rzhetsky, A., Iossifov, I., Koike, T., Krauthammer, M., Kra,P., Morris, M., Yu, H., Duboue?, P. A., Weng, W., Wilbur,W.
J., Hatzivassiloglou, V., and Friedman, C. (2004).GeneWays: a system for extracting, analyzing, visualiz-ing, and integrating molecular pathway data.
Journal ofBiomedical Informatics, 37(1), 43?53.Sandhaus, E. (2008).
The New York Times Annotated Cor-pus.
Linguistic Data Consortium.
LDC2008T19.Sanfilippo, A., Franklin, L., Tratz, S., Danielson, G., Mile-son, N., Riensche, R., and McGrath, L. (2008).
Automat-ing frame analysis.
Social computing, behavioral model-ing, and prediction, pages 239?248.Schrodt, P. (2012).
Precedents, progress, and prospects in po-litical event data.
International Interactions, 38(4), 546?569.Schrodt, P. and Leetaru, K. (2013).
GDELT: Global dataon events, location and tone, 1979-2012.
In InternationalStudies Association Conference.Schrodt, P. A.
(2001).
Automated coding of internationalevent data using sparse parsing techniques.
InternationalStudies Association Conference.Schrodt, P. A.
(2006).
Twenty Years of the Kansas EventData System Project.
Political Methodologist.Schrodt, P. A. and Gerner, D. J.
(1994).
Validity assessmentof a machine-coded event data set for the Middle East,1982-1992.
American Journal of Political Science.Schrodt, P. A. and Gerner, D. J.
(2004).
An event data analy-sis of third-party mediation in the middle east and balkans.Journal of Conflict Resolution, 48(3), 310?330.Shellman, S. M. (2004).
Time series intervals and statisticalinference: The effects of temporal aggregation on eventdata analysis.
Political Analysis, 12(1), 97?104.Titov, I. and Klementiev, A.
(2011).
A Bayesian model forunsupervised semantic parsing.
In Proceedings of ACL.Titov, I. and Klementiev, A.
(2012).
A Bayesian approachto unsupervised semantic role induction.
Proceedings ofEACL.Wallach, H., Mimno, D., and McCallum, A.
(2009).
Rethink-ing lda: Why priors matter.
Advances in Neural Informa-tion Processing Systems, 22, 1973?1981.Yao, L., Haghighi, A., Riedel, S., and McCallum, A.
(2011).Structured relation discovery using generative models.
InProceedings of the Conference on Empirical Methods inNatural Language Processing, pages 1456?1466.
Associ-ation for Computational Linguistics.Zeger, S. L. and Karim, M. R. (1991).
Generalized linearmodels with random effects; a Gibbs sampling approach.Journal of the American Statistical Association, 86(413),79?86.1104
