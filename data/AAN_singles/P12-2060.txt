Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 306?310,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsImproving the IBM Alignment Models Using Variational BayesDarcey Riley and Daniel GildeaComputer Science Dept.University of RochesterRochester, NY 14627AbstractBayesian approaches have been shown to re-duce the amount of overfitting that occurswhen running the EM algorithm, by placingprior probabilities on the model parameters.We apply one such Bayesian technique, vari-ational Bayes, to the IBM models of wordalignment for statistical machine translation.We show that using variational Bayes im-proves the performance of the widely usedGIZA++ software, as well as improving theoverall performance of the Moses machinetranslation system in terms of BLEU score.1 IntroductionThe IBM Models of word alignment (Brown etal., 1993), along with the Hidden Markov Model(HMM) (Vogel et al, 1996), serve as the startingpoint for most current state-of-the-art machine trans-lation systems, both phrase-based and syntax-based(Koehn et al, 2007; Chiang, 2005; Galley et al,2004).Both the IBM Models and the HMM aretrained using the EM algorithm (Dempster et al,1977).
Recently, Bayesian techniques have becomewidespread in applications of EM to natural lan-guage processing tasks, as a very general method ofcontrolling overfitting.
For instance, Johnson (2007)showed the benefits of such techniques when ap-plied to HMMs for unsupervised part of speech tag-ging.
In machine translation, Blunsom et al (2008)and DeNero et al (2008) use Bayesian techniques tolearn bilingual phrase pairs.
In this setting, which in-volves finding a segmentation of the input sentencesinto phrasal units, it is particularly important to con-trol the tendency of EM to choose longer phrases,which explain the training data well but are unlikelyto generalize.However, most state-of-the-art machine transla-tion systems today are built on the basis of word-level alignments of the type generated by GIZA++from the IBM Models and the HMM.
Overfitting isalso a problem in this context, and improving theseword alignment systems could be of broad utility inmachine translation research.Moore (2004) discusses details of how EM over-fits the data when training IBM Model 1.
He dis-covers that the EM algorithm is particularly suscep-tible to overfitting in the case of rare words, due tothe ?garbage collection?
phenomenon.
Suppose asentence contains an English word e1 that occursnowhere else in the data, and its French transla-tion f1.
Suppose that same sentence also contains aword e2 which occurs frequently in the overall databut whose translation in this sentence, f2, co-occurswith it infrequently.
If the translation t(f2|e2) oc-curs with probability 0.1, then the sentence will havea higher probability if EM assigns the rare word andits actual translation a probability of t(f1|e1) = 0.5,and assigns the rare word?s translation to f2 a prob-ability of t(f2|e1) = 0.5, than if it assigns a proba-bility of 1 to the correct translation t(f1|e1).
Mooresuggests a number of solutions to this issue, includ-ing add-n smoothing and initializing the probabili-ties based on a heuristic rather than choosing uni-form probabilities.
When combined, his solutionscause a significant decrease in alignment error rate(AER).
More recently, Mermer and Saraclar (2011)have added a Bayesian prior to IBM Model 1 us-ing Gibbs sampling for inference, showing improve-ments in BLEU scores.In this paper, we describe the results of incorpo-306rating variational Bayes (VB) into the widely usedGIZA++ software for word alignment.
We use VBboth because it converges more quickly than Gibbssampling, and because it can be applied in a fairlystraightforward manner to all of the models imple-mented by GIZA++.
In Section 2, we describe VBin more detail.
In Section 3, we present results forVB for the various models, in terms of perplexity ofheld-out test data, alignment error rate (AER), andthe BLEU scores which result from using our ver-sion of GIZA++ in the end-to-end phrase-based ma-chine translation system Moses.2 Variational Bayes and GIZA++Beal (2003) gives a detailed derivation of a varia-tional Bayesian algorithm for HMMs.
The result isa very slight change to the M step of the originalEM algorithm.
During the M step of the original al-gorithm, the expected counts collected in the E stepare normalized to give the new values of the param-eters:?xi|y =E[c(xi|y)]?j E[c(xj |y)](1)The variational Bayesian M step performs an inexactnormalization, where the resulting parameters willadd up to less than one.
It does this by passingthe expected counts collected in the E step throughthe function f(v) = exp(?
(v)), where ?
is thedigamma function, and ?
is the hyperparameter ofthe Dirichlet prior (Johnson, 2007):?xi|y =f(E[c(xi|y)] + ?
)f(?j(E[c(xj |y)] + ?
))(2)This modified M step can be applied to any modelwhich uses a multinomial distribution; for this rea-son, it works for the IBM Models as well as HMMs,and is thus what we use for GIZA++.In practice, the digamma function has the effectof subtracting 0.5 from its argument.
When ?
isset to a low value, this results in ?anti-smoothing?.For the translation probabilities, because about 0.5is subtracted from the expected counts, small countscorresponding to rare co-occurrences of words willbe penalized heavily, while larger counts will not beaffected very much.
Thus, low values of ?
causethe algorithm to favor words which co-occur fre-quently and to distrust words that co-occur rarely.Sentence pair counte2 9f3e2 2f2e1 e2 1f1 f2Table 1: An example of data with rare words.In this way, VB controls the overfitting that wouldotherwise occur with rare words.
On the other hand,higher values of ?
can be chosen if smoothing is de-sired, for instance in the case of the alignment prob-abilities, which state how likely a word in position iof the English sentence is to align to a word in po-sition j of the French sentence.
For these probabili-ties, smoothing is important because we do not wantto rule out any alignment altogether, no matter howinfrequently it occurs in the data.We implemented VB for the translation probabil-ities as well as for the position alignment probabili-ties of IBM Model 2.
We discovered that adding VBfor the translation probabilities improved the perfor-mance of the system.
However, including VB forthe alignment probabilities had relatively little ef-fect, because the alignment table in its original formdoes some smoothing during normalization by inter-polating the counts with a uniform distribution.
Be-cause VB can itself be a form of smoothing, the twoversions of the code behave similarly.
We did notexperiment with VB for the distortion probabilitiesof the HMM or Models 3 and 4, as these distribu-tions have fewer parameters and are likely to havereliable counts during EM.
Thus, in Section 3, wepresent the results of using VB for the translationprobabilities only.3 ResultsFirst, we ran our modified version of GIZA++ on asimple test case designed to be similar to the exam-ple from Moore (2004) discussed in Section 1.
Ourtest case, shown in Table 1, had three different sen-tence pairs; we included nine instances of the first,two instances of the second, and one of the third.Human intuition tells us that f2 should translate toe2 and f1 should translate to e1.
However, the EMalgorithm without VB prefers e1 as the translation3070.100.150.200.250.300.350.400.450.501e-101e-081e-060.00010.011AERAlphaAER afterEntireTrainingFrench(baseline)French(variational Bayes)Chinese(baseline)Chinese(variationalBayes)German (baseline)German (variational Bayes)Figure 1: Determining the best value of ?
for the transla-tion probabilities.
Training data is 10,000 sentence pairsfrom each language pair.
VB is used for Model 1 only.This table shows the AER for different values of ?
af-ter training is complete (five iterations each of Models 1,HMM, 3, and 4).of f2, due to the ?garbage collection?
phenomenondescribed above.
The EM algorithm with VB doesnot overfit this data and prefers e2 as f2?s translation.For our experiments with bilingual data, we usedthree language pairs: French and English, Chi-nese and English, and German and English.
Weused Canadian Hansard data for French-English,Europarl data for German-English, and newswiredata for Chinese-English.
For measuring align-ment error rate, we used 447 French-English sen-tences provided by Hermann Ney and Franz Ochcontaining both sure and possible alignments, whilefor German-English we used 220 sentences pro-vided by Chris Callison-Burch with sure alignmentsonly, and for Chinese-English we used the first 400sentences of the data provided by Yang Liu, alsowith sure alignments only.
For computing BLEUscores, we used single reference datasets for French-English and German-English, and four referencesfor Chinese-English.
For minimum error rate train-ing, we used 1000 sentences for French-English,2000 sentences for German-English, and 1274 sen-tences for Chinese-English.
Our test sets con-tained 1000 sentences each for French-English andGerman-English, and 686 sentences for Chinese-English.
For scoring the Viterbi alignments of eachsystem against gold-standard annotated alignments,4006008001000120014001600  510152025Test PerplexityIterationsof Model 1Model1 Susceptibility to OverfittingFrench(baseline)French(variational Bayes)Figure 2: Effect of variational Bayes on overfitting forModel 1.
Training data is 10,000 sentence pairs.
Thistable contrasts the test perplexities of Model 1 with vari-ational Bayes and Model 1 without variational Bayes af-ter different numbers of training iterations.
VariationalBayes successfully controls overfitting.we use the alignment error rate (AER) of Och andNey (2000), which measures agreement at the levelof pairs of words.We ran our code on ten thousand sentence pairsto determine the best value of ?
for the transla-tion probabilities t(f |e).
For our training, we ranGIZA++ for five iterations each of Model 1, theHMM, Model 3, and Model 4.
Variational Bayeswas only used for Model 1.
Figure 1 shows how VB,and different values of ?
in particular, affect the per-formance of GIZA++ in terms of AER.
We discoverthat, after all training is complete, VB improves theperformance of the overall system, lowering AER(Figure 1) for all three language pairs.
We find thatlow values of ?
cause the most consistent improve-ments, and so we use ?
= 0 for the translation prob-abilities in the remaining experiments.
Note that,while a value of ?
= 0 does not define a proba-bilistically valid Dirichlet prior, it does not cause anypractical problems in the update equation for VB.Figure 2 shows the test perplexity after GIZA++has been run for twenty-five iterations of Model 1:without VB, the test perplexity increases as trainingcontinues, but it remains stable when VB is used.Thus, VB eliminates the need for the early stoppingthat is often employed with GIZA++.After choosing 0 as the best value of ?
for the3080.050.100.150.200.250.300.350.400.450.501000030000500007000090000AERCorpus SizeAER for Different CorpusSizesFrench(baseline)French(variational Bayes)Chinese(baseline)Chinese(variationalBayes)German (baseline)German (variational Bayes)Figure 3: Performance of GIZA++ on different amountsof test data.
Variational Bayes is used for Model 1 only.Table shows AER after all the training has completed(five iterations each of Models 1, HMM, 3, and 4).AERFrench Chinese GermanBaseline 0.14 0.42 0.43M1 Only 0.12 0.39 0.41HMM Only 0.14 0.42 0.42M3 Only 0.14 0.42 0.43M4 Only 0.14 0.42 0.43All Models 0.19 0.44 0.45Table 2: Effect of Adding Variational Bayes to SpecificModelstranslation probabilities, we reran the test above(five iterations each of Models 1, HMM, 3, and4, with VB turned on for Model 1) on differentamounts of data.
We found that the results for largerdata sizes were comparable to the results for tenthousand sentence pairs, both with and without VB(Figure 3).We then tested whether VB should be used for thelater models.
In all of these experiments, we ranModels 1, HMM, 3, and 4 for five iterations each,training on the same ten thousand sentence pairs thatwe used in the previous experiments.
In Table 2, weshow the performance of the system when no VB isused, when it is used for each of the four models in-dividually, and when it is used for all four modelssimultaneously.
We saw the most overall improve-ment when VB was used only for Model 1; using VBfor all four models simultaneously caused the mostimprovement to the test perplexity, but at the cost ofBLEU ScoreFrench Chinese GermanBaseline 26.34 21.03 21.14M1 Only 26.54 21.58 21.73All Models 26.46 22.08 21.96Table 3: BLEU Scoresthe AER.For the MT experiments, we ran GIZA++ throughMoses, training Model 1, the HMM, and Model 4 on100,0 0 sentence pairs from each language pair.
Weran three experiments, one with VB turned on for allmodels, one with VB turned on for Model 1 only,and one (the baseline) with VB turned off for allmodels.
When VB was turned on, we ran GIZA++for five iterations per model as in our earlier tests,but when VB was turned off, we ran GIZA++ foronly four iterations per model, having determinedthat this was the optimal number of iterations forbaseline system.
VB was used for the translationprobabilities only, with ?
set to 0.As can be seen in Table 3, using VB increasesthe BLEU score for all three language pairs.
ForFrench, the best results were achieved when VB wasused for Model 1 only; for Chinese and German, onthe other hand, using VB for all models caused themost improvements.
For French, the BLEU scoreincreased by 0.20; for German, it increased by 0.82;for Chinese, it increased by 1.05.
Overall, VB seemsto have the greatest impact on the language pairs thatare most difficult to align and translate to begin with.4 ConclusionWe find that applying variational Bayes with aDirichlet prior to the translation models imple-mented in GIZA++ improves alignments, both interms of AER and the BLEU score of an end-to-endtranslation system.
Variational Bayes is especiallybeneficial for IBM Model 1, because its lack of fer-tility and position information makes it particularlysusceptible to the garbage collection phenomenon.Applying VB to Model 1 alone tends to improvethe performance of later models in the training se-quence.
Model 1 is an essential stepping stone inavoiding local minima when training the followingmodels, and improvements to Model 1 lead to im-provements in the end-to-end system.309ReferencesMatthew J. Beal.
2003.
Variational Algorithms for Ap-proximate Bayesian Inference.
Ph.D. thesis, Univer-sity College London.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.Bayesian synchronous grammar induction.
In NeuralInformation Processing Systems (NIPS).Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL-05, pages 263?270, Ann Arbor, MI.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society,39(1):1?21.John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.2008.
Sampling alignment structure under a Bayesiantranslation model.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 314?323, Honolulu, Hawaii, October.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of NAACL-04, pages 273?280, Boston.Mark Johnson.
2007.
Why doesn?t EM find goodHMM POS-taggers?
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 296?305,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of ACL, Demonstration Session, pages 177?180.Coskun Mermer and Murat Saraclar.
2011.
Bayesianword alignment for statistical machine translation.
InProceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics (ACL-11), pages182?187.Robert C. Moore.
2004.
Improving IBM word alignmentModel 1.
In Proceedings of the 42nd Meeting of theAssociation for Computational Linguistics (ACL?04),Main Volume, pages 518?525, Barcelona, Spain, July.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of ACL-00, pages 440?447, Hong Kong, October.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In COLING-96, pages 836?841.310
