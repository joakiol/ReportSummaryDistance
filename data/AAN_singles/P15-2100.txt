Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 604?608,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Computational Approach to Automatic Prediction of Drunk-TextingAditya Joshi1,2,3Abhijit Mishra1Balamurali AR4Pushpak Bhattacharyya1Mark James Carman21IIT Bombay, India,2Monash University, Australia3IITB-Monash Research Academy, India4Aix-Marseille University, France{adityaj, abhijitmishra, pb}@cse.iitb.ac.inbalamurali.ar@lif.univ-mrs.fr,mark.carman@monash.eduAbstractAlcohol abuse may lead to unsociablebehavior such as crime, drunk driving,or privacy leaks.
We introduce auto-matic drunk-texting prediction as the taskof identifying whether a text was writ-ten when under the influence of alcohol.We experiment with tweets labeled usinghashtags as distant supervision.
Our clas-sifiers use a set of N-gram and stylistic fea-tures to detect drunk tweets.
Our observa-tions present the first quantitative evidencethat text contains signals that can be ex-ploited to detect drunk-texting.1 IntroductionThe ubiquity of communication devices has madesocial media highly accessible.
The content onthese media reflects a user?s day-to-day activities.This includes content created under the influenceof alcohol.
In popular culture, this has been re-ferred to as ?drunk-texting?1.
In this paper, we in-troduce automatic ?drunk-texting prediction?
as acomputational task.
Given a tweet, the goal is toautomatically identify if it was written by a drunkuser.
We refer to tweets written under the influ-ence of alcohol as ?drunk tweets?, and the oppositeas ?sober tweets?.A key challenge is to obtain an annotateddataset.
We use hashtag-based supervision so thatthe authors of the tweets mention if they weredrunk at the time of posting a tweet.
We createthree datasets by using different strategies that arerelated to the use of hashtags.
We then presentSVM-based classifiers that use N-gram and stylis-tic features such as capitalisation, spelling errors,etc.
Through our experiments, we make subtlepoints related to: (a) the performance of our fea-tures, (b) how our approach compares against1Source: http://www.urbandictionary.comhuman ability to detect drunk-texting, (c) mostdiscriminative stylistic features, and (d) an erroranalysis that points to future work.
To the best ofour knowledge, this is a first study that shows thefeasibility of text-based analysis for drunk-textingprediction.2 MotivationPast studies show the relation between alcoholabuse and unsociable behaviour such as aggres-sion (Bushman and Cooper, 1990), crime (Carpen-ter, 2007), suicide attempts (Merrill et al., 1992),drunk driving (Loomis and West, 1958), and riskysexual behaviour (Bryan et al., 2005).
Merrill etal.
(1992) state that ?those responsible for assess-ing cases of attempted suicide should be adept atdetecting alcohol misuse?.
Thus, a drunk-textingprediction system can be used to identify individ-uals susceptible to these behaviours, or for inves-tigative purposes after an incident.Drunk-texting may also cause regret.
MailGoggles2prompts a user to solve math questionsbefore sending an email on weekend evenings.Some Android applications3avoid drunk-textingby blocking outgoing texts at the click of a button.However, to the best of our knowledge, these toolsrequire a user command to begin blocking.
An on-going text-based analysis will be more helpful, es-pecially since it offers a more natural setting bymonitoring stream of social media text and not ex-plicitly seeking user input.
Thus, automatic drunk-texting prediction will improve systems aimed toavoid regrettable drunk-texting.
To the best ofour knowledge, ours is the first study that does aquantitative analysis, in terms of prediction of thedrunk state by using textual clues.Several studies have studied linguistic traitsassociated with emotion expression and mental2http://gmailblog.blogspot.in/2008/10/new-in-labs-stop-sending-mail-you-later.html3https://play.google.com/store/apps/details?id=com.oopsapp604health issues, suicidal nature, criminal status, etc.
(Pennebaker, 1993; Pennebaker, 1997).
NLP tech-niques have been used in the past to address so-cial safety and mental health issues (Resnik et al.,2013).3 Definition and ChallengesDrunk-texting prediction is the task of classifyinga text as drunk or sober.
For example, a tweet?Feeling buzzed.
Can?t remember how the eveningwent?
must be predicted as ?drunk?, whereas, ?Re-turned from work late today, the traffic was bad?must be predicted as ?sober?.
The challenges are:1.
More than topic categorisation: Drunk-texting prediction is similar to topic cate-gorisation (that is, classification of docu-ments into a set of categories such as ?news?,?sports?, etc.).
However, Borrill et al.
(1987)show that alcohol abusers have more pro-nounced emotions, specifically, anger.
In thisrespect, drunk-texting prediction lies at theconfluence of topic categorisation and emo-tion classification.2.
Identification of labeled examples: It is dif-ficult to obtain a set of sober tweets.
Theideal label can be possibly given only by theauthor.
For example, whether a tweet suchas ?I am feeling lonely tonight?
is a drunktweet is ambiguous.
This is similar to sar-casm expressed as an exaggeration (for ex-ample, ?This is the best film ever!
), where thecontext beyond the text needs to be consid-ered.3.
Precision/Recall trade-off: The goal that adrunk-texting prediction system must chasedepends on the application.
An applicationthat identifies potential crimes must workwith high precision, since the target popula-tion to be monitored will be large.
On theother hand, when being used to avoid regret-table drunk-texting, a prediction system mustproduce high recall in order to ensure that adrunk message does not pass through.4 Dataset CreationWe use hashtag-based supervision to create ourdatasets, similar to tasks like emotion classifica-tion (Purver and Battersby, 2012).
The tweets aredownloaded using Twitter API (https://dev.twitter.com/).
We remove non-Unicodecharacters, and eliminate tweets that contain hy-perlinks4and also tweets that are shorter than 6words in length.
Finally, hashtags used to indi-cate drunk or sober tweets are removed so thatthey provide labels, but do not act as features.
Thedataset is available on request.
As a result, we cre-ate three datasets, each using a different strategyfor sober tweets, as follows:Figure 1: Word cloud for drunk tweets1.
Dataset 1 (2435 drunk, 762 sober): We col-lect tweets that are marked as drunk andsober, using hashtags.
Tweets containinghashtags #drunk, #drank and #imdrunk areconsidered to be drunk tweets, while thosewith #notdrunk, #imnotdrunk and #sober areconsidered to be sober tweets.2.
Dataset 2 (2435 drunk, 5644 sober): Thedrunk tweets are downloaded using drunkhashtags, as above.
The list of users who cre-ated these tweets is extracted.
For the nega-tive class, we download tweets by these users,which do not contain the hashtags that corre-spond to drunk tweets.3.
Dataset H (193 drunk, 317 sober): A sepa-rate dataset is created where drunk tweets aredownloaded using drunk hashtags, as above.The set of sober tweets is collected using boththe approaches above.
The resultant is theheld-out test set Dataset-H that contains notweets in common with Datasets 1 and 2.The drunk tweets for Datasets 1 and 2 arethe same.
Figure 1 shows a word-cloud forthese drunk tweets (with stop words and formsof the word ?drunk?
removed), created using4This is a rigid criterion, but we observe that tweets withhyperlinks are likely to be promotional in nature.605Feature DescriptionN-gram FeaturesUnigram & Bigram (Presence) Boolean features indicating unigrams and bigramsUnigram & Bigram (Count) Real-valued features indicating unigrams and bigramsStylistic FeaturesLDA unigrams (Presence/Count) Boolean & real-valued features indicating unigrams from LDAPOS Ratio Ratios of nouns, adjectives, adverbs in the tweet#Named Entity Mentions Number of named entity mentions#Discourse Connectors Number of discourse connectorsSpelling errors Boolean feature indicating presence of spelling mistakesRepeated characters Boolean feature indicating whether a character is repeated threetimes consecutivelyCapitalisation Number of capital letters in the tweetLength Number of wordsEmoticon (Presence/Count) Boolean & real-valued features indicating unigramsSentiment Ratio Positive and negative word ratiosTable 1: Our Feature Set for Drunk-texting PredictionWordItOut5.
The size of a word indicates its fre-quency.
In addition to topical words such as ?bar?,?bottle?
and ?wine?, the word-cloud shows senti-ment words such as ?love?
or ?damn?, along withprofane words.Heuristics other than these hashtags could havebeen used for dataset creation.
For example,timestamps were a good option to account for timeat which a tweet was posted.
However, this couldnot be used because user?s local times was notavailable, since very few users had geolocation en-abled.5 Feature DesignThe complete set of features is shown in Table 1.There are two sets of features: (a) N-gram fea-tures, and (b) Stylistic features.
We use unigramsand bigrams as N-gram features- considering bothpresence and count.Table 1 shows the complete set of stylistic fea-tures of our prediction system.
POS ratios are a setof features that record the proportion of each POStag in the dataset (for example, the proportion ofnouns/adjectives, etc.).
The POS tags and namedentity mentions are obtained from NLTK (Bird,2006).
Discourse connectors are identified basedon a manually created list.
Spelling errors areidentified using a spell checker by Aby (2014).The repeated characters feature captures a situ-ation in which a word contains a letter that isrepeated three or more times, as in the case of5www.worditout.comhapppy.
Since drunk-texting is often associatedwith emotional expression, we also incorporate aset of sentiment-based features.
These features in-clude: count/presence of emoticons and sentimentratio.
Sentiment ratio is the proportion of posi-tive and negative words in the tweet.
To deter-mine positive and negative words, we use the sen-timent lexicon in Wilson et al.
(2005).
To identifya more refined set of words that correspond to thetwo classes, we also estimated 20 topics for thedataset by estimating an LDA model (Blei et al.,2003).
We then consider top 10 words per topic,for both classes.
This results in 400 LDA-specificunigrams that are then used as features.A(%)NP(%)PP(%)NR(%)PR(%)Dataset 1N-gram 85.5 72.8 88.8 63.4 92.5Stylistic 75.6 32.5 76.2 3.2 98.6All 85.4 71.9 89.1 64.6 91.9Dataset 2N-gram 77.9 82.3 65.5 87.2 56.5Stylistic 70.3 70.8 56.7 97.9 6.01All 78.1 82.6 65.3 86.9 57.5Table 2: Performance of our features on Datasets1 and 26066 EvaluationUsing the two sets of features, we train SVM clas-sifiers (Chang and Lin, 2011)6.
We show thefive-fold cross-validation performance of our fea-tures on Datasets 1 and 2, in Section 6.1, and onDataset H in Section 6.2.
Section 6.3 presents anerror analysis.
Accuracy, positive/negative preci-sion and positive/negative recall are shown as A,PP/NP and PR/NR respectively.
?Drunk?
formsthe positive class, while ?Sober?
forms the nega-tive class.Top features# Dataset 1 Dataset 21 POS NOUN Spelling error2 Capitalization LDA drinking3 Spelling error POS NOUN4 POS PREPOSITION Length5 Length LDA tonight6 LDA Llife Sentiment Ratio7 POS VERB Char repeat8 LDA today LDA today9 POS ADV LDA drunken10 Sentiment Ratio LDA lmaoTable 3: Top stylistic features for Datasets 1 and 2obtained using Chi-squared test-based ranking6.1 Performance for Datasets 1 and 2Table 2 shows the performance for five-fold cross-validation for Datasets 1 and 2.
In case of Dataset1, we observe that N-gram features achieve an ac-curacy of 85.5%.
We see that our stylistic featuresalone exhibit degraded performance, with an ac-curacy of 75.6%, in the case of Dataset 1.
Ta-ble 3 shows top stylistic features, when trainedon the two datasets.
Spelling errors, POS ratiosfor nouns (POS NOUN)7, length and sentimentratios appear in both lists, in addition to LDA-based unigrams.
However, negative recall reducesto a mere 3.2%.
This degradation implies thatour features capture a subset of drunk tweets andthat there are properties of drunk tweets that maybe more subtle.
When both N-gram and stylis-tic features are used, there is negligible improve-ment.
The accuracy for Dataset 2 increases from6We also repeated all experiments for Na?
?ve Bayes.
Theydo not perform as well as SVM, and have poor recall.7POS ratios for nouns, adjectives and adverbs were nearlysimilar in drunk and sober tweets - with the maximum differ-ence being 0.03%77.9% to 78.1%.
Precision/Recall metrics do notchange significantly either.
The best accuracy ofour classifier is 78.1% for all features, and 75.6%for stylistic features.
This shows that text-basedclues can indeed be used for drunk-texting predic-tion.A1 A2 A3A1 - 0.42 0.36A2 0.42 - 0.30A3 0.36 0.30 -Table 4: Cohen?s Kappa for three annotators (A1-A3)A(%)NP(%)PP(%)NR(%)PR(%)Annotators 68.8 71.7 61.7 83.9 43.5TrainingDatasetOur classifiersDataset 1 47.3 70 40 26 81Dataset 2 64 70 53 72 50Table 5: Performance of human evaluators and ourclassifiers (trained on all features), for Dataset-Has the test set6.2 Performance for Held-out Dataset HUsing held-out dataset H, we evaluate how oursystem performs in comparison to humans.
Threeannotators, A1-A3, mark each tweet in the DatasetH as drunk or sober.
Table 4 shows a moderateagreement between our annotators (for example,it is 0.42 for A1 and A2).
Table 5 compares ourclassifier with humans.
Our human annotators per-form the task with an average accuracy of 68.8%,while our classifier (with all features) trained onDataset 2 reaches 64%.
The classifier trained onDataset 2 is better than which is trained on Dataset1.6.3 Error AnalysisSome categories of errors that occur are:1.
Incorrect hashtag supervision: The tweet?Can?t believe I lost my bag last night, lit-erally had everything in!
Thanks god thebar man found it?
was marked with?#Drunk?.However, this tweet is not likely to be a drunktweet, but describes a drunk episode in retro-spective.
Our classifier predicts it as sober.6072.
Seemingly sober tweets: Human annotatorsas well as our classifier could not identifywhether ?Will you take her on a date?
Butreally she does like you?
was drunk, althoughthe author of the tweet had marked it so.This example also highlights the difficulty ofdrunk-texting prediction.3.
Pragmatic difficulty: The tweet ?Nationaldress of Ireland is one?s one vomit.. my fam-ily is lovely?
was correctly identified by ourhuman annotators as a drunk tweet.
Thistweet contains an element of humour andtopic change, but our classifier could not cap-ture it.7 Conclusion & Future WorkIn this paper, we introduce automatic drunk-texting prediction as the task of predicting a tweetas drunk or sober.
First, we justify the need fordrunk-texting prediction as means of identifyingrisky social behavior arising out of alcohol abuse,and the need to build tools that avoid privacy leaksdue to drunk-texting.
We then highlight the chal-lenges of drunk-texting prediction: one of thechallenges is selection of negative examples (sobertweets).
Using hashtag-based supervision, we cre-ate three datasets annotated with drunk or soberlabels.
We then present SVM-based classifierswhich use two sets of features: N-gram and stylis-tic features.
Our drunk prediction system obtainsa best accuracy of 78.1%.
We observe that ourstylistic features add negligible value to N-gramfeatures.
We use our heldout dataset to comparehow our system performs against human annota-tors.
While human annotators achieve an accuracyof 68.8%, our system reaches reasonably close andperforms with a best accuracy of 64%.Our analysis of the task and experimental find-ings make a case for drunk-texting prediction as auseful and feasible NLP application.ReferencesAby.
2014.
Aby word processing website, January.Steven Bird.
2006.
Nltk: the natural language toolkit.In Proceedings of the COLING/ACL on Interactivepresentation sessions, pages 69?72.
Association forComputational Linguistics.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet allocation.
the Journal of ma-chine Learning research, 3:993?1022.Josephine A Borrill, Bernard K Rosen, and Angela BSummerfield.
1987.
The influence of alcohol onjudgement of facial expressions of emotion.
BritishJournal of Medical Psychology.Angela Bryan, Courtney A Rocheleau, Reuben N Rob-bins, and Kent E Hutchinson.
2005.
Condom useamong high-risk adolescents: testing the influenceof alcohol use on the relationship of cognitive corre-lates of behavior.
Health Psychology, 24(2):133.Brad J Bushman and Harris M Cooper.
1990.
Effectsof alcohol on human aggression: An intergrative re-search review.
Psychological bulletin, 107(3):341.Christopher Carpenter.
2007.
Heavy alcohol use andcrime: Evidence from underage drunk-driving laws.Journal of Law and Economics, 50(3):539?557.Chih-Chung Chang and Chih-Jen Lin.
2011.
Lib-svm: a library for support vector machines.
ACMTransactions on Intelligent Systems and Technology(TIST), 2(3):27.Ted A Loomis and TC West.
1958.
The influence of al-cohol on automobile driving ability: An experimen-tal study for the evaluation of certain medicologi-cal aspects.
Quarterly journal of studies on alcohol,19(1):30?46.John Merrill, GABRIELLE MILKER, John Owens,and Allister Vale.
1992.
Alcohol and attempted sui-cide.
British journal of addiction, 87(1):83?89.James W Pennebaker.
1993.
Putting stress into words:Health, linguistic, and therapeutic implications.
Be-haviour research and therapy, 31(6):539?548.James W Pennebaker.
1997.
Writing about emotionalexperiences as a therapeutic process.
Psychologicalscience, 8(3):162?166.Matthew Purver and Stuart Battersby.
2012.
Experi-menting with distant supervision for emotion classi-fication.
In Proceedings of the 13th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 482?491.
Associationfor Computational Linguistics.Philip Resnik, Anderson Garron, and Rebecca Resnik.2013.
Using topic modeling to improve predictionof neuroticism and depression.
In Proceedings ofthe 2013 Conference on Empirical Methods in Nat-ural, pages 1348?1353.
Association for Computa-tional Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the con-ference on human language technology and empiri-cal methods in natural language processing, pages347?354.
Association for Computational Linguis-tics.608
