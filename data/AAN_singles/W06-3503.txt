Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 17?24,New York City, June 2006. c?2006 Association for Computational LinguisticsUnderstanding Complex Natural Language Explanations in TutorialApplications?Pamela W. Jordan, Maxim Makatchev and Umarani PappuswamyLearning Research and Development CenterUniversity of PittsburghPittsburgh PA, 15260{pjordan,maxim,umarani}@pitt.eduAbstractWe describe the WHY2-ATLAS intelligenttutoring system for qualitative physics thatinteracts with students via natural lan-guage dialogue.
We focus on the is-sue of analyzing and responding to multi-sentential explanations.
We explore an ap-proach that combines a statistical classi-fier, multiple semantic parsers and a for-mal reasoner for achieving a deeper under-standing of these explanations in order toprovide appropriate feedback on them.1 IntroductionMost natural language tutorial applications have fo-cused on coaching either problem solving or proce-dural knowledge (e.g.
Steve (Johnson and Rickel,1997), Circsim-tutor (Evens and Michael, 2006),Atlas (Rose?
et al, 2001), BEETLE (Zinn et al,2002), SCoT (Peters et al, 2004), inter alia).
Whencoaching problem solving, simple short answer anal-ysis techniques are frequently sufficient because theprimary goal is to lead a trainee step-by-step throughproblem solving.
There is a narrow range of possi-ble responses and the context of the previous dia-logue and questions invite short answers.
But whenthe instructional objectives shift and a tutorial sys-tem attempts to explore a student?s chain of reason-ing behind an answer or decision, deeper analysistechniques can begin to pay off.
Having the student?This research was supported by ONR Grant No.
N00014-00-1-0600 and by NSF Grant No.
9720359.construct more on his own is important for learningperhaps in part because it reveals what the studentdoes and does not understand (Chi et al, 2001).When the student is invited to provide a longerchain of reasoning, the explanations become multi-sentential.
Compare the short explanation in Fig-ure 1 to the longer ones in Figures 2 and 3.
The ex-planation in Figure 2 is part of an actual initial stu-dent response and Figure 3 shows the explanationfrom the same student after a follow-up dialoguewith the WHY2-ATLAS tutoring system.WHY2-ATLAS: Fine.
Using this principle, what is the valueof the horizontal component of the acceleration of the egg?Please explain your reasoning.Student: zero because there is no horizontal force acting onthe egg [3 propositions expressed]Figure 1: Eliciting a one sentence explanation froma student.WHY2-ATLAS: Suppose a man is in an elevator that isfalling without anything touching it (ignore the air, too).
Heholds his keys motionless right in front of his face and thenjust releases his grip on them.
What will happen to them?Explain.Student: [omitted 15 correct propositions]...
Yet the gravita-tional pull on the man and the elevator is greater because theyare of a greater weight and therefore they will fall faster thenthe keys.
I believe that the keys will float up to the cieling asthe elevator continues falling.Figure 2: An initial elicitation of a multi-sentenceexplanation from a student.The only previous tutoring system that has at-tempted to address longer explanations is AUTOTU-TOR (Graesser et al, 2004).
It uses a latent semantic17[omitted 16 correct propositions]...
Since <Net force= mass * acceleration> and <F= mass*g> therefore<mass*acceleration= mass*g> and acceleration and grav-itational force end up being equal.
So mass does not ef-fect anything in this problem and the acceleration of both thekeys and the man are the same.
[omitted 46 correct proposi-tions]...we can say that the keys will remain right in front ofthe man?s face.Figure 3: A subsequent response from the same stu-dent in Figure 2 after some interaction with WHY2-ATLAS.analysis (LSA) approach where the structure of sen-tences is not considered.
Thus the degree to whichdetails of the explanation are understood is limited.As can be seen from the examples, a student?s ex-planation about a formal domain such as qualitativephysics may involve a number of phenomena: al-gebraic formulas, NL renderings of formulas, vari-ous degrees of formality, and conveying the logicalstructure of an argument (Makatchev et al, 2005).Tutoring goals involve eliciting correct statementsof the appropriate degree of formality and their jus-tifications to address possible gaps and errors in theexplanation.
To achieve these goals the NL under-standing is required to answer the following ques-tions:?
Does the student explanation contain errors?
Ifyes, what are the likely buggy assumptions thathave led the student to these errors??
What required statements have not been cov-ered by the student?
Does the explanation con-tain statements that are logically close to therequired statements?These requirements imply that a logical structureneeds to be imposed on the space of possible do-main statements.
Considering such a structure tobe a model of the student?s reasoning about the do-main, the two requirements correspond to a solutionof a model-based diagnosis problem (Forbus and deKleer, 1993).How does one build such a model?
A desire tomake the process scalable and feasible necessitatesan automated procedure.
The difficulty is that thisautomated reasoner has to deal with the NL phe-nomena that are relevant for our application.
In turn,this means that the knowledge representation (KR)would have to be able to express these phenomena(e.g.
NL renderings of formulas, various degrees offormality).
The reasoner has to account for commonreasoning fallacies, have flexible consistency con-straints and perform within the tight requirements ofa real-time dialogue application.In this paper, we present a hybrid of symbolicand statistical approaches that attempts to robustlyprovide a model-based diagnosis of a student?s ex-planation.
In the next section, we provide a briefsketch of the KR used in WHY2-ATLAS.
Section 3describes our hybrid approach for analyzing studentexplanations while section 4 covers our most recentevaluations of the system and its explanation analy-sis components.
Section 5 presents our conclusionsalong with future directions.2 Knowledge representationWe selected an order-sorted first-order predicatelogic (FOPL) as a base KR for our domain sinceit is expressive enough to reflect the hierarchy ofconcepts from the qualitative mechanics ontology(Ploetzner and VanLehn, 1997) and has a straight-forward proof theory (Walther, 1987).
Follow-ing the representation used in the abductive rea-soner Tacitus-lite (Thomason et al, 1996), our KRis function-free, does not have quantifiers, Skolemconstants or explicit negation.
Instead all variablesin facts or goals are assumed to be existentiallyquantified, and all variables in rules are either uni-versally quantified (if they appear in premises) or ex-istentially quantified (if they appear in conclusionsonly).Although our KR has no explicit negation, sometypes of negative statements are represented by us-ing (a) complimentary sorts, for example constantand nonconstant; (b) the value nonequal as a fillerof the respective argument of comparison predicates.Instead of parsing arbitrary algebraic expressions,an equation identifier module attempts shallow pars-ing of equation candidates and maps them into a fi-nite set of anticipated equation labels (Makatchev etal., 2005).NL understanding needs to distinguish formalversus informal physics expressions so that the tu-toring system can coach on proper use of terminol-ogy.
Many qualitative mechanics phenomena may18be described informally, for example ?speed up?
in-stead of ?accelerate?
and ?push?
instead of ?applya force.?
The relevant informal expressions fall intothe following categories:?
relative position: ?keys are behind (in front of,above, under, close, far from, etc.)
man??
motion: ?move slower,?
?slow down,?
?movesalong a straight line??
dependency: ?horizontal speed will not dependon the force??
direction: ?the force is downward??
interaction: ?the man pushes the keys,?
?thegravity pulls the keys?Each of these categories (except for the last one)has a dedicated representation.
While represent-ing push and pull expressions via a dedicated predi-cate seems straightforward, we are still assessing theutility of distinguishing ?man pushes the keys?
and?man applies a force on the keys?
for our tutoringapplication and currently represent both expressionsas a nonzero force applied by the man to the keys.One of the tutoring objectives of WHY2-ATLASis to encourage students to provide argumentativesupport for their conclusions.
This requires recog-nizing and representing the justification-conclusionclauses in student explanations.
Recognizing suchclauses is a challenging NLP problem due to the is-sue of quantifier and causality scoping.
It is also dif-ficult to achieve a compromise between two compet-ing requirements for a suitable representation.
First,the KR should be flexible enough to account for avariable number of justifications.
Second, reasoningwith the KR should be computationally feasible.
Weleave representing the logical structure of explana-tions for future work.3 Analyzing Student ExplanationsWhen analyzing a student explanation, first an equa-tion identifier tags any physics equations in the stu-dent?s response and then the explanation is classifiedto complete the assessment.
Explanation classifica-tion is done by using either (a) a statistical classi-fier that maps the explanation directly into a set ofknown facts, principles and misconceptions, or (b)two competing semantic parsers that each generatean FOPL representation that is then matched againstknown facts, principles or misconceptions, as wellas against pre-computed correct and buggy chainsof reasoning.
We present the approaches at a high-level in order to focus on how the approaches workwhen combined and our evaluation results.3.1 Statistical classifierRAINBOW is a tool for developing bag of words(BOW) text classifiers (McCallum and Nigam,1998).
The classes of interest must first be identifiedand then a text corpus annotated for example sen-tences for each class.
From this training data a bagof words representation is derived for each class anda number of algorithms can be tried for measuringsimilarity of a new input segment?s BOW represen-tation to each class.For WHY2-ATLAS, the classes are a subset ofnodes in the correct and buggy chains of reason-ing.
Limiting the number of classes allows us toalleviate the problem of sparseness of training data,but the side-effect is that there are many misclassi-fications of sentences due to overlap in the classes;that is, words that discriminate between classes areshared by many other classes (Pappuswamy et al,2005).
We alleviate this problem some by aggre-gating classes and building three tiers of BOW textclassifiers that use a kNN measure.
By doing so, weobtain a 13% improvement in classification accuracyover a single classifier approach (Pappuswamy et al,2005).
The upper two tiers of classification describethe topic of discussion and the lower tier describesthe specific principle or misconception related to thetopic and subtopic.
The first tier classifier identifieswhich second tier classifier to use and so on.
Thethird tier then identifies which node (if any) in thechain of reasoning a sentence expresses.But because the number of classes is limited,BOW has problems dealing with many of the NLphenomena we described earlier.
For example, al-though it can deal with some informal language use(i.e.
?push the container?
maps to ?apply force onthe container?
), it cannot provide accurate syntactic-semantic mappings between informal and formallanguage on the fly.
This is because the informallanguage use is so varied that it is difficult to cap-ture representative training data in sufficient quanti-ties.
Hence, a large portion of student statements ei-ther cannot be classified with high confidence or are19erroneously classified.
We use a post-classificationheuristic to try to filter out the latter cases.
The filter-ing heuristic depends on the system?s representationlanguage and not on the classification technique.Given a classification of which node in the chainof reasoning the sentence represents, the heuris-tic estimates whether the node?s FOPL representa-tion either over- or under-represents the sentence bymatching the root forms of the words in the naturallanguage sentence to the constants in the system?srepresentation language.For those statements BOW cannot classify or thatthe heuristic filters out, we attempt classification us-ing an FOPL representation derived from semanticparsing, as described in the next two subsections.3.2 Converting NL to FOPLTwo competing methods of sentence analysis eachgenerate a FOPL candidate.
The two candidatesare then passed to a heuristic selection process thatchooses the best one (Jordan et al, 2004).
The ra-tionale for using competing approaches is that thetechniques available vary considerably in accuracy,processing time and whether they tend to be brittleand produce no analysis vs. a partial one.
Thereis also a trade-off between these performance mea-sures and the amount of domain specific setup re-quired for each technique.The first method, CARMEL, provides combinedsyntactic and semantic analysis using the LCFlexsyntactic parser along with semantic constructorfunctions (Rose?, 2000).
Given a specification ofthe desired representation language, it then maps theanalysis to this language.
Then discourse level pro-cessing attempts to resolve nominal and temporalanaphora and ellipsis to produce the candidate FOPLrepresentation for a sentence (Jordan and VanLehn,2002).The second method, RAPPEL, uses MINIPAR (Linand Pantel, 2001) to parse the sentence.
It then ex-tracts syntactic dependency features from the parseto use in mapping the sentence to its FOPL repre-sentation (Jordan et al, 2004).
Each predicate inthe KR language is assigned a predicate templateand a separate classifier is trained for each predicatetemplate.
For example, there is a classifier that spe-cializes in predicate instantiations (atoms) involvingthe velocity predicate and another for instantiationsof the acceleration predicate.
Classes for each tem-plate represent combinations of constants that canfill a predicate template?s slots to cover all possibleinstantiations of that predicate.
Each predicate tem-plate classifier returns either a nil which indicatesthat there is no instantiation involving that predicateor a class label that corresponds to an instantiationof that predicate.
The candidate FOPL representa-tion for a statement is the union of the output of allthe predicate template classifiers.Finally, either the CARMEL or RAPPEL candidateFOPL output is selected using the same heuristic asfor the BOW filtering.
The surviving FOPL repre-sentation is then assessed for correctness and com-pleteness, as described next.3.3 Analyzing correctness and completenessAs the final step in analyzing a student?s explana-tion, an assessment of correctness and complete-ness is performed by matching the FOPL represen-tations of the student?s response to nodes of an aug-mented assumption-based truth maintenance system(ATMS) (Makatchev and VanLehn, 2005).An ATMS for each physics problem is generatedoff-line.
The ATMS compactly represents the de-ductive closure of a problem?s givens with respectto a set of both good and buggy physics rules.
Thatis, each node in the ATMS corresponds to a propo-sition that follows from a problem statement.
Eachanticipated student misconception is treated as an as-sumption (in the ATMS sense), and all conclusionsthat follow from it are tagged with a label that in-cludes it as well as any other assumptions neededto derive that conclusion.
This labeling allows theATMS to represent many interwoven deductive clo-sures, each depending on different misconceptions,without inconsistency.
The labels allow recovery ofhow a conclusion was reached.
Thus a match witha node containing a buggy assumption indicates thestudent has a common error or misconception andwhich error or misconception it is.The completeness of an explanation is relative toa two-column proof generated by a domain expert.A human creates the proof that is used for check-ing completeness since it is probably less work fora person to write an acceptable proof than to findone in the ATMS.
Part of the proof for the prob-lem in Figure 2 is shown in Figure 4 where facts20Step Fact Justification1 The only force on the keys and the man is the force ofgravityForces are either contact forces or the gravitational force... ... ...12 The keys and the man have the same displacements at alltimes<Average velocity = displacement / elapsed time>, so if av-erage velocity and time are the same, so is displacement.13 The keys and the man have the same initial vertical po-sitiongiven14 The keys and the man have the same vertical position atall times<Displacement = difference in position>, so if the initialpositions of two objects are the same and their displacementsare the same, then so is their final position15 The keys stay in front of the man?s face at all timesFigure 4: Part of the proof used in WHY2-ATLAS for the Elevator problem in Figure 2.appear in the left column and justifications that arephysics principles appear in the right column.
Justi-fications are further categorized as vector equations(e.g.
<Average velocity = displacement / elapsedtime>, in step (12) of the proof), or qualitative rules(e.g.
?so if average velocity and time are the same,so is displacement?
in step (12)).
A two-columnproof is represented in the system as a directed graphin which nodes are facts, vector equations, or qual-itative rules that have been translated to the FOPLrepresentation language off-line.
The edges of thegraph represent the inference relations between thepremise and conclusion of modus ponens.Matches of an FOPL input against the ATMS andthe two-column proof (we collectively referred tothese earlier as the correct and buggy chains of rea-soning) do not have to be exact.
In addition, fur-ther flexibility in the matching process is providedby examining a neighborhood of radius N (in termsof graph distance) from matched nodes in the ATMSto determine whether it contains any of the nodes ofthe two-column proof.
This provides an estimate ofthe proximity of a student?s utterance to the facts thatare of interest.Although matching against the ATMS deductiveclosure has been implemented, the current version ofthe system does not yet fully utilize this capability.Instead, the correctness and completeness of expla-nations is evaluated by flexibly matching the FOPLinput against targeted relevant facts, principles andmisconceptions in the chains of reasoning, using aradius of 0.
This kind of matching is referred to asdirect matching in Section 4.2.4 EvaluationsWHY2-ATLAS, as we?ve just described it, has beenfully implemented and was evaluated in the contextof testing the hypothesis that even when content isequivalent, students who engage in more interac-tive forms of instruction learn more.
To test thishypothesis we compared students who received hu-man tutoring with students who read a short text.WHY2-ATLAS and WHY2-AUTOTUTOR provided athird type of condition that served as an interactiveform of instruction where the content is better con-trolled than with human tutoring in that only somesubset of the content covered in the text conditioncan be presented.
In all conditions the students hadto solve four problems that require multi-sententialexplanations, one of which is shown in Figure 2.In earlier evaluations, we found that overall stu-dents learn and learn equally well in all three typesof conditions when the content is appropriate to thelevel of the student (VanLehn et al, 2005), i.e.
thelearning gains for human tutoring and the contentcontrolled text were the same.
For the latest eval-uation of WHY2-ATLAS, which excluded a humantutoring condition, the learning gains on multiple-choice and essay post-tests were the same as forthe other conditions.
However, on fill-in-the-blankpost-tests, the WHY2-ATLAS students scored higherthan the text students (p=0.010; F(1,74)=6.33), andthis advantage persisted when the scores were ad-justed by factoring out pre-test scores in an AN-COVA (p=0.018; F(1,72)=5.83).
Although this dif-ference was in the expected direction, it was not ac-companied by similar differences for the other twopost-tests.These learning measures show that, relative to the21text, the two systems?
overall performance at se-lecting content is good.
A system could performworse than the text condition if it too frequentlymisinterprets multi-sentential answers and skips ma-terial covered in the text that a student may need.But since the dialogue strategies in the two systemsare different and selected relative to the understand-ing techniques used, we next need to do a detailedcorpus analysis of the language data collected totrack successes and failures of understanding and di-alogue strategy selection relative to knowledge com-ponents in the post-test.
Next we will describe somecomponent-level evaluations that focus on the partsof the system we just described.4.1 Evaluating the Benefit of Combining SingleSentence ApproachesThis first component-level evaluation focuses on thebenefits of heuristically choosing between the re-sults of BOW, CARMEL and RAPPEL.
This partic-ular evaluation used a prior version of the systemwhich used BOW without tiers and hand-craftedpattern-matching rules instead of the ATMS ap-proach to assessment.
But this evaluation still re-flects the potential benefits of combining single sen-tence approaches.We used a test suite of 35 held-out multi-sentencestudent explanations (235 sentences total) that areannotated for the elicitation topics that are to be dis-cussed with the student.
We computed recall (R),precision (P) and false alarm rate (FAR) against thefull corpus instead of averaging these measures foreach explanation.
Since F-measure does not allowerror skewing as can be done with ROC areas (Flach,2003) we instead look for cases of high recall with alow false alarm rate.The top part of Table 1 compares the baseline oftutoring all possible topics and the individual perfor-mances of the three approaches when each is usedin isolation from the others.
We see that only thestatistical approach lowers the false alarm rate butdoes so by sacrificing recall.
The rest are not signif-icantly different from tutoring all topics.
The poorperformances of CARMEL and RAPPEL is not totallyunexpected because there are three potential failurepoints for these classification approaches; the syn-tactic analysis, the semantic mapping and the hand-crafted pattern matching rules for assessing correct-ness and completeness.
While the syntactic anal-ysis results for both approaches are good, the se-mantic mapping and assessment of correctness andcompleteness are still big challenges.
The results ofBOW, while better than that of the other two ap-proaches, are clearly not good enough.Table 1: Performance of NL to FOPL for actionstaken in WHY2-ATLAS system.Approach R P FARtutor all topics 1.0 .61 1.0CARMEL 1.0 .61 1.0BOW without tiers .60 .93 .07RAPPEL .94 .59 1.0satisficing heuristic .67 .80 .26highest ranked heuristic .73 .76 .36The bottom part of Table 1, shows the results ofcombining the approaches and choosing one outputheuristically.
The satisficing1 version of the heuris-tic checks each output in the order 1) CARMEL 2)BOW 3) RAPPEL, and stops with the first repre-sentation that is acceptable according to the filteringheuristic.
This heuristic selection process modestlyimproves recall but at the sacrifice of a higher falsealarm rate.
The highest ranking heuristic scores eachoutput and selects the best one.
It provides the mostbalanced results of the combined or individual ap-proaches.
It provides the largest increase in recalland the false alarm rate is still modest compared tothe baseline of tutoring all possible topics.
It is clear,that a combined approach has a positive impact.4.2 Completeness and Correctness EvaluationThe component-level evaluation for completenessand correctness was completed after the studentlearning evaluation.
It focuses on the performanceof just the direct matching procedure.
Figure 5shows the results of classifying 62 student utterancesfor one physics problem with respect to 46 storedstatement representations using only direct match-ing.
To generate these results, the data is manuallydivided into 7 groups based on the quality of the NL1According to Newell & Simon (1972), satisficing is theprocess by which an individual sets an acceptable level as thefinal criterion and simply takes the first acceptable move insteadof seeking an optimal one.221 2 3 4 5 6 70102030405060708090100RecallPrecisionBaseline1 recallBaseline1 precisionBaseline2 recallBaseline2 precisionSize of the datasetQuality of representation%Figure 5: Average recall and precision of utteranceclassification.
The size of a group of entries is shownrelative to the size of the overall data set.
Averageprocessing time is 0.011 seconds per entry on a 1.8GHz Pentium 4 machine with 2Gb of RAM.to FOPL conversion, such that group 7 consists onlyof perfectly formalized entries, and for 1 ?
n ?
6group n includes entries of group n+1 and addition-ally entries of somewhat lesser representation qual-ity, so that group 1 includes all the entries of thedata set.
The flexibility of the direct matching al-gorithm even allows classification of utterances thathave mediocre representations, resulting in 70% av-erage recall and 82.9% average precision for 56.5%of all entries (group 4).
However, large numbersof inadequately represented utterances (38.7% of allentries did not make it into group 3 of the data set)result in 53.2% average recall and 59.7% averageprecision for the whole data set (group 1).
Theseresults are still significantly better compared to thetwo baseline classifiers the best of which peaks at22.2% average recall and precision.
The first base-line classifier always assigns the single label that isdominant in the training set (average number of la-bels per entry of the training set is 1.36).
The sec-ond baseline classifier independently and randomlypicks labels according to their distributions in thetraining set.
The most frequent label in the trainingset corresponds to the answer to the problem.
Sincein the test set the answer always appears as a sepa-rate utterance (sentence), recall and precision ratesfor the first baseline classifier are the same.Although the current evaluation did not involvematching against the ATMS, we did evaluate thetime required for such a match in order to make arough comparison with our earlier approach.
Match-ing a 12 atom input representation against a 128node ATMS that covers 55% of relevant problemfacts takes around 30 seconds, which is a consid-erable improvement over the 170 seconds requiredfor the on-the-fly analysis performed by the Tacitus-lite+ abductive reasoner (Makatchev et al, 2004)?the technique used in the previous version of WHY2-ATLAS.
The matching is done by a version ofa largest common subgraph-based graph-matchingalgorithm (due to the need to account for cross-referencing atoms via shared variables) proposedin (Shearer et al, 2001), that has a time complex-ity O(2nn3), where n is the size of an input graph.The efficiency can be further improved by using anapproximation of the largest common subgraph inorder to evaluate the match.5 ConclusionIn this paper, we discussed an application that in-tegrates a hybrid of semantic parsers and a sym-bolic reasoner with a statistical classifier to analyzestudent explanations.
We attempted to address theproblem that the leap made by statistical classifiersfrom NL to a feasible classification is too big sincetoo many details of what was actually said by thestudent are lost.
On the other hand, we showedthat the hybrid semantic parsers allow for a slightlysmaller leap by mapping to a symbolic representa-tion that is sufficient for domain reasoning.
Usingdeductive closure of problem givens and buggy as-sumptions, the correctness and completeness ana-lyzer allows us to reason about the correctness ofstudent statements that cannot be confidently clas-sified statistically.
Although formal and informallanguage expressions have unique underlying se-mantics, we attempt to paraphrase informal NL intoformal NL by using the forward-chaining rules in-volved in creating the deductive closure for a prob-lem from its givens.
Our current symbolic represen-tation is still too coarse to distinguish some fine nu-ances allowed by the domain of mechanics.
We con-jecture that extending our knowledge representationwith more language-specific predicates would allowus to represent more fine-grained differences in stu-dent statements while still allowing feasible reason-ing with the ATMS.23ReferencesMichelene T. H. Chi, Stephanie A. Siler, Heisawn Jeong,Takashi Yamauchi, and Robert G. Hausmann.
2001.Learning from human tutoring.
Cognitive Science,25(4):471?533.M.
Evens and J. Michael.
2006.
One-on-One Tutoringby Humans and Computers.
Lawrence Erlbaum Asso-ciates, Inc.P.
Flach.
2003.
The geometry of ROC space: Under-standing machine learning metrics through ROC iso-metrics.
In Proceedings of 20th International Confer-ence on Machine Learning.Kenneth D. Forbus and Johan de Kleer.
1993.
Build-ing Problem Solvers.
MIT Press, Cambridge, Mas-sachusetts; London, England.A.C.
Graesser, S. Lu, G.T.
Jackson, H. Mitchell, M. Ven-tura, A. Olney, and M.M.
Louwerse.
2004.
Autotu-tor: A tutor with dialogue in natural language.
Behav-ioral Research Methods, Instruments, and Computers,36:180?193.W.
Lewis Johnson and Jeff Rickel.
1997.
Steve: Ananimated pedagogical agent for procedural training invirtual environments.
SIGART Bulletin, pages 16?21,Fall.Pamela Jordan and Kurt VanLehn.
2002.
Discourse pro-cessing for explanatory essays in tutorial applications.In Proceedings of the 3rd SIGdial Workshop on Dis-course and Dialogue, July.Pamela W. Jordan, Maxim Makatchev, and Kurt Van-Lehn.
2004.
Combining competing language un-derstanding approaches in an intelligent tutoring sys-tem.
In Proceedings of the Intelligent Tutoring Sys-tems Conference.D.
Lin and P. Pantel.
2001.
Discovery of inference rulesfor question answering.
Journal of Natural LanguageEngineering, 7(4):343?360.Maxim Makatchev and Kurt VanLehn.
2005.
Analyzingcompleteness and correctness of utterances using anATMS.
In Proceedings of Int.
Conference on ArtificialIntelligence in Education, AIED2005.
IOS Press, July.Maxim Makatchev, Pamela W. Jordan, and Kurt Van-Lehn.
2004.
Abductive theorem proving for analyzingstudent explanations to guide feedback in intelligenttutoring systems.
Journal of Automated Reasoning,Special issue on Automated Reasoning and TheoremProving in Education, 32:187?226.Maxim Makatchev, Brian S. Hall, Pamela W. Jordan,Umarani Pappuswamy, and Kurt VanLehn.
2005.Mixed language processing in the Why2-Atlas tu-toring system.
In Proceedings of the Workshop onMixed Language Explanations in Learning Environ-ments, AIED2005, pages 35?42, July.Andrew McCallum and Kamal Nigam.
1998.
A compar-ison of event models for naive bayes text classification.In Proceeding of AAAI/ICML-98 Workshop on Learn-ing for Text Categorization.
AAAI Press.A.
Newell and H.A.
Simon.
1972.
Human Problem Solv-ing.
Prentice-Hall, Englewood Cliffs, NJ.Umarani Pappuswamy, Dumisizwe Bhembe, Pamela W.Jordan, and Kurt VanLehn.
2005.
A multi-tier NL-knowledge clustering for classifying students?
essays.In Proceedings of 18th International FLAIRS Confer-ence.S.
Peters, E. Bratt, B. Clark, H. Pon-Barry, andK.
Schultz.
2004.
Intelligent systems for trainingdamage control assistants.
In In the Proceedings ofI/ITSEC 2004, Orlando, Florida.Rolf Ploetzner and Kurt VanLehn.
1997.
The acquisi-tion of qualitative physics knowledge during textbook-based physics training.
Cognition and Instruction,15(2):169?205.Carolyn Rose?, Pamela Jordan, Michael Ringenberg,Stephanie Siler, Kurt VanLehn, and Anders Weinstein.2001.
Interactive conceptual tutoring in atlas-andes.In Proceedings of AI in Education 2001 Conference.Carolyn P. Rose?.
2000.
A framework for robust seman-tic interpretation.
In Proceedings of the First Meetingof the North American Chapter of the Association forComputational Linguistics, pages 311?318.Kim Shearer, Horst Bunke, and Svetha Venkatesh.
2001.Video indexing and similarity retrieval by largest com-mon subgraph detection using decision trees.
PatternRecognition, 34(5):1075?1091.Richmond H. Thomason, Jerry Hobbs, and Johanna D.Moore.
1996.
Communicative goals.
In K. Jokinen,M.
Maybury, M. Zock, and I. Zukerman, editors, Pro-ceedings of the ECAI 96 Workshop Gaps and Bridges:New Directions in Planning and Natural LanguageGeneration.K.
VanLehn, A. Graesser, G. T. Jackson, P. Jordan, A. Ol-ney, and C. P. Rose?.
2005.
When is reading just as ef-fective as one-on-one interactive human tutoring?
InProceedings of CogSci2005.Christof Walther.
1987.
A many-sorted calculus basedon resolution and paramodulation.
Morgan Kauf-mann, Los Altos, California.Claus Zinn, Johanna D. Moore, and Mark G. Core.
2002.A 3-tier planning architecture for managing tutorial di-alogue.
In Proceedings of Intelligent Tutoring SystemsConference (ITS 2002), pages 574?584.24
