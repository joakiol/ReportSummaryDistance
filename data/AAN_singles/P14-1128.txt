Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1360?1369,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsToward Better Chinese Word Segmentation for SMT via BilingualConstraintsXiaodong Zeng?Lidia S. Chao?Derek F. Wong?Isabel Trancoso?Liang Tian?
?NLP2CT Lab / Department of Computer and Information Science, University of Macau?INESC-ID / Instituto Superior T?enico, Lisboa, Portugalnlp2ct.samuel@gmail.com, {lidiasc, derekfw}@umac.mo,isabel.trancoso@inesc-id.pt,tianliang0123@gmail.comAbstractThis study investigates on building abetter Chinese word segmentation mod-el for statistical machine translation.
Itaims at leveraging word boundary infor-mation, automatically learned by bilin-gual character-based alignments, to inducea preferable segmentation model.
Wepropose dealing with the induced wordboundaries as soft constraints to bias thecontinuous learning of a supervised CRF-s model, trained by the treebank data (la-beled), on the bilingual data (unlabeled).The induced word boundary informationis encoded as a graph propagation con-straint.
The constrained model inductionis accomplished by using posterior reg-ularization algorithm.
The experimentson a Chinese-to-English machine transla-tion task reveal that the proposed modelcan bring positive segmentation effects totranslation quality.1 IntroductionWord segmentation is regarded as a critical pro-cedure for high-level Chinese language process-ing tasks, since Chinese scripts are written in con-tinuous characters without explicit word bound-aries (e.g., space in English).
The empirical worksshow that word segmentation can be beneficial toChinese-to-English statistical machine translation(SMT) (Xu et al, 2005; Chang et al, 2008; Zhaoet al, 2013).
In fact most current SMT modelsassume that parallel bilingual sentences should besegmented into sequences of tokens that are meantto be ?words?
(Ma and Way, 2009).
The practicein state-of-the-art MT systems is that Chinese sen-tences are tokenized by a monolingual supervisedword segmentation model trained on the hand-annotated treebank data, e.g., Chinese treebank(CTB) (Xue et al, 2005).
These models are con-ducive to MT to some extent, since they common-ly have relatively good aggregate performance andsegmentation consistency (Chang et al, 2008).But one outstanding problem is that these mod-els may leave out some crucial segmentation fea-tures for SMT, since the output words conform tothe treebank segmentation standard designed formonolingually linguistic intuition, rather than spe-cific to the SMT task.In recent years, a number of works (Xu et al,2005; Chang et al, 2008; Ma and Way, 2009;Xi et al, 2012) attempted to build segmentationmodels for SMT based on bilingual unsegment-ed data, instead of monolingual segmented data.They proposed to learn gainful bilingual knowl-edge as golden-standard segmentation supervi-sions for training a bilingual unsupervised mod-el.
Frequently, the bilingual knowledge refers tothe mappings of an individual English word to oneor more consecutive Chinese characters, generat-ed via statistical character-based alignment.
Theyleverage such mappings to either constitute a Chi-nese word dictionary for maximum-matching seg-mentation (Xu et al, 2004), or form labeled datafor training a sequence labeling model (Paul et al,2011).
The prior works showed that these modelshelp to find some segmentations tailored for SMT,since the bilingual word occurrence feature can becaptured by the character-based alignment (Ochand Ney, 2003).
However, these models tend tomiss out other linguistic segmentation patterns asmonolingual supervised models, and suffer fromthe negative effects of erroneously alignments toword segmentation.This paper proposes an alternative ChineseWord Segmentation (CWS) model adapted to theSMT task, which seeks not only to maintain theadvantages of a monolingual supervised model,having hand-annotated linguistic knowledge, butalso to assimilate the relevant bilingual segmenta-1360tion nature.
We propose leveraging the bilingualknowledge to form learning constraints that guidea supervised segmentation model toward a bettersolution for SMT.
Besides the bilingual motivat-ed models, character-based alignment is also em-ployed to achieve the mappings of the successiveChinese characters and the target language word-s.
Instead of directly merging the characters in-to concrete segmentations, this work attempts toextract word boundary distributions for character-level trigrams (types) from the ?chars-to-word?mappings.
Furthermore, these word boundariesare encoded into a graph propagation (GP) expres-sion, in order to widen the influence of the inducedbilingual knowledge among Chinese texts.
The G-P expression constrains similar types having ap-proximated word boundary distributions.
Crucial-ly, the GP expression with the bilingual knowledgeis then used as side information to regularize aCRFs (conditional random fields) model?s learn-ing over treebank and bitext data, based on theposterior regularization (PR) framework (Ganchevet al, 2010).
This constrained learning amounts toa jointly coupling of GP and CRFs, i.e., integratingGP into the estimation of a parametric structuralmodel.This paper is structured as follows: Section 2points out the main differences with the relatedworks of this study.
Section 3 presents the de-tails of the proposed segmentation model.
Section4 reports the experimental results of the proposedmodel for a Chinese-to-English MT task.
The con-clusion is drawn in Section 5.2 Related WorkIn the literature, many approaches have been pro-posed to learn CWS models for SMT.
They canbe put into two categories, monolingual-motivatedand bilingual-motivated.
The former primarily op-timizes monolingual supervised models accordingto some predefined segmentation properties thatare manually summarized from empirical MT e-valuations.
Chang et al (2008) enhanced a CRF-s segmentation model in MT tasks by tuning theword granularity and improving the segmentationconsistence.
Zhang et al (2008) produced a bet-ter segmentation model for SMT by concatenat-ing various corpora regardless of their differen-t specifications.
Distinct from their behaviors,this work uses automatically learned constraintsinstead of manually defined ones.
Most impor-tantly, the constraints have a better learning guid-ance since they originate from the bilingual texts.On the other hand, the bilingual-motivated CWSmodels typically rely on character-based align-ments to generate segmentation supervisions.
Xuet al (2004) proposed to employ ?chars-to-word?alignments to generate a word dictionary for max-imum matching segmentation in SMT task.
Theworks in (Ma and Way, 2009; Zhao et al, 2013)extended the dictionary extraction strategy.
Maand Way (2009) adopted co-occurrence frequencymetric to iteratively optimize ?candidate words?extract from the alignments.
Zhao et al (2013) at-tempted to find an optimal subset of the dictionarylearned by the character-based alignment to maxi-mize the MT performance.
Paul et al (2011) usedthe words learned from ?chars-to-word?
align-ments to train a maximum entropy segmentationmodel.
Rather than playing the ?hard?
uses ofthe bilingual segmentation knowledge, i.e., direct-ly merging ?char-to-word?
alignments to wordsas supervisions, this study extracts word bound-ary information of characters from the alignmentsas soft constraints to regularize a CRFs model?slearning.The graph propagation (GP) technique providesa natural way to represent data in a variety of tar-get domains (Belkin et al, 2006).
In this tech-nique, the constructed graph has vertices consist-ing of labeled and unlabeled examples.
Pairs ofvertices are connected by weighted edges encod-ing the degree to which they are expected to havethe same label (Zhu et al, 2003).
Many recentworks, such as by Subramanya et al (2010), Dasand Petrov (2011), Zeng et al (2013; 2014) andZhu et al (2014), proposed GP for inferring the la-bel information of unlabeled data, and then lever-age these GP outcomes to learn a semi-supervisedscalable model (e.g., CRFs).
These approaches arereferred to as pipelined learning with GP.
This s-tudy also works with a similarity graph, encodingthe learned bilingual knowledge.
But, unlike theprior pipelined approaches, this study performs ajoint learning behavior in which GP is used as alearning constraint to interact with the CRFs mod-el estimation.One of our main objectives is to bias CRF-s model?s learning on unlabeled data, under anon-linear GP constraint encoding the bilingualknowledge.
This is accomplished by the poste-rior regularization (PR) framework (Ganchev et1361al., 2010).
PR performs regularization on poste-riors, so that the learned model itself remains sim-ple and tractable, while during learning it is drivento obey the constraints through setting appropriateparameters.
The closest prior study is constrainedlearning, or learning with prior knowledge.
Changet al (2008) described constraint driven learning(CODL) that augments model learning on unla-beled data by adding a cost for violating expec-tations of constraint features designed by domainknowledge.
Mann and McCallum (2008) and M-cCallum et al (2007) proposed to employ gener-alized expectation criteria (GE) to specify prefer-ences about model expectations in the form of lin-ear constraints on some feature expectations.3 MethodologyThis work aims at building a CWS model adaptedto the SMT task.
The model induction is shown inAlgorithm 1.
The input data requires two type-s of training resources, segmented Chinese sen-tences from treebank Dcland parallel unsegment-ed sentences of Chinese and foreign language Dcuand Dfu.
The first step is to conduct character-based alignment over bitexts Dcuand Dfu, whereevery Chinese character is an alignment target.Here, we are interested on n-to-1 alignment pat-terns, i.e., one target word is aligned to one ormore source Chinese characters.
The second stepaims to collect word boundary distributions for al-l types, i.e., character-level trigrams, according tothe n-to-1 mappings (Section 3.1).
The third stepis to encode the induced word boundary informa-tion into a k-nearest-neighbors (k-NN) similaritygraph constructed over the entire set of types fromDcland Dcu(Section 3.2).
The final step trains adiscriminative sequential labeling model, condi-tional random fields, on Dcland Dcuunder bilin-gual constraints in a graph propagation expression(Section 3.3).
This constrained learning is carriedout based on posterior regularization (PR) frame-work (Ganchev et al, 2010).3.1 Word Boundaries Learned fromCharacter-based AlignmentsThe gainful supervisions toward a better segmen-tation solution for SMT are naturally extractedfrom MT training resources, i.e., bilingual paralleldata.
This study employs an approximated methodintroduced in (Xu et al, 2004; Ma and Way, 2009;Chung and Gildea, 2009) to learn bilingual seg-Algorithm 1 CWS model induction with bilingualconstraintsRequire:Segmented Chinese sentences from treebankDcl; Parallel sentences of Chinese and foreignlanguage Dcuand DfuEnsure:?
: the CRFs model parameters1: Dc?f?
char align bitext (Dcu,Dfu)2: r ?
learn word bound (Dc?f)3: G ?
encode graph constraint (Dcl,Dcu, r)4: ?
?
pr crf graph (Dcl,Dcu,G)mentation knowledge.
This relies on statisticalcharacter-based alignment: first, every Chinesecharacter in the bitexts is divided by a white s-pace so that individual characters are regarded asspecial ?words?
or alignment targets, and second,they are connected with English words by usinga statistical word aligner, e.g., GIZA++ (Och andNey, 2003).
Note that the aligner is restricted touse an n-to-1 alignment pattern.
The primary ideais that consecutive Chinese characters are groupedto a candidate word, if they are aligned to the sameforeign word.
It is worth mentioning that priorworks presented a straightforward usage for can-didate words, treating them as golden segmenta-tions, either dictionary units or labeled resources.But this study treats the induced candidate word-s in a different way.
We propose to extract theword boundary distributions1for character-leveltrigrams (type)2, as shown in Figure 1, instead ofthe very specific words.
There are two main rea-sons to do so.
First, it is a more general expressionwhich can reduce the impact amplification of er-roneous character alignments.
Second, boundarydistributions can play more flexible roles as con-straints over labelings to bias the model learning.The type-level word boundary extraction is for-mally described as follows.
Given the ith sen-tence pair ?xci, xfi,Ac?fi?
of the aligned bilin-gual corpus Dc?f, the Chinese sentence xcicon-sisting of m characters {xci,1, xci,2, ..., xci,m}, andthe foreign language sentence xfi, consisting of1The distribution is on four word boundary labels indi-cating the character positions in a word, i.e., B (begin), M(middle), E (end) and S (single character).2A word boundary distribution corresponds to the centercharacter of a type.
In fact, it aims at reducing label ambi-guities to collect boundary information of character trigrams,rather than individual characters (Altun et al, 2006).1362n words {xfi,1, xfi,2, ..., xfi,n}, Ac?firepresents aset of alignment pairs aj= ?Cj, xfi,j?
that de-fines connections between a few Chinese char-acters Cj= {xci,j1, xci,j2, ..., xci,jk} and a sin-gle foreign word xfi,j.
For an alignment aj=?Cj, xfi,j?, only the sequence of characters Cj={xci,j1, xci,j2, ..., xci,jk} ?d ?
[1, k?1], jd+1?
jd=1 constitutes a valid candidate word.
For the w-hole bilingual corpus, we assign each characterin the candidate words with a word boundary tagT ?
{B,M,E, S}, and then count across the en-tire corpus to collect the tag distributions ri={ri,t; t ?
T} for each type xci,j?1xci,jxci,j+1.????
?Beijing   OlympusCharacter-based alignment????
?BEBM   EBeijing   OlympusWordboundaries???
???
?
Type-level Wordboundary distributionsBeiPing Shi ??
?BeiJing Ren ???
BeiJingDi ??
?QuanYun Hui ??
?BeiJing Shi ??
?0.8 0.60.30.20.9AoYun Hui ???
0.2Figure 1: An example of similarity graph overcharacter-level trigrams (types).3.2 Constraints Encoded by GraphPropagation ExpressionThe previous step contributes to generate bilingualsegmentation supervisions, i.e., type-level wordboundary distributions.
An intuitive manner is todirectly leverage the induced boundary distribu-tions as label constraints to regularize segmenta-tion model learning, based on a constrained learn-ing algorithm.
This study, however, makes furtherefforts to elevate the positive effects of the bilin-gual knowledge via the graph propagation tech-nique.
We adopt a similarity graph to encodethe learned type-level word boundary distribution-s.
The GP expression will be defined as a PR con-straint in Section 3.3 that reflects the interactionsbetween the graph and the CRFs model.
In otherwords, GP is integrated with estimation of para-metric structural model.
This is greatly differentfrom the prior pipelined approaches (Subramanyaet al, 2010; Das and Petrov, 2011; Zeng et al,2013), where GP is run first and its propagatedoutcomes are then used to bias the structural mod-el.
This work seeks to capture the GP benefits dur-ing the modeling of sequential correlations.In what follows, the graph setting and propa-gation expression are introduced.
As in conven-tional GP examples (Das and Smith, 2012), a sim-ilarity graph G = (V,E) is constructed over Ntypes extracted from Chinese training data, includ-ing treebank Dcland bitexts Dcu.
Each vertex Vihas a |T |-dimensional estimated measure vi={vi,t; t ?
T} representing a probability distribu-tion on word boundary tags.
The induced type-level word boundary distributions ri= {ri,t; t ?T} are empirical measures for the correspondingM graph vertices.
The edges E ?
Vi?Vjconnectall the vertices.
Scores between pairs of graph ver-tices (types), wij, refer to the similarities of theirsyntactic environment, which are computed fol-lowing the method in (Subramanya et al, 2010;Das and Petrov, 2011; Zeng et al, 2013).
Thesimilarities are measured based on co-occurrencestatistics over a set of predefined features (intro-duced in Section 4.1).
Specifically, the point-wisemutual information (PMI) values, between ver-tices and each feature instantiation that they havein common, are summed to sparse vectors, andtheir cosine distances are computed as the sim-ilarities.
The nature of this similarity graph en-forces that the connected types with high weight-s appearing in different texts should have similarword boundary distributions.The quality (smoothness) of the similarity graphcan be estimated by using a standard propagationfunction, as shown in Equation 1.
The square-losscriterion (Zhu et al, 2003; Bengio et al, 2006) isused to formulate this function:P(v) =T?t=1(M?i=1(vi,t?
ri,t)2+?N?j=1N?i=1wij(vi,t?
vj,t)2+ ?N?i=1(vi,t)2)(1)The first term in this equation refers to seed match-es that compute the distances between the estimat-ed measure viand the empirical probabilities ri.The second term refers to edge smoothness thatmeasures how vertices viare smoothed with re-spect to the graph.
Two types connected by anedge with high weight should be assigned similarword boundary distributions.
The third term, a `2norm, evaluates the distribution sparsity (Das and1363Smith, 2012) per vertex.
Typically, the GP processamounts to an optimization process with respectto parameter v such that Equation 1 is minimized.This propagation function can be used to reflectthe graph smoothness, where the higher the score,the lower the smoothness.3.3 PR Learning with GP ConstraintOur learning problem belongs to semi-supervisedlearning (SSL), as the training is done on treebanklabeled data (XL,YL) = {(x1, y1), ..., (xl, yl)},and bilingual unlabeled data (XU) = {x1, ..., xu}where xi= {x1, ..., xm} is an input word se-quence and yi= {y1, ..., ym}, y ?
T is its corre-sponding label sequence.
Supervised linear-chainCRFs can be modeled in a standard conditionallog-likelihood objective with a Gaussian prior:L(?)
= p?(yi|xi)????22?
(2)The conditional probabilities p?are expressed as alog-linear form:p?
(yi|xi) =exp(m?k=1?Tf(yk?1i, yki, xi))Z?
(xi)(3)Where Z?
(xi) is a partition function that normal-izes the exponential form to be a probability dis-tribution, and f(yk?1i, yki, xi) are arbitrary featurefunctions.In our setting, the CRFs model is requiredto learn from unlabeled data.
This work em-ploys the posterior regularization (PR) frame-work3(Ganchev et al, 2010) to bias the CRFsmodel?s learning on unlabeled data, under a con-straint encoded by the graph propagation expres-sion.
It is expected that similar types in the graphshould have approximated expected taggings un-der the CRFs model.
We follow the approach in-troduced by (He et al, 2013) to set up a penalty-based PR objective with GP: the CRFs likelihoodis modified by adding a regularization term, asshown in Equation 4, representing the constraints:RU(?, q) = KL(q||p?)
+ ?P(v) (4)Rather than regularize CRFs model?s posteriorsp?
(Y|xi) directly, our model uses an auxiliarydistribution q(Y|xi) over the possible labelings3The readers are refered to the original paper of Ganchevet al (2010).Y for xi, and penalizes the CRFs marginal log-likelihood by a KL-divergence term4, represent-ing the distance between the estimated posteriorsp and the desired posteriors q, as well as a penal-ty term, formed by the GP function.
The hy-perparameter ?
is used to control the impacts ofthe penalty term.
Note that the penalty is firedif the graph score computed based on the expect-ed taggings given by the current CRFs model isincreased vis-a-vis the previous training iteration.This nature requires that the penalty term P(v)should be formed as a function of posteriors q overCRFs model predictions5, i.e., P(q).
To state this,a mappingM : ({1, ..., u}, {1, ...,m})?
V fromwords in the corpus to vertices in the graph is de-fined.
We can thus decompose vi,tinto a functionof q as follows:vi,t=u?a=1m?b=1;M(a,b)=ViT?c=1?y?Y1(yb= t, yb?1= c)q(y|xa)u?a=1m?b=11(M(a, b) = Vi)(5)The final learning objective combines the CRF-s likelihood with the PR regularization term:J (?, q) = L(?)
+ RU(?, q).
This joint objec-tive, over ?
and q, can be optimized by an expecta-tion maximization (EM) style algorithm as report-ed in (Ganchev et al, 2010).
We start from ini-tial parameters ?0, estimated by supervised CRFsmodel training on treebank data.
The E-step is tominimize RU(?, q) over the posteriors q that areconstrained to the probability simplex.
Since thepenalty term P(v) is a non-linear form, the opti-mization method in (Ganchev et al, 2010) via pro-jected gradient descent on the dual is inefficient6.This study follows the optimization method (He etal., 2013) that uses exponentiated gradient descent(EGD) algorithm.
It allows that the variable up-date expression, as shown in Equation 6, takes amultiplicative rather than an additive form.q(w+1)(y|xi) = q(w)(y|xi) exp(??
?R?q(w)(y|xi))(6)where the parameter ?
controls the optimizationrate in the E-step.
With the contributions from4The form of KL term: KL(q||p) =?q?Yq(y) logq(y)p(y).5The original PR setting also requires that the penalty ter-m should be a linear (Ganchev et al, 2010) or non-linear (Heet al, 2013) function on q.6According to (He et al, 2013), the dual of quadratic pro-gram implies an expensive matrix inverse.1364the E-step that further encourage q and p to agree,the M-step aims to optimize the objective J (?, q)with respect to ?.
The M-step is similar to the stan-dard CRFs parameter estimation, where the gradi-ent ascent approach still works.
This EM-style ap-proach monotonically increases J (?, q) and thusis guaranteed to converge to a local optimum.E-step: q(t+1)= argminqRU(?
(t), q(t))M-step: ?
(t+1)= argmax?L(?
)+?u?i=1?y?Yq(t+1)(y|xi) log p?
(y|xi)(7)4 Experiments4.1 Data and SetupThe experiments in this study evaluated the per-formances of various CWS models in a Chinese-to-English translation task.
The influence ofthe word segmentation on the final translationis our main investigation.
We adopted threestate-of-the-art metrics, BLEU (Papineni et al,2002), NIST (Doddington et al, 2000) and ME-TEOR (Banerjee and Lavie, 2005), to evaluate thetranslation quality.The monolingual segmented data, trainTB, isextracted from the Penn Chinese Treebank (CTB-7) (Xue et al, 2005), containing 51,447 sentences.The bilingual training data, trainMT, is formedby a large in-house Chinese-English parallel cor-pus (Tian et al, 2014).
There are in total 2,244,319Chinese-English sentence pairs crawled from on-line resources, concentrated in 5 different domainsincluding laws, novels, spoken, news and miscel-laneous7.
This in-house bilingual corpus is theMT training data as well.
The target-side lan-guage model is built on over 35 million mono-lingual English sentences, trainLM, crawled fromonline resources.
The NIST evaluation campaigndata, MT-03 and MT-05, are selected to comprisethe MT development data, devMT, and testing da-ta, testMT, respectively.For the settings of our model, we adopted thestandard feature templates introduced by Zhao etal.
(2006) for CRFs.
The character-based align-ment for achieving the ?chars-to-word?
mappingsis accomplished by GIZA++ aligner (Och andNey, 2003).
For the GP, a 10-NNs similarity graph7The in-house corpus has been manually validated, in along process that exceeded 500 hours.was constructed8.
Following (Subramanya et al,2010; Zeng et al, 2013), the features used tocompute similarities between vertices were (Sup-pose given a type ?w2w3w4?
surrounding contexts?w1w2w3w4w5?
): unigram (w3), bigram (w1w2,w4w5, w2w4), trigram (w2w3w4, w2w4w5,w1w2w4), trigram+context (w1w2w3w4w5) andcharacter classes in number, punctuation, alpha-betic letter and other (t(w2)t(w3)t(w4)).
Thereare four hyperparameters in our model to be tunedby using the development data (devMT) amongthe following settings: for the graph propagation,?
?
{0.2, 0.5, 0.8} and ?
?
{0.1, 0.3, 0.5, 0.8};for the PR learning, ?
?
{0 ?
?i?
1} and ?
?
{0 ?
?i?
1} where the step is 0.1.
The best per-formed joint settings, ?
= 0.5, ?
= 0.5, ?
= 0.9and ?
= 0.8, were used to measure the final per-formance.The MT experiment was conducted based ona standard log-linear phrase-based SMT model.The GIZA++ aligner was also adopted to obtainword alignments (Och and Ney, 2003) over thesegmented bitexts.
The heuristic strategy of grow-diag-final-and (Koehn et al, 2007) was used tocombine the bidirectional alignments for extract-ing phrase translations and reordering tables.
A5-gram language model with Kneser-Ney smooth-ing was trained with SRILM (Stolcke, 2002) onmonolingual English data.
Moses (Koehn et al,2007) was used as decoder.
The Minimum ErrorRate Training (MERT) (Och, 2003) was used totune the feature parameters on development data.4.2 Various Segmentation ModelsTo provide a thorough analysis, the MT experi-ments in this study evaluated three baseline seg-mentation models and two off-the-shelf models,in addition to four variant models that also employthe bilingual constraints.
We start from three base-line models:?
Character Segmenter (CS): this model sim-ply divides Chinese sentences into sequencesof characters.?
Supervised Monolingual Segmenter (SM-S): this model is trained by CRFs on treebanktraining data (trainTB).
The same featuretemplates (Zhao et al, 2006) are used.
Thestandard four-tags (B, M, E and S) were used8We evaluated graphs with top k (from 3 to 20) nearestneighbors on development data, and found that the perfor-mance converged beyond 10-NNs.1365as the labels.
The stochastic gradient descentis adopted to optimize the parameters.?
Unsupervised Bilingual Segmenter (UBS):this model is trained on the bitexts (trainMT)following the approach introduced in (Maand Way, 2009).
The optimal set of the mod-el parameter values was found on devMTtobe k = 3, tAC= 0.0 and tCOOC= 15.The comparison candidates also involve two pop-ular off-the-shelf segmentation models:?
Stanford Segmenter: this model, trained byChang et al (2008), treats CWS as a binaryword boundary decision task.
It covers sev-eral features specific to the MT task, e.g., ex-ternal lexicons and proper noun features.?
ICTCLAS Segmenter: this model, trainedby Zhang et al (2003), is a hierarchicalHMM segmenter that incorporates parts-of-speech (POS) information into the probabili-ty models and generates multiple HMM mod-els for solving segmentation ambiguities.This work also evaluated four variant models9that perform alternative ways to incorporate thebilingual constraints based on two state-of-the-artgraph-based SSL approaches.?
Self-training Segmenters (STS): two vari-ant models were defined by the approach re-ported in (Subramanya et al, 2010) that us-es the supervised CRFs model?s decodings,incorporating empirical and constraint infor-mation, for unlabeled examples as additionallabeled data to retrain a CRFs model.
Onevariant (STS-NO-GP) skips the GP step, di-rectly decoding with type-level word bound-ary probabilities induced from bitexts, whilethe other (STS-GP-PL) runs the GP at firstand then decodes with GP outcomes.
Theoptimal hyperparameter values were found tobe: STS-NO-GP (?
= 0.8) and ?
= 0.6) andSTS-GP-PL (?
= 0.5, ?
= 0.3, ?
= 0.8 and?
= 0.6).?
Virtual Evidences Segmenters (VES): T-wo variant models based on the approachin (Zeng et al, 2013) were defined.
The type-level word boundary distributions, induced9Note that there are two variant models working with GP.To be fair, the same similarity graph settings introduced inthis paper were used.by the character-based alignment (VES-NO-GP), and the graph propagation (VES-GP-PL), are regarded as virtual evidences to biasCRFs model?s learning on the unlabeled da-ta.
The optimal hyperparameter values werefound to be: VES-NO-GP (?
= 0.7) andVES-GP-PL (?
= 0.5, ?
= 0.3 and ?
= 0.7).4.3 Main ResultsTable 1 summarizes the final MT performance onthe MT-05 test data, evaluated with ten differentCWS models.
In what follows, we summarizedfour major observations from the results.
First-ly, as expected, having word segmentation doeshelp Chinese-to-English MT.
All other nine CWSmodels outperforms the CS baseline which doesnot try to identify Chinese words at all.
Second-ly, the other two baselines, SMS and UBS, are ona par with each other, showing less than 0.36 av-erage performance differences on the three eval-uation metrics.
This outcome validated that themodels, trained by either the treebank or the bilin-gual data, performed reasonably well.
But theyonly capture partial segmentation features so thatless gains for SMT are achieved when compar-ing to other sophisticated models.
Thirdly, we no-tice that the two off-the-shelf models, Stanford andICTCLAS, just brought minor improvements overthe SMS baseline, although they are trained us-ing richer supervisions.
This behaviour illustratesthat the conventional optimizations to the mono-lingual supervised model, e.g., accumulating moresupervised data or predefined segmentation prop-erties, are insufficient to help model for achiev-ing better segmentations for SMT.
Finally, high-lighting the five models working with the bilingualconstraints, most of them can achieve significantgains over the other ones without using the bilin-gual constraints.
This strongly demonstrates thatbilingually-learned segmentation knowledge doeshelps CWS for SMT.
The models working with G-P, STS-GP-PL, VES-GP-PL and ours outperformall others.
We attribute this to the role of GP inassisting the spread of bilingual knowledge on theChinese side.
Importantly, it can be observed thatour model outperforms STS-GP, VES-GP, whichgreatly supports that joint learning of CRFs andGP can alleviate the error transfer by the pipelinedmodels.
This is one of the most crucial findingsin this study.
Overall, the boldface numbers in thelast row illustrate that our model obtains averageimprovements of 1.89, 1.76 and 1.61 on BLEU,1366NIST and METEOR over others.Models BLEU NIST METEORCS 29.38 59.85 54.07SMS 30.05 61.33 55.95UBS 30.15 61.56 55.39Stanford 30.40 61.94 56.01ICTCLAS 30.29 61.26 55.72STS-NO-GP 31.47 62.35 56.12STS-GP-PL 31.94 63.20 57.09VES-NO-GP 31.98 62.63 56.59VES-GP-PL 32.04 63.49 57.34Our Model 32.75 63.72 57.64Table 1: Translation performances (%) on MT-05testing data by using ten different CWS models.4.4 Analysis & DiscussionThis section aims to further analyze the three pri-mary observations concluded in Section 4.3: i)word segmentation is useful to SMT; ii) the tree-bank and the bilingual segmentation knowledgeare helpful, performing segmentation of differen-t nature; and iii) the bilingual constraints lead tolearn segmentations better tailored for SMT.The first observation derives from the compar-isons between the CS baseline and other model-s. Our results, showing the significant CWS ben-efits to SMT, are consistent with the works re-ported in the literature (Xu et al, 2004; Changet al, 2008).
In our experiment, two additionalevidences found in the translation model are pro-vided to further support that NO tokenization ofChinese (i.e., the CS model?s output) could har-m the MT system.
First, the SMT phrase extrac-tion, i.e., building ?phrases?
on top of the char-acter sequences, cannot fully capture all meaning-ful segmentations produced by the CS model.
Thecharacter based model leads to missing some use-ful longer phrases, and to generate many meaning-less or redundant translations in the phrase table.Moreover, it is affected by translation ambiguities,caused by the cases where a Chinese character hasvery different meanings in different contextual en-vironments.The second observation shifts the emphasis toSMS and UBS, based on the treebank and thebilingual segmentation, respectively.
Our result-s show that both segmentation patterns can bringpositive effects to MT.
Through analyzing bothmodels?
segmentations for trainMTand testMT,we attempted to get a closer inspection on the seg-mentation preferences and their influence on MT.Our first finding is that the segmentation consen-suses between SMS and UBS are positive to MT.There have about 35% identical segmentationsproduced by the two models.
If these identicalsegmentations are removed, and the experimentsare rerun, the translation scores decrease (on av-erage) by 0.50, 0.85 and 0.70 on BLEU, NISTand METEOR, respectively.
Our second findingis that SMS exhibits better segmentation consis-tency than UBS.
One representative example is thesegmentations for ????
(lonely)?.
All the out-puts of SMS were ????
?, while UBS generat-ed three ambiguous segmentations, ??
(alone) ??
(double zero)?, ???
(lonely) ?(zero)?
and??
(alone) ?
(zero) ?(zero)?.
The segmentationconsistency of SMS rests on the high-quality tree-bank data and the robust CRFs tagging mod-el.
On the other hand, the advantage of UB-S is to capture the segmentations matching thealigned target words.
For example, UBS grouped??
(country) ?
(border) ?(between)?
to a word????
(international)?, rather than two word-s ???
(international) ?(between)?
(as given bySMS), since these three characters are aligned toa single English word ?international?.
The aboveanalysis shows that SMS and UBS have their ownmerits and combining the knowledge derived fromboth segmentations is highly encouraged.The third observation concerns the great im-pact of the bilingual constraints to the segmenta-tion models in the MT task.
The use of the bilin-gual constraints is the prime objective of this s-tudy.
Our first contribution for this purpose ison using the word boundary distributions to cap-ture the bilingual segmentation supervisions.
Thisrepresentation contributes to reduce the negativeimpacts of erroneous ?chars-to-word?
alignments.The ambiguous types (having relatively uniformboundary distribution), caused by alignment er-rors, cannot directly bias the model tagging pref-erences.
Furthermore, the word boundary distri-butions are convenient to make up the learningconstraints over the labelings among various con-strained learning approaches.
They have success-fully played in three types of constraints for ourexperiments: PR penalty (Our model), decodingconstraints in self-training (STS) and virtual evi-dences (VES).
The second contribution is the useof GP, illustrated by STS-GP-PL, VES-GP-PL and1367Our model.
The major effect is to multiply the im-pacts of the bilingual knowledge through the sim-ilarity graph.
The graph vertices (types)10, with-out any supervisions, can learn the word bound-ary information from their similar types (neigh-borhoods) having the empirical boundary prob-abilities.
The segmentations given by the threeGP models show about 70% positive segmenta-tion changes, affected by the unlabeled graph ver-tices, with respect to the ones given by the NO-GP models, STS-NO-GP and VES-NO-GP.
In ouropinion, the learning mechanism of our approach,joint coupling of GP and CRFs, rather than thepipelined one as the other two models, contributesto maximizing the graph smoothness effects to theCRFs estimation so that the error propagation ofthe pipelined approaches is alleviated.5 ConclusionThis paper proposed a novel CWS model for theSMT task.
This model aims to maintain the lin-guistic segmentation supervisions from treebankdata and simultaneously integrate useful bilingualsegmentations induced from the bitexts.
This ob-jective is accomplished by three main steps: 1)learn word boundaries from character-based align-ments; 2) encode the learned word boundaries intoa GP constraint; and 3) training a CRFs model, un-der the GP constraint, by using the PR framework.The empirical results indicate that the proposedmodel can yield better segmentations for SMT.AcknowledgmentsThe authors are grateful to the Science andTechnology Development Fund of Macau andthe Research Committee of the University ofMacau (Grant No.
MYRG076 (Y1-L2)-FST13-WF and MYRG070 (Y1-L2)-FST12-CS) for thefunding support for our research.
The work ofIsabel Trancoso was supported by national fundsthrough FCT-Fundac?
?ao para a Ci?ecia e a Tecnolo-gia, under project PEst-OE/EEI/LA0021/2013.The authors also wish to thank the anonymous re-viewers for many helpful comments.10This experiment yielded a similarity graph that consistsof 11,909,620 types from trainTBand trainMT, where therehave 8,593,220 (72.15%) types without any empirical bound-ary distributions.ReferencesYasemin Altun, David McAllester, and Mikhail Belkin.2006.
Maximum margin semi-supervised learningfor structured variables.
Advances in Neural Infor-mation Processing Systems, 18:33.Satanjeev Banerjee and Alon Lavie.
2005.
Meteor: Anautomatic metric for mt evaluation with improvedcorrelation with human judgments.
In Proceedingsof the ACL Workshop on Intrinsic and Extrinsic E-valuation Measures for Machine Translation and/orSummarization, pages 65?72.
Association for Com-putational Linguistics.Yoshua Bengio, Olivier Delalleau, and NicolasLe Roux.
2006.
Label propagation and quadrat-ic criterion.
Semi-Supervised Learning, pages 193?216.Pi-Chuan Chang, Michel Galley, and Christopher DManning.
2008.
Optimizing Chinese word segmen-tation for machine translation performance.
In Pro-ceedings of WMT, pages 224?232.
Association forComputational Linguistics.Tagyoung Chung and Daniel Gildea.
2009.
Unsuper-vised tokenization for machine translation.
In Pro-ceedings of EMNLP, pages 718?726.
Associationfor Computational Linguistics.Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-basedprojections.
In Proceedings of ACL, pages 600?609.Association for Computational Linguistics.Dipanjan Das and Noah A Smith.
2012.
Graph-basedlexicon expansion with sparsity-inducing penalties.In Proceedings of NAACL, pages 677?687.
Associa-tion for Computational Linguistics.George R. Doddington, Mark A. Przybocki, Alvin F.Martin, and Douglas A. Reynolds.
2000.
The nistspeaker recognition evaluation?overview, methodol-ogy, systems, results, perspective.
Speech Commu-nication, 31(2):225?254.Kuzman Ganchev, J?oao Grac?a, Jennifer Gillenwater,and Ben Taskar.
2010.
Posterior regularization forstructured latent variable models.
The Journal ofMachine Learning Research, 11:2001?2049.Luheng He, Jennifer Gillenwater, and Ben Taskar.2013.
Graph-based posterior regularization forsemi-supervised structured prediction.
In Proceed-ings of CoNLL, page 38.
Association for Computa-tional Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertol-di, Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of ACL on Interactive Poster and Demon-stration Sessions, pages 177?180.
Association forComputational Linguistics.1368Yanjun Ma and Andy Way.
2009.
Bilingually motivat-ed domain-adapted word segmentation for statisticalmachine translation.
In Proceedings of EACL, pages549?557.
Association for Computational Linguistic-s.Gideon S. Mann and Andrew McCallum.
2008.Generalized expectation criteria for semi-supervisedlearning of conditional random fields.
In Proceed-ings of ACL, pages 870?878.
Association for Com-putational Linguistics.Andrew McCallum, Gideon Mann, and GregoryDruck.
2007.
Generalized expectation criteri-a.
Computer Science Technical Note, University ofMassachusetts, Amherst, MA.Franz Josef Och and Hermann Ney.
2003.
A systemat-ic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of A-CL, pages 160?167.
Association for ComputationalLinguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic e-valuation of machine translation.
In Proceedings ofACL, pages 311?318.
Association for Computation-al Linguistics.Michael Paul, Finch Andrew, and Sumita Eiichiro.2011.
Integration of multiple bilingually-trainedsegmentation schemes into statistical machine trans-lation.
IEICE Transactions on Information and Sys-tems, 94(3):690?697.Andreas Stolcke.
2002.
SRILM-an extensible lan-guage modeling toolkit.
In Proceedings of Inter-speech.Amarnag Subramanya, Slav Petrov, and FernandoPereira.
2010.
Efficient graph-based semi-supervised learning of structured tagging models.
InProceedings of EMNLP, pages 167?176.
Associa-tion for Computational Linguistics.Liang Tian, Derek F. Wong, Lidia S. Chao, PauloQuaresma, Francisco Oliveira, Shuo Li, YimingWang, and Yi Lu.
2014.
UM-Corpus: A largeEnglish-Chinese parallel corpus for statistical ma-chine translation.
In Proceedings of LREC.
Euro-pean Language Resources Association.Ning Xi, Guangchao Tang, Xinyu Dai, Shujian Huang,and Jiajun Chen.
2012.
Enhancing statistical ma-chine translation with character alignment.
In Pro-ceedings of ACL, pages 285?290.
Association forComputational Linguistics.Jia Xu, Richard Zens, and Hermann Ney.
2004.
Dowe need Chinese word segmentation for statisticalmachine translation?
In Proceedings of the ThirdSIGHAN Workshop on Chinese Language Learning,pages 122?128.
Association for Computational Lin-guistics.Jia Xu, Evgeny Matusov, Richard Zens, and HermannNey.
2005.
Integrated Chinese word segmentationin statistical machine translation.
In Proceedings ofIWSLT, pages 216?223.
Association for Computa-tional Linguistics.Naiwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese TreeBank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238.Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Is-abel Trancoso.
2013.
Graph-based semi-supervisedmodel for joint Chinese word segmentation and part-of-speech tagging.
In Proceedings of ACL, pages770?779.
Association for Computational Linguistic-s.Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, Is-abel Trancoso, Liangye He, and Qiuping Huang.2014.
Lexicon expansion for latent variable gram-mars.
Pattern Recognition Letters, 42:47?55.Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and QunLiu.
2003.
HHMM-based Chinese lexical analyzerICTCLAS.
In Proceedings of the Second SIGHANWorkshop on Chinese Language Processing, pages184?187.
Association for Computational Linguistic-s.Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.2008.
Improved statistical machine translation bymultiple Chinese word segmentation.
In Proceed-ings of WMT, pages 216?223.
Association for Com-putational Linguistics.Hai Zhao, Chang-Ning Huang, and Mu Li.
2006.
Animproved Chinese word segmentation system withconditional random field.
In Proceedings of the FifthSIGHAN Workshop on Chinese Language Process-ing.
Association for Computational Linguistics.Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-Liang Lu.
2013.
An empirical study on word seg-mentation for Chinese machine translation.
In Com-putational Linguistics and Intelligent Text Process-ing, pages 248?263.
Springer.Xiaojin Zhu, Zoubin Ghahramani, and John Laffer-ty.
2003.
Semi-supervised learning using gaussianfields and harmonic functions.
In Proceedings ofICML, volume 3, pages 912?919.Ling Zhu, Derek F. Wong, and Lidia S. Chao.
2014.Unsupervised chunking based on graph propagationfrom bilingual corpus.
The Scientific World Journal,2014(401943):10.1369
