First Joint Conference on Lexical and Computational Semantics (*SEM), pages 529?535,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsETS: Discriminative Edit Models for Paraphrase ScoringMichael Heilman and Nitin MadnaniEducational Testing Service660 Rosedale RoadPrinceton, NJ 08541, USA{mheilman,nmadnani}@ets.orgAbstractMany problems in natural language process-ing can be viewed as variations of the task ofmeasuring the semantic textual similarity be-tween short texts.
However, many systemsthat address these tasks focus on a single taskand may or may not generalize well.
In thiswork, we extend an existing machine transla-tion metric, TERp (Snover et al, 2009a), byadding support for more detailed feature typesand by implementing a discriminative learningalgorithm.
These additions facilitate applica-tions of our system, called PERP, to similar-ity tasks other than machine translation eval-uation, such as paraphrase recognition.
Inthe SemEval 2012 Semantic Textual Similar-ity task, PERP performed competitively, par-ticularly at the two surprise subtasks revealedshortly before the submission deadline.1 IntroductionTechniques for measuring the similarity of two sen-tences have various potential applications: auto-mated short answer scoring (Nielsen et al, 2008;Leacock and Chodorow, 2003), question answering(Wang et al, 2007), machine translation evaluation(Przybocki et al, 2009; Snover et al, 2009a), etc.An important aspect of this problem is that sim-ilarity is not binary.
Sentences can be very seman-tically similar, such that they might be called para-phrases of each other.
They might be completelydifferent.
Or, they might be somewhere in between.Indeed, it is arguable that all sentence pairs (exceptexact duplicates) lie somewhere on a continuum ofsimilarity.
Therefore, it is desirable to develop meth-ods that model sentence pair similarity on a contin-uous, or at least ordinal, scale.In this paper, we describe a system for measuringthe semantic similarity of pairs of short texts.
As astarting point, we use the Translation Error Rate Plus(Snover et al, 2009a), or TERp, system, which wasspecifically developed for machine translation eval-uation.
TERp takes two sentences as input, finds aset of weighted edits that convert one into the otherwith low overall weight, and then produces a length-normalized score.
TERp also has a greedy, heuris-tic learning algorithm for inducing weights from la-beled sentence pairs in order to increase correlationswith human similarity scores.Some features of the original TERp make adap-tation to other semantic similarity tasks difficult, in-cluding its largely one-to-one mapping of featuresto edits and its heuristic, greedy learning algorithm.For example, there is a single feature for lexical sub-stitution, even though it is clear that different typesof substitutions have different effects on similarity(e.g., substituting ?43.6?
with ?17?
versus substitut-ing ?a?
for ?an?).
In addition, the heuristic learn-ing algorithm, which involves perturbing the weightvector by small amounts as in grid search, seems un-scalable to larger sets of overlapping features.Therefore, here, we use TERp?s inference algo-rithms that find low cost edit sequences but use a dis-criminative learning algorithm based on the Percep-tron (Rosenblatt, 1958; Collins, 2002) to estimateedit cost parameters, along with an expanded fea-ture set for broader coverage of the phenomena thatare relevant to sentence-to-sentence similarity.
We529refer to this new approach as Paraphrase Edit Ratewith the Perceptron (PERP).In addition to describing PERP, we discuss how itwas applied for the SemEval 2012 Semantic TextualSimilarity (STS) task.2 Problem DefinitionIn this work, our goal is to create a system that cantake as input two sentences (or short texts) x1 and x2and produce as output a prediction y?
for how simi-lar they are.
Here, we use the 0 to 5 ordinal scalefrom the STS task, where increasing values indicategreater semantic similarity.The STS task data includes five subtasks with textpairs from different sources: the Microsoft ResearchParaphrase Corpus (Dolan et al, 2004) (MSRpar),The Microsoft Research Video corpus (Chen andDolan, 2011) (MSRvid), statistical machine transla-tion output of parliament proceedings (Koehn, 2005)(SMT-eur).
For each of these sources, approxi-mately 750 sentence pairs x1 and x2 and gold stan-dard similarity values y were provided for trainingand development.In addition, there were two surprise data sourcesrevealed shortly before the submission deadline:pairs of sentences from Ontonotes (Pradhan andXue, 2009) and Wordnet (Fellbaum, 1998) (OnWN),and machine translations of sentences from newsconversations (SMT-news).
For all five sources,the held-out test set contained several hundred textpairs.
See the task description (Agirre et al, 2012)for additional details.3 TER, TERp, and PERPIn this section, we briefly describe the TER andTERp machine translation metrics, and how thePERP system extends them in order to better modelsemantic textual similarity.TER (Snover et al, 2006) uses a greedy search al-gorithm to find a set of edits to convert one of thepaired input sentences into the other.
We can viewthis set of edits as an alignment a between the twoinput sentences x1 and x2, and when two words inx1 and x2, respectively, are part of an edit operation,we say that those words are aligned.1 Unlike tradi-1For machine translation evaluation with TERp and PERP,x1 is a system?s hypothesis and x2 is a reference translation.
Fortional edit distance measures, TER allow for shifts?that is, edits that change the positions of words orphrases in the input sentence x1.
Essentially, TERsearches among a set of possible shifts of the phrasesin x1 to find a set of shifts that result in the leastcost alignment, using edits of other types, betweenx2 and the shifted version of x1.
TER allows one tospecify costs for different edit types, but it does notinclude a method for learning those costs from data.TERp (Snover et al, 2009b; Snover et al, 2009a)extends TER in two key ways.
First, TERp in-cludes new types of edits, including edits for substi-tution of synonyms, word stems, and phrasal para-phrases extracted from a pivot-based paraphrase ta-ble (?3.1).
Second, it includes a heuristic learningalgorithm for inferring cost parameters from labeleddata.
TERp includes 8 types of edits: match (M), in-sertion (I), deletion (D), substitution (S), stemming(T), synonymy (Y), shift (Sh), and phrase substitu-tion (P).
The edits are mutually exclusive, such thatsynonymy edits do not count as substitutions, for ex-ample.
TERp has 11 total parameters, with a singleparameter for each edit except for phrase substition,which has four.PERP has a general framework similar to thatof TERp.
It extends TERp, however, by includ-ing additional edit parameters, and by using a dis-criminative learning algorithm (see ?5) to learn pa-rameters rather than the heuristic technique used byTERp.
Thus, PERP uses the same greedy algorithmas TERp for finding the optimal sets of edits giventhe cost parameters, but it allows the cost for an indi-vidual edit to depend on multiple, overlapping fea-tures of that edit.
For example, costs for substitu-tion edits depend on whether the aligned words arepronouns, whether the aligned words represent num-bers, the lengths of the aligned words, etc.
See ?4 forthe full list of features in PERP.An alignment from the MSRpar portion of theSTS training data is illustrated in Figure 1.3.1 Phrasal ParaphrasesPERP uses probabilistic phrasal substitutions toalign phrases in the hypothesis with phrases in theall STS subtasks, we assigned sentences in the first and secondcolumns of the input files to x2 and x1, respectively, so thatthe hypotheses and references in the SMT-eur subtask would beassigned appropriately.530the research firm earlier had forecast an increase of 4.9 percent .the firm earlier had predicted increase this year a 4.9 percent .the firm had predicted earlier this year a 4.9 percent increase .synonymyshift shift insertdelete delete deleteinsertinsertx1x2Figure 1: An example of a PERP alignment for a sentence pair from the Microsoft Research Paraphrase Corpus.The search algorithm first performs shifts on x1 and then performs other edits on x2.
The zero cost edits that matchindividual words are not shown.reference.
It does so by looking up?in a pre-computed phrase table?paraphrases of phrases inthe reference and using its associated edit cost asthe cost of performing a match against the hypoth-esis.
The paraphrase table used in PERP was iden-tical to the one used by Snover et al (2009a).
Itwas extracted using the pivot-based method as de-scribed by Bannard and Callison-Burch (2005) withseveral additional filtering mechanisms to increasethe precision of the extracted pairs.
The pivot-basedmethod utilizes the inherent monolingual semanticknowledge from bilingual corpora: we first iden-tify phrasal correspondences between English and agiven foreign language F , then map from English toEnglish by following translation units from Englishto the other language and back.
For example, if thetwo English phrases e1 and e2 both correspond tothe same foreign phrase f , then they may be consid-ered to be paraphrases of each other with the follow-ing probability:p(e1|e2) ?
p(e1|f)p(f |e2)If there are several pivot phrases that link the twoEnglish phrases, then they are all used in computingthe probability:p(e1|e2) ?
?f ?p(e1|f ?
)p(f ?|e2)We used the same phrasal paraphrase database asin TERp (Snover et al, 2009a), which was extractedfrom an Arabic-English newswire bitext containinga million sentences.
A few examples of the para-phrase pairs used in the MSRpar portion of the STStraining data are shown below:(commission?
panel)(the spying?
espionage)(suffered?
underwent)(room to?
space for)(per cent?
percent)4 FeaturesAs discussed in ?3, PERP expands on TERp?s origi-nal features in order to better model semantic textualsimilarity.PERP models a pair of sentences x1 and x2 us-ing a feature function f(a) that extracts a vector ofreal-valued features from an alignment a betweenx1 and x2.
This alignment is found with TERp?sinference algorithm and consists of a set of editsof various types along with information about thewords on which those edits operate.
For example,the alignment might contain an edit with the infor-mation, ?The token ?the?
in x1 was substituted forthe token ?an?
in x2.?
This edit would increment thefeatures in f(a) for the number of substitutions andthe number of substitutions of stopwords, along withother relevant substitution features.The set of features encoded in f(a) are describedin Table 1.2 It includes general features that alwaysfire for edits of a particular type (e.g., the ?Substi-tution?
feature) as well as specific features that fireonly in specific situations (e.g., the ?Sub-Pronoun-Both?
edit, which fires only when one pronoun issubstituted for another).The function f(a) is normalized for sentence2All words were converted to lower-case.
Word frequen-cies were calculated from the NYT stories in the fifth editionof the English Gigaword corpus.
The stories were tokenizedusing NLTK and words occurring fewer than 100 times wereexcluded.
Words occurring at least 100 times constituted the vo-cabulary used for computing the OOV features.
The OOV andfrequency features only fired for words that consisted only ofletters, and the frequency features did not fire for OOV words.The set of negation words including the following: ?no?, ?not?,?never?, and ?n?t?.
The stopword list contained 158 commonwords and punctuation symbols.531Edits Feature Name Description- Intercept Always 1 (and not normalized by text lengths)T Stemming The number of times that two words with the same stem, according to the Porter(1980) stemmer, were aligned.Y Synonymy The number of times that a pair of synonyms, according to WordNet (Fellbaum,1998), were aligned.Sh Shift The number of shifts.P Paraphrase1 The number of phrasal paraphrasing operations.P Paraphrase2 The sum of q log10(p), where p is the probability in the pivot-based paraphrase tablefor a paraphrase edit and q is the number of edits for that paraphrase edit.
See Snoveret al (2009a) for further explanation.P Paraphrase3 The sum of pq, where p and q are as above.P Paraphrase4 The sum of q, where q is as above.I Insertion The number of insertions.D Deletion The number of deletions.I, D Insert-Delete-LogFreqThe sum of log10 freq(w) over all insertions and deletions, where w is the wordbeing inserted or deleted and freq(w) is the relative frequency of w.I, D Insert-Delete-LogWordLenThe sum of log10 length(w) over all insertions and deletions, where w is the wordbeing inserted or deleted.I, D Insert-Delete-XThe number of insertions and deletions of X in alignment, where X is: (a) punctu-ation, (b) numbers, (c) personal pronouns, (d) negation words, (e) stop words, or (f)out-of-vocabulary (OOV) words (6 features in all).S Substitution The number of substitutions.S Sub-X-Both The number of substitutions where both words are: (a) punctuation, (b) numbers, (c)personal pronouns, (d) negation words, (e) stop words, or (f) OOV words (6 featuresin all).S Sub-X-1only The number of substitutions where only one word is: (a) punctuation, (b) a number,(c) a personal pronoun, (d) a negation word, (e) a stop word, or (f) an OOV word (6features in all).S Sub-LogFreq-DiffThe sum of | log10 freq(w1)?
log10 freq(w2)| over all substitutions.S Sub-Contain The number of substitutions where both words have more than 5 characters and oneis a proper substring of the other.S Sub-Diff-By-NonWordThe number of substitutions where the words differ only by non-alphanumeric char-acters.S Sub-Small-LevDistThe number of substitutions where both words have more than 5 characters and theLevenshtein distance between them is 1.S Sub-Norm-LevDistThe sum of the following over all substitutions: the Levenshtein distance betweenthe words normalized by the length of the longer word.Table 1: The set of features in PERP.
The first column lists which edits for which each feature is relevant.lengths by dividing all the values in Table 1 by thesum of the number of words in x1 and x2, except forthe intercept feature that models the base similarityvalue in the training data and always has value 1.There are 36 features and corresponding parame-ters in all, compared to 11 for TERp.It is worth pointing out that while the mutual ex-clusivity between most of the original TERp editsis preserved, PERP does have shared features be-tween insert and delete edits (e.g., ?Insert-Delete-Number?
), and could in principle share features be-tween substitution, stemming, and synonymy edits.5 LearningGiven a training set consisting of paired sentencesx1 and x2 and gold standard semantic similarity rat-ings y, PERP uses Algorithm 1 to induce a good set532Algorithm 1 learn(w, T , ?, x1,x2,y):An Averaged Perceptron algorithm for learning editcost parameters.
T is the number of iterationsthrough the dataset.
?
is a learning rate.
x1 andx2 are paired lists of sentences, and y is a list ofsimilarities that correspond to those sentence pairs.wsum = 0for t = 1, 2, .
.
.
, T dox1,x2,y = shuffle(x1,x2,y)for i = 1, 2, .
.
.
, |y| doa = TERpAlign(w, x1i, x2i)y?
= w ?
f(a)w = w + ?
(yi ?
y?
)f(a)w = applyShiftConstraint(w)wsum = wsum + wend forend forreturn wsumT |y|of cost parameters for its various features.3 The al-gorithm is a fairly straightforward application of thePerceptron algorithm described by Collins (2002).4The only notable difference is that the algorithmconstrains PERP?s shift parameter to be at least 0.01in the step labeled ?applyShiftConstraint.?
We foundthat TERp?s inference algorithm would fail if theshift cost reached zero.5 In our experiments, we ini-tialized all weights to 0, except for the following: the?Substitution,?
?Insertion,?
and ?Deletion?
weightswere initialized to 1.0, and the ?Shift?
weight wasinitialized to 0.1.
Following Collins (2002), the al-gorithm returns an averaged version of the weights,though this did not appear to substantially impactperformance.3The ?shuffle?
step shuffles the lists of sentence pairs andscores together such that their orderings are randomized but thatthey stay aligned with each other.4There are a few hyperparameters in the learning algorithms.For our experiments, we set the number of iterations throughthe training data T to 200.
We set the learning rate ?
to 0.01 toavoid large oscillations in the parameters.
We did not system-atically tune the hyperparameters.
Other values might lead tobetter performance.5With zero cost shifts, TERp would enter a loop and even-tually exceed the amount of available memory.
We also set thesame minimum cost of 0.01 for shifts in our experiments withthe original TERp.6 ExperimentsIn this section, we report results for the STS sharedtask.
For a full description of the task, see Agirre etal.
(2012).The task consisted of three known subtasks(MSRpar, MSRvid, and SMT-eur) and two surprisesubtasks (On-WN, SMT-news).
For the known sub-tasks, we trained models with task-specific dataonly.
For the On-WN subtask, we used the modeltrained for MSRpar.
For SMT-news, we used themodel trained for SMT-eur.Our submissions to the task included results fromtwo variations, one using the full system (PERP-phrases) and one with the paraphrase substitutionedits disabled (PERP), in order to isolate the effectof including phrasal paraphrases.
In our originalsubmission, the PERPphrases system included a mi-nor bug that affected the calculation of the phrasalparaphrasing features.
Here, we report both the orig-inal results and a corrected version (?PERPphrases(fix)?
), though the correction only minimally af-fected performance.
We also tested two variationsof the original TERp system: one with the weightsset as reported by Snover et al (2009a) (?TERp(default)?
), and one tuned in the same task-specificmanner as PERP (?TERp (tuned)?).
We multipliedTERp?s predictions by ?1 since it produces costsrather than similarities.The results, in terms of Pearson correlations withtest set gold standard scores, are shown in Table 2.In addition to correlations for each subtask, we in-clude the three aggregated measures used for thetask.
The ?ALL?
measure is the Pearson correlationson the concatenation of all the data for all five sub-tasks.
It was the original measured used to aggregatethe results for the different subtasks.
The second ag-gregated measure is the ?Allnrm?
measure, whichwe view as an oracle because it uses the gold stan-dard similarity values from the test set to adjust sys-tem predictions.
The final aggregate measure is themean of the correlations for the subtasks, weightedby the number of examples in each subtask?s test set(?Mean?).
See Agirre et al (2012) for a full descrip-tion of the metrics.For comparison, the table also includes the re-sults from the top-ranked submission according tothe ?ALL?
measure, the results for the word-overlap533Aggregated Measures Subtask MeasuresALL ALLnrm Mean MSRpar MSRvid SMT-eur On-WN SMT-newsUKP (top-ranked) .8239 .8579 .6773 .6830 .8739 .5280 .6641 .4937PERPphrases (fix) ?
.7837 ?
.6405 .6410 .7209 .4852 .7127 .5312PERPphrases .7834 .8089 .6399 .6397 .7200 .4850 .7124 .5312PERP .7808 .8064 .6305 .6211 .7210 .4722 .7080 .5149TERp (tuned) ?
.5558 ?
.5582 .5400 .6099 .4967 .5862 .5135TERp (default) .4477 .7291 .5253 .5049 .5217 .4748 .6169 .4566baseline .3110 .6732 .4356 .4334 .2996 .4542 .5864 .3908mean of submissions .5864 .7773 .5286 .4894 .7049 .3958 .5557 .3731Table 2: Pearson correlations between predictions about the test data and gold standard scores.
???
marks experimentsthat were not parts of the official SemEval task 6 evaluation.
The highest correlation in each column is given in bold.ALLnrm results are not included for all runs because we did not have an implementation of that measure.baseline from the organizers (Agirre et al, 2012),and the means across all 88 submissions (not includ-ing the baseline).Table 3 shows the rankings in the official resultsof the PERPphrases submission, for each subtaskand overall, along with Pearson correlations fromPERP and the best submission for each subtask.Aggregated Measure Rank ?
?bestALL 6 .7834 .8239ALLnrm 27 .8089 .8635Mean 7 .6399 .6773Subtask Measure Rank ?
?bestMSRpar 8 .6397 .7343MSRvid 52 .7200 .8803SMT-eur 21 .4850 .5666On-WN 2 .7124 .7273SMT-news 4 .5312 .6085Table 3: The ranking and correlation (?)
obtained byPERPphrases for each of the five datasets as well for alldatasets combined.
The STS task had a total of 88 sub-missions.
?best shows the correlation for the best submis-sion, across all submissions, for each dataset.7 ConclusionFrom the results in ?6, PERP appears to be com-petitive at measuring semantic textual similarity.
Itperformed particularly well on the surprise subtasks,indicating that it generalizes well to new data.
Fi-nally, with the exception of the SMT-eur machinetranslation evaluation subtask, PERP outperformedthe TERp system for all of the STS subtasks.AcknowledgmentsWe would like to thank the organizers of SemEvaland the Semantic Textual Similarity task.
We wouldalso like to thank Matt Snover for making the origi-nal TERp code available.ReferencesE.
Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre.2012.
SemEval-2012 task 6: A pilot on semantic tex-tual similarity.
In Proc.
of the 6th International Work-shop on Semantic Evaluation (SemEval 2012), in con-junction with the First Joint Conference on Lexical andComputational Semantics (*SEM 2012).C.
Bannard and C. Callison-Burch.
2005.
Paraphrasingwith bilingual parallel corpora.
In Proc.
of ACL, pages597?604.D.
Chen and W. B. Dolan.
2011.
Collecting highly par-allel data for paraphrase evaluation.
In Proc.
of ACL,pages 190?200.M.
Collins.
2002.
Discriminative training methods forhidden Markov models: Theory and experiments withthe perceptron algorithm.
In Proc.
of EMNLP.W.
Dolan, C. Quirk, and C. Brockett.
2004.
Unsuper-vised construction of large paraphrase corpora: Ex-ploiting massively parallel news sources.
In Proc.
ofCOLING, pages 350?356, Geneva, Switzerland.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
Bradford Books.P.
Koehn.
2005.
Europarl: A parallel corpus for statisti-cal machine translation.
In Proc.
of Machine Transla-tion Summit.C.
Leacock and M. Chodorow.
2003. c-rater: Scoring ofshort-answer questions.
Computers and the Humani-ties, 37.534R.
D. Nielsen, W. Ward, and J. H. Martin.
2008.
Clas-sification errors in a domain-independent assessmentsystem.
In Proc.
of the Third Workshop on Innova-tive Use of Natural Language Processing for BuildingEducational Applications.M.
F. Porter.
1980.
An algorithm for suffix stripping.Program, 3(14):130?137.S.
S. Pradhan and N. Xue.
2009.
OntoNotes: The 90%solution.
In Proc.
of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, Companion Volume: Tutorial Abstracts, pages11?12.M.
A. Przybocki, K. Peterson, S. Bronsart, and G. A.Sanders.
2009.
The NIST 2008 metrics for machinetranslation challenge - overview, methodology, met-rics, and results.
Machine Translation, 23(2-3):71?103.F.
Rosenblatt.
1958.
The perceptron: A probabilisticmodel for information storage and organization in thebrain.
Psychological Review, 65.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of Translation Edit Ratewith targeted human annotation.
In Proc.
of the Con-ference of the Association for Machine Translation inthe Americas (AMTA).M.
Snover, N. Madnani, B. Dorr, and R. Schwartz.2009a.
Fluency, adequacy, or HTER?
Exploring dif-ferent human judgments with a tunable MT metric.
InProc.
of the Fourth Workshop on Statistical MachineTranslation at the 12th Meeting of the European Chap-ter of the Association for Computational Linguistics(EACL-2009), March.M.
Snover, N. Madnani, B. Dorr, and R. Schwartz.2009b.
TER-Plus: Paraphrase, semantic, and align-ment enhancements to Translation Edit Rate.
MachineTranslation, 23(2?3):117?127.M.
Wang, N. A. Smith, and T. Mitamura.
2007.
What isthe Jeopardy model?
A quasi-synchronous grammarfor QA.
In Proc.
of of EMNLP.535
