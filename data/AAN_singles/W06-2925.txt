Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 181?185, New York City, June 2006. c?2006 Association for Computational LinguisticsProjective Dependency Parsing with PerceptronXavier Carreras, Mihai Surdeanu and Llu?
?s Ma`rquezTALP Research Centre ?
Software Department (LSI)Technical University of Catalonia (UPC)Campus Nord - Edifici Omega, Jordi Girona Salgado 1?3, E-08034 Barcelona{carreras,surdeanu,lluism}@lsi.upc.eduAbstractWe describe an online learning depen-dency parser for the CoNLL-X SharedTask, based on the bottom-up projectivealgorithm of Eisner (2000).
We experi-ment with a large feature set that mod-els: the tokens involved in dependenciesand their immediate context, the surface-text distance between tokens, and the syn-tactic context dominated by each depen-dency.
In experiments, the treatment ofmultilingual information was totally blind.1 IntroductionWe describe a learning system for the CoNLL-XShared Task on multilingual dependency parsing(Buchholz et al, 2006), for 13 different languages.Our system is a bottom-up projective dependencyparser, based on the cubic-time algorithm by Eisner(1996; 2000).
The parser uses a learning functionthat scores all possible labeled dependencies.
Thisfunction is trained globally with online Perceptron,by parsing training sentences and correcting its pa-rameters based on the parsing mistakes.
The featuresused to score, while based on the previous work independency parsing (McDonald et al, 2005), intro-duce some novel concepts such as better codificationof context and surface distances, and runtime infor-mation from dependencies previously parsed.Regarding experimentation, the treatment of mul-tilingual data has been totally blind, with no spe-cial processing or features that depend on the lan-guage.
Considering its simplicity, our systemachieves moderate but encouraging results, with anoverall labeled attachment accuracy of 74.72% onthe CoNLL-X test set.2 Parsing and Learning AlgorithmsThis section describes the three main components ofthe dependency parsing: the parsing model, the pars-ing algorithm, and the learning algorithm.2.1 ModelLet 1, .
.
.
, L be the dependency labels, defined be-forehand.
Let x be a sentence of n words, x1 .
.
.
xn.Finally, let Y(x) be the space of well-formed depen-dency trees for x.
A dependency tree y ?
Y(x) is aset of n dependencies of the form [h,m, l], whereh is the index of the head word (0 ?
h ?
n,where 0 means root), m is the index of the modi-fier word (1 ?
m ?
n), and l is the dependencylabel (1 ?
l ?
L).
Each word of x participates as amodifier in exactly one dependency of y.Our dependency parser, dp, returns the maximumscored dependency tree for a sentence x:dp(x,w) = argmaxy?Y(x)?
[h,m,l]?ysco([h,m, l], x, y,w)In the formula, w is the weight vector of theparser, that is, the set of parameters used to score de-pendencies during the parsing process.
It is formedby a concatenation of L weight vectors, one for eachdependency label, w = (w1, .
.
.
,wl, .
.
.
,wL).
Weassume a feature extraction function, ?, that repre-sents an unlabeled dependency [h,m] in a vector ofD features.
Each of the wl has D parameters ordimensions, one for each feature.
Thus, the global181weight vector w maintains L ?
D parameters.
Thescoring function is defined as follows:sco([h,m, l], x, y,w) = ?
(h,m, x, y) ?
wlNote that the scoring of a dependency makes useof y, the tree that contains the dependency.
As de-scribed next, at scoring time y just contains the de-pendencies found between h and m.2.2 Parsing AlgorithmWe use the cubic-time algorithm for dependencyparsing proposed by Eisner (1996; 2000).
This pars-ing algorithm assumes that trees are projective, thatis, dependencies never cross in a tree.
While this as-sumption clearly does not hold in the CoNLL-X data(only Chinese trees are actually 100% projective),we chose this algorithm for simplicity.
As it will beshown, the percentage of non-projective dependen-cies is not very high, and clearly the error rates weobtain are caused by other major factors.The parser is a bottom-up dynamic programmingalgorithm that visits sentence spans of increasinglength.
In a given span, from word s to word e, itcompletes two partial dependency trees that coverall words within the span: one rooted at s and theother rooted at e. This is done in two steps.
First, theoptimal dependency structure internal to the span ischosen, by combining partial solutions from inter-nal spans.
This structure is completed with a depen-dency covering the whole span, in two ways: froms to e, and from e to s. In each case, the scoringfunction is used to select the dependency label thatmaximizes the score.We take advantage of this two-step processing tointroduce features for the scoring function that rep-resent some of the internal dependencies of the span(see Section 3 for details).
It has to be noted thatthe parsing algorithm we use does not score depen-dencies on top of every possible internal structure.Thus, by conditioning on features extracted from ywe are making the search approximative.2.3 Perceptron LearningAs learning algorithm, we use Perceptron tailoredfor structured scenarios, proposed by Collins (2002).In recent years, Perceptron has been used in a num-ber of Natural Language Learning works, such as inw = 0for t = 1 to Tforeach training example (x, y) doy?
= dp(x,w)foreach [h,m, l] ?
y\y?
dowl = wl + ?
(h,m, x, y?
)foreach [h,m, l] ?
y?\y dowl = wl ?
?
(h,m, x, y?
)return wFigure 1: Pseudocode of the Perceptron Algorithm.
T is aparameter that indicates the number of epochs that the algorithmcycles the training set.partial parsing (Carreras et al, 2005) or even depen-dency parsing (McDonald et al, 2005).Perceptron is an online learning algorithm thatlearns by correcting mistakes made by the parserwhen visiting training sentences.
The algorithm isextremely simple, and its cost in time and memoryis independent from the size of the training corpora.In terms of efficiency, though, the parsing algorithmmust be run at every training sentence.Our system uses the regular Perceptron workingin primal form.
Figure 1 sketches the code.
Giventhe number of languages and dependency types inthe CoNLL-X exercise, we found prohibitive towork with a dual version of Perceptron, that wouldallow the use of a kernel function to expand features.3 FeaturesThe feature extraction function, ?
(h,m, x, y), rep-resents in a feature vector a dependency from wordpositions m to h, in the context of a sentence x and adependency tree y.
As usual in discriminative learn-ing, we work with binary indicator features: if a cer-tain feature is observed in an instance, the value ofthat feature is 1; otherwise, the value is 0.
For con-venience, we describe ?
as a composition of severalbase feature extraction functions.
Each extracts anumber of disjoint features.
The feature extractionfunction ?
(h,m, x, y) is calculated as:?token(x, h, ?head?)
+ ?tctx(x, h, ?head?)
+?token(x,m, ?mod?)
+ ?tctx(x,m, ?mod?)
+?dep(x,mmdh,m) + ?dctx(x,mmdh,m) +?dist(x,mmdh,m) + ?runtime(x, y, h,m, dh,m)where ?token extracts context-independent tokenfeatures, ?tctx computes context-based token fea-tures, ?dep computes context-independent depen-182?token(x, i, type)type ?
w(xi)type ?
l(xi)type ?
cp(xi)type ?
fp(xi)foreach(ms): type ?ms(xi)type ?
w(xi) ?
cp(xi)foreach(ms): type ?
w(xi) ?ms(xi)?tctx(x, i, type)?token(x, i?
1, type ?
string(i?
1))?token(x, i?
2, type ?
string(i?
2))?token(x, i+ 1, type ?
string(i+ 1))?token(x, i+ 2, type ?
string(i+ 2))type ?
cp(xi) ?
cp(xi?1)type ?
cp(xi) ?
cp(xi?1) ?
cp(xi?2)type ?
cp(xi) ?
cp(xi+1)type ?
cp(xi) ?
cp(xi+1) ?
cp(xi+2)Table 1: Token features, both context-independent (?token)and context-based (?tctx).
type - token type, i.e.
?head?
or?mod?, w - token word, l - token lemma, cp - token coarse part-of-speech (POS) tag, fp - token fine-grained POS tag, ms -token morpho-syntactic feature.
The ?
operator stands for stringconcatenation.
?dep(x, i, j,dir)dir ?
w(xi) ?
cp(xi) ?
w(xj) ?
cp(xj)dir ?
cp(xi) ?
w(xj) ?
cp(xj)dir ?
w(xi) ?
w(xj) ?
cp(xj)dir ?
w(xi) ?
cp(xi) ?
cp(xj)dir ?
w(xi) ?
cp(xi) ?
w(xj)dir ?
w(xi) ?
w(xj)dir ?
cp(xi) ?
cp(xj)?dctx(x, i, j,dir)dir ?
cp(xi) ?
cp(xi+1) ?
cp(xj?1) ?
cp(xj)dir ?
cp(xi?1) ?
cp(xi) ?
cp(xj?1) ?
cp(xj)dir ?
cp(xi) ?
cp(xi+1) ?
cp(xj) ?
cp(xj+1)dir ?
cp(xi?1) ?
cp(xi) ?
cp(xj) ?
cp(xj+1)Table 2: Dependency features, both context-independent(?dep) and context-based (?dctx), between two points i and j,i < j. dir - dependency direction: left to right or right to left.dency features, ?dctx extracts contextual depen-dency features, ?dist calculates surface-distance fea-tures between the two tokens, and finally, ?runtimecomputes dynamic features at runtime based on thedependencies previously built for the given intervalduring the bottom-up parsing.
mmdh,m is a short-hand for a triple of numbers: min(h,m), max(h,m)and dh,m (a sign indicating the direction, i.e., +1 ifm < h, and ?1 otherwise).We detail the token features in Table 1, the depen-dency features in Table 2, and the surface-distancefeatures in Table 3.
Most of these features are in-spired by previous work in dependency parsing (Mc-Donald et al, 2005; Collins, 1999).
What is impor-?dist(x, i, j,dir)foreach(k ?
(i, j)): dir ?
cp(xi) ?
cp(xk) ?
cp(xj)number of tokens between i and jnumber of verbs between i and jnumber of coordinations between i and jnumber of punctuations signs between i and jTable 3: Surface distance features between points i and j. Nu-meric features are discretized using ?binning?
to a small numberof intervals.
?runtime(x,y,h,m,dir)let l1, .
.
.
, lS be the labels of dependenciesin y that attach to h and are found from m to h.foreach i, 1?
i?S : dir ?
cp(xh) ?
cp(xm) ?
liif S?1 , dir ?
cp(xh) ?
cp(xm) ?
l1if S?2 , dir ?
cp(xh) ?
cp(xm) ?
l1 ?
l2if S?3 , dir ?
cp(xh) ?
cp(xm) ?
l1 ?
l2 ?
l3if S?4 , dir ?
cp(xh) ?
cp(xm) ?
l1 ?
l2 ?
l3 ?
l4if S=0 , dir ?
cp(xh) ?
cp(xm) ?
nullif 0<S?4 , dir ?
cp(xh) ?
cp(xm) ?
regularif S>4 , dir ?
cp(xh) ?
cp(xm) ?
bigTable 4: Runtime features of y between m and h.tant for the work presented here is that we constructexplicit feature combinations (see above tables) be-cause we configured our linear predictors in primalform, in order to keep training times reasonable.While the features presented in Tables 1, 2, and 3are straightforward exploitations of the training data,the runtime features (?runtime) take a different, andto our knowledge novel in the proposed framework,approach: for a dependency from m to h, they rep-resent the dependencies found between m and hthat attach also to h. They are described in detailin Table 4.
As we have noted above, these fea-tures are possible because of the parsing scheme,which scores a dependency only after all dependen-cies spanned by it are scored.4 Experiments and ResultsWe experimented on the 13 languages proposedin the CoNLL-X Shared Task (Hajic?
et al, 2004;Simov et al, 2005; Simov and Osenova, 2003; Chenet al, 2003; Bo?hmova?
et al, 2003; Kromann, 2003;van der Beek et al, 2002; Brants et al, 2002;Kawata and Bartels, 2000; Afonso et al, 2002;Dz?eroski et al, 2006; Civit and Mart?
?, 2002; Nilssonet al, 2005; Oflazer et al, 2003; Atalay et al, 2003).Our approach to deal with many different languageswas totally blind: we did not inspect the data to mo-tivate language-specific features or processes.183We did feature filtering based on frequencycounts.
Our feature extraction patterns, that ex-ploit both lexicalization and combination, gener-ate millions of feature dimensions, even with smalldatasets.
Our criterion was to use at most 500,000different dimensions in each label weight vector.
Foreach language, we generated all possible features,and then filtered out most of them according to thecounts.
Depending on the number of training sen-tences, our counts cut-offs vary from 3 to 15.For each language, we held out from training dataa portion of sentences (300, 500 or 1000 depend-ing on the total number of sentences) and trained amodel for up to 20 epochs in the rest of the data.
Weevaluated each model on the held out data for differ-ent number of training epochs, and selected the op-timum point.
Then, we retrained each model on thewhole training set for the selected number of epochs.Table 5 shows the attachment scores obtainedby our system, both unlabeled (UAS) and labeled(LAS).
The first column (GOLD) presents the LASobtained with a perfect scoring function: the loss inaccuracy is related to the projectivity assumption ofour parsing algorithm.
Dutch turns out to be themost non-projective language, with a loss in accu-racy of 5.44%.
In our opinion, the loss in other lan-guages is relatively small, and is not a major limita-tion to achieve a high performance in the task.
Oursystem achieves an overall LAS of 74.72%, withsubstantial variation from one language to another.Turkish, Arabic, Dutch, Slovene and Czech turn outto be the most difficult languages for our system,with accuracies below 70%.
The easiest languageis clearly Japanese, with a LAS of 88.13%, followedby Chinese, Portuguese, Bulgarian and German, allwith LAS above 80%.Table 6 shows the contribution of base feature ex-traction functions.
For four languages, we trainedmodels that increasingly incorporate base functions.It can be shown that all functions contribute to a bet-ter score.
Contextual features (?3) bring the systemto the final order of performance, while distance (?4)and runtime (?)
features still yield substantial im-provements.5 Analysis and ConclusionsIt is difficult to explain the difference in performanceacross languages.
Nevertheless, we have identifiedGOLD UAS LASBulgarian 99.56 88.81 83.30Arabic 99.76 72.65 60.94Chinese 100.0 88.65 83.68Czech 97.78 77.44 68.82Danish 99.18 85.67 79.74Dutch 94.56 71.39 67.25German 98.84 85.90 82.41Japanese 99.16 90.79 88.13Portuguese 98.54 87.76 83.37Slovene 98.38 77.72 68.43Spanish 99.96 80.77 77.16Swedish 99.64 85.54 78.65Turkish 98.41 70.05 58.06Overall 98.68 81.19 74.72Table 5: Results of the system on test data.
GOLD: labeledattachment score using gold scoring functions; the loss in ac-curacy is caused by the projectivity assumption made by theparser.
UAS : unlabeled attachment score.
LAS : labeled at-tachment score, the measure to compare systems in CoNLL-X.Bulgarian is excluded from overall scores.
?1 ?2 ?3 ?4 ?Turkish 33.02 48.00 55.33 57.16 58.06Spanish 12.80 53.80 68.18 74.27 77.16Portuguese 47.10 64.74 80.89 82.89 83.37Japanese 38.78 78.13 86.87 88.27 88.13Table 6: Labeled attachment scores at increasing feature con-figurations.
?1 uses only ?token at the head and modifier.
?2extends ?1 with ?dep.
?3 incorporates context features, namely?tctx at the head and modifier, and ?dctx.
?4 extends ?3 with?dist.
Finally, the final feature extraction function ?
increases?4 with ?runtime.four generic factors that we believe caused the mosterrors across all languages:Size of training sets: the relation between theamount of training data and performance is stronglysupported in learning theory.
We saw the same re-lation in this evaluation: for Turkish, Arabic, andSlovene, languages with limited number of train-ing sentences, our system obtains accuracies below70%.
However, one can not argue that the trainingsize is the only cause of errors: Czech has the largesttraining set, and our accuracy is also below 70%.Modeling large distance dependencies: eventhough we include features to model the distancebetween two dependency words (?dist), our analy-sis indicates that these features fail to capture all theintricacies that exist in large-distance dependencies.Table 7 shows that, for the two languages analyzed,the system performance decreases sharply as the dis-tance between dependency tokens increases.184to root 1 2 3 ?
6 >= 7Spanish 83.04 93.44 86.46 69.97 61.48Portuguese 90.81 96.49 90.79 74.76 69.01Table 7: F?=1 score related to dependency token distance.Modeling context: many attachment decisions, e.g.prepositional attachment, depend on additional con-text outside of the two dependency tokens.
To ad-dress this issue, we have included in our model fea-tures to capture context, both static (?dctx and ?tctx)and dynamic (?runtime).
Nevertheless, our erroranalysis indicates that our model is not rich enoughto capture the context required to address complexdependencies.
All the top 5 focus words with themajority of errors for Spanish and Portuguese ?
?y?,?de?, ?a?, ?en?, and ?que?
for Spanish, and ?em?,?de?, ?a?, ?e?, and ?para?
for Portuguese ?
indicatecomplex dependencies such as prepositional attach-ments or coordinations.Projectivity assumption: Dutch is the languagewith most crossing dependencies in this evaluation,and the accuracy we obtain is below 70%.On the Degree of Lexicalization We conclude theerror analysis of our model with a look at the de-gree of lexicalization in our model.
A quick analy-sis of our model on the test data indicates that only34.80% of the dependencies for Spanish and 42.94%of the dependencies for Portuguese are fully lexical-ized, i.e.
both the head and modifier words appearin the model feature set (see Table 8).
There aretwo reasons that cause our model to be largely un-lexicalized: (a) in order to keep training times rea-sonable we performed heavy filtering of all featuresbased on their frequency, which eliminates manylexicalized features from the final model, and (b)due to the small size of most of the training cor-pora, most lexicalized features simply do not ap-pear in the testing section.
Considering these re-sults, a reasonable question to ask is: how muchare we losing because of this lack of lexical infor-mation?
We give an approximate answer by ana-lyzing the percentage of fully-lexicalized dependen-cies that are correctly parsed by our model.
As-suming that our model scales well, the accuracy onfully-lexicalized dependencies is an indication forthe gain (or loss) to be had from lexicalization.
Ourmodel parses fully-lexicalized dependencies with anFully One token Fullylexicalized unlexicalized unlexicalizedSpanish 34.80% 54.77% 10.43%Portuguese 42.94% 49.26% 7.80%Table 8: Degree of dependency lexicalization.accuracy of 74.81% LAS for Spanish (2.35% lowerthan the overall score) and of 83.77% LAS for Por-tuguese (0.40% higher than the overall score).
Thisanalysis indicates that our model has limited gains(if any) from lexicalization.In order to improve the quality of our dependencyparser we will focus on previously reported issuesthat can be addressed by a parsing model: large-distance dependencies, better modeling of context,and non-projective parsing algorithms.AcknowledgementsThis work was partially funded by the European Union Com-mission (PASCAL - IST-2002-506778) and Spanish Ministryof Science and Technology (TRANGRAM - TIN2004-07925-C03-02).
Mihai Surdeanu was supported by a Ramon y Cajalfellowship of the latter institution.ReferencesS.
Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006.CoNLL-X shared task on multilingual dependency parsing.In Proc.
of the Tenth Conf.
on Computational Natural Lan-guage Learning (CoNLL-X).
SIGNLL.X.
Carreras, Llu?
?s Ma`rquez, and J. Castro.
2005.
Filtering-ranking perceptron learning for partial parsing.
MachineLearning, 1?3(60):41?71.M.
Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, University of Pennsylvania.M.
Collins.
2002.
Discriminative training methods for hiddenmarkov models: Theory and experiments with perceptron al-gorithms.
In Proc.
of EMNLP-2002.J.
Eisner.
1996.
Three new probabilistic models for depen-dency parsing: An exploration.
In Proc.
of the 16th Intern.Conf.
on Computational Linguistics (COLING).J.
Eisner.
2000.
Bilexical grammars and their cubic-time pars-ing algorithms.
In H. C. Bunt and A. Nijholt, editors, NewDevelopments in Natural Language Parsing, pages 29?62.Kluwer Academic Publishers.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Online large-margin training of dependency parsers.
In Proc.
of the 43rdAnnual Meeting of the ACL.185
