The Linguistic ConnectionMartin KayXerox Palo Alto Research CenterandCSLI, StanfordSet theory is the sole foundation of the whole edifice of mathematics, or so I havebeen given to understand.
Sequences are constructed from ordered pairs pairs in afairly obvious way, and ordered pairs result from an artifice that can only cause thelay observer to put his hand on his wallet.
In computing, on the other hand,sequences have always been treated as primitive.
Sets are represented by anarbitrary permutation of their members.
They are sets, and not sequences, only in asmuch as the algorithms that operate on them are expected to produce equivalentresults regardless of the permutation chosen.
Now, I take it that an important effectof connectionism will be to bring computing more into line with mathematics bygiving first-class tatus to sets.This is doubtless good news for the mathematician, for the theoretical computerscientist, and possibly for many others.
But, in the linguist, it seems, on the face of it,to be cause for modified rapture, at best.
Language is probably the most strikingexample of a two dimensional object in this three-dimensional world.
Its mostobvious property is its sequentiality.
But, this has been said before, for example bythose who offered computational l inguists string processing languages like COMITand SNOBOL as the tools most obviously fitting their needs, and sophisticated peopleare no longer beguiled by the argument.
According to the more enlightened view,sequentiality is a very superficial property of language.
By applying the rules ofsyntax, we are able to uncover the richer, more revealing, mult idimensionalstructures that lie behind it, and which are closer to the essence of language.
In fact,there has been much interest in recent imes in languages that are alleged to have amuch more set-like character than English has, in that many of the permutations ofasentence often constitute a logical, or semantic, equivalence class.Linguists have organized their subject in various ways, a famil iar one being bylevel of abstraction.
Phonetics is about the sounds people make when they talk andwhat they do to make them.
Some would have it that there is so little abstraction inphonetics that it should not properly count as part of linguistics at all.
Phonologytalks about how the raw material of phonetics is organized into basic symbols of themost basic kind, about allowable sequences of these symbols, and about the way inwhich the phonetic forms of particular basic symbols are conditioned by the51environment.
This is all unrepentantly sequential, except when the discussion turnsto such things as intonation and stress.Morphology and lexicology are more abstract in the simple sense that they takethe organization that phonology imposes on the primary material as primitive, andimpose a further level of organization on that.
Morphology is about how lexicalitems, themselves represented by sequences of phonologically defined units, arearranged to make words.
It is mostly a matter of sequence, but morphology issometimes conspicuously "nonconcatenative", to use the word that McCarthy (1979,1981) coined in connection with semitic languages.
However, though morphology issometimes not simply a matter of just which sequences of morphemes do make upwords, and with what properties, it is inescapably a matter of how the phonetic orphonological material supplied by morphemes i arranged into a sequence so as toform a word.The next level of abstraction is syntax, the way in which words are collected toform sentences.
Just about all of the multifarious formal theories of grammar thathave been proposed have been particularly strong in the facilities they provided fordescribing the proper ordering of words in a sentence, though it is widely recognizedthat there may be some ethnocentrism in this, for formal linguists have beenoverwhelmingly speakers of languages where word order plays a predominant role.But it was not so in traditional informal grammar, which took Latin as as a model forall languages.
Many formalists are now in search of less strongly sequentialparadigms as they attempt to account for so called free word order andnonconfigurational l nguages.By the time we reach the next level of abstraction, that of semantics, essentiallyno reflection of the ordering of the initial phonetic material remains.
But, by thistime, it is also possible to claim that the territory that falls most clearly within thepurview of linguists has already been traversed.
Linguistics makes contact with thereal world at two points: the sounds that people utter and the meanings that areassociated with them--phonetics and semantics.
At all of the intervening levels ofabstraction, the reflexes of the temporal ordering of the sounds is usually strongly inevidence.If the picture I have painted of language is substantially correct, and if I have notmisunderstood the nature of the connectionist revolution in computing too grossly, itseems that we may have to conclude that the human linguistic faculty, if not humanintelligence at large, have more in common with the yon Neumann machine thanwith the connection machine and that my colleagues, and I will regretfully not bepart of this new adventure.
But now, let us see if we cannot find another side to thecoin.52For all that language gives up its sequentiality grudgingly and emerges into thebrighter set-theoretic world only as its identity is confounded with that ofintelligence at large, it nevertheless remains remarkably context-free.
I say"remarkably" because we know from mathematics that context free languages are asubset--a small subset in some sense--of the theoretically possible languages.
Whatthis means for language users and computational l inguists is that one can analyzeany part of the sequence of phonemes, morphemes, words or whatever, with theexpectation that, if the analysis of the whole string incorporates an analysis of thatpart, then the analysis one has already made will fill the requirement.
Given thatthe subject and the object of the sentence do not overlap, the analysis of each of themcan proceed in parallel.
This is the property of language that makes chart parsers anattractive option.Chart parsers in general, and so-called active chart parsers in particular, arefundamental ly exercises in parallel computing.
If, along with the chart, there isusually a second data structure called the agenda, it is simply to facilitate thesimulation of this parallel architecture on sequential machines.
But what is going onin chart parsing is much better understood if one thinks of each vertex in the chart asan autonomous device responsible for delivering all phrases that begin with the wordat that vertex.
The process of finding these phrases is dependent on similar workgoing on at other vertices only to the extent hat, when phrases are delivered at othervertices, it may become possible to recognize others that begin here.
But therelationships are intrinsic, to use the term in the linguist's pecial sense.
In otherwords, these relationships are not dictated by some general algorithm or external setof principles, but by the obvious requirement that computations that require aparticular piece of information cannot be completed until that information isavailable.Some twenty years ago, when local networks were a relatively new thing, Iharnessed the nocturnal energies of several machines at the Xerox Palo AltoResearch Center to just such a task, more for fun than enlightenment.
Of course, itworked.
Furthermore, if the speed had been multipl ied by a substantial factor, itwould have been quite fast.
The idea behind what I did was simple and obvious.
Anactive edge consisted of a message from one machine to another asking for anyphrases with a certain description that appeared at that other vertex.
An inactiveedge was a phrase already found that enabled a given machine to answer suchrequests.
Each machine kept old requests against he possibility of finding phraseslater with which to amplify its answer to previous requests.
Each machine also had acomplete copy of the grammar so that there could be no contention over access to it.53So, if the sentence to be analyzed was "Brutus ki l led Caesar", three machineswould have been assigned and the talk on the net might have been somewhat likethis:o aob...From Brutus to killed: need a singular, 3rd.
person VP.From killed to Caesar: need a NP.From Caesar to killed: herewith one NP, namely "Caesar", endingwhere the sentence nds.From killed to Brutus: herewith one VP, namely "V(killed)NP(Caesar)", ending where the sentence nds.The Brutus machine is now in a position to deliver an analysis of the wholestring.
The ordering of the work into these three stages is intrinsic.
In part icular thekilled machine cannot honor the request for a VP until information about the NP toits r ight is in.
However, killed does not wait to be asked for a VP to send out itsrequest for a NP.
Each machine goes to work building whatever it can in a bottom upmanner, just in case it may prove useful.
So, if there had been a fourth machine tothe right of'Caesar', then 'Caesar' would have asked it for VP's in the hope of buildingsentences with them, even though no request for sentences was destined to reach itfrom the left.This approach to syntactic analysis falls down because of a property of languagesthat I have not mentioned so far, namely that they all assiduously avoid centerembedding in favor of strongly left- or r ight-branching structures.
It is easy to seethat, if syntactic structures were more or less well balanced trees, the time that myparallel device would require to find a singie analysis of a sentence of n words wouldbe of order log(n).
But, if the most richly developed part of each subtree is almostalways on its r ighthand side, as in English, then the intrinsic ordering of theprocesses will be such as to make this scheme essentially similar to standardsequential ones.
If the language is predominently right recursive, then it will rarelybe possible for a machine to finish its work before all, or almost all, the machines toits right.
The situation is no better for left-recursive languages.One further remark may be in order before I leave the question of chart parsing.Chart parsers exploit the grossly context free nature of natural  languages in adynamic programming scheme that avoids, to a large extent, doing the samecomputation more that once.
The chart is a record of the computations that havebeen attempted, and the results they produced, together with an index that  makes iteasy to check the record before repeating the work.
It does a great deal to speed thebusiness of finding all the structures that a grammar allows for a given sentence.But it is just as bad as a psychological model as it is good as a computational54technique.
If we had charts in our head, we would never be led down the garden path,and we should have no difficulty in reanalyzing the early part of a long sentence,possibly several times, to reconcile it with what occurred much later.
But we do notseem to be able to do this.
The evidence, such as it is, is all to the effect that linguisticproblems are solved on the basis of very local information and that it procedes muchfaster than even the chart model suggests.
The connection machine may be able toprovide a model that accounts for some greater speed, but locality and sequential ityremain.They may be reason to suspect that the most obviously linguistic aspects oflanguage processingmthose that concern phonology, morphology, and syntaxmareeven more sequential even than the best known linguistic theories make them seem.It has often been pointed out that intonation and speech rhythm betray anorganization of utterances into phrases of a different kind than emerges fromconsiderations of syntax and semantics.
It turns out that it is more natural  to pauseat some points in an utterance than at others, but these places are not always atsyntactic boundaries.
So we may have to countenance two different phrasings.Indeed, we may have to go further, because it has also been claimed that there is aninformational, or functional, organization to discourse which does not respect theboundaries of either of the other two that I have mentioned.
In Prague, this is knownas the functional setence perspective and it has to do with the differential t reatmentthat a speaker gives to information he supposes his interlocutor to know already, asagainst he information that he is explicitly offering as new.
These things are poorlyunderstood, but the claim of those who do battle with them is that they are based onessentially sequential, ocal patterns in the text.So far, my attempt o find another side to the coin has failed.
Furthermore,those who know me well may be beginning to suspect that I am talking againstmyself because I have for a long time been singing the song of monotonicity inlinguistics, calling for the banishment of all that is essentially procedural.
Manycurrent linguistic theories attract attention largely for having abandoned erivationsin favor of systems of constraints.
Examples are Lexical Functional Grammar,Generalized Phrase Structure Grammar and its derivatives, and my own FunctionalUnification Grammar.
Government Binding theory seems to me to be moving fast inthe same direction and I suspect hat it would be profitable to formulate a notationalvariant of it in which such procedural notions as "move-a" give way to a static systemof constraints.There are at least two advantages that constraint based systems like these haveover derivation based systems, such as transformational grammar,  at least in itsolder forms.
The first is that it achieves a cleaner and more thoroughgoingseparation of competence from performance, and the other is that it gives f i rst-c lass55Istatus to methods of computing with only partial information.
The second of thesehas also been touted as onr of the strengths of the connectionist apporach.
Thesituation with the first is less clear.
Consider the case of what I will refer togenerically as unification grammar.The view of scientific philosophy that prevails among linguists focuses a lot ofattention on a Utopian situation in which they are called upon to chose between twodescriptively perfect grammars.
They prepare themselves for this challenge bysetting up metrics that will be ready for immediate application when this needarrises.
I take it that, ceteris paribus, a competence grammar will be preferred if itmore readily if it more readily supports ome plausible theory of competence.
In thelong run, it will be even more to be preferred if it supports the right theory ofcompetence.
Now, a competence grammar that is based on a calculus in whichoperations have to be carried out in a specific, very carefully orchestrated way, is lesslikely to have this property than one in which no reliance is placed on carefullyordered sequences of operations.
One might counter that the carefully orderedsequence could be just the one that people in fact follow so that the competencegrammar could could go into immediate service as a performance grammar  withoutsubstantial change.
But this is clearly a forlorn hope if only because the sequence ofopertations that a speaker and a hearer must perform are unlikely to be ordered inthe same way.
The constraint based systems of grammar that have been proposed, onthe other hand, are hospitable to a variety of different processing strategies.
Thefreedom that this gives to performance theorists also extends to computationall inguists with more practical aims in mind; they are much freer to bring theiringenuity and expertise as computer scientists to bear.The constraint based grammars that are now in vogue are based on unificationand, to a lesser extent, on related operations, such as generaliztion and subsumption.These are logical operations, in a strong sense of the word, as evidenced by the factthat unification is also a basic operation of logic programming in general,  and Prologin particular.
Logic programming, and computation with constraint-basedgrammars rests heavily on implementing the notion of a logical variable, as opposedwhat programmers have usually called "variables", and which are really names ofstorage locations.
The values of logical variables, unlike the contents of storagelocations, do not change over time, at least on one path through a nondeterministicprocess.
Unification is an operation as a result of which it sometimes comes to lightthat sets of two or more variables refer to the same thing.
Henceforward, anyconstraints imposed on the value of one of the variables, as a result of its appearancein one expression, must be consonant with those imposed upon other membrs of theset thorugh their appearance in other expressions.
If these conditions are violated atthe outset, the unification operation does not go through.
I do not knowwhat  would56be involved in modeling this situation in something like the connection machine, butsuch uninformed speculations as I have allowed myself on the subject, together withoccasional remarks from others who know better than I, suggest hat this is notentirely in the connectionist pirit.The skepticism I have expressed here on the matter of connectionism inlinguistics is based to some extent on facts and to some extent on speculation onmatters where I believe the evidence to be inconclusive.
If I were wrong about most ofthe matters in the second class, it may be that the role of connectionism in linguisticscould be very great.
It seems to me that our way is clear.
Arguments along the linesof those I have outlined should not be used against he attempt o apply connectionistideas to linguistics.
Quite the contrary.
Connectionism hsould be pursued vigoroulsyin the hope that, if nothing else, it will shed light on these areas of uncertainty, mostof which have resisted attack for far too long a time.BibliographyMcCarthy, J J.
(1979).
Formal problems in Semitic Phonology and Morphology.Doctoral Dissertation, MIT, Cambridge Massachussetts.McCarthy, J J.
(1981).
"A Prosodic Theory of Nonconcatenative Morphology".Linguistic Inquiry, 12.3.57
