Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1077?1086,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsToward Abstractive Summarization Using Semantic RepresentationsFei Liu Jeffrey Flanigan Sam Thomson Norman Sadeh Noah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{feiliu, jflanigan, sthomson, sadeh, nasmith}@cs.cmu.eduAbstractWe present a novel abstractive summarizationframework that draws on the recent develop-ment of a treebank for the Abstract MeaningRepresentation (AMR).
In this framework, thesource text is parsed to a set of AMR graphs,the graphs are transformed into a summarygraph, and then text is generated from thesummary graph.
We focus on the graph-to-graph transformation that reduces the sourcesemantic graph into a summary graph, mak-ing use of an existing AMR parser and assum-ing the eventual availability of an AMR-to-text generator.
The framework is data-driven,trainable, and not specifically designed fora particular domain.
Experiments on gold-standard AMR annotations and system parsesshow promising results.
Code is available at:https://github.com/summarization1 IntroductionAbstractive summarization is an elusive technolog-ical capability in which textual summaries of con-tent are generated de novo.
Demand is on the risefor high-quality summaries not just for lengthy texts(e.g., books; Bamman and Smith, 2013) and textsknown to be prohibitively difficult for people to un-derstand (e.g., website privacy policies; Sadeh et al,2013), but also for non-textual media (e.g., videosand image collections; Kim et al, 2014; Kuznetsovaet al, 2014; Zhao and Xing, 2014), where extractiveand compressive summarization techniques simplydo not suffice.
We believe that the challenge of ab-stractive summarization deserves renewed attentionand propose that recent developments in semanticanalysis have an important role to play.We conduct the first study exploring the feasi-bility of an abstractive summarization system basedon transformations of semantic representations suchas the Abstract Meaning Representation (AMR; Ba-narescu et al, 2013).
Example sentences and theirAMR graphs are shown in Fig.
1.
AMR has muchin common with earlier formalisms (Kasper, 1989;Dorr et al, 1998); today an annotated corpus com-prised of over 20,000 AMR-analyzed English sen-tences (Knight et al, 2014) and an automatic AMRparser (JAMR; Flanigan et al, 2014) are available.In our framework, summarization consists ofthree steps illustrated in Fig.
1: (1) parsing the in-put sentences to individual AMR graphs, (2) com-bining and transforming those graphs into a singlesummary AMR graph, and (3) generating text fromthe summary graph.
This paper focuses on step 2,treating it as a structured prediction problem.
Weassume text documents as input1and use JAMR forstep 1.
We use a simple method to read a bag ofwords off the summary graph, allowing evaluationwith ROUGE-1, and leave full text generation fromAMR (step 3) to future work.The graph summarizer, described in ?4, firstmerges AMR graphs for each input sentence througha concept merging step, in which coreferent nodes ofthe graphs are merged; a sentence conjunction step,which connects the root of each sentence?s AMRgraph to a dummy ?ROOT?
node; and an optional1In principle, the framework could be applied to other in-puts, such as image collections, if AMR parsers became avail-able for them.1077posslocationnameARG1ARG0ARG0 ARG1possARG0-ofARG1see-01dogi?Joe?nameperson run-02chase-01dog catSentence A:  I saw Joe?s dog, which was running in the garden.Sentence B:  The dog was chasing a cat.gardennameop1locationSummary:  Joe?s dog was chasing a cat in the garden.dogchase-01garden catARG0?Joe?namepersonop1132Figure 1: A toy example.
Sentences are parsed into indi-vidual AMR graphs in step 1; step 2 conducts graph trans-formation that produces a single summary AMR graph;text is generated from the summary graph in step 3.graph expansion step, where additional edges areadded to create a fully dense graph on the sentence-level.
These steps result in a single connected sourcegraph.
A subset of the nodes and arcs from thesource graph are then selected for inclusion in thesummary graph.
Ideally this is a condensed repre-sentation of the most salient semantic content fromthe source.We briefly review AMR and JAMR (?2), thenpresent the dataset used in this paper (?3).
The mainalgorithm is presented in ?4, and we discuss our sim-ple generation step in ?5.
Our experiments (?6) mea-sure the intrinsic quality of the graph transformationalgorithm as well as the quality of the terms selectedfor the summary (using ROUGE-1).
We explorevariations on the transformation and the learning al-gorithm, and show oracle upper bounds of variouskinds.2 Background: Abstract MeaningRepresentation and JAMRAMR provides a whole-sentence semantic repre-sentation, represented as a rooted, directed, acyclicgraph (Fig.
1).
Nodes of an AMR graph are labeledwith concepts, and edges are labeled with relations.Concepts can be English words (?dog?
), PropBankevent predicates (?chase-01,?
?run-02?
), or specialkeywords (?person?).
For example, ?chase-01?
rep-resents a PropBank roleset that corresponds to thefirst sense of ?chase?.
According to Banarescu et al(2013), AMR uses approximately 100 relations.
Therolesets and core semantic relations (e.g., ARG0 toARG5) are adopted from the PropBank annotationsin OntoNotes (Hovy et al, 2006).
Other semantic re-lations include ?location,?
?mode,?
?name,?
?time,?and ?topic.?
The AMR guidelines2provide moredetailed descriptions.
Banarescu et al (2013) de-scribe AMR Bank, a 20,341-sentence corpus anno-tated with AMR by experts.Step 1 of our framework converts input documentsentences into AMR graphs.
We use a statistical se-mantic parser, JAMR (Flanigan et al, 2014), whichwas trained on AMR Bank.
JAMR?s current perfor-mance on our test dataset is 63% F -score.3We willanalyze the effect of AMR parsing errors by com-paring JAMR output with gold-standard annotationsof input sentences in the experiments (?6).In addition to predicting AMR graphs for eachsentence, JAMR provides alignments between spansof words in the source sentence and fragments ofits predicted graph.
For example, a graph fragmentheaded by ?date-entity?
could be aligned to the to-kens ?April 8, 2002.?
We use these alignments inour simple text generation module (step 3; ?5).3 DatasetTo build and evaluate our framework, we requirea dataset that includes inputs and summaries, eachwith gold-standard AMR annotations.4This allowsus to use a statistical model for step 2 (graph summa-rization) and to separate its errors from those in step1 (AMR parsing), which is important in determiningwhether this approach is worth further investment.Fortunately, the ?proxy report?
section of theAMR Bank (Knight et al, 2014) suits our needs.
A2http://www.isi.edu/?ulf/amr/help/amr-guidelines.pdf3AMR parse quality is evaluated using smatch (Cai andKnight, 2013), which measures the accuracy of concept and re-lation predictions.
JAMR was trained on the in-domain trainingportion of LDC2014T12 for our experiments.4Traditional multi-document summarization datasets, suchas the ones used in DUC and TAC competitions, do not havegold-standard AMR annotations.1078# Docs.Ave.
# Sents.
Source GraphSumm.
Doc.
Nodes Edges ExpandTrain 298 1.5 17.5 127 188 2,670Dev.
35 1.4 19.2 143 220 3,203Test 33 1.4 20.5 162 255 4,002Table 1: Statistics of our dataset.
?Expand?
shows thenumber of edges after performing graph expansion.
Thenumbers are averaged across all documents in the split.We use the official split, dropping one training documentfor which no summary sentences were annotated.proxy report is created by annotators based on a sin-gle newswire article, selected from the English Gi-gaword corpus.
The report header contains metadataabout date, country, topic, and a short summary.
Thereport body is generated by editing or rewriting thecontent of the newswire article to approximate thestyle of an analyst report.
Hence this is a single doc-ument summarization task.
All sentences are pairedwith gold-standard AMR annotations.
Table 1 pro-vides an overview of our dataset.4 Graph SummarizationGiven AMR graphs for all of the sentences in the in-put (step 1), graph summarization transforms theminto a single summary AMR graph (step 2).
This isaccomplished in two stages: source graph construc-tion (?4.1); and subgraph prediction (?4.2).4.1 Source Graph ConstructionThe ?source graph?
is a single graph constructed us-ing the individual sentences?
AMR graphs by merg-ing identical concepts.
In the AMR formalism, anentity or event is canonicalized and represented bya single graph fragment, regardless of how manytimes it is referred to in the sentence.
This princi-ple can be extended to multiple sentences, ideallyresulting in a source graph with no redundancy.
Be-cause repeated mentions of a concept in the inputcan signal its importance, we will later encode thefrequency of mentions as a feature used in subgraphprediction.Concept merging involves collapsing certaingraph fragments into a single concept, then mergingall concepts that have the same label.
We collapsethe graph fragments that are headed by either a date-entity (?date-entity?)
or a named entity (?name?
), ifdayyeardate-entity?2002??4??8?monthname?Joe?namepersonop1date-entity::year::?2002?::month::?4?
::day::?8?person::name::name::op1::?Joe?Figure 2: Graph fragments are collapsed into a singleconcept and assigned a new concept label.the fragment is a flat structure.
A collapsed namedentity is further combined with its parent (e.g., ?per-son?)
into one concept node if it is the only childof the parent.
Two such graph fragments are illus-trated in Fig.
2.
We choose named and date entityconcepts since they appear frequently, but most of-ten refer to different entities (e.g., ?April 8, 2002?vs.
?Nov.
17?).
No further collapsing is done.
Acollapsed graph fragment is assigned a new label byconcatenating the consisting concept and edge la-bels.
Each fragment that is collapsed into a new con-cept node can then only be merged with other iden-tical fragments.
This process won?t recognize coref-erent concepts like ?Barack Obama?
= ?Obama?
and?say-01?
= ?report-01,?
but future work may incor-porate both entity coreference resolution and eventcoreference resolution, as concept nodes can repre-sent either.Due to the concept merging step, a pair of con-cepts may now have multiple labeled edges betweenthem.
We merge all such edges between a given pairof concepts into a single unlabeled edge.
We remem-ber the two most common labels in such a group,which are used in the edge ?Label?
feature (Table 3).To ensure that the source graph is connected, weadd a new ?ROOT?
node and connect it to every con-cept that was originally the root of a sentence graph(see Fig.
3).
When we apply this procedure to thedocuments in our dataset (?3), source graphs contain144 nodes and 221 edges on average.We investigated how well these automaticallyconstructed source graphs cover the gold-standardsummary graphs produced by AMR annotators.
Ide-ally, a source graph should cover all of the gold-standard edges, so that summarization can be ac-complished by selecting a subgraph of the source1079Graph Expansionsee-01doginameperson run-02chase-01dog catgardenROOT?Joe?MergingCollapsing21Sentence A:  I saw Joe?s dog, which was running in the garden.Sentence B:  The dog was chasing a cat.Figure 3: A source graph formed from two sentenceAMR graphs.
Concept collapsing, merging, and graphexpansion are demonstrated.
Edges are unlabeled.
A?ROOT?
node is added to ensure connectivity.
(1) and(2) are among edges added through the optional expan-sion step, corresponding to sentence- and document-levelexpansion, respectively.
Concept nodes included in thesummary graph are shaded.Summary Edge Coverage (%)ExpandLabeled Unlabeled Sent.
Doc.Train 64.8 67.0 75.5 84.6Dev.
77.3 78.6 85.4 91.8Test 63.0 64.7 75.0 83.3Table 2: Percentage of summary edges that can be cov-ered by an automatically constructed source graph.graph (?4.2).
In Table 2, columns one and two re-port labeled and unlabeled edge coverage.
?Unla-beled?
counts edges as matching if both the sourceand destination concepts have identical labels, butignores the edge label.In order to improve edge coverage, we exploreexpanding the source graph by adding every possi-ble edge between every pair of concepts within thesame sentence.
We also explored adding every pos-sible edge between every pair of concepts in the en-tire source graph.
Edges that are newly introducedduring expansion receive a default label ?null?.
Wereport unlabeled edge coverage in Table 2, columnsthree and four, respectively.
Subgraph predictionbecame infeasable with the document-level expan-sion, so we conducted our experiments using onlysentence-level expansion.
Sentence-level graph ex-pansion increases the average number of edges bya factor of 15, to 3,292.
Fig.
3 illustrates the moti-vation.
Document-level expansion covers the gold-standard summary edge ?chase-01?
?
?garden,?yet the expansion is computationally prohibitive;sentence-level expansion adds an edge ?dog?
??garden,?
which enables the prediction of a struc-ture with similar semantic meaning: ?Joe?s dog wasin the garden chasing a cat.
?4.2 Subgraph PredictionWe pose the selection of a summary subgraph fromthe source graph as a structured prediction prob-lem that trades off among including important in-formation without altering its meaning, maintain-ing brevity, and producing fluent language (Nenkovaand McKeown, 2011).
We incorporate these con-cerns in the form of features and constraints in thestatistical model for subgraph selection.Let G = (V,E) denote the merged source graph,where each node v ?
V represents a unique con-cept and each directed edge e ?
E connects twoconcepts.
G is a connected, directed, node-labeledgraph.
Edges in this graph are unlabeled, and edgelabels are not predicted during subgraph selection.We seek to maximize a score that factorizes overgraph nodes and edges that are included in the sum-mary graph.
For subgraph (V?, E?
):score(V?, E?;?,?)
=?v?V??>f(v)+?e?E?
?>g(e)(1)where f(v) and g(e) are the feature representationsof node v and edge e, respectively.
We describe nodeand edge features in Table 3. ?
and ?
are vectors ofempirically estimated coefficients in a linear model.We next formulate the selection of the subgraphusing integer linear programming (ILP; ?4.2.1) anddescribe supervised learning for the parameters (co-efficients) from a collection of source graphs pairedwith summary graphs (?4.2.2).4.2.1 DecodingWe cast decoding as an ILP whose constraints en-sure that the output forms a connected subcompo-nent of the source graph.
We index source graphconcept nodes by i and j, giving the ?ROOT?
node1080Node Concept Identity feature for concept labelFeatures Freq Concept freq in the input sentence set; one binary feature defined for each frequency threshold ?
= 0/1/2/5/10Depth Average and smallest depth of node to the root of the sentence graph; binarized using 5 depth thresholdsPosition Average and foremost position of sentences containing the concept; binarized using 5 position thresholdsSpan Average and longest word span of concept; binarized using 5 length thresholds; word spans obtained from JAMREntity Two binary features indicating whether the concept is a named entity/date entity or notBias Bias term, 1 for any nodeEdge Label First and second most frequent edge labels between concepts; relative freq of each label, binarized by 3 thresholdsFeatures Freq Edge frequency (w/o label, non-expanded edges) in the document sentences; binarized using 5 frequency thresholdsPosition Average and foremost position of sentences containing the edge (without label); binarized using 5 position thresholdsNodes Node features extracted from the source and target nodes (all above node features except the bias term)IsExpanded A binary feature indicating the edge is due to graph expansion or not; edge freq (w/o label, all occurrences)Bias Bias term, 1 for any edgeTable 3: Node and edge features (all binarized).index 0.
Let N be the number of nodes in the graph.Let viand ei,jbe binary variables.
viis 1 iff sourcenode i is included; ei,jis 1 iff the directed edge fromnode i to node j is included.The ILP objective to be maximized is Equation 1,rewritten here in the present notation:N?i=1vi?>f(i)?
??
?node score+?
(i,j)?Eei,j?>g(i, j)?
??
?edge score(2)Note that this objective is linear in {vi, ei,j}i,jandthat features and coefficients can be folded into nodeand edge scores and treated as constants during de-coding.Constraints are required to ensure that the selectednodes and edges form a valid graph.
In particular, ifan edge (i, j) is selected (ei,jtakes value of 1), thenboth its endpoints i, j must be included:vi?
ei,j?
0, vj?
ei,j?
0, ?i, j ?
N (3)Connectivity is enforced using a set of single-commodity flow variables fi,j, each taking a non-negative integral value, representing the flow fromnode i to j.
The root node sends out up toN units offlow, one to reach each included node (Equation 4).Each included node consumes one unit of flow, re-flected as the difference between incoming and out-going flow (Equation 5).
Flow may only be sent overan edge if the edge is included (Equation 6).?if0,i?
?ivi= 0, (4)?ifi,j??kfj,k?
vj= 0, ?j ?
N, (5)N ?
ei,j?
fi,j?
0, ?i, j ?
N. (6)The AMR representation allows graph reentran-cies (concept nodes having multiple parents), yetreentrancies are rare; about 5% of edges are re-entrancies in our dataset.
In this preliminary studywe force the summary graph to be tree-structured,requiring that there is at most one incoming edge foreach node:?jei,j?
1, ?j ?
N. (7)Interestingly, the formulation so far equates toan ILP for solving the prize-collecting Steiner treeproblem (PCST; Segev, 1987), which is known tobe NP-complete (Karp, 1972).
Our ILP formula-tion is modified from that of Ljubi?c et al (2006).Flow-based constraints for tree structures have alsopreviously been used in NLP for dependency pars-ing (Martins et al, 2009) and sentence compres-sion (Thadani and McKeown, 2013).
In our exper-iments, we use an exact ILP solver,5though manyapproximate methods are available.Finally, an optional constraint can be used to fixthe size of the summary graph (measured by thenumber of edges) to L:?i?jei,j= L (8)The performance of summarization systems dependsstrongly on their compression rate, so systems areonly directly comparable when their compressionrates are similar (Napoles et al, 2011).
L is suppliedto the system to control summary graph size.5http://www.gurobi.com10814.2.2 Parameter EstimationGiven a collection of input and output pairs (here,source graphs and summary graphs), a natural start-ing place for learning the coefficients ?
and ?
isthe structured perceptron (Collins, 2002), which iseasy to implement and often performs well.
Alterna-tively, incorporating factored cost functions througha structured hinge loss leads to a structured supportvector machine (SVM; Taskar et al, 2004) whichcan be learned with a very similar stochastic opti-mization algorithm.
In our scenario, however, thegold-standard summary graph may not actually bea subset of the source graph.
In machine transla-tion, ramp loss has been found to work well in situ-ations where the gold-standard output may not evenbe in the hypothesis space of the model (Gimpel andSmith, 2012).
The structured perceptron, hinge, andramp losses are compared in Table 4.We explore learning by minimizing each of theperceptron, hinge, and ramp losses, each optimizedusing Adagrad (Duchi et al, 2011), a stochastic op-timization procedure.
Let ?
be one model parameter(coefficient from ?
or ?).
Let g(t)be the subgradi-ent of the loss on the instance considered on the tthiteration with respect to ?.
Given an initial step size?, the update for ?
on iteration t is:?(t+1)?
?(t)????t?=1(g(?
))2g(t)(9)5 GenerationGeneration from AMR-like representations has re-ceived some attention, e.g., by Langkilde and Knight(1998) who described a statistical method.
Thoughwe know of work in progress driven by the goal ofmachine translation using AMR, there is currentlyno system available.We therefore use a heuristic approach to gener-ate a bag of words.
Given a predicted subgraph, asystem summary is created by finding the most fre-quently aligned word span for each concept node.
(Recall that the JAMR parser provides these align-ments; ?2).
The words in the resulting spans aregenerated in no particular order.
While this isnot a natural language summary, it is suitable forunigram-based summarization evaluation methodslike ROUGE-1.6 ExperimentsIn Table 5, we report the performance of subgraphprediction and end-to-end summarization on the testset, using gold-standard and automatic AMR parsesfor the input.
Gold-standard AMR annotations areused for model training in all conditions.
Duringtesting, we apply the trained model to source graphsconstructed using either gold-standard or JAMRparses.
In all of these experiments, we use the num-ber of edges in the gold-standard summary graph tofix the number of edges in the predicted subgraph,allowing direct comparison across conditions.Subgraph prediction is evaluated against the gold-standard AMR graphs on summaries.
We report pre-cision, recall, and F1for nodes, and F1for edges.6Oracle results for the subgraph prediction stageare obtained using the ILP decoder to minimize thecost of the output graph, given the gold-standard.We assign wrong nodes and edges a score of ?1,correct nodes and edges a score of 0, then decodewith the same structural constraints as in subgraphprediction.
The resulting graph is the best summarygraph in the hypothesis space of our model, andprovides an upper bound on performance achiev-able within our framework.
Oracle performance onnode prediction is in the range of 80% when usinggold-standard AMR annotations, and 70% when us-ing JAMR output.
Edge prediction has lower perfor-mance, yielding 52.2% for gold-standard and 31.1%for JAMR parses.
When graph expansion was ap-plied, the numbers increased to 64% and 46.7%, re-spectively.
The uncovered summary edge (i.e., thosenot covered by source graph) is a major source forlow recall values on edge prediction (see Table 2);graph expansion slightly alleviates this issue.Summarization is evaluated by comparing sys-tem summaries against reference summaries, usingROUGE-1 scores (Lin, 2004)7.
System summariesare generated using the heuristic approach presentedin ?5: given a predicted subgraph, the approach findsthe most frequently aligned word span for each con-cept node, and then puts them together as a bag ofwords.
ROUGE-1 is particularly usefully for eval-6Precision, recall, and F1are equal since the number ofedges is fixed.7ROUGE version 1.5.5 with options ?-e data -n 4 -m -2 4 -u-c 95 -r 1000 -f A -p 0.5 -t 0 -a -x?1082Structured perceptron loss: ?score(G?)
+ maxGscore(G)Structured hinge loss: ?score(G?)
+ maxG(score(G) + cost(G;G?
))Structured ramp loss: ?maxG(score(G)?
cost(G;G?))
+ maxG(score(G) + cost(G;G?
))Table 4: Loss functions minimized in parameter estimation.
G?denotes the gold-standard summary graph.
score(?
)is as defined in Equation 1.
cost(G;G?)
penalizes each vertex or edge in G ?G?\ (G ?G?).
Since cost factors justlike the scoring function, each max operation can be accomplished using a variant of ILP decoding (?4.2.1) in whichthe cost is incorporated into the linear objective while the constraints remain the same.Subgraph Prediction SummarizationNodes Edges ROUGE-1P (%) R (%) F (%) F (%) P (%) R (%) F (%)gold- Perceptron 39.6 46.1 42.6 24.7 41.4 27.1 32.3standard Hinge 41.2 47.9 44.2 26.4 42.6 28.3 33.5parses Ramp 54.7 63.5 58.7 39.0 51.9 39.0 44.3Ramp + Expand 53.0 61.3 56.8 36.1 50.4 37.4 42.8Oracle 75.8 86.4 80.7 52.289.1 52.8 65.8Oracle + Expand 78.9 90.1 83.9 64.0JAMR Perceptron 42.2 48.9 45.2 14.5 46.1 35.0 39.5parses Hinge 41.7 48.3 44.7 15.8 44.9 33.6 38.2Ramp 48.1 55.6 51.5 20.0 50.6 40.0 44.4Ramp + Expand 47.5 54.6 50.7 19.0 51.2 40.0 44.7Oracle 64.1 74.8 68.9 31.187.5 43.7 57.8Oracle + Expand 66.9 76.4 71.2 46.7Table 5: Subgraph prediction and summarization (to bag of words) results on test set.
Gold-standard AMR annotationsare used for model training in all conditions.
?+ Expand?
means the result is obtained using source graph withexpansion; edge performance is measured ignoring labels.uating such less well-formed summaries, such asthose generated from speech transcripts (Liu andLiu, 2013).Oracle summaries are produced by taking thegold-standard AMR parses of the reference sum-mary, obtaining the most frequently aligned wordspan for each unique concept node using the JAMRaligner (?2), and then generating a bag of wordssummary.
Evaluation of oracle summaries is per-formed in the same manner as for system sum-maries.
The above process does not involve graphexpansion, so summarization performance is thesame for the two conditions ?Oracle?
and ?Oracle+ Expand.
?We find that JAMR parses are a large source ofdegradation of edge prediction performance, and asmaller but still significant source of degradationfor concept prediction.
Surprisingly, using JAMRparses leads to slightly improved ROUGE-1 scores.Keep in mind, though, that under our bag-of-wordsgenerator, ROUGE-1 scores only depend on conceptprediction and are unaffected by edge prediction.The oracle summarization results, 65.8% and 57.8%F1scores for gold-standard and JAMR parses, re-spectively, further suggest that improved graph sum-marization models (step 2) might benefit from futureimprovements in AMR parsing (step 1).Across all conditions and both evaluations, wefind that incorporating a cost-aware loss function(hinge vs. perceptron) has little effect, but that us-ing ramp loss leads to substantial gains.In Table 5, we show detailed results with andwithout graph expansion.
?+ Expand?
means the re-sults are obtained using the expanded source graph.We find that graph expansion only marginally affectssystem performance.
Graph expansion slightly hurtsthe system performance on edge prediction.
For ex-ample, using ramp loss with JAMR parser as input,we obtained 50.7% and 19.0% for node and edgeprediction with graph expansion; 51.5% and 20.0%1083without edge expansion.
On the other hand, it in-creases the oracle performance by a large margin.This suggests that with more training data, or a moresophisticated model that is able to better discrimi-nate among the enlarged output space, graph expan-sion still has promise to be helpful.7 Related and Future WorkAccording to Dang and Owczarzak (2008), the ma-jority of competitive summarization systems are ex-tractive, selecting representative sentences from in-put documents and concatenating them to form asummary.
This is often combined with sentencecompression, allowing more sentences to be in-cluded within a budget.
ILPs and approximationshave been used to encode compression and extrac-tion (McDonald, 2007; Martins and Smith, 2009;Gillick and Favre, 2009; Berg-Kirkpatrick et al,2011; Almeida and Martins, 2013; Li et al, 2014).Other decoding approaches have included a greedymethod exploiting submodularity (Lin and Bilmes,2010), document reconstruction (He et al, 2012),and graph cuts (Qian and Liu, 2013), among others.Previous work on abstractive summarization hasexplored user studies that compare extractive withNLG-based abstractive summarization (Careniniand Cheung, 2008).
Ganesan et al (2010) pro-pose to construct summary sentences by repeatedlysearching the highest scored graph paths.
(Gerani etal., 2014) generate abstractive summaries by modi-fying discourse parse trees.
Our work is similar inspirit to Cheung and Penn (2014), which splices andrecombines dependency parse trees to produce ab-stractive summaries.
In contrast, our work operateson semantic graphs, taking advantage of the recentlydeveloped AMR Bank.Also related to our work are graph-based summa-rization methods (Vanderwende et al, 2004; Erkanand Radev, 2004; Mihalcea and Tarau, 2004).
Van-derwende et al (2004) transform input to logi-cal forms, score nodes using PageRank, and growthe graph from high-value nodes using heuristics.In Erkan and Radev (2004) and Mihalcea and Ta-rau (2004), the graph connects surface terms thatco-occur.
In both cases, the graphs are constructedbased on surface text; it is not a representation ofpropositional semantics like AMR.
However, futurework might explore similar graph-based calculationsto contribute features for subgraph selection in ourframework.Our constructed source graph can easily reachten times or more of the size of a sentence depen-dency graph.
Thus more efficient graph decodingalgorithms, e.g., based on Lagrangian relaxation orapproximate algorithms, may be explored in futurework.
Other future directions may include jointlyperforming subgraph and edge label prediction; ex-ploring a full-fledged pipeline that consists of an au-tomatic AMR parser, a graph-to-graph summarizer,and a AMR-to-text generator; and devising an eval-uation metric that is better suited to abstractive sum-marization.Many domains stand to eventually benefit fromsummarization.
These include books, audio/videosegments, and legal texts.8 ConclusionWe have introduced a statistical abstractive summa-rization framework driven by the Abstract MeaningRepresentation.
The centerpiece of the approach isa structured prediction algorithm that transforms se-mantic graphs of the input into a single summary se-mantic graph.
Experiments show the approach to bepromising and suggest directions for future research.AcknowledgmentsThe authors thank three anonymous reviewers fortheir insightful input.
We are grateful to NathanSchneider, Kevin Gimpel, Sasha Rush, and the ARKgroup for valuable discussions.
The research wassupported by NSF grant SaTC-1330596, DARPAgrant FA8750-12-2-0342 funded under the DEFTprogram, the U. S. Army Research Laboratory andthe U. S. Army Research Office under contract/grantnumber W911NF-10-1-0533, and by IARPA viaDoI/NBC contract number D12PC00337.
The viewsand conclusions contained herein are those of the au-thors and should not be interpreted as necessarilyrepresenting the official policies or endorsements,either expressed or implied, of the sponsors.ReferencesMiguel B. Almeida and Andre F. T. Martins.
2013.
Fastand robust compressive summarization with dual de-1084composition and multi-task learning.
In Proceedingsof ACL.David Bamman and Noah A. Smith.
2013.
New align-ment methods for discriminative book summarization.In arXiv:1305.1319.Laura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
2013.
Abstract meaning representation forsembanking.
In Proceedings of Linguistic AnnotationWorkshop.Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of ACL.Shu Cai and Kevin Knight.
2013.
Smatch: an evaluationmetric for semantic feature structures.
In Proceedingsof ACL.Giuseppe Carenini and Jackie Chi Kit Cheung.
2008.Extractive vs. NLG-based abstractive summarizationof evaluative text: The effect of corpus controversial-ity.
In Proceedings of the Fifth International NaturalLanguage Generation Conference (INLG).Jackie Chi Kit Cheung and Gerald Penn.
2014.
Unsu-pervised sentence enhancement for automatic summa-rization.
In Proceedings of EMNLP.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP.Hoa Trang Dang and Karolina Owczarzak.
2008.Overview of the TAC 2008 update summarization task.In Proceedings of Text Analysis Conference (TAC).Bonnie Dorr, Nizar Habash, and David Traum.
1998.A thematic hierarchy for efficient generation fromlexical-conceptual structure.
In David Farwell, Lau-rie Gerber, and Eduard Hovy, editors, Machine Trans-lation and the Information Soup: Proceedings ofthe Third Conference of the Association for MachineTranslation in the Americas, Lecture Notes in Com-puter Science.
Springer.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
Journal of Machine LearningResearch.G?unes Erkan and Dragomir R. Radev.
2004.
LexRank:Graph-based lexical centrality as salience in text sum-marization.
Journal of Artificial Intelligence Re-search.Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, ChrisDyer, and Noah A. Smith.
2014.
A discriminativegraph-based parser for the abstract meaning represen-tation.
In Proceedings of ACL.Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.2010.
Opinosis: A graph-based approach to abstrac-tive summarization of highly redundant opinions.
InProceedings of COLING.Shima Gerani, Yashar Mehdad, Giuseppe Carenini, Ray-mond T. Ng, and Bita Nejat.
2014.
Abstractive sum-marization of product reviews using discourse struc-ture.
In Proceedings of EMNLP.Dan Gillick and Benoit Favre.
2009.
A scalable globalmodel for summarization.
In Proceedings of theNAACL Workshop on Integer Linear Programming forNatural Langauge Processing.Kevin Gimpel and Noah A. Smith.
2012.
Structuredramp loss minimization for machine translation.
InProceedings of NAACL-HLT.Zhanying He, Chun Chen, Jiajun Bu, Can Wang, LijunZhang, Deng Cai, and Xiaofei He.
2012.
Documentsummarization based on data reconstruction.
In Pro-ceedings of AAAI.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:The 90% solution.
In Proceedings of NAACL.Richard M. Karp.
1972.
Reducibility Among Combina-torial Problems.
In Complexity of Computer Compu-tations, pages 85?103.
Springer US.Robert T. Kasper.
1989.
A flexible interface for linkingapplications to Penman?s sentence generator.
In Pro-ceedings of the DARPA Speech and Natural LanguageWorkshop.Gunhee Kim, Leonid Sigal, and Eric P. Xing.
2014.
Jointsummarization of large-scale collections of web im-ages and videos for storyline reconstruction.
In Pro-ceedings of CVPR.Kevin Knight, Laura Baranescu, Claire Bonial, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, DanielMarcu, Martha Palmer, and Nathan Schneider.
2014.Abstract meaning representation (AMR) annotationrelease 1.0 LDC2014T12.
Web Download.
Philadel-phia: Linguistic Data Consortium.Polina Kuznetsova, Vicente Ordonez, Tamara L. Berg,and Yejin Choi.
2014.
TREETALK: Composition andcompression of trees for image descriptions.
Transac-tions of ACL.Irene Langkilde and Kevin Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InProceedings of COLING.Chen Li, Yang Liu, Fei Liu, Lin Zhao, and Fuliang Weng.2014.
Improving multi-documents summarization bysentence compression based on expanded constituentparse tree.
In Proceedings of EMNLP.Hui Lin and Jeff Bilmes.
2010.
Multi-document sum-marization via budgeted maximization of submodularfunctions.
In Proceedings of NAACL.1085Chin-Yew Lin.
2004.
ROUGE: a package for auto-matic evaluation of summaries.
In Proceedings of ACLWorkshop on Text Summarization Branches Out.Fei Liu and Yang Liu.
2013.
Towards abstractive speechsummarization: Exploring unsupervised and super-vised approaches for spoken utterance compression.IEEE Transactions on Audio, Speech, and LanguageProcessing.Ivana Ljubi?c, Ren?e Weiskircher, Ulrich Pferschy, Gun-nar W. Klau, Petra Mutzel, and Matteo Fischetti.
2006.An Algorithmic Framework for the Exact Solution ofthe Prize-Collecting Steiner Tree Problem.
In Mathe-matical Progamming, Series B.Andre F. T. Martins and Noah A. Smith.
2009.
Summa-rization with a joint model for sentence extraction andcompression.
In Proceedings of the ACL Workshopon Integer Linear Programming for Natural LanguageProcessing.Andre F. T. Martins, Noah A. Smith, and Eric P. Xing.2009.
Concise integer linear programming formula-tions for dependency parsing.
In Proceedings of ACL.Ryan McDonald.
2007.
A study of global inference al-gorithms in multi-document summarization.
In Pro-ceedings of ECIR.Rada Mihalcea and Paul Tarau.
2004.
TextRank: Bring-ing order into text.
In Proceedings of EMNLP.Courtney Napoles, Benjamin Van Durme, and ChrisCallison-Burch.
2011.
Evaluating Sentence Com-pression: Pitfalls and Suggested Remedies.
In Pro-ceedings of the Workshop on Monolingual Text-To-TextGeneration, MTTG ?11, pages 91?97, Stroudsburg,PA, USA.
Association for Computational Linguistics.Ani Nenkova and Kathleen McKeown.
2011.
Automaticsummarization.
Foundations and Trends in Informa-tion Retrieval.Xian Qian and Yang Liu.
2013.
Fast joint compressionand summarization via graph cuts.
In Proceedings ofEMNLP.Norman Sadeh, Alessandro Acquisti, Travis D. Breaux,Lorrie Faith Cranor, Aleecia M. McDonald, Joel R.Reidenberg, Noah A. Smith, Fei Liu, N. CameronRussell, Florian Schaub, and Shomir Wilson.
2013.The usable privacy policy project.
Technical Report,CMU-ISR-13-119, Carnegie Mellon University.Arie Segev.
1987.
The Node-Weighted Steiner TreeProblem.
Networks, 17(1):1?17.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004.Max-margin Markov networks.
In Advances in NeuralInformation Processing Systems 16.Kapil Thadani and Kathleen McKeown.
2013.
Sentencecompression with joint structural inference.
In Pro-ceedings of CoNLL.Lucy Vanderwende, Michele Banko, , and Arul Menezes.2004.
Event-centric summary generation.
In Proceed-ings of DUC.Bin Zhao and Eric P. Xing.
2014.
Quasi real-time sum-marization for consumer videos.
In Proceedings ofCVPR.1086
