Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1375?1384,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA convex and feature-rich discriminative approach todependency grammar induction?Edouard GraveColumbia Universityedouard.grave@gmail.comNo?emie ElhadadColumbia Universitynoemie.elhadad@columbia.eduAbstractIn this paper, we introduce a new methodfor the problem of unsupervised depen-dency parsing.
Most current approachesare based on generative models.
Learningthe parameters of such models relies onsolving a non-convex optimization prob-lem, thus making them sensitive to initial-ization.
We propose a new convex formu-lation to the task of dependency grammarinduction.
Our approach is discriminative,allowing the use of different kinds of fea-tures.
We describe an efficient optimiza-tion algorithm to learn the parameters ofour model, based on the Frank-Wolfe algo-rithm.
Our method can easily be general-ized to other unsupervised learning prob-lems.
We evaluate our approach on tenlanguages belonging to four different fam-ilies, showing that our method is competi-tive with other state-of-the-art methods.1 IntroductionGrammar induction is an important problem incomputational linguistics.
Despite having recentlyreceived a lot of attention, it is still considered tobe an unsolved problem.
In this work, we are inter-ested in unsupervised dependency parsing.
Moreprecisely, our goal is to induce directed depen-dency trees, which capture binary syntactic rela-tions between the words of a sentence.
Since ourmethod is unsupervised, it does not have accessto such syntactic structure and only take as in-put a corpus of words and their associated partsof speech.Most recent approaches to unsupervised depen-dency parsing are based on probabilistic genera-tive models, such as the dependency model withvalence introduced by Klein and Manning (2004).Learning the parameters of such models is oftenAll languages have their own grammarFigure 1: An example of dependency tree.done by maximizing the log-likelihood of unla-beled data, leading to a non-convex optimizationproblem.
Thus, the performance of those methodsrely heavily on the initialization, and practitionershave to find good heuristics to initialize their mod-els.In this paper, we describe a different approachto the problem of dependency grammar induction,inspired by discriminative clustering.
We pro-pose to use a feature-rich discriminative parser,and to learn the parameters of this parser us-ing a convex quadratic objective function.
Inparticular, this approach also allows us to in-duce non-projective dependency structures.
Fol-lowing the work of Naseem et al (2010), weuse language-independent rules between pairs ofparts-of-speech to guide our parser.
More pre-cisely, we make the following contributions:?
Our method is based on a feature-rich dis-criminative parser (section 3);?
Learning the parameters of our parser isachieved using a convex objective, and is thusnot sensitive to initialization (section 4);?
Our method can produce non-projective de-pendency structures (section 3.2.2);?
We propose an efficient algorithm to opti-mize the objective, based on the Frank-Wolfemethod (section 5);?
We evaluate our approach on the universaltreebanks dataset, showing that it is competi-tive with the state-of-the-art (section 6).13752 Related workA lot of research has been carried out in the lastdecade on dependency grammar induction.
Wereview the dependency model with valence, onwhich most unsupervised dependency parsers arebased, before presenting different extensions andlearning algorithms.
Finally, we review discrimi-native clustering, on which our method is based.DMV.
The dependency model with valence(DMV), introduced by Klein and Manning (2004),was the first method to outperform the baselineconsisting in attaching each token to the next one.The DMV is a generative probabilistic model ofthe dependency tree and parts-of-speech of a sen-tence.
It generates the root first, and then recur-sively generates the tokens down the tree.
Theprobability of generating a new dependent for agiven token depends on the direction (left or right)and whether a dependent was already generated inthat direction.
Then, the part-of-speech of the newdependent is generated according to a multinomialdistribution conditioned on the direction and thehead?s POS.Extensions.
Several extensions of the depen-dency model with valence have been proposed.Headden III et al (2009) proposed the lexicalizedextended valence grammar (EVG), in which theprobability of generating a POS also depends onthe valence information.
They rely on smooth-ing to tackle the increased number of parame-ters.
Mare?cek and?Zabokrtsk`y (2012) describedan approach using a n-gram reducibility measure,which capture which words can be deleted froma sentence without making it syntactically incor-rect.
Cohen and Smith (2009) introduced a prior,based on the shared logistic normal distribution.This prior allowed to tie the grammar parameterscorresponding to different POS belonging to thesame coarse groups, such as all the POS corre-sponding to verbs.
Berg-Kirkpatrick and Klein(2010) proposed to tie the parameters of grammarsfor different languages using a prior based on aphylogenetic tree.
Naseem et al (2010) proposeda set of rules between parts-of-speech, encodingsyntactic universals, such as the fact that adjec-tives are often dependents of nouns.
They usedposterior regularization (Ganchev et al, 2010) toimpose that a certain amount of the infered depen-dencies verifies one of these rules.
Also using pos-terior regularization, Gillenwater et al (2011) im-posed a sparsity bias on the infered dependencies,enforcing a small number of unique dependencytypes.
Finally, Blunsom and Cohn (2010) refor-mulated dependency grammar induction using treesubstitution grammars, while Bisk and Hocken-maier (2013) proposed to use combinatory cate-gorial grammars.Learning.
Different algorithms have been pro-posed to improve the learning of the parametersof the dependency model with valence.
Smithand Eisner (2005) proposed to use constrastive es-timation to learn the parameters of a log-linearparametrization of the DMV, while Spitkovsky etal.
(2010b) showed that using Viterbi EM insteadof classic EM leads to higher accuracy.
Observingthat learning from shorter sentences is easier (be-cause less ambiguous), Spitkovsky et al (2010a)presented different techniques to learn grammarfrom increasingly longer sentences.
Gimpel andSmith (2012) introduced a model inspired by theIBM1 translation model for grammar induction,resulting in a concave log-likelihood function.They show that initializing the DMV with theoutput of their model leads to improved depen-dency accuracies.
Hsu et al (2012) and Parikhet al (2014) introduced spectral methods for un-supervised dependency and constituency parsing.Finally, Spitkovsky et al (2013) introduced dif-ferent heuristics for avoiding local minima whileGormley and Eisner (2013) proposed a method tofind the global optimum of non-convex problems,based on branch-and-bound.Discriminative clustering.
Our unsupervisedparser is inspired by discriminative clustering, in-troduced by Xu et al (2004).
Given a set of points,the objective of discriminative clustering is to as-sign labels to these points that can be easily pre-dicted using a discriminative classifier.
Xu et al(2004) introduced a formulation using the hingeloss, Bach and Harchaoui (2007) proposed to usethe squared loss instead, while Joulin et al (2010)proposed a formulation based on the logistic loss.Recently, a formulation based on discriminativeclustering was proposed for the problem of distantsupervision for relation extraction (Grave, 2014)and for the problem of finding the names of char-acters in TV series based on the correspondingscripts (Ramanathan et al, 2014).
Closest to ourapproach, extensions of discriminative clusteringwere used to align sequences of labels or text with1376videos (Bojanowski et al, 2014; Bojanowski et al,2015) or to co-localize objects in videos (Joulin etal., 2014).3 ModelIn this section, we describe the parsing model usedin our approach and briefly review the correspond-ing decoding algorithms.
Following McDonald etal.
(2005b), we propose to cast the problem of de-pendency parsing as a maximum weight spanningtree problem in directed graphs.3.1 Edge-based factorizationLet us start by setting up some notations.
Aninput sentence of length n is represented by ann?uplet x = (x1, ..., xn).
The dependency treecorresponding to that sentence is represented by an ?
(n + 1) binary matrix y, such that yij= 1 ifand only if the head of the token i is the token j(and thus, the integer n + 1 represents the root ofthe tree).In this paper, we follow a common approachby factoring the score of dependency tree as thesum of the scores of the edges forming thattree.
We assume that each pair of tokens (i, j)is represented by a high-dimensional feature vec-tor f(x, i, j) ?
Rd.
Then, the score sijof theedge (i, j) is obtained using the linear modelsij= w>f(x, i, j),where w ?
Rdis a parameter vector.
Thus thescore s corresponding to the tree y is equal tos =?
(i,j) s.t.
yij=1sij=?
(i,j) s.t.
yij=1w>f(x, i, j).Assuming that the parameter vector w is known,parsing a sentence reduces to finding the tree withthe highest score, which is the maximum weightspanning tree.3.2 Maximum spanning treesDifferent sets of spanning trees have been consid-ered in the setting of supervised dependency pars-ing.
We briefly review those sets, and describethe corresponding algorithms to compute the max-imum weight spanning tree over those sets.3.2.1 Projective dependency treesFirst, we consider the set of projective spanningtrees.
A dependency tree is said to be projective ifthe dependencies do not cross when drawn abovethe words in linear order.
Similarly, this meansthat word and all its descendants form a contigu-ous substring of the sentence.
Projective depen-dency trees are thus strongly related to context freegrammars, and it is possible to obtain the maxi-mum weight spanning projective tree using a mod-ified version of the CKY algorithm (Cocke andSchwartz, 1970; Kasami, 1965; Younger, 1967).The complexity of this algorithm is O(n5).
Thisled Eisner (1996) to propose an algorithm for pro-jective parsing which has a complexity of O(n3).Similarly to CKY, the Eisner algorithm is basedon dynamic programming, parsing a sentence ina bottom-up fashion.
Finally, it should be notedthat the dependency model with valence, on whichmost approaches to dependency grammar induc-tion are based, produces projective dependencytrees.3.2.2 Non-projective dependency treesSecond, we consider the set of non-projectivespanning trees.
Indeed, many languages, suchas Czech or Dutch, have a significant number ofnon-projective edges.
In the context of superviseddependency parsing, McDonald et al (2005b)shown that using non-projective trees improvesthe accuracy of dependency parsers for those lan-guages.
The maximum weight spanning tree ina directed graph can be computed using the Chu-Liu/Edmonds algorithm (Chu and Liu, 1965; Ed-monds, 1967), which has a complexity of O(n3).Later, Tarjan (1977) proposed an improved ver-sion of this algorithm for dense graphs, whosecomplexity is O(n2), the same as for undirectedgraphs using Prim?s algorithm.
Thus a second ad-vantage of using non-projective dependency treesis the fact that it leads to more efficient parsers.4 Learning the parameter vectorIn this section, we describe the loss function weuse to learn the parameter vector w from unla-beled sentences.4.1 Problem formulationFrom now on, y is a vector representing the de-pendency trees corresponding to the whole corpus.Thus, each index i corresponds to a potential de-pendency between two words of a given sentence.1377He gave a seminar yesterday about unsupervised dependency parsingFigure 2: Example of a non-projective dependency tree in english.Like before, yi= 1 if and only if there is a de-pendency between those two words, and yi= 0otherwise.
The set of dependencies that form validtrees is denoted by the set T .Inspired by the discriminative clustering frame-work introduced by Xu et al (2004), our goal isto jointly find the dependencies represented by thevector y and the parameter vector w which mini-mize the regularized empirical riskminy?Tminw1nn?i=1`(yi,w>xi) + ??
(w), (1)where ` is a loss function and ?
is a regularizer.The intuition is that we want to find the depen-dency trees y that can be easily predicted by a dis-criminative parser, whose parameters are w.Following Bach and Harchaoui (2007), we pro-pose to use the squared loss ` defined by`(y, y?)
=12(y ?
y?
)2and to use the `2-norm as a regularizer.
In thatcase, we obtain the objective function:miny?Tminw12n?y ?Xw?22+?2?w?22.
(2)One of the main advantages of using the squaredloss is the fact that the corresponding objectivefunction is jointly convex in y and w. Indeed,the objective is the composition of an affine map-ping, defined by (y,w) 7?
y ?Xw, with a con-vex function, defined by u 7?
u>u.
Thus, theobjective function is convex (see section 3.2.2 ofBoyd and Vandenberghe (2004)).
The problem (2)is thus non-convex only because of the combinato-rial constraints on the binary vector y, namely thaty should represents valid trees.4.2 Convex relaxationThe set T of vectors representing valid depen-dency trees is a finite set of binary vectors.
Wecan thus take the convex hull of those points anddenote it by Y:Y = conv(T ).VERB 7?
VERB NOUN 7?
NOUNVERB 7?
NOUN NOUN 7?
ADJVERB 7?
PRON NOUN 7?
DETVERB 7?
ADV NOUN 7?
NUMVERB 7?
ADP NOUN 7?
CONJADJ 7?
ADV ADP 7?
NOUNTable 1: Set of universal rules used in our parser.By definition, this set is a convex polytope.
Wethen propose to replace the combinatorial con-straints on the vector y by the fact that y shouldbe in the convex polytope Y .
We thus obtain aconvex quadratic program, with linear constraints,as follows:miny?Yminw12n?y ?Xw?22+?2?w?22.
(3)We will describe how to compute the optimal so-lution of this problem in section 5.4.3 RoundingGiven a continuous solution yc?
Y of the relaxedproblem, it is possible to obtain a solution of theinteger problem by finding the tree yd?
T whichis closest to yc, by solving the problemminyd?T?yd?
yc?22.The solution of the previous problem can easilybe formulated is a minimum weight spanning treeproblem.
Indeed, by developping the previousexpression, and using the fact that for all treesyd?
T , y>dyd= n, where n is the number oftokens, the previous problem is equivalent to:minyd?T?y>dyc,whose solution is obtained using the minimumweight spanning tree algorithm.
It should be notedthat the rounding solution is not necessarily theoptimal solution of the integer problem.1378Figure 3: Illustration of a Frank-Wolfe step.4.4 Prior on yWe now describe how to guide our unsuper-vised parser, by using universal rules.
FollowingNaseem et al (2010), we want a certain percent-age of the infered dependencies to satisfy one ofthe twelve universal syntactic rules, listed in Ta-ble 1.
Let S be the set of indices correspondingto word pairs that satisfy one of these rules.
Then,imposing that a certain percentage c of dependen-cies satisfy one of those rules can be obtained byimposing the constraint:1n?i?Syi?
c.This linear constraint is equivalent to u>y ?
c,where the vector u is defined byui={1/n if i ?
S,0 otherwise.Using Lagrangian duality, we can obtain the fol-lowing equivalent penalized problem:miny?Yminw12n?y?Xw?22+?2?w?22??
u>y.
(4)The penalized and constrained problems areequivalent, since for every c, there exists a ?
suchthat the two problems have the same optimum.From an optimization point of view, it is easier todeal with the penalized problem and we will thususe it in the next section.5 OptimizationOne could use a general purpose quadratic solverto compute the solution of the previous convexproblem.
However, this might be inefficient sinceAlgorithm 1: Frank-Wolfe algorithmfor t ?
{1, ..., T} doCompute the gradient:gt= ?f(zt)Solve the linear program:st= mins?Ds>gtTake the Frank-Wolfe step:zt+1= ?tst+ (1?
?t)ztendit does not use the structure of the polytope and,in particular, the fact that one can easily minimizea linear function over the tree polytope using theminimum weight spanning tree algorithm.
Insteadwe propose to use the Frank-Wolfe algorithm, thatwe now describe.5.1 Frank-Wolfe algorithmThe Frank-Wolfe algorithm (Frank and Wolfe,1956; Jaggi, 2013) is used to minimize a convexdifferentiable function f over a convex boundedset D. It is an iterative first-order optimizationmethod.
At each iteration t, the convex function fis approximated by a linear function defined by itsgradient at the current point zt.
Then it finds thepoint stthat minimizes that linear function, overthe convex set D:st= minss>?f(zt) s.t.
s ?
D.The point zt+1is then defined as the weighted av-erage between the solution stand the current pointzt: zt+1= ?tst+ (1?
?t) zt,where ?tis the stepsize (such as 2/(t + 2)).
Compared to the gradi-ent descent algorithm, the Frank-Wolfe alogrithmdoes not take a step in the direction of the gradi-ent, but in the direction of the point that minimizesthe linear approximation of the function f over theconvex setD (see Fig 3).
In particular, this ensuresthat the points ztalways stay inside the convex set,and there is thus no need for a projection step.To summarize, in order to use the Frank-Wolfealgorithm, we need to compute the gradient of theobjective function and to minimize a linear func-tion over our convex set.
This is particularly ap-propriate to our problem, since we can easily min-imize a linear function over the tree polytopes (us-ing the minimum weight spanning tree algorithm),while projecting on those polytopes is more ex-pensive.1379Algorithm 2: Optimization algorithm for ourmethod.for t ?
{1, ..., T} doCompute the optimal w:wt= argminw12n?yt?Xw?22+?2?w?22Compute the gradient w.r.t.
y:gt=1n(yt?Xwt)?
?
uSolve the linear program:st= mins?Ys>gtTake the Frank-Wolfe step:yt+1= ?tst+ (1?
?t)ytend5.2 Application to our problemWe now describe how to use the Frank-Wolfe al-gorithm to optimize our objective function with re-spect to y.
First, let us introduce the functions fand h defined byf(w,y) =12n?y ?Xw?22+?2?w?22?
?
u>y,h(y) = minwf(w,y).The original problem is equivalent tominy?Yminwf(w,y) = miny?Yh(y).We will use the Frank-Wolfe algorithm to optimizethe function h.Minimizing w.r.t w. First, we need to minimizethe function f with respect to w, in order to com-pute the function h (and its gradient).
One mustnote that this is an unconstrained quadratic pro-gram, whose solution can be obtained in closedform by solving the linear system:(X>X + ?I)w = X>y.However, in case of a very large feature space, thissystem might be prohibitively expensive to solveexactly.
We instead propose to approximatelycompute the optimal w using stochastic gradientdescent.Computing the gradient of h. Then, the gradi-ent of the function h at the point y is equal to?h(y) = ?yf(w?,y),POSi?
dPOSj?
dPOSi?
POSj?
dPOSi?
POSi?1?
POSj?
dPOSi?
POSi+1?
POSj?
dPOSi?
POSj?
POSj?1?
dPOSi?
POSj?
POSj+1?
dTable 2: Features used in our parser to describe thedependency between tokens i and j, where i is thehead, j the dependent and d = i?
j.where w?is equal tow?= argminwf(w,y).Thus, in order to compute the gradient of h withrespect to y, we start by computing the corre-sponding optimal value of w. Then, the gradientwith respect to y is equal to?h(y) =1n(y ?Xw?)?
?
u.Minimizing a linear function over Y .
We fi-naly need to compute the optimal solution of thefollowing linear problemmins?Y?h(y)>s.The optimal value of a linear function over abounded convex polytope is always attained on atleast one vertex of that polytope.
By definition ofour polytope, those vertices correspond to span-ning trees.
Thus, computing an optimal solutionof this problem is obtained by finding a minimumweight spanning tree.Discussion.
Similarly to the Expectation-Maximization algorithm, our optimizationmethod is a two-steps iterative algorithm.
Inthe first step, the optimal parameter vector w isestimated based on the previous dependency trees,while the second step consist in re-estimating the(relaxed) dependency trees.6 ExperimentsIn this section, we report the results of the experi-ments we have performed to evaluate our approachto grammar induction.1380DMV PR USR OURDE 42.6 58.4 53.4 60.2EN 22.4 57.5 66.2 62.3ES 31.8 57.3 71.5 68.8FR 56.0 66.2 54.1 72.3ID 44.9 21.4 50.3 69.7IT 33.3 40.4 46.5 64.3JA 48.0 58.9 58.2 57.5KO 35.3 50.7 48.8 59.0PT-BR 49.6 40.7 46.4 68.3SV 38.9 61.2 64.3 66.2AVG 40.2 51.3 56.0 64.8Table 3: Directed dependency accuracy, onthe universal treebanks with universal parts-of-speech, on sentences of length 10 or less.
PR refersto posterior regularization, USR to universal rules.6.1 FeaturesThe features used in our unsupervised parser arebased on the parts-of-speech of the head and thedependent of the corresponding dependency, andare given in Table 2.
Following McDonald et al(2005a), we also include features capturing thecontext of the head or the dependent.
These fea-tures are trigrams and are formed by the parts-of-speech of the two tokens of the dependencyand one of the word appearing before/after thehead/dependent.
Finally, all the features are con-joined with the signed distance between the twowords of the dependency.6.2 DatasetWe use the universal treebanks, version 2.0, intro-duced by McDonald et al (2013).
This datasetcontains dependency trees for ten languages be-longing to five different families: Spanish, French,Italian, Portuguese (Romanic family), English,German, Swedish (Germanic family), Korean,Japanese and Indonesian.
The tokens of thosetreebanks are tagged using the universal part-of-speech tagset (Petrov et al, 2012).
We focus oninducing dependency grammars using universalparts-of-speech, and will thus report results whereall methods use (gold) universal POS.6.3 Comparison with baselinesWe will compare our approach to three other un-supervised parsers.
Our first baseline is the DMVmodel, introduced by Klein and Manning (2004).DMV PR USR OUR7 min 1 h 15 h 2 minTable 4: Computational times required to learn agrammar on the English treebank.Our second baseline is the extended valence gram-mar model, with posterior sparsity constraints, asdescribed by Gillenwater et al (2011).
Finally,our last baseline is the model with universal rulesintroduced by Naseem et al (2010).
It shouldbe noted that these two baselines obtain perfor-mances that are near state-of-the-art.
All methodsare trained and tested on sentences of length 10 orless, after stripping punctuation.Parameter selection.
All the parameters werechosen using the English development set.
Ourmethod has two parameters, determined as:?
= 0.001 and ?
= 0.1.
We used T = 200iterations in all the experiments.Discussion.
We report the results in Table 3.First, we observe that our method performs bet-ter than the three baselines on seven out of tenlanguages.
Overall, our approach outperforms thethree baselines, with an absolute improvement of13 points over the extended valence grammar withposterior sparsity and 8 points over the model withuniversal syntactic rules.
We also note that theinter-language variance is lower for our methodthan the baselines (std of 4.6 for our method v.s.8.3 for USR and 12.7 for PR).
For the sake ofcompleteness, we also compared those methodsusing the fine grained POS available in the univer-sal treebanks.
Overall, our method obtains an ac-curacy of 68.4, while USR and PR achieve accura-cies of 67.3 and 58.5 respectively.
Finally, we re-port computational times in Table 4, showing thatour approach is much faster than the baselines.6.4 Non-projective grammar inductionIn this section, we investigate non-projectivegrammar induction.
With our approach, we onlyhave to replace the Eisner algorithm by Chu-Liu/Edmonds.
We report results in Table 5.
First,we observe that the non-projective results areslightly worse than projective one.
This is not re-ally surprising since the amount of non-projectivegold dependencies is very small on the considereddata.
Moreover, non-projective trees are muchmore ambiguous than projective ones, leading to1381PROJECTIVE NON-PROJECTIVEDE 60.2 57.2EN 62.3 60.5ES 68.8 66.5FR 72.3 69.2ID 69.7 68.4IT 64.3 63.1JA 57.5 59.3KO 59.0 60.0PT-BR 68.3 67.7SV 66.2 65.4AVG 64.8 63.7Table 5: Comparison between projective and non-projective unsupervised dependency parsing usingour method.a harder problem.
We still believe those resultsare interesting because the difference is small (lessthan 1.5 points), while non-projective parsing iscomputationaly more efficient.6.5 Evaluation on longer sentencesWe also evaluate our method on longer sentences(while still training on sentences of length 10 orless).
Directed dependency accuracies are re-ported in Figure 4.
On all sentences, our methodachieve an overall accuracy of 55.8.6.6 Feature ablation studyIn this section, we study the importance of thedifferent features used in our parser.
We reportdirected accuracies when different groups of fea-tures are removed, one at a time, in Table 6.
First,we remove the distance information from the fea-tures (line DISTANCE).
We observe that the per-formance of our parser is greatly affected by thisablation, especially for long sentences.
Then, weremove the context features (line CONTEXT) andthe unigram features (line UNIGRAM) from ourmodel.
We observe that the performance decreasesslightly due to this ablations, but the differencesare small.7 DiscussionIn this paper, we introduced a new framework forthe task of unsupervised dependency parsing.
Ourmethod is a based on a feature-rich discrimina-tive model, whose parameters are learned using aconvex objective function.
We demonstrated on|w| ?
10 |w| ?
?DISTANCE 61.8 48.7CONTEXT 64.2 55.1UNIGRAM 64.0 55.3ALL FEATURES 64.8 55.8Table 6: Feature ablation study.the universal treebanks that our approach leads tocompetitive results, while being computationalyvery efficient.
We now describe some directionswe would like to explore as future work.Richer feature set.
In our experiments, we fo-cused on assessing the usefulness of our con-vex, discriminative approach, and thus consideredonly relatively simple features based on parts-of-speech.
Inspired by supervised dependency pars-ing, we would like to explore the use of other fea-tures such as Brown clusters (Brown et al, 1992)or distributed word representations (Mikolov etal., 2013), in order to lexicalize our parser.Higher-order parsing.
So far, our model islacking the notion of valency, that has proven veryuseful for grammar induction.
In future work,we would thus like to replace our edge-based fac-torization by a higher-order one, in order to cap-ture siblings (and grandchilds) interactions.
Wewould then have to use a higher-order parser, suchas the ones described by McDonald and Pereira(2006) and Koo and Collins (2010).
Another po-tential approach would be to use the linear pro-gramming relaxed inference, described by Martinset al (2009).Transfer learning.
In this paper, we used uni-versal syntactic rules, as described by Naseem etal.
(2010) to guide our parser.
We would like toexplore the use of weak supervision, such as theone considered in transfer learning (Hwa et al,2005).
For example, projected dependencies froma resource-rich language could be used as con-straints in our framework.Code.
The code for our method is distributed onthe first author webpage.AcknowledgmentsThis work is supported by National Science Foun-dation award 1344668 and National Institute ofGeneral Medical Sciences award R01 GM090187.138210 15 20 30 all505560657075svende10 15 20 30 all505560657075esfrptit10 15 20 30 all505560657075idkojaFigure 4: Directed dependency accuracies on longer sentences for our approach.ReferencesFrancis R Bach and Za?
?d Harchaoui.
2007.
Diffrac: adiscriminative and flexible framework for clustering.In NIPS.Taylor Berg-Kirkpatrick and Dan Klein.
2010.
Phylo-genetic grammar induction.
In ACL.Yonatan Bisk and Julia Hockenmaier.
2013.
An hdpmodel for inducing combinatory categorial gram-mars.
TACL.Phil Blunsom and Trevor Cohn.
2010.
Unsupervisedinduction of tree substitution grammars for depen-dency parsing.
In EMNLP.Piotr Bojanowski, R?emi Lajugie, Francis Bach, IvanLaptev, Jean Ponce, Cordelia Schmid, and JosefSivic.
2014.
Weakly supervised action labeling invideos under ordering constraints.
In ECCV.Piotr Bojanowski, R?emi Lagugie, Edouard Grave,Francis Bach, Ivan Laptev, Jean Ponce, and CordeliaSchmid.
2015.
Weakly-supervised alignment ofvideo with text.
http://arxiv.org/abs/1505.06027.Stephen Boyd and Lieven Vandenberghe.
2004.
Con-vex optimization.
Cambridge university press.Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n-gram models of natural language.Computational linguistics.Yoeng-Jin Chu and Tseng-Hong Liu.
1965.
Onshortest arborescence of a directed graph.
ScientiaSinica.John Cocke and Jacob Schwartz.
1970.
Programminglanguages and their compilers: Preliminary notes.Technical report.Shay B Cohen and Noah A Smith.
2009.
Shared lo-gistic normal distributions for soft parameter tyingin unsupervised grammar induction.
In NAACL.Jack Edmonds.
1967.
Optimum branchings.
Journalof Research of the National Bureau of Standards.Jason M Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In COL-ING.Marguerite Frank and Philip Wolfe.
1956.
An algo-rithm for quadratic programming.
Naval researchlogistics quarterly.Kuzman Ganchev, Joao Grac?a, Jennifer Gillenwater,and Ben Taskar.
2010.
Posterior regularization forstructured latent variable models.
JMLR.Jennifer Gillenwater, Kuzman Ganchev, Jo?ao Grac?a,Fernando Pereira, and Ben Taskar.
2011.
Poste-rior sparsity in unsupervised dependency parsing.JMLR.Kevin Gimpel and Noah A Smith.
2012.
Concavityand initialization for unsupervised dependency pars-ing.
In NAACL.Matthew R Gormley and Jason Eisner.
2013.
Noncon-vex global optimization for latent-variable models.In ACL.Edouard Grave.
2014.
A convex relaxation for weaklysupervised relation extraction.
In EMNLP.William P Headden III, Mark Johnson, and David Mc-Closky.
2009.
Improving unsupervised dependencyparsing with richer contexts and smoothing.
InNAACL.Daniel Hsu, Percy Liang, and Sham M Kakade.
2012.Identifiability and unmixing of latent parse trees.
InNIPS.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural language engineering.Martin Jaggi.
2013.
Revisiting frank-wolfe:Projection-free sparse convex optimization.
InICML.1383Armand Joulin, Jean Ponce, and Francis R Bach.
2010.Efficient optimization for discriminative latent classmodels.
In NIPS.Armand Joulin, Kevin Tang, and Li Fei-Fei.
2014.
Ef-ficient image and video co-localization with frank-wolfe algorithm.
In ECCV.Tadao Kasami.
1965.
An efficient recognition and syn-tax analysis algorithm for context-free languages.Technical report.Dan Klein and Christopher D Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In ACL.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In ACL.David Mare?cek and Zden?ek?Zabokrtsk`y.
2012.
Ex-ploiting reducibility in unsupervised dependencyparsing.
In EMNLP/CoNLL.Andr?e FT Martins, Noah A Smith, and Eric P Xing.2009.
Polyhedral outer approximations with appli-cation to natural language parsing.
In ICML.Ryan T McDonald and Fernando CN Pereira.
2006.Online learning of approximate dependency parsingalgorithms.
In EACL.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005a.
Online large-margin training of de-pendency parsers.
In ACL.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Haji?c.
2005b.
Non-projective dependency pars-ing using spanning tree algorithms.
In EMNLP.Ryan T McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, KuzmanGanchev, Keith B Hall, Slav Petrov, Hao Zhang, Os-car T?ackstr?om, et al 2013.
Universal dependencyannotation for multilingual parsing.
In ACL.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems.Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using universal linguistic knowl-edge to guide grammar induction.
In EMNLP.Ankur P Parikh, Shay B Cohen, and Eric P Xing.2014.
Spectral unsupervised parsing with additivetree metrics.
In ACL.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
LREC.Vignesh Ramanathan, Armand Joulin, Percy Liang,and Li Fei-Fei.
2014.
Linking people with ?their?names using coreference resolution.
In ECCV.Noah A Smith and Jason Eisner.
2005.
Guiding un-supervised grammar induction using contrastive es-timation.
In Proc.
of IJCAI Workshop on Grammat-ical Inference Applications.Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2010a.
From baby steps to leapfrog: Howless is more in unsupervised dependency parsing.
InNAACL.Valentin I Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,and Christopher D Manning.
2010b.
Viterbi train-ing improves unsupervised dependency parsing.
InCoNLL.Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2013.
Breaking out of local optima withcount transforms and model recombination: A studyin grammar induction.
In EMNLP.Robert Endre Tarjan.
1977.
Finding optimum branch-ings.
Networks.Linli Xu, James Neufeld, Bryce Larson, and DaleSchuurmans.
2004.
Maximum margin clustering.In NIPS.Daniel H Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information andcontrol.1384
