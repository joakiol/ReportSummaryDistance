Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 213?222,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsUtilizing Dependency Language Models for Graph-based DependencyParsing ModelsWenliang Chen, Min Zhang?, and Haizhou LiHuman Language Technology, Institute for Infocomm Research, Singapore{wechen, mzhang, hli}@i2r.a-star.edu.sgAbstractMost previous graph-based parsing models in-crease decoding complexity when they usehigh-order features due to exact-inference de-coding.
In this paper, we present an approachto enriching high-order feature representationsfor graph-based dependency parsing modelsusing a dependency language model and beamsearch.
The dependency language model isbuilt on a large-amount of additional auto-parsed data that is processed by a baselineparser.
Based on the dependency languagemodel, we represent a set of features for theparsing model.
Finally, the features are effi-ciently integrated into the parsing model dur-ing decoding using beam search.
Our ap-proach has two advantages.
Firstly we utilizerich high-order features defined over a viewof large scope and additional large raw cor-pus.
Secondly our approach does not increasethe decoding complexity.
We evaluate the pro-posed approach on English and Chinese data.The experimental results show that our newparser achieves the best accuracy on the Chi-nese data and comparable accuracy with thebest known systems on the English data.1 IntroductionIn recent years, there are many data-driven mod-els that have been proposed for dependency parsing(McDonald and Nivre, 2007).
Among them, graph-based dependency parsing models have achievedstate-of-the-art performance for a wide range of lan-guages as shown in recent CoNLL shared tasks?Corresponding author(Buchholz and Marsi, 2006; Nivre et al, 2007).In the graph-based models, dependency parsing istreated as a structured prediction problem in whichthe graphs are usually represented as factored struc-tures.
The information of the factored structures de-cides the features that the models can utilize.
Thereare several previous studies that exploit high-orderfeatures that lead to significant improvements.McDonald et al (2005) and Covington (2001)develop models that represent first-order featuresover a single arc in graphs.
By extending the first-order model, McDonald and Pereira (2006) and Car-reras (2007) exploit second-order features over twoadjacent arcs in second-order models.
Koo andCollins (2010) further propose a third-order modelthat uses third-order features.
These models utilizehigher-order feature representations and achieve bet-ter performance than the first-order models.
But thisachievement is at the cost of the higher decodingcomplexity, from O(n2) to O(n4), where n is thelength of the input sentence.
Thus, it is very hard todevelop higher-order models further in this way.How to enrich high-order feature representationswithout increasing the decoding complexity forgraph-based models becomes a very challengingproblem in the dependency parsing task.
In this pa-per, we solve this issue by enriching the feature rep-resentations for a graph-based model using a depen-dency language model (DLM) (Shen et al, 2008).The N-gram DLM has the ability to predict the nextchild based on the N-1 immediate previous childrenand their head (Shen et al, 2008).
The basic ideabehind is that we use the DLM to evaluate whether avalid dependency tree (McDonald and Nivre, 2007)213is well-formed from a view of large scope.
The pars-ing model searches for the final dependency treesby considering the original scores and the scores ofDLM.In our approach, the DLM is built on a largeamount of auto-parsed data, which is processedby an original first-order parser (McDonald et al,2005).
We represent the features based on the DLM.The DLM-based features can capture the N-gram in-formation of the parent-children structures for theparsing model.
Then, they are integrated directlyin the decoding algorithms using beam-search.
Ournew parsing model can utilize rich high-order fea-ture representations but without increasing the com-plexity.To demonstrate the effectiveness of the proposedapproach, we conduct experiments on English andChinese data.
The results indicate that the approachgreatly improves the accuracy.
In summary, wemake the following contributions:?
We utilize the dependency language model toenhance the graph-based parsing model.
TheDLM-based features are integrated directly intothe beam-search decoder.?
The new parsing model uses the rich high-orderfeatures defined over a view of large scope andand additional large raw corpus, but without in-creasing the decoding complexity.?
Our parser achieves the best accuracy on theChinese data and comparable accuracy with thebest known systems on the English data.2 Dependency language modelLanguage models play a very important role for sta-tistical machine translation (SMT).
The standard N-gram based language model predicts the next wordbased on the N?1 immediate previous words.
How-ever, the traditional N-gram language model cannot capture long-distance word relations.
To over-come this problem, Shen et al (2008) proposed adependency language model (DLM) to exploit long-distance word relations for SMT.
The N-gram DLMpredicts the next child of a head based on the N ?
1immediate previous children and the head itself.
Inthis paper, we define a DLM, which is similar to theone of Shen et al (2008), to score entire dependencytrees.An input sentence is denoted by x =(x0, x1, ..., xi, ..., xn), where x0 = ROOT anddoes not depend on any other token in x and eachtoken xi refers to a word.
Let y be a depen-dency tree for x and H(y) be a set that includes thewords that have at least one dependent.
For eachxh ?
H(y), we have a dependency structure Dh =(xLk, ...xL1, xh, xR1...xRm), where xLk, ...xL1 arethe children on the left side from the farthest to thenearest and xR1...xRm are the children on the rightside from the nearest to the farthest.
ProbabilityP (Dh) is defined as follows:P (Dh) = PL(Dh)?
PR(Dh) (1)Here PL and PR are left and right side generativeprobabilities respectively.
Suppose, we use a N-gram dependency language model.
PL is defined asfollows:PL(Dh) ?
PLc(xL1|xh)?PLc(xL2|xL1, xh)?...
(2)?PLc(xLk|xL(k?1), ..., xL(k?N+1), xh)where the approximation is based on the nth orderMarkov assumption.
The right side probability issimilar.
For a dependency tree, we calculate theprobability as follows:P (y) =?xh?H(y)P (Dh) (3)In this paper, we use a linear model to calculatethe scores for the parsing models (defined in Section3.1).
Accordingly, we reform Equation 3.
We definefDLM as a high-dimensional feature representationwhich is based on arbitrary features of PLc, PRc andx.
Then, the DLM score of tree y is in turn computedas the inner product of fDLM with a correspondingweight vector wDLM .scoreDLM (y) = fDLM ?
wDLM (4)3 Parsing with dependency languagemodelIn this section, we propose a parsing model whichincludes the dependency language model by extend-ing the model of McDonald et al (2005).2143.1 Graph-based parsing modelThe graph-based parsing model aims to search forthe maximum spanning tree (MST) in a graph (Mc-Donald et al, 2005).
We write (xi, xj) ?
yif there is a dependency in tree y from word xito word xj (xi is the head and xj is the depen-dent).
A graph, denoted by Gx, consists of a setof nodes Vx = {x0, x1, ..., xi, ..., xn} and a set ofarcs (edges) Ex = {(xi, xj)|i 6= j, xi ?
Vx, xj ?
(Vx ?
x0)}, where the nodes in Vx are the wordsin x.
Let T (Gx) be the set of all the subgraphs ofGx that are valid dependency trees (McDonald andNivre, 2007) for sentence x.The formulation defines the score of a depen-dency tree y ?
T (Gx) to be the sum of the edgescores,s(x, y) =?g?yscore(w, x, g) (5)where g is a spanning subgraph of y. g can be asingle dependency or adjacent dependencies.
Theny is represented as a set of factors.
The modelscores each factor using a weight vector w that con-tains the weights for the features to be learned dur-ing training using the Margin Infused Relaxed Algo-rithm (MIRA) (Crammer and Singer, 2003; McDon-ald and Pereira, 2006).
The scoring function isscore(w, x, g) = f(x, g) ?
w (6)where f(x, g) is a high-dimensional feature repre-sentation which is based on arbitrary features of gand x.The parsing model finds a maximum spanningtree (MST), which is the highest scoring tree inT (Gx).
The task of the decoding algorithm for agiven sentence x is to find y?,y?
= argmaxy?T (Gx)s(x, y) = argmaxy?T (Gx)?g?yscore(w, x, g)3.2 Add DLM scoresIn our approach, we consider the scores of the DLMwhen searching for the maximum spanning tree.Then for a given sentence x, we find y?DLM ,y?DLM = argmaxy?T (Gx)(?g?yscore(w, x, g)+scoreDLM (y))After adding the DLM scores, the new parsingmodel can capture richer information.
Figure 1 illus-trates the changes.
In the original first-order parsingmodel, we only utilize the information of single arc(xh, xL(k?1)) for xL(k?1) as shown in Figure 1-(a).If we use 3-gram DLM, we can utilize the additionalinformation of the two previous children (nearer toxh than xL(k?1)): xL(k?2) and xL(k?3) as shown inFigure 1-(b).Figure 1: Adding the DLM scores to the parsing model3.3 DLM-based feature templatesWe define DLM-based features for Dh =(xLk, ...xL1, xh, xR1...xRm).
For each child xch onthe left side, we have PLc(xch|HIS), where HISrefers to the N ?
1 immediate previous right chil-dren and head xh.
Similarly, we have PRc(xch|HIS)for each child on the right side.
Let Pu(xch|HIS)(Pu(ch) in short) be one of the above probabilities.We use the map function ?
(Pu(ch)) to obtain thepredefined discrete value (defined in Section 5.3).The feature templates are outlined in Table 1, whereTYPE refers to one of the types:PL or PR, h posrefers to the part-of-speech tag of xh, h word refersto the lexical form of xh, ch pos refers to the part-of-speech tag of xch, and ch word refers to the lexicalform of xch.4 DecodingIn this section, we turn to the problem of adding theDLM in the decoding algorithm.
We propose twoways: (1) Rescoring, in which we rescore the K-best list with the DLM-based features; (2) Intersect,215< ?
(Pu(ch)),TYPE >< ?
(Pu(ch)),TYPE, h pos >< ?
(Pu(ch)),TYPE, h word >< ?
(Pu(ch)),TYPE, ch pos >< ?
(Pu(ch)),TYPE, ch word >< ?
(Pu(ch)),TYPE, h pos, ch pos >< ?
(Pu(ch)),TYPE, h word, ch word >Table 1: DLM-based feature templatesin which we add the DLM-based features in the de-coding algorithm directly.4.1 RescoringWe add the DLM-based features into the decodingprocedure by using the rescoring technique used in(Shen et al, 2008).
We can use an original parserto produce the K-best list.
This method has the po-tential to be very fast.
However, because the perfor-mance of this method is restricted to the K-best list,we may have to set K to a high number in order tofind the best parsing tree (with DLM) or a tree ac-ceptably close to the best (Shen et al, 2008).4.2 IntersectThen, we add the DLM-based features in the decod-ing algorithm directly.
The DLM-based features aregenerated online during decoding.For our parser, we use the decoding algorithmof McDonald et al (2005).
The algorithm was ex-tensions of the parsing algorithm of (Eisner, 1996),which was a modified version of the CKY chartparsing algorithm.
Here, we describe how to addthe DLM-based features in the first-order algorithm.The second-order and higher-order algorithms canbe extended by the similar way.The parsing algorithm independently parses theleft and right dependents of a word and combinesthem later.
There are two types of chart items (Mc-Donald and Pereira, 2006): 1) a complete item inwhich the words are unable to accept more depen-dents in a certain direction; and 2) an incompleteitem in which the words can accept more dependentsin a certain direction.
In the algorithm, we createboth types of chart items with two directions for allthe word pairs in a given sentence.
The direction ofa dependency is from the head to the dependent.
Theright (left) direction indicates the dependent is on theright (left) side of the head.
Larger chart items arecreated from pairs of smaller ones in a bottom-upstyle.
In the following figures, complete items arerepresented by triangles and incomplete items arerepresented by trapezoids.
Figure 2 illustrates thecubic parsing actions of the algorithm (Eisner, 1996)in the right direction, where s, r, and t refer to thestart and end indices of the chart items.
In Figure2-(a), all the items on the left side are complete andthe algorithm creates the incomplete item (trapezoidon the right side) of s ?
t. This action builds a de-pendency relation from s to t. In Figure 2-(b), theitem of s ?
r is incomplete and the item of r ?
t iscomplete.
Then the algorithm creates the completeitem of s ?
t. In this action, all the children of r aregenerated.
In Figure 2, the longer vertical edge in atriangle or a trapezoid corresponds to the subroot ofthe structure (spanning chart).
For example, s is thesubroot of the span s ?
t in Figure 2-(a).
For the leftdirection case, the actions are similar.Figure 2: Cubic parsing actions of Eisner (Eisner, 1996)Then, we add the DLM-based features into theparsing actions.
Because the parsing algorithm isin the bottom-up style, the nearer children are gen-erated earlier than the farther ones of the same head.Thus, we calculate the left or right side probabil-ity for a new child when a new dependency rela-tion is built.
For Figure 2-(a), we add the features ofPRc(xt|HIS).
Figure 3 shows the structure, wherecRs refers to the current children (nearer than xt) ofxs.
In the figure, HIS includes cRs and xs.Figure 3: Add DLM-based features in cubic parsing216We use beam search to choose the one having theoverall best score as the final parse, where K spansare built at each step (Zhang and Clark, 2008).
Ateach step, we perform the parsing actions in the cur-rent beam and then choose the best K resulting spansfor the next step.
The time complexity of the new de-coding algorithm is O(Kn3) while the original oneis O(n3), where n is the length of the input sentence.With the rich feature set in Table 1, the running timeof Intersect is longer than the time of Rescoring.
ButIntersect considers more combination of spans withthe DLM-based features than Rescoring that is onlygiven a K-best list.5 Implementation Details5.1 Baseline parserWe implement our parsers based on the MSTParser1,a freely available implementation of the graph-basedmodel proposed by (McDonald and Pereira, 2006).We train a first-order parser on the training data (de-scribed in Section 6.1) with the features defined inMcDonald et al (2005).
We call this first-orderparser Baseline parser.5.2 Build dependency language modelsWe use a large amount of unannotated data to buildthe dependency language model.
We first performword segmentation (if needed) and part-of-speechtagging.
After that, we obtain the word-segmentedsentences with the part-of-speech tags.
Then thesentences are parsed by the Baseline parser.
Finally,we obtain the auto-parsed data.Given the dependency trees, we estimate the prob-ability distribution by relative frequency:Pu(xch|HIS) =count(xch,HIS)?x?chcount(x?ch,HIS)(7)No smoothing is performed because we use themapping function for the feature representations.5.3 Mapping functionWe can define different mapping functions for thefeature representations.
Here, we use a simple way.First, the probabilities are sorted in decreasing order.Let No(Pu(ch)) be the position number of Pu(ch)in the sorted list.
The mapping function is:1http://mstparser.sourceforge.net?
(Pu(ch)) ={ PH if No(Pu(ch)) ?
TOP10PM if TOP10 < No(Pu(ch)) ?
TOP30PL if TOP30 < No(Pu(ch))PO if Pu(ch)) = 0where TOP10 and TOP 30 refer to the position num-bers of top 10% and top 30% respectively.
The num-bers, 10% and 30%, are tuned on the developmentsets in the experiments.6 ExperimentsWe conducted experiments on English and Chinesedata.6.1 Data setsFor English, we used the Penn Treebank (Marcus etal., 1993) in our experiments.
We created a stan-dard data split: sections 2-21 for training, section22 for development, and section 23 for testing.
Tool?Penn2Malt?2 was used to convert the data into de-pendency structures using a standard set of headrules (Yamada and Matsumoto, 2003).
Followingthe work of (Koo et al, 2008), we used the MX-POST (Ratnaparkhi, 1996) tagger trained on trainingdata to provide part-of-speech tags for the develop-ment and the test set, and used 10-way jackknifingto generate part-of-speech tags for the training set.For the unannotated data, we used the BLLIP corpus(Charniak et al, 2000) that contains about 43 millionwords of WSJ text.3 We used the MXPOST taggertrained on training data to assign part-of-speech tagsand used the Baseline parser to process the sentencesof the BLLIP corpus.For Chinese, we used the Chinese Treebank(CTB) version 4.04 in the experiments.
We also usedthe ?Penn2Malt?
tool to convert the data and cre-ated a data split: files 1-270 and files 400-931 fortraining, files 271-300 for testing, and files 301-325for development.
We used gold standard segmenta-tion and part-of-speech tags in the CTB.
The datapartition and part-of-speech settings were chosen tomatch previous work (Chen et al, 2008; Yu et al,2008; Chen et al, 2009).
For the unannotated data,we used the XIN CMN portion of Chinese Giga-word5 Version 2.0 (LDC2009T14) (Huang, 2009),2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html3We ensured that the text used for extracting subtrees did notinclude the sentences of the Penn Treebank.4http://www.cis.upenn.edu/?chinese/.5We excluded the sentences of the CTB data from the Giga-word data217which has approximately 311 million words whosesegmentation and POS tags are given.
We discardedthe annotations due to the differences in annotationpolicy between CTB and this corpus.
We used theMMA system (Kruengkrai et al, 2009) trained onthe training data to perform word segmentation andPOS tagging and used the Baseline parser to parseall the sentences in the data.6.2 Features for basic and enhanced parsersThe previous studies have defined four types offeatures: (FT1) the first-order features defined inMcDonald et al (2005), (FT2SB) the second-orderparent-siblings features defined in McDonald andPereira (2006), (FT2GC) the second-order parent-child-grandchild features defined in Carreras (2007),and (FT3) the third-order features defined in (Kooand Collins, 2010).We used the first- and second-order parsers ofthe MSTParser as the basic parsers.
Then we en-hanced them with other higher-order features us-ing beam-search.
Table 2 shows the feature set-tings of the systems, where MST1/2 refers to the ba-sic first-/second-order parser and MSTB1/2 refers tothe enhanced first-/second-order parser.
MSTB1 andMSTB2 used the same feature setting, but used dif-ferent order models.
This resulted in the differenceof using FT2SB (beam-search in MSTB1 vs exact-inference in MSTB2).
We used these four parsers asthe Baselines in the experiments.System FeaturesMST1 (FT1)MSTB1 (FT1)+(FT2SB+FT2GC+FT3)MST2 (FT1+FT2SB)MSTB2 (FT1+FT2SB)+(FT2GC+FT3)Table 2: Baseline parsersWe measured the parser quality by the unlabeledattachment score (UAS), i.e., the percentage of to-kens (excluding all punctuation tokens) with the cor-rect HEAD.
In the following experiments, we used?Inter?
to refer to the parser with Intersect, and?Rescore?
to refer to the parser with Rescoring.6.3 Development experimentsSince the setting of K (for beam search) affects ourparsers, we studied its influence on the developmentset for English.
We added the DLM-based featuresto MST1.
Figure 4 shows the UAS curves on thedevelopment set, where K is beam size for Inter-sect and K-best for Rescoring, the X-axis representsK, and the Y-axis represents the UAS scores.
Theparsing performance generally increased as the Kincreased.
The parser with Intersect always outper-formed the one with Rescoring.0.9120.9140.9160.9180.920.9220.9240.9260.9281 2 4 8 16UASKRescoreInterFigure 4: The influence of K on the development dataK 1 2 4 8 16English 157.1 247.4 351.9 462.3 578.2Table 3: The parsing times on the development set (sec-onds for all the sentences)Table 3 shows the parsing times of Intersect onthe development set for English.
By comparing thecurves of Figure 4, we can see that, while usinglarger K reduced the parsing speed, it improved theperformance of our parsers.
In the rest of the ex-periments, we set K=8 in order to obtain the highaccuracy with reasonable speed and used Intersectto add the DLM-based features.N 0 1 2 3 4English 91.30 91.87 92.52 92.72 92.72Chinese 87.36 87.96 89.33 89.92 90.40Table 4: Effect of different N-gram DLMsThen, we studied the effect of adding different N-gram DLMs to MST1.
Table 4 shows the results.From the table, we found that the parsing perfor-mance roughly increased as the N increased.
WhenN=3 and N=4, the parsers obtained the same scoresfor English.
For Chinese, the parser obtained thebest score when N=4.
Note that the size of the Chi-nese unannotated data was larger than that of En-glish.
In the rest of the experiments, we used 3-gramfor English and 4-gram for Chinese.2186.4 Main results on English dataWe evaluated the systems on the testing data for En-glish.
The results are shown in Table 5, where -DLM refers to adding the DLM-based features to theBaselines.
The parsers using the DLM-based fea-tures consistently outperformed the Baselines.
Forthe basic models (MST1/2), we obtained absoluteimprovements of 0.94 and 0.63 points respectively.For the enhanced models (MSTB1/2), we found thatthere were 0.63 and 0.66 points improvements re-spectively.
The improvements were significant inMcNemar?s Test (p < 10?5)(Nivre et al, 2004).Order1 UAS Order2 UASMST1 90.95 MST2 91.71MST-DLM1 91.89 MST-DLM2 92.34MSTB1 91.92 MSTB2 92.10MSTB-DLM1 92.55 MSTB-DLM2 92.76Table 5: Main results for English6.5 Main results on Chinese dataThe results are shown in Table 6, where the abbrevi-ations used are the same as those in Table 5.
As inthe English experiments, the parsers using the DLM-based features consistently outperformed the Base-lines.
For the basic models (MST1/2), we obtainedabsolute improvements of 4.28 and 3.51 points re-spectively.
For the enhanced models (MSTB1/2),we got 3.00 and 2.93 points improvements respec-tively.
We obtained large improvements on the Chi-nese data.
The reasons may be that we use the verylarge amount of data and 4-gram DLM that captureshigh-order information.
The improvements weresignificant in McNemar?s Test (p < 10?7).Order1 UAS Order2 UASMST1 86.38 MST2 88.11MST-DLM1 90.66 MST-DLM2 91.62MSTB1 88.38 MSTB2 88.66MSTB-DLM1 91.38 MSTB-DLM2 91.59Table 6: Main results for Chinese6.6 Compare with previous work on EnglishTable 7 shows the performance of the graph-basedsystems that were compared, where McDonald06refers to the second-order parser of McDonaldand Pereira (2006), Koo08-standard refers to thesecond-order parser with the features defined inKoo et al (2008), Koo10-model1 refers to thethird-order parser with model1 of Koo and Collins(2010), Koo08-dep2c refers to the second-orderparser with cluster-based features of (Koo et al,2008), Suzuki09 refers to the parser of Suzuki etal.
(2009), Chen09-ord2s refers to the second-orderparser with subtree-based features of Chen et al(2009), and Zhou11 refers to the second-order parserwith web-derived selectional preference features ofZhou et al (2011).The results showed that our MSTB-DLM2 ob-tained the comparable accuracy with the previousstate-of-the-art systems.
Koo10-model1 (Koo andCollins, 2010) used the third-order features andachieved the best reported result among the super-vised parsers.
Suzuki2009 (Suzuki et al, 2009) re-ported the best reported result by combining a Semi-supervised Structured Conditional Model (Suzukiand Isozaki, 2008) with the method of (Koo et al,2008).
However, their decoding complexities werehigher than ours and we believe that the performanceof our parser can be further enhanced by integratingtheir methods with our parser.Type System UAS CostGMcDonald06 91.5 O(n3)Koo08-standard 92.02 O(n4)Koo10-model1 93.04 O(n4)SKoo08-dep2c 93.16 O(n4)Suzuki09 93.79 O(n4)Chen09-ord2s 92.51 O(n3)Zhou11 92.64 O(n4)D MSTB-DLM2 92.76 O(Kn3)Table 7: Relevant results for English.
G denotes the su-pervised graph-based parsers, S denotes the graph-basedparsers with semi-supervised methods, D denotes ournew parsers6.7 Compare with previous work on ChineseTable 8 shows the comparative results, whereChen08 refers to the parser of (Chen et al, 2008),Yu08 refers to the parser of (Yu et al, 2008), Zhao09refers to the parser of (Zhao et al, 2009), andChen09-ord2s refers to the second-order parser withsubtree-based features of Chen et al (2009).
Theresults showed that our score for this data was the219best reported so far and significantly higher than theprevious scores.System UASChen08 86.52Yu08 87.26Zhao09 87.0Chen09-ord2s 89.43MSTB-DLM2 91.59Table 8: Relevant results for Chinese7 AnalysisDependency parsers tend to perform worse on headswhich have many children.
Here, we studied the ef-fect of DLM-based features for this structure.
Wecalculated the number of children for each head andlisted the accuracy changes for different numbers.We compared the MST-DLM1 and MST1 systemson the English data.
The accuracy is the percentageof heads having all the correct children.Figure 5 shows the results for English, where theX-axis represents the number of children, the Y-axis represents the accuracies, OURS refers to MST-DLM1, and Baseline refers to MST1.
For example,for heads having two children, Baseline obtained89.04% accuracy while OURS obtained 89.32%.From the figure, we found that OURS achieved bet-ter performance consistently in all cases and whenthe larger the number of children became, the moresignificant the performance improvement was.0.40.50.60.70.80.911  2  3  4  5  6  7  8  9  10AccuracyNumber of childrenBaselineOURSFigure 5: Improvement relative to numbers of children8 Related workSeveral previous studies related to our work havebeen conducted.Koo et al (2008) used a clustering algorithm toproduce word clusters on a large amount of unan-notated data and represented new features based onthe clusters for dependency parsing models.
Chenet al (2009) proposed an approach that extractedpartial tree structures from a large amount of dataand used them as the additional features to im-prove dependency parsing.
They approaches werestill restricted in a small number of arcs in thegraphs.
Suzuki et al (2009) presented a semi-supervised learning approach.
They extended aSemi-supervised Structured Conditional Model (SS-SCM)(Suzuki and Isozaki, 2008) to the dependencyparsing problem and combined their method withthe approach of Koo et al (2008).
In future work,we may consider apply their methods on our parsersto improve further.Another group of methods are the co-training/self-training techniques.
McClosky etal.
(2006) presented a self-training approach forphrase structure parsing.
Sagae and Tsujii (2007)used the co-training technique to improve perfor-mance.
First, two parsers were used to parse thesentences in unannotated data.
Then they selectedsome sentences which have the same trees producedby those two parsers.
They retrained a parser onnewly parsed sentences and the original labeleddata.
We are able to use the output of our systemsfor co-training/self-training techniques.9 ConclusionWe have presented an approach to utilizing the de-pendency language model to improve graph-baseddependency parsing.
We represent new featuresbased on the dependency language model and in-tegrate them in the decoding algorithm directly us-ing beam-search.
Our approach enriches the featurerepresentations but without increasing the decodingcomplexity.
When tested on both English and Chi-nese data, our parsers provided very competitive per-formance compared with the best systems on the En-glish data and achieved the best performance on theChinese data in the literature.ReferencesS.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proc.
of220CoNLL-X.
SIGNLL.Xavier Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL 2007,pages 957?961, Prague, Czech Republic, June.
Asso-ciation for Computational Linguistics.Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,John Hale, and Mark Johnson.
2000.
BLLIP 1987-89 WSJ Corpus Release 1, LDC2000T43.
LinguisticData Consortium.Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto,Yujie Zhang, and Hitoshi Isahara.
2008.
Dependencyparsing with short dependency relations in unlabeleddata.
In Proceedings of IJCNLP 2008.Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,and Kentaro Torisawa.
2009.
Improving dependencyparsing with subtrees from auto-parsed data.
In Pro-ceedings of EMNLP 2009, pages 570?579, Singapore,August.Michael A. Covington.
2001.
A dundamental algorithmfor dependency parsing.
In Proceedings of the 39thAnnual ACM Southeast Conference, pages 95?102.Koby Crammer and Yoram Singer.
2003.
Ultraconser-vative online algorithms for multiclass problems.
J.Mach.
Learn.
Res., 3:951?991.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proceedings ofCOLING1996, pages 340?345.Chu-Ren Huang.
2009.
Tagged Chinese Gigaword Ver-sion 2.0, LDC2009T14.
Linguistic Data Consortium.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of ACL2010, pages 1?11, Uppsala, Sweden, July.
Associationfor Computational Linguistics.T.
Koo, X. Carreras, and M. Collins.
2008.
Simplesemi-supervised dependency parsing.
In Proceedingsof ACL-08: HLT, Columbus, Ohio, June.Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichiKazama, Yiou Wang, Kentaro Torisawa, and HitoshiIsahara.
2009.
An error-driven word-character hybridmodel for joint Chinese word segmentation and POStagging.
In Proceedings of ACL-IJCNLP2009, pages513?521, Suntec, Singapore, August.
Association forComputational Linguistics.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguisticss, 19(2):313?330.D.
McClosky, E. Charniak, and M. Johnson.
2006.Reranking and self-training for parser adaptation.
InProceedings of Coling-ACL, pages 337?344.R.
McDonald and J. Nivre.
2007.
Characterizing theerrors of data-driven dependency parsing models.
InProceedings of EMNLP-CoNLL, pages 122?131.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing algo-rithms.
In Proceedings of EACL 2006, pages 81?88.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of ACL 2005, pages 91?98.Association for Computational Linguistics.J.
Nivre, J.
Hall, and J. Nilsson.
2004.
Memory-baseddependency parsing.
In Proc.
of CoNLL 2004, pages49?56.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007shared task on dependency parsing.
In Proceedingsof the CoNLL Shared Task Session of EMNLP-CoNLL2007, pages 915?932.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
In Proceedings of EMNLP1996, pages 133?142.K.
Sagae and J. Tsujii.
2007.
Dependency parsing anddomain adaptation with LR models and parser ensem-bles.
In Proceedings of the CoNLL Shared Task Ses-sion of EMNLP-CoNLL 2007, pages 1044?1050.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL-08: HLT, pages 577?585, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-wordscale unlabeled data.
In Proceedings of ACL-08: HLT,pages 665?673, Columbus, Ohio, June.
Associationfor Computational Linguistics.Jun Suzuki, Hideki Isozaki, Xavier Carreras, and MichaelCollins.
2009.
An empirical study of semi-supervisedstructured conditional models for dependency parsing.In Proceedings of EMNLP2009, pages 551?560, Sin-gapore, August.
Association for Computational Lin-guistics.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.
InProceedings of IWPT 2003, pages 195?206.K.
Yu, D. Kawahara, and S. Kurohashi.
2008.
Chi-nese dependency parsing with large scale automati-cally constructed case structures.
In Proceedings ofColing 2008, pages 1049?1056, Manchester, UK, Au-gust.Y.
Zhang and S. Clark.
2008.
A tale of two parsers: In-vestigating and combining graph-based and transition-based dependency parsing.
In Proceedings of EMNLP2008, pages 562?571, Honolulu, Hawaii, October.Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.2009.
Cross language dependency parsing us-221ing a bilingual lexicon.
In Proceedings of ACL-IJCNLP2009, pages 55?63, Suntec, Singapore, Au-gust.
Association for Computational Linguistics.Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011.Exploiting web-derived selectional preference to im-prove statistical dependency parsing.
In Proceedingsof ACL-HLT2011, pages 1556?1565, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.222
