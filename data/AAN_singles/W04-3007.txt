Robustness Issues in a Data-Driven Spoken Language UnderstandingSystemYulan He and Steve YoungCambridge University Engineering DepartmentTrumpington Street, Cambridge CB2 1PZ, England{yh213, sjy}@eng.cam.ac.ukAbstractRobustness is a key requirement in spoken lan-guage understanding (SLU) systems.
Humanspeech is often ungrammatical and ill-formed,and there will frequently be a mismatch be-tween training and test data.
This paper dis-cusses robustness and adaptation issues in astatistically-based SLU system which is en-tirely data-driven.
To test robustness, the sys-tem has been tested on data from the Air TravelInformation Service (ATIS) domain which hasbeen artificially corrupted with varying levelsof additive noise.
Although the speech recog-nition performance degraded steadily, the sys-tem did not fail catastrophically.
Indeed, therate at which the end-to-end performance ofthe complete system degraded was significantlyslower than that of the actual recognition com-ponent.
In a second set of experiments, theability to rapidly adapt the core understandingcomponent of the system to a different appli-cation within the same broad domain has beentested.
Using only a small amount of trainingdata, experiments have shown that a semanticparser based on the Hidden Vector State (HVS)model originally trained on the ATIS corpuscan be straightforwardly adapted to the some-what different DARPA Communicator task us-ing standard adaptation algorithms.
The paperconcludes by suggesting that the results pre-sented provide initial support to the claim thatan SLU system which is statistically-based andtrained entirely from data is intrinsically robustand can be readily adapted to new applications.1 IntroductionSpoken language is highly variable as different peopleuse different words and sentence structures to convey thesame meaning.
Also, many utterances are grammatically-incorrect or ill-formed.
It thus remains an open issue as tohow to provide robustness for large populations of non-expert users in spoken dialogue systems.
The key compo-nent of a spoken language understanding (SLU) system isthe semantic parser, which translates the users?
utterancesinto semantic representations.
Traditionally, most seman-tic parser systems have been built using hand-crafted se-mantic grammar rules and so-called robust parsing (Wardand Issar, 1996; Seneff, 1992; Dowding et al, 1994) isused to handle the ill-formed user input in which wordpatterns corresponding to semantic tokens are used to fillslots in different semantic frames in parallel.
The framewith the highest score then yields the selected semanticrepresentation.Formally speaking, the robustness of language (recog-nition, parsing, etc.)
is a measure of the ability of hu-man speakers to communicate despite incomplete infor-mation, ambiguity, and the constant element of surprise(Briscoe, 1996).
In this paper, two aspects of SLU sys-tem performance are investigated: noise robustness andadaptability to different applications.
For the former, weexpect that an SLU system should maintain acceptableperformance when given noisy input speech data.
Thisrequires, the understanding components of the SLU sys-tem to be able to correctly interpret the meaning of anutterance even when faced with recognition errors.
Forthe latter, the SLU system should be readily adaptable toa different application using a relatively small set (e.g.less than 100) of adaptation utterances.The rest of the paper is organized as follows.
Anoverview of our data-driven SLU system is outlined insection 2.
Experimental results on performance under arange of SNRs are then presented in section 3.
Section 4discusses adaptation of the HVS model to new applica-tions.
Finally, section 5 concludes the paper.2 System OverviewSpoken language understanding (SLU) aims to interpretthe meanings of users?
utterances and respond reason-ably to what users have said.
A typical architecture ofan SLU system is given in Fig.
1, which consists of aspeech recognizer, a semantic parser, and a dialog act de-coder.
Within a statistical framework, the SLU problemcan be factored into three stages.
First the speech recog-nizer recognizes the underlying word string W from eachinput acoustic signal A, i.e.W?
= argmaxWP (W |A) = argmaxWP (A|W )P (W ) (1)then the semantic parser maps the recognized word stringW?
into a set of semantic concepts CC?
= argmaxCP (C|W? )
(2)and finally the dialogue act decoder infers the user?s dia-log acts or goals by solvingG?u = argmaxGuP (Gu|C?)
(3)Dialog ActDecoderSemanticParserSpeechRecognizerAcoustic Signal Words Concepts User?s Dialog ActsPSfrag replacementsA W C GuFigure 1: Typical structure of a spoken language under-standing system.The sequential decoding described above is suboptimalsince the solution at each stage depends on an exact so-lution to the previous stage.
To reduce the effect of thisapproximation, a word lattice or N -best word hypothesescan be retained instead of the single best string W?
as theoutput of the speech recognizer.
The semantic parse re-sults may then be incorporated with the output from thespeech recognizer to rescore the N -best list as below.C?, W?
?
argmaxC,W?LNP (A|W )P (W )P (C|W )?
argmaxC,W?LNP (A|W )P (W )?P (C|W )?
(4)where P (A|W ) is the acoustic probability from thefirst pass, P (W ) is the language modelling likelihood,P (C|W ) is the semantic parse score, LN denotes the N -best list, ?
is a semantic parse scale factor, and ?
is agrammar scale factor.In the system described in this paper, each of thesestages is modelled separately.
We use a standard HTK-based (HTK, 2004) Hidden Markov Model (HMM) rec-ognizer for recognition, the Hidden Vector State (HVS)model for semantic parsing (He and Young, 2003b), andTree-Augmented Naive Bayes networks (TAN) (Fried-man et al, 1997) for dialog act decoding.The speech recognizer comprises 14 mixture Gaus-sian HMM state-clustered cross-word triphones aug-mented by using heteroscedastic linear discriminant anal-ysis (HLDA) (Kumar, 1997).
Incremental speaker adap-tation based on the maximum likelihood linear regression(MLLR) method (Gales and Woodland, 1996) was per-formed during the test with updating being performed inbatches of five utterances per speaker.The Hidden Vector State (HVS) model (He and Young,2003b) is a hierarchical semantic parser which associateseach state of a push-down automata with the state of aHMM.
State transitions are factored into separate stackpop and push operations and then constrained to givea tractable search space.
The result is a model whichis complex enough to capture hierarchical structure butwhich can be trained automatically from unannotateddata.CITY DATESSSETOLOC ONRETURNSSDUMMYSS DUMMYSS SSDallassent_start I want to return toRETURNSSRETURNTOLOCCITYSSRETURNTOLOCRETURNRETURNSSONDATEThursday sent_endSESSONonFigure 2: Example of a parse tree and its vector stateequivalent.Let each state at time t be denoted by a vector of Dtsemantic concept labels (tags) ct = [ct[1], ct[2], ..ct[Dt]]where ct[1] is the preterminal concept and ct[Dt] is theroot concept (SS in Figure 2).
Given a word sequenceW , concept vector sequence C and a sequence of stackpop operations N , the joint probability of P (W,C, N)can be decomposed asP (W,C, N) =T?t=1P (nt|ct?1) ?P (ct[1]|ct[2 ?
?
?Dt]) ?
P (wt|ct) (5)where ct at word position t is a vector of Dt semanticconcept labels (tags), nt is the vector stack shift operationand takes values in the range 0, ?
?
?
, Dt?1 where Dt?1 isthe stack size at word position t ?
1, and ct[1] = cwt isthe new preterminal semantic tag assigned to word wt atword position t.Thus, the HVS model consists of three types of proba-bilistic move:1. popping semantic tags off the stack;2. pushing a pre-terminal semantic tag onto the stack;3. generating the next word.The dialog act decoder was implemented using theTree-Augmented Naive Bayes (TAN) algorithm (Fried-man et al, 1997), which is an extension of Naive BayesNetworks.
One TAN was used for each dialogue act orgoal Gu, the semantic concepts Ci which serve as inputto its corresponding TAN were selected based on the mu-tual information (MI) between the goal and the concept.Naive Bayes networks assume all the concepts are con-ditionally independent given the value of the goal.
TANnetworks relax this independence assumption by addingdependencies between concepts based on the conditionalmutual information (CMI) between concepts given thegoal.
The goal prior probability P (Gu) and the condi-tional probability of each semantic concept Ci given thegoal Gu, P (Ci|Gu) are learned from the training data.Dialogue act detection is done by picking the goal withthe highest posterior probability of Gu given the particu-lar instance of concepts C1 ?
?
?Cn, P (Gu|C1 ?
?
?Cn).3 Noise RobustnessThe ATIS corpus which contains air travel informationdata (Dahl et al, 1994) has been chosen for the SLU sys-tem development and evaluation.
ATIS was developedin the DARPA sponsored spoken language understandingprogramme conducted from 1990 to 1995 and it providesa convenient and well-documented standard for measur-ing the end-to-end performance of an SLU system.
How-ever, since the ATIS corpus contains only clean speech,corrupted test data has been generated by adding samplesof background noise to the clean test data at the waveformlevel.3.1 Experimental SetupThe experimental setup used to evaluate the SLU systemwas similar to that described in (He and Young, 2003a).As mentioned in section 2, the SLU system consists ofthree main components, a standard HTK-based HMMrecognizer, the HVS semantic parser, and the TAN dia-logue act (DA) decoder.
Each of the three major compo-nents are trained separately.
The acoustic speech signalin the ATIS training data is modelled by extracting 39features every 10ms: 12 cepstra, energy, and their firstand second derivatives.
This data is then used to train thespeaker-independent, continuous speech recognizer.
TheHVS semantic parser is trained on the unannotated utter-ances using EM constrained by the domain-specific lex-ical class information and the dominance relations builtinto the abstract annotations (He and Young, 2003b).
Inthe case of ATIS, the lexical classes can be extracted au-tomatically from the relational database, whilst abstractsemantic annotations for each utterance are automaticallyderived from the accompanying SQL queries of the train-ing utterances.
The dialogue act decoder is trained usingthe main topics or goals and the key semantic conceptsextracted automatically from the reference SQL queriesPerformance is measured at both the component andthe system level.
For the former, the recognizer is eval-uated by word error rate, the parser by concept slot re-trieval rate using an F-measure metric (Goel and Byrne,1999), and the dialog act decoder by detection rate.
Theoverall system performance is measured using the stan-dard NIST ?query answer?
rate.In the expriments reported here, car noise from theNOISEX-92 (Varga et al, 1992) database was added tothe ATIS-3 NOV93 and DEC94 test sets.
In order to ob-tain different SNRs, the noise was scaled accordingly be-fore adding to the speech signal.3.2 Experimental ResultsRobust spoken language understanding componentsshould be able to compensate for the weakness of thespeech recognizer.
That is, ideally they should be capableof generating the correct meaning of an utterance evenif it is recognized wrongly by a speech recognizer.
Atminimum, the performance of the understanding compo-nents should degrade gracefully as recognition accuracydegrades.Figure 3 gives the system performance on the cor-rupted test data with additive noise ranging from 25dB to10dB SNR.
The label ?clean?
in the X-axis denotes theoriginal clean speech data without additive noise.
Notethat the recognition results on the corrupted test datawere obtained directly using the original clean speechHMM models without retraining for the noisy condi-tions.
The upper portion of Figure 3 shows the end-to-end performance in terms of query answer error rate forthe NOV93 and DEC94 test sets.
For easy reference,WER is also shown.
The individual component perfor-mance, F-measure for the HVS semantic parser and di-alogue act (DA) detection accuracy for the DA decoder,are illustrated in the lower portion of Figure 3.
For eachtest set, the performance on the rescored word hypothe-ses is given as well.
This incorporates the semantic parsescores into the acoustic and language modelling likeli-hoods to rescore the 25-best word lists from the speechrecognizer.It can be observed that the system gives fairly stableperformance at high SNRs and then the recognition accu-racy degrades rapidly in the presence of increasing noise.At 20dB SNR, the WER for the NOV93 test set increasesby 1.6 times relative to clean whilst the query answererror rate increases by only 1.3 times.
On decreasingthe SNR to 15dB, the system performance degrades sig-nificantly.
The WER increases by 3.1 times relative toclean but the query answer error rate increases by only1.7 times.
Similar figures were obtained for the DEC94test set.The above suggests that the end-to-end performancemeasured in terms of answer error rate degrades moreslowly compared to the recognizer WER as the noiselevel increases.
This demonstrates that the statistically-based understanding components of the SLU system, thesemantic parser and the dialogue act decoder, are rela-tively robust to degrading recognition performance.Regarding the individual component performance, thedialogue act detection accuracy appears to be less sensi-tive to decreasing SNR.
This is probably a consequenceof the fact that the Bayesian networks are set up to re-spond to only the presence or absence of semantic con-cepts or slots, regardless of the actual values assigned tothem.
In another words, the performance of the dialogueact decoder is not affected by the mis-recognition of indi-vidual words, but only by a failure to detect the presenceof a semantic concept.
It can also be observed from Fig-ure 3 that the F-measure needs to be better than 85% inorder to achieve acceptable end-to-end performance.4 Adaptation to New ApplicationsStatistical model adaptation techniques are widely usedto reduce the mismatch between training and test or toadapt a well-trained model to a novel domain.
Com-monly used techniques can be classified into two cat-egories, Bayesian adaptation which uses a maximum aposteriori (MAP) probability criteria (Gauvain and Lee,1994) and transformation-based approaches such as max-imum likelihood linear regression (MLLR) (Gales andWoodland, 1996), which uses a maximum likelihood(ML) criteria.
In recent years, MAP adaptation has beensuccessfully applied to n-gram language models (Bac-chiani and Roark, 2003) and lexicalized PCFG models(Roark and Bacchiani, 2003).
Luo et al have proposedtransformation-based approaches based on the Markovtransform (Luo et al, 1999) and the Householder trans-form (Luo, 2000), to adapt statistical parsers.
However,the optimisation processes for the latter are complex andit is not clear how general they are.Since MAP adaptation is straightforward and has beenapplied successfully to PCFG parsers, it has been selectedfor investigation in this paper.
Since one of the specialforms of MAP adaptation is interpolation between the in-domain and out-of-domain models, it is natural to alsoconsider the use of non-linear interpolation and hence thishas been studied as well 1.1Experiments using linear interpolation have also been con-ducted but it was found that the results are worse than those4.1 MAP AdaptationBayesian adaptation reestimates model parameters di-rectly using adaptation data.
It can be implemented viamaximum a posteriori (MAP) estimation.
Assuming thatmodel parameters are denoted by ?, then given observa-tion samples Y , the MAP estimate is obtained as?MAP = argmax?P (?|Y ) = argmax?P (Y |?
)P (?
)(6)where P (Y |?)
is the likelihood of the adaptation data Yand model parameters ?
are random vectors described bytheir probabilistic mass function (pmf) P (?
), also calledthe prior distribution.In the case of HVS model adaptation, the objective is toestimate probabilities of discrete distributions over vectorstate stack shift operations and output word generation.Assuming that they can be modelled under the multino-mial distribution, for mathematical tractability, the con-jugate prior, the Dirichlet density, is normally used.
As-sume a parser model P (W,C) for a word sequence Wand semantic concept sequence C exists with J compo-nent distributions Pj each of dimension K, then givensome adaptation data Wl, the MAP estimate of the kthcomponent of Pj , P?j(k), isP?j(k) =?j?j + ?P?j(k) +?
?j + ?Pj(k) (7)where ?j =?Kk=1 ?j(k) in which ?j(k) is defined as thetotal count of the events associated with the kth compo-nent of Pj summed across the decoding of all adaptationutterances Wl, ?
is the prior weighting parameter, Pj(k)is the probability of the original unadapted model, andP?j(k) is the empirical distribution of the adaptation data,which is defined asP?j(k) =?j(k)?Ki=1 ?j(i)(8)As discussed in section 2, the HVS model consists ofthree types of probabilistic move.
The MAP adaptationtechnique can be applied to the HVS model by adaptingeach of these three component distributions individually.4.2 Log-Linear InterpolationLog-linear interpolation has been applied to languagemodel adaptation and has been shown to be equivalentto a constrained minimum Kullback-Leibler distance op-timisation problem(Klakow, 1998).Following the notation introduced in section 4.1, wherePj(k) is the probability of the original unadapted model,and P?j(k) is the empirical distribution of the adaptationobtained using MAP adaptation or log-linear interpolation.clean 25dB 20dB 15dB 10dB3.58.513.518.523.528.533.538.543.5Speech to Noise Ratio ?
SNR (NOV93 Test Set)SpokenLanguageUnderstandingError Rate(%)WERWER with RescoringAnswer ErrorAnswer Error with Rescoring(a) NOV93 End-to-End Performanceclean 25dB 20dB 15dB 10dB2.57.512.517.522.527.532.5Speech to Noise Ratio ?
SNR (DEC94 Test Set)SpokenLanguageUnderstandingError Rate(%)WERWER with RescoringAnswer ErrorAnswer Error with Rescoring(c) DEC94 End-to-End Performanceclean 25dB 20dB 15dB 10dB0.70.750.80.850.90.95Speech to Noise Ratio ?
SNR (NOV93 Test Set)F?measureandDADetectionAccuracyF?measureF?measure with RescoringDA Detection AccuracyDA Detection Accuracy with Rescoring(b) NOV93 Component Performanceclean 25dB 20dB 15dB 10dB0.820.840.860.880.90.92Speech to Noise Ratio ?
SNR (DEC94 Test Set)F?measureandDADetectionAccuracyF?measureF?measure with RescoringDA Detection AccuracyDA Detection Accuracy with Rescoring(d) DEC94 Component PerformanceFigure 3: SLU system performance vs SNR.data, denote the final adapted model probability as P?j(k).It is assumed that the Kullback-Leibler distance of theadapted model to the unadapted and empirically deter-mined model isD(P?j(k) ?
Pj(k)) = d1 (9)D(P?j(k) ?
P?j(k)) = d2 (10)Given an additional model probability P?j(k) whosedistance to P?j(k) should be kept small, and introducingLagrange multipliers ?
?1 and ?
?2 to ensure that constraints9 and 10 are satistifed, yieldsD = D(P?j(k) ?
P?j(k))+?
?1(D(P?j(k) ?
Pj(k))?d1)+ ?
?2(D(P?j(k) ?
P?j(k)) ?
d2) (11)Minimizing D with respect to P?j(k) yields the requireddistribution.With some manipulation and redefinition of the La-grange Multipliers, it can be shown thatP?j(k) =1Z?Pj(k)?1 P?j(k)?2 (12)where P?j(k) has been assumed to be a uniform distribu-tion which is then absorbed into the normalization termZ?.The computation of Z?
is very expensive and can usu-ally be dropped without significant loss in performance(Martin et al, 2000).
For the other parameters, ?1 and?2, the generalized iterative scaling algorithm or the sim-plex method can be employed to estimate their optimalsettings.4.3 ExperimentsTo test the portability of the statistical parser, the initialexperiments reported here are focussed on assessing theadaptability of the HVS model when it is tested in a do-main which covers broadly similar concepts, but com-prises rather different speaking styles.
To this end, theflight information subset of the DARPA CommunicatorTravel task has been used as the target domain (CUD-ata, 2004).
By limiting the test in this way, we ensurethat the dimensionalities of the HVS model parametersremain the same and no new semantic concepts are intro-duced by the adaptation training data.The baseline HVS parser was trained on the ATIScorpus using 4978 utterances selected from the context-independent (Class A) training data in the ATIS-2 andATIS-3 corpora.
The vocabulary size of the ATIS trainingcorpus is 611 and there are altogether 110 semantic con-cepts defined.
The parser model was then adapted usingutterances relating to flight reservation from the DARPACommunicator data.
Although the latter bears similari-ties to the ATIS data, it contains utterances of a differentstyle and is often more complex.
For example, Commu-nicator contains utterances on multiple flight legs, infor-mation which is not available in ATIS.To compare the adapted ATIS parser with an in-domainCommunicator parser, a HVS model was trained fromscratch using 10682 Communicator training utterances.The vocabulary size of the in-domain Communicatortraining data is 505 and a total of 99 semantic conceptshave been defined.
For all tests, a set of 1017 Communi-cator test utterances was used.Table 1 lists the recall, precision, and F-measure re-sults obtained when tested on the 1017 utterance DARPACommunicator test set.
The baseline is the unadaptedHVS parser trained on the ATIS corpus only.
The in-domain results are obtained using the HVS parser trainedsolely on the 10682 DARPA training data.
The other rowsof the table give the parser performance using MAP andlog-linear interpolation based adaptation of the baselinemodel using 50 randomly selected adaptation utterances.System Recall Precision F-measureBaseline 79.81% 87.14% 83.31%In-domain 87.18% 91.89% 89.47%MAP 86.74% 91.07% 88.85%Log-Linear 86.25% 92.35% 89.20%Table 1: Performance comparison of adaptation usingMAP or log-linear interpolation.Since we do not yet have a reference database for theDARPA Communicator task, it is not possible to conductthe end-to-end performance evaluation as in section 3.However, the experimental results in section 3.2 indi-cate that the F-measure needs to exceed 85% to give ac-ceptable end-to-end performance (see Figure 3).
There-fore, it can be inferred from Table 1 that the unadaptedATIS parser model would perform very badly in the newCommunicator application whereas the adapted modelswould give performance close to that of a fully trainedin-domain model.Figure 4 shows the parser performance versus the num-ber of adaptation utterances used.
It can be observed thatwhen there are only a few adaptation utterances, MAPadaptation performs significantly better than log-linearinterpolation.
However above 25 adaptation utterances,the converse is true.
The parser performance saturateswhen the number of adaptation utterances reaches 50 forboth techniques and the best performance overall is givenby the parser adapted using log-linear interpolation.
Theperformance of both models however degrades when thenumber of adaptation utterances exceeds 100, possiblydue to model overtraining.
For this particular application,we conclude that just 50 adaptation utterances would besufficient to adapt the baseline model to give comparableresults to the in-domain Communicator model.0 25 50 75 100 125 1500.820.840.860.880.9Adaptation Training Utterance NumberF?measureMAPLog?LinearFigure 4: F-measure vs amount of adaptation trainingdata.5 ConclusionsThe spoken language understanding (SLU) system dis-cussed in this paper is entirely statistically based.
Therecogniser uses a HMM-based acoustic model and an n-gram language model, the semantic parser uses a hid-den vector state model and the dialogue act decoder usesBayesian networks.
The system is trained entirely fromdata and there are no heuristic rules.
One of the majorclaims motivating the design of this type of system isthat its fully-statistical framework makes it intrinsicallyrobust and readily adaptable to new applications.
Theaim of this paper has been to investigate this claim ex-perimentally via two sets of experiments using a systemtrained on the ATIS corpus.In the first set of experiments, the acoustic test datawas corrupted with varying levels of additive car noise.The end-to-end system performance was then measuredalong with the individual component performances.
Itwas found that although the addition of noise had a sub-stantial effect on the word error rate, its relative influ-ence on both the semantic parser slot/value retrieval rateand the dialogue act detection accuracy was somewhatless.
Overall, the end-to-end error rate degraded rela-tively more slowly than word error rate and perhaps mostimportantly of all, there was no catastrophic failure pointat which the system effectively stops working, a situationnot uncommon in current rule-based systems.In the second set of experiments, the ability of the se-mantic decoder component to be adapted to another ap-plication was investigated.
In order, to limit the issues toparameter mismatch problems, the new application cho-sen (Communicator) covered essentially the same set ofconcepts but was a rather different corpus with differentuser speaking styles and different syntactic forms.
Over-all, we found that moving a system trained on ATIS tothis new application resulted in a 6% absolute drop in F-measure on concept accuracy (i.e.
a 62% relative increasein parser error) and by extrapolation with the results inthe ATIS domain, we infer that this would make the non-adapted system essentially unusable in the new applica-tion.
However, when adaptation was applied using only50 adaptation sentences, the loss of concept accuracy wasmostly restored.
Specifically, using log-linear adapta-tion, the out-of-domain F-measure of 83.3% was restoredto 89.2% which is close to the in-domain F-measure of89.5%.Although these tests are preliminary and are based onoff-line corpora, the results do give positive support tothe initial claim made for statistically-based spoken lan-guage systems, i.e.
that they are robust and they are read-ily adaptable to new or changing applications.AcknowledgementsThe authors would like to thank Mark Gales for providingthe software to generate the corrupted speech data withadditive noise.ReferencesM.
Bacchiani and B. Roark.
2003.
Unsupervised lan-guage model adaptation.
In Proc.
of the IEEE Intl.Conf.
on Acoustics, Speech and Signal Processing,Hong Kong, Apr.T.
Briscoe.
1996.
Robust parsing.
In R. Cole, J. Mariani,H.
Uszkoreit, A. Zaenen, and V. Zue, editors, Surveyof the State of the Art of Human Language Technology,chapter 3.7.
Cambridge University Press, Cambridge,England.CUData, 2004.
DARPA Communicator TravelData.
University of Colorado at Boulder.http://communicator.colorado.edu/phoenix.D.A.
Dahl, M. Bates, M. Brown, K. Hunicke-Smith,D.
Pallett, C. Pao, A. Rudnicky, and L. Shriberg.
1994.Expanding the scope of the ATIS task: the ATIS-3corpus.
In ARPA Human Language Technology Work-shop, Princeton, NJ, Mar.J.
Dowding, R. Moore, F. Andry, and D. Moran.1994.
Interleaving syntax and semantics in an efficientbottom-up parser.
In Proc.
of the 32nd Annual Meet-ing of the Association for Computational Linguistics,pages 110?116, Las Cruces, New Maxico, June.N.
Friedman, D. Geiger, and M. Goldszmidt.
1997.Bayesian network classifiers.
Machine Learning,29(2):131?163.M.J.
Gales and P.C.
Woodland.
1996.
Mean and varianceadaptation within the MLLR framework.
ComputerSpeech and Language, 10:249?264, Oct.J.L.
Gauvain and C.-H. Lee.
1994.
Maximum a poste-riori estimation for multivariate Gaussian mixture ob-servations of Markov chains.
IEEE Trans.
on Speechand Audio Processing, 2(2):291?298.V.
Goel and W. Byrne.
1999.
Task dependent lossfunctions in speech recognition: Application to namedentity extraction.
In ESCA ETRW Workshop on Ac-cessing Information from Spoken Audio, pages 49?53,Cambridge, UK.Yulan He and Steve Young.
2003a.
A data-driven spokenlanguage understanding system.
In IEEE AutomaticSpeech Recognition and Understanding Workshop, St.Thomas, U.S. Virgin Islands, Dec.Yulan He and Steve Young.
2003b.
Hidden vector statemodel for hierarchical semantic parsing.
In Proc.
ofthe IEEE Intl.
Conf.
on Acoustics, Speech and SignalProcessing, Hong Kong, Apr.HTK, 2004.
Hidden Markov Model Toolkit (HTK)3.2.
Cambridge University Engineering Department.http://htk.eng.cam.ac.uk/.D.
Klakow.
1998.
Log-linear interpolation of languagemodels.
In Proc.
of Intl.
Conf.
on Spoken LanguageProcessing, Sydney, Australia, Nov.N.
Kumar.
1997.
Investigation of Silicon Auditory Mod-els and Generalization of Linear Discriminant analysisfor Improved Speech Recognition.
Ph.D. thesis, JohnsHopkins University, Baltimore MD.X.
Luo, S. Roukos, and T. Ward.
1999.
Unsupervisedadaptation of statistical parsers based on Markov trans-form.
In IEEE Automatic Speech Recognition and Un-derstanding Workshop, Keystone, Colorado, Dec.X.
Luo.
2000.
Parser adaptation via householder trans-form.
In Proc.
of the IEEE Intl.
Conf.
on Acoustics,Speech and Signal Processing, Istanbul, Turkey, June.S.
Martin, A. Kellner, and T. Portele.
2000.
Interpolationof stochastic grammar and word bigram models in nat-ural language understanding.
In Proc.
of Intl.
Conf.
onSpoken Language Processing, Beijing, China, Oct.B.
Roark and M. Bacchiani.
2003.
Supervised and unsu-pervised PCFG adaptation to novel domains.
In Pro-ceedings of the joint meeting of the North AmericanChapter of the Association for Computational Linguis-tics and the Human Language Technology Conference(HLT-NAACL 2003), Edmonton, Canada, May.S.
Seneff.
1992.
Robust parsing for spoken languagesystems.
In Proc.
of the IEEE Intl.
Conf.
on Acoustics,Speech and Signal Processing, San Francisco.A.P.
Varga, H.J.M.
Steeneken, M. Tomlinson, andD.
Jones.
1992.
The NOISEX-92 study on the ef-fect of additive noise on automatic speech recognition.Technical report, DRA Speech Research Unit.W.
Ward and S. Issar.
1996.
Recent improvements in theCMU spoken language understanding system.
In Proc.of the ARPA Human Language Technology Workshop,pages 213?216.
Morgan Kaufman Publishers, Inc.
