Towards  a Se l f -Extend ing  Lexicon*Uri ZernikMichael G. DyerArtificial Intelligence LaboratoryComputer Science Department3531 Boelt~r HallUniversity of tMifomisLos Angeles, California 90024Abst ractThe problem of manually modifying the lexiconappears with any natural  language processing program.Ideally, a program should be able to acquire new lexiealentries from context, the way people learn.
We addressthe problem of acquiring entire phrases, specificallyJigurative phr~es, through augmenting a phr~al ezico~Facilitating such a self-extending lexicon involves (a)disambiguation~se|ection of the intended phrase from aset of matching phrases, (b) robustparsin~-comprehension of partial ly-matching phrases,and (c) error analysis---use of errors in forming hy-potheses about new phrases.
We have designed and im-plemented a program called RINA which uses demons toimplement funetional-~rammar principles.
RINA receivesnew figurative phrases in context and through the appli-cation of a sequence of failure-driven rules, creates andrefines both the patterns and the concepts which holdsyntactic and semantic information about phrases.David vs. GoliathNative:Learner:Native:Learner:Native:Learner:Native:Remember the s~ory of David and Goliath?David took on Gol iath.David took GoltLth sons,here?No.
David took on Gol iath.He took on him.
He yon the f ight?No.
He took him on.David attacked him.He ~ok him on.He accepted She challenge?Right.Native:Learner:Here in annt,her story.John took on the th ird exam question.He took on a hard problem.Another dialogue involves put one's foot do~-a.
Again,the phrase is unknown while its constituents are known:Going Punk1.
In t roduct ionA language understanding program should be ableto acquire new lexical items from context, forming fornovel phrases their linguistic patterns and figuring outtheir conceptual meanings.
The lexicon of a learningprogram should satisfy three requirements: Each lexicalentry should (1) be learnable, (2) facilitate conceptualanalysis, and (3) facilitate generation.
In this paper wefocus on the first two aspects.1.1 The  Task  DomainTwo examples, which will be used throughout hispaper, are given below.
In the first dialogue the learneris introduced to an unknown phrase: take on.
Thewords take and on are familiar to the learner, who alsoremembers the biblical story of David and Goliath.
Theprogram, modeling a language learner, interacts with anative speaker, as follows:* This work w~s made possible in part by s grant from the KeckFoundation.Native:Learner:Native:Learner:Jenny vant,ed ~o go punk,but, her father put, his toot dovu.He moved his foot dora?It, doen not, mike sense.No.
He put his foot, dora.He put his foot dovu.He refused to le t  her go punk.A figurative phrase such as put one's fooc down is alinguistic pattern whose associated meaning cannot beproduced from the composition of its constituents.Indeed, an interpretation of the phrase based on themeanings of its constituents often exists, but it carries adifferent meaning.
The fact that this literal interpreta-tion of the figurative phrase exists is a misleading clue inlearning.
Furthermore, the learner may not even noticethat a novel phrase has been introduced since she is fam-iliar with dram as well as with foot.
Becker \[Becker?5\]has described a space of phrases ranging in generalityfrom fixed proverbs such as char i ty  begsns at, homethrough idioms such as Xay dove t,he tar  and phrasalverbs such as put, up r ich one's  spouse and look up thename, to literal verb phrases such as sit, on she chair.He suggested employing a phrasal exicon to capture thisentire range o( language structures.2841.2 Issues in Phrase  AequLsitionThree issues must be addressed when learningphrases in context.
(I) Detecting failures: What are the indications thatthe initial interpretation of the phrase take him onas "to take a person to a location" is incorrect?
Sinceall the words in the sentence are known, the problemis detected both as a conceptual discrepancy (whywould he take his enemy anywhere?)
and as a syn-tactic failure (the expected location of the assuniedphysical transfer is missing).
(2) Determining scope and general i ty of patterns:The linguistic pattern of a phrase may be perceivedby the learner at various levels of generalit~l.
For ex-ample, in the second dialogue, incorrect generaliza-tions could yield patterns accepting sentences uchas:Her boss  put  h i s  le f t  foot  down.He moved h is  foot  dora.He put  down h is  foot .He put  dovn h is  leg.
(3)A decision is also required about the scope of thepattern (i.e., the tokens included in the pattern).For instance, the scope of the pattern in John put upwith Mary could be (I) ?x:persoa put:verb up wherewith is associated with l'lmry or (2) ?x:persosput :verb  up with ?y :persou ,  where with is associatedwith put up.Finding appropr ia te  meanings: The conceptualmeaning of the phrase must be extracted from thecontext which contains many concepts, both ap-propriate and inappropriate for hypothesis forma-tion.
Thus there must be strategies for focusing onappropriate lements in the context.1.3 The ProgramRINA \[Dyer85\] is a computer program designed tolearn English phrases.
It takes as input English sentenceswhich may include unknown phrases and conveys as out-put its hypotheses about novel phrases.
The pro~amconsists of four components:(l) Phrasal  lexicon: This is a list of phrases whereeach phrase is a declarative pattern-concept air\[WilenskySl\].
(2) Case-frame parser: In the parsing process, case-frame expectations are handled by spawning demons\[Dyer83\].
The parser detects comprehension failureswhich are used in learning.
(3) Pattern Constructor: Learning of phrase patternsis accomplished by analyzing parsing failures.
Eachfailure situation is associated with a pattern-modification action.
(4) Concept  Constructor :  Learning of phrase conceptsis accomplished by a set of strategies which areselected according to the context.Schematically, the program receives a sequence ofsentence/contezt pairs from which it refines its currentpattern/concept air.
The pattern is derived from thesentence and the concept is derived from the coLtext.However, the two processes are not independent sincethe context influences construction of patterns whilelinguistic clues in the sentence influence formation ofconcepts.2.
Phrasa l  Representat ion of the LexiconParsing in RINA is central since learning isevaluated in terms of parsing ability before and afterphrases are acquired.
Moreover, learning is accomplishedthrough parsing.2.1 The  BackgroundRINA combines elements of the following two ap-proaches to language processing:Phra~-bued pat tern  matching: In the imple-mentation of UC \[Wilensky84\], an intelligent help systemfor UNIX users, both PHRAN \[AJ'ens82 l, the conceptualanalyzer, and PHRED \[Jacobs85\] the generator, share aphrasal lepton.
As outlined by Wilensky {Wilensky81\]this lexicon provides a declarative database, being modu-larly separated from the control part of the system whichcarries out parsing and generation.
This development inrepresentation of linguistic knowledge is paralleled bytheories of functional grammars {Kay79\[, and lezical-functional grammars \[Bresnan78\].Ca~,-b,,-,,ed demon pmming: Boris \[DyerS3 Imodeled reading and understanding stories in depth.
Itsconceptual analyzer employed demon-based templatesfor parsing and for generation.
Demons are used in pars-ing for two purposes: (1) to implement syntactic and se-mantic expectations \[Riesbeck74\] and (2) to implementmemory operations uch as search, match and update.This approach implements Schank's \[Schank77\] theory ofrepresentation of concepts, and follows case-grammar\[Fillmore681 principles.RINA uses a declarative phrasal lexicon as sug-gested by Wilensky \[Wilensky82\], where a lexical phraseis a pattern-concept pair.
The pattern notation isdescribed below and the concept notation is Dyer's\[Dyer83\] i-link notation.2852.2 The  Pat tern  Notat ionTo span English sentences, R INA uses two kindsof patterns: lezical patterns and ordering patterns\[Arens82\].
In Figure I we show sample lexical patterns(patterns of lexical phrases).
Such patterns are viewed asthe generic linguistic forms of their correspondingphrases.I.
?x: (animate.a~ent) n ibble :verb <on ?y: food>2.
?z: Cpernou.Lgent) tLke:verb on ?y :p , t lent3.
?x: (person.a~ent) <put:verb foot :body-par t  do~m>Figure h The  Pat tern  NotationThe notation is explained below:(t) A token is a literal unless otherwise specified.
For ex-ample, on is a literal in the patterns above.
(2) ?x:sort denotes a variable called .~x of a semantictype sort.
?y:food above is a variable which standsfor references to objects of the semantic lass food.
(3) Act.verb denotes any form of the verb s!lntacticclass with the root act.
nibble:vet6 above stands forexpressions uch as: nibbled,  hms never nibbled,etc.
(4) By default, a pattern sequence does not specify theorder of its tokens.
(5) Tokens delimited by < and > are restricted totheir specified order.
In Pattern I above, on mustdirectly precede ?y:food.Ordering patterns pertain to language word-order con-ventions in general.
Some sample ordering patterns are:active: <?x:agenr.
?y: (verb .~t ive)>passive: <?x:pat tent  ?y: (verb.p~,.s?ve)>*<by ?Z : agent>infinitive:<to ?x: verb.
act ive> "?y: Iq~entFigure 2: Order ing  Pat ternsThe additional notation introduced here is:(6) An * preceding a term, such as *<by ?z:~ent> inthe first pattern above indicates that the term is op-tional.
(7) * denotes an omitted term.
The concept for Ty in thethird example above is extracted from the agent ofthe pattern including the current pattern.
(8) By convention, the agent is the case-frame whichprecedes the verb in the lexical pattern.
Notice thatthe notion of agent is necessary since (a) the agent isnot necessarily the subject (i.e., she vu  taken) and{b) the agent is not necessarily the actor {i.e., sherece ived the book, he took a blo~), and (c) in theinfinitive form, the agent must be referred to sincethe agent is omitted from the pattern in the lexicon.
(9) Uni/ieation \[Kay79\] accounts for the interaction oflexical patterns with ordering patterns in matchinginput sentences.So far, we have given a declarative definition of ourgrammar, a definition which is neutral with respect to ei-ther parsing or generation.
The parsing procedure whichis derived from the definitions above still has to be given.2.3 Parsing ObjectivesThree main tasks in phrasal parsing may beidentified, ordered by degree of difficulty.
(1) Phrase  dlaambiguat ion:  When more than one lexi-cat phrase matches the input sentence, the parsermust select the phrase intended by the speaker.
Forexample, the input the vorkeru took to the s t reetscould mean either "they demonstrated" or "they werefond of the streets'.
In this case, the first phrase isselected according to the principle of patternspeci\]icit 9 \[Arens821.
The pattern ?X: persontaXe:verb <to the streets> is more specific then?x:person take:verb  <to ?y:thing> However, interms of our pattern notation, how do we define pat-tern specificity?
{2) I l l - formed input  comprehension:  Even when aninput sentence is not well phrased according to text-book grammar, it may be comprehensible by peopleand so must be comprehensible to the parser.
Forexample, John took Nary school is telegraphic, butcomprehensible, while John took Nzry to conveysonly a partial concept.
Partially matching sentences(or "near misses') are not handled well by syntax-driven pattern matehers.
A deviation in a functionword (such as the word to above) might inhibit thedetection of the phrase which could be detected by asemantics-driven parser.
(3) Error-detection: when the hypothesized phrasedoes not match the input sentence/context pair, theparser is required to detect the failure and returnwith an indication of its nature.
Error analysis re-quires that pattern tokens be assigned a case-significance, as shown in Section 4.Compounding requirements--disambiguation pluserror-analysis capability-- complicate the design of theparser.
On one hand, analysis of "near misses" (theybury a hatchet  instead of they buried the hatchet) can288be performed through a rigorous analysis--assuming thepresence of a single phrase only.
On the other hand, inthe presence of multiple candidate phrases, disambigua-finn could be made efficient by organizing sequences ofpattern tokens into a discrimination net.
However, at-tempting to perform both disambiguation and "nearmiss" recognition and analysis simultaneously presents adifficult problem.
The discrimination net organizationwould not enable comparing the input sentence, the"near miss", with existing phrases.The solution is to organize the discrimination se-quence by order of generality from the general to thespecific.
According to this principle, verb phrases arematched by conceptual features first and by syntacticfeatures only later on.
For example, consider three ini-tial erroneous hypotheses: (a) bury a hatchet (b) burythe gun, and (c) bury the hash.
On hearing the words"bury the hatchet', the first hypothesis would be theeasiest to analyze (it differs only by a function wordwhile the second differs by a content-holding word) andthe third one would be the hardest (as opposed to thesecond, huh does not have a common concept withhlttchet).2.4 Case-FramesSince these requirements are not facilitated by therepresentation of patterns as given above, we slightlymodify our view of patterns.
An entire pattern is con-structed from a set of case-/tames where each case-frameis constructed of single tokens: words and concepts.Each frame has several slots containing informationabout the case and pertaining to: (a) its syntactic ap-pearance (b) its semantic oncept and (c) its phrase role:agent, patient.
Variable identifiers (e.g., ?x.
?y) areused for unification of phrase patterns with theircorresponding phrase concepts.
Two example patternsare given below:The first example pattern denotes a simple literalverb phrase:{id:?x class:person role:agent}(take:verb)(id:?y class:person role:patient}{id:?z class:location marker:to}Figure 3: Cue  Frmmes for "He took her to school"Both the agent and the patient are of the class person;the indirect object is a location marked by the preposi-tion co.
The second phrase is figurative:{id:?x class:person role:agent){take:verb}(marker:to determiner:the word:streets}Figure 4: Case F rames  for "He took to the streets"The third case frame in Figure 4 above, the indirect ob-ject, does not have any corresponding concept.
Rather itis represented as a sequence of words.
However thewords in the sequence are designated as the marker, thedeterminer and the word itself.Using this view of patterns enables the recognitionof "near misses" and facilitate rror-analysis n parsing.3.
Demons Make Patterns OperationalSo far, we have described only the linguistic nota-tion and indicated that unification \[Kay79\] accounts forproduction of sentences from patterns.
However, it is notobvious how to make pattern unification operational inparsing.
One approach \[Arens82\] is to generate word se-quences and to compare generated sequences with the in-put sentence.
Another approach IPereiraS01 is to imple-ment unification using PROLOG.
Since our task is toprovide lenient parsing, namely also ill-formed sentencesmust be handled by the parser, these two approaches arenot suitable.
In our approach, parsing is carried out byconverting patterns into demons.Conceptual analysis is the process which involvesreading input words left to right, matching them withexisting linguistic patterns and instantiating or modify-ing in memory the associated conceptual meanings.
Forexample, assume that these are the phrases for take: inthe lexicon:?x:person take:verb ?y:person ?z:localeJohn took her to Boston.
?x:person take:verb ?y:phys-objHe took the book.
?x:person take:verb off ?y:attireHe took off his coaL.
?x:person take:verb on ?y:personDavid took on Goliath.
?x:person take:verb a bowThe actor took a boy.
?x:thing take:verb a blowThe vail took a blov.
?x:person take:verb ~to  the streets~The vorkern ~ok t,o the streets.The juvenile took t,o the e~reeCs.Figure 5: A Var iety of Phrases for TAKEwhere variables ?x, :y and ?z also appear in correspond-in& concepts (not shown here).
How are these patterns287actually applied in conceptual analysis?3.1 Interact ion of  Lexlcal and Order ing Pat ternsToken order in the lexical patterns themselves(Figure 5) supports the derivation of simple active-voicesentences only.
Sentences such as:Msry vas ~,zken on by John.A veak contender David might, have left, alone,bu~ Goliath he book on.David dec?ded to take on Gol'tath.Figure 6: A Var iety  of  Word  Orderscannot be derived directly by the given hxical patterns.These sentences deviate from the order given by thecorresponding lexical patterns and require interactionwith language conventions uch as passive voice andinfinitive.
Ordering patterns are used to span a widerrange of sentences in the language.
Ordering patternssuch as the one's given in Figure 2 depict the word orderinvolving verb phrases.
In each pattern the case-framepreceding the verb is specified.
(In active voice, the agentappears imediately before the verb, while in the passiveit is the patient hat precedes the verb.
)3.2 How Does It All Work?Ordering patterns are compiled into demons.
Forexample, DAGENT,  the demon anticipating the agentof the phrase is generated by the patterns in Figure 2. rthas three clauses:I f  the verb is in active formthen the agent is immediately be/ore the verbI f  the verb is in passive formthen the agent may appear, preceded by by.I f  the verb is in infinitivethen the agent is omitted.Its concept is obtained from the function verb.Figure T: The  Conatruct ion of D_AGENTIn parsing, this demon is spawned when a verb is en-countered.
For example, consider the process in parsingthe sentenceDa.v~.d ec'ideal ~ bake on ~,o\].
?ath.Through identifying the verbs and their forms, the pro-tess is:decided (active, simple)Search for the agent before the verb, anticipate aninfinitive form.talc, (active, infinitive)Do not  anticipate the agent.
The actor  of the "takeon" concept which is the agent, is extracted from theagent of "decide'.4.
Fai lure-Dr iven Pat tern  Const ruct ionLearning of phrases in RINA is an iterative pro-tess.
The input is a sequence of sentence-context pairs,through which the program refines its current hypothesisabout the new phrase.
The hypothesis pertains to boththe pattern and the concept of the phrase.4.2 The  Learn ing CycleThe basic cycle in the process is:(a) A sentence is parsed on the background of a concep-tual context.
(b) Using the current hypothesis, either the sentence iscomprehended smoothly, or a failure is detected.
(c) If a failure is detected then the current hypothesis isupdated.The crucial point in this scheme is to obtain from theparser an intelligible analysis of failures.
As an example,consider this part of the first dialog:.1 Program: tie took on him.
He von ~he fight?2 User:.
No.
He took him on.
Dav'\[d Lt, ta, cked him.3 Program: He took him on.He accepted the challenge?The first hypothesis i shown in Figure 8.pattern:concept:?x:person take:verb don  ?y:person~?x win the conflict with ?yFigure 8: F i rst  Hypothes isNotice that the preposition on is attached to the object?y, thus assuming that the phrase is similar to He lookedat Iqaar7 which cannot produce the following sentence: H.look.d her at.
This hypothesis underlies Sentence 1which is erroneous in both its form and its meaning.Two observations should be made by comparing this pat-tern to Sentence 2:The object is not preceded by the preposition on.The preposition on does not precede any object.These comments direct the construction of the new hy-pothesis:288pattern:concept:?x:person take:verb on ?y:person?x win the conflict with ?yFigure 9: Second Hypothesiswhere the preposition on is taken as a modifier of theverb itself, thus correctly generating Sentence 3.
In Fig-ure 9 the conceptual hypothesis is still incorrect andmust itself be modified.4.3 Learning StrategiesA subset of RINA's learning strategies, the onesused for the David and OoliaCh Dialog (Section 1.1) aredescribed in this section.
In our exposition of failuresand actions we will illustrate the situations involved inthe dialogues above, where each situation is specified bythe following five ingredients:(1) the input sentence (Sentence),(2) the context (not shown explicitly here),(3} the active pattern: either the pattern under con-struction, or the best matching pattern if this is thefirst sentence in the dialogue (Pattern l ) .
(4) the failures detected in the current situation(Failures),(5) the pattern resulting from the application of the ac-tion to the current pattern (Pattern2).Creat ing a New PhraseA case.role mismatch occurs when the input sen-tence can only be partially matched by the active pat-tern.
A 9oal mismatch occurs when the concept instan-tinted by the selected pattern does not match the goal si-tuation in the context.Sentence:Pat ternt :Failures:Pattern2:David took on Goliath.
?x:person take:verb ?y:person ?z:locationPattern and goal mismatch?x:person take:verbDavid's physically transferring Goliath to a loca-tion fails since {1) a location is not found and (2) the ac-tion does not match David's goals.
If these two failuresare encountered, then a new phrase is created.
In ab-sence of a better alternative, RINA initially generatesDavid Cook him somevhere.Discr iminating a Pat tern  by Freezing a Prepoabtional PhraseA prepoMtional mismatch occurs when a preposi-tion P matches in neither the active pattern nor in oneof the lexical prepositional phrases, such as:<on ?x:platform> (indicating a spatial relation)<on ?x:time-unit> (indicating a time of action)<on ?x:location> (indicating a place)Sentence:Pat tern l :Failures:Pattern2:David took on Goliath.
?x:person take:verbPrepositional mismatch?x:person take:verb <on ?y:person>The preposition on is not part of the active pat-tern.
Neither does it match any of the prepositionalphrases which currently exist for on.
Therefore, since itcannot be interpreted in any other way, the ordering ofthe sub-expression <on ?y,:peraoa> is frozen in the largerpattern, using < and >.Two-word verbs present a di~culty to languagelearners \[Ulm75\] who tend to ignore the separated verb-particle form, generating: take on him instead of cakehim o,s.
In the situation above, the learner produced thistypical error.Relaxing an Undergeneralized PatternTwo failures involving on: (1) case-role mismatch (on?y:p,r6oa is not found)and (2) prepositional mismatch(on appears unmatched at the end of the sentence) areencountered in the situation below:Sentence:Patte~at:Failures:Pattern2:David took him on.
?x:person take:verb <on ?y'personPrepositional and case-role mismatch.
?x:person take:verb on ?y:personThe combination of these two failures indicatethat the pattern is too restrictive.
Therefore, the < and> freezing delimiters are removed, and the pattern maynow account for two-word verbs.
In this case on can beseparated from ?,&ke.Generai is ing a Semant ic  Restr ict ionA semantic mismatch is marked when the seman-tic class of a variable in the pattern does not subsumethe class of the corresponding concept in the sentence.Sentence :Pat ternt :Failures:Pattern2:John took on the third question.
?x:person take:verb on ?y:personSemantic mismatch?x:person take:verb on ?y:taskAs a result, the type of ?y in the pattern is generalized toinclude both cases.289Freez ing  a Reference Which  Re lates  to a MetaphorAn unrelated reference is marked when a referencein the sentence does not relate to the context, but ratherit relates to a metaphor (see elaboration in \[Zernik85\] ).The reference his fooc cannot be resolved in the con-text, rather it is resolved by a metaphoric gesture.Sentence:Pattern1:Fai lures:Pat tern2:Her father put his foot down.
?x:person put:verb down ?y:phys-objGoal mismatch and unrelated reference?x:person put:verb down foot:body-partSince, (I) putting his foot on the floor does notmatch any of the goals of Jenny's father and (2) thereference his foot is related to the domain of metaphor-ic gestures rather than to the context.
Therefore, footbecomes frozen in the pattern.
This method is similar toa method suggested by Fuss and Wilks \[Fuss83\].
In theirmethod, a metaphor is analyzed when an apparently ill-formed input is detected, e.g.
: the car drank ffi lot ofgas.4.4 Concept Const ructorEach pattern has an associated concept which isspecified using Dyer's \[Dyer83\] i-link notation.
The con-cept of a new phrase is extracted from the context,which may contain more than one element.
For example,in the first dialogue above, the given context containssome salient sto W points \[Wilensky82\] which are indexedin episodic memory as two violated expectations:?
David won the fight in spite of Goliath's physical su-periority.?
David accepted the challenge in spite of the risk in-volved.The program extracts meanings from the given set ofpoints.
Concept hypothesis construction is further dis-cussed in \[Zernik85\].5.
P rev ious  Work  in Language Learn ingIn RINA, the stimulus for learning is comprehen-sion failure.
In previous models language learning was,~lso driven by detection of failures.PST \[Reeker76\] learned grammar by acting upondilfercnces detected between the input sentence andinternally generated sentences.
Six types of differenceswere classified, and the detection of a difference whichbelonged to a class caused the associated alteration ofthe grammar.FOUL-UP \[Granger771 learned meanings of singlewords when an unknown word was encountered.
Themeaning was extracted from the script \[Schank77\] whichwas given as the context.
A typical learning situationwas The cffir vas driving on Hvy 66, vhen i t  careenedoff the road.
The meaning of the unknown verbcare.ned was guessed from the SACCIDENT script.POLITICS \[CarbonellTO\], which modeledcomprehension of text involving political concepts, ini-tiated learning when semantic onstraints were violated.Constraints were generalized by analyzing underlyingmetaphors.AMBER \[Langley82\] modeled learning of basicsentence structure.
The process of learning was directedby mismatches between input sentences and sentencesgenerated by the program.
Learning involved recoveryfrom both errors of omission (omitting a function wordsuch as the or is in daddy bouncing ball) and errors ofcommission (producing daddy is l ik ing dinner).Thus, some programs acquired linguistic patternsand some programs acquired meanings from context, butnone of the above programs acquired new phrases.
Ac-quisition of phrases involves two parallel processes: theformation of the pattern from the given set of examplesentences, and the construction of the meaning from thecontext.
These two processes are not independent sincethe construction of the conceptual meaning utilizeslinguistic clues while the selection of pattern elements ofnew figurative phrases bears on concepts in the context.6.
Cur rent  and Future  WorkCurrently, RINA can learn a variety of phrasalverbs and idioms.
For example, RINA implements thebehavior of the learner in vffivtd vs. c, oliffich and in Go-?ng Punk in Section 1.
Modifications of lexicM entries aredriven by analysis of failures.
This analysis is similar toanalysis of ill-formed input, however, detection of failuresmay result in the augmentation of the lexicon.
Failuresappear as semantic discrepancies (e.g., goal-planmismatch}, or syntactic discrepancies (e.g., case-rolemismatch).
Finally, references in figurative phrases areresolved by metaphor mapping.Currently our efforts are focussed on learning theconceptual elements of phrases.
We attempt o developstrategies for generalizing and refining acquired concepts.For example, it is desirable to refine the concept for"take on" by this sequence of examples:David toak on Goliath.The \[t, kers took on ~he Celtics.I took on a, bard ~ffi,,.k.I took on a, hey Job.In selecting ~he naae ?TQvard8 a. Self-EzCendingLeX iCOne.
Ye t,43olc OU in  o ld  nKme.29OThe first three examples "deciding to fight someone',"playing against someone" and "accepting a challenge"could be generalized into the same concept, but the lasttwo examples deviate in their meanings from thatdeveloped concept.
The problem is to determine thedesired level of generality.
Clearly, the phrases in thefollowing examples:~sdce on am enemyLake os  an o ld  name~a~e on the shape of  a essdcedeserve separate ntries in the phrasal lexicon.
Thequestion is, at what stage is the advantage of furthergeneralization diminished?AcknowledgmentsWe wish to thank Erik Muelhr and Mike Gasserfor their incisive comments on drafts of this paper.References{ArensS2J\[Becker75\]\[Bresnan78\]\[Carbonel179\]Areas, Y., "The Context Model:Language Understanding in a Con-text," in Proceedings Fourth AnnualConference of the Cofnitive Science So-ciety, Ann Arbor, Michigan (1982}.Bucker, Joseph D., "The Phrasal Lexi-con," pp.
70-73 in Proceedings Interdis-ciplinary Workshop on Theoretical Is.sues in Natural Lanfaage Processing,Cambridge, Massachusets (June 1975).Bresnan, Joan, "A Realistic Transfor-mational Grammar," pp.
1-59 inLinguistic Theory and PsychologicalReality, ed.
M. Halle J. Bresnan G.Miller, MIT Press, Harvard, Mas-sachusets (1978).Carbonell, J. G., "Towards a Sell'-Extending Parser," pp.
3-7 in Proceed-ings 17th Annual Meeting of the Associ-ation for Computational Linfaistics, LaJolla, California (1070).\[Dyer83\]\[Dyer8S\]Dyer, Michael G., In-Depth Under-standing: A Computer Model of In-tegrated Processing for NarrativeComprehension, MIT Press, Cam-bridge, MA (1983).Dyer, Michael G. and Uri Zernik,"Parsing Paradignm and LanguageLearning," in Proceedings AI-85, LongBeach, California (May 1085).\[Fasss3l\[Fillmore681\[Granger77\]\[Jacobs85\]\[Kay791\[Langley82\[\[PereiraS01\[Reeker76\]\[Riesbeck74\[\[Schank77\]Fans, Dan and Yorick Wilks, "Prefer-ence Semantics, IlbFormedness andMetaphor," American Journal of Com-putational Linguistics 0(3-4), pp.178-1s7 (zoo).Fillmore, C., "The Case for Case," pp.l-g0 in Universals in Linguistic Theory,ed.
E. Bach R. Harms, Holt, Reinhartand Winston, Chicago (1988).Granger, R. H., "FOUL-UP: A Pro-gram That Figures Out Meanings ofWords from Context," pp.
172-178 inProceedings Fifth \[JCAI, Cambridge,Massachusets (August 1977).Jaeobs, Paul S., "PHRED: A Generatorfor Natural Language Interfaces,"UCB/CSD 85/108,.
Computer ScienceDivision, University of CaliforniaBerkeley, Berkeley, California (Janu-ary 1985).Kay, Martin, "Functional Grammar."pp.
142-158 in Proceedings 5th AnnualMeeting of the Berkeley Linguistic So-ciety, Berkeley, California (1979).Langley, Pat, "Language AcquisitionThrough Error Recovery," Cognitionand Brain Theory ~;(3), pp.211-255{I082).Pereira, F. C. N. and David H. D. War-ren, "Definite Clause Grammars forLanguage Analysis- A Survey of theFormalism and a Comparison withAugmented Transition Networks.
"Artificial Intelligence 13, pp.231-278(i~o).Reeker, L. H., "The ComputationalStudy of Language Learning," in .Ad-vances in Computers, ed.
M. Yovits M.Rubinoff, Academic Press, New York(1976).Riesbeck, C. K., "ComputationalUnderstanding: Analysis of Sentencesand Context," Memo 238, AI Labora-tory (1974) .Schank, Roger and Robert AbeLson,Scripts Plans Goals and Understanding,Lawrence Erlbaum Associates, Hills-dale, New Jersey (1977).291 "{Ulm751\[Wilensky81\]\[Wilensky82\]\[Wilensky84\]\[Zernik85\]Ulm, Susan C., "The SeparationPhenomenon i  English Phrasal Verbs,Double trouble," 601, University ofCalifornia Los Angeles (1975).
M.A.Thesis.Wilensky, R., "A Knowledge-Ba~edApproach to Natural Language Pro-eessing:.
A progress Report," inProceedings Seventh International JointConference on Artificial Intelligence,Vancouver, Canada (1981).Wilensky, R., "Points: A Theory ofStructure of Stories in Memory," pp.345-375 in Strategies for NaturalLanfaage Processing, ed.
W. G.Lehnert M. H. Ringle, Laurence Erl-banm Associates, New Jersey (1982).Wilensky, R., Y. Arens, and D. Chin,"Talking to UNIX in English: an Over-view of UC," Communications of theACM 2T(6), pp.574.-593 (June 1984).Zernik, Uri and Michael G. Dyer,Failure-Driven Aquisition of FifarativePhrasea by Second Language Speakers,1985.
(submitted to publication).292
