Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 710?718,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPModeling Latent Biographic Attributes in Conversational GenresNikesh Garera and David YarowskyDepartment of Computer Science, Johns Hopkins UniversityHuman Language Technology Center of ExcellenceBaltimore MD, USA{ngarera,yarowsky}@cs.jhu.eduAbstractThis paper presents and evaluates severaloriginal techniques for the latent classifi-cation of biographic attributes such as gen-der, age and native language, in diversegenres (conversation transcripts, email)and languages (Arabic, English).
First,we present a novel partner-sensitive modelfor extracting biographic attributes in con-versations, given the differences in lexi-cal usage and discourse style such as ob-served between same-gender and mixed-gender conversations.
Then, we explorea rich variety of novel sociolinguistic anddiscourse-based features, including meanutterance length, passive/active usage, per-centage domination of the conversation,speaking rate and filler word usage.
Cu-mulatively up to 20% error reduction isachieved relative to the standard Boulisand Ostendorf (2005) algorithm for classi-fying individual conversations on Switch-board, and accuracy for gender detectionon the Switchboard corpus (aggregate) andGulf Arabic corpus exceeds 95%.1 IntroductionSpeaker attributes such as gender, age, dialect, na-tive language and educational level may be (a)stated overtly in metadata, (b) derivable indirectlyfrom metadata such as a speaker?s phone numberor userid, or (c) derivable from acoustic proper-ties of the speaker, including pitch and f0 contours(Bocklet et al, 2008).
In contrast, the goal ofthis paper is to model and classify such speakerattributes from only the latent information foundin textual transcripts.
In particular, we are inter-ested in modeling and classifying biographic at-tributes such as gender and age based on lexi-cal and discourse factors including lexical choice,mean utterance length, patterns of participationin the conversation and filler word usage.
Fur-thermore, a speaker?s lexical choice and discoursestyle may differ substantially depending on thegender/age/etc.
of the speaker?s interlocutor, andhence improvements may be achived via dyadicmodeling or stacked classifiers.There has been substantial work in the sociolin-guistics literature investigating discourse style dif-ferences due to speaker properties such as gender(Coates, 1997; Eckert, McConnell-Ginet, 2003).Analyzing such differences is not only interestingfrom the sociolinguistic and psycholinguistic pointof view of language understanding, but also froman engineering perspective, given the goal of pre-dicting latent author/speaker attributes in variouspractical applications such as user authenticaion,call routing, user and population profiling on so-cial networking websites such as facebook, andgender/age conditioned language models for ma-chine translation and speech recogntition.
Whilemost of the prior work in sociolinguistics has beenapproached from a non-computational perspec-tive, Koppel et al (2002) employed the use of alinear model for gender classification with manu-ally assigned weights for a set of linguistically in-teresting words as features, focusing on a small de-velopment corpus.
Another computational studyfor gender classification using approximately 30weblog entries was done by Herring and Paolillo(2006), making use of a logistic regression modelto study the effect of different features.While small-scale sociolinguistic studies onmonologues have shed some light on importantfeatures, we focus on modeling attributes fromspoken conversations, building upon the work of710Boulis and Ostendorf (2005) and show how gen-der and other attributes can be accurately predictedbased on the following original contributions:1.
Modeling Partner Effect: A speaker mayadapt his or her conversation style dependingon the partner and we show how conditioningon the predicted partner class using a stackedmodel can provide further performance gainsin gender classification.2.
Sociolinguistic features: The paper exploresa rich set of lexical and non-lexical featuresmotivated by the sociolinguistic literature forgender classification, and show how theycan effectively augment the standard ngram-based model of Boulis and Ostendorf (2005).3.
Application to Arabic Language: We also re-port results for Arabic language and showthat the ngram model gives reasonably highaccuracy for Arabic as well.
Furthmore, wealso get consistent performance gains due topartner effect and sociolingusic features, asobserved in English.4.
Application to Email Genre: We show howthe models explored in this paper extend toemail genre, showing the wide applicabilityof general text-based features.5.
Application to new attributes: We show howthe lexical model of Boulis and Ostendorf(2005) can be extended to Age and Nativevs.
Non-native prediction, with further im-provements gained from our partner-sensitivemodels and novel sociolinguistic features.2 Related WorkMuch attention has been devoted in the sociolin-guistics literature to detection of age, gender, so-cial class, religion, education, etc.
from conversa-tional discourse and monologues starting as earlyas the 1950s, making use of morphological fea-tures such as the choice between the -ing andthe -in variants of the present participle endingof the verb (Fisher, 1958), and phonological fea-tures such as the pronounciation of the ?r?
soundin words such as far, four, cards, etc.
(Labov,1966).
Gender differences has been one of theprimary areas of sociolinguistic research, includ-ing work such as Coates (1998) and Eckert andMcConnell-Ginet (2003).
There has also beensome work in developing computational modelsbased on linguistically interesting clues suggestedby the sociolinguistic literature for detecting gen-der on formal written texts (Singh, 2001; Koppelet al, 2002; Herring and Paolillo, 2006) but it hasbeen primarily focused on using a small number ofmanually selected features, and on a small numberof formal written texts.
Another relevant line ofwork has been on the blog domain, using a bag ofwords feature set to discriminate age and gender(Schler et al, 2006; Burger and Henderson, 2006;Nowson and Oberlander, 2006).Conversational speech presents a challenging do-main due to the interaction of genders, recognitionerrors and sudden topic shifts.
While prosodic fea-tures have been shown to be useful in gender/ageclassification (e.g.
Shafran et al, 2003), their workmakes use of speech transcripts along the lines ofBoulis and Ostendorf (2005) in order to build ageneral model that can be applied to electronicconversations as well.
While Boulis and Osten-dorf (2005) observe that the gender of the part-ner can have a substantial effect on their classifieraccuracy, given that same-gender conversationsare easier to classify than mixed-gender classifi-cations, they don?t utilize this observation in theirwork.
In Section 5.3, we show how the predictedgender/age etc.
of the partner/interlocutor canbe used to improve overall performance via bothdyadic modeling and classifier stacking.
Boulisand Ostendorf (2005) have also constrained them-selves to lexical n-gram features, while we showimprovements via the incorporation of non-lexicalfeatures such as the percentage domination of theconversation, degree of passive usage, usage ofsubordinate clauses, speaker rate, usage profilesfor filler words (e.g.
?umm?
), mean-utterancelength, and other such properties.We also report performance gains of our modelsfor a new genre (email) and a new language (Ara-bic), indicating the robustness of the models ex-plored in this paper.
Finally, we also explore andevaluate original model performance on additionallatent speaker attributes including age and nativevs.
non-native English speaking status.3 Corpus DetailsConsistent with Boulis and Ostendorf (2005), weutilized the Fisher telephone conversation corpus(Cieri et al, 2004) and we also evaluated per-formance on the standard Switchboard conversa-tional corpus (Godfrey et al, 1992), both collectedand annotated by the Linguistic Data Consortium.In both cases, we utilized the provided metadata711(including true speaker gender, age, native lan-guage, etc.)
as only class labels for both train-ing and evaluation, but never as features in theclassification.
The primary task we employed wasidentical to Boulis and Ostendorf (2005), namelythe classification of gender, etc.
of each speakerin an isolated conversation, but we also evaluateperformance when classifying speaker attributesgiven the combination of multiple conversationsin which the speaker has participated.
The Fishercorpus contains a total of 11971 speakers and eachspeaker participated in 1-3 conversations, result-ing in a total of 23398 conversation sides (i.e.
thetranscript of a single speaker in a single conversa-tion).
We followed the preprocessing steps and ex-perimental setup of Boulis and Ostendorf (2005)as closely as possible given the details presentedin their paper, although some details such as theexact training/test partition were not currently ob-tainable from either the paper or personal commu-nication.
This resulted in a training set of 9000speakers with 17587 conversation sides and a testset of 1000 speakers with 2008 conversation sides.The Switchboard corpus was much smaller andconsisted of 543 speakers, with 443 speakers usedfor training and 100 speakers used for testing, re-sulting in a total of 4062 conversation sides fortraining and 808 conversation sides for testing.4 Modeling Gender via Ngram features(Boulis and Ostendorf, 2005)As our reference algorithm, we used the currentstate-of-the-art system developed by Boulis andOstendorf (2005) using unigram and bigram fea-tures in a SVM framework.
We reimplementedthis model as our reference for gender classifica-tion, further details of which are given below:4.1 Training VectorsFor each conversation side, a training example wascreated using unigram and bigram features withtf-idf weighting, as done in standard text classi-fication approaches.
However, stopwords were re-tained in the feature set as various sociolinguis-tic studies have shown that use of some of thestopwords, for instance, pronouns and determin-ers, are correlated with age and gender.
Also, onlythe ngrams with frequency greater than 5 were re-tained in the feature set following Boulis and Os-tendorf (2005).
This resulted in a total of 227,450features for the Fisher corpus and 57,914 featuresfor the Switchboard corpus.Female MaleFisher Corpushusband -0.0291 my wife 0.0366my husband -0.0281 wife 0.0328oh -0.0210 uh 0.0284laughter -0.0186 ah 0.0248have -0.0169 er 0.0222mhm -0.0169 i i 0.0201so -0.0163 hey 0.0199because -0.0160 you doing 0.0169and -0.0155 all right 0.0169i know -0.0152 man 0.0160hi -0.0147 pretty 0.0156um -0.0141 i see 0.0141boyfriend -0.0134 yeah i 0.0125oh my -0.0124 my girlfriend 0.0114i have -0.0119 thats thats 0.0109but -0.0118 mike 0.0109children -0.0115 guy 0.0109goodness -0.0114 is that 0.0108yes -0.0106 basically 0.0106uh huh -0.0105 shit 0.0102Switchboard Corpusoh -0.0122 wife 0.0078laughter -0.0088 my wife 0.0077my husband -0.0077 uh 0.0072husband -0.0072 i i 0.0053have -0.0069 actually 0.0051uhhuh -0.0068 sort of 0.0041and i -0.0050 yeah i 0.0041feel -0.0048 got 0.0039umhum -0.0048 a 0.0038i know -0.0047 sort 0.0037really -0.0046 yep 0.0036women -0.0043 the 0.0036um -0.0042 stuff 0.0035would -0.0039 yeah 0.0034children -0.0038 pretty 0.0033too -0.0036 that that 0.0032but -0.0035 guess 0.0031and -0.0034 as 0.0029wonderful -0.0032 is 0.0028yeah yeah -0.0031 i guess 0.0028Table 1: Top 20 ngram features for gender, ranked by theweights assigned by the linear SVM model4.2 ModelAfter extracting the ngrams, a SVM model wastrained via the SVMlight toolkit (Joachims, 1999)using the linear kernel with the default toolkitsettings.
Table 1 shows the most discriminativengrams for gender based on the weights assignedby the linear SVM model.
It is interesting thatsome of the gender-correlated words proposed bysociolinguistics are also found by this empiricalapproach, including the frequent use of ?oh?
by fe-males and also obvious indicators of gender suchas ?my wife?
or ?my husband?, etc.
Also, namedentity ?Mike?
shows up as a discriminative uni-gram, this maybe due to the self-introduction atthe beginning of the conversations and ?Mike?being a common male name.
For compatibilitywith Boulis and Ostendorf (2005), no special pre-712Figure 1: The effect of varying the amount of each con-versation side utilized for training, based on the utilized % ofeach conversation (starting from their beginning).processing for names is performed, and they aretreated as just any other unigrams or bigrams1.Furthermore, the ngram-based approach scaleswell with varying the amount of conversation uti-lized in training the model as shown in Figure 1.The ?Boulis and Ostendorf, 05?
rows in Table 3show the performance of this reimplemented al-gorithm on both the Fisher (90.84%) and Switch-board (90.22%) corpora, under the identical train-ing and test conditions used elsewhere in our paperfor direct comparison with subsequent results2.5 Effect of Partner?s GenderOur original contribution in this section is the suc-cessful modeling of speaker properties (e.g.
gen-der/age) based on the prior and joint modeling ofthe partner speaker?s gender/age in the same dis-course.
The motivation here is that people tendto use stronger gender-specific, age-specific ordialect-specific word/phrase usage and discourseproperties when speaking with someone of a sim-ilar gender/age/dialect than when speaking withsomeone of a different gender/age/dialect, whenthey may adapt a more neutral speaking style.Also, discourse properties such as relative useof the passive and percentage of the conversa-tion dominated may vary depending on the gen-der or age relationship with the speaking partner.We employ several varieties of classifier stackingand joint modeling to be effectively sensitive tothese differences.
To illustrate the significance of1A natural extension of this work, however, would be todo explicit extraction of self introductions and then do table-lookup-based gender classification, although we did not doso for consistency with the reference algorithm.2The modest differences with their reported results maybe due to unreported details such as the exact training/testsplits or SVM parameterizations, so for the purposes of as-sessing the relative gain of our subsequent enhancementswe base all reported experiments on the internally-consistentconfigurations as (re-)implemented here.Fisher CorpusSame gender conversations 94.01Mixed gender conversations 84.06Switchboard CorpusSame gender conversations 93.22Mixed gender conversations 86.84Table 2: Difference in Gender classification accuracy be-tween mixed gender and same gender conversations using thereference algorithmClassifying speaker?s and partner?sgender simultaneouslyMale-Male 84.80Female-Female 81.96Male-Female 15.58Female-Male 27.46Table 3: Performance for 4-way classification of the entireconversation into (mm, ff, mf, fm) classes using the referencealgorithm on Switchboard corpus.the ?partner effect?, Table 2 shows the differencein the standard algorithm performance betweensame-gender conversations (when gender-specificstyle flourishes) and mixed-gender conversations(where more neutral styles are harder to classify).Table 3 shows the classwise performance of classi-fying the entire conversation into four possible cat-egories.
We can see that the mixed-gender casesare also significantly harder to classify on a con-versation level granularity.5.1 Oracle ExperimentTo assess the potential gains from full exploita-tion of partner-sensitive modeling, we first reportthe result from an oracle experiment, where weassume we know whether the conversation is ho-mogeneous (same gender) or heterogeneous (dif-ferent gender).
In order to effectively utilize thisinformation, we classify both the test conversa-tion side and the partner side, and if the classi-fier is more confident about the partner side thenwe choose the gender of the test conversation sidebased on the heterogeneous/homogeneous infor-mation.
The overall accuracy improves to 96.46%on the Fisher corpus using this oracle (from90.84%), leading us to the experiment where theoracle is replaced with a non-oracle SVM modeltrained on a subset of training data such that all testconversation sides (of the speaker and the partner)are excluded from the training set.5.2 Replacing Oracle by a Homogeneous vsHeterogenous ClassifierGiven the substantial improvement using the Or-acle information, we initially trained another bi-713nary classifier for classifying the conversation asmixed or single-gender.
It turns out that this taskis much harder than the single-side gender clas-sification, task and achieved only a low accuracyvalue of 68.35% on the Fisher corpus.
Intuitively,the homogeneous vs. hetereogeneous partition re-sults in a much harder classification task becausethe two diverse classes of male-male and female-female conversations are grouped into one class(?homogeneous?)
resulting in linearly insepara-ble classes3.
This subsequently lead us to createtwo different classifiers for conversations, namely,male-male vs rest and female-female vs rest4 usedin a classifier combination framework as follows:5.3 Modeling partner via conditional modeland whole-conversation modelThe following classifiers were trained and each oftheir scores was used as a feature in a meta SVMclassifier:1.
Male-Male vs Rest: Classifying the entireconversation (using test speaker and partner?ssides) as male-male or other5.2.
Female-Female vs Rest: Classifying the en-tire conversation (using test speaker and part-ner?s sides) as female-female or other.3.
Conditional model of gender given mostlikely partner?s gender: Two separate clas-sifiers were trained for classifying the gen-der of a given conversation side, one wherethe partner is male and other where the part-ner is female.
Given a test conversation side,we first choose the most likely gender of thepartner?s conversation side using the ngram-based model6 and then choose the gender ofthe test conversation side using the appropri-ate conditional model.4.
Ngram model as explained in Section 4.The row labeled ?+ Partner Model?
in Table 4shows the performance gain obtained via thismeta-classifier incorporating conversation typeand partner-conditioned models.3Even non-linear kernels were not able to find a good clas-sification boundary4We also explored training a 3-way classifier, male-male,female-female, mixed and the results were similar to that ofthe binarized setup5For classifying the conversations as male-male vs rest orfemale-female vs rest, all the conversations with either thespeaker or the partner present in any of the test conversationswere eliminated from the training set, thus creating a disjointtraining and test conversation partitions.6All the partner conversation sides of test speakers wereremoved from the training data and the ngram-based modelwas retrained on the remaining subset.Figure 2: Empirical differences in sociolinguistic featuresfor Gender on the Switchboard corpus6 Incorporating Sociolinguistic FeaturesThe sociolinguistic literature has shown genderdifferences for speakers due to features such asspeaking rate, pronoun usage and filler word us-age.
While ngram features are able to reason-ably predict speaker gender due to their high detailand coverage and the overall importance of lexicalchoice in gender differences while speaking, thesociolinguistics literature suggests that other non-lexical features can further help improve perfor-mance, and more importantly, advance our under-standing of gender differences in discourse.
Thus,on top of the standard Boulis and Ostendorf (2005)model, we also investigated the following featuresmotivated by the sociolinguistic literature on gen-der differences in discourse (Macaulay, 2005):1.
% of conversation spoken: We measured thespeaker?s fraction of conversation spoken viathree features extracted from the transcripts:% of words, utterances and time.2.
Speaker rate: Some studies have shown thatmales speak faster than females (Yuan etal., 2006) as can also be observed in Fig-ure 2 showing empirical data obtained fromSwitchboard corpus.
The speaker rate wasmeasured in words/sec., using starting andending time-stamps for the discourse.3.
% of pronoun usage: Macaulay (2005) arguesthat females tend to use more third-personmale/female pronouns (he, she, him, her andhis) as compared to males.4.
% of back-channel responses such as?(laughter)?
and ?(lipsmacks)?.5.
% of passive usage: Passives were detectedby extracting a list of past-participle verbsfrom Penn Treebank and using occurences of?form of ?to be?
+ past participle?.7146.
% of short utterances (<= 3 words).7.
% of modal auxiliaries, subordinate clauses.8.
% of ?mm?
tokens such as ?mhm?, ?um?,?uh-huh?, ?uh?, ?hm?, ?hmm?,etc.9.
Type-token ratio10.
Mean inter-utterance time: Avg.
time takenbetween utterances of the same speaker.11.
% of ?yeah?
occurences.12.
% of WH-question words.13.
% Mean word and utterance length.The above classes resulted in a total of 16 sociolin-guistic features which were added based on featureablation studies as features in the meta SVM clas-sifier along with the 4 features as explained previ-ously in Section 5.3.The rows in Table 4 labeled ?+ (any sociolinguis-tic feature)?
show the performance gain using therespective features described in this section.
Eachrow indicates an additive effect in the feature ab-lation, showing the result of adding the current so-ciolinguistic feature with the set of features men-tioned in the rows above.7 Gender Classification ResultsTable 4 combines the results of the experiments re-ported in the previous sections, assessed on boththe Fisher and Switchboard corpora for genderclassification.
The evaluation measure was thestandard classifier accuracy, that is, the fraction oftest conversation sides whose gender was correctlypredicted.
Baseline performance (always guessingfemale) yields 57.47% and 51.6% on Fisher andSwitchboard respectively.
As noted before, thestandard reference algorithm is Boulis and Osten-dorf (2005), and all cited relative error reductionsare based on this established standard, as imple-mented in this paper.
Also, as a second reference,performance is also cited for the popular ?GenderGenie?, an online gender-detector7, based on themanually weighted word-level sociolinguistic fea-tures discussed in Argamon et al (2003).
The ad-ditional table rows are described in Sections 4-6,and cumulatively yield substantial improvementsover the Boulis and Ostendorf (2005) standard.7.1 Aggregating results over per-speaker viaconsensus votingWhile Table 4 shows results for classifying thegender of the speaker on a per conversation ba-sis (to be consistent and enable fair comparison7http://bookblog.net/gender/genie.phpModel Acc.
ErrorReduc.Fisher Corpus (57.5% of sides are female)Gender Genie 55.63 -384%Ngram (Boulis & Ostendorf, 05) 90.84 Ref.+ Partner Model 91.28 4.80%+ % of ?yeah?
91.33+ % of (laughter) 91.38+ % of short utt.
91.43+ % of auxiliaries 91.48+ % of subord-clauses, ?mm?
91.58+ % of Participation (in utt.)
91.63+ % of Passive usage 91.68 9.17%Switchboard Corpus (51.6% of sides are female)Gender Genie 55.94 -350%Ngram (Boulis & Ostendorf, 05) 90.22 Ref.+ Partner Model 91.58 13.91%+ Speaker rate, % of fillers 91.71+ Mean utt.
len., % of Ques.
91.96+ % of Passive usage 92.08+ % of (laughter) 92.20 20.25%Table 4: Results showing improvement in accuracy of gen-der classifier using partner-model and sociolinguistic featuresModel Acc.
ErrorReduc.Fisher CorpusNgram (Boulis & Ostendorf, 05) 90.50 Ref.+ Partner Model 91.60 11.58%+ Socioling.
Features 91.70 12.63%Switchboard CorpusNgram (Boulis & Ostendorf, 05) 92.78 Ref.+ Partner Model 93.81 14.27%+ Socioling.
Features 96.91 57.20%Table 5: Aggregate results on a ?per-speaker?
basis via ma-jority consensus on different conversations for the respectivespeaker.
The results on Switchboard are significantly higherdue to more conversations per speaker as compared to theFisher corpuswith the work reported by Boulis and Ostendorf(2005)), all of the above models can be easilyextended to per-speaker evaluation by pooling inthe predictions from multiple conversations of thesame speaker.
Table 5 shows the result of eachmodel on a per-speaker basis using a majority voteof the predictions made on the individual conver-sations of the respective speaker.
The consen-sus model when applied to Switchboard corpusshow larger gains as it has 9.38 conversations perspeaker on average as compared to 1.95 conversa-tions per speaker on average in Fisher.
The results715on Switchboard corpus show a very large reduc-tion in error rate of more than 57% with respect tothe standard algorithm, further indicating the use-fulness of the partner-sensitive model and richersociolinguistic features when more conversationalevidence is available.8 Application to Arabic LanguageIt would be interesting to see how the Boulis andOstendorf (2005) model along with the partner-based model and sociolinguistic features wouldextend to a new language.
We used the LDC GulfArabic telephone conversation corpus (LinguisticData Consortium, 2006).
The training set con-sisted of 499 conversations, and the test set con-sisted of 200 conversations.
Each speaker partic-ipated in only one conversation, resulting in thesame number of training/test speakers as conver-sations, and thus there was no overlap in speak-ers/partners between training and test sets.
Onlynon-lexical sociolinguistic features were used forArabic in addition to the ngram features.
The re-sults for Arabic are shown in table 6.
Based onprior distribution, always guessing the most likelyclass for gender (?male?)
yielded 52.5% accuracy.We can see that the Boulis and Ostendorf (2005)model gives a reasonably high accuracy in Arabicas well.
More importantly, we also see consistentperformance gains via partner modeling and so-ciolinguistic features, indicating the robustness ofthese models and achieving final accuracy of 96%.9 Application to Email GenreA primary motivation for using only the speakertranscripts as compared to also using acousticproperties of the speaker (Bocklet et al, 2008) wasto enable the application of the models to othernew genres.
In order to empirically support thismotivation, we also tested the performance of themodels explored in this paper on the Enron emailcorpus (Klimt and Yang, 2004).
We manually an-notated the sender?s gender on a random collec-tion of emails taken from the corpus.
The resultingtraining and test sets after preprocessing for headerinformation, reply-to?s, forwarded messages con-sisted of 1579 and 204 emails respectively.In addition to ngram features, a subset of so-ciolinguistic features that could be extracted foremail were also utilized.
Based on the prior dis-tribution, always guessing the most likely class(?male?)
resulted in 63.2% accuracy.
We can seefrom Table 7 that the Boulis and Ostendorf (2005)Model Acc.
ErrorReduc.Gulf Arabic (52.5% sides are male)Ngram (Boulis & Ostendorf, 05) 92.00 Ref.+ Partner Model 95.00+ Mean word len.
95.50+ Mean utt.
len.
96.00 50.00%Table 6: Gender classification results for a newlanguage (Gulf Arabic) showing consistent im-provement gains via partner-model and sociolin-guistic features.Model Acc.
ErrorReduc.Enron Email Corpus (63.2% sides are male)Ngram (Boulis & Ostendorf, 05) 76.78 Ref.+ % of subor-claus., Mean 80.19word len., Type-token ratio+ % of pronouns.
80.50 16.02%Table 7: Application of Ngram model and soci-olinguistic features for gender classification in anew genre (Email)model based on lexical features yields a reason-able performance with further improvements dueto the addition of sociolingustic features, resultingin 80.5% accuracy.10 Application to New AttributesWhile gender has been studied heavily in the lit-erature, other speaker attributes such as age andnative/non-native status also correlate highly withlexical choice and other non-lexical features.
Weapplied the ngram-based model of Boulis and Os-tendorf (2005) and our improvements using ourpartner-sensitive model and richer sociolinguisticfeatures for a binary classification of the age of thespeaker, and classifying into native speaker of En-glish vs non-native.Corpus details for Age and Native Language:For age, we used the same training and test speak-ers from Fisher corpus as explained for gender insection 3 and binarized into greater-than or less-than-or-equal-to 40 for more parallel binary eval-uation.
For predicting native/non-native status, weused the 1156 non-native speakers in the Fishercorpus and pooled them with a randomly selectedequal number of native speakers.
The training andtest partitions consisted of 2000 and 312 speakersrespectively, resulting in 3267 conversation sidesfor training and 508 conversation sides for testing.716Age >= 40 Age < 40well 0.0330 im thirty -0.0266im forty 0.0189 actually -0.0262thats right 0.0160 definitely -0.0226forty 0.0158 like -0.0223yeah well 0.0153 wow -0.0189uhhuh 0.0148 as well -0.0183yeah right 0.0144 exactly -0.0170and um 0.0130 oh wow -0.0143im fifty 0.0126 everyone -0.0137years 0.0126 i mean -0.0132anyway 0.0123 oh really -0.0128isnt 0.0118 mom -0.0112daughter 0.0117 im twenty -0.0110well i 0.0116 cool -0.0108in fact 0.0116 think that -0.0107whether 0.0111 so -0.0107my daughter 0.0111 mean -0.0106pardon 0.0110 pretty -0.0106gee 0.0109 thirty -0.0105know laughter 0.0105 hey -0.0103this 0.0102 right now -0.0100oh 0.0102 cause -0.0096young 0.0100 im actually -0.0096in 0.0100 my mom -0.0096when they 0.0100 kinda -0.0095Table 8: Top 25 ngram features for Age ranked by weightsassigned by the linear SVM modelResults for Age and Native/Non-Native:Based on the prior distribution, always guessingthe most likely class for age ( age less-than-or-equal-to 40) results in 62.59% accuracy and al-ways guessing the most likely class for native lan-guage (non-native) yields 50.59% accuracy.Table 9 shows the results for age and native/non-native speaker status.
We can see that the ngram-based approach for gender also gives reasonableperformance on other speaker attributes, and moreimportantly, both the partner-model and sociolin-guistic features help in reducing the error rate onage and native language substantially, indicatingtheir usefulness not just on gender but also onother diverse latent attributes.Table 8 shows the most discriminative ngrams forbinary classification of age, it is interesting to seethe use of ?well?
right on top of the list for olderspeakers, also found in the sociolinguistic studiesfor age (Macaulay, 2005).
We also see that olderspeakers talk about their children (?my daughter?
)and younger speakers talk about their parents (?mymom?
), the use of words such as ?wow?, ?kinda?and ?cool?
is also common in younger speakers.To give maximal consistency/benefit to the Boulisand Ostendorf (2005) n-gram-based model, we didnot filter the self-reporting n-grams such as ?imforty?
and ?im thirty?, putting our sociolinguistic-literature-based and discourse-style-based featuresat a relative disadvantage.Model AccuracyAge (62.6% of sides have age <= 40)Ngram Model 82.27+ Partner Model 82.77+ % of passive, mean inter-utt.
time 83.02, % of pronouns+ % of ?yeah?
83.43+ type/token ratio, + % of lipsmacks 83.83+ % of auxiliaries, + % of short utt.
83.98+ % of ?mm?
84.03(Reduction in Error) (9.93%)Native vs Non-native (50.6% of sides are non-native)Ngram 76.97+ Partner 80.31+ Mean word length 80.51(Reduction in Error) (15.37%)Table 9: Results showing improvement in the accuracy ofage and native language classification using partner-modeland sociolinguistic features11 ConclusionThis paper has presented and evaluated severaloriginal techniques for the latent classification ofspeaker gender, age and native language in diversegenres and languages.
A novel partner-sensitvemodel shows performance gains from the jointmodeling of speaker attributes along with partnerspeaker attributes, given the differences in lexicalusage and discourse style such as observed be-tween same-gender and mixed-gender conversa-tions.
The robustness of the partner-model is sub-stantially supported based on the consistent per-formance gains achieved in diverse languages andattributes.
This paper has also explored a rich va-riety of novel sociolinguistic and discourse-basedfeatures, including mean utterance length, pas-sive/active usage, percentage domination of theconversation, speaking rate and filler word usage.In addition to these novel models, the paper alsoshows how these models and the previous workextend to new languages and genres.
Cumula-tively up to 20% error reduction is achieved rel-ative to the standard Boulis and Ostendorf (2005)algorithm for classifying individual conversationson Switchboard, and accuracy for gender detectionon the Switchboard corpus (aggregate) and GulfArabic exceeds 95%.AcknowledgementsWe would like to thank Omar F. Zaidan for valu-able discussions and feedback during the initialstages of this work.717ReferencesS.
Argamon, M. Koppel, J.
Fine, and A.R.
Shimoni.2003.
Gender, genre, and writing style in formalwritten texts.
Text-Interdisciplinary Journal for theStudy of Discourse, 23(3):321?346.T.
Bocklet, A. Maier, and E. No?th.
2008.
Age Determi-nation of Children in Preschool and Primary SchoolAge with GMM-Based Supervectors and SupportVector Machines/Regression.
In Proceedings ofText, Speech and Dialogue; 11th International Con-ference, volume 1, pages 253?260.C.
Boulis and M. Ostendorf.
2005.
A quantitativeanalysis of lexical differences between genders intelephone conversations.
Proceedings of ACL, pages435?442.J.D.
Burger and J.C. Henderson.
2006.
An ex-ploration of observable features related to bloggerage.
In Computational Approaches to AnalyzingWe-blogs: Papers from the 2006 AAAI Spring Sympo-sium, pages 15?20.C.
Cieri, D. Miller, and K. Walker.
2004.
TheFisher Corpus: a resource for the next generationsof speech-to-text.
In Proceedings of LREC.J.
Coates.
1998.
Language and Gender: A Reader.Blackwell Publishers.Linguistic Data Consortium.
2006.
Gulf Arabic Con-versational Telephone Speech Transcripts.P.
Eckert and S. McConnell-Ginet.
2003.
Languageand Gender.
Cambridge University Press.J.L.
Fischer.
1958.
Social influences on the choice of alinguistic variant.
Word, 14:47?56.JJ Godfrey, EC Holliman, and J. McDaniel.
1992.Switchboard: Telephone speech corpus for researchand development.
Proceedings of ICASSP, 1.S.C.
Herring and J.C. Paolillo.
2006.
Gender andgenre variation in weblogs.
Journal of Sociolinguis-tics, 10(4):439?459.J.
Holmes and M. Meyerhoff.
2003.
The Handbook ofLanguage and Gender.
Blackwell Publishers.H.
Jing, N. Kambhatla, and S. Roukos.
2007.
Extract-ing social networks and biographical facts from con-versational speech transcripts.
Proceedings of ACL,pages 1040?1047.B.
Klimt and Y. Yang.
2004.
Introducing the En-ron corpus.
In First Conference on Email and Anti-Spam (CEAS).M.
Koppel, S. Argamon, and A.R.
Shimoni.
2002.Automatically Categorizing Written Texts by Au-thor Gender.
Literary and Linguistic Computing,17(4):401?412.W.
Labov.
1966.
The Social Stratification of Englishin New York City.
Center for Applied Linguistics,Washington DC.H.
Liu and R. Mihalcea.
2007.
Of Men, Women, andComputers: Data-Driven Gender Modeling for Im-proved User Interfaces.
In International Conferenceon Weblogs and Social Media.R.K.S.
Macaulay.
2005.
Talk that Counts: Age, Gen-der, and Social Class Differences in Discourse.
Ox-ford University Press, USA.S.
Nowson and J. Oberlander.
2006.
The identity ofbloggers: Openness and gender in personal weblogs.Proceedings of the AAAI Spring Symposia on Com-putational Approaches to Analyzing Weblogs.J.
Schler, M. Koppel, S. Argamon, and J. Pennebaker.2006.
Effects of age and gender on blogging.
Pro-ceedings of the AAAI Spring Symposia on Computa-tional Approaches to Analyzing Weblogs.I.
Shafran, M. Riley, and M. Mohri.
2003.
Voice sig-natures.
Proceedings of ASRU, pages 31?36.S.
Singh.
2001.
A pilot study on gender differences inconversational speech on lexical richness measures.Literary and Linguistic Computing, 16(3):251?264.718
