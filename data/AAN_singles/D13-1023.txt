Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 222?232,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAn Efficient Language Model Using Double-Array StructuresMakoto Yasuhara Toru Tanaka ?Jun-ya Norimatsu Mikio YamamotoDepartment of Computer ScienceUniversity of Tsukuba, Japan?norimatsu@mibel.cs.tsukuba.ac.jpAbstractNgram language models tend to increase insize with inflating the corpus size, and con-sume considerable resources.
In this pa-per, we propose an efficient method for im-plementing ngram models based on double-array structures.
First, we propose a methodfor representing backwards suffix trees usingdouble-array structures and demonstrate its ef-ficiency.
Next, we propose two optimizationmethods for improving the efficiency of datarepresentation in the double-array structures.Embedding probabilities into unused spacesin double-array structures reduces the modelsize.
Moreover, tuning the word IDs in thelanguage model makes the model smaller andfaster.
We also show that our method can beused for building large language models usingthe division method.
Lastly, we show that ourmethod outperforms methods based on recentrelated works from the viewpoints of modelsize and query speed when both optimizationmethods are used.1 IntroductionNgram language models (F. Jelinek, 1990) arewidely used as probabilistic models of sentence innatural language processing.
The wide use of theInternet has entailed a dramatic increase in size ofthe available corpora, which can be harnessed to ob-tain a significant improvement in model quality.
Inparticular, Brants et al(2007) have shown that theperformance of statistical machine translation sys-tems is monotonically improved with the increas-ing size of training corpora for the language model.However, models using larger corpora also consumemore resources.
In recent years, many methods forimproving the efficiency of language models havebeen proposed to tackle this problem (Pauls andKlein, 2011; Kenneth Heafield, 2011).
Such meth-ods not only reduce the required memory size butalso raise query speed.In this paper, we propose the double-array lan-guage model (DALM) which uses double-arraystructures (Aoe, 1989).
Double-array structuresare widely used in text processing, especially forJapanese.
They are known to provide a compactrepresentation of tries (Fredkin, 1960) and fast tran-sitions between trie nodes.
The ability to store andmanipulate tries efficiently is expected to increasethe performance of language models (i.e., improvingquery speed and reducing the model size in terms ofmemory) because tries are one of the most commonrepresentations of data structures in language mod-els.
We use double-array structures to implementa language model since we can utilize their speedand compactness when querying the model about anngram.In order to utilize of double-array structures aslanguage models, we modify them to be able tostore probabilities and backoff weights.
We alsopropose two optimization methods: embeddingand ordering.
These methods reduce model sizeand increase query speed.
Embedding is an ef-ficient method for storing ngram probabilities andbackoff weights, whereby we find vacant spaces inthe double-array language model structure and pop-ulate them with language model information, suchas probabilities and backoff weights.
Ordering is222a method for compacting the double-array structure.DALM uses word IDs for all words of the ngram,and ordering assigns a word ID to each wordto reduce the model size.
These two optimizationmethods can be used simultaneously and are also ex-pected to work well.In our experiments, we use a language modelbased on corpora of the NTCIR patent retrievaltask (Atsushi Fujii et al 2007; Atsushi Fujii et al2005; Atsushi Fujii et al 2004; Makoto Iwayama etal., 2003).
The model size is 31 GB in the ARPAfile format.
We conducted experiments focusing onquery speed and model size.
The results indicatethat when the abovementioned optimization meth-ods are used together, DALM outperforms state-of-the-art methods on those points.2 Related Work2.1 Tries and Backwards Suffix TreesTries (Fredkin, 1960) are one of the most widelyused tree structures in ngram language models sincethey can reduce memory requirements by sharingcommon prefix.
Moreover, since the query speedfor tries depends only on the number of input words,the query speed remains constant even if the ngrammodel increases in size.Backwards suffix trees (Bell et al 1990; Stolcke,2002; Germann et al 2009) are among the mostefficient representations of tries for language mod-els.
They contain ngrams in reverse order of historywords.Figure 1 shows an example of a backwards suf-fix tree representation.
In this paper, we denote anngram: by the form w1, w2, ?
?
?
, wn as wn1 .
In thisexample, word lists (represented as rectangular ta-bles) contain target words (here, wn) of ngrams, andcircled words in the tree denote history words (here,wn?11 ) associated with target words.
The historywords ?I eat,?
?you eat?, and ?do you eat?
are storedin reverse order.
Querying this trie about an ngram issimple: just trace history words in reverse and thenfind the target word in a list.
For example, considerquerying about the trigram ?I eat fish?.
First, simplytrace the history in the trie in reverse order (?eat???I?
); then, find ?fish?
in list <1>.
Similarly, query-ing a backwards suffix tree about unknown ngramsis also efficient, because the backwards suffix treeFigure 1: Example of a backwards suffix tree.
Thereare two branch types in a backwards suffix tree: historywords and target words.
History words are shown in cir-cles and target words are stored in word lists.representation is highly suitable for the backoff cal-culation.
For example, in querying about the 4gram?do you eat soup?, we first trace ?eat??
?you???do?
in a manner similar to above.
However, search-ing for the word ?soup?
in list <3> fails becauselist <3> does not contain the word ?soup?.
In thiscase, we return to the node ?you?
to search the list<2>, where we find ?soup?.
This means that the tri-gram ?you eat soup?
is contained in the tree whilethe 4gram ?do you eat soup?
is not.
This behaviorcan be efficiently used for backoff calculation.SRILM (Stolcke, 2002) is a widely used languagemodel toolkit.
It utilizes backwards suffix trees forits data structures.
In SRILM, tries are implementedas 64-bit pointer links, which wastes a lot of mem-ory.
On the other hand, the access speed for ngramprobabilities is relatively high.2.2 Efficient Language ModelsIn recent years, several methods have been proposedfor storing language models efficiently in memory.Talbot and Osborne (2007) have proposed an effi-cient method based on bloom filters.
This methodmodifies bloom filters to store count informationabout training sets.
In prior work, bloom filtershave been used for checking whether certain dataare contained in a set.
To store the count informa-tion, pairs from <ngram,1> to <ngram,count> areall added to the set for each ngram.
To query thislanguage model about the probability of an ngram,probabilities are calculated during querying by us-ing these counts.
Talbot and Brants (2008) have pro-posed a method based on perfect hash functions andbloomier filters.
This method uses perfect hash func-tions to store ngrams and encode values (for exam-223ple, probabilities or counts of ngrams in the trainingcorpus) to a large array.
Guthrie and Hepple (2010)have proposed a language model called ShefLM thatuses minimal perfect hash functions (Belazzougui etal., 2009), which can store ngrams without vacantspaces.
Furthermore, values are compressed by sim-ple dense coding (Fredriksson and Nikitin, 2007).ShefLM achieves a high compression ratio whenit stores counts of ngrams in the training corpus.However, when this method stores probabilities ofngrams, the advantage of using compression is lim-ited because floating-point numbers are difficult tocompress.
Generally, compression is performed bycombining the same values but, two floating-pointnumbers are rarely the same, especially in the caseof probability values1.
These methods implementlossy language models, meaning that, we can re-duce the model size at the expense of model qual-ity.
These methods also reduce the model perfor-mance (perplexity).Pauls and Klein (2011) have proposed Berke-leyLM which is based on an implicit encoding struc-ture, where ngrams are separated according to theirorder, and are sorted by word ID.
The sorted ngramsare linked to each other like a trie structure.
Berke-leyLM provides rather efficient methods.
Variable-length coding and block compression are used ifsmall model size is more important than queryspeed.
In addition, Heafield (2011) has proposedan efficient language model toolkit called KenLMthat has been recently used in machine translationsystems, for which large language models are of-ten needed.
KenLM has two different main structuretypes: trie and probing.
The trie structure iscompact but relatively slower to query, whereas theprobing structure is relatively larger but faster toquery.In this paper, we propose a language model struc-ture based on double-array structures.
As we de-scribe in Section 3, double-array structures can beused as fast and compact representations of tries.We propose several techniques for maximizing theperformance of double-array structures from the per-spective of query speed and model size.1In our experience, it is considerably easier to compressbackoff weights than to compress probabilities, although bothare represented with floating-point numbers.
We use this knowl-edge in our methods.3 Double-Array3.1 Double-Array StructureIn DALM, we use a double-array structure (Aoe,1989) to represent the trie of a language model.Double-array structures are trie representations con-sisting of two parallel arrays: BASE and CHECK .They are not only fast to query, but also provide acompact way to store tries.
In the structure, nodes inthe trie are represented by slots with the same indexin both arrays.
Before proposing several efficientlanguage model representation techniques in Section4, we introduce double-array themselves.
In addi-tion, the construction algorithms for double-arraysare described in Section 3.2 and Section 3.3.The most naive implementation of a trie will havea two-dimensional array NEXT .
Let WORDID (w)be a function that returns a word ID as anumber corresponding to its argument word w;then NEXT [n][WORDID (w)] (that presents theWORDID(w)-th slot of the nth row in the NEXTarray) stores the node number which can be transitfrom the node number n by the word w, and we cantraverse the trie efficiently and easily to serialize thearray in memory.
This idea is simple but wastes themost of the used memory because almost all of theslots are unused and this results in occupying mem-ory space.
The double-array structures solve thisproblem by taking advantage of the sparseness of theNEXT array.
The two-dimensional array NEXTis merged into a one-dimensional array BASE byshifting the entries of each row of the NEXT arrayand combining the set of resulting arrays.
We canstore this result in much less memory than the se-rialization of the naive implementation above.
Ad-ditionally, a CHECK array is introduced to checkwhether the transition is valid or not because we can-not distinguish which node the information in a par-ticular slot comes from.
Using a CHECK array, wecan avoid transition errors and move safely to thechild node of any chosen node.As a definition, a node link from a node ns witha word w to the next node nnext in the trie is definedas follows:next ?
BASE [s] + WORDID (w)if CHECK [next ] == swhere s denotes the index of the slot in the double-224Figure 2: A trie and a corresponding double-array struc-ture.
Node ns is represented by the slots BASE [s] andCHECK [s].
A link from a node ns with a word w isindicated by CHECK [next] == s.array structure which represents ns.
The trie tran-sition from a node ns with a word w is applied ac-cording to the following steps:Step 1 Calculating the ?next?
destination andStep 2 Checking whether the transition is correct.Step 2 specifically means the following:1.
If CHECK [next ] == s, then we can ?move?to the node nnext ;2. otherwise, we can detect that the transitionfrom the node ns with the word w is not con-tained in the trie.Figure 2 shows an example of a transition from aparent node ns with a word w.Next, we describe how the existence of an ngramhistory can be determined (Aoe, 1989).
We can it-erate over the nodes by the transitions shown aboveand may find the node representing an ngram his-tory.
But we should check that it is valid becausenodes except for leaf nodes possiblly represent afragment of some total ngram history.
We can useendmarker symbols to determine whether an ngramhistory is in the trie.
We add nodes meaning the end-marker symbol after the last node of each ngram his-tory.
When querying aboutwn?11 , we transit repeat-edly; in other words, we set s = 0 and start by ap-plying Step 1 and 2 repeatedly for each word.
Whenwe reach the node wn?1, we continue searching foran endmarker symbol.
If the symbol is found, weknow that the ngram history wn?11 is in the trie.The double-array structure consumes 8 bytes pernode because the BASE and CHECK arrays are4 byte array variables.
Therefore, the structure canFigure 3: Greedy insertion of trie elements.
The childrenof a node are collectively inserted into the double-arraywhen the BASE value of the node is fixed.store nodes compactly in case of a high filling rate.Moreover, node transitions are very fast becausethey require only one addition and one comparisonper transition.
We use a double-array structure inDALM, which can maximize its potential.3.2 Greedy ConstructionGreedy algorithms are widely used for construct-ing static double-array structures2.
The constructionsteps are as follows:1.
Define the root node of a trie to correspond toindex 0 of the double-array structure and2.
Find the BASE value greedily (i.e., in order1, 2, 3, ?
?
?)
for all nodes which have fixed theirindices in the double-array structure.In practice, once the BASE value of a node is fixed,the positions of its children are fixed at the sametime, and we can find the BASE values for eachchild recursively.Figure 3 shows an example of such construc-tion.
In this example, three nodes (?I?, ?you?
and?they?)
are inserted at the same time.
This is be-cause the above three node positions are fixed bythe BASE value of the node ?eat?.
To insert nodes2We were unable to find an original source for this tech-nique.
However, this method is commonly used in double-arrayimplementations.225?I?, ?you?
and ?they?, the following three slots mustbe empty (i.e., the slots must not be used by othernodes.):?
BASE [s] + WORDID(?I?)?
BASE [s] + WORDID(?you?)?
BASE [s] + WORDID(?they?
)where s is the index of the node ?eat?.
At the con-struction step, we need to find BASE [s] which sat-isfies the above conditions.3.3 Efficient Construction AlgorithmThe construction time for a double-array structureposes the greatest challenge.
We use a more effi-cient method (Nakamura and Mochizuki, 2006) in-stead of the naive method for constructing a double-array structure because the naive method requires along time.
We call the method ?empty doubly-linkedlist?.
This algorithm is one of the most efficient con-struction methods devised to date.
Figure 4 showsan example of an empty doubly-linked list.
We canefficiently define the BASE value of each node byusing theCHECK array to store the next empty slot.In this example, in searching the BASE value of anode, the first child node can be set to position 1,and if that fails, we can successively try positions3, 4, 6, 8, ?
?
?
by tracing the list instead of searchingall BASE values 0, 1, 2, 3, 4, 5, ?
?
?.As analyzed by Nakamura and Mochizuki(2006),the computational cost of a node insertion is lessthan in the naive method.
The original naive methodrequires O(NM) time for a node insertion, whereM is a number of unique word types and N is anumber of nodes of the trie; the algorithm using anempty double-linked list requires O(UM), where Uis the number of unused slots.As described in Section 5, we divide the trie intoseveral smaller tries and apply the efficient methodfor constructing our largest models.
This is becauseit is not feasible to wait several weeks for the largelanguage model structure to be built.
The dividingmethod is currently the only method allowing us tobuild them.Figure 4: Empty doubly-linked list.
Unused CHECKslots are used to indicate the next unused slots, and un-used BASE slots are used to indicate previous unusedslots.
Thus, the BASE and CHECK arrays are used as adoubly-linked list which can reduce the number of inef-fective trials.4 Proposed Methods4.1 DALMIn this section, we present the application of thedouble-array structure to backwards suffix trees.
Asthis is the most basic structure based on double-arraystructures, we refer to it as the simple structureand improve its performance as described in the fol-lowing sections.To represent a backwards suffix tree as a double-array structure, we should modify the tree because ithas two types of branches (target words and historynodes), which must be distinguished in the double-array structure.
Instead, we should distinguish thebranch type which indicates whether the node is atarget word or a history word.
We use the endmarkersymbol (<#>) for branch discrimination.
In priorwork, the endmarker symbol has been used to indi-cate whether an ngram is in the trie.
However, thereis no need to distinguish whether the node of the treeis included in the language model because all nodesof a backwards suffix tree which represents ngramssurely exist in the model.
We use the endmarkersymbol to indicate nodes which are end-of-historywords.
Therefore, target words of ngrams are chil-dren of the endmarker symbols that they follow.By using the endmarker symbol, target words canbe treated the same as ordinary nodes because all tar-get words are positioned after <#>.
Figure 5 showsan example of such construction.
We can clearly dis-tinguish target words and history words in the back-wards suffix tree.Querying in the tree is rather simple.
For exam-ple, consider the case of a query trigram ?I eat fish?in the trie of Figure 5.
We can trace this trigram in226Figure 5: An example of converting a backwards suffixtree.
We introduce endmarker symbols to distinguish thetwo branch types.
We can treat the tree as an ordinary triethat can be represented by a double-array structure whileretaining the advantages of the tree structure.the same way as the original backwards suffix tree.First, we trace ?eat??
?I?, then trace that to the end-marker symbol <#> and finally find the word ?fish?.Next, we describe the procedure for storing prob-abilities and backoff weights.
We prepare aVALUEarray to store the probabilities and backoff weightsof ngrams.
Figure 6 shows the simple DALMstructure.
The backwards suffix tree stores a back-off weight for each node and a probability for eachtarget word.
In simple DALM, each value isstored for the respective position of the correspond-ing node.4.2 EmbeddingEmbedding is a method for reducing model size.In the simple DALM structure, there are many va-cant spaces in the BASE and CHECK arrays.
Weuse these vacant spaces to store backoff weights andprobabilities.
Figure 7 shows vacant spaces in thesimple DALM structure.First, the BASE array slots of target word nodesare unused because target words are always in leafpositions in the backwards suffix tree and do nothave any children nodes.
In the example of Figure 7,BASE [9] is not used, and therefore can be used forstoring a probability value.
This method can reducethe model size because all probabilities are storedinto the BASE array.
As a result, the VALUE arrayFigure 6: The simple DALM data structure.
TheBASE and CHECK arrays are used in the same wayas in a double-array structure.
To return probabilities andbackoff weights, a VALUE array is introduced.Figure 7: Unused slots in the simple DALM structureused for other types of information, such as probabilities.contains only backoff weights.Next, the CHECK array slots of endmarker sym-bols are also vacant.
We do not need to check forendmarker symbol transition because the endmarkersymbol <#> is seen for all nodes except target wordnodes.
This means that all endmarker symbol tran-sitions are ensured to be correct and the CHECKarray slots of endmarker symbols do not need to beused.
We use this space to store backoff weights.In order to avoid false positives, we cannot storebackoff weights directly.
Instead, we store the po-sitions of the backoff weights in the VALUE arrayas negative numbers.
When a query for an unknownngram encounters an endmarker symbol node, thevalue of the CHECK array is never matched be-cause the corresponding value stored there is neg-ative.
The same values in the VALUE array can beunified to reduce the memory requirements.
Figure8 illustrates an example of the embeddingmethod.227Figure 8: Implementation of the embedding method.We use vacant spaces in the VALUE array to store theprobabilities and indices of backoff weights.
The in-dices of backoff weights are taken with a negative signto avoid false positives.
Backoff weights are stored in theVALUE array, and the same values in the VALUE arraycan be unified.4.3 OrderingOrdering is a method for shortening the double-array structure and increasing the query speed.
Inordering, word IDs are assigned in order of un-igram probability.
This is done at a preprocessingstage, before the DALM is built.Before explaining the reasons why this methodis effective, we present an interpretation of double-array construction in Figure 9 which corresponds tothe case presented in Figure 3.
In the previoussection, we pointed out that the insertion problemis equivalent to the problem of finding the BASEvalue of the parent node.
Here, we expand this fur-ther into the idea that finding the BASE value isequivalent to the problem of finding the shift lengthof an insertion array.
We can create an insertionarray which is an array of flag bits set to 1 at the po-sitions of word IDs of children nodes?
words.
More-over, we prepare a used array which is also a flagbit array denoting whether the original slots in thedouble-array structure are occupied.
In this situ-ation, finding the shift length is equivalent to theproblem of finding the BASE value of the slot forthe node ?eat?, and the combined used array denotesthe size of the double-array structure after insertion.Figure 10 shows an intuitive example illustratingthe efficiency of the ordering method.
Whenword IDs are assigned in order of unigram proba-bility, 1s in the insertion array are gathered towardFigure 9: Interpretation of a double-array construction.The insertion problem for the double-array structure isinterpreted as a finding problem of a shift length of theinsertion array.
We can measure the size of the double-array structure in the used array.the beginning of the array.
This means that 1s inthe insertion array form clusters, which makes in-sertion easier than for unordered insertion arrays.This shortens the shift lengths for each insertion ar-ray: a shorter double-array structure results.5 Experiment5.1 Experimental SetupTo compare the performance of DALM with othermethods, we conduct experiments on two ngrammodels built from small and large training corpora.Table 1 shows the specifications of the model.Training data are extracted from the Publicationof unexamined Japanese patent applications, whichis distributed with the NTCIR 3,4,5,6 patent retrievaltask (Atsushi Fujii et al 2007; Atsushi Fujii et al2005; Atsushi Fujii et al 2004; Makoto Iwayamaet al 2003).
We used data for the period from228Figure 10: An example of word ID ordering effi-ciency.
Word IDs in the insertion array are packed tothe front in advance.
Therefore, shift lengths for orderedarrays are often shorter than unordered ones.
The result-ing size of the double-array structure is expected to besmaller than that of an unordered array.Table 1: Corpus and model specifications.ModelCorpus Unique NgramSize Type Type(words) (words) (1-5gram)100 Mwords 100 M 195 K 31 M5 Gwords 5 G 2,140 K 936 MTest set 100 M 198 K -1,993 to 2,002 and extracted paragraphs containing?background?
and ?example?.
This method is simi-lar to the NTCIR 7 Patent Translation Task(Fujii etal., 2008).
The small and large training data setscontained 100 Mwords and 5 Gwords, respectively.Furthermore, we sampled another 100 Mwords asa test set to measure the access speed for extract-ing ngram probabilities.
We used an Intel R?
Xeon R?X5675 (3.07 GHz) 24-core server with 142 GB ofRAM.Our experiments were performed from the view-points of speed and model size.
We executed eachprogram twice, and the results of the second runwere taken as the final performance.Figure 11: Comparison between tuned and non-tuneddouble-array structures.Table 2: Comparison between tuned and non-tuneddouble-array structures.Method Size Speed(MB) (queries/s)Simple 1,152 1,065,536Embedding 782 1,004,555Ordering 726 1,083,703Both 498 1,057,6075.2 Optimization MethodsWe compared the performance of the DALMsproposed here, namely simple, embedding,ordering and both, where both indicates thatthe language model uses both embedding andordering.
We conducted experiments examin-ing how these methods affect the size of the double-array structures and the query speeds.
We used the100 Mwords model in the comparison because itwas difficult to build a DALM using the 5 Gwordsmodel.The results are shown in Figure 11 and Table 2.While both ordering and embedding decreasedthe model size, the query speed was increased by theformer and decreased by the latter.
Both was thesmallest and most balanced method.5.3 Divided Double-Array StructureBuilding a double-array structure requires a longtime, which can sometimes be impractical.
In fact,as mentioned above, waiting on construction of thedouble-array structure of the 5 Gwords model is in-feasible.229Figure 12: Comparison between divided and originaldouble-array structures.As described in Section 3.3, the efficient algo-rithm requires O(UM) time to insert one node andthe insertion is iterated N (the total number of inser-tions) times.
If we assume that the number of unusedslots at the ith insertion, Ui, is proportional to i, orthat Ui = c ?
i where c is a proportionality con-stant, we can calculate the building time as follows:?Ni=1 UiM = O(MN2).To shorten the build time, we divided the originaltrie into several parts.
Building parts of the origi-nal trie is possible because N is reduced.
Moreover,these double-array structures can be built in parallel.Note that query results for both original and dividedtries are completely equivalent because divided trieshold all the ngram statistics of the original trie.
Thismethod is similar to that used in randomized lan-guage models (Talbot and Brants, 2008).We compared the differences between the meth-ods using the original and divided double-arraystructures.
In the comparison, we also used the 100Mwords model with the both optimization methoddescribed in the previous section (Figure 12 and Ta-ble 3).Although dividing the trie increased the size ofthe DALM slightly, the model size was still smallerthan that without optimization.
Query speed in-creased as the number of parts was increased.
Weattributed this to the divided DALM consisting ofseveral double-array structures, each smaller thanthe undivided structure which results in an increase.Figure 12 shows that there is a trade-off relation be-tween model size and query speed.Below, we use the 5 Gwords model in our exper-iments.
In our environment, building a 5 GwordsTable 3: Comparison between divided and originaldouble-array structures.Number of parts Size Speed(MB) (queries/s)1 498 1,057,6072 502 1,105,3584 510 1,087,6198 540 1,098,594double-array structure required about 4 days whenthe double-array structures were divided into 8 parts,even though we used the more efficient algorithmdescribed in Section 3.3.
The time required forbuilding the model when the original structure wasdivided into less than 8 parts was too long.
Thus,a more efficient building algorithm is essential foradvancing this research further.5.4 Comparison with Other MethodsUsing the 100 Mwords and 5 Gwords mod-els, we compared DALM with other meth-ods (KenLM (Kenneth Heafield, 2011) andSRILM (Stolcke, 2002)).
In this experiment, weused the both method (which is mentioned above)for DALM and divided the original trie into 8 partsand built double-array structures.The results are shown in Figure 13 and Table 4;the group on the left shows the results for the 100Mwords model and the group on the right shows theresults for the 5 Gwords model.The experimental results clearly indicate thatDALM is the fastest of all the compared methodsand that the model size is nearly the same or slightlysmaller than that of KenLM (Probing).
WhereasKenLM (Trie) is the smallest model, it is slowerthan DALM.The differences between the 5 Gwords versionsof DALM and KenLM (Probing) are smaller incomparison with the 100 Mwords models.
This isbecause hash-based language models have an ad-vantage when storing higher-order ngrams.
Largelanguage models have more 5grams, which leads toshorter backoff times.
On the other hand, trie-basedlanguage models have to trace higher-order ngramsfor every query, which requires more time.Finally, we discuss practical situations.
We con-230Table 4: Comparison between DALM and other methods.100 Mwords Model 5 Gwords ModelLM Size Speed Size Speed(MB) (queries/s) (MB) (queries/s)SRILM 1,194 894,138 31,747 729,447KenLM (Probing) 665 1,002,489 18,685 913,208KenLM (Trie) 340 804,513 9,606 635,300DALM (8 parts) 540 1,098,594 15,345 953,186Figure 13: Comparison between DALM and other lan-guage model systems.ducted this study?s experiments using test set textwritten by humans.
In some applications such asstatistical machine translations, language model sys-tems should compute probabilities of many unnatu-ral ngrams which will be unknown.
This may affectquery speed because querying unknown and unnat-ural ngrams generate many backoffs.
They may re-sults in trie-based LM being slightly faster, becausetraversing the trie can stop immediately when it de-tects that a queried ngram history is not contained inthe trie.
On the other hand, hash-based LM such asKenLM probing would repeat queries until findingtruncated ngram histories in the trie.6 ConclusionWe proposed a method for implementing languagemodels based on double-array structures.
We callthis method DALM.
Moreover, we proposed twomethods for optimizing DALM: embedding andordering.
Embedding is a method wherebyempty spaces in arrays are used to store ngram prob-abilities and backoff weights, and ordering is amethod for numbering word IDs; these methods re-duce model size and increase query speed.
Thesetwo optimization methods work well independently,but even better performance can be achieved if theyare combined.We also used a division method to build the modelstructure in several parts in order to speed up theconstruction of double-array structures.
Althoughthis procedure results in a slight increase in modelsize, the divided double-array structures mostly re-tained the compactness and speed of the originalstructure.
The time required for building double-array structures is the bottleneck of DALM as it issometimes too long to be practical, even though themodel structure itself achieves high performance.
Infuture work, we will develop a faster algorithm forbuilding double-array structures.While DALM has outperformed state-of-the-artlanguage model implementations methods in our ex-periments, we should continue to consider ways tooptimize the method for higher-order ngrams.AcknowledgmentsWe thank the anonymous reviewers for many valu-able comments.
This work is supported by JSPSKAKENHI Grant Number 24650063.ReferencesJ.-I.
Aoe.
1989.
An Efficient Digital Search Algorithmby Using a Double-Array Structure.
IEEE Transac-tions on Software Engineering, 15(9):1066?1077.Atsushi Fujii, Makoto Iwayama, and Noriko Kando.2004.
Overview of the Patent Retrieval Task atNTCIR-4.Atsushi Fujii, Makoto Iwayama, and Noriko Kando.2005.
Overview of Patent Retrieval Task at NTCIR-5.231Atsushi Fujii, Makoto Iwayama, and Noriko Kando.2007.
Overview of the Patent Retrieval Task at theNTCIR-6 Workshop.
pages 359?365.Djamal Belazzougui, Fabiano C. Botelho, and Martin Di-etzfelbinger.
2009.
Hash, displace, and compress.
InESA, pages 682?693.Timothy C. Bell, John G. Cleary, and Ian H. Witten.1990.
Text compression.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large Language Models inMachine Translation.
In Proceedings of the 2007 JointConference on EMNLP-CoNLL, pages 858?867.
ACL.B.
Merialdo F. Jelinek.
1990.
Self-organized languagemodeling for speech recognition.Edward Fredkin.
1960.
Trie memory.
Communicationsof the ACM, 3(9):490?499.Kimmo Fredriksson and Fedor Nikitin.
2007.
SimpleCompression Code Supporting Random Access andFast String Matching.
In Proceedings of the 6th in-ternational conference on Experimental algorithms,WEA?07, pages 203?216.
Springer-Verlag.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, andTakehito Utsuro.
2008.
Overview of the Patent Trans-lation Task at the NTCIR-7 Workshop.Ulrich Germann, Eric Joanis, and Samuel Larkin.
2009.Tightly Packed Tries: How to Fit Large Models intoMemory, and Make them Load Fast, Too.
In Proceed-ings of the Workshop on SETQA-NLP, pages 31?39.ACL.David Guthrie and Mark Hepple.
2010.
Storing the Webin Memory: Space Efficient Language Models withConstant Time Retrieval.
In Proceedings of the 2010Conference on EMNLP, pages 262?272.
ACL.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation.
ACL.Makoto Iwayama, Atsushi Fujii, Noriko Kando, and Aki-hiko Takano.
2003.
Overview of Patent Retrieval Taskat NTCIR-3.Yasumasa Nakamura and Hisatoshi Mochizuki.
2006.Fast Computation of Updating Method of a Dictio-nary for Compression Digital Search Tree.
Trans-actions of Information Processing Society of Japan.Data, 47(13):16?27.Adam Pauls and Dan Klein.
2011.
Faster and SmallerN-Gram Language Models.
In Proceedings of the49th Annual Meeting of the ACL-HLT, pages 258?267.ACL.A.
Stolcke.
2002.
SRILM-an Extensible Language Mod-eling Toolkit.
Seventh International Conference onSpoken Language Processing.David Talbot and Thorsten Brants.
2008.
RandomizedLanguage Models via Perfect Hash Functions.
In Pro-ceedings of ACL-08: HLT.David Talbot and Miles Osborne.
2007.
SmoothedBloom Filter Language Models: Tera-Scale LMs onthe Cheap.
In Proceedings of the 2007 Joint Confer-ence on EMNLP-CoNLL, pages 468?476.232
