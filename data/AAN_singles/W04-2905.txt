Using Soundex Codes for Indexing Names in ASR documentsHema Raghavanhema@cs.umass.eduJames Allanallan@cs.umass.eduAbstractIn this paper we highlight the problems thatarise due to variations of spellings of namesthat occur in text, as a result of which links be-tween two pieces of text where the same nameis spelt differently may be missed.
The problemis particularly pronounced in the case of ASRtext.
We propose the use of approximate stringmatching techniques to normalize names in or-der to overcome the problem.
We show how wecould achieve an improvement if we could tagnames with reasonable accuracy in ASR.1 IntroductionProper names are often key to our understanding of theinformation conveyed by a document.
This is particu-larly the case when the domain is news.
For example, adocument with several mentions of George W. Bush, DickCheney, Baghdad and Saddam Hussein, gives us a goodsense of what the contents of the document may be.
Incomparison, other regular English words like death, scudand missiles, may be good indicators of more general top-ics like war, but may not give us any indication of theexact event being discussed.
Linking stories that discussthe same event, like the Attack on Iraq is very useful fora news filtering systems.
When topics are primarily de-termined by specific events, it is easy to see why namesof entities- people places and organizations, play such acritical role in discriminating between events that discussa topic.However, when one considers a real life scenariowhere news is from different media (print and broad-cast) and in many different languages, proper namespose many different problems.
The problem with propernames is that they often have different spelling variations.For example, the names Arafat, Araafat, and Arafaat mayall refer to the same entity.
Human beings can also varyin their spellings of a named entity.
Besides that, the out-put of ASR and Machine Translation systems can also re-sult in different spelling variations of a name.
Such slightspelling variations may be acceptable and discernible byhumans, but for a machine they are harder to match.
Auser who issues a query with the term Arafat in it maynever find a document that discusses Araafat, using cur-rent TF-IDF matching techniques, even though the docu-ment may be pertinent to his or her query.
Although thisloss may not be critical to some applications, one cannotassume that the problem does not exist.
The problem hasbeen addressed by the data-base community in the past bythe use of approximate string matching techniques, but inpure-text, we have the added problem of detecting names.In this paper, we demonstrate with examples howsometimes we may not be able to draw connections be-tween two pieces of text without the use of approximatestring matching techniques.
We indicate the problems weencounter while detecting names, and propose ways toaddress those issues.
In the discussion of previous workin the next section we describe some tasks that use ASRoutput, and which may have been benefited by the useof approximate string matching techniques.
We describesome preliminary experiments and their results.
We thendiscuss the bottlenecks, in the proposed methodology,and how they may be overcome.2 Past Work2.1 StemmingStemming (Porter, 1980; Krovetz, 1993) is a method inwhich the corpus is processed so that semantically andmorphologically related words are reduced to a commonstem.
Thus, race, racing, and racer are all reduced to asingle root ?
race.
Stemming has been found to be ef-fective for Information Retrieval, TDT and other relatedtasks.
Current stemming algorithms work only for regu-lar English words and not names.
In this paper we lookat addressing the problem of grouping together and nor-malizing proper names in the same way that stemminggroups together regular English words.2.2 Approximate String MatchingThere has been some past work (French et al, 1997; Zo-bel and Dart, 1996) that has addressed the problem thatproper names can have different spellings.
Each of thoseworks, however, only addresses the question of how ef-fectively one can match a name to its spelling variants.They measure their performance in terms of the preci-sion and recall with which they are able to retrieve othernames which are variants of a given query name.
Essen-tially, the primary motivation of those works was in find-ing good approximate string matching techniques.
Thosetechniques are directly applicable only in applicationsthat retrieve tuples from a database record.However, there is no work that evaluates the effec-tiveness of approximate string matching techniques fornames in an information retrieval or related task.
Weknow of no work that attempts to detect names automati-cally, and then index names that should go together, in thesame way that words of the same stem class are indexedby one common term.2.3 The TREC SDR and the TDT Link DetectiontasksA single news-source may spell all mentions of a givenname identically.
However, this consistency is lost whenthere are multiple sources of news, where sources spanlanguages and modes (broadcast and print).
The TDT3corpus (ldc, 2003) is representative of such real-life data.The corpus consists of English, Arabic and Mandarinprint and broadcast news.
ASR output is used in the caseof the broadcast sources and in the case of non-Englishstories machine translated output is used for comparingstories.
For both ASR systems and Machine Transla-tion systems, proper names are often out-of-vocabulary(OOV).
A typical speech recognizer has a lexicon ofabout 60K, and for a lexicon of this size about 10% ofthe person names are OOV.
The OOV problem is usuallysolved by the use of transliteration and other such tech-niques.
A breakdown of the OOV rates for names fordifferent lexicon sizes is given in (Miller et al, 2000).We believe the problem of spelling errors is of impor-tance when one wants to index and retrieve ASR docu-ments.
For example, Monica Lewinsky is commonly re-ferred to in the TDT3 corpus.
The corpus has closed- cap-tion transcripts for TV broadcasts.
Closed caption suf-fers from typing errors.
The name Lewinsky is also oftenmisspelt as Lewinskey in the closed caption text.
In theASR text some of the variants that appear are Lewenskey,Linski, Lansky and Lewinsky.
This example is typical,with the errors in the closed caption text highlighting howhumans themselves can vary in their spelling of a nameand the errors in ASR demonstrating how a single ASRsystem can output different spellings for the same name.The ASR errors are largely because ASR systems relyon phonemes for OOV words, and each of the differentvariations in the spellings of the same name is probablya result of different pronounciations and other such fac-tors.
The result of an ASR system then, is several dif-ferent spelling variations of each name.
It is easy to seewhy it would help considerably to group names that referto the same entity together, and index them as one en-tity.
We can exploit the fact that these different spellingvariations of a given name exhibit strong similarity us-ing approximate string matching techniques.
We proposethat in certain domains, where the issue that proper namesexist with many different variations is dominant, the useof approximate string matching techniques to determinewhich names refer to the same entity will help improvethe accuracy with which we can detect links between sto-ries.
Figure 1 shows a snippet of closed caption text andits ASR counterpart.
The names Lewinskey and Trippare misspelt in the ASR text.
The two documents how-ever have high similarity, because of the other words thatthe ASR system gets right.
Allan (Allan, 2002) showedhow ASR errors can cause misses in TDT tasks, and cansometimes be beneficial, resulting in a minimal averageimpact on performance in TDT.
In the case of SpokenDocument Retrieval (Garofolo et al, 2000) also it wasfound that a few ASR errors per document did not re-sult in a big difference to performance as long as we geta reasonable percentage of the words right.
Of course,factors such as the length of the two pieces of text beingcompared make a difference.
Barnett et al(Barnett et al,1997), showed how short queries were affected consid-erably by Word Error rate.
ASR errors may not cause asignificant drop in performance for any of the Topic De-tection and Tracking tasks.
But, consider a system whereretrieving all documents mentioning Lewinskey and Trippis critical, and it is not unrealistic to assume there existsystems with such needs, the ASR document in the abovementioned example would be left out.
We therefore, be-lieve that the problem we are addressing in this paper isan important one.
The preliminary experiments in thispaper, which are on the TDT corpus, only highlight howour approach can help.3 Story Link Detection3.1 Task DefinitionThe Story Link Detection Task is key to all the other tasksin TDT.
The system is handed a set of story pairs, andfor each pair it is asked to judge whether both the storiesdiscuss the same topic or different topics.
In addition toa YES/NO decision the system is also expected to outputa confidence score, where a low confidence score impliesthat the system is more in favor of the NO decision.3.2 Our ApproachSimply stated our approach to the SLD task, is to use ap-proximate string matching techniques to compare entitiesbetween two pieces of text.
The two pieces of text may bea query and a document, or two documents, depending onthe task.
We first need to identify entities in the two doc-uments.
There exist several techniques to automaticallyidentify names.
For properly punctuated text, heuristicslike capitalization work sufficiently well.
However, forASR text we often do not have sentence boundaries oreven punctuation.
Hence we rely on a Hidden MarkovModel based named entity recognizer (Bikel et al, 1999)for our task.A simple strategy that incorporates an approximatestring matching technique is to first preprocess the cor-pus, and then normalize all mentions of a named entityto a given canonical form, where the canonical form isindependent of mentions of other entities in the two doc-uments being compared.
Soundex, Phonix, and othersuch codes offer us a means of normalizing a word toits phonetic form.
The Soundex code is a combination ofthe first letter of the word and a three digit code whichis representative of its phonetic sound.
Hence, similarsounding names like ?Lewinskey?
and ?Lewinsky?
areboth reduced to the same soundex code ?l520?.
We canpre-process the corpus so that all the named entities arereplaced by their Soundex codes.
We then compute thesimilarity between documents in the new corpus as op-posed to the old one, using conventional similarity met-rics like Cosine or TF-IDF.4 Experimental Set up4.1 DataThe corpus (ldc, 2003) has 67111 documents from mul-tiple sources of news in multiple languages (EnglishChinese and Arabic) and media (broadcast news andnewswire).
The English sources are Associated Press andNew York Times, PRI, Voice of America etc.
For thebroadcast news sources we have ASR output and for TVwe have both ASR output as well as closed caption data.Additionally we have the following Mandarin news-wire,web and broadcast sources - Xinhua news, Zaobao, andVoice of America (Mandarin).
For all the Mandarin doc-uments we have the original documents in the native lan-guage as well the English output of Systran- a machinetranslation system.
The data has been collected by LDCby sampling from the above mentioned sources in the pe-riod from October to December 1998.The LDC has annotated 60 topics in the TDT3 corpus.A topic is determined by an event.
For example topic30001 is the Cambodian Government Coalition.
Eachtopic has key entities associated with it and a descriptionof the topic.
A subset of the documents are annotated asbeing on-topic or not according to a well formed strategyas defined by the LDC.4.2 Story Link DetectionTo compute the similarity of two documents, that is, theYES/NO decision threshold, we used the the traditionalcosine similarity metric.
To give some leverage to doc-uments that were very similar even before named entitynormalization, we average the similarity scores betweendocuments before and after the named entities have beennormalized by their Soundex codes as follows:Sim(D1; D2) =12(Cos(D1; D2) + Cos(D01; D02)) (1)Where D1and D2are the original documents and D01and D02are the documents after the names have been nor-malized.4.3 EvaluationAn ROC curve is plotted by making a parameter sweep ofthe YES/NO decision thresholds, and plotting the Missesand False Alarms at each point.
At each point the costis computed using the following empirically determinedformula (Fiscus et al, 1998).Cdet= 0:02P (miss) + 0:098P (fa) (2)This cost function is standard across all tasks.
The pointof minimum cost serves as the comparison between vari-ous systems.5 ResultsWe tested our idea on the TDT3 corpus for the Story LinkDetection Task, using the Cosine similarity metric, andfound that performance actually degraded.
On investiga-tion we found that the named entity recognizer performspoorly on Machine Translated and ASR source data.
Ournamed entity recognizer relies considerably on sentencestructure, to make its predictions.
Machine translated out-put often lacks grammatical structure, and ASR outputdoes not have punctuation, which results in a lot of namedentity tagging errors.We therefore decided to test our idea for newswire text.We created our own test set of 4752 pairs of stories fromnewswire sources.
This test set was created by randomlypicking on and off-topic stories for each topic using thesame policy as employed by the LDC (Fiscus, 2003).
Onthese pairs, we obtained about 10% improvement (Fig-ure 2), suggesting that there is merit in Soundex normal-ization of names.
However, the problem of poor namedentity recognition is a bottle-neck for ASR.
We discuss125102040608090.01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90Missprobability(in%)False Alarms probability (in %)SLD using soundex codes on Newswire-Newswire pairsRandom PerformanceUsing SoundexTW Min DET Norm(Cost) = 0.1588BaselineTW Min DET Norm(Cost) = 0.1709Figure 1: Story Link Detection performancealternative strategies of how to deal with this, and otherways of using approximate string matching in the nextsection.6 Alternative strategies6.1 To not use an entity recognizerWe were not able to benefit from our approach on theASR documents because of the poor performance of thenamed entity recognizer on those types of document.An example of a randomly picked named entity taggedASR document is given below.
The tagging errors areunderlined.< DOC >< DOCNO > CNN19981001:0130:0000 < /DOCNO >< TEXT >< ENAMEX TYPE=?ORGANIZATION?
>BUDGET SURPLUS < /ENAMEX> AND FIGHTINGOVER WHETHER IT?S GOING DOOR POCKETS WILLTELL YOU THE < ENAMEX TYPE=?ORGANIZATION?> VEHICLES CLIMBED DATES THEREAFTER </ENAMEX > AND IF YOU?RE REQUIRED TO PAYCHILD SUPPORT INFORMATION THAT YOURJOB AND COME AND ADDRESS NOW PARTHAVE < ENAMEX TYPE=?ORGANIZATION?
>A NATIONAL REGISTRY THE HEADLINE < /ENAMEX> NEWS I?M < ENAMEX TYPE=?PERSON?>KIMBERLYKENNEDY </ENAMEX> THOSE STORIES IN A MO-MENT BUT FIRST </TEXT></DOC >We need a better performing recognizer, but that maybe hard.
Instead we might be able to use other informa-tion from the speech recognizer to overcome this prob-lem.
We did not have confidence scores for the words inthe ASR output.
If we had had that information, or if wewere able to obtain information about which words wereOOV, we could possibly index all words with low confi-dence scores or all OOV words by their Soundex codes.Or else, one could normalize all words in the ASR out-put, that are not part of the regular English vocabulary bytheir Soundex codes.6.2 Other ways of grouping entitiesAnother direction of research to pursue is the way inwhich approximate string matching is used to comparedocuments.
The way we used approximate string match-ing in this paper was fairly simple.
However, it losesout on some names that ought to go together particularlywhen two names differ in their first alphabet - for exampleKatherine and Catherine.
The Soundex codes are k365and c365 respectively.
This is by virtue of the nature ofthe Soundex code of word.There are other ways to compute the similarity be-tween two documents like the Levenshtein distance oredit distance which is a measure of the number of stringedit operations required to convert one string to the other.The words Katherine and Catherine have an edit distanceof 1.
Given two documents D1and D2, we can computethe distance between them by computing the distance be-tween all pairs of names that occur in the two documents,and using the distances to group entities and finally to findthe similarity of the two documents.
However this meansthat each entity in D1has to be compared to all entities inD1and D2.
Besides, this method brings with it the ques-tion of how to use the distances between the names so asto group together similar names.
This method is probablya good direction for future research, because the Leven-shtein distance could possibly be a better string matchingtechnique.
Another plausible strategy would be to use theedit-distance of the Soundex codes of the names, whencomparing documents.
Katherine and Catherine wouldhave a distance of 1 in this case too.Using cross document coreference resolution tech-niques to find equivalence classes of entities would be yetanother alternative approach.
In Cross document corefer-ence, two mentions of the same name, may or may not beincluded in the same group depending on whether or notthe context of the two mentions is the same or is different.7 Conclusions and Future DirectionsIn this paper we highlighted an important problem thatoccurs with names in ASR text.
We showed how a namemay be spelt differently by humans.
In ASR the samename had many more different spellings.We proposed a simple indexing strategy for names,wherein a name was indexed by its Soundex code.
Wefound that our strategy did not work for ASR, but theproblem was not with the approach, but because we couldnot do a good job of identifying names in ASR text.Ifwe could detect names with reasonable accuracy in ASRtext we should be able to achieve reasonable improve-ment.
We did not have a named entity recognizer thatperformed well on ASR text.
We therefore verified ouridea on news-wire text, which is grammatical, well punc-tuated text.
In the news-wire domain, in spite of there be-ing reasonable consistency in spellings of names, we getabout 10% improvement in minimum cost, and a consis-tent improvement at all points in the ROC curve.
Hence,a simple technique like Soundex served as a useful nor-malization technique for names.
We proposed alternativemechanisms that could be applied to ASR text, whereinall OOV words could be normalized by their Soundexcodes.
We also outlined further directions for research inthe way that approximate string matching may be used.We think the general results of past works that has con-sidered the problems due to ASR errors to be insignificantcannot be assumed to transfer across to other problems.There will arise situations when this problem is materialand research needs to be done in this direction.8 AcknowledgementsThis work was supported in part by the Cen-ter for Intelligent Information Retrieval, in part bySPAWARSYSCEN-SD grant number N66001-02-1-8903.
Any opinions, findings and conclusions or recom-mendations expressed in this material are the author(s)and do not necessarily reflect those of the sponsor.ReferencesJames Allan.
2002.
Detecting and tracking topics inbroadcast news,att speechdays 2002.James Barnett, Steve Anderson, John Broglio, MonaSingh, R. Hudson, and S. W. Kuo.
1997.
Experimentsin spoken queries for document retrieval.
In Proc.
Eu-rospeech ?97, pages 1323?1326, Rhodes, Greece.Daniel M. Bikel, Richard L. Schwartz, and Ralph M.Weischedel.
1999.
An algorithm that learns what?sin a name.
Machine Learning, 34(1-3):211?231.J.
Fiscus, G. Doddington, J. Garofolo, and A. Martin.1998.
Nist?s 1998 topic detection and tracking eval-uation.John Fiscus.
2003.
Personal communication.J.
C. French, A. L. Powell, and E. Schulman.
1997.
Ap-plications of approximate word matching in informa-tion retrieval.
In Proceedings of the Sixth InternationalConference on Knowledge and Information Manage-ment, pages 9?15, New York, NY.
ACM Press.J.
Garofolo, G. Auzanne, and E. Voorhees.
2000.
Thetrec spoken document retrieval track: A success story.R.
Krovetz.
1993.
Viewing Morphology as an InferenceProcess,.
In Proceedings of the Sixteenth Annual In-ternational ACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages 191?203.2003.
http://www.ldc.upenn.edu/tdt/.David Miller, Richard Schwartz, Ralph Weischedel, andRebecca Stone.
2000.
Named entity extraction frombroadcast news.M.F.
Porter.
1980.
An algorithm for suffix stripping.Program.J.
Zobel and P. W. Dart.
1996.
Phonetic string match-ing: Lessons from information retrieval.
In H.-P. Frei,D.
Harman, P. Scha?ble, and R. Wilkinson, editors,Proceedings of the 19th International Conference onResearch and Development in Information Retrieval,pages 166?172, Zurich, Switzerland.
ACM Press.
