Probe bilistic Unification-Based Integration OfSyntactic and Semantic Preferences For Nominal CompoundsDekai Wu*Computer  Science Divis ionUnivers i ty  of Cal i fornia at BerkeleyBerkeley, CA 94720 U.S.A.dekai@ucbvax, berkeley, eduAbst rac tIn this paper, we describe a probabilisticframework for unification-based grammarsthat facilitates'integrating syntactic a~ld se-m~mtic constraints and preferences.
Weshare many of the concerns found in recentwork on massively-parallel language inter-pret'ation models, although the proposal re-flects our belief in the value of a higher-levelaccount hat is not stated in terms of dis-tributed'computati0n.
We also feel that in-adequate l arning theories everely limit ex-'isting massively-parallel language interpre-tation models.
A learning theory is not onlyinteresting in its own right, but must un-derlie,any quantitative account of languageinterpretation, because the complexity ofinteraction between constraints and prefer-ences makes ad hoc trial-and-error strate-gies for picking numbers infeasible, partic-ula~'ly for semantics inrealistically-sized do~fire,ins.In t roduct ionMassively-parallel models of language interpretation~:including markeropassing models and neural net-works of both the connectionist and PDP (paralleldistributed processing) variety--have provoked somefundamental questions about the limits of symbolic,logic- or rule-based frameworks.
Traditional frame-works have difficulty integrating preferences in thepresence of complex dependency relationships, inanalyzing ambiguous phrases, for example, seman-tic information should sometimes override syntac-tic prefcrenccs, and vice versa.
Such interactionscan take place at different levels within a phrase'sconstituent structure, even for a single analysis.Massiw;ly-parallel models excel at integrating dif-ferent sources of preferences in a natural, intuitive*Many thanks to Robert Wilensky and Charles Fill-more for helpful discussions, and to Hans Karlgren andNigel Ward for constructive suggestions Ondrafts.
Thisresearch was sponsored in part by the Defense Ad-vanced Research Projects Agency (DoD), monitored bythe Space and Naval Warfare Systems Command underN00039-88-C-0292, the Office of Naval Research undercontract N00014-89-J-3205, and the Sloan Foundationunder grant 86-10-3.fashion; for example, connectionist models simplytranslate dependency constraints into excitatory orinhibitory links in relaxation etworks (Waltz & Pol-lack 1985).
Furthermore, massively-parallel modelshave shown remarkable ability to compute complexsemantic preferences.We argue that it is possible and desirable to givea more meaningful account of preference integrationat a higher level, without resort to distributed algo-rithms.
One could say that we are interested in char-acterizing the nature of the preferences, rather thanhow they might be efficiently computed.
We do notclaim that all properties of massively-parallel modelscan or should be described at this level.
However,few language interpretation models take advantageof those properties that can only be characterized atthe distributed level.We also propose a quantitative theory that as-signs an interpretation to the numbers used in ourmodel.
A quantitative theory explains the numbers'significance by defining the procedure by which themodel--in principle, at least--can learn the num-bers.
Much of the mystique of neural networks i dueto their potential learning properties, but surpris-ingly, few PDP and no connectionist models of lan-guage interpretation that we know of specify quanti-tative theories, even though numbers must be used torun the models.
Without a quantitative theoreticalbasis, it seems unlikely that the network structureswill generalize much beyond the particular hand-coded examples, if for no other reason than the ina-mense room for variation in constructing such net-works.Case  Study :  Nomina l  CompoundsNominal compounds exemplify the sort of phenom-ena modeled by interacting preferences.
Nounsthemselves are often homonymous--is dream stale asleep condition or California?---necessitating lexicalambiguity resolution.
Structural ambiguity resolu-tion required for nested nominal compounds, whichhave more than one parse; consider \[baby pool\]lable versus baby \[pool tabk\].
Lexicalized nom-inal compounds necessitate syntactic preferences,while semantic preferences are needed to guide se-mantic composition tasks like frame selection andcase/role binding, as nominal compounds nearly al-'ways have multiple possible meanings.
Tradition-ally, linguists have only classified nominal comei 413PREFERREDPARSENNNNNNCOMPETING LEXICALIZED COMPOUNDSFirst compoundmore lexicalizedkiwi fruit juiceLEX\]CALIZEDLEX\]CALIZEDNew York state parkLEXICALIZEDLEXICAIJZEDSecond compoundmore lexicalizednavel oran\[~e juiceLEXICAL~EDLEXICAI.JF~Dbaby poo !
tableLEXICA LIZEDLEXICALIZEDCOMPETING LEXICALIZEDAND IDENTIFICATIVECOMPOUNDSaft.moon rest areaIDENTIFICATIVELEXICALIZEDgold watch chainLEXICALIZEDIDENTIFICATIVEFigure 1.
Nominal compounds requiring integration of semantic preferences.
(a) Waltz & Pollack 1985, Bookman 1987WORDSENSESMICROFEATURESCo) Wermter 1989a, Wermter & Lehnert 1989NOUNI 1 NOUN2 ~1.0 1.0Conceptually equivalent to:INPUTWORDSENSESIMIL.MirIY"I0 OTHERSENSESSENSES ~~CRO~AaXJRBSo.o 1.o o.o --..~ ~4 - .
.
.
~mDEN/ \ Miaaoa LAYm0.6 0.3 O.l0.5 0.3 0.9 GOODNESSFigure 2.
PDP semantic similarity evaluators.pounds according to broad criteria such as part-whole or source-result relationships (Jespersen 1946;Quirk et al 1985); several arge-scale studies haveprovided somewhat finer-grained classifications onthe order of a dozen classes (Lees 1963; Levi 1978;Warren 1978).
However, the emphasis has been onpredicting the possible meanings of a compound,rather than predicting its preferred meaning.
An ex-ception is Leonard's (1984) rule-based model which,howew~r, only produces fairly coarse interpretationswith medium (76%) accuracy.We distinguish three major classes of nominal com-pounds: lexicalized (conventional), such as clock ra-dio; identificative, such as clock gears; and creative,such as clock table.
Both identificative and creativecompounds are novel in Downing's (1977) sense; theydiffer in that an identificative compound serves toidentify a known (but hitherto unnamed) seman-tic category, whereas to interpret a creative com-pound requires constructing a new semantic struc-ture.
There is a bias to use the most specific pre-existing categories that match the compound be-ing analyzed, syntactic or semantic.
Precedence isgiven to a conventional parse if one exists, thena parse with an identificative interpretation, andlastly a parse with a creative interpretation.
How-ever, this "Maximal Conventionality Principle" caneasily be overruled by global considerations aris-ing from the embedding phrase and context.
Fig-ure 1 shows examples where two conventional com-pounds compete, and where global considerationscause an identificative compound to be preferredover a competing conventional compound.
Thesecases require integration of quantitative syntacticand semantic preferences, ince non-quantitative in-tegration schemes (e.g., Marcus 1980; Hirst 1987;Lytinen 1986) do not discriminate adequately be-tween the alternative analyses.What  Do  Mass ive ly -Para l le l  Mode lsRea l ly  Say?One use of massive parallelism is to evaluate thesimilarity or compatibility between two concepts inorder to generate semantic preferences.
Similarityevaluators usually employ PDP networks where se-mantic concepts are internally represented as dis-tributed activation patterns over a set of ~microfea-turcs'.
Conceptually, the network in Figure 2a givesa similarity metric between a given concept and ev-ery other concept, computed as the weighted sum ofshared microfeatures.
1 Likewise, the hidden layer inFigure 2b computes the goodness of every possiblerelation between the given pair of nouns.
In non-massively-parallel terms, what such nets do is cap-ture statistical dependencies between concepts, downto the granularity of the chosen "microfeatures'.
Aprobabilistic feature-structure formalism employingthe same granularity of features should be able tocapture the same dependencies.Connectionist models are often used to integratesyntactic and semantic preferences front differentinformation sources (Cottrell 1984, 1985; Wermter1989b; Wermter & Lehnert 1989).
Nodes representt Ignoring Bookman's persistent activation, which sim-ulates recency-based contextual priming.414 2hypotheses about word senses, parse structures, orrole bindings; links represent either supportive orinhibitory dependencies between hypotheses.
Thelinks constrain the network so that activation prop-agation causes the net to relax into a state wherethe hypotheses are all consistent with one another.~.I'he most severe problem with these models is the~rbitariness of the numbers used; Cottrell, for exam-p\]e, admits "weight twiddling" and notes that lackof formal analysis hampers determination of param-eters.
In other words, although the networks ettleitlto consistent states, there is no principle determin-lag the probability of each state.McClelland & Kawamoto's (1986) PDP modellearns how :syntactic (word-order) cues affect seman-tic frame/case selection, yielding more principledpreference integration.
Like the PDP similarity eval-uators, however, the information encoded in the net-work and its weights is not easily comprehended.Previous non-massively-parallel proposals forquantitative preference integration have used non-probabilistic evidence combination schemes.
Schu-bert (1986) suggests umming "potentials" up thephrase-structure trees; these potentials derive fromsalience, logical-form typicality, and semantic typ-icality conditions.
McDonald's (1982) noun com-pound interpretation model also sums differentsources of syntactic, semantic, and contextual evi-dence.
Though qualitatively appealing, additive cal-culi are liable to count the same evidence more thanonce, and use arbitrary evidence weighting schemes,making it impossible to construct a model that worksfor all cases.
Hobbs el al.
(1988) propose a theorem-proving lnodel that integrates yntactic onstraintswith variable-cost abductive semantic and pragmatica~sumptions.
The danger of these non-probabilisticapproaches, as with connectionist preference integra-tor's, is that the use of poorly defined "magic num-b,:'rs" makes large-scale generalization difficult.A Probab i l l s t l c  Un i f i ca t lon -BasedPre ference  Formulat ionWe are primarily concerned here with the followingp,:oblem: given a nominal compound, determine theranking of its possible interpretations from most toleast likely.
The problem can be formulated in termsof unification.
Unification-based formalisms providean elegant means of describing the information struc-tures used to construct interpretations.
Lexical andstructural ambiguity resolution, as well as semanticcomposition, are readily characterized as choices be-tween alternative sequences of unification operations.A key feature of unification--especially importantfoJ: preference integration--is its neutrality with re-spect to control, i.c., there is no inherent bias in theorder of unifications, and thus, no bias as to whichchoices take precedence ovcr others.
Although nom-inal compound interpretation i volves lcxical andst t'uctural ambiguity resolution and semantic om-p()sition, it is not a good idea to centralize con-trol around any single isolated task, because thereis too much interaction.
For example, the frame se-lection problem affects lexical arnbiguity resolution(consider the special case where the frame selectedis that signified by the lexical item).
Likewise, frameselection and case/role binding are two aspects of thesame semantic omposition problem, and structuralambiguity resolution depends largely on preferencesin semantic omposition.Thus we turn to unification for a clean formu-lation of the problem.
Three classes of feature-structures are used: syntactic, semantic, and con-structions.
The construction is defined in Fillmore's(1988) Construction Grammar as "a pairing of asyntactic pattern with a meaning structure"; theyare similar to signs in HPSG (Pollard & Sag 1987)and pattern-concept airs (Wilensky & Arens 1980;Wilensky et al 1988).
Figure 3 shows a sampleconstruction containing both syntactic and seman-tic feature-structures.
2 Typed feature-structures areused: the value of the special feature TYPE is atype in a multiple-inheritance type hierarchy, andtwo TYPE values unify only if they are not disjoint.This allows (1) easy transformation from semanticfeature-structures to more convenient frame-basedsemantic network representations, and (2) efficientencoding of partially redundant lexical/syntactic cat-egories using inheritance (see, for example, Pollard& Sag 1987; Jurafsky 1990).
Our notation is cho-sen for generality; the exact encoding of significationrelationships i inessential to our purpose here.TYPE: NN.constrl\[ TYPE: NN \]S YN: CONS T1 : 1CONST~: 2SEM: 1\[ TYPE: thing \]TYPE: composlte-thlng \]FRAME: ROLE1: 3ROLE2: ?TYPE: N-constr \]SUBI: SYN: 1SEM: aTYPE: N-constr \]SUB~: SYN: 2SEM: 4Figure 3.
A nominal compound construction.Given a nominal compound (of arbitary length),an intevpretalion is defined as an instantiatedconstruction--including all the syntactic, seman-tic, and sub-construction f-structures--such t at thesyntactic structure parses the nominal compound,and the semantic structure is consistent with all the(sub-)constructions.
Figure 4 shows an interpreta-tion of afternoon rest.
Given this framework, lexicalambiguity resolution is the selection of a particularsub-construction for a lexical item that matches morethan one construction, structural ambiguity resolu-tion is the selection between alternative syntactic f-structures, and semantic omposition is the selectionbetween alternative semantic f-structures.
In eachcase we must be be able to compare alternative in-terpretations and determine the best.Before discussing how to compare interpretations,let us briefly consider the sort of information avail-able.
We extend the unification paradigm with afunction f that returns the relative frequency of anycategory in the type hierarchy, normalized so thatfor any category cat, f(cat) = P\[cat(x)\] where x is a2Ordering constraints are omitted in this paper.3 415TYPE:SYN:SEM:FRAME:SUB1:SUB?
:NN-  eonstr l\[ TYPE: NN \]CONSTI :  1CONST$:  21TYPE: nap- Jrame \]T IME:  3S TATE:  4~Y~.
, \ [  TY.E: "oj.r.oo."
\]SEM:  3 TYPE:  \]SYN:  2 TYPE:  "rest"SEM:  4 TYPE:  rest  \]CONSTI\ /CONSn ~ ~ "  ~ /STATe,3=NN(i3)Sl,,~,~(i3,U),r2(i3,i2) e7=nft-co~tr(iT) .
.
.
.
2g % /'~  / ,~--.
"~ ,~" )  ....eg=~-exml t r l  (i9) .
.
.
.Figure 4.
Bracket and graph representations of an in-terpretation of "afternoon rest".random variable ranging over all categories.
For se-mantic categories, this provides the means of encod-ing typicality information.
For syntactic ategoriesand constructions, this provides a means of encodinginformation about degrees of lexicalization.
Since fis defined in terms of relative frequency, there is alearning procedure for f: given a training set of cor-rect interpretations, we need only count the instancesof each category (and then normalize).The probabilistic goodness metric for an interpre-tation is defined as follows: the goodness of an inter-pretation is the probability of the entire constructiongiven the input words of the nominal compound, e.g.,P\[+c:)l + s1, +82\]= P\[ NN-constrl(ig)l "afternoon"(ix)^ "rest"(i2)\].The metric is global, in that for any set of alternativeinterpretations, the most likely interpretation is thatwith the highest metric.As a simplified example of computing the metric,suppose the feature graph of Figure 4 constituted acomplete dependency graph containing all candida~hypotheses (actually an unrealistic assumption sincethis would preclude any alternative interpretations).For each pair of connected nodes, the conditionalprobability of the child, given the ancestor, is givenby the ratio of their relative frequencies (Figure 5a).The metric only requires computing the probabilityof c9 (Figure 5b).
3 Nodes are clustered into multi-valued compound variables as necessary to eliminateloops, to ensure counting any piece of evidence onlyonce (Figure 5c).The conditional probability vectors P\[+c91zi\] andP\[zll + sl, +s2\] are computed using the disjunctiveinteraction model: 43A natural language processing system needs to prop-agate probabilities to the semantic hypotheses a  well, inorder to make use of the interpreted information.4Jnstification for the disjunctive interaction model isbeyond our scope here; it is a standard approximationP\[+c9\] +83, q-c?, +c8\]= 1 - p\[~cgl + s3\].
P\[~cg\] + c7\].
p\[~cgl + cs\]P\[+~91 + 83, +~7, --~s\] = 1 - P\[--?91 + s~\].
p\[-.cg\] + c~\]P\[+col +83, ~c~, +~s\] = 1 -  Pb~gl + s3\].Pbc~l + cs\]P\ [+~l  + s~, ~c7, ~es\] = 1 - P \ [~ l  + ~3\]P\[+e~l~s3, +c~, +~s\] = 1 - P \ [~ l  + c~\].
F\[~cgt + cs\]P\[+c9l~s3, +c7,--~c8\] = 1 - P\[-~c9\] + c7\]P\[+~91-~s3, ~?~, +~\ ]  = 1 - Pbc~l + cs\]P\[+cgl",s3, ~cr,-~cs\] = 1 - 1P\[+s3, +c7, +cs\] + 81, +82\]= P\[+s31 + si, +s2\]" P\[+cvl + sl, +su\]"P\[+csl + Sl, +s2\]= {1 - P\[~s31 +sx\].
P\["s31 + s2\]} ?
.
.
.P\[+s3, +c~, "csl + 81, +s2\] .
.
.
.Finally, we compute P\[+c91 + s1,+s2\] by condi-tioning on the compound variable Z and takingthe weighted average of P\[+cglZ, +sl, +s2\] over allstates of Z:Ei P\[+cglzi, +sl, +s2\]P\[z~l + 81, +s2\]= E~ P \ [+~l~de\ [~ l  + Sl, +8~\].Both syntactic and semantic preferences are takeninto account.
The influence of semantic preferences isencoded in the conditional probabilities P\[+cg\] + c7\]and P\[+cgl + cs\]J The loops in the original de-pendency graph correspond to support for the in-terpretation via both syntactic and semantic paths.A more complex example demonstrating structuralambiguity resolution is shown in Figure 6; here anafternoon rest schema produces a semantic prefer-ence that overrides a syntactic preference arisingfrom weak lexicalization of the nominal compoundrest area.
6A major unsolved problem with this approach isspecificity selection.
This is a well-known trade-offin classification models: the more general the inter-pretation, the higher its probability is; whereas themore specific the interpretation, the greater its util-ity and the more informative it is.
The probabilisticgoodness metric does not help when comparing twointerpretations whose only difference is that one ismore general than the other.
7 In our initial studieswe attempted to handle this trade-off using thresh-olded marker-passing techniques (Wu 1987, 1989),but we are currently investigating a stronger utilityused to complete the probability model in cases whereis infeasible to gather or store full conditional probabil-ity matrices for all input combinations ( ee Pearl 1988).Heavily biased conditional probability matrices that can-not be satisfactorily approximated by disjunctive inter-action can sometimes be handled by forming additionalcategories.
The apparent schema-organization of humanmemory may well arise for the same reason.~These conditional probabilities cannot be derivedsolely from frequency counts since c9 is an instance ofa novel category--the category of "afternoon rest" con-structions denoting a nap--with zero previous frequency.Instead, the conditional probabilities P\[+c9\] + c~\] andP\[+cgl + cs\] are a function of the ancestral conditionalprobabilities P\[+s31+sl\], P\[+s3i+p~\], P [+x6l+z4\], andP\[+z6\] + zs\] plus the disjunctive interaction assumption.6Note that (a) and (b) are two partitions of the samedependency graph.7Norvig (1989) has also noted the competition be-tween probability and utility in the context of languageinterpretation.416 4(a)~.
~ \ [z2 = ~3, +eT, -e~f3 f3/fl N \ ]~/ r l  ~ N \] _ j  z3--+s3,-e7,+c8s3~.. ~ ~ .~ ~'f6/1:5 s3.~ ~ ~.
Z~.
- ' ' ' 7  z4=+s3''c7,'e8~ e T ~  /8 j6  ~ e T ~  /8  k V'8'-'_s3,.e7,.c8fg/s ~\  l~  fg/r3 ~\  l(a) e9 (b) c9 (e) c9Figure 5.
Computing the goodness metric for an "afternoon rest" interpretation (see text).1.00 1.00 1.00sl =' 'aO~moon"(il) s2="rest"(i2) .3="ama"(i3).
i  )?
0.7, 07?10>o.~ ~ \  ~e 19=Nlq-eonstr=w-nap-c~-ea-semtattic~(il 9)1.00 1.00 1.00sl ~ '~aRemoon"(il) 12=' 'r,e4rt "(i2) s3="a/ea"(i3),7=NN(iT~ clS=afl-eomstrf~lS) e 2 0 = m s t - a r e ~ - ~ ~  0,111e21 =NN.eons~'-w4ime4nte~tate-~em~mtie~(i21)Figure 6.
Semantic overriding syntactic preference in "afternoon rest area".theory to complement the probabilistic model, in-corporating both explicit invariant biases and prob-abilistically learned utility expectations.
It is notyet clear whether we shall also need to incorporatepragmatic utility expectations in the constructions.For methodological reasons we have deliberatelyimpoverished the statistical database, by deprivingthe model of all information except for categoryfrequencies, relying upon disjunctive interaction tocomplete the probability model.
This limitation onthe complexity of statistical information is too re-strictive; disjunctive interaction cannot satisfactorilyapproximate cases whereP\[-{-c3lCl, (:21 ~,  1 - P\[-c3lcl\].
P\[czlq\].Such cases appear to arise often; for example, thepresence of two nouns, rather than one, increasesthe probability of a compound by a much greaterfactor than modeled by disjunctive interaction.
Weintend to test variants of the model empirically ona corpus of nominal compounds, with randomly se-lected training sets; the restrictions on complexity ofconditional probability information will be relaxeddepending upon the resulting prediction accuracy.ConclusionWe have suggested extending unification-based for-malisrrLs to express the sort of interacting prefer-ences used in massively-parallel anguage models, us-ing probabilistic techniques.
In this way, quantita-tive claims that remain hidden in many massively-parallel models can be made more explicit; more-over, the numbers and the calculus are motivatedby a reasonable assumption about language learn-ing.
We hope to see increased use of pr0babilisticmodels rather than arbitrary calculi in language re-search: Charniak & Goldman's (1989) recent anal-ysis of probabilities in semantic story structnres isa promising development in this direction.
Stol-eke (1989) transformed a unification grammar intoa connectionist framework (albeit without prefer-ences); we have taken the opposite tack.
Manylinguists have acknowledged the need to extendtheir frameworks to handle statistically-based syn-tactic and semantic judgements (e.g., Karlgren 1974;Ford et al 1982, p. 745), but only in passing, largely,we suspect, due to the unavailability of adequate rep-resentational tools.
Because our proposal makes di-rect use of traditional unification-based structures,larger grammars hould be easy to construct and5 417incorporate; because of the direct correspondenceto semantic net representations, complex semanticmodels of the type found in AI work may be morereadily exploited.ReferencesBookman, L. A.
(1987).
A microfeature based schemefor modelling semantics.
In Proceedings of the TenthInternational Joint Conference on Artificial lntelli.gence, pp.
611-614.Charniak, E. & R. Goldman (1989).
A semanticsfor probabilistic quantifier-free first-order languages,with particular application to story understanding.In Proceedings of the Eleventh International JointConference on Artificial Intelligence, pp.
1074-1079.Cottrell, G. W. (1984).
A model of lexical access of am-biguous words.
In Proceedings of the Fourth Na-tional Conference on Artificial Intelligence, pp.
61-67.Cottrell, G. W. (1985).
A connectionist approach to wordsense disambiguation.
Technical Report TR 154,Univ.
of Rochester, Dept.
of Comp.
Sci., New York.Downing, P. (1977).
On the creation and use of Englishcompound nouns.
Language, 53(4):810-842.Fillmore, C. J.
(1988).
On grammatical construe-tions.
Unpublished raft, University of Californiaat Berkeley.Ford, M., J. Bresnan, & R. M. Kaplan (1982).
Acompetence-based theory of syntactic losure.
InJ.
Bresnan, editor, The Mental Representation ofGrammatical Relations, pp.
727-796.
MIT Press,Cambridge, MA.Hirst, G. (1987).
Semantic Interpretation and the Res-olution of Ambiguity.
Cambridge University Press,Cambridge.Hobbs, J. R., M. Stickel, P. Martin, & D. Edwards (1988).Interpretation as abduction.
In Proceedings of the?6th Annual Conference of the Association for Com-putational Linguistics, pp.
95-103, Buffalo, NY.Jespersen, O.
(1946).
A Modern English Grammar onHistorical Principles, volume 6.
George Alien & Un-win, London.Jurafsky, D. S. (1990).
Representing and integratinglinguistic knowledge.
In Proceedings of the Thir-teenth International Conference on ComputationalLinguistics, Helsinki.Karlgren, H. (1974).
CategoriM grammar calculus.
Sta-tistical Methods In Linguistics, 1974:1-128.Lees, R. B.
(1963).
The Grammar of English Nominal.izations.
Mouton, The Hague.Leonard, R. (1984).
The Interpretation of English NounSequences on the Computer.
North Holland, Ams-terdam.Levi, J. N. (1978).
The Syntax and Semantics of ComplexNominals.
Academic Press, New York.Lytinen, S. L. (1986).
Dynamically combining syntaxand semantics in natural language processing.
InProceedings of the Fifth National Conference on Ar-tificial Intelligence, pp.
574-578.Marcus, M. P. (1980).
A Theory of Syntactic Recognitionfor Natural Language.
MIT Press, Cambridge.McClelland, J. L. & A. H. Kawamoto (1986).
Mecha-nisms of sentence processing: Assigning roles to con-stituents of sentences.
In J. L. McClelland & D. E.Rumelhart, editors, Parallel Distributed Processing,volume 2, pp.
272-325.
MIT Press, Cambridge, MA.McDonald, D. B.
(1982).
Understanding nouncompounds.
Technical Report CMU-CS-82-102,Carnegie-Mellon Univ., Dept.
of Comp.
Sci., Pitts-burgh, PA.Norvig, P. (1989).
Non-disjunctive ambiguity.
Unpub-lished draft, University of California at Berkeley.Pearl, J.
(1988).
Probabilistie Reasoning in IntelligentSystems: Networks of Plausible Inference.
MorganKaufmann, San Mateo, CA.Pollard, C. & I.
A.
Sag (1987).
Information-Based Syntaxand Semantics: Volume 1: Fundamentals.
Centerfor the Study of Language and Information, Stan-ford, CA.Quirk, R., S. Greenbaum, G. Leech, & J. Svartvik (1985).A Comprehensive Grammer of the English Lan-guage.
Longman, New York.Schubert, L. K. (1986).
Are there preference trade-offs inattachment decisions?
In Proceedings of the FifthNational Conference on Artificial Intelligence, pp.601-605.Stolcke, A.
(1989).
Processing unification-based gram-mars in a connectionist framework.
In Programof the Eleventh Annual Conference of the CognitiveScience Society, pp.
908-915.Waltz, D. L. & J.
B. Pollack (1985).
Massively paral-lel parsing: A strongly interactive model of naturallanguage interpretation.
Cognitive Science, 9:51-74.Warren, B.
(1978).
Semantic Patterns of Noun-NounCompounds.
Acts Universitatis Cothoburgensis,Gothenburg, Sweden.Wermter, S. (1989a).
Integration of semantic and syn-tactic constraints for stuctural noun phrase disam-biguation.
In Proceedings of the Eleventh Inter-national Joint Conference on Artificial Intelligence,pp.
1486-1491.Wermter, S. (1989b).
Learning semantic relationships incompound nouns with connectionist networks.
InProgram of the Eleventh Annual Conference of theCognitive Science Society, pp.
964-971.Wermter, S. & W. G. Lehnert (1989).
Noun phrase anal-ysis with connectionist networks.
In N. Sharkey &R. Reilly, editors, Conneetionist Approaches to Lan-guage Processing.
In press.Wilensky, R. & Y.
Areas (1980).
Phran - a knowledge-based approach to natural anguage analysis.
Tech-nical Report UCB/ERL M80/34, University of Cali-fornia at Berkeley, Electronics Research Laboratory,Berkeley, CA.Wilensky, R., D. Chin, M. Luria, J. Martin, J. Mayfield,& D. Wu (1988).
The Berkeley UNIX Consultantproject.
Computational Linguistics, 14(4):35-84.Wu, D. (1987).
Concretion inferences in natural lan-guage understanding.
In K. Morik, editor, Pro-ceedings of GWA1-87, 11th German Workshop onArtificial Intelligence, pp.
74-83, Geseke.
Springer-Verlag.
Informatik-Fachberichte 152.Wu, D. (1989).
A probabilistic approach to marker prop-agation.
In Proceedings of the Eleventh InternationalJoint Conference on Artificial Intelligence, pp.
574-580, Detroit, MI.
Morgan Kaufmann.418 6
