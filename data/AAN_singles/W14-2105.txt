Proceedings of the First Workshop on Argumentation Mining, pages 29?38,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsIdentifying Appropriate Support for Propositions in Online UserCommentsJoonsuk ParkDepartment of Computer ScienceCornell UniversityIthaca, NY, USAjpark@cs.cornell.eduClaire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY, USAcardie@cs.cornell.eduAbstractThe ability to analyze the adequacy of sup-porting information is necessary for deter-mining the strength of an argument.1Thisis especially the case for online user com-ments, which often consist of argumentslacking proper substantiation and reason-ing.
Thus, we develop a framework forautomatically classifying each propositionas UNVERIFIABLE, VERIFIABLE NON-EXPERIENTIAL, or VERIFIABLE EXPE-RIENTIAL2, where the appropriate type ofsupport is reason, evidence, and optionalevidence, respectively3.
Once the exist-ing support for propositions are identi-fied, this classification can provide an es-timate of how adequately the argumentshave been supported.
We build a gold-standard dataset of 9,476 sentences andclauses from 1,047 comments submittedto an eRulemaking platform and find thatSupport Vector Machine (SVM) classifierstrained with n-grams and additional fea-tures capturing the verifiability and expe-rientiality exhibit statistically significantimprovement over the unigram baseline,achieving a macro-averaged F1of 68.99%.1 IntroductionArgumentation mining is a relatively new fieldfocusing on identifying and extracting argumen-tative structures in documents.
An argument istypically defined as a conclusion with supporting1In this work, even unsupported propositions are considerpart of an argument.
Not disregarding such implicit argu-ments allows us to discuss the types of support that can fur-ther be provided to strengthen the argument, as a form of as-sessment.2Verifiable Experiential propositions are verifiable propo-sitions about personal state or experience.
See Table 1 forexamples.3We are assuming that there is no background knowledgethat eliminates the need of support.premises, which can be conclusions of other argu-ments themselves (Toulmin, 1958; Toulmin et al.,1979; Pollock, 1987).
To date, much of the argu-mentation mining research has been conducted ondomains like news articles, parliamentary recordsand legal documents, where the documents con-tain well-formed explicit arguments, i.e.
proposi-tions with supporting reasons and evidence presentin the text (Moens et al., 2007; Palau and Moens,2009; Wyner et al., 2010; Feng and Hirst, 2011;Ashley and Walker, 2013).Unlike documents written by professionals, on-line user comments often contain arguments withinappropriate or missing justification.
One wayto deal with such implicit arguments is to sim-ply disregard them and focus on extracting ar-guments containing proper support (Villalba andSaint-Dizier, 2012; Cabrio and Villata, 2012).However, recognizing such propositions as partof an argument,4and determining the appropriatetypes of support can be useful for assessing the ad-equacy of the supporting information, and in turn,the strength of the whole argument.
Consider thefollowing examples:How much does a small carton ofmilk cost?1More children should drinkmilk2, because children who drink milkeveryday are taller than those whodon?t3.
Children would want to drinkmilk, anyway4.Firstly, Sentence 1 does not need any support,nor is it part of an argument.
Next, Proposition 2is an unverifiable proposition because it cannot beproved with objective evidence, due to the valuejudgement.
Instead, it can be supported by a rea-son explaining why it may be true.
If the rea-son, Proposition 3, were not true, the whole ar-4Not all sentences in user comments are part of an argu-ment, e.g.
questions and greetings.
We address this in Sec-tion 4.129gument would fall apart, giving little weight toProposition 2.
Thus, an objective evidence sup-porting Proposition 3, which is a verifiable propo-sition, could be provided to strengthen the argu-ment.
Lastly, as Proposition 4 is unverifiable, wecannot expect an objective evidence that proves it,but a reason as its support.
Note that providinga reason why Proposition 3 might be true is notas effective as substantiating it with a proof, butis still better than having no support.
This showsthat not only the presence, but also the type of sup-porting information affects the strength of the ar-gument.Examining each proposition in this way, i.e.with respect to its verifiability, provides a meansto determine the desirable types of support, ifany, and enables the analysis of the argumentsin terms of the adequacy of their support.
Thus,we propose the task of classifying each proposi-tion (the elementary unit of argumentation in thiswork) in an argument as UNVERIFIABLE, VERI-FIABLE PUBLIC, or VERIFIABLE PRIVATE, wherethe appropriate type of support is reason, evidence,and optional evidence, respectively.
To performthe experiments, we annotate 9,476 sentences andclauses from 1,047 comments extracted from aneRulemaking platform.In the remainder of the paper, we describe theannotation scheme and a newly created dataset(Section 2), propose a supervised learning ap-proach to the task (Section 3), evaluate the ap-proach (Section 4), and survey related work (Sec-tion 5).
We find that Support Vector Machines(SVM) classifiers trained with n-grams and otherfeatures to capture the verifiability and experien-tiality exhibit statistically significant improvementover the unigram baseline, achieving a macro-averaged F1score of 68.99%.2 DataWe have collected and manually annotated sen-tences and (independent) clauses from user com-ments extracted from an eRulemaking website,Regulation Room5.
Rulemaking is the process bywhich U.S. government agencies make new reg-ulations and enact public policy; its digital coun-terpart ?
eRulemaking ?
moves the process toonline platforms (see, e.g.
(Park et al., 2012)).By providing platforms in which the public candiscuss regulations that interest them, government5http://www.regulationroom.orgagencies hope to enlist the expertise and experi-ence of participants to create better regulations.In many rulemaking scenarios, agencies are, infact, required to obtain feedback from the pub-lic on the proposed regulation as well as to ad-dress all substantive questions, criticisms or sug-gestions that are raised (Lubbers, 2006).
In thisway, public comments can produce changes in thefinal rule (Hochschild and Danielson, 1998) that,in turn, can affect millions of lives.
It is crucial,therefore, for rule makers to be able to identifycredible comments from those submitted.Regulation Room is an experimental web-site operated by Cornell eRulemaking Initiative(CeRI)6to promote public participation in therulemaking process, help users write more infor-mative comments and build collective knowledgevia active discussions guided by human moder-ators.
Regulation Room hosts actual regulationsfrom government agencies, such as the U.S. De-partment of Transportation.For our research, we collected and manually an-notated 9,476 propositions from 1,047 user com-ments from two recent rules: Airline PassengerRights (serving peanuts on the plane, tarmac de-lay contingency plan, oversales of tickets, baggagefees and other airline traveller rights) and HomeMortgage Consumer Protection (loss mitigation,accounting error resolution, etc.
).2.1 Annotation SchemeTo start, we collected 1,147 comments and ran-domly selected 100 of them to devise an annota-tion scheme for identifying appropriate types ofsupport for propositions and to train annotators.Initially, we allowed the annotators to define thespan for a propositions, leading to various compli-cations and a low inter-annotator reliability.
Thus,we introduced an additional step in which com-ments were manually sliced into propositions (ornon-propositional sentences) before being given tothe annotators.
A proposition or sentence foundthis way was split further if it consisted of two ormore independent clauses.
The sliced commentswere then coded by two annotators into the fol-lowing four disjoint classes (See Figure 1 for anoverview):Verifiable Proposition [Experiential(VERIFEXP)and Non-experiential(VERIFNON)].
A proposi-tion is verifiable if it contains an objective asser-6http://www.lawschool.cornell.edu/ceri/30Figure 1: Flow chart for annotation (It refers to the sentence (or clause) being annotated)# propositionVERIFEXP1 I?ve been a physician for 20 years.2 My son has hypolycemia.3 They flew me to NY in February.4 The flight attendant yelled at the passengers.VERIFNON5 They can have inhalation reactions.6 since they serve them to the whole plane.7 Peanuts do not kill people.8 Clearly, peanuts do not kill people.9 I believe peanuts do not kill people.10 The governor said that he enjoyed it.11 food allergies are rare12 food allergies are seen in less than 20% of thepopulationUNVERIF13 Again, keep it simple.14 Banning peanuts will reduce deaths.15 I enjoy having peanuts on the plane.16 others are of uncertain significance17 banning peanuts is a slippery slopeNONARG18 Who is in charge of this?19 I have two comments20 http://www.someurl.com21 Thanks for allowing me to comment.22 - MikeTable 1: Example Sentences.
* Italics is used to illustrate core clause (Section 3.2).tion, where objective means ?expressing or deal-ing with facts or conditions as perceived withoutdistortion by personal feelings, prejudices, or in-terpretations.
?7Such assertions have truth valuesthat can be proved or disproved with objective ev-idence8:Consider the examples from Table 1. propo-sitions 1 through 7 are clearly verifiable becausethey only contain objective assertions.
proposi-tions 8 and 9 show that adding subjective expres-sions such as ?Clearly?
(e.g.
sentence 8) or ?I be-lieve that?
(e.g.
sentence 9) to an objectively veri-fiable proposition (e.g.
sentence 7) does not affectthe verifiability of the proposition.
Sentence 10 isconsidered verifiable because whether or not the7http://www.merriam-webster.com/8The correctness of the assertion or the availability of theobjective evidence does not matter.governor said ?he enjoyed the peanuts?
can be ver-ified with objective evidence, even though whetherhe really does or not cannot be verified.For the purpose of identifying an appropriatetype of support, we employ a rather lenient no-tion of objectivity: an assertion is objectively veri-fiable if the domain of comparison is free of inter-pretation.
For instance, sentence 11 is regarded asobjectively verifiable, because it is clear, i.e.
it isnot open for interpretation, that percentage of thepopulation is the metric under comparison eventhough the threshold is purely subjective9.
Therationale is that this type of proposition can besufficiently substantiated with objective evidence(e.g.
published statistics showing the percentageof people suffering from food allergies).
Anotherway to think about it is that sentence 11 is a looseway of saying a (more obviously) verifiable sen-tence 12, where the commenter neglected to men-tion the threshold.
This is fundamentally differentfrom propositions 13 through 16 for which objec-tive evidence cannot exist10.A verifiable proposition can further be dis-tinguished as experiential or not, depending onwhether the proposition is about the writer?s per-sonal state or experience (VERIFEXP) or some-thing non-experiential (VERIFNON).
This dif-ference determines whether objective evidence ismandatory or optional with respect to the credibil-ity of the comment.
Evidence is optional when theevidence contains private information or is prac-tically impossible to be provided: While proposi-tions 1 through 3 can be proved with pictures ofofficial documents, for instance, the commentersmay not want to provide them for privacy rea-sons.
Also, the website interface may not al-9One may think anything less frequent than the average israre and another may have more stricter notion.10Objective evidence may exist for propositions that pro-vide reasons for propositions 13 through 16.31Regulation VERIFNONVERIFEXPUNVERIF Subtotal NONARG Total # of CommentsAPR 1106 851 4413 6370 522 6892 820HMCP 251 416 1733 2400 186 2586 227Total 1357 1267 6146 8770 708 9476 1047Table 2: Class Distribution Over Sentences and Clauseslow pictures to be uploaded in comment section,which is the case with most websites.
sentence 4is practically impossible to prove unless the com-menter happened to have recorded the conversa-tion, and the website interface allows multimediafiles to be uploaded.
This is different from propo-sitions 5 through 12, which should be (if valid, thatis) based on non-experiential knowledge the com-menter acquired through objective evidence avail-able to the public.In certain domains, VERIFEXPpropositions?sometimes referred to as anectotal evidence?provide the novel knowledge that readers are seek-ing.
In eRulemaking, for instance, agencies ac-cept a wide variety of comments from the pub-lic, including accounts of personal experience withthe problems or conditions the new regulation pro-poses to address.
If these accounts are relevant andplausible, the agencies may use them, even if theyinclude no independent substantiation.
Taking itto an extreme, even if the ?experience?
is fake, the?experience?
and opinions based on them are valu-able to the agencies as long as the ?experience?
isrealistic.Unverifiable Proposition (UNVERIF).
A propo-sition is unverifiable if it cannot be proved with ob-jective evidence.
UNVERIF propositions are typi-cally opinions, suggestions, judgements, or asser-tions about what will happen in the future.
(Seepropositions 13 through 17.)
Assertions about thefuture are typically unverifiable, because there isno direct evidence that something will happen.
Avery prominent exception is a prediction based ona policy of organizations, i.e.
?The store will beopen this Sunday.?
where the policy serves as a di-rect evidence.Non-Argumentative (NONARG).
A sentence orclause is in this category if it is not a proposition,i.e.
it cannot be verified with objective evidenceand no supporting reason is required for the pur-pose of improving the comment quality.
Exam-ples include question, greeting, citation, and URL.
(See sentences 18 through 21.
)2.2 Annotation ResultsThe resulting distribution of classes is shown inTable 2.
Note that even though we employeda rather lenient definition of objective proposi-tions, the distribution is highly skewed towardsUNVERIF propositions.
This is expected becausethe comments are written by people who want toexpress their opinions about a regulation.
Also,NONARG sentences comprise about 7% of thedata, suggesting that most comment propositionsneed to be supported with a reason or evidence formaximal credibility.The inter-coder reliability checked on 30% ofthe data is moderate, yielding an Unweighted Co-hen?s ?
of 0.73.
Most of the disagreement oc-curred in propositions like ?Airlines have to pro-vide compensation for both fees and lost bags?
inwhich it is not clear from the context whether itis an opinion (UNVERIF) or a law (VERIFNON).Also, opinions that may be verifiable (e.g.
?Theproblems with passenger experience are not de-pendant on aircraft size!?)
seem to cause disagree-ment among annotators.3 Proposition Type Classification3.1 Learning AlgorithmTo classify each proposition in an argument asVERIFNON, VERIFEXP, or UNVERIF, we trainmulticlass Support Vector Machines (SVM) as for-mulated by Crammer and Singer (2002), and laterextended by Keerthi et al.(2008).
We use the Lib-Linear (Fan et al., 2008) implementation.
We ex-perimented with other multiclass SVM approachessuch as 1-vs-all and 1-vs-1 (all-vs-all), but the dif-ferences were statistically insignificant, consistentwith Hsu and Lin?s (2002) empirical comparisonof these methods.
Thus, we only report the per-formance of the Crammer and Singer version ofMulticlass SVM.3.2 FeaturesThe features are binary-valued, and the featurevector for each data point is normalized to havethe unit length: ?Presence?
features are binaryfeatures indicating whether the given feature ispresent in the proposition or not; ?Count?
features32are numeric counts of the occurrence of each fea-ture is converted to a set of three binary featureseach denoting 0, 1 and 2 or more occurrences.We first tried a binning method with each digitas its own interval, resulting in binary features ofthe form featCntn, but the three-interval approachproved to be better empirically, and is consistentwith the approach by Riloff and Shoen (1995).The features can be grouped into three cate-gories by purpose: Verifiability-specific (VER),Experientiality-specific (EXP) and Basic Features,each designed to capture the given proposition?sverifiability, experientiality, and both, respec-tively.
Now we discuss the features in more detail.3.2.1 Basic FeaturesN-gram Presence A set of binary features de-note whether a given unigram or bigram occursin the proposition.
The intuition is that by ex-amining the occurrence of words or phrases inVERIFNON, VERIFEXP, and UNVERIF propo-sitions, the classes that have close ties to certainwords and phrases can be identified.
For instance,when a proposition contains the word happy, theproposition tends to be UNVERIF.
From this ob-servation, we can speculate that happy is highlyassociated with UNVERIF, and went, VERIFEXP.n-gram presence, rather than the raw or normal-ized frequency is chosen for its superior perfor-mance (O?Keefe and Koprinska, 2009).Core Clause Tag (CCT) To correctly classifypropositions with main or subordinate clauses thatdo not affect the verifiability of the proposition(e.g.
propositions 8 through 10 in Table 1, respec-tively), it is necessary to distinguish features thatappear in the main clause from those that appear inthe subordinate clause.
Thus, we employ an auxil-iary feature that adds clausal information to otherfeatures by tagging them as either core or acces-sory clause.Let?s consider propositions 7, 9 and 10 in Ta-ble 1: In all three examples, the core clause is ital-icized.
In single clause cases like proposition 7,the entire proposition is the core clause.
However,for proposition 9, the core clause is the subordi-nate clause introduced by the main clause, i.e.
?Ibelieve?
should be ignored, since the verifiabilityof ?peanuts do not kill people?
is not dependent onit.
It is the opposite for proposition 10: the mainclause ?The governor said?
is the core clause, andthe rest need not be considered.
The reason is that?said?
is a speech event, and it is possible to objec-tively verify whether or not the governor verballyexpressed his appreciation of peanuts.To realize this intuition, we use syntactic parsetrees generated by the Stanford Parser (De Marn-effe et al., 2006).
In particular, Penn Treebank2 Tags contain a clause-level tag SBAR denotinga ?clause introduced by a subordinating conjunc-tion?
(Marcus et al., 1993).
The ?that?
clause inproposition 10 spans a subtree rooted by SBAR,whose left-most child has a lexical value ?that.
?Similarly, the subordinate (non-italicized) clausein proposition 9 falls in a subtree rooted by SBAR,whose only child is S. Once the main clause of agiven proposition is identified, all features set offby the clause are tagged as ?core?
and the rest aretagged as ?accessory.?
If a speech event is present,the tags are flipped.3.2.2 Verifiability-specific Features (VER)Parts-of-Speech (POS) Count Rayson etal.
(2001) have shown that the POS distributionis distinct in imaginative vs. informative writing.We expect this feature to distinguish UNVERIFpropositions from the rest.Sentiment Clue Count Wilson et al.
(2005) pro-vides a subjectivity clue lexicon, which is a list ofwords with sentiment strength tags, either strongor weak, along with additional information, suchas the sentiment polarity, Part-of-Speech Count(POS), etc.
We suspect that propositions contain-ing more sentiment words is more likely to be UN-VERIF.Speech Event Count We use the 50 most frequentObjective-speech-event text anchors crawled fromthe MPQA 2.0 corpus (Wilson and Wiebe, 2005)as a speech event lexicon.
The speech event textanchors refer to words like ?stated?
and ?wrote?that introduce written or spoken propositions at-tributed to a source.
propositions containingspeech events such as proposition 10 in Table 1are generally VERIFNONor VERIFEXP, sincewhether the attributed source has indeed made theproposition he allegedly made is objectively veri-fiable regardless of the subjectivity of the proposi-tion itself.Imperative Expression Count Imperatives, i.e.commands, are generally UNVERIF (e.g.
?Do thehomework now!?
that is, we expect there to beno objective evidence proving that the homeworkshould be done right away.
), unless the sentenceis a law or general procedure (e.g.
?The libraryshould allow you to check out books.?
where the33context makes it clear that the writer is claimingthat the library lends out books.)
This feature de-notes whether the proposition begins with a verbor contains the following: must, should, need to,have to, ought to.Emotion Expression Count These features tar-get specific tokens ?!
?, and ?...?
as well as fullycapitalized word tokens to capture the emotion intext.
The rationale is that expression of emotion islikely to be more prevalent in UNVERIF proposi-tions.3.2.3 Experientiality-specific Features (EXP)Tense Count propositions written in past tenseare rarely VERIFNON, because even in the casethat the statment is verifiable, they are likely to bethe commenter?s past experience, i.e.
VERIFEXP.Future tense are typically UNVERIF becausepropositions about what will happen in the fu-ture are often unverifiable with objective evidence,with exception being propositions like predictionsbased on policy of organizations, i.e.
?Fedex willdeliver on Sunday.?
To take advantage of these ob-servations, three binary features capture each ofthree tenses: past, present, and future.Person Count First person narratives can suggestthat the proposition is UNVERIF or VERIFEXP,except for rare cases like ?We, the passengers,...?in which the first person pronoun refers to a largebody of individuals.
This intuition is captured byhaving binary features for: 1st, 2nd and 3rd per-son.4 Experiments4.1 MethodologyA Note on Argument Detection A natural firststep in argumentation mining is to determinewhich portions of the given document comprisean argument.
It can also be framed as a binaryclassification task in which each proposition in thedocument needs to be classified as either argumen-tative or not.
Some authors choose to skip thisstep (Feng and Hirst, 2011), while others makeuse of various classifiers to achieve high level ofaccuracy, as Palau and Moens achieved over 70%accuracy on Araucaria and ECHR corpus (Reedand Moens, 2008; Palau and Moens, 2009).As we have discussed in Section 1, our setupis a bit unique in that we also consider implicitarguments, where propositions are not supportedwith explicit reason or evidence, as argumentative.As a result, only about 7%(NONARGTOTALin Table 2) ofour entire dataset is marked as non-argumentative,most of which consists of questions and greetings.By simply searching for specific unigrams, suchas ???
and ?thank?, we achieve over 99% F1scorein determining which propositions are part of anargument.The remaining experiments were done withoutnon-argumentative propositions, i.e.
NONARG inTable 2.Experimental Setup We first randomly selected292 comments as held-out test set, resulting in thedistribution shown in Table 4.
Then, VERIFNONand VERIFEXPin the training set were oversam-pled so that the classes are equally distributed.During training, five fold cross-validation wasdone on the training set to tune the C parameterto 32.
Because the micro-averaged F1score canbe easily boosted on datasets with highly skewedclass distribution, we optimize for the macro-averaged F1score.Preprocessing was kept at a minimal level: cap-ital letters were lowercased after counting fullycapitalized words, and numbers were converted toa NUM token.VERIFNONVERIFEXPUNVERIF TotalTrain 987 900 4459 6346Test 370 367 1687 2424Total 1357 1267 6146 8770Table 4: # of propositions in Train and Test Set4.2 Results & AnalysisTable 3 shows a summary of the classification re-sults.
The best overall performance is achievedby combining all features (UNI+BI+VER+EXP),yielding 68.99% macro-averaged F1, where thegain over the baseline is statistically significantaccording to the bootstrap method with 10,000samples (Efron and Tibshirani, 1994; Berg-Kirkpatrick et al., 2012).Core Clause Tag (CCT) We do not report theperformance of employing feature sets with CoreClause Tag (CCT) in Table 3, because the effectof CCT on each of the six sets of features is sta-tistically insignificant.
This is surprising at first,given the strong motivation for distinguishing thecore clause from auxiliary clause, as addressed inthe previous section: Subordinate clauses like ?Ibelieve?
should not cause the entire proposition tobe classified as UNVERIF, and clauses like ?Hesaid?
should serve as a queue for VERIFNONorVERIFEXP, even if an unverifiable clause follows34Feature SetUNVERIF vs All VERIFNONvs All VERIFEXPvs All Average F1Pre.
Rec.
F1Pre.
Rec.
F1Pre.
Rec.
F1Macro MicroUNI(base)85.24 79.43 82.23 42.57 51.89 46.77 61.10 66.76 63.80 64.27 73.31UNI+BI82.14 89.69* 85.75* 51.67* 37.57 43.51 73.48* 62.67 67.65* 65.63 77.64*VER88.52* 52.10 65.60 28.41 61.35* 38.84 42.41 73.02* 53.65 52.70 56.68EXP82.42 4.45 8.44 20.92 76.49* 32.85 31.02 82.83* 45.14 28.81 27.31VER+EXP89.40* 49.50 63.72 29.25 71.62* 41.54 50.00 79.56* 61.41 55.55 57.43UNI+BI+VER+EXP86.86* 83.05* 84.91* 49.88* 55.14 52.37* 66.67* 73.02* 69.70* 68.99* 77.27*Table 3: Three class classification results in % (Crammer & Singer?s Multiclass SVMs)Precision, recall, and F1scores are computed with respect to each one-vs-all classification problem for evaluation purposes,though a single machine is built for the multi-class classification problem, instead of 3 one-vs-all classifiers.
The star (*)indicates that the given result is statistically significantly better than the unigram baseline.Fts UNI UNICCTUNVERIF+should, whatever, respon-sibilityshouldC, shouldA,understandC-previous, solve, florida,exposed, reacted, reply,kindsexposedC, solveC,NUMC, floridaC,reactedC, poolC, owedCVERIFNON+impacted, NUM, solve,cars, pull, kinds, congressimpactedC, solveC,carsC, NUMC, poolC,writingC, deathC, linkC-should, seems, comments shouldC, commentsCVERIFEXP+owed, consumed, saw, ex-pert, interesting, him, re-acted, refinanceowedC, consumedC,expertC, reactedC,happenedC, interestingC-impacted, wo impactedC, woC,concernC, diedCTable 5: Most Informative Features for UNI and UNICCT10 Unigrams with the largest weight (magnitude) withrespect to each class ( + : positive weight / - : negativeweight).it.
Our conjecture turned out to be wrong, mainlybecause such distinction can be made for only asmall subset of the data: For instance, over 83%of the unigrams are tagged as core in the UNI fea-ture set.
Thus, most of the important features forfeature sets with CCT end up being features withcore tag, and the important features for feature setswith and without CCT are practically the same, asshown in Table 5, resulting in statistically insignif-icant performance differences.Informative Features The most informative fea-Feature Set UNI+BI+VER+EXPUNVERIF+should,StrSentClue>2, VB>2-StrSentClue0, VBD>2, air, since, no one, al-lergic, not anVERIFNON+die, death, reaction, person, allergen, air-borne, no one, allergies-PER1st, shouldVERIFEXP+VBD>2, PER1st, i have, his, he, him, time !-VBZ>2, PER2ndTable 6: Most Informative Features for UNI+BI+VER+EXP10 Features with the largest weight (magnitude) with re-spect to each class ( + : positive weight / - : negativeweight).tures reported in Table 6 exhibit interesting differ-ences among the three classes: Sentiment bearingwords, i.e.
?should?
and strong sentiment clues,are good indicators of UNVERIF, whereas personand tense information is crucial for VERIFEXP.As expected, the strong indicators of UNVERIFand VERIFEXP, namely ?should?
and PER1starenegatively associated with VERIFNON.
It is in-triguing to see that the heavily weighted featuresof VERIFNONare non-verb content words, unlikethose of the other classes.
One explanation for thisis that VERIFNONare rarely indicated by specificcues; instead, a good sign of VERIFNONis theabsences of cues for the other classes, which areoften function words and verbs.
What is remain-ing, then, are non-verb content words.
Also, cer-tain content words seem to be more likely to bringabout factual discussions.
For instance, technicalterms like?allergen?
and ?airborne,?
appear in ver-ifiable non-experiential propositions as ?The FDArequires labeling for the following 8 allergens.
?Non-n-gram Features Table 3 clearly shows thatthe three non-n-gram features, VER, EXP, andVER+EXP, do not perform as well as the n-gramfeatures.
But still, the performance is impressive,given the drastic difference in the dimensionalityof the features: Even the combined feature set,VER+EXP, consists of only about 100 features,when there are over 8,000 unigrams and close to70,000 bigrams.
In other words, the non-n-gramfeatures are effectively capturing characteristicsof each class.
This is very promising, since thisshows that a better understanding of the types ofproposition can potentially lead to a more conciseset of features with equal, or even better, perfor-mance.Also notice that VER outperforms EXP for themost part, even with respect to VERIFNONvs Alland VERIFEXPvs All, except for recall.
This is in-35triguing, because VER are mostly from subjectiv-ity detection domain, intended to capture the sub-jectivity of words in the propositions leveragingon pre-built lexia.
Simply considering subjectivityof words should provide no means of distinguish-ing VERIFNONfrom VERIFEXP.
One of the rea-sons for VER?s superior performance over EXP isthat EXP by itself is inadequate for the classifi-cation task: EXP consists of only 6 (or 12 withCCT) features denoting the person and tense infor-mation.
Another reason is that VER, in a limitedfashion, does encode experientiality: For instance,past tense propositions can be identified with theexistence of VBD(verb, past tense) and VBN(verb,past participle).5 Related WorkArgumentation Mining The primary goal of ar-gumentation mining has been to identify and ex-tract argumentative structures present in docu-ments, which are often written by profession-als (Moens et al., 2007; Wyner et al., 2010; Fengand Hirst, 2011; Ashley and Walker, 2013).
In cer-tain cases, the specific document structure allowsadditional means of identify arguments (Mochalesand Moens, 2008).
Even the work on online textdata, which are less rigid in structure and oftencontain insufficiently supported propositions, fo-cus on the extraction of arguments (Villalba andSaint-Dizier, 2012; Cabrio and Villata, 2012).
We,however, are interested in the assessment of theargumentative structure, potentially providing rec-ommendations to readers and feedback to the writ-ers.
Thus it is crucial that we also process unsub-stantiated propositions, which we consider as im-plicit arguments.
Our approach should be valu-able for processing documents like online usercomment where arguments may not have adequatesupport and an automatic means of analysis can beuseful.Subjectivity Detection Work to distinguish sub-jective from objective propositions (e.g.
(Wiebeand Riloff, 2005)), often a subtask for sentimentanalysis (Pang and Lee, 2008), is relevant to ourwork since we are concerned with the objectiveverifiability of propositions.
In particular, previ-ous work attempts to detect certain types of sub-jective proposition: Conrad et al.
(2012) iden-tify arguing subjectivity propositions and tag themwith argument labels in order to cluster argumentparaphrases.
Others incorporate this task as a com-ponent for solving related problems, such as an-swering opinion-based questions and determiningthe writer?s political stance (Somasundaran et al.,2007; Somasundaran and Wiebe, 2010).
Similarly,Rosenthal and McKeown (2012) identify opinion-ated propositions expressing beliefs, leveragingfrom previous work in sentiment analysis and be-lief tagging.
While the class of subjective propo-sitions in subjectivity detection strictly containsUNVERIF propositions, it also partially overlapswith the VERIFEXPand VERIFNONclasses ofour work: We want to identify verifiable assertionswithin propositions, rather than determine the sub-jectivity of the proposition as a whole (e.g.
propo-sition 8 in Table 1 is classified as a VERIFNON,though ?Clearly?
is subjective.).
We also distin-guish two types of verifiable propositions, whichis necessary for the purpose of identifying appro-priate types of support.6 Conclusions and Future WorkWe have proposed a novel task of automaticallyclassifying each proposition as UNVERIFIABLE,VERIFIABLE NONEXPERIENTIAL, or VERIFI-ABLE EXPERIENTIAL, where the appropriate typeof support is reason, evidence, and optional evi-dence, respectively.
This classification, once theexisting support relations among propositions areidentified, can provide an estimate of how well thearguments are supported.
We find that SupportVector Machines (SVM) classifiers trained withn-grams and other features to capture the verifi-ability and experientiality exhibit statistically sig-nificant improvement over the unigram baseline,achieving a macro-averaged F1score of 68.99%.In the process, we have built a gold-standarddataset of 9,476 propositions from 1,047 com-ments submitted to an eRulemaking platform.One immediate avenue for future work is to in-corporate the identification of relations among thepropositions in an argument to the system to ana-lyze the adequacy of the supporting information inthe argument.
This, in turn, can be used to recom-mend comments to readers and provide feedbackto writers so that they can construct better argu-ments.AcknowledgmentsThis work was supported in part by NSF grantsIIS-1111176 and IIS?1314778.
We thank ourannotators, Pamela Ijeoma Amaechi and SimonBoehme, as well as the Cornell NLP Group andthe reviewers for helpful comments.36ReferencesKevin D. Ashley and Vern R. Walker.
2013.
From in-formation retrieval (ir) to argument retrieval (ar) forlegal cases: Report on a baseline study.
In JURIX,pages 29?38.Taylor Berg-Kirkpatrick, David Burkett, and DanKlein.
2012.
An empirical investigation of statis-tical significance in nlp.
In Proceedings of the 2012Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Natu-ral Language Learning, EMNLP-CoNLL ?12, pages995?1005, Stroudsburg, PA, USA.
Association forComputational Linguistics.Elena Cabrio and Serena Villata.
2012.
Combin-ing textual entailment and argumentation theory forsupporting online debates interactions.
In Proceed-ings of the 50th Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), pages 208?212, Jeju Island, Korea, July.
As-sociation for Computational Linguistics.Alexander Conrad, Janyce Wiebe, Hwa, and Rebecca.2012.
Recognizing arguing subjectivity and argu-ment tags.
In Proceedings of the Workshop onExtra-Propositional Aspects of Meaning in Com-putational Linguistics, ExProM ?12, pages 80?88,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Koby Crammer and Yoram Singer.
2002.
On the algo-rithmic implementation of multiclass kernel-basedvector machines.
J. Mach.
Learn.
Res., 2:265?292,March.Marie-Catherine De Marneffe, Bill Maccartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InIn Proc.
Intl Conf.
on Language Resources and Eval-uation (LREC, pages 449?454.B.
Efron and R.J. Tibshirani.
1994.
An Introduction tothe Bootstrap.
Chapman & Hall/CRC Monographson Statistics & Applied Probability.
Taylor & Fran-cis.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: A li-brary for large linear classification.
J. Mach.
Learn.Res., 9:1871?1874, June.Vanessa Wei Feng and Graeme Hirst.
2011.
Classi-fying arguments by scheme.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies - Volume 1, HLT ?11, pages 987?996, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Jennifer L. Hochschild and Michael Danielson, 1998.Can We Desegregate Public Schools and Subsi-dized Housing?
Lessons from the Sorry History ofYonkers, New York, chapter 2, pages 23?44.
Uni-versity Press of Kansas, Lawrence KS, edited byclarence stone edition.Chih-Wei Hsu and Chih-Jen Lin.
2002.
A comparisonof methods for multiclass support vector machines.Trans.
Neur.
Netw., 13(2):415?425, March.S.
Sathiya Keerthi, S. Sundararajan, Kai-Wei Chang,Cho-Jui Hsieh, and Chih-Jen Lin.
2008.
A sequen-tial dual method for large scale multi-class linearsvms.
In Proceedings of the 14th ACM SIGKDD in-ternational conference on Knowledge discovery anddata mining, KDD ?08, pages 408?416, New York,NY, USA.
ACM.Jeffrey S. Lubbers.
2006.
A Guide to Federal AgencyRulemaking.
American Bar Association Chicago,4th ed.
edition.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The penn treebank.
COMPUTA-TIONAL LINGUISTICS, 19(2):313?330.Raquel Mochales and Marie-Francine Moens.
2008.Study on the structure of argumentation in case law.In Proceedings of the 2008 Conference on LegalKnowledge and Information Systems: JURIX 2008:The Twenty-First Annual Conference, pages 11?20,Amsterdam, The Netherlands, The Netherlands.
IOSPress.Marie-Francine Moens, Erik Boiy, Raquel MochalesPalau, and Chris Reed.
2007.
Automatic detectionof arguments in legal texts.
In Proceedings of the11th International Conference on Artificial Intelli-gence and Law, ICAIL ?07, pages 225?230, NewYork, NY, USA.
ACM.Tim O?Keefe and Irena Koprinska.
2009.
Feature se-lection and weighting methods in sentiment analy-sis.
In Proceedings of the 14th Australasian Docu-ment Computing Symposium.Raquel Mochales Palau and Marie-Francine Moens.2009.
Argumentation mining: The detection, classi-fication and structure of arguments in text.
In Pro-ceedings of the 12th International Conference on Ar-tificial Intelligence and Law, ICAIL ?09, pages 98?107, New York, NY, USA.
ACM.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135, January.Joonsuk Park, Sally Klingel, Claire Cardie, MaryNewhart, Cynthia Farina, and Joan-Josep Vallb?e.2012.
Facilitative moderation for online participa-tion in erulemaking.
In Proceedings of the 13th An-nual International Conference on Digital Govern-ment Research, dg.o ?12, pages 173?182, New York,NY, USA.
ACM.John L. Pollock.
1987.
Defeasible reasoning.
Cogni-tive Science, 11:481?518.Paul Rayson, Andrew Wilson, and Geoffrey Leech.2001.
Grammatical word class variation within thebritish national corpus sampler.
Language and Com-puters.37Raquel Mochales Palau Rowe Glenn Reed, Chris andMarie-Francine Moens.
2008.
Language resourcesfor studying argument.
In Proceedings of the 6thconference on language resources and evaluation -LREC 2008, pages 91?100.
ELRA.Ellen Riloff and Jay Shoen.
1995.
Automaticallyacquiring conceptual patterns without an annotatedcorpus.
In In Proceedings of the Third Workshop onVery Large Corpora, pages 148?161.Sara Rosenthal and Kathleen McKeown.
2012.
De-tecting opinionated claims in online discussions.
InICSC, pages 30?37.Swapna Somasundaran and Janyce Wiebe.
2010.
Rec-ognizing stances in ideological on-line debates.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Gener-ation of Emotion in Text, CAAGET ?10, pages 116?124, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Swapna Somasundaran, Josef Ruppenhofer, and JanyceWiebe.
2007.
Detecting arguing and sentiment inmeetings.
In Proceedings of the SIGdial Workshopon Discourse and Dialogue.Stephen E. Toulmin, Richard Rieke, and Allan Janik.1979.
An Introduction to Reasoning.
MacmillanPublishing Company.S.E.
Toulmin.
1958.
The Uses of Argument.
Cam-bridge University Press.Maria Paz Garcia Villalba and Patrick Saint-Dizier.2012.
Some facets of argument mining for opinionanalysis.
In COMMA, pages 23?34.Janyce Wiebe and Ellen Riloff.
2005.
Creating subjec-tive and objective sentence classifiers from unanno-tated texts.
In In CICLing2005, pages 486?497.Theresa Wilson and Janyce Wiebe.
2005.
Annotat-ing attributions and private states.
In Proceedings ofACL Workshop on Frontiers in Corpus AnnotationII: Pie in the Sky.Theresa Wilson.
2005.
Recognizing contextual po-larity in phrase-level sentiment analysis.
In In Pro-ceedings of HLT-EMNLP, pages 347?354.Adam Wyner, Raquel Mochales-Palau, Marie-FrancineMoens, and David Milward.
2010.
Semantic pro-cessing of legal texts.
chapter Approaches to TextMining Arguments from Legal Cases, pages 60?79.Springer-Verlag, Berlin, Heidelberg.38
