Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 1?6,Avignon, France, April 23 - 24 2012. c?2012 Association for Computational LinguisticsVisualization of Linguistic PatternsandUncovering Language History from Multilingual ResourcesMiriam Butt1 Jelena Prokic?2 Thomas Mayer2 Michael Cysouw31Department of Linguistics, University of Konstanz2Research Unit Quantitative Language Comparison, LMU Munich3Research Center Deutscher Sprachatlas, Philipp University of Marburg1 IntroductionThe LINGVIS and UNCLH (Visualization of Lin-guistic Patterns & Uncovering Language His-tory from Multilingual Resources) were originallyconceived of as two separate workshops.
Due toperceived similarities in content, the two work-shops were combined and organized jointly.The overal aim of the joint workshop was toexplore how methods developed in computationallinguistics, statistics and computer science canhelp linguists in exploring various language phe-nomena.
The workshop focused particularly ontwo topics: 1) visualization of linguistic patterns(LINGVIS); 2) usage of multilingual resources incomputational historical linguistics (UNCLH).2 LINGVISThe overall goal of the first half of the work-shop was to bring together researchers work-ing within the emerging subfield of computa-tional linguistics ?
using methods establishedwithin Computer Science in the fields of Infor-mation Visualization (InfoVis) and Visual Ana-lytics in conjunction with methodology and anal-yses from theoretical and computational linguis-tics.
Despite the fact that statistical methods forlanguage analysis have proliferated in the lasttwo decades, computational linguistics has so faronly marginally availed itself of techniques fromInfoVis and Visual Analytics (e.g., Honkela etal.
(1995); Neumann et al (2007); Collins etal.
(2009); Collins (2010); Mayer et al (2010a);Mayer et al (2010b); Rohrdantz et al (2011)).The need to integrate methods from InfoVis andVisual Analytics arises particularly with respectto situations in which the amount of data to beanalyzed is huge and the interactions between rel-evant features are complex.
Both of these situ-ations hold for much of current (computational)linguistic analysis.
The usual methods of sta-tistical analysis do not allow for quick and easygrasp and interpretation of the patterns discoveredthrough statistical processing and an integrationof innovative visualization techniques has becomeimperative.The overall aim of the first half of the workshopwas thus to draw attention to this need and to thenewly emerging type of work that is beginning torespond to the need.
The workshop succeeded inbringing together researchers interesting in com-bining techniques and methodology from theo-retical and computational linguistics with InfoVisand Visual Analytics.Three of the papers in the workshop focusedon the investigation and visualization of lexicalsemantics.
Rohrdantz et al present a diachronicstudy of fairly recently coined derivational suf-fixes (-gate, -geddon, -athon) as used in newspa-per corpora across several languages.
Their anal-ysis is able to pin-point systematic differences incontextual use as well as some first clues as tohow and why certain new coinages spread bet-ter than others.
Heylen et al point out that me-thods such as those used in Rohrdantz et al,while producing interesting results, are essentiallyblack boxes for the researchers ?
it is not clearexactly what is being calculated.
Their paperpresents some first steps towards making the blackbox more transparent.
In particular, they takea close look at individual tokens and their se-mantic use with respect to Dutch synsets.
Cru-cially, they anticipate an interactive visualizationthat will allow linguistically informed lexicogra-1phers to work with the available data and patterns.A slightly different take on synset relations is pre-sented by Lohk et al, who use visualization me-thods to help identify errors in WordNets acrossdifferent languages.Understanding differences and relatedness be-tween languages or types of a language is the sub-ject of another three papers.
Littauer et al usedata from the WALS (World Atlas of LanguageStructures; Dryer and Haspelmath (2011)) tomodel language relatedness via heat maps.
Theyovercome two difficulties: one is the sparsenessof the WALS data; another is that WALS doesnot directly contain information about possible ef-fects of language contact.
Littauer et al attemptto model the latter by taking geographical infor-mation about languages into account (neighboringlanguages and their structure).
A different kindof language relatedness is investigated by Yan-nakoudakis et al, who look at learner corpora anddevelop tools that allow an assessment of learnercompetence with respect to various linguistic fea-tures found in the corpora.
The number of rel-evant features is large and many of them are in-terdependent or interact.
Thus, the amount andcomplexity of the data present a classic case ofcomplex data sets that are virtually impossible toanalyze well without the application of visualiza-tion methods.
Finally, Lyding et al take academictexts and investigate the use of modality acrossacademic registers and across time in order toidentify whether the academic language used indifferent subfields (or adjacent fields) of an aca-demic field has an effect on the language use ofthat field.3 UNCLHThe second half of the workshop focused onthe usage of multilingual resources in computa-tional historical linguistics.
In the past 20 years,the application of quantitative methods in his-torical linguistics has received increasing atten-tion among linguists (Dunn et al, 2005; Heg-garty et al, 2010; McMahon and McMahon,2006), computational linguists (Kondrak, 2001;Hall and Klein, 2010) and evolutionary anthropol-ogists (Gray and Atkinson, 2003).
Due to the ap-plication of these quantitative methods, the fieldof historical linguistics is undergoing a renais-sance.
One of the main problems that researchersface is the limited amount of suitable compara-tive data, often falling back on relatively restricted?Swadesh type?
wordlists.
One solution is to usesynchronic data, like dictionaries or texts, whichare available for many languages.
For example,in Kondrak (2001), vocabularies of four Algo-nquian languages were used in the task of au-tomatic cognate identification.
Another solutionemployed by Snyder et al (2010) is to apply anon-parametric Bayesian framework to two non-parallel texts in the task of text deciphering.
Al-though very promising, these approaches have sofar only received modest attention.
Thus, manyquestions and challenges in the automatizationof language resources in computational historicallinguistics remain open and ripe for investigation.In dialectological studies, there is a long tra-dition, starting with Se?guy (1971), in which lan-guage varieties are grouped together on the ba-sis of their similarity with respect to certain prop-erties.
Later work in this area has incorporatedmethods of string alignment for a quantitativecomparison of individual words to obtain an aver-age measure of the similarity of languages.
Thisline of research became known as dialectome-try.
Unlike traditional dialectology which is basedon the analysis of individual items, dialectometryshifts focus on the aggregate level of differences.Most of the work done so far in dialectometryis based on the carefully selected wordlists andproblems with the limited amount of suitable data(i.e.
computer readable and comparable across di-alects) are also present in this field.This workshop brings together researchers in-terested in computational approaches that uncoversound correspondences and sound changes, auto-matic identification of cognates across languagesand language comparison based both on wordlistsand parallel texts.
First, Wettig et al investigatethe sound correspondences in cognate sets in asample of Uralic languages.
Then, List?s contri-bution to the volume introduces a novel methodfor automatic cognate detection in multilingualwordlists which combines various previous ap-proaches for string comparison.
The paper byMayer & Cysouw presents a first step to use par-allel texts for a quantitative comparison of lan-guages.
The papers by Scherrer and Prokic?
etal.
both are in the spirit of the dialectometric lineof research.
Further, Ja?ger reports on quantify-ing language similarity via phonetic alignment ofcore vocabulary items.
Finally, some of the pa-2pers presented in this workshop deal with furthertopics in quantitative language comparison, likethe application of phylogenetic methods in cre-ole research in the paper by Daval-Markussen &Bakker, and the study of the evolution of the Aus-tralian kinship terms reported on in the paper byMcConvell & Dousset.In the next section, we give a brief introduc-tion into the papers presented in this workshop,ordered according to the program of the oral pre-sentations at the workshop.4 PapersChristian Rohrdantz, Andreas Niekler, AnnetteHautli, Miriam Butt and Daniel A. Keim (?Lex-ical Semantics and Distribution of Suffixes ?A Visual Analysis) present a quantitative cross-linguistic investigation of the lexical semanticcontent expressed by three suffixes originating inEnglish: -gate, -geddon and -athon.
Using datafrom newspapers, they look at the distribution andlexical semantic usage of these morphemes acrossseveral languages and also across time, with atime-depth of 20 years for English.
Using tech-niques from InfoVis and Visual Analytics is cru-cial for the analysis as the occurrence of these suf-fixes in the available corpora is comparatively rareand it is only by dint of processing and visualiz-ing huge amounts of data that a clear pattern canbegin to emerge.Kris Heylen, Dirk Speelman and Dirk Geer-aerts (?Looking at Word Meaning.
An Interac-tive Visualization of Semantic Vector Spaces forDutch synsets?)
focus on the pervasive use of Se-mantic Vector Spaces (SVS) in statistical NLPas a standard technique for the automatic mod-eling of lexical semantics.
They take on thefact that while the method appears to work fairlywell (though they criticize the standardly avail-able evaluation measures via some created goldstandard), it is in fact quite unclear how it capturesword meaning.
That is, the standard technologycan be seen as a black box.
In order to find a wayof providing some transparency to the method,they explore the way an SVS structures the indi-vidual occurrences of words with respect to theoccurrences of 476 Dutch nouns.
These weregrouped into 214 synsets in previous work.
Thispaper looks at a token-by-token similarity matrixin conjunction with a visualization that uses theGoogle Chart Tools and compares the results withprevious work, especially in light of different usesin different versions of Dutch.Ahti Lohk, Kadri Vare and Leo Vo?handu(?First Steps in Checking and Comparing Prince-ton WordNet and Estonian WordNet?)
use visu-alization methods to compare two existing Word-Nets (English and Estonian) in order to identifyerrors and semantic inconsistencies that are a re-sult of the manual coding.
Their method opensup a potentially interesting way of automaticallychecking for inconsistencies and errors not onlyat a fairly basic and surface level, but by work-ing with the lexical semantic classification of thewords in question.Richard Littauer, Rory Turnbull and AlexisPalmer (?Visualizing Typological Relationships:Plotting WALS with Heat Maps?)
present a novelway of visualizing relationships between lan-guages.
The paper is based on data extracted fromthe World Atlas of Language Structures (WALS),which is the most complete set of typological anddigitized data available to date, but which presentstwo challenges: 1) it actually has very low cover-age both in terms of languages represented andin terms of feature description for each language;2) areal effects are not coded for.
While the au-thors find a way to overcome the first challenge,the paper?s real contribution lies in proposing amethod for overcoming the second challenge.
Inparticular, the typological data is filtered by geo-graphical proximity and then displayed by meansof heat maps, which reflect the strength of similar-ity between languages for different linguistic fea-tures.
Thus, the data should allow one to be ableto ascertain areal typological effects via a singleintegrated visualization.Helen Yannakoudakis, Ted Briscoe andTheodora Alexopoulou (?Automatic SecondLanguage Acquisition Research: IntegratingInformation Visualisation and Machine Learn-ing?)
look at yet another domain of application.They show how data-driven approaches tolearner corpora can support Second LanguageAcquisition (SLA) research when integratedwith visualization tools.
Learner corpora areinteresting because their analysis requires a goodunderstanding of a complex set of interactinglinguistic features across corpora with differentdistributional patterns (since each corpus po-tentially diverges from the standard form of thelanguage by a different set of features).
The paper3presents a visual user interface which supportsthe investigation of a set of linguistic featuresdiscriminating between pass and fail examscripts.
The system displays directed graphs tomodel interactions between features and supportsexploratory search over a set of learner texts.A very useful result for SLA is the proposalof a new method for empirically quantifyingthe linguistic abilities that characterize differentlevels of language learning.Verena Lyding, Ekaterina Lapshinova-Koltunski, Stefania Degaetano-Ortlieb, HenrikDittmann and Chris Culy (?Visualizing LinguisticEvolution in Academic Discourse?)
describemethods for visualizing diachronic languagechanges in academic writing.
In particular, theylook at the use of modality across different aca-demic subfields and investigate whether adjacentsubfields affect the use of language in a givenacademic subfield.
Their findings potentiallyprovide crucial information for further NLP taskssuch as automatic text classification.Grzegorz Kondrak?s invited contribution(?Similarity Patterns in Words?)
sketches a num-ber of the author?s research projects on diachroniclinguistics.
He first discusses computational tech-niques for implementing several steps of thecomparative method.
These techniques includealgorithms that deal with a wide range of prob-lems: pairwise and multiple string alignment,calculation of phonetic similarity between twostrings, automatic extraction of recurrent soundcorrespondences, quantification of semanticsimilarity between two words, identification ofsets of cognates and building of phylogenetictrees.
In the second part, Kondrak sketchesseveral NLP projects that directly benefittedfrom his research on diachronic linguistics:statistical machine translation, word align-ment, identification of confusable drug names,transliteration, grapheme-to-phoneme conver-sion, letter-phoneme alignment and mapping ofannotations.Thomas Mayer and Michael Cysouw (?Lan-guage Comparison through Sparse MultilingualWord Alignment?)
present a novel approach onhow to calculate similarities among languageswith the help of massively parallel texts.
In-stead of comparing languages pairwise they sug-gest a simultaneous analysis of languages with re-spect to their co-occurrence statistics for individ-ual words on the sentence level.
These statisticsare then used to group words into clusters whichare considered to be partial (or ?sparse?)
align-ments.
These alignments then serve as the basisfor the similarity count where languages are takento be more similar the more words they share inthe various alignments, regardless of the actualform of the words.
In order to cope with thecomputationally demanding multilingual analysisthey introduce a sparse matrix representation ofthe co-occurrence statistics.Yves Scherrer (?Recovering Dialect Geogra-phy from an Unaligned Comparable Corpus?)
pro-poses a simple metric of dialect distance, basedon the ratio between identical word pairs and cog-nate word pairs occurring in two texts.
Scherrerproceeds from a multidialectal corpus and appliestechniques from machine translation in order toextract identical words and cognate words.
Thedialect distance is defined as as function of thenumber of cognate word pairs and identical wordpairs.
Different variations of this metric are testedon a corpus containing comparable texts from dif-ferent Swiss German dialects and evaluated on thebasis of spatial autocorrelation measures.Jelena Prokic?, C?ag?r?
Co?ltekin and John Ner-bonne (?Detecting Shibboleths?)
propose a gen-eralization of the well-known precision and re-call scores to deal with the case of detecting dis-tinctive, characteristic variants in dialect groups,in case the analysis is based on numerical differ-ence scores.
This method starts from the data thathas already been divided into groups using clus-ter analyses, correspondence analysis or any othertechnique that can identify groups of language va-rieties based on linguistic or extra-linguistic fac-tors (e.g.
geography or social properties).
Themethod seeks items that differ minimally within agroup but differ a great deal with respect to ele-ments outside it.
They demonstrate the effective-ness of their approach using Dutch and Germandialect data, identifying those words that showlow variation within a given dialect area, and highvariation outside a given area.Gerhard Ja?ger (?Estimating and VisualizingLanguage Similarities Using Weighted Align-ment and Force-Directed Graph Layout?)
reportsseveral studies to quantify language similarityvia phonetic alignment of core vocabulary items(taken from the Automated Similarity JudgementProgram data base).
Ja?ger compares several string4comparison measures based on Levenshtein dis-tance and based on Needleman-Wunsch similar-ity score.
He also tests two normalization func-tions, one based on the average score and theother based on the informatic theoretic similar-ity measure.
The pairwise similarity between alllanguages are analyzed and visualized using theCLANS software, a force directed graph layoutthat does not assume an underlying tree structureof the data.Aymeric Daval-Markussen and Peter Bakker(?Explorations in Creole Research with Phyloge-netic Tools?)
employ phylogenetic tools to inves-tigate and visualize the relationship of creole lan-guages to other (non-)creole languages on the ba-sis of structural features.
Using the morphosyn-tactic features described in the monograph onComparative Creole Syntax (Holm and Patrick,2007), they create phylogenetic trees and net-works for the languages in the sample, whichshow the similarity between the various languageswith respect to the grammatical features inves-tigated.
Their results lend support to the uni-versalist approach which assumes that creolesshow creole-specific characteristics, possibly dueto restructuring universals.
They also apply theirmethodology to the comparison of creole lan-guages to other languages, on the basis of typo-logical features from the World Atlas of LanguageStructures.
Their findings confirm the hypothe-sis that creole languages form a synchronicallydistinguishable subgroup among the world?s lan-guages.Patrick McConvell and Laurent Dousset(?Tracking the Dynamics of Kinship and So-cial Category Terms with AustKin II?)
give anoverview of their ongoing work on kinship andsocial category terms in Australian languages.They describe the AustKin I database whichallows for the reconstruction of older kinshipsystems as well as the visualization of patternsand changes.
In particular, their method recon-structs so-called ?Kariera?
kinship systems for theproto-languages in Australia.
This supports ear-lier hypotheses about the primordial world socialorganization from which Dravidian-Kariera sys-tems are considered to have evolved.
They alsoreport on more recent work within the AustKin IIproject which is devoted to the co-evoluation ofmarriage and social category systems.Hannes Wettig, Kirill Reshetnikov and RomanYangarber (?Using Context and Phonetic Fea-tures in Models of Etymological Sound Change?
)present a novel method for a context-sensitivealignment of cognate words, which relies on theinformation theoretic concept of Minimum De-scription Length to decide on the most compactrepresentation of the data given the model.
Start-ing with an initial random alignment for eachword pair, their algorithm iteratively rebuilds de-cision trees for each feature and realigns the cor-pus while monotonically decreasing the cost func-tion until convergence.
They also introduce anovel test for the quality of the models where oneword pair is omitted from the training phase.
Therules that have been learned are then used to guessone word from the other in the pair.
The Lev-enshtein distance of the correct and the guessedword is then computed to give an idea of howgood the model actually learned the regularitiesin the sound correspondences.Johann-Mattis List (?LexStat: Automatic De-tection of Cognates in Multilingual Wordlists?
)presents a new method for automatic cognatedetection in multilingual wordlists.
He com-bines different approaches to sequence compari-son in historical linguistics and evolutionary bi-ology into a new framework which closely mod-els central aspects of the comparative method.The input sequences, i.e.
words, are converted tosound classes and their sonority profiles are deter-mined.
In step 2, a permutation method is used tocreate language specific scoring schemes.
In step3, the pairwise distances between all word pairs,based on the language-specific scoring schemes,are computed.
In step 4, the sequences are clus-tered into cognate sets whose average distance isbeyond a certain threshold.
The method is testedon 9 multilingual wordlists.5 Final remarksThe breadth and depth of the research collectedin this workshop more than testify to the scopeand possibilities for applying new methods thatcombine quantitative methods with not only a so-phisticated linguistic understanding of languagephenomena, but also with visualization methodscoming out of the Computer Science fields of In-foVis and Visual Analytics.
The papers in theworkshop addressed how the emerging new bodyof work can provide advances and new insightsfor questions pertaining to theoretical linguistics5(lexical semantics, derivational morphology, his-torical linguistics, dialectology and typology) andapplied linguistic fields such as second languageacquisition and statistical NLP.6 AcknowledgmentsWe are indebted to the members of the pro-gram committee of the workshop for their ef-fort in thoroughly reviewing the papers: QuentinAtkinson, Christopher Collins, Chris Culy, DanDediu, Michael Dunn, Sheila Embleton, SimonGreenhill, Harald Hammarstro?m, Annette Hautli,Wilbert Heeringa, Gerhard Heyer, Eric Hol-man, Gerhard Ja?ger, Daniel Keim, Tibor Kiss,Jonas Kuhn, Anke Lu?deling, Steven Moran, JohnNerbonne, Gerald Penn, Don Ringe, ChristianRohrdantz, Tandy Warnow, S?ren Wichmann.We also thank the organizers of the EACL 2012conference for their help in setting up the jointworkshop.ReferencesChristopher Collins, Sheelagh Carpendale, and Ger-ald Penn.
2009.
Docuburst: Visualizing documentcontent using language structure.
Computer Graph-ics Forum (Proceedings of Eurographics/IEEE-VGTC Symposium on Visualization (EuroVis ?09)),28(3):1039?1046.Christopher Collins.
2010.
Interactive Visualizationsof Natural Language.
Ph.D. thesis, University ofToronto.Matthew S. Dryer and Martin Haspelmath, editors.2011.
The World Atlas of Language Structures On-line.
Max Planck Digital Library, Munich, 2011edition.Michael Dunn, Angela Terrill, Ger Resnik, Robert A.Foley, and Stephen C. Levinson.
2005.
Structuralphylogenetics and the reconstruction of ancient lan-guage history.
Science, 309(5743):2072?2075.Russell Gray and Quentin Atkinson.
2003.
Language-tree divergence times support the Anatolian theoryof Indo-European origins.
Nature, 426:435?439.David LW Hall and Dan Klein.
2010.
Finding cognategroups using phylogenies.
In Proceedings of theAssociation for Computational Linguistics.Paul Heggarty, Warren Maguire, and April McMahon.2010.
Splits or waves?
trees or webs?
how diver-gence measures and network analysis can unravellanguage histories.
In Philosophical Transactionsof the Royal Society (B), volume 365, pages 3829?3843.John Holm and Peter L. Patrick, editors.
2007.
Com-parative Creole Syntax.
London: Battlebridge.Timo Honkela, Ville Pulkki, and Teuvo Kohonen.1995.
Contextual relations of words in grimm tales,analyzed by self-organizing map.
In Proceedings ofInternational Conference on Artificial Neural Net-works (ICANN-95), pages 3?7.Grzegorz Kondrak.
2001.
Identifying cognates byphonetic and semantic similarity.
In Proceedingsof the North American Chapter of the Associationof Computational Linguistics.Thomas Mayer, Christian Rohrdantz, Miriam Butt,Frans Plank, and Daniel A. Keim.
2010a.
Visualiz-ing vowel harmony.
Linguistic Issues in LanguageTechnology (LiLT), 2(4).Thomas Mayer, Christian Rohrdantz, Frans Plank,Peter Bak, Miriam Butt, and Daniel A. Keim.2010b.
Consonant co-occurrence in stems acrosslanguages: Automatic analysis and visualization ofa phonotactic constraint.
In Proceedings of the2010 Workshop on NLP and Linguistics: Findingthe Common Ground, ACL 2010, pages 70?78.April McMahon and Robert McMahon.
2006.
Lan-guage Classification by Numbers.
OUP.Petra Neumann, Annie Tat, Torre Zuk, and Shee-lagh Carpendale.
2007.
Keystrokes: Personaliz-ing typed text with visualization.
In Proceedingsof Eurographics IEEE VGTC Symposium on Visu-alization.Christian Rohrdantz, Annette Hautli, Thomas Mayer,Miriam Butt, Daniel A. Keim, and Frans Plank.2011.
Towards tracking semantic change by visualanalytics.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics (Short Papers), pages 305?310.
Portland, Ore-gon.Jean Se?guy.
1971.
La relation entre la distance spa-tiale et la distance lexicale.
Revue de LinguistiqueRomane, 35(138):335?357.Benjamin Snyder, Regina Barzilay, and Kevin Knight.2010.
A statistical model for lost language deci-pherment.
In Proceedings of the Association forComputational Linguistics.6
