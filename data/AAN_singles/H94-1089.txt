ROBUSTNESS, PORTABILITY AND SCALABILITYLANGUAGE SYSTEMSRalph Weischedelweischedel@bbn.comBBN Systems and Technologies70 Fawcett StreetCambridge, MA 02138OF NATURAL1.
OBJECTIVEIn the DoD, every unit, from the smallest o the largest,commtinicates through messages.
Message are fundamental incommand and control, in intelligence analysis, and inplanning and replanning.
Our objective is to createalgorithms that will1) robustly process open source text, identifying relevantmessages, and updating a data base based on the relevantmessages;2) reduce the effort required in porting message processingsoftware to a new domain from months to weeks; and3) be scalable to broad domains with vocabularies of tens ofthousands of words.2.
APPROACHOur approach is to apply probabilistic language models andtraining over large corpora in all phases of natural anguageprocessing.
This new approach will enable systems to adaptto both new task domains and linguistic expressions not seenbefore by semi-automatically acquiring I) a domain model, 2)facts required for semantic processing, 3) grammar ules, 4)information about new words, 5) probability models onfrequency of occurrence, and 6) rules for mapping fromrepresentation to application structure.For instance, a statistical model of categories of words enablessystems to predict the most likely category of a word neverencountered by the system before and to focus on its mostlikely interpretation i context, rather than skipping the wordor considering all possible interpretations.
Markovmodelling techniques are used for this problem.In an analogous way, statistical models of language are beingdeveloped and applied at the level of syntax (form), at the levelof semantics (content), and at the contextual level (meaningand impact).3.
RECENT RESULTS?
Consistently achieved high performance in Government-sponsored evaluations (MUC-3, MUC-4, MUC-5 and TIPSTERevaluations) of data extraction systems with significantly lesshuman effort to port the PLUM system to each domain,compared with the effort reported in porting other high-performing systems.?
Achieved very consistent, high performance across bothEnglish and Japanese and across both domains (joint venturesand mieroelectronics) in MUC-5 data extraction performance.?
Applied our probabilistic model of answer correctness toimprove the performance of our PLUM data extraction systemin MUC-5.?
Achieved speedup by a factor of 80 in POST, our HiddenMarkov Model for labeling words in text by part of speech.POST is now distributed to many ARPA contractors (BostonUniv., New Mexico State Univ., New York Univ., Paramax,Syracuse Univ.)
and other sites (Advanced Decision Systems,Duke Univ., Univ.
of Iowa, City Univ.
of New York, and Univ.of Toronto).?
Completed grammar learning experiments showing thatthe error rate of a stochastic parser is a factor of two less thanthe same parser without a statistical language model.?
Integrated a pattern matching component into ourlinguistically motivated framework to give semantics tofragmented parses and discontiguous constituents.?
Created new demonstrations of the PLUM data extractionsystem in processing English texts about microelectronics andJapanese texts about microelectronics.4.
PLANS FOR THE COMING YEARParticipate in MUC-6 evaluation at both the application level(extracting data from text) and the understanding level(parsing/semantic/discourse lev l).Create/revise probabilistic models for?
word sense disambiguation,?
semantic interpretation, and?
co-reference r solution.Contribute to the definition of an evaluation methodology forglass box semantic evaluation (Semeval).446
