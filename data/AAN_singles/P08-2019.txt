Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 73?76,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsMixture Model POMDPs for Efficient Handling of Uncertaintyin Dialogue ManagementJames HendersonUniversity of GenevaDepartment of Computer ScienceJames.Henderson@cui.unige.chOliver LemonUniversity of EdinburghSchool of Informaticsolemon@inf.ed.ac.ukAbstractIn spoken dialogue systems, Partially Observ-able Markov Decision Processes (POMDPs)provide a formal framework for making di-alogue management decisions under uncer-tainty, but efficiency and interpretability con-siderations mean that most current statisticaldialogue managers are only MDPs.
TheseMDP systems encode uncertainty explicitly ina single state representation.
We formalisesuch MDP states in terms of distributionsover POMDP states, and propose a new di-alogue system architecture (Mixture ModelPOMDPs) which uses mixtures of these dis-tributions to efficiently represent uncertainty.We also provide initial evaluation results (withreal users) for this architecture.1 IntroductionPartially Observable Markov Decision Processes(POMDPs) provide a formal framework for mak-ing decisions under uncertainty.
Recent researchin spoken dialogue systems has used POMDPs fordialogue management (Williams and Young, 2007;Young et al, 2007).
These systems represent theuncertainty about the dialogue history using a prob-ability distribution over dialogue states, known asthe POMDP?s belief state, and they use approxi-mate POMDP inference procedures to make dia-logue management decisions.
However, these infer-ence procedures are too computationally intensivefor most domains, and the system?s behaviour can bedifficult to predict.
Instead, most current statisticaldialogue managers use a single state to represent thedialogue history, thereby making them only MarkovDecision Process models (MDPs).
These state rep-resentations have been fine-tuned over many devel-opment cycles so that common types of uncertaintycan be encoded in a single state.
Examples of suchrepresentations include unspecified values, confi-dence scores, and confirmed/unconfirmed features.We formalise such MDP systems as compact encod-ings of POMDPs, where each MDP state representsa probability distribution over POMDP states.
Wecall these distributions ?MDP belief states?.Given this understanding of MDP dialogue man-agers, we propose a new POMDP spoken dialoguesystem architecture which uses mixtures of MDP be-lief states to encode uncertainty.
A Mixture ModelPOMDP represents its belief state as a probabilitydistribution over a finite set of MDP states.
Thisextends the compact representations of uncertaintyin MDP states to include arbitrary disjunction be-tween MDP states.
Efficiency is maintained becausesuch arbitrary disjunction is not needed to encodethe most common forms of uncertainty, and thus thenumber of MDP states in the set can be kept smallwithout losing accuracy.
On the other hand, allow-ing multiple MDP states provides the representa-tional mechanism necessary to incorporate multiplespeech recognition hypotheses into the belief staterepresentation.
In spoken dialogue systems, speechrecognition is by far the most important source ofuncertainty.
By providing a mechanism to incorpo-rate multiple arbitrary speech recognition hypothe-ses, the proposed architecture leverages the main ad-vantage of POMDP systems while still maintainingthe efficiency of MDP-based dialogue managers.2 Mixture Model POMDPsA POMDP belief state bt is a probability distributionP (st|Vt?1, ut) over POMDP states st given the dia-73logue history Vt?1 and the most recent observation(i.e.
user utterance) ut.
We formalise the meaningof an MDP state representation rt as a distributionb(rt) = P (st|rt) over POMDP states.
We representthe belief state bt as a list of pairs ?rit, pit?
such that?i pit = 1.
This list is interpreted as a mixture ofthe b(rit).bt =?ipitb(rit) (1)State transitions in MDPs are specified with anupdate function, rt = f(rt?1, at?1, ht), which mapsthe preceding state rt?1, system action at?1, anduser input ht to a new state rt.
This function is in-tended to encode in rt all the new information pro-vided by at?1 and ht.
The user input ht is the resultof automatic speech recognition (ASR) plus spokenlanguage understanding (SLU) applied to ut.
Be-cause there is no method for handling ambiguity inht, ht is computed from the single best ASR-SLUhypothesis, plus some measure of ASR confidence.In POMDPs, belief state transitions are done bychanging the distribution over states to take into ac-count the new information from the system actionat?1 and an n-best list of ASR-SLU hypotheses hjt .This new belief state can be estimated asbt = P (st|Vt?1, ut)=?hjt?st?1P (st?1|Vt?1)P (hjt |Vt?1, st?1)P (ut|Vt?1, st?1, hjt )P (st|Vt?1, st?1, hjt , ut)P (ut|Vt?1)?
?hjt?st?1P (st?1|Vt?2, ut?1)P (hjt |at?1, st?1)P (hjt |ut)P (st|at?1, st?1, hjt )P (hjt )Z(Vt)where Z(Vt) is a normalising constant.P (st?1|Vt?2, ut?1) is the previous belief state.P (hjt |ut) reflects the confidence of ASR-SLU inhypothesis hjt .
P (st|at?1, st?1, hjt ) is normally 1for st = st?1, but can be used to allow users tochange their mind mid-dialogue.
P (hjt |at?1, st?1)is a user model.
P (hjt ) is a prior over ASR-SLUoutputs.Putting these two approaches together, we get thefollowing update equation for our mixture of MDPbelief states:bt = P (st|Vt?1, ut)?
?hjt?rit?1pit?1P (hjt |at?1, rit?1)P (hjt |ut)b(f(rit?1, at?1, hjt ))P (hjt )Z(Vt)(2)=?i?pi?t b(ri?t )where, for each i?
there is one pair i, j such thatri?t = f(rit?1, at?1, hjt )pi?t =pit?1P (hjt |at?1,rit?1)P (hjt |ut)P (hjt )Z(Vt).
(3)For equation (2) to be true, we require thatb(f(rit?1, at?1, hjt )) ?
P (st|at?1, rit?1, hjt ) (4)which simply ensures that the meaning assigned toMDP state representations and the MDP state tran-sition function are compatible.From equation (3), we see that the numberof MDP states will grow exponentially with thelength of the dialogue, proportionately to the num-ber of ASR-SLU hypotheses.
Some of the state-hypothesis pairs rit?1, hjt may lead to equivalentstates f(rit?1, at?1, hjt ), but in general pruning isnecessary.
Pruning should be done so as to min-imise the change to the belief state distribution, forexample by minimising the KL divergence betweenthe pre- and post- pruning belief states.
We use twoheuristic approximations to this optimisation prob-lem.
First, if two states share the same core features(e.g.
filled slots, but not the history of user inputs),then the state with the lower probability is pruned,and its probability is added to the other state.
Sec-ond, a fixed beam of the k most probable states iskept, and the other states are pruned.
The probabil-ity pit from a pruned state rit is redistributed to un-pruned states which are less informative than rit intheir core features.1The interface between the ASR-SLU module andthe dialogue manager is a set of hypotheses hjt pairedwith their confidence scores P (hjt |ut).
These pairsare analogous to the state-probability pairs rit, pitwithin the dialogue manager, and we can extend ourmixture model architecture to cover these pairs aswell.
Interpreting the set of hjt , P (hjt |ut) pairs as a1In the current implementation, these pruned state probabil-ities are simply added to an uninformative ?null?
state, but ingeneral we could check for logical subsumption between states.74mixture of distributions over more specific hypothe-ses becomes important when we consider pruningthis set before passing it to the dialogue manager.
Aswith the pruning of states, pruning should not sim-ply remove a hypothesis and renormalise, it shouldredistribute the probability of a pruned hypothesis tosimilar hypotheses.
This is not always computation-ally feasible, but all interfaces within the MixtureModel POMDP architecture are sets of hypothesis-probability pairs which can be interpreted as finitemixtures in some underlying hypothesis space.Given an MDP state representation, this formali-sation allows us to convert it into a Mixture ModelPOMDP.
The only additional components of themodel are the user model P (hjt |at?1, rit?1), theASR-SLU prior P (hjt ), and the ASR-SLU confi-dence score P (hjt |ut).
Note that there is no needto actually define b(rit), provided equation (4) holds.3 Decision Making with MM POMDPsGiven this representation of the uncertainty in thecurrent dialogue state, the spoken dialogue systemneeds to decide what system action to perform.There are several approaches to POMDP decisionmaking which could be adapted to this representa-tion, but to date we have only considered a methodwhich allows us to directly derive a POMDP policyfrom the policy of the original MDP.Here again we exploit the fact that the most fre-quent forms of uncertainty are already effectivelyhandled in the MDP system (e.g.
by filled vs. con-firmed slot values).
We propose that an effective di-alogue management policy can be created by sim-ply computing a mixture of the MDP policy appliedto the MDP states in the belief state list.
Moreprecisely, we assume that the original MDP systemspecifies a Q function QMDP(at, rt) which estimatesthe expected future reward of performing action atin state rt.
We then estimate the expected future re-ward of performing action at in belief state bt as themixture of these MDP estimates.Q(at, bt) ?
?ipitQMDP(at, rit) (5)The dialogue management policy is to choose theaction at with the largest value for Q(at, bt).
This isknown as a Q-MDP model (Littman et al, 1995), sowe call this proposal a Mixture Model Q-MDP.4 Related WorkOur representation of POMDP belief states using aset of distributions over POMDP states is similar tothe approach in (Young et al, 2007), where POMDPbelief states are represented using a set of partitionsof POMDP states.
For any set of partitions, the mix-ture model approach could express the same modelby defining one MDP state per partition and givingit a uniform distribution inside its partition and zeroprobability outside.
However, the mixture model ap-proach is more flexible, because the distributions inthe mixture do not have to be uniform within theirnon-zero region, and these regions do not have tobe disjoint.
A list of states was also used in (Hi-gashinaka et al, 2003) to represent uncertainty, butno formal semantics was provided for this list, andtherefore only heuristic uses were suggested for it.5 Initial ExperimentsWe have implemented a Mixture Model POMDP ar-chitecture as a multi-state version of the DIPPER?Information State Update?
dialogue manager (Boset al, 2003).
It uses equation (3) to compute beliefstate updates, given separate models for MDP stateupdates (for f(rit?1, at?1, hjt )), statistical ASR-SLU(for P (hjt |ut)/P (hjt )), and a statistical user model(for P (hjt |at?1, rit?1)).
The state list is pruned asdescribed in section 2, where the ?core features?are the filled information slot values and whetherthey have been confirmed.
For example, the sys-tem will merge two states which agree that the useronly wants a cheap hotel, even if they disagree onthe sequence of dialogue acts which lead to this in-formation.
It also never prunes the ?null?
state, sothat there is always some probability that the systemknows nothing.The system used in the experiments describedbelow uses the MDP state representation and up-date function from (Lemon and Liu, 2007), whichis designed for standard slot-filling dialogues.
Forthe ASR model, it uses the HTK speech recogniser(Young et al, 2002) and an n-best list of three ASRhypotheses on each user turn.
The prior over user in-puts is assumed to be uniform.
The ASR hypothesesare passed to the SLU model from (Meza-Ruiz et al,2008), which produces a single user input for eachASR hypothesis.
This SLU model was trained on75TC % Av.
length (std.
deviation)Handcoded 56.0 7.2 (4.6)MDP 66.6 7.2 (4.0)MM Q-MDP 73.3 7.3 (3.7)Table 1: Initial test results for human-machine dialogues,showing task completion and average length.the TownInfo corpus of dialogues, which was col-lected using the TownInfo human-machine dialoguesystems of (Lemon et al, 2006), transcribed, andhand annotated.
ASR hypotheses which result in thesame user input are merged (summing their proba-bilities), and the resulting list of at most three ASR-SLU hypotheses are passed to the dialogue manager.Thus the number of MDP states in the dialogue man-ager grows by up to three times at each step, beforepruning.
For the user model, the system uses an n-gram user model, as described in (Georgila et al,2005), trained on the annotated TownInfo corpus.2The system?s dialogue management policy is aMixture Model Q-MDP (MM Q-MDP) policy.
Aswith the MDP states, the MDP Q function is from(Lemon and Liu, 2007).
It was trained in an MDPsystem using reinforcement learning with simulatedusers (Lemon and Liu, 2007), and was not modifiedfor use in our MM Q-MDP policy.We tested this system with 10 different users, eachattempting 9 tasks in the TownInfo domain (search-ing for hotels and restaurants in a fictitious town),resulting in 90 test dialogues.
The users each at-tempted 3 tasks with the MDP system of (Lemonand Liu, 2007), 3 tasks with a state-of-the-art hand-coded system (see (Lemon et al, 2006)), and 3 taskswith the MM Q-MDP system.
Ordering of sys-tems and tasks was controlled, and 3 of the userswere not native speakers of English.
We collectedthe Task Completion (TC), and dialogue length foreach system, as reported in table 1.
Task Comple-tion is counted from the system logs when the userreplies that they are happy with their chosen option.Such a small sample size means that these results arenot statistically significant, but there is a clear trendshowing the superiority of the the MM Q-MDP sys-tem, both in terms of more tasks being completedand less variability in overall dialogue length.2Thanks to K. Georgilla for training this model.6 ConclusionsMixture Model POMDPs combine the efficiency ofMDP spoken dialogue systems with the ability ofPOMDP models to make use of multiple ASR hy-potheses.
They can also be constructed from MDPmodels without additional training, using the Q-MDP approximation for the dialogue managementpolicy.
Initial results suggest that, despite its sim-plicity, this approach does lead to better spoken dia-logue systems than MDP and hand-coded models.AcknowledgmentsThis research received funding from UK EPSRCgrant EP/E019501/1 and the European Community?sFP7 under grant no 216594 (CLASSIC project:www.classic-project.org).ReferencesJ Bos, E Klein, O Lemon, and T Oka.
2003.
DIPPER:Description and Formalisation of an Information-StateUpdate Dialogue System Architecture.
In Proc.
SIG-dial Workshop on Discourse and Dialogue, Sapporo.K Georgila, J Henderson, and O Lemon.
2005.
LearningUser Simulations for Information State Update Dia-logue Systems.
In Proc.
Eurospeech.H Higashinaka, M Nakano, and K Aikawa.
2003.Corpus-based discourse understanding in spoken dia-logue systems.
In Proc.
ACL, Sapporo.O Lemon and X Liu.
2007.
Dialogue policy learningfor combinations of noise and user simulation: transferresults.
In Proc.
SIGdial.O Lemon, K Georgila, and J Henderson.
2006.
Evalu-ating Effectiveness and Portability of ReinforcementLearned Dialogue Strategies with real users: theTALK TownInfo Evaluation.
In Proc.
ACL/IEEE SLT.ML Littman, AR Cassandra, and LP Kaelbling.
1995.Learning policies for partially observable environ-ments: Scaling up.
In Proc.
ICML, pages 362?370.I Meza-Ruiz, S Riedel, and O Lemon.
2008.
Accuratestatistical spoken language understanding from limiteddevelopment resources.
In Proc.
ICASSP.
(to appear).JD Williams and SJ Young.
2007.
Partially Observ-able Markov Decision Processes for Spoken Dialog Systems.Computer Speech and Language, 21(2):231?422.S Young, G Evermann, D Kershaw, G Moore, J Odell,D Ollason, D Povey, V Valtchev, and P Woodland.2002.
The HTK Book.
Cambridge Univ.
Eng.
Dept.SJ Young, J Schatzmann, K Weilhammer, and H Ye.2007.
The Hidden Information State Approach to Di-alog Management.
In Proc.
ICASSP, Honolulu.76
