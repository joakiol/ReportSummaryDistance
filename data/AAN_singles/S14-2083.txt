Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 477?481,Dublin, Ireland, August 23-24, 2014.RelAgent: Entity Detection and Normalization for Diseases inClinical Records: a Linguistically Driven ApproachSv RamananRelAgent Tech Pvt LtdAdyar, ChennaiIndiaramanan@relagent.comSenthil NathanRelAgent Tech Pvt LtdAdyar, ChennaiIndiasenthil@relagent.comAbstractWe refined the performance of Co-coa/Peaberry, a linguistically moti-vated system, on extracting disease en-tities from clinical notes in the train-ing and development sets for Task 7.Entities were identified in noun chunksby use of dictionaries, and events (?Theleft atrium is dilated?)
through our ownparser and predicate-argument struc-tures.
We also developed a mod-ule to map the extracted entities tothe SNOMED subset of UMLS.
Themodule is based on direct matchingagainst UMLS entries through regu-lar expressions derived from a smallset of morphological transformations,along with priority rules when multi-ple UMLS entries were matched.
Theperformance on training and develop-ment sets was 81.0% and 83.3% respec-tively (Task A), and the UMLS match-ing scores were respectively 75.3% and78.2% (Task B).
However, the perfor-mance against the test set was lowby comparison, 72.0% for Task A and63.9% for Task B, even while the pureUMLS mapping score was reasonablyhigh (relaxed score in Task B = 91.2%).We speculate that our moderate perfor-mance on the test set derives primarilyfrom chunking/parsing errors.1 IntroductionThe increasing use of electronic health records,both for satisfying mandatory requirements asThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers andproceedings footer are added by the organisers.
Licencedetails: http://creativecommons.org/licenses/by/4.0/well as for administrative reasons, has cre-ated a need for systems to automatically tagand normalize disease/sign/symptom men-tions.
Statistically significant correlationsextracted from automated analysis of largedatabases of clinical records are felt to be use-ful in detecting phenotype-genotype correla-tions (reviewed in Kohane (2011)), phenotype-phenotype correlations (Roque et al., 2011) aswell as in continuous monitoring of events suchas adverse reactions and even early detectionof outbreaks of epidemics/infectious diseases(Botsis et al., 2013; Collier, 2012).
In thiscontext, Task 7 of SemEval 2014, which isa continuation of the ShARe/CLEF eHealth2013 task (Pradhan et al., 2013), provides atestbed to evaluate systems that automaticallytag and normalize mentions of diseases, signsand symptoms in clinical records, which in-clude discharge summaries and echo, radiologyand ECG reports.Our system consists of (i) Cocoa, a chunk-based entity tagger and (ii) Peaberry, a parser,followed by a module for predicate-argumentstructure.
We have tested the system in a va-riety of tasks, such as detecting and normal-izing mentions of chemicals, proteins/genes,diseases and action terms in the BioCreative13 Chemdner and CTD tasks (Ramanan andSenthil Nathan, 2013a; Ramanan and SenthilNathan, 2013b), as well as in detecting cel-lular and pathological events in the BioNLPcancer genetics task (Ramanan and SenthilNathan, 2013c); we also participated in theeHealth 2013 task (Ramanan et al., 2013d).Throughout, we have retained a common coreplatform for simultaneous detection of a mul-tiplicity of entity types as well as for chunk-ing and parsing; we restrict task-specific op-timization primarily to post-processing mod-ules.
While this strategy may not be optimal477for any individual task, we feel that it is neces-sary for multi-document spanning tasks suchas literature-based discovery (Swanson, 1988),where connections are established across a va-riety of scales, e.g.
from molecular eventsto patho-physiological phenotypes.
Moreover,these linkages need to be made across a mul-tiplicity of documents from various sources,which encompass a linguistic range from com-plex syntactical utterances in biomedical pub-lications to free-form phrase-centered clinicalnotes.We refined performance against the pro-vided training and development sets, withreasonable performance in Task A (relaxedf = 0.94, strict f = 0.81 ?
0.83, strict recall0.80 ?
0.82).
A module to match text fromgold-annotated exact spans to UMLS codesalso achieved reasonable performance for TaskB (relaxed accuracy = 0.94?96).
However, theresults against from the test set were quite lowfor Task A, (relaxed f = 0.87, strict f = 0.72,strict recall = 0.70) as well as for Task B(strict f = 0.64).
Comparatively, the modulefor UMLS normalization fared better (relaxedf = 0.91 in Task B).
We speculate that thetest set contains entities that are rare in thetraining/development sets which were chun-ked incorrectly, and also that the parse errorsin the test set arose from syntactic structuresmissing in the training sets.
It is possible thata post-processing statistical module trained ona combination of gold annotations as well aslinguistic output may be needed for improv-ing the performance of our system on clinicalnotes.2 System descriptionThe basic structure of the entity-taggingsystem is unchanged from that used inShare/CLEF eHealth 13 (Pradhan et al.,2013) and BioNLP-ST 13.
In summary, thesystem comprises of a sentence splitter, fol-lowed by a TBL-based POS tagger and chun-ker, entity tagging at the single-token level,a module to handle multi-word entities, anoun phrase coordination module, a depen-dency parser (Ramanan and Senthil Nathan,2013c), and finally a semantic module to tagdisease-related events.The generic system has dictionaries andmorphological rules for detecting diseases andbody parts.
However, there are many exten-sions needed for clinical notes, which (i) makeextensive use of common words and phrasesfor describing symptoms, which requires wordsense disambiguation, (ii) use unusual phrasesfor signs and symptoms and (iii) are full ofundefined acronyms.
We isolated such special-ization to disease-related entities within nounphrases in clinical documents inside a subrou-tine in the multi-word tagger module.
Thesewere identified by a frequency-based analysisof words and phrases in the training and de-velopment corpora.
Thus, a few ambiguouswords and phrases such as ?crackles?, ?com-plaints?, ?mass effect?
and ?focal consolidation?were tagged as disease markers regardless ofcontext.
Generally, however, even commonclinical words such as ?redness?
and ?swelling?were tagged only in the presence of neighbor-ing context words.
The appearance of majorbody parts such as ?Abdomen?, ?Neck, ?Ex-tremities?
at the beginning of a line followedby a colon or a hyphen was taken as a dis-course reference marker for the rest of the lineto tag acronyms such as ?NT/ND?
and dan-gling adjectives such as ?soft?
and ?warm?.
Verycommon acronyms (?
100) both for anatomi-cal parts (?LUQ?)
and diseases (?DMII?)
werealso tagged inside the specialized subroutine,as were common abbreviations (?regurg?
for re-gurgitation) and words with common spellingerrors.
Finally, some event/process wordswhich we found to almost always representclinical conditions in the training text weretagged as disease markers.
Examples are ?as-piration?, agitation?
and ?confusion?.We also extended our generic event pro-cessing module with a task-specific routineto take into account descriptions of (mostly)signs/symptoms specific to clinical documents.These fall into several categories: (i) abnor-mal changes in body parts or organ systems,such as ?The left atrium was moderately en-larged?, ?Nose is bloody?
and ?redistribution ofpulmonary blood flow?
(ii) symptoms such as?The patient was unable to walk?, ?His speechwas slurred?, ?He had difficulty breathing?
and?alteration of consciousness?
(iii) changes in pa-rameters marked by phrases/clauses such as?elevation of troponin?, ?QR interval was pro-478longed?
and ?decreased blood sugar?.
Certainenvironmental conditions such as ?exposure toasbestos?
were also handled.
Finally, eventswith a default animate theme were taggedregardless of their actual arguments to han-dle sentences/phrases where our syntax mod-ule failed to extract the correct theme or thetheme is to be inferred from the discourse; the?
40 words in this set included verbs suchas ?vomit?, ?shivering?, ?lethargic?, ?violent?
and?somnolent?.The above treatment served to demar-cate spans for diseases that overlap withthe gold annotations.
The system mergeswords/phrases denoting a body part with ad-joining words that denote diseases, and alsomerges words denoting severity into the dis-ease span, since our system design strategywas to generate the longest contiguous spanthat can refer to a disease.
However, the pri-mary score in the shared task are with re-spect to exact matches with the gold anno-tations.
We therefore wrote a small post-processing module to omit words in an approx-imate match that refer to severity (?acute?)
aswell as to excise phrases dealing with intra-organ parts or their location (such as ?lobes?or ?left/right?)
- such words/phrases are usu-ally omitted from the UMLS descriptions ofdiseases to which the gold annotations hewclosely.
Also, we noticed that certain wordssuch as ?wounds?
and ?lesions?
do not embedan anatomical entity within their descriptionin the gold annotations.
Yet another point isthat, while parameters are marked up as in-dicative of a symptom only when they take onabnormal values (?elevated LDL?
), the direc-tion of change is almost always omitted fromthe gold annotations.
Descriptors of the pa-tient (?He?)
are also excised.
Altogether, weconstructed about 40 rules to trim the approx-imate span into one more conformant to theexact form in the gold annotations.Task B requires mapping diseases phrasesinto the SNOMED subset of UMLS as spec-ified in the task description.
We proceededon the assumption that the exact (gold)entity spans were constructed by annota-tors to closely map into the UMLS descrip-tions.
Accordingly, we used the text asdefined by the gold spans and attemptedto map them directly into the UMLS def-initions after some preprocessing steps thatconstructed a regular expression: (a) com-mon spelling errors were corrected (b) bodypart and disease acronyms were expanded(c) common variants were added as alter-nates i.e.
?tumou?rs??
were expanded into?(tumou?r|neoplasm|carcinoma)s??
(d) adjec-tival and nominal variants were added e.g.both ?atrium?
and ?atrial?
were converted into?
(atri)(al?|um)?, and more generally, adjec-tival endings were generalized, for example,the ending ?ic?
was converted into ?(i[ac]|ism)?.
(e) singular and plural forms were convertedinto choices e.g.
?artery?
was rendered as?arter(y|ies)?.Altogether, we have ?
120 rules for vari-ant morphological forms, covering adjectives,nouns and number.
The resulting regular ex-pression was directly matched (using ?grep?
)against UMLS text entries.
Generally, sev-eral matches were found.
Matches against thedefining entry (the first one) were prioritized,otherwise the entry with the largest CUID wastaken.
Finally, we noted that some UMLSCUID?s were preferred to others; for exam-ple, ?C0007115 - Malignant neoplasm of thy-roid?
is preferred to ?C0549473 - Thyroid car-cinoma?.
The preferred choices were inferredfrom gold annotation frequencies, and corre-spond to ?
100 remapping rules.3 Results and DiscussionWith a few minor changes to the system usedin the Share/CLEF 2013, we obtained a re-laxed f-measure in Task A of 0.88 in thetraining and development sets.
Thereafterwe alternately refined performance in Task Aagainst the provided training set using thedevelopment set as a testbed, or vice versa.As described in the last section, these re-finements took the form of adding context-sensitive rules for disease-related words andphrases in order of their frequencies in thetraining/development sets.
While we couldthereby improve performance against bothtraining and development sets (relaxed f =0.94), we noticed that improvements in theperformance against the training set did notcorrelate with better performance against thedevelopment set and vice versa, probably im-479plying that 6% or more of the entities areunique to each set, or that we were unable tocatch similarities.
A similar orthogonal situa-tion resulted in our attempt to improve perfor-mance against exact matches on the trainingand development sets, strict f = 0.81 ?
0.83,strict recall 0.80 ?
0.82.
The observation oforthogonal entity sets in different datasets forabout 6% of entities is seemingly validated inthe test set, where the results showed a re-laxed f = 0.87, which is quite close to thebaseline performance (0.88 in the Share/CLEF2013 task); the highest scoring system had re-laxed f = 0.91 by comparison.
We speculatethat our insistence on contextual clues for en-tity tagging is another cause for low relaxedperformance on the test set.Performance of the system for exactmatches on the test set (strict f = 0.72)suffered greatly in comparison to the train-ing/development sets.
This could be partlyascribed to the 7% lower performance on therelaxed f-score (i.e.
we missed many entitiesaltogether) from 0.94 in training/developmentsets to 0.87 in the test set.
Even account-ing for this, there is an additional perfor-mance drop of about 3?4% in exact match onthe test set compared to training/developmentsets.
One implication is that that our rule-base method for pruning approximate matchesto exact spans is probably sub-optimal, andshould be supplemented or replaced by a sta-tistical algorithm.
As noted earlier, gold anno-tations are probably made by annotators withrespect to UMLS definitions, and have somedegree of arbitrariness associated with themdepending on the granularity of the UMLS def-inition e.g.
in the choice of whether to removeor retain a body location in the gold span.Given the size of the UMLS definition set, astatistical approach is probably likely to dobetter than a rule-based system in the task ofreducing approximate matches to exact spans.The poor performance in Task A (strictrecall = 0.70) directly impinges on our low?strict?
score in Task B (= 0.64); this score issimply a product of the strict recall in Task Aand the accuracy of mapping to UMLS, wherethe latter score is given by the Task B ?relaxed?score (= 0.91).
An interesting feature is themapping accuracy for our system on the testset suffered a relatively small drop when com-pared to the mapping accuracies on the train-ing and development sets, which were 0.94 and0.96 respectively.
We interpret this reasonablyhigh figure for the mapping score (the bestamong the top 10+ teams in Task B) as vali-dation of our hypothesis that gold annotationsare made with respect to UMLS definitions,which also strengthens the case (made above)for the need to incorporate a (semi-)statisticalapproach for pruning overlap matches to exactmatches in our system.Clinical documents are terse and full ofphrasal observations and incomplete sen-tences, often with missing punctuation.
Wehave adapted a linguistically based systemto detect disease-related entities and eventswith moderate performance; our observationon the training/development sets is that mosterrors arise from parsing/ chunking errorson grammatically incomplete phrases.
Thesecond task, namely mapping disease-relatedentities/events to SNOMED/UMLS, requirestagged entity spans to correspond closely toUMLS definitions; system performance in thisregard can probably be usefully supplementedby statistical approaches.
Given proper entityspans, a small set of morphological transfor-mations gives high performance in mappingto UMLS ID?s.
We speculate that a chunk-annotated corpus of clinical records may helpin improving performance for linguistically de-rived systems.ReferencesIsaac S. Kohane.
2011.
Using electronic healthrecords to drive discovery in disease genomics.Nat Rev Genet.
2011 Jun;12(6):417-28.Francisco S. Roque, Peter B. Jensen, Henri-ette Schmock, Marlene Dalgaard, Massimo An-dreatta, Thomas Hansen, Karen Soeby, SorenBredkjor, Anders Juul, Thomas Werge, Lars J.Jensen and Soren Brunak.
2011.
Using elec-tronic patient records to discover disease corre-lations and stratify patient cohorts.
PLoS Comp.Bio.
7(8):e1002141.Sv Ramanan and Senthil Nathan.
2013.
Perfor-mance of a multi-class biomedical tagger on theBioCreative IV CTD task.
Proceedings of theFourth BioCreative Challenge Evaluation Work-shop vol.
1.
Bethesda, MD.480Sv Ramanan and Senthil Nathan.
2013.
Adapt-ing Cocoa a multi-class entity detector for theCHEMDNER task of BioCreative IV.
Proceed-ings of the Fourth BioCreative Challenge Eval-uation Workshop vol.
2.
Bethesda, MD.Sv Ramanan and Senthil Nathan.
2013.
Perfor-mance and limitations of the linguistically moti-vated Cocoa/Peaberry system in a broad biomed-ical domain.
Proceedings of Workshop.
BioNLPShared Task 2013.
ACL.
Sofia.Sv Ramanan, Shereen Broido and Senthil Nathan.2013.
Performance of a multi-class biomedi-cal tagger on clinical records.
Proceedings ofShARe/CLEF eHealth Evaluation Labs.Don R. Swanson.
1988.
Migraine and Magnesium:Eleven Neglected Connections.
Persp.
Bio.
Med.31(4), 526-557.Sameer Pradhan, Noemie Elhadad, Brett R.South, David Martinez, Lee Christensen, AmyVogel, Hanna Suominen, Wendy W. Chapmanand Guergana Savova.
2013.
Online Work-ing Notes of the CLEF 2013 Evaluation Labsand Workshop.
Proceedings of ShARe/CLEFeHealth Evaluation Labs, 23-26 September, Va-lencia, SpainTaxiarchis Botsis , Michael D. Nguyen , EmilyJ.
Woo, Marianthi Markatou and Robert Ball.2011.
Text mining for the Vaccine AdverseEvent Reporting System: medical text classifica-tion using informative feature selection.
J AmMed Inform Assoc.
2011 Sep-Oct;18(5):631-8Nigel Collier.
2012.
Uncovering text mining: Asurvey of current work on web-based epidemicintelligence.
Glob Public Health.
Aug 2012;7(7): 731-749.481
