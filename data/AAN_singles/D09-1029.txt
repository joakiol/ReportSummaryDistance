Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 276?285,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPWikipedia as Frame Information RepositorySara TonelliFBK-irstI-38100, Trento, Italysatonelli@fbk.euClaudio GiulianoFBK-irstI-38100, Trento, Italygiuliano@fbk.euAbstractIn this paper, we address the issue of au-tomatic extending lexical resources by ex-ploiting existing knowledge repositories.In particular, we deal with the new taskof linking FrameNet and Wikipedia us-ing a word sense disambiguation systemthat, for a given pair frame ?
lexical unit(F, l), finds the Wikipage that best ex-presses the the meaning of l. The mappingcan be exploited to straightforwardly ac-quire new example sentences and new lex-ical units, both for English and for all lan-guages available in Wikipedia.
In this way,it is possible to easily acquire good-qualitydata as a starting point for the creation ofFrameNet in new languages.
The evalua-tion reported both for the monolingual andthe multilingual expansion of FrameNetshows that the approach is promising.1 IntroductionMany applications in the context of natural lan-guage processing or information retrieval haveproved to convey significant improvement by ex-ploiting lexical databases with high-quality anno-tation such as FrameNet (Fillmore et al, 2003)and WordNet (Fellbaum, 1998).
Nevertheless, thepractical use of similar resources is often biasedby their limited coverage because manual anno-tation is time-consuming and requires a relevantfinancial effort.
For this reason, some research ac-tivities have focused on the automatic enrichmentof such resources with annotated information in(near) manual quality.
The main strategy proposedwas the mapping between resources in order toreciprocally enrich different lexical databases bylinking their information layers.
This has provedto be useful in several tasks, from verb classifica-tion (Chow and Webster, 2007) to semantic rolelabeling (Giuglea and Moschitti, 2006), open textsemantic parsing (Shi and Mihalcea, 2004) andtextual entailment (Burchardt and Frank, 2006).In this work, we focus on the automatic enrich-ment of the FrameNet database for English and wepropose a new framework to extend this procedureto new languages.
While similar works in the pasthave mainly proposed to automatically extend theFrameNet database by mapping frames and Word-Net synsets (Shi and Mihalcea (2005), Johans-son and Nugues (2007), and Tonelli and Pighin(2009)), we present an explorative approach thatfor the first time exploits Wikipedia to this pur-pose.
In particular, given a lexical unit l belong-ing to a frame F , we devise a strategy to linkl to the Wikipedia article that best captures thesense of l in F .
This is basically a word disam-biguation (WSD) problem (Erk, 2004) and to thispurpose we employ a state-of-the-art WSD sys-tem (Gliozzo et al, 2005).
The mapping between(F, l) pairs and Wikipedia pages could then be ex-ploited for three further subtasks: (a) automati-cally extract from Wikipedia all sentences point-ing to the Wikipage mapped with (F, l) and assignthem to F ; (b) automatically expand the lexicalunits sets in the English FrameNet by exploitingthe redirecting and linking strategy of Wikipedia;and (c) since Wikipedia is available in 260 lan-guages, use the English Wikipedia article linked to(F, l) as a bridge to carry out sentence and lexicalunit retrieval in other languages.
The set of auto-matically collected data would represent the start-ing point for the creation of FrameNet in new lan-guages.
In fact, having a repository of sentencesextracted from Wikipedia which have already beendivided by sense would significantly speed up theannotation process.
In this way, the annotatorswould not need to extract all sentences in a cor-pus containing l and classify them by sense.
In-stead, they should simply validate the given sen-tences and assign the correct frame elements.276In the following, we start by providing a briefoverview of FrameNet and Wikipedia and wepresent their structure and organization.
Next, wedescribe the algorithm for mapping lexical unitsand Wikipages and the word sense disambigua-tion algorithm employed by the system.
In Sec-tion 5 we describe the dataset used in the first ex-periment and report evaluation results of the map-ping between (F, l) pairs and Wikipedia senses.
InSection 6 we describe an application of the map-ping, i.e.
the automatic enrichment of EnglishFrameNet.
We describe the data extraction pro-cess and evaluate the quality of the data.
In Section7 we describe and evaluate another application ofthe mapping, i.e.
the acquisition of data for theautomatic creation of Italian FrameNet using theItalian Wikipedia.
Finally, we draw conclusionsand present future research directions.2 FrameNet and WikipediaFrameNet (Fillmore et al, 2003) is a lexical re-source for English based on corpus evidence,whose conceptual model comprises a set of proto-typical situations called frames, the frame-evokingwords or expressions called lexical units (LUs)and the roles or participants involved in these situ-ations, called frame elements.
All lexical units be-longing to the same frame have similar semanticsbut, differently from WordNet synsets, they canbelong to different categories and present differ-ent parts of speech.
For example, the KILLINGframe is described in the FrameNet database1as ?A Killer or Cause causes the death of theVictim?.
The elements in capitals are the se-mantic roles (frame elements) typically involvedin the KILLING situation.
The frame definitioncomes also with the list of frame-evoking lexicalunits, namely annihilate.v, annihilation.n, butch-ery.n, carnage.n, crucify.v, deadly.a, etc.
SinceFrameNet is a corpus-based resource, every lexi-cal unit should be instantiated by a set of exam-ple sentences, where the frame elements are anno-tated as well.
Instead, FrameNet is still an ongoingproject and in the latest release (v. 1.3) there areabout 3,380 lexical units out of 10,195 that comewith no example sentences.
In this work we focuson these lexical units and propose how to automat-ically collect the missing sentences.
Anyhow, thealgorithm we propose is suitable also for expand-ing sentence sets already present in FrameNet.1http://framenet.icsi.berkeley.eduWikipedia2is one of the largest online reposito-ries of encyclopedic knowledge, with millions ofarticles available for a large number of languages(>2,800,000 for English).
The article (or page)is the basic entry in Wikipedia.
Every article hasan unique reference, i.e., one or more words thatidentify the page and are present in its URL.
Forexample, Ball (dance) identifies the page that de-scribes several types of ball intended as formaldance, while Dance (musical form) describes thedance as musical genre.
Every Wikipedia articleis linked to others, and in the body of every pagethere are plenty of links to connect the most rel-evant terms to other pages.
Another importantattribute is the presence of about 3,000,000 redi-rection pages, that given an identifier that is notpresent in Wikipedia, automatically display thepage with the most semantically similar identi-fier (for example Killing is redirected to the Mur-der page).
Wikipedia contains also more than100,000 disambiguation pages listing all senses(pages) for an ambiguous entity.
For example,Book has 9 senses, which correspond to 9 dif-ferent articles.
Wikipedia structure and qualitymake this resource particularly suitable for infor-mation extraction and word sense disambiguationtasks (Csomai and Mihalcea (2008) and Milne andWitten (2008)).
In fact, page references can beseen as senses and Wikipedia as a large sense in-ventory.
From this point of view, also linkinga lexical unit to the correct Wikipedia page is aword sense disambiguation issue because it im-plies recognizing what meaning the lexical unithas in the given frame.
For example, dance.nin the SOCIAL EVENT frame should be linked toBall (dance) and not to Dance (musical form).3 The Mapping AlgorithmIn this section, we describe how to map a frame?
lexical unit pair (F, l) into the Wikipedia arti-cle that best captures the sense of l as defined inF .
The mapping problem is casted as a supervisedWSD problem, in which l must be disambiguatedusing F to provide the context and Wikipedia toprovide the sense inventory and the training data.Even if the idea of using Wikipedia links for dis-ambiguation is not novel (Cucerzan, 2007), it isapplied for the first time to FrameNet lexical units,considering a frame as a sense definition.
The pro-posed algorithm is summarized as follows:2http://en.wikipedia.org277Step 1 For each lexical unit l, we collect fromthe English Wikipedia dump3all contexts4where lis the anchor of an internal link (wiki link).
The setof targets represents the senses of l in Wikipediaand the contexts are used as labelled training ex-amples.
For example, the lexical unit building.n inthe frame Buildings is an anchor in 708 differentcontexts that point to 42 different Wikipedia pages(senses).Step 2 The set of contexts with their correspond-ing senses is then used to train the WSD systemdescribed in Section 4.
For example, the context?The building, which date from the mid-to-late19th century, were built in a variety of High Victo-rian architectural styles.?
is a training example forthe sense defined by the Wikipedia page Building.Step 3 Finally, the disambiguation modellearned in the previous step is used to map a pair(F, l) to a Wikipedia article.
(F, l) is representedas a fictitious-context derived by aggregating theframe definition and all lexical units associated toF .
We used the term ?fictitious-context?
to re-mark the slight difference in structure comparedwith the training contexts (i.e., the Wikipediaparagraphs).
For example, ?.
.
.
structures form-ing an enclosure and providing protection fromthe elements .
.
.
acropolis arena auditorium barbuilding .
.
.
?
is the fictitious-context built forthe pair (Buildings, building.n).
The sense, i.e.,the Wikipedia article, assigned to the fictitious-context by the disambiguation algorithm uniquelydefines the mapping.
The previous example is as-signed to the Wikipedia page Building.4 The WSD AlgorithmGliozzo et al (2005) proposed an elegant approachto WSD based on kernel methods.
The algorithmproved effective at Senseval-3 (Mihalcea and Ed-monds, 2004) and, nowadays, it still representsthe state-of-the-art in WSD (Pradhan et al, 2007).Specifically, they addressed these issues: (i) inde-pendently modeling domain and syntagmatic as-pects of sense distinction to improve feature rep-resentativeness; and (ii) exploiting external knowl-edge acquired from unlabeled data, with the pur-pose of drastically reducing the amount of labeled3http://download.wikimedia.org/enwiki/200903064A context corresponds to a line of text in the Wikipediadump and it is represented as a paragraph in a Wikipedia ar-ticle.training data.
The first direction is based on thelinguistic assumption that syntagmatic and domain(associative) relations are crucial for representingsense distictions, but they are originated by differ-ent phenomena.
Regarding the second direction, itis possible to obtain a more accurate prediction bytaking into account unlabeled data relevant for thelearning problem (Chapelle et al, 2006).On the other hand, kernel methods are theoret-ically well founded in statistical learning theoryand shown good empirical results in many appli-cations (Shawe-Taylor and Cristianini, 2004).
Thestrategy adopted by kernel methods consists ofsplitting the learning problem into two parts.
Theyfirst embed the input data in a suitable featurespace, and then use a linear algorithm (e.g., sup-port vector machines) to discover nonlinear pat-terns in the input space.
The kernel function isthe only task-specific component of the learningalgorithm.
Thus, to develop a WSD system, oneonly needs to define appropriate kernel functionsto represent the domain and syntagmatic aspectsof sense distinction and to exploit the propertiesof kernel functions in order to define a compositekernel that combines and extends individual ker-nels.The WSD system described in the followingconsists of a composite kernel (Section 4.3) thatcombines the domain and syntagmatic kernels.The former (Section 4.1) models the domain as-pects of sense distinction, the latter (Section 4.2)represents the syntagmatic aspects of sense dis-tinction.4.1 Domain KernelIt is been shown that domain information is fun-damental for WSD (Magnini et al, 2002).
For in-stance, the (domain) polysemy between the com-puter science and the medicine senses of the word?virus?
can be solved by considering the domainof the context in which it appears.In the context of kernel methods, domain infor-mation can be exploited by defining a kernel func-tion that estimates the domain similarity betweenthe contexts of the word to be disambiguated.
Thesimplest method to estimate the domain similaritybetween two texts is to compute the cosine simi-larity of their vector representations in the vectorspace model (VSM).
The VSM is a k-dimensionalspace Rk, in which the text tjis represented bya vector~tj, where the ithcomponent is the term278frequency of the term wiin tj.
However, such anapproach does not deal well with lexical variabil-ity and ambiguity.
For instance, despite the factthat the sentences ?he is affected by AIDS?
and?HIV is a virus?
express concepts closely related,their similarity is zero in the VSM because theyhave no words in common (they are representedby orthogonal vectors).
On the other hand, dueto the ambiguity of the word ?virus?
, the simi-larity between the sentences ?the laptop has beeninfected by a virus?
and ?HIV is a virus?
is greaterthan zero, even though they convey very differentmessages.To overcome this problem, Gliozzo et al (2005)introduced the domain model (DM) and show howto define a domain VSM in which texts and termsare represented in a uniform way.
A DM is com-posed of soft clusters of terms.
Each cluster rep-resents a semantic domain, that is, a set of termsthat often co-occur in texts having similar topics.A DM is represented by a k ?
k?rectangular ma-trix D, containing the degree of association amongterms and domains.The matrix D is used to define a function D :Rk?
Rk?, that maps the vector~tjrepresented inthe standard VSM, into the vector~t?jin the domainVSM.
D is defined byD(~tj) =~tj(IIDFD) =~t?j, (1)where~tjis represented as a row vector, IIDFis ak?k diagonal matrix such that iIDFi,i= IDF (wi),and IDF (wi) is the inverse document frequencyof wi.In the domain space, the similarity is esti-mated by taking into account second order rela-tions among terms.
For example, the similarity ofthe two sentences ?He is affected by AIDS?
and?HIV is a virus?
is very high, because the termsAIDS, HIV and virus are strongly associated withthe domain medicine.Singular valued decomposition (SVD) is used toacquire in a unsupervised way the DM from a cor-pus represented by its term-by-document matrixT.
SVD decomposes the term-by-document ma-trix T into three matrixes T ' V?k?UT, whereV and U are orthogonal matrices (i.e., VTV = Iand UTU = I) whose columns are the eigenvec-tors of TTTand TTT respectively, and ?k?isthe diagonal k ?
k matrix containing the highestk?k eigenvalues of T, and all the remainingelements set to 0.
The parameter k?is the dimen-sionality of the domain VSM and can be fixed inadvance.
Under this setting, the domain matrix Dis defined byD = INVp?k?
(2)where INis a diagonal matrix such that iNi,i=1q?~w?i,~w?i?,~w?iis the ithrow of the matrix V?
?k?.The domain kernel is explicitly defined byKD(ti, tj) = ?D(ti),D(tj)?, (3)where D is the domain mapping defined in Equa-tion 1.
Finally, the domain kernel is further ex-tended to include the standard bag-of-word kernel.4.2 Syntagmatic KernelKernel functions are not restricted to operate onvectorial objects ~x ?
Rk.
In principle, kernelscan be defined for any kind of object representa-tion, such as strings and trees.
As syntagmatic re-lations hold among words collocated in a partic-ular temporal order, they can be modeled by ana-lyzing sequences of words.
Therefore, the stringkernel (Shawe-Taylor and Cristianini, 2004) is avalid tool to represent such relations.
It countshow many times a (non-contiguous) subsequenceof symbols u of length n occurs in the input strings, and penalizes non-contiguous occurrences ac-cording to the number of gaps they contain.
For-mally, let V be the vocabulary, the feature spaceassociated with the string kernel of length n is in-dexed by a set I of subsequences over V of lengthn.
The (explicit) mapping function is defined by?nu(s) =Xi:u=s(i)?l(i), u ?
Vn, (4)where u = s(i) is a subsequence of s in the posi-tions given by the tuple i, l(i) is the length spannedby u, and ?
?
]0, 1] is the decay factor used to pe-nalize non-contiguous subsequences.The associated string kernel is defined byKn(si, sj) = ?
?n(si), ?n(sj)?
=Xu?Vn?n(si)?n(sj)(5)Gliozzo et al (2005) modified the generic def-inition of the string kernel in order to take intoaccount (sparse) collocations.
Specifically, theydefined syntagmatic kernels as a combination ofstring kernels applied to sequences of words in afixed-size window centered on the word to be dis-ambiguated.
This formulation allows estimatingthe number of common (sparse) subsequences of279words (i.e., collocations) between two examples,in order to capture syntagmatic similarity.
Thesyntagmatic kernel is defined byKS(si, sj) =pXn=1Kn(si, sj), (6)where Knis the string kernel defined in Equation5 and the parameter n represents the length of thesubsequences analyzed when estimating the sim-ilarity between contexts.
Notice that the syntag-matic kernel is only effective for those fictitiouscontexts in which the lexical units do occur inmeaningful sentences, however this is not guaran-teed for the lexical units without examples.4.3 Composite KernelFinally, to combine domain and syntagmatic infor-mation, the composite kernel is defined byKWSD(ti, tj) =?KD(ti, tj) +?KS(ti, tj), (7)where?KDand?KSare normalized kernels definedin Equation 3 and 6, respectively.5It follows di-rectly from the explicit construction of the featurespace and from closure properties of kernels thatit is a valid kernel.5 Mapping taskIn this section we report the first experiment,namely the mapping between (F, l) pairs and aWikipedia pages.
We describe the experimentalsetup and then present the corresponding evalua-tion.5.1 Experimental setupWe applied our algorithm to all lexical units thatdo not have any example sentence in the FrameNetdatabase.
In principle, the proposed approach canbe applied to every lexical unit, and we expect thealgorithm performance to improve if some exam-ple sentences are already available because theycould be added to the fictitious-context used torepresent (F, l) in the system.
Nevertheless, in thisexplorative study we wanted to focus on the hardercases, even if results are likely to be worse than onthe whole FrameNet database.In FrameNet, 3,305 (F, l) pairs have no exam-ple sentences (536 pairs with adjectival LU, 1313verbal LU, 1456 nominal LU).
Since Wikipedia isbasically a resource organized by concepts, which5?K(xi, xj) =K(xi,xj)?K(xj,xj)K(xi,xi)are generally expressed by nouns, we decided torestrict our experiment to nominal lexical units.Besides, many verbal and adjectival concepts inWikipedia are redirected to nominal identifiers.So, we randomly selected 900 pairs with nominallexical units.
For the moment, we decided to dis-card lexical units expressed by multiwords (about150), which will be taken into account in a futureversion of our system.
The average ambiguity ofthe 900 LUs considered is 1.24 in FrameNet.
In-stead, every LU corresponds to about 35 candidatesenses in Wikipedia.In order to perform WSD, we built the domainmodel from the 200,000 most visited Wikipediaarticles.
After removing terms that occur less than5 times, the resulting dictionaries contain about300,000 terms.
We used the SVDLIBC pack-age6to compute the SVD, truncated to 100 di-mensions.
The experiments were performed usingthe SVM package LIBSVM (Chang and Lin, 2001)customized to embed the kernels described in Sec-tion 4.5.2 EvaluationIn this first evaluation step, we focus on the qualityof the mapping between (F, l) pairs and Wikipediaarticles.
In order to evaluate the system output,we created a gold standard where 250 (F, l) pairsrandomly extracted from the nominal subset de-scribed above have been manually linked to theWikipedia page (if available) that best correspondsto the meaning of l in F .
The pairs have been cho-sen in order to maximize the frame variability, i.e.every pair corresponds to a different frame.
Sinceour gold standard contains 34% of all frames inthe FrameNet database, we believe that, despite itslimited size, it is well representative of FrameNetcharacteristics.
Evaluation was carried out com-paring the system output against the gold stan-dard.
Results are reported in Table 1.
The base-line was computed considering the most frequentsense of every lexical unit in Wikipedia.
This ele-ment is obtained by taking into account all occur-rences in Wikipedia where the lexical unit LU weconsider is anchored to a given page.
The mostfrequent sense for LU is the page to which LU ismost frequently linked in Wikipedia.
Since about14% of the lexical units in the gold standard arenot present in Wikipedia, we also estimated an up-per bound accuracy of 0.86.
This confirms our in-6http://tedlab.mit.edu/?dr/svdlibc/280tuition that FrameNet and Wikipedia are linkableresources to a large extent and that our task is well-founded.AccuracyBaseline 0.66System output 0.71Upper bound 0.86Table 1: Accuracy evaluation.Wrong assignments include also problematiccases that are not directly connected to proper sys-tem errors.
One of the most relevant issues is thedifferent granularity between FrameNet framesand Wikipages.
For example, the NETWORKframe is defined as ?a set of entities of the sameor similar types (Nodes) are linked to each otherby Connections to form a Network allowing forthe flow of information, resources, etc.?.
Evenif the listed lexical units (network.n and web.n)and some examples refer to the informatics do-main, the situation described in the FrameNetdatabase is more general.
Wikipedia instead listsseveral pages that may be seen as subdomainsof NETWORK such as Computer network, So-cial network, Telecommunications network, etc.In the future, it may be worth modifying the sys-tem in order to allow multiple assignments ofWikipages for every frame.In other cases, frame definitions seem not tobe very consistent and it is very difficult to dis-criminate between two frames even for a humanannotator.
For example, ESTIMATED VALUE andESTIMATING include both estimation.n as lexicalunit, but since their frame definitions are almostthe same and the other lexical units in the sameframe are not discriminative, the system links both(F, l) pairs to the same Wikipedia article.6 English FrameNet expansionIn the following part of the experiment, we wantto investigate to what extent the FrameNet ?Wikipedia mapping can be effectively applied toautomatically expand the FrameNet database withnew example sentences, and eventually to acquirenew lexical units.
For every (F, l) pair, we con-sider the linked Wikipedia sense s and extract allsentences Csin Wikipedia with a reference tos.
In this way, we can assume that, if s waslinked to (F, l), Cscan be included in the exam-ple sentences of F .
This repository of sentencesis already divided by sense and can significantlyspeed-up manual annotation.
On the other hand,the extracted sentences could enrich the trainingset of machine learning systems for frame annota-tion to improve the frame identification step.
Infact, this task has raised growing interest in theNLP community, with a devoted challenge at thelast SemEval campaign (Baker et al, 2007).This retrieval process allows also to ex-tract from Csall words Wsthat have anembedded reference to s in the form <ahref=?/wiki/Wiki Sense?...>word</a>.
In thisway, Wsare automatically included in F as newlexical units.
In this phase, redirecting links arevery useful because they automatically connect aword or expression to its nearest sense in casethere is no specific page for this word.
The infor-mation about redirecting allows also to account fororthographic variations of the same lexical unit,for example collectible is redirected to collectable.We explain the data extraction process inthe light of an example from our dataset.Our WSD system assigned to the (F, l) pair(WORD RELATIONS ?
homonym.n) the Wikipagehttp://en.wikipedia.org/wiki/Homonym.So, we extracted from the English Wikipediadump all sentences where the anchor <ahref=?/wiki/Homonym?...
> appears and as-sumed that the word or multiword expression thatis linked to the Homonym site may be a good can-didate as lexical unit for the WORD RELATIONSframe.
In this case, the example sentences were186.
Apart from homonym, the candidate lexicalunits are homograph, homophone, homophonous,homonymic, heteronym, same.
Among them, onlythe latter is not appropriate, even if the sentencewhere it occurs is semantically connected to theWORD RELATIONS frame: ?In Hebrew the word?thus?
has the same triconsonantal root?.
Instead,homonymic and heteronym can be acquired asnew lexical units for WORD RELATIONS, andhomograph, for which no example sentencesare provided in FrameNet, can be automaticallyinstantiated by a set of examples.6.1 Experimental setupWe considered 893 frame ?
lexical unit pairs as-signed to Wikipedia pages following the algorithmdescribed in Section 3.
We discarded 7 pairs forwhich the system reported an assignment failure,i.e.
the best sense delivered is the disambigua-281tion page.
Then we extracted a set of sentencesfor every (F, l) pair as described in the previousparagraph.
Statistics about the retrieved data is re-ported in Table 2.English Wikipedia(F, l) pairs 893N.
of extracted sents 964,268Avg.
sents per (F, l) 1,080Table 2: Extracted data from English Wikipedia6.2 EvaluationThe dimension of the extracted corpus does notallow to carry out a comprehensive evaluation.For this reason, we manually evaluated 1,000 sen-tences, i.e.
we considered 20 (F, l) pairs, and foreach of them we evaluated 50 sentences extractedfrom our large repository.
Both (F, l) pairs andthe assigned sentences were randomly selected.In particular, the 20 (F, l) pairs do not containonly correctly assigned pairs, in fact three of themare wrong.
Anyhow, the 20 pairs seem to bea representative subset of the 893 pairs consid-ered in our experiment because they include bothmonosemic lexical units (gynaecology.n in MED-ICAL SPECIALTIES) and more ambiguous ones(club.n in the WEAPON frame).Our evaluation shows that 78% of the sentenceswere correctly linked to (F, l) pairs.
This value ishigher that the mapping accuracy between (F, l)and Wikipages reported in Section 5.2.
In fact,we noticed that even if the Wikipage assigned to(F, l) is not the article that best corresponds to themeaning of l in F , some sentences pointing to itmay be appropriate to express l.As we already mentioned in Section 5.2,the different granularity of the information en-coded by frames and Wikipages impacts onthe output quality.
For example, conversion.nin CAUSE CHANGE has a causative meaning,while it implies a personal process in UN-DERGO CHANGE.
The mapping, instead, links(CAUSE CHANGE ?
conversion.n) to the Reli-gious conversion page, and all the sentences col-lected point to religious conversion, regardless oftheir causative form or not.
Another characteristicof this approach is that we can acquire new lexi-cal units regardless of their part-of-speech, even ifwe start from nominal lexical units.
This provesthat we do not need to apply the initial mapping toverbal or adjectival LUs to obtain new data for allparts of speech.
For example, we linked (MEDI-CAL SPECIALTIES ?
gynaecology.n) to the Gynae-cology Wikipage.
Consequently, we could includethe adjective gynaecologic, pointing to the Gy-naecology page, into the MEDICAL SPECIALTIESframe for sentences like ?Fellowship training in agynaecologic subspeciality can range from one tofour years?.
However, this advantage can also turninto a weakness, because gynaecologist is alsoredirected to the Gynaecology page, but it belongsto MEDICAL PROFESSIONALS and should not beincluded into MEDICAL SPECIALTIES.For the 20 (F, l) pairs considered in the givensentences, it was possible also to retrieve 8 lex-ical units that are not present in FrameNet, forexample billy-club for the WEAPON frame.
Ex-ploiting redirections and anchoring strategies, ourinduction method can account for orthographicalvariations, for example it acquires both memorizeand memorise.
On the other hand, also misspelledwords may be collected, for instance gynaecolo-gial instead of gynaecological.7 Multilingual FrameNet expansionOne of the great advantages of Wikipedia is itsavailability in several languages.
The English ver-sion is by far the most extended, but a considerablerepository of pages is available also for other lan-guages, esp.
European ones.
In general, articles onthe same object in different languages are editedindependently and do not have to be translationsof one another, but are linked to each other by theirauthors.
In this way, the multilingual versions ofWikipedia can be easily exploited to build compa-rable corpora, with connected Wikipages in differ-ent languages dealing with the same contents.In this research step, we focus on this aspect ofWikipedia and propose a methodology that, usingthe English Wikipages as a bridge, automaticallyacquires new lexical units and example sentencesalso for other languages.
This would represent thestarting point towards the creation of FrameNetfor new languages.
Indeed, FrameNet structurecomprises a language-independent level of infor-mation, namely frame and frame element defini-tions, and a language-dependent one, i.e.
the lex-ical units and the example sentences.
This makesthe resource particularly suitable to corpus-based(semi) automatic creation of FrameNet for newlanguages, because the descriptive part can be pre-282served and the language-dependent layer can bepopulated with new instances in other languages(Crespo and Buitelaar, 2008).We apply our extraction algorithm to the ItalianWikipedia.
Since several approaches have beenexperimented to (semi) automatically build ItalianFrameNet using WordNet (De Cao et al (2008)and Tonelli and Pighin (2009)), we believe thatour new proposal to exploit Wikipedia may be ofinterest in the research community.
Anyhow, theapproach can be exploited in principle for everylanguage available in Wikipedia.7.1 Experimental setupSimilarly to the data extraction process describedin Section 6, we consider for every (F, l) pair inEnglish the linked Wikipedia sense s, in Englishas well.
Then, we retrieve the Italian Wikipediasense silinked to s and extract all sentences Ciin the Italian Wikipedia dump7with a reference tosi.
In this way, we can assume that Ciare exam-ple sentences of F and that the words or expres-sions Wiin Cicontaining an embedded referenceto siare good candidate lexical units of F in theItalian FrameNet.
For example, if we link http://en.wikipedia.org/wiki/Court to the JUDI-CIAL BODY frame, we first retrieve the Italianversion of the site http://it.wikipedia.org/wiki/Tribunale.
Then, with a top-down strat-egy, we further extract all Italian sentences point-ing to the Tribunale page and acquire as lexi-cal units all words with an embedded reference tothis concept, for example tribunale and corte.
Inthis way, we can include the extracted lexical unitsand the sentences where they occur in the JUDI-CIAL BODY frame for Italian.Given the 893 (F, l) pairs in English and thelinked Wikipedia senses described in 6.2, we firstextracted the Italian Wikipages that are linked tothe English ones.
Then for every linked Wikipagein Italian, we retrieved all sentences with a refer-ence pointing to that page in the Italian Wikipediadump.
Statistics about the extracted data are re-ported in Table 3.Since the Italian Wikipedia is about one fifth ofthe English one, it was not possible to map ev-ery English Wikipage with an Italian article.
Infact, only 371 senses out of 893 in English werelinked to an Italian page.
Also the average num-7http://download.wikimedia.org/itwiki/20090203Italian WikipediaLinked Wikipages in Italian 371N.
of extracted sents 23,078Avg.
sents per Italian sense 62Table 3: Extracted data from Italian Wikipediaber of sentences extracted for every sense is muchsmaller (62 vs. 1,080).
Anyhow, this does not rep-resent a problem because in the English FrameNet,the lexical units whose annotation is consideredto be complete are usually instantiated by set of20 annotated sentences on average.
So, accordingto the FrameNet standard, 60 sentences are morethan enough to represent the meaning of a lexicalunit in a frame.7.2 EvaluationIn this evaluation part, we took into account 1,000sentences, in order to have a comparable datasetw.r.t.
the evaluation for English.
However, the setsof Italian sentences extracted for every (F, l), i.e.for every Wikipedia article, were much smaller,so we increased the number of randomly chosen(F, l) pairs to 80.
Our evaluation is focused on thequality of the sentences and aims at assessing if thegiven sentences are correctly assigned to the (F, l)pairs.
We report 69% accuracy, which is 9% lowerthan for English.
Apart from the same errors andissues reported for English, a decrease in perfor-mance can be explained by the fact that, since lessarticles are present w.r.t.
the English version, redi-rections and internal links tend to be less preciseand fine-grained.
For example, the word ?diritti?in the sense of ?
(human) rights?
redirects to the ar-ticle about Diritto, corresponding to Law as a sys-tem of rules.
On the contrary, Law and Rights havetwo different pages in English.
Besides, the differ-ent quality of the two resources can also dependon the smaller number of users that edit and checkthe Italian articles.
From the 1,000 sentences eval-uated we extracted 145 new lexical units: sinceItalian FrameNet does not exist yet, every lexicalunit in a sentence that is correct can be straightfor-wardly included in the first version of the resource.8 Conclusions and Future workIn this work, we have proposed to apply aword sense disambiguation system to a newtask, namely the linking between FrameNet andWikipedia.
Results are promising and show that283the task is adequately substantiated.
The proposedapproach can help enriching FrameNet with newexample sentences and lexical units and provide astarting point for the creation of FrameNet-like re-sources in all Wikipedia languages.
On the onehand, the retrieved data could speed up humanannotation, requiring only a manual validation.On the other hand, the extracted sentences couldprovide enough training data to machine learningsystems for frame assignment, since insufficientframe attestations in the FrameNet database are amajor problem for such systems.In the next research step, we plan to carry out anextended evaluation process in order to computeinter-annotator agreement and eventually point outvalidation problems.
Then, we want to extendthe mapping and the data extraction process to all(F, l) pairs in FrameNet (about 10,000).
The re-trieved sentences will be made available as train-ing or annotation material.
Besides, we wantto create an online resource where the links be-tween (F, l) pairs and Wikipages are made explicitand where users can browse the retrieved sen-tences.
The resource can be produced and madeavailable with a reduced effort for every languagein Wikipedia.
Anyway, the English version hasproved to be more precise, while the resource fornew languages would require a more accurate re-vision.AcknowledgmentsClaudio Giuliano is supported by the ITCHproject (http://itch.fbk.eu), sponsoredby the Italian Ministry of University and Re-search and by the Autonomous Province ofTrento and the X-Media project (http://www.x-media-project.org), sponsored by theEuropean Commission as part of the InformationSociety Technologies (IST) programme under ECgrant number IST-FP6-026978.ReferencesCollin F. Baker, Michael Ellsworth, and Katrin Erk.2007.
SemEval-2007 Task 10: Frame SemanticStructure Extraction.
In Proceedings of the FourthInternational Workshop on Semantic Evaluations(SemEval-2007), pages 99?104, Prague, CZ, June.Aljoscha Burchardt and Anette Frank.
2006.
Approxi-mating Textual Entailment with LFG and FrameNetFrames.
In Proceedings of the 2nd PASCAL RTEWorkshop, pages 92?97, Venice, Italy.Diego De Cao, Danilo Croce, Marco Pennacchiotti,and Roberto Basili.
2008.
Combining Word Senseand Usage for modeling Frame Semantics.
In Pro-ceedings of STEP 2008, Venice, Italy.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a library for support vector machines.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Olivier Chapelle, Bernhard Sch?olkopf, and AlexanderZien.
2006.
Semi-Supervised Learning.
MIT Press,Cambridge, MA.Ian Chow and Jonathan Webster.
2007.
Integra-tion of Linguistic Resources for Verb Classification:FrameNet Frame, WordNet Verb and Suggested Up-per Merged Ontology.
Computational Linguisticsand Intelligent Text Processing, pages 1?11.Mario Crespo and Paul Buitelaar.
2008.
Domain-specific English-to-Spanish Translation ofFrameNet.
In Proc.
of LREC 2008, Marrakech.Andras Csomai and Rada Mihalcea.
2008.
LinkingDocuments to Encyclopedic Knowledge.
IEEE In-telligent Systems, special issue on ?Natural Lan-guage Processing for the Web?.Silviu Cucerzan.
2007.
Large-scale named entitydisambiguation based on Wikipedia data.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 708?716, Prague, Czech Republic,June.
Association for Computational Linguistics.Katrin Erk.
2004.
Frame assignment as WordSense Disambiguation.
In Proceedings of IWCS-6,Tilburg, NL.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.C.J.
Fillmore, C.R.
Johnson, and M. R. L. Petruck.2003.
Background to FrameNet.
InternationalJournal of Lexicography, 16:235?250, September.Ana-Maria Giuglea and Alessandro Moschitti.
2006.Semantic role labeling via FrameNet, VerbNet andPropBank.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th annual ACL meeting, pages 929?936, Morris-town, US.A.
Gliozzo, C. Giuliano, and C. Strapparava.
2005.Domain kernels for word sense disambiguation.
InProceedings of the 43rdannual meeting of the As-sociation for Computational Linguistics (ACL-05),pages 403?410, Ann Arbor, Michigan, June.R.
Johansson and P. Nugues.
2007.
Using Word-Net to extend FrameNet coverage.
In Proc.
of theWorkshop on Building Frame-semantic Resourcesfor Scandinavian and Baltic Languages, at NODAL-IDA, Tartu.284B.
Magnini, C. Strapparava, G. Pezzulo, andA.
Gliozzo.
2002.
The Role of Domain Informationin Word Sense Disambiguation.
Natural LanguageEngineering, 8(4):359?373.R.
Mihalcea and P. Edmonds, editors.
2004.
Proceed-ings of SENSEVAL-3, Barcelona, Spain, July.David Milne and Ian H. Witten.
2008.
Learning tolink with Wikipedia.
In CIKM ?08: Proceedings ofthe 17th ACM conference on Information and knowl-edge management, pages 509?518, NY, USA.
ACM.Sameer Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer.
2007.
Semeval-2007 Task-17: En-glish Lexical Sample, SRL and All Words.
In Pro-ceedings of the Fourth International Workshop onSemantic Evaluations (SemEval-2007), pages 87?92, Prague, Czech Republic, June.
Association forComputational Linguistics.J.
Shawe-Taylor and N. Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge Univer-sity Press.Lei Shi and Rada Mihalcea.
2004.
Open Text Seman-tic Parsing Using FrameNet and WordNet.
In Pro-ceedings of HLT-NAACL 2004.Lei Shi and Rada Mihalcea.
2005.
Putting Pieces To-gether: Combining FrameNet, VerbNet and Word-Net for Robust Semantic Parsing.
In Proceedings ofCICLing 2005, pages 100?111.
Springer.Sara Tonelli and Daniele Pighin.
2009.
New featuresfor FrameNet - WordNet Mapping.
In Proceedingsof the Thirteenth Conference on Computational Nat-ural Language Learning, Boulder, CO, USA.285
