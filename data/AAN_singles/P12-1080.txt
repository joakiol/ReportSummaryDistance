Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 759?767,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsModeling Topic Dependencies in Hierarchical Text CategorizationAlessandro Moschitti and Qi JuUniversity of Trento38123 Povo (TN), Italy{moschitti,qi}@disi.unitn.itRichard JohanssonUniversity of GothenburgSE-405 30 Gothenburg, Swedenrichard.johansson@gu.seAbstractIn this paper, we encode topic dependenciesin hierarchical multi-label Text Categoriza-tion (TC) by means of rerankers.
We rep-resent reranking hypotheses with several in-novative kernels considering both the struc-ture of the hierarchy and the probability ofnodes.
Additionally, to better investigate therole of category relationships, we consider twointeresting cases: (i) traditional schemes inwhich node-fathers include all the documentsof their child-categories; and (ii) more gen-eral schemes, in which children can includedocuments not belonging to their fathers.
Theextensive experimentation on Reuters CorpusVolume 1 shows that our rerankers inject ef-fective structural semantic dependencies inmulti-classifiers and significantly outperformthe state-of-the-art.1 IntroductionAutomated Text Categorization (TC) algorithms forhierarchical taxonomies are typically based on flatschemes, e.g., one-vs.-all, which do not take topicrelationships into account.
This is due to two majorproblems: (i) complexity in introducing them in thelearning algorithm and (ii) the small or no advan-tage that they seem to provide (Rifkin and Klautau,2004).We speculate that the failure of using hierarchi-cal approaches is caused by the inherent complexityof modeling all possible topic dependencies ratherthan the uselessness of such relationships.
More pre-cisely, although hierarchical multi-label classifierscan exploit machine learning algorithms for struc-tural output, e.g., (Tsochantaridis et al, 2005; Rie-zler and Vasserman, 2010; Lavergne et al, 2010),they often impose a number of simplifying restric-tions on some category assignments.
Typically, theprobability of a document d to belong to a subcate-gory Ci of a category C is assumed to depend onlyon d and C, but not on other subcategories of C,or any other categories in the hierarchy.
Indeed, theintroduction of these long-range dependencies leadto computational intractability or more in general tothe problem of how to select an effective subset ofthem.
It is important to stress that (i) there is notheory that can suggest which are the dependenciesto be included in the model and (ii) their exhaustiveexplicit generation (i.e., the generation of all hierar-chy subparts) is computationally infeasible.
In thisperspective, kernel methods are a viable approachto implicitly and easily explore feature spaces en-coding dependencies.
Unfortunately, structural ker-nels, e.g., tree kernels, cannot be applied in struc-tured output algorithms such as (Tsochantaridis etal., 2005), again for the lack of a suitable theory.In this paper, we propose to use the combinationof reranking with kernel methods as a way to han-dle the computational and feature design issues.
Wefirst use a basic hierarchical classifier to generate ahypothesis set of limited size, and then apply rerank-ing models.
Since our rerankers are simple binaryclassifiers of hypothesis pairs, they can encode com-plex dependencies thanks to kernel methods.
In par-ticular, we used tree, sequence and linear kernels ap-plied to structural and feature-vector representationsdescribing hierarchical dependencies.Additionally, to better investigate the role of topi-cal relationships, we consider two interesting cases:(i) traditional categorization schemes in which node-759fathers include all the documents of their child-categories; and (ii) more general schemes, in whichchildren can include documents not belonging totheir fathers.
The intuition under the above settingis that shared documents between categories createsemantic links between them.
Thus, if we removecommon documents between father and children, wereduce the dependencies that can be captured withtraditional bag-of-words representation.We carried out experiments on two entire hierar-chies TOPICS (103 nodes organized in 5 levels) andINDUSTRIAL (365 nodes organized in 6 levels) ofthe well-known Reuters Corpus Volume 1 (RCV1).We first evaluate the accuracy as well as the ef-ficiency of several reranking models.
The resultsshow that all our rerankers consistently and signif-icantly improve on the traditional approaches to TCup to 10 absolute percent points.
Very interestingly,the combination of structural kernels with the lin-ear kernel applied to vectors of category probabil-ities further improves on reranking: such a vectorprovides a more effective information than the jointglobal probability of the reranking hypothesis.In the rest of the paper, Section 2 describes the hy-pothesis generation algorithm, Section 3 illustratesour reranking approach based on tree kernels, Sec-tion 4 reports on our experiments, Section 5 illus-trates the related work and finally Section 6 derivesthe conclusions.2 Hierarchy classification hypotheses frombinary decisionsThe idea of the paper is to build efficient modelsfor hierarchical classification using global depen-dencies.
For this purpose, we use reranking mod-els, which encode global information.
This neces-sitates of a set of initial hypotheses, which are typ-ically generated by local classifiers.
In our study,we used n one-vs.-all binary classifiers, associatedwith the n different nodes of the hierarchy.
In thefollowing sections, we describe a simple frameworkfor hypothesis generation.2.1 Top k hypothesis generationGiven n categories, C1, .
.
.
, Cn, we can definep1Ci(d) and p0Ci(d) as the probabilities that the clas-sifier i assigns the document d to Ci or not, respec-tively.
For example, phCi(d) can be computed fromM132M11 M12 M13 M14M143 M142 M141MCATM131Figure 1: A subhierarchy of Reuters.-M132M11 -M12 M13 M14M143 -M142 -M141MCAT-M131Figure 2: A tree representing a category assignment hy-pothesis for the subhierarchy in Fig.
1.the SVM classification output (i.e., the example mar-gin).
Typically, a large margin corresponds to highprobability for d to be in the category whereas smallmargin indicates low probability1.
Let us indicatewith h = {h1, .., hn} ?
{0, 1}n a classification hy-pothesis, i.e., the set of n binary decisions for a doc-ument d. If we assume independence between theSVM scores, the most probable hypothesis on d ish?
= argmaxh?
{0,1}nn?i=1phii (d) =(argmaxh?
{0,1}phi (d))ni=1.Given h?, the second best hypothesis can be ob-tained by changing the label on the least probableclassification, i.e., associated with the index j =argmini=1,..,nph?ii (d).
By storing the probability of thek ?
1 most probable configurations, the next k besthypotheses can be efficiently generated.3 Structural Kernels for RerankingHierarchical ClassificationIn this section we describe our hypothesis reranker.The main idea is to represent the hypotheses as atree structure, naturally derived from the hierarchyand then to use tree kernels to encode such a struc-tural description in a learning algorithm.
For thispurpose, we describe our hypothesis representation,kernel methods and the kernel-based approach topreference reranking.3.1 Encoding hypotheses in a treeOnce hypotheses are generated, we need a represen-tation from which the dependencies between the dif-1We used the conversion of margin into probability providedby LIBSVM.760M11 M13 M14M143MCATFigure 3: A compact representation of the hypothesis inFig.
2.ferent nodes of the hierarchy can be learned.
Sincewe do not know in advance which are the importantdependencies and not even the scope of the interac-tion between the different structure subparts, we relyon automatic feature engineering via structural ker-nels.
For this paper, we consider tree-shaped hier-archies so that tree kernels, e.g.
(Collins and Duffy,2002; Moschitti, 2006a), can be applied.In more detail, we focus on the Reuters catego-rization scheme.
For example, Figure 1 shows a sub-hierarchy of the Markets (MCAT) category and itssubcategories: Equity Markets (M11), Bond Mar-kets (M12), Money Markets (M13) and Commod-ity Markets (M14).
These also have subcategories:Interbank Markets (M131), Forex Markets (M132),Soft Commodities (M141), Metals Trading (M142)and Energy Markets (M143).As the input of our reranker, we can simply usea tree representing the hierarchy above, marking thenegative assignments of the current hypothesis in thenode labels with ?-?, e.g., -M142 means that the doc-ument was not classified in Metals Trading.
For ex-ample, Figure 2 shows the representation of a classi-fication hypothesis consisting in assigning the targetdocument to the categories MCAT, M11, M13, M14and M143.Another more compact representation is the hier-archy tree from which all the nodes associated witha negative classification decision are removed.
Asonly a small subset of nodes of the full hierarchy willbe positively classified the tree will be much smaller.Figure 3 shows the compact representation of the hy-pothesis in Fig.
2.
The next sections describe how toexploit these kinds of representations.3.2 Structural KernelsIn kernel-based machines, both learning and classi-fication algorithms only depend on the inner prod-uct between instances.
In several cases, this can beefficiently and implicitly computed by kernel func-tions by exploiting the following dual formulation:?i=1..l yi?i?(oi)?
(o) + b = 0, where oi and o aretwo objects, ?
is a mapping from the objects to fea-ture vectors ~xi and ?(oi)?
(o) = K(oi, o) is a ker-nel function implicitly defining such a mapping.
Incase of structural kernels,K determines the shape ofthe substructures describing the objects above.
Themost general kind of kernels used in NLP are stringkernels, e.g.
(Shawe-Taylor and Cristianini, 2004),the Syntactic Tree Kernels (Collins and Duffy, 2002)and the Partial Tree Kernels (Moschitti, 2006a).3.2.1 String KernelsThe String Kernels (SK) that we consider countthe number of subsequences shared by two stringsof symbols, s1 and s2.
Some symbols during thematching process can be skipped.
This modifiesthe weight associated with the target substrings asshown by the following SK equation:SK(s1, s2) =?u???
?u(s1) ?
?u(s2) =?u???
?~I1:u=s1[~I1]?~I2:u=s2[~I2]?d(~I1)+d(~I2)where, ??
=?
?n=0 ?n is the set of all strings, ~I1 and~I2 are two sequences of indexes ~I = (i1, ..., i|u|),with 1 ?
i1 < ... < i|u| ?
|s|, such that u = si1 ..si|u| ,d(~I) = i|u| ?
i1 + 1 (distance between the first andlast character) and ?
?
[0, 1] is a decay factor.It is worth noting that: (a) longer subsequencesreceive lower weights; (b) some characters can beomitted, i.e.
gaps; (c) gaps determine a weight sincethe exponent of ?
is the number of characters andgaps between the first and last character; and (c)the complexity of the SK computation is O(mnp)(Shawe-Taylor and Cristianini, 2004), where m andn are the lengths of the two strings, respectively andp is the length of the largest subsequence we want toconsider.In our case, given a hypothesis represented asa tree like in Figure 2, we can visit it and derivea linearization of the tree.
SK applied to sucha node sequence can derive useful dependenciesbetween category nodes.
For example, using theBreadth First Search on the compact representa-tion, we get the sequence [MCAT, M11, M13,M14, M143], which generates the subsequences,[MCAT, M11], [MCAT, M11, M13, M14],[M11, M13, M143], [M11, M13, M143]and so on.761M11 -M12  M13 M14MCATM11 -M12  M13 M14MCAT-M132 -M131-M132 -M131M14M143 -M142 -M141M11 -M12  M13 M14MCATM143 -M142 -M141   M13Figure 4: The tree fragments of the hypothesis in Fig.
2generated by STKM14-M143 -M142 -M141 -M132M13-M131M11 -M12  M13 M14MCATM11MCAT-M132M13-M131M13MCAT-M131-M132M13 M14-M142 -M141 M11 -M12 M13MCAT MCAT MCATFigure 5: Some tree fragments of the hypothesis in Fig.
2generated by PTK3.2.2 Tree KernelsConvolution Tree Kernels compute the numberof common substructures between two trees T1and T2 without explicitly considering the wholefragment space.
For this purpose, let the setF = {f1, f2, .
.
.
, f|F|} be a tree fragment space and?i(n) be an indicator function, equal to 1 if thetarget fi is rooted at node n and equal to 0 oth-erwise.
A tree-kernel function over T1 and T2 isTK(T1, T2) =?n1?NT1?n2?NT2?
(n1, n2), NT1and NT2 are the sets of the T1?s and T2?s nodes,respectively and ?
(n1, n2) =?|F|i=1 ?i(n1)?i(n2).The latter is equal to the number of common frag-ments rooted in the n1 and n2 nodes.
The ?
func-tion determines the richness of the kernel space andthus different tree kernels.
Hereafter, we considerthe equation to evaluate STK and PTK.2Syntactic Tree Kernels (STK) To compute STK,it is enough to compute ?STK(n1, n2) as follows(recalling that since it is a syntactic tree kernels, eachnode can be associated with a production rule): (i)if the productions at n1 and n2 are different then?STK(n1, n2) = 0; (ii) if the productions at n1and n2 are the same, and n1 and n2 have onlyleaf children then ?STK(n1, n2) = ?
; and (iii) ifthe productions at n1 and n2 are the same, and n1and n2 are not pre-terminals then ?STK(n1, n2) =?
?l(n1)j=1 (1 + ?STK(cjn1 , cjn2)), where l(n1) is the2To have a similarity score between 0 and 1, a normalizationin the kernel space, i.e.
TK(T1,T2)?TK(T1,T1)?TK(T2,T2)is applied.number of children of n1 and cjn is the j-th childof the node n. Note that, since the productionsare the same, l(n1) = l(n2) and the computationalcomplexity of STK is O(|NT1 ||NT2 |) but the aver-age running time tends to be linear, i.e.
O(|NT1 | +|NT2 |), for natural language syntactic trees (Mos-chitti, 2006a; Moschitti, 2006b).Figure 4 shows the five fragments of the hypothe-sis in Figure 2.
Such fragments satisfy the constraintthat each of their nodes includes all or none of itschildren.
For example, [M13 [-M131 -M132]] is anSTF, which has two non-terminal symbols, -M131and -M132, as leaves while [M13 [-M131]] is not anSTF.The Partial Tree Kernel (PTK) The compu-tation of PTK is carried out by the following?PTK function: if the labels of n1 and n2 are dif-ferent then ?PTK(n1, n2) = 0; else ?PTK(n1, n2) =?
(?2 +?~I1,~I2,l(~I1)=l(~I2)?d(~I1)+d(~I2)l(~I1)?j=1?PTK(cn1(~I1j), cn2(~I2j)))where d(~I1) = ~I1l(~I1) ?~I11 and d(~I2) = ~I2l(~I2) ?~I21.
This way, we penalize both larger trees andchild subsequences with gaps.
PTK is more gen-eral than STK as if we only consider the contribu-tion of shared subsequences containing all childrenof nodes, we implement STK.
The computationalcomplexity of PTK isO(p?2|NT1 ||NT2 |) (Moschitti,2006a), where p is the largest subsequence of chil-dren that we want consider and ?
is the maximal out-degree observed in the two trees.
However the aver-age running time again tends to be linear for naturallanguage syntactic trees (Moschitti, 2006a).Given a target T , PTK can generate any subset ofconnected nodes of T , whose edges are in T .
Forexample, Fig.
5 shows the tree fragments from thehypothesis of Fig.
2.
Note that each fragment cap-tures dependencies between different categories.3.3 Preference rerankerWhen training a reranker model, the task of the ma-chine learning algorithm is to learn to select the bestcandidate from a given set of hypotheses.
To useSVMs for training a reranker, we applied PreferenceKernel Method (Shen et al, 2003).
The reductionmethod from ranking tasks to binary classification isan active research area; see for instance (Balcan etal., 2008) and (Ailon and Mohri, 2010).762CategoryChild-free Child-fullTrain Train1 Train2 TEST Train Train1 Train2 TESTC152 837 370 467 438 837 370 467 438GPOL 723 357 366 380 723 357 366 380M11 604 309 205 311 604 309 205 311.. .. .. .. .. .. .. .. ..C31 313 163 150 179 531 274 257 284E41 191 89 95 102 223 121 102 118GCAT 345 177 168 173 3293 1687 1506 1600.. .. .. .. .. .. .. .. ..E31 11 4 7 6 32 21 11 19M14 96 49 47 58 1175 594 581 604G15 5 4 1 0 290 137 153 146Total: 103 10,000 5,000 5,000 5,000 10,000 5,000 5,000 5,000Table 1: Instance distributions of RCV1: the most populated categories are on the top, the medium sized ones followand the smallest ones are at the bottom.
There are some difference between child-free and child-full setting since forthe former, from each node, we removed all the documents in its children.In the Preference Kernel approach, the rerankingproblem ?
learning to pick the correct candidate h1from a candidate set {h1, .
.
.
, hk} ?
is reduced to abinary classification problem by creating pairs: pos-itive training instances ?h1, h2?, .
.
.
, ?h1, hk?
andnegative instances ?h2, h1?, .
.
.
, ?hk, h1?.
This train-ing set can then be used to train a binary classifier.At classification time, pairs are not formed (since thecorrect candidate is not known); instead, the stan-dard one-versus-all binarization method is still ap-plied.The kernels are then engineered to implicitlyrepresent the differences between the objects inthe pairs.
If we have a valid kernel K over thecandidate space T , we can construct a preferencekernel PK over the space of pairs T ?T as follows:PK(x, y) =PK(?x1, x2?, ?y1, y2?)
= K(x1, y1)+K(x2, y2)?K(x1, y2)?K(x2, y1),(1)where x, y ?
T ?
T .
It is easy to show (Shen et al,2003) that PK is also a valid Mercer?s kernel.
Thismakes it possible to use kernel methods to train thereranker.We explore innovative kernels K to be used inEq.
1:KJ = p(x1) ?
p(y1) + S, where p(?)
is the globaljoint probability of a target hypothesis and S isa structural kernel, i.e., SK, STK and PTK.KP = ~x1 ?
~y1 + S, where ~x1={p(x1, j)}j?x1 ,~y1 = {p(y1, j)}j?y1 , p(t, n) is the classifica-tion probability of the node (category) n in theF1 BL BOL SK STK PTKMicro-F1 0.769 0.771 0.786 0.790 0.790Macro-F1 0.539 0.541 0.542 0.547 0.560Table 2: Comparison of rerankers using different kernels,child-full setting (KJ model).F1 BL BOL SK STK PTKMicro-F1 0.640 0.649 0.653 0.677 0.682Macro-F1 0.408 0.417 0.431 0.447 0.447Table 3: Comparison of rerankers using different kernels,child-free setting (KJ model).tree t ?
T and S is again a structural kernel,i.e., SK, STK and PTK.For comparative purposes, we also use for S a lin-ear kernel over the bag-of-labels (BOL).
This issupposed to capture non-structural dependencies be-tween the category labels.4 ExperimentsThe aim of the experiments is to demonstrate thatour reranking approach can introduce semantic de-pendencies in the hierarchical classification model,which can improve accuracy.
For this purpose, weshow that several reranking models based on treekernels improve the classification based on the flatone-vs.-all approach.
Then, we analyze the effi-ciency of our models, demonstrating their applica-bility.4.1 SetupWe used two full hierarchies, TOPICS and INDUS-TRY of Reuters Corpus Volume 1 (RCV1)3 TC cor-3trec.nist.gov/data/reuters/reuters.html763pus.
For most experiments, we randomly selectedtwo subsets of 10k and 5k of documents for train-ing and testing from the total 804,414 Reuters newsfrom TOPICS by still using all the 103 categoriesorganized in 5 levels (hereafter SAM).
The distri-bution of the data instances of some of the dif-ferent categories in such samples can be observedin Table 1.
The training set is used for learningthe binary classifiers needed to build the multiclass-classifier (MCC).
To compare with previous workwe also considered the Lewis?
split (Lewis et al,2004), which includes 23,149 news for training and781,265 for testing.Additionally, we carried out some experiments onINDUSTRY data from RCV1.
This contains 352,361news assigned to 365 categories, which are orga-nized in 6 levels.
The Lewis?
split for INDUSTRY in-cludes 9,644 news for training and 342,117 for test-ing.
We used the above datasets with two differentsettings: the child-free setting, where we removedall the document belonging to the child nodes fromthe parent nodes, and the normal setting which werefer to as child-full.To implement the baseline model, we applied thestate-of-the-art method used by (Lewis et al, 2004)for RCV1, i.e.,: SVMs with the default parameters(trade-off and cost factor = 1), linear kernel, normal-ized vectors, stemmed bag-of-words representation,log(TF + 1) ?
IDF weighting scheme and stoplist4.
We used the LIBSVM5 implementation, whichprovides a probabilistic outcome for the classifica-tion function.
The classifiers are combined using theone-vs.-all approach, which is also state-of-the-artas argued in (Rifkin and Klautau, 2004).
Since thetask requires us to assign multiple labels, we simplycollect the decisions of the n classifiers: this consti-tutes our MCC baseline.Regarding the reranker, we divided the trainingset in two chunks of data: Train1 and Train2.
Thebinary classifiers are trained on Train1 and tested onTrain2 (and vice versa) to generate the hypotheseson Train2 (Train1).
The union of the two sets con-stitutes the training data for the reranker.
We imple-4We have just a small difference in the number of tokens,i.e., 51,002 vs. 47,219 but this is both not critical and rarelyachievable because of the diverse stop lists or tokenizers.5http://www.csie.ntu.edu.tw/?cjlin/libsvm/0.6260.6360.6460.6560.6660.6762 7 12 17 22 27 32Micro-F1Training Data Size (thousands of instances)BL (Child-free)RR (Child-free)FRR (Child-free)Figure 6: Learning curves of the reranking models usingSTK in terms of MicroAverage-F1, according to increas-ing training set (child-free setting).0.3650.3750.3850.3950.4050.4150.4250.4350.4452 7 12 17 22 27 32Macro-F1Training Data Size (thousands of instances)BL (Child-free)RR (Child-free)FRR (Child-free)Figure 7: Learning curves of the reranking models usingSTK in terms of MacroAverage-F1, according to increas-ing training set (child-free setting).mented two rerankers: RR, which use the represen-tation of hypotheses described in Fig.
2; and FRR,i.e., fast RR, which uses the compact representationdescribed in Fig.
3.The rerankers are based on SVMs and the Prefer-ence Kernel (PK) described in Sec.
1 built on top ofSK, STK or PTK (see Section 3.2.2).
These are ap-plied to the tree-structured hypotheses.
We trainedthe rerankers using SVM-light-TK6, which enablesthe use of structural kernels in SVM-light (Joachims,1999).
This allows for applying kernels to pairs oftrees and combining them with vector-based kernels.Again we use default parameters to facilitate replica-bility and preserve generality.
The rerankers alwaysuse 8 best hypotheses.All the performance values are provided by meansof Micro- and Macro-Average F1, evaluated on test6disi.unitn.it/moschitti/Tree-Kernel.htm764Cat.Child-free Child-fullBL KJ KP BL KJ KPC152 0.671 0.700 0.771 0.671 0.729 0.745GPOL 0.660 0.695 0.743 0.660 0.680 0.734M11 0.851 0.891 0.901 0.851 0.886 0.898.. .. .. .. .. .. ..C31 0.225 0.311 0.446 0.356 0.421 0.526E41 0.643 0.714 0.719 0.776 0.791 0.806GCAT 0.896 0.908 0.917 0.908 0.916 0.926.. .. .. .. .. .. ..E31 0.444 0.600 0.600 0.667 0.765 0.688M14 0.591 0.600 0.575 0.887 0.897 0.904G15 0.250 0.222 0.250 0.823 0.806 0.826103 cat.Mi-F1 0.640 0.677 0.731 0.769 0.794 0.815Ma-F1 0.408 0.447 0.507 0.539 0.567 0.590Table 4: F1 of some binary classifiers along with theMicro and Macro-Average F1 over all 103 categoriesof RCV1, 8 hypotheses and 32k of training data forrerankers using STK.data over all categories (103 or 363).
Additionally,the F1 of some binary classifiers are reported.4.2 Classification AccuracyIn the first experiments, we compared the differentkernels using the KJ combination (which exploitsthe joint hypothesis probability, see Sec.
3.3) onSAM.
Tab.
2 shows that the baseline (state-of-the-art flat model) is largely improved by all rerankers.BOL cannot capture the same dependencies as thestructural kernels.
In contrast, when we remove thedependencies generated by shared documents be-tween a node and its descendants (child-free setting)BOL improves on BL.
Very interestingly, TK andPTK in this setting significantly improves on SKsuggesting that the hierarchical structure is more im-portant than the sequential one.To study how much data is needed for thereranker, the figures 6 and 7 report the Micro andMacro Average F1 of our rerankers over 103 cate-gories, according to different sets of training data.This time, KJ is applied to only STK.
We note that(i) a few thousands of training examples are enoughto deliver most of the RR improvement; and (ii) theFRR produces similar results as standard RR.
This isvery interesting since, as it will be shown in the nextsection, the compact representation produces muchfaster models.Table 4 reports the F1 of some individual cate-gories as well as global performance.
In these exper-iments we used STK in KJ and KP .
We note that0501001502002503003504004502 12 22 32 42 52 62Time(min)Training Data Size (thousands of instances)RR trainingTimeRR testTimeFRR trainingTimeFRR testTimeFigure 8: Training and test time of the rerankers trainedon data of increasing size.KP highly improves on the baseline on child-freesetting by about 7.1 and 9.9 absolute percent pointsin Micro-and Macro-F1, respectively.
Also the im-provement on child-full is meaningful, i.e., 4.6 per-cent points.
This is rather interesting as BOL (notreported in the table) achieved a Micro-average of80.4% and a Macro-average of 57.2% when used inKP , i.e., up to 2 points below STK.
This means thatthe use of probability vectors and combination withstructural kernels is a very promising direction forreranker design.To definitely assess the benefit of our rerankerswe tested them on the Lewis?
split of two differentdatasets of RCV1, i.e., TOPIC and INDUSTRY.
Ta-ble 5 shows impressive results, e.g., for INDUSTRY,the improvement is up to 5.2 percent points.
We car-ried out statistical significance tests, which certifiedthe significance at 99%.
This was expected as thesize of the Lewis?
test sets is in the order of severalhundreds thousands.Finally, to better understand the potential ofreranking, Table 6 shows the oracle performancewith respect to the increasing number of hypothe-ses.
The outcome clearly demonstrates that there islarge margin of improvement for the rerankers.4.3 Running TimeTo study the applicability of our rerankers, we haveanalyzed both the training and classification time.Figure 8 shows the minutes required to train the dif-ferent models as well as to classify the test set ac-cording to data of increasing size.It can be noted that the models using the compacthypothesis representation are much faster than those765F1Topic IndustryBL (Lewis) BL (Ours) KJ (BOL) KJ KP BL (Lewis) BL (Ours) KJ (BOL) KJ KPMicro-F1 0.816 0.815 0.818 0.827 0.849 0.512 0.562 0.566 0.576 0.628Macro-F1 0.567 0.566 0.571 0.590 0.615 0.263 0.289 0.243 0.314 0.341Table 5: Comparison between rankers using STK or BOL (when indicated) with the KJ and KP schema.
32kexamples are used for training the rerankers with child-full setting.k Micro-F1 Macro-F11 0.640 0.4082 0.758 0.5044 0.821 0.5668 0.858 0.61016 0.898 0.658Table 6: Oracle performance according to the number ofhypotheses (child-free setting).using the complete hierarchy as representation, i.e.,up to five times in training and eight time in test-ing.
This is not surprising as, in the latter case,each kernel evaluation requires to perform tree ker-nel evaluation on trees of 103 nodes.
When usingthe compact representation the number of nodes isupper-bounded by the maximum number of labelsper documents, i.e., 6, times the depth of the hierar-chy, i.e., 5 (the positive classification on the leavesis the worst case).
Thus, the largest tree would con-tain 30 nodes.
However, we only have 1.82 labelsper document on average, therefore the trees havean average size of only about 9 nodes.5 Related WorkTree and sequence kernels have been successfullyused in many NLP applications, e.g.
: parse rerank-ing and adaptation (Collins and Duffy, 2002; Shenet al, 2003; Toutanova et al, 2004; Kudo et al,2005; Titov and Henderson, 2006), chunking anddependency parsing (Kudo and Matsumoto, 2003;Daume?
III and Marcu, 2004), named entity recog-nition (Cumby and Roth, 2003), text categorization(Cancedda et al, 2003; Gliozzo et al, 2005) and re-lation extraction (Zelenko et al, 2002; Bunescu andMooney, 2005; Zhang et al, 2006).To our knowledge, ours is the first work explor-ing structural kernels for reranking hierarchical textcategorization hypotheses.
Additionally, there is asubstantial lack of work exploring reranking for hi-erarchical text categorization.
The work mostly re-lated to ours is (Rousu et al, 2006) as they directlyencoded global dependencies in a gradient descen-dent learning approach.
This kind of algorithm isless efficient than ours so they could experimentwith only the CCAT subhierarchy of RCV1, whichonly contains 34 nodes.
Other relevant work suchas (McCallum et al, 1998) and (Dumais and Chen,2000) uses a rather different datasets and a differentidea of dependencies based on feature distributionsover the linked categories.
An interesting method isSVM-struct (Tsochantaridis et al, 2005), which hasbeen applied to model dependencies expressed ascategory label subsets of flat categorization schemesbut no solution has been attempted for hierarchicalsettings.
The approaches in (Finley and Joachims,2007; Riezler and Vasserman, 2010; Lavergne et al,2010) can surely be applied to model dependenciesin a tree, however, they need that feature templatesare specified in advance, thus the meaningful depen-dencies must be already known.
In contrast, kernelmethods allow for automatically generating all pos-sible dependencies and reranking can efficiently en-code them.6 ConclusionsIn this paper, we have described several models forreranking the output of an MCC based on SVMsand structural kernels, i.e., SK, STK and PTK.We have proposed a simple and efficient algorithmfor hypothesis generation and their kernel-basedrepresentations.
The latter are exploited by SVMsusing preference kernels to automatically derivefeatures from the hypotheses.
When using treekernels such features are tree fragments, which canencode complex semantic dependencies betweencategories.
We tested our rerankers on the entirewell-known RCV1.
The results show impressiveimprovement on the state-of-the-art flat TC models,i.e., 3.3 absolute percent points on the Lewis?
split(same setting) and up to 10 absolute points onsamples using child-free setting.Acknowledgements This research is partially sup-ported by the EC FP7/2007-2013 under the grants:247758 (ETERNALS), 288024 (LIMOSINE) and 231126(LIVINGKNOWLEDGE).
Many thanks to the reviewersfor their valuable suggestions.766ReferencesNir Ailon and Mehryar Mohri.
2010.
Preference-basedlearning to rank.
Machine Learning.Maria-Florina Balcan, Nikhil Bansal, Alina Beygelzimer,Don Coppersmith, John Langford, and Gregory B.Sorkin.
2008.
Robust reductions from ranking to clas-sification.
Machine Learning, 72(1-2):139?153.Razvan Bunescu and Raymond Mooney.
2005.
A short-est path dependency kernel for relation extraction.
InProceedings of HLT and EMNLP, pages 724?731,Vancouver, British Columbia, Canada, October.Nicola Cancedda, Eric Gaussier, Cyril Goutte, andJean Michel Renders.
2003.
Word sequence kernels.Journal of Machine Learning Research, 3:1059?1082.Michael Collins and Nigel Duffy.
2002.
New rankingalgorithms for parsing and tagging: Kernels over dis-crete structures, and the voted perceptron.
In Proceed-ings of ACL?02, pages 263?270.Chad Cumby and Dan Roth.
2003.
On kernel methodsfor relational learning.
In Proceedings of ICML 2003.Hal Daume?
III and Daniel Marcu.
2004.
Np bracketingby maximum entropy tagging and SVM reranking.
InProceedings of EMNLP?04.Susan T. Dumais and Hao Chen.
2000.
Hierarchical clas-sification of web content.
In Nicholas J. Belkin, PeterIngwersen, and Mun-Kew Leong, editors, Proceedingsof SIGIR-00, 23rd ACM International Conference onResearch and Development in Information Retrieval,pages 256?263, Athens, GR.
ACM Press, New York,US.T.
Finley and T. Joachims.
2007.
Parameter learningfor loopy markov random fields with structural supportvector machines.
In ICML Workshop on ConstrainedOptimization and Structured Output Spaces.Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.2005.
Domain kernels for word sense disambiguation.In Proceedings of ACL?05, pages 403?410.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
Advances in Kernel Methods ?
Sup-port Vector Learning, 13.Taku Kudo and Yuji Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In Proceedings of ACL?03.Taku Kudo, Jun Suzuki, and Hideki Isozaki.
2005.Boosting-based parse reranking with subtree features.In Proceedings of ACL?05.T.
Lavergne, O.
Cappe?, and F. Yvon.
2010.
Practical verylarge scale CRFs.
In Proc.
of ACL, pages 504?513.D.
D. Lewis, Y. Yang, T. Rose, and F. Li.
2004.
Rcv1: Anew benchmark collection for text categorization re-search.
The Journal of Machine Learning Research,(5):361?397.Andrew McCallum, Ronald Rosenfeld, Tom M. Mitchell,and Andrew Y. Ng.
1998.
Improving text classifica-tion by shrinkage in a hierarchy of classes.
In ICML,pages 359?367.Alessandro Moschitti.
2006a.
Efficient convolution ker-nels for dependency and constituent syntactic trees.
InProceedings of ECML?06.Alessandro Moschitti.
2006b.
Making tree kernels prac-tical for natural language learning.
In Proccedings ofEACL?06.S.
Riezler and A. Vasserman.
2010.
Incremental featureselection and l1 regularization for relaxed maximum-entropy modeling.
In EMNLP.Ryan Rifkin and Aldebaro Klautau.
2004.
In defense ofone-vs-all classification.
J. Mach.
Learn.
Res., 5:101?141, December.Juho Rousu, Craig Saunders, Sandor Szedmak, and JohnShawe-Taylor.
2006.
Kernel-based learning of hierar-chical multilabel classification models.
The Journal ofMachine Learning Research, (7):1601?1626.John Shawe-Taylor and Nello Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge UniversityPress.Libin Shen, Anoop Sarkar, and Aravind k. Joshi.
2003.Using LTAG Based Features in Parse Reranking.
InEmpirical Methods for Natural Language Processing(EMNLP), pages 89?96, Sapporo, Japan.Ivan Titov and James Henderson.
2006.
Porting statisti-cal parsers with data-defined kernels.
In Proceedingsof CoNLL-X.Kristina Toutanova, Penka Markova, and ChristopherManning.
2004.
The Leaf Path Projection View ofParse Trees: Exploring String Kernels for HPSG ParseSelection.
In Proceedings of EMNLP 2004.Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-mann, and Yasemin Altun.
2005.
Large marginmethods for structured and interdependent output vari-ables.
J.
Machine Learning Reserach., 6:1453?1484,December.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2002.
Kernel methods for relationextraction.
In Proceedings of EMNLP-ACL, pages181?201.Min Zhang, Jie Zhang, and Jian Su.
2006.
Explor-ing Syntactic Features for Relation Extraction using aConvolution tree kernel.
In Proceedings of NAACL.767
