Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1140?1150,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsStaying Informed: Supervised and Semi-Supervised Multi-viewTopical Analysis of Ideological PerspectiveAmr AhmedSchool of Computer ScienceCarnegie Mellon Universityamahmed@cs.cmu.eduEric P. XingSchool of Computer ScienceCarnegie Mellon Universityepxing@cs.cmu.eduAbstractWith the proliferation of user-generated arti-cles over the web, it becomes imperative to de-velop automated methods that are aware of theideological-bias implicit in a document col-lection.
While there exist methods that canclassify the ideological bias of a given docu-ment, little has been done toward understand-ing the nature of this bias on a topical-level.
Inthis paper we address the problem of modelingideological perspective on a topical level usinga factored topic model.
We develop efficientinference algorithms using Collapsed Gibbssampling for posterior inference, and give var-ious evaluations and illustrations of the util-ity of our model on various document collec-tions with promising results.
Finally we give aMetropolis-Hasting inference algorithm for asemi-supervised extension with decent results.1 IntroductionWith the avalanche of user-generated articles overthe web, it is quite important to develop models thatcan recognize the ideological bias behind a givendocument, summarize where this bias is manifestedon a topical level, and provide the user with alter-nate views that would help him/her staying informedabout different perspectives.
In this paper, we fol-low the notion of ideology as defines by Van Dijkin (Dijk, 1998) as ?a set of general abstract beliefscommonly shared by a group of people.?
In otherwords, an ideology is a set of ideas that directs one?sgoals, expectations, and actions.
For instance, free-dom of choice is a general aim that directs the ac-tions of?liberals?, whereas conservation of values isthe parallel for ?conservatives?.We can attribute the lexical variations of the wordcontent of a document to three factors:?
Writer Ideological Belief.
A liberal writermight use words like freedom and choice re-gardless of the topical content of the document.These words define the abstract notion of be-lief held by the writer and its frequency in thedocument largely depends on the writer?s style.?
Topical Content.
This constitutes the mainsource of the lexical variations in a given docu-ment.
For instance, a document about abortionis more likely to have facts related to abortion,health, marriage and relationships.?
Topic-Ideology Interaction.
When a liberalthinker writes about abortion, his/her abstractbeliefs are materialized into a set of concreteopinions and stances, therefore, we might findwords like: pro-choice and feminism.
On thecontrary, a conservative writer might stress is-sues like pro-life, God and faith.Given a collection of ideologically-labeled docu-ments, our goal is to develop a computer model thatfactors the document collection into a representationthat reflects the aforementioned three sources of lex-ical variations.
This representation can then be usedfor:?
Visualization.
By visualizing the abstract no-tion of belief in each ideology, and the wayeach ideology approaches and views main-stream topics, the user can view and contrasteach ideology side-by-side and build the rightmental landscape that acts as the basis forhis/her future decision making.1140?
Classification or Ideology Identification.Given a document, we would like to tell theuser from which side it was written, and ex-plain the ideological bias in the document at atopical level.?
Staying Informed: Getting alternativeviews1.
Given a document written from per-spective A, we would like the model to providethe user with other documents that represent al-ternative views about the same topic addressedin the original document.In this paper, we approach this problem usingTopic Models (Blei et al, 2003).
We introduce afactored topic model that we call multi-view LatentDirichlet Allocation or mview-LDA for short.
Ourmodel views the word content of each document asthe result of the interaction between the document?sidealogical and topical dimensions.
The rest of thispaper is organized as follows.
First, in Section 2,we review related work, and then present our modelin Section 3.
Then in Section 4, we detail a col-lapsed Gibbs sampling algorithm for posterior infer-ence.
Sections 5 and 6 give details about the datasetused in the evaluation and illustrate the capabilitiesof our model using both qualitative and quantitativemeasures.
Section 7 describes and evaluates the ef-ficacy of a semi-supervised extension, and finally inSection 8 we conclude and list several directions forfuture research.2 Related WorkIdeological text is inherently subjective, thus ourwork is related to the growing area of subjectiv-ity analysis(Wiebe et al, 2004; Riloff et al, 2003).The goal of this area of research is to learn to dis-criminate between subjective and objective text.
Incontrast,in modeling ideology, we aim toward con-trasting two or more ideological perspectives each ofwhich is subjective in nature.
Further more, subjec-tive text can be classified into sentiments which gaverise to a surge of work in automatic opinion min-ing (Wiebe et al, 2004; Yu and Hatzivassiloglou,2003; Pang et al, 2002; Turney and Littman, 2003;Popescu and Etzioni, 2005) as well as sentiment1In this paper, we use the words ideology, view, perspectiveinterchangeably to denote the same conceptanalysis and product review mining (Nasukawa andYi, 2003; Hu and Liu, 2004; Pang and Lee, 2008;Branavan et al, 2008; Titov and McDonald, 2008;Titov and McDonald, 2008; Mei et al, 2007; Linget al, 2008).
The research goal of sentiment anal-ysis and classification is to identify language usedto convey positive and negative opinions, which dif-fers from contrasting two ideological perspectives.While ideology can be expressed in the form of asentiment toward a given topic,like abortion, ideo-logical perspectives are reflected in many ways otherthan sentiments as we will illustrate later in the pa-per.
Perhaps more related to this paper is the workof (Fortuna et al, 2008; Lin et al, 2008) whosegoal is to detect bias in news articles via discrimina-tive and generative approaches, respectively.
How-ever, this work still addresses ideology at an abstractlevel as opposed to our approach of modeling ideol-ogy at a topical level.
Finally, independently, (Pauland Girju, 2009) gives a construction similar to ourshowever for a different task 2.3 Multi-View Topic ModelsIn this section we introduce multi-view topic mod-els, or mview-LDA for short.
Our model, mview-LDA, views each document as the result of the in-teraction between its topical and idealogical dimen-sions.
The model seeks to explain lexical variabili-ties in the document by attributing this variabilitiesto one of those dimensions or to their interactions.Topic models, like LDA, define a generative processfor a document collection based on a set of parame-ters.
LDA employs a semantic entity known as topicto drive the generation of the document in question.Each topic is represented by a topic-specific worddistribution which is modeled as a multinomial dis-tribution over words, denoted by Multi(?).
Thegenerative process of LDA proceeds as follows:1.
Draw topic proportions ?d|?
?
Dir(?).2.
For each word(a) Draw a topic zn|?d ?
Mult(?d).
(b) Draw a word wn|zn, ?
?
Multi(?zn).In step 1 each document d samples a topic-mixingvector ?d from a Dirichlet prior.
The component ?d,k2In fact, we only get to know about this related work afterour paper was accepted1141DNvzwVa2 b2KVa1 b1x2x1Variable Meaningw wordv document?s ideologyz topicx1, x2 word switches, one per word (see text)?
document-specific distribution over topics?
document?s expected usage of the ideology?sbackground topic?
ideology?s background-topic?
ideology-independent topic distribution?
ideology-specific topic distribution?
topic bias across ideologyFigure 1: A plate diagram of the graphical model.of this vector defines how likely topic k will appearin document d. For each word in the document wn,a topic indicator zn is sampled from ?d, and thenthe word itself is sampled from a topic-specific worddistribution specified by this indicator.
Thus LDAcan capture and represent lexical variabilities via thecomponents of ?d which represents the topical con-tent of the document.
In the next section we will ex-plain how our new model mview-LDA can captureother sources of lexical variabilities beyond topicalcontent.3.1 Multi-View LDAAs we noted earlier, LDA captures lexical variabili-ties due to topical content via ?d and the set of top-ics ?1:K .
In mview-LDA each document d is taggedwith the ideological view it represents via the ob-served variable vd which takes values in the discreterange: {1, 2, ?
?
?
, V } as shown in Fig.
1.
For sim-plicity, lets first assume that V = 2.
The topics ?1:Kretain the same meaning: a set of K multinomialdistributions each of which represents a given themeor factual topic.
In addition, we utilize an ideology-specific topic ?v which is again a multinomial dis-tribution over the same vocabulary.
?v models theabstract belief shared by all the documents writtenfrom view v. In other words, if v denotes the liberalperspective, then ?v gives high probability to wordslike progressive, choice, etc.
Moreover, we defineda set of K ?
V topics that we refer to as ideology-specific topics.
For example, topic ?v,k representshow ideology v addresses topic k. The generativeprocess of a document d with ideological view vdproceeds as follows:1.
Draw ?d ?
Beta(a1, b1)2.
Draw topic proportions ?d|?
?
Dir(?2).3.
For each word wn(a) Draw xn,1 ?
Bernoulli(?d)(b) If(xn,1 = 1)i.
Draw wn|xn,1 = 1 ?
Multi(?vd)(c) If(xn,1 = 0)i.
Draw zn|?d ?
Mult(?d).ii.
Draw xn,2|vd, zn ?
Bernoulli(?zn)iii.
If(xn,2 = 1)A.
Draw wn|zn, ?
?
Multi(?zn).iv.
If(xn,2 = 0)A.
Draw wn|vd, zn ?
Multi(?vd,zn).In step 1, we draw a document-specific biasedcoin,?d.
The bias of this coin determines the pro-portions of words in the document that are gener-ated from its ideology background topic ?vd .
As inLDA, we draw the document-specific topic propor-tion ?d from a Dirichlet prior.
?d thus controls thelexical variabilities due to topical content inside thedocument.To generate a word wn, we first generate a coinflip xn,1 from the coin ?d.
If it turns head, thenwe proceed to generate this word from the ideology-specific topic associated with the document?s ideo-logical view vd.
In this case, the word is drawn in-dependently of the topical content of the document,and thus accounts for the lexical variation due to theideology associated with the document.
The propor-tion of such words is document-specific by design1142and depends on the writer?s style to a large degree.If xn,1 turns to be tail,we proceed to the next stepand draw a topic-indicator zn.
Now, we have twochoices: either to generate this word directly fromthe ideology-independent portion of the topic ?zn ,or to draw the word from the ideology-specific por-tion ?vd,zn .
The choice here is not document spe-cific, but rather depends on the interaction betweenthe ideology and the specific topic in question.
Ifthe ideology associated with the document holds astrong opinion or view with regard to this topic,then we expect that most of the time we will takethe second choice, and generate wn from ?vd,zn ;and vice versa.
This decision is controlled by theBernoulli variable ?zn .
Therefore, in step c.ii, wefirst generate a coin flip xn,2 from ?zn .
Based onxn,2 we either generate the word from the ideology-independent portion of the topic ?zn , and this con-stitutes how the model accounts for lexical variationdue to the topical content of the document, or gen-erate the word from the ideology-specific portion ofthe topic ?vd,zn , and this specifies how the modelaccounts for lexical variation due to the interactionbetween the topical and ideological dimensions ofthe document.Finally, it is worth mentioning that the decision tomodel ?zn3 at the topic-ideology level rather than atthe document level, as we have done with ?d, stemsfrom our goal to capture ideology-specific behavioron a corpus level rather than capturing document-specific writing style.
However, it is worth mention-ing that if one truly seeks to measure the degree ofbias associated with a given document,then one cancompute the frequency of the event xn,2 = 0 fromposterior samples.
In this case, ?zn acts as the priorbias only.
Moreover, computing the frequency ofthe event xn,2 = 0 and zn = k gives the document?sbias toward topic k per se.Finally, it is worth mentioning that all multino-mial topics in the model: ?,?, ?
are generated oncefor the whole collection from a symmetric Dirichletprior, similarly, all bias variables, ?1:K are sampledfrom a Beta distribution also once at the beginningof the generative process.3In an earlier version of the work we modeled ?
on a per-ideology basis, however, we found that using a single shared ?results in more robust results4 Posterior Inference Via Collapsed GibbsSamplingThe main tasks can be summarized as follows:?
Learning: Given a collection of documents,find a point estimate of the model parameters(i.e.
?,?, ?, ?,etc.).?
Inference: Given a new document, and a pointestimate of the model parameters, find the pos-terior distribution of the latent variables associ-ated with the document at hand:(?d, {xn,1}, {zn}, {xn,2}).Under a hierarchical Bayesian setting, like the ap-proach we took in this paper, both of these tasks canbe handled via posterior inference.
Under the gener-ative process, and hyperparmaters choices, outlinedin section 3, we seek to compute:P (d1:D, ?1:K ,?1:V , ?1:V,1:K, ?1:K |?, a, b,w,v),where d is a shorthand for the hidden variables(?d, ?d, z,x1,x2) in document d. The above poste-rior probability is unfortunately intractable,and weapproximate it via a collapsed Gibbs sampling pro-cedure (Griffiths and Steyvers, 2004; Gelman et al,2003) by integrating out, i.e.
collapsing, the fol-lowing hidden variables: the topic-mixing vectors?d and the ideology bias ?d for each document, aswell as all the multinomial topic distributions: (?,?and ?)
in addition to the ideology-topic biases givenby the set of ?
random variables.Therefore, the state of the sampler at each itera-tion contains only the following topic indicators andcoin flips for each document:(z,x1,x2).
We alter-nate sampling each of these variables conditioned onits Markov blanket until convergence.
At conver-gence, we can calculate expected values for all theparameters that were integrated out, especially forthe topic distributions, for each document?s latentrepresentation (mixing-vector) and for all coin bi-ases.
To ease the calculation of the Gibbs samplingupdate equations we keep a set of sufficient statistics(SS) in the form of co-occurrence counts and summatrices of the form CEQeq to denote the number oftimes instance e appeared with instance q.
For ex-ample, CWKwk gives the number of times word w wassampled from the ideology-independent portion of1143topic k. Moreover, we follow the standard practiceof using the subscript ?i to denote the same quan-tity it is added to without the contribution of itemi.
For example,CWKwk,?i is the same as CWKwk with-out the contribution of word wi.
For simplicity, wemight drop dependencies on the document wheneverthe meaning is implicit form the context.For word wn in document d, instead of samplingzn, xn,1, xn,2 independently, we sample them as ablock as follows:P (xn,1 = 1|wn = w, vd = v) ?
(CDX1d1,?n + a1)?CVWvw,?n + ?1?w?
(CVWvw?,?n + ?1)P (xn,1 = 0, x2,n = 1, zn = k|wn = w, vd = v)?
(CDX1d0,?n + b1)?CKX2k1,?n + a2CKX2k1,?n + CKX2k0,?n + a2 + b2?CKWkw,?n + ?1?w?
(CKWkw?,?n + ?1)?CDKdk,?n + ?2?k?
(CDKdk?,?n + ?2)P (xn,1 = 0, x2,n = 0, zn = k|wn = w, vd = v)?
(CDX1d0,?n + b1)?CKX2k0,?n + b2CKX2k1,?n + CKX2k0,?n + a2 + b2?CV KWvkw,?n + ?1?w?
(CV KWvkw?,?n + ?1)?CDKdk,?n + ?2?k?
(CDKdk?,?n + ?2)The above three equations can be normalized toform a 2 ?
K + 1 multinomial distribution: onecomponent for generating a word from the ideol-ogy topic, K components for generating the wordfrom the ideology-independent portion of topic k =1, ?
?
?
,K, and finally K components for generat-ing the word from the ideology-specific portion oftopic k = 1, ?
?
?
,K. Each of these 2 ?
K + 1cases corresponds to a unique assignment of thevariables zn, xn,1, xn,2.
Therefore, our Gibbs sam-pler just repeatedly draws sample from this 2?K+1-components multinomial distribution until conver-gence.
Upon convergence, we compute point es-timates for all the collapsed variables by a simplemarginalization of the appropriate count matrices.During inference, we hold the corpus-level countmatrices fixed, and keep sampling from the above2?K+1-component multinomial while only chang-ing the document-level count matrices: CDK , CDX1until convergence.
Upon convergence, we computeestimates for ?d and ?d by normalizing CDK andCDX1 (or possibly averaging this quantity acrossposterior samples).
As we mentioned in Section 3,to compute the ideology-bias in addressing a giventopic say k in a given document, say d, we can sim-ply compute the expected value of the event xn,2 =0 and zn = k across posterior samples.5 Data SetsWe evaluated our model over three datasets: the bit-terlemons croups and a two political blog-data set.Below we give details of each dataset.5.1 The Bitterlemons datasetThe bitterlemons corpus consists ofthe articles published on the websitehttp://bitterlemons.org/.
The websiteis set up to contribute to mutual understandingbetween Palestinians and Israelis through theopen exchange of ideas.
Every week, an issueabout the Israeli-Palestinian conflict is selected fordiscussion, and a Palestinian editor and an Israelieditor contribute one article each addressing theissue.
In addition, the Israeli and Palestinian editorsinvite one Israeli and one Palestinian to expresstheir views on the issue.
The data was collectedand pre-proceed as describes in (Lin et al, 2008).Overall, the dataset contains 297 documents writtenfrom the Israeli?s point of view, and 297 documentswritten from the Palestinian?s point of view.
Onaverage each document contains around 740 words.After trimming words appearing less than 5 times,we ended up with a vocabulary size of 4100 words.We split the dataset randomly and used 80% of thedocuments for training and the rest for testing.5.2 The Political Blog DatasetsThe first dataset refereed to as Blog-1 is a subsetof the data collected and processed in (Yano et al,2009).
The authors in (Yano et al, 2009) collectedblog posts from blog sites focusing on Americanpolitics during the period November 2007 to Oc-tober 2008.
We selected three blog sites from thisdataset: the Right Wing News (right-ideology) ;the Carpetbagger, and Daily Kos as representatives1144palest inianis raelipeaceyearpolit icalproces sstateendrig htg overnmentneedconflictways ecurit ypalest inianis raeliPeacepolit icaloccupationproces sends ecurit yconflictwayg overnmentpeoplet ime yearforceneg ot iationbush US pres ident  americansharon administ ration primes ett lem ent  pres s ure policywashingt on ariel new middleunit  state american georg epowell minister colin visitinternal policy  statementexpres s  pro previous  packagework t ransfer europeanadminist ration receivearafat state leader roadmapg eorg e elect ion mont h iraqweek peace june realist icyasir s enior involvementclint on november postmandate terroris mUS  ro leIsraeli Viewroadmap phase violences ecurit y  ceasefire state planinternational step implementauthorit y  final quartet  is s uemap effortroadmap end s ett lem entimplem entation obligationst op expansion commit mentcons olidate fulfill unit  illegalpres ent  previou assassinationmee t  forward negative calmproces s  force terroris m unitroad demand provideconfidence elem ent  interimdis cus s ion want union s ucceepoint  build pos it ive recog nizepres ent  t imetableRoadmap pro c essis rael sy ria syrian neg ot iatelebanon deal conferenceconces s ion asad agreementreg ional oct ober init iativerelations hipt rack neg ot iation officialleaders hip pos it ionwithdrawal time victorypres ent  s econd standcircumstance repres ent  s ens etalk st rateg y  is s ue participantparti neg ot iatorpeace st rateg ic plo hizballahis lamic neig hbor territ orialradical iran relation t hinkobviou count ri mandateg reater conventional int ifadaaffect  jihad timeArab InvolvementPalestinian  ViewIsraeliBackgro undto picPalestinianBackgro undto picFigure 2: Illustrating the big picture overview over the bitterlemons dataset using few topics.
Each box lists the topwords in the corresponding multinomial topic distribution.
See text for more detailsof the liberal view (left-ideology).
After trimmingshort posts of less than 20 words, we ended up with2040 posts distributed as 1400 from the left-wingand the rest from the right-wing.
On average, eachpost contains around 100 words and the total size ofthe vocabulary is 14276 words.
For this dataset, wefollowed the train-test split in (Yano et al, 2009).In this split each blog is represented in both train-ing and test sets.
Thus this dataset does not measurethe model?s ability to generalize to a totally differentwriting style.The second dataset refereed to as Blog-2 is sim-ilar to Blog-1 in its topical content and time framebut larger in its blog coverage (Eisenstein and Xing,2010).
Blog-2 spans 6 blogs: three from the left-wing and three from the right-wing.
The datasetcontains 13246 posts.
After removing words thatappear less then 20 times, the total vocabulary be-comes 13236 with an average of 200 words per post.We used 4 blogs (2 from each view) for trainingand held two blogs (one from each view) for test-ing.
Thus this dataset measures the model?s abilityto generalize to a totally new blog.6 Experimental ResultsIn this section we gave various qualitative and quan-titative evaluations of our model over the datasetslisted in Section 5.
For all experiments, we set?1 = .01, ?2 = .1, a = 1 and b = 1.
We run Gibbssampling during training for 1000 iterations.
Duringinference, we ran Gibbs sampling for 300 iterations,and took 10 samples, with 50-iterations lag, for eval-uations.6.1 Visualization and BrowsingOne advantage of our approach is its ability to createa ?big-picture?
overview of the interaction betweenideology and topics.
In figure 2 we show a portion ofthat diagram over the bitterlemons dataset.
First notehow the ideology-specific topics in both ideologyshare the top-three words, which highlights that thetwo ideologies seek peace even though they still bothdisagree on other issues.
The figure gives exampleof three topics: the US role, the Roadmap peaceprocess, and the Arab involvement in the conflict1145(the name of these topics were hand-crafted).
Foreach topic, we display the top words in the ideology-independent part of the topic (?
), along with topwords in each ideology?s view of the topic (?
).For example, when discussing the roadmap pro-cess, the Palestinian view brings the following is-sues: [the Israeli side should] implement the oblig-atory points in this agreement, stop expansion ofsettlements, and move forward to the commitmentsbrought by this process.
On the other hand, the Is-raeli side brings the following points: [Israelis] needto build confidence [with Palestinian], address therole of terrorism on the implementation of the pro-cess, and ask for a positive recognition of Israelfrom the different Palestinian political parties.
Aswe can see, the ideology-specific portion of the topicneedn?t always represent a sentiment shared by itsmembers toward a given topic, but it might ratherincludes extra important dimensions that need to betaken into consideration when addressing this topic.Another interesting topic addresses the involve-ment of the neighboring Arab countries in the con-flict.
From the Israeli point of view, Israel is worriedabout the existence of hizballah [in lebanon] and itsrelationship with radical Iran, and how this mightaffect the Palestinian-uprising (Intifada) and Jihad.From the other side, the Palestinians think that theArab neighbors need to be involved in the peace pro-cess and negotiations as some of these countries likeSyria and Lebanon are involved in the conflict.The user can use the above chart as an entry pointto retrieve various documents pertinent to a giventopic or to a given view over a specific topic.
Forinstance, if the user asks for a representative sam-ple of the Israeli(Palestinian) view with regard to theroadmap process, the system can first retrieve docu-ments tagged with the Israeli(Palestinian) view andhaving a high topical value in their latent representa-tion ?
over this topic.
Finally, the system then sortsthese documents by how much bias they show overthis topic.
As we discussed in Section 4, this can bedone by computing the expected value of the eventxn,2 = 0 and zn = k where k is the topic underconsideration.6.2 ClassificationWe have also performed a classification task overall the datasets.
The Scenario proceeded as follows.We train a model over the training data with variousnumber of topics.
Then given a test document, wepredict its ideology using the following equation:vd = argmaxv?V P (wd|v); (1)We use three baselines.
The first baselineis an SVM classifier trained on the normalizedword frequency of each document.
We trainedSVM using a regularization parameter in the range{1, 10, 20, ?
?
?
, 100} and report the best result (i.e.no cross-validation was performed).
The othertwo are supervised LDA models: supervised LDA(sLDA) (Wang et.
al., 2009; Blei and McCauliffe,2007) and discLDA (Lacoste-Julien et al, 2008).discLDA is a conditional model that divides theavailable number of topics into class-specific top-ics and shared-topics.
Since the code is not publiclyavailable, we followed the same strategy in the orig-inal paper and share 0.1K topics across ideologiesand then divide the rest of the topics between ide-ologies4.
However, unlike our model, there are nointernal relationships between these two sets of top-ics.
The decision rule employed by discLDA is verysimilar to the one we used for mview-LDA in Eq(1).
For sLDA, we used the publicly available codeby the authors.As shown in Figure 3, our model performs betterthan the baselines over the three datasets.
We shouldnote from this figure that mview-LDA peaks at asmall number of topics, however, each topic is repre-sented by three multinomials.
Moreover, it is evidentfrom the figure that the experiment over the blog-2 dataset which measures each model?s ability togeneralizes to a totally unseen new blog is a hardertask than generalizing to unseen posts form the sameblog.
However, our model still performs competi-tively with the SVM baseline.
We believe that sep-arating each topic into an ideology-independent partand ideology-specific part is the key behind this per-formance, as it is expected that the new blogs wouldstill share much of the ideology-independent partsof the topics and hopefully would use similar (but4(Lacoste-Julien et al, 2008) gave an optimization algo-rithm for learning the topic structure (transformation matrix),however since the code is not available, we resorted to one ofthe fixed splitting strategies mentioned in the paper.
We triedother splits but this one gives the best results1146(a) (b) (c)Figure 3: Classification accuracy over the Bitterlemons dataset in (a) and over the two blog datasets in (b) and (c).
For SVM wegive the best result obtained across a wide range of the SVM?s regularization parameter(not the cross-validation result).no necessarily all) words from the ideology-specificparts of each topic when addressing this topic.Finally, it should be noted that the bitterlemonsdataset is a multi-author dataset and thus the modelswere tested on some authors that were not seen dur-ing training, however, two factors contributed to thegood performance by all models over this dataset.The first being the larger size of each document (740words per document as compared to 200 words perpost in blog-2) and the second being the more formalwriting style in the bitterlemons dataset.6.3 An Ablation StudyTo understand the contribution of each component ofour model, we conducted an ablation study over thebitterlemons dataset.
In this experiment we turned-off one feature of our model at a time and mea-sured the classification performance.
The results areshown in Figure 4.
Full, refers to the full model; No-?
refers to a model in which the ideology-specificbackground topic ?
is turned-off; and No-?
refersto a model in which the ideology-specific portions ofthe topics are turned-off.
As evident from the figure,?
is more important to the model than ?
and the dif-ference in performance between the full model andthe No-?
model is rather significant.
In fact without?
the model has little power to discriminate betweenideologies beyond the ideology-specific backgroundtopic ?.6.4 Retrieval: Getting the Other ViewsTo evaluate the ability of our model in finding al-ternative views toward a given topic, we conductedthe following experiment over the Bitterlemons cor-pus.
In this corpus each document is associated witha meta-topic that highlights the issues addressed inthis document like: ?A possible Jordanian role?,Figure 4: An Ablation study over the bitterlemons dataset.
?Demography and the conflict?,etc.
There are a to-tal of 148 meta-topics.
These topics were not usedin fitting our model but we use them in the evalu-ation as follows.
We divided the dataset into 60%for training and 40% for testing.
We trained mview-LDA over the training set, and then used the learnedmodel to infer the latent representation of the testdocuments as well as their ideologies.
We then usedeach document in the training set as a query to re-trieve documents from the test set that address thesame meta-topic in the query document but from theother-side?s perspective.
Note that we have access tothe view of the query document but not the view ofthe test document.
Moreover, the value of the meta-topic is only used to construct the ground-truth resultof each query over the test set.
In addition to mview-LDA, we also implemented a strong baseline usingSVM+Dirichlet smoothing that we will refer to asLM.
In this baseline, we build an SVM classifierover the training set, and use Dirichlet-smoothingto represent each document (in test and training set)as a multinomial-distribution over the vocabulary.Given a query document d, we rank documents in1147Figure 5: Evaluating the performance of the view-Retrieval task.
Figure compare performance between mview-LD vs. an SVM+asmoothed language model approach using three measures: average rank, best rank and rank at full recall.
( Lower better).the test set by each model as follows:?
mview-LDA: we computed the cosine-distancebetween ?mv?LDA?sharedd and ?mv?LDA?sharedd?weighted by the probability that d?
is writtenfrom a different view than vd.
The latterquantity can be computed by normaliz-ing P (v|d?).
Moreover, ?mv?LDA?sharedd,k ?
?n I[(xn,1 = 0) and (xn,2 = 1) and (zn = k)],and n ranges over words in document d. In-tuitively, we would like ?mv?LDA?sharedd toreflect variation due to the topical content, butnot ideological view of the document.?
LM: For a document d?, we apply the SVMclassifier to get P (v|d?
), then we measure sim-ilarity by computing the cosine-distance be-tween the smoothed multinomial-distributionof d and d?.
We combine these two componentsas in mview-LDA.Finally we rank documents in the test set in adescending-order and evaluate the resulting rank-ing using three measures: the rank at full recall(lowest rank), average rank, and best rank of theground-truth documents as they appear in the pre-dicted ranking.
Figure 5 shows the results across anumber of topics.
From this figure, it is clear thatour model outperforms this baseline over all mea-sures.
It should be noted that this is a very hardtask since the meta-topics are very fine-grained like:Settlements revisited, The status of the settlements,Is the roadmap still relevant?,The ceasefire and theroadmap: a progress report,etc.
We did not attemptto cluster these meta-topics since our goal is just tocompare our model against the baseline.7 A Semi-Supervised ExtensionIn this section we present and assess the efficacy ofa semi-supervised extension of mview-LDA.
In thissetting, the model is given a set of ideologically-labeled documents and a set of unlabeled docu-ments.
One of the key advantages of using a prob-abilistic graphical model is the ability to deal withhidden variables in a principled way.
Thus the onlychange needed in this case is adding a single step inthe sampling algorithm to sample the ideology v ofan unlabeled document as follows:P (vd = v|rest) ?
P (wd|vd = v, zd,x1,d,x2,d)Note that the probability of the indicators(x1,d,x2,d, zd) do not depend on the view of thedocument and thus got absorbed in the normaliza-tion constant, and thus one only needs to measuresthe likelihood of generating the words in the doc-ument under the view v. We divide the wordsinto three groups: Ad = {wn|xn,1 = 1} is theset of words generated from the view-backgroundtopic, Bd,k = {wn|zn = k, xn,1 = 0, xn,2 =1} is the set of words generated from ?k, andCd,k = {wn|zn = k, xn,1 = 0, xn,2 = 0} is theset of words generated from ?k,v.
The probabil-ity of Bd,k does not depend on the value of v andthus can be absorbed into the normalization factor.Therefore, we only need to compute the followingprobability:P (Ad, Cd,1:K |vd = v, rest)=?k?
?k,vP (Cd,k|?k,v, rest)p(?k,v|rest)d?k,v??
?P (Ad|?, rest)p(?|rest)d?
(2)1148All the integrals in (2) reduce to the ratio of twolog partition functions.
For example, the product ofintegrals containing Cd,k reduce to:?k?w ?
(CDKW,X2=0dkw + CV KWvkw,?d + ?1)?
(?w[CDKW,X2=0dkw + CV KWvkw,?d + ?1])??
(?w[CV KWvkw + ?1])?w ?
(CV KWvkw,?d + ?1) (3)Unfortunately, the above scheme does not mixwell because the value of the integrals in (2) arevery low for any view other than the view of thedocument in the current state of the sampler.
Thishappens because of the tight coupling between vdand the indicators (x1,x2, z).
To remedy this prob-lem we used a Metropolis-Hasting step to sample(vd,x1,x2, z) jointly.
We construct a set of V pro-posals each of which is indexed by a possible view:qv(x1,x2, z) = P (x1,x2, z|vd = v,wd).
Sincewe have a collection of proposal distributions, weselect one of them at random at each step.
Togenerate a sample from qv?
(), we run a few it-erations of a restricted Gibbs scan over the docu-ment d conditioned on fixing vd = v?
and thentake the last sample jointly with v?
as our pro-posed new state.
With probability min(r,1), the newstate (v?,x1?,x2?, z?)
is accepted, otherwise theold state is retained.
The acceptance ratio,r, is com-puted as: r = p(wd|v?,x1?,x2?,z?
)p(wd|v,x1,x2,z) , where the non-*variables represent the current state of the sampler.It is interesting to note that the above acceptance ra-tio is equivalent to a likelihood ratio test.
We com-pute the marginal probability P (wd|..) using theestimated-theta method (Wallach et al, 2009).We evaluated the semi-supervised extension usingthe blog-2 dataset as follows.
We reveal R% of thelabels in the training set; then we train mview-LDAonly over the labeled portion and train the semi-supervised version (ss-mview-LDA) on both the la-beled and unlabeled documents.
Finally we evaluatethe classification performance on the test set.
Weused R = {20, 40, 80}.
The results are given in Ta-ble 1 which shows a decent improvement over thesupervised mview-LDA.R mview-LDA ss-mview-LDA80 65.60 66.4160 62.31 65.4320 60.87 63.25Table 1: Classification performance of the semi-supervised model.
R is the ratio of labeled documents.8 Discussion and Future WorkIn this paper, we addressed the problem of model-ing ideological perspective at a topical level.
Wedeveloped a factored topic model that we calledmultiView-LDA or mview-LDA for short.
mview-LDA factors a document collection into three setof topics: ideology-specific, topic-specific, andideology-topic ones.
We showed that the resultingrepresentation can be used to give a bird-eyes?
viewto where each ideology stands with regard to main-stream topics.
Moreover, we illustrated how the la-tent structure induced by the model can by used toperform bias-detection at the document and topiclevel, and retrieve documents that represent alterna-tive views.It is important to mention that our model inducesa hierarchical structure over the topics, and thus itis interesting to contrast it with hierarchical topicmodels like hLDA (Blei et al, 2003) and PAM (Liand McCallum, 2006; Mimno et al, 2007).
First,these models are unsupervised in nature, while ourmodel is supervised.
Second, the semantic of thehierarchical structure in our model is different thanthe one induced by those models since documents inour model are constrained to use a specific portionof the topic structure while in those models docu-ments can freely sample words from any topic.
Fi-nally,in the future we plan to extend our model toperform joint modeling and summarization of ide-alogical discourse.9 AcknowledgmentWe thank Jacob Eisenstein, John Lafferty, TomMitchell, and the anonymous reviewers for theirhelpful comments and suggestions.
This work issupported in part by grants NSF IIS- 0713379,NSF DBI-0546594 career award to EPX, ONRN000140910758, DARPA NBCH1080007, andAFOSR FA9550010247.
EPX is supported by anAlfred P. Sloan Research Fellowship.1149ReferencesJ.
Wiebe, T. Wilson, R.Bruce, M. Bell, and M. Martin.Learning subjective language.
Computational Linguis-tics, 30(3), 2004.H.
Yu and V. Hatzivassiloglou.
Towards answering opin-ion questions: Separating facts from opinions andidentifying the polarity of opinion sentences.
In Pro-ceedings of EMNLP-2003B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machine learningtechniques.
In Proceedings of EMNLP-2002.P.
Turney and M. Littman.
Measuring praise and criti-cism: Inference of semantic orientation from associa-tion.
ACM TOIS, 21(4):315346, 2003A.
Popescu and O. Etzioni.
Extracting product fea-tures and opinions from reviews.
In Proceedings ofHLT/EMNLP-2005, pages 339346, 2005.T.
Nasukawa and J. Yi.
Sentiment analysis: Capturingfavorability using natural language processing.
In Pro-ceedings of K-CAP, 2003.M.
Hu and B. Liu.
Mining and summarizing customerreviews.
In Proceedings of KDD, 2004.B.
Pang and L. Lee.
Opinion mining and sentiment anal-ysis.
Foundations and Trends in Information Retrieval,2(12), 1135, 2008.S.
Branavan, H. Chen, J. Eisenstein and R. Barzilay.Learning Document-Level Semantic Properties fromFree-text Annotations, Proceedings of ACL, 2008.I.
Titov and R. McDonald.
Modeling Online Reviewswith Multi-Grain Topic Models International WorldWide Web Conference (WWW), 2008.I.
Titov and R. McDonald.
A Joint Model of Text andAspect Ratings for Sentiment Summarization Associ-ation for Computational Linguistics (ACL), 2008.Q.
Mei, X. Ling, M. Wondra, H. Su, ChengXiang Zhai.Topic Sentiment Mixture: Modeling Facets and Opin-ions in Weblogs, Proceedings of the 16th InternationalWorld Wide Web Conference (WWW), pages 171-180, 2007.X.
Ling, Q. Mei, C. Zhai, B. Schatz.
Mining Multi-Faceted Overviews of Arbitrary Topics in a Text Col-lection, Proceedings of the 15th ACM SIGKDD In-ternational Conference on Knowledge Discovery andData Mining (KDD?
08), pages 497-505, 2008B.
Fortuna , C. Galleguillos, N. Cristianini.
Detectingthe bias in media with statistical learning methods.In: Text Mining: Theory and Applications.
Taylor andFrancis Publisher,2008.W.
Lin, E.P.
Xing, and A. Hauptmann.
A Joint Topic andPerspective Model for Ideological Discourse Euro-pean Conference on Machine Learning and Principlesand Practice of Knowledge Discovery in Databases(ECML/PKDD), 2008.T.
A.
Van Dijk.
Ideology: A multidisciplinary approach.Sage Publications, 1998.T.
Griffiths, M. Steyvers Finding scientific topics.PNAS,101:5228-5235, 2004.A.
Gelman, J. Carlin, Hal Stern, and Donald Ru-bin.
Bayesian Data Analysis, Chapman-Hall, 2 edi-tion,2003.D.
Blei, A. Ng, and M. Jordan.
Latent Dirichlet allocation.
Journal of Machine Learning Research,3:9931022, January 2003.T.
Yano, W. W. Cohen, and N. A. Smith.
PredictingResponse to Political Blog Posts with Topic Models.NAACL-HLT 2009, Boulder, CO, MayJune 2009J.
Eisenstein and E.P.
Xing.The CMU-2008 PoliticalBlog Corpus.
CMU-ML-10-101 Technical Report,2010.C.
Wang, D. Blei and L. Fei-Fei.
Simultaneous imageclassification and annotation.
CVPR, 2010.D.
Blei and J. McAuliffe.
Supervised topic models.
NIPS,2007.S.
Lacoste-Julien, F. Sha, and M. Jordan.
DiscLDA:Discriminative Learning for Dimensionality Reduc-tion and Classification.
Neural Information Process-ing Systems Conference (NIPS08), Vancouver, BritishColumbia, December 2008.E.
Riloff, J. Wiebe, and T. Wilson.
Learning subjectivenouns using extraction pattern bootstrapping.
In Pro-ceedings of CoNLL-2003.D.
Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.
Hier-archical topic models and the nested Chinese restau-rant process.
In Neural Information Processing Sys-tems (NIPS)16, 2003.D.
Mimno, W. Li and A. McCallum.
Mixtures of Hier-archical Topics with Pachinko Allocation.
In Interna-tional Conference of Machine Learning, ICML, 2007.W.
Li, and A. McCallum.
Pachinko Allocation: DAG-structured Mixture Models of Topic Correlations.
InInternational Conference of Machine Learning, ICML,2006.M.
Paul and R. Girju.
Cross-cultural Analysis of Blogsand Forums with Mixed-Collection Topic Models.EMNLP 2009.H.
Wallach, I. Murray, R. Salakhutdinov, and D. Mimno.Evaluation Methods for Topic Models.
ICML 2009.1150
