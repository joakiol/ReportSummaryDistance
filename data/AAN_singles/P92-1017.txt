INS IDE-OUTSIDE REEST IMATION FROM PARTIALLYBRACKETED CORPORAFernando Pereira2D-447, AT~zT Bell LaboratoriesPO Box 636, 600 Mountain AveMurray Hill, NJ 07974-0636pereira@research, art.
comYves SchabesDept.
of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104-6389schabes@una~i, c i s .
upenn, eduABSTRACTThe inside-outside algorithm for inferring the pa-rameters of a stochastic ontext-free grammar isextended to take advantage of constituent in-formation (constituent bracketing) in a partiallyparsed corpus.
Experiments on formal and natu-ral language parsed corpora show that the new al-gorithm can achieve faster convergence and bettermodeling of hierarchical structure than the origi-nal one.
In particular, over 90% test set bracket-ing accuracy was achieved for grammars inferredby our algorithm from a training set of hand-parsed part-of-speech strings for sentences in theAir Travel Information System spoken languagecorpus.
Finally, the new algorithm has better timecomplexity than the original one when sufficientbracketing is provided.1.
MOTIVAT IONThe most successful stochastic language modelshave been based on finite-state descriptions suchas n-grams or hidden Markov models (HMMs)(Jelinek et al, 1992).
However, finite-state mod-els cannot represent the hierarchical structure ofnatural anguage and are thus ill-suited to tasksin which that structure is essential, such as lan-guage understanding or translation.
It is thennatural to consider stochastic versions of morepowerful grammar formalisms and their gram-matical inference problems.
For instance, Baker(1979) generalized the parameter stimation meth-ods for HMMs to stochastic ontext-free gram-mars (SCFGs) (Booth, 1969) as the inside-outsidealgorithm.
Unfortunately, the application ofSCFGs and the original inside-outside algorithmto natural-language modeling has been so far in-conclusive (Lari and Young, 1990; Jelinek et al,1990; Lari and Young, 1991).Several reasons can be adduced for the difficul-ties.
First, each iteration of the inside-outside al-gorithm on a grammar with n nonterminals mayrequire O(n3\[wl 3) time per training sentence w,128while each iteration of its finite-state counterparttraining an HMM with s states requires at worstO(s2lwl) time per training sentence.
That com-plexity makes the training of suff?ciently largegrammars computationally impractical.Second, the convergence properties of the algo-rithm sharply deteriorate as the number of non-terminal symbols increases.
This fact can be intu-itively understood by observing that the algorithmsearches for the maximum of a function whosenumber of local maxima grows with the number ofnonterminals.
Finally, while SCFGs do provide ahierarchical model of the language, that structureis undetermined by raw text and only by chancewill the inferred grammar agree with qualitativelinguistic judgments of sentence structure.
For ex-ample, since in English texts pronouns are verylikely to immediately precede a verb, a grammarinferred from raw text will tend to make a con-stituent of a subject pronoun and the followingverb.We describe here an extension of the inside-outsidealgorithm that infers the parameters of a stochas-tic context-free grammar from a partially parsedcorpus, thus providing a tighter connection be-tween the hierarchical structure of the inferredSCFG and that of the training corpus.
The al-gorithm takes advantage of whatever constituentinformation is provided by the training corpusbracketing, ranging from a complete constituentanalysis of the training sentences to the unparsedcorpus used for the original inside-outside algo-rithm.
In the latter case, the new algorithm re-duces to the original one.Using a partially parsed corpus has several advan-tages.
First, the the result grammars yield con-stituent boundaries that cannot be inferred fromraw text.
In addition, the number of iterationsneeded to reach a good grammar can be reduced;in extreme cases, a good solution is found fromparsed text but not from raw text.
Finally, thenew algorithm has better time complexity whensufficient bracketing information is provided.2.
PART IALLY  BRACKETEDTEXTInformally, a partially bracketed corpus is a setof sentences annotated with parentheses markingconstituent boundaries that any analysis of thecorpus should respect.
More precisely, we startfrom a corpus C consisting of bracketed strings,which are pairs e = (w,B) where w is a stringand B is a bracketing of w. For convenience, wewill define the length of the bracketed string c byIcl = Iwl.Given a string w = wl ..-WlM, a span of w is apair of integers ( i , j )  with 0 < i < j g \[w\[, whichdelimits a substring iwj = wi+y ...wj of w. Theabbreviation iw will stand for iWl~ I.A bracketing B of a string w is a finite set of spanson w (that is, a finite set of pairs or integers (i, j )with 0 g i < j < \[w\[) satisfying a consistencycondition that ensures that each span (i, j) can beseen as delimiting a string iwj consisting of a se-quence of one of more.
The consistency conditionis simply that no two spans in a bracketing mayoverlap, where two spans (i, j)  and (k, l) overlap ifeither i < k < j < l or k < i < l < j.Two bracketings of the same string are said to becompatible if their union is consistent.
A span s isvalid for a bracketing B if {s} is compatible withB.Note that there is no requirement that a bracket-ing of w describe fully a constituent structure ofw.
In fact, some or all sentences in a corpus mayhave empty bracketings, in which case the new al-gorithm behaves like the original one.To present he notion of compatibility between aderivation and a bracketed string, we need firstto define the span of a symbol occurrence in acontext-free derivation.
Let (w,B) be a brack-eted string, and c~0 ==~ al  :=?, .
.
.
=~ c~m = w bea derivation of w for (S)CFG G. The span of asymbol occurrence in (~1 is defined inductively asfollows:?
I f j  -- m, c U = w E E*, and the span of wi in~j is ( i -  1, i).?
I f  j < m, then aj  : flAT, aj+l =/3XI"'Xk')', where A -* X I " .Xk  is a ruleof G. Then the span of A in aj  is ( i l , jk) ,where for each 1 < l < k, (iz,jt) is the spanof Xl in a j+l -  The spans in (~j of the symboloccurrences in/3 and 7 are the same as thoseof the corresponding symbols in ~j+l.A derivation of w is then compatible with a brack-eting B of w if the span of every symbol occurrencein the derivation is valid in B.3.
GRAMMAR REEST IMATIONThe inside-outside algorithm (Baker, 1979) is areestimation procedure for the rule probabilitiesof a Chomsky normal-form (CNF) SCFG.
It takesas inputs an initial CNF SCFG and a training cor-pus of sentences and it iteratively reestimates ruleprobabilities to maximize the probabil ity that thegrammar used as a stochastic generator would pro-duce the corpus.A reestimation algorithm can be used both to re-fine the parameter estimates for a CNF SCFG de-rived by other means (Fujisaki et hi., 1989) or toinfer a grammar from scratch.
In the latter case,the initial grammar for the inside-outside algo-r ithm consists of all possible CNF rules over givensets N of nonterrninals and E of terminals, withsuitably assigned nonzero probabilities.
In whatfollows, we will take N, ~ as fixed, n - IN\[, t =\[El, and assume enumerations N - {A1,.
.
.
,An}and E = {hi , .
.
.
,bt}, with A1 the grammar startsymbol.
A CNF SCFG over N,  E can then bespecified by the n~+ nt probabilities Bp,q,r of eachpossible binary rule Ap --* Aq Ar and Up,m of eachpossible unary rule Ap --* bin.
Since for each p theparameters Bp,q,r and Up,rn are supposed to be theprobabilities of different ways of expanding Ap, wemust have for all 1 _< p _< nE Bp,q,r + E Up,m = 1 (7)q, r  mFor grammar inference, we give random initial val-ues to the parameters Bp,q,r and Up,m subject tothe constraints (7).The intended meaning of rule probabilities in aSCFG is directly tied to the intuition of context-freeness: a derivation is assigned a probabil itywhich is the product of the probabilities of therules used in each step of the derivation.
Context-freeness together with the commutativ ity of mul-tiplication thus allow us to identify all derivationsassociated to the same parse tree, and we will129I~( i -  1,i) =I~(i, k) =O~(O, lel) =O~(i,k) =^~,q j r  "--pc  - -P;=Up,m where c = (w, B) and bm= wie(i, k) ~ ~ B,.,.,g(i,i)1,~(.~, k)q,r i< j<k1 i fp=l0 othe~ise.?
~-1 I d~(~,k) ~ (~ O;(j,k)~(~,OB,.,~, + ~ OI(i,jlB,~.d~(k,~)),~,r \ j=o  ~=k+1I -f; ~ B,.,.,g(~,j)~:(j,k)O~(~,k),ec o_</<,f<k<i=,tZ:g/e"cEC1 c ?
E U,,mO;(,-?~c l<i<ld,.=(,.,B),,~,=b..EP;/P"?ECIf(0, Id)I;(i,j)O~(i,j)o_<i<./__.ld(1)(2)(s)(41(5)(6)Table I: Bracketed Reestimationspeak indifferently below of derivation and anal-ysis (parse tree) probabilities.
Finally, the proba-bility of a sentence or sentential form is the sumof the probabilities of all its analyses (equivalently,the sum of the probabilities of all of its leftmostderivations from the start symbol).3.1.
The Inside-Outside AlgorithmThe basic idea of the inside-outside algorithm isto use the current rule probabilities and the train-ing set W to estimate the expected frequencies ofcertain types of derivation step, and then computenew rule probability estimates as appropriate ra-tios of those expected frequency estimates.
Sincethese are most conveniently expressed as relativefrequencies, they are a bit loosely referred to asinside and outside probabilities.
More precisely,for each w E W, the inside probability I~ (i, j) es-timates the likelihood that Ap derives iwj, whilethe outside probability O~(i, j) estimates the like-lihood of deriving sentential form owi Ap j w fromthe start symbol A1.1303.2.
The Extended AlgorithmIn adapting the inside-outside algorithm to par-tially bracketed training text, we must take intoaccount the constraints that the bracketing im-poses on possible derivations, and thus on possi-ble phrases.
Clearly, nonzero values for I~(i,j)or O~(i,j) should only be allowed if iwj is com-patible with the bracketing of w, or, equivalently,if ( i , j )  is valid for the bracketing of w. There-fore, we will in the following assume a corpus C ofbracketed strings c = (w, B), and will modify thestandard formulas for the inside and outside prob-abilities and rule probability reestimation (Baker,1979; Lari and Young, 1990; Jelinek et al, 1990)to involve only constituents whose spans are com-patible with string bracketings.
For this purpose,for each bracketed string c = (w, B) we define theauxiliary function1 if ( i , j )  is valid for b E B~(i,j) = 0 otherwiseThe reestimation formulas for the extended algo-rithm are shown in Table 1.
For each bracketedsentence c in the training corpus, the inside prob-abilities of longer spans of c are computed fromthose for shorter spans with the recurrence givenby equations (1) and (2).
Equation (2) calculatesthe expected relative frequency of derivations ofiwk from Ap compatible with the bracketing B ofc = (w, B).
The multiplier 5(i, k) is i just in case(i, k) is valid for B, that is, when Ap can deriveiwk compatibly with B.Similarly, the outside probabilities for shorterspans of c can be computed from the inside prob-abilities and the outside probabilities for longerspans with the recurrence given by equations (3)and (4).
Once the inside and outside probabili-ties computed for each sentence in the corpus, the^reestimated probability of binary rules, Bp,q,r, andthe reestimated probability of unary rules, (Jp,ra,are computed by the reestimation formulas (5) and(6), which are just like the original ones (Baker,1979; Jelinek et al, 1990; Lari and Young, 1990)except for the use of bracketed strings instead ofunbracketed ones.The denominator of ratios (5) and (6) estimatesthe probability that a compatible derivation of abracketed string in C will involve at least one ex-pansion of nonterminal Ap.
The numerator of (5)estimates the probability that a compatible deriva-tion of a bracketed string in C will involve ruleAp --* Aq At, while the numerator of (6) estimates?
the probability that a compatible derivation of astring in C will rewrite Ap to b,n.
Thus (5) es-timates the probability that a rewrite of Ap in acompatible derivation of a bracketed string in Cwill use rule Ap --~ Aq At, and (6) estimates theprobability that an occurrence of Ap in a compat-ible derivation of a string in in C will be rewrittento bin.
These are the best current estimates forthe binary and unary rule probabilities.The process is then repeated with the reestimatedprobabilities until the increase in the estimatedprobability of the training text given the modelbecomes negligible, or, what amounts to the same,the decrease in the cross entropy estimate (nega-tive log probability)E log pcH(C,G)  = (8)Iclc6Cbecomes negligible.
Note that for comparisonswith the original algorithm, we should use thecross-entropy estimate /~(W, G) of the unbrack-eted text W with respect o the grammar G, not(8).1313.3.
Complex i tyEach of the three steps of an iteration of the origi-nal inside-outside algorithm - -  computation of in-side probabilities, computation of outside proba-bilities and rule probability reestimation - takestime O(Iwl 3) for each training sentence w. Thus,the whole algorithm is O(Iw\[ 3) on each trainingsentence.However, the extended algorithm performs betterwhen bracketing information is provided, becauseit does not need to consider all possible spans forconstituents, but only those compatible with thetraining set bracketing.
In the limit, when thebracketing of each training sentence comes froma complete binary-branching analysis of the sen-tence (a full binary bracketing), the time of eachstep reduces to O(\[w D. This can be seen from thefollowing three facts about any full binary brack-eting B of a string w:1.
B has o(Iwl) spans;2.
For each (i, k) in B there is exactly one splitpoint j such that both (i, j)  and (j, k) are in3.
Each valid span with respect o B must al-ready be a member of B.Thus, in equation (2) for instance, the number ofspans (i, k) for which 5(i, k)  0 is O(\[eD, andthere is a single j between i and k for which6(i, j)  ~ 0 and 5(j,k) ~ 0.
Therefore, the totaltime to compute all the I~(i, k) is O(Icl).
A simi-lar argument applies to equations (4) and (5).Note that to achieve the above bound as well as totake advantage of whatever bracketing is availableto improve performance, the implementation mustpreprocess the training set appropriately so thatthe valid spans and their split points are efficientlyenumerated.4.
EXPERIMENTALEVALUATIONThe following experiments, although preliminary,give some support o our earlier suggested advan-tages of the inside-outside algorithm for partiallybracketed corpora.The first experiment involves an artificial exam-ple used by Lari and Young (1990) in a previousevaluation of the inside-outside algorithm.
In thiscase, training on a bracketed corpus can lead to agood solution while no reasonable solution is foundtraining on raw text only.The second experiment uses a naturally occurringcorpus and its partially bracketed version providedby the Penn Treebank (Brill et al, 1990).
Wecompare the bracketings assigned by grammars in-ferred from raw and from bracketed training mate-rial with the Penn Treebank bracketings of a sep-arate test set.To evaluate objectively the accuracy of the analy-ses yielded by a grammar G, we use a Viterbi-styleparser to find the most likely analysis of each testsentence according to G, and define the bracket-ing accuracy of the grammar as the proportionof phrases in those analyses that are compatiblein the sense defined in Section 2 with the treebank bracketings of the test set.
This criterion isclosely related to the "crossing parentheses" coreof Black et al (1991).
1In describing the experiments, we use the nota-tion GR for the grammar estimated by the originalinside-outside algorithm, and GB for the grammarestimated by the bracketed algorithm.4.1.
Inferring the Palindrome Lan-guageWe consider first an artificial anguage discussedby Lari and Young (1990).
Our training corpusconsists of 100 sentences in the palindrome lan-guage L over two symbols a and bL - (ww R I E {a,b}'}.randomly generatedSwith the SCFG?~A CS?~BDS ?-~ AAS BBC*-~SAD!+SBA *-~ aB&b1 Since the grammar inference procedure is restricted toChomsky normal form grannnars, it cannot avoid difficultdecisions by leaving out brackets (thus making flatter parsetrees), as hunmn annotators often do.
Therefore, the recallcomponent in Black et aL's figure of merit for parser is notneeded.132The initial grammar consists of all possible CNFrules over five nonterminals and the terminals aand b (135 rules), with random rule probabilities.As shown in Figure 1, with an unbracketed train-ing set W the cross-entropy estimate H(W, GR) re-mains almost unchanged after 40 iterations (from1.57 to 1.44) and no useful solution is found.In contrast, with a fully bracketed version C ofthe same training set, the cross-entropy estimate/~(W, GB) decreases rapidly (1.57 initially, 0.88 af-ter 21 iterations).
Similarly, the cross-entropy esti-mate H(C, GB) of the bracketed text with respectto the grammar improves rapidly (2.85 initially,0.89 after 21 iterations).1.61.51.41.3G1.2<" I .
iI0.90.8~-...\\!\Raw - -Bracketed  .
.
.
.
.%i !
i !
, , !1 5 10 15 20 25 30 35 40iterationFigure 1: Convergence for the Palindrome CorpusThe inferred grammar models correctly the palin-drome language.
Its high probability rules (p >0.1, pip' > 30 for any excluded rule p') areS - - *ADS - *CBB- - *SCD- - *SAA --* bB -* aC --* aD ---* bwhich is a close to optimal CNF CFG for the palin-drome language.The results on this grammar are quite sensitiveto the size and statistics of the training corpusand the initial rule probability assignment.
Infact, for a couple of choices of initial grammarand corpus, the original algorithm produces gram-mars with somewhat better cross-entropy esti-mates than those yielded by the new one.
How-ever, in every case the bracketing accuracy ona separate test set for the result of bracketedtraining is above 90% (100% in several cases), incontrast o bracketing accuracies ranging between15% and 69% for unbracketed training.4.2.
Exper iments  on  the  AT IS  Cor -pusFor our main experiment, we used part-of-speechsequences ofspoken-language transcriptions in theTexas Instruments subset of the Air Travel Infor-mation System (ATIS) corpus (Hemphill et el.,1990), and a bracketing of those sequences derivedfrom the parse trees for that subset in the PennTreebank.Out of the 770 bracketed sentences (7812 words)in the corpus, we used 700 as a training set C and70 (901 words) as a test set T. The following is anexample training string( ( ( VB ( DT ~NS ( IB ( ( NN ) (NN CD ) ) ) ) ) ) .
)corresponding to the parsed sentence(((List (the fa res  (for ((f l ight)(number 891))))))  .
)The initial grammar consists of all 4095 possibleCNF rules over 15 nonterminals (the same numberas in the tree bank) and 48 terminal symbols forpart-of-speech tags.A random initial grammar was trained separatelyon the unbracketed and bracketed versions of thetraining corpus, yielding grammars GR and GB.4.64 .44 .243.a3.63 .43 .232 .81i !
| I i !
!~, Raw - -~ Bracketed  .....\\ .I I I I I | II0  20 30  40 50 60 70  75i te ra t ionFigure 2: Convergence for the ATIS CorpusFigure 2 shows that H(W, GB) initially decreasesfaster than the/:/(W, GR), although eventually the133two stabilize at very close values: after 75 itera-tions, /I(W, GB) ~ 2.97 and /:/(W, GR) ~ 2.95.However, the analyses assigned by the resultinggrammars to the test set are drastically different.I0080u 60oo 40rd20' Raw ' ' ' ' 'B racketed  .....
., .......... ~"?
..../lI I I i ' ' iI0 20 30  40  50 60 70  75i te ra t ionFigure 3: Bracketing Accuracy for the ATIS Cor-pusWith the training and test data described above,the bracketing accuracy of GR after 75 iterationswas only 37.35%, in contrast o 90.36% bracket-ing accuracy for GB.
Plotting bracketing accu-racy against iterations (Figure 3), we see that un-bracketed training does not on the whole improveaccuracy.
On the other hand, bracketed trainingsteadily improves accuracy, although not mono-tonically.It is also interesting to look at some the differencesbetween GR and GB, as seen from the most likelyanalyses they assign to certain sentences.
Table2 shows two bracketed test sentences followed bytheir most likely GR and GB analyses, given forreadability in terms of the original words ratherthan part-of-speech tags.For test sentence (A), the only GB constituentnot compatible with the tree bank bracketingis (Delta f l ight  number), although the con-stituent (the cheapest)  is linguistically wrong.The appearance of this constituent can be ex-plained by lack of information in the tree bankabout the internal structure of noun phrases, asexemplified by tree bank bracketing of the samesentence.
In contrast, the GR analysis of the samestring contains 16 constituents incompatible withthe tree bank.For test sentence (B), the G~ analysis is fully com-patible with the tree bank.
However, the Grt anal-ysis has nine incompatible constituents, which for(A)Ga(I would (like (to (take (Delta ((flight number) 83)) (to Atlanta)))).
)(What ((is (the cheapest fare (I can get)))) ?
)(I (would (like ((to ((take (Delta flight)) (number (83 ((to Atlanta) .
)))))((What ( ( ( i s  the) cheapest) fare)) ((I can) (get ? )
) ) ) ) ) )(((I (would (like (to (take (((Delta (flight number)) 83) (to Atlanta))))))) .
)((What ( is  ( ( ( the cheapest) fare)  (I (can get ) ) ) ) )  ?
))GB(B) ((Tell me (about (the public transportation((from SF0) (to San Francisco))))).
)GR (Tell ((me (((about the) public) transportation))((from SF0) ((to San) (Francisco .
)))))GB ((Tell (me (about (((the public) transportation)((from SFO) (to (San Franc isco) ) ) ) ) ) )  .
)Table 2: Comparing Bracketingsexample places Francisco and the final punctua-tion in a lowest-level constituent.
Since final punc-tuation is quite often preceded by a noun, a gram-mar inferred from raw text will tend to bracketthe noun with the punctuation mark.This experiment illustrates the fact that althoughSCFGs provide a hierarchical model of the lan-guage, that structure is undetermined byraw textand only by chance will the inferred grammaragree with qualitative linguistic judgments of sen-tence structure.
This problem has also been previ-ously observed with linguistic structure inferencemethods based on mutual information.
Mater-man and Marcus (1990) addressed the problem byspecifying a predetermined list of pairs of parts ofspeech (such as verb-preposition, pronoun-verb)that can never be embraced by a low-level con-stituent.
However, these constraints are stipulatedin advance rather than being automatically de-rived from the training material, in contrast withwhat we have shown to be possible with the inside-outside algorithm for partially bracketed corpora.5.
CONCLUSIONS ANDFURTHER WORKWe have introduced a modification of the well-known inside-outside algorithm for inferring theparameters of a stochastic ontext-free grammarthat can take advantage of constituent informa-tion (constituent bracketing) in a partially brack-eted corpus.The method has been successfully applied toSCFG inference for formal languages and forpart-of-speech sequences derived from the ATIS134spoken-language corpus.The use of partially bracketed corpus can reducethe number of iterations required for convergenceof parameter reestimation.
In some cases, a goodsolution is found from a bracketed corpus but notfrom raw text.
Most importantly, the use of par-tially bracketed natural corpus enables the algo-rithm to infer grammars pecifying linguisticallyreasonable constituent boundaries that cannot beinferred by the inside-outside algorithm on rawtext.
While none of this is very surprising, it sup-plies some support for the view that purely unsu-pervised, self-organizing grammar inference meth-ods may have difficulty in distinguishing betweenunderlying rammatical structure and contingentdistributional regularities, or, to put it in anotherway, it gives some evidence for the importance ofnondistributional regularities in language, whichin the case of bracketed training have been sup-plied indirectly by the linguists carrying out thebracketing.Also of practical importance, the new algorithmcan have better time complexity for bracketedtext.
In the best situation, that of a training setwith full binary-branching bracketing, the time foreach iteration is in fact linear on the total lengthof the set.These preliminary investigations could be ex-tended in several ways.
First, it is important odetermine the sensitivity of the training algorithmto the initial probability assignments and trainingcorpus, as well as to lack or misplacement of brack-ets.
We have started experiments in this direction,but reasonable statistical models of bracket elisionand misplacement are lacking.Second, we would like to extend our experimentsto larger terminal vocabularies.
As is well known,this raises both computational nd data sparse-ness problems, so clustering of terminal symbolswill be essential.Finally, this work does not address a central weak-ness of SCFGs, their inability to represent lex-ical influences on distribution except by a sta-tistically and computationally impractical pro-liferation of nonterminal symbols.
One mightinstead look into versions of the current algo-rithm for more lexically-oriented formalisms uchas stochastic lexicalized tree-adjoining rammars(Schabes, 1992).ACKNOWLEGMENTSWe thank Aravind Joshi and Stuart Shieber foruseful discussions, and Mitch Marcus, BeatriceSantorini and Mary Ann Marcinkiewicz for mak-ing available the ATIS corpus in the Penn Tree-bank.
The second author is partially supportedby DARPA Grant N0014-90-31863, ARO GrantDAAL03-89-C-0031 and NSF Grant IRI90-16592.REFERENCESJ.K.
Baker.
1979.
Trainable grammars for speechrecognition.
In Jared J. Wolf and Dennis H. Klatt,editors, Speech communication papers presentedat the 97 ~h Meeting of the Acoustical Society ofAmerica, MIT, Cambridge, MA, June.E.
Black, S. Abney, D. Flickenger, R. Grishman,P.
Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A pro-cedure for quantitatively comparing the syntacticcoverage of english grammars.
In DARPA Speechand Natural Language Workshop, pages 306-311,Pacific Grove, California.
Morgan Kaufmann.T.
Fujisaki, F. Jelinek, J. Cocke, E. Black, andT.
Nishino.
1989.
A probabilistic parsing methodfor sentence disambiguation.
In Proceedings of theInternational Workshop on Parsing Technologies,Pittsburgh, August.Charles T. Hemphill, John J. Godfrey, andGeorge R. Doddington.
1990.
The ATIS spokenlanguage systems pilot corpus.
In DARPA Speechand Natural Language Workshop, Hidden Valley,Pennsylvania, June.F.
Jelinek, J. D. Lafferty, and R. L. Mercer.
1990.Basic methods of probabilistic ontext free gram-mars.
Technical Report RC 16374 (72684), IBM,Yorktown Heights, New York 10598.Frederick Jelinek, Robert L. Mercer, and SalimRoukos.
1992.
Principles of lexical language mod-eling for speech recognition.
In Sadaoki Furui andM.
Mohan Sondhi, editors, Advances in SpeechSignal Processing, pages 651-699.
Marcel Dekker,Inc., New York, New York.K.
Lari and S. J.
Young.
1990.
The estimation ofstochastic ontext-free grammars using the Inside-Outside algorithm.
Computer Speech and Lan-guage, 4:35-56.K.
Lari and S. J.
Young.
1991.
Applications ofstochastic ontext-free grammars using the Inside-Outside algorithm.
Computer Speech and Lan-guage, 5:237-257.David Magerman and Mitchell Marcus.
1990.Parsing a natural anguage using mutual informa-tion statistics.
In AAAI-90, Boston, MA.Yves Schabes.
1992.
Stochastic lexicalized tree-adjoining grammars.
In COLING 92.
Forthcom-ing.T.
Booth.
1969.
Probabilistic representation offormal languages.
In Tenth Annual IEEE Sympo-sium on Switching and Automata Theory, Octo-ber.Eric Brill, David Magerman, Mitchell Marcus, andBeatrice Santorini.
1990.
Deducing linguisticstructure from the statistics of large corpora.
InDARPA Speech and Natural Language Workshop.Morgan Kaufmann, Hidden Valley, Pennsylvania,JuDe.135
