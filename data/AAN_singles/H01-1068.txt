A Three-Tiered Evaluation Approach forInteractive Spoken Dialogue SystemsKathleen Stibler and James DennyLockheed Martin Advanced Technology Laboratories1 Federal Street, A&E 3WCamden NJ 08102{kcomegno, jdenny}@atl.lmco.comABSTRACTWe describe a three-tiered approach for evaluation of spokendialogue systems.
The three tiers measure user satisfaction,system support of mission success and component performance.We describe our use of this approach in numerous fielded userstudies conducted with the U.S. military.KeywordsEvaluation, spoken language system, spoken dialogue system1.
INTRODUCTIONEvaluation of spoken language systems is complicated by theneed to balance distinct goals.
For collaboration with othersin the speech technology community, metrics must be genericenough for comparison to analogous systems.
For project man-agement and business purposes, metrics must be specificenough to demonstrate end-user utility and improvement overother approaches to a problem.Since 1998, we have developed a spoken language dialoguetechnology called Listen-Communicate-Show (LCS) andapplied it to demonstration systems for U.S. Marines logistics,U.S.
Army test data collection, and commercial travel reserva-tions.
Our focus is the transition of spoken dialogue technol-ogy to military operations.
We support military users in a widerange of tasks under diverse conditions.
Therefore, our defini-tion of success for LCS is operational success.
It must reflectthe real world success of our military users in performing theirtasks.
In addition, for our systems to be considered successful,they must be widely usable and easy for all users to operatewith minimal training.
Our evaluation methodology mustmodel these objectives.With these goals in mind, we have developed a three-tiermetric system for evaluating spoken language system effective-ness.
The three tiers measure (1) user satisfaction, (2) systemsupport of mission success and (3) component performance.2.
THE THREE-TIERED APPROACHOur three-tier metric scheme evaluates multiple aspects of LCSsystem effectiveness.
User satisfaction is a set of subjectivemeasures that introduces user perceptions into the assessmentof the system.
System support of mission success measuresoverall system performance with respect to our definition ofsuccess.
Component performance scores the individual sys-tem component?s role in overall system success.Collection of user input is essential in evaluation for two rea-sons.
First, it is necessary to consider user perspective duringevaluation to achieve a better understanding of user needs.Second, user preference can influence interpretation of successmeasurements of mission success and component performance.Mission success and component performance are often tradeoffs,with inefficient systems producing higher scores of success.Since some users are willing to overlook efficiency for guaran-teed performance while others opt for efficiency, our collectionof user input helps determine the relative importance of theseaspects.Mission success is difficult to quantify because it is defineddifferently by users with different needs.
Therefore, it is essen-tial to establish a definition of mission success early in theevaluation process.
For our applications, we derive this defini-tion from domain knowledge acquisition with potential users.It is important to evaluate components individually since com-ponent evaluations reveal distinctive component flaws.
Theseflaws can negatively impact mission success because catas-trophic failure of a component can prevent the completion oftasks.
For example, in the Marine logistics domain, if thesystem fails to recognize the user signing onto the radio net-work, it will ignore all subsequent utterances until the usersuccessfully logs on.
If the recognition of sign-on completelyfails, then no tasks can be completed.
In addition, periodicevaluation of component performance focuses attention ondifficult problems and possible solutions to these problems[1].3.
EVALUATION METRICSAt the top level of our approach, measurements of overall usersatisfaction are derived from a collection of user reactions on aLikert-scaled questionnaire.
The questions are associated witheight user satisfaction metrics: ease of use, system response,system understanding, user expertise, task ease, response time,expected behavior and future use.
We have categorized our usersatisfaction questions in terms of specific metrics as per thePARADISE methodology [5, 2].
These metrics are detailed inTable 1.Table 1.
User Satisfaction metricsMetric Description Example Likert Survey QuestionsEase of Use User perception of ease of interaction withoverall systemThe system was easy to useSystemResponseClarity of system response System responses were clear and easy to understandSystemUnderstandingSystem comprehension of the user The system understood what you saidUser Expertise Shows us how prepared the user felt due to ourtrainingYou knew how to interact with the system based onprevious experience or trainingTask Ease User ease in performing a given task It was easy to make a requestResponse Time User?s impression of the speed of system?sreplyThe system responded to you in a timely mannerExpectedBehaviorConnection between the user?s experience andpreconceived notionsThe system worked the way that you expected it toFuture Use Determination of overall acceptance of this typeof system in the futureYou would use a mature system of this type in the futureThe middle tier metrics measure the ability of users tosuccessfully complete their domain tasks in a timely manner.Success, in this case, is defined as completion of a task andsegments of the task utilizing the information supplied by theuser.
A task is considered successful if the system was able tocomprehend and process the user?s request correctly.
It i simportant to determine if success was achieved and at whatcost.
The user?s ability to make a request in a reasonableamount of time with little repetition is also significant.
Themission success metrics fall under nine categories: task com-pletion, task complexity, dialogue complexity, task efficiency,dialogue efficiency, task pace, dialogue pace, user frustrationand intervention rate.For these metrics, we consider the tasks the user is trying toaccomplish and the dialogue in which the user has with thesystem to accomplish those tasks.
A session is a continuousperiod of user interaction with the spoken dialogue system.
Asession can be examined from two perspectives, task and dia-logue, as shown in Figure 1.
Segments are atomic operationsperformed within a task.
The success rate of each segment is animportant part of the analysis of the system, while the successrate of each task is essential for the comprehensive evaluationof the system.
For example, a task of ordering supplies in theMarine logistics domain includes segments of signing onto theradio network, starting the request form, filling in items athrough h, submitting the form and signing off the network.Each segment receives an individual score of successfullycompletion.
The Task Completion metric consists of successscores for the overall task and the segments of the task.Dialogue is the collection of utterances spoken to accomplishthe given task.
It is necessary to evaluate Dialogue Efficiencyto achieve an understanding of how complex the user?s dia-logue is for the associated task.
A turn is one user utterance, astep in accomplishing the task through dialogue.
Concepts areatomic bits of information conveyed in a dialogue.
For example,if the user?s utterance consists of delivery time and deliverylocation for a particular Marine logistic request, the time andlocation are the concepts of that turn.
These metrics aredescribed in greater detail in Table 2.SessionDialogueTurnConcept WordSegmentTaskFigure 1.
Structural Hierarchy of a Spoken DialogueSystem SessionThe lowest level tier measures the effectiveness of individualsystem components along specific dimensions, including com-ponent error rates.
Overall system level success is determinedby how well each component accomplishes its responsibility.This concerns measurements such as word accuracy, utteranceaccuracy, concept accuracy, component speed, processingerrors, and language errors.
These measurements aid systemdevelopers by emphasizing component weakness.
ComponentPerformance metrics also offer explanations for others metrics.For example, bottlenecks within a component may be respon-sible for slow system response time.
Another example i sconcerned with recognition accuracy.
Poor word accuracy mayaccount for low scores of task completion and user satisfactionwith the system.Table 2.
Mission metricsMetric Description MeasurementTask Completion Success rate of a given task?
correct segments??
itemsTask ComplexityIdeal minimal information required toaccomplish a task?
ideal concepts?taskDialogueComplexityIdeal amount of interaction with the systemnecessary to complete a task?
ideal turns?taskTask EfficiencyAmount of extraneous information indialogue?
ideal concepts???
actual conceptsDialogueEfficiencyNumber of extraneous turns in dialogue   ?
ideal turns???
actual turnsTask PaceReal world time spent entering informationinto the system to accomplish the task?
elapsed time?
?task complexityDialogue PaceActual amount of system interaction spententering segments of a task?
turns?????????
?task complexityUser FrustrationRatio of repairs and repeats to useful turns   ?
(rephrases + repeats)???
relevant turnsIntervention RateHow often the user needs help to use thesystem ?
(user questions + moderator corrections + system crashes)Some component performance metrics rely upon measurementsfrom multiple components.
For example, Processing Errorscombines data transfer errors, logic errors, and agent errors.Those measurements map to the Turn Manager which controlsthe system's dialogue logic, the Mobile Agents which interfacewith data sources, and the Hub which coordinates componentcommunication.
The metrics are discussed in Table 3.4.
EVALUATION PROCESSOur LCS systems are built upon MIT?s Galaxy II architecture[3].
Galaxy II is a distributed, plug and play component-basedarchitecture in which specialized servers handle specific tasks,such as translating audio data to text, that communicatethrough a central server (Hub).
The LCS system shown inFigure 2 includes servers for speech recording and playback(Audio I/O), speech synthesis (Synthesis), speech recognition(Recognizer), natural language processing (NL), discourse/response logic (Turn Manager), and an agent server (MobileAgents) for application/database interaction.We implement a number of diverse applications and serve auser population that has varying expertise.
The combination ofthese two factors result in a wide range of expectations ofsystem performance by users.
We have found that the three-tiersystem and related evaluation process not only capture thoseexpectations, but also aid in furthering our development.Our evaluation process begins with conducting a user study,typically in the field.
We refer to these studies as IntegratedFeasibility Experiments (IFE).
Participants involved in theIFEs are trained to use their particular LCS application by amember of our development team.
The training usually takes 15to 30 minutes.
The training specifies the purpose of the LCSapplication in aiding their work, includes a brief description ofthe  LCS  architecture,  and  details  the  speech  commands  andTable 3.
Component metricsMetric Description MeasurementWord Accuracy System recognition per word NIST String Alignment and Scoring ProgramUtteranceAccuracySystem recognition per user utterance  ?
recognized turns??
turnsConceptAccuracy*Semantic understanding of the system  ?
recognized concepts??
conceptsComponent Speed Speed of various components time per turnProcessing Errors Percent of turns with low level system errormeasurements?
(agent errors + frame construction errors + logic errors)??
system turnsLanguage Errors Percent of turns with errors in sentenceconstruction, word parsing and spoken outputof the system?
(parse errors + synthesis errors)?
system turns*Our use of concept accuracy was inspired by the concept accuracy metric of the PARADISE methodology [5].RecognizerNLTurnManagerMobileAgentsSynthesisAudioI/OHUBDistributedHeterogeneousData SourcesFigure 2.
LCS architectureexpected responses through demonstration.
After the intro-ductory instruction and demonstration, participants practiceinteracting with the system.For each study, we develop a set of scenarios based upon ourknowledge of the domain and ask each participant to completethe scenarios as quickly as they can with maximal accuracy andminimal moderator assistance.
The study usually consists ofapproximately five task scenarios of varying difficulty.
Thescenarios are carried out in fixed order and are given a timelimit, generally no longer than 30 minutes.
The system logskey events at the Hub, including times and values for theuser?s speech recording, recognition hypotheses, grammaticalparse, resultant query, component speeds, any internal errors,and the system?s response.
In addition, the moderator notesany assistance or intervention, such as reminding the user ofproper usage or fixing an application error.
Once the tasks arecompleted, the user fills out a web-based survey and partici-pates in a brief interview.
These determine user satisfactionwith the system.Upon conclusion of a user study, we extract the log files andcode the users?
recordings through manual transcription.
Weadd diagnostic tags to the log files, noting such events asrephrased utterances and causes of errors and then audit all ofthe logs for accuracy and consistency.
Some of the diagnostictags that we annotate are number of items and concepts withinan utterance, frame construction errors, repeated or rephrasedutterances and deficiencies of the training sentence corpus.This is a very time consuming process.
Therefore, it is nec-essary to involve multiple people in this phase of theevaluation.
However, one individual is tasked with the finalresponsibility of examining the annotations for consistency.A series of scripts and spreadsheets calculate our metrics fromthe log files.
These scripts take the log files as parameters andproduce various metric values.
While interpreting the metricsvalues, we may re-examine the log files for an exploration ofdetail related to particular tasks or events in order tounderstand any significant and surprising results or trends.Finally, through a mixture of automated formatting and manualcommentary, we create a summary presentation of the user studyresults.
Web pages are generated that contain some of themetrics collected throughout the study.5.
APPROACH VERIFICATIONWe have applied our approach in four separate IFEs to date.
Ineach case, our metrics revealed areas for improvement.
As theseimprovements were made, the problems discovered in the nextIFE were more subtle and deeply ingrained within the system.Mission success and component metrics aided in the interpre-tation of user perception and drove future system development.A top-level summary of IFEs, metrics and system improvementsis described.The first IFE was our pilot study, which took place in-house inSeptember 1999.
Five subjects with varying military experi-ence were asked to complete three tasks, which were scriptedfor them.
The tier one metrics revealed the users?
dissatisfactionwith the system responses and the time required in receivingthem.
These perceptions led to system changes within ourAgent and Turn Manager structures that improved the speed ofour database agents and more appropriate responses from theLCS system.The second IFE took place during the Desert Knight 1999Marine exercise at Twentynine Palms, CA in December 1999.Ten subjects, each an active duty Marine with varying radiooperator experience, were given five tasks.
This user studyoffered the subjects the option of following scripts in theirtasks.
The metrics of tier one showed an increase in overall usersatisfaction and revealed the users?
difficulty using the systemand anticipating its behavior.
These concerns influenced futureuser training and the development of more explicit systemresponses.The third IFE occurred during the Marine CAX 6 (CombinedArms Exercise) at Twentynine Palms, CA in April 2000.
Theseven subjects were active duty Marines, some with minimalradio training.
They were required to complete five tasks thathad scenario-based, non-scripted dialogues.
A combination oftier one, tier two and tier three metrics exposed a deficiency inthe speech recognition server, prompting us to increaserecognizer training for subsequent IFEs.
A recognizer trainingcorpus builder was developed to boost recognition scores.The most recent IFE was conducted in Gulfport, MS during theAugust 2000 Millennium Dragon Marine exercise.
Six activeduty Marines with varied radio experience completed fivescenario-based tasks.
This time the users expressed concernwith system understanding and ease of use through the tier onemetrics.
The tier three metrics revealed an error in our naturallanguage module, which sometimes had been selecting theincorrect user utterance from recognizer output.
This error hassince been removed from the system.The three-tiered approach organizes analysis of the inter-dependence among metrics.
It is useful to study the impact of ametric in one tier against metrics in another tier throughprincipal component analysis.
These statistics do not neces-sarily evidence causality, of course, but they do suggestinsightful correlation.
This insight exposes the relative signifi-cance of various factors' contribution to particular assessmentsof mission success or user satisfaction.6.
FUTURE ENHANCEMENTSAlthough this three-tier evaluation process provides usefulmetrics, we have identified three improvements that we plan toincorporate into our process: (1) an annotation aide, (2) com-munity standardization, and (3) increased automation.
Theannotation aide would allow multiple annotators to reviewand edit logs independently.
With this tool, we could autom-atically measure and control cross-annotator consistency,currently a labor-intensive chore.
Community standardizationentails a logging format, an annotation standard, and calcula-tion tools common to the DARPA Communicator project [4],several of which have been developed, but we are still workingto incorporate them.
The advantage of community standardiza-tion is the benefit from tools developed by peer organizationsand the ability to compare results.
Accomplishing the first twoimprovements largely leads to the third improvement, increasedautomation, because most (if not all) aspects from measurementthrough annotation to calculation then have a controlledformat and assistive tools.
These planned improvements willmake our evaluation process more reliable and less time-consuming while simultaneously making it more controlledand more comparable.7.
CONCLUSIONWe have found that structuring evaluation according to thethree tiers described above improves the selection of metricsand interpretation of results.
While the essence of our approachis domain independent, it does guide the adaptation of metricsto specific applications.
First, the three tiers impose a structurethat selects certain metrics to constitute a broad pragmaticassessment with minimal data, refining the subject of evalua-tion.
Second, the three tiers organize metrics so that usersatisfaction and mission metrics have clear normative semantics(results interpreted as good/bad) and they reveal the impact oflow-level metrics (results tied to particular components whichmay be faulted/lauded).
Finally, improvements in selection andinterpretation balance satisfaction, effectiveness, and perform-ance, thus imbuing the evaluation process with focus towardutility for practical applications of spoken language dialogue.8.
ACKNOWLEDGEMENTThanks to members of the LCS team: Ben Bell, Jody Daniels,Jerry Franke, Ray Hill, Bob Jones, Steve Knott, Dan Miksch,Mike Orr, and Mike Thomas.
This research was supported byDARPA contract N66001-98-D-8507 and Naval contractN47406-99-C-7033.9.
REFERENCES[1] Hirschman, L. and Thompson, H. Survey of the State of theArt in Human Language Technology.
Edited by J.Mariani.
Chapter 13.1, Overview of Evaluation in Speechand Natural Language Processing.
Cambridge UniversityPress ISBN 0-521-592777-1, 1996.
[2] Kamm, C., Walker, M. and Litman, D. Evaluating SpokenLanguage Systems, American Voice Input/Output Society,AVIOS, 1999.
[3] Seneff, S., Lau, R., and Polifroni, J.
Organization, Com-munication, and Control in the Galaxy-ii ConversationalSystem.
Proc.
Eurospeech, 1999.
[4] Walker, M. Hirschman, L. and Aberdeen, J.
Evaluation forDARPA Communicator Spoken Dialogue Systems.Language Resources and Evaluation Conference, LREC,2000.
[5] Walker, M. Litman, Kamm, D.C. and Abella, A. PARA-DISE: A Framework for Evaluating Spoken DialogueAgents.
35th Annual Meeting of the Association ofComputational Linguistics, ACL 97, 1997.
