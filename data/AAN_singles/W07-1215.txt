Proceedings of the 5th Workshop on Important Unresolved Matters, pages 112?119,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsExtracting a verb lexicon for deep parsing from FrameNetMark McConville and Myroslava O. DzikovskaSchool of InformaticsUniversity of Edinburgh2 Buccleuch Place, Edinburgh EH8 9LW, Scotland{Mark.McConville,M.Dzikovska}@ed.ac.ukAbstractWe examine the feasibility of harvestinga wide-coverage lexicon of English verbsfrom the FrameNet semantically annotatedcorpus, intended for use in a practical naturallanguage understanding (NLU) system.
Weidentify a range of constructions for whichcurrent annotation practice leads to prob-lems in deriving appropriate lexical entries,for example imperatives, passives and con-trol, and discuss potential solutions.1 IntroductionAlthough the lexicon is the primary source of infor-mation in lexicalised formalisms such as HPSG orCCG, constructing one manually is a highly labour-intensive task.
Syntactic lexicons have been derivedfrom other resources ?
the LinGO ERG lexicon(Copestake and Flickinger, 2000) contains entriesextracted from ComLex (Grishman et al, 1994),and Hockenmaier and Steedman (2002) acquire aCCG lexicon from the Penn Treebank.
However,one thing these resources lack is information on howthe syntactic subcategorisation frames correspond tomeaning.The output representation of many ?deep?
widecoverage parsers is therefore limited with respect toargument structure ?
sense distinctions are strictlydetermined by syntactic generalisations, and arenot always consistent.
For example, in the logi-cal form produced by the LinGO ERG grammar,the verb end can have one of two senses depend-ing on its subcategorisation frame: end v 1 relor end v cause rel, corresponding to the cel-ebrations ended and the storm ended the celebra-tions respectively.
Yet a very similar verb, stop, hasa single sense, stop v 1 rel, for both the cele-brations stopped and the storm stopped the celebra-tions.
There is no direct connection between thesedifferent verbs in the ERG lexicon, even thoughthey are intuitively related and are listed as belong-ing to the same or related word classes in semanticlexicons/ontologies such as VerbNet (Kipper et al,2000) and FrameNet (Baker et al, 1998).If the output of a deep parser is to be used witha knowledge representation and reasoning compo-nent, for example in a dialogue system, then we needa more consistent set of word senses, linked by spec-ified semantic relations.
In this paper, we investi-gate how straightforward it is to harvest a compu-tational lexicon containing this kind of informationfrom FrameNet, a semantically annotated corpus ofEnglish.
In addition, we consider how the FrameNetannotation system could be made more transparentfor lexical harvesting.Section 2 introduces the FrameNet corpus, andsection 3 discusses the lexical information requiredby frame-based NLU systems, with particular em-phasis on linking syntactic and semantic structure.Section 4 presents the algorithm which converts theFrameNet corpus into a frame-based lexicon, andevaluates the kinds of entries harvested in this way.We then discuss a number of sets of entries whichare inappropriate for inclusion in a frame-based lex-icon: (a) ?subjectless?
entries; (b) entries derivedfrom passive verbs; (c) entries subcategorising formodifiers; and (d) entries involving ?control?
verbs.1122 FrameNetFrameNet1 is a corpus of English sentences an-notated with both syntactic and semantic informa-tion.
Underlying the corpus is an ontology of795 ?frames?
(or semantic types), each of whichis associated with a set of ?frame elements?
(orsemantic roles).
To take a simple example, theApply heat frame describes a situation involvingframe elements such as a COOK, some FOOD, anda HEATING INSTRUMENT.
Each frame is, in addi-tion, associated with a set of ?lexical units?
whichare understood as evoking it.
For example, theApply heat frame is evoked by such verbs asbake, blanch, boil, broil, brown, simmer, steam, etc.The FrameNet corpus proper consists of 139,439sentences (mainly drawn from the British NationalCorpus), each of which has been hand-annotatedwith respect to a particular target word in the sen-tence.
Take the following example: Matilde friedthe catfish in a heavy iron skillet.
The process of an-notating this sentence runs as follows: (a) identify atarget word for the annotation, for example the mainverb fried; (b) identify the semantic frame which isevoked by the target word in this particular sentence?
in this case the relevant frame is Apply heat;(c) identify the sentential constituents which realiseeach frame element associated with the frame, i.e.
:[COOK Matilde] [Apply heat fried] [FOOD thecatfish] [HEATING INSTR in a heavy iron skillet]Finally, some basic syntactic information about thetarget word and the constituents realising the vari-ous frame elements is also added: (a) the part-of-speech of the target word (e.g.
V, N, A, PREP); (b)the syntactic category of each constituent realising aframe element (e.g.
NP, PP, VPto, Sfin); and (c)the syntactic role, with respect to the target word,of each constituent realising a frame element (e.g.Ext, Obj, Dep).
Thus, each sentence in the corpuscan be seen to be annotated on at least three inde-pendent ?layers?, as exemplified in Figure 1.3 Frame-based NLUThe core of any frame-based NLU system is a parserwhich produces domain-independent semantic rep-1The version of FrameNet discussed in this paper isFrameNet II release 1.3 from 22 August 2006.resentations like the following, for the sentence Johnbilled the champagne to my account:????
?commerce-payAGENT JohnTHEME champagneSOURCE[accountOWNER me]????
?Deep parsers/grammars such as the ERG, OpenCCG(White, 2006) and TRIPS (Dzikovska, 2004) pro-duce more sophisticated representations with scop-ing and referential information, but still contain aframe-based representation as their core.
The lex-ical entries necessary for constructing such repre-sentations specify information about orthography,part-of-speech, semantic type and subcategorisationproperties, including a mapping between a syntacticsubcategorisation frame and the semantic frame.An example of a TRIPS lexical entry is presentedin Figure 2, representing the entry for the verb billas used in the sentence discussed above.
Note thatfor each subcategorised argument the syntactic role,syntactic category, and semantic role are specified.Much the same kind of information is included inERG and OpenCCG lexical entries.When constructing a computational lexicon, thereare a number of issues to take into account, sev-eral of which are pertinent to the following discus-sion.
Firstly, computational lexicons typically listonly the ?canonical?
subcategorisation frames, cor-responding to a declarative sentence whose mainverb is in the active voice, as in Figure 1.
Other vari-ations, such as passive forms, imperatives and dativealternations are generated automatically, for exam-ple by lexical rules.
Secondly, parsers that build se-mantic representations typically make a distinctionbetween ?complements?
and ?modifiers?.
Comple-ments are those dependents whose meaning is com-pletely determined by the verb, for example the PPon him in the sentence Mary relied on him, and arethus listed in lexical entries.
Modifiers, on the otherhand, are generally not specified in verb entries ?although they may be associated with the underlyingverb frame, their meaning is determined indepen-dently, usually by the preposition, such as the timeadverbial next week in I will see him next week.Finally, for deep parsers, knowledge about whichargument of a matrix verb ?controls?
the implicit113Matilde fried the catfish in a heavy iron skillettarget Apply heatframe element COOK FOOD HEATING INSTRsyntactic category NP V NP PPsyntactic role Ext Obj DepFigure 1: A FrameNet annotated sentence?????????????
?ORTH ?bill?SYNCAT vSEMTYPE?
?commerce-payASPECT boundedTIME-SPAN atomic??ARGS??
?SYNROLE subjSYNCAT npSEMROLE agent??,?
?SYNROLE objSYNCAT npSEMROLE theme??,???
?SYNROLE compSYNCAT[ppPTYPE to]SEMROLE source??????????????????
?Figure 2: A TRIPS lexical entrysubject of an embedded complement verb phrase isnecessary in order to to build the correct semanticform.
In a unification parser such as TRIPS, controlis usually represented by a relation of token-identity(i.e.
feature structure reentrancy) between the sub-ject or object of a control verb and the subject of averbal complement.4 Harvesting a computational lexicon fromFrameNetIn order to harvest a computational lexicon from theFrameNet corpus, we took each of the 60,309 an-notated sentences whose target word is a verb andderived a lexical entry directly from the annotatedinformation.
For example, from the sentence in Fig-ure 1, the lexical entry in Figure 3 is derived.2In order to remove duplicate entries, we made twoassumptions: (a) the value of the ARGS feature is aset of arguments, rather than, say, a list or multiset;and (b) two arguments are identical just in case theyspecify the same syntactic role and semantic role.These assumptions prevent a range of inappropriateentries from being created, for example entries de-2Our original plan was to use the automatically generated?lexical entry?
files included with the most recent FrameNet re-lease as a basis for deep parsing.
However, these entries containso many inappropriate subcategorisation frames, of the typesdiscussed in this paper, that we decided to start from scratchwith the corpus annotations.rived from sentences involving a ?split?
argument,both parts of which are annotated independently inFrameNet, e.g.
[Ext Serious concern] arose [Extabout his motives].
A second group of inappropri-ate entries which are thus avoided are those derivingfrom relative clause constructions, where the rela-tive pronoun and its antecedent are also annotatedseparately:[Ext Perp The two boys] [Ext Perp who] ab-ducted [Obj Victim James Bulger] are likely tohave been his murderersFinally, assuming that the arguments constitute a setmeans that entries derived from sentences involvingboth canonical3 and non-canonical word order aretreated as equivalent.
The kinds of construction im-plicated here include ?quotative inversion?
(e.g.
?OrElectric Ladyland,?
added Bob), and leftwards ex-traction of objects and dependents, for example:Are there [Obj any places] [Ext you] want to praise[Dep for their special facilities]?In this paper we are mainly interested in extract-ing the possible syntax-semantics mappings fromFrameNet, rather than the precise details of their rel-ative ordering.
Since dependents in the harvested3The canonical word order in English involves a pre-verbalsubject, with all other dependents following the verb.114??????
?ORTH ?fry?SYNCAT VSEMTYPE Apply heatARGS??
?SYNROLE ExtSYNCAT NPSEMROLE Cook???
?SYNROLE ObjSYNCAT NPSEMROLE Food??,?
?SYNROLE DepSYNCAT PPSEMROLE Heating Instr?????????
?Figure 3: The lexical entry derived from Figure 1lexicon are fully specified for semantic role, syn-tactic category and syntactic role, post-verbal con-stituent ordering can-be regulated extra-lexically bymeans of precedence rules.
For example, for theTRIPS and LFG formalisms, there is a straightfor-ward correspondence between their native syntacticrole specifications and the FrameNet syntactic roles.After duplicate entries were removed from the re-sulting lexicon, we were left with 26,022 distinctentries.
The harvested lexicon incorporated 2,002distinct orthographic forms, 358 distinct frames,and 2,661 distinct orthography-frame pairs, givinga functionality ratio (average number of lexical en-tries per orthography-type pair) of 9.8.Next, we evaluated a random sample of the de-rived lexical entries by hand.
The aim here was toidentify general classes of the harvested verb entrieswhich are not appropriate for inclusion in a frame-based verb lexicon, and which would need to beidentified and fixed in some way.
The main groupsidentified were: (a) entries with no Ext argument(section 4.1); (b) entries derived from verbs in thepassive voice (section 4.2); (c) entries which subcat-egorise for modifiers (section 4.3); and (d) entriesfor control verbs (section 4.4).4.1 Subjectless entriesThe harvested lexicon contains 2,201 entries (i.e.9% of the total) which were derived from sentenceswhich do not contain an argument labelled with theExt syntactic role, in contravention of the gener-ally accepted constraint on English verbs that theyalways have a subject.Three main groups of sentences are implicatedhere: (a) those featuring imperative uses of the tar-get verb, e.g.
Always moisturise exposed skin withan effective emollient like E45; (b) those featuringother non-finite forms of the target verb whose un-derstood subject is not controlled by (or even coref-erential with) some other constituent in the sentence,e.g.
Being accused of not having a sense of humouris a terrible insult; and (c) those involving a non-referential subject it, for example It is raining heav-ily or It is to be regretted that the owner should havecut down the trees.
In FrameNet annotations, non-referential subjects are not identified on the syntacticrole annotation layer, and this makes it more difficultto harvest appropriate lexical entries for these verbsfrom the corpus.These entries are easy to locate in the harvestedlexicon, but more difficult to repair.
Typically onewould want to discard the entries generated from(a) and (b) as they will be derived automatically inthe grammar, but keep the entries generated from (c)while adding a non-referential it as a subject.Although the FrameNet policy is to annotate the(a) and (b) sentences as having a ?non-overt?
real-isation of the relevant frame element, this is con-fined to the frame element annotation layer itself,with the syntactic role and syntactic category lay-ers containing no clues whatsoever about understoodsubjects.
One rather roundabout way of differentiat-ing between these cases would involve attempting toidentify the syntactic category and semantic role ofthe missing Ext argument by looking at other en-tries with the same orthography and semantic type.However, this whole problem could be avoided ifunderstood and expletive subjects were identified onthe syntactic layers in FrameNet annotations.4.2 ?Passive?
entriesMany entries in the harvested lexicon were derivedfrom sentences where the target verb is used in thepassive voice, for example:[Ext NP Victim The men] had allegedly been ab-ducted [Dep PP Perp by Mrs Mandela?s body-115guards] [Dep PP Time in 1988]As discussed above, computational lexicons do notusually list the kinds of lexical entry derived directlyfrom such sentences.
Thus, it is necessary to identifyand correct or remove them.In FrameNet annotated sentences, the voice of tar-get verbs is not marked explicitly.4 We applied thefollowing simple diagnostic to identify ?passive?
en-tries: (a) there is an Ext argument realising frameelement e; and (b) there is some other entry with thesame orthographic form and semantic frame, whichhas an Obj argument realising frame element e.Initially we applied this diagnostic to the entriesin the harvested lexicon together with a part-of-speech tag filter.
The current FrameNet release in-cludes standard POS-tag information for each wordin each annotated sentence.
We considered onlythose lexical entries derived from sentences whosetarget verb is tagged as a ?past-participle?
form (i.e.VVN).
This technique identified 4,160 entries in theharvested lexicon (i.e.
16% of the total) as being?passive?.
A random sample of 10% of these wasexamined and no false positives were found.The diagnostic test was then repeated on the re-maining lexical entries, this time without the POS-tag filter.
This was deemed necessary in order topick up false negatives caused by the POS-taggerhaving assigned the wrong tag to a passive targetverb (generally the past tense form tag VVD).
Thistest identified a further 1007 entries as ?passive?
(4%of the total entries).
As well as mis-tagged instancesof normal passives, this test picked up a further threeclasses of entry derived from target verbs appearingin passive-related constructions.
The first of theseinvolves cases where the target verb is in the com-plement of a ?raising adjective?
(e.g.
tough, difficult,easy, impossible), for example:[Ext NP Goal Both planning and control] are dif-ficult to achieve [Dep PP Circs in this form ofproduction]The current FrameNet annotation guidelines (Rup-penhofer et al, 2006) state that the extracted objectin these cases should be tagged as Obj.
However,in practice, the majority of these instances appear tohave been tagged as Ext.4Whilst there are dedicated subcorpora containing only pas-sive targets, it is not the case that all passive targets are in these.The second group of passive-related entries in-volve verbs in the need -ing construction5 , e.g.
:[Ext NP Content Many private problems] needairing [Dep PP Medium in the family]The third group involved sentences where the targetverb is used in the ?middle?
construction:[Ext Experiencer You] frighten [DepManner easily]Again, linguistically-motivated grammars generallytreat these three constructions in the rule componentrather than the lexicon.
Thus, the lexical entries de-rived from these sentences need to be located andrepaired, perhaps by comparison with other entries.Of the 1007 lexical entries identified by the sec-ond, weaker form of the passive test, 224 (i.e.
22%)turn out to be false positives.
The vast majorityof these involve verbs implicated in the causative-inchoative alternation (e.g.
John?s back arched vs.John arched his back).
The official FrameNet pol-icy is to distinguish between frames encoding achange-of-state and those encoding the causationof such a change, for example Amalgamationversus Cause to amalgamate, Motion versusCause motion etc.
In each case, the two framesare linked by the Causative of frame relation.Most of the false positives are the result of a fail-ure to consistently apply this principle in annotationpractice, for example where no causative counterparthas been defined for a particular inchoative frame,or where an inchoative target has been assigned to acausative frame, or a causative target to an inchoa-tive frame.
For example, 94 of the false positivesare accounted for simply by the lack of a causativecounterpart for the Body movement frame, mean-ing that both inchoative and causative uses of verbslike arch, flutter and wiggle have all been assignedto the same frame.For reasons of data sparsity, it is expected that theapproach to identifying passive uses of target verbsdiscussed here will result in false negatives, since itrelies on there being at least one corresponding ac-tive use in the corpus.
We checked a random sampleof 400 of the remaining entries in the harvested lex-icon and found nine false negatives, suggesting that5Alternatively merit -ing, bear -ing etc.116the test successfully identifies 91% of those lexicalentries derived from passive uses of target verbs.4.3 ModifiersGeneral linguistic theory makes a distinction be-tween two kinds of non-subject dependent of a verb,depending on the notional ?closeness?
of the seman-tic relation ?
complements vs. modifiers.
Take forexample the following sentence:[Ext Performer She]?s [Dep Time currently]starring [Dep Performance in The CemeteryClub] [Dep Place at the Wyvern Theatre]Of the three constituents annotated here as Dep,only one is an complement (the Performance);the Time and Place dependents are modifiers.Frame-based NLU systems do not generally listmodifiers in the argument structure of a verb?s lexi-cal entry.
Thus, we need to find a means of identify-ing those dependents in the harvested lexicon whichare actually modifiers.The FrameNet ontology provides some informa-tion to help differentiate complements and modi-fiers.
A frame element can be marked as Core,signifying that it ?instantiates a conceptually nec-essary component of a frame, while making theframe unique and different from other frames?.
Theannotation guidelines state that the distinction be-tween Core and non-Core frame elements cov-ers ?the semantic spirit?
of the distinction betweencomplements and modifiers.
Thus, for example,obligatory dependents are always Core, as are:(a) those which, when omitted, receive a definiteinterpretation (e.g.
the Goal in John arrived);and (b) those whose semantics cannot be predictedfrom their form.
In the Performers and rolesframe used in the example above, the Performerand Performance frame elements are marked asCore, whilst Time and Place are not.However, it is not clear that the notion of on-tological ?coreness?
used in FrameNet correspondswell with the intuitive distinction between syntacticcomplements and modifiers.
This is exemplified bythe existence of numerous constituents in the corpuswhich have been marked as direct objects, despiteinvoking non-Core frame elements, for example:[Agent I] ripped [Subregion the top][Patient from my packet of cigarettes]The relevant frame here is Damaging, where theSubregion frame element is marked as non-Core, based on examples like John ripped histrousers [below the knee].
Thus in this case, thedecision to retain all senses of the verb rip withinthe same frame has led to a situation where seman-tic and syntactic coreness have become dislocated.Thus, although the Core vs. non-Core property onframe elements does yield a certain amount of in-formation about which arguments are complementsand which are modifiers, greater care needs to betaken when assigning different subcategorisation al-ternants to the same frame.
For example, it wouldhave been more convenient to have assigned the verbrip in the above example to the Removing frame,where the direct object would then be assigned theCore frame element Theme.In the example discussed above, FrameNet doesprovide syntactic role information (Obj) allowingus to infer that a non-Core role is a complementrather than a modifier.
Where the syntactic role issimply marked as Dep however, it is not possibleto make the decision without recourse to other lexi-cal resources (e.g.
ComLex).
Since different parsersmay utilise different criteria for distinguishing com-plements from modifiers, it might be better to post-pone this task to the syntactic alignment module.4.4 Control verbsUnification-based parsers generally handle the dis-tinction between subject (John promised Mary togo) and object (John persuaded Mary to go) con-trol verbs in the lexicon, using coindexation of thesubject/object of the control verb and the understoodsubject of the embedded verb.
The parser can usethis lexical information to assign the correct refer-ent to the understood subject in a sentence like Johnasked Mary to go:?????
?commandAGENT JohnTHEME Mary 1EFFECT[motionTHEME 1]?????
?Control verbs are annotated in FrameNet in the fol-lowing manner:Perhaps [Ext NP Speaker we] can persuade[Obj NP Addressee Tammuz] [Dep VPto117Content to entertain him]The lexical entries for transitive control verbs thatwe can harvest directly from these annotations thusfail to identify whether it is the subject or the directobject which controls the understood subject of theembedded verb.We attempted to automatically distinguish subjectfrom object control in FrameNet by looking for theannotated sentences that contain independently an-notated argument structures for both the control verband embedded verb.
For example, let?s assume thefollowing annotation also exists in the corpus:Perhaps we can persuade [Ext NP Agent Tam-muz] to entertain [Obj NP Experiencer him]We can then use the fact that it is the object of thecontrol verb which is coextensive with the Ext ofthe embedded verb to successfully identify persuadeas an object-control verb.The problem with this approach is data sparsity.The harvested lexicon contains 135 distinct verbswhich subcategorise for both a direct object anda controlled VP complement.
In a random sam-ple of ten of these none of the annotated sentenceshad been annotated independently from the perspec-tive of the governed verb.
As the proportion of theFrameNet corpus which involves annotation of run-ning text, rather than cherry-picked example sen-tences, increases, we would expect this to improve.65 Implementation and discussionThe revised version of the harvested lexicon con-tains 9,019 entries for 2,626 orthography-framepairs, yielding a functionality ratio of 3.4.This lexicon still requires a certain amount ofcleaning up.
For example, the verb accompany isassigned to a number of distinct lexical entries de-pending on the semantic role associated with the PPcomplement (i.e.
Goal, Path or Source).
Caseslike this, where the role name is determined by theparticular choice of preposition, could be handledoutside the lexicon.
Alternatively, it may be possibleto use the ?core set?
feature of the FrameNet ontol-ogy (which groups together roles that are judged to6An alternative approach would be to consult an externallexical resource, e.g.
the LinGO ERG lexicon, which has goodcoverage of control verbs.be equivalent in some sense) to locate this kind of re-dundancy.
Other problems involve sentences wherea possessive determiner has been annotated as thesubject of a verb, e.g.
It was [his] intention to aidLarsen, resulting in numerous spurious entries.The harvested lexical entries are encoded ac-cording to a framework-independent XML schema,which we developed with the aim of deriving lexi-cons for use with a diverse range of parsers.
At themoment, several additional steps are required to con-vert the entries we extracted into a format suitablefor a particular parser.Firstly, the syntactic categories used by FrameNetand the target lexicon have to be reconciled.
Whilebasic constituent types such as noun and adjectivephrases do not change between the theories, smalldifferences may still exist.
For example, the TRIPSparser classifies all wh-clauses such as what he didin I saw what he did and What he did was good asnoun phrases, the LinGO ERG grammar interpretsthem as either noun phrases or clauses depending onthe context, and FrameNet annotation classifies allof them as clauses.
The alignment, however, shouldbe relative straightforward as there is, in general,good agreement on the basic syntactic categories.7Secondly, the information relevant to constituentordering may need to be derived, as discussed inSection 4.
Finally, the more abstract features such ascontrol have to be converted into feature structuresappropriate for the unification parsers.
Our schemaincorporates the possibility for embedded categorystructure, as in the treatment of control verbs in CCGand HPSG where the verbal complement is an ?un-saturated?
category.
We plan to use our schemaas a platform for deriving richer lexical represen-tations from the ?flatter?
entries harvested directlyfrom FrameNet.As part of our future work, we expect to creategeneric algorithms that help automate these steps.
Inparticular, we plan to include a domain-independentset of constituent categories and syntactic role la-bels, and add algorithms that convert between a lin-ear ordering and a set of functional labels, for exam-ple (Crabbe?
et al, 2006).
We also plan to developalgorithms to import information from other seman-7http://www.cl.cam.ac.uk/users/alk23/classes/Classes2.txtcontains a list of mappings between three different deep parsersand ComLex subcategorisation frames118tic lexicons such as VerbNet into the same schema.Currently, we have implemented an algorithm forconverting the harvested entries into the TRIPS lex-icon format, resulting in a 6133 entry verb lexiconinvolving 2654 distinct orthography-type pairs.
Thislexicon has been successfully used with the TRIPSparser, but additional work remains to be done be-fore the conversion process is complete.
For exam-ple, we need a more sophisticated approach to re-solving the complement-modifier distinction, alongwith a means of integrating the FrameNet semantictypes with the TRIPS ontology so the parser can useselectional restrictions to disambiguate.The discussion in this paper has been mainly fo-cused on extracting entries for a deep lexicons us-ing frame-based NLU, but similar issues have beenfaced also by the developers of shallow semanticparsers from semantically annotated corpora.
Forexample, Gildea and Jurafsky (2002) found thatidentifying passives was important in training a se-mantic role classifier from FrameNet, using a parsertrained on the Penn Treebank along with a set oftemplates to distinguish passive constructions fromactive ones.
Similarly, Chen and Rambow (2003)argue that the kind of deep linguistic features weharvest from FrameNet is beneficial for the success-ful assignment of PropBank roles to constituents, inthis case using TAGs generated from PropBank togenerate the relevant features.
From this perspec-tive, our harvested lexicon can be seen as providing a?cleaned-up?, filtered version of FrameNet for train-ing semantic interpreters.
It may also be utilised toprovide information for a separate lexical interpreta-tion and disambiguation module to be built on top ofa syntactic parser.6 ConclusionWe have developed both a procedure and aframework-independent representation schema forharvesting lexical information for deep NLP systemsfrom the FrameNet semantically annotated corpus.In examining the feasibility of this approach to in-creasing lexical coverage, we have identified a num-ber of constructions for which current FrameNet an-notation practice leads to problems in deriving ap-propriate lexical entries, for example imperatives,passives and control.7 AcknowledgementsThe work reported here was supported by grantsN000140510043 and N000140510048 from the Of-fice of Naval Research.ReferencesC.
F. Baker, C. Fillmore, and J.
B. Lowe.
1998.The Berkeley FrameNet Project.
In Proceedings ofCOLING-ACL?98, Montreal, pages 86?90.J.
Chen and O. Rambow.
2003.
Use of deep linguisticfeatures for the recognition and labeling of semanticarguments.
In Proceedings of EMNLP?03, Sapporo,Japan.A.
Copestake and D. Flickinger.
2000.
An open-source grammar development environment and broad-coverage English grammar using HPSG.
In Proceed-ings of LREC?00, Athens, Greece, pages 591?600.B.
Crabbe?, M. O. Dzikovska, W. de Beaumont, andM.
Swift.
2006.
Increasing the coverage of a domainindependent dialogue lexicon with VerbNet.
In Pro-ceedings of ScaNaLU?06, New York City.M.
O. Dzikovska.
2004.
A Practical Semantic Repre-sentation for Natural Language Parsing.
Ph.D. thesis,University of Rochester, Rochester NY.D.
Gildea and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.R.
Grishman, C. MacLeod, and A. Meyers.
1994.
Com-lex syntax: Building a computational lexicon.
In Pro-ceedings of COLING?94, Kyoto, Japan, pages 268?272.J.
Hockenmaier and M. Steedman.
2002.
AcquiringCompact Lexicalized Grammars from a Cleaner Tree-bank.
In Proceedings of LREC?02, Las Palmas, Spain.K.
Kipper, H. T. Dang, and M. Palmer.
2000.
Class-based construction of a verb lexicon.
In Proceedingsof AAAI?00, Austin TX.J.
Ruppenhofer, M. Ellsworth, M. R. L. Petruck, C. R.Johnson, and J. Scheffczyk, 2006.
FrameNet II: Ex-tended Theory and Practice.
The Berkeley FrameNetProject, August.M.
White.
2006.
Efficient realization of coordinate struc-tures in Combinatory Categorial Grammar.
Researchon Language and Computation, 4(1):39?75.119
