Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 1?5,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLexicographic Semirings for Exact Automata Encoding of Sequence ModelsBrian Roark, Richard Sproat, and Izhak Shafran{roark,rws,zak}@cslu.ogi.eduAbstractIn this paper we introduce a novel use of thelexicographic semiring and motivate its usefor speech and language processing tasks.
Weprove that the semiring allows for exact en-coding of backoff models with epsilon tran-sitions.
This allows for off-line optimizationof exact models represented as large weightedfinite-state transducers in contrast to implicit(on-line) failure transition representations.
Wepresent preliminary empirical results demon-strating that, even in simple intersection sce-narios amenable to the use of failure transi-tions, the use of the more powerful lexico-graphic semiring is competitive in terms oftime of intersection.1 Introduction and MotivationRepresenting smoothed n-gram language models asweighted finite-state transducers (WFST) is mostnaturally done with a failure transition, which re-flects the semantics of the ?otherwise?
formulationof smoothing (Allauzen et al, 2003).
For example,the typical backoff formulation of the probability ofa word w given a history h is as followsP(w | h) ={P(w | h) if c(hw) > 0?hP(w | h?)
otherwise(1)where P is an empirical estimate of the probabil-ity that reserves small finite probability for unseenn-grams; ?h is a backoff weight that ensures nor-malization; and h?
is a backoff history typicallyachieved by excising the earliest word in the his-tory h. The principle benefit of encoding the WFSTin this way is that it only requires explicitly storingn-gram transitions for observed n-grams, i.e., countgreater than zero, as opposed to all possible n-gramsof the given order which would be infeasible in forexample large vocabulary speech recognition.
Thisis a massive space savings, and such an approach isalso used for non-probabilistic stochastic languagemodels, such as those trained with the perceptronalgorithm (Roark et al, 2007), as the means to ac-cess all and exactly those features that should firefor a particular sequence in a deterministic automa-ton.
Similar issues hold for other finite-state se-quence processing problems, e.g., tagging, bracket-ing or segmenting.Failure transitions, however, are an implicitmethod for representing a much larger explicit au-tomaton ?
in the case of n-gram models, all pos-sible n-grams for that order.
During compositionwith the model, the failure transition must be inter-preted on the fly, keeping track of those symbolsthat have already been found leaving the originalstate, and only allowing failure transition traversalfor symbols that have not been found (the semanticsof ?otherwise?).
This compact implicit representa-tion cannot generally be preserved when composingwith other models, e.g., when combining a languagemodel with a pronunciation lexicon as in widely-used FST approaches to speech recognition (Mohriet al, 2002).
Moving from implicit to explicit repre-sentation when performing such a composition leadsto an explosion in the size of the resulting trans-ducer, frequently making the approach intractable.In practice, an off-line approximation to the modelis made, typically by treating the failure transitionsas epsilon transitions (Mohri et al, 2002; Allauzenet al, 2003), allowing large transducers to be com-posed and optimized off-line.
These complex ap-proximate transducers are then used during first-passdecoding, and the resulting pruned search graphs(e.g., word lattices) can be rescored with exact lan-guage models encoded with failure transitions.Similar problems arise when building, say, POS-taggers as WFST: not every pos-tag sequence willhave been observed during training, hence failuretransitions will achieve great savings in the size ofmodels.
Yet discriminative models may includecomplex features that combine both input stream(word) and output stream (tag) sequences in a singlefeature, yielding complicated transducer topologiesfor which effective use of failure transitions may not1be possible.
An exact encoding using other mecha-nisms is required in such cases to allow for off-linerepresentation and optimization.In this paper, we introduce a novel use of a semir-ing ?
the lexicographic semiring (Golan, 1999) ?which permits an exact encoding of these sorts ofmodels with the same compact topology as with fail-ure transitions, but using epsilon transitions.
Unlikethe standard epsilon approximation, this semiring al-lows for an exact representation, while also allow-ing (unlike failure transition approaches) for off-linecomposition with other transducers, with all the op-timizations that such representations provide.In the next section, we introduce the semiring, fol-lowed by a proof that its use yields exact represen-tations.
We then conclude with a brief evaluation ofthe cost of intersection relative to failure transitionsin comparable situations.2 The Lexicographic SemiringWeighted automata are automata in which the tran-sitions carry weight elements of a semiring (Kuichand Salomaa, 1986).
A semiring is a ring that maylack negation, with two associative operations?
and?
and their respective identity elements 0 and 1.
Acommon semiring in speech and language process-ing, and one that we will be using in this paper, isthe tropical semiring (R?
{?
},min,+,?, 0), i.e.,min is the ?
of the semiring (with identity?)
and+ is the ?
of the semiring (with identity 0).
This isappropriate for performing Viterbi search using neg-ative log probabilities ?
we add negative logs alonga path and take the min between paths.A ?W1,W2 .
.
.Wn?-lexicographic weight is a tu-ple of weights where each of the weight classesW1,W2 .
.
.Wn, must observe the path property(Mohri, 2002).
The path property of a semiring Kis defined in terms of the natural order on K suchthat: a <K b iff a ?
b = a.
The tropical semiringmentioned above is a common example of a semir-ing that observes the path property, since:w1 ?
w2 = min{w1, w2}w1 ?
w2 = w1 + w2The discussion in this paper will be restricted tolexicographic weights consisting of a pair of tropi-cal weights ?
henceforth the ?T, T ?-lexicographicsemiring.
For this semiring the operations ?
and ?are defined as follows (Golan, 1999, pp.
223?224):?w1, w2?
?
?w3, w4?
=????????
?if w1 < w3 or?w1, w2?
(w1 = w3 &w2 < w4)?w3, w4?
otherwise?w1, w2?
?
?w3, w4?
= ?w1 + w3, w2 + w4?The term ?lexicographic?
is an apt term for thissemiring since the comparison for ?
is like the lexi-cographic comparison of strings, comparing the firstelements, then the second, and so forth.3 Language model encoding3.1 Standard encodingFor language model encoding, we will differentiatebetween two classes of transitions: backoff arcs (la-beled with a ?
for failure, or with  using our newsemiring); and n-gram arcs (everything else, labeledwith the word whose probability is assigned).
Eachstate in the automaton represents an n-gram historystring h and each n-gram arc is weighted with the(negative log) conditional probability of the word wlabeling the arc given the history h. For a given his-tory h and n-gram arc labeled with a word w, thedestination of the arc is the state associated with thelongest suffix of the string hw that is a history in themodel.
This will depend on the Markov order of then-gram model.
For example, consider the trigrammodel schematic shown in Figure 1, in which onlyhistory sequences of length 2 are kept in the model.Thus, from history hi = wi?2wi?1, the word witransitions to hi+1 = wi?1wi, which is the longestsuffix of hiwi in the model.As detailed in the ?otherwise?
semantics of equa-tion 1, backoff arcs transition from state h to a stateh?, typically the suffix of h of length |h| ?
1, withweight (?
log?h).
We call the destination state abackoff state.
This recursive backoff topology ter-minates at the unigram state, i.e., h = , no history.Backoff states of order k may be traversed eithervia ?-arcs from the higher order n-gram of order k+1 or via an n-gram arc from a lower order n-gram oforder k?1.
This means that no n-gram arc can enterthe zeroeth order state (final backoff), and full-orderstates ?
history strings of length n?
1 for a modelof order n ?
may have n-gram arcs entering fromother full-order states as well as from backoff statesof history size n?
2.3.2 Encoding with lexicographic semiringFor an LM machineM on the tropical semiring withfailure transitions, which is deterministic and has the2h i =wi-2wi-1 hi+1 =wi-1wiwi /-logP(wi |h i)wi-1?/-log ?hiwi?/-log ?h i+1wi /-logP(wi|wi-1)?/-log ?w i-1 wi /-logP(wi)Figure 1: Deterministic finite-state representation of n-grammodels with negative log probabilities (tropical semiring).
Thesymbol ?
labels backoff transitions.
Modified from Roark andSproat (2007), Figure 6.1.path property, we can simulate ?-arcs in a standardLM topology by a topologically equivalent machineM ?
on the lexicographic ?T, T ?
semiring, where ?has been replaced with epsilon, as follows.
For everyn-gram arc with label w and weight c, source statesi and destination state sj , construct an n-gram arcwith label w, weight ?0, c?, source state s?i, and des-tination state s?j .
The exit cost of each state is con-structed as follows.
If the state is non-final, ??,?
?.Otherwise if it final with exit cost c it will be ?0, c?.Let n be the length of the longest history string inthe model.
For every ?-arc with (backoff) weightc, source state si, and destination state sj repre-senting a history of length k, construct an -arcwith source state s?i, destination state s?j , and weight???
(n?k), c?, where ?
> 0 and ??
(n?k) takes ?
tothe (n ?
k)th power with the ?
operation.
In thetropical semiring, ?
is +, so ??
(n?k) = (n ?
k)?.For example, in a trigram model, if we are backingoff from a bigram state h (history length = 1) to aunigram state, n ?
k = 2 ?
0 = 2, so we set thebackoff weight to ?2?,?
log?h) for some ?
> 0.In order to combine the model with another au-tomaton or transducer, we would need to also con-vert those models to the ?T, T ?
semiring.
For theseautomata, we simply use a default transformationsuch that every transition with weight c is assignedweight ?0, c?.
For example, given a word latticeL, we convert the lattice to L?
in the lexicographicsemiring using this default transformation, and thenperform the intersection L?
?M ?.
By removing ep-silon transitions and determinizing the result, thelow cost path for any given string will be retainedin the result, which will correspond to the pathachieved with ?-arcs.
Finally we project the seconddimension of the ?T, T ?
weights to produce a latticein the tropical semiring, which is equivalent to theresult of L ?M , i.e.,C2(det(eps-rem(L?
?M ?)))
= L ?Mwhere C2 denotes projecting the second-dimensionof the ?T, T ?
weights, det(?)
denotes determiniza-tion, and eps-rem(?)
denotes -removal.4 ProofWe wish to prove that for any machine N ,ShortestPath(M ?
?
N ?)
passes through the equiv-alent states in M ?
to those passed through in M forShortestPath(M ?
N).
Therefore determinizationof the resulting intersection after -removal yieldsthe same topology as intersection with the equiva-lent ?
machine.
Intuitively, since the first dimensionof the ?T, T ?
weights is 0 for n-gram arcs and > 0for backoff arcs, the shortest path will traverse thefewest possible backoff arcs; further, since higher-order backoff arcs cost less in the first dimension ofthe ?T, T ?
weights in M ?, the shortest path will in-clude n-gram arcs at their earliest possible point.We prove this by induction on the state-sequenceof the path p/p?
up to a given state si/s?i in the respec-tive machines M/M ?.Base case: If p/p?
is of length 0, and therefore thestates si/s?i are the initial states of the respective ma-chines, the proposition clearly holds.Inductive step: Now suppose that p/p?
visitss0...si/s?0...s?i and we have therefore reached si/s?iin the respective machines.
Suppose the cumulatedweights of p/p?
are W and ?
?,W ?, respectively.
Wewish to show that whichever sj is next visited on p(i.e., the path becomes s0...sisj) the equivalent states?
is visited on p?
(i.e., the path becomes s?0...s?is?j).Let w be the next symbol to be matched leavingstates si and s?i.
There are four cases to consider:(1) there is an n-gram arc leaving states si and s?i la-beled with w, but no backoff arc leaving the state;(2) there is no n-gram arc labeled with w leaving thestates, but there is a backoff arc; (3) there is no n-gram arc labeled with w and no backoff arc leavingthe states; and (4) there is both an n-gram arc labeledwith w and a backoff arc leaving the states.
In cases(1) and (2), there is only one possible transition totake in either M or M ?, and based on the algorithmfor construction of M ?
given in Section 3.2, thesetransitions will point to sj and s?j respectively.
Case(3) leads to failure of intersection with either ma-chine.
This leaves case (4) to consider.
In M , sincethere is a transition leaving state si labeled with w,3the backoff arc, which is a failure transition, can-not be traversed, hence the destination of the n-gramarc sj will be the next state in p. However, in M ?,both the n-gram transition labeled with w and thebackoff transition, now labeled with , can be tra-versed.
What we will now prove is that the shortestpath through M ?
cannot include taking the backoffarc in this case.In order to emit w by taking the backoff arc outof state s?i, one or more backoff () transitions mustbe taken, followed by an n-gram arc labeled withw.
Let k be the order of the history representedby state s?i, hence the cost of the first backoff arcis ?(n?
k)?,?
log(?s?i)?
in our semiring.
If wetraverse m backoff arcs prior to emitting the w,the first dimension of our accumulated cost will bem(n?
k+ m?12 )?, based on our algorithm for con-struction of M ?
given in Section 3.2.
Let s?l be thedestination state after traversing m backoff arcs fol-lowed by an n-gram arc labeled with w. Note that,by definition, m ?
k, and k ?
m + 1 is the or-der of state s?l.
Based on the construction algo-rithm, the state s?l is also reachable by first emit-ting w from state s?i to reach state s?j followed bysome number of backoff transitions.
The order ofstate s?j is either k (if k is the highest order in themodel) or k + 1 (by extending the history of states?i by one word).
If it is of order k, then it will re-quire m?
1 backoff arcs to reach state s?l, one fewerthan the path to state s?l that begins with a back-off arc, for a total cost of (m?
1)(n?
k + m?12 )?which is less than m(n?
k + m?12 )?.
If states?j is of order k + 1, there will be m backoffarcs to reach state s?l, but with a total cost ofm(n?
(k + 1) + m?12 )?
= m(n?
k +m?32 )?which is also less than m(n?
k + m?12 )?.
Hencethe state s?l can always be reached from s?i with alower cost through state s?j than by first taking thebackoff arc from s?i.
Therefore the shortest path onM ?
must follow s?0...s?is?j .
2This completes the proof.5 Experimental Comparison of , ?
and?T, T ?
encoded language modelsFor our experiments we used lattices derived from avery large vocabulary continuous speech recognitionsystem, which was built for the 2007 GALE Ara-bic speech recognition task, and used in the workreported in Lehr and Shafran (2011).
The lexico-graphic semiring was evaluated on the developmentset (2.6 hours of broadcast news and conversations;18K words).
The 888 word lattices for the develop-ment set were generated using a competitive base-line system with acoustic models trained on about1000 hrs of Arabic broadcast data and a 4-gram lan-guage model.
The language model consisting of122M n-grams was estimated by interpolation of 14components.
The vocabulary is relatively large at737K and the associated dictionary has only singlepronunciations.The language model was converted to the automa-ton topology described earlier, and represented inthree ways: first as an approximation of a failuremachine using epsilons instead of failure arcs; sec-ond as a correct failure machine; and third using thelexicographic construction derived in this paper.The three versions of the LM were evaluated byintersecting them with the 888 lattices of the de-velopment set.
The overall error rate for the sys-tems was 24.8%?comparable to the state-of-the-art on this task1.
For the shortest paths, the failureand lexicographic machines always produced iden-tical lattices (as determined by FST equivalence);in contrast, 81% of the shortest paths from the ep-silon approximation are different, at least in termsof weights, from the shortest paths using the failureLM.
For full lattices, 42 (4.7%) of the lexicographicoutputs differ from the failure LM outputs, due tosmall floating point rounding issues; 863 (97%) ofthe epsilon approximation outputs differ.In terms of size, the failure LM, with 5.7 mil-lion arcs requires 97 Mb.
The equivalent ?T, T ?-lexicographic LM requires 120 Mb, due to the dou-bling of the size of the weights.2 To measure speed,we performed the intersections 1000 times for eachof our 888 lattices on a 2993 MHz Intel R?
Xeon R?CPU, and took the mean times for each of our meth-ods.
The 888 lattices were processed with a meanof 1.62 seconds in total (1.8 msec per lattice) us-ing the failure LM; using the ?T, T ?-lexicographicLM required 1.8 seconds (2.0 msec per lattice), andis thus about 11% slower.
Epsilon approximation,where the failure arcs are approximated with epsilonarcs took 1.17 seconds (1.3 msec per lattice).
The1The error rate is a couple of points higher than in Lehr andShafran (2011) since we discarded non-lexical words, which areabsent in maximum likelihood estimated language model andare typically augmented to the unigram backoff state with anarbitrary cost, fine-tuned to optimize performance for a giventask.2If size became an issue, the first dimension of the ?T, T ?-weight can be represented by a single byte.4slightly slower speeds for the exact method using thefailure LM, and ?T, T ?
can be related to the over-head of computing the failure function at runtime,and determinization, respectively.6 ConclusionIn this paper we have introduced a novel applica-tion of the lexicographic semiring, proved that itcan be used to provide an exact encoding of lan-guage model topologies with failure arcs, and pro-vided experimental results that demonstrate its ef-ficiency.
Since the ?T, T ?-lexicographic semiringis both left- and right-distributive, other optimiza-tions such as minimization are possible.
The par-ticular ?T, T ?-lexicographic semiring we have usedhere is but one of many possible lexicographic en-codings.
We are currently exploring the use of alexicographic semiring that involves different semir-ings in the various dimensions, for the integration ofpart-of-speech taggers into language models.An implementation of the lexicographic semir-ing by the second author is already available aspart of the OpenFst package (Allauzen et al, 2007).The methods described here are part of the NGramlanguage-model-training toolkit, soon to be releasedat opengrm.org.AcknowledgmentsThis research was supported in part by NSF Grant#IIS-0811745 and DARPA grant #HR0011-09-1-0041.
Any opinions, findings, conclusions or recom-mendations expressed in this publication are those ofthe authors and do not necessarily reflect the viewsof the NSF or DARPA.
We thank Maider Lehr forhelp in preparing the test data.
We also thank theACL reviewers for valuable comments.ReferencesCyril Allauzen, Mehryar Mohri, and Brian Roark.
2003.Generalized algorithms for constructing statistical lan-guage models.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics, pages 40?47.Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-ciech Skut, and Mehryar Mohri.
2007.
OpenFst: Ageneral and efficient weighted finite-state transducerlibrary.
In Proceedings of the Twelfth InternationalConference on Implementation and Application of Au-tomata (CIAA 2007), Lecture Notes in Computer Sci-ence, volume 4793, pages 11?23, Prague, Czech Re-public.
Springer.Jonathan Golan.
1999.
Semirings and their Applications.Kluwer Academic Publishers, Dordrecht.Werner Kuich and Arto Salomaa.
1986.
Semirings,Automata, Languages.
Number 5 in EATCS Mono-graphs on Theoretical Computer Science.
Springer-Verlag, Berlin, Germany.Maider Lehr and Izhak Shafran.
2011.
Learning a dis-criminative weighted finite-state transducer for speechrecognition.
IEEE Transactions on Audio, Speech, andLanguage Processing, July.Mehryar Mohri, Fernando C. N. Pereira, and MichaelRiley.
2002.
Weighted finite-state transducers inspeech recognition.
Computer Speech and Language,16(1):69?88.Mehryar Mohri.
2002.
Semiring framework and algo-rithms for shortest-distance problems.
Journal of Au-tomata, Languages and Combinatorics, 7(3):321?350.Brian Roark and Richard Sproat.
2007.
ComputationalApproaches to Morphology and Syntax.
Oxford Uni-versity Press, Oxford.Brian Roark, Murat Saraclar, and Michael Collins.
2007.Discriminative n-gram language modeling.
ComputerSpeech and Language, 21(2):373?392.5
