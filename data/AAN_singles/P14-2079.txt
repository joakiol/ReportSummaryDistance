Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 482?487,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsHow Well can We LearnInterpretable Entity Types from Text?Dirk HovyCenter for Language TechnologyUniversity of CopenhagenNjalsgade 140, 2300 Copenhagendirk@cst.dkAbstractMany NLP applications rely on type sys-tems to represent higher-level classes.Domain-specific ones are more informa-tive, but have to be manually tailored toeach task and domain, making them in-flexible and expensive.
We investigate alargely unsupervised approach to learninginterpretable, domain-specific entity typesfrom unlabeled text.
It assumes that anycommon noun in a domain can function aspotential entity type, and uses those nounsas hidden variables in a HMM.
To con-strain training, it extracts co-occurrencedictionaries of entities and common nounsfrom the data.
We evaluate the learnedtypes by measuring their prediction ac-curacy for verb arguments in several do-mains.
The results suggest that it is pos-sible to learn domain-specific entity typesfrom unlabeled data.
We show significantimprovements over an informed baseline,reducing the error rate by 56%.1 IntroductionMany NLP applications, such as question answer-ing (QA) or information extraction (IE), use typesystems to represent relevant semantic classes.Types allow us to find similarities at a higher levelto group lexically different entities together.
Thishelps to filter out candidates that violate certainconstraints (e.g., in QA, if the intended answertype is PERSON, we can ignore all candidate an-swers with a different type), but is also used forfeature generation and fact-checking.A central question is: where do the typescome from?
Typically, they come from a hand-constructed set.
This has some disadvantages.Domain-general types, such as named entities orWordNet supersenses (Fellbaum, 1998), often failto capture critical domain-specific information (inthe medical domain, we might want ANTIBI-OTIC, SEDATIVE, etc., rather than just ARTI-FACT).
Domain-specific types perform much bet-ter (Ferrucci et al, 2010), but must be manuallyadapted to each new domain, which is expensive.Alternatively, unsupervised approaches (Ritter etal., 2010) can be used to learn clusters of similarwords, but the resulting types (=cluster numbers)are not human-interpretable, which makes analy-sis difficult.
Furthermore, it requires us to definethe number of clusters beforehand.Ideally, we would like to learn domain-specifictypes directly from data.
To this end, pattern-based approaches have long been used to inducetype systems (Hearst, 1992; Kozareva et al, 2008).Recently, Hovy et al (2011) proposed an ap-proach that uses co-occurrence patterns to find en-tity type candidates, and then learns their appli-cability to relation arguments by using them as la-tent variables in a first-order HMM.
However, theyonly evaluate their method using human sensibil-ity judgements for one domain.
While this showsthat the types are coherent, it does not tell us muchabout their applicability.We extend their approach with three importantchanges:1. we evaluate the types by measuring accuracywhen using them in an extrinsic task,2.
we evaluate on more than one domain, and3.
we explore a variety of different models.We measure prediction accuracy when us-ing the learned types in a selectional restrictiontask for frequent verbs.
E.g., given the rela-tion throw(X, pass) in the football domain, wecompare the model prediction to the gold dataX=QUARTERBACK.
The results indicate that thelearned types can be used to in relation extractiontasks.482Our contributions in this paper are:?
we empirically evaluate an approach to learn-ing types from unlabeled data?
we investigate several domains and models?
the learned entity types can be used to predictselectional restrictions with high accuracy2 Related WorkIn relation extraction, we have to identify the re-lation elements, and then map the arguments totypes.
We follow an open IE approach (Banko andEtzioni, 2008) and use dependencies to identifythe elements.
In contrast to most previous work(Pardo et al, 2006; Yao et al, 2011; Yao et al,2012), we have no pre-defined set of types, buttry to learn it along with the relations.
Some ap-proaches use types from general data bases suchas Wikipedia, Freebase, etc.
(Yan et al, 2009;Eichler et al, 2008; Syed and Viegas, 2010), side-stepping the question how to construct those DBsin the first place.
We are less concerned with ex-traction performance, but focus on the accuracy ofthe learned type system by measuring how well itperforms in a prediction task.Talukdar et al (2008) and Talukdar and Pereira(2010) present graph-based approaches to the sim-ilar problem of class-instance learning.
Whilethis provides a way to discover types, it requiresa large graph that does not easily generalize tonew instances (transductive), since it produces nopredictive model.
The models we use are trans-ductive and can be applied to unseen data.
Ourapproach follows Hovy et al (2011).
However,they only evaluate one model on football by col-lecting sensibility ratings from Mechanical Turk.Our method provides extrinsic measures of perfor-mance on several domains.3 ModelOur goal is to find semantic type candidates in thedata, and apply them in relation extraction to seewhich ones are best suited.
We restrict ourselvesto verbal relations.
We build on the approach byHovy et al (2011), which we describe briefly be-low.
It consists of two parts: extracting the typecandidates and fitting the model.The basic idea is that semantic types are usu-ally common nouns, often frequent ones from they3y1y2x3x1Montana throw bally3y1y2x3x2throw Montana ballquarterbackplayer throw ballthrowquarterbackplayer balla)b)Figure 1: Example of input sentence x and out-put types for the HMM.
Note that the verb type istreated as observed variable.domain at hand.
Thus all common nouns are pos-sible types, and can be used as latent variables inan HMM.
By estimating emission and transitionparameters with EM, we can learn the subset ofnouns to apply.However, assuming the set of all commonnouns as types is intractable, and would not al-low for efficient learning.
To restrict the searchspace and improve learning, we first have to learnwhich types modify entities and record their co-occurrence, and use this as dictionary.Kleiman: professor:25, expert:13, (specialist:1)Tilton: executive:37, economist:17, (chairman:4, presi-dent:2)Figure 2: Examples of dictionary entries withcounts.
Types in brackets are not considered.Dictionary Construction The number of com-mon nouns in a domain is generally too high toconsider all of them for every entity.
A com-mon way to restrict the number of types is to pro-vide a dictionary that lists all legal types for eachentity (Merialdo, 1994; Ravi and Knight, 2009;T?ackstr?om et al, 2013).
To construct this dictio-nary, we collect for each entity (i.e., a sequenceof words labeled with NNP or NNPS tags) in ourdata all common nouns (NN, NNS) that modify it.These are1.
nominal modifiers (?judge Scalosi ...?),2.
appositions (?Tilton, a professor at ...?
), and3.
copula constructions (?Finton, who is the in-vestor ...?
).These modifications can be collected from the de-pendency parse trees.
For each entity, we store the483type candidates and their associated counts.
SeeFigure 2 for examples.
We only consider typesobserved more than 10 times.
Any entity with-out type information, as well as dictionary entitieswith only singleton types are treated as unknowntokens (?UNK?).
We map UNK to the 50 mostcommon types in the dictionary.
Verbs are con-sidered to each have their own type, i.e., token andlabel for verbs are the same.We do not modify this step.Original Model Hovy et al (2011) constructa HMM using subject-verb-object (SVO) parsetriples as observations, and the type candidates ashidden variables.
Similar models have been usedin (Abney and Light, 1999; Pardo et al, 2006).We estimate the free model parameters with EM(Dempster et al, 1977), run for a fixed number ofiterations (30) or until convergence.Note that Forward-backward EM has time com-plexity of O(N2T ), where N is the number ofstates, and T the number of time steps.
T = 3 inthe model formulations used here, but N is muchlarger than typically found in NLP tasks (see alsoTable 3).
The only way to make this tractable isto restrict the free parameters the model needs toestimate to the transitions.The model is initialized by jointly normalizing1the dictionary counts to obtain the emission pa-rameters, which are then fixed (except for the un-known entities (P (word = UNK|type = ?)).
Tran-sition parameters are initialized uniformly (re-stricted to potentially observable type sequences),and kept as free parameters for the model to opti-mize.Common nouns can be both hidden variablesand observations in the model, so they act like an-notated items: their legal types are restricted to theidentity.
All entities are thus constrained by thedictionary, as in (Merialdo, 1994).
To further con-strain the model, only the top three types of eachentity are considered.
Since the type distributiontypically follows a Zipf curve, this still capturesmost of the information.1This preserves the observed entity-specific distributions.Under conditional normalization, the type candidates fromfrequent entities tend to dominate those of infrequent entities.I.e., the model favors an unlikely candidate for entity a if it isfrequent for entity b.The model can be fully specified asP (x,y) = P (y1)?P (x1|y1)3?i=2P (yi|yi?1)?P (xi|yi)(1)where x is an input triple of a verb and its argu-ments, and y a sequence of types.4 Extending the ModelThe model used by Hovy et al (2011) was a sim-ple first order HMM, with the elements in SVO or-der (see Figure 3a).
We observe two points: we al-ways deal with the same number of elements, andwe have observed variables.
We can thus movefrom a sequential model to a general graphicalmodel by adding transitions and re-arranging thestructure.Since we do not model verbs (they each havetheir identity as type), they act like observed vari-ables.
We can thus move them in first position andcondition the subject on it (3b).y3y1y2OVSy2y1OVSVy2y1OSVy2y1OSa) b)c) d)Figure 3: Original SVO.
model (a), modified VSOorder (b), extension to general models (c and d)By adding additional transitions, we can con-strain the latent variables further.
This is similarto moving from a first to a second order HMM.
Incontrast to the original model, we also distinguishbetween unknown entities in the first and secondargument position.The goal of these modifications is to restrict thenumber of potential values for the argument po-sitions.
This allows us to use the models to typeindividual instances.
In contrast, the objective inHovy et al (2011) was to collect frequent relationtemplates from a domain to populate a knowledgebase.The modifications presented here extend to484Football Finances Lawsystem arg1 arg2 avg ?BL arg1 arg2 avg ?BL arg1 arg2 avg ?BLbaseline 0.28 0.26 0.27 ?
0.39 0.42 0.41 ?
0.37 0.32 0.35 ?orig.
0.05 0.23 0.14 ?0.13 0.08 0.39 0.23 ?0.18 0.06 0.31 0.18 ?0.17VSO, seq.
0.37 0.28 0.32 +0.05 0.38 0.45 0.41 0.0 0.45 0.37 0.41 +0.06SVO, net 0.63 0.60 0.62 +0.35 0.55 0.63 0.59 +0.18 0.69 0.68 0.68 +0.33VSO, net 0.66 0.58 0.62 +0.35 0.61 0.54 0.57 +0.16 0.71 0.62 0.66 +0.31Table 1: Accuracy for most frequent sense baseline and different models on three domains.
Italic num-bers denote significant improvement over baseline (two-tailed t-test at p < 0.01).
?BL = difference tobaseline.Football Finances Lawsystem arg1 arg2 avg arg1 arg2 avg arg1 arg2 avgorig.
0.17 0.38 0.27 0.18 0.52 0.35 0.17 0.48 0.32VSO, seq.
0.56 0.42 0.49 0.55 0.58 0.57 0.61 0.51 0.56SVO, net 0.75 0.69 0.72 0.68 0.73 0.71 0.78 0.77 0.78VSO, net 0.78 0.67 0.72 0.74 0.66 0.70 0.81 0.72 0.76Table 2: Mean reciprocal rank for models on three domains.verbs with more than two arguments, but in thepresent paper, we focus on binary relations.5 ExperimentsSince the labels are induced dynamically from thedata, traditional precision/recall measures, whichrequire a known ground truth, are difficult to ob-tain.
Hovy et al (2011) measured sensibility byobtaining human ratings and measuring weightedaccuracies over all relations.
While this gives anintuition of the general methodology, it is harderto put in context.
Here, we want to evaluate themodel?s performance in a downstream task.
Wemeasure its ability to predict the correct types forverbal arguments.
We evaluate on three differentdomains.As test case, we use a cloze test, or fill-in-the-blank.
We select instances that contain a type-candidate word in subject or object position andreplace that word with the unknown token.
We canthen compare the model?s prediction to the origi-nal word to measure accuracy.5.1 DataLike Yao et al (2012) and Hovy et al (2011), wederive our data from the New York Times (NYT)corpus (Sandhaus, 2008).
It contains several yearsworth of articles, manually annotated with meta-data such as author, content, etc.
Similar to Yaoet al (2012), we use articles whose content meta-data field contains certain labels to distinguish datafrom different domains.
We use the labels Foot-ball2, Law and Legislation, and Finances.We remove meta-data and lists, tokenize, parse,and lemmatize all articles.
We then automaticallyextract subject-verb-object (SVO) triples from theparses, provided the verb is a full verb.
Similarlyto (Pardo et al, 2006), we focus on the top 100full verbs for efficiency reasons, though nothingin our approach prevents us from extending it toall verbs.
For each domain, we select all instanceswhich have a potential type (common noun) in atleast one argument position.
These serve as cor-pus.Football Finances Lawunique types 7,139 18,186 10,618unique entities 38,282 27,528 12,782Table 3: Statistics for the three domains.As test data, we randomly select a subset of1000 instances for each argument, provided theycontain one of the 50 most frequent types in sub-ject or object position, such as player in ?playerthrow pass?.
This serves as gold data.
We thenreplace those types by UNK (i.e., we get ?UNKthrow pass?)
and use this as test set for our model.3Table 3 shows that the domains vary with re-2The data likely differs from Hovy et al (2011).3We omit cases with two unknown arguments, since this485spect to the ratio of unique types to unique enti-ties.
Football uses many different entities (e.g.,team and player names), but has few types (e.g.,player positions), while the other domains usemore types, but fewer entities (e.g., companynames, law firms, etc.
).5.2 EvaluationWe run Viterbi decoding on each test set with ourtrained model to predict the most likely type forthe unknown entities.
We then compare these pre-dictions to the type in the respective gold data andcompute the accuracy for each argument position.As baseline, we predict the argument types mostfrequently observed for the particular verb in train-ing, e.g., predict PLAYER as subject of tackle infootball.
We evaluate the influence of the differentmodel structures on performance.6 ResultsTable 1 shows the accuracy of the different mod-els in the prediction task for the three different do-mains.
The low results of the informed baselineindicate the task complexity.We note that the original model, a bigram HMMwith SVO order (Figure 3a), fails to improve accu-racy over the baseline (although its overall resultswere judged sensible).
Changing the input orderto VSO (Figure 3b) improves accuracy for botharguments over SVO order and the baseline, albeitnot significantly.
The first argument gains more,since conditioning the subject type on the (unam-biguous) verb is more constrained than starting outwith the subject.
Conditioning the object directlyupon the subject creates sparser bigrams, whichcapture ?who does what to whom?.Moving from the HMMs to a general graphi-cal model structure (Figures 3c and d) creates asparser distribution and significantly improves ac-curacy across the board.
Again, the position of theverb makes a difference: in SVO order, accuracyfor the second argument is better, while in VSOorder, accuracy for the subject increases.
This in-dicates that direct conditioning on the verb is thestrongest predictor.
Intuitively, knowing the verbrestricts the possible arguments much more thanknowing the arguments restrict the possible verbs(the types of entities who can throw something arebecomes almost impossible to predict without further context,even for humans (compare ?UNK make UNK?
).limited, but knowing that the subject is a quarter-back still allows all kinds of actions).We also compute the mean reciprocal rank(MRR) for each condition (see Table 2).
MRR de-notes the inverse rank in the model?s k-best outputat which the correct answer occurs, i.e.,1k.
Theresult gives us an intuition of ?how far off?
themodel predictions are.
Across domains, the cor-rect answer is found on average among the toptwo (rank 1.36).
Note that since MRR require k-best outputs, we cannot compute a measure for thebaseline.7 ConclusionWe evaluated an approach to learning domain-specific interpretable entity types from unlabeleddata.
Type candidates are collected from patternsand modeled as hidden variables in graphical mod-els.
Rather than using human sensibility judge-ments, we evaluate prediction accuracy for selec-tional restrictions when using the learned types inthree domains.
The best model improves 35 per-centage points over an informed baseline.
On av-erage, we reduce the error rate by 56%.
We con-clude that it is possible to learn interpretable typesystems directly from data.AcknowledgementsThe author would like to thank Victoria Fossum,Eduard Hovy, Kevin Knight, and the anonymousreviewers for their invaluable feedback.ReferencesSteven Abney and Marc Light.
1999.
Hiding a seman-tic hierarchy in a Markov model.
In Proceedingsof the ACL Workshop on Unsupervised Learning inNatural Language Processing, volume 67.Michele Banko and Oren.
Etzioni.
2008.
The trade-offs between open and traditional relation extraction.Proceedings of ACL-08: HLT, pages 28?36.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incompletedata via the EM algorithm.
Journal of the Royal Sta-tistical Society.
Series B (Methodological), 39(1):1?38.Kathrin Eichler, Holmer Hemsen, and G?unter Neu-mann.
2008.
Unsupervised relation extractionfrom web documents.
LREC.
http://www.
lrecconf.org/proceedings/lrec2008.Christiane Fellbaum.
1998.
WordNet: an electroniclexical database.
MIT Press USA.486David Ferrucci, Eric Brown, Jennifer Chu-Carroll,James Fan, David Gondek, Aditya A Kalyanpur,Adam Lally, J William Murdock, Eric Nyberg, JohnPrager, et al 2010.
Building Watson: An overviewof the DeepQA project.
AI magazine, 31(3):59?79.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 14th conference on Computational linguistics-Volume 2, pages 539?545.
Association for Compu-tational Linguistics.Dirk Hovy, Chunliang Zhang, Eduard Hovy, andAnselmo Pe?nas.
2011.
Unsupervised discovery ofdomain-specific knowledge from text.
In Proceed-ings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 1466?1475, Portland, Oregon,USA, June.
Association for Computational Linguis-tics.Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.2008.
Semantic class learning from the web withhyponym pattern linkage graphs.
Proceedings ofACL-08: HLT, pages 1048?1056.Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational linguistics,20(2):155?171.Thiago Pardo, Daniel Marcu, and Maria Nunes.
2006.Unsupervised Learning of Verb Argument Struc-tures.
Computational Linguistics and IntelligentText Processing, pages 59?70.Sujith Ravi and Kevin Knight.
2009.
MinimizedModels for Unsupervised Part-of-Speech Tagging.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Pro-cessing of the AFNLP, pages 504?512.
Associationfor Computational Linguistics.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A la-tent dirichlet alocation method for selectional pref-erences.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, pages 424?434, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.Evan Sandhaus, editor.
2008.
The New York Times An-notated Corpus.
Number LDC2008T19.
LinguisticData Consortium, Philadelphia.Zareen Syed and Evelyne Viegas.
2010.
A hybridapproach to unsupervised relation discovery basedon linguistic analysis and semantic typing.
In Pro-ceedings of the NAACL HLT 2010 First Interna-tional Workshop on Formalisms and Methodologyfor Learning by Reading, pages 105?113.
Associ-ation for Computational Linguistics.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, RyanMcDonald, and Joakim Nivre.
2013.
Token andtype constraints for cross-lingual part-of-speech tag-ging.
Transactions of the ACL.Partha P. Talukdar and Fernando Pereira.
2010.
Ex-periments in graph-based semi-supervised learningmethods for class-instance acquisition.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 1473?1481.Association for Computational Linguistics.Partha P. Talukdar, Joseph Reisinger, Marcus Pas?ca,Deepak Ravichandran, Rahul Bhagat, and FernandoPereira.
2008.
Weakly-supervised acquisition of la-beled class instances using graph random walks.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 582?590.
Association for Computational Linguistics.Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, ZhengluYang, and Mitsuru Ishizuka.
2009.
Unsupervisedrelation extraction by mining wikipedia texts usinginformation from the web.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 2-Volume 2, pages 1021?1029.
Association forComputational Linguistics.Limin Yao, Aria Haghighi, Sebastian Riedel, and An-drew McCallum.
2011.
Structured relation discov-ery using generative models.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 1456?1466.
Associationfor Computational Linguistics.Limin Yao, Sebastian Riedel, and Andrew McCallum.2012.
Unsupervised relation discovery with sensedisambiguation.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics: Long Papers-Volume 1, pages 712?720.Association for Computational Linguistics.487
