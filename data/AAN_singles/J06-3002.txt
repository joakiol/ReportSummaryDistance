The Notion of Argument in PrepositionalPhrase AttachmentPaola Merlo?University of GenevaEva Esteve Ferrer?University of SussexIn this article we refine the formulation of the problem of prepositional phrase (PP) attachment asa four-way disambiguation problem.
We argue that, in interpreting PPs, both knowledge aboutthe site of the attachment (the traditional noun?verb attachment distinction) and the nature ofthe attachment (the distinction of arguments from adjuncts) are needed.
We introduce a methodto learn arguments and adjuncts based on a definition of arguments as a vector of features.
Ina series of supervised classification experiments, first we explore the features that enable us tolearn the distinction between arguments and adjuncts.
We find that both linguistic diagnosticsof argumenthood and lexical semantic classes are useful.
Second, we investigate the best methodto reach the four-way classification of potentially ambiguous prepositional phrases.
We find thatwhereas it is overall better to solve the problem as a single four-way classification task, verbarguments are sometimes more precisely identified if the classification is done as a two-stepprocess, first choosing the attachment site and then labeling it as argument or adjunct.1.
MotivationIncorrect attachment of prepositional phrases (PPs) often constitutes the largest singlesource of errors in current parsing systems.
Correct attachment of PPs is necessary toconstruct a parse tree that will support the proper interpretation of constituents in thesentence.
Consider the timeworn example(1) I saw the man with the telescope.It is important to determine if the PP with the telescope is to be attached as a sister tothe noun the man, restricting its interpretation, or if it is to be attached to the verb,thereby indicating the instrument of the main action described by the sentence.
Based onexamples of this sort, recent approaches have formalized the problem of disambiguatingPP attachments as a binary choice, distinguishing between attachment of a PP to a givenverb or to the verb?s direct object (Hindle and Rooth 1993; Ratnaparkhi, Reynar, andRoukos 1994; Collins and Brooks 1995; Merlo, Crocker, and Berthouzoz 1997; Stetinaand Nagao 1997; Ratnaparkhi 1997; Zhao and Lin 2004).This is, however, a simplification of the problem, which does not take the natureof the attachment into account.
Precisely, it does not distinguish PP arguments from?
Linguistics Department, University of Geneva, 2 rue de Candolle, 1211 Gene`ve 4, Switzerland.?
Department of Informatics, University of Sussex, Falmer, Brighton BN1 9QH, UK.Submission received: 28 November 2003; revised submission received: 22 June 2005; accepted for publication:4 November 2005.?
2006 Association for Computational LinguisticsComputational Linguistics Volume 32, Number 3PP adjuncts.
Consider the following example, which contains two PPs, both modifyingthe verb.
(2) Put the block on the table in the morning.The first PP is a locative PP required by the subcategorization frame of the verbput, whereas in the morning is an optional descriptor of the time at which the ac-tion was performed.
Although both are attached to the verb, the two PPs entertaindifferent relationships with the verb?the first is an argument whereas the latter isan adjunct.
Analogous examples could be built for attachments to the noun.
(Seeexamples 7a, b.
)Thus, PPs cannot only vary depending on the site to which they attach in thestructure, such as in example (1), but they can fulfill different functions in the sen-tence, such as in example (2).
In principle, then, a given PP could be four-way am-biguous.
In practice, it is difficult and moderately unnatural to construct examplesof four-way ambiguous sentences, sentences that only a good amount of linguis-tic and extralinguistic knowledge can disambiguate among the noun-attached andverb-attached option, with an argument or adjunct interpretation.
It is, however, notimpossible.Consider benefactive constructions, such as the sentence below.
(3) Darcy baked a cake for Elizabeth.In this case the for is a benefactive, hence an argument of the verb bake.
However, thefor-PP is optional; thus other non-argument PPs can occur in the same position.
(4) Darcy baked a cake for 5 shillings/for an hour.Whereas in sentence (3) the PP is an argument, in (4) the PP is an adjunct, as indicatedby the different status of the corresponding passive sentences and by the ordering of thePPs (arguments prefer to come first), as shown in (5) and (6).
(5a) Elizabeth was baked a cake by Darcy(5b) *5 shillings/an hour were baked a cake by Darcy(6a) Darcy baked a cake for Elizabeth for 5 shillings/for an hour(6b) ?
?Darcy baked a cake for 5 shillings/for an hour for ElizabethThis kind of ambiguity also occurs in sentences in which the for-PP is modifyingthe object noun phrase.
Depending on the head noun in object position, and underthe assumption that a beneficiary is an argument, as we have assumed in the sen-tences above, the PP will be an argument or an adjunct, as in the following examples,respectively.
(7a) Darcy baked [cakes for children](7b) Darcy baked [cakes for 5 shillings]342Merlo and Esteve Ferrer Argument in Prepositional Phrase AttachmentModeling both the site and the nature of the attachment of a PP into the treestructure is important.
Distinguishing arguments from adjuncts is key to identifyingthe elements that belong to the semantic kernel of a sentence.
Extracting the kernelof a sentence or phrase, in turn, is necessary for automatic acquisition of importantlexical knowledge, such as subcategorization frames and argument structures, whichis used in several natural language processing (NLP) tasks and applications, suchas parsing, machine translation, and information extraction (Srinivas and Joshi 1999;Dorr 1997; Phillips and Riloff 2002).
This task is fundamentally syntactic in natureand complements the task of assigning thematic role labels (Gildea and Jurafsky 2002;Nielsen and Pradhan 2004; Xue and Palmer 2004; Swier and Stevenson 2005).
See alsothe common task of CoNNL (2004, 2005) and SENSEVAL-3 (2004).
Both a distinction ofarguments from adjuncts and an appropriate thematic labeling of the complements of apredicate, verb, or noun are necessary, as confirmed by the annotations adopted by cur-rent corpora.
Framenet makes a distinction between complements and satellites (Baker,Fillmore, and Lowe 1998).
The developers of PropBank integrate the difference betweenarguments and adjuncts directly into the level of specificity of their annotation.
Theyadopt labels that are common across verbs for adjuncts.
They inherit these labels fromthe Penn Treebank annotation.
Arguments are annotated instead with labels specific toeach verb (Xue 2004; Palmer, Gildea, and Kingsbury 2005).From a quantitative point of view, arguments and adjuncts have different statisticalproperties.
For example, Hindle and Rooth (1993) clearly indicate that their lexicalassociation technique performs much better for arguments than for adjuncts, whetherthe attachment is to the verb or to the noun.Researchers have abstracted away from this distinction, because identifying ar-guments and adjuncts is a notoriously difficult task, taxing many native speakers?intuitions.
The usual expectation has been that this discrimination is not amenable toa corpus-based treatment.
In recent preliminary work, however, we have succeededin distinguishing arguments from adjuncts using corpus evidence (Merlo and Leybold2001; Merlo 2003).
Our method develops corpus-based statistical correlates for thediagnostics used in linguistics to decide whether a PP is an argument or an adjunct.A numerical vectorial representation of the notion of argumenthood is provided, whichsupports automatic classification.
In the current article, we expand and improve on thiswork, by developing new measures and refining the previous ones.
We also extend thatwork to attachment to nouns.
This extension enables us to explore in what way the dis-tinction between argument and adjunct is best integrated in the traditional attachmentdisambiguation problem.We treat PP attachment as a four-way classification of PPs into noun argumentPPs, noun adjunct PPs, verb argument PPs, and verb adjunct PPs.
We investigate thisnew approach to PP attachment disambiguation through several sets of experiments,testing different hypotheses on the argument/adjunct distinction of PPs and on itsinteraction with the disambiguation of the PP attachment site.
The two main claimscan be formulated as follows. Hypothesis 1: The argument/adjunct distinction can be performed basedon information collected from a minimally annotated corpus,approximating deeper semantic information statistically. Hypothesis 2: The learning features developed for the notion of argumentand adjunct can be usefully integrated in a finer-grained formulation ofthe problem of PP attachment as a four-way classification.343Computational Linguistics Volume 32, Number 3To test these two hypotheses, we illustrate our technique to distinguish argu-ments from adjuncts (Section 2), and we report results on this binary classification(Sections 3 and 4).
The intuition behind the technique is that we do not need to representthe distinction between arguments and adjuncts directly, but that the distinction canbe indirectly represented as a numerical vector.
The feature values in the vector arecorpus-based numerical equivalents of the grammaticality diagnostics used by linguiststo decide whether a PP is an argument or an adjunct.
For example, one of the values inthe vector indicates if the PP is optional, whereas another one indicates if the PP can beiterated.
Optionality and iterability are two of the criteria used by linguists to determinewhether a PP is an argument or an adjunct.
In Section 5, we show how this distinctionsupports a more refined formulation of the problem of PP attachment.
We compare twomethods to reach a four-way classification.
One method is a two-step process that firstclassifies PPs as attached to the noun or to the verb, and then refines the classificationby assigning argument or adjunct status to the disambiguated PPs.
The other methodis a one-step process that performs the four-way classification directly.
We find thatthe latter has better overall performance, confirming our expectation (Hypothesis 2).In Section 6 we discuss the implications of the results for a definition of the notion ofargument and compare our work to that of the few researchers who have attempted toperform the same distinction.2.
Distinguishing Arguments from AdjunctsSolving the four-way classification task described in the introduction crucially relieson the ability to distinguish arguments from adjuncts, using corpus counts.
The abilityto automatically make this distinction is necessary for the correct automatic acquisi-tion of important lexical knowledge, such as subcategorization frames and argumentstructures, which is used in parsing, generation, machine translation, and informationextraction (Srinivas and Joshi 1999; Stede 1998; Dorr 1997; Riloff and Schmelzenbach1998).
Yet, few attempts have been made to make this distinction automatically.The core difficulty in this enterprise is to define the notion of argument preciselyenough that it can be used automatically.
There is a consensus in linguistics that argu-ments and adjuncts are different both with respect to their function in the sentence andin the way they themselves are interpreted (Jackendoff 1977; Marantz 1984; Pollard andSag 1987; Grimshaw 1990).
With respect to their function, an argument fills a role in therelation described by its associated head, whereas an adjunct predicates a separate prop-erty of its associate head or phrase.
With respect to their interpretation, a complementis an argument if its interpretation depends exclusively on the head with which it isassociated, whereas it is an adjunct if its interpretation remains relatively constant whenassociating with different heads (Grimshaw 1990, page 108).
These semantic differencesgive rise to some observable distributional consequences: for a given interpretation, anadjunct can co-occur with a relatively broad range of heads, whereas arguments arelimited to co-occurrence with a (semantically restricted) class of heads (Pollard and Sag1987, page 136).Restricting the discussion to PPs, these differences are illustrated in the followingexamples (PP-argument in bold); see also Schu?tze (1995, page 100).
(8a) Maria is a student of physics.
(8b) Maria is a student from Phoenix.344Merlo and Esteve Ferrer Argument in Prepositional Phrase AttachmentIn example (8a), the head student implies that a subject is being studied.
The sentencetells us only one property of Maria: that she is a student of physics.
In example (8b),the PP instead predicates a different property of the student, namely her geographicalorigin, which is not implied by the head student.
(9a) Kim camps/jogs/meditates on Sunday.
(9b) Kim depended/blamed the arson on Sandy.In example (9a) the PP on Sunday can be construed without any reference to the pre-ceding part of the sentence, and it preserves its meaning even when combining withdifferent heads.
This is, however, not the case for (9b).
Here, the PP can only be properlyunderstood in connection with the rest of the sentence: Sandy is the person on whomsomeone depends or the person on which the arson is blamed.These semantic distinctions between arguments and adjuncts surface in observablesyntactic differences and can be detected automatically both by using general formalfeatures and by specific lexical semantic features, which group together the argumentsof a lexical head.
Unfortunately, the linguistic diagnostics that are used to determinewhether a PP is an adjunct or an argument are not accurate in all circumstances, theyoften partition the set of the examples differently, and they give rise to relative, and notabsolute, acceptability judgments.We propose a methodology that retains both the linguistic insight of the grammat-ical tests and the ability to effectively combine several gradient, partial diagnostics,typical of automatic induction methods.
Specifically, we first find countable diagnosticsfor the argument?adjunct distinction, which we approximate statistically and estimateusing corpus counts.
We also augment the feature vector with information encodingthe semantic classes of the input words.
The diagnostics and the semantic classes arethen automatically combined in a decision tree induction algorithm.
The diagnosticsare presented below.2.1 The DiagnosticsMany diagnostics for argumenthood have been proposed in the literature (Schu?tze1995).
Some of them require complex syntactic manipulation of the sentence, such as wh-extraction, and are therefore too difficult to apply automatically.
We choose six formaldiagnostics that can be captured by simple corpus counts: head dependence, optionality,iterativity, ordering, copular paraphrase, and deverbal nominalization.
These diagnos-tics tap into the deeper semantic properties that distinguish arguments from adjuncts,without requiring that the distinctions be made explicit.Head Dependence.
Arguments depend on their lexical heads because they form an inte-gral part of the phrase.
Adjuncts do not.
Consequently, PP-arguments can only appearwith the specific verbal head by which they are lexically selected, whereas PP-adjunctscan co-occur with a far greater range of different heads than arguments because theyare necessary for the correct interpretation of the semantics of the verb, as illustrated inthe following example sentences.
(10a) a man/woman/dog/moppet/scarecrow with gray hair(10b) a menu/napkin/glass/waitress/matchbook from Rosie?s345Computational Linguistics Volume 32, Number 3(11a) a member/*dog/*moppet/*scarecrow of Parliament(11b) a student/*punk/*watermelon/*Martian/*poodle/*VCR of physicsWe expect an argument PP to occur with fewer heads, whereas an adjunct PP will occurwith more heads, as it is not required by a specific verb, but it can in principle adjoin toany verb or noun head.We capture this insight by estimating the dispersion of the distribution of thedifferent heads that co-occur with a given PP in a corpus.
We expect adjunct PPs to havehigher dispersion than argument PPs.
We use entropy as a measure of the dispersionof the distribution, as indicated in equation (1) (h indicates the noun or verb head towhich the PP is attached, X is the random variable whose outcomes are the valuesof h).hdep(PP) ?
HPP(X) = ?
?h?Xp(h)log2p(h) (1)Optionality.
In most cases, PP-arguments are obligatory elements of a given sentencewhose absence leads to ungrammaticality, while adjuncts do not contribute to thesemantics of any particular verb, hence they are optional, as illustrated in the followingexamples (PP-argument in bold):(12a) John put the book in the room.
(12b) ?John put the book.
(12c) John saw/read the book in the room.
(12d) John saw/read the book.Since arguments are obligatory complements of a verb, whereas adjuncts are not, weexpect knowledge of a given verb to be more informative with respect to the probabilityof existence of an argument than of an adjunct.
Thus we expect that the predictivepower of a verb with regard to its complements will be greater for arguments than foradjuncts.1 The notion of optionality can be captured by the conditional probability of aPP given a particular verbal head, as indicated in equation (2).opt(PP) ?
P(PP|v) (2)Iterativity and Ordering.
Because they receive a semantic role from the selecting verb,arguments of the same type cannot be iterated because verbs can only assign any giventype of role once.
Moreover, in English, arguments must be adjacent to the selectinglexical head.
Neither of these two restrictions apply to adjuncts, which can be iterated,and follow arguments in a sequence of PPs.
Consequently, in a sequence of several PPs1 Notice that this diagnostic can only be interpreted as a statistical tendency, and not as a strict test,because not all arguments are obligatory (but all adjuncts are indeed optional).
The best knowndescriptive exception to the criterion of optionality is the class of so-called object-drop verbs (Levin 1993).Here a given verb may tolerate the omission of its argument.
In other words, a transitive verb, such askiss, can also act like an intransitive.
With respect to optional PPs, it has been argued that instrumentalsare arguments (Schu?tze 1995).
While keeping these exceptions in mind, we maintain optionality as avalid diagnostic here.346Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachmentonly the first one can be an argument, whereas the others must be adjuncts, as illustratedin the examples below.
(13a) ?Chris rented the gazebo to yuppies, to libertarians.
(13b) Kim met Sandy in Baltimore in the hotel lobby in a corner.These two criteria combined give rise to one diagnostic.
The probability of a PPbeing able to iterate, and consequently being an adjunct, can be approximated asthe probability of its occurrence in second position in a sequence of PPs, as indi-cated in equation (3), where we indicate the position of the PP in a sequence as asubscript.iter(PP1) ?
P(PP2) (3)Copular Paraphrase.
The diagnostic of copular paraphrase is specific to the distinction ofNPs arguments and adjuncts, following Schu?tze (1995, page 103).
It does not apply toVP arguments and adjuncts, as it requires paraphrasing the PP with a relative clause.Arguments cannot be paraphrased by a copular relative clause, as the examples in (15)show, whereas adjuncts can, as is shown in (14):(14) a. a man from Paris a man who was from Parisb.
the albums on the shelf the albums that were on the shelfc.
the people on the payroll the people who were on the payroll(15) a. the destruction of the city *the destruction that was of the cityb.
the weight of the cow *the weight that was of the cowc.
a member of Parliament *a member who was of ParliamentThis is because a PP attached to a noun as an argument does not predicate a secondaryproperty of the noun, but it specifies the same property that is indicated by the head.To be able to use the relative clause construction, there must be two properties that arebeing predicated of the same entity.Thus, the probability that a PP is able to be paraphrased, and therefore that itis an adjunct, can be approximated by the probability of its occurrence following aconstruction headed by a copular verb, be, become, appear, seem, remain (Quirk et al 1985),as indicated in equation (4), where ?
indicates linear precedence.para(PP) ?
P(vcopula ?
PP) (4)Deverbal Nouns.
This diagnostic is based on the observation that PPs following a de-verbal noun are likely to be arguments, as the noun shares the argument structureof the verb.2 Proper counting of this feature requires identifying a deverbal noun inthe head noun position of a noun phrase.
We identify deverbal nouns by inspect-ing their morphology (Quirk et al 1985).
Specifically, the suffixes that can combine2 Doubts have been cast on the validity of this diagnostic (Schu?tze 1995), based on work in theoreticallinguistics (Grimshaw 1990).
Argaman and Pearlmutter (2002), however, have shown that the argumentstructures of verbs and related nouns are highly correlated.
Hence, we keep deverbal noun as a validdiagnostic here, although we show later that it is not very effective.347Computational Linguistics Volume 32, Number 3with verb bases to form deverbal nouns are listed and exemplified in Figure 1 onpage 348.
This diagnostic can be captured by a probability indicator function, whichassigns probability 1 of being an argument to PPs following a deverbal noun and0 otherwise.deverb(PP) ={1 if deverbal n ?
PP0 otherwise (5)In conclusion, the diagnostics of head dependence, optionality, iterativity, ordering,copular paraphrase, and deverbal nominalization are promising indicators of the statusof PPs as either arguments or adjuncts.
In Section 3 we illustrate how they can bequantified in a faithful way and, thanks to their simplicity, how they can be estimatedin a sufficiently large corpus by simple counts.Another class of features is also very important for the distinction between argu-ments and adjuncts, the lexical semantic class to which the lexical heads belong, as weillustrate below.Lexical Semantic Class Features.
According to Levin (1993), there is a regular mappingbetween the syntactic and semantic behavior of a verb.
This gives rise to a lexicon whereverbs that share similar syntactic and semantic properties are organized into classes.More specifically, it is assumed that similar underlying components of meaning give riseto similar subcategorization frames and projections of arguments at the syntactic level.Since an argument participates in the subcategorization frame of the verb, whereasan adjunct does not, we expect the argument-taking properties of verbs to be alsoorganized around semantic classes.
We expect, therefore, that knowledge of the class ofthe verb will be beneficial to the acquisition of the distinction between arguments andadjuncts for an individual verb.
For example, all verbs of giving take a dative indirectobject and all benefactive verbs can take a benefactive prepositional phrase complement(see examples 16).
An analogous prediction can be made for nouns.
Unlike the diagnos-tics features, these lexical features do not have a quantitative counterpart, but they arerepresented as discrete nominal values that indicate the lexical semantic class the wordsbelong to.
(16a) Darcy offered a gift to Elizabeth.
(16b) Darcy cooked a roast for Elizabeth.-ANT inhabitant, contestant, informant, participant, lubricant.-EE appointee, payee, nominee, absentee, refugee.-ER, OR singer, writer, driver, employer, accelerator, incubator, supervisor.-AGE breakage, coverage, drainage, leverage, shrinkage, wastage.-AL refusal, revival, dismissal.-ION exploration, starvation, ratification, victimization, foundation.-SION invasion, evasion.-ING building, opening, filling, earnings, savings, shavings, wedding.-MENT arrangement, amazement, puzzlement, embodiment, equipment.Figure 1Nominal endings that indicate deverbal derivation.348Merlo and Esteve Ferrer Argument in Prepositional Phrase AttachmentWe use all these diagnostics, which in linguistics are used as tests of the argu-ment status of PPs, as a distributed representation of argumenthood itself.
We do notassume that the syntactic representation of arguments is different from the represen-tation of adjuncts; for example, we do not assume they have a different attachmentconfiguration, rather the diagnostics themselves determine a multidimensional spacein which PPs are positioned with different degrees of argumenthood.
For such anapproach to work, we need to be able to transform each diagnostic into a symbolicor numeric feature and combine the features in a precise way.
In the two followingsections we illustrate how to calculate the values of each diagnostic using corpus-basedapproximations and how to combine them with widely used automatic acquisitiontechniques.3.
MethodologyThe diagnostics described above can be estimated by simple corpus counts.
The accu-racy of the data collection is key to the success of the classifier induction based on thesecounts.
We explain the details of our methodology below.3.1 MaterialsWe construct two corpora comprising examples of PP sequences.
A PP is approximatedas the preposition and the PP-internal head noun.
For example, with very many little chil-dren will be represented as the bigram with children.
One corpus contains data encodinginformation for attachment of single PPs in the form of four head words (verb, objectnoun, preposition, and PP internal noun) indicating the two possible attachment sitesand the most important words in the PP for each instance of PP attachments found inthe corpus.
We also create an auxiliary corpus of sequences of two PPs, where each dataitem consists of verb, direct object, and the two following PPs.
This corpus is only usedto estimate the feature Iterativity.
All the data were extracted from the Penn Treebankusing the tgrep tools (Marcus, Santorini, and Marcinkiewicz 1993).
Our goal is to create amore comprehensive and possibly more accurate corpus than the corpora used by Merloand Leybold (2001), Merlo, Crocker, and Berthouzoz (1997), and Collins and Brooks(1995), among others.
To improve coverage, we extracted all cases of PPs followingtransitive and intransitive verbs and following nominal phrases.
We include passivesentences and sentences containing a sentential object.
To improve accuracy, we insuredthat we did not extract overlapping data, contrary to practice in previous PP corporaconstruction, where multiple PP sequences were extracted more than once, each timeas part of a different structural configuration.
For example, in previous corpora, thesequence using crocidolite in filters in 1956, which is a sequence of two PPs, is countedboth as an example of a two PPs sequence as well as an example of a single PPsequence, using crocidolite in filters.
This technique of using subsequences as independentexamples is used both in the corpora used in Merlo and Leybold (2001) and (Merlo,Crocker, and Berthouzoz 1997), and to an even larger extent in the corpus used inCollins and Brooks (1995), who would also have in their corpus the artificially con-structed sequence using crocidolites in 1956.
This method increases the number of avail-able examples, and it is therefore hoped that it will be beneficial to the learning accuracy.However, as shown in Merlo, Crocker, and Berthouzoz (1997), it rests on the incorrectassumption that the probability of an attachment is independent of the position of thePP to be disambiguated in a sequence of multiple PPs.
Therefore, we have decided not349Computational Linguistics Volume 32, Number 3to decompose the examples into smaller sequences.
The possible grammatical config-urations that we have taken into account to construct the corpus are exemplified inAppendix 1.Once the quadruples constituting the data are extracted from the text corpus,it is necessary to separate the statistics corpus from the training and test corpus.3Before illustrating the adopted solution to this problem, let us define the followingterms. The statistics corpus CSt is the part of the corpus that is used to extract thetuples that are used to calculate the features. The training corpus CTr is the part of the corpus that is used to extract thetuples that are used as training data for the classifier. The testing corpus CTe is the part of the corpus that used to extract thetuples that are used as testing data to evaluate the classifier. The training data STr is the set of tuples in CTr, augmented with thefeatures calculated using CSt. The testing data STe is the set of tuples in CTe, augmented with the featurescalculated using CSt.Note that for the testing data STe to be an independent test set, the testing corpusCTe must be disjoint both from CTr, the training corpus, but also it must be disjointfrom CSt, the corpus on which the statistics are calculated.
One possible solution, forexample, is to equate the statistics and the training corpus, CSt = CTr, and to assign themSections 1?22, while CTe = Section 23, thus making the testing corpus CTe be disjointfrom both the statistics and the training corpus.
The problem with this solution is thatthe training data STr is no longer extracted using the same process as the testing dataSTe, and is therefore not good data from which to generalize.
In particular, all tuples inthe training data STr necessarily also occur in the statistics corpus CSt, and therefore novectors in the training data STr involve data unseen in the statistics corpus.
In contrast,the testing data STe can be expected to include tuples that did not also occur in thestatistics corpus, and so the classifier might not generalize to these tuples using thefeatures we calculate.The solution we adopt is to split Sections 1 to 22 into two disjoint subcorpora.Because the Penn Treebank is not uniformly annotated across sections, we do notassign whole sections to either the statistical or the training corpus, but instead ran-domly assign individual sentences to either corpus.
Since data sparseness is a moreimportant issue when calculating the features than it is for training the decision tree, weassigned a larger proportion of the corpus to the statistical subcorpus.
We assigned 25%of Sections 1?22 to CTr and the rest to CSt.
Section 24 is used as a development corpusand Section 23 is the testing corpus CTe.
In this setting, since the statistics, training, andtesting corpora are all mutually disjoint, all issues of dependence are resolved, and thetraining data are representative of the real data we are interested in, so we can expectour classifier to be able to generalize to new data.3 We thank Eric Joanis for his help in correctly sampling the corpus and calculating the features.350Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment3.2 The Counts of the Learning FeaturesAs we said above, accurate estimates of the values of the features are crucial for the au-tomatic learning algorithm to be successful.
We illustrate the estimation of the featuresbelow.
Often several ways of estimating the features have been implemented, mostly toaddress sparseness of data.Lexical Word Classes.
As indicated in the previous section, the head word of the governorof the PP, the noun or the verb, is very directly related to the status of the PP.
Forexample, all giving verbs take a PP introduced by to, which is an argument of the verb.The lexical semantic class of the head words is therefore going to be very relevant to thedisambiguation task.The semantic grouping has been done automatically, using the lexicographic classesin WordNet 1.7 (Miller et al 1990).
Nouns are classified in different classes, amongwhich, for example, are animal, artifact, attribute, body, cognition, communication, event,feeling, food, location, motive, person, plant, process, quantity, relation, shape, and substance.This classification required selecting the most frequent WordNet sense for those polyse-mous nouns being classified and generalizing to its class.4For all the features below and where appropriate, we assume the following no-tation.
Let h be the head, that is, the verb in the features related to verb attachmentsand the noun in the features related to noun attachment.
Let p be the preposition, andn2 be the object of the preposition.
Let hcl and n2cl be the WordNet class of h and n2,respectively.
Let C(h, p, n2) be the frequency with which p, n2 co-occurs as a preposi-tional phrase with h. Let C(h) be the frequency of h.Head Dependence.
Head dependence of a PP on a head is approximated by estimatingthe dispersion of the PP over the possible heads.
In a previous attempt to capture thisnotion, we approximated by simply measuring the cardinality of the set of heads thatco-occur with a given PP in a corpus, as indicated in equation (6).
The expectation wasthat a low number indicated argument status, whereas a high number indicated adjunctstatus (Merlo and Leybold 2001).hdep(h, p, n2) = |{h1, h2, h3, .
.
.
, hn}p,n2| (6)By measuring the cardinality of the set of heads, we approximate the dispersionof the distribution of heads by its range.
This is a very rough approximation, as therange of a distribution does not give any information on the distribution?s shape.
Therange of a distribution might have the same value for a uniform distribution or a veryskewed distribution.
Intuitively, we would like a measure that tells us that the formercorresponds to a verb with a much lower head dependence than the latter.
Entropy is4 The automatic annotation of nouns and verbs in the corpus has been done by matching them withthe WordNet database files.
Before doing the annotation, though, some preprocessing of the datawas required to maximize the matching between our corpus and WordNet.
The changes madewere inspired by those described in Stetina and Nagao (1997, page 75).
To lemmatize the wordswe used ?morpha,?
a lemmatizer developed by John A. Carroll and freely available at the address:http://www.informatics.susx.ac.uk./research/nlp/carroll/morph.html.
Upon simple observation,it showed a better performance than the frequently used Porter Stemmer for this task.351Computational Linguistics Volume 32, Number 3a more informative measure of the dispersion of a distribution, which depends both onthe range and on the shape of a distribution.The head dependence measure based on entropy, then, is calculated as indicatedin equation (7), which calculates the entropy of the probability distribution gener-ated by the random variable X, whose values are all the heads that co-occur with agiven PP.hdep(h, p, n2) = Hp,n2(X) ?
?
?h?XC(h, p, n2)?iC(hi, p, n2)log2C(h, p, n2)?iC(hi, p, n2)(7)The counts that are used to estimate this measure will depend on finding exactlyPPs with the same PP internal noun, and on attaching to exactly the same lexicalhead.
We can expect these measures to suffer from sparse data.
We implement thensome variants of this measure, where we cluster PPs according to the semantic contentof the PP-internal nouns and we cluster nominal heads according to their class.
Thesemantic grouping has been done automatically, as indicated in the paragraph above,on calculating word classes.
Since WordNet has a much finer-grained top-level classifi-cation for nouns than for verbs, we found that grouping head nouns into classes yieldeduseful generalizations, but it did not do so for verbs.Therefore, we calculate head dependency in three different variants: One measure isbased on PP-internal noun tokens, another variant is based on noun classes for the PP-internal noun position, and another variant is based on classes for both the PP-internaland the head noun position, as indicated in equation (8).hdep(h, p, n2) =????????????
?Hp,n2(X) ?
?
?h?X C(h,p,n2)?iC(hi,p,n2) log2C(h,p,n2)?iC(hi,p,n2), orHp,n2cl (X) ?
?
?h?XC(h,p,n2cl )?iC(hi,p,n2cl )log2C(h,p,n2cl )?iC(hi,p,n2cl ), orHp,n2cl (Xcl) ?
?
?hcl?XC(hcl,p,n2cl )?iC(hcl,i,p,n2cl )log2C(hcl,p,n2cl )?iC(hcl,i,p,n2cl ), if h = noun.(8)Optionality.
As explained above, we expect that the predictive power of a verbalhead?recall that optionality does not apply to noun attachments?about its com-plements will be greater for arguments than for adjuncts.
This insight can be cap-tured by the conditional probability of a PP given a particular verb, as indicated inequation (9).opt(v, p, n2) ?
C(v, p, n2)C(v)(9)Analogously to the measure of head dependence for noun attachments, optionalityis measured in three variants.
First, it is calculated as a conditional probability basedon simple word counts in the corpus of single PPs, as indicated in equation (9) above.Second, we also implement a variant that relies on verb classes instead of individualverbs to address the problem of sparse data.
Finally, we also implement a variant thatrelies on noun classes for the PP-internal noun and verb classes instead of individualverbs.
For both these measures, verbs and nouns were grouped into classes using352Merlo and Esteve Ferrer Argument in Prepositional Phrase AttachmentWordNet 1.7 with the same method as for head dependence.
The three measures ofoptionality are indicated in equation (10).opt(v, p, n2) =????????????
?P(p, n2|v) ?
C(v,p,n2)C(v) , orP(p, n2|vcl ) ?
C(vcl,p,n2)C(vcl ) , orP(p, n2cl|vcl ) ?
C(vcl,p,n2cl )C(vcl ) .
(10)Iterativity and Ordering.
Iterativity and ordering are approximated by collecting countsindicating the proportion of cases in which a given PP in first position had been foundin second position in a sequence of multiple PPs over the total of occurrences in anyposition, as indicated in equation (11).
The problem of sparse data here is especiallyserious because of the small frequencies of multiple PPs.
We addressed this problem byusing a backed-off estimation, where we replace lexical items by their WordNet classesand collect counts on this representation.
Specifically, the iterativity measure has beenimplemented as follows.Let C2nd(h, p, n2) be the frequency with which p, n2 occurs as a second prepositionalphrase with h, and Cany(h, p, n2) be the frequency with which it occurs with h in anyposition.5 Then:iter(h, p, n2) ???????????????????
?C2nd(h, p, n2)Cany(h, p, n2), if Cany(h, p, n2) 	= 0, or elseC2nd(h, p, n2cl)Cany(h, p, n2cl), if Cany(h, p, n2cl) 	= 0, or elseC2nd(hcl, p, n2cl)Cany(hcl, p, n2cl).
(11)Copular Paraphrase.
Copular paraphrase is captured by calculating the proportion oftimes a given PP is found in a copular paraphrase.
We approximate this diagnostic bymaking the hypothesis that a PP following a nominal head is an adjunct if it is alsofound following a copular verb, be, become, appear, seem, remain (Quirk et al 1985).
Wecalculate then the proportion of times a given PP follows a copular verb over the timesit appears following any verb.
This count is an approximation because even when wefind a copular verb, it might not be part of a relative clause.
Here again, we back off tothe noun classes of the PP-internal noun to address the problem of sparse data.para(h, p, n2) ?????????
?C(vcopula ?
(p, n2))?iC(vi ?
(p, n2)), if C(vcopula) 	= 0, or elseC(vcopula ?
(p, n2cl))?iC(vi ?
(p, n2cl)).
(12)5 Note that we approximate the prepositions occurring in any position by looking only at the first twoprepositions attached to the verb phrase.353Computational Linguistics Volume 32, Number 3Deverbal Nominalization.
The diagnostic of deverbal nouns is implemented as a binaryfeature that simply indicates if the PP follows a deverbal noun or not.deverb(n, p, n2) ={1 if deverbal n ?
(p,n2)0 otherwise (13)Deverbal nouns are identified by inspecting their morphology (Quirk et al 1985).As our corpus is lemmatized, we are confident that all the nouns in it are in their baseforms.
The suffixes that can combine with verb bases to form deverbal nouns are shownin Figure 1.The counts that are collected in the way described above constitute a quanti-fied vector corresponding to a single PP exemplar.
These exemplars are the input toan automatic classifier that distinguishes arguments from adjuncts, as described inSection 4.
Before we describe the experiments, however, attention must be paid to themethod that will be used to determine the target attribute?argument or adjunct?thatwill be used to train the learner in the learning phase and to evaluate the accuracy ofthe learned classifier in the testing phase.3.3 The Target AttributeSince we are planning to use a supervised learning method, we need to label each exam-ple with a target attribute.
Deciding whether an example is an instance of an argumentor of an adjunct requires making a distinction that the Penn Treebank annotators didnot intend to make.
The automatic annotation of this attribute therefore must rely onthe existing labels for the PP that have been given by the Penn Treebank annotators,inferring from them information that was not explicitly marked.
We discuss here themotivation for our interpretation.The PTB annotators found that consistent annotation of argument status and se-mantic role was not possible (Marcus et al 1994).
The solution adopted, then, wasto structurally distinguish arguments from adjuncts only when the distinction wasstraightforward and to label only some clearly distinguishable semantic roles.
Doubt-ful cases were left untagged.
In the Penn Treebank structural distinctions concerningarguments and adjuncts have been oversimplified: All constituents attached to VP arestructurally treated as arguments, whereas all constituents attached to NP are treatedas adjuncts.
The only exception are the arguments of some deverbal nouns, which arerepresented as arguments.
Information about the distinction between arguments andadjuncts, then, must be gleaned from the semantic and function tags that have beenassigned to the nodes.
Constituents are labeled with up to four tags (including numer-ical indices) that account for the syntactic category of the constituent, its grammaticalfunction, and its semantic role (Bies et al 1995).
Figure 2 illustrates the tags that involvePP constituents.From the description of this set of tags we can already infer some information aboutthe argument status of the PPs.
PPs with a semantic tag (LOC, MNR, PRP, TMP) areadjuncts, whereas labels indicating PP complements of ditransitive verbs (BNF, DTV)or locative verbs like put are arguments.
There are, though, some cases that remain am-biguous and therefore require a deeper study.
These are untagged PPs and PPs tagged-CLR.
For these cases, we will necessarily only approximate the desired distinction.
Wehave interpreted untagged PPs as arguments of the verb.
The motivation for this choicecomes both from an overall observation of sentences and from the documentation,354Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment-CLR dative object if dative shift not possible (e.g., donate); phrasal verbs;predication adjuncts-DTV dative object if dative shift possible (e.g., give)-BNF benefactive (dative object of for)-PRD non VP predicates-PUT locative complement of put-DIR direction and trajectory-LOC location-MNR manner-PRP purpose and reason-TMP temporal phrasesFigure 2Grammatical function and semantic tags that involve PP constituents in the Penn Treebank.in which it is stated that ?NPs and Ss which are clearly arguments of the verb areunmarked by any tag?
(Marcus et al 1994, page 4), and that ?Direct Object NPs andIndirect Object NPs are all untagged?
(Bies et al 1995, page 12).
Although the case ofPP constituents is not specifically addressed, we have interpreted these statements assupporting evidence for our choice.The tag -CLR stands for ?closely related,?
and its meaning varies, depending onthe element it is attached to.
It indicates argument status when it labels the dativeobject of ditransitive verbs that cannot undergo dative shift, such as in donate money tothe museum, and in phrasal verbs, such as pay for the horse.
It indicates adjunct statuswhen it labels a predication adjunct as defined by Quirk et al (1985).
We interpretthe tag -CLR as an argument tag in order not to lose the few cases for which thedifferentiation is certain: the ditransitive verbs and some phrasal verbs.
This choiceapparently misclassifies predication adjuncts as arguments.
However, for some cases,such as obligatory predication adjuncts, an argument status might in fact be moreappropriate than an adjunct status.
According to Quirk et al (1985, Sections 8.27?35,15.22, pages 16?48), there are three types of adjuncts, differentiated by the degree of?centrality?
they have in the sentence.
They can be classified into predication adjunctsand sentence adjuncts.
Predication adjuncts can be obligatory or optional.
Obligatorypredication adjuncts resemble objects as they are obligatory in the sentence and theyhave a relatively fixed position, as in He lived in Chicago.
Optional predication adjunctsare similarly central in the sentence but are not obligatory, as in He kissed his motheron the cheek.
Sentence adjuncts, on the contrary, have a lower degree of centrality in thesentence, as in He kissed his mother on the platform.
As a conclusion, obligatory predicationadjuncts as described in Quirk et al (1985) could be interpreted as arguments, as theyare required to interpret the verb (the interpretation of lived in He lived differs from theone in He lived in Chicago).To recapitulate, we have labeled our examples as follows: Adjuncts: All PPs tagged with a semantic tag (DIR, LOC, MNR, PRP, TMP)are adjuncts. Arguments: All untagged PPs or PPs tagged with CLR, EXT, PUT, DTV,BNF, or PRD are arguments.Validating the Target Attribute and Creating a Gold Standard.
The overall mapping ofPenn Treebank function labels onto the argument?adjunct distinction is certainly toocoarse, as some function types of PPs can be both arguments or adjuncts, depending355Computational Linguistics Volume 32, Number 3on the head they co-occur with.
We assessed the overall validity of the mapping asfollows.
First, for each of the function tags mentioned earlier, we sampled the PennTreebank (one example from each section) for a total of 22 examples for each tag.
Then,we manually inspected the examples to determine if the mapping onto argument oradjunct was correct.
On a first inspection, the tags PUT, DTV, and PRD are correctlymapped as argument in the majority of cases, as well as DIR, LOC, TMP, and PRP,which are correctly considered adjuncts.
Those samples tagged MNR, CLR, BNF, oruntagged show a more mixed behavior, sometimes appearing to label arguments andsometimes adjuncts.
For these labels, we used a more elaborate procedure to determineif the example was an argument or an adjunct.
We concentrate on PPs attached to theverb, as these cases appear to be more ambiguous.All the test examples attached to a verb that had a CLR, PP, or MNR label wereextracted.
We did not investigate BNF as there aren?t any in our test file.
We constructedtest suites for each example by applying to it the typical linguistic diagnostics used todetermine argumenthood, along the lines already discussed in Section 2.
Five tests wereselected, which were found to be the most discriminating in a pilot study over 224 sen-tences: optionality, ordering, head dependence, extraction with preposition stranding,and extraction with pied-piping, as illustrated in Figure 3.
It can be noticed that we wereable to use more complex tests than those used by the algorithm; in particular we useextraction tests (Schu?tze 1995).6 A native speaker gave binary acceptability judgmentsover the 1,100 sentences thus generated.
The acceptability judgments were assigned tothe sentences over the course of several days.
Once the judgments to each quintuple ofsentences were collected, they were combined into a single binary-valued target featurefor each sentence by the first author.
The decision was based on the relative importanceand reliability of the tests according to the linguistic literature (Schu?tze 1995), as follows:If the optionality test is negative then the example is an argument, else if extraction can takeplace then the example is an argument, else the majority label according to the outcome ofthe grammaticality judgments is assigned.
In a few cases, the judgment was overiddenif the negative outcome of the tests clearly derived from the fact that the V + PP wasan idiom rather than an adjunct: for example, make fools of themselves, have a ring to it,and live up to.In the end, we find the following correlations between the automatic labels andthose assigned based on the accurate collection of native speaker judgments.
Among thePPs that do not have a function label, 65 are adjuncts and 42 are arguments, accordingto the manual annotation procedure.
The label PP-CLR corresponds to 18 adjuncts and159 arguments, according to our manual annotation procedure, whereas the label PP-MNR corresponds to 5 adjuncts and 1 argument.
Clearly, the assignments of CLR PPs toarguments and MNR PPs to adjunct are confirmed.
Prepositional phrases without anyfunctional labels, on the other hand, are much more evenly divided between argumentand adjunct, as we suspected, given the heterogeneous nature of the label (the label PP6 Some of the extracted sentences had to be simplified so that the verb and prepositional phrases were in amain clause.
For example buy shares from sellers, which is generated from the sentence On days like Friday,that means they must buy shares from sellers when no one else is willing to becomes They must buy shares fromsellers.
In some cases the sentences had to be further simplified to allow extraction tests to apply, whichwould be violated for reasons unrelated to the argument-adjunct distinction in the PP, such as negation,or complex NP islands.
For example, Hill democrats are particularly angry over Mr. Bush?s claim that thecapital-gains cut was part of April?s budget accord and his insistence on combining it with the deficit-reductionlegislation yields Mr. Bush combines capital gains cut with the deficit-reduction legislation, which gives rise tothe following extraction examples: What do you wonder whether Mr. Bush combines capital gains cut with?With what do you wonder whether Mr. Bush combines capital gains cut?356Merlo and Esteve Ferrer Argument in Prepositional Phrase AttachmentAmericans will learn more about making products [ for the Soviets ].optionality Americans will learn more about making products.order Americans will learn more about making products these coming yearsfor the Soviets.head dependence Americans will learn more about making/selling/reading products forthe Soviets.extraction Who do you wonder whether Americans will learn more about makingproducts for?For who(m) do you wonder whether Americans will learn more aboutmaking products?Figure 3Example sentence and related list of tests and test sentences.is assigned not only to those cases that are clear arguments, but also to those cases forwhich a label cannot be decided).
Moreover, if the hand-annotated label is reliable, it in-dicates that untagged PPs are somewhat more likely to be adjuncts.
Our initial mappingwas incorrect.
In retrospect, we must conclude that other researchers had applied whatappears to be the correct mapping for this data set (Buchholz 1999).
We do not, however,modify the label of our training set, as that would be methodologically incorrect.
Wehave relabeled the test set and are therefore bound to ignore any further knowledgewe have gathered in relabeling, as that would amount to tailoring our training set toour test set.
The consequence of this difference between our label and what we foundto be true for the gold standard is that all results tested on the manually labeled testset will have to be interpreted as lower bounds of the performance to be expected on aconsistently labeled data set.
Besides validating the automatically annotated test set, themanually annotated test set serves as a gold standard.
Performance measures on this setwill support comparison across methodologies.Therefore, we conclude that the mapping we have assumed is coherent with thejudgments of a native speaker, although the agreement is not perfect.
PPs withouta function tag are an exception.
Thus, the automatic mapping we have defined willprovide the value of the target feature in the experiments that we illustrate in the twofollowing sections.
When appropriate we report two performance measures, one forthe automatic label and one for the partly manual labels.
We will also report somecomparative results on a test set that does not contain the noisy PPs without functionlabels.4.
Distinguishing Arguments from AdjunctsHaving collected the necessary data and established the value of the target attributefor each example, we can now perform experiments to test several different hypothesesconcerning the learning of the argument-adjunct distinction.
First of all we need to showthat the distinction under discussion can be learned to a good degree.
Furthermore, weinvestigate the properties that, singly or in combination, lead to an improvement inlearning.
We are particularly interested in comparing the baseline to a simple model,where learning is done using only lexical heads.
In this case, we investigate the rele-vance of the simple words in detecting the difference between arguments and adjuncts.We also verify the usefulness of knowing lexical classes on the accuracy of learning.Finally, we show that the diagnostic features and the lexical classes developed abovebring more information to the classification than what can be gathered by simple357Computational Linguistics Volume 32, Number 3lexical heads.
We summarize these expectations below, where we repeat and elaborateHypothesis 1 formulated in the introduction. Hypothesis 1: The argument-adjunct distinction can be performed basedon information collected from an annotated corpus. Hypothesis 1?
: The argument-adjunct distinction can be improved bylexical classes and linguistic diagnostic features, (i) over a simple baseline,but also (ii) over a model using lexical features.Demonstration of these hypotheses requires showing that the distinction can belearned from corpus evidence, even with a simple method (performance is better thanchance).
Hypothesis 1?
imposes the more stringent condition that we can considerablyimprove a simple learner by using more linguistically informed statistics (performanceis better than the simple method).4.1 The Input Data and the ClassifierIn order to test the argument and adjunct attachment problem independently ofwhether the PP is attached to a verb or to a noun, we create two sets of input data,one for verb attachment and one for noun attachment.Each input vector for verb attachment contains training features comprising thefour lexical heads and their WordNet classes (v, n1, p, n2, vcl, n1cl, and n2cl), all thedifferent variants of the implementation of the diagnostics for the argument-adjunctdistinction concerning PPs attached to a verb, and one binary target feature, indicatingthe type of attachment, whether argument or adjunct.
More specifically, the featuresimplementing the diagnostics for verbs consist of the variants of the measures of headdependence (hdepv1,hdepv2), the variants of the measure of optionality (opt1, opt2, opt3),and the measure of iterativity (iterv).Each input vector for noun attachment contains 14 training features.
They comprisethe four lexical heads and their WordNet classes, as above (v, n1, p, n2, vcl, n1cl, andn2cl), all the different variants of the implementation of the diagnostics for PPs attachedto a noun, and one binary target feature, indicating the type of attachment.
The featuresimplementing the diagnostics for nouns are: the variants of the measures of head de-pendence (hdepn1, hdepn2, hdepn3), the measures of iterativity (itern), and the measuresfor copular paraphrase and deverbal noun, respectively (para, deverb).For both types of experiments?distinction of arguments from adjuncts of PPs at-tached to the noun or PPs attached to the verb?we use the C5.0 Decision Tree InductionAlgorithm (Quinlan 1993) and Support Vector Machines (LIBSVM), version 2.71 (Changand Lin 2001).4.2 Results on V Attachment CasesWe have run experiments on the automatically labeled training and test sets withvery many different feature combinations.
A summary of the most interesting patternsof results are indicated in Tables 1 and 2.
Significance of results is tested using aMcNemar test.Contribution of Lexical Items and Their Classes.
In this set of experiments we use onlylexical features or features that encode the lexical semantic classes of the open class358Merlo and Esteve Ferrer Argument in Prepositional Phrase AttachmentTable 1Accuracy of the argument?adjunct distinction for VP-attached PPs, using combinations of lexicalfeatures.
The training and test sets are automatically annotated.Feature used Auto accuracy (%)1.
Chance (args) 55.82.
Prep (baseline) 67.93.
Lexical features (verb, prep, n2) 67.94. vcl, prep 73.85.
Prep, n2cl 71.96.
Lexical features and classes (v, vcl, p, n2, n2cl) 68.27.
Only classes (vcl, p, n2cl) 75.08.
Only verb classes (vcl, p, n2) 73.19.
Only noun classes (v, p, n2cl) 70.310.
All features 68.2words in question.
Lines 2 to 5 of Table 1 report better classification values thanline 1.
These results indicate that it is possible, at different degrees of accuracy, toleverage information encoded in lexical items to infer the semantic distinction betweenarguments and adjuncts without explicit deep semantic knowledge.
One interestingfact (lines 2 and 3) is that the preposition is very informative, as informative as allthe lexical items together.
An analysis of the distribution of arguments and adjunctsby preposition indicates that whereas most prepositions are ambiguous, they have astrong preference for either arguments or adjuncts.
Only a few equibiased prepositions,such as for, exist.
An expected result (lines 4 and 5) is that the PP-internal noun classis useful, in combination with the preposition, as well as the combination of verbclass and preposition (the difference is marginally significant).
We find the best re-sult (line 7) in the experiment that uses class combinations (difference from baselinep < .001).
We see that classes of open class items, nouns, and verbs associated withthe closed class item preposition give the best performance.
Line 7 is considerablybetter than line 6 (p < .001), indicating that the actual presence of individual lexicalitems is disruptive.
This indicates that regularities in the distinction of argumentsfrom adjuncts is indeed a class phenomenon and not an item-specific phenomenon.This is expected, according to current linguistic theories, such as the one proposed byLevin (1993).This analysis is confirmed by observing which are the most discriminative features,those that are closest to the root of the decision tree.
The topmost feature of the bestresult (line 7) is preposition, followed by the class of the verb and the class of the PPinternal noun.
Predictably, the class of the object noun phrase is not used because it isnot very informative.
The same features constitute the tree yielding the results of line 6.However, the presence of lexical items makes a difference to the tree that is built, whichis much more compact, but in the end less accurate.Combinations of All Features.
Table 2 reports the accuracy in the argument-adjunct dis-tinction of experiments that use only the most useful lexical and class features, thepreposition and the verb class, and the diagnostic-based features, using combinationsof diagnostic features.
The combinations of features shown are those that yielded thebest results over a development set of tuples extracted from Section 24 of the PennTreebank.
The results reported are calculated over a test corresponding to the tuples359Computational Linguistics Volume 32, Number 3Table 2Best results using preposition and combination of diagnostic-based features, in differentvariants.
The training and test sets are automatically annotated.Feature used Auto accuracy (%)1. vcl, prep, hdepv1, hdepv2, opt1, opt2, opt3, iterv 79.32. vcl, prep, hdepv1, hdepv2, opt1, opt3, iterv 79.83. vcl, prep, hdepv1, hdepv2, opt2, opt3, iterv 79.04. vcl, prep, hdepv1, opt2, opt3, iterv 78.2in Section 23 of the Penn Treebank.
The best combination, line 2, yields a 37% reductionof the error rate over the baseline.
All the differences in performance among theseconfigurations are significant.
What all these combinations have in common is thatthey are combinations of three levels of granularity, mixing lexical information, classinformation, and higher level syntactic-semantic information, encoded indirectly in thediagnostics.
All the combinations that are not shown here have lower accuracies.Table 3 shows the confusion matrix for the combination of features with the bestaccuracy listed above.
These figures yield a precision and recall for arguments of 80%and 85%, respectively (F measure = 82%); and a precision and recall for adjuncts of 80%and 73%, respectively (F measure = 76%).
Clearly, although both kinds of PPs are wellidentified, arguments are better identified than adjuncts, an observation already madeby several other authors, especially Hindle and Rooth (1993) in their detailed discussionof the errors in a noun or verb PP-attachment task.
In particular, we notice that moreadjuncts are misclassified as arguments than vice versa.The results of these experiments confirm that corpus information is conduciveto learning the distinction under discussion without explicitly represented complexsemantic knowledge.
They also confirm that this distinction is essentially a word classphenomenon?and not an individual lexical-item phenomenon?as would be expectedunder current theories of the syntax?semantics interface.
Finally, the combination oflexical items, classes, and linguistic diagnostics yields the best results.
This indicatesthat using features of different levels of granularity is beneficial, probably becausethe algorithm has the option of using more specific information when reliable, whileabstracting to coarser-grained information when lexical features suffer from sparse data.This interpretation of the results is supported by observing which features are at the topof the tree.
Interestingly, here the topmost feature is head dependence (the lexical variant,hdepv1), on one side of which we find preposition as the second most discriminativefeature, followed by head dependence (hdepv2) again, and optionality (class variants).
OnTable 3Confusion matrix of the best classification of PPs attached to the verb.
Training and test setestablished automatically.Assigned classesArguments Adjuncts TotalActual classes Arguments 300 51 351adjuncts 76 202 278Total 376 253 629360Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachmentthe other side of the tree, we find preposition as the second most informative feature andverb class as the third most discriminative feature.Results on Partly Manually Labeled Set.
Tables 4 and 5 report the results obtained by train-ing the classifier on the automatically labeled training set and testing on the manuallylabeled test set.
They illustrate the effect of training the decision tree classifier on atraining set that has different properties from the test set.
This experiment provides alower bound of performance across different samples and shows which are the featureswith the greatest generalization ability.
We can draw several conclusions.
First, thelexical features do better than chance, but do not do better than the baseline establishedby using only the preposition as a feature (lines 1, 2, and 3 of Table 4).
Secondly,classes do better than the baseline (line 7 of Table 4) and so do the diagnostic features(Table 5).
Since we are using a training and a test set with different properties, theseresults indicate that classes and diagnostics capture a level of generality that the lexicalfeatures do not have and will be more useful across domains and corpora.
Finally,the rank of performance for different feature combinations holds across training andtesting methods, whether established automatically or manually, as can be confirmedby a comparison of Tables 1 and 4 and also 2 and 5.
The difference in performance withdiagnostics (line 3 of Table 5) and without, using only classes (line 7 of Table 4), is onlymarginally significant, indicating that diagnostics are not useless.Table 4Accuracy of the argument?adjunct distinction for VP-attached PPs, using combinationsof lexical features.
The training set is automatically annotated while the test set is in partannotated by hand.Feature used Manual accuracy (%)1.
Chance (args) 37.02.
Prep (baseline) 61.23.
Lexical features (verb, prep, n2) 61.24. vcl, prep 67.15.
Prep, n2cl 66.86.
Lexical features and classes (v, vcl, p, n2, n2cl) 62.87.
Only classes (vcl, p, n2cl) 70.08.
Only verb classes (vcl, p, n2) 67.79.
Only noun classes (v, p, n2cl) 64.910.
All features 66.0Table 5Best results using prepositions and combination of diagnostic-based features in differentvariants.
The training set is automatically annotated, whereas the test set is in part annotatedby hand.Feature used Manual accuracy (%)1.
Baseline (prep) 67.32. vcl, prep, hdepv1, hdepv2, opt1, opt2, opt3, iter 67.23. vcl, prep, hdepv1, hdepv2, opt1, opt3, iter 69.04. vcl, prep, hdepv1, hdepv2, opt2, opt3, iter 67.65. vcl, prep, hdepv1, opt2, opt3, iter 65.7361Computational Linguistics Volume 32, Number 3Results on Test Set without Bare PPs.
The biggest discrepancy in validating the automaticlabeling was found for PPs without functional tags.
The automatic labeling had classi-fied bare PPs as argument but the manual gold standard assigns more than half of themto the adjunct class.
They are therefore a source of noise in establishing reliable results.
Ifwe remove these PPs from the training and test set, results improve and become almostidentical across the manually and automatically labeled sets, as illustrated in Table 6.74.3 Results on N Attachment CasesExperiments on learning the distinction between argument PPs and adjunct PPs at-tached to a noun show a different pattern, probably due to a ceiling effect.
The ex-periments reported below are performed on examples of prepositional phrases whosepreposition is not of .
The reason to exclude the preposition of is that it is 99.8% ofthe time attached as an argument.
Moreover, it accounts for approximately half of thecases of NP attachment.
Results including this preposition would therefore be overlyoptimistic and not be representative of the performance of the algorithm in general.The size of the resulting corpus?without of prepositional phrases?is 3,364 tuples.The results illustrated in Tables 7 and 8 show that head dependence is the only featurethat improves numerically the performance above the already very high baseline thatcan be obtained by using only the feature preposition.
This difference is not statisticallysignificant, indicating that neither class nor diagnostics add useful information.4.4 Results Using a Different Learning AlgorithmIn the previous sections, we have shown that different combinations of features yieldsignificantly different performances.
We would like to investigate, at least on a firstapproximation, if these results hold across different learning algorithms.
To test thestability of the results, we compare the main performances obtained with a decision treeto those obtained with a different learning algorithm.
In recent years, a lot of attentionhas been paid to large margin classifiers, in particular support vector machines (SVMs)(Vapnik 1995).
They have been shown to perform quite well in many NLP tasks.
Thefact that they search for a large separating margin between classes makes them lessprone to overfitting.
We expect them, then, to perform well on the task trained onautomatically labeled data and tested on manually labeled data, where a great abilityto generalize is needed.
Despite being very powerful, however, SVMs are complexalgorithms, often opaque in their output.
They are harder to interpret than decisiontrees, where the position of the features in the tree is a clear indication of their relevancefor the classification.
Finally, SVMs take a longer time to train than decision trees.
Forthese reasons they constitute an interestingly different learning technique from decisiontrees, but not a substitute for them if clear interpretation of the induced learner is neededand many experiments need to be run.All the experiments reported below were performed with the LIBSVM package(Chang and Lin 2001, version 2.71).
The SVM parameters were set by a grid search onthe training set, by 10-fold cross-validation.
Given the longer training times, we performonly a few experiments.
For V attachment, the baseline using only the preposition7 The features indicated here as best are the ones used in the other experiments and kept for the sake ofcomparison.
But, in fact, a different feature combination found by accident gives a result of 79.3%accuracy in classifying the automatically labeled set.362Merlo and Esteve Ferrer Argument in Prepositional Phrase AttachmentTable 6Best results using preposition and combination of diagnostic-based features, in differentvariants, taking out PP examples.Feature used Manual accuracy (%) Auto accuracy (%)1.
Chance baseline (args) 67.3 37.02.
Prep baseline 64.4 64.63.
Classes 74.5 74.74.
Best features combination 76.1 77.0Table 7Baselines and performances using lexical heads and classes for N-attached PPs.Features used Accuracy (%)chance (arg) 58.3p (baseline) 93.3n1, p, n2 93.3n1cl, p, n2cl 93.3n1, n1cl, p, n2, n2cl 93.3reaches an accuracy of 62.2% on the manually labeled test set, whereas performanceusing the same best combination of features as the decision tree reaches 70.6% accuracy.There is, then, a little improvement over the 69% of the decision tree learner, as expected.The performance on the N-attached cases, on the other hand, is surprisingly poor, witha low 43% accuracy on testing data.
This result is probably due to overfitting, since thebest accuracy on the training set is around 95%.4.5 ConclusionsThe results reported in this section show that the argument-adjunct distinction can belearned based on information collected from an annotated corpus with good accuracy.For verb attachment, they show in particular that using lexical features yields betterperformance than the baseline, especially when we use lexical classes.
For automaticallylabeled data, diagnostics based on linguistic theory improve the performance evenfurther.
Thus, the hypotheses we were testing with these experiments are confirmed.The reported results are good enough to be practically useful.
In particular, thedistinction between arguments and adjuncts attached to nouns is probably performedas well as possible with an automatic method, even by simply using prepositionsas features.
For the attachment to verbs, known to be more difficult, more room forTable 8Performances using some combinations of features for N-attached PPs.Features used Accuracy (%)n1cl, prep, n2cl, hdepn1, hdepn2, hdepn3, itern, para, deverb 93.9prep, hdepn1, hdepn2, hdepn3, itern, para, deverb 93.9prep, hdepn1 94.1363Computational Linguistics Volume 32, Number 3improvement exists, especially in the recovery of adjuncts.
The comparison of decisiontrees to SVMs does not appear to indicate that one learning algorithm is consistentlybetter than the other.5.
Hypothesis 2: PP Attachment DisambiguationOnce we have established the fact that arguments and adjuncts can be learned froma corpus with reasonable accuracy using cues correlated to linguistic diagnostics, weare ready to investigate how this distinction can be integrated in the disambiguation ofambiguously attached PPs, the PP attachment problem as usually defined.The first question that must be asked is whether the distinction between argumentsand adjuncts is so highly correlated with the attachment site of the ambiguous PP tobe almost entirely derivative.
For example, the PTB annotators have annotated all nounattachments as adjuncts and all verb attachments as arguments.
If this were the correctrepresentation of the linguistic facts, having established an independent procedure todiscriminate argument from adjuncts PPs would be of little value in the disambiguationproblem.
In fact, there is no theoretical reason to think that the notion of argument isclosely correlated to the choice of attachment site of a PP, given that both verb and nounattached PPs can have either an argument or an adjunct function.
It might be, however,that some distributional differences that are lexically related, or simply nonlinguistic,exist and that they can be exploited in an automatic learning process.We can test the independence of the distribution of arguments and adjuncts fromthe distribution of noun or verb attachment with a ?2 test.
The test tells us that the twodistributions are not independent (p < .001).
It remains, however, to be established ifthe dependence of the two distributions is sufficiently strong to improve learning ofone of the two classifications, if the other is known.
This question can be investigatedempirically by augmenting the training features for a learning algorithm that solvesthe usual binary attachment problem with the diagnostic features for argumenthood.If this augmentation results in an improvement in the accuracy of the PP attachment,then we can say that the notion of argument is, at least in part, related to the attachmentsite of a PP.
If no improvement is found, then this confirms that the argument statusof a PP must be established independently.
Conversely, we can augment the input tothe classification into argument and adjuncts with information related to the attach-ment site to reach analogous conclusions about the distinction between argumentsand adjuncts.Corpora and Materials.
The data for the experiments illustrated below are drawn fromthe same corpora as those used in the previous experiments.
In particular, recall that thecorpus from which we draw the statistics is different from the corpus from which wedraw the training examples and also from the testing corpus.
In both sets of experimentsdescribed below, we restrict our observation to those examples in the corpus that areambiguous between the two attachment sites, as is usual in studies of PP attachment.The values of the learning features were calculated on all the instances in the statisticscorpus, so in practice we use both the unambiguous cases and ambiguous cases in theestimation of the features of the ambiguous cases.The Input Data.
Each input vector represents an instance of an ambiguous PP attachment,which could be both noun or verb attached, either as an argument or as an adjunct.Each vector contains 20 training features.
They comprise the four lexical heads, their364Merlo and Esteve Ferrer Argument in Prepositional Phrase AttachmentWordNet classes (v, n1, p, n2, vcl, n1cl, and n2cl), and all the different variants of the im-plementation of the diagnostics.
Finally, depending on the experiments reported belowwe use either a two-valued target feature (N or V) or a four-valued target feature (Narg,Nadj, Varg, Vadj), indicating the type of attachment.
More specifically, the featuresimplementing the diagnostics are the variants of the measures of head dependence forthe PPs attached to verbs and nouns, respectively (hdepv1, hdepv2, and hdepn1, hdepn2,hdepn3); the variants of the measure of optionality (opt1, opt2, opt3); the measures ofiterativity for verb-attached and noun-attached PPs, respectively (iterv, itern); and finallythe measures for copular paraphrase and deverbal noun, respectively (para, deverb).We use the C5.0 Decision Tree Induction Algorithm (Quinlan 1993), and the imple-mentation of SVMs provided in version 2.71 of LIBSVM (Chang and Lin 2001).5.1 Relationship of Noun?Verb Attachment Disambiguation to theArgument-Adjunct Distinction and Vice VersaHere we report on results for the task of disambiguating noun or verb attachmentfirst, using, among other input features, those that have been established to make theargument-adjunct distinction.
The same corpora described above were used, with atwo-valued target (N or V).
We report results for two sets of experiments.
One set ofexperiments takes all examples into account.
In another set of experiments, examplescontaining the preposition of were not considered, as this preposition is extremelyfrequent (it adds up to almost half of the noun attachment cases) and it is almost alwaysattached to a noun as argument.
It has therefore a very peculiar behavior.
The bestcombination of features reported below was established using Section 24 of the PennTreebank; all the tests reported here are in Section 23.Table 9 reports the disambiguation accuracy of the comparative experiments per-formed.
The first line reports the baseline accuracy for the task, calculated by per-forming the classification using only the feature preposition.
The best result is obtainedby a combination of features in which lexical classes act as the predominant learningfeature, either in combination with the lexical items or alone (line 2 with of , line 3without of ).
We note, however, that the diagnostic features that are included in the bestdiagnostic feature combination are those based in part on individual words and notthose based entirely on classes.
Most importantly, those diagnostics that are meant todirectly indicate the argument or adjunct status of a PP do not help in the resolution ofPP attachment, as expected.Table 9Percent accuracy using combinations of features for two-way attachment disambiguation.Best combinations for experiments with of is (opt1, hdepv1, hdepn1, para) and forexperiments without of is (opt1, opt2, hdepv1, hdepv2, hdepn1, hdepn2, para).Features used Accuracy with of (%) Accuracy without of (%)1.
Prep (baseline) 70.9 59.52.
Prep + classes 78.1 71.33.
Only classes 70.2 72.34.
Only all diagnostics 75.9 64.55.
Prep + all diagnostics 77.1 68.26.
Prep + best feature combination 75.8 67.87.
Prep + classes + best feature combination 76.6 67.5365Computational Linguistics Volume 32, Number 3We conclude, then, that the notion of argument and adjunct is only partially cor-related to the classification of PPs into those that attach to the noun and those thatattach to the verb.
Clearly, diagnostics are not related to the attachment site, but lexicalclasses appear to be.
On the one hand, this result indicates that the notion of argumentis not entirely derivative from the attachment site.
On the other hand, it shows thatsome features developed to distinguish arguments from adjuncts could improve thedisambiguation of the attachment site.The same conclusion is confirmed by a simpler, much more direct experiment,where the classification into noun or verb attachment is performed with a single inputattribute.
This attribute is the feature indicating if the example is an argument or anadjunct, and it is calculated by a binary decision tree classifier using the best featurecombination on the argument-adjunct discrimination task.
In this case too the classifi-cation accuracy is a little (2.5%) better than chance baseline.The converse experiment does not reveal any correlations, confirming that theinterdependence between the two factors is weak.
The attachment status of the am-biguous PP, whether noun or verb attached, is input among other features, to determinewhether the PP is an argument or an adjunct.
Results are shown in Table 10, wherethe attachment feature is called NVstatus.
Lines 1 and 2 show that NVstatus is abetter baseline than chance.
Lines 3 and 4 indicate that the feature preposition offers agood baseline over which NVstatus improves only if the preposition of is included,as expected.
Lines 5 and 6 and lines 7 and 8 show that adding NVstatus to the otherfeatures does not improve performance.
As previously, the lexical classes are the bestperforming features.The same conclusion is reached by a simple direct experiment where we classifyPPs into arguments and adjuncts using as only input feature the output of a classifierbetween noun or verb attachment.
This attachment classifier is trained on the bestfeature combination, preposition, and word classes.
We find that the attachment statushas no effect on the accuracy of the classification, as the feature is not used.Overall, these results indicate that there is a small interdependence between the twoclassification problems and therefore weakly support a view of the PP disambiguationproblem where both the site of the attachment and the argument or adjunct functionof the PP are disambiguated together in one step.
In the next section, we explore thishypothesis, while also investigating which feature combinations give the best results ina four-way PP classification, where PPs are classified as noun arguments, noun adjuncts,verb arguments, and verb adjuncts.Table 10Percent accuracy using combinations of features for argument adjunct classification, includingNV as input feature.
Best features combination is (opt1, opt3, hdepv1, hdepv2, hdepn1).Features used Accuracy with of (%) Accuracy without of (%)1.
Chance (args) 69.6 55.92.
NVstatus 73.0 62.33.
Prep (baseline) 81.6 82.04.
NVstatus + prep 87.2 81.55.
Prep + classes 89.2 84.66.
Prep + classes + NVstatus 89.0 84.17.
Prep + classes + best features + NVstatus 88.2 82.98.
Prep + classes + best features 88.2 83.6366Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment5.2 One- and Two-step Four-way ClassificationHaving shown that argumenthood of a PP is not entirely derivative of its attachmentsite, but that the two tasks are weakly correlated, the task of PP attachment itself isreformulated as a four-way classification, yielding a finer-grained and more informativeclassification of PP types.As discussed in the introduction, those applications for which the resolution of thePP attachment ambiguity is important often need to know more about the PP than itsattachment site, in particular one might need to know whether the PP is an argumentor not.
For example, the output of a parser might be used to determine the kernel of asentence?the predicate with its arguments?for further text processing, for translation,or for constructing a lexicon.
We redefine therefore the problem of PP attachment as afour-way classification problem.
We investigate here what features are best predictorsof this classification.
Again, we report results for two sets of experiments.
One set ofexperiments takes all examples into account, whereas the other excludes all examplesincluding the preposition of .
As usual, the best combination of features reported belowwas established using Section 24; all the tests reported here are in Section 23.To classify PPs into four classes, we have two options: We can construct a singlefour-class classifier or we can build a sequence of binary classifiers.
The discriminationbetween noun and verb attachment can be performed first, and then further refined intoattachment as argument or adjunct, performing the four-way classification in two steps.The two-step approach would be the natural way of extending current PP attachmentdisambiguation methods to the more specific four-way attachment we propose here.However, based on the previous experiments, which showed a limited amount ofdependence between the two tasks, previous work on a similar data set (Merlo 2003),and general wisdom in machine learning, there is reason to believe that it is betterto solve the four-way classification problem directly rather than first solving a moregeneral problem and then specializing the classification.To test these expectations, we performed both kinds of experiments?a direct four-way classification experiment and a two-step classification experiment?to investigatewhich of the two methods is better.
The direct four-way classification uses the attributesdescribed above to build a single classifier.
For comparability, we created a two-stepexperimental setup as follows.
We created three binary classifiers.
The first one performsthe noun?verb attachment classification.
Its learning features comprise the four lexicalheads and their WordNet classes.
We also train two classifiers that learn to distinguisharguments from adjuncts.
One classifier is trained only on verb-attachment exemplarsand uses only the best verb-attachment-related features.
The third classifier is trainedonly on noun-attachment exemplars and utilizes only the best noun-attachment-relatedfeatures.
The test data is first given to the noun?verb attachment classifier.
Then, thetest examples classified as verbs are given to the verb argument-adjunct classifier, andthe test examples classified as nouns are given to the noun argument-adjunct classifier.Thus, this cascade of classifiers performs the same task as the four-way classifier, but itdoes so in two passes.Table 11 shows that overall the one-step classification is better than the two-stepclassification, confirming the intuition that the two labeling problems should be solvedat the same time.8 However, if we break down the performance, we see that recall of8 The difference between the two results is significant (p < .05) according to the randomized test describedin Yeh (2000).367Computational Linguistics Volume 32, Number 3Table 11Percent precision, recall, and F score for the best two-step and one-step four-way classificationof PPs, including and not including the preposition of.Two-step + of One-step + ofPrec Rec F Prec Rec FV-arg 37.5 45.6 41.2 42.2 29.3 34.6V-adj 56.2 52.2 54.1 59.6 60.2 59.9N-arg 83.0 83.5 83.2 81.3 91.3 86.0N-adj 71.2 57.5 63.6 69.5 56.2 62.1Accuracy 68.9 72.0Two-step ?
of One-step ?
ofPrec Rec F Prec Rec FV-arg 41.3 50.0 45.3 42.2 31.4 36.0V-adj 52.8 41.6 46.5 59.6 60.2 59.9N-arg 67.3 70.0 68.6 65.4 80.7 71.9N-adj 60.3 60.3 60.3 69.5 56.2 62.1Accuracy 56.6 60.9V-arg is lower in the one-step procedure than in the two-step procedure, in both cases,and that the overall performance for V-arg is worse in the one-step procedure.
Thismight indicate that which procedure to use depends on whether precision or recall oroverall performance is most important for the application at hand.Table 12 reports the confusion matrix of the classification that reaches the bestperformance without the preposition of, which corresponds to the lower right panelof Table 11.
It can be observed that performances are reasonably good for verb adjuncts,and noun arguments and adjuncts, but they are quite poor for the classification of prepo-sitional phrases that are arguments of the verb.
It is not clear why verb arguments are sobadly classified.
We tested the hypothesis that this result is a side effect of the mappingwe have defined from the Penn Treebank label to the label we use in this classifier,arguments or adjuncts.
Recall that untagged PPs have been mapped onto the argumentlabel, but these are highly inconsistent labels, as we have seen in the manual validationof the target attribute in Section 3.
Then, verb arguments might be represented by noisiertraining examples.
This hypothesis is not confirmed.
In a little experiment where thetraining data did not contain untagged verb-attached PPs, the overall performance inidentifying the verb argument class did not improve.
An improvement in precision wascounteracted by a loss in recall, yielding slightly worse F measures.
Another observationrelated to verb arguments can be drawn by comparing the experiments reported inSection 4 to the experiments reported in the current section.
This comparison showsthat the low performance in classifying verb arguments does not arise because of aninability to distinguish verb arguments from verb adjuncts.
Rather, it is the interac-tion of making this distinction and disambiguating the attachment site as a singleclassification task that creates the problem.
This is also confirmed by the considerablenumber of cases of noun arguments and verb arguments that are incorrectly classi-fied, as shown in Table 12.
Clearly, further study of the properties of verb argumentsis needed.368Merlo and Esteve Ferrer Argument in Prepositional Phrase AttachmentTable 12Confusion matrix of the best one-step four-way classification of PPs without the preposition of .Assigned classesV-arg V-adj N-arg N-adj TotalActual classes V-arg 27 17 39 3 86V-adj 15 68 18 12 113N-arg 19 7 121 3 150N-adj 3 22 7 41 73Total 64 144 185 59 422Overall, it is interesting to notice that solving the four-way task causes only a littledegradation to the accuracy of the original disambiguation between attachment to thenoun or to the verb.
On this data set, the accuracy of disambiguating the attachmentsite is of 83.6% (without PPs containing of ).
Accuracy decreases a little to 82.7% if thebinary attachment disambiguation result is calculated on the output of the four-waytask.
This little degradation is to be expected as the four-way task is more difficult.
Theaccuracy of the four-way task on the simple noun or verb binary attachment distinc-tion seems, however, acceptable, if one considers that a finer-grained discriminationis provided.Table 13 reports the classification accuracy of a set of comparative experiments.Here again, the first line reports the baseline accuracy for the task, calculated byperforming the classification using only the feature preposition.
We notice that in bothcolumns the best results are obtained by the same combination of features that includessome lexical features, some classes, and some diagnostic features.
This shows that thedistinction between arguments and adjuncts is not exclusively a syntactic phenom-enon and lexical variability plays an important role.
Similarly to the previous set ofexperiments, we note that the diagnostic features that are included in the best featurecombination are those based, at least in part, on individual words, and not those basedentirely on classes.
The importance of lexical classes, however, is confirmed by thefact that the best result is only marginally better than the second best result, in whichlexical classes act as the predominant learning feature, either in combination with thelexical items or alone (line 3 with of , line 2 without of ).
We can conclude from theseobservations that using various features defined at different levels of granularity allowsthe learner to better use lexical information when available, and to use more abstractTable 13Percent accuracy using combinations of features for a one-step four-way classification of PPs.Best combination = (vcl, n1cl, p, opt1, opt2, hdepv1, hdepv2, hdepn1, para).Features Accuracy (%) with of Without of (%)1.
Prep (baseline) 64.2 49.52.
Prep + classes 68.9 60.23.
Only classes 71.5 49.34.
All features 68.9 54.55.
Only diagnostics 67.4 54.36.
Best combination 72.0 60.9369Computational Linguistics Volume 32, Number 3levels of generalization when finer-grained information is not available.
Across the twotasks (illustrated in Tables 9 and 13) we notice that the best diagnostics features arealmost always the same.
Variants change, but the kinds of diagnostics that are usefulremain stable.
This probably indicates that some diagnostics are reliably estimated,whereas others are not, and cannot be used fruitfully.5.3 Results Using Support Vector MachinesAs mentioned above, SVMs have yielded very good results in many important applica-tions in NLP.
It is reasonable to wonder if we can improve the results for the four-wayclassification, and especially the less than satisfactory performance on verb arguments,using this learning algorithm.
Table 14 shows the results to be compared to those in theright-hand panel of Table 11.If we consider the F-measures of this table and the right-hand panel of Table 11, themost striking difference is that V-arguments, although still the worst cell in the table,have improved by almost 20% (36.0% vs. 55.9%) in performance for the experimentswithout of.
Notice that verb arguments are now better classified than in the two-stepmethod.
Also, the overall accuracy is significantly improved by several percentagepoints, especially for the condition without the preposition of (p < .02).5.4 ConclusionIn this section we have shown that the notion of argument is of limited help in dis-ambiguating the attachment of ambiguous PPs, indicating that the two notions arenot strictly related and must be established independently.
In a series of four-wayclassification experiments, we show that the classification performances are reasonablygood for verb adjuncts, noun arguments, and noun adjuncts, but they are poor forthe classification of prepositional phrases that are arguments of the verb, if decisiontrees are used.
Overall performance and especially identification of verb argumentsis improved if support vector machines are used.
We also show that better accuracyis achieved by performing the four-way classification in one step.
The features thatappear to be most effective are lexical classes, thus confirming current linguistic theoriesthat postulate that a given head?s argument structure depends on the head?s lexicalsemantics, especially for verbs (Levin 1993).Table 14Percent precision, recall, and F-score for the best four-way classification of PPs, includingand not including the preposition of using SVMs.One-step + of One-step ?
ofPrec Rec F Prec Rec FV-arg 59.5 47.8 53.0 60.0 52.3 55.9V-adj 63.2 63.7 63.4 62.8 62.8 62.8N-arg 83.6 93.4 88.2 69.9 85.3 76.9N-adj 72.5 50.7 59.7 72.5 50.7 59.7Accuracy 75.9 66.6370Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachment6.
Related WorkThe resolution of the attachment of ambiguous PPs is one of the staple problems incomputational linguistics.
It serves as a good testing ground for new methods as itis clearly defined and self-contained.
We have argued, however, that it is somewhatoversimplified, because knowing only the attachment site of a PP is of relativelylittle value in a real application.
It would be more useful to know where the PPis attached and with what function.
We review below the few pieces of work thathave tackled the problem of labeling PPs by function (arguments or adjuncts) as aseparate labeling problem.
Other pieces of work have asked a similar question inthe context of acquiring high-precision subcategorization frames.
We review a few ofthem below.6.1 On the Automatic Distinction of Arguments and AdjunctsA few other pieces of work attempt to distinguish PP arguments from adjuncts auto-matically (Buchholz 1999; Merlo and Leybold 2001; Villavicencio 2002).
We extend andmodify here the preliminary work reported in Merlo and Leybold (2001) by extendingthe method to noun attachment, elaborating more learning features, including casesspecifically developed for noun attachment, refining all the counting methods, thusvalidating and extending the approach.The current work on automatic binary argument-adjunct classifiers appears tocompare favorably to the only other study on this topic (Buchholz 1999).
Buchholz(1999) reports an accuracy of 77% for the argument-adjunct distinction of PPs per-formed with a memory-based learning approach, to be compared with our 80% and94% for verb and noun attachments, respectively.
However, the comparison cannotbe very direct, as Buchholz considers all types of attachment sites, not just verbsand nouns.More recently, Villavicencio (2002) has explored the performance of an argu-ment identifier, developed in the framework of a model of child language learning.Villavicencio concentrates on locative PPs proposing that the distinction between oblig-atory arguments, optional arguments, and adjuncts is made based on two features: afeature derived from a semantically motivated hierarchy of prepositions and predicatesand a simple frequency cutoff of 80% of co-occurrence between the verb and the PPthat distinguishes obligatory arguments from the other two classes.
She evaluates theverbs put, come, and draw (whose locative arguments belong to the three classes above,respectively).
The approach is not directly comparable, as it is not entirely corpus-based(the input to the algorithm is an impoverished logical form), and the evaluation is on asmaller scale than the present work.
On a test set of the occurrences of three verbs, whichis the same set inspected to develop the learning features, Villavicencio gets perfectperformance.
These are very promising results, but because they are not calculated ona previously unseen test set, the generability of the approach is not clear.
Moreover,Villavicencio applies only one diagnostic test to determine if a PP is an argument oran adjunct, whereas our extensive validation study has shown that several tests arenecessary to reach a reliable judgment.In a study about the automatic acquisition of lexical dependencies for lexicon build-ing, Fabre and Bourigault (2001) discuss the relation of the problem of PP attachmentand the notion of argument and adjunct.
They correctly notice that approaches suchas theirs, inspired by Hindle and Rooth (1993), are based on the assumption that high371Computational Linguistics Volume 32, Number 3co-occurrence between words is an indication of a lexical argumenthood relation.
Asalso noticed in Merlo and Leybold (2001), this is not always the case: some adjunctsfrequently co-occur with certain heads too.
Fabre and Bourigault propose a notion ofproductivity that strongly resembles our notions of optionality and head dependence tocapture the two intuitions about the distribution of arguments and adjuncts.
Argumentsare strongly selected by the head (the head to complement relation is not productive),whereas adjuncts can be selected by a wide spread of heads (the complement to headselection is highly productive).
They propose, but do not test, the hypothesis thatthis notion might be useful for the general problem of PP attachment.
The resultsin the current article show that this is not the case.
In fact, we have argued thatthere is no real reason to believe that the two notions should be related, other thanmarginally.6.2 On the Distinction of Argument from Adjunct PPs forSubcategorization AcquisitionAs far as we are aware, this is the first attempt to integrate the notion of argumenthoodin a more comprehensive formulation of the problem of disambiguating the attachmentof PPs.
Hindle and Rooth (1993) mention the interaction between the structural and thesemantic factors in the disambiguation of a PP, indicating that verb complements are themost difficult.
We confirm their finding that noun arguments are more easily identified,whereas verb complements (either arguments or adjuncts) are more difficult.
Otherpieces of work address the current problem in the larger perspective of distinguishingarguments from adjuncts for subcategorization acquisition (Korhonen 2002a; Aldezabalet al 2002).The goal of the work of Korhonen (2002a, 2002b) is to develop a semantically drivenapproach to subcategorization frame hypothesis selection that can be used to improvelarge-scale subcategorization frame acquisition.
The main idea underlying the approachis to leverage the well-known mapping between syntax and semantics, inspired byLevin?s (1993) work.
Korhonen uses statistics over semantic classes of verbs to smootha distribution of subcategorization frames and then applies a simple frequency cutoff toselect the most reliable subcategorization frames.
Her work is related to ours in severalways.
First, the automatic acquisition task leverages correspondences between syntaxand semantics, particularly clear in the organization of the verb lexicon, similarlyto Merlo and Stevenson (2001).
Some of our current results are also based on thiscorrespondence, as we assume that the notion of argument is a notion at the interfaceof the syntactic and semantic levels, and participates in both, determining not only thevalency of a verb but also its subcategorization frame.
Our work confirms the resultsreported in Korhonen (2002a), which indicate that using word classes improves theextraction of subcategorization frames.
Differently from Korhonen, however, we donot allow feedback between levels.
In her work, syntactic similarity of verbs?
subcat-egorization sets based on an external resource (LDOCE codes) are used to determine asemantic classification of verbs?or rather to partially reorganize Levin?s classification.This semantic classification is then used to collect statistics that are used to smooth asubcategorization distribution.
In our work, instead, we do use WordNet in some placesto give us information on verb classes, but we never use explicit semantic informationon the set of verb subcategorization frames to determine the notion of argumentor adjunct.Aldezabal et al (2002) is another piece of work related to our current proposal.In this article, the distinction between arguments and adjuncts is made to determine372Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachmentthe subcategorization frames of verbs for a language, Basque, for which not many de-veloped resources exist.
This illustrates another use of the distinction between argumentand adjuncts, which is not apparent when working on English.6.3 On Learning Abstract Notions Using Corpus-based Statistical MethodsLearning arguments and adjuncts is an example of learning simplified semantic in-formation by using syntactic and lexical semantic correlates.
We learn the target con-cepts of arguments and adjuncts by using corpus-based indicators of their proper-ties.
It remains to be determined if we just learn correlates of a unified notion, or ifthe distinction between arguments and adjuncts is a clustering of possibly unrelatedproperties.As explained in the introduction, native speakers?
judgments on the argument andadjunct status of PPs are very unstable.
No explanation is usually proposed of thefact that the tests of argumenthood are often difficult to judge or even contradict eachother.
As a possible explanation for the difficulty in pinpointing exactly the propertiesof arguments and adjuncts, Manning (2003) suggests that the notion of argument oradjunct is not categorical.
The different properties of argument and adjuncts are not thereflex of a single grammatical underlying notion, but they can be ascribed to differentmechanisms.
What appears as a not entirely unified behavior is in fact better explainedas separate properties.The current article provides a representation that can support both the categor-ical and the gradient approach to the distinction between arguments and adjuncts.We have decomposed the notion of argument into a vector of features.
The notionof argumenthood is no longer necessarily binary, but it allows several dimensions ofvariation, each potentially related to a different principle of grammar.
In the currentarticle, we have adopted a supervised approach to the learning task and adopteda binary classification.
To pursue a line of reasoning where a gradient representa-tion of the notion of argument is preferred, we would no longer be interested inclassifying the vectorial information according to a predetermined binary or four-way target value, as was done in the supervised learning experiments.
The appropri-ate framework would then be unsupervised learning, where several algorithms areavailable to explore the hidden regularities of a vectorial representation of clustersof PPs.Whether a linguistic notion should be considered categorical or gradient is botha matter of empirical fact and of the explanatory power of the theory into which thenotion is embedded.
Assessing the strengths and weaknesses of these two approachesis beyond the scope of the current article.7.
ConclusionsWe have proposed an augmentation of the problem of PP attachment as a four-waydisambiguation problem, arguing that what is needed in interpreting prepositionalphrases is knowledge about both the structural attachment site (the traditional noun?verb attachment distinction) and the nature of the attachment (the distinction ofarguments from adjuncts).
Practically, we have proposed a method to learn argumentsand adjuncts based on a definition of arguments as a vector of features.
Each featureis either a lexical element or its semantic class or it is a numerical representation ofa diagnostic that is used by linguists to determine if a PP is an argument or not.
We373Computational Linguistics Volume 32, Number 3have shown in particular that using lexical classes as features yields good results, andthat diagnostics based on linguistic theory improve the performance even further.
Wehave also argued that the notion of argument does not help much in disambiguatingthe attachment site of PPs, indicating that the two notions are not closely correlatedand must be established independently.
We have performed a series of four-wayclassification experiments, where we classify PPs as arguments or adjuncts of a noun,and as arguments or adjuncts of a verb.
We show that the classification performances arereasonably good for verb adjuncts, noun arguments, and noun adjuncts, independentof the learning algorithm.
Classification performances of prepositional phrases that arearguments of the verb are poor if decision trees are used, but are greatly improved bythe use of a large margin classifier.
The features that appear to be most effective arelexical classes, thus confirming current linguistic theories that postulate that a verb?sargument structure depends on a verb?s lexical semantics (Levin 1993).
Future worklies in further investigating the difference between arguments and adjuncts to achieveeven finer-grained classifications and to model more precisely the semantic core ofa sentence.1.
Appendix: PP ConfigurationsSequence of single PP attached to a verbConfiguration Structure ExampleTransitive [vp V NP PP] join board as directorPassive [vp NP PP] tracked (yield) by reportSentential Object [vp V NP PP] continued (to slide) amid signsIntransitive [vp V PP] talking about yearsSequence of single PP attached to a nounNoun phrase [np NP PP] form of asbestosTransitive [vp V [np NP PP]] have information on usersTransitive with two PPs, one attached to verb, other to noun[vp V [np NP PP] PP] dumped sacks of material into binNoun phrase with two PPs attached low[np NP [pp P [np NP PP]]] exports at end of yearTransitive verb with two PPs attached low[vp V [np NP [pp P [np NP PP]]]] lead team of researchers from instituteTransitive with two PPs, one attached to verb, other to other PP[vp V NP [pp P [np NP PP]]] imposed ban on all of asbestosIntransitive with two PPs, one attached to verb, other to noun[vp V [pp P [np NP PP]]] appear in journal of medicinePhrasal object with two PPs[vp V NP [pp P [np NP PP]]] continued (to surge) on rumors of buyingPassive form with two PPs[vp V NP [pp P [np NP PP]]] approved (request) by houses of CongressSequence of 2 PPs attached to a verbIntransitive [vp V PP PP] grew by billion during weekPassive [vp V NP PP PP] passed (bill) by Senate in formsTransitive [vp V NP PP PP] giving 16 to graders at schoolSequence of 2 PPs attached to a nounNoun [np NP PP PP] sales of buses in OctoberTransitive [vp V [np NP PP PP]] meet demands for products in Korea374Merlo and Esteve Ferrer Argument in Prepositional Phrase AttachmentAcknowledgmentsMost of this research was conducted thanksto the generous support of the SwissNational Science Foundation, under grant1114-065328.01, while the second authorwas a master?s student at the Universityof Geneva.
We thank Eric Joanis for hisprecious help in constituting the database ofPP tuples.
We also thank Jamie Hendersonfor providing native-speaker judgments.ReferencesAldezabal, Izaskun, Maxux Aranzabe, KoldoGojenola, Kepa Sarasola, and AitziberAtutxa.
2002.
Learning argument/adjunctdistinction for Basque.
In Proceedings of theWorkshop of the ACL Special Interest Groupon the Lexicon on Unsupervised LexicalAcquisition, pages 42?50, Philadelphia, PA.Argaman, Vered and Neal Pearlmutter.
2002.Lexical semantics as a basis for argumentstructure frequency bias.
In Paola Merloand Suzanne Stevenson, editors, TheLexical Basis of Sentence Processing: Formal,Computational and Experimental Issues.
JohnBenjamins, Amsterdam/Philadelphia,pages 303?324.Baker, Collin F., Charles J. Fillmore, andJohn B. Lowe.
1998.
The BerkeleyFrameNet project.
In Proceedingsof the Thirty-sixth Annual Meetingof the Association for ComputationalLinguistics and Seventeenth InternationalConference on Computational Linguistics(ACL-COLING?98), pages 86?90,Montreal, Canada.Bies, Ann, Mark Ferguson, Karen Katz, andRobert MacIntyre.
1995.
Bracketingguidelines for Treebank II style, PennTreebank Project.
Technical report,University of Pennsylvania, Philadephia.Buchholz, Sabine.
1999.
Distinguishingcomplements from adjuncts usingmemory-based learning.
ILK,Computational Linguistics, TilburgUniversity.Chang, Chih-Chung and Chih-Jen Lin.2001.
LIBSVM: A Library for SupportVector Machines.
Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.Collins, Michael and James Brooks.
1995.Prepositional phrase attachmentthrough a backed-off model.
InProceedings of the Third Workshop on VeryLarge Corpora, pages 27?38,Cambridge, MA.CoNNL.
2004.
Eighth Conference onComputational Natural Language Learning(CoNLL-2004).
Boston, MA.CoNLL.
2005.
Ninth Conference onComputational Natural Language Learning(CoNLL-2005).
Ann Arbor, MI.Dorr, Bonnie.
1997.
Large-scale dictionaryconstruction for foreign language tutoringand interlingual machine translation.Machine Translation, 12(4):1?55.Fabre, Ce?cile and Didier Bourigault.
2001.Linguistic clues for corpus-basedacquisition of lexical dependencies.
InProceedings of the Corpus LinguisticsConference, pages 176?184, Lancaster, UK.Gildea, Daniel and Daniel Jurafsky.
2002.Automatic labeling of semantic roles.Computational Linguistics, 28(3):245?288.Grimshaw, Jane.
1990.
Argument Structure.MIT Press.Hindle, Donald and Mats Rooth.
1993.Structural ambiguity and lexical relations.Computational Linguistics, 19(1):103?120.Jackendoff, Ray.
1977.
X?
Syntax:A Study of Phrase Structure.
MIT Press,Cambridge, MA.Korhonen, Anna.
2002a.
Semanticallymotivated subcategorization acquisition.In Proceedings of the Workshop of the ACLSpecial Interest Group on the Lexicon onUnsupervised Lexical Acquisition,pages 51?58, Philadelphia, PA, July.Korhonen, Anna.
2002b.
SubcategorisationAcquisition.
Ph.D. thesis, University ofCambridge.Levin, Beth.
1993.
English Verb Classes andAlternations.
University of Chicago Press,Chicago, IL.Manning, Christopher.
2003.
Probabilisticsyntax.
In Rens Bod, Jennifer Hay, andStephanie Jannedy, editors, ProbabilisticLinguistics.
MIT Press, pages 289?314.Marantz, Alex.
1984.
On the Nature ofGrammatical Relations.
MIT Press,Cambridge, MA.Marcus, M., G. Kim, A. Marcinkiewicz,R.
Macintyre, A. Bies, M. Ferguson,K.
Katz, and B. Schasberger.
1994.
ThePenn Treebank: Annotating predicateargument structure.
In Proceedings of theARPA Workshop on Human LanguageTechnology, pages 114?119, Plainsboro, NJ.Marcus, Mitch, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19:313?330.Merlo, Paola.
2003.
GeneralisedPP-attachment disambiguation usingcorpus-based linguistic diagnostics.375Computational Linguistics Volume 32, Number 3In Proceedings of the Tenth Conferenceof the European Chapter of theAssociation for Computational Linguistics(EACL?03), pages 251?258, Budapest,Hungary.Merlo, Paola, Matt Crocker, and CathyBerthouzoz.
1997.
Attaching multipleprepositional phrases: Generalizedbacked-off estimation.
In Proceedingsof the Second Conference on EmpiricalMethods in Natural Language Processing,pages 145?154, Providence, RI.Merlo, Paola and Matthias Leybold.
2001.Automatic distinction of arguments andmodifiers: The case of prepositionalphrases.
In Proceedings of the FifthComputational Natural Language LearningWorkshop (CoNLL-2001), pages 121?128,Toulouse, France.Merlo, Paola and Suzanne Stevenson.
2001.Automatic verb classification based onstatistical distributions of argumentstructure.
Computational Linguistics,27(3):373?408.Miller, George, Richard Beckwith,Christiane Fellbaum, Derek Gross,and Katherine Miller.
1990.
Fivepapers on Wordnet.
Technical report,Cognitive Science Laboratory, PrincetonUniversity.Nielsen, Rodney and Sameer Pradhan.
2004.Mixing weak learners in semantic parsing.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMNLP-2004), pages 80?87, Barcelona,Spain, July.Palmer, Martha, Daniel Gildea, and PaulKingsbury.
2005.
The Proposition Bank:An annotated corpus of semantic roles.Computational Linguistics, 31:71?105.Phillips, William and Ellen Riloff.
2002.Exploiting strong syntactic heuristics andco-training to learn semantic lexicons.
InProceedings of the 2002 Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP 2002), pages 125?132,Philadelphia, PA.Pollard, Carl and Ivan Sag.
1987.
AnInformation-based Syntax and Semantics,volume 13.
CSLI Lecture Notes,Stanford University.Quinlan, J. Ross.
1993.
C4.5 : Programs forMachine Learning.
Series in MachineLearning.
Morgan Kaufmann,San Mateo, CA.Quirk, Randolph, Sidney Greenbaum,Geoffrey Leech, and Jan Svartvik.
1985.
AComprehensive Grammar of the EnglishLanguage.
Longman, London.Ratnaparkhi, Adwait.
1997.
A linearobserved time statistical parser based onmaximum entropy models.
In Proceedingsof the Second Conference on EmpiricalMethods in Natural Language Processing,pages 1?10, Providence, RI.Ratnaparkhi, Adwait, Jeffrey Reynar, andSalim Roukos.
1994.
A maximum entropymodel for prepositional phraseattachment.
In Proceedings of the ARPAWorkshop on Human Language Technology,pages 250?255, Plainsboro, NJ.Riloff, Ellen and Mark Schmelzenbach.1998.
An empirical approach to conceptualcase frame acquisition.
In Proceedingsof the Sixth Workshop on Very Large Corpora,pages 49?56, Montreal.Schu?tze, Carson T. 1995.
PP Attachment andArgumenthood.
MIT Working Papers inLinguistics, 26:95?151.SENSEVAL-3.
2004.
Third InternationalWorkshop on the Evaluation of Systems for theSemantic Analysis of Text (SENSEVAL-3).Barcelona, Spain.Srinivas, Bangalore and Aravind K. Joshi.1999.
Supertagging: An approach toalmost parsing.
Computational Linguistics,25(2):237?265.Stede, Manfred.
1998.
A generativeperspective on verb alternations.Computational Linguistics, 24(3):401?430.Stetina, Jiri and Makoto Nagao.
1997.Corpus based PP attachment ambiguityresolution with a semantic dictionary.
InProceedings of the Fifth Workshop on VeryLarge Corpora, pages 66?80, Beijing/Hong Kong.Swier, Robert and Suzanne Stevenson.2005.
Exploiting a verb lexicon inautomatic semantic role labelling.In Proceedings of the Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP-05), pages 883?890,Vancouver, Canada.Vapnik, V. 1995.
The Nature of StatisticalLearning Theory.
Springer.Villavicencio, Aline.
2002.
Learning todistinguish PP arguments from adjuncts.In Proceedings of the 6th Conference onNatural Language Learning (CoNLL-2002),pages 84?90, Taipei, Taiwan.Xue, Nianwen.
2004.
Handling dislocatedand discontinuous constituents in Chinesesemantic role labelling.
In Proceedings of theFourth Workshop on Asian LanguageResources (ALR04), pages 19?26, HainanIsland, China.Xue, Nianwen and Martha Palmer.
2004.Calibrating features for semantic role376Merlo and Esteve Ferrer Argument in Prepositional Phrase Attachmentlabeling.
In Proceedings of the 2004Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-2004),pages 88?94, Barcelona, Spain.Yeh, Alexander.
2000.
More accurate tests forthe statistical significance of resultdifferences.
In Proceedings of the 18thInternational Conference in ComputationalLinguistics (COLING 2000), pages 947?953,Saarbruecken, Germany.Zhao, Shaojun and Dekang Lin.
2004.
Anearest-neighbor method for resolvingPP-attachment ambiguities.
In The FirstInternational Joint Conference on NaturalLanguage Processing (IJCNLP-04),pages 545?554, Hainan Island, China.377
