Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1138?1147,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsTopic Models for Word Sense Disambiguation andToken-based Idiom DetectionLinlin Li, Benjamin Roth, and Caroline SporlederSaarland University, Postfach 15 11 5066041 Saarbru?cken, Germany{linlin, beroth, csporled}@coli.uni-saarland.deAbstractThis paper presents a probabilistic modelfor sense disambiguation which choosesthe best sense based on the conditionalprobability of sense paraphrases given acontext.
We use a topic model to decom-pose this conditional probability into twoconditional probabilities with latent vari-ables.
We propose three different instanti-ations of the model for solving sense dis-ambiguation problems with different de-grees of resource availability.
The pro-posed models are tested on three differenttasks: coarse-grained word sense disam-biguation, fine-grained word sense disam-biguation, and detection of literal vs. non-literal usages of potentially idiomatic ex-pressions.
In all three cases, we outper-form state-of-the-art systems either quan-titatively or statistically significantly.1 IntroductionWord sense disambiguation (WSD) is the task ofautomatically determining the correct sense for atarget word given the context in which it occurs.WSD is an important problem in NLP and an es-sential preprocessing step for many applications,including machine translation, question answeringand information extraction.
However, WSD is adifficult task, and despite the fact that it has beenthe focus of much research over the years, state-of-the-art systems are still often not good enoughfor real-world applications.
One major factor thatmakes WSD difficult is a relative lack of manu-ally annotated corpora, which hampers the perfor-mance of supervised systems.To address this problem, there has been asignificant amount of work on unsupervisedWSD that does not require manually sense-disambiguated training data (see McCarthy (2009)for an overview).
Recently, several researchershave experimented with topic models (Brody andLapata, 2009; Boyd-Graber et al, 2007; Boyd-Graber and Blei, 2007; Cai et al, 2007) for sensedisambiguation and induction.
Topic models aregenerative probabilistic models of text corpora inwhich each document is modelled as a mixtureover (latent) topics, which are in turn representedby a distribution over words.Previous approaches using topic models forsense disambiguation either embed topic featuresin a supervised model (Cai et al, 2007) or relyheavily on the structure of hierarchical lexiconssuch as WordNet (Boyd-Graber et al, 2007).
Inthis paper, we propose a novel framework whichis fairly resource-poor in that it requires only 1)a large unlabelled corpus from which to estimatethe topics distributions, and 2) paraphrases for thepossible target senses.
The paraphrases can beuser-supplied or can be taken from existing re-sources.We approach the sense disambiguation task bychoosing the best sense based on the conditionalprobability of sense paraphrases given a context.We propose three models which are suitable fordifferent situations: Model I requires knowledgeof the prior distribution over senses and directlymaximizes the conditional probability of a sensegiven the context; Model II maximizes this condi-tional probability by maximizing the cosine valueof two topic-document vectors (one for the senseand one for the context).
We apply these modelsto coarse- and fine-grained WSD and find that theyoutperform comparable systems for both tasks.We also test our framework on the related taskof idiom detection, which involves distinguishingliteral and nonliteral usages of potentially ambigu-ous expressions such as rock the boat.
For thistask, we propose a third model.
Model III cal-culates the probability of a sense given a contextaccording to the component words of the sense1138paraphrase.
Specifically, it chooses the sense typewhich maximizes the probability (given the con-text) of the paraphrase component word with thehighest likelihood of occurring in that context.This model also outperforms state-of-the-art sys-tems.2 Related WorkThere is a large body of work on WSD, cover-ing supervised, unsupervised (word sense induc-tion) and knowledge-based approaches (see Mc-Carthy (2009) for an overview).
While most su-pervised approaches treat the task as a classifica-tion task and use hand-labelled corpora as train-ing data, most unsupervised systems automaticallygroup word tokens into similar groups using clus-tering algorithms, and then assign labels to eachsense cluster.
Knowledge-based approaches ex-ploit information contained in existing resources.They can be combined with supervised machine-learning models to assemble semi-supervised ap-proaches.Recently, a number of systems have been pro-posed that make use of topic models for sensedisambiguation.
Cai et al (2007), for example,use LDA to capture global context.
They com-pute topic models from a large unlabelled corpusand include them as features in a supervised sys-tem.
Boyd-Graber and Blei (2007) propose an un-supervised approach that integrates McCarthy etal.
?s (2004) method for finding predominant wordsenses into a topic modelling framework.
In ad-dition to generating a topic from the document?stopic distribution and sampling a word from thattopic, the enhanced model also generates a distri-butional neighbour for the chosen word and thenassigns a sense based on the word, its neighbourand the topic.
Boyd-Graber and Blei (2007) testtheir method on WSD and information retrievaltasks and find that it can lead to modest improve-ments over state-of-the-art results.In another unsupervised system, Boyd-Graberet al (2007) enhance the basic LDA algorithm byincorporating WordNet senses as an additional la-tent variable.
Instead of generating words directlyfrom a topic, each topic is associated with a ran-dom walk through the WordNet hierarchy whichgenerates the observed word.
Topics and synsetsare then inferred together.
While Boyd-Graberet al (2007) show that this method can lead toimprovements in accuracy, they also find that id-iosyncracies in the hierarchical structure of Word-Net can harm performance.
This is a general prob-lem for methods which use hierarchical lexiconsto model semantic distance (Budanitsky and Hirst,2006).
In our approach, we circumvent this prob-lem by exploiting paraphrase information for thetarget senses rather than relying on the structureof WordNet as a whole.Topic models have also been applied to the re-lated task of word sense induction.
Brody andLapata (2009) propose a method that integrates anumber of different linguistic features into a singlegenerative model.Topic models have been previously consid-ered for metaphor extraction and estimating thefrequency of metaphors (Klebanov et al, 2009;Bethard et al, 2009).
However, we have a differ-ent focus in this paper, which aims to distinguishliteral and nonliteral usages of potential idiomaticexpressions.
A number of methods have been ap-plied to this task.
Katz and Giesbrecht (2006)devise a supervised method in which they com-pute the meaning vectors for the literal and non-literal usages of a given expression in the trainningdata.
Birke and Sarkar (2006) use a clustering al-gorithm which compares test instances to two au-tomatically constructed seed sets (one literal andone nonliteral), assigning the label of the closestset.
An unsupervised method that computes co-hesive links between the component words of thetarget expression and its context have been pro-posed (Sporleder and Li, 2009; Li and Sporleder,2009).
Their system predicts literal usages whenstrong links can be found.3 The Sense Disambiguation Model3.1 Topic ModelAs pointed out by Hofmann (1999), the startingpoint of topic models is to decompose the con-ditional word-document probability distributionp(w|d) into two different distributions: the word-topic distribution p(w|z), and the topic-documentdistribution p(z|d) (see Equation 1).
This allowseach semantic topic z to be represented as a multi-nominal distribution of words p(w|z), and eachdocument d to be represented as a multinominaldistribution of semantic topics p(z|d).
The modelintroduces a conditional independence assumptionthat document d and word w are independent con-1139ditioned on the hidden variable, topic z.p(w|d) =?zp(z|d)p(w|z) (1)LDA is a Bayesian version of this framework withDirichlet hyper-parameters (Blei et al, 2003).The inference of the two distributions given anobserved corpus can be done through Gibbs Sam-pling (Geman and Geman, 1987; Griffiths andSteyvers, 2004).
For each turn of the sampling,each word in each document is assigned a seman-tic topic based on the current word-topic distribu-tion and topic-document distribution.
The result-ing topic assignments are then used to re-estimatea new word-topic distribution and topic-documentdistribution for the next turn.
This process re-peats until convergence.
To avoid statistical co-incidence, the final estimation of the distributionsis made by the average of all the turns after con-vergence.3.2 The Sense Disambiguation ModelAssigning the correct sense s to a target word woccurring in a context c involves finding the sensewhich maximizes the conditional probability ofsenses given a context:s = argmaxsip(si|c) (2)In our model, we represent a sense (si) as a col-lection of ?paraphrases?
that capture (some aspectof) the meaning of the sense.
These paraphrasescan be taken from an existing resource such asWordNet (Miller, 1995) or supplied by the user(see Section 4).This conditional probability is decomposed byincorporating a hidden variable, topic z, intro-duced by the topic model.
We propose three varia-tions of the basic model, depending on how muchbackground information is available, i.e., knowl-edge of the prior sense distribution available andtype of sense paraphrases used.
In Model I andModel II, the sense paraphrases are obtained fromWordNet, and both the context and the sense para-phrases are treated as documents, c = dc ands = ds.WordNet is a fairly rich resource which pro-vides detailed information about word senses(glosses, example sentences, synsets, semantic re-lations between senses, etc.).
Sometimes such de-tailed information may not be available, for in-stance for languages for which such a resourcedoes not exist or for expressions that are notvery well covered in WordNet, such as idioms.For those situations, we propose another model,Model III, in which contexts are treated as docu-ments while sense paraphrases are treated as se-quences of independent words.1Model I directly maximizes the conditionalprobability of the sense given the context, wherethe sense is modeled as a ?paraphrase document?ds and the context as a ?context document?
dc.The conditional probability of sense given contextp(ds|dc) can be rewritten as a joint probability di-vided by a normalization factor:p(ds|dc) =p(ds, dc)p(dc)(3)This joint probability can be rewritten as a gen-erative process by introducing a hidden variable z.We make the conditional independence assump-tion that, conditioned on the topic z, a paraphrasedocument ds is generated independently of thespecific context document dc:p(ds, dc) =?zp(ds)p(z|ds)p(dc|z) (4)We apply the same process to the conditionalprobability p(dc|z).
It can be rewritten as:p(dc|z) =p(dc)p(z|dc)p(z)(5)Now, the disambiguation model p(ds|dc) can berewritten as a prior p(ds) times a topic functionf(z):p(ds|dc) = p(ds)?zp(z|dc)p(z|ds)p(z)(6)As p(z) is a uniform distribution according tothe uniform Dirichlet priors assumption, Equation6 can be rewritten as:p(ds|dc) ?
p(ds)?zp(z|dc)p(z|ds) (7)Model I:argmaxdsip(dsi)?zp(z|dc)p(z|dsi) (8)Model I has the disadvantage that it requiresinformation about the prior distribution of senses1The idea is that these key words capture the meaning ofthe idioms.1140p(ds), which is not always available.
We use sensefrequency information from WordNet to estimatethe prior sense distribution, although it must bekept in mind that, depending on the genre of thetexts, it is possible that the distribution of sensesin the testing corpus may diverge greatly from theWordNet-based estimation.
If there is no meansfor estimating the prior sense distribution of anexperimental corpus, generally a uniform distri-bution must be assumed.
However, this assump-tion does not hold, as the true distribution of wordsenses is often highly skewed (McCarthy, 2009).To overcome this problem, we propose ModelII, which indirectly maximizes the sense-contextprobability by maximizing the cosine value of twodocument vectors that encode the document-topicfrequencies from sampling, v(z|dc) and v(z|ds).The document vectors are represented by topics,with each dimension representing the number oftimes that the tokens in this document are assignedto a certain topic.Model II:argmaxdsicos(v(z|dc), v(z|dsi)) (9)If the prior distribution of senses is known, ModelI is the best choice.
However, Model II has to bechosen instead when this knowledge is not avail-able.
In our experiments, we test the performanceof both models (see Section 5).If the sense paraphrases are very short, it is diffi-cult to reliably estimate p(z|ds).
In order to solvethis problem, we treat the sense paraphrase ds asa ?query?, a concept which is used in informationretrieval.
One model from information retrievaltakes the conditional probability of the query giventhe document as a product of all the conditionalprobabilities of words in the query given the doc-ument.
The assumption is that the query is gener-ated by a collection of conditionally independentwords (Song and Croft, 1999).We make the same assumption here.
How-ever, instead of taking the product of all the condi-tional probabilities of words given the document,we take the maximum.
There are two reasons forthis: (i) taking the product may penalize longerparaphrases since the product of probabilities de-creases as there are more words; (ii) we do notwant to model the probability of generating spe-cific paraphrases, but rather the probability of gen-erating a sense, which might only be representedby one or two words in the paraphrases (e.g., thepotentially idiomatic phrase ?rock the boat?
can beparaphrased as ?break the norm?
or ?cause trou-ble?.
A similar topic distribution to that of theindividual words ?norm?
or ?trouble?
would bestrong supporting evidence of the correspondingidiomatic reading.).
We propose Model III:argmaxqsimaxwi?qs?zp(wi|z)p(z|dc) (10)where qs is a collection of words contained in thesense paraphrases.3.3 InferenceOne possible inference approach is to combine thecontext documents and sense paraphrases into acorpus and run Gibbs sampling on this corpus.The problem with this approach is that the test setand sense paraphrase set are relatively small, andtopic models running on a small corpus are lesslikely to capture rich semantic topics.
One sim-ple explanation for this is that a small corpus usu-ally has a relatively small vocabulary, which is lessrepresentative of topics, i.e., p(w|z) cannot be es-timated reliably.In order to overcome this problem, we infer theword-topic distribution from a very large corpus(Wikipedia dump, see Section 4).
All the follow-ing inference experiments on the test corpus arebased on the assumption that the word-topic dis-tribution p(w|z) is the same as the one estimatedfrom the Wikipedia dump.
Inference of topic-document distributions for context and sense para-phrases is done by fixing the word-topic distribu-tion as a constant.4 Experimental SetupWe evaluate our models on three different tasks:coarse-grained WSD, fine-grained WSD and lit-eral vs. nonliteral sense detection.
In this sectionwe discuss our experimental set-up.
We start bydescribing the three datasets for evaluation and an-other dataset for probability estimation.
We alsodiscuss how we choose sense paraphrases and in-stance contexts.Data We use three datasets for evaluation.
Thecoarse-grained task is evaluated on the Semeval-2007 Task-07 benchmark dataset released by Nav-igli et al (2009).
The dataset consists of 5377words of running text from five different articles:the first three were obtained from the WSJ cor-pus, the fourth was the Wikipedia entry for com-puter programming, and the fifth was an excerpt of1141Amy Steedman?s Knights of the Art, biographiesof Italian painters.
The proportion of the non newstext, the last two articles, constitutes 51.87% of thewhole testing set.
It consists of 1108 nouns, 591verbs, 362 adjectives, and 208 adverbs.
The datawere annotated with coarse-grained senses whichwere obtained by clustering senses from the Word-Net 2.1 sense inventory based on the procedureproposed by Navigli (2006).To determine whether our model is also suitablefor fine-grained WSD, we test on the data providedby Pradhan et al (2009) for the Semeval-2007Task-17 (English fine-grained all-words task).This dataset is a subset of the set from Task-07.
Itcomprises the three WSJ articles from Navigli etal.
(2009).
A total of 465 lemmas were selected asinstances from about 3500 words of text.
There are10 instances marked as ?U?
(undecided sense tag).Of the remaining 455 instances, 159 are nouns and296 are verbs.
The sense inventory is from Word-Net 2.1.Finally, we test our model on the related sensedisambiguation task of distinguishing literal andnonliteral usages of potentially ambiguous expres-sions such as break the ice.
For this, we use thedataset from Sporleder and Li (2009) as a test set.This dataset consists of 3964 instances of 17 po-tential English idioms which were manually anno-tated as literal or nonliteral.A Wikipedia dump2 is used to estimate themultinomial word-topic distribution.
This dataset,which consists of 320,000 articles,3 is significantlylarger than SemCor, which is the dataset used byBoyd-Graber et al (2007).
All markup from theWikipedia dump was stripped off using the samefilter as the ESA implementation (Sorg and Cimi-ano, 2008), and stopwords were filtered out usingthe Snowball (Porter, October 2001) stopword list.In addition, words with a Wikipedia document fre-quency of 1 were filtered out.
The lemmatizedversion of the corpus consists of 299,825 lexicalunits.The test sets were POS-tagged and lemmatizedusing RASP (Briscoe and Carroll, 2006).
The in-ference processes are run on the lemmatized ver-sion of the corpus.
For the Semeval-2007 Task 17English all-words, the organizers do not supply thepart-of-speech and lemma information of the tar-get instances.
In order to avoid the wrong predic-2We use the English snapshot of 2009-07-133All articles of fewer than 100 words were discarded.tions caused by tagging or lemmatization errors,we manually corrected any bad tags and lemmasfor the target instances.4Sense Paraphrases For word sense disam-biguation tasks, the paraphrases of the sense keysare represented by information from WordNet 2.1.
(Miller, 1995).
To obtain the paraphrases, we usethe word forms, glosses and example sentencesof the synset itself and a set of selected referencesynsets (i.e., synsets linked to the target synset byspecific semantic relations, see Table 1).
We ex-cluded the ?hypernym reference synsets?, since in-formation common to all of the child synsets mayconfuse the disambiguation process.For the literal vs. nonliteral sense detectiontask, we selected the paraphrases of the nonlit-eral meaning from several online idiom dictionar-ies.
For the literal senses, we used 2-3 manu-ally selected words with which we tried to cap-ture (aspects of) the literal meaning of the expres-sion.5 For instance, the literal ?paraphrases?
thatwe chose for ?break the ice?
were ice, water andsnow.
The paraphrases are shorter for the idiomtask than for the WSD task, because the mean-ing descriptions from the idiom dictionaries areshorter than what we get from WordNet.
In thelatter case, each sense can be represented by itssynset as well as its reference synsets.Instance Context We experimented with differ-ent context sizes for the disambiguation task.
Thefive different context settings that we used for theWSD tasks are: collocations (1w), ?5-word win-dow (5w), ?10-word window (10w), current sen-tence, and whole text.
Because the idiom corpusalso includes explicitly marked paragraph bound-aries, we included ?paragraph?
as a sixth type ofcontext size for the idiom sense detection task.5 ExperimentsAs mentioned above, we test our proposed sensedisambiguation framework on three tasks.
Westart by describing the sampling experiments for4This was done by comparing the predicted sense keysand the gold standard sense keys.
We only checked instancesfor which the POS-tags in the predicted sense keys are notconsistent with those in the gold standard.
This was the casefor around 20 instances.5Note that we use the word ?paraphrase?
in a fairly widesense in this paper.
Sometimes it is not possible to obtain ex-act paraphrases.
This applies especially to the task of distin-guishing literal from nonliteral senses of multi-word expres-sions.
In this case we take as paraphrases some key wordswhich capture salient aspects of the meaning.1142POS Paraphrase reference synsetsN hyponyms, instance hyponyms, member holonyms, substance holonyms, part holonyms,member meronyms, part meronyms, substance meronyms, attributes, topic members,region members, usage members, topics, regions, usagesV Troponyms, entailments, outcomes, phrases, verb groups, topics, regions, usages, sentence framesA similar, pertainym, attributes, related, topics, regions, usagesR pertainyms, topics, regions, usagesTable 1: Selected reference synsets from WordNet that were used for different parts-of-speech to obtainword sense paraphrase.
N(noun), V(verb), A(adj), R(adv).estimating the word-topic distribution from theWikipedia dump.
We used the package providedby Wang et al (2009) with the suggested Dirich-let hyper-parameters 6.
In order to avoid statisticalinstability, the final result is averaged over the last50 iterations.
We did four rounds of sampling with1000, 500, 250, and 125 topics respectively.
Thefinal word-topic distribution is a normalized con-catenate of the four distributions estimated in eachround.
In average, the sampling program run onthe Wikipedia dump consumed 20G memory, andeach round took about one week on a single AMDDual-Core 1000MHZ processor.5.1 Coarse-Grained WSDIn this section we first describe the landscape ofsimilar systems against which we compare ourmodels, then present the results of the comparison.The systems that participated in the SemEval-2007coarse-grained WSD task (Task-07) can be di-vided into three categories, depending on whethertraining data is needed and whether other typesof background knowledge are required: What wecall Type I includes all the systems that need an-notated training data.
All the participating sys-tems that have the mark TR fall into this cate-gory (see Navigli et al (2009) for the evaluationfor all the participating systems).
Type II con-sists of systems that do not need training data butrequire prior knowledge of the sense distribution(estimated sense frequency).
All the participatingsystems that have the mark MFS belong to this cat-egory.
Systems that need neither training data norprior sense distribution knowledge are categorizedas Type III.We make this distinction based on two princi-ples: (i) the cost of building a system; (ii) theportability of the established resource.
Type IIIis the cheapest system to build, while Type I and6They were set as: ?
= 50#topics and ?
= 0.01.Type II both need extra resources.
Type II hasan advantage over Type I since the prior knowl-edge of the sense distribution can be estimatedfrom annotated corpora (e.g.
: SemCor, Senseval).In contrast, training data in Type I may be sys-tem specific (e.g.
: different input format, differentannotation guidelines).
McCarthy (2009) also ad-dresses the issue of performance and cost by com-paring supervised word sense disambiguation sys-tems with unsupervised ones.We exclude the system provided by one ofthe organizers (UoR-SSI) from our categorization.The reason is that although this system is claimedto be unsupervised, and it performs better thanall the participating systems (including the super-vised systems) in the SemEval-2007 shared task, itstill needs to incorporate a lot of prior knowledge,specifically information about co-occurrences be-tween different word senses, which was obtainedfrom a number of resources (SSI+LKB) includ-ing: (i) SemCor (manually annotated); (ii) LDC-DSO (partly manually annotated); (iii) collocationdictionaries which are then disambiguated semi-automatically.
Even though the system is not?trained?, it needs a lot of information which islargely dependent on manually annotated data, soit does not fit neatly into the categories Type II orType III either.Table 2 lists the best participating systems ofeach type in the SemEval-2007 task (Type I:NUS-PT (Chan et al, 2007); Type II: UPV-WSD(Buscaldi and Rosso, 2007); Type III: TKB-UO(Anaya-Sa?nchez et al, 2007)).
Our Model I be-longs to Type II, and our Model II belongs to TypeIII.Table 2 compares the performance of our mod-els with the Semeval-2007 participating systems.We only compare the F-score, since all the com-pared systems have an attempted rate7 of 1.0,7Attempted rate is defined as the total number of disam-biguated output instances divided by the total number of input1143which makes both the precision and recall rates thesame as the F-score.
We focus on comparisons be-tween our models and the best SemEval-2007 par-ticipating systems within the same type.
Model I iscompared with UPV-WSD, and Model II is com-pared with TKB-UO.
In addition, we also compareour system with the most frequent sense baselinewhich was not outperformed by any of the systemsof Type II and Type III in the SemEval-2007 task.Comparison on Type III is marked with ?, whilecomparison on Type II is marked with ?.
We findthat Model II performs statistically significantlybetter than the best participating system of thesame type TKB-UO (p<<0.01, ?2 test).
Whenencoded with the prior knowledge of sense distri-bution, Model I outperforms by 1.36% the bestType II system UPV-WSD, although the differ-ence is not statistically significant.
Furthermore,Model I also quantitatively outperforms the mostfrequent sense baseline BLmfs, which, as men-tioned above, was not beat by any participatingsystems that do not use training data.We also find that our model works best fornouns.
The unsupervised Type III model ModelII achieves better results than the most frequentsense baseline on nouns, but not on other parts-of-speech.
This is in line with results obtainedby previous systems (Griffiths et al, 2005; Boyd-Graber and Blei, 2008; Cai et al, 2007).
While theperformance on verbs can be increased to outper-form the most frequent sense baseline by includingthe prior sense probability, the performance on ad-jectives and adverbs remains below the most fre-quent sense baseline.
We think that there are threereasons for this: first, adjectives and adverbs havefewer reference synsets for paraphrases comparedwith nouns and verbs (see Table 1); second, adjec-tives and adverbs tend to convey less key semanticcontent in the document, so they are more difficultto capture by the topic model; and third, adjectivesand adverbs are a small portion of the test set, sotheir performances are statistically unstable.
Forexample, if ?already?
appears 10 times out of 20adverb instances, a system may get bad result onadverbs only because of its failure to disambiguatethe word ?already?.Paraphrase analysis Table 2 also shows theeffect of different ways of choosing sense para-phrases.
MII+ref is the result of including the ref-erence synsets, while MII-ref excludes the refer-instances.System Noun Verb Adj Adv AllUoR-SSI 84.12 78.34 85.36 88.46 83.21NUS-PT 82.31 78.51 85.64 89.42 82.50UPV-WSD 79.33 72.76 84.53 81.52 78.63?TKB-UO 70.76 62.61 78.73 74.04 70.21?MII?ref 78.16 70.39 79.56 81.25 76.64MII+ref 80.05 70.73 82.04 82.21 78.14?MI+ref 79.96 75.47 83.98 86.06 79.99?BLmfs 77.44 75.30 84.25 87.50 78.99?Table 2: Model performance (F-score) on thecoarse-grained dataset (context=sentence).
Para-phrases with/without reference synsets (+ref/-ref).Context Ate.
Pre.
Rec.
F1?1w 91.67 75.05 68.80 71.79?5w 99.29 77.14 76.60 76.87?10w 100 77.92 77.92 77.92text 100 76.86 76.86 76.86sent.
100 78.14 78.14 78.14Table 3: Model II performance on different con-text size.
attempted rate (Ate.
), precision (Pre.
),recall (Rec.
), F-score (F1).ence synsets.
As can be seen from the table, in-cluding all reference synsets in sense paraphrasesincreases performance.
Longer paraphrases con-tain more information, and they are statisticallymore stable for inference.We find that nouns get the greatest perfor-mance boost from including reference synsets, asthey have the largest number of different types ofsynsets.
We also find the ?similar?
reference synsetfor adjectives to be very useful.
Performance onadjectives increases by 2.75% when including thisreference synset.Context analysis In order to study how the con-text influences the performance, we experimentwith Model II on different context sizes (see Ta-ble 3).
We find sentence context is the best size forthis disambiguation task.
Using a smaller contextnot only reduces the precision, but also reduces therecall rate, which is caused by the all-zero topic as-signment by the topic model for documents onlycontaining words that are not in the vocabulary.As a result, the model is unable to disambiguate.The context based on the whole text (article) doesnot perform well either, possibly because using thefull text folds in too much noisy information.1144System F-scoreRACAI 52.7 ?4.5BLmfs 55.91?4.5MI+ref 56.99?4.5Table 4: Model performance (F-score) for the fine-grained word sense disambiguation task.5.2 Fine-grained WSDWe saw in the previous section that our frame-work performs well on coarse-grained WSD.
Fine-grained WSD, however, is a more difficult task.
Todetermine whether our framework is also able todetect subtler sense distinctions, we tested Model Ion the English all-words subtask of SemEval-2007Task-17 (see Table 4).We find that Model I performs better than boththe best unsupervised system, RACAI (Ion andTufis?, 2007) and the most frequent sense baseline(BLmfs), although these differences are not sta-tistically significant due to the small size of theavailable test data (465).5.3 Idiom Sense DisambiguationIn the previous section, we provided the resultsof applying our framework to coarse- and fine-grained word sense disambiguation tasks.
Forboth tasks, our models outperform the state-of-the-art systems of the same type either quantita-tively or statistically significantly.
In this section,we apply Model III to another sense disambigua-tion task, namely distinguishing literal and nonlit-eral senses of ambiguous expressions.WordNet has a relatively low coverage for id-iomatic expressions.
In order to represent non-literal senses, we replace the paraphrases obtainedautomatically from WordNet by words selectedmanually from online idiom dictionaries (for thenonliteral sense) and by linguistic introspection(for the literal sense).
We then compare the topicdistributions of literal and nonliteral senses.As the paraphrases obtained from the idiom dic-tionary are very short, we treat the paraphraseas a sequence of independent words instead ofas a document and apply Model III (see Sec-tion 3).
Table 5 shows the results of our pro-posed model compared with state-of-the-art sys-tems.
We find that the system significantly out-performs the majority baseline (p<<0.01, ?2 test)and the cohesion-graph based approach proposedby Sporleder and Li (2009) (p<<0.01, ?2 test).The system also outperforms the bootstrappingSystem Precl Recl Fl Acc.Basemaj - - - 78.25co-graph 50.04 69.72 58.26 78.38boot.
71.86 66.36 69.00 87.03Model III 67.05 81.07 73.40 87.24Table 5: Performance on the literal or nonliteralsense disambiguation task on idioms.
literal pre-cision (Precl), literal recall (Recl), literal F-score(Fl), accuracy(Acc.
).system by Li and Sporleder (2009), although notstatistically significantly.
This shows how a lim-ited amount of human knowledge (e.g., para-phrases) can be added to an unsupervised systemfor a strong boost in performance ( Model III com-pared with the cohesion-graph and the bootstrap-ping approaches).For obvious reasons, this approach is sensitiveto the quality of the paraphrases.
The paraphraseschosen to characterise (aspects of) the meaning ofa sense should be non-ambiguous between the lit-eral or idiomatic meaning.
For instance, ?fire?
isnot a good choice for a paraphrase of the literalreading of ?play with fire?, since this word canbe interpreted literally as ?fire?
or metaphoricallyas ?something dangerous?.
The verb componentword ?play?
is a better literal paraphrase.For the same reason, this approach works wellfor expressions where the literal and nonliteralreadings are well separated (i.e., occur in differentcontexts), while the performance drops for expres-sions whose literal and idiomatic readings can ap-pear in a similar context.
We test the performanceon individual idioms on the five most frequent id-ioms in our corpus8 (see Table 6).
We find that?drop the ball?
is a difficult case.
The words ?fault?,?mistake?, ?fail?
or ?miss?
can be used as the nonlit-eral paraphrases.
However, it is also highly likelythat these words are used to describe a scenario ina baseball game, in which ?drop the ball?
is usedliterally.
In contrast, the performance on ?rock theboat?
is much better, since the nonliteral readingof the phrases ?break the norm?
or ?cause trouble?are less likely to be linked with the literal reading?boat?.
This may also be because ?boat?
is not of-ten used metaphorically in the corpus.As the topic distribution of nouns and verbsexhibit different properties, topic comparisonsacross parts-of-speech do not make sense.
We8We tested only on the most frequent idioms in order toavoid statistically unreliable observations.1145Idiom Acc.drop the ball 75.86play with fire 91.17break the ice 87.43rock the boat 95.82set in stone 89.39Table 6: Performance on individual idioms.make the topic distributions comparable by mak-ing sure each type of paraphrase contains the samesets of parts-of-speech.
For instance, we do notpermit combinations of literal paraphrases whichonly consist of nouns and nonliteral paraphraseswhich only consist of verbs.6 ConclusionWe propose three models for sense disambigua-tion on words and multi-word expressions.
Thebasic idea of these models is to compare the topicdistribution of a target instance with the candidatesense paraphrases and choose the most probableone.
While Model I and Model III model theproblem in a probabilistic way, Model II uses avector space model by comparing the cosine val-ues of two topic vectors.
Model II and Model IIIare completely unsupervised, while Model I needsthe prior sense distribution.
Model I and ModelII treat the sense paraphrases as documents, whileModel III treats the sense paraphrases as a collec-tion of independent words.We test the proposed models on three tasks.
Weapply Model I and Model II to the WSD tasks dueto the availability of more paraphrase information.Model III is applied to the idiom detection tasksince the paraphrases from the idiom dictionaryare smaller.
We find that all models outperformcomparable state-of-the-art systems either quanti-tatively or statistically significantly.By testing our framework on three differentsense disambiguation tasks, we show that theframework can be used flexibly in different ap-plication tasks.
The system also points out apromising way of solving the granularity problemof word sense disambiguation, as new applicationtasks which need different sense granularities canutilize this framework when new paraphrases ofsense clusters are available.
In addition, this sys-tem can also be used in a larger context such asfigurative language identification (literal or figu-rative) and sentiment detection (positive or nega-tive).AcknowledgmentsThis work was funded by the DFG within theCluster of Excellence ?Multimodal Computingand Interaction?.ReferencesH.
Anaya-Sa?nchez, A. Pons-Porrata, R. Berlanga-Llavori.
2007.
TKB-UO: using sense clustering forWSD.
In SemEval ?07: Proceedings of the 4th Inter-national Workshop on Semantic Evaluations, 322?325.S.
Bethard, V. T. Lai, J. H. Martin.
2009.
Topic modelanalysis of metaphor frequency for psycholinguisticstimuli.
In CALC ?09: Proceedings of the Workshopon Computational Approaches to Linguistic Creativ-ity, 9?16, Morristown, NJ, USA.
Association forComputational Linguistics.J.
Birke, A. Sarkar.
2006.
A clustering approachfor the nearly unsupervised recognition of nonliterallanguage.
In Proceedings of EACL-06.D.
M. Blei, A. Y. Ng, M. I. Jordan.
2003.
Latentdirichlet alocation.
Journal of Machine LearningReseach, 3:993?1022.J.
Boyd-Graber, D. Blei.
2007.
PUTOP: turningpredominant senses into a topic model for wordsense disambiguation.
In Proceedings of the FourthInternational Workshop on Semantic Evaluations(SemEval-2007), 277?281.J.
Boyd-Graber, D. Blei.
2008.
Syntactic topic models.Computational Linguistics.J.
Boyd-Graber, D. Blei, X. Zhu.
2007.
A topicmodel for word sense disambiguation.
In Proceed-ings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), 1024?1033.T.
Briscoe, J. Carroll.
2006.
Evaluating the accuracyof an unlexicalized statistical parser on the PARCDepBank.
In Proceedings of the COLING/ACL onMain conference poster sessions, 41?48.S.
Brody, M. Lapata.
2009.
Bayesian word senseinduction.
In Proceedings of the 12th Conferenceof the European Chapter of the ACL (EACL 2009),103?111.A.
Budanitsky, G. Hirst.
2006.
Evaluating wordnet-based measures of lexical semantic relatedness.Computational Linguistics, 32(1):13?47.D.
Buscaldi, P. Rosso.
2007.
UPV-WSD: Combiningdifferent WSD methods by means of Fuzzy BordaVoting.
In SemEval ?07: Proceedings of the 4thInternational Workshop on Semantic Evaluations,434?437.J.
Cai, W. S. Lee, Y. W. Teh.
2007.
Improving wordsense disambiguation using topic features.
In Pro-ceedings of the 2007 Joint Conference on Empirical1146Methods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), 1015?1023.Y.
S. Chan, H. T. Ng, Z. Zhong.
2007.
NUS-PT: ex-ploiting parallel texts for word sense disambiguationin the English all-words tasks.
In SemEval ?07: Pro-ceedings of the 4th International Workshop on Se-mantic Evaluations, 253?256.S.
Geman, D. Geman.
1987.
Stochastic relaxation,Gibbs distributions, and the Bayesian restorationof images.
In Readings in computer vision: is-sues, problems, principles, and paradigms, 564?584.
Morgan Kaufmann Publishers Inc., San Fran-cisco, CA, USA.T.
L. Griffiths, M. Steyvers.
2004.
Finding scientifictopics.
Proceedings of the National Academy of Sci-ences, 101(Suppl.
1):5228?5235.T.
L. Griffiths, M. Steyvers, D. M. Blei, J.
B. Tenen-baum.
2005.
Integrating topics and syntax.
In InAdvances in Neural Information Processing Systems17, 537?544.
MIT Press.T.
Hofmann.
1999.
Probabilistic latent semantic in-dexing.
In SIGIR ?99: Proceedings of the 22nd an-nual international ACM SIGIR conference on Re-search and development in information retrieval,50?57.R.
Ion, D. Tufis?.
2007.
Racai: meaning affinity mod-els.
In SemEval ?07: Proceedings of the 4th Inter-national Workshop on Semantic Evaluations, 282?287, Morristown, NJ, USA.
Association for Compu-tational Linguistics.G.
Katz, E. Giesbrecht.
2006.
Automatic identifi-cation of non-compositional multi-word expressionsusing latent semantic analysis.
In Proceedings of theACL/COLING-06 Workshop on Multiword Expres-sions: Identifying and Exploiting Underlying Prop-erties.B.
B. Klebanov, E. Beigman, D. Diermeier.
2009.
Dis-course topics and metaphors.
In CALC ?09: Pro-ceedings of the Workshop on Computational Ap-proaches to Linguistic Creativity, 1?8, Morristown,NJ, USA.
Association for Computational Linguis-tics.L.
Li, C. Sporleder.
2009.
Contextual idiom detectionwithout labelled data.
In Proceedings of EMNLP-09.D.
McCarthy, R. Koeling, J. Weeds, J. Carroll.
2004.Finding predominant word senses in untagged text.In Proceedings of the 42nd Meeting of the Associa-tion for Computational Linguistics (ACL?04), MainVolume, 279?286.D.
McCarthy.
2009.
Word sense disambiguation:An overview.
Language and Linguistics Compass,3(2):537?558.G.
A. Miller.
1995.
WordNet: a lexical database forenglish.
Commun.
ACM, 38(11):39?41.R.
Navigli, K. C. Litkowski, O. Hargraves.
2009.SemEval-2007 Task 07: Coarse-grained English all-words task.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluation (SemEval-2007).R.
Navigli.
2006.
Meaningful clustering of senseshelps boost word sense disambiguation perfor-mance.
In Proceedings of the 44th Annual Meetingof the Association for Computational Liguistics jointwith the 21st International Conference on Computa-tional Liguistics (COLING-ACL 2006).M.
Porter.
October 2001.
Snowball: A lan-guage for stemming algorithms.
http://snowball.tartarus.org/texts/introduction.html.S.
S. Pradhan, E. Loper, D. Dligach, M. Palmer.
2009.SemEval-2007 Task 07: Coarse-grained English all-words task.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluation (SemEval-2007).F.
Song, W. B. Croft.
1999.
A general language modelfor information retrieval (poster abstract).
In Re-search and Development in Information Retrieval,279?280.P.
Sorg, P. Cimiano.
2008.
Cross-lingual informationretrieval with explicit semantic analysis.
In In Work-ing Notes for the CLEF 2008 Workshop.C.
Sporleder, L. Li.
2009.
Unsupervised recognition ofliteral and non-literal use of idiomatic expressions.In Proceedings of EACL-09.Y.
Wang, H. Bai, M. Stanton, W.-Y.
Chen, E. Y. Chang.2009.
Plda: Parallel latent dirichlet alocation forlarge-scale applications.
In Proc.
of 5th Interna-tional Conference on Algorithmic Aspects in Infor-mation and Management.
Software available athttp://code.google.com/p/plda.1147
