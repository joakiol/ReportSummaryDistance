Parsing spoken language without syntaxJean-Yves AntoineCLIPS- IMAGBP 53 - -  F-38040 GRENOBLE Cedex 9, FRANCEJean-Yves Antoine@imag.frAbstractParsing spontaneous speech is a difficult taskbecause of the ungrammatical nature of mostspoken utterances.
To overpass this problem, wepropose in this paper to handle the spokenlanguage without considering syntax.
Wedescribe thus a microsemantic parser which isuniquely based on an associative network ofsemantic priming.
Experimental results onspontaneous speech show that this parser standsfor a robust alternative to standard ones.i.
IntroductionThe need of a robust parsing of spontaneousspeech is a more and more essential as spokenhuman - machine communication meets a reallyimpressive development.
Now, the extremestructural variability of the spoken languagebalks seriously the attainment of such anobjective.
Because of its dynamic anduncontrolled nature, spontaneous speechpresents indeed a high rate of ungrammaticalconstructions (hesitations, repetitious, a.s.o...).As a result, spontaneous speech catch rapidly outmost syntactic parsers, in spite of the frequentaddition of some ad hoc corrective methods\[Seneff 92\].
Most speech systems excludetherefore a complete syntactic parsing of thesentence.
They on the contrary restrict theanalysis to a simple keywords extraction \[Appelt92\].
This selective approach led to significantresults in some restricted applications (ATIS...).It seems however unlikely that it is appropriatefor higher level tasks, which involve a morecomplex communication between the user andthe computer.Thus, neither the syntactic methods nor theselective approaches can fully satisfy theconstraints of robustness and of exhaustivityspoken human-machine communication eeds.This paper presents a detailed semantic parserwhich masters most spoken utterances.
In a firstpart, we describe the semantic knowledge ourparser relies on.
We then detail itsimplementation.
Experimental results, whichsuggest he suitability of this model, are finallyprovided.2.
MicrosemanticsMost syntactic formalisms (LFG \[Bresnan 82\],HPSG \]Pollard 87\], TAG \[Joshi 87\]) give amajor importance to subcategorization, whichaccounts for the grammatical dependenciesinside the sentence.
We consider on the contrarythat subcategorization issue from a lexicalsemantic knowledge we will further namemicrosemantics \[Rastier 94\].
(to select)77 (dcv ice) .~~(the) ?Figure 1: Microsemantic structure of thesentence I select he left deviceOur parser aims thus at building a microsemanticstructure (figure 1) which fully describes themeaning dependencies inside the sentence.
Thecorresponding relations are labeled by severalmicrosemantic cases (Table 1) which only intendto cover the system's application field(computer-helped drawing).The microsemantic parser achieves a fullylexicalized analysis.
It relies indeed on amicrosemantic lexicon in which every inputrepresents a peculiar lexeme I.
Each lexeme isdescribed by the following features tructure :PRED lexeme identifierMORPI1 morphological realizationsSEM semantic domainSUBCAT subcategorization frameI Lexeme = lexical unit of meaning.47Exam )le : to drawPr  ed = 'to draw'Morph = {' draw', '  draws' , '  drew', '  drawn'}I AGT = / e lement / + / animate /Subcat = /oBJ = / element / + / concrete /\[( LOC)  = / property / + / placeSere = / task - domain /The microsemantic subcategorization framesdescribe the meaning dependencies the lexemedominate.
Their arguments are not ordered.
Theoptional arguments are in brackets, byopposition with the compulsory ones.
At last, theadverbial phrases are not subcategorized.Table 1 : Some examples of microsemantic cases.Label Semantic aseDET determinerAGT agentATT attributeOBJ object / themeLOC location / destinationOWN meronomy / ownershipMOD modalityINS instrumentCOO coordinationTAG case marker (prdposition)REF anaphoric reference3.
Semant ic  P r imingAny speech recognition system involves a highperplexity which requires the definition of top-down parsing constraints.
This is why we basedthe microsemantic parsing on a priming process.3.1.
Priming processThe semantic priming is a predictive processwhere some already uttered words (primingwords) are calling some other ones (primedwords) through various meaning associations.
Itaims a double goal :?
It constrains the speech recognition.?
It characterizes the meaning dependenciesinside the sentence.Each priming step involves two successiveprocesses.
At first, the contextual adaptationfavors the priming words which are consistentwith the semantic context.
The latter is roughlymodeled by two semantic fields: the task domainand the computing domain.
On the other hand,the relational priming identifies the lexemeswhich share a microsemantic relation with oneof the already uttered words.
These relationsissue directly from the subcategorization framesof these priming words.3.2.
Priming networkThe priming process is carried out by anassociative multi-layered network (figure 2)which results from the compilation of thelexicon.
Each cell of the network corresponds toa specific lexeme.
The inputs represent thepriming words.
Their activities are propagatedup to the output layer which corresponds to theprimed words.
An additional layer (Structurallayer S) handles furthermore the coordinationsand the prepositions.We will now describe the propagation of thepriming activities.
Let us consider :?
t current step of analysis?
a;/(t) activity of the cell j of the layer i atstept (i e {1, 2, 3, 4, 5, 6, S} )?
~J(t) synaptic weight between the cell k ofthe layer i and the cell I of the layer j.Tempora l  forgett ing - -  At first, the inputactivities are slightly modulated by a process oftemporal forgetting :ail(t) =amax if i is to the current word.ail(t) = amax if i is to the primer of thisword.a~l(t) = Max (0, ail(t- 1)- Afo, g~t ) otherwise.Although it favors the most recent lexemes, thisprocess does not prevent long distance primings.Contextual adaptat ion - -  Each cell of thesecond layer represents a peculiar semantic field.Its activity depends on the semantic affiliationsof the priming words :a~ (t)= Eoli,:~(t).air (t) (1)iC0il',~(t) = COma x if i belongs to j. with:c01j:~(t) = -Olma x otherwise.Then, these contextual cells modulate the initialpriming activities :( t )= ai i (t) + y__.
i i i m2,3(t) .a 2 (t)iwith: coi2'i (t) = Aoo,~,ex, if j belongs to i.coi2'i (t) = - Aco,,ext otherwise.The priming words which are consistent with thecurrent semantic ontext are therefore favored.48Re la t iona l  P r iming  - -  The priming activitiesare then dispatched mnong several sub-networkswhich perform parallel analyses on distinct cases(fig.
3).
The dispatched activities representstherefore the priming power of the priminglexemes o51 each microselnantic case :? '
(t)= y o+~:+(t) <(t) " ( t ) .
; ( t )  d4t  z , = (0:L+ t .JThe dispatching weights are dynamicallyadapted uring the parsing (see section 4)+ Theirinitial values issue front the compilation ol thelexical subcategorization flames :ml4~J,,5,(t) : C?min otherwise.The outputs of the case-based sub-networks, aswell as the final priming excitations, are thencalculated through a maximum heuristic :a{,,Ct) = Max ( i,j ro .... (t).
a{.
(t ) )  (4)(3) ia>) : (+,',<t))The lexical units are t'inally sorted in threecoarse sets :ai,(t) > +\[i~,gh primed wordsprimingwords(inputs)i,j(03 ,4n  t (0 )  ~--- 0(O  3,4i'i (0 ) :  ~ lWtXi,i m3,4(0) = 8,,,+,,i i .
().m3',4a( ) = 0coordination prepositionsfi< .,,,.,,aa,..,,,y,,,.
Em.d;.+ll+ contextual ++ L~._+# + .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
........................... ++,;~.#7:~-+++.,~+/ layer '.:".
: .
- .
({(~~\]'~-M>~7~ ' " ' , ' ' ' '  .... ....... ,,, ,," ,, " .
.- - } , ' + ,, ' ,i7- - - !!
'~i+ } +2; - -  -~t?lgettlnl2case-based networks(1) (2) (33 ?It  I I  ~1 .
.
.
.
.
spccif i  catitm focusing d ispatch ing prmung \] / col lect ioncontextual daptation relational priming L II : igure 2 --.
S t ructure  o f  lhe pr imb~g networkifi :/:jif the case (Z COITCS\[)onds to acompulsory argument of thelexeme i or if the latter shouklfulfill c~ alone.if the case (z corresponds to anoptional arg, tltllCllt of i or if thelatter should fulfill c~ thanks to apreposition.otherwise.The inner synaptic weights of the case-basedsub-networks represent the relations between thepriming and the primed words :031~'~,5<,(t) = mmax if iandj  share a microscmanlicrelation which corresponds io the case <z.
?primedwords(outputs)i qiaigh > a?,(t) > q\]owaie,(t) < "lio wprimable wordsrejected wordsThe primed words aims at constraining thespeech recognition, thereby warranting thesemantic coherence of the analysis.
Theseconstraints can be relaxed by considering theprimable words.
Every recognized word isfinally handled by the parsing process with itspriming relation (see section 4).3.3.
P repos i t ionsPrepositions restrict the microsemanticassignment of lhe objects they introduce.
As aresttlt, the in'epositional cells of the st ructura llayer tnodulate dynamically the case-based49dispatching weights to prohibit any inconsistentpriming.
The rule (3') stands therefore for (3) :i i,i (~ .a~(t) ) ai3(t) a4c ~ (t) = O3,4c~(t ) .
?Ok~x(t)with: c0k(t) = ~0ma x if ~ is consistent with thepreposition k.?ok(t) = 0 otherwise.and ?
ask(t) = area x while the object of k isnot assigned a case.ak(t) = 0 otherwise.At last, the preposition is assigned the TAGargument of the introduced object.3.4.
CoordinationsThe parser deals only for the moment being withlogical coordinations (and, or, but...).
In suchcases, the coordinated elements must share thesame microsemantic ase.
This constraint isworked out by the recall of the already fulfilledmicrosemantic relations, which were allpreviously stacked.
The dispatching is thusrestricted to the recalled relations every time acoordination occurs :i,j i,j m3.4~(t) = m3.4~(0) for a stacked relationi,j 033,4, ~(t) = 0 otherwise.The coordinate words are finally considered thecoo arguments of the conjunction, which isassigned to the shared microsemantic case.3,5.
Back primingGenerally speaking, the priming processprovides a set of words that should follow thealready uttered lexemes.
In some cases, a lexememight however occur before its priming word :(a) I want  to enlarge the smal l  w indowBack priming situations are handled through thefollowing algorithm :Evm~?
time a new word occurs :1.
If this word was not primed, it is pushedit in a back priming stack.2.
Otherwise, one checks whether this wordback primes some stacked ones.
Backprimed words are then popped out.4.
Microsemantic parsing4.1.
UnificationThe microsemantic parsing relies on theunification of the subcategorization frames ofthe lexemes that are progressively recognized.This unification must respect four principles :Unicity - -  Any argume~B'~nust be at the mostfulfilled by a unique lexeme or a coordination.Coherence-  Any lexeme must fulfil at themost a unique argument.Coordinat ion - -  Coordinate lexemes mustfulfil the same subcategorized argument.Relative completeness - -  Any argumentmight remain unfulfilled although the parsermust always favor the more complete analysis.The principle of relative completeness ismotivated by the high frequency of incompleteutterances (ellipses, interruptions...) spontaneousspeech involves.
The parser aims only atextracting an unfinished microsemantic structurepragmatics hould then complete.
As noticedpreviously with the coordinations, theseprinciples govern preventively the contextualadaptation of the network weights, so that anyincoherent priming is excluded.5.
LINGUISTIC ABILITIESAs illustrated by the previous example, themicrosemantic parser masters rather complexsentences.
The study of its linguistic abilitiesoffers a persuasive view of its structural power.5.1.
Linguistic coverageAlthough our parser is dedicated to Frenchapplications, we expect our semantic approachto be easily extended to other languages.
We willnow study several linguistic phenomena theparser masters easily.Compound tenses and passive - -  According tothe microsemantic point of view, the auxiliariesappear as a mark of modality of the verb.
As aresult, the parser considers ordinarily anyauxiliary an ordinary MOD argument of the verb.
(d) J 'ai mangd \[Pred =' manger' "*I has eaten.
\ [MOD = \[Pred ='avoir'\]I ate.
LAGT = \[Pred =' je'\](e) Le carrd est effacd " \[Pred ='carr6' \]The square is erased OBJ = \[DET = \[Pred ='le'\]JPred ='effacer'MOD = \[Pred ='e te~\]~Pred --'logidel,' ,AGT =/DET = \[Pred = le \]LTAG =' par'50I n te r rogat ions -  Three interrogative forms aremet in French : subject inversion (fl), est-ce-quequestions (f2) and intonative questions (f3).
(fl) ddpla~'ons nous le carrd ?
(f2) est-ce-que nous ddplafons le carrd ?
(f3) nous dgplacfons le carrd ?Since the parser ignores most word-orderconsiderations, the interrogative utterances areprocessed like any declarative ones.
Thisapproach suits perfectly to spontaneous speech,which rarely involves a subject inversion.
Closedquestions are consequently characterized eitherby a prosodic analysis or by the adverbial phraseest-ce-que.
(g) oft ddplafons nous le carrd ?Open questions (g) are on the contraryintroduced explicitly by an interrogative pronounwhich stands for the missing argument.Relat ive c lauses - - -Every  relative clause isconsidered an argtunent of the lexeme therelative pronoun refers to.
(h) It encumbers  the window which is hereThe microsemantic structures of the main andthe relative clauses are however kept distinct torespect the principle of coherence.
The twoparse trees are indirectly related by an anaphorica~lation (REF).Subord inate  c lauses -  Provided the dependentclause is not a relative one, the subordinate verbis subcategorized by the main one.
(i) Draw a circle as soon as the square is erasedAs a result, subordinate clauses are parsed likeany ordinary object.5.2.
Spontaneous constructionsThe suitability of the semantic parser is rcallypatent when considering spontaneous peech.The parser masters indeed most of thespontaneous ungrammatical constructionswithout any specific mechanism :Repet i t ions and  self -correct ions - -  Repetitionsand self-corrections seem to violate the principleof unicity.
They involve indeed sevcral lexemeswhich share the same lnicroselnantic case :(1l) *Select he device ... the right (_tevice.
(12) *Close the display ~ ... the window.These constructions are actually considered apeculiar coordination where the conjunction ismissing \[De Smedt 87\].
Then, they are parsedlike any coordination?Ell ipses and  interrupt ions - -  The principle ofrelative completeness is mainly designed for theellipses and the interruptions, Our parser is thusable to extract alone the incomplete structure ofany interrupted utterance.
On the contrary, thecriterion of relative completeness is deficient formost of the ellipses like (t), where the upperpredicate to move is omitted :(n) * \[Movc l The left door on the right too.Such wide ellipses should nevertheless berecovered at a upper pragmatic level.Comments  - -Genera l ly  speaking, comments donot share any microsemantic relation with thesentence they are inserted in :(o) * Draw a line ... that's it ... on the right..For instance, the idiomatic phrase that's it isrelated to (o) at the pragmatic level and not atthe semantic one.
As a result, the microsemanticparser can not unify the main clause and thecomment.
We expect however filrther studies onpragmatic marks to enhance the parsing of theseconstructions.
Despite this weakness, therobustness of the microsemantic parser isalready substantial.
The following experimentalresults will thus suggest he suitability of ourmnodcl for spontaneous speech parsing.6.
ResultsThis section presents several experiments thatwere carr ied out on our microsemantic analyzeras well as on a LFG parser \[Zweigenbaum 91\].These experiments were achieved on the literalwritten transcription of three corpora ofspontaneous speech (table 2) which allcorrespond to a collaborative task of drawingbetween two human subjects (wizard of Ozexperiment).7?~ble 2 .'
Description of  the experimental corpora.Corpus Number of Average lengthutterances of utterancescorpus 1 260 11.8corpus 2 157 l 1.3corpus 3 179 5.7The dialogues were totally unconstrained, so thatthe corpora are corresponding to natural53_spontaneous peech.
We compared the twoparser according on their robustness and theirperplexity.6.1.
RobustnessThe table 3 provides the accuracy rates of thetwo parsers.
These results show the benefits ofour approach.
Around four utterances over five(-?=83.5%) are indeed processed correctly bythe microsemantic parser whereas the LFG'saccuracy is limited to 40% on the two firstcorpora.
Its robustness is noticeably higher onthe third corpus, which presents a moderate ratioof ungrammatical utterances.
The overallperformances of the LFG suggest neverthelessthat a syntactic approach is not suitable forspontaneous peech, by opposition with themicrosemantic one.Table 3 ."
Average robustness of the LFG and themicrosemantic.
Accuracy rate = number of correctanalyses /number of tested utterances.Parser corpus 1 corpus 2 corpus 3 ~- ~nLFG 0.408 0.401 0.767 0.525 0.170Semantics 0.853 0.785 0.866 0.835 0.036Besides, the independence of microsemanticsfrom the grammatical shape of the utteranceswarrants its robustness remains relativelyunaltered (standard eviation CYn = 0.036).6.2.
PerplexityAs mentioned above, the microsemantic parserignores in a large extent most of the constraintsof linear precedence.
This tolerant approach ismotivated by the frequent ordering violationsspontaneous speech involves.
It however leads toa noticeable increase of perplexity.
Thisdeterioration is particularly patent for sentenceswhich include at least eight lexemes (Table 4).Table 4 : Number of parallel hypothetic structureslaccording to utterances' lengthLength LFG parser Microsemantic4 words 1,5 2,56 words 1,5 3,58 words 2 810 words 2 12,512 words 1,25 19,75At first, we proposed to reduce this perplexitythrough a cooperation between themicrosemantic analyzer and a LFG parser\[Antoine 9411.
Although this cooperationachieves a noticeable reduction of the perplexity,it is however ineffective when the LFG parsercollapses.
This is why we intend at present oinserl, directly some ordering constraintsspontaneous peech never violates.
\[Rainbow9411 established that any ordering rule should beexpressed lexically.
We suggest consequently toorder partially the arguments of every lexicalsubcategorization.
Thus, each frame will beassigned few equations which will characterizesome ordering priorities among its arguments.7.
ConclusionIn this paper, we argued the structural variabilityof spontaneous speech prevents its parsing bystandard syntactic analyzers.
We have thendescribed a semantic analyzer, based on anassociative priming network, which aims atparsing spontaneous speech without consideringsyntax.
The linguistic coverage of this parser, aswell as several its robustness, have clearlyshown the benefits of this approach.
We expectfurthermore the insertion of word-orderconstraints to noticeably decrease the perplexityof the microsemantic analyzer.ReferencesJ.Y.
Antoine, J. Caelen, B. Caillaud (1994).
"Automatic adaptive understanding of spokenlanguage", ICSLP'94, Yokoham, Japan, 799:802.D.
Appelt, E. Jakson (1992), "SRI International ATISBenchmark Test Results", 5th DARPA Workshopon Speech and Natural Language, Harriman, NY.J.
Bresnan, J. Kanerva (1989).
"Locative inversion inChichewa", Linguistic Inquiry, 20, 1-50.A.
Joshi (1987) "The relevance of TAG togeneration", in G. Kempen (ed.
), "NaturalLanguage Generation", Reidel, Dordrecht, NL.W.
Levelt (1989).
"Speaking : from intention toarticulation", MIT Press, Cambridge, Ma.C.
Pollard, 1.
Sag (1987), "Information based syntaxand semantics", CSLI Lectures notes, 13,University of Chicago Press, IL.O.
Rainbow, A. Joslfi (1994).
"A Formal Look atDependancy Grammars and Phrase-StructureGrammars ", in L. Wanner (ed.
), "Current Issuesin Meaning-Text Theory", Pinter, London, 1994.F.
Rastier et al(1994).
"S6mantique pour l'analyse",Masson, Paris.S.
Seneff (1992).
"Robust Parsing for SpokenLanguage Systems"~ ICASSP'92, volo I, 189-192,San Francisco, CA,52
