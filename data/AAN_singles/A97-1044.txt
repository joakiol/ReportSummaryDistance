Building Effective Queries In Natural Language InformationRetrievalTomek Strzalkowski 1, Fang Lin 1, Jose Perez-Carballo 2 and Jin Wang 11GE Corporate  Research  & Deve lopment1 Research  Circle,  N iskayuna,  NY  123092School  o f  Communicat ion ,  In fo rmat ion  and L ibrary  Studies,  Rutgers  Un ivers i ty4 Hunt ington  Street, New Brunswick ,  N J  08903ABSTRACTIn this paper we report on our natural language informa-tion retrieval (NLIR) project as related to the recentlyconcluded 5th Text Retrieval Conference (TREC-5).The main thrust of this project is to use natural languageprocessing techniques to enhance the effectiveness offull-text document retrieval.
One of our goals was todemonstrate hat robust if relatively shallow NLP canhelp to derive a better epresentation f text documentsfor statistical search.
Recently, we have turned ourattention away from text representation issues and moretowards query development problems.
While our NLIRsystem still performs extensive natural language pro-cessing in order to extract phrasal and other indexingterms, our focus has shifted to the problems of buildingeffective search queries.
Specifically, we are interestedin query construction that uses words, sentences, andentire passages to expand initial topic specifications inan attempt o cover their various angles, aspects andcontexts.
Based on our earlier results indicating thatNLP is more effective with long, descriptive queries, weallowed for long passages from related ocuments to beliberally imported into the queries.
This method appearsto have produced adramatic improvement in the perfor-mance of two different statistical search engines that wetested (Cornell's SMART and NIST's Prise) boostingthe average precision by at least 40%.
In this paper wediscuss both manual and automatic procedures for queryexpansion within a new stream-based informationretrieval model.1.
INTRODUCTIONA typical (full-text) information retrieval (IR) task is toselect documents from a database in response to a user'squery, and rank these documents according to relevance.This has been usually accomplished using statisticalmethods (often coupled with manual encoding) that (a)select erms (words, phrases, and other units) from doc-uments that are deemed to best represent their content,and (b) create an inverted index file (or files) that pro-vide an easy access to documents containing theseterms.
A subsequent search process will attempt omatch preprocessed user queries against term-based rep-resentations of documents in each case determining adegree of relevance between the two which dependsupon the number and types of matching terms.
Althoughmany sophisticated search and matching methods areavailable, the crucial problem remains to be that of anadequate representation of content for both the docu-ments and the queries.In term-based representation, a document (as well as aquery) is transformed into a collection of weightedterms, derived directly from the document text or indi-rectly through thesauri or domain maps.
The representa-tion is anchored on these terms, and thus their carefulselection is critical.
Since each unique term can bethought o add a new dimensionality to the representa-tion, it is equally critical to weigh them properly againstone another so that the document is placed at the correctposition in the N-dimensional term space.
Our goal hereis to have the documents on the same topic placed closetogether, while those on different topics placed suffi-ciently apart.
Unfortunately, we often do not know howto compute terms weights.
The statistical weighting for-mulas, based on terms distribution within the database,such as tf*idf, are far from optimal, and the assumptionsof term independence which are routinely made arefalse in most cases.
This situation is even worse whensingle-word terms are intermixed with phrasal terms andthe term independence b comes harder to justify.The simplest word-based representations of content,while relatively better understood, are usually inade-quate since single words are rarely specific enough foraccurate discrimination, and their grouping is often acci-dental.
A better method is to identify groups of words299that create meaningful phrases, especially if thesephrases denote important concepts in the databasedomain.
For example, "joint venture" is an importantterm in the Wall Street Journal (WSJ henceforth) data-base, while neither "joint" nor "venture" are importantby themselves.There are a number of ways to obtain "phrases" fromtext.
These include generating simple collocations, ta-tistically validated N-grams, part-of-speech taggedsequences, yntactic structures, and even semantic on-cepts.
Some of these techniques are aimed primarily atidentifying multi-word terms that have come to functionlike ordinary words, for example "white collar" or"electric car", and capturing other co-occurrence idio-syncrasies associated with certain types of texts.
Thissimple approach as proven quite effective for somesystems, for example the Cornell group reported (Buck-ley et al, 1995) that adding simple bigram collocationsto the list of available terms can increase retrieval preci-sion by as much as 10%.Other more advanced techniques of phrase extraction,including extended N-grams and syntactic parsing,attempt o uncover "concepts", which would captureunderlying semantic uniformity across various surfaceforms of expression.
Syntactic phrases, for example,appear easonable indicators of content, arguably betterthan proximity-based phrases, since they can adequatelydeal with word order changes and other structural varia-tions (e.g., "college junior" vs. "junior in college" vs."junior college").
A subsequent regularization process,where alternative structures are reduced to a "normalform", helps to achieve a desired uniformity, for exam-ple, "college+junior" will represent a college for jun-iors, while "junior+college" will represent a junior in acollege.
A more radical normalization would have also"verb object", "noun rel-clause", etc.
convened into col-lections of such ordered pairs.
This head+modifier nor-malization has been used in our system, and is furtherdescribed in this paper.
It has to be noted, however, thatthe use of full-scale syntactic analysis is severely push-ing the limits of practicality of an information retrievalsystem because of the increased emand for computingpower and storage.
At the same time, while the gain inrecall and precision has not been negligible (werecorded 10-20% increases in precision), no dramaticbreakthrough has occurred either.lCurrently, the state-of-the artstatistical nd probabilistic IR sys-tem perform at about 20-40% precision range for arbitrary ad-hocretrieval tasks.This state of affairs has prompted us take a closer lookat the term selection and representation process.
Ourearlier experiments demonstrated that an improvedweighting scheme for compound terms, includingphrases and proper names, leads to an overall gain inretrieval accuracy.
The fundamental problem, however,remained to be the system's inability to recognize, in thedocuments searched, the presence or absence of the con-cepts or topics that the query is asking for.
The main rea-son for this was, we noted, the limited amount ofinformation that the queries could convey on variousaspects of topics they represent.
Therefore, we startedexperimenting with manual and automatic query build-ing techniques.
The purpose was to devise a method forfull-text query expansion that would allow for creatingexhaustive search queries uch that: (1) the performanceof any system using these queries would be significantlybetter than when the system is run using the originaltopics, and (2) the method could be eventually auto-mated or semi-automated so as to be useful to a non-expert user.
Our preliminary results from TREC-5 eval-uations how that this approach is indeed very effective.In the rest of this paper we describe the overall organi-zation of our TREC-5 system, and then discuss someexperiments hat we performed and their results, as wellas our future research plans.2.
STREAM-BASED INFORMATIONRETRIEVAL MODELOur NLIR system encompasses several statistical andnatural language processing (NLP) techniques forrobust ext analysis.
These has been organized togetherinto a "stream model" in which alternative methods ofdocument indexing are strung together to perform inparallel.
Stream indexes are built using a mixture of dif-ferent indexing approaches, term extracting and weight-ing strategies, even different search engines.
The finalresults are produced by merging ranked lists of docu-ments obtained from searching all stream indexes withappropriately preprocessed queries, i.e., phrases forphrase stream, names for names tream, etc.
The merg-ing process weights contributions from each streamusing a combination that was found the most effective intraining runs.
This allows for an easy combination ofalternative retrieval and routing methods, creating ameta-search strategy which maximizes the contributionof each stream.
Both Cornell's SMART version 11, andNIST's Prise search engines were used as basicengines .2Our NLIR system employs a suite of advanced naturallanguage processing techniques in order to assist he sta-3O0tistical retrieval engine in selecting appropriate indexingterms for documents in the database, and to assign themsemantically validated weights.
The following termextraction methods have been used; they correspond tothe indexing streams in our system.1.
Eliminate stopwords: original text words minus cer-tain no-content words are used to index documents.2.
Morphological stemming: we normalize across mor-phological word variants (e.g., "proliferation", pro-liferate", "proliferating") using a lexicon-basedstemmer.3.
Phrase extraction: we use various shallow text pro-cessing techniques, such as part-of-speech tagging,phrase boundary detection, and word co-occurrencemetrics to identify stable strings of words, such as"joint venture".4.
Phrase normalization: we identify "head+modifier"pairs in order to normalize across syntactic variantssuch as "weapon proliferation", "proliferation ofweapons", "proliferate weapons", etc.
into"weapon+proliferate".5.
Proper names: we identify proper names for index-ing, including people names and titles, locationnames, organization ames, etc.Among the advantages of the stream architecture wemay include the following:?
stream organization makes it easier to compare thecontributions of different indexing features or repre-sentations.
For example, it is easier to design exper-iments which allow us to decide if a certainrepresentation adds information which is not contrib-uted by other streams.?
it provides a convenient testbed to experiment withalgorithms designed to merge the results obtainedusing different IR engines and/or techniques.?
it becomes easier to fine-tune the system in order toobtain optimum performance?
it allows us to use any combination of IR engineswithout having to modify their code at all.While our stream architecture may be unique among IRsystems, the idea of combining evidence from multiplesources has been around for some time.
Severalresearchers have noticed in the past that different sys-tems may have similar performance but retrieve differ-ent documents, thus suggesting that they may2.
SMART version 11 is freely available, unlike the more advancedversion 12.complement one another.
It has been reported that theuse of different sources of evidence increases the perfor-mance of a system (see for example, Callan et al, 1995;Fox et a1.,1993; Saracevic & Kantor, 1988).3.
STREAMS USED IN  NL IR  SYSTEM3.1 Head-Mod i f ie r  Pa i rs  S t reamOur most linguistically advanced stream is thehead+modifier pairs stream.
In this stream, documentsare reduced to collections of word pairs derived via syn-tactic analysis of text followed by a normalization pro-cess intended to capture semantic uniformity across avariety of surface forms, e.g., "information retrieval","retrieval of information", retrieve more information","information that is retrieved", etc.
are all reduced to"retrieve+information" pair, where "retrieve" is a heador operator, and "information" is a modifier or argu-ment.The pairs stream is derived through a sequence of pro-cessing steps that include:?
Part-of-speech tagging?
Lexicon-based word normalization (extended "stem-ming")?
Syntactic analysis with the qq'P parser (cf.
Strza-lkowski & Scheyen, 1996)?
Extraction of head+modifier pairs?
Corpus-based decomposition/disambiguation of l gnoun phrases.Syntactic phrases extracted from T IP  parse trees arehead-modifier pairs.
The head in such a pair is a centralelement of a phrase (main verb, main noun, etc.
), whilethe modifier is one of the adjunct arguments of the head.It should be noted that the parser's output is a predicate-argument structure centered around main elements ofvarious phrases.
The following types of pairs are consid-ered: (1) a head noun and its left adjective or nounadjunct, (2) a head noun and the head of its fightadjunct, (3) the main verb of a clause and the head of itsobject phrase, and (4) the head of the subject phrase andthe main verb.
These types of pairs account for most ofthe syntactic variants for relating two words (or simplephrases) into pairs carrying compatible semantic on-tent.
This also gives the pair-based representation suffi-cient flexibility to effectively capture content elementseven in complex expressions.
Long, complex phrasesare similarly decomposed into collections of pairs, usingcorpus statistics to resolve structural ambiguities.3013.2 Linguistic Phrase StreamWe used a regular expression pattern matcher on thepart-of-speech tagged text to extract noun groups andproper noun sequences.
The major rules we used are:1. a sequence of modifiers (vbnlvbgljj) followed by atleast one noun, such as: "cryonic suspend", air traf-fic control system";2. proper noun(s) modifying anoun, such as: "u.s. citi-zen", "china trade";3. proper noun(s) (might contain '&'), such as: "warrencommission", national air traffic controller".In these experiments, the length of phrases was limitedto maximum 7 words.sion, whereas lnc.ntc slightly sacrifices the average pre-cision, but gives better ecall (see Buckley, 1993).We used also a plain text stream.
This stream wasobtained by indexing the text of the documents "as is"without stemming or any other processing and runningthe unprocessed text of the queries against hat index.Finally, some experiments involved the fragmentsstream.
This was the result of spliting the documents ofthe STEM stream into fragments of constant length(1024 characters) and indexing each fragment as if itwere a different document.
The queries used with thisstream were the usual stem queries.
For each query, theresulting ranking was filtered to keep, for each docu-ment, the highest score obtained by the fragments ofthat document.3.3 Name StreamProper names, of people, places, events, organizations,etc., are often critical in deciding relevance of a docu-ment.
Since names are traditionally capitalized inEnglish text, spotting them is relatively easy, most of thetime.
Many names are composed of more than a singleword, in which case all words that make up the name arecapitalized, except for prepositions and such, e.g., TheUnited States of America.
It is important that all namesrecognized in text, including those made up of multiplewords, e.g., South Africa or Social Security, are repre-sented as tokens, and not broken into single words, e.g.,South and Africa, which may turn out to be differentnames altogether by themselves.
On the other hand, weneed to make sure that variants of the same name areindeed recognized as such, e.g., U.S. President BillClinton and President Clinton, with a degree of confi-dence.
One simple method, which we use in our system,is to represent a compound name dually, as a compoundtoken and as a set of single-word terms.
This way, if acorresponding full name variant cannot be found in adocument, its component words matches can still add tothe document score.
A more accurate, but arguably moreexpensive method would be to use a substring compari-son procedure to recognize variants before matching.3.4 Other  Streams usedThe stems stream is the simplest, yet, it turns out, themost effective of all streams, a backbone in our multi-stream model.
It consists of stemmed non-stop single-word tokens (plus hyphenated phrases).
Our earlyexperiments with multi-stream indexing using SMARTsuggested that the most effective weighting of thisstream is lnc.ltc, which yields the best average preci-Table 1 shows relative performance of each streamtested for this evaluation.
Note that the standardstemmed-word epresentation (stems stream) is still themost efficient one, but linguistic processing becomesmore important in longer queries.
In this evaluation, theshort queries are one-sentence s arch directives uch asthe following:What steps are being taken by governmental or even pri-vate entities world-wide to stop the smuggling of aliens.The long queries, on the other hand, contain substan-tially more text as the result of full-text expansiondescribed in section 5 below.TABLE 1.
How different streams performrelative to one another (ll-pt avg.
Prec)short longSTREAM queries queriesStems 0.1682 0.2626Phrases 0.1233 0.2365H+M Pairs 0.0755 0.2040Names 0.0844 0.06084.
STREAM MERGING STRATEGYThe results obtained from different streams are list ofdocuments ranked in order of relevance: the higher therank of a retrieved ocument, he more relevant it is pre-sumed to be.
In order to obtain the final retrieval result,ranking lists obtained from each stream have to be com-bined together by a process known as merging or fusion.The final ranking is derived by calculating the com-bined relevance scores for all retrieved ocuments.
Thefollowing are the primary factors affecting this process:3021. document relevancy scores from each stream2.
retrieval precision distribution estimates withinranks from various streams, e.g., projected precisionbetween ranks 10 and 20, etc.;3.
the overall effectiveness of each stream (e.g.
mea-sured as average precision on training data)4. the number of streams that retrieve aparticular docu-ment, and5.
the ranks of this document within each stream.Generally, a more effective stream will more effect onshaping the final ranking.
A document which isretrieved at a high rank from such a stream is morelikely to end up ranked high in the final result.
In addi-tion, the performance of each stream within a specificrange of ranks is taken into account.
For example, ifphrases stream tends to pack relevant documents intotop 10-20 retrieved ocuments (but not so much into 1-10) we would give premium weights to the documentsfound in this region of phrase-based ranking, etc.
Table2 gives some additional data on the effectiveness ofstream merging.
Further details are available in a TRECconference article.TABLE 2.
Precision improvements over stems-only retrievalshort longqueries queriesStreams merged % change %changeAll streams +5.4 +20.94Stems+Phrases+Pairs +6.6 +22.85Stems+Phrases +7.0 +24.94Stems+Pairs +2.2 + 15.27Stems+Names +0.6 +2.59relationships, etc.
Unfortunately, an average searchquery does not look anything like this, most of the time.It is more likely to be a statement specifying the seman-tic criteria of relevance.
This means that except for thesemantic or conceptual resemblance (which we cannotmodel very well as yet) much of the appearance of thequery (which we can model reasonably well) may be,and often is, quite misleading for search purposes.Where can we get the right queries?In today's information retrieval systems, query expan-sion usually pertains content and typically is limited toadding, deleting or re-weighting of terms.
For example,content terms from documents judged relevant areadded to the query while weights of all terms areadjusted in order to reflect the relevance information.Thus, terms occurring predominantly in relevant docu-ments will have their weights increased, while thoseoccurring mostly in non-relevant documents will havetheir weights decreased.
This process can be performedautomatically using a relevance feedback method, e.g.,Roccio's (1971), with the relevance information eithersupplied manually by the user (Harman, 1988), or other-wise guessed, e.g.
by assuming top 10 documents rele-vant, etc.
(Buckley, et al, 1995).
A serious problem withthis content-term expansion is its limited ability to cap-ture and represent many important aspects of whatmakes some documents relevant to the query, includingparticular term co-occurrence patterns, and other hard-to-measure t xt features, such as discourse structure orstylistics.
Additionally, relevance-feedback expansiondepends on the inherently partial relevance information,which is normally unavailable, or unreliable.Other types of query expansions, including general pur-pose thesauri or lexical databases (e.g., Wordnet) havebeen found generally unsuccessful in informationretrieval (cf.
Voorhees & Hou, 1993; Voorhees, 1994)Note that again, long text queries benefit more from lin-guistic processing.5.
QUERY EXPANSION EXPERIMENTS5.1 Why query expansion?The purpose of query expansion in information retrievalis to make the user query resemble more closely thedocuments it is expected to retrieve.
This includes bothcontent, as well as some other aspects uch as composi-tion, style, language type, etc.
If the query is indeedmade to resemble a "typical" relevant document, thensuddenly everything about this query becomes a validsearch criterion: words, collocations, phrases, variousAn alternative to term-only expansion is a full-textexpansion which we tried for the first time in TREC-5.In our approach, queries are expanded by pasting inentire sentences, paragraphs, and other sequencesdirectly from any text document.
To make this processefficient, we first perform a search with the original, un-expanded queries (short queries), and then use top N(I 0, 20) returned ocuments for query expansion.
Thesedocuments are not judged for relevancy, nor assumedrelevant; instead, they are scanned for passages that con-tain concepts referred to in the query.
Expansion mate-rial can be found in both relevant and non-relevantdocuments, benefitting the final query all the same.
Infact, the presence of such text in otherwise non-relevantdocuments underscores the inherent limitations of distri-bution-based term reweighting used in relevance feed-303back.
Subject o some further "fitness criteria", theseexpansion passages are then imported verbatim into thequery.
The resulting expanded queries undergo the usualtext processing steps, before the search is run again.Full-text expansion can be accomplished manually, aswe did initially to test feasibility of this approach, orautomatically, as we tried in later with promisingresults.
We first describe the manual process focussingon guidelines et forth in such a way as to minimize andstreamline human effort, and lay the ground for eventualautomation.
We then describe our first attempt at auto-mated expansion, and discuss the results from both.The initial evaluations indicate that queries expandedmanually following the prescribed guidelines areimproving the system's performance (precision andrecall) by as much as 40%.
This appear to be true notonly for our own system, but also for other systems: weasked other groups participating in TREC-5 to runsearch using our expanded queries, and they reportednearly identical improvements.
At this time, automatictext expansion produces less effective queries thanmanual expansion, primarily due to a relatively unso-phisticated mechanism used to identify and match con-cepts in the queries.5.2 Guidelines for manual query expansionWe have adopted the following guidelines for queryexpansion.
They were constructed to observe realisticlimits of the manual process, and to prepare ground foreventual automation.1.
NLIR retrieval is run using the 50 original "short"queries.2.
Top 10 documentss retrieved by each query areretained for expansion.
We obtain 50 expansionsub-collections, one per query.3.
Each query is manually expanded using phrases,sentences, and entire passages found in any of thedocuments from this query's expansion subcollec-tion.
Text can both added and deleted, but care istaken to assure that the final query has the same for-mat as the original, and that all expressions addedare well-formed English strings, though not neces-sarily well-formed sentences.
A limit of 30 minutesper query in a single block of time is observed.4.
Expanded queries are sent through all text process-ing steps necessary to run the queries against multi-ple stream indexes.5.
Rankings from all streams are merged into the finalresult.There are two central decision making points that affectthe outcome of the query expansion process followingthe above guidelines.
The first point is how to locate textpassages that are worth looking at -- it is impractical, ifnot downright impossible to read all 10 documents,some quite long, in under 30 minutes.
The second pointis to actually decide whether to include a given passage,or a portion thereof, in the query.
To facilitate passagespotting, we used simple word search, using key con-cepts from the query to scan down document text.
Eachtime a match was found, the text around (usually theparagraph containing it) was read, and if found "fit",imported into the query.
We experimented also with var-ious "pruning" criteria: passages could be eitherimported verbatim into the query, or they could be"pruned" of "obviously" undesirable noise terms.
Inevaluating the expansion effects on query-by-querybasis we have later found that the most liberal expan-sion mode with no pruning was in fact the most effec-tive.
This would suggest hat relatively self-containedtext passages, such as paragraphs, provide a balancedrepresentation f content, that cannot be easily approxi-mated by selecting only some words.5.3 Automat ic  Query Expans ionQueries obtained through the full-text manual expansionproved to be overwhelmingly better than the originalsearch queries, providing as much as 40% precisiongain.
These results were sufficiently encouraging tomotivate us to investigate ways of performing suchexpansions automatically.One way to approximate he manual text selection pro-cess, we reasoned, was to focus on those text passagesthat refer to some key concepts identified in the query,for example, "alien smuggling" for query 252 below.The key concepts (for now limited to simple noungroups) were identified by either their pivotal ocationwithin the query (in the Title field), or by their repeatedoccurrences within the query Description and Narrativefields.
As in the manual process, we run a "short" queryretrieval, this time retaining 100 top documentsretrieved by each query.
An automated process thenscans these 100 documents for all paragraphs whichcontain occurrences, including some variants, of any ofthe key concepts identified in the original query.
Theparagraphs are subsequently pasted verbatim into thequery.
The original portion of the query may be saved ina special field to allow differential weighting.
Finally,the expanded queries were run to produce the finalresult.304The above, clearly simplistic technique has producedsome interesting results.
Out of the fifty queries wetested, 34 has undergone the expansion.
Among these34 queries, we noted precision gains in 13, precisionloss in 18 queries, with 3 more basically unchanged.However, for these queries where the improvement didoccur it was very substantial indeed: the average gainwas 754% in 11-pt precision, while the average loss (forthe queries that lost precision) was only 140%.
Overall,we still can see a 7% improvement on all 50 queries (vs.40%+ when manual expansion is used).Our experiments how that selecting the right para-graphs from documents to expand the queries can dra-matically improve the performance of a text retrievalsystem.
This process can be automated, however, thechallenge is to devise more precise automatic means of"paragraph picking".6.
SUMMARY OF RESULTSIn this section we summarize the results obtained fromquery expansion and other elated experiments.An automatic run means that there was no human inter-vention in the process at any time.
A manual run meansthat some human processing was done to the queries,and possibly multiple test runs were made to improvethe queries.
A short query is derived using only onesection of a TREC-5 topic, namely the DESCRIPTIONfield.
A full query is derived from any or all fields inthe original topic.
A long query is obtained through ourfull-text expansion method (manual, or automatic).
Anexample TREC-5 query is show below; note that theDescription field is what one may reasonably expect obe an initial search query, while Narrative providessome further explanation of what relevant material maylook like.
The Topic field provides a single concept ofinterest to the searcher; it was not permitted in the shortqueries.<top><num> Number: 252<title> Topic: Combating AlienSmuggling<desc> Description:What steps are being taken by gov-ernmental oreven private ntitiesworld-wide to stop the smuggling ofaliens.<narr> Narrative:To be relevant, a document mustdescribe an effort being made (otherthan routine border patrols) in anycountry of the world to prevent theillegal penetration f aliens acrossborders.</top>Table 3 summarizes selected runs performed with ourNLIR system on TREC-5 database using queries 251through 300.
Table 4 gives the performance of Cornell's(now Sabir Inc.) SMART system version 12, usingadvanced Lnu.ltu term weighting scheme, and queryexpansion through automatic relevance feedback(rel.fbk), on the same database and with the same que-ries.
Sabir used our long queries to obtain long queryrun.
Note the consistently large improvements inretrieval precision attributed to the expanded queries.TABLE 3.
Precision improvement in NLIRsystemPREC.llpt.
avg%change@10docs%change@100doc%changeRecall%changeshortqueries0.14780.15780.05440.59fullqueries0.2078+41.00.2044+30.00.0696+28.00.65+10.0longqueriesauto.0.2220+50.00.2089+32.00.0709+30.00.64+8.5longqueriesman.0.3176+115.00.3156+100.00.0998+83.00.77+31.0TABLE 4.
Results for Cornell's SMARTPREC,l lpt.avg%change@5 docs%change@ 100docs%changeshortqueries0.14990.21780.0578fullqueries0.2142+43.00.2889+33.00.0709+23.0fullqueriesrel.fbk0.2416+62.00.2756+27.00.0771+33.0longqueriesman.0.2983+99.0013600+65.00.0904+56.0Recall 0.58 0.64 0.70 0.73@change +10.0 +21.0 +26.03057.
CONCLUSIONSWe presented in some detail our natural language infor-mation retrieval system consisting of an advanced NLPmodule and a "pure' statistical core engine.
While manyproblems remain to be resolved, including the questionof adequacy of term-based representation f documentcontent, we attempted to demonstrate hat the architec-ture described here is nonetheless viable.
In particular,we demonstrated that natural anguage processing cannow be done on a fairly large scale and that its speedand robustness have improved to the point where it canbe applied to real IR problems.The main observation to make is that natural languageprocessing is not as effective as we have once hoped toobtain better indexing and better term representations ofqueries.
Using linguistic terms, such as phrases, head-modifier pairs, names, or even simple concepts doeshelp to improve retrieval precision, but the gainsremained quite modest.
On the other hand, full textquery expansion works remarkably well.
Our maineffort in the immediate future will be to explore ways toachieve at least partial automation of this process.
Aninitial experiment in this direction has been performedas part of NLP Track (genlp3 run), and the results areencouraging.ACKNOWLEDGEMENTS.
We would like to thankDonna Harman of NIST for making her PRISE systemavailable to us since the beginning of TREC.
Will Rog-ers and Paul Over provided valuable assistance ininstalling updated versions of PRISE.
We would alsolike to thank Ralph Weischedel for providing the BBN'spart of speech tagger.
We acknowledge the followingmembers of our TREC-5 team who participated in thequery expansion experiments: Louise Guthrie, JussiKarlgren, Jim Leistensnider, Troy Straszheim, and JonWilding.
This paper is based upon work supported inpart by the Advanced Research Projects Agency underTipster Phase-2 Contract 94-FI57900-000, TipsterPhase-3 Contract 97-FI56800-000, and the National Sci-ence Foundation under Grant IRI-93-02615.REFERENCESBuckley, Chris.
1993.
"The Importance of ProperWeighting Methods."
Proc.
of ARPA's Human Lan-guage Technology Workshop.
pp.
349-352.Buckley, Chris, Amit Singhal, Mandar Mitra, GerardSalton.
1995.
"New Retrieval Approaches UsingSMART: TREC 4".
Proceedings of the Third TextREtrieval Conference (TREC-4), NIST Special Publ.Callan, James, Zhihong Lu, and Bruce Croft.
1995.Searching Distributed Collections with Inference Net-works."
Proceedings of SIGIR-95.
pp.
21-28.Fox, Ed, Prabhakar Kushik, Joseph Shaw, Russell Mod-lin and Durgesh Rao.
1993.
"Combining Evidence fromMultiple Searches.".
Proc.
of First Text Retrieval Con-ference (TREC-1).
NIST Spec.
Publ.
500-207. pp.
319-328.Harman, Donna.
1988.
"Towards interactive queryexpansion."
Proceedings of ACM SIGIR-88, pp.
321-331.Roccio, J.
1971.
"Relevance Feedback in InformationRetrieval."
In G. Salton (ed), "The SMART RetrievalSystem.
Prentice-Hall, pp.
313-323.Saracevic, T., Kantor, P. 1988.
"A Study of InformationSeeking and Retrieving.
III.
Searchers, Searches, andOverlap".
Journal of the American Society for Informa-tion Science.
39(3):197-216.Strzalkowski, Tomek and Jose Perez-Carballo.
1994.
"Recent Developments in Natural Language TextRetrieval."
Proceedings of the Second Text REtrievalConference (TREC-2), NIST Special Publication 500-215, pp.
123-136.Strzalkowski, Tomek, Jose Perez-Carballo and MihneaMarinescu.
1995.
"Natural Language Information Retir-ieval: TREC-3 Report."
Proceedings of the Third TextREtrieval Conference (TREC-3), NIST Special Publica-tion 500-225, pp.
39-53.Strzalkowski, Tomek, Jose Perez-Carballo and MihneaMarinescu.
1996.
"Natural Language Information Retir-ieval: TREC-4 Report."
Proceedings of the Third TextREtrieval Conference (TREC-4), NIST Special Publ.Strzalkowski, Tomek.
1995.
"Natural Language Infor-mation Retrieval" Information Processing and Manage-ment, Vol.
31, No.
3, pp.
397-417.
Pergamon/Elsevier.Strzalkowski, Tomek, and Peter Scheyen.
1996.
"AnEvaluation of TIP Parser: a preliminary report."
In H.Bunt, M. Tomita (eds), Recent Advances in ParsingTechnology, Kluwer Academic Publishers, pp.
201-220.Voorhees, Ellen.
1994.
"Query Expansion Using Lexi-cal-Semantic Relations."
Proc.
of SIGIR-94, pp.
61-70.Voorhees, Ellen, Yuan-Wang Hou.
1993.
"VectorExpansion in a Large Collection."
Proc of First TextRetrieval Conference (TREC-1).
NIST Spec.
Pub.
500-207. pp.
343-351.306
