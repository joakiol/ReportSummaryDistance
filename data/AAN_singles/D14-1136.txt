Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1296?1307,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsSemantic Parsing Using Content and Context:A Case Study from Requirements ElicitationReut TsarfatyWeizmann InstituteRehovot, IsraelIlia PogrebezkyInterdisciplinary CenterHerzliya, IsraelGuy WeissWeizmann InstituteRehovot, IsraelYaarit NatanWeizmann InstituteRehovot, IsraelSmadar SzekelyWeizmann InstituteRehovot, IsraelDavid HarelWeizmann InstituteRehovot, IsraelAbstractWe present a model for the automatic se-mantic analysis of requirements elicitationdocuments.
Our target semantic repre-sentation employs live sequence charts, amulti-modal visual language for scenario-based programming, which can be directlytranslated into executable code.
The ar-chitecture we propose integrates sentence-level and discourse-level processing in agenerative probabilistic framework for theanalysis and disambiguation of individualsentences in context.
We show empiri-cally that the discourse-based model con-sistently outperforms the sentence-basedmodel when constructing a system that re-flects all the static (entities, properties) anddynamic (behavioral scenarios) require-ments in the document.1 IntroductionRequirements elicitation is a process whereby asystem analyst gathers information from a stake-holder about a desired system (software or hard-ware) to be implemented.
The knowledge col-lected by the analyst may be static, referring tothe conceptual model (the entities, their properties,the possible values) or dynamic, referring to thebehavior that the system should follow (who doeswhat to whom, when, how, etc).
A stakeholder in-terested in the system typically has a specific staticand dynamic domain in mind, but he or she cannotnecessarily prescribe any formal models or codeartifacts.
The term requirements elicitation we usehere refers to a piece of discourse in natural lan-guage, by means of which a stakeholder commu-nicates their desiderata to the system analyst.The role of a system analyst is to understandthe different requirements and transform them intoformal constructs, formal diagrams or executablecode.
Moreover, the analyst needs to consolidatethe different pieces of information to uncover asingle shared domain.
Studies in software engi-neering aim to develop intuitive symbolic systemswith which human agents can encode require-ments that would then be unambiguously trans-lated into a formal model (Fuchs and Schwitter,1995; Bryant and Lee, 2002).More recently, Gordon and Harel (2009) de-fined a natural fragment of English that can beused for specifying requirements which can beeffectively translated into live sequence charts(LSC) (Damm and Harel, 2001; Harel andMarelly, 2003), a formal language for specifyingthe dynamic behavior of reactive systems.
How-ever, the grammar that underlies this languagefragment is highly ambiguous, and all disam-biguation has to be conducted manually by a hu-man agent.
Indeed, it is commonly accepted thatthe more natural a controlled language fragmentis, the harder it is to develop an unambiguoustranslation mechanism (Kuhn, 2014).In this paper we accept the ambiguity of re-quirements descriptions as a premise, and aim toanswer the following question: can we automati-cally recover a formal representation of the com-plete system ?
one that best reflects the human-perceived interpretation of the entire document?Recent advances in natural language processing,with an eye to semantic parsing (Zettlemoyer andCollins, 2005; Liang et al., 2011; Artzi and Zettle-moyer, 2013; Liang and Potts, 2014), use differ-ent formalisms and various kinds of learning sig-nals for statistical semantic parsing.
In particu-lar, the model of Lei et al.
(2013) induces inputparsers from format descriptions.
However, rarelydo these models take into account the entire docu-ment?s context.The key idea we promote here is that discoursecontext provides substantial disambiguating infor-mation for sentence analysis.
We suggest a novel1296Figure 1: An LSC scenario: ?When the user clicksthe button, the display color must change to red.
?model for integrated sentence-level and discourse-level processing, in a joint generative probabilisticframework.
The input for the requirements elici-tation task is given in a simplified, yet highly am-biguous, fragment of English, as specified in Gor-don and Harel (2009).
The output, in contrast, isa sequence of unambiguous and well-formed livesequence charts (LSC) (Damm and Harel, 2001;Harel and Marelly, 2003) describing the dynamicbehavior of the system, tied to a single sharedcode-base called a system model (SM).Our solution takes the form of a hidden markovmodel (HMM) where emission probabilities re-flect the grammaticality and interpretability of tex-tual requirements via a probabilistic grammar andtransition probabilities model the overlap betweenSM snapshots of a single, shared, domain.
Usingefficient viterbi decoding, we search for the bestsequence of domain snapshots that has most likelygenerated the entire requirements document.
Weempirically show that such an integrated modelconsistently outperforms a sentence-based modellearned from the same set of data.The remainder of this document is organized asfollows.
In Section 2 we describe the task, andspell out our formal assumptions concerning theinput and the output.
In Section 3 we presentour target semantic representation and a speciallytailored notion of grounding for anchoring therequirements in a code-base.
In Section 4 wedevelop our sentence-based and discourse-basedmodels, and in Section 5 we evaluate the modelson various case studies.
In Section 6 we discussapplications and future extensions, and in Sec-tion 7 we summarize and conclude.2 Parsing Requirements ElicitationDocuments: Task DescriptionThere is an inherent discrepancy between the in-put and the output of the software engineering pro-cess.
The input, system requirements, is specifiedin a natural, informal, language.
The output, thesystem, is ultimately implemented in a formal un-ambiguous programming language.
Can we auto-matically recover such a formal representation ofa complete system from a set of requirements?
Inthis work we explore this challenge empirically.The Input.
We assume a scenario-based pro-gramming paradigm (a.k.a behavioral program-ming (BP) (Harel et al., 2012)) in which systemdevelopment is seen as a process whereby humansdescribe the expected behavior of the system bymeans of ?short-stories?, formally called scenar-ios (Harel, 2001).
We further assume that a givenrequirements document describes exactly one sys-tem, and that each sentence describes a single,possibly complex, scenario.
The requirements weaim to parse are given in a simplified form of En-glish (specifically, the English fragment describedin Gordon and Harel (2009)).
Contrary to strictlyformal specification languages, which are closedand unambiguous, this fragment of English em-ploys an open-ended lexicon and exhibits exten-sive syntactic and semantic ambiguity.1The Output.
Our target semantic representationemploys live sequence charts (LSC), a diagram-matic formal language for scenario-based pro-gramming (Damm and Harel, 2001).
Formally,LSCs are an extension of the well-known UMLmessage sequence diagrams (Harel and Maoz,2006), and they have a direct translation into ex-ecutable code (Harel and Marelly, 2003).2UsingLSC diagrams for software modelling enjoys theadvantages of being easily learnable (Harel andGordon, 2009), intuitively interpretable (Eitan etal., 2011) and straightforwardly amenable to exe-cution (Harel et al., 2002) and verification (Harelet al., 2013).
The LSC language is particularlysuited for representing natural language require-ments, since its basic formal constructs, scenar-ios, nicely align with events, the primitive objectsof Neo-Davidsonian Semantics (Parsons, 1990).1Formally, this variant may be viewed as a CNL of degreeP2 E3 N4 S4 with properties C,F,W,A (Kuhn, 2014, pp 6-12).2It can be shown that the execution semantics of the LSClanguage is embedded in a fragment of a branching temporallogic called CTL* (Kugler et al., 2005).1297Live Sequence Charts and Code Artifacts.
Alive sequence chart (LSC) is a diagram that de-scribes a possible or necessary run of a specifiedsystem.
In a single LSC diagram, entities are rep-resented as vertical lines called lifelines, and inter-actions between entities are represented using hor-izontal arrows between lifelines called messages,connecting a sender to a receiver.
Messages mayrefer to other entities (or properties of entities) asarguments.
Time in LSCs proceeds from top tobottom, imposing a partial order on the executionof messages.
LSC messages can be hot (red, ?musthappen?)
or cold (blue, ?may happen?).
A mes-sage may have an execution status, which desig-nates it as monitored (dashed arrow, ?wait for?
)or executed (full arrow, ?execute?).
The LSC lan-guage also encompasses conditions and controlstructures, and it allows defining requirements interms of the negation of charts.
Figure 1 illustratesthe LSC for the scenario ?When the user clicksthe button, the display color must change to red.
?.The respective system model (SM) is a code-basehierarchy containing the classes USER, BUTTON,DISPLAY, the method BUTTON.CLICK() and theproperty DISPLAY.COLOR.3 Formal SettingsIn the text-to-code generation task, we aim to im-plement a prediction function f : D ?
M, suchthatD ?
D is a piece of discourse consisting of anordered set of requirements D = d1, d2...dn, andf(D) = M ?
M is a code-base hierarchy thatgrounds the semantic interpretation of D; we de-note this by M .
sem(d1, ..., dn).
We now definethe objects D,M , and describe how to constructthe semantic interpretation function (sem(.)).
Wethen spell out the notion of grounding (.
).Surface Structures: Let ?
be a finite lexiconand let Lreq?
?
?be a language for specifyingrequirements.
We assume the sentences in Lreqhave been generated by a context-free grammarG = ?N ,?, S ?
N ,R?, where N is a set of non-terminals, ?
is the aforementioned lexicon, S ?N is the start symbol andR is a set of context-freerules {A ?
?|A ?
N , ?
?
(N ?
?)?}.
For eachutterance u ?
Lreq, we can find a sequential appli-cation of rules that generates it: u = r1?
... ?
rk;?i : ri?
R. We call such a sequence a deriva-tion of u.
These derivations may be graphicallydepicted as parse trees, where the utterance u de-fines the sequence of tree terminals in the leaves.We define Treqto be the set of trees stronglygenerated by G, and utilize an auxiliary yieldfunction yield : Treq?
Lreqreturning the leavesof the given tree t ?
.Lreq.
Different parse-treescan generate the same utterance, so the task of an-alyzing the structure of an utterance u ?
Lreqismodeled via a function syn : Lreq?
Treqthatreturns the correct, human-perceived, parse of u.Semantic Structures: Our target semantic rep-resentation of a requirement d ?
Lreqis a dia-grammatic structure called a live sequence chart(LSC).
The LSC formal definition we provide hereis based on the appendix of Harel and Marelly(2003), but rephrased in set-theoretic, event-based,terms.
We defined this alternative formalizationin order to make LSCs compatible with Neo-Davidsonian, event-based, semantic theories.
Asa result, this form of LSC formalization is well-suited for representing the semantics of naturallanguage utterances.Let us assume that L is a dictionary of entities(lifelines), A is a dictionary of actions, P is a dic-tionary of attribute names and V a dictionary ofattribute values.
The set of simple events in theLSC formal system is defined as follows:Eactive?
L?A?
L?
(L?
P ?
V )??
{hot, cold} ?
{executed,monitored}where e = ?l1, a, l2, {li: pi: vi}ki=3, temp, exe?and li?
L, a ?
A, pi?
P, temp ?
{hot, cold},exe ?
{executed,monitored}.
The event e iscalled a message in which an action a is carriedover from a sender l1to a receiver l2.3The set{li: pi: vi}ki=3depicts a set of attribute:valuepairs provided as arguments to action a.
The tem-perature temp marks the modality of the action(may, must), and the status exe distinguishes ac-tions to be taken from actions to be waited for.An event e can also refer to a state, where alogical expression is being evaluated over a set ofproperty:value pairs.
We call such an event a con-dition, and specify the set of possible conditionsas follows:Econd?
Exp?
(L?
P ?
V )??
{hot, cold} ?
{executed,monitored}3The LSC language also distinguishes static lifelines fromdynamically-bound lifelines.
For brevity, we omit this fromthe formal description of events, and simply assert that it maybe listed as one of the properties of the relevant lifeline.1298Specifically, e = ?exp, {l : p : v}ki=0, temp, exe?is a condition to be evaluated, where li?
L, pi?P, vi?
V, temp ?
{hot, cold} and exe ?
{executed,monitored} are as specified above.The condition exp ?
Exp is a first-order logic for-mula using the usual operators (?,?,?,?,?,?
).The set {l : p : v}ki=0depicts a (possibly empty)set of attribute:value pairs that participates as pred-icates in exp.
Executing a condition, that is, evalu-ating the logical expression specified by exp, alsohas a modality (may/must) and an execution status(performed/waited for).The LSC language further allows us to definemore complex events by combining partially or-dered sets of events with control structures.Ecomplex?
N ?
Econd?
{?Ec, <?|?Ec, <?
is a poset }N is a set of non-negative integers, Econdis a setof conditions as described above, and each ele-ment ?Ec, <?
is a partially ordered set of events.This structure allows us to derive three kinds ofcontrol structures:?
e = ?#, ?, ?E,<??
is a loop in which ?E,<?is executed # times.?
e = ?0, cond, ?E,<??
is a conditioned exe-cution.
If cond holds, ?E,<?
is executed.?
e = ?#, {cond}#i=1, {?Ec, <?}#i=1?
is aswitch: in case i, if the condition i holds,?Ec, <?iis executed.Definition 1 (LSC) An LSC c = ?E,<?
is apartially ordered set of events such that?e ?
E : e ?
Eactive?
e ?
Econd?
e ?
EcomplexGrounded Semantics: The information repre-sented in the LSC provides the recipe for a rig-orous construction of the code-base that will im-plement the program.
This code-base is said toground the semantic representation.
For exam-ple, if our target programming language is anObject-Oriented programming language such asJava, then the code-base will include the objects,the methods and the properties that are minimallyrequired for executing the scenario that is repre-sented by the LSC.
We refer to this code-base asa system model (henceforth, SM), and define it asfollows.Definition 2: (SM) Let Lmbe a set of imple-mented objects, Ama set of implemented meth-ods, Pma set of arguments and Vmargumentvalues.
We further define the auxiliary functionsmethods : Am?
Lm, props : Pm?
Lmandvalues : Vm?
Lm?
Pm, for identifying theentity l ?
Lmthat implements the method a ?Am, the entity l ?
Lmthat contains the propertyp ?
Pm, and the entity property ?l, p?
?
Lm?Pmthat assumes that value v ?
Vm, respectively.
Asystem model (SM) is a tuple m representing theimplemented architecture.m = ?Lm, Am, Pm, Vm,methods, props, values?Analogously to interpretation functions in logicand natural language semantics, we assume herean implementation function, denoted [[.
]], whichmaps each formal entity in the LSC semantic rep-resentation to its instantiation in the code-base.Using this function we define a notion of ground-ing that captures the fact that a certain code-basepermits the execution of a given LSC c.Definition 3(a): (Grounding) LetM be the setof system models and let C be the set of LSCcharts.
We say that m grounds c = ?E,<?, andwrite m .
c, if ?e ?
E : m .
e, where:?
if e ?
Eactivethenm .
e?
[[l1]], [[l2]] ?
L &[[a]] ?
methods([[l2]]) &?i : ?l : p : v?i?
[[l]] ?
Lm&[[p]] ?props[[l]]&v ?
values([[l]], [[p]])?
if e ?
Econdthenm .
e?
?i : ?l : p : v?i?
[[l]] ?
Lm&[[p]] ?props[[l]]&v ?
values([[l]], [[p]])?
if e = ?#, es, ?Ec, <?
?
Ecomplexthenm .
e?m .
es& ?e??
Ec: m .
e?We have thus far defined how the semantics ofa single LSC can be grounded in a single SM.
Inthe real world, however, a requirements documenttypically contains multiple different requirements,but it is interpreted as a complete whole.
The de-sired SM is then one that represents a single do-main shared by all the specified requirements.
Letus then assume a document d = d1, ..., dncon-taining n requirements, where ?i : di?
Lreq, and1299let unionsq be a unification operation that returns the for-mal unification of two SMs if such exists, and anempty SM otherwise.
We define a discourse in-terpretation function sem(d) that returns a singleSM for the entire document, where different men-tions across sentences may share the same refer-ence.
The discourse interpretation function semcan be as simple as unifying all individual SMsfor di, and asserting that all elements that have thesame name in different SMs refer to a single ele-ment in the overall SM.
Or, it can be as complex astaking into account synonyms (?clicks the button?and ?presses the button?
), anaphora (?when theuser clicks the button, it changes colour?
), bind-ing (?when the user clicks any button, this buttonis highlighted?
), and so on.
We can now define thegrounding of an entire requirements document.Definition 3(b): (Grounding) Let d = d1...dnbe a requirements document and let m = m1...mnbe a sequence of system models.
M = ?m,unionsq?
isa sequence of models and a unification operation,and M .
sem(d) if and only if ?i : mi.
sem(di)and ((m1unionsqm2).... unionsqmn) .
sem(d1, ...., dn).In this work we assume that sem(d) is a simplediscourse interpretation function, where entities,methods, properties, etc.
that are referred to usingthe same name in different local SMs refer to a sin-gle element in the overall code-base.
This simpleassumption already carries a substantial amount ofdisambiguating information concerning individualrequirements.
For example, assume that we haveseen a ?click?
method over a ?button?
object insentence i.
This may help us disambiguate futureattachment ambiguity, favoring structures wherea ?button?
is attached to ?click?
over other at-tachment alternatives.
Our goal is then to modeldiscourse-level context for supporting the accuratesemantic analysis of individual requirements.4 Probabilistic ModelingIn this section we set out to explicitly modelthe requirement?s context, formally captured as adocument-level SM, in order to support the accu-rate disambiguation of the requirements?
content.We first specify our probabilistic content model,a sentence-level model which is based on a prob-abilistic grammar augmented with compositionalsemantic rules.
We then specify our probabilisticcontext model, a document-level sequence modelthat takes into account the content as well as therelation between SMs at different time points.4.1 Sentence-Based ModelingThe task of our sentence-based model is to learna function that maps each requirement sentenceto its correct LSC diagram and SM snapshot.In a nutshell, we do this via a (partially lexi-calized) probabilistic context-free grammar aug-mented with a semantic interpretation function.More formally, given a discourse D = d1...dnwe think of each dias having been generated bya probabilistic context-free grammar (PCFG) G.The syntactic analysis of dimay be ambiguous,so we first implement a syntactic analysis functionsyn : Lreq?
Trequsing a probabilistic modelthat selects the most likely syntax tree t of eachd individually.
We can simplify syn(d), with dconstant with respect to the maximization:syn(d) = argmaxt?TreqP (t|d)= argmaxt?TreqP (t,d)p(d)= argmaxt?TreqP (t, d)= argmaxt?
{t|t?Treq,yield(t)=d}P (t)Because of the context-freeness assumption, itholds that P (t) =?r?der(t)P (r), where der(t)returns the rules that derive t. The resulting proba-bility distribution P : Treq?
[0, 1] defines a prob-abilistic language model over all requirements d ?Lreq, i.e.,?d?Lreq?t?Treq,yield(t)=dP (t) = 1.We assume a function sem : T ?
C mappingsyntactic parse trees to semantic constructs in theLSC language.
Syntactic parse trees are complexentities, assigning structures to the flat sequencesof words.
The principle of compositionality as-serts that the meaning of a complex syntactic en-tity is a function of the meaning of its parts andtheir mode of combination.
Here, the semantics ofa tree t ?
Treqis derived compositionally from theinterpretation of the rules in the grammar G. Weoverload the sem notation to define sem : R ?
Cas a function assigning rules to LSC constructs(events or parts of events),4with??
merging theresulting sets of events.
Our sentence-based com-positional semantics is summarized as:sem(u) = sem(syn(u)) = sem(r1?
... ?
rn) =sem(r1)??...?
?sem(rn) = c1??...?
?cn= c4Here, it suffices to say that sem maps edges in thesyntax tree to functions in the API of an existing LSCeditor.
For example: sem(NP ?
DET NN) =fCreateObject(DET.sem,NN.sem).
We specify thefunction sem in the supplementary materials.
The code ofsem is available as part of PlayGo (www.playgo.co).1300For a single chart c, one can easily construct animplementation for every entity, action and prop-erty in the chart.
Then, by design, we get anSM m such that m .
c. To construct the SM ofthe entire discourse in the sentence-based modelwe simply return f(d1, ..., dn) = unionsqni=1miwhere?i : mi.
sem(syn(di)) and unionsq unifies differentmentions of the same string to a single element.4.2 Discourse-Based ModelingWe assume a given document D ?
D and aim tofind the most probable system modelM ?M thatsatisfies the requirements.
We assume that M re-flects a single domain that the stakeholders have inmind, and we are provided with an ambiguous nat-ural language evidence, an elicited discourseD, inwhich they convey it.
We instantiate this view as anoisy channel model (Shannon, 1948), which pro-vides the foundation for many NLP applications,such as speech recognition (Bahl et al., 1983) andmachine translation (Brown et al., 1993).According to the noisy channel model, when asignal is received it does not uniquely identify themessage being sent.
A probabilistic model is thenused to decode the original message.
In our case,the signal is the discourse and the message is theoverall system model.
In formal terms, we want tofind a model M that maximises the following:f(D) = argmaxM?MP (M |D)We can simplify further, using Bayes law, whereD is constant with respect to the maximisation.f(D) = argmaxM?MP (M |D)= argmaxM?MP (D|M)?P (M)P (D)= argmaxM?MP (D|M)?
P (M)We would thus like to estimate two types of prob-ability distributions, P (M) over the source andP (D|M) over the channel.BothM andD are structured objects with com-plex internal structure.
In order to assign prob-abilities to objects involving such complex struc-tures it is customary to break them down into sim-pler, more basic, events.
We know that D =d1, d2, ..., dnis composed of n individual sen-tences, each representing a certain aspect of themodel M .
We assume a sequence of snapshots ofM that correspond to the timestamps 1...n, that is:m1,m2, ...,mn?
M where ?i : mi.
sem(di).The complete SM is given by the union of thedifferent snapshots reflected in different require-ments, i.e., M =?imi.
We then rephrase:P (M) = P (m1, ...,mn)P (D|M) = P (d1, ...., dn|m1, ...,mn)These events may be seen as points in a high di-mensional space.
In actuality, they are too com-plex and would be too hard to estimate directly.We then define two independence assumptions.First, we assume that a system model snapshot attime i depends only on k previous snapshots (astationary distribution).
Secondly, we assume thateach sentence i depends only on the SM snapshotat time i.
We now get:P (m1...mn) ?
?iP (mi|mi?1...mi?k)P (d1...dn|m1...mn) ?
?iP (di|mi)Furthermore, assuming bi-gram transitions, ourobjective function is now represented as follows:f(D) = argmaxM?Mn?i=1P (mi|mi?1)P (di|mi)Note that m0may be empty if the system is im-plemented from scratch, and non-empty if the re-quirements assume an existing code-base, whichmakes p(m1|m0) a non-trivial transition.4.3 Training and DecodingOur model is in essence a Hidden Markov Modelin which states capture SM snapshots, state-transition probabilities model transitions betweenSM snapshots, and emission probabilities modelthe verbal description of each state.
To implementthis, we need to implement a decoding algorithmthat searches through all possible state sequences,and a training algorithm that can automaticallylearn the values of the still rather complex param-eters P (mi|mi?1), P (di|mi) from data.f(D) = argmaxM?M?
??
?decodingn?i=1P (mi|mi?1)P (di|mi)?
??
?trainingTraining: We assume a supervised training set-ting in which we are given a set of examples anno-tated by a human expert.
For instance, these canbe requirements an analyst has formulated and en-coded using an LSC editor, while manually pro-viding disambiguating information.
We are pro-vided with a set of pairs {Di,Mi}ni=1containing ndocuments, where each of the pairs in i = 1..n is a1301tuple set {dij, tij, cij,mij}nij=1.
For all i, j, it holdsthat tij= syn(dij), cij= sem(tij), and mij.sem(syn(dij)).
The union of the niSM snapshotsyields the entire model unionsqjmij= Mi, that satisfiesthe set of requirements Mi.
sem(di1, ..., dini).
(i) Emission Parameters Our emission parame-ters P (di|mi) represent the probability of a verbaldescription of a requirement given an SM snap-shot which grounds the semantics of the descrip-tion.
A single SM may result from different syn-tactic derivations.
We calculate this probabilityby marginalizing over the syntactic trees that aregrounded in the same SM snapshot.P (d,m)P (m)=?t?
{t|yield(t)=d,m.sem(t)}P (t)?t?
{t|t?Treq,m.sem(t)}P (t)The probability of P (t) is estimated using a tree-bank PCFG (Charniak, 1996), based on all pairs?dij, tij?
in the annotated corpus.
We estimaterule probabilities using maximum-likelihood es-timates, and use simple smoothing for unknownlexical items, using rare-words distributions.
(ii) Transition Parameters Our transition pa-rameters P (mi|mi?1) represent the amount ofoverlap between the SM snapshots.
We look at thecurrent and the previous system model, and aimto estimate how likely the current SM is given theprevious one.
There are different assumptions thatmay underly this probability distribution, reflect-ing different principles of human communication.We first define a generic estimator as follows:?P (mi|mj) =gap(mi,mj)?mjgap(mi,mj)where gap(.)
quantifies the information sharingbetween SM snapshots.
Regardless of the im-plementation of gap, it can be easily shown that?P is a conditional probability distribution where?P : M ?
M ?
[0, 1] and, for all mi,mj, :?mj?P (mi|mj) = 1.
(For efficiency reasons, weconsiderM to be a restricted universe that is con-sidered be the decoder, as specified shortly.
)We define different gap implementations, re-flecting different assumptions about the discourse.Our first assumption here is that different SMsnapshots refer to the same conceptual world, sothere should be a large overlap between them.
Wecall this the max-overlap assumption.
A secondassumption is that, in collaborative communica-tion, a new requirement will only be stated if itTransition: gap(mcurr,mprev)max-overlap|set(mcurr)?set(mprev)||set(mcurr)|max-expansion 1?|set(mcurr)?set(mprev)||set(mprev)?set(mcurr)|min-distance 1?ted(mprev,mcurr)|set(mprev)|+|set(mcurr)|Table 1: Quantifying the gap between snapshots.set(mi) is a set of nodes marked by path to root.provides new information, akin to Grice (1975).This is the max-expansion assumption.
An addi-tional assumption prefers ?easy?
transitions over?hard?
ones, this is the min-distance assumption.The different gap calculations are listed in Table 1.Decoding An input document contains n re-quirements.
Our decoding algorithm considers theN-best syntactic analyses for each requirement.
Ateach time step i = 1...n we assume N, states rep-resenting the semantics of the N best syntax trees,retrieved via a CKY chart parser.
Thus, settingN = 1 is equal to a sentence-based model, inwhich for each sentence we simply select the mostlikely tree according to a probabilistic grammar,and construct a semantic representation for it.For each document of length n, we assume thatour entire universe of system models M is com-posed of N ?
n SM snapshots, reflecting the Nmost-likely analyses of n sentences, as providedby the probabilistic syntactic model.
(As shall beseen shortly, even with this restricted5universe ap-proximating M, our discourse-based model pro-vides substantial improvements over a sentence-based model).Our discourse-based model is an HMM whereeach requirement is an observed signal, and eachi = 1..N is a state representing the SM thatgrounds the i th best tree.
Because of theMarkov independence assumption our setup satis-fies the optimal subproblem and overlapping prob-lem properties, and we can use efficient viterbi de-coding to exhaustively search through the differ-ent state sequences, and find the most probablesequence that has generated the sequence of re-quirements according to our discourse-based prob-abilistic model.5This restriction is akin to pseudo-likelihood estimation,as in Arnold and Strauss (1991).
In pseudo-likelihood estima-tion, instead of normalizing over the entire set of elements,one uses a subset that reflects only the possible outcomes.Here, instead of summing SM probabilities over all possiblesentences in the language, we sum up the SM analyses of thesentences observed in the document only.
This estimationcould also be addressed via, e.g., sampling methods.1302The overall complexity decoding a documentwith n sentences of which max length is l, using agrammar G of size |G| and a fixed N , is given by:O(n?
l3?
|G|3+ l2?N2?
n+ n3?N2)We can break this expression down as follows: (i)In O(n ?
l3?
|G|3) we generate N best trees foreach one of the n requirements, using a CKY chart(Younger, 1967).
(ii) In O(l2?N2?n) we createthe universe M based on the N best trees of then requirements, and calculate N ?
N transitions.
(iii) InO((N?n)2?n) = O(N2?n3) we decodethe n?N grid using Viterbi (1967) decoding.5 ExperimentsGoal.
We set out to evaluate the accuracy of a se-mantic parser for requirements documents, in thetwo modes of analysis presented above.
Our eval-uation methodology is as standardly assumed inmachine learning and NLP: given a set of anno-tated examples ?
that is, given a set of require-ments documents, where each requirement is an-notated with its correct LSC representation andeach document is associated with a complete SM?
we partition this set into a training set and a testset that are disjoint.
We train our statistical modelon the examples in the training set and automati-cally analyze the requirements in the test set.
Wethen compare the predicted semantic analyses ofthe test set with the human-annotated (henceforth,gold) semantic analyses of this test set, and empir-ically quantify our prediction accuracy.Metrics.
Our semantic LSC objects have theform of a tree (reflecting the sequence of nestedevents in our scenarios).
Therefore, we can usestandard tree evaluation metrics, such as ParseE-val (Black et al., 1992), to evaluate the accuracyof the prediction.
Overall, we define three metricsto evaluate the accuracy of the LSC trees:POS: the POS metric is the percentage ofpart-of-speech tags predicted correctly.LSC-F1: F1 is the harmonic means of theprecision and recall of the predicted tree.LSC-EM: EM is 1 if the predicted tree is anexact match to the gold tree, and 0 otherwise.In the case of SM trees, as opposed to the LSCtrees, we cannot assume identity of the yield be-tween the gold and parse trees for the same sen-System #Scenarios avg sentence lengthPhone 21 24.33WristWatch 15 29.8Chess 18 15.83Baby Monitor 14 20Total 68 22.395Table 2: Seed Gold-Annotated RequirementsN=1 POS LSC-F1 LSC-EM SM-TED SM-EMGen-Only 85.52 84.40 9.52 84.25 9.52Gen+Seed 91.59 88.05 14.29 85.17 14.29Table 3: Sentence-Based modeling: Accuracy re-sults on the Phone development set.tence,6so we cannot use ParseEval.
Therefore, weimplement a distance-based metrics in the spirit ofTsarfaty et al.
(2012).
Then, to evaluate the accu-racy of the SM, we use two kinds of scores:SM-TED: TED is the normalized edit dis-tance between the predicted and gold SMtrees, subtracted from a unity.SM-EM: EM is 1 if the predicted SM is anexact match with the gold SM, 0 otherwise.Data.
We have a small seed of correctly anno-tated requirements-specification case studies thatdescribe simple reactive systems in the LSC lan-guage.
Each document contains a sequence ofrequirements, each of which is annotated withthe correct LSC diagram.
The entire program isgrounded in a java implementation.
As trainingdata, we use the case studies provided by Gordonand Harel (2009).
Table 2 lists the case studies andbasic statistics concerning these data.As our annotated seed is quite small, it is hard togeneralize from it to unseen examples.
In particu-lar, we are not guaranteed to have observed all pos-sible structures that are theoretically permitted bythe assumed grammar.
To cope with this, we cre-ate a synthetic set of examples using the grammarof Gordon and Harel (2009) in generation mode,and randomly generate trees t ?
Treq.The grammar we use to generate the syntheticexamples clearly over-generates.
That is to say,it creates many trees that do not have a sound in-terpretation.
In fact, only 3000 our of 10000 gen-erated examples turn out to have a sound seman-tic interpretation grounded in an SM.
Nonetheless,these data allow us to smooth the syntactic distri-butions that are observed in the seed, and increasethe coverage of the grammar learned from it.6This is because the LSC trees are predicted bottom upand the SM trees are predicted top-down.1303Results.
Table 3 presents the results for pars-ing the Phone document, our development set,with the sentence-based model, varying the train-ing data.
We see that despite the small size of theseed, adding it to our set if synthetics examplessubstantially improves results over a model trainedon synthetic examples only.In our next experiment, we provide empiricalupper-bounds and lower-bounds for the discourse-based model.
Table 4 presents the results ofthe discourse-based model for N > 1 on thePhone example.
Gen-Only presents the results ofthe discourse-based model with a PCFG learnedfrom synthetic trees only, incorporating transitionsobeying the max-overlap assumption.
Alreadyhere, we see a mild improvement for N > 1 rel-ative to the N = 1 results, indicating that even aweak signal such as the overlap between neighbor-ing sentences already improves sentence disam-biguation in context.
We next present the results ofan Oracle experiment, where every requirement isassigned the highest scoring tree in terms of LSC-F1 with respect to the gold tree, keeping the sametransitions.
Again we see that results improve withN , indicating that the syntactic model alone doesnot provide optimal disambiguation.
These re-sults provides an upper bound on the parser perfor-mance for each N .
Gen+Seed presents results ofthe discourse-based model where the PCFG inter-polates the seed set and the synthetic train set, withmax-overlap transitions.
Here, we see larger im-provements over the synthetic-only PCFG.
That is,modeling grammaticality of individual sentencesimproves the interpretation of the document.Next we compare the performance for differ-ent implementations of the gap(mi,mj) function.We estimate probability distributions that reflecteach of the assumptions we discussed, and addan additional method called hybrid, in which weinterpolate the max-expansion and max-overlapestimates (equal weights).
In Table 5, the trendfrom the previous experiment persists.
Notably,the hybrid model provides a larger error reduc-tion than its components used separately, indicat-ing that in order to capture discourse context wemay need to balance possibly conflicting factors.In no emissions we rely solely on the probabilityof state transitions, and again increasing N leadsto improvement.
This result confirms that con-text is indispensable for sentence interpretation ?even when probabilities for the sentence?s seman-System N=2 4 8 16 32 64 128Gen-OnlyPOS 85.52 86.30 87.67 88.45 88.85 88.85 88.85LSC-F1 84.40 85.35 86.31 87.51 88.81 89.30 89.51LSC-EM 9.52 9.52 14.29 14.29 14.29 14.29 14.29SM-TED 84.25 85.94 89.14 91.90 92.81 93.31 92.70SM-EM 9.52 19.05 33.33 33.33 33.33 38.10 33.33Gen+SeedPOS 91.78 92.95 93.54 93.35 94.32 94.52 93.93LSC-F1 88.11 90.18 91.00 90.99 91.81 92.09 91.73LSC-EM 19.05 38.10 42.86 42.86 42.86 42.86 42.86SM-TED 85.49 90.78 93.59 93.02 94.81 95.69 93.76SM-EM 19.05 38.10 52.38 52.38 52.38 52.38 52.38OraclePOS 91.98 93.54 94.91 95.30 96.09 96.67 96.87LSC-F1 88.73 91.33 93.19 94.39 95.11 95.91 96.70LSC-EM 23.81 42.86 61.90 61.90 66.67 76.19 76.19SM-TED 86.54 91.28 94.28 94.88 96.24 97.51 98.80SM-EM 23.81 42.86 66.67 71.43 76.19 76.19 76.19Table 4: Discourse-Based Modeling: Accuracy re-sults on the Phone dev set.
The Oracle selects thehighest scoring LSC tree among the N-candidates,providing an upper bound on accuracy.
Gen-Onlyselects the most probable tree, relying on syntheticexamples only, providing a lower bound.tics (content) are entirely absent.We finally perform a cross-fold experiment inwhich we leave one document out as a test setand take the rest as our seed.
The results are pro-vided in Table 6.
The discourse-based model out-performs the sentence-based model N = 1 in allcases.
Moreover, the drop in N = 128 for Phoneseems incidental to this set, and the other caseslevel off beforehand.
Despite our small seed, thepersistent improvement on all metrics is consistentwith our hypothesis that modeling the interpreta-tion process within the discourse has substantialbenefits for automatic understanding of the text.6 Applications and DiscussionThe statistical models we present here are ap-plied in the context of PlayGo,7a comprehensivetool for behavioral, scenario-based, programming.PlayGo now provides two modes of playing-innatural language requirements: interactive play-in,where a user manually disambiguates the analysesof the requirements (Gordon and Harel, 2009), andstatistical play-in, where disambiguation decisionsare taken using our discourse-based model.The fragment of English we use is very ex-pressive.
It covers not only entities and predi-cates, but also temporal and aspectual information,modalities, and program flow.
Beyond that, we as-sume an open-ended lexicon.
Overall, we are not7www.playgo.co.1304Transitions N=2 4 8 16 32 64 128Min DistPOS 91.98 92.76 93.54 93.35 94.32 94.52 93.93LSC-F1 88.39 89.77 91.00 90.99 91.81 92.09 91.73LSC-EM 23.81 42.86 47.62 47.62 47.62 47.62 47.62SM-TED 86.54 91.71 94.38 93.81 95.57 96.43 94.53SM-EM 23.81 42.86 57.14 57.14 57.14 57.14 57.14Max OverlapPOS 91.78 92.95 93.54 93.35 94.32 94.52 93.93LSC-F1 88.11 90.18 91.00 90.99 91.81 92.09 91.73LSC-EM 19.05 38.10 42.86 42.86 42.86 42.86 42.86SM-TED 85.49 90.78 93.59 93.02 94.81 95.69 93.76SM-EM 19.05 38.10 52.38 52.38 52.38 52.38 52.38Max ExpandPOS 91.98 92.76 93.74 93.54 94.32 94.52 93.93LSC-F1 88.39 89.71 91.00 90.99 91.68 91.96 91.60LSC-EM 23.81 42.86 47.62 47.62 47.62 47.62 47.62SM-TED 86.54 91.93 93.75 93.18 94.79 95.66 93.75SM-EM 23.81 42.86 57.14 57.14 57.14 57.14 57.14HybridPOS 91.78 92.95 93.93 93.74 94.72 94.91 94.32LSC-F1 88.11 90.18 91.34 91.33 92.15 92.42 92.07LSC-EM 19.05 38.10 47.62 47.62 47.62 47.62 47.62SM-TED 85.49 90.78 93.66 93.09 94.87 95.75 93.83SM-EM 19.05 38.10 57.14 57.14 57.14 57.14 57.14No EmissionsPOS 91.78 91.98 92.37 92.37 92.17 92.76 93.15LSC-F1 88.11 88.79 89.12 89.12 89.39 89.67 89.89LSC-EM 19.05 19.05 23.81 23.81 23.81 23.81 23.81SM-TED 85.49 85.74 85.82 85.82 85.87 86.85 86.92SM-EM 19.05 19.05 23.81 23.81 23.81 23.81 23.81Table 5: Discourse-Based modeling: Experimentson the Phone development set.
Estimation proce-dure for transition probabilities.
All experimentsuse the Gen+Seed emission probablities.only translating English sentences into executableLSCs ?
we provide a fully generative model fortranslating a complete document (text) into a com-plete system model (code).This text-to-code problem may be thought of asa machine translation (MT) problem, where oneaims to translate sentences in English to the formallanguage of LSCs.
However, standard statisticalMT techniques rely on the assumption that textualrequirements and code are aligned at a sentencelevel.
Creating a formal model that aligns text andcode on a sentence-by-sentence basis is preciselyour technical contribution in Section 3.To our knowledge, modeling syntax and dis-course processing via a fully joint generativemodel, where a discourse level HMM is in-terleaved with PCFG sentence-based emissions,is novel.
By plugging in different models forp(d|m), different languages may be parsed.
Thismethod may further be utilized for relating contentand context in other tasks: parsing and document-level NER, parsing and document-level IE, etc.
Todo so, one only needs to redefine the PCFG (emis-sions) and state-overlap (transition) parameters, asappropriate for their data.88Our code, annotated data, four case studies, and the LSCData Set N=1 32 64 128Baby MonitorPOS 94.29 96.07 96.07 96.07LSC-F1 91.50 94.96 94.96 94.96LSC-EM 14.29 21.43 21.43 21.43SM-TED 88.63 91.11 91.11 91.11SM-EM 28.57 50.00 50.00 50.00ChessPOS 92.63 93.68 93.68 93.68LSC-F1 95.79 96.16 96.16 96.16LSC-EM 5.56 11.11 11.11 11.11SM-TED 94.90 97.10 97.10 97.10SM-EM 61.11 66.67 66.67 66.67PhonePOS 91.59 94.72 94.91 94.32LSC-F1 88.05 92.15 92.42 92.07LSC-EM 14.29 47.62 47.62 47.62SM-TED 85.17 94.87 95.75 93.83SM-EM 14.29 57.14 57.14 57.14WristWatchPOS 34.23 34.45 34.45 34.45LSC-F1 50.06 51.05 51.05 51.05LSC-EM 26.67 26.67 26.67 26.67SM-TED 71.15 72.73 72.73 72.73SM-EM 26.67 33.33 33.33 33.33Table 6: Cross-Fold Validation for N=1..128.Seed+Generated emissions, Hybrid transitions.7 ConclusionThe requirements understanding task presents anexciting challenge for CL/NLP.
We ought to au-tomatically discover the entities in the discourse,the actions they take, conditions, temporal con-straints, and execution modalities.
Furthermore, itrequires us to extract a single ontology that satis-fies all individual requirements.
The contributionsof this paper are three-fold: we formalize the text-to-code prediction task, propose a semantic rep-resentation with well-defined grounding, and em-pirically evaluate models for this prediction.
Weshow consistent improvement of discourse-basedover sentence-based models, in all case studies.In the future, we intend to extend this model forinterpreting requirements in un-restricted, or less-restricted, English, endowed with a more sophisti-cated discourse interpretation function.AcknowledgementsWe thank Shahar Maoz, Rami Marelly, YoavGoldberg and three anonymous reviewers fortheir insightful comments on an earlier draft.This research was supported by an AdvancedResearch Grant to D. Harel from the Euro-pean Research Council (ERC) under the Eu-ropean Community?s Seventh Framework Pro-gramme (FP7/2007-2013), and by a grant to D.Harel from the Israel Science Foundation (ISF).visual editor are available via http://wiki.weizmann.ac.il/playgo/index.php/Download_PlayGo.1305ReferencesB.
C. Arnold and D. Strauss.
1991.
PseudolikelihoodEstimation: Some Examples.
Sankhy?a: The IndianJournal of Statistics, Series B (1960-2002), 53(2).Y.
Artzi and L. Zettlemoyer.
2013.
Weakly super-vised learning of semantic parsers for mapping in-structions to actions.
TACL, 1:49?62.L.
R. Bahl, F. Jelinek, and R. L. Mercer.
1983.
Amaximum likelihood approach to continuous speechrecognition.
IEEE Trans.
Pattern Anal.
Mach.
In-tell., 5(2):179?190.E.
Black, J. D. Lafferty, and S. Roukos.
1992.
De-velopment and evaluation of a broad-coverage prob-abilistic grammar of English-language computermanuals.
In Proceedings of ACL, pages 185?192.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Comput.Linguist., 19(2):263?311, June.B.
Bryant and B.-S. Lee.
2002.
Two-level gram-mar as an object-oriented requirements specifica-tion language.
In Proceedings of the 35th AnnualHawaii International Conference on System Sci-ences (HICSS?02)-Volume 9 - Volume 9, HICSS ?02,pages 280?, Washington, DC, USA.
IEEE ComputerSociety.E.
Charniak.
1996.
Tree-bank grammars.
In Proceed-ings of the Thirteenth National Conference on Arti-ficial Intelligence, pages 1031?1036.W.
Damm and D. Harel.
2001.
LSCs: Breathing lifeinto message sequence charts.
Form.
Methods Syst.Des., 19(1):45?80, July.N.
Eitan, M. Gordon, D. Harel, A. Marron, andG.
Weiss.
2011.
On visualization and compre-hension of scenario-based programs.
In Proceed-ings of the 2011 IEEE 19th International Conferenceon Program Comprehension, ICPC ?11, pages 189?192, Washington, DC, USA.
IEEE Computer Soci-ety.N.
E. Fuchs and R. Schwitter.
1995.
Attempto: Con-trolled natural language for requirements specifica-tions.
In Markus P. J. Fromherz, Marc Kirschen-baum, and Anthony J. Kusalik, editors, LPE.M.
Gordon and D. Harel.
2009.
Generating executablescenarios from natural language.
In Proceedings ofthe 10th International Conference on ComputationalLinguistics and Intelligent Text Processing, CICLing?09, pages 456?467, Berlin, Heidelberg.
Springer-Verlag.H.
P. Grice.
1975.
Logic and conversation.
In P. Coleand J. L. Morgan, editors, Syntax and Semantics:Vol.
3: Speech Acts, pages 41?58.
Academic Press,San Diego, CA.D.
Harel and M. Gordon.
2009.
On teaching visualformalisms.
IEEE Softw., 26(3):87?95, May.D.
Harel and S. Maoz.
2006.
Assert and negate revis-ited: Modal semantics for UML sequence diagrams.In Proceedings of the 2006 International Workshopon Scenarios and State Machines: Models, Algo-rithms, and Tools, SCESM ?06, pages 13?20, NewYork, NY, USA.
ACM.D.
Harel and R. Marelly.
2003.
Come, Let?s Play:Scenario-Based Programming Using LSCs and thePlay-Engine.
Springer-Verlag New York, Inc., Se-caucus, NJ, USA.D.
Harel, H. Kugler, R. Marelly, and A. Pnueli.
2002.Smart play-out of behavioral requirements.
In Pro-ceedings of the 4th International Conference on For-mal Methods in Computer-Aided Design, FMCAD?02, pages 378?398, London, UK.
Springer-Verlag.D.
Harel, A. Marron, and G. Weiss.
2012.
Behavioralprogramming.
Commun.
ACM, 55(7):90?100, July.D.
Harel, A. Kantor, G. Katz, A. Marron, L. Mizrahi,and G. Weiss.
2013.
On composing and provingthe correctness of reactive behavior.
In EmbeddedSoftware (EMSOFT), 2013 Proceedings of the Inter-national Conference on, pages 1?10, Sept.D.
Harel.
2001.
From play-in scenarios to code: Anachievable dream.
Computer, 34(1):53?60, January.H.
Kugler, D. Harel, A. Pnueli, Y. Lu, and Y. Bon-temps.
2005.
Temporal logic for scenario-basedspecifications.
In Proceedings of the 11th In-ternational Conference on Tools and Algorithmsfor the Construction and Analysis of Systems,TACAS?05, pages 445?460, Berlin, Heidelberg.Springer-Verlag.T.
Kuhn.
2014.
A survey and classification of con-trolled natural languages.
Computational Linguis-tics, 40(1):121?170.T.
Lei, F. Long, R. Barzilay, and M. C. Rinard.
2013.From natural language specifications to program in-put parsers.
In ACL (1), pages 1294?1303.P.
Liang and C. Potts.
2014.
Bringing machine learn-ing and compositional semantics together.
AnnualReviews of Linguistics (submitted), 0.P.
Liang, M. I. Jordan, and D. Klein.
2011.
Learn-ing dependency-based compositional semantics.
InAssociation for Computational Linguistics (ACL),pages 590?599.T.
Parsons.
1990.
Events in the Semantics of English:A study in subatomic semantics.
MIT Press, Cam-bridge, MA.C.
Shannon.
1948.
A mathematical theory of com-munication.
Bell System Technical Journal, 27:379?423, 623?656, July, October.1306R.
Tsarfaty, J. Nivre, and E. Andersson.
2012.
Cross-framework evaluation for statistical parsing.
InW.
Daelemans, M. Lapata, and L. M`arquez, editors,Proceedings of EACL, pages 44?54.
The Associa-tion for Computer Linguistics.A.
Viterbi.
1967.
Error bounds for convolutional codesand an asymptotically optimum decoding algorithm.IEEE Trans.
Inf.
Theor.D.
H. Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information andControl, 10(2):189?208.L.
S. Zettlemoyer and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In UAI,pages 658?666.
AUAI Press.1307
