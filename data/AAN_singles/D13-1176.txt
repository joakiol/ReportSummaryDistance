Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700?1709,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsRecurrent Continuous Translation ModelsNal Kalchbrenner Phil BlunsomDepartment of Computer ScienceUniversity of Oxford{nal.kalchbrenner,phil.blunsom}@cs.ox.ac.ukAbstractWe introduce a class of probabilistic con-tinuous translation models called Recur-rent Continuous Translation Models that arepurely based on continuous representationsfor words, phrases and sentences and do notrely on alignments or phrasal translation units.The models have a generation and a condi-tioning aspect.
The generation of the transla-tion is modelled with a target Recurrent Lan-guage Model, whereas the conditioning on thesource sentence is modelled with a Convolu-tional Sentence Model.
Through various ex-periments, we show first that our models ob-tain a perplexity with respect to gold transla-tions that is > 43% lower than that of state-of-the-art alignment-based translation models.Secondly, we show that they are remarkablysensitive to the word order, syntax, and mean-ing of the source sentence despite lackingalignments.
Finally we show that they match astate-of-the-art system when rescoring n-bestlists of translations.1 IntroductionIn most statistical approaches to machine transla-tion the basic units of translation are phrases that arecomposed of one or more words.
A crucial com-ponent of translation systems are models that esti-mate translation probabilities for pairs of phrases,one phrase being from the source language and theother from the target language.
Such models countphrase pairs and their occurrences as distinct if thesurface forms of the phrases are distinct.
Althoughdistinct phrase pairs often share significant similari-ties, linguistic or otherwise, they do not share statis-tical weight in the models?
estimation of their trans-lation probabilities.
Besides ignoring the similar-ity of phrase pairs, this leads to general sparsity is-sues.
The estimation is sparse or skewed for thelarge number of rare or unseen phrase pairs, whichgrows exponentially in the length of the phrases, andthe generalisation to other domains is often limited.Continuous representations have shown promiseat tackling these issues.
Continuous representationsfor words are able to capture their morphological,syntactic and semantic similarity (Collobert and We-ston, 2008).
They have been applied in continu-ous language models demonstrating the ability toovercome sparsity issues and to achieve state-of-the-art performance (Bengio et al 2003; Mikolov etal., 2010).
Word representations have also showna marked sensitivity to conditioning information(Mikolov and Zweig, 2012).
Continuous repre-sentations for characters have been deployed incharacter-level language models demonstrating no-table language generation capabilities (Sutskever etal., 2011).
Continuous representations have alsobeen constructed for phrases and sentences.
The rep-resentations are able to carry similarity and task de-pendent information, e.g.
sentiment, paraphrase ordialogue labels, significantly beyond the word leveland to accurately predict labels for a highly diverserange of unseen phrases and sentences (Grefenstetteet al 2011; Socher et al 2011; Socher et al 2012;Hermann and Blunsom, 2013; Kalchbrenner andBlunsom, 2013).Phrase-based continuous translation models werefirst proposed in (Schwenk et al 2006) and re-1700cently further developed in (Schwenk, 2012; Le etal., 2012).
The models incorporate a principled wayof estimating translation probabilities that robustlyextends to rare and unseen phrases.
They achievesignificant Bleu score improvements and yield se-mantically more suggestive translations.
Althoughwide-reaching in their scope, these models are lim-ited to fixed-size source and target phrases and sim-plify the dependencies between the target words tak-ing into account restricted target language modellinginformation.We describe a class of continuous translationmodels called Recurrent Continuous TranslationModels (RCTM) that map without loss of generalitya sentence from the source language to a probabil-ity distribution over the sentences in the target lan-guage.
We define two specific RCTM architectures.Both models adopt a recurrent language model forthe generation of the target translation (Mikolov etal., 2010).
In contrast to other n-gram approaches,the recurrent language model makes no Markov as-sumptions about the dependencies of the words inthe target sentence.The two RCTMs differ in the way they condi-tion the target language model on the source sen-tence.
The first RCTM uses the convolutional sen-tence model (Kalchbrenner and Blunsom, 2013) totransform the source word representations into a rep-resentation for the source sentence.
The source sen-tence representation in turn constraints the genera-tion of each target word.
The second RCTM intro-duces an intermediate representation.
It uses a trun-cated variant of the convolutional sentence model tofirst transform the source word representations intorepresentations for the target words; the latter thenconstrain the generation of the target sentence.
Inboth cases, the convolutional layers are used to gen-erate combined representations for the phrases in asentence from the representations of the words in thesentence.An advantage of RCTMs is the lack of latentalignment segmentations and the sparsity associatedwith them.
Connections between source and targetwords, phrases and sentences are learnt only implic-itly as mappings between their continuous represen-tations.
As we see in Sect.
5, these mappings of-ten carry remarkably precise morphological, syntac-tic and semantic information.
Another advantage isthat the probability of a translation under the modelsis efficiently computable requiring a small numberof matrix-vector products that is linear in the lengthof the source and the target sentence.
Further, trans-lations can be generated directly from the probabil-ity distribution of the RCTM without any externalresources.We evaluate the performance of the models in fourexperiments.
Since the translation probabilities ofthe RCTMs are tractable, we can measure the per-plexity of the models with respect to the referencetranslations.
The perplexity of the models is signifi-cantly lower than that of IBM Model 1 and is> 43%lower than the perplexity of a state-of-the-art variantof the IBM Model 2 (Brown et al 1993; Dyer etal., 2013).
The second and third experiments aim toshow the sensitivity of the output of the RCTM IIto the linguistic information in the source sentence.The second experiment shows that under a randompermutation of the words in the source sentences,the perplexity of the model with respect to the refer-ence translations becomes significantly worse, sug-gesting that the model is highly sensitive to wordposition and order.
The third experiment inspectsthe translations generated by the RCTM II.
Thegenerated translations demonstrate remarkable mor-phological, syntactic and semantic agreement withthe source sentence.
Finally, we test the RCTMson the task of rescoring n-best lists of translations.The performance of the RCTM probabilities joinedwith a single word penalty feature matches the per-formance of the state-of-the-art translation systemcdec that makes use of twelve features includingfive alignment-based translation models (Dyer et al2010).We proceed as follows.
We begin in Sect.
2 bydescribing the general modelling framework under-lying the RCTMs.
In Sect.
3 we describe the RCTMI and in Sect.
4 the RCTM II.
Section 5 is dedicatedto the four experiments and we conclude in Sect.
6.12 FrameworkWe begin by describing the modelling frameworkunderlying RCTMs.
An RCTM estimates the proba-bility P (f|e) of a target sentence f = f1, ..., fm beinga translation of a source sentence e = e1, ..., ek.
Let1Code and models available at nal.co1701us denote by fi:j the substring of words fi, ..., fj .
Us-ing the following identity,P (f|e) =m?i=1P (fi|f1:i?1, e) (1)an RCTM estimates P (f|e) by directly computingfor each target position i the conditional probabilityP (fi|f1:i?1, e) of the target word fi occurring in thetranslation at position i, given the preceding targetwords f1:i?1 and the source sentence e. We see thatan RCTM is sensitive not just to the source sentencee but also to the preceding words f1:i?1 in the targetsentence; by doing so it incorporates a model of thetarget language itself.To model the conditional probability P (f|e), anRCTM comprises both a generative architecture forthe target sentence and an architecture for condition-ing the latter on the source sentence.
To fully cap-ture Eq.
1, we model the generative architecture witha recurrent language model (RLM) based on a re-current neural network (Mikolov et al 2010).
Theprediction of the i-th word fi in a RLM depends onall the preceding words f1:i?1 in the target sentenceensuring that conditional independence assumptionsare not introduced in Eq.
1.
Although the predic-tion is most strongly influenced by words closelypreceding fi, long-range dependencies from acrossthe whole sentence can also be exhibited.
The con-ditioning architectures are model specific and aretreated in Sect.
3-4.
Both the generative and con-ditioning aspects of the models deploy continuousrepresentations for the constituents and are trainedas a single joint architecture.
Given the modellingframework underlying RCTMs, we now proceed todescribe in detail the recurrent language model un-derlying the generative aspect.2.1 Recurrent Language ModelA RLM models the probability P (f) that the se-quence of words f occurs in a given language.
Letf = f1, ..., fm be a sequence of m words, e.g.
a sen-tence in the target language.
Analogously to Eq.
1,using the identity,P (f) =m?i=1P (fi|f1:i?1) (2)the model explicitly computes without simpli-fying assumptions the conditional distributionsRI OfiP(f   )i+1hRfi-1P(f )ifi+1P(f   )i+2OIh h hi-1 i i+1Figure 1: A RLM (left) and its unravelling to depth 3(right).
The recurrent transformation is applied to the hid-den layer hi?1 and the result is summed to the represen-tation for the current word fi.
After a non-linear transfor-mation, a probability distribution over the next word fi+1is predicted.P (fi|f1:i?1).
The architecture of a RLM comprisesa vocabulary V that contains the words fi of thelanguage as well as three transformations: an in-put vocabulary transformation I ?
Rq?|V |, a re-current transformation R ?
Rq?q and an outputvocabulary transformation O ?
R|V |?q.
For eachword fk ?
V , we indicate by i(fk) its index in Vand by v(fk) ?
R|V |?1 an all zero vector with onlyv(fk)i(fk) = 1.For a word fi, the result of I ?
v(fi) ?
Rq?1 isthe input continuous representation of fi.
The pa-rameter q governs the size of the word representa-tion.
The prediction proceeds by successively ap-plying the recurrent transformation R to the wordrepresentations and predicting the next word at eachstep.
In detail, the computation of each P (fi|f1:i?1)proceeds recursively.
For 1 < i < m,h1 = ?
(I ?
v(f1)) (3a)hi+1 = ?
(R ?
hi + I ?
v(fi+1)) (3b)oi+1 = O ?
hi (3c)and the conditional distribution is given by,P (fi = v|f1:i?1) =exp (oi,v)?Vv=1 exp(oi,v)(4)In Eq.
3, ?
is a nonlinear function such as tanh.
Biasvalues bh and bo are included in the computation.
Anillustration of the RLM is given in Fig.
1.The RLM is trained by backpropagation throughtime (Mikolov et al 2010).
The error in the pre-dicted distribution calculated at the output layer is1702backpropagated through the recurrent layers and cu-mulatively added to the errors of the previous predic-tions for a given number d of steps.
The procedure isequivalent to standard backpropagation over a RLMthat is unravelled to depth d as in Fig.
1.RCTMs may be thought of as RLMs, in whichthe predicted distributions for each word fi are con-ditioned on the source sentence e. We next definetwo conditioning architectures each giving rise to aspecific RCTM.3 Recurrent Continuous TranslationModel IThe RCTM I uses a convolutional sentence model(CSM) in the conditioning architecture.
The CSMcreates a representation for a sentence that is pro-gressively built up from representations of the n-grams in the sentence.
The CSM embodies a hierar-chical structure.
Although it does not make use of anexplicit parse tree, the operations that generate therepresentations act locally on small n-grams in thelower layers of the model and act increasingly moreglobally on the whole sentence in the upper layersof the model.
The lack of the need for a parse treeyields two central advantages over sentence modelsthat require it (Grefenstette et al 2011; Socher etal., 2012).
First, it makes the model robustly appli-cable to a large number of languages for which accu-rate parsers are not available.
Secondly, the transla-tion probability distribution over the target sentencesdoes not depend on the chosen parse tree.The RCTM I conditions the probability of eachtarget word fi on the continuous representation of thesource sentence e generated through the CSM.
Thisis accomplished by adding the sentence representa-tion to each hidden layer hi in the target recurrentlanguage model.
We next describe the procedure inmore detail, starting with the CSM itself.3.1 Convolutional Sentence ModelThe CSM models the continuous representation ofa sentence based on the continuous representationsof the words in the sentence.
Let e = e1...ek bea sentence in a language and let v(ei) ?
Rq?1 bethe continuous representation of the word ei.
LetEe ?
Rq?k be the sentence matrix for e defined by,Ee:,i = v(ei) (5)(K    M)*:,1M M M:,1:,2:,3the catsaton themateEeK2K3L3Ki:,1Ki:,2Ki:,3iFigure 2: A CSM for a six word source sentence e and thecomputed sentence representation e. K2,K3 are weightmatrices and L3 is a top weight matrix.
To the right, aninstance of a one-dimensional convolution between someweight matrix Ki and a generic matrix M that could forinstance correspond to Ee2.
The color coding of weightsindicates weight sharing.The main component of the architecture of the CSMis a sequence of weight matrices (Ki)2?i?r that cor-respond to the kernels or filters of the convolutionand can be thought of as learnt feature detectors.From the sentence matrix Ee the CSM computes acontinuous vector representation e ?
Rq?1 for thesentence e by applying a sequence of convolutionsto Ee whose weights are given by the weight matri-ces.
The weight matrices and the sequence of con-volutions are defined next.We denote by (Ki)2?i?r a sequence of weightmatrices where each Ki ?
Rq?i is a matrix of icolumns and r = d?2Ne, where N is the length ofthe longest source sentence in the training set.
Eachrow of Ki is a vector of i weights that is treated asthe kernel or filter of a one-dimensional convolution.Given for instance a matrix M ?
Rq?j where thenumber of columns j ?
i, each row of Ki can beconvolved with the corresponding row in M, result-ing in a matrix Ki ?M, where ?
indicates the con-volution operation and (Ki ?M) ?
Rq?(j?i+1).
Fori = 3, the value (Ki ?M):,a is computed by:Ki:,1M:,a+Ki:,2M:,a+1 +Ki:,3M:,a+2 (6)where  is component-wise vector product.
Ap-plying the convolution kernel Ki yields a matrix(Ki?M) that has i?1 columns less than the originalmatrix M.Given a source sentence of length k, the CSMconvolves successively with the sentence matrix Ee1703the sequence of weight matrices (Ki)2?i?r, one af-ter the other starting with K2 as follows:Ee1 = Ee (7a)Eei+1 = ?
(Ki+1 ?Eei ) (7b)After a few convolution operations, Eei is either avector in Rq?1, in which case we obtained the de-sired representation, or the number of columns inEei is smaller than the number i + 1 of columns inthe next weight matrix Ki+1.
In the latter case, weequally obtain a vector in Rq?1 by simply apply-ing a top weight matrix Lj that has the same num-ber of columns as Eei .
We thus obtain a sentencerepresentation e ?
Rq?1 for the source sentence e.Note that the convolution operations in Eq.
7b areinterleaved with non-linear functions ?.
Note alsothat, given the different levels at which the weightmatrices Ki and Li are applied, the top weightmatrix Lj comes from an additional sequence ofweight matrices (Li)2?i?r distinct from (Ki)2?i?r.Fig.
2 depicts an instance of the CSM and of a one-dimensional convolution.23.2 RCTM IAs defined in Sect.
2, the RCTM I models the condi-tional probability P (f|e) of a sentence f = f1, ..., fmin a target language F being the translation of a sen-tence e = e1, ..., ek in a source language E. Accord-ing to Eq.
1, the RCTM I explicitly computes theconditional distributions P (fi|f1:i?1, e).
The archi-tecture of the RCTM I comprises a source vocabu-lary V E and a target vocabulary V F, two sequencesof weight matrices (Ki)2?i?r and (Li)2?i?r thatare part of the constituent CSM, transformationsI ?
Rq?|VF|, R ?
Rq?q and O ?
R|VF|?q that arepart of the constituent RLM and a sentence transfor-mation S ?
Rq?q.
We write e = csm(e) for theoutput of the CSM with e as the input sentence.The computation of the RCTM I is a simple mod-ification to the computation of the RLM described inEq.
3.
It proceeds recursively as follows:s = S ?
csm(e) (8a)h1 = ?
(I ?
v(f1) + s) (8b)hi+1 = ?
(R ?
hi + I ?
v(fi+1) + s) (8c)oi+1 = O ?
hi (8d)2For a formal treatment of the construction, see (Kalchbren-ner and Blunsom, 2013).and the conditional distributions P (fi+1|f1:i, e) areobtained from oi as in Eq.
4. ?
is a nonlinear func-tion and bias values are included throughout thecomputation.
Fig.
3 illustrates an RCTM I.Two aspects of the RCTM I are to be remarked.First, the length of the target sentence is predictedby the target RLM itself that by its architecture hasa bias towards shorter sentences.
Secondly, the rep-resentation of the source sentence e constraints uni-formly all the target words, contrary to the fact thatthe target words depend more strongly on certainparts of the source sentence and less on other parts.The next model proposes an alternative formulationof these aspects.4 Recurrent Continuous TranslationModel IIThe central idea behind the RCTM II is to first es-timate the length m of the target sentence indepen-dently of the main architecture.
Given m and thesource sentence e, the model constructs a represen-tation for the n-grams in e, where n is set to 4.
Notethat each level of the CSM yields n-gram represen-tations of e for a specific value of n. The 4-gramrepresentation of e is thus constructed by truncat-ing the CSM at the level that corresponds to n = 4.The procedure is then inverted.
From the 4-gramrepresentation of the source sentence e, the modelbuilds a representation of a sentence that has thepredicted length m of the target.
This is similarlyaccomplished by truncating the inverted CSM for asentence of length m.We next describe in detail the Convolutional n-gram Model (CGM).
Then we return to specify theRCTM II.4.1 Convolutional n-gram modelThe CGM is obtained by truncating the CSM at thelevel where n-grams are represented for the chosenvalue of n. A column g of a matrix Eei obtainedaccording to Eq.
7 represents an n-gram from thesource sentence e. The value of n corresponds tothe number of word vectors from which the n-gramrepresentation g is constructed; equivalently, n isthe span of the weights in the CSM underneath g(see Fig.
2-3).
Note that any column in a matrixEei represents an n-gram with the same span valuen.
We denote by gram(Eei ) the size of the n-grams1704RCTM IIRCTM IP( f | e )P( f | m, e )eeeFTSScsmcgmicgmEFggFigure 3: A graphical depiction of the two RCTMs.
Arrows represent full matrix transformations while lines arevector transformations corresponding to columns of weight matrices.represented by Eei .
For example, for a sufficientlylong sentence e, gram(Ee2) = 2, gram(Ee3) = 4,gram(Ee4) = 7.
We denote by cgm(e, n) that matrixEei from the CSM that represents the n-grams of thesource sentence e.The CGM can also be inverted to obtain a repre-sentation for a sentence from the representation ofits n-grams.
We denote by icgm the inverse CGM,which depends on the size of the n-gram represen-tation cgm(e, n) and on the target sentence lengthm.
The transformation icgm unfolds the n-gramrepresentation onto a representation of a target sen-tence with m words.
The architecture correspondsto an inverted CGM or, equivalently, to an invertedtruncated CSM (Fig.
3).
Given the transformationscgm and icgm, we now detail the computation of theRCTM II.4.2 RCTM IIThe RCTM II models the conditional probabilityP (f|e) by factoring it as follows:P (f|e) = P (f|m, e) ?
P (m|e) (9a)=m?i=1P (fi+1|f1:i,m, e) ?
P (m|e) (9b)and computing the distributions P (fi+1|f1:i,m, e)and P (m|e).
The architecture of the RCTM IIcomprises all the elements of the RCTM I togetherwith the following additional elements: a translationtransformation Tq?q and two sequences of weightmatrices (Ji)2?i?s and (Hi)2?i?s that are part ofthe icgm3.The computation of the RCTM II proceeds recur-sively as follows:Eg = cgm(e, 4) (10a)Fg:,j = ?
(T ?Eg:,j) (10b)F = icgm(Fg,m) (10c)h1 = ?
(I ?
v(f1) + S ?
F:,1) (10d)hi+1 = ?
(R ?
hi + I ?
v(fi+1) + S ?
F:,i+1) (10e)oi+1 = O ?
hi (10f)and the conditional distributions P (fi+1|f1:i, e) areobtained from oi as in Eq.
4.
Note how each re-constructed vector F:,i is added successively to thecorresponding layer hi that predicts the target wordfi.
The RCTM II is illustrated in Fig.
3.3Just like r the value s is small and depends on the lengthof the source and target sentences in the training set.
SeeSect.
5.1.2.1705For the separate estimation of the length of thetranslation, we estimate the conditional probabilityP (m|e) by letting,P (m|e) = P (m|k) = Poisson(?k) (11)where k is the length of the source sentence e andPoisson(?)
is a Poisson distribution with mean ?.This concludes the description of the RCTM II.
Wenow turn to the experiments.5 ExperimentsWe report on four experiments.
The first experimentconsiders the perplexities of the models with respectto reference translations.
The second and third ex-periments test the sensitivity of the RCTM II to thelinguistic aspects of the source sentences.
The fi-nal experiment tests the rescoring performance ofthe two models.5.1 TrainingBefore turning to the experiments, we describe thedata sets, hyper parameters and optimisation algo-rithms used for the training of the RCTMs.5.1.1 Data setsThe training set used for all the experiments com-prises a bilingual corpus of 144953 pairs of sen-tences less than 80 words in length from the newscommentary section of the Eighth Workshop on Ma-chine Translation (WMT) 2013 training data.
Thesource language is English and the target languageis French.
The English sentences contain about4.1M words and the French ones about 4.5M words.Words in both the English and French sentencesthat occur twice or less are substituted with the?unknown?
token.
The resulting vocabularies V Eand V F contain, respectively, 25403 English wordsand 34831 French words.For the experiments we use four different test setscomprised of the Workshop on Machine Transla-tion News Test (WMT-NT) sets for the years 2009,2010, 2011 and 2012.
They contain, respectively,2525, 2489, 3003 and 3003 pairs of English-Frenchsentences.
For the perplexity experiments unknownwords occurring in these data sets are replaced withthe ?unknown?
token.
The respective 2008 WMT-NT set containing 2051 pairs of English-French sen-tences is used as the validation set throughout.5.1.2 Model hyperparametersThe parameter q that defines the size of the En-glish vectors v(ei) for ei ?
V E, the size of the hid-den layer hi and the size of the French vectors v(fi)for v(fi) ?
V F is set to q = 256.
This yields arelatively small recurrent matrix and correspondingmodels.
To speed up training, we factorize the targetvocabulary V F into 256 classes following the proce-dure in (Mikolov et al 2011).The RCTM II uses a convolutional n-gram modelCGM where n is set to 4.
For the RCTM I, the num-ber of weight matrices r for the CSM is 15, whereasin the RCTM II the number r of weight matrices forthe CGM is 7 and the number s of weight matricesfor the inverse CGM is 9.
If a test sentence is longerthan all training sentences and a larger weight matrixis required by the model, the larger weight matrix iseasily factorized into two smaller weight matriceswhose weights have been trained.
For instance, if aweight matrix of 10 weights is required, but weightmatrices have been trained only up to weight 9, thenone can factorize the matrix of 10 weights with oneof 9 and one of 2.
Across all test sets the proportionof sentence pairs that require larger weight matricesto be factorized into smaller ones is < 0.1%.5.1.3 Objective and optimisationThe objective function is the average of the sumof the cross-entropy errors of the predicted wordsand the true words in the French sentences.
The En-glish sentences are taken as input in the predictionof the French sentences, but they are not themselvesever predicted.
An l2 regularisation term is added tothe objective.
The training of the model proceeds byback-propagation through time.
The cross-entropyerror calculated at the output layer at each step isback-propagated through the recurrent structure fora number d of steps; for all models we let d = 6.The error accumulated at the hidden layers is thenfurther back-propagated through the transformationS and the CSM/CGM to the input vectors v(ei) ofthe English input sentence e. All weights, includ-ing the English vectors, are randomly initialised andinferred during training.The objective is minimised using mini-batchadaptive gradient descent (Adagrad) (Duchi et al2011).
The training of an RCTM takes about 15hours on 3 multicore CPUs.
While our experiments1706WMT-NT 2009 2010 2011 2012KN-5 218 213 222 225RLM 178 169 178 181IBM 1 207 200 188 197FA-IBM 2 153 146 135 144RCTM I 143 134 140 142RCTM II 86 77 76 77Table 1: Perplexity results on the WMT-NT sets.are relatively small, we note that in principle ourmodels should scale similarly to RLMs which havebeen applied to hundreds of millions of words.5.2 Perplexity of gold translationsSince the computation of the probability of a trans-lation under one of the RCTMs is efficient, we cancompute the perplexities of the RCTMs with respectto the reference translations in the test sets.
The per-plexity measure is an indication of the quality thata model assigns to a translation.
We compare theperplexities of the RCTMs with the perplexity of theIBM Model 1 (Brown et al 1993) and of the Fast-Aligner (FA-IBM 2) model that is a state-of-the-artvariant of IBM Model 2 (Dyer et al 2013).
We addas baselines the unconditional target RLM and a 5-gram target language model with modified Kneser-Nay smoothing (KN-5).
The results are reported inTab.
1.
The RCTM II obtains a perplexity that is> 43% lower than that of the alignment based mod-els and that is 40% lower than the perplexity of theRCTM I.
The low perplexity of the RCTMs suggeststhat continuous representations and the transforma-tions between them make up well for the lack of ex-plicit alignments.
Further, the difference in perplex-ity between the RCTMs themselves demonstratesthe importance of the conditioning architecture andsuggests that the localised 4-gram conditioning inthe RCTM II is superior to the conditioning with thewhole source sentence of the RCTM I.5.3 Sensitivity to source sentence structureThe second experiment aims at showing the sensi-tivity of the RCTM II to the order and position ofwords in the English source sentence.
To this end,we randomly permute in the training and testing setsWMT-NT PERM 2009 2010 2011 2012RCTM II 174 168 175 178Table 2: Perplexity results of the RCTM II on the WMT-NT sets where the words in the English source sentencesare randomly permuted.the words in the English source sentence.
The re-sults on the permuted data are reported in Tab.
2.
Ifthe RCTM II were roughly comparable to a bag-of-words approach, there would be no difference underthe permutation of the words.
By contrast, the dif-ference of the results reported in Tab.
2 with thosereported in Tab.
1 is very significant, clearly indicat-ing the sensitivity to word order and position of thetranslation model.5.3.1 Generating from the RCTM IITo show that the RCTM II is sensitive not only toword order, but also to other syntactic and semantictraits of the sentence, we generate and inspect can-didate translations for various English source sen-tences.
The generation proceeds by sampling fromthe probability distribution of the RCTM II itself anddoes not depend on any other external resources.Given an English source sentence e, we let m bethe length of the gold translation and we search thedistribution computed by the RCTM II over all sen-tences of length m. The number of possible targetsentences of length m amounts to |V |m = 34831mwhere V = V F is the French vocabulary; directlyconsidering all possible translations is intractable.We proceed as follows: we sample with replace-ment 2000 sentences from the distribution of theRCTM II, each obtained by predicting one word ata time.
We start by predicting a distribution for thefirst target word, restricting that distribution to thetop 5 most probable words and sampling the firstword of a candidate translation from the restricteddistribution of 5 words.
We proceed similarly forthe remaining words.
Each sampled sentence has awell-defined probability assigned by the model andcan thus be ranked.
Table 3 gives various Englishsource sentences and some candidate French trans-lations generated by the RCTM II together with theirranks.The results in Tab.
3 show the remarkable syn-tactic agreements of the candidate translations; the1707English source sentence French gold translation RCTM II candidate translation Rankthe patient is sick .
le patient est malade .
le patient est insuffisante .
1le patient est mort .
4la patient est insuffisante .
23the patient is dead .
le patient est mort .
le patient est mort .
1le patient est de?passe?
.
4the patient is ill .
le patient est malade .
le patient est mal .
3the patients are sick .
les patients sont malades .
les patients sont confronte?s .
2les patients sont corrompus .
5the patients are dead .
les patients sont morts .
les patients sont morts .
1the patients are ill .
les patients sont malades .
les patients sont confronte?s .
5the patient was ill .
le patient e?tait malade .
le patient e?tait mal .
2the patients are not dead .
les patients ne sont pas morts .
les patients ne sont pas morts .
1the patients are not sick .
les patients ne sont pas malades .
les patients ne sont pas ?unknown?
.
1les patients ne sont pas mal .
6the patients were saved .
les patients ont e?te?
sauve?s .
les patients ont e?te?
sauve?es .
6Table 3: English source sentences, respective translations in French and candidate translations generated from theRCTM II and ranked out of 2000 samples according to their decreasing probability.
Note that end of sentence dots (.
)are generated as part of the translation.WMT-NT 2009 2010 2011 2012RCTM I + WP 19.7 21.1 22.5 21.5RCTM II + WP 19.8 21.1 22.5 21.7cdec (12 features) 19.9 21.2 22.6 21.8Table 4: Bleu scores on the WMT-NT sets of each RCTMlinearly interpolated with a word penalty WP.
The cdecsystem includes WP as well as five translation models andtwo language modelling features, among others.large majority of the candidate translations are fullywell-formed French sentences.
Further, subtle syn-tactic features such as the singular or plural endingof nouns and the present and past tense of verbs arewell correlated between the English source and theFrench candidate targets.
Finally, the meaning ofthe English source is well transferred on the Frenchcandidate targets; where a correlation is unlikely orthe target word is not in the French vocabulary, a se-mantically related word or synonym is selected bythe model.
All of these traits suggest that the RCTMII is able to capture a significant amount of bothsyntactic and semantic information from the Englishsource sentence and successfully transfer it onto theFrench translation.5.4 Rescoring and BLEU EvaluationThe fourth experiment tests the ability of the RCTMI and the RCTM II to choose the best translationamong a large number of candidate translations pro-duced by another system.
We use the cdec sys-tem to generate a list of 1000 best candidate trans-lations for each English sentence in the four WMT-NT sets.
We compare the rescoring performance ofthe RCTM I and the RCTM II with that of the cdecitself.
cdec employs 12 engineered features includ-ing, among others, 5 translation models, 2 languagemodel features and a word penalty feature (WP).
Forthe RCTMs we simply interpolate the log probabil-ity assigned by the models to the candidate transla-tions with the word penalty feature WP, tuned on thevalidation data.
The results of the experiment arereported in Tab.
4.While there is little variance in the resulting Bleuscores, the performance of the RCTMs shows thattheir probabilities correlate with translation qual-ity.
Combining a monolingual RLM feature withthe RCTMs does not improve the scores, while re-ducing cdec to just one core translation probabilityand language model features drops its score by twoto five tenths.
These results indicate that the RCTMshave been able to learn both translation and languagemodelling distributions.17086 ConclusionWe have introduced Recurrent Continuous Transla-tion Models that comprise a class of purely contin-uous sentence-level translation models.
We haveshown the translation capabilities of these modelsand the low perplexities that they obtain with respectto reference translations.
We have shown the abilityof these models at capturing syntactic and semanticinformation and at estimating during reranking thequality of candidate translations.The RCTMs offer great modelling flexibility dueto the sensitivity of the continuous representations toconditioning information.
The models also suggesta wide range of potential advantages and extensions,from being able to include discourse representationsbeyond the single sentence and multilingual sourcerepresentations, to being able to model morpholog-ically rich languages through character-level recur-rences.ReferencesYoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research,3:1137?1155.Peter F. Brown, Vincent J.Della Pietra, Stephen A. DellaPietra, and Robert.
L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19:263?311.R.
Collobert and J. Weston.
2008.
A unified architecturefor natural language processing: Deep neural networkswith multitask learning.
In International Conferenceon Machine Learning, ICML.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
J. Mach.
Learn.
Res.,12:2121?2159, July.Chris Dyer, Jonathan Weese, Hendra Setiawan, AdamLopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitke-vitch, Phil Blunsom, and Philip Resnik.
2010. cdec: Adecoder, alignment, and learning framework for finite-state and context-free translation models.
In Proceed-ings of the ACL 2010 System Demonstrations, pages7?12.
Association for Computational Linguistics.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameterizationof ibm model 2.
In Proc.
of NAACL.Edward Grefenstette, Mehrnoosh Sadrzadeh, StephenClark, Bob Coecke, and Stephen Pulman.
2011.
Con-crete sentence spaces for compositional distributionalmodels of meaning.
CoRR, abs/1101.0309.Karl Moritz Hermann and Phil Blunsom.
2013.
The Roleof Syntax in Vector Space Models of CompositionalSemantics.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), Sofia, Bulgaria, August.
Asso-ciation for Computational Linguistics.
Forthcoming.Nal Kalchbrenner and Phil Blunsom.
2013.
RecurrentConvolutional Neural Networks for Discourse Com-positionality.
In Proceedings of the Workshop on Con-tinuous Vector Space Models and their Composition-ality, Sofia, Bulgaria, August.
Association for Compu-tational Linguistics.Hai Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous space translation models with neu-ral networks.
In HLT-NAACL, pages 39?48.Tomas Mikolov and Geoffrey Zweig.
2012.
Context de-pendent recurrent neural network language model.
InSLT, pages 234?239.Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-nocky?, and Sanjeev Khudanpur.
2010.
Recurrentneural network based language model.
In TakaoKobayashi, Keikichi Hirose, and Satoshi Nakamura,editors, INTERSPEECH, pages 1045?1048.
ISCA.Tomas Mikolov, Stefan Kombrink, Lukas Burget, JanCernocky?, and Sanjeev Khudanpur.
2011.
Exten-sions of recurrent neural network language model.
InICASSP, pages 5528?5531.
IEEE.Holger Schwenk, Daniel De?chelotte, and Jean-Luc Gau-vain.
2006.
Continuous space language models forstatistical machine translation.
In ACL.Holger Schwenk.
2012.
Continuous space translationmodels for phrase-based statistical machine transla-tion.
In COLING (Posters), pages 1071?1080.Richard Socher, Eric H. Huang, Jeffrey Pennin, An-drew Y. Ng, and Christopher D. Manning.
2011.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In J. Shawe-Taylor, R.S.Zemel, P. Bartlett, F.C.N.
Pereira, and K.Q.
Wein-berger, editors, Advances in Neural Information Pro-cessing Systems 24, pages 801?809.Richard Socher, Brody Huval, Christopher D. Manning,and Andrew Y. Ng.
2012.
Semantic Compositional-ity Through Recursive Matrix-Vector Spaces.
In Pro-ceedings of the 2012 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP).Ilya Sutskever, James Martens, and Geoffrey E. Hinton.2011.
Generating text with recurrent neural networks.In Lise Getoor and Tobias Scheffer, editors, ICML,pages 1017?1024.
Omnipress.1709
