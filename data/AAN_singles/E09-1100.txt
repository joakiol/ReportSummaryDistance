Proceedings of the 12th Conference of the European Chapter of the ACL, pages 879?887,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsCharacter-Level Dependencies in Chinese: Usefulness and LearningHai ZhaoDepartment of Chinese, Translation and LinguisticsCity University of Hong KongTat Chee Avenue, Kowloon, Hong Kong, Chinahaizhao@cityu.edu.hkAbstractWe investigate the possibility of exploit-ing character-based dependency for Chi-nese information processing.
As Chinesetext is made up of character sequencesrather than word sequences, word in Chi-nese is not so natural a concept as in En-glish, nor is word easy to be defined with-out argument for such a language.
There-fore we propose a character-level depen-dency scheme to represent primary lin-guistic relationships within a Chinese sen-tence.
The usefulness of character depen-dencies are verified through two special-ized dependency parsing tasks.
The firstis to handle trivial character dependenciesthat are equally transformed from tradi-tional word boundaries.
The second fur-thermore considers the case that annotatedinternal character dependencies inside aword are involved.
Both of these resultsfrom character-level dependency parsingare positive.
This study provides an alter-native way to formularize basic character-and word-level representation for Chinese.1 IntroductionIn many human languages, word can be naturallyidentified from writing.
However, this is not thecase for Chinese, for Chinese is born to be writtenin character1 sequence rather than word sequence,namely, no natural separators such as blanks ex-ist between words.
As word does not appear ina natural way as most European languages2 , it1Character here stands for various tokens occurring ina naturally written Chinese text, including Chinese charac-ter(hanzi), punctuation, and foreign letters.
However, Chi-nese characters often cover the most part.2Even in European languages, a naive but necessarymethod to properly define word is to list them all by hand.Thank the first anonymous reviewer who points this fact.brings the argument about how to determine theword-hood in Chinese.
Linguists?
views aboutwhat is a Chinese word diverge so greatly thatmultiple word segmentation standards have beenproposed for computational linguistics tasks sincethe first Bakeoff (Bakeoff-1, or Bakeoff-2003)3(Sproat and Emerson, 2003).Up to Bakeoff-4, seven word segmentation stan-dards have been proposed.
However, this does noteffectively solve the open problem what a Chi-nese word should exactly be but raises another is-sue: what a segmentation standard should be se-lected for the successive application.
As wordoften plays a basic role for the further languageprocessing, if it cannot be determined in a uni-fied way, then all successive tasks will be affectedmore or less.Motivated by dependency representation forsyntactic parsing since (Collins, 1999) that hasbeen drawn more and more interests in recentyears, we suggest that character-level dependen-cies can be adopted to alleviate this difficulty inChinese processing.
If we regard traditional wordboundary as a linear representation for neighboredcharacters, then character-level dependencies canprovide a way to represent non-linear relations be-tween non-neighbored characters.
To show thatcharacter dependencies can be useful, we developa parsing scheme for the related learning task anddemonstrate its effectiveness.The rest of the paper is organized as fol-lows.
The next section shows the drawbacks ofthe current word boundary representation throughsome language examples.
Section 3 describesa character-level dependency parsing scheme fortraditional word segmentation task and reports itsevaluation results.
Section 4 verifies the useful-ness of annotated character dependencies inside aword.
Section 5 looks into a few issues concern-3First International Chinese Word Segmentation Bakeoff,available at http://www.sighan.org/bakeoff2003.879ing the role of character dependencies.
Section 6concludes the paper.2 To Segment or Not: That Is theQuestionThough most words can be unambiguously de-fined in Chinese text, some word boundaries arenot so easily determined.
We show such three ex-amples as the following.The first example is from the MSRA segmentedcorpus of Bakeoff-2 (Bakeoff-2005) (Emerson,2005):?
/ ?
/ / / ?????????
/ ?l /\| /y /0a / piece of / ?
/ Beijing City Beijing OperaOK Sodality / member / entrance / ticket / ?As the guideline of MSRA standard requires anyorganization?s full name as a word, many longwords in this form are frequently encountered.Though this type of ?words?
may be regarded as aneffective unit to some extent, some smaller mean-ingful constituents can be still identified insidethem.
Some researchers argue that these shouldbe seen as phrases rather than words.
In fact, e.g.,a machine translation system will have to segmentthis type of words into some smaller units for aproper translation.The second example is from the PKU corpus ofBakeoff-2,?I / 7 / H?
/ ?,China / in / South Africa / embassy(the Chinese embassy in South Africa)This example demonstrates how researchers canalso feel inconvenient if an organization name issegmented into pieces.
Though the word ??,?
(embassy) is right after ?H??
(South Africa)in the above phrase, the embassy does not belongto South Africa but China, and it is only located inSouth Africa.The third example is an abbreviation that makesuse of the characteristics of Chinese characters.(?
/ ?
/ n / ?Week / one / three / five(Monday, Wednesday and Friday)This example shows that there will be in adilemma to perform segmentation over these char-acters.
If a segmentation position locates before?n?
(three) or ???
(five), then this will make themmeaningless or losing its original meaning at leastbecause either of these two characters should log-ically follow the substring ?(??
(week) to con-struct the expected word ?(?n?
(Wednesday) or?(???
(Friday).
Otherwise, to make all theabove five characters as a word will have to ig-nore all these logical dependent relations amongthese characters and segment it later for a propertackling as the above first example.All these examples suggest that dependenciesexist between discontinuous characters, and wordboundary representation is insufficient to handlethese cases.
This motivates us to introduce char-acter dependencies.3 Character-Level Dependency ParsingCharacter dependency is proposed as an alterna-tive to word boundary.
The idea itself is extremelysimple, character dependencies inside sequenceare annotated or formally defined in the similarway that syntactic dependencies over words areusually annotated.We will initially develop a character-level de-pendency parsing scheme in this section.
Es-pecially, we show character dependencies, eventhose trivial ones that are equally transformedfrom pre-defined word boundaries, can be effec-tively captured in a parsing way.3.1 FormularizationUsing a character-level dependency representa-tion, we first show how a word segmentation taskcan be transformed into a dependency parsingproblem.
Since word segmentation is traditionallyformularized as an unlabeled character chunkingtask since (Xue, 2003), only unlabeled dependen-cies are concerned in the transformation.
There aremany ways to transform chunks in a sequence intodependency representation.
However, for the sakeof simplicity, only well-formed and projective out-put sequences are considered for our processing.Borrowing the notation from (Nivre and Nils-son, 2005), an unlabeled dependency graph is for-mally defined as follows:An unlabeled dependency graph for a stringof cliques (i.e., words and characters) W =880Figure 1: Two character dependency schemesw1...wn is an unlabeled directed graph D =(W,A), where(a) W is the set of ordered nodes, i.e.
cliquetokens in the input string, ordered by alinear precedence relation <,(b) A is a set of unlabeled arcs (wi, wj),where wi, wj ?
W ,If (wi, wj) ?
A, wi is called the head of wjand wj a dependent of wi.
Traditionally, the no-tation wi ?
wj means (wi, wj) ?
A; wi ?
?wj denotes the reflexive and transitive closure ofthe (unlabeled) arc relation.
We assume that thedesigned dependency structure satisfies the fol-lowing common constraints in existing literature(Nivre, 2006).
(1) D is weakly connected, that is, the cor-responding undirected graph is connected.
(CONNECTEDNESS)(2) The graph D is acyclic, i.e., if wi ?
wj thennot wj ??
wi.
(ACYCLICITY)(3) There is at most one arc (wi, wj) ?
A, ?wj ?W .
(SINGLE-HEAD)(4) An arc wi ?
wk is projective iff, for everyword wj occurring between wi and wk in thestring (wi < wj < wk or wi > wj > wk),wi ??
wj .
(PROJECTIVITY)We say that D is well-formed iff it is acyclic andconnected, and D is projective iff every arcs in Aare projective.
Note that the above four conditionsentail that the graph D is a single-rooted tree.
Foran arc wi ?
wj , if wi < wj , then it is called right-arc, otherwise left-arc.Following the above four constraints and con-sidering segmentation characteristics, we mayhave two character dependency representationschemes as shown in Figure 1 by using a seriesof trivial dependencies inside or outside a word.Note that we use arc direction to distinguish con-nected and segmented relation among characters.The scheme with the assistant root node before thesequence in Figure 1 is called Scheme B, and theother Scheme E.3.2 Shift-reduce ParsingAccording to (McDonald and Nivre, 2007), alldata-driven models for dependency parsing thathave been proposed in recent years can be de-scribed as either graph-based or transition-based.Since both dependency schemes that we constructfor parsing are well-formed and projective, the lat-ter is chosen as the parsing framework for the sakeof efficiency.
In detail, a shift-reduce method isadopted as in (Nivre, 2003).The method is step-wise and a classifier is usedto make a parsing decision step by step.
In eachstep, the classifier checks a clique pair 4, namely,TOP, the top of a stack that consists of the pro-cessed cliques, and, INPUT, the first clique in theunprocessed sequence, to determine if a dependentrelation should be established between them.
Be-sides two arc-building actions, a shift action and areduce action are also defined, as follows,Left-arc: Add an arc from INPUT to TOP andpop the stack.Right-arc: Add an arc from TOP to INPUT andpush INPUT onto the stack.Reduce: Pop TOP from the stack.Shift: Push INPUT onto the stack.In this work, we adopt a left-to-right arc-eagerparsing model, that means that the parser scans theinput sequence from left to right and right depen-dents are attached to their heads as soon as possi-ble (Hall et al, 2007).
In the implementation, asfor Scheme E, all four actions are required to passthrough an input sequence.
However, only threeactions, i.e., reduce action will never be used, areneeded for Scheme B.3.3 Learning Model and FeaturesWhile memory-based and margin-based learn-ing approaches such as support vector machinesare popularly applied to shift-reduce parsing, weapply maximum entropy model as the learningmodel for efficient training and producing somecomparable results.
Our implementation of max-imum entropy adopts L-BFGS algorithm for pa-rameter optimization as usual.
No additional fea-ture selection techniques are used.With notations defined in Table 1, a feature setas shown in Table 2 is adopted.
Here, we explainsome terms in Tables 1 and 2.4Here, clique means character or word in a sequence,which depends on what constructs the sequence.881Table 1: Feature NotationsNotation Meanings The character in the top of stacks?1,...
The first character below the top of stack, etc.i, i+1,...
The first (second) character in theunprocessed sequence, etc.dprel Dependent labelh Headlm Leftmost childrm Rightmost childrn Right nearest childchar Character form.
?s, i.e., ?s.dprel?
means dependent labelof character in the top of stack+ Feature combination, i.e., ?s.char+i.char?means both s.char and i.char work as afeature function.Since we only considered unlabeled depen-dency parsing, dprel means the arc direction fromthe head, either left or right.
The feature cur-root returns the root of a partial parsing tree thatincludes a specified node.
The feature cnseq re-turns a substring started from a given character.
Itchecks the direction of the arc that passes the givencharacter and collects all characters with the samearc direction to yield an output substring until thearc direction is changed.
Note that all combina-tional features concerned with this one can be re-garded as word-level features.The feature av is derived from unsupervisedsegmentation as in (Zhao and Kit, 2008a), andthe accessor variety (AV) (Feng et al, 2004) isadopted as the unsupervised segmentation crite-rion.
The AV value of a substring s is defined asAV (s) = min{Lav(s), Rav(s)},where the left and right AV values Lav(s) andRav(s) are defined, respectively, as the numbersof its distinct predecessor and successor charac-ters.
In this work, AV values for substrings arederived from unlabeled training and test corporaby substring counting.
Multiple features are usedto represent substrings of various lengths identi-fied by the AV criterion.
Formally put, the featurefunction for a n-character substring s with a scoreAV (s) is defined asavn = t, if 2t ?
AV (s) < 2t+1, (1)where t is an integer to logarithmize the score andtaken as the feature value.
For an overlap characterof several substrings, we only choose the one withTable 2: Features for ParsingBasic Extensionx.char itself, its previous two and next twocharacters, and all bigrams within thefive-character window.
(x is s or i.
)s.h.chars.dprels.rm.dprels?1.cnseqs?1.cnseq+s.chars?1.curroot.lm.cnseqs?1.curroot.lm.cnseq+s.chars?1.curroot.lm.cnseq+i.chars?1.curroot.lm.cnseq+s?1.cnseqs?1.curroot.lm.cnseq+s.char+s?1.cnseqs?1.curroot.lm.cnseq+i.char+s?1.cnseqs.avn+i.avn, n = 1, 2, 3, 4, 5preact?1preact?2preact?2+preact?1the greatest AV score to activate the above featurefunction for that character.The feature preactn returns the previous pars-ing action type, and the subscript n stands for theaction order before the current action.3.4 DecodingWithout Markovian feature like preact?1, a shift-reduce parser can scan through an input sequencein linear time.
That is, the decoding of a parsingmethod for word segmentation will be extremelyfast.
The time complexity of decoding will be 2Lfor Scheme E, and L for Scheme B, where L isthe length of the input sequence.However, it is somewhat complicated as Marko-vian features are involved.
Following the work of(Duan et al, 2007), the decoding in this case is tosearch a parsing action sequence with the maximalprobability.Sdi = argmax?ip(di|di?1di?2...),where Sdi is the object parsing action sequence,p(di|di?1...) is the conditional probability, and diis i-th parsing action.
We use a beam search al-gorithm as in (Ratnaparkhi, 1996) to find the ob-ject parsing action sequence.
The time complex-ity of this beam search algorithm will be 4BL forScheme E and 3BL for Scheme B, where B is thebeam width.3.5 Related MethodsAmong character-based learning techniques forword segmentation, we may identify two main882types, classification (GOH et al, 2004) and tag-ging (Low et al, 2005).
Both character classifi-cation and tagging need to define the position ofcharacter inside a word.
Traditionally, the fourtags, b, m, e, and s stand, respectively, for thebeginning, midle, end of a word, and a single-character as word since (Xue, 2003).
The follow-ing n-gram features from (Xue, 2003; Low et al,2005) are used as basic features,(a) Cn(n = ?2,?1, 0, 1, 2),(b) CnCn+1(n = ?2,?1, 0, 1),(c) C?1C1,where C stands for a character and the subscriptsfor the relative order to the current character C0.
Inaddition, the feature av that is defined in equation(1) is also taken as an option.
avn (n=1,...,5) isapplied as feature for the current character.While word segmentation is conducted as aclassification task, each individual character willbe simply assigned a tag with the maximal prob-ability given by the classifier.
In this case, we re-store word boundary only according to two tagsb and s. However, the output tag sequence givenby character classification may include illegal tagtransition (e.g., m is after e.).
In (Low et al, 2005),a dynamic programming algorithm is adopted tofind a tag sequence with the maximal joint prob-ability from all legal tag sequences.
If such a dy-namic programming decoding is adopted, then thismethod for word segmentation is regarded as char-acter tagging 5.The time complexity of character-based classifi-cation method for decoding is L, which is the bestresult in decoding velocity.
As dynamic program-ming is applied, the time complexity will be 16Lwith four tags.Recently, conditional random fields (CRFs) be-comes popular for word segmentation since it pro-vides slightly better performance than maximumentropy method does (Peng et al, 2004).
How-ever, CRFs is a structural learning tool rather thana simple classification framework.
As shift-reduceparsing is a typical step-wise method that checks5Someone may argue that maximum entropy Markovmodel (MEMM) is truly a tagging tool.
Yes, this method wasinitialized by (Xue, 2003).
However, our empirical resultsshow that MEMM never outperforms maximum entropy plusdynamic programming decoding as (Low et al, 2005) in Chi-nese word segmentation.
We also know that the latter reportsthe best results in Bakeoff-2.
This is why MEMM method isexcluded from our comparison.each character one by one, it is reasonable to com-pare it to a classification method over characters.3.6 Evaluation ResultsTable 3: Corpus size of Bakeoff-2 in number ofwordsAS CityU MSRA PKUTraining(M) 5.45 1.46 2.37 1.1Test(K) 122 41 107 104The experiments in this section are performedin all four corpora from Bakeoff-2.
Corpus sizeinformation is in Table 3.Traditionally, word segmentation performanceis measured by F-score ( F = 2RP/(R + P ) ),where the recall (R) and precision (P ) are the pro-portions of the correctly segmented words to allwords in, respectively, the gold-standard segmen-tation and a segmenter?s output.
To compute theword F-score, all parsing results will be restoredto word boundaries according to the direction ofoutput arcs.Table 4: The results of parsing and classifica-tion/tagging approaches using different featurecombinationsS.a Feature AS CityU MSRA PKUBasicb .935 .922 .950 .917B +AVc .941 .933 .956 .927+Prevd .937 .923 .951 .918+AV+Prev .942 .935 .958 .929Basic .940 .932 .957 .926E +AV .948 .947 .964 .942+Prev .944 .940 .962 .931+AV+Prev .949 .951 .967 .943n-gram/ce .933 .923 .948 .923Cf +AV/c .942 .936 .957 .933n-gram/dg .945 .938 .956 .936+AV/d .950 .949 .966 .945aSchemebFeatures in top two blocks of Table 2.cFive av features are added on the above basic features.dThree Markovian features in Table 2 are added on the abovebasic features.e/c: ClassificationfCharacter classification or tagging using maximum entropyg/d: Only search in legal tag sequences.Our comparison with existing work will be con-ducted in closed test of Bakeoff.
The rule for theclosed test is that no additional information be-yond training corpus is allowed, while open testof Bakeoff is without such restrict.883The results with different dependency schemesare in Table 4.
As the feature preact is involved,a beam search algorithm with width 5 is used todecode, otherwise, a simple shift-reduce decod-ing is used.
We see that the performance givenby Scheme E is much better than that by SchemeB.
The results of character-based classificationand tagging methods are at the bottom of Table 46.It is observed that the parsing method outperformsclassification and tagging method without Marko-vian features or decoding throughout the whole se-quence.
As full features are used, the former andthe latter provide the similar performance.Due to using a global model like CRFs, our pre-vious work in (Zhao et al, 2006; Zhao and Kit,2008c) reported the best results over the evaluatedcorpora of Bakeoff-2 until now7.
Though thoseresults are slightly better than the results here, westill see that the results of character-level depen-dency parsing approach (Scheme E) are compara-ble to those state-of-the-art ones on each evaluatedcorpus.4 Character Dependencies inside a WordWe further consider exploiting annotated charac-ter dependencies inside a word (internal depen-dencies).
A parsing task for these internal de-pendencies incorporated with trivial external de-pendencies 8 that are transformed from commonword boundaries are correspondingly proposed us-ing the same parsing way as the previous section.4.1 Annotation of Internal DependenciesIn Subsection 3.1, we assign trivial character de-pendencies inside a word for the parsing task ofword segmentation, i.e., each character as the headof its predecessor or successor.
These trivial for-mally defined dependencies may be against thesyntactic or semantic senses of those characters,as we have discussed in Section 2.
Now we willconsider human annotated character dependenciesinside a word.As such an corpus with annotated inter-nal dependencies has not been available until6Only the results of open track are reported in (Low etal., 2005), while we give a comparison following closed trackrules, so, our results here are not comparable to those of (Lowet al, 2005).7As n-gram features are used, F-scores in (Zhao et al,2006) are, AS:0.953, CityU:0.948, MSRA:0.974,PKU:0.952.8We correspondingly call dependencies that mark wordboundary external dependencies that correspond to internaldependencies.now, we launched an annotation job based onUPUC segmented corpus of Bakeoff-3(Bakeoff-2006)(Levow, 2006).
The training corpus is with880K characters and test corpus 270K.
However,the essential of the annotation job is actually con-ducted in a lexicon.After a lexicon is extracted from CTB seg-mented corpus, we use a top-down strategy to an-notate internal dependencies inside these wordsfrom the lexicon.
A long word is first splitinto some smaller constituents, and dependenciesamong these constituents are determined, char-acter dependencies inside each constituents arethen annotated.
Some simple rules are adoptedto determine dependency relation, e.g., modifiersare kept marking as dependants and the onlyrest constituent will be marked as head at last.Some words are hard to determine internal depen-dency relation, such as foreign names, e.g., ??:??
(Portugal) and ??.?B?
(Maradona), anduninterrupted words (??
), e.g., ????
(ant)and ?"h?(clover).
In this case, we simply adopta series of linear dependencies with the last char-acter as head to mark these words.In the previous section, we have shown thatScheme E is a better dependency representationfor encoding word boundaries.
Thus annotatedinternal dependencies are used to replace thosetrivial internal dependencies in Scheme E to ob-tain the corpus that we require.
Note that nowwe cannot distinguish internal and external de-pendencies only according to the arc directionany more, as both left- and right-arc can ap-pear for internal character dependency represen-tation.
Thus two labeled left arcs, external andinternal, are used for the annotation disambigua-tion.
As internal dependencies are introduced,we find that some words (about 10%) are con-structed by two or more parallel constituent partsaccording to our annotations, this not only letstwo labeled arcs insufficiently distinguish internal-and external dependencies, but also makes pars-ing extremely difficult, namely, a great amountof non-projective dependencies will appear if wedirectly introduce these internal dependencies.Again, we adopt a series of linear dependencieswith the last character as head to represent in-ternal dependencies for these words by ignor-ing their parallel constituents.
To handle the re-mained non-projectivities, a strengthened pseudo-projectivization technique as in (Zhao and Kit,884Figure 2: Annotated internal dependencies (Arclabel e notes trivial external dependencies.
)Table 5: Features for internal dependency parsingBasic Extensions.char itself, its next two characters, and all bigramswithin the three-character window.i.char its previous one and next three characters, andall bigrams within the four-character window.s.char+i.chars.h.chars.rm.dprels.curtrees.curtree+s.chars?1.curtree+s.chars.curroot.lm.curtrees?1.curroot.lm.curtrees.curroot.lm.curtree+s.chars?1.curroot.lm.curtree+s.chars.curtree+s.curroot.lm.curtrees?1.curtree+s?1.curroot.lm.curtrees.curtree+s.curroot.lm.curtree+s.chars?1.curtree+s?1.curroot.lm.curtree+s.chars?1.curtree+s?1.curroot.lm.curtree+i.charx.avn, n = 1, ..., 5 (x is s or i.
)s.avn+i.avn, n = 1, ..., 5preact?1preact?2preact?2+preact?12008b) is used during parsing.
An annotated ex-ample is illustrated in Figure 2.4.2 Learning of Internal DependenciesTo demonstrate internal character dependenciesare helpful for further processing.
A series ofsimilar word segmentation experiments as in Sub-section 3.6 are performed.
Note that this task isslightly different from the previous one, as it is afive-class parsing action classification task as leftarc has two labels to differ internal and externaldependencies.
Thus a different feature set has tobe used.
However, all input sequences are still pro-jective.Features listed in Table 5 are adopted for theparsing task that annotated character dependenciesexist inside words.
The feature curtree in Table5 is similar to cnseq of Table 2.
It first greedilysearches all connected character started from thegiven one until an arc with external label is foundover some character.
Then it collects all charactersthat has been reached to yield an output substringas feature value.A comparison of classification/tagging andparsing methods is given in Table 6.
To evalu-ate the results with word F-score, all external de-pendencies in outputs are restored as word bound-aries.
There are three models are evaluated in Ta-ble 6.
It is shown that there is a significant perfor-mance enhancement as annotated internal charac-ter dependency is introduced.
This positive resultshows that annotated internal character dependen-cies are meaningful.Table 6: Comparison of different methodsApproach a basic +AV +Prevb +AV+PrevClass/Tagc .918 .935 .928 .941Parsing/wod .921 .937 .924 .942Parsing/w e .925 .940 .929 .945aThe highest F-score in Bakeoff-3 is 0.933.bAs for the tagging method, this means dynamic pro-gramming decoding; As for the parsing method, this meansthree Markovian features.cCharacter-based classification or tagging methoddUsing trivial internal dependencies in Scheme E.eUsing annotated internal character dependencies.5 Is Word Still Necessary?Note that this work is not about joint learningof word boundaries and syntactic dependenciessuch as (Luo, 2003), where a character-based tag-ging method is used for syntactic constituent pars-ing from unsegmented Chinese text.
Instead, thiswork is to explore an alternative way to repre-sent ?word-hood?
in Chinese, which is based oncharacter-level dependencies instead of traditionalword boundaries definition.Though considering dependencies amongwords is not novel (Gao and Suzuki, 2004),we recognize that this study is the first workconcerned with character dependency.
Thisstudy originally intends to lead us to consider analternative way that can play the similar role asword boundary annotations.In Chinese, not word but character is the actualminimal unit for either writing or speaking.
Word-hood has been carefully defined by many means,and this effort results in multi-standard segmentedcorpora provided by a series of Bakeoff evalu-ations.
However, from the view of linguistics,Bakeoff does not solve the problem but technicallyskirts round it.
As one asks what a Chinese wordis, Bakeoff just answers that we have many def-initions and each one is fine.
Instead, motivatedfrom the results of the previous two sections, we885suggest that character dependency representationcould present a natural and unified way to allevi-ate the drawbacks of word boundary representa-tion that is only able to represent the relation ofneighbored characters.Table 7: What we have done for character depen-dencyInternal External Our worktrivial trivial Section 3annotated trivial Section 4annotated ?If we regard that our current work is steppinginto more and more annotated character dependen-cies as shown in Table 7, then it is natural to ex-tend annotated internal character dependencies tothe whole sequence without those unnatural wordboundary constraints.
In this sense, internal andexternal character dependency will not need bediffered any more.
A full character-level depen-dency tree is illustrated as shown in Figure 3(a)9With the help of such a tree, we may define wordor even phrase according to what part of subtree ispicked up.
Word-hood, if we still need this con-cept, can be freely determined later as further pro-cessing purpose requires.
(a)(b)Figure 3: Extended character dependenciesBasically we only consider unlabeled depen-dencies in this work, and dependant labels can beemptied to do something else, e.g., Figure 3(b)shows how to extend internal character dependen-cies of Figure 2 to accommodate part-of-speechtags.
This extension can also be transplanted to afull character dependency tree of Figure 3(a), thenthis may leads to a character-based labeled syntac-tic dependency tree.
In brief, we see that charac-9We may easily build such a corpus by embedding an-notated internal dependencies into a word-level dependencytree bank.
As UPUC corpus of Bakeoff-3 just follows theword segmentation convention of Chinese tree bank, we havebuilt such a full character-level dependency tree corpus.ter dependencies provide a more general and nat-ural way to reflect character relations within a se-quence than word boundary annotations do.6 Conclusion and Future WorkIn this study, we initially investigate the possibil-ity of exploiting character dependencies for Chi-nese.
To show that character-level dependencycan be a good alternative to word boundary rep-resentation for Chinese, we carry out a series ofparsing experiments.
The techniques are devel-oped step by step.
Firstly, we show that word seg-mentation task can be effectively re-formularizedcharacter-level dependency parsing.
The results ofa character-level dependency parser can be com-parable with traditional methods.
Secondly, weconsider annotated character dependencies insidea word.
We show that a parser can still effectivelycapture both these annotated internal character de-pendencies and trivial external dependencies thatare transformed from word boundaries.
The exper-imental results show that annotated internal depen-dencies even bring performance enhancement andindirectly verify the usefulness of them.
Finally,we suggest that a full annotated character depen-dency tree can be constructed over all possiblecharacter pairs within a given sequence, though itsusefulness needs to be explored in the future.AcknowledgementsThis work is beneficial from many sources, in-cluding three anonymous reviewers.
Especially,the authors are grateful to two colleagues, one re-viewer from EMNLP-2008 who gave some veryinsightful comments to help us extend this work,and Mr. SONG Yan who annotated internal depen-dencies of top frequent 22K words extracted fromUPUC segmentation corpus.
Of course, it is theduty of the first author if there still exists anythingwrong in this work.ReferencesMichael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Xiangyu Duan, Jun Zhao, and Bo Xu.
2007.
Proba-bilistic parsing action models for multi-lingual de-pendency parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages940?946, Prague, Czech, June 28-30.886Thomas Emerson.
2005.
The second internationalChinese word segmentation bakeoff.
In Proceed-ings of the Fourth SIGHAN Workshop on ChineseLanguage Processing, pages 123?133, Jeju Island,Korea, October 14-15.Haodi Feng, Kang Chen, Xiaotie Deng, and WeiminZheng.
2004.
Accessor variety criteria for Chi-nese word extraction.
Computational Linguistics,30(1):75?93.Jianfeng Gao and Hisami Suzuki.
2004.
Capturinglong distance dependency in language modeling: Anempirical study.
In K.-Y.
Su, J. Tsujii, J. H. Lee, andO.
Y. Kwong, editors, Natural Language Processing- IJCNLP 2004, volume 3248 of Lecture Notes inComputer Science, pages 396?405, Sanya, HainanIsland, China, March 22-24.Chooi-Ling GOH, Masayuki Asahara, and Yuji Mat-sumoto.
2004.
Chinese word segmentation by clas-sification of characters.
In ACL SIGHAN Workshop2004, pages 57?64, Barcelona, Spain, July.
Associ-ation for Computational Linguistics.Johan Hall, Jens Nilsson, Joakim Nivre,Gu?lsen Eryig?it, Bea?ta Megyesi, Mattias Nils-son, and Markus Saers.
2007.
Single malt orblended?
a study in multilingual parser optimiza-tion.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007, pages 933?939,Prague, Czech, June.Gina-Anne Levow.
2006.
The third international Chi-nese language processing bakeoff: Word segmen-tation and named entity recognition.
In Proceed-ings of the Fifth SIGHAN Workshop on Chinese Lan-guage Processing, pages 108?117, Sydney, Aus-tralia, July 22-23.Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.
2005.A maximum entropy approach to Chinese word seg-mentation.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing, pages161?164, Jeju Island, Korea, October 14-15.Xiaoqiang Luo.
2003.
A maximum entropy chinesecharacter-based parser.
In Proceedings of the 2003Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2003), pages 192 ?
199,Sapporo, Japan, July 11-12.Ryan McDonald and Joakim Nivre.
2007.
Charac-terizing the errors of data-driven dependency pars-ing models.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL 2007), pages 122?131,Prague, Czech, June 28-30.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics (ACL-2005), pages 99?106, AnnArbor, Michigan, USA, June 25-30.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing Technologies(IWPT 03), pages 149?160, Nancy, France, April23-25.Joakim Nivre.
2006.
Constraints on non-projective de-pendency parsing.
In Proceedings of 11th Confer-ence of the European Chapter of the Association forComputational Linguistics (EACL-2006), pages 73?80, Trento, Italy, April 3-7.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detec-tion using conditional random fields.
In COLING2004, pages 562?568, Geneva, Switzerland, August23-27.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the Empiri-cal Method in Natural Language Processing Confer-ence, pages 133?142, University of Pennsylvania.Richard Sproat and Thomas Emerson.
2003.
The firstinternational Chinese word segmentation bakeoff.In The Second SIGHAN Workshop on Chinese Lan-guage Processing, pages 133?143, Sapporo, Japan.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, 8(1):29?48.Hai Zhao and Chunyu Kit.
2008a.
Exploiting unla-beled text with different unsupervised segmentationcriteria for chinese word segmentation.
In Researchin Computing Science, volume 33, pages 93?104.Hai Zhao and Chunyu Kit.
2008b.
Parsing syn-tactic and semantic dependencies with two single-stage maximum entropy models.
In Twelfth Confer-ence on Computational Natural Language Learning(CoNLL-2008), pages 203?207, Manchester, UK,August 16-17.Hai Zhao and Chunyu Kit.
2008c.
Unsupervisedsegmentation helps supervised learning of charac-ter tagging for word segmentation and named en-tity recognition.
In The Sixth SIGHAN Workshopon Chinese Language Processing, pages 106?111,Hyderabad, India, January 11-12.Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-LiangLu.
2006.
Effective tag set selection in Chineseword segmentation via conditional random fieldmodeling.
In Proceedings of the 20th Asian PacificConference on Language, Information and Compu-tation, pages 87?94, Wuhan, China, November 1-3.887
