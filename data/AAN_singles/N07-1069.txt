Proceedings of NAACL HLT 2007, pages 548?555,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsCan Semantic Roles Generalize Across Genres?Szu-ting YiDept of Computer ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104Edward LoperDept of Computer ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104Martha PalmerDept of Computer ScienceUniversity of Colorado at BoulderBoulder, CO 80309AbstractPropBank has been widely used as train-ing data for Semantic Role Labeling.However, because this training data istaken from the WSJ, the resulting machinelearning models tend to overfit on idiosyn-crasies of that text?s style, and do not portwell to other genres.
In addition, sincePropBank was designed on a verb-by-verbbasis, the argument labels Arg2 - Arg5 getused for very diverse argument roles withinconsistent training instances.
For exam-ple, the verb ?make?
uses Arg2 for the?Material?
argument; but the verb ?multi-ply?
uses Arg2 for the ?Extent?
argument.As a result, it can be difficult for auto-matic classifiers to learn to distinguish ar-guments Arg2-Arg5.
We have created amapping between PropBank and VerbNetthat provides a VerbNet thematic role la-bel for each verb-specific PropBank label.Since VerbNet uses argument labels thatare more consistent across verbs, we areable to demonstrate that these new labelsare easier to learn.1 IntroductionCorrectly identifying semantic entities and success-fully disambiguating the relations between them andtheir predicates is an important and necessary stepfor successful natural language processing applica-tions, such as text summarization, question answer-ing, and machine translation.
For example, in or-der to determine that question (1a) is answered bysentence (1b), but not by sentence (1c), we must de-termine the relationships between the relevant verbs(eat and feed) and their arguments.
(1) a.
What do lobsters like to eat?b.
Recent studies have shown that lobsters pri-marily feed on live fish, dig for clams, seaurchins, and feed on algae and eel-grass.c.
In the early 20th century, Mainers wouldonly eat lobsters because the fish theycaught was too valuable to eat themselves.An important part of this task is Semantic RoleLabeling (SRL), where the goal is to locate the con-stituents which are arguments of a given verb, and toassign them appropriate semantic roles that describehow they relate to the verb.
Many researchers haveinvestigated applying machine learning to corpusspecifically annotated with this task in mind, Prop-Bank, since 2000 (Chen and Rambow, 2003; Gildeaand Hockenmaier, 2003; Hacioglu et al, 2003; Mos-chitti, 2004; Yi and Palmer, 2004; Pradhan et al,2005b; Punyakanok et al, 2005; Toutanova et al,2005).
For two years, the CoNLL workshop hasmade this problem the shared task (Carreras andMa?rquez, 2005).
However, there is still little con-sensus in the linguistic and NLP communities aboutwhat set of role labels are most appropriate.
TheProposition Bank (PropBank) corpus (Palmer et al,2005) avoids this issue by using theory-agnostic la-bels (Arg0, Arg1, .
.
.
, Arg5), and by defining thoselabels to have verb-specific meanings.
Under thisscheme, PropBank can avoid making any claims548about how any one verb?s arguments relate to otherverbs?
arguments, or about general distinctions be-tween verb arguments and adjuncts.However, there are several limitations to this ap-proach.
The first is that it can be difficult to makeinferences and generalizations based on role labelsthat are only meaningful with respect to a singleverb.
Since each role label is verb-specific, we cannot confidently determine when two different verbs?arguments have the same role; and since no encodedmeaning is associated with each tag, we can notmake generalizations across verb classes.
In con-trast, the use of a shared set of role labels, such asthematic roles, would facilitate both inferencing andgeneralization.The second issue with PropBank?s verb-specificapproach is that it can make training automatic se-mantic role labeling (SRL) systems more difficult.A vast amount of data would be needed to train theverb-specific models that are theoretically mandatedby PropBank?s design.
Instead, researchers typicallybuild a single model for the numbered arguments(Arg0, Arg1, .
.
.
, Arg5).
This approach works sur-prisingly well, mainly because an explicit effort wasmade to use arguments Arg0 and Arg1 consistentlyacross different verbs; and because those two argu-ment labels account for 85% of all arguments.
How-ever, this approach causes the system to conflatedifferent argument types, especially with the highlyoverloaded arguments Arg2-Arg5.
As a result, theseargument labels are quite difficult to learn.A final difficulty with PropBank?s current ap-proach is that it limits SRL system robustness inthe face of verb senses, verbs or verb constructionsthat were not included in the training data, and thetraining data is all Wall Street Journal corpora.
Ifa PropBank-trained SRL system encounters a novelverb or verb usage, then there is no way for it toknow which role labels are used for which argumenttypes, since role labels are defined so specifically.This is especially problematic for Arg2-5.
Similarly,PropBank-trained SRL systems can have difficultygeneralizing when a known verb is encountered ina novel construction.
These problems can happenquite frequently if the training data comes from adifferent genre than the test data.
This issue is re-flected in the relatively poor performance of moststate-of-the-art SRL systems when tested on a novelgenre, the Brown corpus, during CoNLL 2005.
Forexample, the SRL system described in (Pradhan etal., 2005b; Pradhan et al, 2005a) achieves an F-score of 81% when tested on the same genre as itis trained on (WSJ); but that score drops to 68.5%when the same system is tested on a different genre(the Brown corpus).
DARPA-GALE is funding anongoing effort to PropBank additional genres, butbetter techniques for generalizing the semantic rolelabeling task are still needed.In this paper, we demonstrate an increase in thegenerality of our semantic role labeling based on amapping that has been developed between PropBankand another lexical resource, VerbNet.
By taking ad-vantage of VerbNet?s more consistent set of labels,we can generate more useful role label annotationswith a resulting improvement in SRL performanceon novel genres.2 Background2.1 PropBankPropBank (Palmer et al, 2005) is an annotation ofone million words of the Wall Street Journal por-tion of the Penn Treebank II (Marcus et al, 1994)with predicate-argument structures for verbs, usingsemantic role labels for each verb argument.
In or-der to remain theory neutral, and to increase anno-tation speed, role labels were defined on a per-verb-sense basis.
Although the same tags were used forall verbs, (namely Arg0, Arg1, ..., Arg5), these tagsare meant to have a verb-specific meaning.Thus, the use of a given argument label shouldbe consistent across different uses of that verb, in-cluding syntactic alternations.
For example, theArg1 (underlined) in ?John broke the window?
is thesame window that is annotated as the Arg1 in ?Thewindow broke?, even though it is the syntactic sub-ject in one sentence and the syntactic object in theother.
However, there is no guarantee that an argu-ment label will be used consistently across differentverbs.
For example, the Arg2 label is used to des-ignate the destination of the verb ?bring;?
but theextent of the verb ?rise.?
Generally, the argumentsare simply listed in the order of their prominencefor each verb.
However, an explicit effort was madewhen PropBank was created to use Arg0 for argu-ments that fulfill Dowty?s criteria for ?prototypical549agent,?
and Arg1 for arguments that fulfill the cri-teria for ?prototypical patient.?
(Dowty, 1991) Asa result, these two argument labels are significantlymore consistent across verbs than the other three.But nevertheless, there are still some inter-verb in-consistencies for even Arg0 and Arg1.2.2 VerbNetVerbNet (Schuler, 2005) consists of hierarchicallyarranged verb classes, inspired by and extendedfrom classes of Levin 1993 (Levin, 1993).
Eachclass and subclass is characterized extensionally byits set of verbs, and intensionally by a list of thearguments of those verbs and syntactic and seman-tic information about the verbs.
The argument listconsists of thematic roles (23 in total) and pos-sible selectional restrictions on the arguments ex-pressed using binary predicates.
The syntactic infor-mation maps the list of thematic arguments to deep-syntactic arguments (i.e., normalized for voice alter-nations, and transformations).
The semantic predi-cates describe the participants during various stagesof the event described by the syntactic frame.The same thematic role can occur in differentclasses, where it will appear in different predicates,providing a class-specific interpretation of the role.VerbNet has been extended from the original Levinclasses, and now covers 4526 senses for 3769 verbs.A primary emphasis for VerbNet is the grouping ofverbs into classes that have a coherent syntactic andsemantic characterization, that will eventually facil-itate the acquisition of new class members based onobservable syntactic and semantic behavior.
The hi-erarchical structure and small number of thematicroles is aimed at supporting generalizations.2.3 Mapping PropBank to VerbNetBecause PropBank includes a large corpus of man-ually annotated predicate-argument data, it can beused to train supervised machine learning algo-rithms, which can in turn provide PropBank-styleannotations for novel or unseen text.
However, aswe discussed in the introduction, PropBank?s verb-specific role labels are somewhat problematic.
Fur-thermore, PropBank lacks much of the informationthat is contained in VerbNet, including informationabout selectional restrictions, verb semantics, andinter-verb relationships.We have therefore created a mapping betweenVerbNet and PropBank (Loper et al, 2007), whichwill allow us to use the machine learning tech-niques that have been developed for PropBank anno-tations to generate more semantically abstract Verb-Net representations.
Additionally, the mapping canbe used to translate PropBank-style numbered ar-guments (Arg0.
.
.Arg5) to VerbNet thematic roles(Agent, Patient, Theme, etc.
), which should allow usto overcome the verb-specific nature of PropBank.The mapping between VerbNet and PropBankconsists of two parts: a lexical mapping and an in-stance classifier.
The lexical mapping is responsiblefor specifying the potential mappings between Prop-Bank and VerbNet for a given word; but it does notspecify which of those mappings should be used forany given occurrence of the word.
That is the jobof the instance classifier, which looks at the wordin context, and decides which of the mappings ismost appropriate.
In essence, the instance classi-fier is performing word sense disambiguation, de-ciding which lexeme from each database is correctfor a given occurrence of a word.
In order to trainthe instance classifier, we semi-automatically anno-tated each verb in the PropBank corpus with Verb-Net class information.1 This mapped corpus wasthen used to build the instance classifier.
More de-tails about the mapping, and how it was created, canbe found in (Loper et al, 2007).3 Analysis of the MappingIn order to confirm our belief that PropBank rolesArg0 and Arg1 are relatively coherent, while rolesArg2-5 are much more overloaded, we performeda preliminary analysis of how argument roles weremapped.
Figure 1 shows how often each PropBankrole was mapped to each VerbNet thematic role, cal-culated as a fraction of instances in the mapped cor-pus.
From this figure, we can see that Arg0 maps toagent-like roles, such as ?agent?
and ?experiencer,?over 94% of the time; and Arg1 maps to patient-like roles, including ?theme,?
?topic,?
and ?patient,?over 82% of the time.
In contrast, arguments Arg2-5get mapped to a much broader variety of roles.
It isalso worth noting that the sample size for arguments1Excepting verbs whose senses are not present in VerbNet(24.5% of instances).550Arg3-5 is quite small in comparison with argumentsArg0-2, suggesting that any automatically built clas-sifier for arguments Arg3-5 will suffer severe sparsedata problems for those arguments.4 Training a SRL system with VerbNetRoles to Achieve RobustnessAn important issue for state-of-the-art automaticSRL systems is robustness: although they receivehigh performance scores when tested on the WallStreet Journal (WSJ) corpus, that performance dropssignificantly when the same systems are tested on acorpus from another genre.
This performance dropreflects the fact that the WSJ corpus is highly spe-cialized, and tends to use genre-specific word sensesfor many verbs.
The 2005 CoNLL shared task hasaddressed this issue of robustness by evaluating par-ticipating systems on a test set extracted from theBrown corpus, which is very different from the WSJcorpus that was used for training.
The results sug-gest that there is much work to be done in order toimprove system robustness.One of the reasons that current SRL systems havedifficulty deciding which role label to assign to agiven argument is that role labels are defined on aper-verb basis.
This is less problematic for Arg0and Arg1, where a conscious effort was made to beconsistent across verbs; but is a significant problemfor Args[2-5], which tend to have very verb-specificmeanings.
This problem is exacerbated even fur-ther on novel genres, where SRL systems are morelikely to encounter unseen verbs and uses of argu-ments that were not encountered in the training data.4.1 Addressing Current SRL Problems viaLexical MappingsBy exploiting the mapping between PropBank andVerbNet, we can transform the data to make it moreconsistent, and to expand the size and variety of thetraining data.
In particular, we can use the map-ping to transform the verb-specific PropBank rolelabels into the more general thematic role labels thatare used by VerbNet.
Unlike the PropBank labels,the VerbNet labels are defined consistently acrossverbs; and therefore it should be easier for statisti-cal SRL systems to model them.
Furthermore, sincethe VerbNet role labels are significantly less verb-Arg0 (45,579)Agent 85.4%Experiencer 7.2%Theme 2.1%Cause 1.9%Actor1 1.8%Theme1 0.8%Patient1 0.2%Location 0.2%Theme2 0.2%Product 0.1%Patient 0.0%Attribute 0.0%Arg1 (59,884)Theme 47.0%Topic 23.0%Patient 10.8%Product 2.9%Predicate 2.5%Patient1 2.4%Stimulus 2.0%Experiencer 1.9%Cause 1.8%Destination 0.9%Theme2 0.7%Location 0.7%Source 0.7%Theme1 0.6%Actor2 0.6%Recipient 0.5%Agent 0.4%Attribute 0.2%Asset 0.2%Patient2 0.2%Material 0.2%Beneficiary 0.0%Arg2 (11,077)Recipient 22.3%Extent 14.7%Predicate 13.4%Destination 8.6%Attribute 7.6%Location 6.5%Theme 5.5%Patient2 5.3%Source 5.2%Topic 3.1%Theme2 2.5%Product 1.5%Cause 1.2%Material 0.8%Instrument 0.6%Beneficiary 0.5%Experiencer 0.3%Actor2 0.2%Asset 0.0%Theme1 0.0%Arg3 (609)Asset 38.6%Source 25.1%Beneficiary 10.7%Cause 9.7%Predicate 9.0%Location 2.0%Material 1.8%Theme1 1.6%Theme 0.8%Destination 0.3%Instrument 0.3%Arg4 (18)Beneficiary 61.1%Product 33.3%Location 5.6%Arg5 (17)Location 100.0%Figure 1: The frequency with which each PropBanknumbered argument is mapped to each VerbNet the-matic role in the mapped corpus.
The numbersnext to each PropBank argument reflects the num-ber of occurrences of that numbered argument in themapped corpus.551dependent than the PropBank roles, the SRL?s mod-els should generalize better to novel verbs, and tonovel uses of known verbs.5 SRL Experiments on Linked LexicalResourcesIn order to verify the feasibility of performing se-mantic role labeling with VerbNet thematic roles, were-trained our existing SRL system, which originallyused PropBank role labels, with a new label set thatmakes use of VerbNet thematic role information.5.1 The SRL SystemOur SRL system is a Maximum Entropy basedpipelined system which consists of four compo-nents: Pre-processing, Argument Identification, Ar-gument Classification, and Post Processing.
ThePre-processing component pipes a sentence througha syntactic parser and filters out constituents whichare unlikely to be semantic arguments based on aconstituents location in the parse tree.
The Argu-ment Identification component is a binary MaxEntclassifier, which tags candidate constituents as ar-guments or non-arguments.
The Argument Classifi-cation component is a multi-class MaxEnt classifierwhich assigns a semantic role to each constituent.The Post Processing component further selects thefinal arguments based on global constraints.
Our ex-periments mainly focused on changes to the Argu-ment Classification stage of the SRL pipeline, andin particular, on changes to the set of output tags.For more information on our SRL system, see (Yiand Palmer, 2004; Yi and Palmer, 2005).The evaluation of SRL systems is typically ex-pressed by precision, recall and the F1-measure.Precision is the number of correct arguments pre-dicted by a system divided by the total number ofarguments proposed.
Recall is the number of cor-rect arguments divided by the number of the totalnumber of arguments in the Gold Standard Data.
F1computes the harmonic mean of precision and recall.5.2 SRL Experiments on Mapped VerbNetThematic RolesSince PropBank arguments Arg0 and Arg1 are al-ready quite coherent, we left them as-is in the newlabel set.
But since arguments Arg2-Arg5 are highlyGroup 1 Group 2 Group 3 Group 4 Group 5Recipient Extent Predicate Patient2 InstrumentDestination Asset Attribute Product CauseLocation Theme ExperiencerSource Theme1 Actor2Material Theme2Beneficiary TopicFigure 2: Thematic Role Groupings for the exper-iments on linked lexical resources; and for Arg2 inthe experiments on arguments with different verb in-dependency.overloaded, we replaced them by mapping themto their corresponding VerbNet thematic role.
Wefound that mapping directly to individual role labelscreated a significant sparse data problem, since thenumber of output tags was increased from 6 to 23.We therefore grouped the VerbNet thematic rolesinto five coherent groups of similar thematic roles,shown in Figure 2.2 Our new tag set therefore in-cluded the following tags: Arg0 (agent); Arg1 (pa-tient); Group1 (goal); Group2 (extent); Group3(predicate/attrib); Group4 (product); and Group5(instrument/cause).Training our SRL system using these thematicrole groups, we obtained performance similar to theoriginal SRL system.
However, it is important tonote that these performance figures are not directlycomparable, since the two systems are performingdifferent tasks: The Original system labels Arg0-5,ArgA and ArgM and the Mapped system labelsArg0, Arg1, ArgA, ArgM and Group1-5.
In partic-ular, the role labels generated by the original systemare verb-specific, while the role labels generated bythe new system are less verb-dependent.5.2.1 ResultsFor our testing and training, we used the portionof Penn Treebank II that is covered by the mapping,and where at least one of Arg2-5 is used.
Trainingwas performed using sections 2-21 of the Treebank(10,783 instances of argument); and testing was per-formed on section 23 (859 instances).
Table 1 dis-plays the performance score for the SRL system us-ing the augmented tag set (?Mapped?).
The per-formance score of the original system (?Original?
)is also listed, for reference; however, as was dis-2Karin Kipper assisted in creating the groupings.552System Precision Recall F1Original 90.65 85.43 87.97Mapped 88.85 84.56 86.65Table 1: Overall SRL System performance using thePropBank tag set (?Original?)
and the augmentedtag set (?Mapped?
)System Precision Recall F1Original 97.60 83.67 90.10Mapped 91.70 82.86 87.06Table 2: SRL System performance evaluated on onlyArg2-5 (Original) or Group1-5 (Mapped).cussed above, these results are not directly compara-ble because the two systems are performing differenttasks.The results indicate that the performance dropswhen we train on the new argument labels, espe-cially on precision when we evaluate the systemson only Arg2-5/Group1-5 (see Table 2).
However,it is premature to conclude that there is no benefitfrom the VerbNet thematic role labels.
Firstly, wehave very few mapped Arg3-5 instances (less than1,000 instances); secondly, we lack test data gen-erated from a genre other than WSJ to allow us toevaluate the robustness (generality) of SRL trainedon the new argument labels.We therefore redesigned our experiments by lim-iting the scope to mapped instances of Arg1 andArg2.
By doing this, we should be able to accom-plish the following: 1) we can map new argument la-bels back to the original PropBank labels; thereforewe can directly compare results; 2) With the abilityof testing our systems on other test data, we can eval-uate the influence of the mapping on SRL robust-ness; 3) We can validate our original hypothesis thatthe behavior of Arg1 is primarily verb-independentwhile Arg2 is more verb-specific.5.3 SRL Experiments on Arguments withDifferent Verb IndependencyWe conducted two further sets of experiments: oneto test the effect of the mapping on learning Arg2;and one to test the effect on learning Arg1.
SinceArg2 is used in very verb-dependent ways, we ex-pect that mapping it to VerbNet role labels will in-Group 1 Group 2 Group 3 Group 4 Group 5Theme Source Patient Agent TopicTheme1 Location Product Actor2Theme2 Destination Patient1 Experiencer Group 6Predicate Recipient Patient2 Cause AssetStimulus BeneficiaryAttribute MaterialFigure 3: Thematic Role Groupings for Arg1 in theexperiments on arguments with different verb inde-pendency.crease our performance.
However, since a consciouseffort was made to keep the meaning of Arg1 consis-tent across verbs, we expect that mapping it to Verb-Net labels will provide less of an improvement.Each experiment compares two SRL systems: onetrained using the original PropBank role labels; theother trained with the argument role under consid-eration (Arg1 or Arg2) subdivided based on whichVerbNet role label it maps to.
In order to preventthe training data from these subdivided labels frombecoming too sparse (which would impair systemperformance) we grouped similar thematic roles to-gether.
For Arg2, we used the same groupings as theprevious experiment, shown in Figure 2.
The argu-ment role groupings we used for Arg1 are shown inFigure 3.The training data for both experiments is the por-tion of Penn Treebank II (sections 02-21) that is cov-ered by the mapping.
We evaluated each experi-mental system using two test sets: section 23 of thePenn Treebank II, which represents the same genreas the training data; and the PropBank-ed portion ofthe Brown corpus, which represents a very differentgenre.5.3.1 Results and DiscussionTable 3 describes the results of SRL overall per-formance tested on the WSJ corpus Section 23; Ta-ble 4 demonstrates the SRL overall system perfor-mance tested on the Brown corpus.
Systems Arg1-Original and Arg2-Original are trained using theoriginal PropBank labels, and show the baselineperformance of our SRL system.
Systems Arg1-Mapped and Arg2-Mapped are trained using Prop-Bank labels augmented with VerbNet thematic rolegroups.
In order to allow comparison between thesystem using the original PropBank labels and thesystems that augmented those labels with VerbNet553System Precision Recall F1Arg1-Original 89.24 77.32 82.85Arg1-Mapped 90.00 76.35 82.61Arg2-Original 73.04 57.44 64.31Arg2-Mapped 84.11 60.55 70.41Table 3: SRL System Performance on Arg1 Map-ping and Arg2 Mapping, tested using the WSJ cor-pus (section 23).
This represents performance on thesame genre as the training corpus.System Precision Recall F1Arg1-Original 86.01 71.46 78.07Arg1-Mapped 88.24 71.15 78.78Arg2-Original 66.74 52.22 58.59Arg2-Mapped 81.45 58.45 68.06Table 4: SRL System Performance on Arg1 Map-ping and Arg2 Mapping, tested using the PropBank-ed Brown corpus.
This represents performance on adifferent genre from the training corpus.thematic role groups, system performance was eval-uated based solely on the PropBank role label thatwas assigned.We had hypothesized that with the use of thematicroles, we would be able to create a more consis-tent training data set which would result in an im-provement in system performance.
In addition, thethematic roles would behave more consistently thanthe overloaded Args[2-5] across verbs, which shouldenhance robustness.
However, since in practice weare also increasing the number of argument labelsan SRL system needs to tag, the system might suf-fer from data sparseness.
Our hope is that the en-hancement gained from the mapping will outweighthe loss due to data sparseness.From Table 3 and Table 4 we see the F1 scores ofArg1-Original and Arg1-Mapped are statistically in-different both on the WSJ corpus and the Brown cor-pus.
These results confirm the observation that Arg1in the PropBank behaves fairly verb-independentlyso that the VerbNet mapping does not provide muchbenefit.
The increase of precision due to a more co-herent training data set is compensated for by theloss of recall due to data sparseness.The results of the Arg2 experiments tell a differ-Confusion ARG2-OriginalMatrix ARG1 ARG2 ARGMARG2- ARG0 53 50 -Mapped ARG1 - 716 -ARG2 1 - 2ARG3 - 1 -ARGM 1 482 -233 ARG2-Mapped arguments are not labeled by ARG2-OriginalTable 5: Confusion matrix on the 1,539 instanceswhich ARG2-Mapped tags correctly and ARG2-Original fails to predict.ent story.
Both precision and recall are improvedsignificantly, which demonstrates that the Arg2 labelin the PropBank is quite overloaded.
The Arg2 map-ping improves the overall results (F1) on the WSJby 6% and on the Brown corpus by almost 10%.
Asa more diverse corpus, the Brown corpus providesmany more opportunities for generalizing to new us-ages.
Our new SRL system handles these cases morerobustly, demonstrating the consistency and useful-ness of the thematic role categories.5.4 Improved Argument Distinction viaMappingThe ARG2-Mapped system generalizes well bothon the WSJ corpus and the Brown corpus.
In or-der to explore the improved robustness brought bythe mapping, we extracted and observed the 1,539instances to which the system ARG2-Mapped as-signed the correct semantic role label, but which thesystem ARG2-Original failed to predict.
From theconfusion matrix depicted in Table 5, we discoverthe following:The mapping makes ARG2 more clearly defined,and as a result there is a better distinction be-tween ARG2 and other argument labels: Amongthe 1,539 instances that ARG2-Original didn?t tagcorrectly, 233 instances are not assigned an argu-ment label, and 1,252 instances ARG2-Original con-fuse the ARG2 label with another argument label:the system ARG2-Original assigned the ARG2 la-bel to 50 ARG0?s, 716 ARG1?s, 1 ARG3 and 482ARGM?s, and assigned other argument labels to 3ARG2?s.5546 ConclusionsIn conclusion, we have described a mapping fromthe annotated PropBank corpus to VerbNet verbclasses with associated thematic role labels.
We hy-pothesized that these labels would be more verb-independent and less overloaded than the PropBankArgs2-5, and would therefore provide more consis-tent training instances which would generalize betterto new genres.
Our preliminary experiments confirmthis hypothesis, with a 6% performance improve-ment on the WSJ and a 10% performance improve-ment on the Brown corpus for Arg2.In future work, we will map the PropBank-edBrown corpus to VerbNet as well, which will allowmuch more thorough testing of our hypothesis.
Wewill also examine back-off to verb class membershipas a technique for improving performance on out ofvocabulary verbs.
Finally, we plan to explore the ef-fect of different thematic role groupings on systemperformance.ReferencesXavier Carreras and Llu?
?s Ma?rquez.
2005.
Introductionto the conll-2005 shared task: Semantic role labeling.In Proceedings of CoNLL.John Chen and Owen Rambow.
2003.
Use of deep lin-guistic features for the recognition and labeling of se-mantic arguments.
In Proceedings of EMNLP-2003,Sapporo, Japan.D.
R. Dowty.
1991.
Thematic proto-roles and argumentselection.
Language, 67:574?619.Daniel Gildea and Julia Hockenmaier.
2003.
Identifyingsemantic roles using Combinatory Categorial Gram-mar.
In 2003 Conference on Empirical Methods inNatural Language Processing (EMNLP), pages 57?64,Sapporo, Japan.Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H.Martin, and Daniel Jurafsky.
2003.
Shallow semanticparsing using support vector machines.
Technical re-port, The Center for Spoken Language Research at theUniversity of Colorado (CSLR).Beth Levin.
1993.
English Verb Classes and Alterna-tions: A Preliminary Investigation.
The University ofChicago Press.Edward Loper, Szu-ting Yi, and Martha Palmer.
2007.Empirical evidence for useful semantic role categories.In Proceedings of the International Workshop on Com-putational Linguistics.M.
Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,A.
Bies, M. Ferguson, K. Katz, and B. Schasberger.1994.
The Penn treebank: Annotating predicate argu-ment structure.Alessandro Moschitti.
2004.
A study on convolutionkernel for shallow semantic parsing.
In Proceedingsof the 42-th Conference on Association for Computa-tional Linguistic (ACL-2004), Barcelona, Spain.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: A corpus annotated withsemantic roles.
Computational Linguistics, 31(1):71?106.Sameer Pradhan, Kadri Hacioglu, Wayne Ward, H. Mar-tin, James, and Daniel Jurafsky.
2005a.
Semantic rolechunking combining complementary syntactic views.In Proceedings of CoNLL-2005.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin, and Dan Jurafsky.
2005b.
Semantic role la-beling using different syntactic views.
In Proceedingsof the Association for Computational Linguistics 43rdannual meeting (ACL-2005), Ann Arbor, MI.V.
Punyakanok, D. Roth, and W. Yih.
2005.
The ne-cessity of syntactic parsing for semantic role labeling.In Proceedings of the 19th International Joint Confer-ence on Artificial Intelligence (IJCAI-05).Karin Kipper Schuler.
2005.
VerbNet: A broad-coverage, comprehensive verb lexicon.
Ph.D. thesis,University of Pennsylvania.Kristina Toutanova, Aria Haghighi, and Christopher D.2005.
Joint learning improves semantic role labeling.In Proceedings of the Association for ComputationalLinguistics 43rd annual meeting (ACL-2005), Ann Ar-bor, MI.Szu-ting Yi and Martha Palmer.
2004.
Pushing theboundaries of semantic role labeling with svm.
In Pro-ceedings of the International Conference on NaturalLanguage Processing.Szu-ting Yi and Martha Palmer.
2005.
The integration ofsyntactic parsing and semantic role labeling.
In Pro-ceedings of CoNLL-2005.555
