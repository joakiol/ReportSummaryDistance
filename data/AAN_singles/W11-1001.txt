Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1?9,ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational LinguisticsAutomatic Projection of Semantic Structures:an Application to Pairwise Translation RankingDaniele Pighin Llu?
?s Ma`rquezTALP Research CenterUniversitat Polite`cnica de Catalunya{pighin,lluism}@lsi.upc.eduAbstractWe present a model for the inclusion of se-mantic role annotations in the framework ofconfidence estimation for machine translation.The model has several interesting properties,most notably: 1) it only requires a linguis-tic processor on the (generally well-formed)source side of the translation; 2) it doesnot directly rely on properties of the transla-tion model (hence, it can be applied beyondphrase-based systems).
These features makeit potentially appealing for system ranking,translation re-ranking and user feedback eval-uation.
Preliminary experiments in pairwisehypothesis ranking on five confidence estima-tion benchmarks show that the model has thepotential to capture salient aspects of transla-tion quality.1 IntroductionThe ability to automatically assess the quality oftranslation hypotheses is a key requirement to-wards the development of accurate and depend-able translation models.
While it is largely agreedthat proper transfer of predicate-argument structuresfrom source to target is a very strong indicator oftranslation quality, especially in relation to ade-quacy (Lo and Wu, 2010a; 2010b), the incorpora-tion of this kind of information in the Statistical Ma-chine Translation (SMT) evaluation pipeline is stilllimited to few and isolated cases, e.g., (Gime?nez andMa`rquez, 2010).In this paper, we propose a general model forthe incorporation of predicate-level semantic anno-tations in the framework of Confidence Estimation(CE) for machine translation, with a specific focuson the sub-problem of pairwise hypothesis ranking.The model is based on the following underlying as-sumption: by observing how automatic alignmentsproject semantic annotations from source to targetin a parallel corpus, it is possible to isolate featuresthat are characteristic of good translations, such asmovements of specific arguments for some classesof predicates.
The presence (or absence) of thesefeatures in automatic translations can then be used asan indicator of their quality.
It is important to stressthat we are not claiming that the projections pre-serve the meaning of the original annotation.
Still,it should be possible to observe regularities that canbe helpful to rank alternative translation hypotheses.The general workflow (which can easily be ex-tended to cope with different annotation layers,such as sequences of meaningful phrase boundaries,named entities or sequences of chunks or POS tags)is exemplified in Figure 1.
During training (on theleft), the system receives a parallel corpus of sourcesentences and the corresponding reference transla-tions.
Source sentences are annotated with a lin-guistic processor.
The annotations are projected us-ing training alignments, obtaining gold projectionsthat we can use to learn a model that captures cor-rect annotation movements, i.e., observed in refer-ence translations.
At test time, we want to assessthe quality of a translation hypothesis given a sourcesentence.
As shown on the right side of Figure 1, thefirst part of the process is the same as during train-ing: the source sentence is annotated, and the an-notation is projected onto the translation hypothesisvia automatic alignments.
The model is then used1References Source Source HypothesisAlign Annotate Annotate AlignProject ProjectLearn Compare ScoreAlignmentsParallelAnnotationsModelAlignmentsAnnotationsTraining TestFigure 1: Architectural overview.to compare the observed projection against the ex-pected projection given the source annotation.
Thedistance between the two projections (observed andexpected) can then be used as a measure of the qual-ity of the hypothesis.As it only considers one-sided annotations, ourframework does not require the availability of com-parable linguistic processors and linguistic annota-tions, tagsets, etc., on both sides of the translationprocess.
In this way, it overcomes one of the mainobstacles to the adoption of linguistic analysis forMT confidence estimation.
Furthermore, the factthat source data is generally well-formed lowers therequirements on the linguistic processor in terms ofrobustness to noisy data, making it possible to em-ploy a wider range of linguistic processors.Within this framework, in this paper we describeour attempt to bridge Semantic Role Labeling (SRL)and CE by modeling proposition-level semantics forpairwise translation ranking.
The extent to whichthis kind of annotations are transferred from sourceto target has indeed a very high correlation with re-spect to human quality assessments (Lo and Wu,2010a; 2010b).
The measure that we propose is thenan ideal addition to already established CE mea-sures, e.g., (Specia et al, 2009; Blatz et al, 2004),as it attempts to explicitly model the adequacy oftranslation hypotheses as a function of predicate-argument structure coverage.
While we are aware ofthe fact that the current definition of the model canbe improved in many different ways, our preliminaryinvestigation, on five English to Spanish translationbenchmarks, shows promising accuracy on the dif-ficult task of pairwise translation ranking, even fortranslations with very few distinguishing features.To capture different aspects of the projection ofSRL annotations we employ two instances of theabstract architecture shown in Figure 1.
The firstworks at the proposition level, and models the cor-rect movement of arguments from source to target.The second works at the argument level, and modelsthe fluency and adequacy of individual argumentswithin each predicate-argument structure.
The mod-els that we learn during training are simple phrase-based translation models working on different kindsof sequences, i.e., role labels in the former case andwords in the latter.
To evaluate the adequacy of anautomatically projected proposition or argument, weforce the corresponding translation model to gener-ate it (via constrained decoding).
The reachabilityand confidence of each translation are features thatwe exploit to compare alternative translations, bycombining them in a simple voting scheme.To score systems which are not under our directcontrol (the typical scenario in CE benchmarks), weintroduce a component that generates source-targetalignments for any pair of aligned test sentences.This addition has the nice property of allowing usto handle the translation as a black-box, decouplingthe evaluation from a specific system and, in theory,allowing the model to cope with phrase-based, rule-based or hierarchical systems alike, as well as withhuman-generated translations.The rest of the paper is structured as follows: inSection 2 we will review a selection of related work;in Section 3 we will detail our approach; in Section 4we will present the results of our evaluation; finally,in Section 5 we will draw our conclusions.2 Related workConfidence estimation is the sub-problem withinMT evaluation concerned with the assessment oftranslation quality in the absence of reference trans-lations.
A relevant initial work on this topic isthe survey by Blatz et al (2004), in which the au-thors define a rich set of features based on sourcedata, translation hypotheses, n-best lists and modelcharacteristics to classify translations as ?good?or ?bad?.
In their observations, they conclude2that the most relevant features are those based onsource/target pairs and on characteristics of thetranslation model.Specia et al (2009) build on top these results bydesigning a feature-selection framework for confi-dence estimation.
Translations are considered asblack-boxs (i.e., no system or model-dependent fea-tures are employed), and novel features based on thenumber of content words, a POS language model onthe target side, punctuation and number matchers insource and target translations and the percentage ofuni-grams are introduced.
Features are selected viaPartial Least Squares (PLS) regression (Wold et al,1984).
Inductive Confidence Machines (Papadopou-los et al, 2002) are used to estimate an optimalthreshold to distinguish between ?good?
and ?bad?translations.
Even though the authors show that asmall set of shallow features and some supervisioncan produce good results on a specific benchmark,we are convinced that more linguistic features areneeded for these methods to perform better across awider spectrum of domains and applications.Concerning the usage of SRL for SMT, Wu andFung (2009) reported a first successful application ofsemantic role labels to improve translation quality.They note that improvements in translation qualityare not reflected by traditional MT evaluation met-rics (Doddington, 2002; Papineni et al, 2002) basedon n-gram overlaps.
To further investigate the topic,Lo and Wu (2010a; 2010b) involved human annota-tors to demonstrate that the quality of semantic roleprojection on translated sentences is very highly cor-related with human assessments.Gime?nez and Ma`rquez (2010) describe a frame-work for MT evaluation and meta-evaluation com-bining a rich set of n-gram-based and linguistic met-rics, including several variants of a metric based onSRL.
Automatic and reference translations are anno-tated independently, and the lexical overlap betweencorresponding arguments is employed as an indica-tor of translation quality.
The authors show that syn-tactic and semantic information can achieve higherreliability in system ranking than purely lexical mea-sures.Our original contribution lies in the attempt to ex-ploit SRL for assessing translation quality in a CEscenario, i.e., in the absence of reference transla-tions.
By accounting for whole predicate-argumentsequences as well as individual arguments, ourmodel has the potential to capture aspects whichrelate both to the adequacy and to the fluency ofa translation.
Furthermore, we outline a generalframework for the inclusion of linguistic processorsin CE that has the advantage of requiring resourcesand software tools only on the source side of thetranslation, where well-formed input can reasonablybe expected.3 ModelThe task of semantic role labeling (SRL) consistsin recognizing and automatically annotating seman-tic relations between a predicate word (not nec-essarily a verb) and its arguments in natural lan-guage texts.
The resulting predicate-argument struc-tures are commonly referred to as propositions, eventhough we will also use the more general term anno-tations.In PropBank (Palmer et al, 2005) style anno-tations, which our model is based on, predicatesare generally verbs and roles are divided into twoclasses: core roles (labeled A0, A1, .
.
.
A5), whosesemantic value is defined by the predicate syntacticframe, and adjunct roles (labeled AM-*, e.g., AM-TMP or AM-LOC) 1 which are a closed set of verb-independent semantic labels accounting for predi-cate aspects such as temporal, locative, manner orpurpose.
For instance, in the sentence ?The com-mission met to discuss the problem?
we can iden-tify two predicates, met and discuss.
The corre-sponding annotations are ?
[A0 The commission] [predmet] [AM-PRP to discuss the problem]?
and ?
[A0 Thecommission] met to [pred discuss] [A1 the problem]?.Here, A0 and A1 play the role of prototypical sub-ject and object, respectively, and AM-PRP is an ad-junct modifier expressing a notion of purpose.Sentence annotations are inherently non-sequential, as shown by the previous example inwhich the predicate and one of the arguments ofthe second proposition (i.e., discuss and A1) arecompletely embedded within an argument of thefirst proposition (i.e., AM-PRP).
Following a widelyadopted simplification, the annotations in a sentenceare modeled independently.
Furthermore we de-1The actual role labels are in the form Arg0, .
.
.
Arg1 andArgM-*, but we prefer to adopt their shorter form.3scribe each annotation at two levels: a propositionlevel, where we model the movement of argumentsfrom source to target; and an argument level, werewe model the adequacy and fluency of individualargument translations.
The comparison of twoalternative translations takes into account all thesefactors but it models each of them independently,i.e., we consider how properly each propositions isrendered in each hypothesis, and how properly eachargument is translated within each proposition.3.1 Annotation and argument projectionAt the proposition level, we simply represent the se-quence of role-label in each proposition, ignoringtheir lexical content with the exception of the pred-icate word.
Considering the previous example, thesentence would then be represented by the two se-quences ?A0 met AM-PRP?
and ?A0 * discuss A1?.In the latter case, the special character ?*?
marksa ?gap?
between A0 and the predicate word.
Theannotation is projected onto the translation via di-rect word alignments obtained through a constrainedmachine translation process (i.e., we force the de-coder to generate the desired translation).
Eventualdiscontinuities in the projection of an argument aremodeled as gaps.
If two arguments insist on a sharedsubset of words, then their labels are combined.
Ifthe projection of an argument is a subset of the pro-jection of the predicate word, then the argument isdiscarded.
If the overlap is partial, then the non-overlapping part of the projection is represented.If a word insertion occurs next to an argumentor the predicate, then we include it in the final se-quence.
This decision is motivated by the consider-ation that insertions at the boundary of an argumentmay be a clue of different syntactic realizations ofthe same predicate across the two languages (Levin,1993).
For example, the English construct ?A0 giveA2 A1?
could be rendered as ?doy A1 a A2?
in Span-ish.
Here, the insertion of the preposition ?a?
at de-coding can be an important indicator of translationquality.This level of detail is insufficient to model someimportant features of predicate-argument structures,such as inter-argument semantic or syntactic depen-dencies, but it is sufficient to capture a variety ofinteresting linguistic phenomena.
For instance, A0-predicate inversion translating SVO into VSO lan-guages, or the convergence of multiple source argu-ments into a single target argument when translatinginto a morphologically richer language.
We shouldalso stress again that we are not claiming that thestructures that we observe on the target side are lin-guistically motivated, but only that they contain rel-evant clues to assess quality aspects of translation.As for the representation of individual arguments,we simply represent their surface form, i.e., thesequence of words spanning each argument.
So,for example, the argument representations extractedfrom ?
[A0 The commission] [pred met] [AM-PRP todiscuss the problem]?
would be ?The commission?,?met?, ?to discuss the problem?.
To project each ar-gument we align all its words with the target side.The leftmost and the rightmost aligned words de-fine the boundaries of the argument in the target sen-tence.
All the words in between (including eventualgaps) are considered as part of the projection of theargument.
This approach is consistent with Prop-Bank style annotations, in which arguments are con-tiguous word sequences, and it allows us to employ astandard translation model to evaluate the fluency ofthe argument projection.
The rationale here is thatwe rely on proposition level annotations to conveythe semantic structure of the sentence, while at theargument level we are more interested in evaluatingthe lexical appropriateness of their realization.The projection of a proposition and its argumentsfor an example sentence is shown in Figure 2.
Here,s is the original sentence and h1 and h2 are twotranslation hypotheses.
The figure shows how thewhole proposition (p) and the predicate word (pred)along with its arguments (A0, A1 and A2) are repre-sented after projection on the two hypotheses.
As wecan observe, in both cases thank (the predicate word)gets aligned with the word gracias.
For h1, the de-coder aligns I (A0) to doy, leaving a gap between A0and the predicate word.
The gap gets filled by gen-erating the word las.
Since the gap is adjacent to atleast one argument, las is included in the representa-tion of p for h1.
In h2, the projection of A0 exactlyoverlaps the projection of the predicate (?Gracias?
),and therefore A0 is not included in n for h2.3.2 Comparing hypothesesAt test time, we want to use our model to com-pare translation pairs and recognize the most reli-4s I thank the commissioner for the detailed replyh1 Doy las gracias al comisario por la detallada respuestah2 Gracias , al sen?or comisario por para el respuestap A0 thank A1 A2 pred thankh1 A0 +las gracias A1 A2 h1 graciash2 Gracias A1 A2 h2 GraciasA1 the commissioner A0 Ih1 al comisario h1 doyh2 al sen?or comisario h2 GraciasA2 for the detailed replyh2 por la detallada respuestah2 para el respuestaFigure 2: Comparison between two alternative transla-tions h1 and h2 for the source sentence s.able.
Let s be the source sentence, and h1 and h2be two translation hypotheses.
For each propositionp in s, we assign a confidence value to its represen-tation in h1 and h2, i.e., p1 and p2, by forcing theproposition-level translation system to generate theprojection observed in the corresponding hypothe-sis.
The reachability of p1 (respectively, p2) and thedecoder confidence in translating p as p1 are used asfeatures to estimate p1 (p2) accuracy.
Similarly, foreach argument a in each proposition p we generateits automatic projection on h1 and h2, i.e., a1 anda2.
We force the argument-level decoder to translatea into a1 and a2, and use the respective reachabilityand translation confidence as features accounting fortheir appropriateness.The best translation hypothesis (h1 or h2) is thenselected according to the following decision func-tion:h?
= argmaxi?
{0,1}?kfk(hi, hj 6=i, s) (1)where each feature function fk(?, ?, ?)
defines a com-parison measure between its first two arguments, andreturns 1 if the first argument is greater (better) thanthe second, and 0 otherwise.
In short, the decisionfunction selects the hypothesis that wins the highestnumber of comparisons.The feature functions that we defined accountfor the following factors, the last three being eval-uated once for each proposition in s: (1) Num-ber of successfully translated propositions; (2) Av-erage translation confidence for projected proposi-tions; (3) Number of times that a proposition in hihas higher confidence than the corresponding propo-sition in hi 6=j ; (4) Number of successfully translatedarguments; (5) Average translation confidence forprojected arguments; (6) Number of times that anargument in hi has higher confidence than the corre-sponding argument in hi 6=j .With reference to Figure 2, the two translation hy-potheses have been scored 4 (very good) and 2 (bad)by human annotators.
The score assigned by theproposition decoder to p1 is higher than p2, hencecomparisons (2) and (3) are won by h1.
Accord-ing to the arguments decoder, h1 does a better jobat representing A0 and A2; h2 is better at renderingA1, and pred is a tie.
Therefore, h1 also prevailsaccording to (6).
Given the very high confidence as-signed to the translation of A2 in h1, the hypothesisalso prevails in (5).
In this case, (1) and (4) do notcontribute to the decision as the two projections havethe same coverage.4 EvaluationIn this section, we present the results obtained byapplying the proposed method to the task of rank-ing consistency, or pairwise ranking of alternativetranslations: that is, given a source sentence s, andtwo candidate translations h1 and h2, decide whichone is a better translation for s. Pairwise rankingis a simplified setting for CE that is general enoughto model the selection of the best translation amonga finite set of alternatives.
Even though it cannotmeasure translation quality in isolation, a reliablepairwise ranking model would be sufficient to solvemany common practical CE problems, such as sys-tem ranking, user feedback filtering or hypothesesre-ranking.4.1 DatasetsWe ran our experiments on the human assessmentsreleased as part of the ACL Workshops on MachineTranslations in 2007 (Callison-Burch et al, 2007),2008 (Callison-Burch et al, 2008), 2009 (Callison-Burch et al, 2009) and 2010 (Callison-Burch et al,2010).
These datasets will be referred to as wm-tYY(t) in the remainder, YY being the last two digitsof the year of the workshop and t = n for newswiredata or t = e for Europarl data.
So, for example,wmt08e is the Europarl test set of the 2008 edition5of the workshop.
As our system is trained on Eu-roparl data, newswire test sets are to be consideredout-of-domain.
All the experiments are relative toEnglish to Spanish translations.The wmt08, wmt09 and wmt10 datasets providea ranking among systems within the range [1,5] (1being the worst system, and 5 the best).
The dif-ferent datasets contain assessments for a differentnumber of systems, namely: 11 for wmt08(e), 10 forwmt08(n), 9 for wmt09 and 16 for wmt10n.
Gener-ally, multiple annotations are available for each an-notated sentence.
In all cases in which multiple as-sessments are available, we used the average of theassessments.The wmt07 dataset would be the most interestingof all, in that it provides separate assessments forthe two main dimensions of translation quality, ade-quacy and fluency, as well as system rankings.
Un-luckily, the number of annotations in this dataset isvery small, and after eliminating the ties the num-bers are even smaller.
As results on such small num-bers would not be very representative, we decidednot to include them in our evaluation.We also evaluated on the dataset describedin (Specia et al, 2010), which we will refer to asspecia.
As the system is based on Europarl data, itis to be considered an in-domain benchmark.
Thedataset includes results produced by four differentsystems, each translation being annotated by onlyone judge.
Given the size of the corpus (the outputof each system has been annotated on the same setof 4,000 sentences), this dataset is the most repre-sentative among those that we considered.
It is alsoespecially interesting for two other reasons: 1) sys-tems are assigned a score ranging from 1 (bad) to 4(good as it is) based on the number of edits requiredto produce a publication-ready translation.
There-fore, here we have an absolute measure of transla-tion accuracy, as opposed to relative rankings; 2)each system involved in the evaluation has very pe-culiar characteristics, hence they are very likely togenerate quite different translations for the same in-put sentences.4.2 SetupOur model consists of four main components: anautomatic semantic role labeler (to annotate sourcesentences); a lexical translation model (to gener-ate the alignments required to map the annotationsonto a translation hypothesis); a translation modelfor predicate-argument structures, to assign a scoreto projected annotations; and a translation model forrole fillers, to assign a score to the projection of eachargument.To automatically label our training data with se-mantic roles we used the Swirl system2 (Surdeanuand Turmo, 2005) with the bundled English mod-els for syntactic and semantic parsing.
On theCoNLL-2005 benchmark (Carreras and Ma`rquez,2005), Swirl sports an F1-measure of 76.46.
Thisfigure drops to 75 for mixed data, and to 65.42 onout-of-domain data, which we can regard as a con-servative estimate of the accuracy of the labeler onwmt benchmarks.For all the translation tasks we employed theMoses phrase-based decoder3 in a single-factor con-figuration.
The -constraint command line pa-rameter is used to force Moses to output the desiredtranslation.
For the English to Spanish lexical trans-lation model, we used an already available modellearned using all available wmt10e data.To build the proposition level translation system,we first annotated all the English sentences from thewmt10e (en?es) training set with Swirl; then, weforced the lexical translation model to generate thealignments for the reference translations and pro-jected the annotations on the target side.
The processresulted in 2,493,476 parallel annotations.
5,000 an-notations were held-out for model tuning.
The train-ing data was used to estimate a 5-gram languagemodel and the translation model, which we later op-timized on held-out data.As for the argument level translator, we trainedit on parallel word sequences spanning the samerole in an annotation and its projection.
Each suchpair constitutes a training example for the argu-ment translator, each argument representation beingmodeled independently from the others.
With thesame setup used for the proposition translator, wecollected 4,578,480 parallel argument fillers fromwmt10e en?es training data, holding out 20,000pairs for model tuning.2http://www.surdeanu.name/mihai/swirl/3http://www.statmt.org/moses/64.3 A note on recallThe main limitation of the model in its current im-plementation is its low recall.
The translation modelthat we use to generate the alignments is mostly re-sponsible for it.
In fact, in approximately 35% of thecases the constrained translation model is not ableto generate the required hypothesis.
An obvious im-provement would consist in using just an alignmentmodel for this task, instead of resorting to transla-tion, for instance following the approach adopted in(Espla` et al, 2011).
It should also be noted that,while this component adds the interesting propertyof decoupling the measure from the system that pro-duced the hypothesis, it is not strictly necessary inall those cases in which translation alignments arealready available, e.g., for N-best re-ranking.The second component that suffers from recallproblems is the semantic role labeler, which fails inannotating sentences in approximately 6% of the re-maining cases.
These failures are by and large dueto the lack of proper verbal predicates in the targetsentence, and as such expose a limiting factor of theunderlying model.
In another 3% of the cases, anannotation is produced but it cannot be projected onthe hypothesis, since the predicate word on the targetside gets deleted during translation.Another important consideration is that no mea-sure for CE is conceived to be used in isolation, andour measure is no exception.
In combination withothers, the measure should only trigger when ap-propriate, i.e., when it is able to capture interestingpatterns that are significant to discriminate transla-tion quality.
If it abstains, the other measures wouldcompensate for the missing values.
In this respect,we should also consider that not being able to pro-duce a translation may be inherently considered anindicator of translation quality.4.4 ResultsTable 1 lists, in each block of rows, pairwise classifi-cation accuracy results obtained on a specific bench-mark.
The benchmarks are sorted in order of re-verse relevance, the largest benchmark (specia) be-ing listed first.
In each row, we show results obtainedfor different configurations in which the variable isthe distance d between two assessment scores.
So,for example, the row d = 1 accounts for all thespecia Corr Wrong Und(%) Acc(%)d = 1 1076 656 14.26 62.12d = 2 272 84 11.00 76.40d = 3 30 8 13.64 78.95d ?
1 1378 748 13.72 64.82d ?
2 302 92 11.26 76.65d ?
3 30 8 13.64 78.95wmt10n Corr Wrong Und(%) Acc(%)d = 1 428 374 15.04 53.37d = 2 232 196 18.01 54.21d = 3 98 74 16.50 56.98d ?
1 784 664 16.20 54.14d ?
2 356 290 17.60 55.11d ?
3 124 94 16.79 56.88wmt09n Corr Wrong Und(%) Acc(%)d = 1 70 60 19.75 53.85d = 2 30 40 20.45 42.86d = 3 26 10 18.18 72.22d ?
1 134 116 19.87 53.60d ?
2 64 56 20.00 53.33d ?
3 34 16 19.35 68.00wmt08n Corr Wrong Und(%) Acc(%)d = 1 64 36 12.28 64.00d = 2 26 24 19.35 52.00d = 3 12 6 18.18 66.67d ?
1 104 70 14.71 59.77d ?
2 40 34 17.78 54.05d ?
3 14 10 14.29 58.33wmt08e Corr Wrong Und(%) Acc(%)d = 1 62 34 21.31 64.58d = 2 40 30 10.26 57.14d = 3 22 8 11.76 73.33d ?
1 134 80 15.75 62.62d ?
2 72 46 10.61 61.02d ?
3 32 16 11.11 66.67Table 1: Results on five confidence estimation bench-marks.
An n next to the task name (e.g.
wmt08n) standsfor a news (i.e.
out of domain) corpus, whereas an e (e.g.wmt08e) stands for a Europarl (i.e.
in domain) corpus.The specia corpus is in-domain.comparisons in which the distance between scoresis exactly one, while row d ?
2 considers all thecases in which the distance is at least 2.
For eachtest, the columns show: the number of correct (Corr)and wrong (Wrong) decisions, the percentage of un-decidable cases (Und), i.e., the cases in which thescoring function cannot decide between the two hy-potheses, and the accuracy of classification (Acc)measured without considering the unbreakable ties.7The accuracy for d ?
1, i.e., on all the availableannotations, is shown in bold.First, we can observe that the results are above thebaseline (an accuracy of 50% for evenly distributedbinary classification) on all the benchmarks and forall configurations.
The only outlier is wmt09n ford = 2, with an accuracy of 42.86%.
Across thedifferent datasets, results vary from promising (spe-cia and wmt08e, where accuracy is generally above60%) to mildly good (wmt10n), but across all theboard the method seems to be able to provide usefulclues for confidence estimation.As expected, the accuracy of classification tendsto increase as the difference between hypotheses be-comes more manifest.
In four cases out of six, theaccuracy for d = 3 is above 60%, with the notablepeaks on specia, wmt09n and wmt08e where it goesover 70% (on the first, it arrives almost at 80%).Unluckily, very few translations have very differentquality (a measure of the difficulty of the task).
Nev-ertheless, the general trend seems to support the re-liability of the approach.When we consider the results on the wholedatasets (i.e., d ?
1), pairwise classification accu-racy ranges from 54% (for wmt09n and wmt10n,both out-of-domain), to 63-64% (for specia andwmt08e, both in-domain).
Interestingly, the perfor-mance on wmt08n, which is also out-of-domain, iscloser to in-domain benchmarks, i.e., 60%.
Thesefigures suggest that the method is consistently reli-able on in-domain data, but also out-of-domain eval-uation can benefit from its application.
The differ-ence in performance between wmt08n and the otherout-of-domain benchmarks will be reason of furtherinvestigation as future work, as well as the drop inperformance for d = 2 on three of the benchmarks.5 ConclusionsWe have presented a model to exploit the rich in-formation encoded by predicate-argument structuresfor confidence estimation in machine translation.The model is based on a battery of translation sys-tems, which we use to study the movement andthe internal representation of propositions and ar-guments projected from source to target via auto-matic alignments.
Our preliminary results, obtainedon five different benchmarks, suggest that the ap-proach is well grounded and that semantic annota-tions have the potential to be successfully employedfor this task.The model can be improved in many ways, its ma-jor weakness being its low recall as discussed in Sec-tion 4.3.
Another area in which there is margin forimprovement is the representation of predicate ar-gument structures.
It is reasonable to assume thatdifferent representations could yield very differentresults.
Introducing more clues about the seman-tic content of the whole predicate argument struc-ture, e.g., by including argument head words in therepresentation of the proposition, or considering amore fine-grained representation at the propositionlevel, could make it possible to assess the quality ofa translation reducing the need to back-off to indi-vidual arguments.
As for the representation of ar-guments, a first and straightforward improvementwould be to train a separate model for each argumentclass, or to move to a factored model that would al-low us to model explicitly the insertion of words orthe overlap of argument words due to the projection.Another important research direction involves thecombination of this measure with already assessedmetric sets for CE, e.g., (Specia et al, 2010), to un-derstand to what extent it can contribute to improvethe overall performance.
In this respect, we wouldalso like to move from a heuristic scoring functionto a statistical model.Finally, we would like to test the generality of theapproach by designing other features based on thesame ?annotate, project, measure?
framework, as westrongly believe that it is an effective yet simple wayto combine several linguistic features for machinetranslation evaluation.
For example, we would liketo apply a similar framework to model the movementof chunks or POS sequences.AcknowledgmentsWe would like to thank the anonymous reviewers for theirvaluable comments.
This research has been partially fundedby the Spanish Ministry of Education and Science (OpenMT-2, TIN2009-14675-C03) and the European Community?s Sev-enth Framework Programme (FP7/2007-2013) under grantagreement numbers 247762 (FAUST project, FP7-ICT-2009-4-247762) and 247914 (MOLTO project, FP7-ICT-2009-4-247914).8ReferencesJohn Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2004.
Confidence estimation formachine translation.
In Proceedings of the 20th in-ternational conference on Computational Linguistics,COLING ?04, Stroudsburg, PA, USA.
ACL.Chris Callison-Burch, Philipp Koehn, Cameron ShawFordyce, and Christof Monz, editors.
2007.
Proceed-ings of the Second Workshop on Statistical MachineTranslation.
ACL, Prague, Czech Republic.Chris Callison-Burch, Philipp Koehn, Christof Monz,Josh Schroeder, and Cameron Shaw Fordyce, editors.2008.
Proceedings of the Third Workshop on Statisti-cal Machine Translation.
ACL, Columbus, Ohio.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder, editors.
2009.
Proceedings of theFourth Workshop on Statistical Machine Translation.ACL, Athens, Greece.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, and Omar Zaidan, editors.
2010.
Pro-ceedings of the Joint Fifth Workshop on Statistical Ma-chine Translation and MetricsMATR.
ACL, Uppsala,Sweden.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduc-tion to the CoNLL-2005 shared task: Semantic rolelabeling.
In Proceedings of the Ninth Conference onComputational Natural Language Learning (CoNLL-2005), pages 152?164, Ann Arbor, Michigan, June.Association for Computational Linguistics.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proceedings of the second interna-tional conference on Human Language TechnologyResearch, HLT ?02, pages 138?145, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Miquel Espla`, Felipe Sa?nchez-Mart?
?nez, and Mikel L.Forcada.
2011.
Using word alignments to assistcomputer-aided translation users by marking whichtarget-side words to change or keep unedited.
In Pro-ceedings of the 15th Annual Conference of the Euro-pean Associtation for Machine Translation.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2010.
Lin-guistic measures for automatic machine transla-tion evaluation.
Machine Translation, 24:209?240.10.1007/s10590-011-9088-7.Beth Levin.
1993.
English Verb Classes and Alterna-tions.
The University of Chicago Press.Chi-kiu Lo and Dekai Wu.
2010a.
Evaluating machinetranslation utility via semantic role labels.
In Pro-ceedings of the Seventh conference on InternationalLanguage Resources and Evaluation (LREC?10), Val-letta, Malta.
European Language Resources Associa-tion (ELRA).Chi-kiu Lo and Dekai Wu.
2010b.
Semantic vs. syntac-tic vs. n-gram structure for machine translation evalu-ation.
In Proceedings of the 4th Workshop on Syntaxand Structure in Statistical Translation, pages 52?60,Beijing, China.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Corpusof Semantic Roles.
Comput.
Linguist., 31(1):71?106.Harris Papadopoulos, Kostas Proedrou, Volodya Vovk,and Alexander Gammerman.
2002.
Inductive confi-dence machines for regression.
In AMAI?02.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.
ACL.Lucia Specia, Marco Turchi, Zhuoran Wang, JohnShawe-Taylor, and Craig Saunders.
2009.
Improv-ing the confidence of machine translation quality es-timates.
In Machine Translation Summit XII, Ottawa,Canada.Lucia Specia, Nicola Cancedda, and Marc Dymetman.2010.
A dataset for assessing machine translationevaluation metrics.
In Proceedings of the Seventhconference on International Language Resources andEvaluation (LREC?10), Valletta, Malta.
European Lan-guage Resources Association (ELRA).Mihai Surdeanu and Jordi Turmo.
2005.
Seman-tic role labeling using complete syntactic analysis.In Proceedings of the Ninth Conference on Compu-tational Natural Language Learning (CoNLL-2005),pages 221?224, Ann Arbor, Michigan, June.
Associ-ation for Computational Linguistics.S.
Wold, A. Ruhe, H Wold, and W.J.
Dunn.
1984.
Thecollinearity problem in linear regression.
the partialleast squares (pls) approach to generalized inverses.5:735?743.Dekai Wu and Pascale Fung.
2009.
Semantic roles forSMT: a hybrid two-pass model.
In Proceedings of Hu-man Language Technologies: The 2009 Annual Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics, Companion Vol-ume: Short Papers, NAACL-Short ?09, pages 13?16,Stroudsburg, PA, USA.
ACL.9
