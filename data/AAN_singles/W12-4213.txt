Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 111?118,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsZero Pronoun Resolution can Improve the Quality of J-E TranslationHirotoshi Taira, Katsuhito Sudoh, Masaaki NagataNTT Communication Science Laboratories2-4, Hikaridai, Seika-cho, Keihanna Science CityKyoto 619-0237, Japan{taira.hirotoshi,sudoh.katsuhito,nagata.masaaki}@lab.ntt.co.jpAbstractIn Japanese, particularly, spoken Japanese,subjective, objective and possessive cases arevery often omitted.
Such Japanese sentencesare often translated by Japanese-English sta-tistical machine translation to the English sen-tence whose subjective, objective and posses-sive cases are omitted, and it causes to de-crease the quality of translation.
We per-formed experiments of J-E phrase based trans-lation using Japanese sentence, whose omittedpronouns are complemented by human.
Weintroduced ?antecedent F-measure?
as a scorefor measuring quality of the translated En-glish.
As a result, we found that it improvesthe scores of antecedent F-measure while theBLEU scores were almost unchanged.
Everyeffectiveness of the zero pronoun resolutiondiffers depending on the type and case of eachzero pronoun.1 IntroductionToday, statistical translation systems have been ableto translate between languages at high accuracy us-ing a lot of corpora .
However, the quality of trans-lation of Japanese to English is not high compar-ing with the other language pairs that have the sim-ilar syntactic structure such as the French-Englishpair.
Particularly, the quality of translation fromspoken Japanese to English is in low.
There aremany reasons for the low quality.
One is the dif-ferent syntactic structures, that is, Japanese sentencestructure is SOV while English one is SVO.
Thisproblem has been partly solved by head finalizationtechniques (Isozaki et al, 2010).
Another big prob-lem is that subject, object and possessive cases areoften eliminated in Japanese, particularly, spokenJapanese (Nariyama, 2003).
In the case of Japaneseto English translation, the source language has lesserinformation in surface than the target language, andthe quality of the translation tends to be low.
Weshow the example of the omissions in Fig 1.
In thisexample, the Japanese subject watashi wa (?I?)
andthe object anata ni (?to you?)
are eliminated in thesentence.
These omissions are not problems for hu-man speakers and hearers because people easily rec-ognize who is the questioner or responder (that is,?I?
and ?you?)
from the context.
However, gener-ally speaking, the recognition is difficult for statisti-cal translation systems.Some European languages allow the eliminationof subject.
We show an example in Spanish in Fig 2.In this case, the subject is eliminated, and it leavestraces including the case and the sex, on the relatedverb.
The Spanish word, tengo is the first personsingular form of the verb, tener (it means ?have?
).So it is easier to resolve elimination comparing withJapanese one for SMT.Otherwise, Japanese verbs usually have no inflec-tional form depending on the case and sex.
So,we need take another way for elimination resolu-tion.
For example, if the eliminated Japanese sub-ject is always ?I?
when the sentence is declara-tive, and the subject is always ?you?
when the sen-tence is a question sentence, phrase based transla-tion systems are probably able to translate subject-eliminated Japanese sentences to correct Englishsentences.
However, the hypothesis is not always111Jpn: (watashi wa)  (anata ni) shoushou ukagai tai  koto ga ari masu .Eng:   I have     some  questions    to        ask     to   you    .Omission ofsubjectOmission ofobjectFigure 1: Example of Japanese Ellipsis (Zero Pronoun)Spa: (yo) Tengo   algunas preguntas  para  hacerle a  usted  .Eng:   I have     some  questions    to        ask     to   you    .Omission of subjectFigure 2: Spanish Ellipsistrue.In this paper, we show that the quality of spokenJapanese to English translation can improve usinga phrase-based translation system if we can use anideal elimination resolution system.
However, wealso show that a simple elimination resolution sys-tem is not effective to the improvement and it is nec-essary to recognize correctly the modality of the sen-tence.2 Previous WorkThere are a few researches for adaptation of ellip-sis resolution to statistical translation systems whilethere are a lot of researches for one to rule-basedtranslation systems in Japanese (Yoshimoto, 1988;Dohsaka, 1990; Nakaiwa and Yamada, 1997; Ya-mamoto et al, 1997).As a research of SMT using elimination resolu-tion, we have (Furuichi et al, 2011).
However, thetarget of the research is illustrative sentences in En-glish to Japanese dictionary.
Our research aims spo-ken language translation and it is different from thepaper.3 Setup of the Data of Subjects andObjects Ellipsis in Spoken Japanese3.1 Ellipsis Resolved Data by HumanIn this section, we describe the data used in our ex-periments.
We used BTEC (Basic Travel Expres-sion Corpus) corpus (Kikui et al, 2003) distributedin IWSLT07 (Fordyce, 2007).
The corpus consistsof tourism-related sentences similar to those thatare usually found in phrasebooks for tourists goingabroad.
The characteristics of the dataset are shownin Table 1.
We used ?train?
for training, ?devset1-3?
for tuning, and ?test?
for evaluation.
We did notuse the ?devset4?
and ?devset5?
sets because of thedifferent number of English references.We annotated zero pronouns and the antecedentsto the sentences by hand.
Here, zero pronoun is de-fined as an obligatory case noun phrase that is notexpressed in the utterance but can be understoodthrough other utterances in the discourse, context, orout-of-context knowledge (Yoshimoto, 1988).
Weannotated the zero pronouns based on pronouns inthe translated English sentences.
The BTEC corpushas multi-references in English.
We first chose themost syntactically and lexically similar translationin the references and annotated zero pronouns in it.Our target pronouns are I, my, me, mine, myself, we,our, us, ours, ourselves, you, your, yourself, your-selves, he, his, him, himself, she, her, herself, it, its,itself, they, their, them, theirs and themselves in En-glish.
We show the distribution of the annotationtypes in the test set in Table 2.3.2 Baseline SystemWe also examined a simple baseline zero pronounresolution system for the same data.
We defined112Table 1: Data distributiontrain devset1-3 devset4 devset5 test# of References 1 16 7 7 16# of Source Segments 39,953 1,512 489 500 489Japanese predicate as verb, adjective, and copula (daform) in the experiments.
If the inputted Japanesesentence contains predicates and it does not contain?wa?
(a binding particle and a topic marker), ?mo?
(abinding particle, which means ?also?
and can oftenreplace ?wa?
and ?ga?
), and ?ga?
(a case particle andsubjective marker), the system regards the sentenceas a candidate sentence to solve the zero pronouns.Then, if the candidate sentence is declarative, thesystem inserts ?watashi wa (I)?
when the predicateis a verb, and ?sore wa (it)?
when the predicate is aadjective or a copula.
In the same way, if the candi-date sentence is a question, the system inserts ?anatawa (you)?
when the predicate is a verb, and ?sore wa(it)?
when the predicate is a adjective or a copula.These inserted position is the beginning of the sen-tence.
In the case that the sentence is imperative, thesystem does not solve the zero pronouns (Fig.
3).4 Experiments4.1 Experimental SettingFig.
4 shows the outline of the procedure of our ex-periment.
We used Moses (Koehn et al, 2007) forthe training of the translation and language models,tuning with MERT (Och, 2003) and the decoding.First, we prepared the data for learning which con-sists of parallel English and Japanese sentences.
Weused MeCab 1 as Japanese tokenizer and the tok-enizer in Moses Tool kit as English tokenizer.
Weused default settings for the parameters of Moses.Next, Moses learns language model and translationmodel from the Japanese and English sentence pairs.Then, the learned model was tuned by completedsentences with MERT.
and Moses decoded the com-pleted Japanese sentences to English sentences.4.2 Evaluation MethodWe used BLEU (Papineni et al, 2002) and an-tecedent Precision, Recall and F-measure for the1http://mecab.sourceforge.net/evaluation of the performances, comparing the sys-tem outputs with the English references of test data.Using only BLEU score is not adequate for evalua-tion of pronoun translation (Hardmeier et al, 2010).We were inspired empty node recovery evaluationby (Johnson, 2002) and defined antecedent Preci-sion (P), Recall (R) and F-measure (F) as follows,P =|G ?
S||S|R =|G ?
S||G|F =2PRP +RHere, S is the set of each pronoun in Englishtranslated by decoder, G is the set of the gold stan-dard zero pronoun.We evaluated the effect of performance of everycase among completed sentences by human, ones bythe baseline system, and the original sentences.4.3 Experimental ResultWe show the BLEU scores in Table 3. and the an-tecedent precision, recall and F-measure in Table 4.The BLEU scores for experiments using our base-line system and human annotation, are slightly bet-ter than for one without ellipsis resolution, 45.4%and 45.6%, respectively.
However, the scores of an-tecedent F-measure have major difference between?original?
and ?human?.
Particularly, the recall is im-proved.
Each 1st, 2nd and 3rd person score is betterthan original one.5 Discussion and ConclusionWe performed experiments of J-E phrase basedtranslation using Japanese sentences, whose omit-ted pronouns are complemented by human and abaseline system.
Using ?antecedent F-measure?
as ascore for measuring the quality of the translated En-glish, it improves the score of antecedent F-measure.Every effectiveness of the zero pronoun resolution113ano eiga-wo mimashita.the movie-OBJ     watchedDeclarative sentenceWatashi-wa ano eiga-wo mimashita.I-TOP the     movie-OBJ     watched(=  ?I watched the movie.?
)Question sentenceano eiga-wo mimashita ka ?the    movie-OBJ     watched       QUES  ?Anata-wa ano eiga-wo mimashita ka ?You-TOP the     movie-OBJ     watched      QUES  ?
(=  ?Did you watch the movie??
)Imperative sentenceano eiga-wo minasai.the    movie-OBJ     watch-IMPano eiga-wo minasai.the    movie-OBJ     watch-IMP(=  ?Watch the movie.?
)Figure 3: Our baseline system of zero pronoun resolutiondiffered, depending on the type and case of each zeropronoun.
The F-measures for the first person pro-noun were smaller than expected ones, Rather, thescores for and possessive pronouns second personwere greater (Table.
3).We show a better, a worse, and an unchangedcases of translation using the baseline system ofthe elimination resolution in Fig.
5.
The left-handis the result of the alignment between the origi-nal Japanese sentence and the decoded English sen-tence.
The right-hand is the result of one usingthe Japanese the baseline system solved zero pro-nouns.
In the ?better?
case, the alignment of todoke-te (send) is better than one of the original sen-tence, and ?Can you?
is compensated by the solvedzero pronoun anata-wa (you-TOP).
Otherwise, inthe ?worse?
case, our baseline system could not rec-ognize that the sentence is imperative, and insertedwatashi-wa (I-TOP) incorrectly into the sentence.
Itindicates that we need a highly accurate recogni-tion of the modalities of sentences for more correctcompletion of the antecedent of zero pronouns.
Inthe ?unchanged?
case, the translation results are thesame.
However, the alignment of the right-hand ismore correct than one of the left-hand.ReferencesKohji Dohsaka.
1990.
Identifying the referents of zero-pronouns in japanese based on pragmatic constraint in-terpretation.
In Proceedings of ECAI, pages 240?245.C.S.
Fordyce.
2007.
Overview of the iwslt 2007 eval-uation campaign.
In Proceedings of the InternationalWorkshop on Spoken Language Translation, pages 1?12.M.
Furuichi, J. Murakami, M. Tokuhisa, and M. Murata.2011.
The effect of complement subject in japaneseto english statistical machine translation (in Japanese).In Proceedings of the 17th Annual Meeting of The114EnglishParallel Corpus for TrainingJapaneseShoushou ukagai tai koto ga ari masu ga.?I have some questions to ask .Decoder ?Moses?Parallel Corpus for TestCompleted Sentenceshonkon  ryokou ni tsuitesiri  tain  desu  ga.exo1 wa  honkon  ryokou ni tsuitesiri  tain  desu  ga.System OutputTrainingTranslation ModelLanguage ModelDecodingI?d like to know aboutthe Hong Kong trip.EnglishI would like to know aboutthe Hong Kong trip.EvaluationJapanese- - - -- - - -- - - -- - - -Zero pronoun annotation by handor baseline systemTuningJapanese EnglishParallel Corpus for Tuning- - - - - - - -Zero pronoun annotationby hand or baseline systemCompleted Sentences- - - - - - - -Figure 4: Outline of the experimentAssociation for Natural Language Processing (NLP-2012).C.
Hardmeier, M. Federico, and F.B.
Kessler.
2010.Modelling pronominal anaphora in statistical machinetranslation.
In Proceedings of the seventh Inter-national Workshop on Spoken Language Translation(IWSLT), pages 283?289.H.
Isozaki, K. Sudoh, H. Tsukada, and K. Duh.
2010.Head finalization: A simple reordering rule for sovlanguages.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Metrics-MATR, pages 244?251.
Association for ComputationalLinguistics.Mark Johnson.
2002.
A simple pattern-matching al-gorithm for recovering empty nodes and their an-tecedents.
In Proceedings of 40th Annual Meeting ofthe Association for Computational Linguistics, pages136?143, Philadelphia, Pennsylvania, USA, July.
As-sociation for Computational Linguistics.G.
Kikui, E. Sumita, T. Takezawa, and S. Yamamoto.2003.
Creating corpora for speech-to-speech transla-tion.
In Proceedings of EUROSPEECH, pages 381?384.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proc.of the 45th Annual Conference of the Association for115BetterworseUnchangedmap-BY   point_out would       QUESchizu-de   sashi-te morae-masu ka.Would you point them out on this map ?You-TOP map-BY   point_out would     QUESanata-wa chizu-de   sashi-te morae-masu ka.Would you point them out on this map ?Hurry upIsoi-de  .Hurry up .
(Ref)  Hurry up.I-TOP    hurry upwatashi-wa Isoi-de  .I  ?m  in a hurry .
(Ref)  Would you point one out on this map?Today?s    evening       by      send          would           QUESKyou-no  yuugata made-ni todoke-te morae-masu ka .It   by  this  evening  ?
(Ref) Can you deliver them by this evening?you-TOP Today?s    evening       by      send          would           QUESanata-wa kyou-no  yuugata made-ni todoke-te morae-masu ka .Can you   send it   by   this evening  ?Figure 5: Effectiveness of zero pronoun resolution for decodingComputational Linguistics (ACL-07), DemonstrationSession, pages 177?180.H.
Nakaiwa and S. Yamada.
1997.
Automatic identifi-cation of zero pronouns and their antecedents withinaligned sentence pairs.
In Proc.
of the 3rd AnnualMeeting of the Association for Natural Language Pro-cessing.S.
Nariyama.
2003.
Ellipsis and reference trackingin Japanese, volume 66.
John Benjamins PublishingCompany.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proc.
of the ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eval-uation of machine translation.
In Proc.
of the 40th An-nual Conference of the Association for ComputationalLinguistics (ACL-02).K.
Yamamoto, E. Sumita, O. Furuse, and H. Iida.
1997.Ellipsis resolution in dialogues via decision-tree learn-ing.
In Proc.
of NLPRS, volume 97.
Citeseer.K.
Yoshimoto.
1988.
Identifying zero pronouns injapanese dialogue.
In Proceedings of the 12th con-ference on Computational linguistics-Volume 2, pages779?784.
Association for Computational Linguistics.116Table 2: The Type Distributions of Zero Pronouns in Test SetType Pronoun #First personal pronoun i 121my 39me 32mine 1myself 0we 7our 2us 2ours 0ourselves 0total 204Second personal pronoun you 95your 23yours 0yourself 0yourselves 0total 118Third personal pronoun he 1his 0him 0himself 0she 0her 2hers 0herself 0it 51its 0itself 0they 2their 0them 5theirs 0themselves 0total 61all total 383Table 3: BLEU scoreBLEU F(Avg.)
P R F (1st person) F (2nd person) F (3rd person)original 45.1 59.7 63.8 56.1 61.6 59.9 52.3baseline 45.4 58.5 64.1 53.7 61.2 59.2 47.7human 45.6 71.8 67.5 76.7 70.6 77.6 63.7117Table 4: Antecedent precision, recall and F-measure for every pronouni (ref:121) my (ref:39) me (ref:32)BLEU P R F P R F P R Foriginal 45.1 56.8 51.2 53.9 55.5 51.2 53.3 58.0 56.2 57.1baseline 45.4 51.8 46.2 48.9 67.8 48.7 56.7 66.6 50.0 57.1human 45.6 50.9 68.6 58.4 65.2 76.9 70.5 61.2 59.3 60.3we (ref:7) our (ref:2) us (ref:2)P R F P R F P R Foriginal 20.0 14.2 16.6 100.0 50.0 66.6 0.00 0.00 0.00baseline 25.0 14.2 18.1 100.0 50.0 66.6 0.00 0.00 0.00human 40.0 28.5 33.3 100.0 50.0 66.6 0.00 0.00 0.00you (ref:95) your (ref:23)P R F P R Foriginal 55.3 54.7 55.0 80.0 52.1 63.1baseline 57.1 54.7 55.9 58.8 43.4 50.0human 68.4 80.0 73.7 73.0 82.6 77.5it (ref:51) its (ref:0)P R F P R Foriginal 56.1 45.1 50.0 0.00 0.00 0.00baseline 51.2 41.1 45.6 0.00 0.00 0.00human 58.3 54.9 56.5 0.00 0.00 0.00they (ref:2) their (ref:0) them (ref:5)P R F P R F P R Foriginal 100.0 50.0 66.6 0.00 0.00 0.00 0.00 0.00 0.00baseline 100.0 50.0 66.6 0.00 0.00 0.00 0.00 0.00 0.00human 58.3 54.9 56.5 0.00 0.00 0.00 0.00 0.00 0.00118
