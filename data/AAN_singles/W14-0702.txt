Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 10?19,Gothenburg, Sweden, April 26, 2014.c?2014 Association for Computational LinguisticsAnnotating causality in the TempEval-3 corpusParamita MirzaFBK, Trento, ItalyUniversity of Trentoparamita@fbk.euRachele SprugnoliFBK, Trento, ItalyUniversity of Trentosprugnoli@fbk.euSara TonelliFBK, Trento, Italysatonelli@fbk.euManuela SperanzaFBK, Trento, Italymanspera@fbk.euAbstractWhile there is a wide consensus in the NLPcommunity over the modeling of temporalrelations between events, mainly based onAllen?s temporal logic, the question on howto annotate other types of event relations, inparticular causal ones, is still open.
In thiswork, we present some annotation guide-lines to capture causality between eventpairs, partly inspired by TimeML.
We thenimplement a rule-based algorithm to auto-matically identify explicit causal relationsin the TempEval-3 corpus.
Based on thisannotation, we report some statistics on thebehavior of causal cues in text and performa preliminary investigation on the interac-tion between causal and temporal relations.1 IntroductionThe annotation of events and event relations innatural language texts has gained in recent years in-creasing attention, especially thanks to the develop-ment of TimeML annotation scheme (Pustejovskyet al., 2003), the release of TimeBank (Pustejovskyet al., 2006) and the organization of several eval-uation campaigns devoted to automatic temporalprocessing (Verhagen et al., 2007; Verhagen et al.,2010; UzZaman et al., 2013).However, while there is a wide consensus in theNLP community over the modeling of temporalrelations between events, mainly based on Allen?sinterval algebra (Allen, 1983), the question on howto model other types of event relations is still open.In particular, linguistic annotation of causal rela-tions, which have been widely investigated froma philosophical and logical point of view, are stillunder debate.
This leads, in turn, to the lack ofa standard benchmark to evaluate causal relationextraction systems, making it difficult to comparesystems performances, and to identify the state-of-the-art approach for this particular task.Although several resources exist in which causal-ity has been annotated, they cover only few aspectsof causality and do not model it in a global way,comparable to what as been proposed for temporalrelations in TimeML.
See for instance the annota-tion of causal arguments in PropBank (Bonial et al.,2010) and of causal discourse relations in the PennDiscourse Treebank (The PDTB Research Group,2008).In this work, we propose annotation guidelinesfor causality inspired by TimeML, trying to take ad-vantage of the clear definition of events, signals andrelations proposed by Pustejovsky et al.
(2003).
Be-sides, as a preliminary investigation of causality inthe TempEval-3 corpus, we perform an automaticanalysis of causal signals and relations observed inthe corpus.
This work is a first step towards the an-notation of the TempEval-3 corpus with causality,with the final goal of investigating the strict connec-tion between temporal and causal relations.
In fact,there is a temporal constraint in causality, i.e.
thecause must occur BEFORE the effect.
We believethat investigating this precondition on a corpus ba-sis can contribute to improving the performance oftemporal and causal relation extraction systems.2 Existing resources on CausalitySeveral attempts have been made to annotate causalrelations in texts.
A common approach is to lookfor specific cue phrases like because or since or tolook for verbs that contain a cause as part of theirmeaning, such as break (cause to be broken) orkill (cause to die) (Khoo et al., 2000; Sakaji et al.,2008; Girju et al., 2007).
In PropBank (Bonial etal., 2010), causal relations are annotated in the formof predicate-argument relations, where ARGM-CAUis used to annotate ?the reason for an action?, forexample: ?They [PREDICATEmoved] to London[ARGM-CAUbecause of the baby].
?Another scheme annotates causal relations be-tween discourse arguments, in the framework of10the Penn Discourse Treebank (PDTB).
As opposedto PropBank, this kind of relations holds only be-tween clauses and do not involve predicates andtheir arguments.
In PDTB, the Cause relation typeis classified as a subtype of CONTINGENCY.Causal relations have also been annotated as re-lations between events in a restricted set of linguis-tic constructions (Bethard et al., 2008), betweenclauses in text from novels (Grivaz, 2010), or innoun-noun compounds (Girju et al., 2007).Several types of annotation guidelines for causalrelations have been presented, with varying de-grees of reliability.
One of the simpler approachesasks annotators to check whether the sentence theyare reading can be paraphrased using a connectivephrase such as and as a result or and as a conse-quence (Bethard et al., 2008).Another approach to annotate causal relationstries to combine linguistic tests with semantic rea-soning tests.
In Grivaz (2010), the linguistic para-phrasing suggested by Bethard et al.
(2008) isaugmented with rules that take into account othersemantic constraints, for instance if the potentialcause occurs before or after the potential effect.3 Annotation of causal informationAs part of a wider annotation effort aimed to an-notate texts at the semantic level (Tonelli et al.,2014), we propose guidelines for the annotation ofcausal information.
In particular, we define causalrelations between events based on the TimeML def-inition of events (ISO TimeML Working Group,2008), as including all types of actions (punctualand durative) and states.
Syntactically, events canbe realized by a wide range of linguistic expres-sions such as verbs, nouns (which can realize even-tualities in different ways, for example through anominalization process of a verb or by possessingan eventive meaning), and prepositional construc-tions.Following TimeML, our annotation of events in-volved in causal relations includes the polarityattribute (see Section 3.3); in addition to this, wehave defined the factuality and certaintyevent attributes, which are useful to infer informa-tion about actual causality between events.Parallel to the TimeML tag <SIGNAL> as anindicator for temporal links, we have also intro-duced the notion of causal signals through the useof the <C-SIGNAL> tag.3.1 C-SIGNALThe <C-SIGNAL> tag is used to mark-up textualelements that indicate the presence of a causal rela-tion (i.e.
a CLINK, see 3.2).
Such elements includeall causal uses of:?
prepositions, e.g.
because of, on account of,as a result of, in response to, due to, from, by;?
conjunctions, e.g.
because, since, so that,hence, thereby;?
adverbial connectors, e.g.
as a result, so,therefore, thus;?
clause-integrated expressions, e.g.
the resultis, the reason why, that?s why.The extent of C-SIGNALs corresponds to thewhole expression, so multi-token extensions areallowed.3.2 CLINK (Causal Relations)For the annotation of causal relations betweenevents, we use the <CLINK> tag, a directionalone-to-one relation where the causing event is thesource (the first argument, indicated asSin theexamples) and the caused event is the target (thesecond argument, indicated asT).
The annotationof CLINKs includes the c-signalID attribute,whose value is the ID of the C-SIGNAL indicatingthe causal relation (if available).A seminal research in cognitive psychologybased on the force dynamics theory (Talmy, 1988)has shown that causation covers three main kinds ofcausal concepts (Wolff, 2007), which are CAUSE,ENABLE, and PREVENT, and that these causalconcepts are lexicalized as verbs (Wolff and Song,2003): (i) CAUSE-type verbs: bribe, cause, com-pel, convince, drive, have, impel, incite, induce,influence, inspire, lead, move, persuade, prompt,push, force, get, make, rouse, send, set, spur, start,stimulate; (ii) ENABLE-type verbs: aid, allow, en-able, help, leave, let, permit; (iii) PREVENT-typeverbs: bar, block, constrain, deter, discourage, dis-suade, hamper, hinder, hold, impede, keep, prevent,protect, restrain, restrict, save, stop.
CAUSE, EN-ABLE, and PREVENT categories of causation andthe corresponding verbs are taken into account inour guidelines.As causal relations are often not overtly ex-pressed in text (Wolff et al., 2005), we restrict theannotation of CLINKs to the presence of an explicit11causal construction linking two events in the samesentence1, as detailed below:?
Basic constructions for CAUSE, ENABLEand PREVENT categories of causation asshown in the following examples:The purchaseScaused the creationTof the cur-rent buildingThe purchaseSenabled the diversificationToftheir businessThe purchaseSprevented a future transferT?
Expressions containing affect verbs, such asaffect, influence, determine, and change.
Theycan be usually rephrased using cause, enable,or prevent:Ogun ACN crisisSaffects the launchTof theAll Progressives Congress?
Ogun ACN cri-sis causes/enables/prevents the launch of theAll Progressives Congress?
Expressions containing link verbs, such aslink, lead, and depend on.
They can usuallybe replaced only with cause and enable:An earthquakeTin North America was linkedto a tsunamiSin Japan ?
An earthquakein North America was caused/enabled by atsunami in Japan*An earthquake in North America was pre-vented by a tsunami in Japan?
Periphrastic causatives are generally com-posed of a verb that takes an embedded clauseor predicate as a complement; for example,in the sentence The blastScaused the boatto heelTviolently, the verb (i.e.
caused) ex-presses the notion of CAUSE while the em-bedded verb (i.e.
heel) expresses a particularresult.
Note that the notion of CAUSE canbe expressed by verbs belonging to the threecategories previously mentioned (which areCAUSE-type verbs, ENABLE-type verbs andPREVENT-type verbs).?
Expressions containing causative conjunc-tions and prepositions as listed in Section3.1.
Causative conjunctions and prepositionsare annotated as C-SIGNALs and their ID is1A typical example of implicit causal construction is rep-resented by lexical causatives; for example, kill has the em-bedded meaning of causing someone to die (Huang, 2012).
Inthe present guidelines, these cases are not included.to be reported in the c-signalID attributeof the CLINK.2In some contexts, the coordinating conjunctionand can imply causation; given the ambiguity ofthis construction and the fact that it is not an ex-plicit causal construction, however, we do not an-notate CLINKs between two events connected byand.
Similarly, the temporal conjunctions after andwhen can also implicitly assert a causal relationbut should not be annotated as C-SIGNALs and noCLINKs are to be created (temporal relations haveto be created instead).3.3 Polarity, factuality and certaintyThe polarity attribute, present both in TimeMLand in our guidelines, captures the grammaticalcategory that distinguishes affirmative and negativeevents.
Its values are NEG for events which arenegated (for instance, the event cause in SerotonindeficiencySmay not cause depressionT) and POSotherwise.The annotation of factuality that we addedto our guidelines is based on the situation to whichan event refers.
FACTUAL is used for facts, i.e.
sit-uations that have happened, COUNTERFACTUALis used for counterfacts, i.e.
situations that have noreal counterpart as they did not take place, NON-FACTUAL is used for possibilities, i.e.
speculativesituations, such as future events, events for whichit is not possible to determine whether they havehappened, and general statements.The certainty attribute expresses the binarydistinction between certain (value CERTAIN) anduncertain (value UNCERTAIN) events.
Uncer-tain events are typically marked in the text by thepresence of modals or modal adverbs (e.g.
per-haps, maybe) indicating possibility.
In the sentenceDrinkingSmay cause memory lossT, the causal con-nector cause is an example of a NON-FACTUALand UNCERTAIN event.In the annotation algorithm presented in the fol-lowing section, only the polarity attribute istaken into account, given that information aboutfactuality and certainty of events is not annotatedin the TempEval-3 corpus.
In particular, at thetime of the writing the algorithm considers only thepolarity of causal verbal connectors, because thisinformation is necessary to extract causal chains2The absence of a value for the c-signalID attributemeans that the causal relation is encoded by a verb.12between events in a text.
However, adding informa-tion on the polarity of the single events involved inthe relations would make possible also the identifi-cation of positive and negative causes and effects.4 Automatic annotation of explicitcausality between eventsIn order to verify the soundness of our annotationframework for event causality, we implement somesimple rules based on the categories and linguisticcues listed in Section 3.
Our goal is two-fold: first,we want to check how accurate rule-based identifi-cation of (explicit) event causality can be.
Second,we want to have an estimate of how frequentlycausality can be explicitly found in text.The dataset we annotate has been released forthe TempEval-3 shared task3on temporal and eventprocessing.
The TBAQ-cleaned corpus is the train-ing set provided for the task, consisting of the Time-Bank (Pustejovsky et al., 2006) and the AQUAINTcorpora.
It contains around 100K words in total,with 11K words annotated as events (UzZaman etal., 2013).
We choose this corpus because goldevents are already provided, and because it allowsus to perform further analyses on the interactionbetween temporal and causal relations.Our automatic annotation pipeline takes as in-put the TBAQ-cleaned corpus with gold annotatedevents and tries to automatically recognize whetherthere is a causal relation holding between them.The annotation algorithm performs the followingsteps in sequence:1.
The TBAQ-cleaned corpus is PoS-tagged andparsed using the Stanford dependency parser(de Marneffe and Manning, 2008).2.
The corpus is further analyzed with the ad-dDiscourse tagger (Pitler and Nenkova, 2009),which automatically identifies explicit dis-course connectives and their sense, i.e.
EX-PANSION, CONTINGENCY, COMPARISONand TEMPORAL.
This is used to disambiguatecausal connectives (e.g.
we consider only theoccurrences of since when it is a causal con-nective, meaning that it falls into CONTIN-GENCY class instead of TEMPORAL).3.
Given the list of affect, link, causative verbs(basic and periphrastic constructions) andcausal signals listed in Sections 3.1 and 3.2,3http://www.cs.york.ac.uk/semeval-2013/task1/the algorithm looks for specific dependencyconstructions where the causal verb or signalis connected to two events, as annotated in theTBAQ-cleaned corpus.4.
If such dependencies are found, a CLINK isautomatically set between the two events iden-tifying the source (S) and the target (T) of therelation.5.
When a causal connector corresponds to anevent, the algorithm uses the polarity of theevent to assign a polarity to the causal link.Specific approaches to detect when ambiguousconnectors have a causal meaning are implemented,as in the case of from and by, where the algorithmlooks for specific structures.
For instance, in ?Thebuilding was damagedTby the earthquakeS?, by isgoverned by a passive verb annotated as event.Also the preposition due to is ambiguous asshown in the following sentences where it acts as acausal connector only in b):a) It had been due to expire Friday evening.b) It cutTthe dividend due to its third-quarter lossSof $992,000.The algorithm performs the disambiguation bychecking the dependency structures: in sentence a)there is only one dependency relation xcomp(due,expire), while in sentence b) the dependency rela-tions are xcomp(cut, due) and prep to(due, loss).Besides, both cut and loss are annotated as events.We are aware that this type of automatic anno-tation may be prone to errors because it takes intoaccount only a limited list of causal connectors.Besides, it only partially accounts for possible am-biguities of causal cues and may suffer from pars-ing errors.
However, this allows us to make somepreliminary remarks on the amount of causal in-formation found in the TempEval-3 corpus.
Somestatistics are reported in the following subsection.4.1 Statistics of Automatic AnnotationBasic construction.
In Table 1 we report somestatistics on the non-periphrastic structuresidentified starting from verbs expressing the threecategories of causation.
Note that for the verbshave, start, hold and keep, even though theyconnect two events, we cannot say that thereis always a causal relation between them, asexemplified in the following sentence taken fromthe corpus:a) Gen. Schwarzkopf secretly pickedSSaturday13night as the optimal time to start the offensiveT.b) On Tuesday, the National Abortion andReproductive Rights Action League plansSto holda news conferenceTto screen a TV advertisement.Types Verbs CLINKCAUSEhave 1start 2cause 1compel 1PREVENThold 1keep 3block 7prevent 1ENABLE - -Total 17Table 1: Statistics of CLINKs with basic construc-tionAffect verbs.
The algorithm does not annotateany causal relation containing affect verbs mostlybecause the majority of the 36 affect verb occur-rences found in the corpus connect two elementsthat are not events, as in ?These big stocks greatlyinfluence the Nasdaq Composite Index.
?Link verbs.
In total, we found 50 occurrences oflink verbs in the corpus, but the algorithm identifiesonly 4 causal links.
Similar to affect verbs, this ismainly due to the fact that two events are not foundto be involved in the relation.
For instance, thesystem associated only one CLINK to link (outof 12 occurrences of the verb) and no CLINKsto depend (which occurs 3 times).
Most of theCLINKs identified are signaled by the verb lead;for example, ?Pol Pot is considered responsible forthe radical policiesSthat led to the deathsTof asmany as 1.7 million Cambodians.
?Periphrastic causative verbs.
Overall, there arearound 1K potential occurrences of periphrasticcausative verbs in the corpus.
However, the algo-rithm identifies only around 14% of them as partof a periphrastic construction, as shown in Table 2.This is because some verbs are often used in non-periphrastic structures, e.g.
make, have, get, keepand hold.
Among the 144 cases of periphrastic con-structions, 41 causal links are found by our rules.In Table 2, for each verb type, we report the listof verbs that appear in periphrastic constructionsin the corpus, specifying the number of CLINKsidentified by the system for each of them.Some other CAUSE-type (move, push, drive, in-fluence, compel, spur), PREVENT-type (hold, save,impede, deter, discourage, dissuade, restrict) andENABLE-type (aid) verbs occur in the corpus butare not involved in periphrastic structures.
Someothers do not appear in the corpus at all (bribe, im-pel, incite, induce, inspire, rouse, stimulate, hinder,restrain).Types Verbs Periphr.
CLINK AllCAUSEhave 34 0 239make 6 2 125get 1 0 50lead 2 1 38send 5 1 34set 2 0 23start 1 0 22force 2 1 15cause 3 2 12prompt 3 2 6persuade 2 1 3convince 1 1 2PREVENTkeep 1 1 58stop 3 0 24block 2 2 21protect 2 1 15prevent 6 2 12hamper 1 0 2bar 1 0 1constrain 1 0 1ENABLEhelp 31 13 45leave 2 2 45allow 22 3 39permit 2 1 6enable 4 2 5let 4 3 5Total 144 41 848Table 2: Statistics of periphrastic causative verbsCausal signals.
Similar to periphrastic causativeverbs, out of around 1.2K potential causal connec-tors found in the corpus, only 194 are automaticallyrecognized as actual causal signals after disam-biguation, as detailed in Table 3.
Based on theseidentified causal signals, the algorithm derives 111CLINKs.Even though the addDiscourse tool labels 11occurrences of the adverbial connector so as havinga causal meaning, our algorithm does not annotateany CLINKs for such connector.
In most cases, itis because it acts as an inter-sentential connector,while we limit the annotation of CLINKs only toevents occurring within the same sentence.CLINKs polarity.
Table 4 shows the distributionof the positive and negative polarity of the detectedCLINKs.Only two cases of negated CLINKs are automat-ically identified in the corpus.
One example is thefollowing: ?Director of the U.S. Federal Bureau of14Types C-SIGNALs Causal CLINK Allprep.because of 32 11 32on account of 0 0 0as a result of 13 9 13in response to 7 1 7due to 2 1 6from 2 2 500by 23 24 465conj.because 58 37 58since 26 19 72so that 5 4 5adverbialas a result 3 0 3so 11 0 69therefore 4 0 4thus 6 2 6hence 0 0 0thereby 1 0 1consequently 1 1 1clausalthe result is 0 0 0the reason why 0 0 0that is why 0 0 0Total 194 111 1242Table 3: Statistics of causal signals in CLINKsInvestigation (FBI) Louis Freeh said here Fridaythat U.S. air raidTon Afghanistan and Sudan isnot directly linked with the probeSinto the August7 bombings in east Africa.
?Connector types POS NEGBasicCAUSE 5 0PREVENT 12 0ENABLE - -Affect verbs - -Link verbs 3 1PeriphrasticCAUSE 10 1PREVENT 6 0ENABLE 24 0Total 60 2Table 4: Statistics of CLINKs?
polarityCLINKs vs TLINKs.
In total, the algorithm iden-tifies 173 CLINKs in the TBAQ-cleaned corpus,while the total number of TLINKs between pairs ofevents is around 5.2K.
For each detected CLINKbetween an event pair, we identify the underlyingtemporal relations (TLINKs) if any.
We found thatfrom the total of CLINKs extracted, around 33%of them have an underlying TLINK, as detailed inTable 5.
Most of them are CLINKs signaled bycausal signals.For causative verbs, the BEFORE relation is theonly underlying temporal relation type, with theexception of one SIMULTANEOUS relation.As for C-SIGNALs, the distribution of temporalrelation types is less homogeneous, as shown in Ta-ble 6.
In most of the cases, the underlying temporalrelation is BEFORE.
In few cases, CLINKs sig-Connector types CLINK TLINKBasicCAUSE 5 2PREVENT 12 0ENABLE - -Affect verbs - -Link verbs 4 1PeriphrasticCAUSE 11 1PREVENT 6 0ENABLE 24 0C-SIGNALs 111 54Total 173 58Table 5: Statistics of CLINKs?
overlapping withTLINKsnaled by the connector because overlap with an AF-TER relation, as in ?But some analysts questionedThow much of an impact the retirement package willhave, because few jobs will endSup being elimi-nated.
?In some cases, CLINKs signaled by the con-nector since match with a BEGINS relation.
Thisshows that since expresses merely a temporal andnot a causal link.
As it has been discussed before,the connector since is highly ambiguous and theCLINK has been wrongly assigned because of adisambiguation mistake of the addDiscourse tool.5 EvaluationWe perform two types of evaluation.
The first isa qualitative one, and is carried out by manuallyinspecting the 173 CLINKs that have been auto-matically annotated.
The second is a quantitativeevaluation, and is performed by comparing the au-tomatic annotated data with a gold standard corpusof 100 documents taken from TimeBank.5.1 Qualitative EvaluationThe automatically annotated CLINKs have beenmanually checked in order to measure the precisionof the adopted procedure.
Out of 173 annotatedCLINKs, 105 were correctly identified obtaining aprecision of 0.61.Details on precision calculated on the differenttypes of categories and linguistic cues defined inSection 3.2 are provided in Table 7.
Statistics showthat performances vary widely depending on thecategory and linguistic cue taken into consideration.In particular, relations expressing causation of PRE-VENT type prove to be extremely difficult to becorrectly detected with a rule-based approach: thealgorithm precision is 0.25 for basic constructionsand 0.17 for periphrastic constructions.During the manual evaluation, two main types15C-SIGNALs BEFORE AFTER IS INCLUDED BEGINS othersbecause of 5 - - - -as a result of 2 - - - -in response to 1 - - - -due to 1 - - - -by 11 - 1 2 3because 14 2 1 - 1since 4 1 - 3 -so that 1 - - - -thus 1 - - - -Total 40 3 2 5 4Table 6: Statistics of CLINKs triggered by C-SIGNALs overlapping with TLINKsConnector types Extracted Correct PBasicCAUSE 5 3 0.60PREVENT 12 3 0.25ENABLE 0 n.a.
n.a.Affect Verbs 0 n.a.
n.a.Link Verbs 4 3 0.75PeriphrasticCAUSE 11 8 0.73PREVENT 6 1 0.17ENABLE 24 17 0.71C-SIGNALs 111 70 0.63Total 173 105 0.61Table 7: Precision of automatically annotatedCLINKsof mistakes have been observed: the wrong iden-tification of events involved in CLINKs and theannotation of sentences that do not contain causalrelations.The assignment of a wrong source or a wrongtarget to a CLINK is primarily caused by the de-pendency parser output that tends to establish aconnection between a causal verb or signal and theclosest previous verb.
For example, in the sentence?StatesWest Airlines said it withdrewTits offer toacquire Mesa Airlines because the Farmington car-rier did not respondSto its offer?, the CLINK isannotated between respond and acquire instead ofbetween respond and withdrew.
On the other hand,dependency structure is very effective in identify-ing cases where one event is the consequence orthe cause of multiple events, as in ?The presidentoffered to offsetTJordan?s costs because 40% ofits exports goSto Iraq and 90% of its oil comesSfrom there.?
In this case, the algorithm annotates acausal link between go and offset, and also betweencomes and offset.The annotation of CLINKs in sentences not con-taining causal relations is strongly related to theambiguous nature of many verbs, prepositions andconjunctions, which encode a causal meaning orexpress a causal relation only in some specificcontexts.
For instance, many mistakes are due tothe erroneous disambiguation of the conjunctionsince.
According to the addDiscourse tool, since isa causal connector in around one third of the cases,as in ?For now, though, that would be a theoreticaladvantage since the authorities have admitted theyhave no idea where Kopp is.?
However, there aremany cases where the outcome of the tool is notperfect, as in ?Since then, 427 fugitives have beentaken into custody or located, 133 of them as aresult of citizen assistance, the FBI said?, wheresince acts as a temporal conjunction.5.2 Quantitative EvaluationIn order to perform also a quantitative evaluation ofour automatic annotation, we manually annotated100 documents taken from the TimeBank corpusaccording to the annotation guidelines discussedbefore.
We then used this data set as a gold stan-dard.The agreement reached by two annotators on asubset of 5 documents is 0.844 Dice?s coefficienton C-SIGNALS (micro-average over markables)and of 0.73 on CLINKS.We found that there are several cases where thealgorithm failed to recognize causal links due toevents that were originally not annotated in Time-Bank.
Therefore, as we proceed with the manualannotation, we also annotated missing events thatare involved in causal relations.
Table 8 shows that,in creating the gold standard, we annotated 61 newevents.
As a result, we have around 52% increasein the number of CLINKs.
Nevertheless, explicitcausal relations between events are by far less fre-quent than temporal ones, with an average of 1.4relations per document.If we compare the coverage of automatic anno-tation with the gold standard data (without newlyadded events, to be fair), we observe that automaticannotation covers around 76% of C-SIGNALs andonly around 55% of CLINKs.
This is due to thelimitation of the algorithm that only considers a16Annotation EVENT C-SIGNAL CLINKmanual 3933 78 144manual-w/o new events 3872 78 95automatic 3872 59 52Table 8: Statistics of causality annotation in manualversus automatic annotationprecision recall F1-scoreC-SIGNAL 0.64 0.49 0.55CLINK 0.42 0.23 0.30Table 9: Automatic annotation performancesmall list of causal connectors.
Some examples ofmanually annotated causal signals that are not inthe list used by the algorithm include due mostlyto, thanks in part to and in punishment for.Finally, we evaluate the performance of the algo-rithm for automatic annotation (shown in Table 9)by computing precision, recall and F1 on gold stan-dard data without newly added events.
We observethat our rule-based approach is too rigid to capturethe causal information present in the data.
In partic-ular, it suffers from low recall as regards CLINKs.We believe that this issue may be alleviated byadopting a supervised approach, where the list ofverbs and causal signals would be included in alarger feature set, considering among others theevents?
position, their PoS tags, the dependencypath between the two events, etc.6 ConclusionsIn this paper, we presented our guidelines for an-notating causality between events.
We further triedto automatically identify in TempEval-3 corpus thetypes of causal relations described in the guide-lines by implementing some simple rules based oncausal cues and dependency structures.In a manual revision of the annotated causallinks, we observe that the algorithm obtains a pre-cision of 0.61, with some issues related to the classof PREVENT verbs.
Some mistakes are introducedby the tools used for parsing and for disambiguat-ing causal signals, which in turn impact on ourannotation algorithm.
Another issue, more relatedto recall, is that in the TBAQ-cleaned corpus not allevents are annotated, because it focuses originallyon events involved in temporal relations.
There-fore, the number of causal relations identified auto-matically would be higher if we did not take intoaccount this constraint.From the statistics presented in Section 4.1, wecan observe that widely used verbs such as have orkeep express causality relations only in few cases.The same holds for affect verbs, which are neverfound in the corpus with a causal meaning, and forlink verbs.
This shows that the main sense of causalverbs usually reported in the literature is usuallythe non-causal one.Recognizing CLINKs based on causal signals ismore straightforward, probably because very fre-quent ones such as because of and as a result arenot ambiguous.
Others, such as by, can be identi-fied based on specific syntactic constructions.As for the polarity of CLINKs, which is a veryimportant feature to discriminate between actualand negated causal relations, this phenomenon isnot very frequent (only 2 cases) and can be easilyidentified through dependency relations.We chose to automatically annotate TBAQ-cleaned corpus because one of our goals was toinvestigate how TLINKs and CLINKs interact.However, this preliminary study shows that thereare only few overlaps between the two relations,again with C-SIGNALs being more informativethan causal verbs.
This may be biased by the factthat, according to our annotation guidelines, onlyexplicit causal relations are annotated.
Introducingalso the implicit cases would probably increase theoverlap between TLINKs and CLINKs, becauseannotator would be allowed to capture the tempo-ral constrains existing in causal relations even ifthe are not overtly expressed.In the near future, we will complete the manualannotation of TempEval-3 corpus with causal in-formation in order to have enough data for traininga supervised system, in which we will incorpo-rate the lessons learnt with this first analysis.
Wewill also investigate the integration of the proposedguidelines into the Grounded Annotation Format(Fokkens et al., 2013), a formal framework for cap-turing semantic information related to events andparticipants at a conceptual level.AcknowledgmentsThe research leading to this paper was partiallysupported by the European Union?s 7th Frame-work Programme via the NewsReader Project (ICT-316404).ReferencesJames F. Allen.
1983.
Maintaining knowledge abouttemporal intervals.
Commun.
ACM, 26(11):832?843, November.17Steven Bethard, William Corvey, Sara Klingenstein,and James H. Martin.
2008.
Building a corpus oftemporal-causal structure.
In European LanguageResources Association (ELRA), editor, Proceedingsof the Sixth International Language Resources andEvaluation (LREC?08), Marrakech, Morocco, may.Claire Bonial, Olga Babko-Malaya, Jinho D.Choi, Jena Hwang, and Martha Palmer.2010.
Propbank annotation guidelines, De-cember.
http://www.ldc.upenn.edu/Catalog/docs/LDC2011T03/propbank/english-propbank.pdf.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies rep-resentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation, pages 1?8.
Association for Com-putational Linguistics.Antske Fokkens, Marieke van Erp, Piek Vossen, SaraTonelli, Willem Robert van Hage, Luciano Ser-afini, Rachele Sprugnoli, and Jesper Hoeksema.2013.
GAF: A Grounded Annotation Frameworkfor Events.
In Workshop on Events: Definition, De-tection, Coreference, and Representation, pages 11?20, Atlanta, Georgia, June.
Association for Compu-tational Linguistics.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.Semeval-2007 task 04: Classification of semanticrelations between nominals.
In Proceedings of theFourth International Workshop on Semantic Evalua-tions (SemEval-2007), pages 13?18, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.C?ecile Grivaz.
2010.
Human Judgements on Causationin French Texts.
In Proceedings of the Seventh con-ference on International Language Resources andEvaluation (LREC?10), Valletta, Malta, may.
Euro-pean Language Resources Association (ELRA).Li-szu Agnes Huang.
2012.
The Effectiveness of aCorpus-based Instruction in Deepening EFL Learn-ers?
Knowledge of Periphrastic Causatives.
TESOLJournal, 6:83?108.ISO TimeML Working Group.
2008.
ISO TC37 draftinternational standard DIS 24617-1, August 14.http://semantic-annotation.uvt.nl/ISO-TimeML-08-13-2008-vankiyong.pdf.Christopher S. G. Khoo, Syin Chan, and Yun Niu.2000.
Extracting causal knowledge from a medi-cal database using graphical patterns.
In In Proceed-ings of 38th Annual Meeting of the ACL, Hong Kong,2000, pages 336?343.Emily Pitler and Ani Nenkova.
2009.
Using syn-tax to disambiguate explicit discourse connectivesin text.
In Proceedings of the ACL-IJCNLP 2009Conference Short Papers, ACLShort ?09, pages 13?16, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.James Pustejovsky, J. Castano, R. Ingria, Roser Saur??,R.
Gaizauskas, A. Setzer, G. Katz, and D. Radev.2003.
TimeML: Robust specification of event andtemporal expressions in text.
In Proceedings of theFifth International Workshop on Computational Se-mantics.James Pustejovsky, Jessica Littman, Roser Saur?
?, andMarc Verhagen.
2006.
Timebank 1.2 documenta-tion.
Technical report, Brandeis University, April.Hiroki Sakaji, Satoshi Sekine, and Shigeru Masuyama.2008.
Extracting causal knowledge using cluephrases and syntactic patterns.
In Proceedings of the7th International Conference on Practical Aspectsof Knowledge Management, PAKM ?08, pages 111?122, Berlin, Heidelberg.
Springer-Verlag.Leonard Talmy.
1988.
Force dynamics in languageand cognition.
Cognitive science, 12(1):49?100.The PDTB Research Group.
2008.
The PDTB 2.0.
An-notation Manual.
Technical Report IRCS-08-01, In-stitute for Research in Cognitive Science, Universityof Pennsylvania.Sara Tonelli, Rachele Sprugnoli, and Manuela Sper-anza.
2014.
NewsReader Guidelines for Annotationat Document Level, Extension of DeliverableD3.1.
Technical Report NWR-2014-2, FondazioneBruno Kessler.
https://docs.google.com/viewer?url=http%3A%2F%2Fwww.newsreader-project.eu%2Ffiles%2F2013%2F01%2FNWR-2014-2.pdf.Naushad UzZaman, Hector Llorens, Leon Derczyn-ski, Marc Verhagen, James Allen, and James Puste-jovsky.
2013.
Semeval-2013 task 1: Tempeval-3:Evaluating events, time expressions, and temporalrelations.
In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation (SemEval 2013).Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
Semeval-2007 task 15: Tempeval tempo-ral relation identification.
In Proceedings of theFourth International Workshop on Semantic Evalua-tions (SemEval-2007), pages 75?80, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Marc Verhagen, Roser Saur?
?, Tommaso Caselli, andJames Pustejovsky.
2010.
Semeval-2010 task 13:Tempeval-2.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 57?62, Up-psala, Sweden, July.
Association for ComputationalLinguistics.Phillip Wolff and Grace Song.
2003.
Models of cau-sation and the semantics of causal verbs.
CognitivePsychology, 47(3):276?332.18Phillip Wolff, Bianca Klettke, Tatyana Ventura, andGrace Song.
2005.
Expressing causation in englishand other languages.
Categorization inside and out-side the laboratory: Essays in honor of Douglas L.Medin, pages 29?48.Phillip Wolff.
2007.
Representing causation.
Journalof experimental psychology: General, 136(1):82.19
