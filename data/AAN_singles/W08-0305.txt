Proceedings of the Third Workshop on Statistical Machine Translation, pages 35?43,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsLearning Performance of a Machine Translation System: a Statistical andComputational AnalysisMarco Turchi Tijl De BieDept.
of Engineering MathematicsUniversity of Bristol,Bristol, BS8 1TR, UK{Marco.Turchi, Tijl.DeBie}@bristol.ac.uknello@support-vector.netNello CristianiniAbstractWe present an extensive experimental studyof a Statistical Machine Translation system,Moses (Koehn et al, 2007), from the pointof view of its learning capabilities.
Very ac-curate learning curves are obtained, by us-ing high-performance computing, and extrap-olations are provided of the projected perfor-mance of the system under different condi-tions.
We provide a discussion of learningcurves, and we suggest that: 1) the represen-tation power of the system is not currently alimitation to its performance, 2) the inferenceof its models from finite sets of i.i.d.
datais responsible for current performance limita-tions, 3) it is unlikely that increasing datasetsizes will result in significant improvements(at least in traditional i.i.d.
setting), 4) it is un-likely that novel statistical estimation methodswill result in significant improvements.
Thecurrent performance wall is mostly a conse-quence of Zipf?s law, and this should be takeninto account when designing a statistical ma-chine translation system.
A few possible re-search directions are discussed as a result ofthis investigation, most notably the integra-tion of linguistic rules into the model inferencephase, and the development of active learningprocedures.1 Introduction and BackgroundThe performance of every learning system is the re-sult of (at least) two combined effects: the repre-sentation power of the hypothesis class, determin-ing how well the system can approximate the targetbehaviour; and statistical effects, determining howwell the system can approximate the best element ofthe hypothesis class, based on finite and noisy train-ing information.
The two effects interact, with richerclasses being better approximators of the target be-haviour but requiring more training data to reliablyidentify the best hypothesis.
The resulting trade-off, equally well known in statistics and in machinelearning, can be expressed in terms of bias variance,capacity-control, or model selection.
Various theo-ries on learning curves have been proposed to dealwith it, where a learning curve is a plot describingperformance as a function of some parameters, typ-ically training set size.In the context of Statistical Machine Translation(SMT), where large bilingual corpora are used totrain adaptive software to translate text, this task isfurther complicated by the peculiar distribution un-derlying the data, where the probability of encoun-tering new words or expressions never vanishes.
Ifwe want to understand the potential and limitationsof the current technology, we need to understand theinterplay between these two factors affecting perfor-mance.
In an age where the creation of intelligentbehaviour is increasingly data driven, this is a ques-tion of great importance to all of Artificial Intelli-gence.These observations lead us to an analysis of learn-ing curves in machine translation, and to a number ofrelated questions, including an analysis of the flexi-bility of the representation class used, an analysis ofthe stability of the models with respect to perturba-tions of the parameters, and an analysis of the com-putational resources needed to train these systems.Using the open source package Moses (Koehn et35al., 2007) and the Spanish-English Europarl corpus(Koehn, 2005) we have performed a complete inves-tigation of the influence of training set size on thequality of translations and on the cost of training; theinfluence of several design choices; the role of datasizes in training various components of the system.We use this data to inform a discussion about learn-ing curves.
An analysis of learning curves has pre-viously been proposed by (Al-Onaizan et al, 1999).Recent advances in software, data availability andcomputing power have enabled us to undertake thepresent study, where very accurate curves are ob-tained on a large corpus.Since our goal was to obtain high accuracy learn-ing curves, that can be trusted both for compar-ing different system settings, and to extrapolate per-formance under unseen conditions, we conducted alarge-scale series of tests, to reduce uncertainty inthe estimations and to obtain the strongest possiblesignals.
This was only possible, to the degree of ac-curacy needed by our analysis, by the extensive useof a high performance computer cluster over severalweeks of computation.One of our key findings is that the current per-formance is not limited by the representation powerof the hypothesis class, but rather by model estima-tion from data.
And that increasing of the size ofthe dataset is not likely to bridge that gap (at leastnot for realistic amounts in the i.i.d.
setting), nor isthe development of new parameter estimation prin-ciples.
The main limitation seems to be a directconsequence of Zipf?s law, and the introduction ofconstraints from linguistics seems to be an unavoid-able step, to help the system in the identification ofthe optimal models without resorting to massive in-creases in training data, which would also result insignificantly higher training times, and model sizes.2 Statistical Machine TranslationWhat is the best function class to map Spanish doc-uments into English documents?
This is a questionof linguistic nature, and has been the subject of along debate.
The de-facto answer came during the1990?s from the research community on StatisticalMachine Translation, who made use of statisticaltools based on a noisy channel model originally de-veloped for speech recognition (Brown et al, 1994;Och and Weber, 1998; R.Zens et al, 2002; Och andNey, 2001; Koehn et al, 2003).
A Markovian lan-guage model, based on phrases rather than words,coupled with a phrase-to-phrase translation table areat the heart of most modern systems.
Translating atext amounts to computing the most likely transla-tion based on the available model parameters.
Infer-ring the parameters of these models from bilingualcorpora is a matter of statistics.
By model inferencewe mean the task of extracting all tables, parametersand functions, from the corpus, that will be used totranslate.How far can this representation take us towardsthe target of achieving human-quality translations?Are the current limitations due to the approximationerror of this representation, or to lack of sufficienttraining data?
How much space for improvementis there, given new data or new statistical estima-tion methods or given different models with differ-ent complexities?We investigate both the approximation and the es-timation components of the error in machine transla-tion systems.
After analysing the two contributions,we focus on the role of various design choices indetermining the statistical part of the error.
We in-vestigate learning curves, measuring both the role ofthe training set and the optimization set size, as wellas the importance of accuracy in the numeric param-eters.We also address the trade-off between accuracyand computational cost.
We perform a completeanalysis of Moses as a learning system, assessing thevarious contributions to its performance and whereimprovements are more likely, and assessing com-putational and statistical aspects of the system.A general discussion of learning curves in Moses-like systems and an extrapolation of performanceare provided, showing that the estimation gap is un-likely to be closed by adding more data in realisticamounts.3 Experimental SetupWe have performed a large number of detailed ex-periments.
In this paper we report just a few, leavingthe complete account of our benchmarking to a fulljournal version (Turchi et al, In preparation).
Threeexperiments allow us to assess the most promis-36ing directions of research, from a machine learningpoint of view.1.
Learning curve showing translation perfor-mance as a function of training set size, wheretranslation is performed on unseen sentences.The curves, describing the statistical part of theperformance, are seen to grow very slowly withtraining set size.2.
Learning curve showing translation perfor-mance as a function of training set size, wheretranslation is performed on known sentences.This was done to verify that the hypothesisclass is indeed capable of representing highquality translations in the idealized case whenall the necessary phrases have been observedin training phase.
By limiting phrase lengthto 7 words, and using test sentences mostlylonger than 20 words, we have ensured that thiswas a genuine task of decoding.
We observedthat translation in these idealized conditions isworse than human translation, but much betterthan machine translation of unseen sentences.3.
Plot of performance of a model when the nu-meric parameters are corrupted by an increas-ing amount of noise.
This was done to simu-late the effect of inaccurate parameter estima-tion algorithms (due either to imprecise objec-tive functions, or to lack of sufficient statisticsfrom the corpus).
We were surprised to observethat accurate estimation of these parameters ac-counts for at most 10% of the final score.
It isthe actual list of phrases that forms the bulk ofthe knowledge in the system.We conclude that the availability of the right mod-els in the system would allow the system to have amuch higher performance, but these models will notcome from increased datasets or estimation proce-dures.
Instead, they will come from the results of ei-ther the introduction of linguistic knowledge, or theintroduction of query algorithms, themselves result-ing necessarily from confidence estimation meth-ods.
Hence these appear to be the two most pressingquestions in this research area.3.1 SoftwareMoses (Koehn et al, 2007) is a complete translationtoolkit for academic purposes.
It provides all thecomponents needed to create a machine translationsystem from one language to another.
It contains dif-ferent modules to preprocess data, train the languagemodels and the translation models.
These mod-els can be tuned using minimum error rate training(Och, 2003).
Moses uses standard external tools forsome of these tasks, such as GIZA++ (Och and Ney,2003) for word alignments and SRILM (Stolcke,2002) for language modeling.
Notice that Moses is avery sophisticated system, capable of learning trans-lation tables, language models and decoding param-eters from data.
We analyse the contribution of eachcomponent to the overall score.Given a parallel training corpus, Moses prepro-cesses it removing long sentences, lowercasing andtokenizing sentences.
These sentences are used totrain the language and translation models.
Thisphase requires several steps as aligning words, com-puting the lexical translation, extracting phrases,scoring the phrases and creating the reorderingmodel.
When the models have been created, the de-velopment set is used to run the minimum error ratetraining algorithm to optimize their weights.
We re-fer to that step as the optimization step in the rest ofthe paper.
Test set is used to evaluate the quality ofmodels on the data.
The translated sentences are em-bedded in a sgm format, such that the quality of thetranslation can be evaluated using the most commonmachine translation scores.
Moses provides BLEU(K.Papineni et al, 2001) and NIST (Doddington,2002), but Meteor (Banerjee and Lavie, 2005; Lavieand Agarwal, 2007) and TER (Snover et al, 2006)can easily be used instead.
NIST is used in this paperas evaluation score after we observed its high corre-lation to the other scores on the corpus (Turchi et al,In preparation).All experiments have been run using the defaultparameter configuration of Moses.
It means thatGiza++ has used IBM model 1, 2, 3, and 4 withnumber of iterations for model 1 equal to 5, model2 equal to 0, model 3 and 4 equal to 3; SRILMhas used n-gram order equal to 3 and the Kneser-Ney smoothing algorithm; Mert has been run fix-ing to 100 the number of nbest target sentence for37each develop sentence, and it stops when none ofthe weights changed more than 1e-05 or the nbestlist does not change.The training, development and test set sentencesare tokenized and lowercased.
The maximum num-ber of tokens for each sentence in the training pairhas been set to 50, whilst no limit is applied to thedevelopment or test set.
TMs were limited to aphrase-length of 7 words and LMs were limited to3.3.2 DataThe Europarl Release v3 Spanish-English corpushas been used for the experiments.
All the pairs ofsentences are extracted from the proceedings of theEuropean Parliament.This dataset is made of three sets of pairs of sen-tences.
Each of them has a different role: training,development and test set.
The training set contains1,259,914 pairs, while there are 2,000 pairs for de-velopment and test sets.This work contains several experiments on differ-ent types and sizes of data set.
To be consistentand to avoid anomalies due to overfitting or par-ticular data combinations, each set of pairs of sen-tences have been randomly sampled.
The number ofpairs is fixed and a software selects them randomlyfrom the whole original training, development or testset using a uniform distribution (bootstrap).
Redun-dancy of pairs is allowed inside each subset.3.3 HardwareAll the experiments have been run on a cluster ma-chine, http://www.acrc.bris.ac.uk/acrc/hpc.htm.
Itincludes 96 nodes each with two dual-core opteronprocessors, 8 GB of RAM memory per node (2 GBper core); 4 thick nodes each with four dual-coreopteron processors, 32 GB of RAM memory pernode (4 GB per core); ClearSpeed accelerator boardson the thick nodes; SilverStorm Infiniband high-speed connectivity throughout for parallel code mes-sage passing; General Parallel File System (GPFS)providing data access from all the nodes; storage -11 terabytes.
Each experiment has been run usingone core and allocating 4Gb of RAM.4 Experiments4.1 Experiment 1: role of training set size onperformance on new sentencesIn this section we analyse how performance is af-fected by training set size, by creating learningcurves (NIST score vs training set size).We have created subsets of the complete corpusby sub-sampling sentences from a uniform distribu-tion, with replacement.
We have created 10 randomsubsets for each of the 20 chosen sizes, where eachsize represents 5%, 10%, etc of the complete cor-pus.
For each subset a new instance of the SMTsystem has been created, for a total of 200 mod-els.
These have been optimized using a fixed sizedevelopment set (of 2,000 sentences, not includedin any other phase of the experiment).
Two hun-dred experiments have then been run on an indepen-dent test set (of 2,000 sentences, also not included inany other phase of the experiment).
This allowed usto calculate the mean and variance of NIST scores.This has been done for the models with and withoutthe optimization step, hence producing the learningcurves with error bars plotted in Figure 1, represent-ing translation performance versus training set size,in the two cases.The growth of the learning curve follows a typi-cal pattern, growing fast at first, then slowing down(traditional learning curves are power laws, in theo-retical models).
In this case it appears to be grow-ing even slower than a power law, which would bea surprise under traditional statistical learning the-ory models.
In any case, the addition of massiveamounts of data from the same distribution will re-sult into smaller improvements in the performance.The small error bars that we have obtained also al-low us to neatly observe the benefits of the optimiza-tion phase, which are small but clearly significant.4.2 Experiment 2: role of training set size onperformance on known sentencesThe performance of a learning system depends bothon the statistical estimation issues discussed in theprevious subsection, and on functional approxima-tion issues: how well can the function class repro-duce the desired behaviour?
In order to measure thisquantity, we have performed an experiment muchlike the one described above, with one key differ-380 2 4 6 8 10 12 14x 1056.86.977.17.27.37.47.57.6Nist Score vs Training SizeTraining SizeNist ScoreOptimizedNot OptimizedFigure 1: ?Not Optimized?
has been obtained using afixed test set and no optimization phase.
?Optimized?using a fixed test set and the optimization phase.ence: the test set was selected randomly from thetraining set (after cleaning phase).
In this way weare guaranteed that the system has seen all the nec-essary information in training phase, and we can as-sess its limitations in these very ideal conditions.We are aware this condition is extremely idealizedand it will never happen in real life, but we wantedto have an upper bound on the performance achiev-able by this architecture if access to ideal data wasnot an issue.
We also made sure that the perfor-mance on translating training sentences was not dueto simple memorization of the entire sentence, ver-ifying that the vast majority of the sentences werenot present in the translation table (where the max-imal phrase size was 7), not even in reduced form.Under these favourable conditions, the system ob-tained a NIST score of around 11, against a scoreof about 7.5 on unseen sentences.
This suggeststhat the phrase-based Markov-chain representationis sufficiently rich to obtain a high score, if the nec-essary information is contained in the translation andlanguage models.For each model to be tested on known sentences,we have sampled ten subsets of 2,000 sentences eachfrom the training set.The ?Optimized, Test on Training Set?
learn-ing curve, see figure 2, represents a possible upperbound on the best performance of this SMT sys-tem, since it has been computed in favourable con-ditions.
It does suggest that this hypothesis classhas the power of approximating the target behaviourmore accurately than we could think based on per-formance on unseen sentences.
If the right informa-tion has been seen, the system can reconstruct thesentences rather accurately.
The NIST score com-puted using the reference sentences as target sen-tences is around 15, we identify the relative curve as?Human Translation?.
At this point, it seems likelythat the process with which we learn the necessarytables representing the knowledge of the system isresponsible for the performance limitations.The gap between the ?Optimized, Test on Train-ing Set?
and the ?Optimized?
curves is even more in-teresting if related to the slow growth rate in the pre-vious learning curve: although the system can repre-sent internally a good model of translation, it seemsunlikely that this will ever be inferred by increasingthe size of training datasets in realistic amounts.The training step results in various forms ofknowledge: translation table, language model andparameters from the optimization.
The internalmodels learnt by the system are essentially listsof phrases, with probabilities associated to them.Which of these components is mostly responsiblefor performance limitations?4.3 Experiment 3: effect on performance ofincreasing noise levels in parametersMuch research has focused on devising improvedprinciples for the statistical estimation of the pa-rameters in language and translation models.
Theintroduction of discriminative graphical models hasmarked a departure from traditional maximum like-lihood estimation principles, and various approacheshave been proposed.The question is: how much information is con-tained in the fine grain structure of the probabilitiesestimated by the model?
Is the performance improv-ing with more data because certain parameters areestimated better, or just because the lists are grow-ing?
In the second case, it is likely that more sophis-ticated statistical algorithms to improve the estima-tion of probabilities will have limited impact.In order to simulate the effect of inaccurate esti-mation of the numeric parameters, we have addedincreasing amount of noise to them.
This can eitherrepresent the effect of insufficient statistics in esti-mating them, or the use of imperfect parameter esti-390 2 4 6 8 10 12 14x 105678910111213141516Nist Score vs Training SizeTraining SizeNist ScoreNot OptimizedOptimizedOptimized, Test On Training SetHuman TranslationFigure 2: Four learning curves have been compared.
?Not Optimized?
has been obtained using a fixed test set and nooptimization phase.
?Optimized?
using a fixed test set and the optimization phase.
?Optimized Test On Training Set?a test set selected by the training set for each training set size and the optimization phase.
?Human Translation?
hasbeen obtained by computing NIST using the reference English sentence of the test set as target sentences.mation biases.
We have corrupted the parameters inthe language and translation models, by adding in-creasing levels of noise to them, and measured theeffect of this on performance.One model trained with 62,995 pairs of sentenceshas been chosen from the experiments in Section4.1.
A percentage of noise has been added to eachprobability in the language model, including condi-tional probability and back off, translation model,bidirectional translation probabilities and lexical-ized weighting.
Given a probability p and a percent-age of noise, pn, a value has been randomly selectedfrom the interval [-x,+x], where x = p * pn, andadded to p. If this quantity is bigger than one it hasbeen approximated to one.
Different values of per-centage have been used.
For each value of pn, fiveexperiment have been run.
The optimization stephas not been run.We see from Figure 3 that the performance doesnot seem to depend crucially on the fine structure ofthe parameter vectors, and that even a large additionof noise (100%) produces a 10% decline in NISTscore.
This suggests that it is the list itself, rather0 10 20 30 40 50 60 70 80 90 100 1106.66.656.76.756.86.85"Perturbed" Nist Score vs Percentage of PerturbationPercentage of Perturbation"Perturbed" NistScoreFigure 3: Each probability of the language and translationmodels has been perturbed adding a percentage of noise.This learning curve reports the not optimized NIST scoreversus the percentage of perturbation applied.
These re-sults have been obtained using a fixed training set sizeequal to 62,995 pairs of sentences.400 2 4 6 8 10 12 14x 10505001000150020002500CPU Computational Time in minutes vs Training SizeTraining SizeCPUComputational TimeinminutesTraining TimeTuning TimeFigure 4: Training and tuning user time vs training setsize.
Time quantities are expressed in minutes.than the probabilities in it, that controls the perfor-mance.
Different estimation methods can producedifferent parameters, but this does not seem to mat-ter very much.
The creation of a more complete listof words, however, seems to be the key to improvethe score.
Combined with the previous findings, thiswould mean that neither more data nor better statis-tics will bridge the performance gap.
The solutionmight have to be found elsewhere, and in our Dis-cussion section we outline a few possible avenues.5 Computational CostThe computational cost of models creation anddevelopment-phase has been measured during thecreation of the learning curves.
Despite its efficiencyin terms of data usage, the development phase has ahigh cost in computational terms, if compared withthe cost of creating the complete language and trans-lation models.For each experiment, the user CPU time is com-puted as the sum of the user time of the main processand the user time of the children.These quantities are collected for training, devel-opment, testing and evaluation phases.
In figure 4,training and tuning user times are plotted as a func-tion of the training set size.
It is evident that increas-ing the training size causes an increase in trainingtime in a roughly linear fashion.It is hard to find a similar relationship for the tun-ing time of the development phase.
In fact, the tun-ing time is strictly connected with the optimizationalgorithm and the sentences in the development set.We can also see in figure 4 that even a small devel-opment set size can require a large amount of tun-ing time.
Each point of the tuning time curve has abig variance.
The tuning phase involves translatingthe development set many times and hence its costdepends very weakly on the training set size, since alarge training set leads to larger tables and these leadto slightly longer test times.6 DiscussionThe impressive capability of current machine trans-lation systems is not only a testament to an incredi-bly productive and creative research community, butcan also be seen as a paradigm for other Artificial In-telligence tasks.
Data driven approaches to all mainareas of AI currently deliver the state of the art per-formance, from summarization to speech recogni-tion to machine vision to information retrieval.
Andstatistical learning technology is central to all ap-proaches to data driven AI.Understanding how sophisticated behaviour canbe learnt from data is hence not just a concern formachine learning, or to individual applied commu-nities, such as Statistical Machine Translation, butrather a general concern for modern Artificial Intelli-gence.
The analysis of learning curves, and the iden-tification of the various limitations to performanceis a crucial part of the machine learning method,and one where statistics and algorithmics interactclosely.In the case of Statistical Machine Translation, theanalysis of Moses suggests that the current bottle-neck is the lack of sufficient data, not the functionclass used for the representation of translation sys-tems.
The clear gap between performance on train-ing and testing set, together with the rate of thelearning curves, suggests that improvements may bepossible but not by adding more data in i.i.d.
way asdone now.
The perturbation analysis suggests thatimproved statistical principles are unlikely to makea big difference either.Since it is unlikely that sufficient data will beavailable by simply sampling a distribution, oneneeds to address a few possible ways to transferlarge amounts of knowledge into the system.
All ofthem lead to open problems either in machine learn-41ing or in machine translation, most of them havingbeen already identified by their respective communi-ties as important questions.
They are actively beingworked on.The gap between performances on training andon test sets is typically affected by model selectionchoices, ultimately controlling the trade off betweenoverfitting and underfitting.
In these experiments thesystem used phrases of length 7 or less.
Changingthis parameter might reflect on the gap and this isthe focus of our current work.A research programme naturally follows fromour analysis.
The first obvious approach is an ef-fort to identify or produce datasets on demand (ac-tive learning, where the learning system can requesttranslations of specific sentences, to satisfy its infor-mation needs).
This is a classical machine learningquestion, that however comes with the need for fur-ther theoretical work, since it breaks the traditionali.i.d.
assumptions on the origin of data.
Further-more, it would also require an effective way to doconfidence estimation on translations, as traditionalactive learning approaches are effectively based onthe identification (or generation) of instances wherethere is low confidence in the output (Blatz et al,2004; Ueffing and Ney, 2004; Ueffing and Ney,2005b; Ueffing and Ney, 2005a).The second natural direction involves the intro-duction of significant domain knowledge in the formof linguistic rules, so to dramatically reduce theamount of data needed to essentially reconstructthem by using statistics.
These rules could take theform of generation of artificial training data, basedon existing training data, or a posteriori expansion oftranslation and language tables.
Any way to enforcelinguistic constraints will result in a reduced needfor data, and ultimately in more complete models,given the same amount of data (Koehn and Hoang,2007).Obviously, it is always possible that the identifi-cation of radically different representations of lan-guage might introduce totally different constraintson both approximation and estimation error, and thismight be worth considering.What is not likely to work.
It does not seem thatthe introduction of more data will change the situ-ation significantly, as long as the data is sampledi.i.d.
from the same distribution.
It also does notseem that more flexible versions of Markov mod-els would be likely to change the situation.
Finally,it does not seem that new and different methods toestimate probabilities would make much of a differ-ence.
Our perturbation studies show that significantamounts of noise in the parameters result into verysmall variations in the performance.
Note also thatthe current algorithm is not even working on refin-ing the probability estimates, as the rate of growth ofthe tables suggests that new n-grams are constantlyappearing, reducing the proportion of time spent re-fining probabilities of old n-grams.It does seem that the control of the performancerelies on the length of the translation and languagetables.
Ways are needed to make these tables growmuch faster as a function of training set size; theycan either involve active selection of documents totranslate, or the incorporation of linguistic rules toexpand the tables without using extra data.It is important to note that many approaches sug-gested above are avenues currently being activelypursued, and this analysis might be useful to decidewhich one of them should be given priority.7 ConclusionsWe have started a series of extensive experimentalevaluations of performance of Moses, using highperformance computing, with the goal of under-standing the system from a machine learning pointof view, and use this information to identify weak-nesses of the system that can lead to improvements.We have performed many more experiments thatcannot be reported in this workshop paper, and willbe published in a longer report (Turchi et al, Inpreparation).
In general, our goal is to extrapolatethe performance of the system under many condi-tions, to be able to decide which directions of re-search are most likely to deliver improvements inperformance.AcknowledgmentsMarco Turchi is supported by the EU ProjectSMART.
The authors thank Callum Wright, Bris-tol HPC Systems Administrator, and Moses mailinglist.42ReferencesY.
Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty,D.
Melamed, F. J. Och, D. Purdy, N. A. Smith, andD.
Yarowsky.
1999.
Statistical machine translation:Final report.
Technical report, Johns Hopkins Univer-sity 1999 Summer Workshop on Language Engineer-ing, Center for Speech and Language Processing.S.
Banerjee and A. Lavie.
2005.
Meteor: An auto-matic metric for mt evaluation with improved correla-tion with human judgments.
In ACL ?05: Proceedingsof the 43rd Annual Meeting on Association for Com-putational Linguistics, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.J.
Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,A.
Kulesza, A. Sanchis, and N. Ueffing.
2004.
Confi-dence estimation for machine translation.
In COLING?04: Proceedings of the 20th international conferenceon Computational Linguistics, page 315, Morristown,NJ, USA.
Association for Computational Linguistics.P.
F. Brown, S. Della Pietra, V.t J. Della Pietra, and R. L.Mercer.
1994.
The mathematic of statistical machinetranslation: Parameter estimation.
Computational Lin-guistics, 19(2):263?311.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics.
In Proceedings of the second international con-ference on Human Language Technology Research,pages 138?145, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.P.
Koehn and H. Hoang.
2007.
Factored translationmodels.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 868?876.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In NAACL ?03: Proceedingsof the 2003 Conference of the North American Chap-ter of the Association for Computational Linguisticson Human Language Technology, pages 48?54, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In the Annual Meet-ing of the Association for Computational Linguistics,demonstration session.P.
Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In Machine TranslationSummit X, pages 79?86.K.Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL ?02, pages 311?318, Morristown, NJ, USA.
Association for Computa-tional Linguistics.A.
Lavie and A. Agarwal.
2007.
Meteor: An automaticmetric for mt evaluation with high levels of correla-tion with human judgments.
In ACL ?07: Proceedingsof 45th Annual Meeting of the Association for Com-putational Linguistics.
Association for ComputationalLinguistics.F.
J. Och and H. Ney.
2001.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In Proceedings of ACL ?02, pages 295?302, Morristown, NJ, USA.
Association for Computa-tional Linguistics.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.F.
J. Och and H. Weber.
1998.
Improving statistical nat-ural language translation with categories and rules.
InCOLING-ACL, pages 985?989.F.
J. Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proceedings of ACL ?03,pages 160?167, Morristown, NJ, USA.
Association forComputational Linguistics.R.Zens, F. J.Och, and H. Ney.
2002.
Phrase-based sta-tistical machine translation.
In KI ?02: Proceedingsof the 25th Annual German Conference on AI, pages18?32, London, UK.
Springer-Verlag.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Proceedings of the7th Conference of the Association for Machine Trans-lation in the Americas, pages 223?231.
Association forMachine Translation in the Americas.A.
Stolcke.
2002.
Srilm ?
an extensible language mod-eling toolkit.
In Intl.
Conf.
on Spoken Language Pro-cessing.M.
Turchi, T. De Bie, and N. Cristianini.
In preparation.Learning analysis of a machine translation system.N.
Ueffing and H. Ney.
2004.
Bayes decision rulesand confidence measures for statistical machine trans-lation.
In EsTAL-2004, pages 70?81.N.
Ueffing and H. Ney.
2005a.
Application of word-levelconfidence measures in interactive statistical machinetranslation.
In EAMT-2005, pages 262?270.N.
Ueffing and H. Ney.
2005b.
Word-level confidenceestimation for machine translation using phrase-basedtranslation models.
In Proceedings of HLT ?05, pages763?770, Morristown, NJ, USA.
Association for Com-putational Linguistics.43
