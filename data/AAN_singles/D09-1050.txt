Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 478?486,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPAcquiring Translation Equivalences of Multiword Expressions byNormalized Correlation FrequenciesMing-Hong Bai1,2 Jia-Ming You1 Keh-Jiann Chen1 Jason S. Chang21 Institute of Information Science, Academia Sinica, Taiwan2 Department of Computer Science, National Tsing-Hua University, Taiwanmhbai@sinica.edu.tw, swimming@hp.iis.sinica.edu.tw,kchen@iis.sinica.edu.tw, jschang@cs.nthu.edu.twAbstractIn this paper, we present an algorithm for ex-tracting translations of any given multiwordexpression from parallel corpora.
Given amultiword expression to be translated, themethod involves extracting a short list of tar-get candidate words from parallel corporabased on scores of normalized frequency,generating possible translations and filteringout common subsequences, and selecting thetop-n possible translations using the Dicecoefficient.
Experiments show that our ap-proach outperforms the word alignment-based and other naive association-based me-thods.
We also demonstrate that adopting theextracted translations can significantly im-prove the performance of the Moses machinetranslation system.1 IntroductionTranslation of multiword expressions (MWEs),such as compound words, phrases, collocationsand idioms, is important for many NLP tasks,including the techniques are helpful for dictio-nary compilation, cross language informationretrieval, second language learning, and machinetranslation.
(Smadja et al, 1996; Gao et al, 2002;Wu and Zhou, 2003).
However, extracting exacttranslations of MWEs is still an open problem,possibly because the senses of many MWEs arenot compositional (Yamamoto and Matsumoto,2000), i.e., their translations are not composi-tions of the translations of individual words.
Forexample, the Chinese idiom ????
should betranslated as ?turn a blind eye,?
which has nodirect relation with respect to the translation ofeach constituent (i.e., ?to sit?, ?to see?
and ?toignore?)
at the word level.Previous SMT systems (e.g., Brown et al,1993) used a word-based translation modelwhich assumes that a sentence can be translatedinto other languages by translating each wordinto one or more words in the target language.Since many concepts are expressed by idiomaticmultiword expressions instead of single words,and different languages may realize the sameconcept using different numbers of words (Ma etal., 2007; Wu, 1997), word alignment based me-thods, which are highly dependent on the proba-bility information at the lexical level, are notwell suited for this type of translation.To address the above problem, some methodshave been proposed for extending word align-ments to phrase alignments.
For example, Och etal.
(1999) proposed the so-called grow-diag-final heuristic method for extending wordalignments to phrase alignments.
The method iswidely used and has achieved good results forphrase-based statistical machine translation.
(Och et al, 1999; Koehn et al, 2003; Liang et al,2006).
Instead of using heuristic rules, Ma et al(2008) showed that syntactic information, e.g.,phrase or dependency structures, is useful in ex-tending the word-level alignment.
However, theabove methods still depend on word-basedalignment models, so they are not well suited toextracting the translation equivalences of seman-tically opaque MWEs due to the lack of wordlevel relations between the translational corres-pondences.
Moreover, the aligned phrases arenot precise enough to be used in many NLP ap-plications like dictionary compilation, whichrequire high quality translations.Association-based methods, e.g., the Dicecoefficient, are widely used to extract transla-tions of MWEs.
(Kupiec, 1993; Smadja et al,1996; Kitamura and Matsumoto, 1996; Yama-moto and Matsumoto, 2000; Melamed, 2001).The advantage of such methods is that associa-tion relations are established at the phrase levelinstead of the lexical level, so they have the po-tential to resolve the above-mentioned transla-tion problem.
However, when applying associa-tion-based methods, we have to consider the fol-lowing complications.
The first complication,which we call the contextual effect, causes theextracted translation to contain noisy words.
For478example, translations of the Chinese idiom ????
(best of both worlds) extracted by a naiveassociation-based method may contain noisycollocation words like difficult, try and cannot,which are not part of the translation of the idiom.They are actually translations of its collocationcontext, such as ??
(difficult), ??
(try), and??(cannot).
This problem arises because naiveassociation methods do not deal with the effectof strongly collocated contexts carefully.
If wecan incorporate lexical-level information to dis-count the noisy collocation words, the contextualeffect could be resolved.English (y) fy fx,y Dice(x,y)quote out of context 22 19 0.56take out of context 17 11 0.35interpret out of context 2 2 0.08out of context 53 32 0.65Table 1.
The Dice coefficient tends to select a com-mon subsequence of translations.
(The frequency of????
,fx, is 46.
)The second complication, which we call thecommon subsequence problem, is that the Dicecoefficient tends to select the common subse-quences of a set of similar translations instead ofthe full translations.
Consider the translations of????
(quote out of context) shown in thefirst three rows of Table 1.
The Dice coefficientof each translation is smaller than that of thecommon subsequence ?out of context?
in the lastrow.
If we can tell common subsequence apartfrom correct translations, the common subse-quence problem could be resolved.In this paper, we propose an improved preci-sion method for extracting MWE translationsfrom parallel corpora.
Our method is similar tothat of Smadja et al (1996), except that we in-corporate lexical-level information into the asso-ciation-based method.
The algorithm works ef-fectively for various types of MWEs, such asphrases, single words, rigid word sequences (i.e.,no gaps) and gapped word sequences.
Our expe-riment results show that the proposed translationextraction method outperforms word alignment-based methods and association-based methods.We also demonstrate that precise translationsderived by our method significantly improve theperformance of the Moses machine translationsystem.The remainder of this paper is organized asfollows.
Section 2 describes the methodology forextracting translation equivalences of MWEs.Section 3 describes the experiment and presentsthe results.
In Section 4, we consider the appli-cation of our results to machine translation.
Sec-tion 5 contains some concluding remarks.2 Extracting Translation EquivalencesOur MWE translation extraction method is simi-lar to the two-phase approach proposed bySmadja et al (1996).
The two phases can bebriefly described as follows:Phase 1: Extract candidate words correlated tothe given MWE from parallel text.Phase 2:1.
Generate possible translations for theMWE by combining the candidate words.2.
Select possible translations by the Dicecoefficient.We propose an association function, called thenormalized correlation frequency, to extractcandidate words in the phase 1.
This methodincorporates lexical-level information with asso-ciation measure to overcome the contextual ef-fect.
In phase 2, we also propose a weighted fre-quency function to filter out false common sub-sequences from possible translations.
The filter-ing step is applied before the translation select-ing step of phase 2.Before describing our extraction method, wedefine the following important terms usedthroughout the paper.Focused corpus (FC): This is the corpuscreated for each targeted MWE.
It is a subset ofthe original parallel corpora, and is comprised ofthe selected aligned sentence pairs that containthe source MWE and its translations.Candidate word list (CW): A list of extractedcandidate words for the translations of thesource MWE.2.1 Selecting Candidate WordsFor a source MWE, we try to extract from theFC a set of k candidate words CW that are high-ly correlated to the source MWE.
We then as-sume that the target translation is a combinationof some words in CW.
As noted by Smadja et al(1996), this two-step approach drastically reduc-es the search space.However, translations of collocated contextwords in the source word sequence create noisycandidate words, which might cause incorrectextraction of target translations by naive statis-tical correlation measures, such as the Dice coef-479ficient used by Smadja et al (1996).
The need toavoid this context effect motivates us to proposea candidate word selection method that uses thenormalized correlation frequency as an associa-tion measure.The rationale behind the proposed method isas follows.
When counting the word frequency,each word in the target corpus normally contri-butes a frequency count of one.
However, we areonly interested in the word counts correlated to aMWE.
Therefore, intuitively, we define thenormalized count of a target word e as the trans-lation probability of e given the MWE.We explain the concept of normalizing thecorrelation count in Section 2.1.1 and the com-putation of the normalized correlation frequencyin Section 2.1.2.2.1.1 Normalizing Correlation CountWe propose an association measure called thenormalized correlation frequency, which ranksthe association strength of target words with thesource MWE.
For ease of explanation, we usethe following notations: let f=f1,f2,?,fm ande=e1,e2,?,en be a pair of parallel Chinese andEnglish sentences; and let w=t1,t2,?,tr be theChinese source MWE.
Hence, w is a subse-quence of f.When counting the word frequency, eachword in the target corpus normally contributes afrequency count of one.
However, since we areinterested in the word counts that correlate to w,we adopt the concept of the translation modelproposed by Brown et al(1993).
Each word e ina sentence e might be generated by some words,denoted as r, in the source sentence f. If r isnon-empty the relation between r and w shouldfit one of the following cases:1) All words in r belong to w, i.e., wr ?
, sowe say that e is only generated by w.2) No words in r belong to w, i.e., wfr ??
,so we say that e is only generated by contextwords.3) Some words in r belong to w, while othersare context words.Intuitively, In Cases 1 and 2, the correlationcount of an instance e should be 1 and 0 respec-tively.
In Case 3, the normalized count of e isthe expected frequency generated by w dividedby the expected frequency generated by f. Withthat in mind, we define the weighted correlationcount, wcc, as follows:??????
?+?+=fwfwwfejif jf ifepfepewcc||)|(||)|(),,;( ,where ?
is a very small smoothing factor in casee is not generated by any word in f. The proba-bility p(e | f) is the word translation probabilitytrained by IBM Model 1 on the whole parallelcorpus.The rationale behind the weighted correlationcount, wcc, is that if e is part of the translation ofw, then its association with w should be strongerthan other words in the context.
Hence its wccshould be closer to 1.
Otherwise, the associationis weaker and the wcc should be closer to 0.2.1.2 Normalized CorrelationOnce the weighted correlation counts wcc iscomputed for each word in FC, we compute thenormalized correlation frequency for each worde as the total sum of the  of all win bilingual sentences (e, f)  in FC.
The norma-lized correlation frequency (ncf) is defined asfollows:),,;( wfeewcc?==niiiewccencf1)()( ),,;();( wfew .We choose the top-n English words ranked byncf as our candidate words and filter out thosewhose ncf is less than a pre-defined threshold.Table 2 shows the candidate words for the Chi-nese term ????
(quote/take/interpret out ofcontext) sorted by their ncf values.
To illustratethe effectiveness ncf, we also display candidatewords of the term with their Dice values inTables 3.
As shown in the tables, noise wordssuch as justify, meaning and unfair are rankedlower using ncf than using Dice, while correctcandidates, such as out, take and remark areranked higher.
We present more experimentalresults in Section 3.2.2 Generation and Ranking of Candi-date TranslationsAfter determining the candidate words, candi-date translations of w can be generated by mark-ing the candidate words in each sentence of FC.The word sequences marked in each sentenceare deemed possible translations.
At the sametime, the weakly associated function words,480Candidate words e freq ncf(e,w)context 54 31.55out 58 24.58quote 26 5.84take 23 4.81remark 8 1.84interpret 3 1.38piecemeal 1 0.98deliberate 3 0.98Table 2.
Candidate words for the Chinese term????
sorted by their global normalized correla-tion frequencies.Candidate words e freq dice(e,w)context 54 0.0399quote 26 0.0159deliberate 3 0.0063justify 3 0.0034interpretation 7 0.0032meaning 3 0.0029cite 3 0.0025unfair 4 0.0023Table 3.
Candidate words for the Chinese term ????
sorted by their Dice coefficient values.which we fail to select in the candidate wordselection stage, should be recovered.
The rule isquite simple: if a function word is adjacent toany candidate word, it should be recovered.
Forexample, in the following sentence, the functionword of would be recovered and added to themarked sequence:?The financial secretary hasbeen quoted out of context.???
??
?
??
?
????.
?The marked words are shown in boldface.2.2.1 Generating Possible TranslationsAlthough we have selected a reliable candidateword list, it may still contain some noisy wordsdue to the MWE?s collocation context.
Considerthe following example:...as quoted in the auditreport, if taken out of con-text...In this instance, quoted is a false positive; there-fore, the marked word sequence m ?quoted tak-en out of context?
is not the correct translation.To avoid such false positives, we include m andall its subsequences as possible translations.quoted taken out of contextquoted taken out ofquoted taken out contextquoted taken of contextquoted out of contexttaken out of context?quoted outtaken outquotedtakenoutcontextTable 4.
Example subsequences generated of w andadd them to the candidate translation list.Table 4 shows the subsequences of m in theabove example.
The generation process is usedto increase the coverage of correct translations inthe candidate list; otherwise, many correct trans-lations will be lost.
However, the process mayalso trigger the side effect of the common sub-sequence problem described in Section 1.
Sinceall candidates compete for the best translationsby comparing their association strength with w,the common subsequences will have an advan-tage.2.2.2 Filtering Common SubsequencesTo resolve the common subsequence effect prob-lem, we evaluate each candidate translation, in-cluding its subsequences, by a concept similar tothe normalized correlation frequency.
As men-tioned in Section 1, the Dice coefficient tends toselect the common subsequences of some candi-dates because they have higher frequencies.
Toavoid this problem, we use the normalized corre-lation frequency to filter out false common sub-sequences from the candidate translation list.Here, we also use the weighted correlation countwcc to weight the frequency count of a candidatetranslation.
Suppose we have a marked sequencein a sentence, m, whose subsequences are gener-ated in the way described in the previous section.If the weighted count of m is assigned the score1, the weighted count (wc) of a subsequence t isthen defined as follows:????
?=tmwfewmfeteewccwc )),,;(1(),,,;( .The underlying concept of wc is that the originalmarked sequence m is supposed to be the most481likely translation of w and the weighted count isset to 1.
Then, if a subsequence t is generated byremoving a word e from m, the weighted countof the subsequence is reduced by multiplying thecomplement probability of e generated by w.Note that the weighted correlation count wcc isthe probability of the word e generated by w.After all  in each sentence ofthe FC have been computed, the weighted fre-quency for a sequence t can be determined bysumming the weighted frequencies over FC asfollows:),,,;( wmfetwc??
?=FCwcwf),(),,,;();(fewmfetwt .We compute the wf for each candidate transla-tion and then sort the candidate translations bytheir wf values.Next, we filter out common subsequencesbased on the following rule: for a sequence t, ifthere is a super-sequence t' on the sorted candi-date translation list and the wf value of t is lessthan that of t', then t is assumed be a commonsubsequence of real translations and removedfrom the list.candidate translation list freq wfquote out of context 19 17.55of context 35 15.45out of context 32 14.82quote of context 19 13.32out 35 11.92quote 23 11.63quote out 19 9.42Table 5.
Part of the candidate translation list for theChinese idiom, ???
?, sorted by the wf values.Table 5 shows an example of the rule?s appli-cation.
The candidate translation list is sorted bythe translations?
wf values.
Then, candidates 2-7are removed because they are subsequences ofthe first candidate and their wf values are smallerthan that of the first candidate.2.3 Selection of Candidate TranslationsHaving removed the common subsequences ofreal translations from the candidate translationlist of w, we can select the best translations bycomparing their association strength with w forthe remaining candidates.
The Dice coefficientis a good measure for assessing the associationstrength and selecting translations from the can-didate list.
For a candidate translation t, the Dicecoefficient is defined as follows:)()(),(2),(wtwtwtpppDice += .Where p(t,w), p(t), p(w) are probabilities of(t,w), t, w derived from the training corpus.After obtaining the Dice coefficients of thecandidate translations, we select the top-n candi-date translations as possible translations of w.3 ExperimentsIn our experiments, we use the Hong Kong Han-sard and the Hong Kong News parallel corporaas training data.
The training data was prepro-cessed by Chinese word segmentation to identifywords and parsed by Chinese parser to extractMWEs.
To evaluate the proposed approach, werandomly extract 309 Chinese MWEs fromtraining data, including dependent word pairsand rigid idioms.
We then randomly select 103of those MWEs as the development set and usethe other 206 as the test set.
The reference trans-lations of each Chinese MWE are manually ex-tracted from the parallel corpora.3.1 Evaluation of Word CandidatesTo evaluate the method for selecting candidatewords, we use the coverage rate, which is de-fined as follows:??
?=w www||||1ACAncoverage ,where n is the number of MWEs in the test set,Aw denotes the word set of the reference transla-tions of w, and Cw denotes a candidate word listextracted by the system.Table 6 shows the coverage of our method,NCF, compared with the coverage of the IBMmodel 1 and the association-based methods MI,Chi-square, and Dice.
As we can see, the top-10candidate words of NCF cover almost 90% ofthe words in the reference translations.
Whereas,the coverage of the association-based methodsand IBM model 1 is much lower than 90%.
Theresult implies that the candidate extraction me-thod can extract a more precise candidate setthan other methods.482Method Top10 Top20 Top30MI 0.514 0.684 0.760Chi-square 0.638 0.765 0.828Dice 0.572 0.735 0.803IBM 1 0.822 0.900 0.948NCF 0.899 0.962 0.973Table 6.
The coverage rates of the candidate wordsextracted by the compared methodsFigure 1 shows the curve diagram of the cov-erage rate of each method.
As the figure shows,when the size of the candidate list is increased,the coverage rate of using NCF rises rapidly as nincreases but levels off after n=20.
Whereas, thecoverage rates of other measures grow muchslowly.Figure 1.
The curve diagram of the coverage ofthe candidate word list compiled by each method.From the evaluation of candidate word selec-tion, we find that the ncf method, which incorpo-rates lexical-level information into association-based measure, can effectively filter out noisywords and generates a highly reliable list of can-didate words for a given MWE.3.2 Evaluating Extracted TranslationsTo evaluate the quality of MWE translationsextracted automatically, we use the followingthree criteria:1) Translation accuracy:This criterion is used to evaluate the top-ntranslations of the system.
It treats eachtranslation produced as a string and com-pares the whole string with the given ref-erence translations.
If any one of the top-nhypothesis translations is included in thereference translations, it is deemed correct.2) WER (word error rate):This criterion compares the top-1 hypo-thesis translation with the reference trans-lations by computing the edit distance (i.e.,the minimum number of substitutions, in-sertions, and deletions) between the hypo-thesis translation and the given referencetranslations.3) PER (position-independent word errorrate):This criterion ignores the word order andcomputes the edit distance between thetop-1 hypothesis translation and the givenreference translations.We also use the MT task to evaluated our me-thod with other systems.
For that, we use theGIZA++ toolkit (Och et al, 2000 ) to align theHong Kong Hansard and Hong Kong News pa-rallel corpora.
Then, we extract the translationsof the given source sequences from the alignedcorpus as the baseline.
We use the following twomethods to extract translations from the alignedresults.1) Uni-directional alignmentWe mark all English words that werelinked to any constituent of w in the pa-rallel Chinese-English aligned corpora.Then, we extract the marked sequencesfrom the corpora and compute the fre-quency of each sequence.
The top-n highfrequency sequences are returned as thepossible translations of w.2) Bi-directional alignmentsWe use the grow-diag-final heuristic (Ochet al, 1999) to combine the Chinese-English and English-Chinese alignments,and then extract the top-n high frequencysequences as described in method 1.To determine the effect of the common subse-quence filtering method, FCS, we divide theevaluation of our system into two phases:1) NCF+Dice:This system uses the normalized correla-tion frequency, NCF, to select candidatewords as described in Section 2.1.
It thenextracts candidate translations (describedin Section 2.2), but FCS is not used.2) NCF+FCS+Dice:This is similar to system 1, but it usesFCS to filter out common subsequences(described in subsection 2.2.2).483Method WER(%) PER(%)Uni-directional 4.84 4.02Bi-directional 5.84 5.12NCF+Dice 3.55 3.24NCF+FCS+Dice 2.45 2.23Table 7.
Translation error rates of the systems.Method Top1 Top2 Top3Uni-directional 67.5 79.6 83.0Bi-directional 65.5 77.7 81.1NCF+Dice 72.8 85.9 88.3NCF+FCS+Dice 78.2 89.3 91.7Table 8.
Translation accuracy rates of the systems.
(%)Table 7 shows the word error rates for theabove systems.
As shown in the first and secondrows, the translations extracted from uni-directional alignments are better than those ex-tracted from bi-directional alignments.
Thismeans that the grow-diag-final heuristic reducesthe accuracy rate when extracting MWE transla-tions.The results in the third row show that theNCF+Dice system outperforms the methodsbased on GIZA++.
In other words, the NCF me-thod can effectively resolve the difficulties ofextracting MWE translations discussed in Sec-tion 1.In addition, the fourth row shows that theNCF+FCS+Dice system also outperforms theNCF+Dice system.
Thus, the FCS method canresolve the common subsequence problem effec-tively.Table 8 shows the translation accuracy ratesof each system.
The NCF+FCS+Dice systemachieves the best translation accuracy.
Moreover,it significantly improves the performance offinding MWE translation equivalences.4 Applying  MWE Translations to MTTo demonstrate the usefulness of extractedMWE translations to existing statistical machinetranslation systems, we use the XML markupscheme provided by the Moses decoder, whichallows the specification of translations for partsof a sentence.
The procedure for this experimentconsists of three steps: (1) the extracted MWEtranslations are added to the test set with theXML markup scheme, (2) after which the data isinput to the Moses decoder to complete thetranslation task, (3) the results are evaluatedMoses  MWE +MosesNIST06-sub 23.12 23.49NIST06 21.57 21.79Table 9.
BLEU scores of the translation results.using the BLEU metric (Papineni et al, 2002).4.1 Experimental SettingsTo train a translation model for Moses, we usethe Hong Kong Hansard and the Hong KongNews parallel corpora as training data(2,222,570 sentence pairs).
We also use thesame parallel corpora to extract translations ofMWEs.
The NIST 2008 evaluation data (1,357sentences, 4 references) is used as developmentset and NIST 2006 evaluation data (1,664 sen-tences, 4 references) is used as test set.4.2 Selection of MWEsDue to the limitation of the XML markupscheme, we only consider two types of MWEs:continuous bigrams and idioms.
Since the goalof this experiment is not focus on extraction ofMWEs, simple methods are applied to extractMWEs from the training data: (1) we collect allcontinuous bigrams from Chinese sentences inthe training data and then simply filter out thebigrams by mutual information (MI) with a thre-shold1, (2) we also extract all idioms from Chi-nese sentences of the training data by collectingall 4-syllables words from the training data andfiltering out obvious non-idioms, such as deter-minative-measure words and temporal words bytheir part-of-speeches, because most Chineseidioms are 4-syllables words.In total, 33,767 Chinese bigram types and20,997 Chinese idiom types were extracted fromtraining data; and the top-5 translations of eachMWE were extracted by the method described inSection 2.
Meanwhile 1,171 Chinese MWEswere added to the translations in the test set.
TheChinese words covered by the MWEs in testdata set were 2,081 (5.3%).4.3 Extra InformationWhen adding the translations to the test data,two extra types of information are required bythe Moses decoder.
The first type comprises thefunction words between the translation and itscontext.
For example, if ??
?
?/economiccooperation is added to the test data, possible1 We set the threshold at 5.484source sentence ...
????<MWE>????</MWE>?????
...Moses ... entered blinded by the colourful community ...MWE+Moses ... entered the colourful community ...reference ... entered the colorful society ...source sentence ...
?????
<MWE>???
?
?</MWE> ???
...Moses ... do not want to see an escalation of crisis ...MWE+Moses ... do not want to see a further escalation of crisis ...reference ... don 't want to see the further escalation of the crisis ...source sentence ...
????????<MWE>????
?</MWE> ...Moses ... the people 's interests ...MWE+Moses ... the people of the fundamental interests ...reference ... the fundamental interests of the masses ...Table 10.
Examples of improved translation quality with the MWE translation equivalences.function words, such as ?in?
or ?with?, should beprovided for the translation.
Because the Mosesdecoder does not generate function words thatare context dependent, it treats a function wordas a part of the translation.
Therefore, we collectpossible function words for each translationfrom the corpora when the conditional probabili-ty is larger than a threshold2.The second type of information is the phrasetranslation probability and lexical weighting.Computing the phrase translation probability istrivial in the training corpora, but lexical weight-ing (Koehn et al, 2003) needs lexical-levelalignment.
For convenience, we assume thateach word in an MWE links to each word in thetranslations.
Under this assumption, the lexicalweighting is simplified as follows:???
?= ?= aji jiniw efpajijap),(1)|(|}),(|{|1),|( ef?
?= ??
?ni ejijefp1)|(||1ee.Then, it is trivial to compute the simplified lexi-cal weighting of each MWE correspondencewhen the word translation probability table isprovided.
Here, we use the IBM model 1 to learnthe table from the training data.4.4 Evaluation ResultsWe trained a model using Moses toolkit (Koehnet al, 2007) on the training data as our baselinesystem.Table 9 shows the influence of adding theMWE translations to the test data.
In the firstrow (NIST06-sub), we only consider sentencescontaining MWE translations for BLEU scoreevaluation (726 sentences).
In the second row,we took the whole NIST 2006 evaluation setinto consideration (1,664 sentences).
The Chi-nese words covered by the MWEs in NIST06-sub and NIST06 were 9.9% and 5.3% respec-tively.Adding MWE translations to the test data sta-tistically significantly lead to better results thanthose of the baseline.
Significance was testedusing a paired bootstrap (Koehn, 2004) with1000 samples (p<0.02).
Although the improve-ment in BLEU score seems small, it is actuallyreasonably good given that the MWEs accountfor only 5% of the NIST06 test set.
Examples ofimproved translations are shown in Table 10.There is still room for improvement of the pro-posed MWE extraction method in order to pro-vide more MWE translation pairs or design afeasible way to incorporate discontinuous bilin-gual MWEs to the decoder.5 Conclusions and Future WorkWe have proposed a high precision algorithm forextracting translations of multiword expressionsfrom parallel corpora.
The algorithm can be usedto translate any language pair and any type ofword sequence, including rigid sequences anddiscontinuous sequences.
Our evaluation resultsshow that the algorithm can cope with the diffi-culties caused by indirect association and thecommon subsequence effects, leading to signifi-cant improvement over the word alignment-based extraction methods used by the state of theart systems and other association-based extrac-tion methods.
We also demonstrate that ex-tracted translations significantly improve the2 We set the threshold at 0.1.485performance of the Moses machine translationsystem.In future work, it would be interesting to de-velop a machine translation model that can beintegrated with the translation acquisition algo-rithm in a more effective way.
Using the norma-lized-frequency score to help phrase alignmenttasks, as the grow-diag-final heuristic, wouldalso be interesting direction to explore.AcknowledgementThis research was supported in part by the Na-tional Science Council of Taiwan under the NSCGrants: NSC 96-2221-E-001-023-MY3.ReferencesBrown, Peter F., Stephen A. Della Pietra, Vincent J.Della Pietra, Robert L. Mercer.
1993.
The Mathe-matics of Statistical Machine Translation: Parame-ter Estimation.
Computational Linguistics,19(2):263-311.Gao, Jianfeng, Jian-Yun Nie, Hongzhao He, WeijunChen, Ming Zhou.
2002.
Resolving Query Trans-lation Ambiguity using a Decaying Co-occurrenceModel and Syntactic Dependence Relations.
InProc.
of SIGIR?02.
pp.
183 -190.Kitamura, Mihoko and Yuji Matsumoto.
1996.
Au-tomatic Extraction of Word Sequence Correspon-dences in Parallel Corpora.
In Proc.
of the 4th An-nual Workshop on Very Large Corpora.
pp.
79-87.Koehn, Philipp, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Proc.of HLT/NAACL?03.
pp.
127-133.Koehn, Philipp.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.EMNLP?04.
pp.
388-395.Koehn, Philipp, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertol-di, Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In ACL?07, demonstration session.Kupiec, Julian.
1993.
An Algorithm for FindingNoun Phrase Correspondences in Bilingual Corpo-ra.
In Proc.
of ACL?93 .
pp.
17-22.Liang, Percy, Ben Taskar, Dan Klein.
2006.
Align-ment by Agreement.
In Proc.
of HLT/NAACL?06.pp.
104-111.Ma, Yanjun, Nicolas Stroppa, Andy Way.
2007.Bootstrapping Word Alignment via Word Packing.In Proc.
of ACL?07.
pp.
304-311.Ma, Yanjun, Sylwia Ozdowska, Yanli Sun, and AndyWay.
2008.
Improving Word Alignment UsingSyntactic Dependencies.
In Proc.
of ACL/HLT?08Second Workshop on Syntax and Structure in Sta-tistical Translation.
pp.
69-77.Melamed, Ilya Dan.
2001.
Empirical Methods forExploiting parallel Texts.
MIT press.Och, Franz Josef and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
In Proc.
of ACL?00.pp.
440-447.Och, Franz Josef, Christoph Tillmann, and HermannNey.
1999.
Improved Alignment Models for Sta-tistical Machine Translation.
In Proc.
ofEMNLP/VLC?99.
pp.
20-28.Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a Method for Auto-matic Evaluation of Machine Translation.
In Proc.of ACL?02.
pp.
311-318.Smadja, Frank, Kathleen R. McKeown, and VasileiosHatzivassiloglou.
1996.
Translating Collocationsfor Bilingual Lexicons: A Statistical Approach.Computational Linguistics, 22(1):1-38.Wu, Dekai.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Cor-pora.
Computational Linguistics, 23(3):377-403.Wu, Hua, Ming Zhou.
2003.
Synonymous Colloca-tion Extraction Using Translation Information.
InProc.
of ACL?03.
pp.
120-127.Yamamoto, Kaoru, Yuji Matsumoto.
2000.
Acquisi-tion of Phrase-level Bilingual Correspondence us-ing Dependency Structure.
In Proc.
of COL-ING?00.
pp.
933-939.486
