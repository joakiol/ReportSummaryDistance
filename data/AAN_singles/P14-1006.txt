Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 58?68,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsMultilingual Models for Compositional Distributed SemanticsKarl Moritz Hermann and Phil BlunsomDepartment of Computer ScienceUniversity of OxfordOxford, OX1 3QD, UK{karl.moritz.hermann,phil.blunsom}@cs.ox.ac.ukAbstractWe present a novel technique for learn-ing semantic representations, which ex-tends the distributional hypothesis to mul-tilingual data and joint-space embeddings.Our models leverage parallel data andlearn to strongly align the embeddings ofsemantically equivalent sentences, whilemaintaining sufficient distance betweenthose of dissimilar sentences.
The mod-els do not rely on word alignments orany syntactic information and are success-fully applied to a number of diverse lan-guages.
We extend our approach to learnsemantic representations at the documentlevel, too.
We evaluate these models ontwo cross-lingual document classificationtasks, outperforming the prior state of theart.
Through qualitative analysis and thestudy of pivoting effects we demonstratethat our representations are semanticallyplausible and can capture semantic rela-tionships across languages without paral-lel data.1 IntroductionDistributed representations of words provide thebasis for many state-of-the-art approaches to var-ious problems in natural language processing to-day.
Such word embeddings are naturally richerrepresentations than those of symbolic or discretemodels, and have been shown to be able to captureboth syntactic and semantic information.
Success-ful applications of such models include languagemodelling (Bengio et al, 2003), paraphrase detec-tion (Erk and Pad?o, 2008), and dialogue analysis(Kalchbrenner and Blunsom, 2013).Within a monolingual context, the distributionalhypothesis (Firth, 1957) forms the basis of mostapproaches for learning word representations.
InFigure 1: Model with parallel input sentences a and b. Themodel minimises the distance between the sentence level en-coding of the bitext.
Any composition functions (CVM) canbe used to generate the compositional sentence level repre-sentations.this work, we extend this hypothesis to multilin-gual data and joint-space embeddings.
We presenta novel unsupervised technique for learning se-mantic representations that leverages parallel cor-pora and employs semantic transfer through com-positional representations.
Unlike most methodsfor learning word representations, which are re-stricted to a single language, our approach learnsto represent meaning across languages in a sharedmultilingual semantic space.We present experiments on two corpora.
First,we show that for cross-lingual document clas-sification on the Reuters RCV1/RCV2 corpora(Lewis et al, 2004), we outperform the prior stateof the art (Klementiev et al, 2012).
Second,we also present classification results on a mas-sively multilingual corpus which we derive fromthe TED corpus (Cettolo et al, 2012).
The re-sults on this task, in comparison with a number ofstrong baselines, further demonstrate the relevanceof our approach and the success of our methodin learning multilingual semantic representationsover a wide range of languages.582 OverviewDistributed representation learning describes thetask of learning continuous representations for dis-crete objects.
Here, we focus on learning seman-tic representations and investigate how the use ofmultilingual data can improve learning such rep-resentations at the word and higher level.
Wepresent a model that learns to represent eachword in a lexicon by a continuous vector in Rd.Such distributed representations allow a model toshare meaning between similar words, and havebeen used to capture semantic, syntactic and mor-phological content (Collobert and Weston, 2008;Turian et al, 2010, inter alia).We describe a multilingual objective functionthat uses a noise-contrastive update between se-mantic representations of different languages tolearn these word embeddings.
As part of this, weuse a compositional vector model (CVM, hence-forth) to compute semantic representations of sen-tences and documents.
A CVM learns seman-tic representations of larger syntactic units giventhe semantic representations of their constituents(Clark and Pulman, 2007; Mitchell and Lapata,2008; Baroni and Zamparelli, 2010; Grefenstetteand Sadrzadeh, 2011; Socher et al, 2012; Her-mann and Blunsom, 2013, inter alia).A key difference between our approach andthose listed above is that we only require sentence-aligned parallel data in our otherwise unsuper-vised learning function.
This removes a number ofconstraints that normally come with CVM mod-els, such as the need for syntactic parse trees, wordalignment or annotated data as a training signal.At the same time, by using multiple CVMs totransfer information between languages, we en-able our models to capture a broader semantic con-text than would otherwise be possible.The idea of extracting semantics from multilin-gual data stems from prior work in the field ofsemantic grounding.
Language acquisition in hu-mans is widely seen as grounded in sensory-motorexperience (Bloom, 2001; Roy, 2003).
Basedon this idea, there have been some attempts atusing multi-modal data for learning better vec-tor representations of words (e.g.
Srivastava andSalakhutdinov (2012)).
Such methods, however,are not easily scalable across languages or to largeamounts of data for which no secondary or tertiaryrepresentation might exist.Parallel data in multiple languages provides analternative to such secondary representations, asparallel texts share their semantics, and thus onelanguage can be used to ground the other.
Somework has exploited this idea for transferring lin-guistic knowledge into low-resource languages orto learn distributed representations at the wordlevel (Klementiev et al, 2012; Zou et al, 2013;Lauly et al, 2013, inter alia).
So far almost allof this work has been focused on learning multi-lingual representations at the word level.
As dis-tributed representations of larger expressions havebeen shown to be highly useful for a number oftasks, it seems to be a natural next step to attemptto induce these, too, cross-lingually.3 ApproachMost prior work on learning compositional se-mantic representations employs parse trees ontheir training data to structure their compositionfunctions (Socher et al, 2012; Hermann and Blun-som, 2013, inter alia).
Further, these approachestypically depend on specific semantic signals suchas sentiment- or topic-labels for their objectivefunctions.
While these methods have been shownto work in some cases, the need for parse trees andannotated data limits such approaches to resource-fortunate languages.
Our novel method for learn-ing compositional vectors removes these require-ments, and as such can more easily be applied tolow-resource languages.Specifically, we attempt to learn semantics frommultilingual data.
The idea is that, given enoughparallel data, a shared representation of two paral-lel sentences would be forced to capture the com-mon elements between these two sentences.
Whatparallel sentences share, of course, are their se-mantics.
Naturally, different languages expressmeaning in different ways.
We utilise this di-versity to abstract further from mono-lingual sur-face realisations to deeper semantic representa-tions.
We exploit this semantic similarity acrosslanguages by defining a bilingual (and triviallymultilingual) energy as follows.Assume two functions f : X ?
Rdandg : Y ?
Rd, which map sentences from lan-guages x and y onto distributed semanticrepresentations in Rd.
Given a parallel corpus C,we then define the energy of the model given twosentences (a, b) ?
C as:Ebi(a, b) = ?f(a)?
g(b)?2(1)59We want to minimize Ebifor all semanticallyequivalent sentences in the corpus.
In order toprevent the model from degenerating, we fur-ther introduce a noise-constrastive large-marginupdate which ensures that the representations ofnon-aligned sentences observe a certain marginfrom each other.
For every pair of parallel sen-tences (a, b) we sample a number of additionalsentence pairs (?, n) ?
C, where n?with highprobability?is not semantically equivalent to a.We use these noise samples as follows:Ehl(a, b, n) = [m+ Ebi(a, b)?
Ebi(a, n)]+where [x]+= max(x, 0) denotes the standardhinge loss and m is the margin.
This results inthe following objective function:J(?)
=?
(a,b)?C(k?i=1Ehl(a, b, ni) +?2??
?2)(2)where ?
is the set of all model variables.3.1 Two Composition ModelsThe objective function in Equation 2 could be cou-pled with any two given vector composition func-tions f, g from the literature.
As we aim to applyour approach to a wide range of languages, we fo-cus on composition functions that do not requireany syntactic information.
We evaluate the follow-ing two composition functions.The first model, ADD, represents a sentence bythe sum of its word vectors.
This is a distributedbag-of-words approach as sentence ordering is nottaken into account by the model.Second, the BI model is designed to capture bi-gram information, using a non-linearity over bi-gram pairs in its composition function:f(x) =n?i=1tanh (xi?1+ xi) (3)The use of a non-linearity enables the model tolearn interesting interactions between words in adocument, which the bag-of-words approach ofADD is not capable of learning.
We use the hy-perbolic tangent as activation function.3.2 Document-level SemanticsFor a number of tasks, such as topic modelling,representations of objects beyond the sentencelevel are required.
While most approaches to com-positional distributed semantics end at the wordFigure 2: Description of a parallel document-level composi-tional vector model (DOC).
The model recursively computessemantic representations for each sentence of a document andthen for the document itself, treating the sentence vectors asinputs for a second CVM.level, our model extends to document-level learn-ing quite naturally, by recursively applying thecomposition and objective function (Equation 2)to compose sentences into documents.
This isachieved by first computing semantic representa-tions for each sentence in a document.
Next, theserepresentations are used as inputs in a higher-levelCVM, computing a semantic representation of adocument (Figure 2).This recursive approach integrates document-level representations into the learning process.We can thus use corpora of parallel documents?regardless of whether they are sentence aligned ornot?to propagate a semantic signal back to theindividual words.
If sentence alignment is avail-able, of course, the document-signal can simplybe combined with the sentence-signal, as we didwith the experiments described in ?5.3.This concept of learning compositional repre-sentations for documents contrasts with prior work(Socher et al, 2011; Klementiev et al, 2012, interalia) who rely on summing or averaging sentence-vectors if representations beyond the sentence-level are required for a particular task.We evaluate the models presented in this paperboth with and without the document-level signal.We refer to the individual models used as ADD andBI if used without, and as DOC/ADD and DOC/BIis used with the additional document compositionfunction and error signal.4 CorporaWe use two corpora for learning semantic rep-resentations and performing the experiments de-scribed in this paper.60The Europarl corpus v71(Koehn, 2005) wasused during initial development and testing ofour approach, as well as to learn the representa-tions used for the Cross-Lingual Document Clas-sification task described in ?5.2.
We consideredthe English-German and English-French languagepairs from this corpus.
From each pair the final100,000 sentences were reserved for development.Second, we developed a massively multilin-gual corpus based on the TED corpus2for IWSLT2013 (Cettolo et al, 2012).
This corpus containsEnglish transcriptions and multilingual, sentence-aligned translations of talks from the TED confer-ence.
While the corpus is aimed at machine trans-lation tasks, we use the keywords associated witheach talk to build a subsidiary corpus for multilin-gual document classification as follows.3The development sections provided with theIWSLT 2013 corpus were again reserved for de-velopment.
We removed approximately 10 per-cent of the training data in each language to cre-ate a test corpus (all talks with id ?
1,400).
Thenew training corpus consists of a total of 12,078parallel documents distributed across 12 languagepairs4.
In total, this amounts to 1,678,219 non-English sentences (the number of unique Englishsentences is smaller as many documents are trans-lated into multiple languages and thus appear re-peatedly in the corpus).
Each document (talk) con-tains one or several keywords.
We used the 15most frequent keywords for the topic classificationexperiments described in section ?5.3.Both corpora were pre-processed using the setof tools provided by cdec5for tokenizing and low-ercasing the data.
Further, all empty sentences andtheir translations were removed from the corpus.5 ExperimentsWe report results on two experiments.
First, wereplicate the cross-lingual document classificationtask of Klementiev et al (2012), learning dis-tributed representations on the Europarl corpusand evaluating on documents from the ReutersRCV1/RCV2 corpora.
Subsequently, we design a1http://www.statmt.org/europarl/2https://wit3.fbk.eu/3http://www.clg.ox.ac.uk/tedcldc/4English to Arabic, German, French, Spanish, Italian,Dutch, Polish, Brazilian Portuguese, Romanian, Russian andTurkish.
Chinese, Farsi and Slowenian were removed due tothe small size of those datasets.5http://cdec-decoder.org/multi-label classification task using the TED cor-pus, both for training and evaluating.
The use ofa wider range of languages in the second experi-ments allows us to better evaluate our models?
ca-pabilities in learning a shared multilingual seman-tic representation.
We also investigate the learnedembeddings from a qualitative perspective in ?5.4.5.1 LearningAll model weights were randomly initialised us-ing a Gaussian distribution (?=0, ?2=0.1).
Weused the available development data to set ourmodel parameters.
For each positive sample weused a number of noise samples (k ?
{1, 10, 50}),randomly drawn from the corpus at each trainingepoch.
All our embeddings have dimensionalityd=128, with the margin set to m=d.6Further, weuse L2 regularization with ?=1 and step-size in{0.01, 0.05}.
We use 100 iterations for the RCVtask, 500 for the TED single and 5 for the jointcorpora.
We use the adaptive gradient method,AdaGrad (Duchi et al, 2011), for updating theweights of our models, in a mini-batch setting (b ?
{10, 50}).
All settings, our model implementationand scripts to replicate our experiments are avail-able at http://www.karlmoritz.com/.5.2 RCV1/RCV2 Document ClassificationWe evaluate our models on the cross-lingual doc-ument classification (CLDC, henceforth) task firstdescribed in Klementiev et al (2012).
This task in-volves learning language independent embeddingswhich are then used for document classificationacross the English-German language pair.
For this,CLDC employs a particular kind of supervision,namely using supervised training data in one lan-guage and evaluating without further supervisionin another.
Thus, CLDC can be used to establishwhether our learned representations are semanti-cally useful across multiple languages.We follow the experimental setup described inKlementiev et al (2012), with the exception thatwe learn our embeddings using solely the Europarldata and use the Reuters corpora only during forclassifier training and testing.
Each document inthe classification task is represented by the aver-age of the d-dimensional representations of all itssentences.
We train the multiclass classifier usingan averaged perceptron (Collins, 2002) with thesame settings as in Klementiev et al (2012).6On the RCV task we also report results for d=40 whichmatches the dimensionality of Klementiev et al (2012).61Model en?
de de?
enMajority Class 46.8 46.8Glossed 65.1 68.6MT 68.1 67.4I-Matrix 77.6 71.1dim = 40ADD 83.7 71.4ADD+ 86.2 76.9BI 83.4 69.2BI+ 86.9 74.3dim = 128ADD 86.4 74.7ADD+ 87.7 77.5BI 86.1 79.0BI+ 88.1 79.2Table 1: Classification accuracy for training on English andGerman with 1000 labeled examples on the RCV corpus.Cross-lingual compositional representations (ADD, BI andtheir multilingual extensions), I-Matrix (Klementiev et al,2012) translated (MT) and glossed (Glossed) word baselines,and the majority class baseline.
The baseline results are fromKlementiev et al (2012).We present results from four models.
The ADDmodel is trained on 500k sentence pairs of theEnglish-German parallel section of the Europarlcorpus.
The ADD+ model uses an additional 500kparallel sentences from the English-French cor-pus, resulting in one million English sentences,each paired up with either a German or a Frenchsentence, with BI and BI+ trained accordingly.The motivation behind ADD+ and BI+ is to inves-tigate whether we can learn better embeddings byintroducing additional data from other languages.A similar idea exists in machine translation whereEnglish is frequently used to pivot between otherlanguages (Cohn and Lapata, 2007).The actual CLDC experiments are performedby training on English and testing on German doc-uments and vice versa.
Following prior work, weuse varying sizes between 100 and 10,000 docu-ments when training the multiclass classifier.
Theresults of this task across training sizes are in Fig-ure 3.
Table 1 shows the results for training on1,000 documents compared with the results pub-lished in Klementiev et al (2012).
Our modelsoutperform the prior state of the art, with the BImodels performing slightly better than the ADDmodels.
As the relative results indicate, the addi-tion of a second language improves model perfor-mance.
It it interesting to note that results improvein both directions of the task, even though no addi-tional German data was used for the ?+?
models.5.3 TED Corpus ExperimentsHere we describe our experiments on the TED cor-pus, which enables us to scale up to multilinguallearning.
Consisting of a large number of rela-tively short and parallel documents, this corpus al-lows us to evaluate the performance of the DOCmodel described in ?3.2.We use the training data of the corpus to learndistributed representations across 12 languages.Training is performed in two settings.
In the sin-gle mode, vectors are learnt from a single lan-guage pair (en-X), while in the joint mode vector-learning is performed on all parallel sub-corporasimultaneously.
This setting causes words fromall languages to be embedded in a single semanticspace.First, we evaluate the effect of the document-level error signal (DOC, described in ?3.2), as wellas whether our multilingual learning method canextend to a larger variety of languages.
We trainDOC models, using both ADD and BI as CVM(DOC/ADD, DOC/BI), both in the single and jointmode.
For comparison, we also train ADD andDOC models without the document-level error sig-nal.
The resulting document-level representationsare used to train classifiers (system and settings asin ?5.2) for each language, which are then evalu-ated in the paired language.
In the English casewe train twelve individual classifiers, each usingthe training data of a single language pair only.As described in ?4, we use 15 keywords for theclassification task.
Due to space limitations, wereport cumulative results in the form of F1-scoresthroughout this paper.MT System We develop a machine translationbaseline as follows.
We train a machine translationtool on the parallel training data, using the devel-opment data of each language pair to optimize thetranslation system.
We use the cdec decoder (Dyeret al, 2010) with default settings for this purpose.With this system we translate the test data, andthen use a Na?
?ve Bayes classifier7for the actualexperiments.
To exemplify, this means the de?arresult is produced by training a translation systemfrom Arabic to German.
The Arabic test set istranslated into German.
A classifier is then trained7We use the implementation in Mallet (McCallum, 2002)62Setting LanguagesArabic German Spanish French Italian Dutch Polish Pt-Br Roman.
Russian Turkishen?
L2MT System 0.429 0.465 0.518 0.526 0.514 0.505 0.445 0.470 0.493 0.432 0.409ADD single 0.328 0.343 0.401 0.275 0.282 0.317 0.141 0.227 0.282 0.338 0.241BI single 0.375 0.360 0.379 0.431 0.465 0.421 0.435 0.329 0.426 0.423 0.481DOC/ADD single 0.410 0.424 0.383 0.476 0.485 0.264 0.402 0.354 0.418 0.448 0.452DOC/BI single 0.389 0.428 0.416 0.445 0.473 0.219 0.403 0.400 0.467 0.421 0.457DOC/ADD joint 0.392 0.405 0.443 0.447 0.475 0.453 0.394 0.409 0.446 0.476 0.417DOC/BI joint 0.372 0.369 0.451 0.429 0.404 0.433 0.417 0.399 0.453 0.439 0.418L2 ?
enMT System 0.448 0.469 0.486 0.358 0.481 0.463 0.460 0.374 0.486 0.404 0.441ADD single 0.380 0.337 0.446 0.293 0.357 0.295 0.327 0.235 0.293 0.355 0.375BI single 0.354 0.411 0.344 0.426 0.439 0.428 0.443 0.357 0.426 0.442 0.403DOC/ADD single 0.452 0.476 0.422 0.464 0.461 0.251 0.400 0.338 0.407 0.471 0.435DOC/BI single 0.406 0.442 0.365 0.479 0.460 0.235 0.393 0.380 0.426 0.467 0.477DOC/ADD joint 0.396 0.388 0.399 0.415 0.461 0.478 0.352 0.399 0.412 0.343 0.343DOC/BI joint 0.343 0.375 0.369 0.419 0.398 0.438 0.353 0.391 0.430 0.375 0.388Table 2: F1-scores for the TED document classification task for individual languages.
Results are re-ported for both directions (training on English, evaluating on L2 and vice versa).
Bold indicates bestresult, underline best result amongst the vector-based systems.TrainingLanguageTest LanguageArabic German Spanish French Italian Dutch Polish Pt-Br Rom?n Russian TurkishArabic 0.378 0.436 0.432 0.444 0.438 0.389 0.425 0.420 0.446 0.397German 0.368 0.474 0.460 0.464 0.440 0.375 0.417 0.447 0.458 0.443Spanish 0.353 0.355 0.420 0.439 0.435 0.415 0.390 0.424 0.427 0.382French 0.383 0.366 0.487 0.474 0.429 0.403 0.418 0.458 0.415 0.398Italian 0.398 0.405 0.461 0.466 0.393 0.339 0.347 0.376 0.382 0.352Dutch 0.377 0.354 0.463 0.464 0.460 0.405 0.386 0.415 0.407 0.395Polish 0.359 0.386 0.449 0.444 0.430 0.441 0.401 0.434 0.398 0.408Portuguese 0.391 0.392 0.476 0.447 0.486 0.458 0.403 0.457 0.431 0.431Romanian 0.416 0.320 0.473 0.476 0.460 0.434 0.416 0.433 0.444 0.402Russian 0.372 0.352 0.492 0.427 0.438 0.452 0.430 0.419 0.441 0.447Turkish 0.376 0.352 0.479 0.433 0.427 0.423 0.439 0.367 0.434 0.411Table 3: F1-scores for TED corpus document classification results when training and testing on twolanguages that do not share any parallel data.
We train a DOC/ADD model on all en-L2 language pairstogether, and then use the resulting embeddings to train document classifiers in each language.
Theseclassifiers are subsequently used to classify data from all other languages.Setting LanguagesEnglish Arabic German Spanish French Italian Dutch Polish Pt-Br Roman.
Russian TurkishRaw Data NB 0.481 0.469 0.471 0.526 0.532 0.524 0.522 0.415 0.465 0.509 0.465 0.513Senna 0.400Polyglot 0.382 0.416 0.270 0.418 0.361 0.332 0.228 0.323 0.194 0.300 0.402 0.295single SettingDOC/ADD 0.462 0.422 0.429 0.394 0.481 0.458 0.252 0.385 0.363 0.431 0.471 0.435DOC/BI 0.474 0.432 0.362 0.336 0.444 0.469 0.197 0.414 0.395 0.445 0.436 0.428joint SettingDOC/ADD 0.475 0.371 0.386 0.472 0.451 0.398 0.439 0.304 0.394 0.453 0.402 0.441DOC/BI 0.378 0.329 0.358 0.472 0.454 0.399 0.409 0.340 0.431 0.379 0.395 0.435Table 4: F1-scores on the TED corpus document classification task when training and evaluating on thesame language.
Baseline embeddings are Senna (Collobert et al, 2011) and Polyglot (Al-Rfou?
et al,2013).63100 200 500 1000 500010k607080Training Documents (de)ClassificationAccuracy(%)100 200 500 1000 500010k5060708090Training Documents (en)ADD+ BI+ I-Matrix MT GlossedFigure 3: Classification accuracy for a number of models (see Table 1 for model descriptions).
The left chart shows results forthese models when trained on German data and evaluated on English data, the right chart vice versa.on the German training data and evaluated on thetranslated Arabic.
While we developed this systemas a baseline, it must be noted that the classifier ofthis system has access to significantly more infor-mation (all words in the document) as opposed toour models (one embedding per document), andwe do not expect to necessarily beat this system.The results of this experiment are in Table 2.When comparing the results between the ADDmodel and the models trained using the document-level error signal, the benefit of this additional sig-nal becomes clear.
The joint training mode leadsto a relative improvement when training on En-glish data and evaluating in a second language.This suggests that the joint mode improves thequality of the English embeddings more than itaffects the L2-embeddings.
More surprising, per-haps, is the relative performance between the ADDand BI composition functions, especially whencompared to the results in ?5.2, where the BI mod-els relatively consistently performed better.
Wesuspect that the better performance of the additivecomposition function on this task is related to thesmaller amount of training data available whichcould cause sparsity issues for the bigram model.As expected, the MT system slightly outper-forms our models on most language pairs.
How-ever, the overall performance of the models iscomparable to that of the MT system.
Consider-ing the relative amount of information availableduring the classifier training phase, this indicatesthat our learned representations are semanticallyuseful, capturing almost the same amount of infor-mation as available to the Na?
?ve Bayes classifier.We next investigate linguistic transfer acrosslanguages.
We re-use the embeddings learnedwith the DOC/ADD joint model from the previ-ous experiment for this purpose, and train clas-sifiers on all non-English languages using thoseembeddings.
Subsequently, we evaluate their per-formance in classifying documents in the remain-ing languages.
Results for this task are in Table 3.While the results across language-pairs might notbe very insightful, the overall good performancecompared with the results in Table 2 implies thatwe learnt semantically meaningful vectors and infact a joint embedding space across thirteen lan-guages.In a third evaluation (Table 4), we apply the em-beddings learnt with out models to a monolingualclassification task, enabling us to compare withprior work on distributed representation learning.In this experiment a classifier is trained in one lan-guage and then evaluated in the same.
We againuse a Na?
?ve Bayes classifier on the raw data to es-tablish a reasonable upper bound.We compare our embeddings with the SENNAembeddings, which achieve state of the art per-formance on a number of tasks (Collobert et al,2011).
Additionally, we use the Polyglot embed-dings of Al-Rfou?
et al (2013), who publishedword embeddings across 100 languages, includingall languages considered in this paper.
We repre-sent each document by the mean of its word vec-tors and then apply the same classifier training andtesting regime as with our models.
Even thoughboth of these sets of embeddings were trained onmuch larger datasets than ours, our models outper-form these baselines on all languages?even out-performing the Na?
?ve Bayes system on on several64Figure 4: t-SNE projections for a number of English, Frenchand German words as represented by the BI+ model.
Eventhough the model did not use any parallel French-Germandata during training, it learns semantic similarity betweenthese two languages using English as a pivot, and semanti-cally clusters words across all languages.Figure 5: t-SNE projections for a number of short phrases inthree languages as represented by the BI+ model.
The pro-jection demonstrates linguistic transfer through a pivot by.
Itseparates phrases by gender (red for female, blue for male,and green for neutral) and aligns matching phrases across lan-guages.languages.
While this may partly be attributed tothe fact that our vectors were learned on in-domaindata, this is still a very positive outcome.5.4 Linguistic AnalysisWhile the classification experiments focused onestablishing the semantic content of the sentencelevel representations, we also want to briefly in-vestigate the induced word embeddings.
We usethe BI+ model trained on the Europarl corpus forthis purpose.
Figure 4 shows the t-SNE projec-tions for a number of English, French and Germanwords.
Even though the model did not use any par-allel French-German data during training, it stillmanaged to learn semantic word-word similarityacross these two languages.Going one step further, Figure 5 shows t-SNEprojections for a number of short phrases in thesethree languages.
We use the English the presi-dent and gender-specific expressions Mr Presidentand Madam President as well as gender-specificequivalents in French and German.
The projec-tion demonstrates a number of interesting results:First, the model correctly clusters the words intothree groups, corresponding to the three Englishforms and their associated translations.
Second, aseparation between genders can be observed, withmale forms on the bottom half of the chart and fe-male forms on the top, with the neutral the presi-dent in the vertical middle.
Finally, if we assumea horizontal line going through the president, thisline could be interpreted as a ?gender divide?, withmale and female versions of one expression mir-roring each other on that line.
In the case of thepresident and its translations, this effect becomeseven clearer, with the neutral English expressionbeing projected close to the mid-point betweeneach other language?s gender-specific versions.These results further support our hypothesis thatthe bilingual contrastive error function can learnsemantically plausible embeddings and further-more, that it can abstract away from mono-lingualsurface realisations into a shared semantic spaceacross languages.6 Related WorkDistributed Representations Distributed repre-sentations can be learned through a number of ap-proaches.
In their simplest form, distributional in-formation from large corpora can be used to learnembeddings, where the words appearing within acertain window of the target word are used to com-pute that word?s embedding.
This is related totopic-modelling techniques such as LSA (Dumaiset al, 1988), LSI, and LDA (Blei et al, 2003), butthese methods use a document-level context, andtend to capture the topics a word is used in ratherthan its more immediate syntactic context.Neural language models are another popular ap-proach for inducing distributed word representa-tions (Bengio et al, 2003).
They have received alot of attention in recent years (Collobert and We-ston, 2008; Mnih and Hinton, 2009; Mikolov etal., 2010, inter alia) and have achieved state of theart performance in language modelling.
Collobertet al (2011) further popularised using neural net-work architectures for learning word embeddingsfrom large amounts of largely unlabelled data byshowing the embeddings can then be used to im-prove standard supervised tasks.65Unsupervised word representations can easilybe plugged into a variety of NLP related tasks.Tasks, where the use of distributed representationshas resulted in improvements include topic mod-elling (Blei et al, 2003) or named entity recogni-tion (Turian et al, 2010; Collobert et al, 2011).Compositional Vector Models For a number ofimportant problems, semantic representations ofindividual words do not suffice, but instead a se-mantic representation of a larger structure?e.g.
aphrase or a sentence?is required.
Self-evidently,sparsity prevents the learning of such representa-tions using the same collocational methods as ap-plied to the word level.
Most literature instead fo-cuses on learning composition functions that rep-resent the semantics of a larger structure as a func-tion of the representations of its parts.Very simple composition functions have beenshown to suffice for tasks such as judging bi-gram semantic similarity (Mitchell and Lapata,2008).
More complex composition functions us-ing matrix-vector composition, convolutional neu-ral networks or tensor composition have proveduseful in tasks such as sentiment analysis (Socheret al, 2011; Hermann and Blunsom, 2013), rela-tional similarity (Turney, 2012) or dialogue analy-sis (Kalchbrenner and Blunsom, 2013).Multilingual Representation Learning Mostresearch on distributed representation inductionhas focused on single languages.
English, with itslarge number of annotated resources, has enjoyedmost attention.
However, there exists a corpus ofprior work on learning multilingual embeddingsor on using parallel data to transfer linguistic in-formation across languages.
One has to differen-tiate between approaches such as Al-Rfou?
et al(2013), that learn embeddings across a large va-riety of languages and models such as ours, thatlearn joint embeddings, that is a projection into ashared semantic space across multiple languages.Related to our work, Yih et al (2011) proposedS2Nets to learn joint embeddings of tf-idf vectorsfor comparable documents.
Their architecture op-timises the cosine similarity of documents, usingrelative semantic similarity scores during learn-ing.
More recently, Lauly et al (2013) proposed abag-of-words autoencoder model, where the bag-of-words representation in one language is used totrain the embeddings in another.
By placing theirvocabulary in a binary branching tree, the prob-abilistic setup of this model is similar to that ofMnih and Hinton (2009).
Similarly, Sarath Chan-dar et al (2013) train a cross-lingual encoder,where an autoencoder is used to recreate words intwo languages in parallel.
This is effectively thelinguistic extension of Ngiam et al (2011), whoused a similar method for audio and video data.Hermann and Blunsom (2014) propose a large-margin learner for multilingual word representa-tions, similar to the basic additive model proposedhere, which, like the approaches above, relies on abag-of-words model for sentence representations.Klementiev et al (2012), our baseline in ?5.2,use a form of multi-agent learning on word-aligned parallel data to transfer embeddings fromone language to another.
Earlier work, Haghighiet al (2008), proposed a method for inducingbilingual lexica using monolingual feature repre-sentations and a small initial lexicon to bootstrapwith.
This approach has recently been extendedby Mikolov et al (2013a), Mikolov et al (2013b),who developed a method for learning transforma-tion matrices to convert semantic vectors of onelanguage into those of another.
Is was demon-strated that this approach can be applied to im-prove tasks related to machine translation.
TheirCBOW model is also worth noting for its sim-ilarities to the ADD composition function usedhere.
Using a slightly different approach, Zou etal.
(2013), also learned bilingual embeddings formachine translation.7 ConclusionTo summarize, we have presented a novel methodfor learning multilingual word embeddings usingparallel data in conjunction with a multilingual ob-jective function for compositional vector models.This approach extends the distributional hypoth-esis to multilingual joint-space representations.Coupled with very simple composition functions,vectors learned with this method outperform thestate of the art on the task of cross-lingual docu-ment classification.
Further experiments and anal-ysis support our hypothesis that bilingual signalsare a useful tool for learning distributed represen-tations by enabling models to abstract away frommono-lingual surface realisations into a deeper se-mantic space.AcknowledgementsThis work was supported by a Xerox FoundationAward and EPSRC grant number EP/K036580/1.66ReferencesR.
Al-Rfou?, B. Perozzi, and S. Skiena.
2013.
Poly-glot: Distributed word representations for multilin-gual nlp.
In Proceedings of CoNLL.M.
Baroni and R. Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP.Y.
Bengio, R. Ducharme, P. Vincent, and C. Janvin.2003.
A neural probabilistic language model.
Jour-nal of Machine Learning Research, 3:1137?1155,March.D.
M. Blei, A. Y. Ng, and M. I. Jordan.
2003.
Latentdirichlet alocation.
Journal of Machine LearningResearch, 3:993?1022.P.
Bloom.
2001.
Precis of how children learn themeanings of words.
Behavioral and Brain Sciences,24:1095?1103.M.
Cettolo, C. Girardi, and M. Federico.
2012.
Wit3:Web inventory of transcribed and translated talks.
InProceedings of EAMT.S.
Clark and S. Pulman.
2007.
Combining symbolicand distributional models of meaning.
In Proceed-ings of AAAI Spring Symposium on Quantum Inter-action.
AAAI Press.T.
Cohn and M. Lapata.
2007.
Machine translation bytriangulation: Making effective use of multi-parallelcorpora.
In Proceedings of ACL.M.
Collins.
2002.
Discriminative training methodsfor hidden markov models: Theory and experimentswith perceptron algorithms.
In Proceedings of ACL-EMNLP.R.
Collobert and J. Weston.
2008.
A unified architec-ture for natural language processing: Deep neuralnetworks with multitask learning.
In Proceedings ofICML.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2493?2537.J.
Duchi, E. Hazan, and Y.
Singer.
2011.
Adaptive sub-gradient methods for online learning and stochas-tic optimization.
Journal of Machine Learning Re-search, 12:2121?2159, July.S.
T. Dumais, G. W. Furnas, T. K. Landauer, S. Deer-wester, and R. Harshman.
1988.
Using latent se-mantic analysis to improve access to textual infor-mation.
In Proceedings of the SIGCHI Conferenceon Human Factors in Computing Systems.C.
Dyer, A. Lopez, J. Ganitkevitch, J. Weese,F.
Ture, P. Blunsom, H. Setiawan, V. Eidelman, andP.
Resnik.
2010. cdec: A Decoder, Alignment, andLearning framework for finite-state and context-freetranslation models.
In Proceedings of ACL.K.
Erk and S. Pad?o.
2008.
A structured vector spacemodel for word meaning in context.
Proceedings ofEMNLP.J.
R. Firth.
1957.
A synopsis of linguistic theory 1930-55.
1952-59:1?32.E.
Grefenstette and M. Sadrzadeh.
2011.
Experi-mental support for a categorical compositional dis-tributional model of meaning.
In Proceedings ofEMNLP.A.
Haghighi, P. Liang, T. Berg-Kirkpatrick, andD.
Klein.
2008.
Learning bilingual lexicons frommonolingual corpora.
In Proceedings of ACL-HLT.K.
M. Hermann and P. Blunsom.
2013.
The Role ofSyntax in Vector Space Models of CompositionalSemantics.
In Proceedings of ACL.K.
M. Hermann and P. Blunsom.
2014.
MultilingualDistributed Representations without Word Align-ment.
In Proceedings of ICLR.N.
Kalchbrenner and P. Blunsom.
2013.
Recurrentconvolutional neural networks for discourse compo-sitionality.
Proceedings of the ACL Workshop onContinuous Vector Space Models and their Compo-sitionality.A.
Klementiev, I. Titov, and B. Bhattarai.
2012.
In-ducing crosslingual distributed representations ofwords.
In Proceedings of COLING.P.
Koehn.
2005.
Europarl: A Parallel Corpus for Sta-tistical Machine Translation.
In Proceedings of theMachine Translation Summit.S.
Lauly, A. Boulanger, and H. Larochelle.
2013.Learning multilingual word representations using abag-of-words autoencoder.
In Deep Learning Work-shop at NIPS.D.
D. Lewis, Y. Yang, T. G. Rose, and F. Li.
2004.Rcv1: A new benchmark collection for text catego-rization research.
Journal of Machine Learning Re-search, 5:361?397, December.A.
K. McCallum.
2002.
Mallet: A machine learningfor language toolkit.
http://mallet.cs.umass.edu.T.
Mikolov, M. Karafi?at, L. Burget, J.?Cernock?y, andS.
Khudanpur.
2010.
Recurrent neural networkbased language model.
In Proceedings of INTER-SPEECH.T.
Mikolov, K. Chen, G. Corrado, and J.
Dean.
2013a.Efficient Estimation of Word Representations inVector Space.
CoRR.T.
Mikolov, Q. V. Le, and I. Sutskever.
2013b.
Ex-ploiting Similarities among Languages for MachineTranslation.
CoRR.J.
Mitchell and M. Lapata.
2008.
Vector-based modelsof semantic composition.
In In Proceedings of ACL.67A.
Mnih and G. Hinton.
2009.
A scalable hierarchi-cal distributed language model.
In Proceedings ofNIPS.J.
Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, andA.
Y. Ng.
2011.
Multimodal deep learning.
InICML.D.
Roy.
2003.
Grounded spoken language acquisition:Experiments in word learning.
IEEE Transactionson Multimedia, 5(2):197?209, June.A.
P. Sarath Chandar, M. K. Mitesh, B. Ravindran,V.
Raykar, and A. Saha.
2013.
Multilingual deeplearning.
In Deep Learning Workshop at NIPS.R.
Socher, J. Pennington, E. H. Huang, A. Y. Ng, andC.
D. Manning.
2011.
Semi-supervised recursiveautoencoders for predicting sentiment distributions.In Proceedings of EMNLP.R.
Socher, B. Huval, C. D. Manning, and A. Y. Ng.2012.
Semantic compositionality through recursivematrix-vector spaces.
In Proceedings of EMNLP-CoNLL, pages 1201?1211.N.
Srivastava and R. Salakhutdinov.
2012.
Multimodallearning with deep boltzmann machines.
In Pro-ceedings of NIPS.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: a simple and general method for semi-supervised learning.
In Proceedings of ACL.P.
D. Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.Journal of Artificial Intelligence Research, 44:533?585.W.-T. Yih, K. Toutanova, J. C. Platt, and C. Meek.2011.
Learning discriminative projections for textsimilarity measures.
In Proceedings of CoNLL.W.
Y. Zou, R. Socher, D. Cer, and C. D. Manning.2013.
Bilingual word embeddings for phrase-basedmachine translation.
In Proceedings of EMNLP.68
