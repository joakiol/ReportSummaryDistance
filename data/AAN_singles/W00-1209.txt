The Research of Word Sense Disambiguation Method Based onCo-occurrence Frequency of Hownet*Erhong Yang, Guoqing Zhang, and Yongkui ZhangDept of Computer Science, Shanxi University,TaiYuan 030006 !
P. R. ChinaEmail: zyk@sxu.edu.cnAbstractWord sense disambiguafion (WSD) is a difficultproblem in natural language processing.
In thispaper, a sememe co-occurrence frequencybased WSD method was introduced.
In thismethod, Hownet was used as our informationsource , and a co-occurrence frequencydatabase of sememes was constructed and thenused for WSD.
The experimental result showedthat this method is successful.Keywordsword sense disambiguation, Hownet, sememe,co-occurrence1.
IntroductionWord sense disarnbiguafion (WSD) is one of?
the most difficult problems in NLP.
It is helpfuland in some instances required for suchapplications as machine translation, informationretrieval, content and thematic analysis,hypertext navigation and so on.
The problem ofWSD was first put forward in 1949.
And thenin the following decades researchers adoptedmany methods to solve the problem ofautomatic word sense disambiguation,including:l) AI-based method, 2) knowledge-based method and 3) corpus-based method.
01Although some useful results have been got, theproblem of word sense disambiguation is farfrom being solved.The difficult of WSD is as follow: 1)Evaluation of word sense disambiguationsystems is not yet standardized.
2) The potentialfor WSD varies by task.
3) Adequately largesense-tagged data sets are difficult to obtain.
4)The field has narrowed own approaches, butonly a little.
\[21In this paper, we use a statistical based methodto solve the problem of automatic word sensedisambiguafion.
\[31 In this method, a newknowledge base- .
.
.
.
.
Hownet t4'5\] was use asknowledge resources.
And instead of words, thesememes which are defined in Hownet wereused to get the statistical figure.
By doing this,the problem of data sparseness was solved to alarge degree.2.
A Brief Introduction Of HownetHownet is a knowledge base which wasreleased recently on Intemet.
In Hownet, theconcept which were represented by Chinese orEnglish words were described and the relationsbetween concepts and the attributes of conceptswere revealed.
In this paper, we use Chineseknowledge base, which is an important p.art ofHownet, as the resource of our disambiguafion.The format of this file is as follow:W_X =wordE_X = some examples of this wordG X= the pos of this wordDEF= the definition of this word"This research project is supported by a grant from Shanxi Natural Science Foundation fChina60A important concept used in Hownet thatwe must introduce is sememe.
In Hownet,sememes refer to some basic unit of senses.They are used to descnbe all the entries inHownet and there are more than 1,500 sememeall together.3.
Sense Co-occurrence FrequencyDatabaseIt is well known that some words tend toco-occur frequently with some words than withothers\[6\].
Similarly, some meaning of wordstend to co-occur more often with some meaningof words than with others.
If we can got therelations of word meanings quantitatively, itwould have some help on word sensedisambiguafion.
In Hownet, all words aredefined with limited sememes and thecombination of sememes i fixed.
If we makestatistic on the co-occurrence frequency ofsememe so as to reflect the co-occurrence ofwords, the problem of data sparseness would besolved to a large degree.
Based on the abovethought, we built a sense co-occurrencefrequency database to disambiguate wordsenses.3.1 The Preproeessing Of HownetThe Hownet we downloaded from Intemet is inthe form of plain text.
It is not convenient forcomputer to use and it must been converted intoa database.
In the database, ach lexical entry isconverted into a record.
The formalizationdescription of the records is as follow:<lexical entry> ::= <NO.><morphology><part-of-speech><definifion>Where NO.
is the corresponding umber ofthis lexical entry in Hownet.
And the definitionis composed of several sememes (short for SU)which were divided by comma.
In addition, wehave deleted the Engfish sememees in order tosaving space and speeding up the processing.Here are some examples after preprocessing:NO.
Morphology21424 t~tSb18888188891888718890Part-of-speechADJADJ  ,VVNdefinitionI~ ,~,~,~3.2 The Creation Of Sememe Co-occurrenceFrequency DatabaseThe sememe co-occurrence frequency databaseis the basic of sense disambiguafion.
Now wewill introduce it briefly.The sememe co-occurrence frequencydatabase is a table of two dimension.
Each itemcorresponding to the co-occurrence frequencyof a pair of sememes.Before introducing the sememeco-occurrence frequency database, we gave thefollowing definition:Definiton: suppose word W has m senseitems in hownet, and the correspondingdefinition of  each sense item is: Yn, Y\]2, .... Y1(,1);Y21, Y22, .... Y2(,a); ...; Ym\],Ym2, .... Y~,~>respectively.
We call \[Yu,Y~ .... Yioadasememe set of  W(short for  SS), and call \[{ ymYI2, .... Yl(,a)},{ Y21, Y22, .... Y2(,a)}, ....\[ Yml.Ym2, .... y.c~m)}}the s meme xpansion ofW (short for  SE).For example, in the above mentionedexample, the word "~f l ' "  has only one senseitem.
The corresponding sememe set of this61sense item is {\]~'\]~i,~.l .
l : ,~,~} andthesememe xpansion of "~1"" is {()~'l~i,~.1.1:,@, ~} } .
The word "~" has four senseitems, and the corresponding sememe set ofeach item is { )~ i~,~,~,~},  {~.~-}, {~} and { ~3~,,'~ } respectively.
The sememeexpansion of word "~"  is {{)~'l~,~ff~;~,~}, {~:} ,  { :~},  {?,,~,:~}}.When building the sememe co-occurrencefrequency database, the corpus is segmentedfirst and each word is tagged with its sememeexpansion in Hownet.
Then for each uniquepair of words co-occurred in a sentence (here asentence is a string of characters delimited bypunctuations.
), the co-occurrence data ofsememes which belong to the definition of eachwords respectively were collect, whencoUecting co0occurrence data, we adopt aprinciple that every pair of word whichco-occurred in a sentence should have equalcontribution to the sememe co-occurrence dataregardless of the number of sense items of thisword and the length of the definition.
Moreover,the contribution of a word should be evenlydistributed between all the senses of a word andthe contribution of a sense should been evenlydistributed between all the sememe in a sense.The algorithm is as follow:1.Initial each cell in the  sememeco-occurrence frequency database(short forSCFD) with 0.2.For each sentence S in training corpus, do3-7.3.For each word in sentense S, tag thesememe xpansion to it.4.For each unique pair of sememeexpansion (SEi, SEj), do 5-7.5.For each sememe SUimp in each sememeset SSim in S~, do 6-7.6.For each sememe SUjm in each sememeset SSj, in SEj, do 7.7.Increase the value of cell SCFD(SUimp,SUjnq) and SCFD(SUjnq,SUimp) by the productof w(SUimp) and w(SUj~), where w(SUxyO isweight of SUxyz given by1 w(su >--ls ,l?lss lIt can be concluded from the abovealgorithm that the SCFD are symmetrical.
Inorder to saving space and speeding up theprocessing, we only save those cells (SUi,SUj)that satisfying SUi~<SUj.3.3 The Sememe Co-occurrence FrequencyDatabase Based Disambiguafion Method3.3.1 The Sememe Co-occurrence FrequencyBased Scoring MethodWhen disambiguate a polysemous word, wegiven the following equation as the score of asense item of the polysemous word and thecontext containing this polysemous word.
Thecontext of the word is the sentence containingthis word.score(S, C)(1) = score(SS, C') - score(SS, GlobalSS)Where S is a sense item of polysemouseword W, C is the context containing W, SS isthe corresponding sememe set of S, C' is the setof sememe expansion of words in C andGlobalSS is the sememe set that containing allof the sememe defined in Hownet.score(SS, C') = vsE~c~ SC?re(SS'SE')/lC' I (2)for any sememe set SS and sememeexpansion set C'.score(SS, SE') = max score(SS, SS') (3)SS" eSE'for any sememe set SS and sememeexpansion SE'.score(SS, SS') = vsuX ss.
(4)for any sememe set SS and SS'.62score(SS,SU')=vsu~JsCOre(SU,SU') / S I(5)for any sememe set SS and sememe SU'.score( SU , SU') = I ( SU, SU') (6)for any sememe SU and SU'.I (SU, SU') = log 2 f (SU,SU').
N ~g(SU), g(SU') (7)Where f(SU,SU') is the co-occurrencefrequency corresponding to sememe pair (SU,SU' ) in SCFD.
And for g(SU) and N, we havethe following equation:g(SU) = ~f(SU,SU')  (8)vsu'N= vsu~ f ( SU' SU') (9)In equation (7), the mutual-information-like measure deviated from the stardardmutual-information measure by multiple a extramultiplicative factor N, this is because that thescale of the corpus is not large enough that themutual-information f some sememes pairswould be negtive if it was not normalized by aextra multiplicative factor N. In equation (9),the sum of f(SU, SU') was divided by 2, this isbecause for each pair of sememes,~ f (SU, SU') is increaseby2.VSU,VSU"When disambiguation, we tag the sememeT that satisfying the following equation topolysemous word W.T = arg max score(S, C) (10)s3.3.2 The Creation Of Mutual InformationDatabaseWe have created a mutual information databaseaccording to (7),(8) and(9) Here is someexamples:The examples in table 1 have a high mutualinformation.
The sememe pairs in this tablehave certain semantic relations.
While theexamples in table 2 have a low mutualinformation.
And the sememe pairs in this tablehave no patency semantic relations.Table 1 example of sememe pairs which have a high mutual informationSememe 1 Sememe 2 Mutual-Inf?rml Sememe 1 Sememe 2 Mutual-Informaation \[ t ion~~.
,~-~ 33.811057 :~'I~ :~'~ 27.418417~ ~gk: 29.441937 ~ ~ 27.2346305~ ~ 28.024560 ~ ~: 27.093292,~Ir~ 28.023521 'I~,~:~j ~ ~ 26.984521~ ~ 27.571478 {~i ~ 26.710478Table 2: example of sememe pairs which have a low mutual informationMutual-Inform Mutual-Inform Sememe 1 Sememe 2 Sememe I Sememe 2 ation %ion"~r~n i~ 8.693242 ~J(~ ~g 9.171023.~=t ~ 8.754611 ~ ~ 9.357734:~Z \[\] 8.793914 ~\[~\]~ IS  9.448947~'~ ~3~ 9.121846 ~-~\]~ ~-~ 9.528801}~\]~J 9.150412 ,~ ~.
9.599495It can been concluded from ruble 1 and table 2 that the mutual information can reflect63the tightness of semantic relations.4.
Experiment And AnalysisWe did the experiment on a corpus of 10,000characters from People's Dialy.Firstly, the corpus is segmented, and thenthe sememe co-occurrence frequecny databaseand mutual information database is created.
Inthe mutual-informationdatabase, th re is709,496 data items corresponding to differentsememes pairs.
In order to speeding up theprocessing, the mutual-information databasewas sorted and indexed according to the firsttwo bytes of each sememe pair.
At last theexperiment of disambiguation of somepolysemous words was done.
Here is twoexamples:Example 1:Example 2 :~1:~1~1~1~"~1~11~1?
1--1 IWe use the following euqation to access theaccuracy ratio of disambiguafion:accuracy ratio = the number of correctlytagged xample~the total number of examplesin testing se;(11)the experimental result is shown in table 4.Tab~3: Two examples that disambiguate using sememe co-occurrence frequency databaseThe definition ofword " ~ "The score of sense items andthe context of word "~"  inexample 1The score of sense items andthe context of word "~"  inexample 2~3~'-~ 14.
459068 8.
659968Z~'F~ 9.
817648 i0.
817648M'I~ -~ 7.
415986 12.
415986~ ~ -0.
134779 -0.
134779i,~3~: :k~/..W... ~ji,~" ~.i..~$.~ -0.
818518 -0.
818518~9k:~ 14.
459068 12.
415986Table 4: the experiment resultTotal number of testing The number of correctly Accurracyexamples tagged examples rat ioClose test I00 75 75%Open test I00 71 71%The disambiguation method introducedabove have the following charatristics:(1) The problem of data spraseness isolved in a large degree.
(2) This disambiguation method avoidsthe laborious hand tagging of training corpus.
(3) This method can been easily appliedto other kind of corpus.Reference\[1\].
Nancy Ide, Jean Veronis, Introduction tothe Special Issue on Word SenseDisambiguation: The State of the Art,Computational Linguistics, 1998, Volume24, number 1, pp 1-40\[2\].
Philip Resnik, David Yarowsky, APerspective on Word SenseDisambiguation Methods and their64Evaluation,http://www.cs.jhu.edu/~yarowsky/pubs.html\[3\].
Alpha K. Luk, Statistical SenseDisambiguation with Relatively SmallCorpus Using Dictionary Definitions, 33rdAnnual Meeting of the Association forComputational Linguistics,26-30 June,1995, Massachusetts Institute ofTechnology, Cambridge, Massachusetts.USA, pp.181-188\[411 -~'~3}~, i .~?~j~3~j2~II~I l j ,R~j~f~,~,~3~/~ff\], 1998 ~lz~ 3 ~, ,~,~ 27 ~,pp.76-82\[5\].
~ ,  ~11~, http://www.how-net.com.\[6\].
Kenneth Ward Church, Word AssociationNorms, Mutual Information, andLexicography, Computational Linguistics,1990,Volume 16, Number 1, pp.22-2965
