Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1090?1099,Honolulu, October 2008. c?2008 Association for Computational LinguisticsSoft-Supervised Learning for Text ClassificationAmarnag Subramanya & Jeff BilmesDept.
of Electrical Engineering,University of Washington, Seattle, WA 98195, USA.
{asubram,bilmes}@ee.washington.eduAbstractWe propose a new graph-based semi-supervised learning (SSL) algorithm anddemonstrate its application to documentcategorization.
Each document is representedby a vertex within a weighted undirectedgraph and our proposed framework minimizesthe weighted Kullback-Leibler divergencebetween distributions that encode the classmembership probabilities of each vertex.
Theproposed objective is convex with guaranteedconvergence using an alternating minimiza-tion procedure.
Further, it generalizes ina straightforward manner to multi-classproblems.
We present results on two stan-dard tasks, namely Reuters-21578 andWebKB, showing that the proposed algorithmsignificantly outperforms the state-of-the-art.1 IntroductionSemi-supervised learning (SSL) employs smallamounts of labeled data with relatively largeamounts of unlabeled data to train classifiers.
Inmany problems, such as speech recognition, doc-ument classification, and sentiment recognition,annotating training data is both time-consumingand tedious, while unlabeled data are easily ob-tained thus making these problems useful appli-cations of SSL.
Classic examples of SSL algo-rithms include self-training (Yarowsky, 1995) andco-training (Blum and Mitchell, 1998).
Graph-based SSL algorithms are an important class of SSLtechniques that have attracted much of attention oflate (Blum and Chawla, 2001; Zhu et al, 2003).Here one assumes that the data (both labeled andunlabeled) is embedded within a low-dimensionalmanifold expressed by a graph.
In other words,each data sample is represented by a vertex withina weighted graph with the weights providing a mea-sure of similarity between vertices.Most graph-based SSL algorithms fall under oneof two categories ?
those that use the graph structureto spread labels from labeled to unlabeled samples(Szummer and Jaakkola, 2001; Zhu and Ghahra-mani, 2002) and those that optimize a loss functionbased on smoothness constraints derived from thegraph (Blum and Chawla, 2001; Zhu et al, 2003;Joachims, 2003; Belkin et al, 2005).
Sometimes thetwo categories are similar in that they can be shownto optimize the same underlying objective (Zhu andGhahramani, 2002; Zhu et al, 2003).
In generalgraph-based SSL algorithms are non-parametric andtransductive.1 A learning algorithm is said to betransductive if it is expected to work only on a closeddata set, where a test set is revealed at the time oftraining.
In practice, however, transductive learnerscan be modified to handle unseen data (Zhu, 2005a;Sindhwani et al, 2005).
A common drawback ofmany graph-based SSL algorithms (e.g.
(Blum andChawla, 2001; Joachims, 2003; Belkin et al, 2005))is that they assume binary classification tasks andthus require the use of sub-optimal (and often com-putationally expensive) approaches such as one vs.rest to solve multi-class problems, let alne struc-tured domains such as strings and trees.
There arealso issues related to degenerate solutions (all un-labeled samples classified as belonging to a single1Excluding Manifold Regularization (Belkin et al, 2005).1090class) (Blum and Chawla, 2001; Joachims, 2003;Zhu and Ghahramani, 2002).
For more backgroundon graph-based and general SSL and their applica-tions, see (Zhu, 2005a; Chapelle et al, 2007; Blitzerand Zhu, 2008).In this paper we propose a new algorithm forgraph-based SSL and use the task of text classifica-tion to demonstrate its benefits over the current state-of-the-art.
Text classification involves automaticallyassigning a given document to a fixed number of se-mantic categories.
Each document may belong toone, many, or none of the categories.
In general,text classification is a multi-class problem (morethan 2 categories).
Training fully-supervised textclassifiers requires large amounts of labeled datawhose annotation can be expensive (Dumais et al,1998).
As a result there has been interest is us-ing SSL techniques for text classification (Joachims,1999; Joachims, 2003).
However past work in semi-supervised text classification has relied primarily onone vs. rest approaches to overcome the inherentmulti-class nature of this problem.
We believe suchan approach may be sub-optimal because, disregard-ing data overlap, the different classifiers have train-ing procedures that are independent of one other.In order to address the above drawback we pro-pose a new framework based on optimizing a lossfunction composed of Kullback-Leibler divergence(KL-divergence) (Cover and Thomas, 1991) termsbetween probability distributions defined for eachgraph vertex.
The use of probability distributions,rather than fixed integer labels, not only leads to astraightforward multi-class generalization, but alsoallows us to exploit other well-defined functions ofdistributions, such as entropy, to improve systemperformance and to allow for the measure of uncer-tainty.
For example, with a single integer, at most allwe know is its assignment.
With a distribution, wecan continuously move from knowing an assignmentwith certainty (i.e., an entropy of zero) to expres-sions of doubt or multiple valid possibilities (i.e., anentropy greater than zero).
This is particularly use-ful for document classification as we will see.
Wealso show how one can use the alternating minimiza-tion (Csiszar and Tusnady, 1984) algorithm to op-timize our objective leading to a relatively simple,fast, easy-to-implement, guaranteed to converge, it-erative, and closed form update for each iteration.2 Proposed Graph-Based LearningFrameworkWe consider the transductive learning problem, i.e.,given a training setD = {Dl,Du}, whereDl andDuare the sets of labeled and unlabeled samples respec-tively, the task is to infer the labels for the samplesin Du.
In other words, Du is the ?test-set.?
HereDl = {(xi, yi)}li=1, Du = {xi}l+ui=l+1, xi ?
X (theinput space of the classifier, and corresponds to vec-tors of features) and yi ?
Y (the space of classifieroutputs, and for our case is the space of non-negativeintegers).
Thus |Y| = 2 yields binary classifica-tion while |Y| > 2 yields multi-class.
We definen = l + u, the total number of samples in the train-ing set.
Given D, most graph-based SSL algorithmsutilize an undirected weighted graph G = (V,E)where V = {1, .
.
.
, n} are the data points in Dand E = V ?
V are the set of undirected edgesbetween vertices.
We use wij ?
W to denote theweight of the edge between vertices i and j. W isreferred to as the weight (or affinity) matrix of G.As will be seen shortly, the input features xi effectthe final classification results via W, i.e., the graph.Thus graph construction is crucial to the success ofany graph-based SSL algorithm.
Graph construction?is more of an art, than science?
(Zhu, 2005b) andis an active research area (Alexandrescu and Kirch-hoff, 2007).
In general the weights are formed aswij = sim(xi,xj)?
(j ?
K(i)).
Here K(i) is the setof i?s k-nearest-neighbors (KNN), sim(xi,xj) is agiven measure of similarity between xi and xj , and?
(c) returns a 1 if c is true and 0 otherwise.
Gettingthe similarity measure right is crucial for the successof any SSL algorithm as that is what determines thegraph.
Note that setting K(i) = |V | = n resultsin a fully-connected graph.
Some popular similaritymeasures includesim(xi,xj) = e?
?xi?xj?22?2 orsim(xi,xj) = cos(xi,xj) =?xi,xj??
xi ?22?
xj ?22where ?
xi ?2 is the L2 norm, and ?xi,xj?
is theinner product of xi and xj .
The first similarity mea-sure is an RBF kernel applied on the squared Eu-clidean distance while the second is cosine similar-ity.
In this paper all graphs are constructed usingcosine similarity.1091We next introduce our proposed approach.
Forevery i ?
V , we define a probability distribution piover the elements of Y.
In addition let rj , j = 1 .
.
.
lbe another set of probability distributions again overthe elements of Y (recall, Y is the space of classi-fier outputs).
Here {rj}j represents the labels of thesupervised portion of the training data.
If the labelfor a given labeled data point consists only of a sin-gle integer, then the entropy of the corresponding rjis zero (the probability of that integer will be unity,with the remaining probabilities being zero).
If, onthe other hand, the ?label?
for a given labeled datapoint consists of a set of integers (e.g., if the objectis a member of multiple classes), then rj is able torepresent this property accordingly (see below).
Weemphasize again that both pi and rj are probabilitydistributions, with rj fixed throughout training.
Thegoal of learning in this paper is to find the best setof distributions pi, ?i that attempt to: 1) agree withthe labeled data rj wherever it is available; 2) agreewith each other (when they are close according to agraph); and 3) be smooth in some way.
These cri-teria are captured in the following new multi-classSSL optimization procedure:minpC1(p), where C1(p) =[l?i=1DKL(ri||pi)+?n?i?jwijDKL(pi||pj)?
?n?i=1H(pi)?
?,(1)and where p , (p1, .
.
.
, pn) denotes the en-tire set of distributions to be learned, H(pi) =?
?ypi(y) log pi(y) is the standard Shannon en-tropy function of pi, DKL(pi||qj) is the KL-divergence between pi and qj , and ?
and ?
are hy-perparameters whose selection we discuss in section5.
The distributions ri are derived from Dl (as men-tioned above) and this can be done in one of the fol-lowing ways: (a) if y?i is the single supervised labelfor input xi then ri(y) = ?
(y = y?i), which meansthat ri gives unity probability for y equaling the la-bel y?i; (b) if y?i = {y?
(1)i , .
.
.
, y?
(k)i }, k ?
|Y| is a setof possible outputs for input xi, meaning an objectvalidly falls into all of the corresponding categories,we set ri(y) = (1/k)?
(y ?
y?i) meaning that ri isuniform over only the possible categories and zerootherwise; (c) if the labels are somehow providedin the form of a set of non-negative scores, or evena probability distribution itself, we just set ri to beequal to those scores (possibly) normalized to be-come a valid probability distribution.
Among thesethree cases, case (b) is particularly relevant to textclassification as a given document many belong to(and in practice may be labeled as) many classes.The final classification results, i.e., the final labelsfor Du, are then given by y?
= argmaxy?Ypi(y).We next provide further intuition on our objectivefunction.
SSL on a graph consists of finding a la-belingDu that is consistent with both the labels pro-vided in Dl and the geometry of the data inducedby the graph.
The first term of C1will penalizethe solution pi i ?
{1, .
.
.
, l}, when it is far awayfrom the labeled training data Dl, but it does not in-sist that pi = ri, as allowing for deviations from rican help especially with noisy labels (Bengio et al,2007) or when the graph is extremely dense in cer-tain regions.
As explained above, our framework al-lows for the case where supervised training is uncer-tain or ambiguous.
We consider it reasonable to callour approach soft-supervised learning, generalizingthe notion of semi-supervised learning, since thereis even more of a continuum here between fully su-pervised and fully unsupervised learning than whattypically exists with SSL.
Soft-supervised learningallows uncertainty to be expressed (via a probabilitydistribution) about any of the labels individually.The second term of C1penalizes a lack of con-sistency with the geometry of the data and can beseen as a graph regularizer.
If wij is large, we prefera solution in which pi and pj are close in the KL-divergence sense.
While KL-divergence is asym-metric, given that G is undirected implies W is sym-metric (wij = wji) and as a result the second termis inherently symmetric.The last term encourages each pi to be close tothe uniform distribution if not preferred to the con-trary by the first two terms.
This acts as a guardagainst degenerate solutions commonly encounteredin SSL (Blum and Chawla, 2001; Joachims, 2003).For example, consider the case where part of thegraph is almost completely disconnected from anylabeled vertex (which is possible in the k-nearestneighbor case).
In such situations the third term en-1092sures that the nodes in this disconnected region areencouraged to yield a uniform distribution, validlyexpressing the fact that we do not know the labels ofthese nodes based on the nature of the graph.
Moregenerally, we conjecture that by maximizing the en-tropy of each pi, the classifier has a better chance ofproducing high entropy results in graph regions oflow confidence (e.g.
close to the decision boundaryand/or low density regions).
This overcomes a com-mon drawback of a large number of state-of-the-artclassifiers that tend to be confident even in regionsclose to the decision boundary.We conclude this section by summarizing some ofthe features of our proposed framework.
It shouldbe clear that C1uses the ?manifold assumption?for SSL (see chapter 2 in (Chapelle et al, 2007))?
it assumes that the input data can be embed-ded within a low-dimensional manifold (the graph).As the objective is defined in terms of probabilitydistributions over integers rather than just integers(or to real-valued relaxations of integers (Joachims,2003; Zhu et al, 2003)), the framework general-izes in a straightforward manner to multi-class prob-lems.
Further, all the parameters are estimatedjointly (compare to one vs. rest approaches whichinvolve solving |Y| independent problems).
Fur-thermore, the objective is capable of handling labeltraining data uncertainty (Pearl, 1990).
Of course,this objective would be useless if it wasn?t possibleto efficiently and easily optimize it on large data sets.We next describe a method that can do this.3 Learning with Alternating MinimizationAs long as ?, ?
?
0, the objective C1(p) is con-vex.
This follows since DKL(pi||pj) is convex inthe pair (pi, pj) (Cover and Thomas, 1991), nega-tive entropy is convex, and a positive-weighted lin-ear combination of a set of convex functions is con-vex.
Thus, the problem of minimizing C1over thespace of collections of probability distributions (aconvex set) constitutes a convex programming prob-lem (Bertsekas, 2004).
This property is extremelybeneficial since there is a unique global optimumand there are a variety of methods that can be usedto yield that global optimum.
One possible methodmight take the derivative of the objective along withLagrange multipliers to ensure that we stay withinthe space of probability distributions.
This methodcan sometimes yield a closed form single-step an-alytical expression for the globally optimum solu-tion.
Unfortunately, however, our problem does notadmit such a closed form solution because the gra-dient of C1(p) with respect to pi(y) is of the form,k1pi(y) log pi(y) + k2pi(y) + k3 (where k1, k2, k3are fixed constants).
Sometimes, optimizing the dualof the objective can also produce a solution, but un-fortunately again the dual of our objective also doesnot yield a closed form solution.
The typical nextstep, then, is to resort to iterative techniques suchas gradient descent along with modifications to en-sure that the solution stays within the set of proba-bility distributions (the gradient of C1alone will notnecessarily point in the direction where p is still avalid distribution) - one such modification is calledthe method of multipliers (MOM).
Another solu-tion would be to use computationally complex (andcomplicated) algorithms like interior point methods(IPM).
While all of the above methods (describedin detail in (Bertsekas, 2004)) are feasible ways tosolve our problem, they each have their own draw-backs.
Using MOM, for example, requires the care-ful tuning of a number of additional parameters suchas learning rates, growth factors, and so on.
IPM in-volves inverting a matrix of the order of the numberof variables and constraints during each iteration.We instead adopt a different strategy based on al-ternating minimization (Csiszar and Tusnady, 1984).This approach has a single additional optimizationparameter (contrasted with MOM), admits a closedform solution for each iteration not involving anymatrix inversion (contrasted with IPM), and yieldsguaranteed convergence to the global optimum.
Inorder to render our approach amenable to AM, how-ever, we relax our objective C1by defining a new(third) set of distributions for all training samples qi,i = 1, .
.
.
, n denoted collectively like the above us-ing the notation q , (q1, .
.
.
, qn).
We define a newobjective to be optimized as follows:minp,qC2(p, q), where C2(p, q) =[l?i=1DKL(ri||qi)+?n?i=1?j?N (i)w?ijDKL(pi||qj)?
?n?i=1H(pi)?
?.1093Before going further, the reader may be wonderingat this juncture how might it be desirable for us tohave apparently complicated the objective functionin an attempt to yield a more computationally andmethodologically superior machine learning proce-dure.
This is indeed the case as will be spelled outbelow.
First, in C2we have defined a new weightmatrix [W ?
]ij = w?ij of the same size as the originalwhere W ?
= W + ?In, where In is the n?
n iden-tity matrix, and where ?
?
0 is a non-negative con-stant (this is the optimization related parameter men-tioned above).
This has the effect that w?ii ?
wii.In the original objective C1, wii is irrelevant sinceDKL(p||p) = 0 for all p, but since there are now twodistributions for each training point, there should beencouragement for the two to approach each other.Like C1, the first term of C2ensures that the la-beled training data is respected and the last term isa smoothness regularizer, but these are done via dif-ferent sets of distributions, q and p respectively ?this choice is what makes possible the relatively sim-ple analytical update equations given below.
Next,we see that the two objective functions in fact haveidentical solutions when the optimization enforcesthe constraint that p and q are equal:min(p,q):p=qC2(p, q) = minpC1(p).Indeed, as ?
gets large, the solutions considered vi-able are those only where p = q.
We thus have that:lim??
?minp,qC2(p, q) = minpC1(p).Therefore, the two objectives should yield the samesolution as long as ?
?
wij for all i, j.
A key advan-tage of this relaxed objective is that it is amenable toalternating minimization, a method to produce a se-quence of sets of distributions (pn, qn) as follows:pn = argminpC2(p, qn?1), qn = argminqC2(pn, q).It can be shown (we omit the rather lengthy proofdue to space constraints) that the sequence gener-ated using the above minimizations converges to theminimum of C2(p, q), i.e.,limn?
?C2(p(n), q(n)) = infp,qC2(p, q),provided we start with a distribution that is initial-ized properly q(0)(y) > 0 ?
y ?
Y.
The updateequations for p(n) and q(n) are given byp(n)i (y) =1Ziexp?
(n?1)i (y)?i,q(n)i (y) =ri(y)?
(i ?
l) + ?
?j w?jip(n)j (y)?
(i ?
l) + ?
?j w?ji,where?i = ?
+ ?
?jw?ij ,?
(n?1)i (y) = ??
+ ?
?jw?ij(log q(n?1)j (y)?
1)and where Zi is a normalizing constant to ensure piis a valid probability distribution.
Note that each it-eration of the proposed framework has a closed formsolution and is relatively simple to implement, evenfor very large graphs.
Henceforth we refer to theproposed objective optimized using alternating min-imization as AM.4 Connections to Other ApproachesLabel propagation (LP) (Zhu and Ghahramani,2002) is a graph-based SSL algorithms that per-forms Markov random walks on the graph and hasa straightforward extension to multi-class problems.The update equations for LP (which also we use forour LP implementations) may be written asp(n)i (y) =ri(y)?
(i ?
l) + ?
(i > l)?j wijp(n?1)j (y)?
(i ?
l) + ?
(i > l)?j wijNote the similarity to the update equation for q(n)i inour AM case.
It has been shown that the squared-loss based SSL algorithm (Zhu et al, 2003) and LPhave similar updates (Bengio et al, 2007).The proposed objective C1is similar in spirit tothe squared-loss based objective in (Zhu et al, 2003;Bengio et al, 2007).
Our method, however, differsin that we are optimizing the KL-divergence overprobability distributions.
We show in section 5 thatKL-divergence based loss significantly outperformsthe squared-loss.
We believe that this could be due1094to the following: 1) squared loss is appropriate un-der a Gaussian loss model which may not be opti-mal under many circumstances (e.g.
classification);2) KL-divergence DKL(p||q) is based on a relative(relative to p) rather than an absolute error; and 3)under certain natural assumptions, KL-divergence isasymptotically consistent with respect to the under-lying probability distributions.AM is also similar to the spectral graph trans-ducer (Joachims, 2003) in that they both attemptto find labellings over the unlabeled data that re-spect the smoothness constraints of the graph.
Whilespectral graph transduction is an approximate solu-tion to a discrete optimization problem (which is NPhard), AM is an exact solution obtained by optimiz-ing a convex function over a continuous space.
Fur-ther, while spectral graph transduction assumes bi-nary classification problems, AM naturally extendsto multi-class situations without loss of convexity.Entropy Minimization (EnM) (Grandvalet andBengio, 2004) uses the entropy of the unlabeled dataas a regularizer while optimizing a parametric lossfunction defined over the labeled data.
While theobjectives in the case of both AM and EnM makeuse of the entropy of the unlabeled data, there areseveral important differences: (a) EnM is not graph-based, (b) EnM is parametric whereas our proposedapproach is non-parametric, and most importantly,(c) EnM attempts to minimize entropy while the pro-posed approach aims to maximize entropy.
Whilethis may seem a triviality, it has catastrophic conse-quences in terms of both the mathematics and mean-ing.
The objective in case of EnM is not convex,whereas in our case we have a convex formulationwith simple update equations and convergence guar-antees.
(Wang et al, 2008) is a graph-based SSL al-gorithm that also employs alternating minimiza-tion style optimization.
However, it is inherentlysquared-loss based which our proposed approachout-performs (see section 5).
Further, they do notprovide or state convergence guarantees and oneside of their update approximates an NP-completeoptimization procedure.The information regularization (IR) (Corduneanuand Jaakkola, 2003) algorithm also makes use ofa KL-divergence based loss for SSL.
Here the in-put space is divided into regions {Ri} which mightor might not overlap.
For a given point xi ?
Ri,IR attempts to minimize the KL-divergence betweenpi(yi|xi) and p?Ri(y), the agglomerative distributionfor region Ri.
Given a graph, one can define a re-gion to be a vertex and its neighbor thus making IRamenable to graph-based SSL.
In (Corduneanu andJaakkola, 2003), the agglomeration is performed bya simple averaging (arithmetic mean).
While IR sug-gests (without proof of convergence) the use of al-ternating minimization for optimization, one of thesteps of the optimization does not admit a closed-form solution.
This is a serious practical drawbackespecially in the case of large data sets.
(Tsuda,2005) (hereafter referred to as PD) is an extension ofthe IR algorithm to hypergraphs where the agglom-eration is performed using the geometric mean.
Thisleads to closed form solutions in both steps of the al-ternating minimization.
There are several importantdifferences between IR and PD on one side and ourproposed approach: (a) neither IR nor PD use anentropy regularizer, and (b) the update equation forone of the steps of the optimization in the case ofPD (equation 13 in (Tsuda, 2005)) is actually a spe-cial case of our update equation for pi(y) and maybe obtained by setting wij = 1/2.
Further, our workhere may be easily extended to hypergraphs.5 ResultsWe compare our algorithm (AM) with otherstate-of-the-art SSL-based text categorization al-gorithms, namely, (a) SVM (Joachims, 1999),(b) Transductive-SVM (TSVM) (Joachims, 1999),(c) Spectral Graph Transduction (SGT) (Joachims,2003), and (d) Label Propagation (LP) (Zhu andGhahramani, 2002).
Note that only SGT and LPare graph-based algorithms, while SVM is fully-supervised (i.e., it does not make use of any of theunlabeled data).
We implemented SVM and TSVMusing SVM Light (Joachims, b) and SGT using SGTLight (Joachims, a).
In the case of SVM, TSVM andSGT we trained |Y| classifiers (one for each class) ina one vs. rest manner precisely following (Joachims,2003).5.1 Reuters-21578We used the ?ModApte?
split of the Reuters-21578dataset collected from the Reuters newswire in10951987 (Lewis et al, 1987).
The corpus has 9,603training (not to be confused with D) and 3,299 testdocuments (which representsDu).
Of the 135 poten-tial topic categories only the 10 most frequent cate-gories are used (Joachims, 1999).
Categories outsidethe 10 most frequent were collapsed into one classand assigned a label ?other?.
For each document iin the training and test sets, we extract features xi inthe following manner: stop-words are removed fol-lowed by the removal of case and information aboutinflection (i.e., stemming) (Porter, 1980).
We thencompute TFIDF features for each document (Saltonand Buckley, 1987).
All graphs were constructed us-ing cosine similarity with TFIDF features.For this task Y = { earn, acq, money, grain,crude, trade, interest, ship, wheat, corn, average}.For LP and AM, we use the output space Y?
= Y?
{other }.
For documents in Dl that are labeled withmultiple categories, we initialize ri to have equalnon-zero probability for each such category.
Forexample, if document i is annotated as belongingto classes { acq, grain, wheat}, then ri(acq) =ri(grain) = ri(wheat) = 1/3.We created 21 transduction sets by randomly sam-pling l documents from the training set with the con-straint that each of 11 categories (top 10 categoriesand the class other) are represented at least once ineach set.
These samples constitute Dl.
All algo-rithms used the same transduction sets.
In the caseof SGT, LP and AM, the first transduction set wasused to tune the hyperparameters which we then heldfixed for all the remaining 20 transduction sets.
Forall the graph-based approaches, we ran a search overK ?
{2, 10, 50, 100, 250, 500, 1000, 2000, n} (noteK = n represents a fully connected graph).
In addi-tion, in the case of AM, we set ?
= 2 for all exper-iments, and we ran a search over ?
?
{1e?8, 1e?4,0.01, 0.1, 1, 10, 100} and ?
?
{1e?8, 1e?6, 1e?4,0.01, 0.1}, for SGT the search was over c ?
{3000,3200, 3400, 3800, 5000, 100000} (see (Joachims,2003)).We report precision-recall break even point(PRBEP) results on the 3,299 test documents in Ta-ble 1.
PRBEP has been a popular measure in infor-mation retrieval (see e.g.
(Raghavan et al, 1989)).It is defined as that value for which precision andrecall are equal.
Results for each category in Ta-ble 1 were obtained by averaging the PRBEP overCategory SVM TSVM SGT LP AMearn 91.3 95.4 90.4 96.3 97.9acq 67.8 76.6 91.9 90.8 97.2money 41.3 60.0 65.6 57.1 73.9grain 56.2 68.5 43.1 33.6 41.3crude 40.9 83.6 65.9 74.8 55.5trade 29.5 34.0 36.0 56.0 47.0interest 35.6 50.8 50.7 47.9 78.0ship 32.5 46.3 49.0 26.4 39.6wheat 47.9 44.4 59.1 58.2 64.3corn 41.3 33.7 51.2 55.9 68.3average 48.9 59.3 60.3 59.7 66.3Table 1: P/R Break Even Points (PRBEP) for the top10 categories in the Reuters data set with l = 20 andu = 3299.
All results are averages over 20 randomlygenerated transduction sets.
The last row is the macro-average over all the categories.
Note AM is the proposedapproach.the 20 transduction sets.
The final row ?average?was obtained by macro-averaging (average of av-erages).
The optimal value of the hyperparame-ters in case of LP was K = 100; in case of AM,K = 2000, ?
= 1e?4, ?
= 1e?2; and in the caseof SGT, K = 100, c = 3400.
The results showthat AM outperforms the state-of-the-art on 6 out of10 categories and is competitive in 3 of the remain-ing 4 categories.
Further it significantly outperformsall other approaches in case of the macro-averages.AM is significant over its best competitor SGT atthe 0.0001 level according to the difference of pro-portions significance test.Figure 1 shows the variation of ?average?
PRBEPagainst the number of labeled documents (l).
Foreach value of l, we tuned the hyperparameters overthe first transduction set and used these values forall the other 20 sets.
Figure 1 also shows error-bars (?
standard deviation) all the experiments.
Asexpected, the performance of all the approachesimproves with increasing number of labeled docu-ments.
Once again in this case, AM, outperformsthe other approaches for all values of l.5.2 WebKB CollectionWorld Wide Knowledge Base (WebKB) is a collec-tion of 8282 web pages obtained from four academic10960 50 100 150 200 250 300 350 400 450 500455055606570758085Number of Labeled DocumentsAveragePRBEPAMSGTLPTSVMSVMFigure 1: Average PRBEP over all classes vs.number of labeled documents (l) for Reuters dataset0 100 200 300 400 500 60020304050607080Number of Labeled DocumentsAveragePRBEPAMSGTLPTSVMSVMFigure 2: Average PRBEP over all classes vs.number of labeled documents (l) for WebKB col-lection.domains.
The web pages in the WebKB set are la-beled using two different polychotomies.
The firstis according to topic and the second is according toweb domain.
In our experiments we only consid-ered the first polychotomy, which consists of 7 cat-egories: course, department, faculty, project, staff,student, and other.
Following (Nigam et al, 1998)we only use documents from categories course, de-partment, faculty, project which gives 4199 docu-ments for the four categories.
Each of the documentsis in HTML format containing text as well as otherinformation such as HTML tags, links, etc.
We usedboth textual and non-textual information to constructthe feature vectors.
In this case we did not use ei-ther stop-word removal or stemming as this has beenfound to hurt performance on this task (Nigam et al,1998).
As in the the case of the Reuters data setwe extracted TFIDF features for each document andconstructed the graph using cosine similarity.As in (Bekkerman et al, 2003), we created fourroughly-equal random partitions of the data set.
Inorder to obtain Dl, we first randomly choose a splitand then sample l documents from that split.
Theother three splits constitute Du.
We believe this ismore realistic than sampling the labeled web-pagesfrom a single university and testing web-pages fromthe other universities (Joachims, 1999).
This methodof creating transduction sets allows us to better eval-uate the generalization performance of the variousalgorithms.
Once again we create 21 transductionsets and the first set was used to tune the hyperpa-rameters.
Further, we ran a search over the same gridas used in the case of Reuters.
We report precision-Class SVM TSVM SGT LP AMcourse 46.5 43.9 29.9 45.0 67.6faculty 14.5 31.2 42.9 40.3 42.5project 15.8 17.2 17.5 27.8 42.3student 15.0 24.5 56.6 51.8 55.0average 23.0 29.2 36.8 41.2 51.9Table 2: P/R Break Even Points (PRBEP) for the WebKBdata set with l = 48 and u = 3148.
All results are aver-ages over 20 randomly generated transduction sets.
Thelast row is the macro-average over all the classes.
AM isthe proposed approach.recall break even point (PRBEP) results on the 3,148test documents in Table 2.
For this task, we foundthat the optimal value of the hyperparameter were:in the case of LP, K = 1000; in case of AM,K = 1000, ?
= 1e?2, ?
= 1e?4; and in case ofSGT, K = 100, c = 3200.
Once again, AM is sig-nificant at the 0.0001 level over its closest competi-tor LP.
Figure 2 shows the variation of PRBEP withnumber of labeled documents (l) and was generatedin a similar fashion as in the case of the Reuters dataset.6 DiscussionWe note that LP may be cast into an AM-like frame-work by using the following sequence of updates,p(n)i (y) = ?
(i ?
l)ri(y) + ?
(i > l)q(n?1)i ,q(n)i (y) =?j wijp(n)i (y)?j wij1097To compare the behavior of AM and LP, we ap-plied this form of LP along with AM on a simple5-node binary-classification SSL graph where twonodes are labeled (node 1 and 2) and the remainingnodes are unlabeled (see Figure 3, top).
Since this isbinary classification (|Y | = 2), each distribution pior qi can be depicted using only a single real num-ber between 0 and 1 corresponding to the probabilitythat each vertex is class 2 (yes two).
We show howboth LP and AM evolve starting from exactly thesame random starting point q0 (Figure 3, bottom).For each algorithm, the figure shows that both algo-rithms clearly converge.
Each alternate iteration ofLP is such that the labeled vertices oscillate due toits clamping back to the labeled distribution, but thatis not the case for AM.
We see, moreover, qualitativedifferences in the solutions as well ?
e.g., AM?s so-lution for the pendant node 5 is less confident than isLP?s solution.
More empirical comparative analysisbetween the two algorithms of this sort will appearin future work.We have proposed a new algorithm for semi-supervised text categorization.
Empirical resultsshow that the proposed approach significantly out-performs the state-of-the-art.
In addition the pro-posed approach is relatively simple to implementand has guaranteed convergence properties.
Whilein this work, we use relatively simple features toconstruct the graph, use of more sophisticated fea-tures and/or similarity measures could lead to furtherimproved results.AcknowledgmentsThis work was supported by ONR MURI grantN000140510388, by NSF grant IIS-0093430, bythe Companions project (IST programme under ECgrant IST-FP6-034434), and by a Microsoft Re-search Fellowship.ReferencesAlexandrescu, A. and Kirchhoff, K. (2007).
Data-drivengraph construction for semi-supervised graph-basedlearnning in nlp.
In Proc.
of the Human LanguageTechnologies Conference (HLT-NAACL).Bekkerman, R., El-Yaniv, R., Tishby, N., and Winter, Y.(2003).
Distributional word clusters vs. words for textcategorization.
J. Mach.
Learn.
Res., 3:1183?1208.0.80.6 0.20.80.8Node 1Label 1Node 2Label 2Node 3UnlabeledNode 4UnlabeledNode 5Unlabeled12345 0.10.20.30.40.50.60.70.80.9AM iteration (and distribution pair) numbervertex  (datapiont) numberq(0)p(1)q(1)p(2)q(2)p(3)q(3)p(4)q(4)p(5)q(5)p(6)q(6)p(7)q(7)p(8)q(8)p(9)q(9)p(15)q(15)p(14)q(14)p(13)q(13)p(12)q(12)p(11)q(11)p(10)q(10)LP iteration (and distribution pair) numbervertex  (datapiont) number12345 0.10.20.30.40.50.60.70.80.91q(0)p(1)q(1)p(2)q(2)p(3)q(3)p(4)q(4)p(5)q(5)p(6)q(6)p(7)q(7)p(8)q(8)p(9)q(9)p(15)q(15)p(14)q(14)p(13)q(13)p(12)q(12)p(11)q(11)p(10)q(10)Figure 3: Graph (top), and alternating values of pn, qnfor increasing n for AM and LP.1098Belkin, M., Niyogi, P., and Sindhwani, V. (2005).
Onmanifold regularization.
In Proc.
of the Conference onArtificial Intelligence and Statistics (AISTATS).Bengio, Y., Delalleau, O., and Roux, N. L. (2007).
Semi-Supervised Learning, chapter Label Propogation andQuadratic Criterion.
MIT Press.Bertsekas, D. (2004).
Nonlinear Programming.
AthenaScientific Publishing.Blitzer, J. and Zhu, J.
(2008).
ACL 2008 tutorial onSemi-Supervised learning.
http://ssl-acl08.wikidot.com/.Blum, A. and Chawla, S. (2001).
Learning from labeledand unlabeled data using graph mincuts.
In Proc.
18thInternational Conf.
on Machine Learning, pages 19?26.
Morgan Kaufmann, San Francisco, CA.Blum, A. and Mitchell, T. (1998).
Combining labeledand unlabeled data with co-training.
In COLT: Pro-ceedings of the Workshop on Computational LearningTheory.Chapelle, O., Scholkopf, B., and Zien, A.
(2007).
Semi-Supervised Learning.
MIT Press.Corduneanu, A. and Jaakkola, T. (2003).
On informa-tion regularization.
In Uncertainty in Artificial Intelli-gence.Cover, T. M. and Thomas, J.
A.
(1991).
Elements of In-formation Theory.
Wiley Series in Telecommunica-tions.
Wiley, New York.Csiszar, I. and Tusnady, G. (1984).
Information Geome-try and Alternating Minimization Procedures.
Statis-tics and Decisions.Dumais, S., Platt, J., Heckerman, D., and Sahami, M.(1998).
Inductive learning algorithms and represen-tations for text categorization.
In CIKM ?98: Proceed-ings of the seventh international conference on Infor-mation and knowledge management, New York, NY,USA.Grandvalet, Y. and Bengio, Y.
(2004).
Semi-supervisedlearning by entropy minimization.
In Advances inNeural Information Processing Systems (NIPS).Joachims, T. SGT Light.
http://sgt.joachims.org.Joachims, T. SVM Light.
http://svmlight.joachims.org.Joachims, T. (1999).
Transductive inference for text clas-sification using support vector machines.
In Proc.
ofthe International Conference on Machine Learning(ICML).Joachims, T. (2003).
Transductive learning via spectralgraph partitioning.
In Proc.
of the International Con-ference on Machine Learning (ICML).Lewis, D. et al (1987).
Reuters-21578.
http://www.daviddlewis.com/resources/testcollections/reuters21578.Nigam, K., McCallum, A., Thrun, S., and Mitchell, T.(1998).
Learning to classify text from labeled and un-labeled documents.
In AAAI ?98/IAAI ?98: Proceed-ings of the fifteenth national/tenth conference on Arti-ficial intelligence/Innovative applications of artificialintelligence, pages 792?799.Pearl, J.
(1990).
Jeffrey?s Rule, Passage of Experienceand Neo-Bayesianism in Knowledge Representationand Defeasible Reasoning.
Kluwer Academic Pub-lishers.Porter, M. (1980).
An algorithm for suffix stripping.
Pro-gram, 14(3):130?137.Raghavan, V., Bollmann, P., and Jung, G. S. (1989).
Acritical investigation of recall and precision as mea-sures of retrieval system performance.
ACM Trans.Inf.
Syst., 7(3):205?229.Salton, G. and Buckley, C. (1987).
Term weighting ap-proaches in automatic text retrieval.
Technical report,Ithaca, NY, USA.Sindhwani, V., Niyogi, P., and Belkin, M. (2005).
Be-yond the point cloud: from transductive to semi-supervised learning.
In Proc.
of the International Con-ference on Machine Learning (ICML).Szummer, M. and Jaakkola, T. (2001).
Partially la-beled classification with Markov random walks.
InAdvances in Neural Information Processing Systems,volume 14.Tsuda, K. (2005).
Propagating distributions on a hyper-graph by dual information regularization.
In Proceed-ings of the 22nd International Conference on MachineLearning.Wang, J., Jebara, T., and Chang, S.-F. (2008).
Graphtransduction via alternating minimization.
In Proc.
ofthe International Conference on Machine Learning(ICML).Yarowsky, D. (1995).
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proceed-ings of the 33rd Annual Meeting of the Association forComputational Linguistics.Zhu, X.
(2005a).
Semi-supervised learning literature sur-vey.
Technical Report 1530, Computer Sciences, Uni-versity of Wisconsin-Madison.Zhu, X.
(2005b).
Semi-Supervised Learning withGraphs.
PhD thesis, Carnegie Mellon University.Zhu, X. and Ghahramani, Z.
(2002).
Learning fromlabeled and unlabeled data with label propagation.Technical report, Carnegie Mellon University.Zhu, X., Ghahramani, Z., and Lafferty, J.
(2003).
Semi-supervised learning using gaussian fields and har-monic functions.
In Proc.
of the International Con-ference on Machine Learning (ICML).1099
