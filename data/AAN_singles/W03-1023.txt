Using the Web in Machine Learning for Other-Anaphora ResolutionNatalia N. ModjeskaSchool of InformaticsUniversity of Edinburgh andDepartment of Computer ScienceUniversity of Torontonatalia@cs.utoronto.caKatja MarkertSchool of ComputingUniversity of Leeds andSchool of InformaticsUniversity of Edinburghmarkert@inf.ed.ac.ukMalvina NissimSchool of InformaticsUniversity of Edinburghmnissim@inf.ed.ac.ukAbstractWe present a machine learning frame-work for resolving other-anaphora.
Be-sides morpho-syntactic, recency, and se-mantic features based on existing lexi-cal knowledge resources, our algorithmobtains additional semantic knowledgefrom the Web.
We search the Web vialexico-syntactic patterns that are specificto other-anaphors.
Incorporating this in-novative feature leads to an 11.4 percent-age point improvement in the classifier?sF -measure (25% improvement relative toresults without this feature).1 IntroductionOther-anaphors are referential NPs with the mod-ifiers ?other?
or ?another?
and non-structural an-tecedents:1(1) An exhibition of American design and architec-ture opened in September in Moscow and willtravel to eight other Soviet cities.
(2) [.
.
. ]
the alumni director of a Big Ten university?I?d love to see sports cut back and so would alot of my counterparts at other schools, [.
.
.
]?
(3) You either believe Seymour can do it again oryou don?t.
Beside the designer?s age, otherrisk factors for Mr. Cray?s company includethe Cray-3?s [.
.
. ]
chip technology.1All examples are from the Wall Street Journal; the correctantecedents are in italics and the anaphors are in bold font.In (1), ?eight other Soviet cities?
refers to a set of So-viet cities excluding Moscow, and can be rephrasedas ?eight Soviet cities other than Moscow?.
In (2),?other schools?
refers to a set of schools excludingthe mentioned Big Ten university.
In (3), ?other riskfactors for Mr. Cray?s company?
refers to a set ofrisk factors excluding the designer?s age.In contrast, in list-contexts such as (4), the an-tecedent is available both anaphorically and struc-turally, as the left conjunct of the anaphor.2(4) Research shows AZT can relieve dementia andother symptoms in children [.
.
.
]We focus on cases such as (1?3).Section 2 describes a corpus of other-anaphors.We present a machine learning approach to other-anaphora, using a Naive Bayes (NB) classifier (Sec-tion 3) with two different feature sets.
In Section 4we present the first feature set (F1) that includesstandard morpho-syntactic, recency, and string com-parison features.
However, there is evidence that,e.g., syntactic features play a smaller role in resolv-ing anaphors with full lexical heads than in pronom-inal anaphora (Strube, 2002; Modjeska, 2002).
In-stead, a large and diverse amount of lexical orworld knowledge is necessary to understand exam-ples such as (1?3), e.g., that Moscow is a (Soviet)city, that universities are informally called schoolsin American English and that age can be viewed asa risk factor.
Therefore we add lexical knowledge,which is extracted from WordNet (Fellbaum, 1998)and from a Named Entity (NE) Recognition algo-rithm, to F1.2Antecedents are also available structurally in constructions?other than?, e.g., ?few clients other than the state?.
For a com-putational treatment of ?other?
with structural antecedents see(Bierner, 2001).The algorithm?s performance with this feature setis encouraging.
However, the semantic knowledgethe algorithm relies on is not sufficient for manycases of other-anaphors (Section 4.2).
Many expres-sions, word senses and lexical relations are miss-ing from WordNet.
Whereas it includes Moscowas a hyponym of city, so that the relation betweenanaphor and antecedent in (1) can be retrieved, itdoes not include the sense of school as university,nor does it allow to infer that age is a risk factor.There have been efforts to extract missing lexicalrelations from corpora in order to build new knowl-edge sources and enrich existing ones (Hearst, 1992;Berland and Charniak, 1999; Poesio et al, 2002).3However, the size of the used corpora still leadsto data sparseness (Berland and Charniak, 1999)and the extraction procedure can therefore requireextensive smoothing.
Moreover, some relationsshould probably not be encoded in fixed context-independent ontologies at all.
Should, e.g., under-specified and point-of-view dependent hyponymyrelations (Hearst, 1992) be included?
Should age,for example, be classified as a hyponym of risk fac-tor independent of context?Building on our previous work in (Markert et al,2003), we instead claim that the Web can be usedas a huge additional source of domain- and context-independent, rich and up-to-date knowledge, with-out having to build a fixed lexical knowledge base(Section 5).
We describe the benefit of integratingWeb frequency counts obtained for lexico-syntacticpatterns specific to other-anaphora as an additionalfeature into our NB algorithm.
This feature raisesthe algorithm?s F -measure from 45.5% to 56.9%.2 Data Collection and PreparationWe collected 500 other-anaphors with NP an-tecedents from the Wall Street Journal corpus (PennTreebank, release 2).
This data sample excludesseveral types of expressions containing ?other?
: (a)list-contexts (Ex.
4) and other-than contexts (foot-note 2), in which the antecedents are available struc-turally and thus a relatively unsophisticated proce-dure would suffice to find them; (b) idiomatic anddiscourse connective ?other?, e.g., ?on the other3In parallel, efforts have been made to enrich WordNet byadding information in glosses (Harabagiu et al, 1999).hand?, which are not anaphoric; and (c) reciprocal?each other?
and ?one another?, elliptic phrases e.g.
?one X .
.
.
the other(s)?
and one-anaphora, e.g., ?theother/another one?, which behave like pronouns andthus would require a different search method.
Alsoexcluded from the data set are samples of other-anaphors with non-NP antecedents (e.g., adjectivaland nominal pre- and postmodifiers and clauses).Each anaphor was extracted in a 5-sentence con-text.
The correct antecedents were manually an-notated to create a training/test corpus.
For eachanaphor, we automatically extracted a set of po-tential NP antecedents as follows.
First, we ex-tracted all base NPs, i.e., NPs that contain no furtherNPs within them.
NPs containing a possessive NPmodifier, e.g., ?Spain?s economy?, were split into apossessor phrase, ?Spain?, and a possessed entity,?economy?.
We then filtered out null elements andlemmatised all antecedents and anaphors.3 The AlgorithmWe use a Naive Bayes classifier, specifically the im-plementation in the Weka ML library.4The training data was generated following theprocedure employed by Soon et al (2001) forcoreference resolution.
Every pair of an anaphorand its closest preceding antecedent created a pos-itive training instance.
To generate negative train-ing instances, we paired anaphors with each of theNPs that intervene between the anaphor and its an-tecedent.
This procedure produced a set of 3,084antecedent-anaphor pairs, of which 500 (16%) werepositive training instances.The classifier was trained and tested using 10-foldcross-validation.
We follow the general practice ofML algorithms for coreference resolution and com-pute precision (P), recall (R), and F-measure (F ) onall possible anaphor-antecedent pairs.As a first approximation of the difficulty of ourtask, we developed a simple rule-based baseline al-gorithm which takes into account the fact that thelemmatised head of an other-anaphor is sometimesthe same as that of its antecedent, as in (5).4http://www.cs.waikato.ac.nz/ml/weka/.We also experimented with a decision tree classifier, withNeural Networks and Support Vector Machines with SequentialMinimal Optimization (SMO), all available from Weka.
Theseclassifiers achieved worse results than NB on our data set.Table 1: Feature set F1Type Feature Description ValuesGramm NP FORM Surface form (for all NPs) definite, indefinite, demonstrative, pronoun,proper name, unknownMatch RESTR SUBSTR Does lemmatized antecedent string contain lemma-tized anaphor string?yes, noSyntactic GRAM FUNC Grammatical role (for all NPs) subject, predicative NP, dative object, directobject, oblique, unknownSyntactic SYN PAR Anaphor-antecedent agreement with respect togrammatical functionyes, noPositional SDIST Distance between antecedent and anaphor in sen-tences1, 2, 3, 4, 5Semantic SEMCLASS Semantic class (for all NPs) person, organization, location, date, money,number, thing, abstract, unknownSemantic SEMCLASS AGR Anaphor-antecedent agreement with respect to se-mantic classyes, no, unknownSemantic GENDER AGR Anaphor-antecedent agreement with respect to gen-dersame, compatible, incompatible, unknownSemantic RELATION Type of relation between anaphor and antecedent same-predicate, hypernymy, meronymy,compatible, incompatible, unknown(5) These three countries aren?t completely off thehook, though.
They will remain on a lower-priority list that includes other countries [.
.
.
]For each anaphor, the baseline string-compares itslast (lemmatised) word with the last (lemmatised)word of each of its possible antecedents.
If thewords match, the corresponding antecedent is cho-sen as the correct one.
If several antecedents pro-duce a match, the baseline chooses the most re-cent one among them.
If string-comparison returnsno antecedent, the baseline chooses the antecedentclosest to the anaphor among all antecedents.
Thebaseline assigns ?yes?
to exactly one antecedent peranaphor.
Its P, R and F -measure are 27.8%.4 Naive Bayes without the WebFirst, we trained and tested the NB classifier witha set of 9 features motivated by our own work onother-anaphora (Modjeska, 2002) and previous MLresearch on coreference resolution (Aone and Ben-nett, 1995; McCarthy and Lehnert, 1995; Soon etal., 2001; Ng and Cardie, 2002; Strube et al, 2002).4.1 FeaturesA set of 9 features, F1, was automatically acquiredfrom the corpus and from additional external re-sources (see summary in Table 1).Non-semantic features.
NP FORM is based on thePOS tags in the Wall Street Journal corpus andheuristics.
RESTR SUBSTR matches lemmatisedstrings and checks whether the antecedent stringcontains the anaphor string.
This allows to resolveexamples such as ?one woman ringer .
.
.
anotherwoman?.
The values for GRAM FUNC were approxi-mated from the parse trees and Penn Treebank anno-tation.
The feature SYN PAR captures syntactic par-allelism between anaphor and antecedent.
The fea-ture SDIST measures the distance between anaphorand antecedent in terms of sentences.5Semantic features.
GENDER AGR captures agree-ment in gender between anaphor and antecedent,gender having been determined using gazetteers,kinship and occupational terms, titles, and Word-Net.
Four values are possible: ?same?, if both NPshave same gender; ?compatible?, if antecedent andanaphor have compatible gender, e.g., ?lawyer .
.
.other women?
; ?incompatible?, e.g., ?Mr.
Johnson.
.
.
other women?
; and ?unknown?, if one of theNPs is undifferentiated, i.e., the gender value is ?un-known?.
SEMCLASS: Proper names were classifiedusing ANNIE, part of the GATE2 software package(http://gate.ac.uk).
Common nouns werelooked up in WordNet, considering only the mostfrequent sense of each noun (the first sense in Word-Net).
In each case, the output was mapped onto oneof the values in Table 1.
The SEMCLASS AGR fea-5We also experimented with a feature MDIST that measuresintervening NP units.
This feature worsened the overall perfor-mance of the classifier.ture compares the semantic class of the antecedentwith that of the anaphor NP and returns ?yes?
ifthey belong to the same class; ?no?, if they belongto different classes; and ?unknown?
if the seman-tic class of either the anaphor or antecedent has notbeen determined.
The RELATION between other-anaphors and their antecedents can partially be de-termined by string comparison (?same-predicate?
)6or WordNet (?hypernymy?
and ?meronymy?).
Asother relations, e.g.
?redescription?
(Ex.
(3), cannotbe readily determined on the basis of the informationin WordNet, the following values were used: ?com-patible?, for NPs with compatible semantic classes,e.g., ?woman .
.
.
other leaders?
; and ?incompati-ble?, e.g., ?woman .
.
.
other economic indicators?.Compatibility can be defined along a variety of pa-rameters.
The notion we used roughly correspondsto the root level of the WordNet hierarchy.
Twonouns are compatible if they have the same SEM-CLASS value, e.g., ?person?.
?Unknown?
was usedif the type of relation could not be determined.4.2 ResultsTable 2 shows the results for the Naive Bayes clas-sifier using F1 in comparison to the baseline.Table 2: Results with F1Features P R Fbaseline 27.8 27.8 27.8F1 51.7 40.6 45.5Our algorithm performs significantly better than thebaseline.7 While these results are encouraging, therewere several classification errors.Word sense ambiguity is one of the reasons formisclassifications.
Antecedents were looked up inWordNet for their most frequent sense for a context-independent assignment of the values of semanticclass and relations.
However, in many cases eitherthe anaphor or antecedent or both are used in a sensethat is ranked as less frequent in Wordnet.
Thismight even be a quite frequent sense for a specificcorpus, e.g., the word ?issue?
in the sense of ?shares,stocks?
in the WSJ.
Therefore there is a strong inter-6Same-predicate is not really a relation.
We use it when thehead noun of the anaphor and antecedent are the same.7We used a t-test with confidence level 0.05 for all signifi-cance tests.action between word sense disambiguation and ref-erence resolution (see also (Preiss, 2002)).Named Entity resolution is another weak link.Several correct NE antecedents were classified as?antecedent=no?
(false negatives) because the NERmodule assigned the wrong class to them.The largest class of errors is however due to insuf-ficient semantic knowledge.
Problem examples canroughly be classified into five partially overlappinggroups: (a) examples that suffer from gaps in Word-Net, e.g., (2); (b) examples that require domain-,situation-specific, or general world knowledge, e.g.,(3); (c) examples involving bridging phenomena(sometimes triggered by a metonymic or metaphoricantecedent or anaphor), e.g., (6); (d) redescriptionsand paraphrases, often involving semantically vagueanaphors and/or antecedents, e.g., (7) and (3); and(e) examples with ellipsis, e.g., (8).
(6) The Justice Department?s view is shared byother lawyers [.
.
.
](7) While Mr. Dallara and Japanese officials saythe question of investors?
access to the U.S.and Japanese markets may get a disproportion-ate share of the public?s attention, a number ofother important economic issues will be onthe table at next week?s talks.
(8) He sees flashy sports as the only way the last-place network can cut through the clutter of ca-ble and VCRs, grab millions of new viewersand tell them about other shows premiering afew weeks later.In (6), the antecedent is an organization-for-peoplemetonymy.
In (7), the question of investors?
accessto the U.S. and Japanese markets is characterized asan important economic issue.
Also, the head ?is-sues?
is lexically uninformative to sufficiently con-strain the search space for the antecedent.
In (8), theantecedent is not the flashy sports, but rather flashysport shows, and thus an important piece of infor-mation is omitted.
Alternatively, the antecedent is acontent-for-container metonymy.Overall, our approach misclassifies antecedentswhose relation to the other-anaphor is based on sim-ilarity, property-sharing, causality, or is constrainedto a specific domain.
These relation types are not ?and perhaps should not be ?
encoded in WordNet.5 Naive Bayes with the WebWith its approximately 3033M pages8 the Web isthe largest corpus available to the NLP community.Building on our approach in (Markert et al, 2003),we suggest using the Web as a knowledge sourcefor anaphora resolution.
In this paper, we show howto integrate Web counts for lexico-syntactic patternsspecific to other-anaphora into our ML approach.5.1 Basic IdeaIn the examples we consider, the relation betweenanaphor and antecedent is implicitly expressed, i.e.,anaphor and antecedent do not stand in a structuralrelationship.
However, they are linked by a strongsemantic relation that is likely to be structurally ex-plicitly expressed in other texts.
We exploit this in-sight by adopting the following procedure:1.
In other-anaphora, a hyponymy/similarity rela-tion between the lexical heads of anaphor andantecedent is exploited or stipulated by the con-text,9 e.g.
that ?schools?
is an alternative termfor universities in Ex.
(2) or that age is viewedas a risk factor in Ex.
(3).2.
We select patterns that structurally explicitlyexpress the same lexical relations.
E.g., the list-context NP1and other NP2(as Ex.
(4))usually expresses hyponymy/similarity rela-tions between the hyponym NP1and its hyper-nym NP2(Hearst, 1992).3.
If the implicit lexical relationship betweenanaphor and antecedent is strong, it is likelythat anaphor and antecedent also frequentlycooccur in the selected explicit patterns.
Weinstantiate the explicit pattern for all anaphor-antecedent pairs.
In (2) the pattern NP1and other NP2is instantiated with e.g.,counterparts and other schools, sportsand other schools and universities andother schools.10 These instantiations can be8http://www.searchengineshowdown.com/stats/sizeest.shtml, data from March 2003.9In the Web feature context, we will often use?anaphor/antecedent?
instead of the more cumbersome?lexical heads of the anaphor/antecedent?.10These simplified instantiations serve as an example and areneither exhaustive nor the final instantiations we use; see Sec-tion 5.3.searched in any corpus to determine their fre-quencies.
The rationale is that the most fre-quent of these instantiated patterns is a goodclue for the correct antecedent.4.
As the patterns can be quite elaborate, mostcorpora will be too small to determine the cor-responding frequencies reliably.
The instantia-tion universities and other schools, e.g.,does not occur at all in the British National Cor-pus (BNC), a 100M words corpus of BritishEnglish.11 Therefore we use the largest corpusavailable, the Web.
We submit all instantiatedpatterns as queries making use of the GoogleAPI technology.
Here, universities andother schools yields over 700 hits, whereasthe other two instantiations yield under 10 hitseach.
High frequencies do not only occurfor synonyms; the corresponding instantiationfor the correct antecedent in Ex.
(3) age andother risk factors yields over 400 hits onthe Web and again none in the BNC.5.2 Antecedent PreparationIn addition to the antecedent preparation describedin Section 2, further processing is necessary.
First,pronouns can be antecedents of other-anaphors butthey were not used as Web query input as they arelexically empty.
Second, all modification was elim-inated and only the rightmost noun of compoundswas kept, to avoid data sparseness.
Third, using pat-terns containing NEs such as ?Will Quinlan?
in (9)also leads to data sparseness (see also the use of NErecognition for feature SEMCLASS).
(9) [.
.
. ]
Will Quinlan had not inherited a damagedretinoblastoma supressor gene and, therefore,faced no more risk than other children [.
.
.
]We resolved NEs in two steps.
In additionto GATE?s classification into ENAMEX and NU-MEX categories, we used heuristics to automati-cally obtain more fine-grained distinctions for thecategories LOCATION, ORGANIZATION, DATE andMONEY, whenever possible.
No further distinc-tions were made for the category PERSON.
Weclassified LOCATIONS into COUNTRY, (US) STATE,CITY, RIVER, LAKE and OCEAN, using mainly11http://info.ox.ac.uk/bncTable 3: Patterns and Instantiations for other-anaphoraANTECEDENT PATTERN INSTANTIATIONScommon noun (O1): (N1fsgg OR N1fplg) and other N2fplg Ic1: ?
(university OR universities) and other schools?proper name (O1): (N1fsgg OR N1fplg) and other N2fplg Ip1: ?
(person OR persons) and other children?Ip2: ?
(child OR children) and other persons?
(O2): N1and other N2fplg Ip3: ?Will Quinlan and other children?gazetteers.12 If an entity classified by GATE asORGANIZATION contained an indication of the or-ganization type, we used this as a subclassifica-tion; therefore ?Bank of America?
is classified asBANK.
For DATE and MONEY entities we usedsimple heuristics to classify them further into DAY,MONTH, YEAR as well as DOLLAR.From now on we call A the list of possible an-tecedents and ana the anaphor.
For (2), this listis A2=fcounterpart, sport, universityg (the pronoun?I?
has been discarded) and ana2=school.
For (9),they are A9=frisk, gene, person [=Will Quinlan]gand ana9=child.5.3 Queries and Scoring MethodWe use the list-context pattern:13(O1) (N1fsgg OR N1fplg) and other N2fplgFor common noun antecedents, we instantiate thepattern by substituting N1with each possible an-tecedent from set A, and N2with ana, as normallyN1is a hyponym of N2in (O1), and the antecedentis a hyponym of the anaphor.
An instantiated pat-tern for Ex.
(2) is (university OR universities)and other schools (Ic1in Table 3).14For NE antecedents we instantiate (O1) by substi-tuting N1with the NE category of the antecedent,and N2with ana.
An instantiated pattern forExample (9) is (person OR persons) and otherchildren (Ip1in Table 3).
In this instantiation, N1(?person?)
is not a hyponym of N2(?child?
), insteadN2is a hyponym of N1.
This is a consequence ofthe substitution of the antecedent (?Will Quinlan?
)12They were extracted from the Web.
Small gazetteers, con-taining in all about 500 entries, are sufficient.
This is the onlyexternal knowledge collected for the Web feature.13In all patterns in this paper, ?OR?
is the boolean operator,?N1?
and ?N2?
are variables, all other words are constants.14Common noun instantiations are marked by a superscript?c?
and proper name instantiations by a superscript ?p?.with its NE category (?person?
); such an instanti-ation is not frequent, since it violates standard re-lations within (O1).
Therefore, we also instantiate(O1) by substituting N1with ana, and N2with theNE type of the antecedent (Ip2in Table 3).
Finally,for NE antecedents, we use an additional pattern:(O2) N1and other N2fplgwhich we instantiate by substituting N1with theoriginal NE antecedent and N2with ana (Ip3in Ta-ble 3).Patterns and instantiations are summarised in Ta-ble 3.
We submit these instantiations as queries tothe Google search engine.For each antecedent ant in A we obtain the rawfrequencies of all instantiations it occurs in (Ic1forcommon nouns, or Ip1, Ip2, Ip3for proper names) fromthe Web, yielding freq(Ic1), or freq(Ip1), freq(Ip2)and freq(Ip3).
We compute the maximum Mantover these frequencies for proper names.
For com-mon nouns Mantcorresponds to freq(Ic1).
The in-stantiation yielding Mantis then called Imaxant.Our scoring method takes into account the indi-vidual frequencies of ant and ana by adapting mu-tual information.
We call the first part of Imaxant(e.g.
?university OR universities?, or ?child OR chil-dren?)
Xant, and the second part (e.g.
?schools?or ?persons?)
Yant.
We compute the probability ofImaxant, Xantand Yant, using Google to determinefreq(Xant) and freq(Yant).Pr(Imaxant) =Mantnumber of GOOGLE pagesPr(Xant) =freq(Xant)number of GOOGLE pagesPr(Yant) =freq(Yant)number of GOOGLE pagesWe then compute the final score MIant.MIant= logPr(Imaxant)Pr(Xant)Pr(Yant)5.4 Integration into ML Framework andResultsFor each anaphor, the antecedent in A with thehighest MIantgets feature value ?webfirst?.15 Allother antecedents (including pronouns) get the fea-ture value ?webrest?.
We chose this method insteadof e.g., giving score intervals for two reasons.
First,since score intervals are unique for each anaphor,it is not straightforward to incorporate them into aML framework in a consistent manner.
Second, thismethod introduces an element of competition be-tween several antecedents (see also (Connolly et al,1997)), which the individual scores do not reflect.We trained and tested the NB classifier with thefeature set F1, plus the Web feature.
The last rowin Table 4 shows the results.
We obtained a 9.1 per-centage point improvement in precision (an 18% im-provement relative to the F1 feature set) and a 12.8percentage point improvement in recall (32% im-provement relative to F1), which amounts to an 11.4percentage point improvement in F -measure (25%improvement relative to F1 feature set).
In particu-lar, all the examples in this paper were resolved.Our algorithm still misclassified several an-tecedents.
Sometimes even the Web is not largeenough to contain the instantiated pattern, espe-cially when this is situation or speaker specific.
An-other problem is the high number of NE antecedents(39.6%) in our corpus.
While our NER module isquite good, any errors in NE classification lead toincorrect instantiations and thus to incorrect classi-fications.
In addition, the Web feature does not yettake into account pronouns (7.43% of all correct andpotential antecedents in our corpus).6 Related Work and DiscussionModjeska (2002) presented two hand-crafted algo-rithms, SAL and LEX, which resolve the anaphoricreferences of other-NPs on the basis of grammati-cal salience and lexical information from WordNet,respectively.
In our own previous work (Markert et15If several antecedents have the highest MIantthey all getvalue ?webfirst?.Table 4: Results with F1 and F1+WebFeatures P R Fbaseline 27.8 27.8 27.8F1 51.7 40.6 45.5F1+Web 60.8 53.4 56.9al., 2003) we presented a preliminary symbolic ap-proach that uses Web counts and a recency-basedtie-breaker for resolution of other-anaphora andbridging descriptions.
(For another Web-based sym-bolic approach to bridging see (Bunescu, 2003).
)The approach described in this paper is the first ma-chine learning approach to other-anaphora.
It isnot directly comparable to the symbolic approachesabove for two reasons.
First, the approaches dif-fer in the data and the evaluation metrics they used.Second, our algorithm does not yet constitute afull resolution procedure.
As the classifier oper-ates on the whole set of antecedent-anaphor pairs,more than one potential antecedent for each anaphorcan be classified as ?antecedent=yes?.
This canbe amended by e.g.
incremental processing.
Also,the classifier does not know that each other-NP isanaphoric and therefore has an antecedent.
(Thiscontrasts with e.g.
definite NPs.)
Thus, it can clas-sify all antecedents as ?antecedent=no?.
This can beremedied by using a back-off procedure, or a compe-tition learning approach (Connolly et al, 1997).
Fi-nally, the full resolution procedure will have to takeinto account other factors, e.g., syntactic constraintson antecedent realization.Our approach is the first ML approach to any kindof anaphora that integrates the Web.
Using the Webas a knowledge source has considerable advantages.First, the size of the Web almost eliminates the prob-lem of data sparseness for our task.
For this rea-son, using the Web has proved successful in sev-eral other fields of NLP, e.g., machine translation(Grefenstette, 1999) and bigram frequency estima-tion (Keller et al, 2002).
In particular, (Keller et al,2002) have shown that using the Web handles datasparseness better than smoothing.
Second, we donot process the returned Web pages in any way (tag-ging, parsing, e.g.
), unlike e.g.
(Hearst, 1992; Poe-sio et al, 2002).
Third, the linguistically motivatedpatterns we use reduce long-distance dependenciesbetween anaphor and antecedent to local dependen-cies.
By looking up these patterns on the Web weobtain semantic information that is not and perhapsshould not be encoded in an ontology (redescrip-tions, vague relations, etc.).
Finally, these local de-pendencies also reduce the need for prior word sensedisambiguation, as the anaphor and the antecedentconstrain each other?s sense within the context of thepattern.7 ConclusionsWe presented a machine learning approach to other-anaphora, which uses a NB classifier and two setsof features.
The first set consists of standardmorpho-syntactic, recency, and semantic featuresbased on WordNet.
The second set alo incorpo-rates semantic knowledge obtained from the Web vialexico-semantic patterns specific to other-anaphora.Adding this knowledge resulted in a dramatic im-provement of 11.4% points in the classifier?s F -measure, yielding a final F -measure of 56.9%.To our knowledge, we are the first to integrate aWeb feature into a ML framework for anaphora reso-lution.
Adding this feature is inexpensive, solves thedata sparseness problem, and allows to handle ex-amples with non-standard relations between anaphorand antecedent.
The approach is easily applicable toother anaphoric phenomena by developing appropri-ate lexico-syntactic patterns (Markert et al, 2003).AcknowledgmentsNatalia N.Modjeska is supported by EPSRC grantGR/M75129; Katja Markert by an Emmy NoetherFellowship of the Deutsche Forschungsgemen-schaft.
We thank three anonymous reviewers forhelpful comments and suggestions.ReferencesC.
Aone and S. W. Bennett.
1995.
Evaluating automatedand manual acquisition of anaphora resolution strate-gies.
In Proc.
of ACL?95, pages 122?129.M.
Berland and E. Charniak.
1999.
Finding parts in verylarge corpora.
In Proc.
of ACL?99, pages 57?64.G.
Bierner.
2001.
Alternative phrases and natural lan-guage information retrieval.
In Proc.
of ACL?01.R.
Bunescu.
2003.
Associative anaphora resolution: AWeb-based approach.
In R. Dale, K. van Deemter, andR.
Mitkov, editors, Proc.
of the EACL Workshop on theComputational Treatment of Anaphora.D.
Connolly, J. D. Burger, and D. S. Day.
1997.
Amachine learning approach to anaphoric reference.
InDaniel Jones and Harold Somers, editors, New Meth-ods in Language Processing, pages 133?144.
UCLPress, London.C.
Fellbaum, editor.
1998.
WordNet: An Electronic Lex-ical Database.
The MIT Press.G.
Grefenstette.
1999.
The WWW as a resource forexample-based MT tasks.
In Proc.
of ASLIB?99 Trans-lating and the Computer 21, London.S.
Harabagiu, G. Miller, and D. Moldovan.
1999.
Word-net 2 - a morphologically and semantically enhancedresource.
In Proc.
of SIGLEX-99, pages 1?8.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proc.
of COLING-92.F.
Keller, M. Lapata, and O. Ourioupina.
2002.
Using theWeb to overcome data sparseness.
In Proc.
of EMNLP2002, pages 230?237.K.
Markert, M. Nissim, and N. N. Modjeska.
2003.Using the Web for nominal anaphora resolution.
InR.
Dale, K. van Deemter, and R. Mitkov, editors, Proc.of the EACL Workshop on the Computational Treat-ment of Anaphora, pages 39?46.J.
F. McCarthy and W. G. Lehnert.
1995.
Using decisiontrees for coreference resolution.
In Proc.
of IJCAI-95,pages 1050?1055.N.
N. Modjeska.
2002.
Lexical and grammatical roleconstraints in resolving other-anaphora.
In Proc.
ofDAARC 2002, pages 129?134.V.
Ng and C. Cardie.
2002.
Improving machine learn-ing approaches to coreference resolution.
In Proc.
ofACL?02, pages 104?111.M.
Poesio, T. Ishikawa, S. Schulte im Walde, andR.
Viera.
2002.
Acquiring lexical knowledge foranaphora resolution.
In Proc.
of LREC 2002, pages1220?1224.J.
Preiss.
2002.
Anaphora resolution with word sensedisambiguation.
In Proc.
of SENSEVAL-2, pages 143?146.W.
M. Soon, H. T. Ng, and D. C. Y. Lim.
2001.
A ma-chine learning approach to coreference resolution ofnoun phrases.
Computational Linguistics, 27(4):521?544.M.
Strube, S. Rapp, and C. Mu?ller.
2002.
The influenceof minimum edit distance on reference resolution.
InProc.
of EMNLP 2002, pages 312?319.M.
Strube.
2002.
NLP approaches to reference resolu-tion.
Tutorial notes, ACL?02.
