Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 149?153,Dublin, Ireland, August 23-24, 2014.BUAP: Evaluating Features for Multilingual and Cross-Level SemanticTextual SimilarityDarnes Vilarin?o, David Pinto, Sau?l Leo?n, Mireya Tovar,Beatriz Beltra?nBeneme?rita Universidad Auto?noma de PueblaFaculty of Computer Science14 Sur y Av.
San Claudio, CUPuebla, Puebla, Me?xico{darnes,dpinto,saul.leon,mtovar,bbeltran}@cs.buap.mxAbstractIn this paper we present the evaluation ofdifferent features for multiligual and cross-level semantic textual similarity.
Three dif-ferent types of features were used: lexical,knowledge-based and corpus-based.
The re-sults obtained at the Semeval competition rankour approaches above the average of the restof the teams highlighting the usefulness of thefeatures presented in this paper.1 IntroductionSemantic textual similarity aims to capture whetherthe meaning of two texts are similar.
This conceptis somehow different from the textual similarity def-inition itself, because in the latter we are only in-terested in measuring the number of lexical com-ponents that the two texts share.
Therefore, tex-tual similarity can range from exact semantic equiv-alence to a complete unrelatedness pair of texts.Finding the semantic similarity between a pairof texts has become a big challenge for specialistsin Natural Language Processing (NLP), because ithas applications in some NLP task such as machinetranslation, automatic construction of summaries,authorship attribution, machine reading comprehen-sion, information retrieval, among others, whichusually need a manner to calculate degrees of simi-larity between two given texts.This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/Semantic textual similarity can be calculated us-ing texts of different sizes, for example between, aparagraph and a sentence, or a sentence and a phrase,or a phrase and a word, or even a word and a sense.When we consider this difference, we say the task iscalled ?Cross-Level Semantic Similarity?, but whenthis distinction is not considered, then we call thetask just as ?Semantic Textual Similarity?.In this paper, we evaluate different features for de-termining those that obtain the best performances forcalculating both, cross-level semantic similarity andmultilingual semantic textual similarity.The remaining of this paper is structured as fol-lows.
Section 2 presents the features used in bothexperiments.
Section 3 shows the manner we usedthe features for determining the degree of seman-tic textual similarity.
Section 4, on the other hand,shows the experiments we have carried out for de-termining cross-level semantic similarity.
Finally, inSection 5 the conclusions and findings are given.2 Description of FeaturesIn this section we describe the different features usedfor evaluation semantic textual similarity.
Basically,we have used three different types of features: lex-ical, knowledge-based and corpus-based.
The firstone, counts the frequency of ocurrence of lexicalfeatures which include n-grams of characters, skip-grams1, words and some lexical relationships suchas synonymy or hypernymy.
Additionally, we haveused two other features: the Jaccard coefficient be-tween the two text, expanding each term with a set of1They are also known as disperse n-grams because they con-sider to ?skip?
a certain number of characters.149synonyms taken from WordReference Carrillo et al.
(2012), and the cosine between the two texts repre-sented each by a bag of character n-grams and skip-grams.
In this case, we did not applied any wordsense disambiguation system before expanding withsynonyms, a procedure that may be performed in afurther work.The second set of features considers the followingsix word similarity metrics offered by NLTK: Lea-cock & Chodorow (Leacock and Chodorow, 1998),Lesk (Lesk, 1986), Wu & Palmer (Wu and Palmer,1994), Resnik (Resnik, 1995), Lin (Lin, 1998), andJiang & Conrath2 (Jiang and Conrath, 1997).
Inthis case, we determine the similarity between twotexts as the maximum possible pair of words similar-ity.
The third set of features considers two corpus-based measures, both based on Rada Mihalcea?s tex-tual semantic similarity (Mihalcea et al., 2006).
Thefirst one uses Pointwise Mutual Information (PMI)(Turney, 2001) for calculating the similarity betweenpairs of words, whereas the second one uses LatentSemantic Analysis (LSA) (Landauer et al., 1998)(implemented in the R software environment for sta-tistical computing) for that purpose.
In particular,the PMI and LSA values were obtained using a cor-pus built on the basis of Europarl, Project-Gutenbergand Open Office Thesaurus.
A summary of thesefeatures can be seen in Table 1.3 Multilingual Semantic Textual SimilarityThis task aims to find the semantic textual similar-ity between two texts written in the same language.Two different languages were considered: Englishand Spanish.
The degree of semantic similarityranges from 0 to 5; the bigger this value, the best se-mantic match between the two texts.
For the experi-ments we have used the training datasets provided at2012, 2013 and 2014 Semeval competitions.
Thesedatasets are completely described at the task descrip-tion papers of these Semeval editions Agirre et al.
(2013, 2014).In order to calculate the semantic textual simi-larity for the English language, we have used allthe features mentioned at Section 2.
We have con-structed a single vector for each pair of texts of thetraining corpus, thus resulting 6,627 vectors in total.2Natural Language Toolkit of Python; http://www.nltk.org/The resulting set of vectors fed a supervised classi-fier, in particular, a logistic regression model3.
Thisapproach has been named as BUAP-EN-run1.
Themost representative results obtained at the competi-tion for the English language can be seen in Table 2.As can be seen, we outperformed the average resultin all the cases, except on the case that the OnWNcorpus was used.In order to calculate the semantic textual similar-ity for the Spanish language, we have submitted tworuns, the first one is a supervised approach whichconstructs a regression model, similar that the oneconstructed for the English language, but consider-ing only the following features: character n-grams,character skip-grams, and the cosine similarity ofbag of character n-grams and skip-grams.
This ap-proach was named BUAP-run1.
Given that the num-ber of Spanish samples was so small, we decidedto investigate the behaviour of training with Englishand testing with Spanish language.
It is quite inter-esting that this approach obtained a relevant ranking(17 from 22 runs), even if the type of features usedwere na?
?ve.The second approach submitted for determiningthe semantic textual similarity for the Spanish lan-guage is an unsupervised one.
It uses the same fea-tures of the supervised approach for Spanish, butthese features were used to create a representationvector for each text (independently), so that we maybe able to calculate the similarity by means of thecosine measure between the two vectors.
The ap-proach was named BUAP-run2.The most representative results obtained at thecompetition for the Spanish language can be seenin Table 3.
There we can see that our unsupervisedapproach slightly outperformed the overall average,but the supervised approach was below the overallaverage, a fact that is expected since we have trainedusing the English corpus and testing with the Span-ish language.
Despite this, it is quite interesting thatthe result obtained with this supervised approach isnot so bad.Due to space constraints, we did not reported thecomplete set of results of the competition, however,these results can be seen at the task 10 description3We used the version of the logistic classifier implementedin the the Weka toolkit150Table 1: Features used for calculating semantic textual similarityFeature Typen-grams of characters (n = 2, ?
?
?
, 5) Lexicalskip-grams of characters (skip = 2, ?
?
?
, 5) LexicalNumber of words shared LexicalNumber of synonyms shared LexicalNumber of hypernyms shared LexicalJaccard coefficient with synonyms expansion LexicalCosine of bag of character n-grams and skip-grams LexicalLeacock & Chodorow?s word similarity Knowledge-basedLesk?s word similarity Knowledge-basedWu & Palmer?s word similarity Knowledge-basedResnik?s word similarity Knowledge-basedLin?s word similarity Knowledge-basedJiang & Conrath?s word similarity Knowledge-basedRada Mihalcea?s metric using PMI Corpus-basedRada Mihalcea?s metric using LSA Corpus-basedTable 2: Results obtained at the Task 10 of the Semeval competition for the English languageTeam Name deft-forum deft-news headlines images OnWN tweet-news Weighted mean RankDLS@CU-run2 0.4828 0.7657 0.7646 0.8214 0.8589 0.7639 0.7610 1Meerkat Mafia-pairingWords 0.4711 0.7628 0.7597 0.8013 0.8745 0.7793 0.7605 2NTNU-run3 0.5305 0.7813 0.7837 0.8343 0.8502 0.6755 0.7549 3BUAP-EN-run1 0.4557 0.6855 0.6888 0.6966 0.6539 0.7706 0.6715 19Overall average 0.3607 0.6198 0.5885 0.6760 0.6786 0.6001 0.6015 27-28Bielefeld SC-run2 0.2108 0.4307 0.3112 0.3558 0.3607 0.4087 0.3470 36UNED-run22 p np 0.1043 0.3148 0.0374 0.3243 0.5086 0.4898 0.3097 37LIPN-run2 0.0843 - - - - - 0.0101 38Our difference against the average 9% 7% 10% 2% -2% 17% 7% -Table 3: Results obtained at the Task 10 of the Semeval competition for the Spanish language (NOTE: The * symboldenotes a system that used Wikipedia to build its model for the Wikipedia test dataset)Team Name System type Wikipedia News Weighted correlation RankUMCC DLSI-run2 supervised 0.7802 0.8254 0.8072 1Meerkat Mafia-run2 unsupervised 0.7431 0.8454 0.8042 2UNAL-NLP-run1 weakly supervised 0.7804 0.8154 0.8013 3BUAP-run2 unsupervised 0.6396 0.7637 0.7137 14Overall average - 0.6193 0.7504 0.6976 14-15BUAP-run1 supervised 0.5504 0.6785 0.6269 17RTM-DCU-run2 supervised 0.3689 0.6253 0.5219 20Bielefeld SC-run2 unsupervised* 0.2646 0.5546 0.4377 21Bielefeld SC-run1 unsupervised* 0.2632 0.5545 0.4371 22Difference between our run1 and the overall average - -7% -7% -7% -Difference between our run2 and the overall average - 2% 1% 2% -paper (Agirre et al., 2014) of Semeval 2014.4 Cross-Level Semantic SimilarityThis task aims to find semantic similarity betweena pair of texts of different length written in En-glish language, actually each text belong to a dif-ferent level of representation of language (para-graph, sentence, phrase, word, and sense).
Thus,the pair of levels that were required to be comparedin order to determine their semantic similarity were:paragraph-to-sentence, sentence-to-phrase, phrase-to-word, and word-to-sense.The task cross level similarity judgments arebased on five rating levels which goes from 0 to1514.
The first (0) implies that the two items do notmean the same thing and are not on the same topic,whereas the last one (4) implies that the two itemshave very similar meanings and the most importantideas, concepts, or actions in the larger text are rep-resented in the smaller text.
The remaining ratinglevels imply something in the middle.For word-to-sense comparison, a sense is pairedwith a word and the perceived meaning of the wordis modulated by virtue of the comparison with thepaired sense?s definition.
For the experiments pre-sented at the competition, a corpus of 2,000 pairsof texts were provided for training and other 2,000pairs for testing.
This dataset considered 500 pairsfor each type of level of semantic similarity.
Thecomplete description of this task together with thedataset employed is given in the task description pa-per Jurgens et al.
(2014).We submitted two supervised approaches, to thistask employing all the features presented at Section2.
The first approach simply constructs a single vec-tor for each pair of training texts using the afore-mentioned features.
These vectors are introduced inWeka for constructing a classification model basedon logistic regression.
This approach was namedBUAP-run1.We have observed that when comparing texts ofdifferent length, there may be a high discrepancybetween those texts because a very small length inthe texts may difficult the process of determining thesemantic similarity.
Therefore, we have proposedto expand small text with the aim of having moreterm useful in the process of calculating the degreeof semantic similarity.
In particular, we have ex-panded words for the phrase-to-word and word-to-sense cases.
The expansion has been done as fol-lows.
When we calculated the similarity betweenphrases and words, we expanded the word compo-nent with those related terms obtained by means ofthe Related-Tags Service of Flickr.
When we cal-culated the semantic similarity between words andsenses, we expanded the word component with theirWordNet Synsets (none word sense disambiguationmethod was employed).
This second approach wasnamed BUAP-run2.The most representative results for the cross-levelsemantic similarity task (which include our results)are shown in Table 4.
There we can see that the fea-tures obtained a good performance when we com-puted the semantic similarity between paragraphsand sentences, and when we calculated the simili-raty between sentences to phrases.
Actually, bothruns obtained exactly the same result, because themain difference between these two runs is that thesecond one expands the word/sense using the Re-lated Tags of Flickr.
However, the set of expansionwords did not work properly, in particular when cal-culating the semantic similarity between phrases andwords.
We consider that this behaviour is due tothe domain of the expansion set do not match withthe domain of the dataset to be evaluated.
In thecase of expanding words for calculating the similar-ity between words and senses, we obtained a slightlybetter performance, but again, this values are notsufficient to highly outperform the overall average.As future work we consider to implement a self-expansion technique for obtaining a set of relatedterms by means of the same training corpus.
Thistechnique has proved to be useful when the expan-sion process is needed in restricted domains Pintoet al.
(2011).5 ConclusionsThis paper presents the results obtained by theBUAP team at the Task 3 and 10 of SemEval 2014.In both task we have used a set of similar features,due to the aim of these two task are quite similar:determining semantic similarity.
Some special mod-ifications has been done according to each task inorder to tackle some issues like the language or thetext length.In general, the features evaluated performed wellover the two approaches, however, some issues arisethat let us know that we need to tune the approachespresented here.
For example, a better expansion setis required in the case of the Task 3, and a great num-ber of samples for the spanish samples of Task 10will be required.ReferencesEneko Agirre, Daniel Cer, Mona Diab, AitorGonzalez-Agirre, and Weiwei Guo.
*sem 2013shared task: Semantic textual similarity.
In 2ndJoint Conference on Lexical and Computational152Table 4: Results obtained at Task 3 of Semeval 2014Team System Paragraph-to-Sentence Sentence-to-Phrase Phrase-to-Word Word-to-Sense RankSimCompass run1 0.811 0.742 0.415 0.356 1ECNU run1 0.834 0.771 0.315 0.269 2UNAL-NLP run2 0.837 0.738 0.274 0.256 3BUAP run1 0.805 0.714 0.162 0.201 9BUAP run2 0.805 0.714 0.142 0.194 10Overall average - 0.728 0.651 0.198 0.192 11-12Our run1 - Overall average 8% 6% -4% 1% -Our run2 - Overall average 8% 6% -6% 0% -Semantics (*SEM), pages 32?43, Atlanta, Geor-gia, USA, 2013.Eneko Agirre, Carmen Banea, Claire Cardie, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, WeiweiGuo, Rada Mihalcea, German Rigau, and JanyceWiebe.
Semeval-2014 task 10: Multilingual se-mantic textual similarity.
In Proceedings of the8th International Workshop on Semantic Evalua-tion (SemEval-2014), Dublin, Ireland, 2014.Maya Carrillo, Darnes Vilarin?o, David Pinto,Mireya Tovar, Saul Leo?n, and Esteban Castillo.Fcc: Three approaches for semantic textual sim-ilarity.
In Proceedings of the 1st Joint Con-ference on Lexical and Computational Seman-tics (SemEval 2012), pages 631?634, Montre?al,Canada, 2012.Jay J. Jiang and David W. Conrath.
Semantic simi-larity based on corpus statistics and lexical taxon-omy.
In Proc of 10th International Conferenceon Research in Computational Linguistics, RO-CLING?97, pages 19?33, 1997.David Jurgens, Mohammad Taher Pilehvar, andRoberto Navigli.
Semeval-2014 task 3: Cross-level semantic similarity.
In Proceedings of the8th International Workshop on Semantic Evalua-tion (SemEval-2014), Dublin, Ireland, 2014.Thomas K. Landauer, Peter W. Foltz, and DarrellLaham.
An Introduction to Latent Semantic Anal-ysis.
Discourse Processes, (25):259?284, 1998.Claudia Leacock and Martin Chodorow.
Combin-ing local context and wordnet similarity for wordsense identification.
In Christiane Fellbaum, edi-tor, MIT Press, pages 265?283, 1998.Michael Lesk.
Automatic sense disambiguation us-ing machine readable dictionaries: How to tell apine cone from an ice cream cone.
In Proceed-ings of the 5th Annual International Conferenceon Systems Documentation, pages 24?26.
ACM,1986.Dekang Lin.
An information-theoretic definition ofsimilarity.
In Proceedings of the Fifteenth Inter-national Conference on Machine Learning, ICML?98, pages 296?304, San Francisco, CA, USA,1998.
Morgan Kaufmann Publishers Inc.Rada Mihalcea, Courtney Corley, and Carlo Strap-parava.
Corpus-based and knowledge-based mea-sures of text semantic similarity.
In Proceedingsof the 21st National Conference on Artificial In-telligence, pages 775?780, 2006.David Pinto, Paolo Rosso, and He?ctor Jime?nez-Salazar.
A self-enriching methodology for clus-tering narrow domain short texts.
Computer Jour-nal, 54(7):1148?1165, 2011.Philip Resnik.
Using information content to evalu-ate semantic similarity in a taxonomy.
In Proceed-ings of the 14th International Joint Conference onArtificial Intelligence, IJCAI?95, pages 448?453,San Francisco, CA, USA, 1995.Peter D. Turney.
Mining the web for synonyms:Pmi-ir versus lsa on toefl.
In Proceedings of the12th European Conference on Machine Learning,pages 491?502.
Springer-Verlag, 2001.Zhibiao Wu and Martha Stone Palmer.
Verb seman-tics and lexical selection.
In Proceedings of the32nd Annual Meeting of the Association for Com-putational Linguistics, pages 133?138, 1994.153
