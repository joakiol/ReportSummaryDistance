The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 302?306,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsHOO 2012 Shared Task: UKP Lab System DescriptionTorsten Zesch??
and Jens Haase?
?Ubiquitous Knowledge Processing Lab (UKP-TUDA)Department of Computer Science, Technische Universita?t Darmstadt?Ubiquitous Knowledge Processing Lab (UKP-DIPF)German Institute for Educational Research and Educational Informationwww.ukp.tu-darmstadt.deAbstractIn this paper, we describe the UKP Lab systemparticipating in the HOO 2012 Shared Task onpreposition and determiner error correction.Our focus was to implement a highly flexi-ble and modular system which can be easilyaugmented by other researchers.
The systemmight be used to provide a level playgroundfor subsequent shared tasks and enable furtherprogress in this important research field on topof the state of the art identified by the sharedtask.1 IntroductionUKP Lab already participated in the previous HOOShared Task in 2011.
Our knowledge-based system(Zesch, 2011) was targeted towards detecting real-word spelling errors, but performed also well on anumber of other error classes.1 However, it was notcompetitive for article and preposition errors wheresupervised systems based on confusion sets consti-tute the state of the art.
Thus, we tailor the HOO2011 system towards correcting article and prepo-sition errors, but also implement a supervised ap-proach based on confusion sets (Golding and Sch-abes, 1996; Jones and Martin, 1997; Carlson et al,2001).We decided to implement a basic system thatshould be as flexible as possible and might serve as abasis for experiments in future rounds of the sharedtask.
We also plan to model the most successful1http://clt.mq.edu.au/research/projects/hoo/hoo2011/reports/hoo2011-UDposter.pdfsystems in our framework as soon as the system de-scriptions are made available.2 This might providea level playground for subsequent shared tasks andenable real progress in this important field on top ofthe state of the art identified by the HOO shared task.2 Supervised Error DetectionWe implement a generic framework for article andpreposition error detection based on the open-sourceDKPro framework.3 DKPro is a collection of soft-ware components for natural language processingbased on the Apache UIMA framework (Ferrucciand Lally, 2004).
It comes with a collection ofready-made modules which can be combined toform more complex applications.Our goal is to develop a system which is as flex-ible as possible with respect to (i) linguistic pre-processing, (ii) the extraction of features, and (iii)the applied classification method.
We will make thesource code publicly available as part of the DKProinfrastructure and hope that this will lower the ob-stacles for participating in future rounds of the HOOShared Task.We also provide a reference implementation of theHOO 2012 experiments based on the DKPro Labframework (Eckart de Castilho and Gurevych, 2011)which enables (i) parameter sweeping, (ii) modelingof interdependent tasks (like e.g.
training and testcycles), (iii) generating performance reports, and(iv) storing all experimental results in a convenientmanner.2We invite other participating teams to help with this effort.3http://code.google.com/p/dkpro-core-asl/3022.1 Linguistic PreprocessingFor our basic implementation, we only use a fewpreprocessing steps.
We tokenize and sentence splitthe data with the default DKPro segmenter, andthen use TreeTagger (Schmid, 2004) to POS-tag andchunk the sentences.
However, the framework al-lows the effortless addition of other preprocessingcomponents, e.g.
parsing or named-entity recogni-tion.2.2 Feature ExtractionWe implement a generic feature extraction processbased on the ClearTK project (Ogren et al, 2008).ClearTK provides a set of highly flexible feature ex-tractors that access the annotations (e.g.
POS tags,chunks, etc.)
created by the linguistic preprocessing.One important decision during training is to de-cide which instances should be used for feature ex-traction.
In the simplest setting, each token is used togenerate an instance, but this would result in a veryhigh number of negative instances for every positiveinstance.
For the error classes RT/UT and RD/UD, amore balanced distribution of instances can be easilyenforced by only creating a positive instance if thetoken equals an element in the corresponding confu-sion set.
We create a negative instance by removingor changing the article/preposition.For articles, we use the confusion set:{a, an, the, this}4For prepositions, we use the confusion set:{as, at, but, by, for, from, in, of, on, out,over, since, than, to, up, with}The confusion set is a parameter to the feature ex-traction method and can be changed easily.
This alsomakes it possible to apply the framework to other er-ror classes, e.g.
for correcting frequently confusedwords like (accept, except) or (than, then).Table 1 lists the set of basic features implementedin the reference system.
As our goal was to imple-ment a highly flexible system, we put more effortin the overall architecture than in the feature engi-neering.
N-gram features are computed based on the4In the official runs, an was not part of the confusion set, butwas specially handled in a post-processing step.
In the currentversion of the framework, we removed this heuristic and nowtreat an as a normal part of the confusion set.Google Web1T n-gram corpus (Brants and Franz,2006) which is accessed using jWeb1T.5The listed features can be improved in manyways, e.g.
the chunk feature could also encode thetype of the chunk.
As the framework allows to easilyadd new feature extractors, we are going to integratethe most successful features from the shared task.Due to the modular architecture of ClearTK, the im-plemented feature extractors could even be re-usedfor other classification tasks unrelated to spellingcorrection.2.3 ClassificationClearTK provides a wide range of adapters to wellknown machine learning frameworks and classifica-tion tools.
As of April 2012, the following adaptersare supported:?
LIBSVM6?
MALLET7 (McCallum, 2002)?
OpenNLP Maxent8?
SVMlight9 (Joachims, 1999)?
SVMlight-TK10 (Moschitti, 2006)?
Weka11 (Hall et al, 2009)As we can easily switch the classifier, we trieda wide range of classifiers, but SVM worked gen-erally best.
For the official runs, we used SVM asimplemented in the Weka toolkit with the parameter?BuildLogisticModels?
which allows to base a de-tection decision on the confidence of the classifier inorder to improve precision.3 Knowledge-based Error DetectionBesides the supervised system described above, wealso apply our knowledge-based system from theHOO 2011 Pilot Round (Zesch, 2011).
We re-implemented two state-of-the-art approaches: the5code.google.com/p/jweb1t/6http://www.csie.ntu.edu.tw/?cjlin/libsvm/7http://mallet.cs.umass.edu/8http://opennlp.apache.org/9http://svmlight.joachims.org/10http://disi.unitn.it/moschitti/Tree-Kernel.htm11http://www.cs.waikato.ac.nz/ml/weka/303Name Description Range of Values / Examplespos?2?1The neighboring POS tags.
For the example weassume ?IN NNP DT NN VBD?.IN-NNPpos?2 INpos?1 NNPpos+1 NNpos+2 VBDpos+1+2 NN-VBDchunk?1Whether the neighboring tokens are part of a chunk.For the example, we assume ?in the [United States]?.Ochunk+1 Bchunk+2 Ichunk+1+2 B-Ivowel+1 Whether the next token starts with a vowel or not.
0/1cons+1 Whether the next token starts with a consonant or not.
0/1sign+1 Any sign that is not an alphabetic character.
0/1n-gram(t?1{x?
y})Let f(n-gram) be the frequency of the n-gram in acertain corpus.
All n-gram features are then computedas f(xt+1t+2)f(t+1t+2) ?f(yt+1t+2)f(t+1t+2)n-gram(?{the?
a} big house?
);f(?the big house?)
= 100;f(?a big house?)
= 50;f(?big house?)
= 1000;1001000 ?501000 = 0.05n-gram({x?
y}t+1)n-gram(t?1{x?
y}t+1)n-gram(t?2t?1{x?
y})n-gram({x?
y}t+1t+2)Table 1: List of features used for classification.knowledge-based approach (Hirst and Budanitsky,2005) and the statistical approach (Mays et al, 1991;Wilcox-OHearn et al, 2008).
Both approaches mea-sure the contextual fitness of a word and the sur-rounding context.
For that purpose, the knowledge-based approach computes the semantic relatednessof a target word with all other words in a certain con-text window.
This approach is not suitable for cor-recting article or preposition errors, as these wordclasses are not linked to the context via lexical-semantic relations.
Thus, we only use the statisti-cal approach that computes the probability of a sen-tence based on a n-gram language model.
We usethe Google Web1T n-gram data (Brants and Franz,2006).Although being generally applicable to articleand preposition errors, the statistical approach needssome adaptations in order to achieve acceptable per-formance.
In the original definition, the approachcomputes the probability of all alternative sentenceswhere the target word is replaced with a word fromthe vocabulary that has low edit distance to the tar-get word.
This results in a very high false detectionrate.
Thus, we (i) limit detections to positions wherean article or preposition is already present, and (ii)select the substitution candidate not from all tokenswith low edit distance to the original token, but onlyfrom the appropriate confusion set.As the statistical approach is purely based on n-gram frequencies, while this is only one feature ofthe supervised approach, we expect the supervisedapproach to outperform our adapted knowledge-based system by a wide margin.4 Experimental SetupWe model all experiment pipelines in the previouslydescribed framework.
As training data, we use thepublicly available Brown corpus (Francis W. Nelsonand Kuc?era, 1964), but limit training to 3,700 ran-domly selected sentences in order to speed up thetraining process.4.1 Unofficial RunsDue to technical problems, we were not able to sub-mit all runs in time.
We therefore report also unof-ficial runs which we evaluated on the test data thatwas available for participants for a limited amountof time.12 Although we did not tailor the unofficialruns in any way towards the test data, they have cer-tainly a different status than the official runs.
We donot consider this as a major problem, as our basic12HOO 2012 test data was subject to a strict license andneeded to be deleted after the evaluation period.304Detection Recognition CorrectionDescription Run P R F P R F P R FBaselinesAlways the - 7.09 6.13 6.58 7.09 6.13 6.58 2.00 1.69 1.81Always of - 11.51 28.54 16.40 11.51 28.54 16.40 1.53 3.81 2.19Unofficial2011 Articles; ?
= .005 - 9.62 8.47 9.00 9.62 8.47 9.00 0.96 0.85 0.902011 Prepositions; ?
= .005 - 18.11 23.04 20.28 18.11 23.04 20.28 9.00 11.42 10.052012 Naive Bayes - 9.35 39.32 15.11 9.35 39.32 15.11 1.26 5.29 2.032012 SVM - 10.46 33.40 15.93 10.46 33.40 15.93 2.71 8.67 4.13Official?RD = 0.95; ?RT = 0.8 UD0 8.64 7.73 8.16 4.94 4.42 4.66 1.48 1.32 1.40?RD = 0.8; ?RT = 0.7 UD1 8.36 15.45 10.85 4.18 7.73 5.43 1.19 2.21 1.55?RD = 0.5; ?RT = 0.3 UD2 8.94 31.13 13.88 5.51 19.21 8.57 1.20 4.19 1.87Table 2: HOO 2012 test data: Results (in %) for article and preposition errors combined.feature set is not competitive with the best perform-ing systems anyway.We implemented two baseline systems, one forarticles and one for prepositions.
The baselines re-place every occurrence of an article/preposition withthe most frequent article/preposition from the confu-sion set (the for articles, of for prepositions).We also apply the adapted HOO 2011 statisticalapproach in two versions as described above: oneadapted towards articles, and one adapted towardsprepositions.Finally, we use the new framework for supervisederror correction based on the basic feature set de-scribed above with two classifiers: Naive Bayes andSVM as implemented in the Weka toolkit version3.7.5.
We treat the correction task as a multi-classproblem and only target the error classes RD, RT,UD, UT.
The remaining error classes MD and MT(missing articles and prepositions) are more chal-lenging, as it is less obvious how to create goodtraining data from a non-error annotated corpus.4.2 Official RunsThe three runs that were officially submitted arealso based on the SVM implementation in Weka,but we applied the parameter ?BuildLogisticMod-els?
which allows to base a detection decision onthe confidence of the classifier in order to improveprecision.
We tuned parameters on the training dataand report three runs for the threshold combinations(?RD, ?RT ) = (0.95, 0.8), (0.8, 0.7), and (0.5, 0.3).5 ResultsTable 2 summarizes the results of all runs.
As ex-pected, the basic feature set used in our experimentsis not competitive with the top-performing systemsin the shared task.13 However, some observationscan be made from the relative differences betweenthe scores.
The thresholds applied in the official runsare not working as expected, as precision is not in-fluenced, while recall drops a lot.
The HOO 2011system based on the statistical approach performsquite well for prepositions, but not for articles.
Itsperformance is comparable to the supervised runs,but this is only due to the limited feature set used inour experiment.As mentioned above, our focus was to implementa highly flexible and modular system for supervisederror correction which can be easily augmented byother researchers.
We plan to model the most suc-cessful systems in our framework as soon as the sys-tem descriptions are made available, and we inviteother participating teams to help with this effort.
Thesystem might provide a level playground for subse-quent shared tasks and enable further progress in thisimportant field of research.AcknowledgmentsThis work has been supported by the VolkswagenFoundation as part of the Lichtenberg-ProfessorshipProgram under grant No.
I/82806.13The best performing systems achieve about 40% F-scorefor detection, 35% for recognition, and 28% for correction.
See(Dale et al, 2012) for an overview of the results.305ReferencesThorsten Brants and Alex Franz.
2006.
Web 1T 5-gramVersion 1.Andrew J Carlson, Jeffrey Rosen, and Dan Roth.
2001.Scaling Up Context-Sensitive Text Correction.
In Pro-ceedings of IAAI.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A Report on the Preposition andDeterminer Error Correction Shared Task.
In Proceed-ings of the Seventh Workshop on Innovative Use ofNLP for Building Educational Applications, Montreal,Canada.Richard Eckart de Castilho and Iryna Gurevych.
2011.A lightweight framework for reproducible parame-ter sweeping in information retrieval.
In Proceed-ings of the 2011 workshop on Data infrastructurEs forsupporting information retrieval evaluation (DESIRE?11), New York, NY, USA.
ACM.David Ferrucci and Adam Lally.
2004.
UIMA: An Ar-chitectural Approach to Unstructured Information Pro-cessing in the Corporate Research Environment.
Nat-ural Language Engineering, 10(3-4):327?348.Francis W. Nelson and Henry Kuc?era.
1964.
Man-ual of information to accompany a standard corpus ofpresent-day edited American English, for use with dig-ital computers.Andrew R. Golding and Yves Schabes.
1996.
Com-bining Trigram-based and feature-based methods forcontext-sensitive spelling correction.
In Proceedingsof the 34th annual meeting on Association for Com-putational Linguistics, pages 71?78, Morristown, NJ,USA.
Association for Computational Linguistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Update.SIGKDD Explorations, 11(1).Graeme Hirst and Alexander Budanitsky.
2005.
Correct-ing real-word spelling errors by restoring lexical cohe-sion.
Natural Language Engineering, 11(1):87?111,March.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In B. Scholkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods - Sup-port Vector Learning.Michael P Jones and James H Martin.
1997.
Contex-tual spelling correction using latent semantic analy-sis.
In Proceedings of the Fifth Conference on Ap-plied Natural Language Processing, pages 166?173,Morristown, NJ, USA.
Association for ComputationalLinguistics.Eric Mays, Fred.
J Damerau, and Robert L Mercer.
1991.Context based spelling correction.
Information Pro-cessing & Management, 27(5):517?522.Andrew Kachites McCallum.
2002.
MALLET: A Ma-chine Learning for Language Toolkit.Alessandro Moschitti.
2006.
Making tree kernels practi-cal for natural language learning.
In Proceedings ofthe Eleventh International Conference on EuropeanAssociation for Computational Linguistics, Trento,Italy.Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.2008.
ClearTK: A UIMA Toolkit for Statistical Nat-ural Language Processing.
In Towards EnhancedInteroperability for Large HLT Systems: UIMA forNLP workshop at Language Resources and EvaluationConference (LREC).Helmut Schmid.
2004.
Efficient Parsing of Highly Am-biguous Context-Free Grammars with Bit Vectors.
InProceedings of the 20th International Conference onComputational Linguistics (COLING 2004), Geneva,Switzerland.Amber Wilcox-OHearn, Graeme Hirst, and AlexanderBudanitsky.
2008.
Real-word spelling correction withtrigrams: A reconsideration of the Mays, Damerau,and Mercer model.
In Proceedings of the 9th inter-national conference on Computational linguistics andintelligent text processing (CICLing).Torsten Zesch.
2011.
Helping Our Own 2011: UKPLab System Description.
In Proceedings of the Help-ing Our Own Working Group Session at the 13th Eu-ropean Workshop on Natural Language Generation,pages 260?262.306
