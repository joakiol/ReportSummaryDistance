Proceedings of the 43rd Annual Meeting of the ACL, pages 443?450,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsPosition Specific Posterior Lattices for Indexing SpeechCiprian Chelba and Alex AceroMicrosoft ResearchMicrosoft CorporationOne Microsoft WayRedmond, WA 98052{chelba, alexac}@microsoft.comAbstractThe paper presents the Position SpecificPosterior Lattice, a novel representationof automatic speech recognition latticesthat naturally lends itself to efficient in-dexing of position information and subse-quent relevance ranking of spoken docu-ments using proximity.In experiments performed on a collec-tion of lecture recordings ?
MIT iCam-pus data ?
the spoken document rank-ing accuracy was improved by 20% rela-tive over the commonly used baseline ofindexing the 1-best output from an auto-matic speech recognizer.
The Mean Aver-age Precision (MAP) increased from 0.53when using 1-best output to 0.62 when us-ing the new lattice representation.
The ref-erence used for evaluation is the output ofa standard retrieval engine working on themanual transcription of the speech collec-tion.Albeit lossy, the PSPL lattice is also muchmore compact than the ASR 3-gram lat-tice from which it is computed ?
whichtranslates in reduced inverted index sizeas well ?
at virtually no degradation inword-error-rate performance.
Since newpaths are introduced in the lattice, the OR-ACLE accuracy increases over the origi-nal ASR lattice.1 IntroductionEver increasing computing power and connectivitybandwidth together with falling storage costs re-sult in an overwhelming amount of data of vari-ous types being produced, exchanged, and stored.Consequently, search has emerged as a key applica-tion as more and more data is being saved (Church,2003).
Text search in particular is the most activearea, with applications that range from web and in-tranet search to searching for private information re-siding on one?s hard-drive.Speech search has not received much attentiondue to the fact that large collections of untranscribedspoken material have not been available, mostlydue to storage constraints.
As storage is becomingcheaper, the availability and usefulness of large col-lections of spoken documents is limited strictly bythe lack of adequate technology to exploit them.Manually transcribing speech is expensive andsometimes outright impossible due to privacy con-cerns.
This leads us to exploring an automatic ap-proach to searching and navigating spoken docu-ment collections.Our current work aims at extending the standardkeyword search paradigm from text documents tospoken documents.
In order to deal with limitationsof current automatic speech recognition (ASR) tech-nology we propose an approach that uses recogni-tion lattices ?
which are considerably more accu-rate than the ASR 1-best output.A novel contribution is the use of a representationof ASR lattices which retains only position informa-tion for each word.
The Position Specific Posterior443Lattice (PSPL) is a lossy but compact representa-tion of a speech recognition lattice that lends itselfto the standard inverted indexing done in text search?
which retains the position as well as other con-textual information for each hit.Since our aim is to bridge the gap between textand speech -grade search technology, we take as ourreference the output of a text retrieval engine thatruns on the manual transcription.The rest of the paper is structured as follows: inthe next section we review previous work in thearea, followed by Section 3 which presents a briefoverview of state-of-the-art text search technology.We then introduce the PSPL representation in Sec-tion 4 and explain its use for indexing and searchingspeech in the next section.
Experiments evaluatingASR accuracy on iCampus, highlighting empiricalaspects of PSPL lattices as well as search accuracyresults are reported in Section 6.
We conclude byoutlining future work.2 Previous WorkThe main research effort aiming at spoken docu-ment retrieval (SDR) was centered around the SDR-TREC evaluations (Garofolo et al, 2000), althoughthere is a large body of work in this area prior tothe SDR-TREC evaluations, as well as more recentwork outside this community.
Most notable are thecontributions of (Brown et al, 1996) and (James,1995).One problem encountered in work published prioror outside the SDR-TREC community is that itdoesn?t always evaluate performance from a doc-ument retrieval point of view ?
using a metriclike Mean Average Precision (MAP) or similar, seetrec_eval (NIST, www) ?
but rather uses word-spotting measures, which are more technology-rather than user- centric.
We believe that ultimatelyit is the document retrieval performance that mattersand the word-spotting accuracy is just an indicatorfor how a SDR system might be improved.The TREC-SDR 8/9 evaluations ?
(Garofolo etal., 2000) Section 6 ?
focused on using BroadcastNews speech from various sources: CNN, ABC,PRI, Voice of America.
About 550 hrs of speechwere segmented manually into 21,574 stories eachcomprising about 250 words on the average.
Theapproximate manual transcriptions ?
closed cap-tioning for video ?
used for SDR system compar-ison with text-only retrieval performance had fairlyhigh WER: 14.5% video and 7.5% radio broadcasts.ASR systems tuned to the Broadcast News domainwere evaluated on detailed manual transcriptionsand were able to achieve 15-20% WER, not far fromthe accuracy of the approximate manual transcrip-tions.
In order to evaluate the accuracy of retrievalsystems, search queries ??topics?
?
along with bi-nary relevance judgments were compiled by humanassessors.SDR systems indexed the ASR 1-best output andtheir retrieval performance ?
measured in terms ofMAP ?
was found to be flat with respect to ASRWER variations in the range of 15%-30%.
Simplyhaving a common task and an evaluation-driven col-laborative research effort represents a huge gain forthe community.
There are shortcomings however tothe SDR-TREC framework.It is well known that ASR systems are very brit-tle to mismatched training/test conditions and it isunrealistic to expect error rates in the range 10-15%when decoding speech mismatched with respect tothe training data.
It is thus very important to con-sider ASR operating points which have higher WER.Also, the out-of-vocabulary (OOV) rate was verylow, below 1%.
Since the ?topics?/queries werelong and stated in plain English rather than usingthe keyword search paradigm, the query-side OOV(Q-OOV) was very low as well, an unrealistic situ-ation in practice.
(Woodland et al, 2000) evaluatesthe effect of Q-OOV rate on retrieval performanceby reducing the ASR vocabulary size such that theQ-OOV rate comes closer to 15%, a much more re-alistic figure since search keywords are typically rarewords.
They show severe degradation in MAP per-formance ?
50% relative, from 44 to 22.The most common approach to dealing with OOVquery words is to represent both the query and thespoken document using sub-word units ?
typicallyphones or phone n-grams ?
and then match se-quences of such units.
In his thesis, (Ng, 2000)shows the feasibility of sub-word SDR and advo-cates for tighter integration between ASR and IRtechnology.
Similar conclusions are drawn by theexcellent work in (Siegler, 1999).As pointed out in (Logan et al, 2002), word level444indexing and querying is still more accurate, wereit not for the OOV problem.
The authors argue infavor of a combination of word and sub-word levelindexing.
Another problem pointed out by the pa-per is the abundance of word-spotting false-positivesin the sub-word retrieval case, somewhat masked bythe MAP measure.Similar approaches are taken by (Seide and Yu,2004).
One interesting feature of this work is a two-pass system whereby an approximate match is car-ried out at the document level after which the costlydetailed phonetic match is carried out on only 15%of the documents in the collection.More recently, (Saraclar and Sproat, 2004) showsimprovement in word-spotting accuracy by usinglattices instead of 1-best.
An inverted index fromsymbols ?
word or phone ?
to links allows toevaluate adjacency of query words but more gen-eral proximity information is harder to obtain ?
seeSection 4.
Although no formal comparison has beencarried out, we believe our approach should yield amore compact index.Before discussing our architectural design deci-sions it is probably useful to give a brief presentationof a state-of-the-art text document retrieval enginethat is using the keyword search paradigm.3 Text Document RetrievalProbably the most widespread text retrieval model isthe TF-IDF vector model (Baeza-Yates and Ribeiro-Neto, 1999).
For a given query Q = q1 .
.
.
qi .
.
.
qQand document Dj one calculates a similarity mea-sure by accumulating the TF-IDF score wi,j for eachquery term qi, possibly weighted by a document spe-cific weight:S(Dj ,Q) =Q?i=1wi,jwi,j = fi,j ?
idfiwhere fi,j is the normalized frequency of word qi indocument Dj and the inverse document frequencyfor query term qi is idfi = log Nni where N is thetotal number of documents in the collection and niis the number of documents containing qi.The main criticism to the TF-IDF relevance scoreis the fact that the query terms are assumed to beindependent.
Proximity information is not taken intoaccount at all, e.g.
whether the words LANGUAGEand MODELING occur next to each other or not ina document is not used for relevance scoring.Another issue is that query terms may be encoun-tered in different contexts in a given document: ti-tle, abstract, author name, font size, etc.
For hy-pertext document collections even more context in-formation is available: anchor text, as well as othermark-up tags designating various parts of a givendocument being just a few examples.
The TF-IDFranking scheme completely discards such informa-tion although it is clearly important in practice.3.1 Early Google ApproachAside from the use of PageRank for relevance rank-ing, (Brin and Page, 1998) also uses both proxim-ity and context information heavily when assigninga relevance score to a given document ?
see Sec-tion 4.5.1 of (Brin and Page, 1998) for details.For each given query term qi one retrieves the listof hits corresponding to qi in document D. Hitscan be of various types depending on the context inwhich the hit occurred: title, anchor text, etc.
Eachtype of hit has its own type-weight and the type-weights are indexed by type.For a single word query, their ranking algorithmtakes the inner-product between the type-weightvector and a vector consisting of count-weights (ta-pered counts such that the effect of large counts isdiscounted) and combines the resulting score withPageRank in a final relevance score.For multiple word queries, terms co-occurring in agiven document are considered as forming differentproximity-types based on their proximity, from adja-cent to ?not even close?.
Each proximity type comeswith a proximity-weight and the relevance score in-cludes the contribution of proximity information bytaking the inner product over all types, including theproximity ones.3.2 Inverted IndexOf essence to fast retrieval on static document col-lections of medium to large size is the use of an in-verted index.
The inverted index stores a list of hitsfor each word in a given vocabulary.
The hits aregrouped by document.
For each document, the listof hits for a given query term must include position?
needed to evaluate counts of proximity types ?445as well as all the context information needed to cal-culate the relevance score of a given document us-ing the scheme outlined previously.
For details, thereader is referred to (Brin and Page, 1998), Sec-tion 4.4 Position Specific Posterior LatticesAs highlighted in the previous section, position in-formation is crucial for being able to evaluate prox-imity information when assigning a relevance scoreto a given document.In the spoken document case however, we arefaced with a dilemma.
On one hand, using 1-bestASR output as the transcription to be indexed is sub-optimal due to the high WER, which is likely to leadto low recall ?
query terms that were in fact spo-ken are wrongly recognized and thus not retrieved.On the other hand, ASR lattices do have much bet-ter WER ?
in our case the 1-best WER was 55%whereas the lattice WER was 30% ?
but the posi-tion information is not readily available: it is easy toevaluate whether two words are adjacent but ques-tions about the distance in number of links betweenthe occurrences of two query words in the lattice arevery hard to answer.The position information needed for recording agiven word hit is not readily available in ASR lat-tices ?
for details on the format of typical ASRlattices and the information stored in such latticesthe reader is referred to (Young et al, 2002).
Tosimplify the discussion let?s consider that a tradi-tional text-document hit for given word consists ofjust (document id, position).The occurrence of a given word in a lattice ob-tained from a given spoken document is uncertainand so is the position at which the word occurs inthe document.The ASR lattices do contain the informationneeded to evaluate proximity information, since on agiven path through the lattice we can easily assign aposition index to each link/word in the normal way.Each path occurs with a given posterior probability,easily computable from the lattice, so in principleone could index soft-hits which specify(document id, position,posterior probability)for each word in the lattice.
Since it is likely thats_1s_is_qnP(l_1)P(l_i)P(l_q)Figure 1: State Transitionsmore than one path contains the same word in thesame position, one would need to sum over all pos-sible paths in a lattice that contain a given word at agiven position.A simple dynamic programming algorithm whichis a variation on the standard forward-backward al-gorithm can be employed for performing this com-putation.
The computation for the backward passstays unchanged, whereas during the forward passone needs to split the forward probability arrivingat a given node n, ?n, according to the length l ?measured in number of links along the partial paththat contain a word; null (?)
links are not countedwhen calculating path length ?
of the partial pathsthat start at the start node of the lattice and end atnode n:?n[l] .=?pi:end(pi)=n,length(pi)=lP (pi)The backward probability ?n has the standard defi-nition (Rabiner, 1989).To formalize the calculation of the position-specific forward-backward pass, the initialization,and one elementary forward step in the forward passare carried out using Eq.
(1), respectively ?
see Fig-ure 1 for notation:?n[l + 1] =q?i=1?si [l + ?
(li, ?)]
?
P (li)?start[l] ={1.0, l = 00.0, l 6= 0 (1)The ?probability?
P (li) of a given link li is storedas a log-probability and commonly evaluated inASR using:logP (li) = FLATw ?
[1/LMw ?
logPAM (li)+logPLM (word(li))?
1/LMw ?
logPIP ] (2)446where logPAM (li) is the acoustic model score,logPLM (word(li)) is the language model score,LMw > 0 is the language model weight, logPIP >0 is the ?insertion penalty?
and FLATw is a flat-tening weight.
In N -gram lattices where N ?
2,all links ending at a given node n must contain thesame word word(n), so the posterior probability ofa given word w occurring at a given position l canbe easily calculated using:P (w, l|LAT ) =?n s.t.
?n[l]??n>0?n[l]?
?n?start ?
?
(w,word(n))The Position Specific Posterior Lattice (PSPL) is arepresentation of the P (w, l|LAT ) distribution: foreach position bin l store the words w along with theirposterior probability P (w, l|LAT ).5 Spoken Document Indexing and SearchUsing PSPLSpoken documents rarely contain only speech.
Of-ten they have a title, author and creation date.
Theremight also be a text abstract associated with thespeech, video or even slides in some standard for-mat.
The idea of saving context information whenindexing HTML documents and web pages can thusbe readily used for indexing spoken documents, al-though the context information is of a different na-ture.As for the actual speech content of a spoken doc-ument, the previous section showed how ASR tech-nology and PSPL lattices can be used to automati-cally convert it to a format that allows the indexingof soft hits ?
a soft index stores posterior proba-bility along with the position information for termoccurrences in a given document.5.1 Speech Content Indexing Using PSPLSpeech content can be very long.
In our case thespeech content of a typical spoken document was ap-proximately 1 hr long; it is customary to segment agiven speech file in shorter segments.A spoken document thus consists of an orderedlist of segments.
For each segment we generate acorresponding PSPL lattice.
Each document andeach segment in a given collection are mapped to aninteger value using a collection descriptor file whichlists all documents and segments.
Each soft hit inour index will store the PSPL position and posteriorprobability.5.2 Speech Content Relevance Ranking UsingPSPL RepresentationConsider a given query Q = q1 .
.
.
qi .
.
.
qQ anda spoken document D represented as a PSPL.
Ourranking scheme follows the description in Sec-tion 3.1.The words in the document D clearly belong tothe ASR vocabulary V whereas the words in thequery may be out-of-vocabulary (OOV).
As arguedin Section 2, the query-OOV rate is an importantfactor in evaluating the impact of having a finiteASR vocabulary on the retrieval accuracy.
We as-sume that the words in the query are all containedin V; OOV words are mapped to UNK and cannot bematched in any document D.For all query terms, a 1-gram score is calculatedby summing the PSPL posterior probability acrossall segments s and positions k. This is equivalentto calculating the expected count of a given queryterm qi according to the PSPL probability distribu-tion P (wk(s)|D) for each segment s of documentD.
The results are aggregated in a common valueS1?gram(D,Q):S(D, qi) = log[1 +?s?kP (wk(s) = qi|D)]S1?gram(D,Q) =Q?i=1S(D, qi) (3)Similar to (Brin and Page, 1998), the logarithmic ta-pering off is used for discounting the effect of largecounts in a given document.Our current ranking scheme takes into accountproximity in the form of matching N -grams presentin the query.
Similar to the 1-gram case, we cal-culate an expected tapered-count for each N-gramqi .
.
.
qi+N?1 in the query and then aggregate the re-sults in a common value SN?gram(D,Q) for eachorder N :S(D, qi .
.
.
qi+N?1) = (4)log[1 +?s?k?N?1l=0 P (wk+l(s) = qi+l|D)]SN?gram(D,Q) =Q?N+1?i=1S(D, qi .
.
.
qi+N?1)447The different proximity types, one for each N -gram order allowed by the query length, are com-bined by taking the inner product with a vector ofweights.S(D,Q) =Q?N=1wN ?
SN?gram(D,Q) (5)Only documents containing all the terms in thequery are returned.
In the current implementationthe weights increase linearly with the N-gram order.Clearly, better weight assignments must exist, andas the hit types are enriched beyond using just N -grams, the weights will have to be determined usingmachine learning techniques.It is worth noting that the transcription for anygiven segment can also be represented as a PSPLwith exactly one word per position bin.
It is easy tosee that in this case the relevance scores calculatedaccording to Eq.
(3-4) are the ones specified by 3.1.6 ExperimentsWe have carried all our experiments on the iCampuscorpus prepared by MIT CSAIL.
The main advan-tages of the corpus are: realistic speech recordingconditions ?
all lectures are recorded using a lapelmicrophone ?
and the availability of accurate man-ual transcriptions ?
which enables the evaluation ofa SDR system against its text counterpart.6.1 iCampus CorpusThe iCampus corpus (Glass et al, 2004) consistsof about 169 hours of lecture materials: 20 Intro-duction to Computer Programming Lectures (21.7hours), 35 Linear Algebra Lectures (27.7 hours), 35Electro-magnetic Physics Lectures (29.1 hours), 79Assorted MIT World seminars covering a wide vari-ety of topics (89.9 hours).
Each lecture comes witha word-level manual transcription that segments thetext into semantic units that could be thought of assentences; word-level time-alignments between thetranscription and the speech are also provided.
Thespeech style is in between planned and spontaneous.The speech is recorded at a sampling rate of 16kHz(wide-band) using a lapel microphone.The speech was segmented at the sentence levelbased on the time alignments; each lecture is consid-ered to be a spoken document consisting of a set ofone-sentence long segments determined this way ?see Section 5.1.
The final collection consists of 169documents, 66,102 segments and an average docu-ment length of 391 segments.We have then used a standard large vocabularyASR system for generating 3-gram ASR lattices andPSPL lattices.
The 3-gram language model used fordecoding is trained on a large amount of text data,primarily newswire text.
The vocabulary of the ASRsystem consisted of 110kwds, selected based on fre-quency in the training data.
The acoustic modelis trained on a variety of wide-band speech and itis a standard clustered tri-phone, 3-states-per-phonemodel.
Neither model has been tuned in any way tothe iCampus scenario.On the first lecture L01 of the Introduction toComputer Programming Lectures the WER of theASR system was 44.7%; the OOV rate was 3.3%.For the entire set of lectures in the Introductionto Computer Programming Lectures, the WER was54.8%, with a maximum value of 74% and a mini-mum value of 44%.6.2 PSPL latticesWe have then proceeded to generate 3-gram latticesand PSPL lattices using the above ASR system.
Ta-ble 1 compares the accuracy/size of the 3-gram lat-tices and the resulting PSPL lattices for the first lec-ture L01.
As it can be seen the PSPL represen-Lattice Type 3-gram PSPLSize on disk 11.3MB 3.2MBLink density 16.3 14.6Node density 7.4 1.11-best WER 44.7% 45%ORACLE WER 26.4% 21.7%Table 1: Comparison between 3-gram and PSPL lat-tices for lecture L01 (iCampus corpus): node andlink density, 1-best and ORACLE WER, size on disktation is much more compact than the original 3-gram lattices at a very small loss in accuracy: the1-best path through the PSPL lattice is only 0.3%absolute worse than the one through the original 3-gram lattice.
As expected, the main reduction comesfrom the drastically smaller node density ?
7 timessmaller, measured in nodes per word in the refer-ence transcription.
Since the PSPL representation448introduces new paths compared to the original 3-gram lattice, the ORACLE WER path ?
least error-ful path in the lattice ?
is also about 20% relativebetter than in the original 3-gram lattice ?
5% ab-solute.
Also to be noted is the much better WER inboth PSPL/3-gram lattices versus 1-best.6.3 Spoken Document RetrievalOur aim is to narrow the gap between speech andtext document retrieval.
We have thus taken as ourreference the output of a standard retrieval engineworking according to one of the TF-IDF flavors, seeSection 3.
The engine indexes the manual transcrip-tion using an unlimited vocabulary.
All retrieval re-sults presented in this section have used the stan-dard trec_eval package used by the TREC eval-uations.The PSPL lattices for each segment in the spo-ken document collection were indexed as explainedin 5.1.
In addition, we generated the PSPL repre-sentation of the manual transcript and of the 1-bestASR output and indexed those as well.
This allowsus to compare our retrieval results against the resultsobtained using the reference engine when workingon the same text document collection.6.3.1 Query Collection and Retrieval SetupThe missing ingredient for performing retrievalexperiments are the queries.
We have asked a fewcolleagues to issue queries against a demo shell us-ing the index built from the manual transcription.The only information1 provided to them was thesame as the summary description in Section 6.1.We have collected 116 queries in this manner.
Thequery out-of-vocabulary rate (Q-OOV) was 5.2%and the average query length was 1.97 words.
Sinceour approach so far does not index sub-word units,we cannot deal with OOV query words.
We havethus removed the queries which contained OOVwords ?
resulting in a set of 96 queries ?
whichclearly biases the evaluation.
On the other hand, theresults on both the 1-best and the lattice indexes areequally favored by this.1Arguably, more motivated users that are also more famil-iar with the document collection would provide a better querycollection framework6.3.2 Retrieval ExperimentsWe have carried out retrieval experiments in theabove setup.
Indexes have been built from:?
trans: manual transcription filtered throughASR vocabulary?
1-best: ASR 1-best output?
lat: PSPL lattices.No tuning of retrieval weights, see Eq.
(5), or linkscoring weights, see Eq.
(2) has been performed.
Ta-ble 2 presents the results.
As a sanity check, the re-trieval results on transcription ?
trans ?
matchalmost perfectly the reference.
The small differencecomes from stemming rules that the baseline engineis using for query enhancement which are not repli-cated in our retrieval engine.
The results on lat-tices (lat) improve significantly on (1-best) ?20% relative improvement in mean average preci-sion (MAP).trans 1-best lat# docs retrieved 1411 3206 4971# relevant docs 1416 1416 1416# rel retrieved 1411 1088 1301MAP 0.99 0.53 0.62R-precision 0.99 0.53 0.58Table 2: Retrieval performance on indexes builtfrom transcript, ASR 1-best and PSPL lattices, re-spectively6.3.3 Why Would This Work?A legitimate question at this point is: why wouldanyone expect this to work when the 1-best ASR ac-curacy is so poor?In favor of our approach, the ASR lattice WER ismuch lower than the 1-best WER, and PSPL haveeven lower WER than the ASR lattices.
As re-ported in Table 1, the PSPL WER for L01 was22% whereas the 1-best WER was 45%.
Considermatching a 2-gram in the PSPL ?the average querylength is indeed 2 wds so this is a representative sit-uation.
A simple calculation reveals that it is twice?
(1 ?
0.22)2/(1 ?
0.45)2 = 2 ?
more likely tofind a query match in the PSPL than in the 1-best ?if the query 2-gram was indeed spoken at that posi-tion.
According to this heuristic argument one couldexpect a dramatic increase in Recall.
Another aspect449is that people enter typical N-grams as queries.
Thecontents of adjacent PSPL bins are fairly random innature so if a typical 2-gram is found in the PSPL,chances are it was actually spoken.
This translatesin little degradation in Precision.7 Conclusions and Future workWe have developed a new representation for ASRlattices ?
the Position Specific Posterior Lattice(PSPL) ?
that lends itself naturally to indexingspeech content and integrating state-of-the-art IRtechniques that make use of proximity and contextinformation.
In addition, the PSPL representation isalso much more compact at no loss in WER ?
both1-best and ORACLE.The retrieval results obtained by indexing thePSPL and performing adequate relevance rankingare 20% better than when using the ASR 1-best out-put, although still far from the performance achievedon text data.The experiments presented in this paper are trulya first step.
We plan to gather a much larger num-ber of queries.
The binary relevance judgments ?
agiven document is deemed either relevant or irrele-vant to a given query in the reference ?ranking?
?assumed by the standard trec_eval tool are alsoa serious shortcoming; a distance measure betweenrankings of documents needs to be used.
Finally, us-ing a baseline engine that in fact makes use of prox-imity and context information is a priority if suchinformation is to be used in our algorithms.8 AcknowledgmentsWe would like to thank Jim Glass and T J Hazen atMIT for providing the iCampus data.
We would alsolike to thank Frank Seide for offering valuable sug-gestions and our colleagues for providing queries.ReferencesRicardo Baeza-Yates and Berthier Ribeiro-Neto, 1999.Modern Information Retrieval, chapter 2, pages 27?30.
Addison Wesley, New York.Sergey Brin and Lawrence Page.
1998.
The anatomy ofa large-scale hypertextual Web search engine.
Com-puter Networks and ISDN Systems, 30(1?7):107?117.M.
G. Brown, J. T. Foote, G. J. F. Jones, K. Spa?rck Jones,and S. J.
Young.
1996.
Open-vocabulary speech in-dexing for voice and video mail retrieval.
In Proc.ACM Multimedia 96, pages 307?316, Boston, Novem-ber.Kenneth Ward Church.
2003.
Speech and language pro-cessing: Where have we been and where are we going?In Proceedings of Eurospeech, Geneva, Switzerland.J.
Garofolo, G. Auzanne, and E. Voorhees.
2000.
TheTREC spoken document retrieval track: A successstory.
In Proceedings of the Recherche d?InformationsAssiste par Ordinateur: ContentBased Multimedia In-formation Access Conference, April.James Glass, T. J. Hazen, Lee Hetherington, and ChaoWang.
2004.
Analysis and processing of lecture audiodata: Preliminary investigations.
In HLT-NAACL 2004Workshop: Interdisciplinary Approaches to SpeechIndexing and Retrieval, pages 9?12, Boston, Mas-sachusetts, May.David Anthony James.
1995.
The Application of Classi-cal Information Retrieval Techniques to Spoken Docu-ments.
Ph.D. thesis, University of Cambridge, Down-ing College.B.
Logan, P. Moreno, and O. Deshmukh.
2002.
Wordand sub-word indexing approaches for reducing the ef-fects of OOV queries on spoken audio.
In Proc.
HLT.Kenney Ng.
2000.
Subword-Based Approaches for Spo-ken Document Retrieval.
Ph.D. thesis, MassachusettsInstitute of Technology.NIST.
www.
The TREC evaluation package.
In www-nlpir.nist.gov/projects/trecvid/trecvid.tools/trec eval.L.
R. Rabiner.
1989.
A tutorial on hidden markov mod-els and selected applications in speech recognition.
InProceedings IEEE, volume 77(2), pages 257?285.Murat Saraclar and Richard Sproat.
2004.
Lattice-basedsearch for spoken utterance retrieval.
In HLT-NAACL2004, pages 129?136, Boston, Massachusetts, May.F.
Seide and P. Yu.
2004.
Vocabulary-independent searchin spontaneous speech.
In Proceedings of ICASSP,Montreal, Canada.Matthew A. Siegler.
1999.
Integration of ContinuousSpeech Recognition and Information Retrieval for Mu-tually Optimal Performance.
Ph.D. thesis, CarnegieMellon University.P.
C. Woodland, S. E. Johnson, P. Jourlin, and K. Spa?rckJones.
2000.
Effects of out of vocabulary words inspoken document retrieval.
In Proceedings of SIGIR,pages 372?374, Athens, Greece.Steve Young, Gunnar Evermann, Thomas Hain, DanKershaw, Gareth Moore, Julian Odell, Dan PoveyDave Ollason, Valtcho Valtchev, and Phil Woodland.2002.
The HTK Book.
Cambridge University Engi-neering Department, Cambridge, England, December.450
