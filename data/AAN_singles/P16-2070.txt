Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 432?437,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsUsing Mention Accessibility to Improve Coreference ResolutionKellie Webster and Joel NothmanSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australia{kellie.webster, jnothman}@sydney.edu.auAbstractModern coreference resolution systems re-quire linguistic and general knowledgetypically sourced from costly, manuallycurated resources.
Despite their intuitiveappeal, results have been mixed.
In thiswork, we instead implement fine-grainedsurface-level features motivated by cogni-tive theory.
Our novel fine-grained featurespecialisation approach significantly im-proves the performance of a strong base-line, achieving state-of-the-art results of65.29 and 61.13% on CoNLL-2012 usinggold and automatic preprocessing, withsystem extracted mentions.1 IntroductionCoreference resolution (Pradhan et al, 2011,2012) is the task of clustering mentions in a docu-ment according to their referent.
For instance, weneed to resolve Ehud Barak, his, and he as corefer-ential to understand the meaning of the excerpt:Israeli Prime Minister Ehud Barak called his cabinet intospecial session late Wednesday , to discuss what he called agrave escalation of the level of violence ...While knowledge-poor approaches establish areasonable baseline, they perform poorly when po-sitional and surface form heuristics break down.To address this, research has extracted worldknowledge from manually curated resources in-cluding Wikipedia, Yago, Freebase, and FrameNet(e.g.
Uryupina et al, 2011; Rahman and Ng, 2011;Ratinov and Roth, 2012; Hajishirzi et al, 2013;Durrett and Klein, 2014).
Despite their intuitiveappeal, results have been mixed.
We instead focuson linguistic knowledge which can be extractedcompletely automatically, guided by insights fromAccessibility theory (Ariel, 2001).
This result isconsistent with Wiseman et al (2015) which sim-ilarly finds performance gains above state-of-the-art from extending simple, surface-level features.We implement a mention classification schemebased on the Accessibility hierarchy and use thisfor feature specialisation, yielding state-of-the-artresults of 65.29 and 61.13% on CoNLL-2012 ongold and automatic preprocessing, with system ex-tracted mentions.
Our approach is simple and ef-fective, contributing to arguments for incorporat-ing cognitive insights in computational modelling.2 Accessibility HierarchyAccessibility theory (Ariel, 2001) builds on a bodyof cognitively motivated theories of discourse pro-cessing, notably Centering Theory (Grosz et al,1995).
Where Centering describes pronoun in-terpretation in terms of relative discourse en-tity salience, Accessibility theory expands fromthis, describing discourse entities as having corre-sponding human memory nodes which fluctuate intheir degree of activation as the entity features ina discourse.
The surface form of a reference indi-cates to the hearer how activated its correspondingnode is expected to be.
That is, surface form is aninstruction for how to retrieve suitable referents,guiding the resolution of coreference.
Relative de-gree of activation is captured in the theory?s hier-archy of reference expression types, reproduced inFigure 1.
Section 4 proposes a mapping of thishierarchy (derived for spoken Hebrew) to writtenEnglish.The hierarchy encodes and expands the widely-used rule of thumb that full names introduce an en-tity (their referent has low accessibility; it has notyet been discussed) and pronouns are anaphoric(their referent is a highly accessible, active dis-432Full name + modifier < Full name < Long definite description < Short definite description < Last name < First name <Distal demonstrative + modifier < Proximate demonstrative + modifier < Distal demonstrative + NP < Proximatedemonstrative + NP < Distal demonstrative < Proximate demonstrative < Stressed pronoun + gesture < Stressed pronoun <Unstressed pronoun < Cliticised pronoun < Verbal inflections < ZeroFigure 1: Accessibility hierarchy from Ariel (2001)course entity); the accessibility of definite descrip-tions is intermediate.
In this work, we show thatthe fine-grained categorisation in the Accessibil-ity hierarchy can be leveraged to improve the dis-criminative power of a strong system, comparedto coarser-grained typologies from previous work.That is, this work contributes valuable empiricalsupport for the psycholinguistic theory.3 Related WorkA particularly successful way to leverage men-tion classification has been to specialise mod-elling by mention type.
Denis and Baldridge(2008) learn five different models, one each forproper name, definite nominal, indefinite nomi-nal, third person pronoun, and non-third personpronoun.
Bengtson and Roth (2008) and Dur-rett and Klein (2013) implement specialisationat the level of features within a model, ratherthan explicitly learning separate models.
Bengt-son and Roth (2008) prefix each base featuregenerated with the type of the current mention,one of proper name, nominal, or pronoun, forinstance nominal-head match:true.
Dur-rett and Klein (2013) extend from this bylearning a model over three versions of eachbase feature: unprefixed, conjoined with thetype of the current mention, and conjoinedwith concatenation of the types of the cur-rent mention and candidate antecedent mention:nominal+nominal-head match=true.The success of Durrett and Klein is possi-ble due to the large training dataset provided byOntoNotes (Pradhan et al, 2007).
In this work,we successfully extend data-driven specialisationstill further: Section 4 shows how we can dis-cover fine-grained patterns in reference expressionusage, and Section 5 how these patterns can beused to significantly improve the performance ofa strong coreference system.4 Accessibility Transitions in OntoNotesIn this section, we propose an implementationof the Accessibility hierarchy for written En-AR Description %1 Multi-word name + modifier 7.72 Multi-word name 8.73 Long indefinite description 18.94 Short indefinite description 16.35 Long definite description 10.26 Short definite description 5.07 Single-word name 8.88 Distal demonstrative + modifier 0.29 Proximate demonstrative + modifier 0.010 Distal demonstrative + NP 0.711 Proximate demonstrative + NP 1.212 Distal demonstrative 0.813 Proximate demonstrative 0.514 Pronoun 21.0- Zero -Table 1: Accessibility rank values used in our ex-periments, with their base distribution over ex-tracted NPs.glish and how this can be used to encode fine-grained discourse transitions.
We discover trendsin OntoNotes, over mentions automatically ex-tracted from the DEV portion of English CoNLL-2012 (Pradhan et al, 2011).4.1 Mention classificationOur experiments start by classifying a mention?sAccessibility rank value, AR.
Table 1 gives theschema we propose for written English, along withthe base distribution over extracted mentions.
Thismapping is a simple ordinal numbering of Figure 1with the following refinements.We have generalised last name and first nameto single-word name (AR = 7) and full name tomulti-word name (AR = 2) to handle non-personentities.
Name modifiers are tokens without thehead NER label, excluding determiners, possessivemarkers, and punctuation.
We have introducedindefinite descriptions above definite descriptionssince they are more likely to introduce discourseentities than definite descriptions are.
We labelany nominal started by the or a possessive pro-noun as a definite; otherwise it is indefinite.
Longdescriptions comprise more than one token whenpossessive markers, punctuation, and articles areexcluded.
Distals start with those or that while433Table 2: Accessibility transitions (>0.05) CoNLL-2012 DEV.proximates start with these or this.4.2 Discourse TransitionsDiscourse transitions are then AR tuples whosevalues come from mentions aligned to the samegold cluster.
We chose 2-tuples, whose val-ues come from mention-antecedent pairs, sincemention-pair models have dominated the researchspace.
However, we generate up to three pairs permention since antecedents are latent at the entitylevel.
That is, for he in the following, we generatepairs (1, 14) and (14, 14).Israeli Prime Minister Ehud BarakAR=1called hisAR=14cabinet into special session late Wednesday , to discuss whatheAR=14called a grave escalation of the level of violence ...The aggregated counts for each pair type arerepresented in Table 2, with AR(antecedent) onthe vertical and AR(anaphor) on the horizontal.The first column gives the proportion of cluster-initial mentions of each AR type (e.g.
21% of goldclusters have a long indefinite description as theirfirst mention).
Each row is normalised to sum to1 so each row indicates the probability distribu-tion for the expected next mention of a cluster.
Forclarity, only values 0.05 and higher are shown.We can see that commonly used rules of thumbare borne out in this data, though with some ex-tra granularity.
Modified and multi-word namesreduce to single-word names, and both reduce topronouns.
Single word names retain their men-tion form and reduce to pronouns with roughlyequal probability.
All mention types reduce to bepronouns and, once reference has reduced to bepronominal, there is a high likelihood (82%) thatthis form will be retained.Encouragingly, we can also see transitions inTable 3: Proportion of singletons by AR.Table 2 can not be expressed with the coarser-grained typologies of prior work.
Firstly, men-tion article is important.
Long indefinite descrip-tions are more likely to start coreference clustersthan long definite descriptions (21% vs. 14%),which are in turn much more likely to start clus-ters than demonstratives.
Mention length is alsoimportant: short indefinite descriptions are morelikely to reduce to pronouns than long definitedescriptions and short definite descriptions havea higher chance of being retained throughout thediscourse than long definite descriptions.
Explor-ing further, of coreferential pairs where both men-tions are short definite descriptions, 86% are headmatched, compared to 60% of long definite de-scriptions; 60% of short definite descriptions arestring matched, compared to 27% of long.4.3 AnaphoricityTable 3 gives the proportion of extracted mentionswhich can not be aligned to gold mentions, byAR value.
Modelling these discourse singletons isimportant for models jointly learning coreferenceand anaphoricity (Webster and Curran, 2014).434Gold AutoMUC B3CEAFE CoNLL MUC B3CEAFE CoNLLFernandes et al (2012) 72.18 59.17 55.72 62.36 70.51 57.58 53.86 60.65Bj?orkelund and Kuhn (2014) 73.80 62.00 59.06 64.95 70.72 58.58 55.61 61.63LIMERIC Baseline 74.07 60.91 58.57 64.52 70.36 56.60 54.42 60.46+ Fine-Grained Specialisation 74.73 61.72 59.43 65.29 70.72 57.40 55.26 61.13Table 4: Performance on CoNLL-2012 TEST evaluated with gold and automatic annotations and systemextracted mentions.After pronouns, demonstratives and propernames have low proportion of singletons.
Sin-gle word names are less likely to be singletonsthan modified and multi-word names.
We high-light two contributing factors.
The first is that cer-tain names, particularly the children of an apposi-tion, are not markable in OntoNotes.
The secondis that the burden of supplying disambiguation willbe more worthwhile for important entities.Consistent with Recasens et al (2013), indef-inites are more likely to be singletons than defi-nites, and long definites are more likely than shortdefinites.
Since length and article are the key fac-tors for AR typing, this is good evidence in favourof using the hierarchy?s fine-grained classification.5 ExperimentsIn this section, we show how fine-grained featurespecialisation can significantly improve the per-formance of LIMERIC, a competitive coreferenceresolution system.
This strength demonstrates thatsimple surface-form features have yet to be fullyutilised in current modelling, and that cognitivetheory can guide their development.5.1 LIMERICThe system we base our work on is LIMERIC(Webster and Curran, 2014).
We choose this sys-tem due to its cognitive motivation and strongperformance.
Importantly, this system alreadyuses the coarse-grained featurisation of Durrettand Klein (2013), allowing us to directly measurethe impact of our proposed fine-grained featurisa-tion.We, however, improve it in a number of ways.The biggest performance boosts came from usingMIRA (Margin Infused Relaxation Algorithm) up-dates in place of standard perceptron updates andimplementing the full range of common featuresfrom the literature.
We also fix a number of bugfixes and improve mention extraction.
This im-proved system forms our LIMERIC baseline in Ta-ble 4.5.2 Fine-Grained Feature SpecialisationWe build on work in discourse transition prefix-ing (particularly Durrett and Klein, 2013), whichexpands the feature space of a learner by includ-ing multiple versions of each generated feature.LIMERIC previously used three versions of eachfeature: one unprefixed, one prefixed with the cur-rent mention?s type (one of name, nominal, or pro-noun), and one prefixed with the concatenation ofthe types of the current and candidate antecedents.In this work, we introduce a fourth prefix, formedby concatenating the AR of the current mentionwith that of the closest mention in the candidateantecedent cluster.The power of such transition features is thatthey allow us to learn, for instance, that pronoun toname transitions are preferred when the anaphor isdistant from its antecedent and the name mentionis one token, or that head match is a particularlystrong indicator of coreferentiality between shortdefinite nominals: 6+6-head match=true.5.3 ResultsTable 4 tabulates system performance on CoNLL-2012 TEST using system extracted mentions andv8.01 of the official scorer (Pradhan et al, 2014).Comparing feature specialisation against theLIMERIC baseline, we can see that it yields sub-stantial performance gains on all metrics and bothevaluation settings.
Performance gains indicatedin bold are statistically significant for the conser-vative p = 0.01 using bootstrap re-sampling1.
Per-formance gains indicated in italics are significantat the standard threshold of p = 0.05.We benchmark against the state-of-the-art by1Since Specialisation is a development of LIMERIC, thetwo models are not independent which means we would ex-pect to see relatively high confidence values for relativelysmall gains in score (see Berg-Kirkpatrick et al, 2012).435comparing performance to the winner of theshared task (Fernandes et al, 2012), as well asthe best documented system at the time of thiswork (Bj?orkelund and Kuhn, 2014).
Fine-grainedfeature specialisation improves LIMERIC?s perfor-mance to push past that of Bj?orkelund and Kuhn(2014) when using gold preprocessing.
Further-more, on the difficult automatic setting, we out-perform Fernandes et al (2012) and are not signif-icantly worse than Bj?orkelund and Kuhn (2014).On the link-based MUC and B3metrics, ourrecall gains are larger than our precision gains.That is, specialisation enables coreference indica-tors to accrue sufficient weight so as to promotenew coreference links, a known problem case formodern systems.
We found particularly enhancedweight on features for relaxed string matching.6 ConclusionIn this paper, we have found fine-grained patternsin reference expression usage based on the Ac-cessibility hierarchy and shown how these can beused to significantly improve the performance ofa strong system, LIMERIC.
Despite being simpleto implement, we achieve comparable or improvedperformance than the best reported results, further-ing arguments for incorporating cognitive insightsin computational modelling.7 AcknowledgementsThe authors thank their anonymous reviewers andmembers of the Schwa Lab at the University ofSydney for their insightful and helpful feedback.The first author was supported by an AustralianPostgraduate Award scholarship.ReferencesMira Ariel.
2001.
Accessibility theory: An overview.
Textrepresentation: Linguistic and psycholinguistic aspects,pages 29?87.Eric Bengtson and Dan Roth.
2008.
Understanding the valueof features for coreference resolution.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing, pages 294?303.
Association for Com-putational Linguistics.Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein.2012.
An empirical investigation of statistical significancein nlp.
In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Processing andComputational Natural Language Learning, pages 995?1005.
Association for Computational Linguistics, Jeju Is-land, Korea.Anders Bj?orkelund and Jonas Kuhn.
2014.
Learning struc-tured perceptrons for coreference resolution with latentantecedents and non-local features.
ACL, Baltimore, MD,USA, June.Pascal Denis and Jason Baldridge.
2008.
Specialized mod-els and ranking for coreference resolution.
In Proceedingsof the Conference on Empirical Methods in Natural Lan-guage Processing, pages 660?669.
Association for Com-putational Linguistics.Greg Durrett and Dan Klein.
2013.
Easy victories and uphillbattles in coreference resolution.
In Proceedings of theConference on Empirical Methods in Natural LanguageProcessing.Greg Durrett and Dan Klein.
2014.
A joint model for entityanalysis: Coreference, typing, and linking.
In Proceedingsof the Transactions of the Association for ComputationalLinguistics.Eraldo Fernandes, C?
?cero dos Santos, and Ruy Milidi?u.
2012.Latent structure perceptron with feature induction for un-restricted coreference resolution.
In Joint Conference onEMNLP and CoNLL - Shared Task, pages 41?48.
Associ-ation for Computational Linguistics, Jeju Island, Korea.Barbara J Grosz, Scott Weinstein, and Aravind K Joshi.
1995.Centering: A framework for modeling the local coherenceof discourse.
Computational Linguistics, 21(2):203?225.Hannaneh Hajishirzi, Leila Zilles, Daniel S Weld, and LukeZettlemoyer.
2013.
Joint coreference resolution andnamed-entity linking with multi-pass sieves.
pages 289?299.Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, EduardHovy, Vincent Ng, and Michael Strube.
2014.
Scoringcoreference partitions of predicted mentions: A referenceimplementation.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, OlgaUryupina, and Yuchen Zhang.
2012.
Conll-2012 sharedtask: Modeling multilingual unrestricted coreference inontonotes.
In Joint Conference on EMNLP and CoNLL- Shared Task, pages 1?40.
Association for ComputationalLinguistics, Jeju Island, Korea.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, MarthaPalmer, Ralph Weischedel, and Nianwen Xue.
2011.Conll-2011 shared task: Modeling unrestricted corefer-ence in ontonotes.
In Proceedings of the Fifteenth Con-ference on Computational Natural Language Learning:Shared Task, pages 1?27.
Association for ComputationalLinguistics, Portland, Oregon, USA.Sameer S. Pradhan, Eduard H. Hovy, Mitchell P. Mar-cus, Martha Palmer, Lance A. Ramshaw, and Ralph M.Weischedel.
2007.
Ontonotes: a unified relational seman-tic representation.
Int.
J. Semantic Computing, 1(4):405?419.Altaf Rahman and Vincent Ng.
2011.
Coreference resolu-tion with world knowledge.
In Proceedings of the 49thAnnual Meeting of the Association for Computational Lin-guistics: Human Language Technologies, volume 1, pages814?824.Lev Ratinov and Dan Roth.
2012.
Learning-based multi-sieveco-reference resolution with knowledge.
In EMNLP.Marta Recasens, Marie-Catherine de Marneffe, and Christo-pher Potts.
2013.
The life and death of discourse enti-ties: Identifying singleton mentions.
In Proceedings ofthe 2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: HumanLanguage Technologies, pages 627?633.
Association forComputational Linguistics, Atlanta, Georgia.436Olga Uryupina, Massimo Poesio, Claudio Giuliano, andKateryna Tymoshenko.
2011.
Disambiguation and filter-ing methods in using web knowledge for coreference res-olution.
In Proceedings of the 24th International FloridaArtificial Intelligence Research Society Conference, pages317?322.Kellie Webster and James R Curran.
2014.
Limited mem-ory incremental coreference resolution.
In Proceed-ings of COLING 2014, the 25th International Conferenceon Computational Linguistics: Technical Papers, pages2129?2139.
Dublin,Ireland.Sam Wiseman, Alexander M Rush, Stuart M Shieber, JasonWeston, Heather Pon-Barry, Stuart M Shieber, NicholasLongenbaugh, Sam Wiseman, Stuart M Shieber, Elif Ya-mangil, et al 2015.
Learning anaphoricity and antecedentranking features for coreference resolution.
In Proceed-ings of the 53rd Annual Meeting of the Association forComputational Linguistics, volume 1, pages 92?100.
As-sociation for Computational Linguistics.437
