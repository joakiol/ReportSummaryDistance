A Practical Comparison of Parsing StrategiesJonathan SlocumSiemens CorporationINTRODUCTIONAlthough the l i terature dealing with formal and naturallanguages abounds with theoretical arguments of worst-case performance by various parsing strategies \[e.g.,Griffiths & Petrick, 1965; Aho & Ullman, 1972; Graham,Harrison & Ruzzo, Ig80\], there is l i t t le  discussion ofcomparative performance based on actual practice inunderstanding natural language.
Yet important practicalconsiderations do arise when writing programs to under-stand one aspect or another of natural language utteran-ces.
Where, for example, a theorist wi l l  characterize aparsing strategy according to its space and/or timerequirements in attempting to analyze the worst possibleinput acc3rding to ~n arbitrary grammar st r ic t ly  limitedin expressive power, the researcher studying NaturalLanguage Processing can be just i f ied in concerninghimself more with issues of practical performance inparsing sentences encountered in language as humansActually use i t  using a grammar expressed in a formcorve~ie: to the human linguist who is writing i t .Moreover, ~ry  occasional poor performance may be quiteacceptabl:, particularly i f  real-time considerations arenot invo~ed, e.g., i f  a human querant is not waitingfor the answer to his question), provided the overallaverage performance is superior.
One example of such asituation is of f - l ine Machine Translation.This paper has two purposes.
One is to report an eval-uation of the performance of several parsing strategiesin a real-world setting, pointing out practical problemsin making the attempt, indicating which of the strate-gies is superior to the others in which situations, andmost of all  determining the reasons why the best strate-gy outclasses its competition in order to stimulate anddirect the design of improvements.
The other, moreimportant purpose is to assist in establishing suchevaluation as a meaningful and valuable enterprise thatcontributes to the evolution of Natural LanguagePrcJessing from an art form into an empirical science.T~t  is, our concern for parsing efficiency transcendsthe issue of mere practical ity.
At slow-to-averageparsing rates, the cost of verifying l inguistic theorieson a large, general sample of natural language can s t i l lbe prohibitive.
The author's experience in MT hasdemonstrated the enormous impetus to l inguistic theoryformulation and refinement hat a suitably fast parserwi l l  impart: when a linguist can formalize and encode atheory, then within an hour test i t  on a few thousandwords of natural text, he wi l l  be able to rejectinadequate ideas at a fa i r ly  high rate.
This argumentmay even be applied to the production of the semantictheory we all hope for: i t  is not l ike ly  that its earlyformulations wil l  be adequate, and unless they can beexplored inexpensively on significant language samplesthey may hardly be explored at a l l ,  perhaps to theextent that the theory's qualities remain undiscovered.The search for an optimal natural language parsingtechnique, then, can be seen as the search for aninstrument to assist in extending the theoreticalfrontiers of the science of Natural Language Processing.Following an outline below of some of the historicalcircumstances that led the author to design and conductthe parsing experiments, we wil l  detail our experimentalsetting and approach, present the results, discuss theimplications of those results, and conclude with someremarks on what has been l~rned.The SRI ConnectionAt SRI International the~thor was responsible for thedevelopment of the English front-end for the LADDERsystem \[Hendrix e ta l .
,  1978\].
LADDER was developed asa prototype system for understanding questions posed inEnglish about a naval domain; i t  translated each Englishquestion into one or more relational database queries,prosecuted the queries on a remote computer, andresponded with the requested information in a readableformat tailored to the characteristics of the answer.The basis for the development of the NLP component ofthe LADDER system was the LIFER parser, whichinterpreted sentences according to a 'semantic grammar'\[Burton, 1976\] whose rules were carefully ordered toproduce the most plausible interpretation f i rs t .After more than two years of intensive development, thehuman costs of extending the coverage began to mountsignificantly.
The semantic grammar interpreted byLIFER had become large and unwieldy.
Any change,however small, had the potential to produce "rippleeffects" which eroded the integrity of the system.
Amore l inguist ical ly motivated grammar was required.
Thequestion arose, "Is LIFER as suited to more traditionalgrammars as i t  is to semantic grammars?"
At the time,there were available at SRI three production-qualityparsers: LIFER; DIAMOND, an implementation of the Cocke-Kasami~nger parsing algorithm programmed by WilliamPaxton of SRI; and CKY, an implementation of theidentical algorithm programmed in i t ia l l y  by Prof. DanielChester at the University of Texas.
In thisenvironment, experiments comparing various aspects ofperformance were inevitable.The LRC ConnectionIn 1979 the author began research in Machine Translationat the Linguistics Research Center of the University ofTexas.
The LRC environment stimulated the design of anew strategy variation, though in retrospect i t  isobviously applicable to any parser supporting a fac i l i tyfor testing right-hand-side rule constituents.
I t  alsostimulated the production of another parser.
(Thesewi l l  be defined and discussed later.)
To test theeffects of various strategies on the two LRC parsers, anexperiment was designed to determine whether theyinteract with the different parsers and/or each other,whether any gains are offset by introduced overhead,and whether the source and precise effects of anyoverhead could be identified and explained.THE SRI EXPERIMENTSIn this section we report the experiments conducted atSRI.
First, the parsers and their strategy variationsare described and intu i t ive ly  compared; second, thegrammars are described in terms of their purpose andtheir coverage; third, the sentences employed in thecomparisons are discussed with regard to their sourceand presumed generality; next, the methods of comparingperformance are detailed; then the results of the majorexperiment are presented.
Finally, three small follow-up experiments are reported as anecdotal evidence.The Parsers and StrategiesOne of the parsers employed in the SRI experiments wasLIFER: a top-down, depth-first parser with automaticback-up \[Hendrix, 1977\].
LIFER employs special "lookdown" logic based on the current word in the sentence toeliminate obviously fruitless downward expansion whenthe current word cannot be accepted as the leftmoseelement in any expansion of the currently proposedsyntactic category \[Griff iths and Petrick, 1965\] and a"well-formed substring table" \[Woods, 1975\] to eliminateredundant pursuit of paths after back-up.
LIFER sup-ports a traditional style of rule writing where phrase-structure rules are augmented by (LISP) procedures whichcan reject the application of the rule when proposed bythe parser, and which construct an interpretation of thephrase when the rule's application is acceptable.
Thespecial user-definable routine responsible forevaluating the S-level rule-body procedures was modifiedto collect certain statistics but reject an otherwiseacceptable interpretation; this forced LIFER into itsback-up mode where i t  sought out an alternateinterpretation, which was recorded and rejected in thesame fashion.
In this way LIFER proceeded to derive al lpossible interpretations of each sentence according tothe grammar.
This rejection behavior was not entirelyunusual, in that LIFER specifically provides for such aneventuality, and because the grammars themselves werealready making use of this fac i l i ty  to reject faultyinterpretations.
By forcing LIFER to compute allinterpretations in this natural manner, i t  couldmeaningfully be compared with the other parsers.The second parser employed,in the 5RI experiments wasDIAMOND: an all-paths bottom-up parser \[Paxton, lg77\]developed at SRI as an outgrowth of the SRI SpeechUnderstanding Project \[Walker, 1978\].
The basis of theimplementation was the Cocke-Kasami-Younger algorithm\[Aho and Ullman, 1972\], augmented by an "oracle" \[Pratt,1975\] to restr ict the number of syntax rules considered.DIAMOND is used during the primarily syntactic,bottom-up phase of analysis; subsequent analysis phaseswork top-down through the parse tree, computing moredetailed semantic information, but these do not involveDIAMOND per se.
DIAMOND also supports a style of ruleswherein the grammar is augmented by LISP procedures toeither reject rule application, or compute aninterpretation of the phrase.The third parser used in the SR~ experiments is dubbedCKY.
It  too is an i~lementation of the Cocke-Kasami-Younger algorithm.
Shortly after the main experiment i tWAS augmented by "top-down f i l ter ing,"  and some shrill-scale tests were conducted.
Like Pratt's oracle, top-down f i l ter ing rejects the application of certain rulesdlstovered'up by the bottom-up parser specifically,those that a top-aown parser would not discover.
Forexample, assuming a grammar for English in a traditionalstyle, and the sentence, "The old man ate fish," anordinary bottom-up parser will propose three S phrases,one each for: "man ate fish," "old man ate fish," and"The old man ate fish."
In isolation each is a possiblesentence.
But a top-down parser will normally proposeonly the last string as a sentence, since the leftcontexts "The old" and "The" prohibit the sentencereading for the remaining strings.
Top-down filtering,then, is like running a top-down parser in parallel witha bottom-up parser.
The bottom-up parser (being fasterat discovering potential rules) proposes the rules, andthe top-down parser (being more sensitive to context)passes judgement.
Rejects are discarded immediately;those that pass muster are considered further, forexample being submitted for feature checking and/orsemantic interpretation.An intuit ive prediction of practical performance is asomewhat d i f f i cu l t  matter.
~FER, while not originallyintended to produce all interpretations, does support areasonably natural mechanism for forcing that style ofanalysis.
A large amount of effort was invested inmaking LIFER more and more efficient as the LADDERlinguistic component grew and began to consume morespace and time.
In CPU time its speed was increased bya factor of at least twenty with respect to itsoriginal, and rather eff ic ient,  implementation.
Onemight therefore expect LIFER to compare favorably withthe other parsers, particularly when interpreting theLADDER grammar written with LIFER, and only LIFER, inmind.
DIAMOND, while implementeing the very eff icientCocke-Kasami-Younger algorithm and being augmented withan oracle and special programming tricks (e.g., assemblycode) intended to enhance its performance, is a rathermassive program and might be considered suspect for thatreason alone; on the other hand, its predecessor wasdeveloped for the purpose of speech understanding, whereefficiency issues predominate, and this strongly arguesfor good performance xpectations.
Chester'simplementation of the Cocke-Kasami-Younger algorithmrepresents the opposite extreme of startl ing simplicity.His central algorithm is expressed in a dozen lines ofLISP code and requires l i t t le  else in a basicimplementation.
Expectations here might be bi-modal: i tshould either perform well due to its concise nature, orpoorly due to the lack of any efficiency aids.
There isone further consideration of merit: that of inter-programmer var iabi l i ty.
Both LIFER and Chester's parserwere rewritten for increased efficiency by the author;DIAMOND was used without modification.
Thus differencesbetween DIAMOND and the others might be due to differentprogramming styles -- indeed, between DIAMOND and CKYthis represents the only difference aside from theoracle --while differences between LIFER and CKY shouldreflect real performance distinctions because the sameprogrammer ( e)implemented them both.The GrammarsThe "semantic grammar" employed in the SRI experimentshad been developed for the specific purpose of answeringquestions posed in English about the domain of ships atsea \[Sacerdoti, 1977\].
There was no pretense of itsbeing a general grammar of English; nor was i t  adept atinterpreting questions posed by users unfamiliar withthe naval domain.
That is, the grammar was attuned toquestions posed by knowledgeable users, answerable fromthe available database.
The syntactic categories werelabelled with semantically meaningful names like <SHIP>,<ARRIVE>, <PORT>, and the l ike, and the words andphrases encompassed by such categories were restrictedin the obvious fashion.
Its adequacy of coverage issuggested by the success of LADDER as a demonstrationvehicle for  natural language access to databases\[Hendrix et a l .
,  1978\].The l ingu is t i c  grammar employed in the SRI experimentscame from an ent i re ly  d i f fe rent  project  concerned withdiscourse understanding \[Grosz, 1978\].
In the projectscenario a human apprentice technician consults with acomputer which (s expert at the disassembly, repai r ,  andreassembly of mechanical devices such as a pump.
Thecomputer guides the apprentice through the task, issuinginstruct ions and explanations at whatever levels ofdeta i l  are required; i t  may answer questions, describeappropriate tools for specific tasks, etc.
The grammarused to interpret these interactions was stronglyl inguist ical ly motivated \[Robinson, Ig8O\].
Developed ina domain primarily composed of declarative andimperative sentences, its generality is suggested by theshort time (a few weeks) required to extend its coverageto the wide range of questions'encountered in the LADDERdomain.In order to prime the various parsers with the differentframmars, four programs were written to transform eachgrammar into the formalism expected by the two parsersfor which i t  was not originally writtten.
Specifically,the l inguistic grammar had to be reformatted for inputto LIFER and CKY; the semantic grammar, for input to CKYand DIAMDNO.
Once each of six systems was loaded withone parser and one grammar, the stage would be set forthe experiment.2The SentencesSince LADDER's semantic grammar had been written forsentences in a limited domain, and was not intended forgeneral English, i t  was not possible to test thatgrammar on any corpus outside of its domain.
Therefore,all  sentences in the experiment were drawn from theLADDER benchmark: the broad collection of queriesdesigned to verify the overall integrity of the LADDERsystem after extensions had been incorporated.
Thesesentences, almost all of them questions, had beencarefully selected to exercise most of LADDER'sl inguistic and database capabilities.
Each of the sixsy~ems, then, was to be applied to the analysis of thesame 249 benchmark sentences; these ranged in lengthfrom 2 to 23 words and averaged 7.82 words.Methods of ComparisonSoftware instrumentation was used to measure thefollowing: the CPU time; the number of phrases(instantiations of grammar ules) proposed by theparser; the number of these rejected by the rule-bodyprocedures in the usual fashion; and the storagerequirements (number of CONSes) of the analysis attempt.Each of these was recorded separately for sentenceswhich were parsed vs. not parsed, and in the former casethe number of interpretations was recorded as we11.
Forthe experiment, the database access code wasshort-circuited; thus only analysis, not questionanswering, was performed.
The collected data wascategorized by sentence length and treatment (parser andgrammar) for analysis purposes.Summary of the First ExperimentThe f i r s t  experiment involved the production of sixdifferent instrumented systems -- three parsers, eachwith two grammars -- and six test runs on the identicalset of 249 entences comprising the LADDER benchmark.The benchmark, established quite independently of theexperiment, had as its raison d'etre the vigorousexercise of the LADDER system for the purpose ofvalidationg its integrity.
The sentences containedtherein were intended to constitute a representativesample of what might be expected in that domain.
Theexperiment was conducted on a DEC KL-IO; the systemswere run separately, during low-load conditions in orderto minimize competition with other programs which couldconfound the results.The Experimental ResultsAs i t  turned out, the large internal grammar storageoverhead of the DIAMOND parser prohibited its beingloaded with the LADDER semantic grammar: the availablememory space was exhausted before the grammar could beful ly defined.
Although eventually a method was workedout whereby the semantic grammar could be loaded intoDIAMOND, the resulting system was not tested due to itsnon-standard mode of operation, and because the workingspace left  over for parsing was minimal.
Therefore, theresults and discussion wil l  include data for only fivecombinations of parser and grammar.Linguistic GrammarIn terms of the number of grammar rules found applicableby the parsers, DIAMOND instantiated the fewest (aver-aging 58 phrases per sentence); CKY, the most (121); andLIFER fe l l  in between (IO7).
LIFER makes copious use ofCONS cells for internal processing purposes, and thusrequired the most storage (averaging 5294 CQNSes perparsed sentence); DIAMOND required the least (llO7); CKYfel l  in between (1628).
But in terms of parse time, CKYwas by far the best (averaging .386 seconds per sen-tence, exclusive of garbage collection); DIAMOND wasnext best (.976); and LIFER was worst (2.22).
The totalrun time on the SRI-KL machine for the batch jobs inter-preting the l inguistic grammar ( i .e .
,  'pure' parse timeplus all  overhead charges such as garbage collection,I/O, swapping and paging) was 12 minutes, 50 seconds forLIFER, 7 minutes, 13 seconds for DIAMOND, and 3 minutes15 seconds for CKY.
The surprising indication here isthat, even though CKY proposed more phrases than itscompetition, and used more storage than DIAMOND (thoughless than LIFER), i t  is the fastest parser.
This istrue whether considering successful or unsuccessfulanalysis attempts, using the l inguist ic grammar.Semantic GrammarWe wi l l  now consider the corresponding data for CKY vs.LIFER using the semantic grammar (remembering thatDIAMOND was not testable in this configuration).
Interms of the number of phrases per parsed sentence, CKYaveraged f ive times as many as LIFER (151 compared to29).
In terms of storage requirements CKY was better(averaging 1552 CONSes per sentence) but LIFER was onlysl ightly worse (1498).
But in CPU time, discountinggarbage collection, CKY was again significantly fasterthan LIFER (averaging .286 seconds per sentence comparedto .635).
The total run time on the SRI-KL machine forthe batch jobs interpreting the semantic grammar ( i .e .
,"pure" parse time plus al l  overhead charges such asgarbage collections, I/O, swapping and paging) was 5minutes, IO seconds for LIFER, and 2 minutes, 56 secondsfor CKY.
As with the l inguistic grammar, CKY wassignificantly more eff ic ient,  whether consideringsuccessful or unsuccessful analysis attempts, whileusing the same grammar and analyzing the same sentences.Three Follow-up ExperimentsThree follow-up mini-experiments were conducted.
Thenumber of sentences was relat ively small (a few dozen),and the results were not permanently recorded, thus theyare reported here as anecdotal evidence.
In the f i r s t ,CKY and LIFER were compared in their natural modes ofoperation -- that is, with CKY finding all interpreta-tions and LIFER fCnding the f i r s t  -- using both grammarsbut just a few sentences.
This was in response to thehypothesis that forcing LIFER to derive al l  interpreta-tions is necessarily unfair.
The results showed thatCKY derived all interpretations of the sentences insl ightly less time than LIFER found its f i rs t .The discovery that DIAMOND appeared to be considerablyless eff ic ient than CKY was quite surprising.Implementing the same algorithm, but augmented with thephrase-limiting "oracle" and special assembly code forefficiency, one might expect DIAMOND to be faster thanCKY.
A second mini-experiment was conducted to test thentost l ike ly explanation -- that the overhead ofDIAMOND's oracle might be greater than the savings i tproduced.
The results clearly indicated that DIAMONDwas yet slower without its oracle.The question then arose as to whether CKY might be yetfaster i f  i t  too were similarly augmented.
A top-downf i l te r  modification was soon implemented and anothersmall experiment was conducted.
Paradoxically, theeffect of f i l ter ing in this instance was to degradeperformance.
The overhead incurred was greater than theobserved savings.
This remained a puzzlement, andeventually helped to inspire the LRC experiment.THE LRC EXPERIMENTIn this section we discuss the experiment conducted atthe Lingui~icsResearch Center.
First, the parsers andtheir strategy variations are described and ~ntuitivelycompared; second, the grammar is described in terms ofits purpose and its coverage; third, the sentencesemployed in the comparisons are discussed with regard totheir source and presumed generality; next, the methodsof comparing performance are discussed; f ina l ly ,  the3results are presented.The Parsers and StrategiesOne of the parsers employed in the LRC experiment wasthe CKY parser.
The other parser employed in the LRCexperiment is a left-corner parser, inspired again byChester \[1980\] but programmed from scratch by theauthor.
Unlike a Cocke-Kasami-Younger parser, whichindexes a syntax rule by its right-most constituent, aleft-corner parser indexes a syntax rule by the le f t -most constituent in its right-hand side.
Once theparser has found an instance of the left-corner constit-uent, the remainder of the rule can be used to predictwhat may come next.
When augmented by top-down filter-ing, this parser strongly resembles the Earley algorithm\[Earley, Ig70\].Since the small-scale experiments with top-downfiltering at SRI had revealed conflicting results withrespect to DIAMOND and CKY, and since the author'sintuition continued to argue for increased efficiency inconjunction with this strategy despite the empiricalevidence to the contrary, it was decided to compare theperformance of both parsers with and without top-downfiltering in a larger, more carefully controlledexperiment.
Another strategy variation was engenderedduring the course of work at the LRC, based on the styleof grammar rules written by the linguistic staff.
Thisstrategy, called "early constituent tests," is intendedto take advantage of the extent of testing of individualconstituents in the right-hand-sides of the rules.
Nor-mally a parser searches its chart for contiguous phrasesin order as specified by the right-hand-side of a rule,then evaluates the rule-body procedures which mightreject the application due to a deficiency in one of ther-h-s constituent phrases; the early constituent teststrategy calls for the parser to evaluate that portionof the rule-body procedure which tests the first con-stituent, as soon as it is discovered, to determine ifit is acceptable; if so, the parser may proceed tosearch for the next constituent and similarly evaluateits test.
In addition to the potential savings due toearlier rule rejection, another potential benefit arisesfrom ATN-style sharing of individual constituent testsamong such rules as pose the same requirements on thesame in i t ia l  sequence of r-h-s constituents.
Thus onetest  could re ject  many apparently appl icable rules atonce, early in the search - -  a large potential  savingswhen compared with the alternative of discovering allconstituents of each rule and separately applying therule-body procedures, each of which might reject (thesame constituent) for  the same reason.
On the ocherhand, the overhead of invoking the extra constituenttests and saving the results for  eventual passage to theremainder of the rule-body procedure w i l l  to some extentoffset the gains.It is commonly considered that the Cocke-Kasami-Youngeralgorithm is generally superior to the left-corneralgorithm in practical application; it is also thoughtthat top-filtering is beneficial.
But in addition?o intuitions about the performance of the parsers andstrategy variations individually, there is the issue ofpossible interactions between them.
Since a significantportion of the sentence analysis effort may be investedin evaluating the rule-body procedures, the author'sintuition argued that the best cond}inatlon could be theleft-corner parser augmented by early constituent testsand top-down filtering -- which would seem to maximallyreduce the number of such procedures evaluated.The GrammarThe grammar employed uring the LRC experiment was theGerman analysis grammar being developed at the LRC for?
use in Machine Translation \[Lehmann et e l .
,  1981\].Under development for about two years up to the time ofthe experiment, i t  had been tested on several moderatelylarge technical corpora \[Slocum, Ig80\] total l ing about23,000 words.
Although by no means a complete grammar,i t  was able to account for between 60 and gO percent ofthe sentences in the various texts, depending on theincidence of problems such as highly unusual constructs,outright errors, the degree of complexity in syntax andsemantics, and on whether the tests were conducted withor without prior experience with the text.
The broadrange of l inguistic phenomena represented by thismaterial far outstrips that encountered in most NLPsystems to date.
Given the amount of text described bythe LRC German grammar, i t  may be presumedto operate ina fashion reasonably representative of the generalgrammar for German yet to be written?The SentencesThe sentences employed in the LRC experiment wereextracted from three different technical texts on whichthe LRC MT system had been previously tested.
Certaingrammar and dictionary extensions based on those tests ,however, had not yet been incorporated; thus i t  wasknown in advance that a s ign i f i cant  portion of thesentences might not be analyzed.
Three sentences ofeach length were randomly extracted from each text ,where possible; not a l l  sentence lengths weresufficiently represented to allow this in all cases.The 262 sentences ranged in length from 1 to 39 words,averaging 15.6 words each - -  twice as long as thesentences employed in the SRI experiments.Methods of ComparisonThe LRC experiment was intended to reveal more of theunderlying reasons for  d i f fe rent ia l  parser performance,including strategy interact ions;  thus i t  was necessaryto instrument the systems much more thoroughly.
Datawas gathered for 35 variables measuring various aspectsof behavior, including general information (13var iables) ,  search space (8 var iab les) ,  processing time(7 var iables) ,  and mamory requirements (7 var iables) .One of the simpler methods measured the amount of timedevoted to storage management (garbage co l lect ion inINTERLISP) in order to determine a " fa i r "  measure of CPUtime by pro-rat ing the storage management ime accordingto storage used (CONSes executed); simply cred i t inggarbage co l lect  time to the analysis of the sentenceimmediately at hand, or alternately neglecting i tentirely, would not represent a fa i r  distribution ofcosts.
More d i f f i cu l t  was the problem of measuringsearch space.
I t  was not fe l t  that an average branchingfactor computed for the static grammar would be repre-sentative o f  the search space encountered during thedynamic analysis of sentences.
An e f fo r t  was thereforemade to measure the search space actual ly encountered bythe parsers, d i f fe rent ia ted  into grammar vs. chartsearch; in the former instance, a further differentia-tion was based on whether the grammar space was beingconsidered from the bottom-up (discovery) vs. top-down( f i l te r )  perspective.
Moreover, the time and spaceinvolved in analyzing words and idioms and operating therule-body procedures was separately measured in order todetermine the computational effort expended by theparser proper.
For the experiment, the translationprocess was shor t -c i rcu i ted;  thus only analysis,  nottransfer and synthesis, was performed.Summary of the LRC ExperimentThe LRC experiment involved the production of eightdifferent instrumented systems -- two parsers ( le f t -corner and Cocke-Kasami-Younger), each with all fourcombinations of two independent strategy variations(top-down filtering and early constituent tests)-- andeight test runs on the identical set of 262 sentencesselected pseudo-randemly from three technical texts sup-plied by the MT project sponsor.
The sentences con-talned therein may reasonably be expected to constitutea nearly-representative sample of text in that domain,and presumably constitute a somewhat less-representative(but by no means t r iv ia l )  sample of the types of syntac-t ic structures encountered in more general German text.The usual ( i .e .
,  complete) analysis procedures for thepurpose of subsequent translation were in effect, whichincludes production of a full syntactic and semanticanalysis via phrase-structure rules, feature tests andoperations, transformations, and case frames.
I t  wasknown in advance that not all constructions would behandled by the grammar; further, that for some sentencessome or all of the parsers would exhaust the availablespace before achieving an analysis.
The latter problemin particular would indicate differential performancecharacteristics when working with limited memory.
Oneof the parsers, the version of the CKY parser lackingboth top-down f i l ter ing and early constituent ests, isQssentially identical to the CKY parser employed in theSRI experiments.
The experiment was conducted on a DEC2060; the systems were run separately, late at night inorder to minimize competition with other programs whichcould confound the results.The Experimental ResultsThe various parser and strategy combinations weres!igl~tly u-,~ual in their abi l i ty to analyze (or, alter-nate~y, de~ ~trate the ungran~naticality of) sentenceswithin the available space.
Of the three strategy choi-ces (parser, f i l ter ing,  constituent ests), f i l ter ingconstituted the most effective discriminant: the foursystems with top-down f i l ter ing were 4% more l ikely tofind an interpretation than the four without; but mostof this diiference occurred within the systems employingthe left-corner parser, where the likelihood was IO%greater.
The likelihood of deriving an interpretationat all is a matter that must be considered when contem-plating application on machines with relatively limitedaddress space.
The summaries below, however, have beenbalanced to reflect a situation in which all systemshave sufficient space to conclude the analysis effort,so that the comparisons may be drawn on an equal basis.Not surprisingly, the data reveal differences betweensingle strategies and between joint strategies, but thedifferences are sometimes much larger than one mightsuppose.
Top-down f i l ter ing overall reduced the numberof phrases by 35%, but when combined with CKY withoutearly constituent ests the difference increased to 46%.In the latter case, top-down f i l ter ing increased theoverall search space by a factor of 46-- to well over300,000 nodes per sentence.
For the Left-Corner Parserwithout early constituent ests, the growth rate is muchmilder -- an increase in search space of less than afactor- of 6 for a 42% reduction in the number of phrases-- but the original (unfiltered)search space was over 3times as large as that of CKY.
CKY overall required 84%fewer CONSes than did LCP (considering the parsersalone); for one matched pair of jo int  strategies, pureLCP required over twice as much storage as pure CKY.Evaluating the'parsers and strategies via CPU time is atricky business, for one must define and just i fy what isto be included.
A common practice is to exclude almosteverything (e.g., the time spent in storage management,paging, evaluating rule-body procedures, building parsetrees, etc.).
One commonly employed ideal metric is tocount the number of trips through the main parser loops.We argue that such practices are indefensible.
Forinstance, the "pure parse times" measured in thisexperiment differ by a factor of 3.45 in the worst case,but overall run times vary by 46% at most.
But theimportant point is that i f  one chose the "best" parseron the basis of pure parse time measured in thisexperiment, one would have the fourth-best overallsystem; to choose the best overall system, one mustsettle for the "sixth-best" parser!
Employing the loop-counter metric, we can indeed get a perfect predictionof rank-order via pure parse time based on the inner-loop counters; what is more, a formula can be worked outto.predict he observed pure parse times given the threeloop counters.
But such predictions have already beenshown to be useless.
(or worse) in predicting totalprogram runtime.
Thus in measuring performance weprefer to include everything one actually pays for inthe real computing world: Paging, storage management,building interpretations, etc., as well as parse time.In terms of overall performance, then, top-down f i l te r -ing in general reduced analysis times by 17% (though i tincreased pure parse times by 58%); LCP was 7% lesstime-consuming than CKY; and early constituent estslost by 15% compared to not performing the tests early.As one would expect, the joint strategy LCP with top-down f i l ter ing \[ON\] and Late (i.e.
not Early) Constitu-ent Tests \[LCT\] ranked f i r s t  among the eight systems.However, due to beneficial interactions the joint strat-egy \[LCP ON ECT\] (which on intuit ive grounds we predict-ed would be most efficient) came in a close second; \[CKYON LCT\] came in third.
The remainder anked as follows:\[CKY OFF LCT\], \[LCP OFF LCT\], \[CRY ON ECT\], \[CKY OFFECT\], \[LCP OFF ECT\].
Thus we see that beneficial inter-action with ECT is restricted to \[LCP ON\].Two interesting findings are related to sentence length.One, average parse times (however measured) do notexhibit cubic or even polynomial behavior, but insteadappear linear.
Two, the benefits of top-down f i l ter ingare dependent on sentence length; in fact, f i l ter ing isdetrimental for shorter sentences.
Averaging over allother strategies, the break-even point for top-downf i l ter ing occurs at about 7 words.
(Filtering alwaysincreases pure parse time, PPT, because the parser seesi t  as pure overhead.
The benefits are only observablein overall system performance, due primarily to asignificant reduction in the time/space spent evaluatingrule-body procedures.)
With respect to particularstrategy combinations, the break-even point comes atabout lO words for \[LCP LCT\], 6 words for \[CKY ECT\], 6words for \[LCP LCT\], and 7 words for \[LCP ECT\].
Thereason for this length dependency becomes rather obviousin retrospect, and suggests why top-down f i l ter ing inthe SRI follow-up experiment was detrimental: the testsentences were probably too short.DISCUSSIONThe immediate practical purpose of the SRI experimentswas not to stimulate a parser-writing contest, but todetermine the comparative merits of parsers in actualuse with the particular aim of extablishing a rationalbasis for choosing one to become the core of a futureNLP system.
The aim of the LRC experiment was todiscover which implementation details are responsiblefor the observed performance with an eye toward bothsuggesting and directing future improvements.The SRI ParsersThe question of relative efficiency was answereddecisively.
I t  would seem that the CKY parser performsbetter than LIFER due to its much greater speed at find-ing applicable rules, with either the semantic or thelinguistic grammar.
CKY certainly performs better thanDIAMOND for this reason, presumably due to programmardifferences since the algorithms are the same.
Thequestion of efficiency gains due to top-down f i l ter ingremained open since i t  enhanced one implementation butdegraded another.
Unfortunately, there is nothing inthe data which gets at the underlying reasons for theefficiency of the CKY parser.The LRC ParsersPredictions of performance with respect to all eightsystems are identical, i f  based on their theoreticallyequivalent search space.
The data, however, displaysome rather dramatic practical differences in searchspace.
LCP's chart search space, for example, is some25 times that of CKY; CKY's f i l te r  search space is al-most 45% greater than that of LCP.
Top-down f i l ter ingincreases search space, hence compute time, in ideal-ized models which bother to take i t  into account.
Evenin this experiment, the observed slight reduction inchart and grammar search space due to top-down f i l te r -ing is offset by its enormous earch space overhead ofover I00,000 nodes for LCP, and over 300,000 nodes for\[CKY LCT\], for the average sentence.
But the overheadis more than made up in practice by the advantages ofgreater storage efficiency and particularly the reducedrule-body procedure "overhead."
The f i l te r  search spacewith late column tests is three times that with earlycolumn tests, but again other factors combine to re-verse the advantage.The overhead for f i l ter ing in LCP is less than that inCKY.
This situation is due to the fact that LCP main-rains a natural left-r ight ordering of the rule con-stituents in its internal representation, whereas CKYdoes not and must therefore compute i t  at run time.
(The actual truth is slightly more complicated becauseCKY stores the grammar in both forms, but this carica-ture illustrates the effect of the differences.)
Thisis balanced somewhat by LCP's greatly increased chartsearch space; by way of caricature again, LCP is doingsome things with its chart that CKY does with its f i l -ter.
(That is, LCP performs some "f i l ter ing" as anatural consequence of its algorithm.)
The large vari-ations in the search space data would lead one to ex-pect large differences in performance.
This turns outnot to be the case, at least not in overall performance.CONCLUSIONSWe have seen that theoretical arguments can be quiteinaccurate in their  predictions when one makes the tran-sit ion from a worst-case model to an actual, real-worldsituation.
"Order n-cubed" performance does not appearto be realized in practice; what is more, the oft-ne-glected constants of theoretical calculations seem toexert a dominating effect in practical situations.Arguments about relative efficlencles of parsing methodsbased on idealized models such as inner-loop counterssimilarly fail to account for relative efficlenciesobserved in practice.
In order to meaningfully describeperformance, one must take into account the completeoperational context of the Natural Language Processingsystem, particularly the expenses encountered in storagemanagement and applying rule-body procedures.BIBLIOGRAPHYAho, A. V., and J. D. Ullman.
The Theory of Parsing,Translation, and Compiling, Vol.
I. Prentice-Hall,Englewood Cliffs, New Jersey, lg72.Burton, R. R., "Semantic Grammar: ~n engineeringtechnique for constructing natural languageunderstanding systems," BBN Report 3453, Bolt, Beranek,and Newman, Inc., Cambridge, Mass., Dec. 1976.Chester, 0?, "A Parsing Algorithm that Extends Phrases,"AJCL 6 (2), April-June 1980, pp.87-g6.Earley, J., "An Efficient Context-free ParsingAlgorithm," CACM 13 (2), Feb. IgTO, pp.
94-102.Graham, S. L., M. A. Harrison, and W. L. Ruzzo, "AnImproved Context-Free Recognizer," ACM Transactions onProgramming Languages and Systems, 2 (3), July 1980,pp.
415-462.Griffiths, T. V., and S. R. Petrick, "On the RelativeEfficiencies of Context-free Grammar Recognizers," CACM8 (.51, May lg65, pp.
289-300.Grosz, B. J., "Focusing in Dialog," Proceedings ofTheoretical Issues in Natural Language Processlng-2: AnInterdisciplinary Workshop, University of I l l ino is  atUrbana-Champaign, 25-27 July 1978,Hendrix, G. G., "Human Engineering for Applied NaturalLanguage Processing," Proceedings of the 5thInternational Conference on Art i f ic ia l  Intelligence,Cambridge, Mass., Aug. 1977.Hendrix, 6.
G., E. 0.
Sacerdoti, D. Sagalowicz, and J.Slocum, "Developlng a Natural Language Interface toComplex Data," ACM Transactions on Database Systems, 3{21, June 1978, pp.
105-147.Lehmenn, W. P., g. S. Bennett, J. Slocum, et el .
,  "TheMETAL System," Final Technlcal Report RAOC-TR-80-374.Rdme Air Development Center, Grifflss AFB, New York,Jan.
Ig81.
Available from NTIS.Paxton, W. U., "A Framework for Speech Understanding, ~Teoh.
Note 142, AS Center, SRI International, MenloPark, Callf., June 1977.Pratt, V. R., "LINGOL: A'progress report," Proceedingsof the Fourth International Joint Conference onArt i f ic ia l  Intelligence, l 'o i l i s i ,  Georgia, USSR, 3-8Sept.
1275, pp.
422-428.Robinson, J J., "DIAGRAM: A grammar for dialogues,"Tecb.
Note 205, AI Center, SRI International, MenloPark, Ca l i f .
,  Feb. 1980.Sacerdoti, E. 0., "Language Access to Distributed Datawith Error Recovery," Proceedings of the Fi fthInternational Joint Conference on Ar t i f i c ia lIntalligience, Cambridge, Mass., Aug. 1977.Slocum, J., An Experiment in Machine Translation,"Proceedings of the 18th Annual Meeting of theAssociation for Computational Linguistics, Philadelphia,19-12 June Ig80, pp.
163-167.Walker, D. E.
Cad.).
Understanding Spoken Language.North-Holland, New York, 1978.Woods, W. A., "Syntax, Semantics, and Speech," BBNReport 3067, Bolt, Beranek, and Newman, Inc., Cambridge,Mass., Apr.
1975.6
