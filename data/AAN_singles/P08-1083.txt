Proceedings of ACL-08: HLT, pages 728?736,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsUnsupervised Lexicon-Based Resolution of Unknown Words for FullMorphological AnalysisMeni Adler and Yoav Goldberg and David Gabay and Michael ElhadadBen Gurion University of the NegevDepartment of Computer Science?POB 653 Be?er Sheva, 84105, Israel{adlerm,goldberg,gabayd,elhadad}@cs.bgu.ac.ilAbstractMorphological disambiguation proceeds in 2stages: (1) an analyzer provides all possibleanalyses for a given token and (2) a stochasticdisambiguation module picks the most likelyanalysis in context.
When the analyzer doesnot recognize a given token, we hit the prob-lem of unknowns.
In large scale corpora, un-knowns appear at a rate of 5 to 10% (depend-ing on the genre and the maturity of the lexi-con).We address the task of computing the distribu-tion p(t|w) for unknown words for full mor-phological disambiguation in Hebrew.
We in-troduce a novel algorithm that is language in-dependent: it exploits a maximum entropy let-ters model trained over the known words ob-served in the corpus and the distribution ofthe unknown words in known tag contexts,through iterative approximation.
The algo-rithm achieves 30% error reduction on dis-ambiguation of unknown words over a com-petitive baseline (to a level of 70% accuratefull disambiguation of unknown words).
Wehave also verified that taking advantage of astrong language-specific model of morpholog-ical patterns provides the same level of disam-biguation.
The algorithm we have developedexploits distributional information latent in awide-coverage lexicon and large quantities ofunlabeled data.
?This work is supported in part by the Lynn and WilliamFrankel Center for Computer Science.1 IntroductionThe term unknowns denotes tokens in a text that can-not be resolved in a given lexicon.
For the task offull morphological analysis, the lexicon must pro-vide all possible morphological analyses for anygiven token.
In this case, unknown tokens can becategorized into two classes of missing informa-tion: unknown tokens are not recognized at all bythe lexicon, and unknown analyses, where the setof analyses for a lexeme does not contain the cor-rect analysis for a given token.
Despite efforts onimproving the underlying lexicon, unknowns typi-cally represent 5% to 10% of the number of tokensin large-scale corpora.
The alternative to continu-ously investing manual effort in improving the lex-icon is to design methods to learn possible analy-ses for unknowns from observable features: theirletter structure and their context.
In this paper, weinvestigate the characteristics of Hebrew unknownsfor full morphological analysis, and propose a newmethod for handling such unavoidable lack of in-formation.
Our method generates a distribution ofpossible analyses for unknowns.
In our evaluation,these learned distributions include the correct anal-ysis for unknown words in 85% of the cases, con-tributing an error reduction of over 30% over a com-petitive baseline for the overall task of full morpho-logical analysis in Hebrew.The task of a morphological analyzer is to pro-duce all possible analyses for a given token.
InHebrew, the analysis for each token is of the formlexeme-and-features1: lemma, affixes, lexical cate-1In contrast to the prefix-stem-suffix analysis format of728gory (POS), and a set of inflection properties (ac-cording to the POS) ?
gender, number, person, sta-tus and tense.
In this work, we refer to the mor-phological analyzer of MILA ?
the Knowledge Cen-ter for Processing Hebrew2 (hereafter KC analyzer).It is a synthetic analyzer, composed of two data re-sources ?
a lexicon of about 2,400 lexemes, and aset of generation rules (see (Adler, 2007, Section4.2)).
In addition, we use an unlabeled text cor-pus, composed of stories taken from three Hebrewdaily news papers (Aruts 7, Haaretz, The Marker),of 42M tokens.
We observed 3,561 different com-posite tags (e.g., noun-sing-fem-prepPrefix:be) overthis corpus.
These 3,561 tags form the large tagsetover which we train our learner.
On the one hand,this tagset is much larger than the largest tagset usedin English (from 17 tags in most unsupervised POStagging experiments, to the 46 tags of the WSJ cor-pus and the about 150 tags of the LOB corpus).
Onthe other hand, our tagset is intrinsically factored asa set of dependent sub-features, which we explicitlyrepresent.The task we address in this paper is morphologi-cal disambiguation: given a sentence, obtain the listof all possible analyses for each word from the an-alyzer, and disambiguate each word in context.
Onaverage, each token in the 42M corpus is given 2.7possible analyses by the analyzer (much higher thanthe average 1.41 POS tag ambiguity reported in En-glish (Dermatas and Kokkinakis, 1995)).
In previ-ous work, we report disambiguation rates of 89%for full morphological disambiguation (using an un-supervised EM-HMM model) and 92.5% for part ofspeech and segmentation (without assigning all theinflectional features of the words).In order to estimate the importance of unknownsin Hebrew, we analyze tokens in several aspects: (1)the number of unknown tokens, as observed on thecorpus of 42M tokens; (2) a manual classificationof a sample of 10K unknown token types out of the200K unknown types identified in the corpus; (3) thenumber of unknown analyses, based on an annotatedcorpus of 200K tokens, and their classification.About 4.5% of the 42M token instances in theBuckwalter?s Arabic analyzer (2004), which looks for any le-gal combination of prefix-stem-suffix, but does not provide fullmorphological features such as gender, number, case etc.2http://mila.cs.technion.ac.il.htmltraining corpus were unknown tokens (45% of the450K token types).
For less edited text, such as ran-dom text sampled from the Web, the percentage ismuch higher ?
about 7.5%.
In order to classify theseunknown tokens, we sampled 10K unknown tokentypes and examined them manually.
The classifica-tion of these tokens with their distribution is shownin Table 13.
As can be seen, there are two mainclasses of unknown token types: Neologisms (32%)and Proper nouns (48%), which cover about 80%of the unknown token instances.
The POS distribu-tion of the unknown tokens of our annotated corpusis shown in Table 2.
As expected, most unknownsare open class words: proper names, nouns or adjec-tives.Regarding unknown analyses, in our annotatedcorpus, we found 3% of the 100K token instanceswere missing the correct analysis in the lexicon(3.65% of the token types).
The POS distribution ofthe unknown analyses is listed in Table 2.
The highrate of unknown analyses for prepositions at about3% is a specific phenomenon in Hebrew, whereprepositions are often prefixes agglutinated to thefirst word of the noun phrase they head.
We observethe very low rate of unknown verbs (2%) ?
which arewell marked morphologically in Hebrew, and wherethe rate of neologism introduction seems quite low.This evidence illustrates the need for resolutionof unknowns: The naive policy of selecting ?propername?
for all unknowns will cover only half of theerrors caused by unknown tokens, i.e., 30% of thewhole unknown tokens and analyses.
The other 70%of the unknowns ( 5.3% of the words in the text inour experiments) will be assigned a wrong tag.As a result of this observation, our strategy is tofocus on full morphological analysis for unknowntokens and apply a proper name classifier for un-known analyses and unknown tokens.
In this paper,we investigate various methods for achieving fullmorphological analysis distribution for unknown to-kens.
The methods are not based on an annotatedcorpus, nor on hand-crafted rules, but instead ex-ploit the distribution of words in an available lexiconand the letter similarity of the unknown words withknown words.3Transcription according to Ornan (2002)729Category Examples DistributionTypes InstancesProper names ?asulin (family name) oileq`?a?udi (Audi) ice`` 40% 48%Neologisms ?agabi (incidental) iab`tizmur (orchestration) xenfz 30% 32%Abbreviation mz?p (DIFS) t"fnkb?t (security officer) h"aw 2.4% 7.8%Foreignpresentacyah (presentation) divhpfxt?a?ut (out) he``right3.8% 5.8%Wrong spelling?abibba?ah.ronah (springatlast) dpexg`aaia`?idiqacyot (idication) zeivwici`ryus?alaim (Rejusalem) milyeix1.2% 4%Alternative spelling ?opyynim (typical) mipiite`priwwilegyah (privilege ) diblieeixt 3.5% 3%Tokenization ha?sap (the?threshold) sq"d?al/17 (on/17) 71/lr 8% 2%Table 1: Unknown Hebrew token categories and distribution.Part of Speech Unknown Tokens Unknown Analyses TotalProper name 31.8% 24.4% 56.2%Noun 12.6% 1.6% 14.2%Adjective 7.1% 1.7% 8.8%Junk 3.0% 1.3% 4.3%Numeral 1.1% 2.3% 3.4%Preposition 0.3% 2.8% 3.1%Verb 1.8% 0.4% 2.2%Adverb 0.9% 0.9% 1.8%Participle 0.4% 0.8% 1.2%Copula / 0.8% 0.8%Quantifier 0.3% 0.4% 0.7%Modal 0.3% 0.4% 0.7%Conjunction 0.1% 0.5% 0.6%Negation / 0.6% 0.6%Foreign 0.2% 0.4% 0.6%Interrogative 0.1% 0.4% 0.5%Prefix 0.3% 0.2% 0.5%Pronoun / 0.5% 0.5%Total 60% 40% 100%Table 2: Unknowns Hebrew POS Distribution.7302 Previous WorkMost of the work that dealt with unknowns in the lastdecade focused on unknown tokens (OOV).
A naiveapproach would assign all possible analyses for eachunknown token with uniform distribution, and con-tinue disambiguation on the basis of a learned modelwith this initial distribution.
The performance of atagger with such a policy is actually poor: there aredozens of tags in the tagset (3,561 in the case of He-brew full morphological disambiguation) and onlya few of them may match a given token.
Severalheuristics were developed to reduce the possibilityspace and to assign a distribution for the remaininganalyses.Weischedel et al (1993) combine several heuris-tics in order to estimate the token generation prob-ability according to various types of information ?such as the characteristics of particular tags withrespect to unknown tokens (basically the distribu-tion shown in Table 2), and simple spelling fea-tures: capitalization, presence of hyphens and spe-cific suffixes.
An accuracy of 85% in resolving un-known tokens was reported.
Dermatas and Kokki-nakis (1995) suggested a method for guessing un-known tokens based on the distribution of the ha-pax legomenon, and reported an accuracy of 66% forEnglish.
Mikheev (1997) suggested a guessing-ruletechnique, based on prefix morphological rules, suf-fix morphological rules, and ending-guessing rules.These rules are learned automatically from raw text.They reported a tagging accuracy of about 88%.Thede and Harper (1999) extended a second-orderHMM model with a C = ck,i matrix, in order to en-code the probability of a token with a suffix sk tobe generated by a tag ti.
An accuracy of about 85%was reported.Nakagawa (2004) combine word-level andcharacter-level information for Chinese andJapanese word segmentation.
At the word level, asegmented word is attached to a POS, where thecharacter model is based on the observed charactersand their classification: Begin of word, In themiddle of a word, End of word, the character is aword itself S. They apply Baum-Welch training overa segmented corpus, where the segmentation of eachword and its character classification is observed, andthe POS tagging is ambiguous.
The segmentation(of all words in a given sentence) and the POStagging (of the known words) is based on a Viterbisearch over a lattice composed of all possible wordsegmentations and the possible classifications ofall observed characters.
Their experimental resultsshow that the method achieves high accuracy overstate-of-the-art methods for Chinese and Japaneseword segmentation.
Hebrew also suffers fromambiguous segmentation of agglutinated tokens intosignificant words, but word formation rules seem tobe quite different from Chinese and Japanese.
Wealso could not rely on the existence of an annotatedcorpus of segmented word forms.Habash and Rambow (2006) used theroot+pattern+features representation of Arabictokens for morphological analysis and generationof Arabic dialects, which have no lexicon.
Theyreport high recall (95%?98%) but low precision(37%?63%) for token types and token instances,against gold-standard morphological analysis.
Wealso exploit the morphological patterns characteris-tic of semitic morphology, but extend the guessingof morphological features by using contextualfeatures.
We also propose a method that reliesexclusively on learned character-level features andcontextual features, and eventually reaches the sameperformance as the patterns-based approach.Mansour et al (2007) combine a lexicon-basedtagger (such as MorphTagger (Bar-Haim et al,2005)), and a character-based tagger (such as thedata-driven ArabicSVM (Diab et al, 2004)), whichincludes character features as part of its classifica-tion model, in order to extend the set of analysessuggested by the analyzer.
For a given sentence, thelexicon-based tagger is applied, selecting one tag fora token.
In case the ranking of the tagged sentence islower than a threshold, the character-based tagger isapplied, in order to produce new possible analyses.They report a very slight improvement on Hebrewand Arabic supervised POS taggers.Resolution of Hebrew unknown tokens, over alarge number of tags in the tagset (3,561) requiresa much richer model than the the heuristics usedfor English (for example, the capitalization featurewhich is dominant in English does not exist in He-brew).
Unlike Nakagawa, our model does not useany segmented text, and, on the other hand, it aimsto select full morphological analysis for each token,731including unknowns.3 MethodOur objective is: given an unknown word, providea distribution of possible tags that can serve as theanalysis of the unknown word.
This unknown anal-ysis step is performed at training and testing time.We do not attempt to disambiguate the word ?
butonly to provide a distribution of tags that will be dis-ambiguated by the regular EM-HMM mechanism.We examined three models to construct the distri-bution of tags for unknown words, that is, wheneverthe KC analyzer does not return any candidate anal-ysis, we apply these models to produce possible tagsfor the token p(t|w):Letters A maximum entropy model is built forall unknown tokens in order to estimate their tagdistribution.
The model is trained on the knowntokens that appear in the corpus.
For each anal-ysis of a known token, the following features areextracted: (1) unigram, bigram, and trigram lettersof the base-word (for each analysis, the base-wordis the token without prefixes), together with theirindex relative to the start and end of the word.
Forexample, the n-gram features extracted for the wordabc are { a:1 b:2 c:3 a:-3 b:-2 c:-1ab:1 bc:2 ab:-2 bc:-1 abc:1 abc:-1} ; (2) the prefixes of the base-word (as a singlefeature); (3) the length of the base-word.
The classassigned to this set of features, is the analysis of thebase-word.
The model is trained on all the knowntokens of the corpus, each token is observed with itspossible POS-tags once for each of its occurrences.When an unknown token is found, the modelis applied as follows: all the possible linguisticprefixes are extracted from the token (one of the 76prefix sequences that can occur in Hebrew); if morethan one such prefix is found, the token is analyzedfor each possible prefix.
For each possible suchsegmentation, the full feature vector is constructed,and submitted to the Maximum Entropy model.We hypothesize a uniform distribution among thepossible segmentations and aggregate a distributionof possible tags for the analysis.
If the proposedtag of the base-word is never found in the corpuspreceded by the identified prefix, we remove thispossible analysis.
The eventual outcome of themodel application is a set of possible full morpho-logical analyses for the token ?
in exactly the sameformat as the morphological analyzer provides.Patterns Word formation in Hebrew is based onroot+pattern and affixation.
Patterns can be used toidentify the lexical category of unknowns, as wellas other inflectional properties.
Nir (1993) investi-gated word-formation in Modern Hebrew with a spe-cial focus on neologisms; the most common word-formation patterns he identified are summarized inTable 3.
A naive approach for unknown resolutionwould add all analyses that fit any of these patterns,for any given unknown token.
As recently shown byHabash and Rambow (2006), the precision of sucha strategy can be pretty low.
To address this lack ofprecision, we learn a maximum entropy model onthe basis of the following binary features: one fea-ture for each pattern listed in column Formation ofTable 3 (40 distinct patterns) and one feature for ?nopattern?.Pattern-Letters This maximum entropy model islearned by combining the features of the lettersmodel and the patterns model.Linear-Context-based p(t|c) approximationThe three models above are context free.
Thelinear-context model exploits information about thelexical context of the unknown words: to estimatethe probability for a tag t given a context c ?
p(t|c)?
based on all the words in which a context occurs,the algorithm works on the known words in thecorpus, by starting with an initial tag-word estimatep(t|w) (such as the morpho-lexical approximation,suggested by Levinger et al (1995)), and iterativelyre-estimating:p?
(t|c) =?w?W p(t|w)p(w|c)Zp?
(t|w) =?c?C p(t|c)p(c|w)allow(t, w)Zwhere Z is a normalization factor, W is the set ofall words in the corpus, C is the set of contexts.allow(t, w) is a binary function indicating whether tis a valid tag for w. p(c|w) and p(w|c) are estimatedvia raw corpus counts.Loosely speaking, the probability of a tag given acontext is the average probability of a tag given any732Category Formation ExampleVerb Template?iCCeC ?ibh.en (diagnosed) oga`miCCeC mih.zer (recycled) xfgnCiCCen timren (manipulated) oxnzCiCCet tiknet (programmed) zpkztiCCeC ti?arek (dated) jx`zParticiple TemplatemeCuCaca ms?wh.zar (reconstructed) xfgeynmuCCaC muqlat.
(recorded) hlwenmaCCiC malbin (whitening) oialnNounSuffixationut h.aluciyut (pioneership) zeivelgay yomanay (duty officer) i`pneian ?egropan (boxer) otexb`on pah.on (shack) oegtiya marakiyah (soup tureen) diiwxnit t.iyulit (open touring vehicle) zileiha lomdah (courseware) dcnelTemplatemaCCeC mas?neq (choke) wpynmaCCeCa madgera (incubator) dxbcnmiCCaC mis?ap (branching) srqnmiCCaCa mignana (defensive fighting) dppbnCeCeCa pelet.
(output) hlttiCCoCet tiproset (distribution) zqextztaCCiC tah.rit.
(engraving) hixgztaCCuCa tabru?ah (sanitation) d`exazmiCCeCet micrepet (leotard) ztxvnCCiC crir (dissonance) xixvCaCCan bals?an (linguist) oylaCaCeCet s?ah.emet (cirrhosis) zngyCiCul t.ibu?
(ringing) reaihhaCCaCa hanpas?a (animation) dytpdheCCeC het?em (agreement) m`zdAdjectiveSuffixationbi nora?i (awful) i`xepani yeh.idani (individual) ipcigioni t.elewizyonic (televisional) ipeifieelha?i yed.ida?i (unique) i`cigiali st.udentiali (student) il`ihpcehqTemplate C1C2aC3C2aC3d metaqtaq (sweetish) wzwznCaCuC rapus (flaccid ) qetxAdverb Suffixationot qcarot (briefly) zexvwit miyadit (immediately) zicinPrefixation b bekeip (with fun) sikaaCoCeC variation: wzer ?wyeq (a copy).bThe feminine form is made by the t and iya suffixes: ipcigi yeh.idanit (individual), dixvep nwcriya (Christian).cIn the feminine form, the last h of the original noun is omitted.dC1C2aC3C2oC3 variation: oehphw qt.ant.wn (tiny).Table 3: Common Hebrew Neologism Formations.733Model Analysis Set MorphologicalDisambiguationCoverage Ambiguity ProbabilityBaseline 50.8% 1.5 0.48 57.3%Pattern 82.8% 20.4 0.10 66.8%Letter 76.7% 5.9 0.32 69.1%Pattern-Letter 84.1% 10.4 0.25 69.8%WordContext-Pattern 84.4% 21.7 0.12 66.5%TagContext-Pattern 85.3% 23.5 0.19 64.9%WordContext-Letter 80.7% 7.94 0.30 69.7%TagContext-Letter 83.1% 7.8 0.22 66.9%WordContext-Pattern-Letter 85.2% 12.0 0.24 68.8%TagContext-Pattern-Letter 86.1% 14.3 0.18 62.1%Table 4: Evaluation of unknown token full morphological analysis.of the words appearing in that context, and similarlythe probability of a tag given a word is the averagedprobability of that tag in all the (reliable) contextsin which the word appears.
We use the functionallow(t, w) to control the tags (ambiguity class) al-lowed for each word, as given by the lexicon.For a given word wi in a sentence, we examinetwo types of contexts: word context wi?1, wi+1,and tag context ti?1, ti+1.
For the case of word con-text, the estimation of p(w|c) and p(c|w) is simplythe relative frequency over all the events w1, w2, w3occurring at least 10 times in the corpus.
Since thecorpus is not tagged, the relative frequency of thetag contexts is not observed, instead, we use thecontext-free approximation of each word-tag, in or-der to determine the frequency weight of each tagcontext event.
For example, given the sequenceicnl ziznerl daebz tgubah l?umatit lmadai (a quiteoppositional response), and the analyses set pro-duced by the context-free approximation: tgubah[NN 1.0] l?umatit [] lmadai [RB 0.8, P1-NN 0.2].The frequency weight of the context {NN RB} is1 ?
0.8 = 0.8 and the frequency weight of the con-text {NN P1-NN} is 1 ?
0.2 = 0.2.4 EvaluationFor testing, we manually tagged the text which isused in the Hebrew Treebank (consisting of about90K tokens), according to our tagging guideline (?
).We measured the effectiveness of the three mod-els with respect to the tags that were assigned to theunknown tokens in our test corpus (the ?correct tag?
),according to three parameters: (1) The coverage ofthe model, i.e., we count cases where p(t|w) con-tains the correct tag with a probability larger than0.01; (2) the ambiguity level of the model, i.e., theaverage number of analyses suggested for each to-ken; (3) the average probability of the ?correct tag?,according to the predicted p(t|w).
In addition, foreach experiment, we run the full morphology dis-ambiguation system where unknowns are analyzedaccording by the model.Our baseline proposes the most frequent tag(proper name) for all possible segmentations of thetoken, in a uniform distribution.
We compare thefollowing models: the 3 context free models (pat-terns, letters and the combined patterns and letters)and the same models combined with the word andtag context models.
Note that the context modelshave low coverage (about 40% for the word contextand 80% for the tag context models), and therefore,the context models cannot be used on their own.
Thehighest coverage is obtained for the combined model(tag context, pattern, letter) at 86.1%.We first show the results for full morphologicaldisambiguation, over 3,561 distinct tags in Table 4.The highest coverage is obtained for the model com-bining the tag context, patterns and letters models.The tag context model is more effective becauseit covers 80% of the unknown words, whereas theword context model only covers 40%.
As expected,our simple baseline has the highest precision, sincethe most frequent proper name tag covers over 50%of the unknown words.
The eventual effectiveness of734Model Analysis Set POS TaggingCoverage Ambiguity ProbabilityBaseline 52.9% 1.5 0.52 60.6%Pattern 87.4% 8.7 0.19 76.0%Letter 80% 4.0 0.39 77.6%Pattern-Letter 86.7% 6.2 0.32 78.5%WordContext-Pattern 88.7% 8.8 0.21 75.8%TagContext-Pattern 89.5% 9.1 0.14 73.8%WordContext-Letter 83.8% 4.5 0.37 78.2%TagContext-Letter 87.1% 5.7 0.28 75.2%WordContext-Pattern-Letter 87.8 6.5 0.32 77.5%TagContext-Pattern-Letter 89.0% 7.2 0.25 74%Table 5: Evaluation of unknown token POS tagging.the method is measured by its impact on the eventualdisambiguation of the unknown words.
For full mor-phological disambiguation, our method achieves anerror reduction of 30% (57% to 70%).
Overall, withthe level of 4.5% of unknown words observed in ourcorpus, the algorithm we have developed contributesto an error reduction of 5.5% for full morphologicaldisambiguation.The best result is obtained for the model com-bining pattern and letter features.
However, themodel combining the word context and letter fea-tures achieves almost identical results.
This is aninteresting result, as the pattern features encapsulatesignificant linguistic knowledge, which apparentlycan be approximated by a purely distributional ap-proximation.While the disambiguation level of 70% is lowerthan the rate of 85% achieved in English, it mustbe noted that the task of full morphological disam-biguation in Hebrew is much harder ?
we manageto select one tag out of 3,561 for unknown words asopposed to one out of 46 in English.
Table 5 showsthe result of the disambiguation when we only takeinto account the POS tag of the unknown tokens.The same models reach the best results in this caseas well (Pattern+Letters and WordContext+Letters).The best disambiguation result is 78.5% ?
still muchlower than the 85% achieved in English.
The mainreason for this lower level is that the task in He-brew includes segmentation of prefixes and suffixesin addition to POS classification.
We are currentlyinvestigating models that will take into account thespecific nature of prefixes in Hebrew (which encodeconjunctions, definite articles and prepositions) tobetter predict the segmentation of unknown words.5 ConclusionWe have addressed the task of computing the distri-bution p(t|w) for unknown words for full morpho-logical disambiguation in Hebrew.
The algorithmwe have proposed is language independent: it ex-ploits a maximum entropy letters model trained overthe known words observed in the corpus and the dis-tribution of the unknown words in known tag con-texts, through iterative approximation.
The algo-rithm achieves 30% error reduction on disambigua-tion of unknown words over a competitive baseline(to a level of 70% accurate full disambiguation ofunknown words).
We have also verified that tak-ing advantage of a strong language-specific modelof morphological patterns provides the same levelof disambiguation.
The algorithm we have devel-oped exploits distributional information latent in awide-coverage lexicon and large quantities of unla-beled data.We observe that the task of analyzing unknown to-kens for POS in Hebrew remains challenging whencompared with English (78% vs. 85%).
We hy-pothesize this is due to the highly ambiguous patternof prefixation that occurs widely in Hebrew and arecurrently investigating syntagmatic models that ex-ploit the specific nature of agglutinated prefixes inHebrew.735ReferencesMeni Adler.
2007.
Hebrew Morphological Disambigua-tion: An Unsupervised Stochastic Word-based Ap-proach.
Ph.D. thesis, Ben-Gurion University of theNegev, Beer-Sheva, Israel.Roy Bar-Haim, Khalil Sima?an, and Yoad Winter.
2005.Choosing an optimal architecture for segmentation andpos-tagging of modern Hebrew.
In Proceedings ofACL-05 Workshop on Computational Approaches toSemitic Languages.Tim Buckwalter.
2004.
Buckwalter Arabic morphologi-cal analyzer, version 2.0.Evangelos Dermatas and George Kokkinakis.
1995.
Au-tomatic stochastic tagging of natural language texts.Computational Linguistics, 21(2):137?163.Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004.Automatic tagging of Arabic text: From raw text tobase phrase chunks.
In Proceeding of HLT-NAACL-04.Michael Elhadad, Yael Netzer, David Gabay, and MeniAdler.
2005.
Hebrew morphological tagging guide-lines.
Technical report, Ben-Gurion University, Dept.of Computer Science.Nizar Habash and Owen Rambow.
2006.
Magead: Amorphological analyzer and generator for the arabicdialects.
In Proceedings of the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Linguis-tics, pages 681?688, Sydney, Australia, July.
Associa-tion for Computational Linguistics.Moshe Levinger, Uzi Ornan, and Alon Itai.
1995.
Learn-ing morpholexical probabilities from an untagged cor-pus with an application to Hebrew.
ComputationalLinguistics, 21:383?404.Saib Mansour, Khalil Sima?an, and Yoad Winter.
2007.Smoothing a lexicon-based pos tagger for Arabic andHebrew.
In ACL07 Workshop on Computational Ap-proaches to Semitic Languages, Prague, Czech Repub-lic.Andrei Mikheev.
1997.
Automatic rule induction forunknown-word guessing.
Computational Linguistics,23(3):405?423.Tetsuji Nakagawa.
2004.
Chinese and Japanese wordsegmentation using word-level and character-level in-formation.
In Proceedings of the 20th internationalconference on Computational Linguistics, Geneva.Raphael Nir.
1993.
Word-Formation in Modern Hebrew.The Open University of Israel, Tel-Aviv, Israel.Uzi Ornan.
2002.
Hebrew in Latin script.
Le?s?one?nu,LXIV:137?151.
(in Hebrew).Scott M. Thede and Mary P. Harper.
1999.
A second-order hidden Markov model for part-of-speech tag-ging.
In Proceeding of ACL-99.R.
Weischedel, R. Schwartz, J. Palmucci, M. Meteer, andL.
Ramshaw.
1993.
Coping with ambiguity and un-known words through probabilistic models.
Computa-tional Linguistics, 19:359?382.736
