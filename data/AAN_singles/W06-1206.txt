Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 36?44,Sydney, July 2006. c?2006 Association for Computational LinguisticsAutomated Multiword Expression Prediction for Grammar EngineeringYi Zhang & Valia KordoniDept.
of Computational LinguisticsSaarland UniversityD-66041 Saarbru?cken, Germany{yzhang,kordoni}@coli.uni-sb.deAline Villavicencio & Marco IdiartInstitutes of Informatics & PhysicsFederal University of Rio Grande do SulAv.
Bento Gonc?alves, 9500Porto Alegre - RS, Brazilavillavicencio@inf.ufrgs.bridiart@if.ufrgs.brAbstractHowever large a hand-crafted wide-coverage grammar is, there are always go-ing to be words and constructions thatare not included in it and are going tocause parse failure.
Due to their hetero-geneous and flexible nature, MultiwordExpressions (MWEs) provide an endlesssource of parse failures.
As the numberof such expressions in a speaker?s lexi-con is equiparable to the number of singleword units (Jackendoff, 1997), one ma-jor challenge for robust natural languageprocessing systems is to be able to dealwith MWEs.
In this paper we proposeto semi-automatically detect MWE can-didates in texts using some error miningtechniques and validating them using acombination of the World Wide Web as acorpus and some statistical measures.
Forthe remaining candidates possible lexico-syntactic types are predicted, and they aresubsequently added to the grammar as newlexical entries.
This approach providesa significant increase in the coverage ofthese expressions.1 IntroductionHand-crafted large-scale grammars like the En-glish Resource Grammar (Flickinger, 2000), thePargram grammars (Butt et al, 1999) and theDutch Alpino Grammar (Bouma et al, 2001)are extremely valuable resources that have beenused in many NLP applications.
However, dueto the open-ended and dynamic nature of lan-guages, and the difficulties of grammar engineer-ing, such grammars are likely to contain errorsand be incomplete.
An error can be roughly clas-sified as under-generating (if it prevents a gram-matical sentence to be generated/parsed) or over-generating (if it allows an ungrammatical sen-tence to be generated/parsed).
In the context ofwide-coverage parsing, we focus on the under-generating errors which normally lead to parsingfailure.Traditionally, the errors of the grammar are tobe detected manually by the grammar develop-ers.
This is usually done by running the grammarover a carefully designed test suite and inspectingthe outputs.
This procedure becomes less reliableas the grammar gets larger, and is especially dif-ficult when the grammar is developed in a dis-tributed manner.
Baldwin et al (2004), amongmany others, for instance, have investigated themain causes of parse failure, parsing a randomsample of 20,000 strings from the written com-ponent of the British National Corpus (hencefor-ward BNC) using the English Resource Gram-mar (Flickinger, 2000), a broad-coverage preci-sion HPSG grammar for English.
They have foundthat the large majority of failures are caused bymissing lexical entries, with 40% of the cases, andmissing constructions, with 39%.To this effect, as mentioned above, in recentyears, some approaches have been developed inorder to (semi)automatically detect and/or repairthe errors in linguistic grammars.
van Noord(2004), for instance, takes a statistical approachtowards semi-automated error detection using theparsability metric for word sequences.
He reportson a simple yet practical way of identifying gram-mar errors.
The method is particularly useful fordiscovering systematic problems in a large gram-mar with reasonable coverage.
The idea behind itis that each (under-generating) error in the gram-36mar leads to the parsing failure of some specificgrammatical sentences.
By running the grammarover a large corpus, the corpus can be split intotwo subsets: the set of sentences covered by thegrammar and the set of sentences that failed toparse.
The errors can be identified by comparingthe statistical difference between these two setsof sentences.
By statistical difference, any kindof uneven distribution of linguistic phenomena ismeant.
In the case of van Noord (2004), the wordsequences are used, mainly because the cost tocompute and count the word sequences is mini-mum.
The parsability of a sequence wi .
.
.
wj isdefined as:R(wi .
.
.
wj) =C(wi .
.
.
wj, OK)C(wi .
.
.
wj)(1)where C(wi .
.
.
wj) is the number of sentencesin which the sequence wi .
.
.
wj occurs, andC(wi .
.
.
wj , OK) is the number of sentences witha successful parse which contain the sequence.A frequency cut is used to eliminate the infre-quent sequences.
With suffix arrays and perfecthashing automata, the parsability of all word se-quences (with arbitrary length) can be computedefficiently.
The word sequences are then sortedaccording to their parsabilities.
Those sequenceswith the lowest parsabilities are taken as direct in-dication of grammar errors.Among them, one common error, and sub-sequently very common cause of parse failureis due to Multiword Expressions (MWEs), likephrasal verbs (break down), collocations (breadand butter), compound nouns (coffee machine),determiner-less PPs (in hospital), as well as so-called ?frozen expressions?
(by and large), as dis-cussed by both Baldwin et al (2004) and van No-ord (2004).
Indicatively, in the experiments re-ported in Baldwin et al (2004), for instance, fromall the errors due to missing lexical entries, onefifth were due to missing MWEs (8% of total er-rors).
If an MWE is syntactically marked, the stan-dard grammatical rules and lexical entries cannotgenerate the string, as for instance in the case ofa phrasal verb like take off, even if the individualwords that make up the MWE are contained in thelexicon.In this paper we investigate semi-automaticmethods for error mining and detection of miss-ing lexical entries, following van Noord (2004),with the subsequent handling of the MWEs amongthem.
The output of the error mining phase pro-poses a set of n-grams, which also contain MWEs.Therefore, the task is to distinguish the MWEsfrom the other cases.
To do this, first we proposeto use the World Wide Web as a very large corpusfrom which we collect evidence that enables us torule out noisy cases (due to spelling errors, for in-stance), following Grefenstette (1999), Keller etal.
(2002), Kilgarriff and Grefenstette (2003) andVillavicencio (2005).
The candidates that are keptcan be semi-automatically included in the gram-mar, by employing a lexical type predictor, whoseoutput we use in order to add lexical entries to thelexicon, with a possible manual check by a gram-mar writer.
This procedure significantly speeds upthe process of grammar development, relieving thegrammar developer of some of the burden by au-tomatically detecting parse failures and providingsemi-automatic means for handling them.The paper starts with a discussion of MWEs andof some of the characteristics that make them sochallenging for NLP, in section 2.
This is followedby a more detailed discussion of the techniqueemployed for error detection, in section 3.
Theapproach used for distinguishing noisy sequencesfrom MWE-related constructions using the WorldWide Web is then presented.
How this informationis used for extending the grammar and the resultsobtained are then addressed in section 5.2 Multiword ExpressionsThe term Multiword Expressions (MWEs) hasbeen used to describe expressions for which thesyntactic or semantic properties of the whole ex-pression cannot be derived from its parts ((Sag etal., 2002), (Villavicencio et al, 2005)), includinga large number of related but distinct phenomena,such as phrasal verbs (e.g.
come along), nomi-nal compounds (e.g.
frying pan), institutionalisedphrases (e.g.
bread and butter), and many oth-ers.
They are used frequently in language, andin English, Jackendoff (1997) estimates the num-ber of MWES in a speaker?s lexicon to be com-parable to the number of single words.
This is re-flected in several existing grammars and lexical re-sources, where almost half of the entries are Mul-tiword Expressions.
However, due to their hetero-geneous characteristics, MWEs present a toughchallenge for both linguistic and computationalwork (Sag et al, 2002).
Some MWEs are fixed,and do not present internal variation, such as ad37hoc, while others allow different degrees of inter-nal variability and modification, such as touch anerve (touch/find a nerve) and spill beans (spillseveral/musical/mountains of beans).
In terms ofsemantics, some MWEs are more opaque in theirmeaning (e.g.
to kick the bucket as to die), whileothers have more transparent meanings that can beinferred from the words in the MWE (e.g.
eat up,where the particle up adds a completive sense toeat).
Therefore, to provide a unified account forthe detection of these distinct but related phenom-ena is a real challenge for NLP systems.3 Detection of Errors: Overviewvan Noord (2004) reports on various errors thathave been discovered for the Dutch Alpino Gram-mar (Bouma et al, 2001) semi-automatically, us-ing the Twente Nieuws Corpus.
The idea pur-sued by van Noord (2004) has been to locate thosen-grams in the input that might be the cause ofparsing failure.
By processing a huge amountof data, the parsability metrics briefly presentedin section 1 have been used to successfully lo-cate various errors introduced by the tokenizer,erroneous/incomplete lexical descriptions, frozenexpressions with idiosyncratic syntax, or incom-plete grammatical descriptions.
However, the re-covery of these errors has been shown to still re-quire significant efforts from the grammar devel-oper.
Moreover, there is no concrete data givenabout the distribution of the different types of er-rors discovered.As also mentioned before, among the n-gramsthat usually cause parse failures, there is a largenumber of missing MWEs in the lexicon suchas phrasal verbs, collocations, compound nouns,frozen expressions (e.g.
by and large, centre ofattention, put forward by, etc).For the purpose of the detection of MWEs, weare interested in seeing what the major types of er-ror for a typical large-scale deep grammar are.
Inthis context, we have run the error mining experi-ment reported by van Noord with the English Re-source Grammar (ERG; (Flickinger, 2000))1 andthe British National Corpus 2.0 (BNC; (Burnard,2000)).We have used a subset of the BNC written com-ponent.
The sentences in this collection containno more than 20 words and only ASCII characters.1ERG is a large-scale HPSG grammar for English.
In thispaper, we have used the January 2006 release of the grammar.That is about 1.8M distinct sentences.These sentences have then be fed into an effi-cient HPSG parser (PET; (Callmeier, 2000)) withERG loaded.
The parser has been configured witha maximum edge number limit of 100K and hasrun in the best-only mode so that it does not ex-haustively find all the possible parses.
The resultof each sentence is marked as one of the followingfour cases:?
P means at least one parse is found for thesentence;?
L means the parser halted after the morpho-logical analysis and has not been able to con-struct any lexical item for the input token;?
N means the search has finished normallyand there is no parse found for the sentence;?
E means the search has finished abnormallyby exceeding the edge number limit.It is interesting to notice that when the ambigu-ity packing mechanism (Oepen and Carroll, 2000)is used and the unpacking is turned off 2, E doesnot occur at all for our test corpus.
Running theparsability checking over the entire collection ofsentences has taken the parser less than 2 days ona 64bit machine with 3GHz CPU.
The results areshown in Table 1.Result # Sentences PercentageP 644,940 35.80%L 969,452 53.82%N 186,883 10.38%Table 1: Distribution of Parsing Results?From the results shown in Table 1, one can seethat ERG has full lexical span for less than half ofthe sentences.
For these sentences, about 80% aresuccessfully parsed.
These numbers show that thegrammar coverage has a significant improvementas compared to results reported by Baldwin et al(2004) and Zhang and Kordoni (2006), mainly at-tributed to the increase in the size of the lexiconand the new rules to handle punctuations and frag-ments.Obviously, L indicates the unknown words inthe input sentence.
But for N , it is not clear where2For the experiment of error mining, only the parsabilitychecking is necessary.
There is no need to record the exactparses.38and what kind of error has occurred.
In orderto pinpoint the errors, we used the error miningtechniques proposed by van Noord (2004) on thegrammar and corpus.
We have taken the sentencesmarked as N (because the errors in L sentencesare already determined) and calculate the word se-quence parsabilities against the sentences markedas P .
The frequency cut is set to be 5.
The wholeprocess has taken no more than 20 minutes, result-ing in total the parsability scores for 35K n-grams(word sequences).
The distribution of n-grams inlength with parsability below 0.1 is shown in Ta-ble 2.Number Percentageuni-gram 798 20.84%bi-gram 2,011 52.52%tri-gram 937 24.47%Table 2: Distribution of N-gram in Length in ErrorMining Results (R(x) < 0.1)Although pinpointing the problematic n-gramsstill does not tell us what the exact errors are, itdoes shed some light on the cause.
From Table 2we see quite a lot of uni-grams with low parsabil-ities.
Table 3 gives some examples of the wordsequences.
By intuition, we make the bold as-sumption that the low parsability of uni-grams iscaused by the missing appropriate lexical entriesfor the corresponding word.3For the bi-grams and tri-grams, we do see a lotof cases where the error can be repaired by justadding a multiword lexical entry into the grammar.N-gram Countprofessionals 248the flat 62indication of 21tone of voice 19as always is 7Table 3: Some Examples of the N-grams in ErrorMining ResultsIn order to distinguish those n-grams that canbe added into the grammar as MWE lexical en-tries from the other cases, we propose to vali-date them using evidence collected from the WorldWide Web.3It has later been confirmed with the grammar developerthat almost all of the errors detected by these low parsabilityuni-grams can be fixed by adding correct lexical entries.4 Detection of MWEs and relatedconstructionsRecently, many researchers have started using theWorld Wide Web as an extremely large corpus,since, as pointed out by Grefenstette (1999), theWeb is the largest data set available for NLP((Grefenstette, 1999), (Keller et al, 2002), (Kil-garriff and Grefenstette, 2003) and (Villavicencio,2005)).
For instance, Grefenstette employs theWeb to do example-based machine translation ofcompounds from French into English.
The methodhe employs would suffer considerably from datasparseness, if it were to rely only on corpus data.So for compounds that are sparse in the BNC healso obtains frequencies from the Web.
The scaleof the Web can help to minimise the problem ofdata sparseness, that is especially acute for MWEs,and Villavicencio (2005) uses the Web to find ev-idence to verify automatically generated VPCs.This work is built on these, in that we proposeto employ the Web as a corpus, using frequenciescollected from the Web to detect MWEs amongthe n-grams that cause parse failure.
We concen-trate on the 482 most frequent candidates, to verifyt he method.The candidate list has been pre-processed to re-move systematic unrelated entries, like those in-cluding acronyms, names, dates and numbers, fol-lowing Bouma and Villada (2002).
Using Googleas a search engine, we have looked for evidenceon the Web for each of the candidate MWEs, thathave occurred as an exact match in a webpage.
Foreach candidate searched, Google has provided uswith a measure of frequency in the form of thenumber of pages in which it appears.
Table 4shows the 10 most frequent candidates, and amongthese there are parts of formulae, frozen expres-sions and collocations.
Table 5 on the other hand,shows the 10 least frequent candidates.
From thetotal of candidates, 311 have been kept while theother have been discarded as noise.A manual inspection of the candidates has re-vealed that indeed the list contains a large amountof MWEs and frozen expressions like taking intoaccount the, good and evil, by and large, put for-ward by and breach of contract.
Some of thesecases, like come into effect in, have very spe-cific subcategorisation requirements, and this is re-flected by the presence of the prepositions into andin in the ngram.
Other cases seem to be part offormulae, like but also in, as part of not only X but39Table 4: Top 10 Candidate Multiword ExpressionsMWE Pages Entropy Prob(%)the burden of 36600000 0.366 79.4and cost effective 34400000 0.372 70.7the likes of 34400000 0.163 93.1but also in 27100000 0.038 98.9to bring together 25700000 0.086 96.6points of view 24500000 0.017 99.6and the more 23700000 0.512 61.5with and without 23100000 0.074 97.4can do for 22300000 0.003 99.9taking into account the 22100000 0.009 99.6but what about 21000000 0.045 98.7the ultimate in 17400000 0.199 90.0Table 5: Bottom 10 Candidate Multiword ExpressionsMWE Pages Entropy Prob (%)stand by and 1350000 0.399 65.5discharged from hospital 553000 0.001 99.9shock of it 92300 0.541 44.6was woken by 91400 0.001 99.9telephone rang and 43700 0.026 99.2glanced across at 36900 0.003 99.9the citizens charter 22900 0.070 97.9input is complete 13900 0.086 97.2from of government 706 0.345 0.1the to infinitive 561 0.445 1.440also Y, but what about, and the more the (part ofthe more the Yer).However, among the candidates there still re-main those that are not genuine MWEs, like of al-cohol and and than that in, which contain very fre-quent words that enable them to obtain a very highfrequency count without being an MWE.
There-fore, to detect these cases, the remainder of thecandidates could be further analysed using somestatistical techniques to try to distinguish themfrom the more likely MWEs among the candi-dates.
This is done by Bouma and Villada (2002)who investigated some measures that have beenused to identify certain kinds of MWEs, focusingon collocational prepositional phrases, and on thetests of mutual information, log likelihood and ?2.One significant difference here is that this work isnot constrained to a particular type of MWEs, buthas to deal with them in general.
Moreover, thestatistical measures used by Bouma and Villadademand the knowledge of single word frequencieswhich can be a problem when using Google espe-cially for common words like of and a.In Tables 4 and 5 we present two alternativemeasures that combined can help to detect falsecandidates.
The rational is similar to the statis-tical tests, without the need of searching for thefrequency of each of the words that make up theMWE.
We assume that if a candidate is just aresult of the random occurrence of very frequentwords most probably the order of the words in thengram is not important.
Therefore, given a can-didate, such as the likes of, we measure the fre-quency of occurrence of all its permutations (e.g.the of likes, likes the of, etc) and we calculate thecandidate?s entropy asS = ?
1logNN?k=1Pi logPi (2)where Pi is the probability of occurrence of agiven permutation, and N the total number of per-mutations.
The entropy above defined has its max-imum at S = 1 when all permutations are equallyprobably, which indicates a clear signature of arandom nature.
On the other hand, when order isvery important and only a single configuration isallowed the entropy has its minimum, S = 0.
Anngram with low entropy has good chances of beingan MWE.
A close inspection on Table 4 shows thatthe top two candidate ngrams have relatively highentropies ( here we consider high entropy whenS > 0.3 ).
In the first case this can be explainedby the fact that the word the can appear after theword of without compromising the MWE mean-ing as in the burden of the job.
In the second caseit shows that the real MWE is cost effective andthe word and can be either in the beginning or inthe end of the trigram.
In fact for a trigram withonly two acceptable permutations the entropy isS = log 2/ log 6 ' 0.39, very close to what isobtained .We also show the probability of occurrenceof each candidate ngram among its permutations(P1).
Most of the candidates in the list are morefrequent than their permutations.
In Table 4 wefind two exceptions which are clearly spelling er-rors in the last 2 ngrams.
Therefore low P1 canbe a good indicative of a noisy candidate.
Anothergood predictor is the relative frequency betweenthe candidates.
Given the occurrence values forthe most frequent candidates, we consider that byusing a threshold of 20,000 occurrences, it is pos-sible to remove the more noisy cases.We note that the grammar can also impose somerestrictions in the order of the elements in thengram, in the sense that some of the generatedpermutations are ungrammatical (e.g.
the of likes)and will most probably have null or very low fre-quencies.
Therefore, on top of the constraints onthe lexical order there are also constraints on theconstituent order of a candidate which will be re-flected in these measures.4The remainder candidates can be semi-automatically included in the grammar, by usinga lexical type predictor, as described in the nextsection.
With this information, each candidate isadded as a lexical entry, with a possible manualcheck by a grammar writer prior to inclusion inthe grammar.4Google ignores punctuation between the elements of thengram.
This can lead to some hits being returned for someof the ungrammatical permuted ngrams, such as one one byin the sentence We?re going to catch people one by one.
Oneday,... from www.beertravelers.com/lists/drafttech.html.
Onthe other hand, Google only returns the number of pageswhere a given ngram occurred, but not the number of times itoccurred in that page.
This can result in a huge underestima-tion especially for very frequent ngrams and words, whichcan be used mo re than once in a given page.
Therefore,a conservative view of these frequencies must be adopted,given that for some ngrams they might be inflated and forothers deflated.415 Automated Deep Lexical AcquisitionIn section (3), we have seen that more than 50%of the sentences contain one or more unknownwords.
And about half of the other parsing failuresare also due to lexicon missing.
In this section, wepropose a statistical approach towards lexical typeprediction for unknown words, including multi-word expressions.5.1 Atomic Lexical TypesLexicalist grammars are normally composed of alimited number of rules and a lexicon with richlinguistic features attached to each entry.
Somegrammar formalisms have a type inheriting systemto encode various constraints, and a flat structureof the lexicon with each entry mapped onto onetype in the inheritance hierarchy.
The followingdiscussion is based on Head-driven Phrase Struc-ture Grammar (HPSG) (Pollard and Sag, 1994),but should be easily adapted to other formalisms,as well.The lexicon of HPSG consists of a list of well-formed Typed Feature Structures (TFSs) (Carpen-ter, 1992), which convey the constraints on spe-cific words by two ways: the type compatibility,and the feature-value consistency.
Although it ispossible to use both features and types to con-vey the constraints on lexical entries, large gram-mars prefer the use of types in the lexicon becausethe inheritance system prevents the redundant def-inition of feature-values.
And the feature-valueconstraints in the lexicon can be avoided by ex-tending the types.
Say we have n lexical entriesLi :t[F a1] .
.
.
Ln :t[F an].
They share the samelexical type t, but take different values for the fea-ture F .
If a1, .
.
.
, an are the only possible valuesfor F in the context of type t, we can extend thetype t with subtypes ta1 :t[F a1] .
.
.
tan :t[F an]and modify the lexical entries to use these newtypes, respectively.
Based on the fact that largegrammars normally have a very restricted num-ber of feature-values constraints for each lexicaltype, the increase of the types is acceptable.
It isalso typical that the types assigned to lexical en-tries are maximum on the type hierarchy, whichmeans that they have no further subtypes.
We willcall the maximum lexical types after extension theatomic lexical types.
Then the lexicon will be amulti-valued mapping from the word stems to theatomic lexical types.Needless to underline here that all we havementioned above is not applicable exclusively toHPSG, but to many other formalisms based onTFSs, which makes our assumptions about atomiclexical types all the more relevant for a wide rangeof systems and applications.5.2 Statistical Lexical Type PredictorGiven that the lexicon of deep grammars can bemodelled by a mapping from word stems to atomiclexical types, we now go on designing the statisti-cal methods that can automatically ?guess?
suchmappings for unknown words.Similar to Baldwin (2005), we also treat theproblem as a classification task.
But there is an im-portant difference.
While Baldwin (2005) makespredictions for each unknown word, we create anew lexical entry for each occurrence of the un-known word.
The assumption behind this is thatthere should be exactly one lexical entry that cor-responds to the occurrence of the word in the givencontext5.We use a single classifier to predict the atomiclexical type.
There are normally hundreds ofatomic lexical types for a large grammar.
So theclassification model should be able to handle alarge number of output classes.
We choose theMaximum Entropy-based model because it caneasily handle thousands of features and a largenumber of possible outputs.
It also has the ad-vantages of general feature representation and noindependence assumption between features.
Withthe efficient parameter estimation algorithms dis-cussed by Malouf (2002), the training of the modelis now very fast.For our prediction model, the probability of alexical type t given an unknown word and its con-text c is:p(t|c) = exp(?i ?ifi(t, c))?t?
?T exp(?i ?ifi(t?, c))(3)where feature fi(t, c) may encode arbitrary char-acteristics of the context.
The parameters <?1, ?2, .
.
.
> can be evaluated by maximising thepseudo-likelihood on a training corpus (Malouf,2002).
The detailed design and feature selec-tion for the lexical type predictor are described inZhang and Kordoni (2006).5Lexical ambiguity is not considered here for the un-knowns.
In principle, this constraint can be relaxed by allow-ing the classifier to return more than one results by, setting aconfidence threshold, for example.42In the experiment described here, we have usedthe latest version of the Redwoods Treebank in or-der to train the lexical type predictor with morpho-logical features and context words/POS tags fea-tures 6.
We have then extracted from the BNC6248 sentences, which contain at least one of the311 MWE candidates verified with World WideWeb in the way described in the previous section.For each occurrence of the MWE candidates inthis set of sentences, our lexical type predictor haspredicted a lexical entry candidate.
This has re-sulted in 1936 distinct entries.
Only those entrieswith at least 5 counts have been added into thegrammar.
This has resulted in an extra 373 MWElexical entries for the grammar.This addition to the grammar has resulted in asignificant increase in coverage (table 6) of 14.4%.This result is very promising, as only a subset ofthe candidate MWEs has been analysed, and couldresult in an even greater increase in coverage, ifthese techniques were applied to the complete setof candidates.However, we should also point out that the cov-erage numbers reported in Table 6 are for a setof ?difficult?
sentences which contains a lot ofMWEs.
When compared to the numbers reportedin Table 1, the coverage of the parser on this dataset after adding the MWE entries is still signifi-cantly lower.
This indicates that not all the MWEscan be correctly handled by simply adding morelexical entries.
Further investigation is still re-quired.6 ConclusionsOne of the important challenges for robust naturallanguage processing systems is to be able to dealwith the systematic parse failures caused in greatpart by Multiword Expressions and related con-structions.
Therefore, in this paper we have pro-posed an approach for the semi-automatic exten-sion of grammars by using an error mining tech-nique for the detection of MWE candidates in textsand for predicting possible lexico-syntactic typesfor them.
The approach presented is based on thatof van Noord (2004) and proposes a set of MWEcandidates.
For this set of candidates, using theWorld Wide Web as a large corpus, frequencies aregathered for each candidate.
These in conjunctionwith some statistical measures are employed forruling out noisy cases like spelling mistakes (from6The POS tags are produced with the TnT tagger.of government) and frequent non-MWE sequenceslike input is complete.With this information the remaining sequencesare analysed by a statistical type predictor that as-signs the most likely lexical type for each of thecandidates in a given context.
By adding these tothe grammar as new lexical entries, a considerableincrease in coverage of 14.4% was obtained.The approach proposed employs simple andself-contained techniques that are language-independent and can help to semi-automaticallyextend the coverage of a grammar without rely-ing on external resources, like electronic dictio-naries and ontologies that are expensive to obtainand not available for all languages.
Therefore, itprovides an inexpensive and reusable manner ofhelping and speeding up the grammar engineer-ing process, by relieving the grammar developerof some of the burden of extending the coverageof the grammar.As future work we intend to investigate furtherstatistical measures that can be applied robustly todifferent types of MWEs for refining even morethe list of candidates and distinguishing false pos-itives, like of alcohol and from MWEs, like putforward by.
The high frequency with which theformer occur in corpora and the more accute prob-lem of data sparseness that affects the latter makethis a difficult task.ReferencesTimothy Baldwin, Emily M. Bender, Dan Flickinger,Ara Kim, and Stephan Oepen.
2004.
Road-testingthe English Resource Grammar over the British Na-tional Corpus.
In Proceedings of the Fourth Interna-tional Conference on Language Resources and Eval-uation (LREC 2004), Lisbon, Portugal.Timothy Baldwin.
2005.
Bootstrapping deep lexicalresources: Resources for courses.
In Proceedingsof the ACL-SIGLEX Workshop on Deep Lexical Ac-quisition, pages 67?76, Ann Arbor, Michigan, June.Association for Computational Linguistics.Gosse Bouma and Begon?a Villada.
2002.
Corpus-based acquisition of collocational prepositionalphrases.
In Proceedings of the Computational Lin-guistics in the Netherlands (CLIN) 2001, Universityof Twente.Gosse Bouma, Gertjan van Noord, and Robert Malouf.2001.
Alpino: Wide-coverage computational anal-ysis of dutch.
In Computational Linguistics in TheNetherlands 2000.43Entries Added Item # Covered # CoverageERG 0 6246 268 4.3%ERG+MWE(Web) 373 6246 1168 18.7%Table 6: Parser coverage on ?difficult?
sentences before/after adding MWE lexical entriesLou Burnard.
2000.
User Reference Guide for theBritish National Corpus.
Technical report, OxfordUniversity Computing Services.M.
Butt, S. Dipper, A. Frank, and T.H.
King.
1999.Writing large-scale parallel grammars for english,french, and german.
In Proceedings of the LFG99Conference.
CSLI Publications.Ulrich Callmeier.
2000.
PET ?
a platform for ex-perimentation with efficient HPSG processing tech-niques.
Journal of Natural Language Engineering,6(1):99?108.Bob Carpenter.
1992.
The Logic of Typed Fea-ture Structures.
Cambridge University Press, Cam-bridge, England.Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering, 6(1):15?28.Gregory Grefenstette.
1999.
The World Wide Webas a resource for example-based machine transla-tion tasks.
In Proceedings of ASLIB, Conference onTranslating and the Computer, London.Ray Jackendoff.
1997.
Twistin?
the night away.
Lan-guage, 73:534?59.Frank Keller, Maria Lapata, and Olga Ourioupina.2002.
Using the Web to overcome data sparse-ness.
In Jan Hajic?
and Yuji Matsumoto, editors, Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 230?237,Philadelphia.Adam Kilgarriff and Gregory Grefenstette.
2003.
In-troduction to the special issue on web as corpus.Computational Linguistics, 29.Robert Malouf.
2002.
A comparison of algorithmsfor maximum entropy parameter estimation.
In Pro-ceedings of the Sixth Conferencde on Natural Lan-guage Learning (CoNLL-2002), pages 49?55.Stephan Oepen and John Carroll.
2000.
Ambiguitypacking in constraint-based parsing ?
practical re-sults.
In Proceedings of the 1st Conference of theNorth American Chapter of the ACL, pages 162?169, Seattle, WA.Carl J. Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University of ChicagoPress, Chicago, Illinois.Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-take, and Dan Flickinger.
2002.
Multiword expres-sions: A pain in the neck for NLP.
In Proceed-ings of the 3rd International Conference on Intelli-gent Text Processing and Computational Linguistics(CICLing-2002), pages 1?15, Mexico City, Mexico.Gertjan van Noord.
2004.
Error mining for wide-coverage grammar engineering.
In Proceedings ofthe 42nd Meeting of the Association for Compu-tational Linguistics (ACL?04), Main Volume, pages446?453, Barcelona, Spain, July.Aline Villavicencio, Francis Bond, Anna Korhonen,and Diana McCarthy.
2005.
Introduction to the spe-cial issue on multiword expressions: having a crackat a hard nut.
Journal of Computer Speech and Lan-guage Processing, 19.Aline Villavicencio.
2005.
The availability of verb-particle constructions in lexical resources: Howmuch is enough?
Journal of Computer Speech andLanguage Processing, 19.Yi Zhang and Valia Kordoni.
2006.
Automated deeplexical acquisition for robust open texts processing.In Proceedings of the Fifth International Confer-ence on Language Resources and Evaluation (LREC2006), Genoa, Italy.44
