Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 720?725,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsThe Surprising Variance in Shortest-Derivation ParsingMohit Bansal and Dan KleinComputer Science DivisionUniversity of California, Berkeley{mbansal,klein}@cs.berkeley.eduAbstractWe investigate full-scale shortest-derivationparsing (SDP), wherein the parser selects ananalysis built from the fewest number of train-ing fragments.
Shortest derivation parsingexhibits an unusual range of behaviors.
Atone extreme, in the fully unpruned case, itis neither fast nor accurate.
At the other ex-treme, when pruned with a coarse unlexical-ized PCFG, the shortest derivation criterionbecomes both fast and surprisingly effective,rivaling more complex weighted-fragment ap-proaches.
Our analysis includes an investi-gation of tie-breaking and associated dynamicprograms.
At its best, our parser achieves anaccuracy of 87% F1 on the English WSJ taskwith minimal annotation, and 90% F1 withricher annotation.1 IntroductionOne guiding intuition in parsing, and data-drivenNLP more generally, is that, all else equal, it is ad-vantageous to memorize large fragments of trainingexamples.
Taken to the extreme, this intuition sug-gests shortest derivation parsing (SDP), wherein atest sentence is analyzed in a way which uses as fewtraining fragments as possible (Bod, 2000; Good-man, 2003).
SDP certainly has appealing properties:it is simple and parameter free ?
there need not evenbe an explicit lexicon.
However, SDP may be toosimple to be competitive.In this paper, we consider SDP in both its pureform and with several direct modifications, finding arange of behaviors.
In its pure form, with no prun-ing or approximation, SDP is neither fast nor accu-rate, achieving less than 70% F1 on the English WSJtask.
Moreover, basic tie-breaking variants and lexi-cal augmentation are insufficient to achieve compet-itive accuracies.1 On the other hand, SDP is dramat-ically improved in both speed and accuracy whena simple, unlexicalized PCFG is used for coarse-to-fine pruning (and tie-breaking).
On the EnglishWSJ, the coarse PCFG and the fine SDP togetherachieve 87% F1 with basic treebank annotation (seeTable 2) and up to 90% F1 with richer treebank an-notation (see Table 4).The main contribution of this work is to analyzethe behavior of shortest derivation parsing, showingboth when it fails and when it succeeds.
Our finalparser, which combines a simple PCFG coarse passwith an otherwise pure SPD fine pass, can be quiteaccurate while being straightforward to implement.2 Implicit Grammar for SDPThe all-fragments grammar (AFG) for a (binarized)treebank is formally the tree-substitution grammar(TSG) (Resnik, 1992; Bod, 1993) that consists ofall fragments (elementary trees) of all training treesin the treebank, with some weighting on each frag-ment.
AFGs are too large to fully extract explicitly;researchers therefore either work with a tractablesubset of the fragments (Sima?an, 2000; Bod, 2001;Post and Gildea, 2009; Cohn and Blunsom, 2010) oruse a PCFG reduction like that of Goodman (1996a),in which each treebank node token Xi is given itsown unique grammar symbol.We follow Bansal and Klein (2010) in choosingthe latter, both to permit comparison to their resultsand because SDP is easily phrased as a PCFG re-duction.
Bansal and Klein (2010) use a carefully pa-1Bod (2000) presented another SDP parser, but with a sam-pled subset of the training fragments.720rameterized weighting of the substructures in theirgrammar in an effort to extend the original DOP1model (Bod, 1993; Goodman, 1996a).
However, forSDP, the grammar is even simpler (Goodman, 2003).In principle, the implicit SDP grammar needs justtwo rule schemas: CONTINUE (Xp ?
Yq Zr) andSWITCH (Xp ?
Xq), with additive costs 0 and 1,respectively.
CONTINUE rules walk along trainingtrees, while SWITCH rules change between trees fora unit cost.2 Assuming that the SWITCH rules are inpractice broken down into BEGIN and END sub-rulesas in Bansal and Klein (2010), the grammar is linearin the size of the treebank.3 Note that no lexiconis needed in this grammar: lexical switches are likeany other.A derivation in our grammar has weight (cost) wwhere w is the number of switches (or the num-ber of training fragments minus one) used to buildthe derivation (see Figure 1).
The Viterbi dy-namic program for finding the shortest derivation isquite simple: it requires CKY to store only byte-valued switch-counts s(Xp, i, j) (i.e., the numberof switches) for each chart item and compute thederivation with the least switch-count.
Specifically,in the dynamic program, if we use a SWITCH ruleXp ?
Xq, then we updates(Xp, i, j) := s(Xq, i, j) + 1.If we use a continue rule Xp ?
Yq Zr, then the up-date iss(Xp, i, j) := s(Yq, i, k) + s(Zr, k, j),where k is a split point in the chart.
Using thisdynamic program, we compute the exact shortestderivation parse in the full all-fragments grammar(which is reduced to a PCFG with 2 rules schemasas described above).3 Basic SDP: Inaccurate and SlowSDP in its most basic form is appealingly simple,but has two serious issues: it is both slow and in-accurate.
Because there are millions of grammar2This grammar is a very minor variant of the reduction ofSDP suggested by Goodman (2003).3For a compact WSJ training set with graph packing (seeBansal and Klein (2010)) and one level of parent annotationand markovization, our grammar has 0.9 million indexed sym-bols compared to 7.5 million unbinarized (and 0.75 million bi-narized) explicitly-extracted fragments of just depth 1 and 2.Test SentenceTest ParseThe  girlTraining DataDT-2The girlNP-4DT-5 NN-6girl TheNP-1DT-2 NN-3Derivation 2 Derivation 1NPDT NNThe girlNP-1DT-2 NN-3The girlNP-4DT-5A girlNN-6SWITCHFigure 1: SDP - the best parse corresponds to the shortestderivation (fewest switches).symbols, exact SDP parsing takes more than 45 sec-onds per sentence in our implementation (in additionto being highly memory-intensive).
Many methodsexist for speeding up parsing through approxima-tion, but basic SDP is too inaccurate to merit them.When implemented as described in Section 2, SDPachieves only 66% F1 on the WSJ task (dev set, ?40 words).Why does SDP perform so poorly?
One reasonfor low accuracy may be that there are many short-est derivations, i.e.
derivations that are all built withthe fewest number of fragments, and that tie break-ing could be at fault.
To investigate this, we triedvarious methods for tie-breaking: FIRST/LAST (pro-cedurally break ties), UNIFORM (sample derivationsequally), FREQ (use the frequency of local rules).However, none of these methods help much, giv-ing results within a percentage of F1.
In fact, evenoracle tie-breaking, where ties are broken to favorthe number of gold constituents in the derivationachieves only 80% F1, indicating that correct deriva-tions are often not the shortest ones.
Another rea-son for the poor performance of SDP may be thatthe parameter-free treatment of the lexical layer isparticularly pathological.
Indeed, this hypothesis ispartially verified by the result that using a lexicon(similar to that in Petrov et al (2006)) at the termi-nal layer brings the uniform tie-breaking result up to80% F1.
However, combining a lexicon with oracletie-breaking yields only 81.8% F1.These results at first seem quite discouraging, butwe will show that they can be easily improved withinformation from even a simple PCFG.7214 Improvements from a Coarse PCFGThe additional information that makes shortestderivation parsing work comes from a coarse un-lexicalized PCFG.
In the standard way, our PCFGconsists of the local (depth-1) rules X ?
Y Z withprobability P (Y Z|X) computed using the countof the rule and the count of the nonterminal X inthe given treebank (no smoothing was used).
Ourcoarse grammar uses a lexicon with unknown wordclasses, similar to that in Petrov et al (2006).
Whentaken from a binarized treebank with one level ofparent annotation (Johnson, 1998) and horizontalmarkovization, the PCFG is quite small, with around3500 symbols and 25000 rules; it achieves an accu-racy of 84% on its own (see Table 2), so the PCFGon its own is better than the basic SDP, but still rela-tively weak.When filtered by a coarse PCFG pass, how-ever, SDP becomes both fast and accurate, even forthe basic, lexicon-free SDP formulation.
Summedmarginals (posteriors) are computed in the coarsePCFG and used for pruning and tie-breaking in theSDP chart, as described next.
Pruning works in thestandard coarse-to-fine (CTF) way (see Charniak etal.
(2006)).
If a particular base symbol X is prunedby the PCFG coarse pass for a particular span (i, j)(i.e., the posterior marginal P (X, i, j|s) is less thana certain threshold), then in the full SDP pass we donot allow building any indexed symbol Xl of type Xfor span (i, j).
In all our pruning-based experiments,we use a log posterior threshold of ?3.8, tuned onthe WSJ development set.We also use the PCFG coarse pass for tie-breaking.
During Viterbi shortest-derivation pars-ing (after coarse-pruning), if two derivations havethe same cost (i.e., the number of switches), then webreak the tie between them by choosing the deriva-tion which has a higher sum of coarse posteriors(i.e., the sum of the coarse PCFG chart-cell pos-teriors P (X, i, j|s) used to build the derivation).4The coarse PCFG has an extremely beneficial in-teraction with the fine all-fragments SDP grammar,wherein the accuracy of the combined grammarsis significantly higher than either individually (see4This is similar to the maximum recall objective for approx-imate inference (Goodman, 1996b).
The product of posteriorsalso works equally well.dev (?
40) test (?
40)Model F1 EX F1 EXB&K2010 pruned 88.4 33.7 88.5 33.0B&K2010 unpruned 87.9 32.4 88.1 31.9Table 1: Accuracy (F1) and exact match (EX) for Bansal andKlein (2010).
The pruned row shows their original results withcoarse-to-fine pruning.
The unpruned row shows new resultsfor an unpruned version of their parser; these accuracies arevery similar to their pruned counterparts.Table 2).
In addition, the speed of parsing andmemory-requirements improve by more than an or-der of magnitude over the exact SDP pass alone.It is perhaps surprising that coarse-pass pruningimproves accuracy by such a large amount for SDP.Indeed, given that past all-fragments work has useda coarse pass for speed, and that we are the first (toour knowledge) to actually parse at scale with animplicit grammar without such a coarse pass, it isa worry that previous results could be crucially de-pendent on fortuitous coarse-pass pruning.
To checkone such result, we ran the full, weighted AFG con-struction of Bansal and Klein (2010) without anypruning (using the maximum recall objective as theydid).
Their results hold up without pruning: the re-sults of the unpruned version are only around 0.5%less (in parsing F1) than the results achieved withpruning (see Table 1).
However, in the case of ourshortest-derivation parser, the coarse-pass is essen-tial for high accuracies (and for speed and memory,as always).5 ResultsWe have seen that basic, unpruned SDP is both slowand inaccurate, but improves greatly when comple-mented by a coarse PCFG pass; these results areshown in Table 2.
Shortest derivation parsing with aPCFG coarse-pass (PCFG+SDP) achieves an accu-racy of nearly 87% F1 (on the WSJ test set, ?
40word sentences), which is significantly higher thanthe accuracy of the PCFG or SDP alone.5 Whenthe coarse PCFG is combined with basic SDP, themajority of the improvement comes from pruningwith the coarse-posteriors; tie-breaking with coarse-posteriors contributes around 0.5% F1 over pruning.5PCFG+SDP accuracies are around 3% higher in F1 and10% higher in EX than the PCFG-only accuracies.722dev (?
40) test (?
40) test (all)Model F1 EX F1 EX F1 EXSDP 66.2 18.0 66.9 18.4 64.9 17.3PCFG 83.8 20.0 84.0 21.6 83.2 20.1PCFG+SDP 86.4 30.6 86.9 31.5 86.0 29.4Table 2: Our primary results on the WSJ task.
SDP is thebasic unpruned shortest derivation parser.
PCFG results arewith one level of parent annotation and horizontal markoviza-tion.
PCFG+SDP incorporates the coarse PCFG posteriors intoSDP.
See end of Section 5 for a comparison to other parsingapproaches.Figure 2 shows the number of fragments for short-est derivation parsing (averaged for each sentencelength).
Note that the number of fragments is ofcourse greater for the combined PCFG+SDP modelthan the exact basic SDP model (which is guaranteedto be minimal).
This result provides some analysisof how coarse-pruning helps SDP: it illustrates thatthe coarse-pass filters out certain short but inaccu-rate derivations (that the minimal SDP on its own isforced to choose) to improve performance.Figure 3 shows the parsing accuracy of thePCFG+SDP model for various pruning thresholdsin coarse-to-fine pruning.
Note how this is differ-ent from the standard coarse-pass pruning graphs(see Charniak et al (1998), Petrov and Klein (2007),Bansal and Klein (2010)) where only a small im-provement is achieved from pruning.
In contrast,coarse-pass pruning provides large accuracy benefitshere, perhaps because of the unusual complementar-ity of the two grammars (typical coarse passes aredesigned to be as similar as possible to their finecounterparts, even explicitly so in Petrov and Klein(2007)).Our PCFG+SDP parser is more accurate than re-cent sampling-based TSG?s (Post and Gildea, 2009;Cohn and Blunsom, 2010), who achieve 83-85% F1,and it is competitive with more complex weighted-fragment approaches.6 See Bansal and Klein (2010)for a more thorough comparison to other parsingwork.
In addition to being accurate, the PCFG+SDPparser is simple and fast, requiring negligible train-ing and tuning.
It takes 2 sec/sentence, less than 2GB of memory and is written in less than 2000 lines6Bansal and Klein (2010) achieve around 1.0% higher F1than our results without a lexicon (character-level parsing) and1.5% higher F1 with a lexicon.05101520250 4 8 12 16 20 24 28 32 36 40# of fragmentssentence lengthPCFG + SDPSDPFigure 2: The average number of fragments in shortest deriva-tion parses, computed using the basic version (SDP) and thepruned version (PCFG+SDP), for WSJ dev-set (?
40 words).65.07 0.075.08 0.085.09 0.0-3 -5 -7 -9 -11 -13 -15 -17Coarse-pass Log Posterior Threshold (PT)F1No Pruning (PT = -inf)Figure 3: Parsing accuracy for various coarse-pass pruningthresholds (on WSJ dev-set ?
40 words).
A larger thresholdmeans more pruning.
These are results without the coarse-posterior tie-breaking to illustrate the sole effect of pruning.of Java code, including I/O.75.1 Other TreebanksOne nice property of the parameter-free, all-fragments SDP approach is that we can easily trans-fer it to any new domain with a treebank, or anynew annotation of an existing treebank.
Table 3shows domain adaptation performance by the re-sults for training and testing on the Brown andGerman datasets.8 On Brown, we perform betterthan the relatively complex lexicalized Model 1 ofCollins (1999).
For German, our parser outperformsDubey (2005) and we are not far behind latent-variable parsers, for which parsing is substantially7These statistics can be further improved with standard pars-ing micro-optimization.8See Gildea (2001) and Petrov and Klein (2007) for the ex-act experimental setup that we followed here.723test (?
40) test (all)Model F1 EX F1 EXBROWNGildea (2001) 84.1 ?
?
?This Paper (PCFG+SDP) 84.7 34.6 83.1 32.6GERMANDubey (2005) 76.3 ?
?
?Petrov and Klein (2007) 80.8 40.8 80.1 39.1This Paper (PCFG+SDP) 78.1 39.3 77.1 38.2Table 3: Results for training and testing on the Brown andGerman treebanks.
Gildea (2001) uses the lexicalized Collins?Model 1 (Collins, 1999).test (?
40) test (all)Annotation F1 EX F1 EXSTAN-ANNOTATION 88.1 34.3 87.4 32.2BERK-ANNOTATION 90.0 38.9 89.5 36.8Table 4: Results with richer WSJ-annotations from Stanfordand Berkeley parsers.more complex.5.2 Treebank AnnotationsPCFG+SDP achieves 87% F1 on the English WSJtask using basic annotation only (i.e., one levelof parent annotation and horizontal markoviza-tion).
Table 4 shows that by pre-transforming theWSJ treebank with richer annotation from previ-ous work, we can obtain state-of-the-art accuraciesof up to 90% F1 with no change to our simpleparser.
In STAN-ANNOTATION, we annotate thetreebank symbols with annotations from the Stan-ford parser (Klein and Manning, 2003).
In BERK-ANNOTATION, we annotate with the splits learnedvia hard-EM and 5 split-merge rounds of the Berke-ley parser (Petrov et al, 2006).6 ConclusionOur investigation of shortest-derivation parsingshowed that, in the exact case, SDP performs poorly.When pruned (and, to a much lesser extent, tie-broken) by a coarse PCFG, however, it is competi-tive with a range of other, more complex techniques.An advantage of this approach is that the fine SDPpass is actually quite simple compared to typical finepasses, while still retaining enough complementarityto the coarse PCFG to increase final accuracies.
Oneaspect of our findings that may apply more broadlyis the caution that coarse-to-fine methods may some-times be more critical to end system quality thangenerally thought.AcknowledgmentsWe would like to thank Adam Pauls, Slav Petrovand the anonymous reviewers for their helpful sug-gestions.
This research is supported by BBN un-der DARPA contract HR0011-06-C-0022 and by theOffice of Naval Research under MURI Grant No.N000140911081.ReferencesMohit Bansal and Dan Klein.
2010.
Simple, AccurateParsing with an All-Fragments Grammar.
In Proceed-ings of ACL.Rens Bod.
1993.
Using an Annotated Corpus as aStochastic Grammar.
In Proceedings of EACL.Rens Bod.
2000.
Parsing with the Shortest Derivation.In Proceedings of COLING.Rens Bod.
2001.
What is the Minimal Set of Fragmentsthat Achieves Maximum Parse Accuracy?
In Proceed-ings of ACL.Eugene Charniak, Sharon Goldwater, and Mark Johnson.1998.
Edge-Based Best-First Chart Parsing.
In Pro-ceedings of the 6th Workshop on Very Large Corpora.Eugene Charniak, Mark Johnson, et al 2006.
Multi-level Coarse-to-fine PCFG Parsing.
In Proceedings ofHLT-NAACL.Trevor Cohn and Phil Blunsom.
2010.
Blocked Inferencein Bayesian Tree Substitution Grammars.
In Proceed-ings of NAACL.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania, Philadelphia.A.
Dubey.
2005.
What to do when lexicalization fails:parsing German with suffix analysis and smoothing.In ACL ?05.Daniel Gildea.
2001.
Corpus variation and parser perfor-mance.
In Proceedings of EMNLP.Joshua Goodman.
1996a.
Efficient Algorithms for Pars-ing the DOP Model.
In Proceedings of EMNLP.Joshua Goodman.
1996b.
Parsing Algorithms and Met-rics.
In Proceedings of ACL.Joshua Goodman.
2003.
Efficient parsing of DOP withPCFG-reductions.
In Bod R, Scha R, Sima?an K (eds.
)Data-Oriented Parsing.
University of Chicago Press,Chicago, IL.724Mark Johnson.
1998.
PCFG Models of Linguistic TreeRepresentations.
Computational Linguistics, 24:613?632.Dan Klein and Christopher Manning.
2003.
AccurateUnlexicalized Parsing.
In Proceedings of ACL.Slav Petrov and Dan Klein.
2007.
Improved Inferencefor Unlexicalized Parsing.
In Proceedings of NAACL-HLT.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, andInterpretable Tree Annotation.
In Proceedings ofCOLING-ACL.Matt Post and Daniel Gildea.
2009.
Bayesian Learningof a Tree Substitution Grammar.
In Proceedings ofACL-IJCNLP.Philip Resnik.
1992.
Probabilistic Tree-AdjoiningGrammar as a Framework for Statistical Natural Lan-guage Processing.
In Proceedings of COLING.Khalil Sima?an.
2000.
Tree-gram Parsing: Lexical De-pendencies and Structural Relations.
In Proceedingsof ACL.725
