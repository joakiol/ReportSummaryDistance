An Experiment in Semantic Tagging using Hidden Markov Model TaggingFr~d~rique Segond, Anne Schiller, Gregory Grefenstette, Jean-Pierre ChanodRank Xerox Research Centre, 6 Chemin de Maupertuis, F-38240 Meylan, France{Segond, Schiller, Grefenstette, Chanod}@grenoble.rxrc.xerox.comAbstractThe same word can have many different mean-ings depending on the context in which it isused.
Discovering the meaning of a word, giventhe text around it, has been an interesting prob-lem for both the psychology and the artificialintelligence research communities.
In this arti-cle, we present a series of experiments, usingmethods which have proven to be useful foreliminating part-of-speech ambiguity, to see ifsuch simple methods can be used to resolve se-mantic ambiguities.
Using a publicly availablesemantic lexicon, we find the Hidden MarkovModels work surprising well at choosing theright semantic ategories, once the sentence hasbeen stripped of purely functional words.1 IntroductionAny natural language processing system treating any-thing beyond the most restricted omains is confrontedwith the problem of distinguishing between uses ofpolysemous words.
The idea behind semantically tag-ging words is that sense markings added to words maybe used by some automatic process in order to choosethe proper senses of words in a given context.
For exam-ple, the word bark would receive at least two possiblesemantic tags and these tags along with the tags of otherwords in the surrounding context would allow the proc-ess to distinguish between the senses the bark of a tree,and the bark of a dog.
(See \[Dagan and Itai, 1994; Galeet al, 1992a; Gale et al 1992b; Ng and Lee, 1996;Wilks, 1996; Yarowski, 1992; Yarowski, 1995\] for re-cent work on word sense disambiguation).Semantic tagging is considered to be a much more dif-ficult task than part-of-speech tagging.
Despite thiscurrent hinking, we decided to perform an experimentto see how well words can be semantically disambigu-ated using techniques that have proven to be effective inpart-of-speech tagging.
We decided to use the 45 se-mantic tags available through the WordNet package.
Inthis typology, the word bark has two a priori semantictags: bark as a "'covering, natural covering, cover" re-ceives tag 20 (nouns denoting plants); and bark as"'noise, cry" has tag 11 (nouns denoting natural events).This semantic tagset has two advantages: it is a reason-able size, so that statistical techniques that we are testingdo not need an inordinate amount of training data; andsecondly, a semantically tagged corpus is available thatwe can use for testing.2 WordNet Semantic tagsPart-of-speech tagging is better understood than seman-tic tagging.
For one thing, no consensus on semantic tagsexists, contrary to the general consensus on the higherlevel part-of-speech tags.
And it seems more likely thatsyntactic tags be generalizable over wider textual do-mains than semantic ones.Despite this, the WordNet eam has taken upon them-selves to create a general semantic tagging scheme andto apply it on a large scale: every set of synonymoussenses, synsets, are tagged with one of 45 tags as Word-Net version 1.5 ~.
In their schema, there are3 tags foradjectives (relational adjectives, participial adjectivesand all others), 1 tag for all adverbs, 26 tags for nouns(act, animal, man-made artifact, attributes, body parts,.... substance, and time), and 15 tags for verbs (fromgrooming and dressing verbs, to verbs of weather).These tags are assigned for the most general uses ofwords.
For example, the noun blood is tagged as 07 (anattribute of people and objects), as 08 (body part) and as14 (groupings of people and objects).
Blood is nottagged as 27 (substance) or as 13 (food), though it mightwell be considered as such in certain contexts.1 Ftp-able at clarity princeton edu783 HMM TaggingWe wanted to see how well these WordNet semantic tagscould be disambiguated using the same well-understoodtechniques employed in statistical part-of-speech disam-biguation.
Part-of-speech disambiguation relies on thefact that certain sequences of parts of speech are moreprobable than others.
Often, this probability is estimatedfrom the frequency of sequences of tags in hand taggedtexts.In our experiments, we used the Hidden MarkovModel (HMM) tagging method escribed in \[Cutting etaL, 1992\].
In this method, the probability of seeing agiven tag depends on the ambiguity class of the wordand on the ambiguity class of the words preceding it.
Anambigmty class of a word is the set of words which eachhave exactly the same set of ambiguous tags.
This classis used during the Xerox HMM tagging in place of morespecific lexical (= word-based) probabilities.
Lexicalprobabilities would more accurately inform the taggerwith the frequency with which a certain word receives acertain tag, but acquiring this frequency requires muchgreater amounts of tagged text than is necessary with theambiguity class method.
The HMM training and taggingprograms in our experiment \[Wilkens and Kupiec, 1995\]are based on bigrams, i.e.
only the immediate context ofa word is taken into account.The use of this statistical disambiguation combines withthe advantage of the limited number of WordNet ags sothat training can be performed on a relatively small cor-pus.4 Data Preparation and Tagger Train-ingIn order to make a HMM for semantic tags we per-formed the following steps:1.
We derived a lexicon from the WordNet data fileswhich contains all possible semantic tags for each noun,adjective, adverb and verb.
Words having no semantictags (determiners, prepositions, auxiliary verbs, etc.)
areassigned their part of speech tags.2.
With version 1.5 of WordNet is delivered aboutone-fifth of the Brown corpus which has been semanti-cally tagged by the WordNet team.
From these 11,182sentences, we constructed a traming corpus and a testcorpus of equal size, taking all even numbered sentencesfor the training corpus and all odd-numbered sentencesfor the test corpus.
From both corpora, in order to use"'semantically relevant" tokens for the HMM bigrams,we retained all nouns, verbs, adverbs, and adjectives anddeleted all function words except prepositions, commas,final stops, personal pronouns and interrogative adverbs.3.
We computed a HMM model based on the trainingcorpus, ran the resulting semantic tagger on an untaggedversion of test corpus and we compared the tags as-signed by the semantic tagger to original tags in the testcorpus.5 Tagging Results5.1 Test 1As described above, the semantically tagged text pro-vided by WordNet (CO) was transformed into a trainingcorpus (C 1).
(co) The/DT Fulton_County_Grand_Jury/03said/32 Friday/28 an/DT investlgation/09of/IN Atlanta/15 's/POS recent/00pmmary_election/04 produced/39 "'/'"no/DT evxdence/09 "/" that/IN any/DTirregularmes/04 took_place/30 ./.
(CI) Fulton_County_Grand Jury/03 sald/32Friday/28 investigation/09 of/INAtlanta/15 recent/00 prlmary_election/04produced/39 evldence/09 that/INirregularities/04 took place/30 ./.The lexicon used for this experiment contains 3,282different ambiguity classes made of 52 semantic tags(45 WordNet tags + 6 pan-of-speech tags + 1 tag fornon-lexicalized word-forms).The training corpus consists of 75,000 tokens andcovers about 72% of all possible ambiguity classes.
Thetest corpus contains 90,000 tokens.
46% of the words areambiguous, i.e.
the lexicon provides at least two (and atmost 15) different semantic tags for these words.For the test corpus the overall accuracy was of 86%and the accuracy over ambiguous tokens of 71% cor-rectly chosen WordNet semantic tags.5.2 Test 2In fact, the first experiment combined syntactic andsemantic tagging, as the WordNet ags are classified bypart-of-speech categories.Thus we run a second experiment which applies se-mantic tagging after part-of-speech tagging.
We simu-lated the part-of-speech tagging step by adding a syntac-tic category to the training and test corpus:(C3) FFulton_County_Grand_Jury=NOUN / 0379said=VERB/32 Friday=NOUN/28mvestlgation/09 of/IN Atlanta=NOUN/15recent=ADJ/00 primary_election=NOUN / 04produced=VERB/39 evidence=NOUN/09that/IN irregularities=NOUN / 04tookplace=VERB/30 ./.We modified the lexicon accordingly.
For example, asingle lexicon entry for bark was divided into two en-tries for the verb and for the noun reading:(LI) bark {06, 11, 20, 30, 32, 35}(L2} bark=VERB {30, 32, 35}bark=NOUN {06, 11, 20}.Using part-of-speech pre-tagging, the number of am-biguity classes decreases (1685) and only 27% of theword forms in the test corpus are ambiguous.With this method, the accuracy over the entire text isof 89%.
This improvement is mainly due to the loweroverall ambiguity rate: part-of-speech pre-tagging solvedthe "'semantic" ambiguity for 40% of the ambiguouswords in Test 1.
The error rate for those words whichremain ambiguous after part-of-speech disambiguation isalmost identical (71% correctly chosen tags) for bothtest cases.5.3 Test 3For the part-of-speech tagging problem, it is known thatassigning the most common part of speech for each lexi-cal item gives a baseline of 90% accuracy \[Brill, 1992\].In order to see what a similar baseline is for semantictagging over part-of-speech tagged text, we performedthe following experiment.
From the training corpus, wecalculated the most frequent semantic tag for each part-of-speech tagged lemma 2.
On the test corpus, we as-signed the most frequent semantic tag to each knownword, and for unknown nouns, verbs, adverbs, and ad-jectives, we assigned the most common semantic tag perpart-of-speech.
Capitalized unknown nouns were as-signed the S03 tag.
Non-semantically tagged words wereconsidered correctly tagged.
The result of this taggingresulted in a baseline of 81% of correctly chosen seman-tic tags over all words, worse than the two precedingtests.6 D iscuss ion  and Conc lus ionWe found it surprising that the same statistical tech-niques that improve part-of-speech tag disambiguationfrom a baseline of 90% to 95-96% work almost as wellwith semantic tags once function words are removedfrom the text to be tagged.
The HMM technique im-proved the baseline 81% to 89% correctly chosen se-2 Ties were resolved by randomly choosing one of the se-mantic tags.mantic tags.
These experiments show renewed promisefor a statistical approach to the problem of sense disam-biguation, with a relatively small training set.Future plans include analyzing the kind of errors weget, to classify them.
Starting from this classification wehope to be able to answer the following questions: whattype of semantic tags should be used, should a non-binary HMM be used, and how much ambiguity can beresolved using local clues.We also plan to consider easonable applications forsemantic tagging.
One possibility would be to use se-mantic tagging in the framework of an intelligent on linedictionary lookup such as LocoLex \[Bauer et al 1995\].LocoLex is a tool that has been developed at RXRC andwhich looks up a word in a bilingual dictionary takingthe syntactic ontext into account.
For instance, in asentence such as They like to swzm the part of speechtagger in LocoLex determines that hke is a verb and nota preposition.
Accordingly, the dictionary lookup com-ponent provides the user with the translation for the verbonly.
LocoLex also detects multi-word expressions 3.
Forinstance, when stuck appears in the sentence my ownparents tuck together the translation displayed after theuser clicks on stuck is the one for the whole phrase sttcktogether and not only for the word stick.Currently LocoLex is purely syntactic and cannot dis-tinguish between the different meanings of a noun likebark.
If, in addition to the current syntactic tags, we hadaccess to the semantic tags provided by WordNet for thisword (natural event and plants) and we were able toinclude this label in the online dictionary, this wouldimprove the bilingual dictionary access of Locolex evenfurther.Current bilingual dictionaries often include some se-mantic marking.
For instance, in the OUP-HachetteEnglish French dictionary, under bark we find the labelBot(anical) attached to one meaning and the collocator(of dog) associated with the other one.
It is possible thatsome type of automated matching between these indica-tions and the WordNet semantic tags 4would allow theintegration of a semantic tagger into LocoLex.Using only existing dictionary labels might still not becompletely satisfying for machine translation.
Indeedlooking back at the example my own parents stuck to-gether even if we retrieved the multi-word expressionmeaning it will be difficult o decide which translation tochoose with existing dictionary indications s.3 Multi-words expressions i clude ldtomattc expression (tosweep something under the rug), phrasal verbs (to spa up), orcompounds (warmng hght)4 Or some other derwed tag set.5 Especially considering that WordNet provides only twosenses of stick together $35 and $41.80For instance for stzck together the Oxford-HachetteEnglish French dictionary gives:stick together1.
(become fixed to each other)(pages) se coller2.
(CoU) (remain loyal~se serrer les coudes (Faro) 6tre sohdalre3.
(Coil) (not separate)rester ensembleIt appears clearly that using general dictionary labelswould not be enough to choose the third meaning only.We would need to investigate further how to make betteruse of dictionary information such as collocators, etc.Another interesting application we would like to ex-amine is how useful semantic tagging could be in deter-mining the genre or topic of a text.
Here, an initial ideawould be to just count the number of occurrence of agiven semantic tag and from this to determine the topicor the genre of a given text.
This could be useful in ma-chine translation system to help, for instance, in choos-ing the appropriate l xicon (containing the specific ter-minology).
Assuming that such dictionaries are lessambiguous, this could in return, improve the accuracy ofthe lexical semantic hoice in automatic translation.Integrating Multiple Knowledge sources to disambiguateword sense.
Proceedings of ACL 96.
1996.\[Wilkens and Kupiec, 1995\] Mike Wilkens and JulianKupiec.
Training Hidden Markov Models for Part-of-speech Tagging.
Internal document, Xerox Corporation.1995.\[Wilks, 1996\] Wilks Y.
Oral communication.
Cour-mayeur, 1996.\[Yarowski, 1992\] Yarowsky D. Word Sense Disam-biguation Using Statistical Models of Roget's CategoriesTrained on Large Corpora.
Proceedings of COLING-92.1992.\[Yarowski, 1995\] Yarowsky D. Unsupervised WordSense Disambiguation Methods Rivaling SupervisedMethods.
Proceedmgs of ACL-95.
1995.References\[Bauer et al 1995\] Daniel Bauer, Fr6d6rique Segond,Annie Zaenen.
LOCOLEX : the translation rolls off yourtongue.
Proceedings of ACH-ALLC95.
Santa-Barbara,USA, July 1995.\[Brill, 1992\] Eric Brill.
A simple Rule-Bases Part ofSpeech Tagger.
Proceedings OfANLP-92.
Trento, Italy,1992.\[Cutting et al 1992\] Doug Cutting, Julian Kupiec, JanPedersen, and Penelope Sibun.
A Practical Part-of-speech Tagger.
Proceedmgs of ANLP-92.
Trento, Italy,1992.\[Dagan and Itai, 1994\] Dagan I. and Itai A.
Word SenseDisambiguation Using a Second Language MonolingualCorpus.
Computational Lmgutsttcs, 20-4, 563-596,1994.\[Gale et al 1992a\] Gale, Church and Yarowsky.
AMethod for Disambiguating Word Senses in a Corpus.Computers and the Humantties 26, 415-439, 1992.\[Gale t al, 1992b\] Gale, Church and Yarowsky.
UsingBilingual Materials to Develop Word Sense Disam-biguation Methods.
Proceedmgs of TMI-92.
1992.\[Ng and Lee,1996} Hwee Tou Ng and Hian Beng Lee.81
