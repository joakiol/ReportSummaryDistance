Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 291?295,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsTranslation Model Size Reduction forHierarchical Phrase-based Statistical Machine TranslationSeung-Wook Lee?
Dongdong Zhang?
Mu Li?
Ming Zhou?
Hae-Chang Rim??
Dept.
of Computer & Radio Comms.
Engineering, Korea University, Seoul, South Korea{swlee,rim}@nlp.korea.ac.kr?
Microsoft Research Asia, Beijing, China{dozhang,muli,mingzhou}@microsoft.comAbstractIn this paper, we propose a novel method ofreducing the size of translation model for hier-archical phrase-based machine translation sys-tems.
Previous approaches try to prune in-frequent entries or unreliable entries based onstatistics, but cause a problem of reducing thetranslation coverage.
On the contrary, the pro-posed method try to prune only ineffectiveentries based on the estimation of the infor-mation redundancy encoded in phrase pairsand hierarchical rules, and thus preserve thesearch space of SMT decoders as much aspossible.
Experimental results on Chinese-to-English machine translation tasks show thatour method is able to reduce almost the halfsize of the translation model with very tinydegradation of translation performance.1 IntroductionStatistical Machine Translation (SMT) has gainedconsiderable attention during last decades.
From abilingual corpus, all translation knowledge can beacquired automatically in SMT framework.
Phrase-based model (Koehn et al, 2003) and hierarchicalphrase-based model (Chiang, 2005; Chiang, 2007)show state-of-the-art performance in various lan-guage pairs.
This achievement is mainly benefitfrom huge size of translational knowledge extractedfrom sufficient parallel corpus.
However, the errorsof automatic word alignment and non-parallelizedbilingual sentence pairs sometimes have caused theunreliable and unnecessary translation rule acquisi-tion.
According to Bloodgood and Callison-Burch(2010) and our own preliminary experiments, thesize of phrase table and hierarchical rule table con-sistently increases linearly with the growth of train-ing size, while the translation performance tends togain minor improvement after a certain point.
Con-sequently, the model size reduction is necessary andmeaningful for SMT systems if it can be performedwithout significant performance degradation.
Thesmaller the model size is, the faster the SMT de-coding speed is, because there are fewer hypothesesto be investigated during decoding.
Especially, in alimited environment, such as mobile device, and fora time-urgent task, such as speech-to-speech transla-tion, the compact size of translation rules is required.In this case, the model reduction would be the oneof the main techniques we have to consider.Previous methods of reducing the size of SMTmodel try to identify infrequent entries (Zollmannet al, 2008; Huang and Xiang, 2010).
Several sta-tistical significance testing methods are also exam-ined to detect unreliable noisy entries (Tomeh et al,2009; Johnson et al, 2007; Yang and Zheng, 2009).These methods could harm the translation perfor-mance due to their side effect of algorithms; simi-lar multiple entries can be pruned at the same timedeteriorating potential coverage of translation.
Theproposed method, on the other hand, tries to mea-sure the redundancy of phrase pairs and hierarchi-cal rules.
In this work, redundancy of an entry isdefined as its translational ineffectiveness, and esti-mated by comparing scores of entries and scores oftheir substituents.
Suppose that the source phrases1s2 is always translated into t1t2 with phrase en-try <s1s2?t1t2> where si and ti are correspond-291ing translations.
Similarly, source phrases s1 ands2 are always translated into t1 and t2, with phraseentries, <s1?t1> and <s2?t2>, respectively.
Inthis case, it is intuitive that <s1s2?t1t2> could beunnecessary and redundant since its substituent al-ways produces the same result.
This paper presentsstatistical analysis of this redundancy measurement.The redundancy-based reduction can be performedto prune the phrase table, the hierarchical rule table,and both.
Since the similar translation knowledgeis accumulated at both of tables during the train-ing stage, our reduction method performs effectivelyand safely.
Unlike previous studies solely focus oneither phrase table or hierarchical rule table, thiswork is the first attempt to reduce phrases and hi-erarchical rules simultaneously.2 Proposed ModelGiven an original translation model, TM , our goalis to find the optimally reduced translation model,TM?, which minimizes the degradation of trans-lation performance.
To measure the performancedegradation, we introduce a new metric named con-sistency:C(TM,TM?)
=BLEU(D(s;TM),D(s;TM?))
(1)where the function D produces the target sentenceof the source sentence s, given the translation modelTM .
Consistency measures the similarity betweenthe two groups of decoded target sentences producedby two different translation models.
There are num-ber of similarity metrics such as Dices coefficient(Kondrak et al, 2003), and Jaccard similarity coef-ficient.
Instead, we use BLEU scores (Papineni etal., 2002) since it is one of the primary metrics formachine translation evaluation.
Note that our con-sistency does not require the reference set while theoriginal BLEU does.
This means that only (abun-dant) source-side monolingual corpus is needed topredict performance degradation.
Now, our goal canbe rewritten with this metric; among all the possiblereduced models, we want to find the set which canmaximize the consistency:TM?
= argmaxTM ?
?TMC(TM,TM ?)
(2)In minimum error rate training (MERT) stages,a development set, which consists of bilingual sen-tences, is used to find out the best weights of fea-tures (Och, 2003).
One characteristic of our methodis that it isolates feature weights of the transla-tion model from SMT log-linear model, trying tominimize the impact of search path during decod-ing.
The reduction procedure consists of threestages: translation scoring, redundancy estimation,and redundancy-based reduction.Our reduction method starts with measuring thetranslation scores of the individual phrase and thehierarchical rule.
Similar to the decoder, the scoringscheme is based on the log-linear framework:PS(p) =?i?ihi(p) (3)where h is a feature function and ?
is its weight.As the conventional hierarchical phrase-based SMTmodel, our features are composed of P (e|f ), P (f |e),Plex(e|f ), Plex(f |e), and the number of phrases,where e and f denote a source phrase and a targetphrase, respectively.
Plex is the lexicalized proba-bility.
In a similar manner, the translation scores ofhierarchical rules are calculated as follows:HS(r) =?i?ihi(r) (4)The features are as same as those that are used forphrase scoring, except the last feature.
Instead of thephrase number penalty, the hierarchical rule num-ber penalty is used.
The weight for each feature isshared from the results of MERT.
With this scoringscheme, our model is able to measure how importantthe individual entry is during decoding.Once translation scores for all entries are es-timated, our method retrieves substituent candi-dates with their combination scores.
The combina-tion score is calculated by accumulating translationscores of every member as follows:CS(p1...n) =n?i=1PS(pi) (5)This scoring scheme follows the same mannerwhat the conventional decoder does, finding the bestphrase combination during translation.
By compar-ing the original translation score with combination292scores of its substituents, the redundancy scores areestimated, as follows:Red(p) = minp1...n?Sub(p)PS(p)?CS(p1...n) (6)where Sub is the function that retrieves all possi-ble substituents (the combinations of sub-phrases,and/or sub-rules that exactly produce the same tar-get phrase, given the source phrase p).
If the com-bination score of the best substituent is same as thetranslation score of p, the redundancy score becomeszero.
In this case, the decoder always produces thesame translation results without p. When the redun-dancy score is negative, the best substituent is morelikely to be chosen instead of p. This implies thatthere is no risk to prune p; the search space is notchanged, and the search path is not changed as well.Our method can be varied according to the desig-nation of Sub function.
If both of the phrase tableand the hierarchical rule table are allowed, cross re-duction can be possible; the phrase table is reducedbased on the hierarchical rule table and vice versa.With extensions of combination scoring and redun-dancy scoring schemes like following equations, ourmodel is able to perform cross reduction.CS(p1...n, h1...m) =n?i=1PS(pi) +m?i=1HS(hi) (7)Red(p) = min<p1...n,h1...m>?Sub(p)PS(p)?
CS(p1...n, h1...m) (8)The proposed method has some restrictions forreduction.
First of all, it does not try to prune thephrase that has no substituents, such as unigramphrases; the phrase whose source part is composedof a single word.
This restriction guarantees thatthe translational coverage of the reduced model isas high as those of the original translation model.In addition, our model does not prune the phrasesand the hierarchical rules that have reordering withinit to prevent information loss of reordering.
Forinstance, if we prune phrase, <s1s2s3?t3t1t2>,phrases, <s1s2?t1t2> and <s3?t3> are not ableto produce the same target words without appropri-ate reordering.Once the redundancy scores for all entries havebeen estimated, the next step is to select the bestN entries to prune to satisfy a desired model size.We can simply prune the first N from the list of en-tries sorted by increasing order of redundancy score.However, this method may not result in the opti-mal reduction, since each redundancy scores are es-timated based on the assumption of the existence ofall the other entries.
In other words, there are depen-dency relationships among entries.
We examine twomethods to deal with this problem.
The first is toignore dependency, which is the more efficient man-ner.
The other is to prune independent entries first.After all independent entries are pruned, the depen-dent entries are started to be pruned.
We present theeffectiveness of each method in the next section.Since our goal is to reduce the size of all transla-tion models, the reduction is needed to be performedfor both the phrase table and the hierarchical ruletable simultaneously, namely joint reduction.
Sim-ilar to phrase reduction and hierarchical rule reduc-tion, it selects the best N entries of the mixture ofphrase and hierarchical rules.
This method resultsin safer pruning; once a phrase is determined to bepruned, the hierarchical rules, which are related tothis phrase, are likely to be kept, and vice versa.3 ExperimentWe investigate the effectiveness of our reductionmethod by conducting Chinese-to-English transla-tion task.
The training data, as same as Cui etal.
(2010), consists of about 500K parallel sentencepairs which is a mixture of several datasets pub-lished by LDC.
NIST 2003 set is used as a devel-opment set.
NIST 2004, 2005, 2006, and 2008 setsare used for evaluation purpose.
For word align-ment, we use GIZA++1, an implementation of IBMmodels (Brown et al, 1993).
We have implementeda hierarchical phrase-based SMT model similar toChiang (2005).
The trigram target language modelis trained from the Xinhua portion of English Gi-gaword corpus (Graff and Cieri, 2003).
Sampled10,000 sentences from Chinese Gigaword corpus(Graff, 2007) was used for source-side developmentdataset to measure consistency.
Our main met-ric for translation performance evaluation is case-1http://www.statmt.org/moses/giza/GIZA++.html2930.600.700.800.901.00ConsistencyFreq-CutoffNoDepDepCrossNoDepCrossDep0.2860.2900.2940.2980% 10% 20% 30% 40% 50% 60%BLEUPhrase Reduction Ratio0% 10% 20% 30% 40% 50% 60%Hierarchical Rule Reduction Ratio0% 10% 20% 30% 40% 50% 60%Joint Reduction RatioFigure 1: Performance comparison.
BLEU scores and consistency scores are averaged over four evaluation sets.insensitive BLEU-4 scores (Papineni et al, 2002).As a baseline system, we chose the frequency-based cutoff method, which is one of the mostwidely used filtering methods.
As shown in Fig-ure 1, almost half of the phrases and hierarchicalrules are pruned when cutoff=2, while the BLEUscore is also deteriorated significantly.
We intro-duced two methods for selecting the N pruningentries considering dependency relationships.
Thenon-dependency method does not consider depen-dency relationships, while the dependency methodprunes independent entries first.
Each method can becombined with cross reduction.
The performance ismeasured in three different reduction tasks: phrasereduction, hierarchical rule reduction, and joint re-duction.
As the reduction ratio becomes higher,the model size, i.e., the number of entries, is re-duced while BLEU scores and coverage are de-creased.
The results show that the translation per-formance is highly co-related with the consistency.The co-relation scores measured between them onthe phrase reduction and the hierarchical rule reduc-tion tasks are 0.99 and 0.95, respectively, which in-dicates very strong positive relationship.For the phrase reduction task, the dependencymethod outperforms the non-dependency method interms of BLEU score.
When the cross reductiontechnique was used for the phrase reduction task,BLEU score is not deteriorated even when more thanhalf of phrase entries are pruned.
This result impliesthat there is much redundant information stored inthe hierarchical rule table.
On the other hand, for thehierarchical rule reduction task, the non-dependencymethod shows the better performance.
The depen-dency method sometimes performs worse than thebaseline method.
We expect that this is caused bythe unreliable estimation of dependency among hi-erarchical rules since the most of them are automat-ically generated from the phrases.
The excessive de-pendency of these rules would cause overestimationof hierarchical rule redundancy score.4 ConclusionWe present a novel method of reducing the size oftranslation model for SMT.
The contributions of theproposed method are as follows: 1) our method isthe first attempt to reduce the phrase table and the hi-erarchical rule table simultaneously.
2) our methodis a safe reduction method since it considers the re-dundancy, which is the practical ineffectiveness ofindividual entry.
3) our method shows that almostthe half size of the translation model can be reducedwithout significant performance degradation.
It maybe appropriate for the applications running on lim-ited environment, e.g., mobile devices.294AcknowledgementThe first author performed this research during aninternship at Microsoft Research Asia.
This researchwas supported by the MKE(The Ministry of Knowl-edge Economy), Korea and Microsoft Research, un-der IT/SW Creative research program supervised bythe NIPA(National IT Industry Promotion Agency).
(NIPA-2010-C1810-1002-0025)ReferencesMichael Bloodgood and Chris Callison-Burch.
2010.Bucking the Trend: Large-Scale Cost-Focused ActiveLearning for Statistical Machine Translation.
In Pro-ceedings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 854?864.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The Mathematicsof Statistical Machine Translation: Parameter Estima-tion.
Computational Linguistics, 19:263?311, June.David Chiang.
2005.
A Hierarchical Phrase-basedModel for Statistical Machine Translation.
In Pro-ceedings of the 43th Annual Meeting on Associationfor Computational Linguistics, pages 263?270.David Chiang.
2007.
Hierarchical Phrase-based Transla-tion.
Computational Linguistics, 33:201?228, June.Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, andTiejun Zhao.
2010.
Hybrid Decoding: Decoding withPartial Hypotheses Combination Over Multiple SMTSystems.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,COLING ?10, pages 214?222, Stroudsburg, PA, USA.Association for Computational Linguistics.David Graff and Christopher Cieri.
2003.
English Giga-word.
In Linguistic Data Consortium, Philadelphia.David Graff.
2007.
Chinese Gigaword Third Edition.
InLinguistic Data Consortium, Philadelphia.Fei Huang and Bing Xiang.
2010.
Feature-Rich Discrim-inative Phrase Rescoring for SMT.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics, COLING ?10, pages 492?500, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Howard Johnson, Joel Martin, George Foster, and RolandKuhn.
2007.
Improving Translation Quality by Dis-carding Most of the Phrasetable.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 967?975.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-based Translation.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1,NAACL ?03, pages 48?54, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.2003.
Cognates can Improve Statistical TranslationModels.
In Proceedings of the 2003 Conference of theNorth American Chapter of the Association for Com-putational Linguistics on Human Language Technol-ogy: companion volume of the Proceedings of HLT-NAACL 2003?short papers - Volume 2, NAACL-Short?03, pages 46?48, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics - Volume 1, ACL ?03, pages 160?167, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedings ofthe 40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.2009.
Complexity-based Phrase-Table Filtering forStatistical Machine Translation.Mei Yang and Jing Zheng.
2009.
Toward Smaller, Faster,and Better Hierarchical Phrase-based SMT.
In Pro-ceedings of the ACL-IJCNLP 2009 Conference ShortPapers, ACLShort ?09, pages 237?240, Stroudsburg,PA, USA.
Association for Computational Linguistics.Andreas Zollmann, Ashish Venugopal, Franz Och, andJay Ponte.
2008.
A Systematic Comparison of Phrase-based, Hierarchical and Syntax-Augmented StatisticalMT.
In Proceedings of the 22nd International Con-ference on Computational Linguistics (Coling 2008),pages 1145?1152, troudsburg, PA, USA.
Associationfor Computational Linguistics.295
