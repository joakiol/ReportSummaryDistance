A Model of Lexical Attract ion and Repulsion*Doug Beeferman Adam Berger  John  La f fe r tySchool  of Computer  ScienceCarnegie  Mel lon Univers i tyP i t t sburgh ,  PA  15213 USA<dougb, aberger, lafferty>@cs, cmu.
eduAbst ractThis paper introduces new methods basedon exponential families for modeling thecorrelations between words in text andspeech.
While previous work assumed theeffects of word co-occurrence statistics tobe constant over a window of several hun-dred words, we show that their influenceis nonstationary on a much smaller timescale.
Empirical data drawn from En-glish and Japanese text, as well as conver-sational speech, reveals that the "attrac-tion" between words decays exponentially,while stylistic and syntactic ontraints cre-ate a "repulsion" between words that dis-courages close co-occurrence.
We showthat these characteristics are well describedby simple mixture models based on two-stage exponential distributions which canbe trained using the EM algorithm.
Theresulting distance distributions can then beincorporated as penalizing features in anexponential language model.1 In t roduct ionOne of the fundamental characteristics of language,viewed as a stochastic process, is that it is highlynonstationary.
Throughout a written documentand during the course of spoken conversation, thetopic evolves, effecting local statistics on word oc-currences.
The standard trigram model disregardsthis nonstationarity, as does any stochastic grammarwhichassigns probabilities to sentences in a context-independent fashion.
*Research supported in part by NSF grant IRI-9314969, DARPA AASERT award DAAH04-95-1-0475,and the ATR Interpreting Telecommunications Re earchLaboratories.Stationary models are used to describe such a dy-namic source for at least two reasons.
The first isconvenience: stationary models require a relativelysmall amount of computation to train and to apply.The second is ignorance: we know so little abouthow to model effectively the nonstationary charac-teristics of language that we have for the most partcompletely neglected the problem.
From a theoreti-cal standpoint, we appeal to the Shannon-McMillan-Breiman theorem (Cover and Thomas, 1991) when-ever computing perplexities on test data; yet thisresult only rigorously applies to stationary and er-godic sources.To allow a language model to adapt to its recentcontext, some researchers have used techniques toupdate trigram statistics in a dynamic fashion bycreating a cache of the most recently seen n-gramswhich is smoothed together (typically by linear in-terpolation) with the static model; see for example(Jelinek et al, 1991; Kuhn and de Mori, 1990).
An-other approach, using maximum entropy methodssimilar to those that we present here, introduces aparameter for trigger pairs of mutually informativewords, so that the occurrence of certain words in re-cent context boosts the probability of the words thatthey trigger (Rosenfeld, 1996).
Triggers have alsobeen incorporated through different methods (Kuhnand de Mori, 1990; Ney, Essen, and Kneser, 1994).All of these techniques treat the recent context as a"bag of words," so that a word that appears, say, fivepositions back makes the same contribution to pre-diction as words at distances of 50 or 500 positionsback in the history.In this paper we introduce new modeling tech-niques based on exponential families for captur-ing the long-range correlations between occurrencesof words in text and speech.
We show how forboth written text and conversational speech, theempirical distribution of the distance between trig-373s t ger words exhibits a striking behavior in which the"attraction" between words decays exponentially,while stylistic and syntactic onstraints create a "re-pulsion" between words that discourages close co-occur rence .We have discovered that this observed behavioris well described by simple mixture models based ontwo-stage xponential distributions.
Though in com-mon use in queueing theory, such distributions havenot, to our knowledge, been previously exploitedin speech and language processing.
It is remark-able that the behavior of a highly complex stochas-tic process uch as the separation between word co-occurrences i well modeled by such a simple para-metric family, just as it is surprising that Zipf's lawcan so simply capture the distribution of word fre-quencies in most languages.In the following section we present examples of theempirical evidence for the effects of distance.
In Sec-tion 3 we outline the class of statistical models thatwe propose to model this data.
After completingthis work we learned of a related paper (Niesler andWoodland, 1997) which constructs imilar models.In Section 4 we present aparameter estimation algo-rithm, based on the EM algorithm, for determiningthe maximum likelihood estimates within the class.In Section 5 we explain how distance models can beincorporated into an exponential language model,and present sample perplexity results we have ob-tained using this class of models.2 The Empirical EvidenceThe work described in this paper began with thegoal of building a statistical language model usinga static trigram model as a "prior," or default dis-tribution, and adding certain features to a family ofconditional exponential models to capture some ofthe nonstationary features of text.
The features weused were simple "trigger pairs" of words that werechosen on the basis of mutual information.
Figure 1provides a small sample of the 41,263 (s,t) triggerpairs used in most of the experiments we will de-scribe.In earlier work, for example (Rosenfeld, 1996), thedistance between the words of a trigger pair (s,t)plays no role in the model, meaning that the "boost"in probability which t receives following its trigger sis independent of how long ago s occurred, so longas s appeared somewhere in the history H, a fixed-length window of words preceding t. It is reasonableto expect, however, that the relevance of a word s tothe identity of the next word should decay as s fallsMs.changesenergycommitteeboardlieutenantAIDSSovietunderwaterpatientstelevisionVoyagermedicalIGulfherrevisionsgasrepresentativeboardcolonelAIDSmissilesdivingdrugsairwavesNeptunesurgicalmeGulfFigure 1: A sample of the 41,263 trigger pairs ex-tracted from the 38 million word Wall Street Journalcorpus.s tUNelectricityelectionsilkcourt,~WH~ --HungaryJapan AirsentencetransplantforestcomputerSecurity Councilkilovattsmall electoral districtCOCO0~imprisonmentBulgariato fly cargoproposed punishmentorga/%wastepaperhostFigure 2: A sample of triggers extracted from the33 million word Nikkei corpus.further and further back into the context.
Indeed,there are tables in (Rosenfeld, 1996) which suggestthat this is so, and distance-dependent "memoryweights" are proposed in (Ney, Essen, and Kneser,1994).
We decided to investigate the effect of dis-tance in more detail, and were surprised by whatwe found.374++L ?
, 0.01:1 \ ] - -  ,0.01= 0.01 ?
~O.G04 ~ O.
(X31*~o* O.g040++ ?
.
.?
~ ',;,0 ,=' ~ ?
}Yq,tgO 150 ~ 2S0 ~ 360Figure 3: The observed istance distributions--collected from five million words of the Wall Street Journalcorpus--for one of the non-self trigger groups (left) and one of the self trigger groups (right).
For a givendistance 0 < k < 400 oa the z-axis, the value on the y-axis is the empirical probability that two trigger wordswithin the group are separated by exactly k + 2 words, conditional on the event that they co-occur withina 400 word window.
(We exclude separation of one or two words because of our use of distance models toimprove upon trigrams.
)The set of 41,263 trigger pairs was partitionedinto 20 groups of non-self triggers (s, t), s ?
t, suchas (Soviet ,  Kremlin's), and 20 groups of self trig-gers (s, s), such as (business, business).
Figure 3displays the empirical probability that a word t ap-pears for the first time k words after the appearanceof its mate s in a trigger pair (s,t), for two repre-sentative groups.The curves are striking in both their similaritiesand their differences.
Both curves eem to have moreor less flattened out by N = 400, which allows us tomake the approximating assumption (of great prac-tical importance) that word-triggering effects maybe neglected after several hundred words.
The mostprominent distinction between the two curves is thepeak near k = 25 in the self trigger plots; the non-self trigger plots suggest a monotonic decay.
Theshape of the self trigger curve, in particular the risebetween k = 1 and/?
~ 25, reflects the stylistic andsyntactic injunctions against repeating a word toosoon.
This effect, which we term the lexical exclu-sion principle, does not appear for non-self triggers.In general, the lexical exclusion principle seems tobe more in effect for uncommon words, and thus thepeak for such words is shifted further to the right.While the details of the curves vary depending onthe particular triggers, this behavior appears to beuniversal.
For triggers that appear too few times inthe data for this behavior to exhibit itself, the curvesemerge when the counts are pooled with those froma collection of other rare words.
An example of thislaw of large numbers is shown in Figure 4.These empirical phenomena are not restricted tothe Wall Street Journal corpus.
In fact, we have ob-served similar behavior in conversational speech and.Japanese text.
The corresponding data for self trig-gers in the Switchboard ata (Godfrey, Holliman,and McDaniel, 1992), for instance, exhibits the samebump in p(k) for small k, though the peak is closerto zero.
The lexical exclusion principle, then, seemsto be less applicable when two people are convers-ing, perhaps because the stylistic concerns of writtencommunication are not as important in conversation.Several examples from the Switchboard and Nikkeicorpora are shown in Figure 5.3 Exponential Models of DistanceThe empirical data presented in the previous ectionexhibits three salient characteristics.
First is the de-cay of the probability of a word t as the distancek from the most recent occurrence of its mate s in-creases.
The most important (continuous-time) dis-tribution with this property is the single-parameterexponential familyp~(x) = ~e :~.
(We'll begin by showing the continuous analoguesof the discrete formulas we actually use, since theyare simpler in appearance.)
This family is uniquelycharacterized by the mernoryless properly that theprobability of waiting an additional ength of timeAt is independent of the time elapsed so far, and375~ o o  I\ I\ tFigure 4: The law of large numbers emerging for distance distributions.
Each plot shows the empiricaldistance curve for a collection of self triggers, each of which appears fewer than 100 times in the entire 38million word Wall Street Journal corpus.
The plots include statistics for 10, 50,500, and all 2779 of the selftriggers which occurred no more than 100 times each."
o.m4 .~ ~ ~ ~ ~ ~ ~ 11a~a~\a~cu~\ ~CIOIo.o~do~@w IOO ~lo ~3o 21o ~ooo,~01Figure 5: Empirical distance distributions of triggers in the :Iapanese Nikkei corpus, and the Switchboardcorpus of conversational speech.
Upper row: All non-self (left) and self triggers (middle) appearing fewerthan 100 times in the Nikkei corpus, and the curve for the possessive particle ?9 (right).
Bottom row:self trigger Utl (left), YOU-KNOW (middle), and all self triggers appearing fewer than 100 times in the entireSwitchboard corpus (right).the distribution p, has mean 1/y and variance 1/y 2.This distribution is a good candidate for modelingnon-self triggers.Figure 6: A two-stage queueThe second characteristic is the bump between 0and 25 words for self triggers.
This behavior appearswhen two exponential distributions are arranged inserial, and such distributions are an important toolin the "method of stages" in queueing theory (Klein-rock, 1975).
The time it takes to travel through twoservice facilities arranged in serial, where the firstprovides exponential service with rate /~1 and thesecond provides exponential service with rate Y2, issimply the convolution of the two exponentials:# P.~,~2(z) = Y1Y2 e-~':te -~'~(=-Od~_ ~1~2 (e - ?
'=-  e -~'~=) ~x ?
/ J2 ./~2 - #1The mean and variance of the two-stage exponen-tial p.,,,: are 1/#, + l/p2 and 1/y~ + 1//J~ respec-tively.
As #1 (or, by symmetry, P2) gets large, thepeak shifts towards zero and the distribution ap-proaches the single-parameter exponential Pu= (by376symmetry, Pro)- A sequence of two-stage models isshown in Figure 7.0.01O+OOgO.QI\]I0..007O.OOG0.~60.0040,00?I0.002O,G010Figure 7: A sequence of two-stage xponential mod-els pt`~,t`~(x) with/Jl = 0.01, 0.02, 0.06, 0.2, oo and/~ = 0.01.The two-stage xponential is a good candidate fordistance modeling because of its mathematical prop-erties, but it is also well-motivated for linguistic rea-sons.
The first queue in the two-stage model rep-resents the stylistic and syntactic onstraints thatprevent a word from being repeated too soon.
Afterthis waiting period, the distribution falls off expo-nentially, with the memoryless property.
For non-self triggers, the first queue has a waiting time ofzero, corresponding to the absence of linguistic con-straints against using t soon after s when the wordss and t are different.
Thus, we are directly model-ing the "lexical exclusion" effect and long-distancedecay that have been observed empirically.The third artifact of the empirical data is the ten-dency of the curves to approach a constant, positivevalue for large distances.
While the exponential dis-tribution quickly approaches zero, the empirical datasettles down to a nonzero steady-state value.Together these three features suggest modelingdistance with a three-parameter family of distribu-tions:= + c)where c > 0 and 7 is a normalizing constant.Rather than a continuous-time exponential, we usethe discrete-time analoguep.
(k )  = (1 - - t`kIn this case, the two-stage model becomes thediscrete-time convolutionkpt=l,t`2(k) = ~ p/=l(t)pt`~(k -- t) .t=ORemark .
It should be pointed out that there isanother parametric family that is an excellent can-didate for distance models, based on the first twofeatures noted above: This is the Gamma dislribu.lion/~a xot-le -#~ =This distribution has mean a//~ and variance a//~ 2and thus can afford greater flexibility in fitting theempirical data.
For Bayesian analysis, this distribu-tion is appropriate as the conjugate prior for the ex-ponential parameter p (Gelman et al, 1995).
Usingthis family, however, sacrifices the linguistic inter-pretation of the two-stage model.4 Estimating the ParametersIn this section we present a solution to the problemof estimating the parameters of the distance modelsintroduced in the previous section.
We use the max-imum likelihood criterion to fit the curves.
Thus, if0 E 0 represents the parameters of our model, and/3(k) is the empirical probability that two triggersappear a distance of k words apart, then we seek tomaximize the log-likelihoodC(0) = ~ ~(k)logp0(k).k>0First suppose that {PO}oE?
is the family of continu-ous one-stage xponential models p~(k) = pe -t`k.In this case the maximum likelihood problem isstraightforward: the mean is the sufficient statisticfor this exponential family, and its maximum likeli-hood estimate is determined by1 1- Ek>o k~(k) - E~ \[k\]"In the case where we instead use the discrete modelpt`(k) = (1 - e -t') e -t`k, a little algebra shows thatthe maximum likelihood estimate is thenNow suppose that our parametric family {PO}OE?is the collection of two-stage xponential models; thelog-likelihood in this case becomes?
(/~1,/~2) = ~--~iS(k)log pm( j )p t` , (k - j )  .k_>0Here it is not obvious how to proceed to obtain themaximum likelihood estimates.
The difficulty is thatthere is a sum inside the logarithm, and direct dif-ferentiation results in coupled equations for Pi and377#2.
Our solution to this problem is to view the con-volving index j as a hidden variable and apply theEM algorithm (Dempster, Laird, and Rubin, 1977).Recall that the interpretation of j is the time usedto pass through the first queue; that is, the numberof words used to satisfy the linguistic constraints oflexical exclusion.
This value is hidden given only thetotal time k required to pass through both queues.Applying the standard EM argument, the dif-ference in log-likelihood for two parameter pairs(#~,#~) and (/tt,#2) can be bounded from below asc( . '
) -  =  ( )log(p.:,.
;(.,j'))/:>_0 j=0A(i,',~,)>whereandp.,, .
.
(~, J) = p., (J) p.~ (~ - i)Pu,,~,=(jlk) = Pm'"2(k'J)p.,,.~(k)Thus, the auxiliary function A can be written ask- it' z E~(k)EJPm,~,2(J \[k)k_>0 j=0kk>0 j=0+ constant(#).Differentiating .A(#',#) with respect o #~, we getthe EM updates ( 1 )#i = log 1 + )-~k>0/3(k) k E j  =0 J P;,,t'2 (J \[ k) ( 1 )k #~ -- log 1 + ~ka0/3(k) y'~j__0(k - j)pm,.~(jlk)l:l.emark.
It appears that the above updates re-quire O(N 2) operations if a window of N wordsis maintained in the history.
However, us-ing formulas for the geometric series, such as~ k ~k=0 kz = z / (1 -  x) 2, we can write the expec-k ?
tation ~":~j=o 3 Pm,~,,(Jlk) in closed form.
Thus, theupdates can be calculated in linear time.Finally, suppose that our parametric family{pc}see is the three-parameter collection of two-stage exponential models together with an additiveconstant:p.,,.~,o(k) = .
-~(p. , , .=(k)  + e).Here again, the maximum likelihood problem canbe solved by introducing a hidden variable.
In par-c ticular, by setting a "- ~ we can express thismodel as a mizture of a two-stage xponential anda uniform distribution:Thus, we can again apply the EM algorithm to de-termine the mixing parameter a.
This is a standardapplication of the EM algorithm, and the details areomitted.In summary, we have shown how the EM algo-rithm can be applied to determine maximum like-lihood estimates of the three-parameter family ofdistance models {Pm,~=,a} of distance models.
InFigure 8 we display typical examples of this trainingalgorithm at work.5 A Nonstat ionary  Language Mode lTo incorporate triggers and distance models intoa long-distance language model, we begin byconstructing a standard, static backoff trigrammodel (Katz, 1987), which we will denote asq(wo\[w-l,w-2).
For the purposes of building amodel for the Wall Street Journal data, this trigrammodel is quickly trained on the entire 38-millionword corpus.
We then build a family of conditionalexponential models of the general formp(w I H) = 1 (= )Z~-ff~ exp Aifi(H,w) q(wlw_l,w_2 )where H = w-t,  w-2 .
.
.
.
, w_N is the word history,and Z(H) is the normalization constantZ( H)~= E exp ( E Aifi( H' , q(w l w_l, w-2)The functions fl, which depend both on the wordhistory H and the word being predicted, are calledfeatures, and each feature fi is assigned a weight Ai.In the models that we built, feature fi is an indicatorfunction, testing for the occurrence of a trigger pair(si,ti):1 i f s iEHandw=t ifi(H,w) = 0 otherwise.The use of the trigram model as a default dis-tribution (Csiszhr, 1996) in this manner is new inlanguage modeling.
(One might also use the termprior, although q(w\[H) is not a prior in the strictBayesian sense.)
Previous work using maximum en-tropy methods incorporated trigram constraints as3780.0140.0120.01O.00e0.0040.0040.002r "\-.~..0.0120.01 !~ i l  "IIIol i i I i I * " '1  'Figure 8: The same empirical distance distributions of Figure 2 fit to the three-parameter mixture modelPm,#2,a using the EM algorithm.
The dashed line is the fitted curve.
For the non-self trigger plot/J1 = 7,/~ = 0.0148, and o~ = 0.253.
For the self trigger plot/~1 = 0.29,/J2 = 0.0168, and a = 0.224.explicit features (Rosenfeld, 1996), using the uni-form distribution as the default model.
There areseveral advantages to incorporating trigrams in thisway.
The trigram component can be efficiently con-structed over a large volume of data, using standardsoftware or including the various sophisticated tech-niques for smoothing that have been developed.
Fur-thermore, the normalization Z(H)  can be computedmore efficiently when trigrams appear in the defaultdistribution.
For example, in the case of trigger fea-tures, sinceZ(H)  = 1 + ~ 6(si E H)(e x' - 1)q(ti lw-1, w-z)ithe normalization involves only a sum over thosewords that are actively triggered.
Finally, assumingrobust estimates for the parameters hl, the resultingmodel is essentially guaranteed to be superior to thetrigram model.
The training algorithm we use forestimating the parameters i  the Improved IterativeScaling (IIS) algorithm introduced in (Della Pietra,Della Pietra, and Lafferty, 1997).To include distance models in the word predic-tions, we treat the distribution on the separation kbetween sl and ti in a trigger pair (si,ti) as a prior.Suppose first that our distance model is a simpleone-parameter xponential, p(k I sl E H ,w = ti) =#i e -m~.
Using Bayes' theorem, we can then writep(w = ti \[sl E H, si = w-A)p(w = ti \[si E H) p(k \[si E H ,w = ti)p(k I si E H)oc e x'-"'k q(tl I wi - l ,wi -~) .Thus, the distance dependence is incorporated as apenalizing feature, the effect of which is to discour-age a large separation between si and ti.
A simi-lar interpretation holds when the two-stage mixturemodels P,1,,2,~ are used to model distance, but theformulas are more complicated.In this fashion, we first trained distance modelsusing the algorithm outlined in Section 4.
We thenincorporated the distance models as penalizing fea-tures, whose parameters remained fixed, and pro-ceeded to train the trigger parameters hi using theIIS algorithm.
Sample perplexity results are tabu-lated in Figure 9.One important aspect of these results is that be-cause a smoothed trigram model is used as a de-fault distribution, we are able to bucket the triggerfeatures and estimate their parameters on a modestamount of data.
The resulting calculation takes onlyseveral hours on a standard workstation, in com-parison to the machine-months of computation thatprevious language models of this type required.The use of distance penalties gives only a smallimprovement, in terms of perplexity, over the base-line trigger model.
However, we have found thatthe benefits of distance modeling can be sensitive toconfiguration of the trigger model.
For example, inthe results reported in Table 9, a trigger is only al-lowed to be active once in any given context.
Byinstead allowing multiple occurrences of a trigger sto contribute to the prediction of its mate t, boththe perplexity reduction over the baseline trigramand the relative improvements due to distance mod-eling are increased.379Experiment PerplexityBaseline: trigrams trained on 5M words 170Trigram prior + 41,263 triggers 145Same as above + distance modeling 142Baseline: trigrams trained on 38M words 107Trigram prior + 41,263 triggers 92Same as above + distance modeling 90Figure 9: Models constructed using trigram priors.
Training the largerDEC Alpha workstation.Reduction14.7%I6.5%14.0%15.9%model required about 10 hours on a6 ConclusionsWe have presented empirical evidence showing thatthe distribution of the distance between word pairsthai; have high mutual information exhibits a strik-ing behavior that is well modeled by a three-parameter family of exponential models.
The prop-erties of these co-occurrence statistics appear to beexhibited universally in both text and conversationalspeech.
We presented a training algorithm for thisclass of distance models based on a novel applica-tion of the EM algorithm.
Using a standard backofftrigram model as a default distribution, we built aclass of exponential language models which use non-stationary features based on trigger words to allowthe model to adapt to the recent context, and thenincorporated the distance models as penalizing fea-tures.
The use of distance modeling results in animprovement over the baseline trigger model.AcknowledgementWe are grateful to Fujitsu Laboratories, and in par-ticular to Akira Ushioda, for providing access to theNikkei corpus within Fujitsu Laboratories, and as-sistance in extracting Japanese trigger pairs.ReferencesBerger, A., S. Della Pietra, and V. Della Pietra.
1996.
Amaximum entropy approach to natural anguage pro-cessing.
Computational Linguistics, 22(1):39-71.Cover, T.M.
and J.A.
Thomas.
1991.
Elements of In..\[ormation Theory.
John Wiley.Csisz?r, I.
1996.
Maxent, mathematics, and informationtheory.
In K. Hanson and It.
Silver, editors, Max-imum Entropy and Bayesian Methods.
Kluwer Aca-demic Publishers.DeLia Pietra, S., V. Della Pietra, and J. Lafferty.
1997.Inducing features of random fields.
IEEE Trans.on Pattern Analysis and Machine Intelligence, 19(3),March.Dempster, A.P., N.M. Laird, and D.B.
RubEn.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal o\] the Royal Statistical Society,39(B):1-38.Gelman, A., J. Car\]in, H. Stern, and D. RubEn.
1995.Bayesian Data Analysis.
Chapman &: Hall, London.Godfrey, J., E. HoUiman, and J. McDaniel.
1992.SWITCHBOARD: Telephone speech corpus for re-search and development.
In Proc.
ICASSP-9~.Jelinek, F., B. MeriMdo, S. Roukos, and M. Strauss.1991.
A dynamic language model for speech recog-nition.
In Proceedings o/the DARPA Speech and Nat.ural Language Workshop, pages 293-295, February.Katz, S. 1987.
Estimation of probabilities from sparsedata for the langauge model component of a speechrecognizer.
IEEE Transactions on Acoustics, Speechand Signal Processing, ASSP-35(3):400-401, March.Kleinrock, L. 1975.
Queueing Systems.
Volume I: The-ory.
Wiley, New York.Kuhn, R. and R. de Mori.
1990.
A cache-based nat-ural language model for speech recognition.
IEEETrans.
on Pattern Analysis and Machine Intelligence,12:570-583.Ney, H., U. Essen, and R. Kneser.
1994.
On structur-ing probabilistic dependencies in stochastic languagemodeling.
Computer Speech and Language, 8:1-38.Niesler, T. and P. Woodland.
1997.
Modelling word-pair relations in a category-based language model.
InProceedings o\] ICASSP-97, Munich, Germany, April.Rosenfeld, R. 1996.
A maximum entropy approachto adaptive statistical language modeling.
ComputerSpeech and Language, 10:187-228.380
