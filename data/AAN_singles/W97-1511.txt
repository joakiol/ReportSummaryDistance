Exploiting Contextual Information in Hypothesis Selection forGrammar RefinementThanaruk  Theeramunkong Yasunobu Kawaguch i  Manabu OkumuraJapan Advanced Inst i tute  of Japan Advanced Inst i tute  of Japan Advanced Inst i tute  ofScience and Technology Science and Technology Science and Technology1-1 Asahidai Tatsunokuchi  1-1 Asahidai Tatsunokuchi  1-1 Asahidai Tatsunokuch iNomi Ishikawa Japan  Nomi Ishikawa Japan  Nomi Ishikawa Japanping~j aist.
ac.
j p kawagut i?j aist.
ac.
j p oku~j aist.
ac.
j pAbst ractIn this paper, we propose a new frame-work of grammar development and sometechniques for exploiting contextual infor-mation in a process of grammar efine-ment.
The proposed framework involvestwo processes, partial grammar acquisitionand grammar refinement.
In the formerprocess, a rough grammar is constructedfrom a bracketed corpus.
The grammar islater refined by the latter process wherea combination of rule-based and corpus-based approaches i applied.
Since theremay be more than one rules introduced asalternative hypotheses to recover the anal-ysis of sentences which cannot be parsed bythe current grammar, we propose amethodto give priority to these hypotheses basedon local contextual information.
By experi-ments, our hypothesis selection isevaluatedand its effectiveness is shown.1 In t roduct ionOne of the essential tasks to realize an efficientnatural anguage processing system is to constructa broad-coverage and high-accurate grammar.
Inmost of the currently working systems, such gram-mars have been derived manually by linguistsor lexicographers.
Unfortunately, this task re-quires time-consuming skilled effort and, in mostcases, the obtained grammars may not be com-pletely satisfactory and frequently fail to covermany unseen sentences.
Toward these problems,there were several attempts developed for automat-ically learning grammars based on rule-based ap-proach(Ootani and Nakagawa, 1995), corpus-basedapproach(Srill, 1992)(Mori and Nagao, 1995) or hy-brid approach(Kiyono and Tsujii, 1994b)(Kiyonoand Tsujii, 1994a).Unlike previous works, we have introduced anew framework for grammar development, which isa combination of rule-based and corpus-based ap-proaches where contextual information can be ex-ploited.
In this framework, a whole grammar is notacquired from scratch(Mori and Nagao, 1995) or aninitial grammar does not need to be assumed(Kiyonoand Tsujii, 1994a).
Instead, a rough but effectivegrammar is learned, in the first place, from a largecorpus based on a corpus-based method and thenlater refined by the way of the combination of rule-based and corpus-based methods.
We call the formerstep of the framework partial grammar acquisitionand the latter grammar efinement.
For the partialgrammar acquisition, in our previous works, we haveproposed a mechanism to acquire a partial gram-mar automatically from a bracketed corpus based onlocal contextual information(Theeramunkong andOkumura, 1996) and have shown the effectivenessof the derived grammar(Theeramunkong and Oku-mura, 1997).
Through some preliminary experi-ments, we found out that it seems difficult to learngrammar rules which are seldom used in the corpus.This causes by the fact that rarely used rules oc-cupy too few events for us to catch their properties.Therefore in the first step, only grammar rules withrelatively high occurrence are first learned.In this paper, we focus on the second step, gram-mar refinement, where some new rules can be addedto the current grammar in order to accept un-parsable sentences.
This task is achieved by twocomponents: (1) the rule-based component, whichdetects incompleteness of the current grammar andgenerates a set of hypotheses of new rules and (2)the corpus-based component, which selects plausiblehypotheses based on local contextual information.In addition, this paper also describes a stochasticparsing model which finds the most likely parse of asentence and then evaluates the hypothesis selectionbased on the plausible parse.In the rest, we give an explanation of our frame-work and then describe the grammar refinement pro-cess and hypothesis selection based on local contex-tual information.
Next, a stochastic parsing modelwhich exploits contextual information is described.Finally, the effectiveness of our approach is shownthrough some experiments investigating the correct-ness of selected hypotheses and parsing accuracy.782 The  F ramework  o f  GrammarDeve lopmentThe proposed framework is composed of two phases:partial grammar acquisition and grammar refine-ment.
The graphical representation of the frame-work is shown in Figure 1.
In the process of grammardevelopment, a partial grammar is automatically ac-quired in the first phase and then it is refined inthe second phase.
In the latter phase, the systemgenerates new rules and ranks them in the order ofpriority before displaying a user a list of plausiblerules as candidates for refining the grammar.
Thenthe user can select the best one among these rules.Currently, the corpus used for grammar developmentin the framework is EDR corpus(EDR, 1994) wherelexical tags and bracketings are assigned for wordsand phrase structures of sentences in the corpus re-spectively but no nonterminal labels are given.Partial Grammar\[ P ,l 1 lGrammar I ~"~l ' - z ' - ' - ' - ' -~  .
.
.
.
.\] Acquisit ion I \[ t ' - "  I -I====:-?":"':!
.
.
.
.
.
.
1 .
, o , .
, ?
?o .o.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
\[ Grammar.................... D I Refinement.................. L Phase:::::::7 I|New RuleHypothesesDo,,olomrFigure 1: The overview of our grammar developmentframework2.1 Partial Grammar  AcquisitionIn this section, we give a brief explanation for par-tial grammar  acquisition.
More detail can be foundin (Theeramunkong and Okumura, 1996).
In par-tial grammar  acquisition, a rough grammar is con-structed from the corpus based on clustering anal-ysis.
As mentioned above, the corpus used is atagged corpus with phrase structures marked withbrackets.
At the first place, brackets covering asame sequence of categories, are assumed to havea same nonterminal label.
We say they have thesame bracket type.
The basic idea is to group brack-ets (bracket types) in a corpus into a number ofsimilar bracket groups.
Then the corpus is auto-matically labeled with some nonterminal labels, andconsequently a grammar is acquired.
The similaritybetween any two bracket types is calculated basedon divergencel(Harris, 1951) by utilizing local con-textual information which is defined as a pair ofcategories of words immediately before and after abracket ype.
This approach was evaluated throughsome experiments and the obtained result was al-most consistent with that given by human evalua-tots.
However, in this approach, when the numberof occurrences of a bracket type is low, the simi-larity between this bracket type and other brackettypes is not so reliable.
Due to this, only brackettypes with relatively frequent occurrence are takeninto account.
To deal with rarely occurred brackettypes, we develop the second phase where the sys-tem shows some candidates to grammar developersand then they can determine the best one amongthese candidates, as shown in the next section.2.2 Grammar Ref inement with Addit ionalHypothes is  RuleThe grammar acquired in the previous phase is apartial one.
It is insufficient for analyzing all sen-tences in the corpus and then the parser fails toproduce any complete parse for some sentences.
Inorder to deal with these unparsable sentences, wemodify the conventional chart parser to keep recordof all inactive edges as partial parsing results.
Twoprocesses are provoked to find the possible plausibleinterpretations of an unparsable sentence by hypoth-esizing some new rules and later to add them to thecurrent grammar.
These processes are (1) the rule-based process, which detects incompleteness of thecurrent grammar and generates a set of hypothe-ses of new rules and (2) the corpus-based process,which selects plausible hypotheses based on localcontextual information.
In the rule-based process,the parser generates partial parses of a sentence asmuch as possible in bottom-up style under the gram-mar constraints.
Utilizing these parses, the processdetects a complete parse of a sentence by startingat top category (i.e., sentence) covering the sentenceand then searching down, in top-down manner, tothe part of the sentence that cannot form any parse.At this point, a rule is hypothesized.
In many cases,there may be several possibilities for hypothesizedrules.
The corpus-based process, as the second pro-cess, uses the probability information from parsablesentences to rank these hypotheses.
In this research,local contextual information is taken into account forthis task.1 The effectiveness of divergence for detecting phrasestructures in a sentence is also shown in (Brill, 1992).793 Hypothes is  Generat ionWhen the parser fails to parse a sentence, there ex-ists no inactive edge of category S (sentence) span-ning the whole sentence in the parsing result.
Thenthe hypothesis generation process is provoked to findall possible hypotheses in top-down manner by start-ing at a single hypothesis of the category S coveringthe whole sentence.
This process uses the partialchart constructed uring parsing the sentence.
Thishypothesis generation is similar to one applied in(Kiyono and Tsujii, 1994a).\ [Hypothes is  generat ion \ ]An inactive edge \ [ is(A) : xo, xn\] can be in-troduced from x0 to x , ,  with label A, foreach of the hypotheses generated by the fol-lowing two steps.1.
For each sequence of inactive edges, \[ is(B1) :xo, x l \] , ..., \[ ie( Bn ) : Xn-  l , agn \] , spanning from x0to xn, generate a new rule, A ---, Bz ,  ..., Bn,  andpropose a new inactive edge as a hypothesis,\ [hypo(A) : xo, xn\].
(Figure 2(1))2.
For each existing rule A --+ A1, ..., An, find anincomplete sequence of inactive edges, \[ ie(A1) :xo, x l \ ] , .
.
.
, \ [ ie (a i -1 )  : xi-2, zi-1\], \ [ ie(Ai+l)  :xi, xi+l\], ..., \[ ie(An) : xn -z ,  xn\], and call this al-gorithm for \ [ ie (A i ) :  xl-z, xl\].
(Figure 2(2))(1) B1 Bn9~D-  e -  e -  ( ~ - ~  .
.
.
.
.
.
.
.
.
.
.
.xO X l Xn.
f xn#Assumeeru le :  A ..~ B I  ..... Bn(2) An existing rule : A.-~ A1,...,Ai-I, Ai, Ai+ I,...AnA1 Ai-1 Ai + l An.
.
.
.
.
.
.
..~  x l  xl,~ xl-1 ~ x l .
1 x~1 Xn#Find Ai between xi-1 and XiFigure 2: Hypothesis Rule GenerationBy this process, all of possible single hypotheses(rules) which enable the parsing process to succeed,are generated.
In general, among these rules, most ofthem may be linguistically unnatural.
To filter outsuch unnatural hypotheses, some syntactical crite-ria are introduced.
For example, (1) the maximumnumber of daughter constituents of a rule is limitedto three, (2) a rule with one daughter is not pre-ferred, (3) non-lexical categories are distinguishedfrom lexical categories and then a rule with lexicalcategories as its mother is not generated.
By thesesimple syntactical constraints, a lot of useless rulescan be discarded.4 Hypothes is  Se lec t ion  w i th  Loca lContextua l  In fo rmat ionHypothesis election utilizes information from localcontext o rank the rule hypotheses generated in theprevious phase.
In the hypothesis generation, al-though we use some syntactical constraints to reducethe number of hypotheses of new rules that shouldbe registered into the current grammar, there maystill be several candidates remaining.
At this point,a scoring mechanism is needed for ranking these can-didates and then one can select the best one as themost plausible hypothesis.This section describes a scoring mechanism whichlocal contextual information can be exploited for thispurpose.
As mentioned in the previous section, lo-cal contextual information referred here is definedas a pair of categories of words immediately beforeand after the brackets.
This information can be usedas an environment for characterizing a nonterminalcategory.
The basic idea in hypothesis election isthat the rules with a same nonterminal category astheir mother tend to have similar environments.
Lo-cal contextual information is gathered beforehandfrom the sentences in the corpus which the currentgrammar is enough for parsing.When the parser faces with a sentence which can-not be analyzed by the current grammar, some newrule hypotheses are proposed by the hypothesis gen-erator.
Then the mother categories of these ruleswill be compared by checking the similarity with thelocal contextual information of categories gatheredfrom the parsable sentences.
Here, the most likelycategory is selected and that rule will be the mostplausible candidate.
The scoring function (probabil-ity p) for a rule hypothesis Cat  ---* a is defined asfollows.p(Cat  --* ~) -" p(Cat\[ l ,  r) - g (Cat ,  l, r)N( l ,  r) (1)where N(Cat ,  l, r) is the number of times that Catis occurred in the environment (l, r).
I is the cat-egory immediately before Cat  and r is the lexicalcategory of the word immediately after Cat .
N( l ,  r)is the number of times that i and r are occurredimmediately before and after any categories.
Notethat because it is not possible for us to calculate theprobability of Cat  ---+ ot in the environment of (l, r),we estimate this number by the probability that Catoccurs in the environment of (l, r).
That is, how easythe category Cat  appears under a certain environ-ment (l, r).805 The  Stochast i c  Mode lThis section describes a statistical parsing modelwhich finds the most plausible interpretation of asentence when a hypothesis i  introduced for recov-ering the parsing process of the sentence.
In thisproblem, there are two components taken into ac-count: a statistical model and parsing process.
Themodel assigns a probability to every candidate parsetree for a sentence.
Formally, given a sentence Sand a tree T, the model estimates the conditionalprobability P(TIS).
The most likely parse under themodel is argrnaxT P(TIS ) and the parsing process isa method to find this parse.
In general, a model ofa simple probabilistic ontext free grammar (CFG)applies the probability of a parse which is defined asthe multiplication of the probability of all appliedrules.
However, for the purposes of our model whereleft and right contexts of a constituent are taken intoaccount, the model can be defined as follows.P(T\]S) = H p(rli, li, ri) (2)(rl, ,I,,ri)ETwhere rli is an application rule in the tree and l~ andri are respectively the left and right contexts at theplace the rule is applied.
In a parsing tree, there isa hypothesis rule for which we cannot calculate theprobability because it does not exist in the currentgrammar.
Thus we estimate its probability by usingthe formula (1) in section 4.Similar to most probabilistic models, there isa problem of low-frequency events in this model.Although some statistical NL applications applybacking-off estimation techniques to handle low-frequency events, our model uses a simple interpola-tion estimation by adding a uniform probability toevery events.
Moreover, we make use of the geomet-ric mean of the probability instead of the originalprobability in order to eliminate the effect of thenumber of rule applications as done in (Magermanand Marcus, 1991).
The modified model is:P(T\]S) =( H (~*p(rl"l"r')+(1-c~)*N-'~))ff ' I(rll ,l, ,r~)ET(3)Here, o~ is a balancing weight between the observeddistribution and the uniform distribution.
It is as-signed with 0.8 in our experiments.
Nrl is the num-ber of rules and Nc is the number of possible con-texts, i.e., the left and right categories.
The appliedparsing algorithm is a simple bottom-up chart parserwhose scoring function is based on this model.
Adynamic programming algorithm is used to find theViterbi parse: if there are two proposed constituentswhich span the same set of words and have the samelabel, then the lower probability constituent can besafely discarded.816 Exper imenta l  Eva luat ionSome evaluation experiments and their results aredescribed.
For the experiments, we use texts fromthe EDR corpus, where bracketings are given.
Thesubject is 48,100 sentences including around 510,000words.
Figure 3 shows some example sentences inthe EDR corpus(((ART," a" )((ADJ ," large" )(NOUN ,"festival" )))((VT,"held")(ADV,"biennially")))((AOV,"again")((PRON,"he")((VT,"says")((P RON," he")((A DV," completely")((VT,"forgot")((PaEe,"about")( (eaOi , "  his" )(NOUN," homework" )))))))))Figure 3: Some example sentences in the EDR cor-pusThe initial grammar is acquired from the samecorpus using divergence shown in section 2.1.
Thenumber of rules is 272, the maximum length of rulesis 4, and the numbers of terminal and nonterminalcategories are 18 and 55 respectively.
A part of theinitial grammar is enumerated in Figure 4.
In thegrammar, llnl is expected to be noun phrase with anarticle, lln2 is expected to be noun phrase withoutan article, and iln3 is expected to be verb phrase.Moreover, among 48,100 sentences, 5,083 sentencescannot be parsed by the grammar.
We use thesesentences for evaluating our hypothesis selection ap-proach.l ln l  ---.
adv, nounl ln l  ---* adv, l ln ll ln l  ~ adv, lln2l ln l  ---* art, nounlln2 ---* adj, nounlln2 ---* adj, l ln llln2 ---* adj, lln2lln2 ~ adj, lln8lln3 -* adv, lln3lln3 ---* aux, vtlln3 ---* aux, l ln l3lln3 ---* l lnl2, vtFigure 4: A part of initial grammar6.1 The CriterionIn the experiments, we use bracket crossing as a cri-terion for checking the correctness of the generatedhypothesis.
Each result hypothesis i compared withthe brackets given in the EDR corpus.
The correct-ness of a hypothesis i  defined as follows.Ranking A/all ?
At least one of the derivations inside the hy-pothesis include the brackets which do not crosswith those given in the corpus?
When the hypothesis is applied, it can be usedto form a tree whose brackets do not cross withthose given in the corpus.6.2 Hypothes is  Level  Eva luat ionFrom 5,083 unparsable sentences, the hypothesisgenerator can produce some hypotheses for 4,730sentences (93.1%).
After comparing them with theparses in the EDR corpus, the hypothesis ets of3,127 sentences (61.5 %) include correct hypothe-ses.
Then we consider the sentences for which somecorrect hypotheses can be generated (i.e., 3,127 sen-tences) and evaluate our scoring function in selectingthe most plausible hypothesis.
For each sentence,we rank the generated hypotheses by their prefer-ence score according to our scoring function.
Theresult is shown in Table 1.
From the table, eventhough only 12.3 % of the whole generated hypothe-ses are correct, our hypothesis election can choosethe correct hypothesis for 41.6 % of the whole sen-tences when the most plausible hypothesis i selectedfor each sentence.
Moreover, 29.8 % of correct hy-potheses are ordered at the ranks of 2-5, 24.3 % atthe ranks of 6-10 and just only 6.2 % at the ranks ofmore than 50.
This indicates that the hypothesis se-lection is influential for placing the correct hypothe-ses at the higher ranks.
However, when we considerthe top 10 hypotheses, we found out that the accu-racy is (1362+3368+3134)/(3217+11288+12846)=28.8 %.
This indicates that there are a lot of hy-potheses generated for a sentence.
This suggests usto consider the correct hypothesis for each sentenceinstead of all hypotheses.Ranking12-56-1011-2021-3031-5051-allwhole (A)hypotheses32171128812846221051774327001102015196214correct (B) A/Bhypotheses1340 41.6%3368 29.8%3134 24.3%4300 19.4%2673 i5.0%3033 11.2%6315 6.2%24203 12.3%Table 1: Hypothesis Level Evaluation6.3 Sentence  Level  Eva luat ionIn this section, we consider the accuracy of our hy-pothesis election for each sentence.
Table 2 displaysthe accuracy of hypothesis selection by changing thenumber of selected hypotheses.From the table, the number of sentences whosebest hypothesis is correct, is 1,340 (41.6%) and we12-56-1011-2021-3031-5051-allsentences withcorrect hypo.
(A)13401006277225111121136321741.6 %31.2 %8.6 %7.0 %3.5 %3.8 %4.2 %100.0 %Table 2: Sentence Level Evaluationcan get up to 2,623 (81.5%) accuracy when the top10 of the ordered hypotheses are considered.
Theresult shows that our hypothesis selection is effectiveenough to place the correct hypothesis at the higherranks.6.4 Pars ing  Eva luat ionAnother experiment is also done for evaluating theparsing accuracy.
The parsing model we considerhere is one described in section 5.
The chart parseroutputs the best parse of the sentence.
This parseis formed by using grammar ules and a single rulehypothesis.
The result is shown in Table 3.
In thisevaluation, the PARSEVAL measures as defined in(Black and et al, 1991) are used:Precision :number of correct brackets in proposed parsesRecall =number of brackets in proposed parsesnumber of correct brackets in proposed parsesnumber of brackets in corpus parsesFrom this result, we found out that the parser cansucceed 57.3 % recall and 65.2 % precision for theshort sentences (3-9 words).
In this case, the aver-aged crossings are 1.87 per sentence and the numberof sentences with less than 2 crossings is 69.2 % ofthe comparisons.
For long sentences not so much ad-vantage is obtained.
However, our parser can achieve51.4 % recall and 56.3 % precision for all unparsablesentences.7 D iscuss ion  and Conc lus ionIn this paper, we proposed a framework for exploit-ing contextual information in a process of grammarrefinement.
In this framework, a rough grammaris first learned from a bracketed corpus and thenthe grammar is refined by the combination of rule-based and corpus-based methods.
Unlike stochasticparsing such as (Magerman, 1995)(Collins, 1996),our approach can parse sentences which fall out thecurrent grammar and suggest he plausible hypoth-esis rules and the best parses.
The grammar is notacquired from scratch like the approaches shown in82Sent.
LengthComparisonsAvg.
Sent.
Len.Corpus ParsesSystem's ParsesCrossings/Sent.Sent.
cross.= 0Sent.
cross.< 1Sent.
cross.< 2RecallPrecision3-919806.95.155.781.8720.1%43.9%69.2%57.3%65.2%3-1538649.57.658.273.3210.6%25.0%41.7%53.2%58.7%10-19249113.411.4712.075.690.4%3.9%9.7%47.3%50.0%all length473010.88.959.574.188.9%21.1%35.1%51.4%56.3%Table 3: Parsing Accuracy(Pereira and Schabes, 1992)(Mort and Nagao, 1995).Through some experiments, our method can achieveeffective hypothesis selection and parsing accuracyto some extent.
As our further work, we are on theway to consider the correctness of the selected hy-pothesis of the most plausible parses proposed bythe parser.
Some improvements are needed to gradeup the parsing accuracy.
Another work is to usean existing grammar, instead of an automaticallylearned one, to investigate the effectiveness of con-textual information.
By providing a user interface,this method will be useful for grammar developers.AcknowledgementsWe would like to thank the EDR organization forpermitting us to access the EDR corpus.
Specialthanks go to Dr. Ratana Rujiravanit, who helps meto keenly proofread a draft of this paper.
We alsowish to thank the members in Okumura laboratoryat JAIST for their useful comments and their tech-nical supports.Re ferencesBlack, E. and et al 1991.
A procedure for quantita-tively comparing the syntactic overage of Englishgrammars.
In Proc.
of the 1991 DARPA Speechand Natural Language Workshop, pages 306-311.Brill, Eric.
1992.
Automatically acquiring phrasestructure using distributional nalysis.
In Proc.of Speech and Natural Language Workshop, pages155-159.Collins, Michael John.
1996.
A new statisticalparser based on bigram lexical dependencies.
InProc.
of the 3~th Annual Meeting of the ACL,pages 184-191.EDR: Japan Electronic Dictionary Research Insti-tute, 1994.
EDR Electric Dictionary User's Man-ual (in Japanese), 2.1 edition.Harris, Zellig.
1951.
Structural Linguistics.Chicago: University of Chicago Press.Kiyono, Masaki and Jun'iehi Tsujii.
1994a.
Combi-nation of symbolic and statistical approaches forgrammatical knowledge acquisition.
In Proc.
of4th Conference on Applied Natural Language Pro-cessing (ANLP'94), pages 72-77.Kiyono, Masaki and Jun'ichi Tsujii.
1994b.
Hy-pothesis selection in grammar acquisition.
InCOLING-94, pages 837-841.Magerman, D. M. and M. P. Marcus.
1991.
Pearl:A probabilistic chart parser.
In Proceedings of theEuropean A CL Conference.Magerman, David M. 1995.
Statistical decision-treemodels for parsing.
In Proceeding of 33rd AnnualMeeting of the ACL, pages 276-283.Mort, Shinsuke and Makoto Nagao.
1995.
Parsingwithout grammar.
In Proc.
of the 4th Interna-tional Workshop on Parsing Technologies, pages174-185.Ootani, K. and S Nakagawa.
1995.
A semi-automatic learning method of grammar rules forspontaneous speech.
In Proc.
of Natural LanguageProcessing Pacific Rim Symposium'95, pages 514-519.Pereira, F. and Y. Schabes.
1992.
Inside-outsidereestimation from partially bracketed corpora.
InProceedings of 30th Annual Meeting of the ACL,pages 128-135.Theeramunkong, Thanaruk and Manabu Okumura.1996.
Towards automatic grammar acquisitionfrom a bracketed corpus.
In Proc.
of the 4th Inter-national Workshop on Very Large Corpora, pages168-177.Theeramunkong, Thanaruk and Manabu Okumura.1997.
Statistical parsing with a grammar acquiredfrom a bracketed corpus based on clustering anal-ysis.
In International Joint Conference on Artifi-cial Intelligence (IJCAI-97), Poster Session.?
83
