Acquiring Lexical Generalizations from Corpora:A Case Study for Diathesis AlternationsMaria LapataSchool of Cognitive ScienceDivision of Informatics, University of Edinburgh2 Buccleuch Place, Edinburgh EH8 9LW, UKmlap@cogsci.ed.ac.ukAbstractThis paper examines the extent to which verbdiathesis alternations are empirically attested incorpus data.
We automatically acquire alternatingverbs from large balanced corpora by using partial-parsing methods and taxonomic information, anddiscuss how corpus data can be used to quantify lin-guistic generalizations.
We estimate the productiv-ity of an alternation and the typicality of its mem-bers using type and token frequencies.1 IntroductionDiathesis alternations are changes in the realizationof the argument structure of a verb that are some-times accompanied by changes in meaning (Levin,1993).
The phenomenon i English is illustrated in(1)-(2) below.
(1) a. John offers shares to his employees.b.
John offers his employees shares.
(2) a.
Leave a note for her.b.
Leave her a note.Example (1) illustrates the dative alternation, whichis characterized by an alternation between theprepositional frame 'V NP1 to NP2' and the doubleobject frame 'V NP 1 NP2'.
The benefactive alterna-tion (cf.
(2)) is structurally similar to the dative, thedifference being that it involves the preposition forrather than to.Levin (1993) assumes that the syntactic realiza-tion of a verb's arguments i directly correlated withits meaning (cf.
also Pinker (1989) for a similar pro-posal).
Thus one would expect verbs that undergothe same alternations to form a semantically co-herent class.
Levin's study on diathesis alternationshas influenced recent work on word sense disam-biguation (Dorr and Jones, 1996), machine transla-tion (Dang et al, 1998), and automatic lexical ac-quisition (McCarthy and Korhonen, 1998; Schulteim Walde, 1998).The objective of this paper is to investigate the ex-tent to which diathesis alternations are empiricallyattested in corpus data.
Using the dative and bene-factive alternations as a test case we attempt o de-termine: (a) if some alternations are more frequentthan others, (b) if alternating verbs have frame pref-erences and (c) what the representative members ofan alternation are.In section 2 we describe and evaluate the set ofautomatic methods we used to acquire verbs under-going the dative and benefactive alternations.
Weassess the acquired frames using a filtering methodpresented in section 3.
The results are detailed insection 4.
Sections 5 and 6 discuss how the derivedtype and token frequencies can be used to estimatehow productive an alternation is for a given verb se-mantic class and how typical its members are.
Fi-nally, section 7 offers some discussion on futurework and section 8 conclusive remarks.2 Method2.1 The parserThe part-of-speech tagged version of the British Na-tional Corpus (BNC), a 100 million word collec-tion of written and spoken British English (Burnard,1995), was used to acquire the frames characteris-tic of the dative and benefactive alternations.
Sur-face syntactic structure was identified using Gsearch(Keller et al, 1999), a tool which allows the searchof arbitrary POS-tagged corpora for shallow syntac-tic patterns based on a user-specified context-freegrammar and a syntactic query.
It achieves this bycombining a left-corner parser with a regular ex-pression matcher.Depending on the grammar specification (i.e., re-cursive or not) Gsearch can be used as a full context-free parser or a chunk parser.
Depending on the syn-tactic query, Gsearch can parse full sentences, iden-tify syntactic relations (e.g., verb-object, adjective-noun) or even single words (e.g., all indefinite pro-397nouns in the corpus).Gsearch outputs all corpus sentences containingsubstrings that match a given syntactic query.
Giventwo possible parses that begin at the same point inthe sentence, the parser chooses the longest match.If there are two possible parses that can be producedfor the same substring, only one parse is returned.This means that if the number of ambiguous rules inthe grammar is large, the correctness of the parsedoutput is not guaranteed.2.2 AcquisitionWe used Gsearch to extract okens matching thepatterns 'V NP1 NP2', 'VP NP1 to NP2', and 'VNPI for NP2' by specifying a chunk grammar forrecognizing the verbal complex and NPs.
POS-tagswere retained in the parser's output which was post-processed to remove adverbials and interjections.Examples of the parser's output are given in (3).Although there are cases where Gsearch producesthe right parse (cf.
(3a)), the parser wrongly iden-tifies as instances of the double object frame to-kens containing compounds (cf.
(3b)), bare relativeclauses (cf.
(3c)) and NPs in apposition (cf.
(3d)).Sometimes the parser attaches prepositional phrasesto the wrong site (cf.
(3e)) and cannot distinguishbetween arguments and adjuncts (cf.
(3f)) or be-tween different ypes of adjuncts (e.g., temporal(cf.
(3f)) versus benefactive (cf.
(3g))).
Erroneousoutput also arises from tagging mistakes.
(3) a.
The police driver \[v shot\] \[NP Jamie\] \[ie alook of enquiry\] which he missed.b.
Some also \[v offer\] \[ipa free bus\] lip ser-vice\], to encourage customers who do nothave their own transport.c.
A Jaffna schoolboy \[v shows\] \[NP a draw-ing\] lip he\] made of helicopters trafinghis home town.d.
For the latter catalogue Barr \[v chose\]\[NP the Surrealist writer\] \[yp GeorgesHugnet\] to write a historical essay.e.
It \[v controlled\] \[yp access\] [pp to \[Nr' thevault\]\].f.
Yesterday he \[v rang\] \[NP the bell\] \[Pl, for\[NP a long time\]\].g.
Don't Iv save\] \[NP the bread\] \[pp for\[NP the birds\]\].We identified erroneous subcategorization frames(cf.
(3b)-(3d)) by using linguistic heuristics anda process for compound noun detection (cf.
sec-tion 2.3).
We disambiguated the attachment site ofPPs (cf.
(3e)) using Hindle and Rooth's (1993) lex-ical association score (cf.
section 2.4).
Finally, werecognized benefactive PPs (cf.
(3g)) by exploitingthe WordNet axonomy (cf.
section 2.5).2.3 Guessing the double object frameWe developed a process which assesses whether thesyntactic patterns (called cues below) derived fromthe corpus are instances of the double object frame.Linguistic Heuristics.
We applied several heuris-tics to the parser's output which determined whethercorpus tokens were instances of the double objectframe.
The 'Reject' heuristics below identified er-roneous matches (cf.
(3b-d)), whereas the 'Accept'heuristics identified true instances of the double ob-ject frame (cf.
(3a)).1.
Reject if cue contains at least two propernames adjacent to each other (e.g., killedHenry Phipps ).2.
Reject if cue contains possessive noun phrases(e.g., give a showman's award).3.
Reject if cue's last word is a pronoun or ananaphor (e.g., ask the subjects themselves).4.
Accept if verb is followed by a personal or in-definite pronoun (e.g., found him a home).5.
Accept if verb is followed by an anaphor(e.g., made herself a snack).6.
Accept if cue's surface structure is either 'VMOD l NP MOD NP' or 'V NP MOD NP'(e.g., send Bailey a postcard).7.
Cannot decide if cue's surface structure is'V MOD* N N+' (e.g., offer a free bus ser-vice).Compound Noun Detection.
Tokens identifiedby heuristic (7) were dealt with separately by a pro-cedure which guesses whether the nouns followingthe verb are two distinct arguments or parts of acompound.
This procedure was applied only to nounsequences of length 2 and 3 which were extractedfrom the parser's output 2 and compared against acompound noun dictionary (48,661 entries) com-piled from WordNet.
13.9% of the noun sequenceswere identified as compounds in the dictionary.I Here MOD represents any prenominal modifier (e.g., arti-cles, pronouns, adjectives, quantifiers, ordinals).2Tokens containing noun sequences with length largerthan 3 (450 in total) were considered negative instances ofthedouble object frame.398G-score ~" 2-word compound1967.68775.2187.0245.4030.5829.9424.04bank managertax liabilityincome taxbook reviewerdesigner gearsafety plandrama schoolTable 1 : Random sample of two word compounds TableG-score 3-word compound574.48382.9277.7848.8436.4432.3523.98\[\[energy efficiency\] office\]\[\[council tax\] bills\]\[alcohol \[education course\]\]\[hospital \[out-patient department\]\[\[turnout suppressor\] function\]\[\[nature conservation\] resources\]\[\[quality amplifier\] circuits\]2: Random sample of three word compoundsFor sequences of length 2 not found in WordNet,we used the log-likelihood ratio (G-score) to esti-mate the lexical association between the nouns, inorder to determine if they formed a compound noun.We preferred the log-likelihood ratio to other statis-tical scores, such as the association ratio (Churchand Hanks, 1990) or ;(2, since it adequately takesinto account the frequency of the co-occurringwords and is less sensitive to rare events and corpus-size (Dunning, 1993; Daille, 1996).
We assumedthat two nouns cannot be disjoint arguments of theverb if they are lexically associated.
On this basis,tokens were rejected as instances of the double ob-ject frame if they contained two nouns whose G-score had a p-value less than 0.05.A two-step process was applied to noun se-quences of length 3: first their bracketing was de-termined and second the G-score was computed be-tween the single noun and the 2-noun sequence.We inferred the bracketing by modifying an al-gorithm initially proposed by Pustejovsky et al(1993).
Given three nouns n 1, n2, n3, if either \[n I n2\]or \[n2 n3\] are in the compound noun dictionary, webuilt structures \[\[nt n2\] n3\] or \[r/l \[n2 n3\]\] accord-ingly; if both \[n I n2\] and In2 n3\] appear in the dic-tionary, we chose the most frequent pair; if neither\[n l n2\] nor \[n2 n3\] appear in WordNet, we computedthe G-score for \[nl n2\] and \[n2 n3\] and chose thepair with highest value (p < 0.05).
Tables 1 and2 display a random sample of the compounds themethod found (p < 0.05).2.3.1 EvaluationThe performance of the linguistic heuristics and thecompound etection procedure were evaluated byrandomly selecting approximate!y 3,000 corpus to-kens which were previously accepted or rejected asinstances of the double object frame.
Two judges de-cided whether the tokens were classified correctly.The judges' agreement on the classification task wascalculated using the Kappa coefficient (Siegel andMethod l\[ Prec l\[ KappaReject heuristics 96.9% K = 0.76, N = 1000Accept heuristics 73.6% K = 0.82, N = 10002-word compounds 98.9% K = 0.83, N = 5533-word compounds 99.1% K = 0.70, N = 447Verb attach-to 74.4% K = 0.78, N = 494Noun attach-to 80.0% K = 0.80, N = 500Verb attach-for 73.6% K = 0.85, N = 630Noun attach-for 36.0% K = 0.88, N = 500Table 3: Precision of heuristics, compound noun de-tection and lexical associationCastellan, 1988) which measures inter-rater agree-ment among a set of coders making category judg-ments.The Kappa coefficient of agreement (K) is the ra-tio of the proportion of times, P(A), that k ratersagree to the proportion of times, P(E), that wewould expect he raters to agree by chance (cf.
(4)).If there is a complete agreement among the raters,then K = 1.P(A) -- P(E)(4) K -1 - -  P (E )Precision figures 3 (Prec) and inter-judge agreement(Kappa) are summarized in table 3.
In sum, theheuristics achieved a high accuracy in classifyingcues for the double object frame.
Agreement on theclassification was good given that the judges weregiven minimal instructions and no prior training.2.4 Guessing the prepositional framesIn order to consider verbs with prepositional framesas candidates for the dative and benefactive alterna-tions the following requirements needed to be met:1. the PP must be attached to the verb;3Throught the paper the reported percentages are the aver-age of the judges' individual classifications.3992.
in the case of the 'V NPI to NP2' structure, theto-PP must be an argument of the verb;3. in the case of the 'V NPI for NP2' structure,the for-PP must be benefactive.
4In older to meet requirements (1)-(3), we first de-termined the attachment site (e.g., verb or noun) ofthe PP and secondly developed a procedure for dis-tinguishing benefactive from non-benefactive PPs.Several approaches have statistically addressedthe problem of prepositional phrase ambiguity,with comparable results (Hindle and Rooth, 1993;Collins and Brooks, 1995; Ratnaparkhi, 1998).
Hin-dle and Rooth (1993) used a partial parser to extract(v, n, p) tuples from a corpus, where p is the prepo-sition whose attachment is ambiguous between theverb v and the noun n. We used a variant of themethod described in Hindle and Rooth (1993), themain difference being that we applied their lexicalassociation score (a log-likelihood ratio which com-pares the probability of noun versus verb attach-ment) in an unsupervised non-iterative manner.
Fur-thermore, the procedure was applied to the specialcase of tuples containing the prepositions to and foronly.2.4.1 EvaluationWe evaluated the procedure by randomly select-ing 2,124 tokens containing to-PPs and for-PPsfor which the procedure guessed verb or noun at-tachment.
The tokens were disambiguated by twojudges.
Precision figures are reported in table 3.The lexicai association score was highly accu-rate on guessing both verb and noun attachment forto-PPs.
Further evaluation revealed that for 98.6%(K = 0.9, N = 494, k -- 2) of the tokens clas-sified as instances of verb attachment, he to-PPwas an argument of the verb, which meant hat thelog-likelihood ratio satisfied both requirements (1)and (2) for to-PPs.A low precision of 36% was achieved in detectinginstances of noun attachment for for-PPs.
One rea-son for this is the polysemy of the preposition for:for-PPs can be temporal, purposive, benefactive orcausal adjuncts and consequently can attach to var-ious sites.
Another difficulty is that benefactive for-PPs semantically license both attachment sites.To further analyze the poor performance of thelog-likelihood ratio on this task, 500 tokens con-4Syntactically speaking, benefactive for-PPs are not argu-ments but adjuncts (Jackendoff, 1990) and can appear on anyverb with which they are semantically compatible.taining for-PPs were randomly selected from theparser's output and disambiguated.
Of these 73.9%(K = 0.9, N = 500, k ---- 2) were instances of verbattachment, which indicates that verb attachmentsoutnumber noun attachments for for-PPs, and there-fore a higher precision for verb attachment (cf.
re-quirement (1)) can be achieved without applying thelog-likelihood ratio, but instead classifying all in-stances as verb attachment.2.5 Benefactive PPsAlthough surface syntactic ues can be importantfor determining the attachment site of prepositionalphrases, they provide no indication of the semanticrole of the preposition in question.
This is particu-larly the case for the preposition for which can haveseveral roles, besides the benefactive.Two judges discriminated benefactive from non-benefactive PPs for 500 tokens, randomly selectedfrom the parser's output.
Only 18.5% (K ---- 0.73,N ---- 500, k = 2) of the sample contained bene-factive PPs.
An analysis of the nouns headed by thepreposition for revealed that 59.6% were animate,17% were collective, 4.9% denoted locations, andthe remaining 18.5% denoted events, artifacts, bodyparts,'or actions.
Animate, collective and locationnouns account for 81.5% of the benefactive data.We used the WordNet taxonomy (Miller et al,1990) to recognize benefactive PPs (cf.
require-ment (3)).
Nouns in WordNet are organized intoan inheritance system defined by hypernymic rela-tions.
Instead of being contained in a single hier-archy, nouns are partitioned into a set of seman-tic primitives (e.g., act, animal, time) which aretreated as the unique beginners of separate hier-archies.
We compiled a "concept dictionary" fromWordNet (87,642 entries), where each entry con-sisted of the noun and the semantic primitive dis-tinguishing each noun sense (cf.
table 4).We considered a for-PP to be benefactive if thenoun headed by for was listed in the concept dic-tionary and the semantic primitive of its primesense (Sense 1) was person, animal, group or lo-cation.
PPs with head nouns not listed in the dictio-nary were considered benefactive only if their headnouns were proper names.
Tokens containing per-sonal, indefinite and anaphoric pronouns were alsoconsidered benefactive ( .g., build a home for him).Two judges evaluated the procedure by judging1,000 randomly selected tokens, which were ac-cepted or rejected as benefactive.
The procedureachieved a precision of 48.8% (K ----- 0.89, N =400giftcookingteacheruniversitycitypencilSense 1 Sense 2 Sense 3possessionfoodpersongrouplocationartifactcognitionactcognitionartifactlocationactgroupgroupTable 4: Sample entries from WordNet concept dic-tionary500, k = 2) in detecting benefactive tokens and90.9% (K = .94, N = 499, k = 2) in detectingnon-benefactive ones.3 Fi l ter ingFiltering assesses how probable it is for a verb to beassociated with a wrong frame.
Erroneous framescan be the result of tagging errors, parsing mistakes,or errors introduced by the heuristics and proce-dures we used to guess syntactic structure.We discarded verbs for which we had very littleevidence (frame frequency = 1) and applied a rela-tive frequency cutoff: the verb's acquired frame fre-quency was compared against its overall frequencyin the BNC.
Verbs whose relative frame frequencywas lower than an empirically established thresh-old were discarded.
The threshold values variedfrom frame to flame but not from verb to verb andwere determined by taking into account for eachframe its overall frame frequency which was es-timated from the COMLEX subcategorization dic-tionary (6,000 verbs) (Grishman et al, 1994).
Thismeant hat the threshold was higher for less frequentframes (e.g., the double object frame for which only79 verbs are listed in COMLEX).We also experimented with a method suggestedby Brent (1993) which applies the binomial teston frame frequency data.
Both methods yieldedcomparable r sults.
However, the relative frequencythreshold worked slightly better and the results re-ported in the following section are based on thismethod.4 ResultsWe acquired 162 verbs for the double object frame,426 verbs for the 'V NP1 to NP2' frame and 962for the 'V NPl for NP2' frame.
Membership in al-ternations was judged as follows: (a) a verb partic-ipates in the dative alternation if it has the doubleobject and 'V NP1 to NP2' frames and (b) a verbDative AlternationAlternatingV NPI NP2allot, assign, bring, fax, feed, flick,give, grant, guarantee, leave, lendoffer, owe, take pass, pay, render,repay, sell, show, teach, tell, throw,toss, write, serve, send, awardallocate, bequeath, carry, catapult,cede, concede, drag, drive, extend,ferry, fly, haul, hoist, issue, lease,peddle, pose, preach, push, relay,ship, tug, yieldV NPI to NP2 ask, chuck, promise, quote, read,shoot, slipBenefactive AlternationAlternating bake, build, buy, cast, cook, earn,fetch, find, fix, forge, gain, get,keep, knit, leave, make, pour, saveprocure, secure, set, toss, win, writeV NPI NP2 arrange, assemble, carve, choose,compile, design, develop, dig,gather, grind, hire, play, prepare,reserve, run, sewV NP1 for NP2 boil, call, shootTable 5: Verbs common in corpus and Levinparticipates in the benefactive alternation if it hasthe double object and 'V NP1 for NP2' frames.
Ta-ble 5 shows a comparison of the verbs found in thecorpus against Levin's list of verbs; 5rows 'V NP1 toNP2' and 'V NP1 for NP2' contain verbs listed asalternating in Levin but for which we acquired onlyone frame.
In Levin 115 verbs license the dative and103 license the benefactive alternation.
Of these weacquired 68 for the dative and 43 for the benefactivealternation (in both cases including verbs for whichonly one frame was acquired).The dative and benefactive alternations were alsoacquired for 52 verbs not listed in Levin.
Of these,10 correctly alternate (cause, deliver, hand, refuse,report and set for the dative alternation and cause,spoil, afford and prescribe for the benefactive), and12 can appear in either frame but do not alter-nate (e.g., appoint, fix, proclaim).
For 18 verbs twoframes were acquired but only one was correct (e.g.,swap and forgive which take only the double objectframe), and finally 12 verbs neither alternated norhad the acquired frames.
A random sample of theacquired verb frames and their (log-transformed)frequencies i shown in figure 1.5The comparisons reported henceforth exclude verbs listedin Levin with overall corpus frequency less than 1 per million.401I080=.=., -  4 ==,,d2NP-PP  to frameNP-PP_for frameNP-NP framei1\]Figure 1: Random sample of acquired frequenciesfor the dative and benefactive alternationsclass the number of verbs acquired from the cor-pus against he number of verbs listed in Levin.
Ascan be seen in figure 2, Levin and the corpus ap-proximate ach other for verbs of FUTURE HAVING(e.g., guarantee), verbs of MESSAGE TRANSFER(e.g., tell) and BRING-TAKE verbs (e.g., bring).The semantic lasses of GIVE (e.g., sell), CARRY(e.g., drag), SEND (e.g., ship), GET (e.g., buy) andPREPARE (e.g., bake) verbs are also fairly well rep-resented in the corpus, in contrast o SLIDE verbs(e.g., bounce) for which no instances were found.Note that the corpus and Levin did not agreewith respect o the most popular classes licensingthe dative and benefactive alternations: THROWING(e.g., toss) and BUILD verbs (e.g., carve) are thebiggest classes in Levin allowing the dative andbenefactive alternations respectively, in contrast oFUTURE HAVING and GET verbs in the corpus.This can be explained by looking at the average cor-pus frequency of the verbs belonging to the seman-tic classes in question: FUTURE HAVING and GETLevi, I 1 1 verbs outnumber THROWING and BUILD verbs by30 ~ Corpus dative .
II 1 I a factor of two to one.5 ProductivityThe relative productivity of an alternation for a se-20 mantic class can be estimated by calculating the ra-tio of acquired to possible verbs undergoing the al-ternation (Aronoff, 1976; Briscoe and Copestake, Zl0 1996):(5) P(acquired\[class) = f (acquired, class)f (class)o We express the productivity of an alternation foro =.
"~ ~= ~ ,~.. ~=.~ ?
.-= ~Figure 2: Semantic lasses for the dative and bene-factive alternationsLevin defines 10 semantic lasses of verbs forwhich the dative alternation applies (e.g., GIVEverbs, verbs of FUTURE HAVING, SEND verbs), and5 classes for which the benefactive alternation ap-plies (e.g., BUILD, CREATE, PREPARE verbs),  as-suming  that verbs participating in the same classshare certain meaning components.We partitioned our data according to Levin's pre-defined classes.
Figure 2 shows for each semantica given class as f(acquired, class), the number ofverbs which were found in the corpus and are mem-bers of the class, over f(class), the total numberof verbs which are listed in Levin as members ofthe class (Total).
The productivity values (Prod) forboth the dative and the benefactive alternation (Alt)are summarized in table 6.Note that productivity is sensitive to class size.The productivity of BRING-TAKE verbs is esti-mated to be 1 since it contains only 2 memberswhich were also found in the corpus.
This is intu-itively correct, as we would expect he alternationto be more productive for specialized classes.The productivity estimates discussed here can bepotentially useful for treating lexical rules proba-bilistically, and for quantifying the degree to whichlanguage users are willing to apply' a rule in order402BRING-TAKE 2 2 1 0.327FUTURE HAVING 19 17 0.89 0.313GIVE 15 9 0.6 0.55M.TRANSFER 17 10 0.58 0.66CARRY 15 6 0.4 0.056DRIVE 11 3 0.27 0.03THROWING 30 7 0.23 0.658SEND 23 3 0.13 0.181INSTR.
COM.
18 1 0.05 0.648SLIDE 5 0 0 0Benefactive alternationClass Total Alt Prod TypGET 33 17 0.51 0.54PREPARE 26 9 0.346 0.55BUILD 35 12 0.342 0.34PERFORMANCE 19 1 0.05 0.56CREATE 20 2 0.1 0.05Table 6: Productivity estimates and typicality valuesfor the dative and benefactive alternationto produce a novel form (Briscoe and Copestake,1996).6 TypicalityEstimating the productivity of an alternation for agiven class does not incorporate information aboutthe frequency of the verbs undergoing the alterna-tion.
We propose to use frequency data to quantifythe typicality of a verb or verb class for a given alter-nation.
The underlying assumption is that a verb istypical for an alternation if it is equally frequent forboth frames which are characteristic for the alter-nation.
Thus the typicality of a verb can be definedas the conditional probability of the frame given theverb:f (framei, verb)(6) P(frameilverb) =y~ f fframe n, verb)nWe calculate Pfframeilverb) by dividingf(frame i, verb), the number of times the verbwas attested in the corpus with frame i, by~-~.,, f(frame,,, verb), the overall number of timesthe verb was attested.
In our case a verb has twoframes, hence P(frameilverb) is close to 0.5 fortypical verbs (i.e., verbs with balanced frequencies)and close to either 0 or 1 for peripheral verbs,depending on their preferred frame.
Consider theverb owe as an example (cf.
figure 1).
648 instancesof owe were found, of which 309 were instancesof the double object frame.
By dividing the latterby the former we can see that owe is highly typicalof the dative alternation: its typicality score for thedouble object frame is 0.48.By taking the average of P(framei, verb) for allverbs which undergo the alternation and belong tothe same semantic lass, we can estimate how typi-cal this class is for the alternation.
Table 6 illustratesthe typicality (Typ) of the semantic lasses for thetwo alternations.
(The typicality values were com-puted for the double object frame).
For the dativealternation, the most typical class is GIVE, and themost peripheral is DRIVE (e.g., ferry).
For the bene-factive alternation, PERFORMANCE (e.g., sing),PREPARE (e.g., bake) and GET (e.g., buy) verbs arethe most typical, whereas CREATE verbs (e.g., com-pose) are peripheral, which seems intuitively cor-rect.7 Future WorkThe work reported in this paper relies on framefrequencies acquired from corpora using partial-parsing methods.
For instance, frame frequency datawas used to estimate whether alternating verbs ex-hibit different preferences for a given frame (typi-cality).However, it has been shown that corpus id-iosyncrasies can affect subcategorization frequen-cies (cf.
Roland and Jurafsky (1998) for an exten-sive discussion).
This suggests that different corporamay give different results with respect o verb al-ternations.
For instance, the to-PP frame is poorly'represented in the syntactically annotated version ofthe Penn Treebank (Marcus et al, 1993).
There areonly 26 verbs taking the to-PP frame, of which 20have frame frequency of 1.
This indicates that a verysmall number of verbs undergoing the dative alter-nation can be potentially acquired from this corpus.In future work we plan to investigate the degree towhich corpus differences affect he productivity andtypicality estimates for verb alternations.8 ConclusionsThis paper explored the degree to which diathesisalternations can be identified in corpus data via shal-low syntactic processing.
Alternating verbs were ac-quired from the BNC by using Gsearch as a chunkparser.
Erroneous frames were discarded by apply-ing linguistic heuristics, statistical scores (the log-likelihood ratio) and large-scale lexical resources403(e.g., WordNet).We have shown that corpus frequencies can beused to quantify linguistic intuitions and lexicalgeneralizations such as Levin's (1993) semanticclassification.
Furthermore, corpus frequencies canmake explicit predictions about word use.
This wasdemonstrated by using the frequencies to estimatethe productivity of an alternation for a given seman-tic class and the typicality of its members.AcknowledgmentsThe author was supported by the AlexanderS.
Onassis Foundation and the UK Economic andSocial Research Council.
Thanks to Chris Brew,Frank Keller, Alex Lascarides and Scott McDonaldfor valuable comments.ReferencesMark Aronoff.
1976.
Word Formation in GenerativeGrammar.
Linguistic Inquiry Monograph 1.
MITPress, Cambridge, MA.Michael Brent.
1993.
From grammar to lexicon: Un-supervised learning of lexical syntax.
ComputationalLinguistics, 19(3):243-262.Ted Briscoe and Ann Copestake.
1996.
Contolling theapplication of lexical rules.
In Proceedings of ACLSIGLEX Workshop on Breadth and Depth of SemanticLexicons, pages 7-19, Santa Cruz, CA.Lou Burnard, 1995.
Users Guide for the British NationalCorpus.
British National Corpus Consortium, OxfordUniversity Computing Service.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation orms, mutual information, and lexicog-raphy.
Computational Linguistics, 16(1):22-29.COLING/ACL 1998.
Proceedings of the 17th Interna-tional Conference on Computational Linguistics and36th Annual Meeting of the Association for Computa-tional Linguistics, Montr6al.Michael Collins and James Brooks.
1995.
Prepositionalphrase attachment through a backed-off model.
InProceedings of the 3rdWorkshop on Very Large Cor-pora, pages 27-38.B6atrice Daille.
1996.
Study and implementation fcombined techniques for automatic extraction of ter-minology.
In Judith Klavans and Philip Resnik, ed-itors, The Balancing Act: Combining Symbolic andStatistical Approaches to Language, pages 49-66.MIT Press, Cambridge, MA.Hoa Trang Dang, Karin Kipper, Martha Palmer, andJoseph Rosenzweig.
1998.
Investigating regularsense extensions based on intersective Levin classes.In COLING/ACL 1998, pages 293-299.Bonnie J. Dorr and Doug Jones.
1996.
Role of wordsense disambiguation i  lexical acquisition: Predict-ing semantics from syntactic ues.
In Proceedings ofthe 16th International Conference on ComputationalLinguistics, pages 322-327, Copenhagen.Ted Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics, 19(1):61-74.Ralph Grishman, Catherine Macleod, and Adam Meyers.1994.
Comlex syntax: Building a computational lexi-con.
In Proceedings of the 15th International Confer-ence on Computational Linguistics, pages 268-272,Kyoto.Donald Hindle and Mats Rooth.
1993.
Structural am-biguity and lexical relations.
Computational Linguis-tics, 19(1):103-120.Ray Jackendoff.
1990.
Semantic Structures.
MIT Press,Cambridge, MA.Frank Keller, Martin Corley, Steffan Corley, Matthew W.Crocker, and Shari Trewin.
1999.
Gsearch: A tool forsyntactic investigation of unparsed corpora.
In Pro-ceedings of the EACL Workshop on Linguistically In-terpreted Corpora, Bergen.Beth Levin.
1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
University ofChicago Press, Chicago.Mitchell R Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19(2):313-330.Diana McCarthy and Anna Korhonen.
1998.
Detectingverbal participation i diathesis alternations.
In COL-ING/ACL 1998, pages 1493-1495.
Student Session.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.
1990.Introduction to WordNet: An on-line lexical database.International Journal of Lexicography, 3(4):235-244.
'Steven Pinker.
1989.
Learnability and Cognition: TheAcquisition of Argument Structure.
MIT Press, Cam-bridge MA.James Pustejovsky, Sabine Bergler, and Peter Anick.1993.
Lexical semantic techniques for corpus anal-ysis.
ComputationalLinguistics, ~\[9(3):331-358.Adwait Ratnaparkhi.
1998.
Unsupervised statisticalmodels for prepositional phrase attachment.
In Pro-ceedings of the 7th International Conference on Com-putational Linguistics, pages 1079-1085.Douglas Roland and Daniel Jurafsky.
1998.
How verbsubcategorization frequencies are affected by corpuschoice.
In COLING/ACL 1998, pages 1122-1128.Sabine Schulte im Walde.
1998.
Automatic semanticclassification of verbs according to their alternationbehaviour.
Master's thesis, Institut f"ur MaschinelleSprachverarbeitung, University of Stuttgart.Sidney Siegel and N Castellan.
1988.
Non ParametricStatistics for the Behavioral Sciences.
McGraw-Hill,New York.404
