PLANS FOR A TASK-ORIENTED EVALUATION OFNATURAL LANGUAGE UNDERSTANDING SYSTEMSBeth M. SundheimNaval Ocean Systems Center, Code 444San Diego, CA 92152-5000ABSTRACTA plan is presented for evaluating natural language processing (NLP) systems that have focused on the issues oftext understanding asexemplified in short exts from military messages.
The plan includes definition of bodies oftext to use as development and test data, namely the narrative lines from one type of naval message, and definitionof a simulated atabase update task that requires NLP systems to fill a template with information found in the texts.Documentation related to the naval messages and examples of filled templates have been prepared to assist NLPsystem developers.
It is anticipated that developers of a number of different NLP systems will participate in theevaluation and will meet afterwards to present and interpret the results and to critique the test design.INTRODUCTIONThis project undertakes to provide meaningful measures of progress in the field of natural language processing(NLP).
In particular, it is intended to result in definition of a theory- and implementation-independent test of thetext analysis capabilities of text understanding systems that analyze short (paragraph-length) exts taken frommilitary messages.
The test is task-oriented in order to facilitate assessment of the general state of the art andprovide a meaningful basis for comparing notes across systems.
This design would seem to have two majorproblems, however: the reduction of a system's capabilities to a simple quantification of right versus wronganswers, and the lack of desired focus on understanding capabilities versus application capabilities.It is claimed, however, that if the task performance is recorded on development data as well as test data and isrepeated on the test data after updates are made, additional insights can be gained into a sytem's robustness, breadthand depth of coverage, and potential for handling novel text.
A measurement of utility can be gained as well, bymeasuring performance on the original task, versus performance using a version of the inputs in which punctuationand spelling errors, highly elliptical constructions and sublanguage constructions have been eliminated.
Theseadditional measurements open up the black box to some extent, providing information that far exceeds what wouldbe obtainable from a single measurement of performance on the test data in a blind test.Also, despite the fact that the NLP systems are treated as black boxes, the evaluation should provide significantinsights into their understanding versus application capabilities, because successful performance ofthe task does notrequire that back end modules contribute substantive information to the template fills.
For example, the templatefills do not require that any computations be performed on the data.
This aspect of the test design is another way inwhich the black box has been opened up or narrowed own to increase the meaningfulness of the results.
It isimportant, however, to recognize that the test is applicable only to "complete" and non-interactive systems, onesthat are capable of accepting unseen texts and working essentially without human intervention tounderstand them.This project isfunded by DARPA/ISTO under ARPA Order No.
6359, Program Code 8E20, Program Element Code 62301E.197THE TEXT CORPUSIt is important that the amount of time and effort required for a system to be able to participate in the evaluation beas short as possible.
For this reason, a serous effort has been made to collect texts in a narrow domain and toprovide types of documentation that will reduce the amount of knowledge acquisition and engineering required.
Wehave selected and prepared ocumentation a set of 155 Navy messages written in a format known as OPREP-3Pinnacle/Front Burner (OPREP-3 PFB), whose use and format are prescribed in OPNAVINST 3100.6D, "SpecialIncident Reporting," an unclassified Navy instruction.
The examples elected concern encounters among aircraft,surface ships, and/or submarines.
The encounters ange in intensity from simple detection through overt hostilitydirected toward one of these "platform" types or an ashore facility.
The nature of these messages i felt to beconstrained indomain but not overly specialized.OPREP-3 PFBs consist of several different paragraphs, each containing a prescribed type of information.
Theformat of the information provided in each paragrph is generally unrestricted, and much of the information issupplied by message originators as free-form English text.
The three major free-text paragraphs are (1) a narrativeaccount of the incident, (2) a description of casualties suffered by personnel and equipment, and (3) miscellaneousremarks on the incident.The OPREP-3 PFBs in the corpus have many features which make them tractable texts for current NLP systems:.
They usually report on one or more closely-related general topics.
The reported events fit into a fairlycircumscribed set of scenarios concerning basic kinds of interaction between opposing forces of differenttypes.
Thus, the vocabulary is relatively limited, and so are the semantics of the domain...They contain little speculation.
At least in the narrative line, the author is attempting to report events asthey occurred and not to speculate on those events.
Thus, there is not too much in the way of complexconstructions that convey an analysis (e.g.
"\[I\] Believe that \[the\] attack was successful.
").They contain little embellishing information.
They typically give only time, location and sensor/weaponinformation to supplement the recounting of the events.
The succinct style preferred for Navy messagesdiscourages the use of nonessential descnptve or qualifying expressions.
This further educes the number ofdifferent English constructions that a system would need to be able to syntactically parse, and restrictssemantic interpretation mainly to representing fundamental ttributes of agent, object, time, place, andinstrument.4.
They stick basically to one topic per message.
For the most part, it is not necessary to unravel a complexstory, matching various events with different agents, objects, etc., and figuring out the time sequence.Of course, there are also reasons why the text portions of OPREP-3 PFBs are in some ways very difficult toanalyze.
Some of the more superficial features that distinguish them from standard expository texts are1.
Poorer than average use of punctuation.
Periods, especially, are sometimes omitted, leading to run-onsentences and increased amounts of ambiguity.2.
Heavy evidence of ellipsis (telegraphic style).
Subjects, objects, articles, and prepositions are frequentlyomitted.3.
Use of special constructions, e.g., for representing time, date, and location.4.
Frequent misspellings.
This is much more evident than in highly edited texts.198Some1..of the difficult distinguishing semantic features of OPREP-3 PFBs areAssumption of knowledge of a specialized omain.
The events, objects, and relationships in the Navydomain, e.g., what types of weapons can be used by what type of ship for what purpose, are not commonknowledge.
Frequently, the meaning of some part of a narrative will be somewhat mbiguous or vague to anonspecialist, but completely clear to a knowledgeable p rson.
Until a system developer has acquired asound knowledge of the domain and has imparted it to the system's knowledge bases, the system is unlikelyto perform any task very well.Assumption of knowledge of contents of other paragraphs inthe message.
The narrative paragraphs are notintended to stand alone.
The first paragraph of the message, for example, alerts the reader to the generalsubject of the message, so the narrative may omit some information that it would otherwise have included.That information may not be absolutely necessary for understanding the narrative in isolation but wouldhelp at least o reduce the degree of vagueness and ambiguity that he reader or system must resolve.INPUTS TO NLP  SYSTEMS:  DEVELOPMENT AND TEST  SETSA total of 155 OPREP-3 PFBs are in the current corpus.
Of these, 105 have been designated as development (i.e.,training) data, and 50 have been set aside as test data.
The current plan is to divide the test data into two sets of 25messages ach so that hey can be used at different times in the future.The corpus has been subdivided into four groups, according to the types of platforms involved in the interaction.There is one group each for incidents involving aircraft, surface ships, submarines, and land targets.
The test dataincludes examples from each of these groups, in numbers proportional to the number of messages the developmentset contains for each group.The inputs to the NLP systems are expected to be the OPREP-3 PFB narrative lines only.
The intent is to limitthe input to free text only and to about one paragraph in length.
In that way, the task will focus on textunderstanding capabilities in general rather than on the understanding of a specialized message format, and it willinclude some, but not overwhelming, challenges for discourse-level processing.As an alternative to the verbatim narrative lines, a set of modified versions is being prepared.
The purpose is toallow systems that have not dealt extensively with the problems of telegraphic, often ill-formed texts to participatein the evaluation without having to undergo the extensive amount of development effort hat would be requiredbefore they could be expected to have much success with the original narratives.
Modifications will be made thatminimize the superficial problems identified in the previous ection (ellipsis, bad punctuation, specialized notationand misspellings).
The evaluation of a system may be carried out using either the verbatim narratives or themodified versions, or both.
For those systems which can analyze the unmodified inputs, a partial measurement ofsystem utility can be obtained.OUTPUTS FROM NLP  SYSTEMS:DESCRIPT ION OF THE TEMPLATE F ILL  TASKThe outputs are in the form of templates, imulating a simple database.
No formal database management system isrequired.
The software which must be developed especially for the benchmark test is a back end that takes theresults of the analysis and extracts or derives the desired information to fill the slots in the template.
This processis portrayed graphically below:199NLP SYSTEM NLP SYSTEMINPUTS F.RD.,ET_F.,,X0.OPREP-3 PFB -> NL ANALYSIS -> DATA EXTRACTOR/ ->NARRATIVES MODULES DERIVERQUTPUTSTEMPLATE FILLS(DB UPDATES)The intention is that the back-end module required for the task be quite small and simple, since the test is meant ofocus on the understanding capabilities, not on the sophistication of the system's database update capabilities.Systems will have to have mechanisms for mapping many kinds of data into canonical forms (see below), but thereis no requirement for performing calculations on the data nor for other non-linguistic manipulation of the data.The simulated atabase that will be created by the NLP systems is intended to capture basic information aboutevents that are of significant interest.
The events that will cause the system to fill in a template concern hostileor potentially hostile encounters between one or more members of the U.S. forces and one or more members of anenemy force -- detecting the enemy, tracking it, targeting it, harassing it, or attacking it.
A template is also to befilled in if the action goes the opposite direction, i.e., where it is the enemy platform that is detecting, tracking,targeting, harassing, or attacking.
Thus, the simulated atabase that is being created consists of the equivalent oftwo tables, one where the U.S. force carries out the action, and one where the enemy force carries out the action.Each time a new template is filled out, the equivalent of a new record is created for that table.Not all OPREP-3 PFBs report one of the events mentioned above, however.
There are some which reportintentions rather than past events, and ones which report events that are "not of interest" to the database.
Only theMESSAGE ID and EVENT slots (see below) should be filled out in these cases.
This provides a check on thedegree of understanding that a system is capable of, since there are times when a system that depended too heavilyon key words, such as "attack," would mistakenly fill out a template.SPECIFICATION OF THE TEMPLATE SLOTSThe template used in the benchmark test bears little resemblance toa comprehensive template schema such as thatused by Logicon's Data Base Generator system for stofing information on space event messages.
It is intentionallysimple, in an attempt to limit the amount of specialized back-end software the task requires, to limit the anticipatedconfusion and debate among system developers over what the expected "fight answers" are, and to increase thecomprehensibility of the output for all concerned.
Unfortunately, by keeping the template simple, some specificityis lost that one would like to have in a database.There are ten main slots in the template, plus one to identify the message that the data comes from.
The slots andtheir fill requirements are given on the next page.
The slots are meant o provide answers to the questions of What?Who?
How?
Where?
When?
With what outcome?
The expected fill for each slot falls into one of twocategofies: selection of an item from a set list of possible answers, or strings (phrases) from the input text.
Asmany of the fills as possible will come from predefined sets of possible names and categories.
For thenomenclature identifying specific agents, objects, instruments, and locations, there will be correspondence tablesthat can be implemented tooutput a canonical form of identification.Slot #1, which answers the question What?, is intended to indicate how sefious the incident is by identifying thegreatest level of hostility reported.
In ascending order of hostility, the events are DETECT, TRACK, TARGET,HARASS, and A'VFACK.
The other possible fill for that slot is OTHER, meaning that the event is not of interestto the database.
The remainder of the template should be left blank in that case.
If the event is of interest to thedatabase, the rest of the slots should be filled in; if information is not available for any of them, the phrase NODATA should be given as the fill.200SLOT# SLOT NAME DATA FILL REQUIREMENTS678910MESSAGE IDEVENT: HIGHEST LEVEL OF ACTIONFORCE INITIATING EVENTCATEGORY(S) OF EVENT AGENT(S)CATEGORY(S) OF EVENT OBJECT(S)ID(S) OF 0-TH LEVEL AGENT(S)ID(S) OF 0-TH LEVEL OBJECT(S)INSTRUMENT(S) OF 0-TH AGENT(S)LOC OF OBJECT(S) AT EVENT TIMETIME(S) OF EVENTRESULT(S) OF EVENTFrom input header (DEV-GROUP1-N09722-001)DETECT, TRACK, TARGET, HARASS,ATTACK, OTHERFRIENDLY, HOSTILE, NO DATAAIR, SURF, SUB, NO DATAAIR, SURF, SUB, LAND, NO DATACanonical form of name(s), else taxonomic categoryname(s) or organizational entity I.D., else NO DATASame as slot 5Same as slot 5, where item(s) is/are:1. sensor - for CONTACT, TRACK, TARGET2.
weapon - for HARASS, ATTACKCanonical form of location ame(s), or text stringwith absolute or relative location(s), else NO DATAString with absolute time(s) of1.
use of sensor - for DETECT, TRACK, TARGET2.
weapon launch or impact- for HARASS, ATTACK;else NO DATA1.
RESPONSE BY OPPOSING FORCE2.
HOLDING CONTACT, LOST CONTACT3.
CONTINUING TO TRACK,STOPPED TRACKING4.
HOLDING TARGET, LOST TARGET5.
(NO) DAMAGE OR LOSS TO AGENT,(NO) DAMAGE OR LOSS TO OBJECT -6. else, NO DATATable 1.
Specifications for Output TemplateA number of problems arose in preparing examples of filled templates, e.g., questions of how many templates werewarranted and cases where the answers were unclear or did not fit the requirements exactly.
On the other hand, therewere many cases where the task showed promise of providing significant insights into the ability of NLP systemsto correlate data, make inferences, fdter out negative cases, and accommodate complex or ill-formed structures.TEST  PARAMETERS AND MEASURESSeveral different measurements can be obtained from tests using the OPREP-3 PFB corpus.
These can be termed"recall," precision," "generality/potential," "utility," and "progress."
The table below describes how measurementsof them will be obtained and summarizes their significance as evaluation measures.
Tests will be conducted by thesystem developers at their own sites at two different times.
They will test the system upon receipt of 25 testnarratives, which will come after a two-month period of updates for the development set.
At that time, tests willbe run separately for the development and test sets.
After an additional month of updating to better handle the testset, the test will be rerun.
As a final data point and stimulus for discussion, approximately 10 previously unseennarratives will be run by developers at the meeting following the period of updating.
These narratives will bemanufactured to be variations of narratives already seen, using the same situations and terminology in novel ways.201MEASURERECALLPRECISIONGENERALITY/POTENTIALUTILITYPROGRESSMEANS OF CALCULATION1) Percent emplates for test set filled that shouldbe filled (a template is to be filled in only if themessage r ports apast DETECT, TRACK,TARGET, HARASS or A'YI'ACK event)2) Percent slots for test set filled that should be filledOf those slots correctly recalled for test set, percentcorrectly filledComparison of recall and precision measures fordevelopment vstest setRecall and precision measurements using originalnarratives as input (vs measurements obtained formodified narrative inputs)Results of run using first test set compared to resultsobtained later using second test setIn/SIGHTS GAINED1) Coverage of English2) Depth of understanding3) RobustnessSame as for RECALL1) Generality of software2) Potential for handling novel textAbility to handle telegraphic,ill-formed text and sublanguageconstructionsPace at which state of the art isadvancingTable 2.
Evaluation Criteria and SignificanceSUMMARYThis evaluation project represents a small step in the direction of benchmarking NLP systems.
The results of theevaluation are not expected to be statistically significant but will begin to bring quantitative criteria to bear.
Theywill also provide common ground for discussion of the language processing issues raised by military texts and thetask-related issues of extracting, deriving and inferring needed information for a database.
Developers of a number ofdifferent NLP systems that have processed military message texts are being invited to participate in this evaluationeffort and will meet afterward to discuss the results and critique the test design.
Although the test is designed to treatthe NLP systems as black boxes, it is expected to yield a variety of meaningful measurements.
It is also expectedthat discussion of the test results at the end of the evaluation will provide a great deal of insight not only into whatsystems can do but also how they do it.REFERENCESDepartment of the Navy.
Special Incident Reporting.
OPNAVINST 3100.6D.Montgomery, C., and Glover, B.
(1986).
A Sublanguage for Reporting and Analysis of Space Events.
In R.Grishman and R. Kittredge (Eds.
), Analyzing Language in Restricted Domains: Sublanguage Description andProcessing (pp.
129-161).
Hillsdale, NJ: Erlbaum.Sundheim, B.
(1989).
Navy Tactical Incident Reporting in a Highly Constrained Sublanguage: Examples andAnalysis.
Naval Ocean Systems Center Technical Document 1477 (in press).202
