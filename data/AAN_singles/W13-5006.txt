Proceedings of the TextGraphs-8 Workshop, pages 39?43,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational LinguisticsFrom Global to Local Similarities:A Graph-Based Contextualization Method using Distributional ThesauriChris Biemann and Martin RiedlComputer Science Department, Technische Universita?t DarmstadtHochschulstrasse 10, D-64289 Darmstadt, Germany{riedl,biem}@cs.tu-darmstadt.deAbstractAfter recasting the computation of a distribu-tional thesaurus in a graph-based frameworkfor term similarity, we introduce a new con-textualization method that generates, for eachterm occurrence in a text, a ranked list of termsthat are semantically similar and compatiblewith the given context.
The framework is in-stantiated by the definition of term and con-text, which we derive from dependency parsesin this work.
Evaluating our approach on astandard data set for lexical substitution, weshow substantial improvements over a strongnon-contextualized baseline across all parts ofspeech.
In contrast to comparable approaches,our framework defines an unsupervised gener-ative method for similarity in context and doesnot rely on the existence of lexical resources asa source for candidate expansions.1 IntroductionFollowing (de Saussure, 1916) we consider two dis-tinct viewpoints: syntagmatic relations consider theassignment of values to a linear sequence of terms,and the associative (also: paradigmatic) viewpointassigns values according to the commonalities anddifferences to other terms in the reader?s memory.Based on these notions, we automatically expandterms in the linear sequence with their paradigmati-cally related terms.Using the distributional hypothesis (Harris,1951), and operationalizing similarity of terms(Miller and Charles, 1991), it became possible tocompute term similarities for a large vocabulary(Ruge, 1992).
Lin (1998) computed a distributionalthesaurus (DT) by comparing context features de-fined over grammatical dependencies with an ap-propriate similarity measure for all reasonably fre-quent words in a large collection of text, and evalu-ated these automatically computed word similaritiesagainst lexical resources.
Entries in the DT consistof a ranked list of the globally most similar terms fora target term.
While the similarities are dependenton the instantiation of the context feature as well ason the underlying text collection, they are global inthe sense that the DT aggregates over all occurrencesof target and its similar elements.
In our work, wewill use a DT in a graph representation and movefrom a global notion of similarity to a contextual-ized version, which performs context-dependent textexpansion for all word nodes in the DT graph.2 Related WorkThe need to model semantics just in the same wayas local syntax is covered by the n-gram-model, i.e.trained from a background corpus sparked a largebody of research on semantic modeling.
This in-cludes computational models for topicality (Deer-wester et al 1990; Hofmann, 1999; Blei et al2003), and language models that incorporate topical(as well as syntactic) information, see e.g.
(Boyd-Graber and Blei, 2008; Tan et al 2012).
In theComputational Linguistics community, the vectorspace model (Schu?tze, 1993; Turney and Pantel,2010; Baroni and Lenci, 2010; Pucci et al 2009;de Cruys et al 2013) is the prevalent metaphor forrepresenting word meaning.While the computation of semantic similarities onthe basis of a background corpus produces a globalmodel, which e.g.
contains semantically similarwords for different word senses, there are a num-ber of works that aim at contextualizing the infor-mation held in the global model for particular oc-currences.
With his predication algorithm, Kintsch(2001) contextualizes LSA (Deerwester et al 1990)for N-VP constructions by spreading activation overneighbourhood graphs in the latent space.In particular, the question of operationalizing se-mantic compositionality in vector spaces (Mitchell39and Lapata, 2008) received much attention.
The lex-ical substitution task (McCarthy and Navigli, 2009)(LexSub) sparked several approaches for contextual-ization.
While LexSub participants and subsequentworks all relied on a list of possible substitutionsas given by one or several lexical resources, we de-scribe a graph-based system that is knowledge-freeand unsupervised in the sense that it neither requiresan existing resource (we compute a DT graph forthat), nor needs training for contextualization.3 Method3.1 Holing SystemFor reasons of generality, we introduce the holingoperation (cf.
(Biemann and Riedl, 2013)), to splitany sort of observations on the syntagmatic level(e.g.
dependency relations) into pairs of term andcontext features.
These pairs are then both used forthe computation of the global DT graph similarityand for the contextualization.
This holing systemis the only part of the system that is dependent ona pre-processing step; subsequent steps operate ona unified representation.
The representation is givenby a list of pairs <t,c> where t is the term (at a cer-tain offset) and c is the context feature.
The positionof t in c is denoted by a hole symbol ?@?.
As an ex-ample, the dependency triple (nsub;gave2;I1)could be transferred to <gave2,(nsub;@;I1)>and <I1,(nsub;gave2;@)>.3.2 Distributional SimilarityHere, we present the computation of the distribu-tional similarity between terms using three graphs.For the computation we use the Apache HadoopFramework, based on (Dean and Ghemawat, 2004).We can describe this operation using a bipartite?term?-?context feature?
graph TC(T,C,E) withT the set terms, C the set of context features ande(t, c, f) ?
E edges between t ?
T , c ?
Cwith f = count(t, c) frequency of co-occurrence.Additionally, we define count(t) and count(c) asthe counts of the term, respectively as the countof the context feature.
Based on the graph TCwe can produce a first-order graph FO(T,C,E),with e(t, c, sig) ?
E. First, we calculate a signif-icance score sig for each pair (t, c) using Lexicog-rapher?s Mutual Information (LMI): score(t, c) =LMI(t, c, ) = count(t, c) log2(count(t,c)count(t)count(c))(Evert, 2004).
Then, we remove all edges withscore(t, c) < 0 and keep only the p most signif-icant pairs per term t and remove the remainingedges.
Additionally, we remove features which co-occur with more then 1000 words, as these featuresdo not contribute enough to similarity to justify theincrease of computation time (cf.
(Rychly?
and Kil-garriff, 2007; Goyal et al 2010)).
The second-order similarity graph between terms is defined asSO(T,E) for t1, t2 ?
T with the similarity scores = |{c|e(t1, c) ?
FO, e(t2, c) ?
FO}|, which isthe number of salient features two terms share.
SOdefines a distributional thesaurus.In contrast to (Lin, 1998) we do not count how of-ten a feature occurs with a term (we use significanceranking instead), and do not use cosine or other sim-ilarities (Lee, 1999) to calculate the similarity overthe feature counts of each term, but only count sig-nificant common features per term.
This constraintmakes this approach more scalable to larger data, aswe do not need to know the full list of features fora term pair at any time.
Seemingly simplistic, weshow in (Biemann and Riedl, 2013) that this mea-sure outperforms other measures on large corpora ina semantic relatedness evaluation.3.3 Contextual SimilarityThe contextualization is framed as a ranking prob-lem: given a set of candidate expansions as pro-vided by the SO graph, we aim at ranking them suchthat the most similar term in context will be rankedhigher, whereas non-compatible candidates shouldbe ranked lower.First, we run the holing system on the lexicalmaterial containing our target word tw ?
T ?
?T and select all pairs <tw,ci> ci ?
C ?
?
Cthat are instantiated in the current context.
Wethen define a new graph CON(T ?, C ?, S) with con-text features ci ?
C ?.
Using the second-ordersimilarity graph SO(T,E) we extract the top nsimilar terms T ?={ti, .
.
.
, tn}?T from the second-order graph SO for tw and add them to the graphCON .
We add edges e(t, c, sig) between all tar-get words and context features and label the edgewith the significance score from the first order graphFO.
Edges e(t, c, sig), not contained in FO, geta significance score of zero.
We can then calcu-40late a ranking score for each ti with the harmonicmean, using a plus one smoothing: rank(ti) =?cj(sig(ti,cj)+1)/count(term(cj))?cj(sig(ti,cj)+1)/count(term(cj))(term(cj) extractsthe term out of the context notation).
Using thatranking score we can re-order the entries t1, .
.
.
, tnaccording to their ranking score.In Figure 1, we exemplify this, using the tar-get word tw= ?cold?
in the sentence ?I caughta nasty cold.?.
Our dependency parse-basedFigure 1: Contextualized ranking for target ?cold?
in thesentence ?I caught a nasty cold?
for the 10 most similarterms from the DT.holing system produced the following pairs for?cold?
: <cold5 ,(amod;@;nasty4)>,<cold5,(dobj;caught2;@)>.
The top 10candidates for ?cold?
are T ?={heat, weather, tem-perature, rain, flue, wind, chill, disease}.
The scoresper pair are e.g.
<heat, (dobj;caught;@)>with an LMI score of 42.0, <weather,(amod;@;nasty)> with a score of 139.4.The pair <weather, (dobj;caught;@)>was not contained in our first-order data.
Rankingthe candidates by their overall scores as given in thefigure, the top three contextualized expansions are?disease, flu, heat?, which are compatible with bothpairs.
For the top 200 words, the ranking of fullycompatible candidates is: ?virus, disease, infection,flu, problem, cough, heat, water?, which is clearlypreferring the disease-related sense of ?cold?
overthe temperature-related sense.In this way, each candidate t?
gets as manyscores as there are pairs containing c?
in the holingsystem output.
An overall score per t?
then given bythe harmonic mean of the add-one-smoothed singlescores ?
smoothing is necessary to rank candidatest?
that are not compatible to all pairs.
This schemecan easily be extended to expand all words in a givensentence or paragraph, yielding a two-dimensionalcontextualized text, where every (content) word isexpanded by a list of globally similar words from thedistributional thesaurus that are ranked according totheir compatibility with the given context.4 EvaluationThe evaluation of contextualizing the thesaurus (CT)was performed using the LexSub dataset, introducedin the Lexical Substitution task at Semeval 2007(McCarthy and Navigli, 2009).
Following the setupprovided by the task organizers, we tuned our ap-proach on the 300 trial sentences, and evaluate iton the official remaining 1710 test sentences.
Forthe evaluation we used the out of ten (oot) preci-sion and oot mode precision.
Both measures cal-culate the number of detected substitutions withinten guesses over the complete subset.
Whereas en-tries in the oot precision measures are consideredcorrect if they match the gold standard, without pe-nalizing non-matching entries, the oot mode preci-sion includes also a weighting as given in the goldstandard1.
For comparison, we use the results of theDT as a baseline to evaluate the contextualization.The DT was computed based on newspaper corpora(120 million sentences), taken from the Leipzig Cor-pora Collection (Richter et al 2006) and the Giga-word corpus (Parker et al 2011).
Our holing systemuses collapsed Stanford parser dependencies (Marn-effe et al 2006) as context features.
The contextual-ization uses only context features that contain wordswith part-of-speech prefixes V,N,J,R.
Furthermore,we use a threshold for the significance value of theLMI values of 50.0, p=1000, and the most similar 30terms from the DT entries.5 ResultsSince out contextualization algorithm is dependenton the number of context features containing the tar-get word, we report scores for targets with at leasttwo and at least three dependencies separately.
Inthe Lexical Substitution Task 2007 dataset (LexSub)test data we detected 8 instances without entries inthe gold standard and 19 target words without any1The oot setting was chosen because it matches the expan-sions task better than e.g.
precision@141dependency, as they are collapsed into the depen-dency relation.
The remaining entries have at leastone, 49.2% have at least two and 26.0% have at leastthree dependencies.
Furthermore, we also evalu-ated the results broken down into separate part-of-speeches of the target.
The results on the LexSubtest set are shown in Table 1.Precision Mode Precisionmin.
# dep.
1 2 3 1 2 3POS Alg.noun CT 26.64 26.55 28.36 38.68 38.24 37.68noun DT 25.35 25.09 28.07 34.96 34.31 36.23verb CT 23.39 23.75 23.05 32.05 33.09 33.33verb DT 22.46 22.13 21.32 29.17 28.78 28.25adj.
CT 32.65 34.75 36.08 45.09 48.24 46.43adj.
DT 32.13 33.25 35.02 43.56 43.53 42.86adv.
CT 20.47 29.46 36.23 30.14 40.63 100.00adv.
DT 28.91 26.75 29.88 41.63 34.38 66.67ALL CT 26.46 26.43 26.61 37.21 37.40 37.38ALL DT 27.06 24.83 25.24 36.96 33.06 33.11Table 1: Results of the LexSub test dataset.Inspecting the results for all POS (denoted asALL), we only observe a slight decline for the preci-sion score with at least only one dependency, whichis caused by adverbs.
For targets with more thanone dependency, we observe overall improvementsof 1.6 points in precision and more than 4 points inmode precision.Regarding the results of different part-of-speechtags, we always improve over the DT ranking, ex-cept for adverbs with only one dependency.
Mostnotably, the largest relative improvements are ob-served on verbs, which is a notoriously difficultword class in computational semantics.
For adverbs,at least two dependencies seem to be needed; thereare only 7 adverb occurrences with more than twodependencies in the dataset.
Regarding performanceon the original lexical substitution task (McCarthyand Navigli, 2009), we did not come close to the per-formance of the participating systems, which rangebetween 32?50 precision points, respectively 43?66mode precision points (only taking systems with-out duplicate words in the result set into account).However, all participants used one or several lexicalresources for generating substitution candidates, aswell as a large number of features.
Our system, onthe other hand, merely requires a holing system ?
inthis case based on a dependency parser ?
and a largeamount of unlabeled text, and a very small numberof contextual clues.For an insight of the coverage for the entries deliv-ered by the DT graph, we extended the oot precisionmeasure, to consider not only the first 10 entries, butthe first X={1,10,50,100,200} entries (see Figure 2).Here we also show the coverage for different sizedFigure 2: Coverage on the LexSub test dataset for differ-ent DT graphs, using out of X entries.datasets (10 and 120 million sentences).
Amongstthe 200 most similar words from the DT, a cover-age of up to 55.89 is reached.
DT quality improveswith corpus size, especially due to increased cover-age.
This shows that there is considerable headroomfor optimization for our contextualization method,but also shows that our automatic candidate expan-sions can provide a coverage that is competitive tolexical resources.6 ConclusionWe have provided a way of operationalizing seman-tic similarity by splitting syntagmatic observationsinto terms and context features, and representingthem a first-order and second-order graph.
Then,we introduced a conceptually simple and efficientmethod to perform a contextualization of semanticsimilarity.
Overall, our approach constitutes an un-supervised generative model for lexical expansionin context.
We have presented a generic methodon contextualizing distributional information, whichretrieves the lexical expansions from a target termfrom the DT graph, and ranks them with respect totheir context compatibility.
Evaluating our methodon the LexSub task, we were able to show improve-ments, especially for expansion targets with manyinforming contextual elements.
For further work,we will extend our holing system and combine sev-eral holing systems, such as e.g.
n-gram contexts.42Additionally, we would like to adapt more advancedmethods for the contextualization (Viterbi, 1967;Lafferty et al 2001) that yield an all-words simulta-neous expansion over the whole sequence, and con-stitutes a probabilistic model of lexical expansion.ReferencesM.
Baroni and A. Lenci.
2010.
Distributional mem-ory: A general framework for corpus-based semantics.Computational Linguistics, 36(4):673?721.Chris Biemann and Martin Riedl.
2013.
Text: Now in2D!
a framework for lexical expansion with contextualsimilarity.
Journal of Language Modelling, 1(1):55?95.D.
M. Blei, A. Y. Ng, and M. I. Jordan.
2003.
Latentdirichlet alcation.
J. Mach.
Learn.
Res., 3:993?1022.J.
Boyd-Graber and D. M. Blei.
2008.
Syntactic topicmodels.
In Neural Information Processing Systems,Vancouver, BC, USA.T.
Van de Cruys, T. Poibeau, and A. Korhonen.
2013.
Atensor-based factorization model of semantic compo-sitionality.
In Proc.
NAACL-HLT 2013, Atlanta, USA.F.
de Saussure.
1916.
Cours de linguistique ge?ne?rale.Payot, Paris, France.J.
Dean and S. Ghemawat.
2004.
MapReduce: Simpli-fied Data Processing on Large Clusters.
In Proc.
ofOperating Systems, Design & Implementation, pages137?150, San Francisco, CA, USA.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-dauer, and R. Harshman.
1990.
Indexing by latent se-mantic analysis.
Journal of the American Society forInformation Science, 41(6):391?407.S.
Evert.
2004.
The statistics of word cooccurrences:word pairs and collocations.
Ph.D. thesis, IMS, Uni-versita?t Stuttgart.A.
Goyal, J. Jagarlamudi, H.
Daume?, III, and T. Venkata-subramanian.
2010.
Sketch techniques for scaling dis-tributional similarity to the web.
In Proc.
of the 2010Workshop on GEometrical Models of Nat.
Lang.
Se-mantics, pages 51?56, Uppsala, Sweden.Z.
S. Harris.
1951.
Methods in Structural Linguistics.University of Chicago Press, Chicago, USA.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proc.
22nd ACM SIGIR, pages 50?57,New York, NY, USA.W.
Kintsch.
2001.
Predication.
Cognitive Science,25(2):173?202.J.
D. Lafferty, A. McCallum, and F. C. N. Pereira.
2001.Conditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In Proc.
ofthe 18th Int.
Conf.
on Machine Learning, ICML ?01,pages 282?289, San Francisco, CA, USA.L.
Lee.
1999.
Measures of distributional similarity.
InProc.
of the 37th ACL, pages 25?32, College Park,MD, USA.D.
Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proc.
COLING?98, pages 768?774,Montreal, Quebec, Canada.M.-C. De Marneffe, B. Maccartney, and C. D. Man-ning.
2006.
Generating typed dependency parses fromphrase structure parses.
In Proc.
of the Int.
Conf.
onLanguage Resources and Evaluation, Genova, Italy.D.
McCarthy and R. Navigli.
2009.
The english lexicalsubstitution task.
Language Resources and Evalua-tion, 43(2):139?159.G.
A. Miller and W. G. Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and CognitiveProcesses, 6(1):1?28.J.
Mitchell and M. Lapata.
2008.
Vector-based modelsof semantic composition.
In Proceedings of ACL-08:HLT, pages 236?244, Columbus, OH, USA.R.
Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.2011.
English Gigaword Fifth Edition.
LinguisticData Consortium, Philadelphia, USA.D.
Pucci, M. Baroni, F. Cutugno, and R. Lenci.
2009.Unsupervised lexical substitution with a word spacemodel.
In Workshop Proc.
of the 11th Conf.
of theItalian Association for Artificial Intelligence, ReggioEmilia, Italy.M.
Richter, U. Quasthoff, E. Hallsteinsdo?ttir, and C. Bie-mann.
2006.
Exploiting the leipzig corpora collection.In Proceesings of the IS-LTC 2006, Ljubljana, Slove-nia.G.
Ruge.
1992.
Experiments on linguistically-basedterm associations.
Information Processing & Manage-ment, 28(3):317 ?
332.P.
Rychly?
and A. Kilgarriff.
2007.
An efficient algo-rithm for building a distributional thesaurus (and othersketch engine developments).
In Proc.
45th ACL,pages 41?44, Prague, Czech Republic.Hinrich Schu?tze.
1993.
Word space.
In Advances inNeural Information Processing Systems 5, pages 895?902.
Morgan Kaufmann.Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun Wang.2012.
A scalable distributed syntactic, semantic, andlexical language model.
Computational Linguistics,38(3):631?671.P.
D. Turney and P. Pantel.
2010.
From frequency tomeaning: vector space models of semantics.
J. Artif.Int.
Res., 37(1):141?188.A.
J. Viterbi.
1967.
Error bounds for convolutional codesand an asymptotically optimum decoding algorithm.IEEE Transactions on Information Theory, 13(2):260?269.43
