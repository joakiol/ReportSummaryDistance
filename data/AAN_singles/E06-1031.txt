CDER: Efficient MT Evaluation Using Block MovementsGregor Leusch and Nicola Ueffing and Hermann NeyLehrstuhl fu?r Informatik VI, Computer Science DepartmentRWTH Aachen UniversityD-52056 Aachen, Germany{leusch,ueffing,ney}@i6.informatik.rwth-aachen.deAbstractMost state-of-the-art evaluation measuresfor machine translation assign high coststo movements of word blocks.
In manycases though such movements still resultin correct or almost correct sentences.
Inthis paper, we will present a new eval-uation measure which explicitly modelsblock reordering as an edit operation.Our measure can be exactly calculated inquadratic time.Furthermore, we will show how someevaluation measures can be improvedby the introduction of word-dependentsubstitution costs.
The correlation of thenew measure with human judgment hasbeen investigated systematically on twodifferent language pairs.
The experimentalresults will show that it significantlyoutperforms state-of-the-art approaches insentence-level correlation.
Results fromexperiments with word dependent substi-tution costs will demonstrate an additionalincrease of correlation between automaticevaluation measures and human judgment.1 IntroductionResearch in machine translation (MT) dependsheavily on the evaluation of its results.
Espe-cially for the development of an MT system,an evaluation measure is needed which reliablyassesses the quality of MT output.
Such a measurewill help analyze the strengths and weaknesses ofdifferent translation systems or different versionsof the same system by comparing output atthe sentence level.
In most applications ofMT, understandability for humans in terms ofreadability as well as semantical correctnessshould be the evaluation criterion.
But as humanevaluation is tedious and cost-intensive, automaticevaluation measures are used in most MT researchtasks.
A high correlation between these automaticevaluation measures and human evaluation is thusdesirable.State-of-the-art measures such as BLEU (Pap-ineni et al, 2002) or NIST (Doddington, 2002)aim at measuring the translation quality ratheron the document level1 than on the level ofsingle sentences.
They are thus not well-suitedfor sentence-level evaluation.
The introductionof smoothing (Lin and Och, 2004) solves thisproblem only partially.In this paper, we will present a new automaticerror measure for MT ?
the CDER ?
which isdesigned for assessing MT quality on the sentencelevel.
It is based on edit distance ?
such as thewell-known word error rate (WER) ?
but allowsfor reordering of blocks.
Nevertheless, by definingreordering costs, the ordering of the words ina sentence is still relevant for the measure.
Inthis, the new measure differs significantly fromthe position independent error rate (PER) by(Tillmann et al, 1997).
Generally, finding anoptimal solution for such a reordering problem isNP hard, as is shown in (Lopresti and Tomkins,1997).
In previous work, researchers have tried toreduce the complexity, for example by restrictingthe possible permutations on the block-level, or byapproximation or heuristics during the calculation.Nevertheless, most of the resulting algorithms stillhave high run times and are hardly applied inpractice, or give only a rough approximation.
Anoverview of some better-known measures can befound in Section 3.1.
In contrast to this, our newmeasure can be calculated very efficiently.
Thisis achieved by requiring complete and disjointcoverage of the blocks only for the referencesentence, and not for the candidate translation.
Wewill present an algorithm which computes the newerror measure in quadratic time.The new evaluation measure will be investi-gated and compared to state-of-the-art methodson two translation tasks.
The correlation withhuman assessment will be measured for severaldifferent statistical MT systems.
We will seethat the new measure significantly outperforms theexisting approaches.1The n-gram precisions are measured at the sentence leveland then combined into a score over the whole document.241As a further improvement, we will introduceword dependent substitution costs.
This methodwill be applicable to the new measure as wellas to established measures like WER and PER.Starting from the observation that the substitutionof a word with a similar one is likely to affecttranslation quality less than the substitution witha completely different word, we will show howthe similarity of words can be accounted for inautomatic evaluation measures.This paper is organized as follows: In Section 2,we will present the state of the art in MTevaluation and discuss the problem of blockreordering.
Section 3 will introduce the newerror measure CDER and will show how it canbe calculated efficiently.
The concept of word-dependent substitution costs will be explained inSection 4.
In Section 5, experimental results onthe correlation of human judgment with the CDERand other well-known evaluation measures will bepresented.
Section 6 will conclude the paper andgive an outlook on possible future work.2 MT Evaluation2.1 Block Reordering and State of the ArtIn MT ?
as opposed to other natural languageprocessing tasks like speech recognition ?
thereis usually more than one correct outcome of atask.
In many cases, alternative translations ofa sentence differ from each other mostly by theordering of blocks of words.
Consequently, anevaluation measure for MT should be able todetect and allow for block reordering.
Neverthe-less, a higher ?amount?
of reordering between acandidate translation and a reference translationshould still be reflected in a worse evaluationscore.
In other words, the more blocks there areto be reordered between reference and candidatesentence, the higher we want the measure toevaluate the distance between these sentences.State-of-the-art evaluation measures for MTpenalize movement of blocks rather severely: n-gram based scores such as BLEU or NIST stillyield a high unigram precision if blocks arereordered.
For higher-order n-grams, though, theprecision drops.
As a consequence, this affects theoverall score significantly.
WER, which is basedon Levenshtein distance, penalizes the reorderingof blocks even more heavily.
It measures thedistance by substitution, deletion and insertionoperations for each word in a relocated block.PER, on the other hand, ignores the orderingof the words in the sentences completely.
Thisoften leads to an overly optimistic assessment oftranslation quality.2.2 Long JumpsThe approach we pursue in this paper is toextend the Levenshtein distance by an additionaloperation, namely block movement.
The numberof blocks in a sentence is equal to the numberof gaps among the blocks plus one.
Thus, theblock movements can equivalently be expressedas long jump operations that jump over thegaps between two blocks.
The costs of along jump are constant.
The blocks are readin the order of one of the sentences.
Theselong jumps are combined with the ?classical?Levenshtein edit operations, namely insertion,deletion, substitution, and the zero-cost operationidentity.
The resulting long jump distance dLJgives the minimum number of operations whichare necessary to transform the candidate sentenceinto the reference sentence.
Like the Levenshteindistance, the long jump distance can be depictedusing an alignment grid as shown in Figure 1:Here, each grid point corresponds to a pair ofinter-word positions in candidate and referencesentence, respectively.
dLJ is the minimum cost ofa path between the lower left (first) and the upperright (last) alignment grid point which covers allreference and candidate words.
Deletions andinsertions correspond to horizontal and verticaledges, respectively.
Substitutions and identityoperations correspond to diagonal edges.
Edgesbetween arbitrary grid points from the same rowcorrespond to long jump operations.
It is easy tosee that dLJ is symmetrical.In the example, the best path contains one dele-tion edge, one substitution edge, and three longjump edges.
Therefore, the long jump distancebetween the sentences is five.
In contrast, thebest Levenshtein path contains one deletion edge,four identity and five consecutive substitutionedges; the Levenshtein distance between the twosentences is six.
The effect of reordering on theBLEU measure is even higher in this example:Whereas 8 of the 10 unigrams from the candidatesentence can be found in the reference sentence,this holds for only 4 bigrams, and 1 trigram.
Not asingle one of the 7 candidate four-grams occurs inthe reference sentence.3 CDER: A New Evaluation Measure3.1 Approach(Lopresti and Tomkins, 1997) showed that findingan optimal path in a long jump alignment grid isan NP-hard problem.
Our experiments showedthat the calculation of exact long jump distancesbecomes impractical for sentences longer than 20words.242wemetattheairportatseveno?clock.we metat seveno?clockon theairport.havecandidatereferencedeletioninsertionsubstitutionidentity best pathstart/end nodelong jumpblockFigure 1: Example of a long jump alignmentgrid.
All possible deletion, insertion, identity andsubstitution operations are depicted.
Only longjump edges from the best path are drawn.A possible way to achieve polynomial run-time is to restrict the number of admissible blockpermutations.
This has been implemented by(Leusch et al, 2003) in the inversion word errorrate.
Alternatively, a heuristic or approximativedistance can be calculated, as in GTM by (Turian etal., 2003).
An implementation of both approachesat the same time can be found in TER by (Snoveret al, 2005).
In this paper, we will present anotherapproach which has a suitable run-time, whilestill maintaining completeness of the calculatedmeasure.
The idea of the proposed method is todrop some restrictions on the alignment path.The long jump distance as well as the Lev-enshtein distance require both reference andcandidate translation to be covered completelyand disjointly.
When extending the metric byblock movements, we drop this constraint for thecandidate translation.
That is, only the wordsin the reference sentence have to be coveredexactly once, whereas those in the candidatesentence can be covered zero, one, or multipletimes.
Dropping the constraints makes an efficientcomputation of the distance possible.
We dropthe constraints for the candidate sentence and notfor the reference sentence because we do not wantany information contained in the reference to beomitted.
Moreover, the reference translation willnot contain unnecessary repetitions of blocks.The new measure ?
which will be calledCDER in the following ?
can thus be seen as ameasure oriented towards recall, while measureslike BLEU are guided by precision.
The CDERis based on the CDCD distance2 introducedin (Lopresti and Tomkins, 1997).
The authorsshow there that the problem of finding the optimalsolution can be solved in O(I2 ?
L) time, whereI is the length of the candidate sentence and Lthe length of the reference sentence.
Within thispaper, we will refer to this distance as dCD .
Inthe next subsection, we will show how it can becomputed in O(I ?L) time using a modification ofthe Levenshtein algorithm.We also studied the reverse direction of thedescribed measure; that is, we dropped thecoverage constraints for the reference sentenceinstead of the candidate sentence.
Addition-ally, the maximum of both directions has beenconsidered as distance measure.
The results inSection 5.2 will show that the measure using theoriginally proposed direction has a significantlyhigher correlation with human evaluation than theother directions.3.2 AlgorithmOur algorithm for calculating dCD is basedon the dynamic programming algorithm for theLevenshtein distance (Levenshtein, 1966).
TheLevenshtein distance dLev(eI1, e?L1)between twostrings eI1 and e?L1 can be calculated in con-stant time if the Levenshtein distances of thesubstrings, dLev(eI?11 , e?L1), dLev(eI1, e?L?11), anddLev(eI?11 , e?L?11), are known.Consequently, an auxiliary quantityDLev(i, l) := dLev(ei1, e?l1)is stored in an I ?L table.
This auxiliary quantitycan then be calculated recursively from DLev(i ?1, l), DLev(i, l ?
1), and DLev(i ?
1, l ?
1).Consequently, the Levenshtein distance can becalculated in time O(I ?
L).This algorithm can easily be extended for thecalculation of dCD as follows: Again we definean auxiliary quantity D(i, l) asD(i, l) := dCD(ei1, e?l1)Insertions, deletions, and substitutions arehandled the same way as in the Levenshteinalgorithm.
Now assume that an optimal dCD pathhas been found: Then, each long jump edge within2C stands for cover and D for disjoint.
We adopted thisnotion for our measures.243...ildeletion insertion subst/id long jumpl-1i-1Figure 2: Predecessors of a grid point (i, l) inEquation 1this path will always start at a node with the lowestD value in its row3.Consequently, we use the following modifica-tion of the Levenshtein recursion:D(0, 0) = 0D(i, l) = min??????
?D(i?1, l?1) + (1??
(ei, e?l)) ,D(i?
1, l) + 1,D(i, l ?
1) + 1,mini?D(i?, l) + 1???????
(1)where ?
is the Kronecker delta.
Figure 2 shows thepossible predecessors of a grid point.The calculation of D(i, l) requires all values ofD(i?, l) to be known, even for i?
> i.
Thus, thecalculation takes three steps for each l:1.
For each i, calculate the minimum of the firstthree terms.2.
Calculate mini?D(i?, l).3.
For each i, calculate the minimum accordingto Equation 1.Each of these steps can be done in time O(I).Therefore, this algorithm calculates dCD in timeO(I ?
L) and space O(I).3.3 Hypothesis Length and PenaltiesAs the CDER does not penalize candidate trans-lations which are too long, we studied the useof a length penalty or miscoverage penalty.
Thisdetermines the difference in sentence lengthsbetween candidate and reference.
Two definitionsof such a penalty have been studied for this work.3Proof: Assume that the long jump edge goes from (i?, l)to (i, l), and that there exists an i??
such that D(i?
?, l) <D(i?, l).
This means that the path from (0, 0) to (i?
?, l) isless expensive than the path from (0, 0) to (i?, l).
Thus, thepath from (0, 0) through (i?
?, l) to (i, l) is less expensive thanthe path through (i?, l).
This contradicts the assumption.Length DifferenceThere is always an optimal dCD alignment paththat does not contain any deletion edges, becauseeach deletion can be replaced by a long jump, atthe same costs.
This is different for a dLJ path,because here each candidate word must be coveredexactly once.
Assume now that the candidatesentence consists of I words and the referencesentence consists of L words, with I > L.Then, at most L candidate words can be coveredby substitution or identity edges.
Therefore, theremaining candidate words (at least I ?
L) mustbe covered by deletion edges.
This means that atleast I?L deletion edges will be found in any dLJpath, which leads to dLJ ?
dCD ?
I ?
L in thiscase.Consequently, the length difference betweenthe two sentences gives us a useful miscoveragepenalty lplen:lplen := max(I ?
L, 0)This penalty is independent of the dCD alignmentpath.
Thus, an optimal dCD alignment pathis optimal for dCD + lplen as well.
Thereforethe search algorithm in Section 3.2 will find theoptimum for this sum.Absolute MiscoverageLet coverage(i) be the number of substitution,identity, and deletion edges that cover a candidateword ei in a dCD path.
If we had a complete anddisjoint alignment for the candidate word (i.e., adLJ path), coverage(i) would be 1 for each i.In general this is not the case.
We can use theabsolute miscoverage as a penalty lpmisc for dCD:lpmisc :=?i|1 ?
coverage(i)|This miscoverage penalty is not independent ofthe alignment path.
Consequently, the proposedsearch algorithm will not necessarily find anoptimal solution for the sum of dCD and lpmisc.The idea behind the absolute miscoverage isthat one can construct a valid ?
but not necessarilyoptimal ?
dLJ path from a given dCD path.
Thisprocedure is illustrated in Figure 3 and takes placein two steps:1.
For each block of over-covered candidatewords, replace the aligned substitution and/oridentity edges by insertion edges; move thelong jump at the beginning of the blockaccordingly.2.
For each block of under-covered candidatewords, add the corresponding number of244candidatereference2 2 0coveragecandidate1 1 1 1 1 1 1 1 1 1 1dCD dLJdeletion insertion subst/id long jumpFigure 3: Transformation of a dCD path into a dLJpath.deletion edges; move the long jump at thebeginning of the block accordingly.This also shows that there cannot be4 apolynomial time algorithm that calculates theminimum of dCD + lpmisc for arbitrary pairs ofsentences, because this minimum is equal to dLJ.With these miscoverage penalties, inexpensivelower and upper bounds for dLJ can be calculated,because the following inequality holds:(2) dCD + lplen ?
dLJ ?
dCD + lpmisc4 Word-dependent Substitution Costs4.1 IdeaAll automatic error measures which are basedon the edit distance (i.e.
WER, PER, andCDER) apply fixed costs for the substitutionof words.
However, this is counter-intuitive,as replacing a word with another one whichhas a similar meaning will rarely change themeaning of a sentence significantly.
On the otherhand, replacing the same word with a completelydifferent one probably will.
Therefore, it seemsadvisable to make substitution costs dependent onthe semantical and/or syntactical dissimilarity ofthe words.To avoid awkward case distinctions, we assumethat a substitution cost function cSUB for twowords e, e?
meets the following requirements:1. cSUB depends only on e and e?.2.
cSUB is a metric; especially(a) The costs are zero if e = e?, and largerthan zero otherwise.
(b) The triangular inequation holds: it isalways cheaper to replace e by e?
than toreplace e by e?
and then e?
by e?.4provided that P 6= NP , of course.3.
The costs of substituting a word e by e?
arealways equal or lower than those of deletinge and then inserting e?.
In short, cSUB ?
2.Under these conditions the algorithms forWER and CDER can easily be modified to useword-dependent substitution costs.
For example,the only necessary modification in the CDERalgorithm in Equation 1 is to replace 1 ?
?
(e, e?
)by cSUB(e, e?
).For the PER, it is no longer possible to use alinear time algorithm in the general case.
Instead,a modification of the Hungarian algorithm (Knuth,1993) can be used.The question is now how to define the word-dependent substitution costs.
We have studied twodifferent approaches.4.2 Character-based Levenshtein DistanceA pragmatic approach is to compare the spellingof the words to be substituted with each other.The more similar the spelling is, the more similarwe consider the words to be, and the lower wewant the substitution costs between them.
InEnglish, this works well with similar tenses ofthe same verb, or with genitives or plurals of thesame noun.
Nevertheless, a similar spelling is noguarantee for a similar meaning, because prefixessuch as ?mis-?, ?in-?, or ?un-?
can change themeaning of a word significantly.An obvious way of comparing the spelling is theLevenshtein distance.
Here, words are comparedon character level.
To normalize this distanceinto a range from 0 (for identical words) to 1(for completely different words), we divide theabsolute distance by the length of the Levenshteinalignment path.4.3 Common Prefix LengthAnother character-based substitution cost functionwe studied is based on the common prefix lengthof both words.
In English, different tenses ofthe same verb share the same prefix; which isusually the stem.
The same holds for differentcases, numbers and genders of most nouns andadjectives.
However, it does not hold if verbprefixes are changed or removed.
On the otherhand, the common prefix length is sensitive tocritical prefixes such as ?mis-?
for the samereason.
Consequently, the common prefix length,normalized by the average length of both words,gives a reasonable measure for the similarity oftwo words.
To transform the normalized commonprefix length into costs, this fraction is thensubtracted from 1.Table 1 gives an example of these two word-dependent substitution costs.245Table 1: Example of word-dependent substitution costs.Levenshtein prefixe e?
distance substitution cost similarity substitution costusual unusual 2 27 = 0.29 1 1 ?
16 = 0.83understanding misunderstanding 3 316 = 0.19 0 1.00talk talks 1 15 = 0.20 4 1 ?
44.5 = 0.114.4 PerspectivesMore sophisticated methods could be consideredfor word-dependent substitution costs as well.Examples of such methods are the introduction ofinformation weights as in the NIST measure or thecomparison of stems or synonyms, as in METEOR(Banerjee and Lavie, 2005).5 Experimental Results5.1 Experimental SettingThe different evaluation measures were assessedexperimentally on data from the Chinese?Englishand the Arabic?English task of the NIST 2004evaluation workshop (Przybocki, 2004).
In thisevaluation campaign, 4460 and 1735 candidatetranslations, respectively, generated by differentresearch MT systems were evaluated by humanjudges with regard to fluency and adequacy.Four reference translations are provided for eachcandidate translation.
Detailed corpus statisticsare listed in Table 2.For the experiments in this study, the candidatetranslations from these tasks were evaluated usingdifferent automatic evaluation measures.
Pear-son?s correlation coefficient r between automaticevaluation and the sum of fluency and adequacywas calculated.
As it could be arguable whetherPearson?s r is meaningful for categorical data likehuman MT evaluation, we have also calculatedKendall?s correlation coefficient ?
.
Because ofthe high number of samples (= sentences, 4460)versus the low number of categories (= out-comes of adequacy+fluency, 9), we calculated?
separately for each source sentence.
Theseexperiments showed that Kendall?s ?
reflects thesame tendencies as Pearson?s r regarding theranking of the evaluation measures.
But onlythe latter allows for an efficient calculation ofconfidence intervals.
Consequently, figures of ?are omitted in this paper.Due to the small number of samples for eval-uation on system level (10 and 5, respectively),all correlation coefficients between automaticand human evaluation on system level are veryclose to 1.
Therefore, they do not show anysignificant differences for the different evaluationTable 2: Corpus statistics.
TIDES corpora,NIST 2004 evaluation.Source language Chinese ArabicTarget language English EnglishSentences 446 347Running words 13 016 10 892Ref.
translations 4 4Avg.
ref.
length 29.2 31.4Candidate systems 10 5measures.
Additional experiments on data fromthe NIST 2002 and 2003 workshops and fromthe IWSLT 2004 evaluation workshop confirmthe findings from the NIST 2004 experiments;for the sake of clarity they are not includedhere.
All correlation coefficients presented herewere calculated for sentence level evaluation.For comparison with state-of-the-art evaluationmeasures, we have also calculated the correlationbetween human evaluation and WER and BLEU,which were both measures of choice in severalinternational MT evaluation campaigns.
Further-more, we included TER (Snover et al, 2005) asa recent heuristic block movement measure insome of our experiments for comparison with ourmeasure.
As the BLEU score is unsuitable forsentence level evaluation in its original definition,BLEU-S smoothing as described by (Lin andOch, 2004) is performed.
Additionally, weadded sentence boundary symbols for BLEU, anda different reference length calculation schemefor TER, because these changes improved thecorrelation between human evaluation and the twoautomatic measures.
Details on this have beendescribed in (Leusch et al, 2005).5.2 CDERTable 3 presents the correlation of BLEU, WER,and CDER with human assessment.
It can beseen that CDER shows better correlation thanBLEU and WER on both corpora.
On theChinese?English task, the smoothed BLEU scorehas a higher sentence-level correlation than WER.However, this is not the case for the Arabic?246Table 3: Correlation (r) between human evalua-tion (adequacy + fluency) and automatic evalu-ation with BLEU, WER, and CDER (NIST 2004evaluation; sentence level).Automatic measure Chin.?E.
Arab.
?E.BLEU 0.615 0.603WER 0.559 0.589CDER 0.625 0.623CDER reverseda 0.222 0.393CDER maximumb 0.594 0.599aCD constraints for candidate instead of reference.bSentence-wise maximum of normal and reversed CDERTable 4: Correlation (r) between human evalua-tion (adequacy + fluency) and automatic evalua-tion for CDER with different penalties.Penalty Chin.?E.
Arab.?E.?
0.625 0.623lplen 0.581 0.567lpmisc 0.466 0.528(lplen + lpmisc)/2 0.534 0.557English task.
So none of these two measuresis superior to the other one, but they are bothoutperformed by CDER.If the direction of CDER is reversed (i.e, theCD constraints are required for the candidateinstead of the reference, such that the measurehas precision instead of recall characteristics), thecorrelation with human evaluation is much lower.Additionally we studied the use of the maxi-mum of the distances in both directions.
This hasa lower correlation than taking the original CDER,as Table 3 shows.
Nevertheless, the maximum stillperforms slightly better than BLEU and WER.5.3 Hypothesis Length and PenaltiesThe problem of how to avoid a preference ofoverly long candidate sentences by CDER remainsunsolved, as can be found in Table 4: Each ofthe proposed penalties infers a significant decreaseof correlation between the (extended) CDER andhuman evaluation.
Future research will aim atfinding a suitable length penalty.
Especiallyif CDER is applied in system development,such a penalty will be needed, as preliminaryoptimization experiments have shown.5.4 Substitution CostsTable 5 reveals that the inclusion of word-dependent substitution costs yields a raise by morethan 1% absolute in the correlation of CDERwith human evaluation.
The same is true forTable 5: Correlation (r) between human evalua-tion (adequacy + fluency) and automatic evalu-ation for WER and CDER with word-dependentsubstitution costs.Measure Subst.
costs Chin.?E.
Arab.
?E.WER const (1) 0.559 0.589prefix 0.571 0.605Levenshtein 0.580 0.611CDER const (1) 0.625 0.623prefix 0.637 0.634Levenshtein 0.638 0.637WER: the correlation with human judgment isincreased by about 2% absolute on both languagepairs.
The Levenshtein-based substitution costsare better suited for WER than the scheme basedon common prefix length.
For CDER, there ishardly any difference between the two methods.Experiments on five more corpora did not give anysignificant evidence which of the two substitutioncosts correlates better with human evaluation.
Butas the prefix-based substitution costs improvedcorrelation more consistently across all corpora,we employed this method in our next experiment.5.5 Combination of CDER and PERAn interesting topic in MT evaluation researchis the question whether a linear combination oftwo MT evaluation measures can improve thecorrelation between automatic and human evalu-ation.
Particularly, we expected the combinationof CDER and PER to have a significantly highercorrelation with human evaluation than the mea-sures alone.
CDER (as opposed to PER) has theability to reward correct local ordering, whereasPER (as opposed to CDER) penalizes overly longcandidate sentences.
The two measures werecombined with linear interpolation.
In orderto determine the weights, we performed dataanalysis on seven different corpora.
The result wasconsistent across all different data collections andlanguage pairs: a linear combination of about 60%CDER and 40% PER has a significantly highercorrelation with human evaluation than each ofthe measures alone.
For the two corpora studiedhere, the results of the combination can be foundin Table 6: On the Chinese?English task, there isan additional gain of more than 1% absolute incorrelation over CDER alone.
The combined errormeasure is the best method in both cases.The last line in Table 6 shows the 95%-confidence interval for the correlation.
We seethat the new measure CDER, combined with PER,has a significantly higher correlation with humanevaluation than the existing measures BLEU, TER,247Table 6: Correlation (r) between human evalua-tion (adequacy + fluency) and automatic evalua-tion for different automatic evaluation measures.Automatic measure Chin.?E.
Arab.
?E.BLEU 0.615 0.603TER 0.548 0.582WER 0.559 0.589WER + Lev.
subst.
0.580 0.611CDER 0.625 0.623CDER +prefix subst.
0.637 0.634CDER +prefix+PER 0.649 0.63595%-confidence ?0.018 ?0.028and WER on both corpora.6 Conclusion and OutlookWe presented CDER, a new automatic evalua-tion measure for MT, which is based on editdistance extended by block movements.
CDERallows for reordering blocks of words at constantcost.
Unlike previous block movement measures,CDER can be exactly calculated in quadratictime.
Experimental evaluation on two differenttranslation tasks shows a significantly improvedcorrelation with human judgment in comparisonwith state-of-the-art measures such as BLEU.Additionally, we showed how word-dependentsubstitution costs can be applied to enhance thenew error measure as well as existing approaches.The highest correlation with human assessmentwas achieved through linear interpolation of thenew CDER with PER.Future work will aim at finding a suitable lengthpenalty for CDER.
In addition, more sophisticateddefinitions of the word-dependent substitutioncosts will be investigated.
Furthermore, it willbe interesting to see how this new error measureaffects system development: We expect it toallow for a better sentence-wise error analysis.For system optimization, preliminary experimentshave shown the need for a suitable length penalty.AcknowledgementThis material is partly based upon work supportedby the Defense Advanced Research ProjectsAgency (DARPA) under Contract No.
HR0011-06-C-0023, and was partly funded by the Euro-pean Union under the integrated project TC-STAR?
Technology and Corpora for Speech to SpeechTranslationReferencesS.
Banerjee and A. Lavie.
2005.
METEOR: Anautomatic metric for MT evaluation with improvedcorrelation with human judgments.
ACL Workshopon Intrinsic and Extrinsic Evaluation Measures forMachine Translation and/or Summarization, pages65?72, Ann Arbor, MI, Jun.G.
Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
ARPA Workshop on HumanLanguage Technology.D.
E. Knuth, 1993.
The Stanford GraphBase: aplatform for combinatorial computing, pages 74?87.ACM Press, New York, NY.G.
Leusch, N. Ueffing, and H. Ney.
2003.
A novelstring-to-string distance measure with applicationsto machine translation evaluation.
MT Summit IX,pages 240?247, New Orleans, LA, Sep.G.
Leusch, N. Ueffing, D. Vilar, and H. Ney.
2005.Preprocessing and normalization for automatic eval-uation of machine translation.
ACL Workshop onIntrinsic and Extrinsic Evaluation Measures forMachine Translation and/or Summarization, pages17?24, Ann Arbor, MI, Jun.V.
I. Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions and reversals.
SovietPhysics Doklady, 10(8):707?710, Feb.C.-Y.
Lin and F. J. Och.
2004.
Orange: amethod for evaluation automatic evaluation metricsfor machine translation.
COLING 2004, pages 501?507, Geneva, Switzerland, Aug.D.
Lopresti and A. Tomkins.
1997.
Block edit modelsfor approximate string matching.
TheoreticalComputer Science, 181(1):159?179, Jul.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.2002.
BLEU: a method for automatic evaluationof machine translation.
40th Annual Meeting of theACL, pages 311?318, Philadelphia, PA, Jul.M.
Przybocki.
2004.
NIST machine translation 2004evaluation: Summary of results.
DARPA MachineTranslation Evaluation Workshop, Alexandria, VA.M.
Snover, B. J. Dorr, R. Schwartz, J. Makhoul,L.
Micciulla, and R. Weischedel.
2005.
Astudy of translation error rate with targeted humanannotation.
Technical Report LAMP-TR-126,CS-TR-4755, UMIACS-TR-2005-58, University ofMaryland, College Park, MD.C.
Tillmann, S. Vogel, H. Ney, A. Zubiaga, andH.
Sawaf.
1997.
Accelerated DP based search forstatistical translation.
European Conf.
on SpeechCommunication and Technology, pages 2667?2670,Rhodes, Greece, Sep.J.
P. Turian, L. Shen, and I. D. Melamed.
2003.Evaluation of machine translation and its evaluation.MT Summit IX, pages 23?28, New Orleans, LA, Sep.248
