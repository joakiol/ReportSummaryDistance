Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp.
1112?1118,Prague, June 2007. c?2007 Association for Computational LinguisticsMultilingual Dependency Parsing and Domain Adaptation using DeSRGiuseppe AttardiFelice Dell?OrlettaMaria SimiDipartimento di Informaticalargo B. Pontecorvo 3I-56127 Pisa, Italyattardi@di.unipi.itfelice.dellorletta@ilc.cnr.itsimi@di.unipi.itAtanas ChanevUniversit?
di Trentovia Matteo del Ben 5I-38068 Rovereto, ItalyFondazione Bruno Kessler-irstvia Sommarive 18I-38050 Povo, Italychanev@form.unitn.itMassimiliano CiaramitaYahoo!
Research BarcelonaOcata 1S-08003 Barcelona, Spainmassi@yahoo-inc.comAbstractWe describe our experiments using theDeSR parser in the multilingual and do-main adaptation tracks of the CoNLL 2007shared task.
DeSR implements an incre-mental deterministic Shift/Reduce parsingalgorithm, using specific rules to handlenon-projective dependencies.
For the multi-lingual track we adopted a second orderaveraged perceptron and performed featureselection to tune a feature model for eachlanguage.
For the domain adaptation trackwe applied a tree revision method whichlearns how to correct the mistakes made bythe base parser on the adaptation domain.1 IntroductionClassifier-based dependency parsers (Yamada andMatsumoto, 2003; Nivre and Scholz, 2004) learnfrom an annotated corpus how to select anappropriate sequence of Shift/Reduce actions toconstruct the dependency tree for a sentence.Learning is based on techniques such as SVM(Vapnik 1998) or Memory Based Learning(Daelemans 2003), which provide high accuracybut are often computationally expensive.
For themultilingual track in the CoNLL 2007 SharedTask, we employed a Shift/Reduce parser whichuses a perceptron algorithm with second-orderfeature maps, in order to verify whether a simplerand faster algorithm can still achieve comparableaccuracy.For the domain adaptation track we wished toexplore the use of tree revisions in order toincorporate language knowledge from a newdomain.2 Multilingual TrackThe overall parsing algorithm is a deterministicclassifier-based statistical parser, which extendsthe approach by Yamada and Matsumoto (2003),by using different reduction rules that ensuredeterministic incremental processing of the inputsentence and by adding specific rules for handlingnon-projective dependencies.
The parser alsoperforms dependency labeling within a singleprocessing step.The parser is modular and can use severallearning algorithms.
The submitted runs used asecond order Average Perceptron, derived from themulticlass perceptron of Crammer and Singer(2003).No additional resources were used.
No pre-processing or post-processing was used, exceptstemming for English, by means of the Snowballstemmer (Porter 2001).3 Deterministic Classifier-based ParsingDeSR (Attardi, 2006) is an incremental determinis-tic classifier-based parser.
The parser constructsdependency trees employing a deterministic bot-tom-up algorithm which performs Shift/Reduceactions while analyzing input sentences in left-to-right order.Using a notation similar to (Nivre and Scholz,2003), the state of the parser is represented by a1112quadruple ?S, I, T, A?, where S is the stack of pasttokens, I is the list of (remaining) input tokens, T isa stack of temporary tokens and A is the arc rela-tion for the dependency graph.Given an input string W, the parser is initializedto ?
(), W, (), ()?, and terminates when it reaches aconfiguration ?S, (), (), A?.The three basic parsing rule schemas are as fol-lows: ?S, n|I, T, A?
Shift ?n|S, I, T, A?
?s|S, n|I, T, A?
Rightd ?S, n|I, T, A?
{(s, d, n)}?
?s|S, n|I, T, A?
Leftd ?S, s|I, T, A?
{(n, d, s)}?The schemas for the Left and Right rules are in-stantiated for each dependency type d ?
D, for atotal of 2|D| + 1 rules.
These rules perform bothattachment and labeling.At each step the parser uses classifiers trainedon a treebank corpus in order to predict which ac-tion to perform and which dependency label to as-sign given the current configuration.4 Non-Projective RelationsFor handling non-projective relations, Nivre andNilsson (2005) suggested applying a pre-processing step to a dependency parser, which con-sists in lifting non-projective arcs to their head re-peatedly, until the tree becomes pseudo-projective.A post-processing step is then required to restorethe arcs to the proper heads.In DeSR non-projective dependencies are han-dled in a single step by means of the following ad-ditional parsing rules, slightly different from thosein (Attardi, 2006):?s1|s2|S, n|I, T, A?
Right2d ?
S, s1|n|I, T, A?
{(s2, d, n)}?
?s1|s2|S, n|I, T, A?
Left2d ?s2|S, s1|I, T, A?
{(n, d, s2)}?
?s1|s2|s3|S, n|I, T, A?
Right3d ?
S, s1|s2|n|I, T, A?
{(s3, d, n)}?
?s1|s2|s3|S, n|I, T, A?
Left3d ?s2|s3|S, s1|I, T, A?
{(n, d, s3)}?
?s1|s2|S, n|I, T, A?
Extract ?n|s1|S, I, s2|T, A?
?S, I, s1|T, A?
Insert ?s1|S, I, T, A?Left2, Right2 are similar to Left and Right, exceptthat they create links crossing one intermediatenode, while Left3 and Right3 cross two intermedi-ate nodes.
Notice that the RightX actions put backon the input the intervening tokens, allowing theparser to complete the linking of tokens whoseprocessing had been delayed.
Extract/Insert gener-alize the previous rules by moving one token to thestack T and reinserting the top of T into S.5 Perceptron Learning and 2nd-OrderFeature MapsThe software architecture of the DeSR parser ismodular.
Several learning algorithms are available,including SVM, Maximum Entropy, Memory-Based Learning, Logistic Regression and a fewvariants of the perceptron algorithm.We obtained the best accuracy with a multiclassaveraged perceptron classifier based on theultraconservative formulation of Crammer andSinger (2003) with uniform negative updates.
Theclassifier function is: { }xxF kk?= ?maxarg)(where each parsing action k is associated with aweight vector ?k.
To regularize the model the finalweight vectors are computed as the average of allweight vectors posited during training.
The numberof learning iterations over the training data, whichis the only adjustable parameter of the algorithm,was determined by cross-validation.In order to overcome the limitations of a linearperceptron, we introduce a feature map ?
: IRd ?IRd(d+1)/2 that maps a feature vector x into a higherdimensional feature space consisting of all un-ordered feature pairs: ?
(x) = ?xixj | i = 1, ?, d, j = i, ?, d?In other words we expand the originalrepresentation in the input space with a featuremap that generates all second-order featurecombinations from each observation.
We call thisthe 2nd-order model, where the inner products arecomputed as ?k ?
?
(x), with ?k a vector of dimen-sion d(d+1)/2.
Applying a linear perceptron to thisfeature space corresponds to simulating a polyno-mial kernel of degree two.A polynomial kernel of degree two for SVMwas also used by Yamada and Matsumoto (2003).However, training SVMs on large data sets likethose arising from a big training corpus was too1113computationally expensive, forcing them to resortto partitioning the training data (by POS) and tolearn several models.Our implementation of the perceptron algorithmuses sparse data structures (hash maps) so that itcan handle efficiently even large feature spaces ina single model.
For example the feature space forthe 2nd-order model for English contains over 21million.
Parsing unseen data can be performed attens of sentences per second.
More details on suchaspects of the DeSR parser can be found in (Ci-aramita and Attardi 2007).6 TuningThe base parser was tuned on several parameters tooptimize its accuracy as follows.6.1 Feature SelectionGiven the different characteristics of languages andcorpus annotations, it is worth while to select adifferent set of features for each language.
For ex-ample, certain corpora do not contain lemmas ormorphological information so lexical informationwill be useful.
Vice versa, when lemmas are pre-sent, lexical information might be avoided, reduc-ing the size of the feature set.We performed a series of feature selection ex-periments on each language, starting from a fairlycomprehensive set of 43 features and trying allvariants obtained by dropping a single feature.
Thebest of these alternatives feature models was cho-sen and the process iterated until no further gainswere achieved.
The score for the alternatives wascomputed on a development set of approximately5000 tokens, extracted from a split of the originaltraining corpus.Despite the process is not guaranteed to producea global optimum, we noticed LAS improvementsof up to 4 percentage points on some languages.The set of features to be used by DeSR is con-trolled by a number of parameters supplied througha parameter file.
Each parameter describes a fea-ture and from which tokens to extract it.
Tokensare referred through positive numbers for inputtokens and negative numbers for tokens on thestack.
For examplePosFeatures -2 -1 0 1 2 3means to use the POS tag of the first two tokens onthe stack and of the first four tokens on the input.The parameter PosPrev refers to the POS of thepreceding token in the original sentence, PosLeftChild refers to the POS of the left chil-dren of a token, PastActions tells how manyprevious actions to include as features.The selection process was started from the fol-lowing base feature model:LexFeatures -1 0 1 LemmaFeatures -2 -1 0 1 2 3 LemmaPrev  -1 0 LemmaSucc  -1 0 LemmaLeftChild -1 0 LemmaRightChild -1 MorphoFeatures -1 0 1 2 PosFeatures -2 -1 0 1 2 3 PosNext  -1 0 PosPrev  -1 0 PosLeftChild -1 0 PosRightChild -1 0 CPosFeatures -1 0 1 DepFeatures -1 0 DepLeftChild -1 0 DepRightChild -1 PastActions 1The selection process produced different variantsfor each language, sometimes suggesting droppingcertain intermediate features, like the lemma of thethird next input token in the case of Catalan:LemmaFeatures -2 -1 0 1 3 LemmaPrev  0 LemmaSucc  -1 LemmaLeftChild 0 LemmaRightChild -1 PosFeatures -2 -1 0 1 2 3 PosPrev  0 PosSucc  -1 PosLeftChild -1 0 PosRightChild -1 0 CPosFeatures -1 0 1 MorphoFeatures 0 1 DepLeftChild -1 0 DepRightChild -1For Italian, instead, we ran a series of tests in par-allel using a set of manually prepared feature mod-els.
The best of these models achieved a LAS of80.95%.
The final run used this model with theaddition of the morphological agreement featurediscussed below.English was the only language for which no featureselection was done and for which lexical features1114were used.
English is also the language where theofficial score is significantly lower than what wehad been getting on our development set (90.01%UAS).6.2 Prepositional AttachmentCertain languages, such as Catalan, use detaileddependency labeling, that for instance distinguishbetween adverbials of location and time.
We ex-ploited this information by introducing a featurethat captures the entity type of a child of the topword on the stack or in the input.
During training alist of nouns occurring in the corpus as dependenton prepositions with label CCL (meaning ?com-plement of location?
for Catalan) was created andsimilarly for CCT (complement of time).
The en-tity type TIME is extracted as a feature dependingon whether the noun occurs in the time list morethan ?
times than in the location list, and similarlyfor the feature LOCATION.
?
was set to 1.5 in ourexperiments.6.3 Morphological AgreementCertain languages require gender and numberagreement between head and dependent.
The fea-ture MorphoAgreement is computed for such lan-guages and provided noticeable accuracyimprovements.For example, for Italian, the improvement wasfrom:LAS: 80.95%,  UAS: 85.03%to:LAS: 81.34%,  UAS: 85.54%For Catalan, adding this feature we obtained anunofficial score of:LAS: 87.64%,  UAS: 92.20%with respect to the official run:LAS: 86.86%,  UAS: 91.41%7 AccuracyTable 1 reports the accuracy scores in the multilin-gual track.
They are all considerably above theaverage and within 2% from the best for Catalan,3% for Chinese, Greek, Italian and Turkish.8 PerformanceThe experiments were performed on a 2.4 GhzAMD Opteron machine with 32 GB RAM.
Train-ing the parser using the 2nd-order perceptron on theEnglish corpus required less than 3 GB of memoryand about one hour for each iteration over thewhole dataset.
Parsing the English test set required39.97 sec.
For comparison, we tested the MSTparser version 0.4.3 (Mstparser, 2007), configuredfor second-order, on the same data: training took73.9 minutes to perform 10 iterations and parsingtook 97.5 sec.
MST parser achieved:LAS: 89.01%, UAS: 90.17%9 Error Analysis on CatalanThe parser achieved its best score on Catalan, sowe performed an analysis on its output for this lan-guage.Among the 42 dependency relations that theparser had to assign to a sentence, the largest num-ber of errors occurred assigning CC (124), SP (33), CD (27), SUJ (26), CONJUNCT (22), SN (23).The submitted run for Catalan did not use theentity feature discussed earlier and indeed 67 er-rors were due to assigning CCT or CCL instead ofCC (generic complement of circumstance).
How-ever over half of these appear as underspecifiedannotation errors in the corpus rather than parsererrors.By adding the ChildEntityType feature,which distinguishes better between CCT and CCL,the UAS improved, while the LAS droppedslightly, due to the effect of underspecified annota-tions in the corpus:LAS: 87.22%,    UAS: 91.71%Table 1.
Multilingual track official scores.LAS UASTask1st DeSR Avg 1st DeSR AvgArabic  76.52  72.66 68.34  86.09  82.53 78.84Basque  76.92  69.48 68.06  82.80  76.86 75.15Catalan  88.70  86.86 79.85  93.40  91.41 87.98Chinese  84.69  81.50 76.59  88.94  86.73 81.98Czech  80.19  77.37 70.12  86.28  83.40 77.56English  89.61  85.85 80.95  90.63  86.99 82.67Greek  76.31  73.92 70.22  84.08  80.75 77.78Hungarian  80.27  76.81 71.49  83.55  81.81 76.34Italian  84.40  81.34 78.06  87.91  85.54 82.45Turkish  79.81  76.87 73.19  86.22  83.56 80.331115A peculiar aspect of the original Catalan corpuswas the use of a large number (195) of dependencylabels.
These labels were reduced to 42 in the ver-sion used for CoNNL 2007, in order to make itcomparable to other corpora.
However, performingsome preliminary experiments using the originalCatalan collection with all 195 dependency labels,the DeSR parser achieved a significantly betterscore:LAS: 88.80%, UAS: 91.43%while with the modified one, the score dropped to:LAS: 84.55%, UAS: 89.38%This suggests that accuracy might improve forother languages as well if the training corpus waslabeled with more precise dependencies.10 Adaptation TrackThe adaptation track originally covered two do-mains, the CHILDES and the Chemistry domain.The CHILDES (Brown, 1973; MacWhinney,2000) consists of transcriptions of dialogues withchildren, typically short sentences of the kind:Would you like more grape juice ?That 's a nice box of books .Phrases are short, half of them are questions.
Theonly difficulty that appeared from looking at theunlabeled collection supplied for training in thedomain was the presence of truncated terms like goin (for going), d (for did), etc.
However noneof these unusually spelled words appeared in thetest set, so a normal English parser performed rea-sonably well on this task.
Because of certain in-consistencies in the annotation guidelines, theorganizers decided to make this task optional andhence we submitted just the parse produced by theparser trained for English.For the second adaptation task we were given alarge collection of unlabeled data in the chemistrydomain (Kulick et al 2004) as well as a test set of5000 tokens (200 sentences) to parse (eng-lish_pchemtbtb_test.conll).There were three sets of unlabeled documents:we chose the smallest (unlab1) consisting of over300,000 tokens (11663 sentences).
unlab1 wastokenized, POS and lemmas were added using ourversion of TreeTagger (Schmid, 1994), and lem-mas replaced with stems, which had turned out tobe more effective than lemmas.
We call this set pchemtb_unlab1.conll.We trained the DeSR parser on English using english_ptb_train.conll, the WSJ PTB col-lection provided for CoNLL 2007.
This consists ofWSJ sections 02-11, half of the usual set 02-23, fora total of 460,000 tokens with dependencies gener-ated with the converter by Johansson and Nugues(2007).We added stems and produced a parser called DeSRwsj.
By parsing eng-lish_pchem_test.conll with DeSRwsj weobtained pchemtb_test_base.desr, our base-line for the task.By visual inspection using DgAnnotator(DgAnnotator, 2006), the parses looked generallycorrect.
Most of the errors seemed due to improperhandling of conjunctions and disjunctions.
Thecollection in fact contains several phrases like:Specific antibodies raised againstP450IIB1 , P450 IA1 or IA2 ,P450IIE1 , and P450IIIA2 inhibitedthe activation in liver microsomesfrom rats pretreated with PB , BNF ,INH and DEX respectivelyThe parser did not seem to have much of a problemwith terminology, possibly because the suppliedgold POS were adequate.For the adaptation we proceeded as follows.
Weparsed pchemtb_unlab1.conll using DeSRwsjobtaining pchemtb_unlab1.desr.We then extracted a set of 12,500 sentencesfrom ptb_train.conll and 7,500 sentencesfrom pchemtb_unlab1.desr, creating a corpusof 20,000 sentences called combined.conll.
Inboth cases the selection criteria was to choose sen-tences shorter than 30 tokens.We then trained a low accuracy parser (called DesrCombined) on combined.conll, by usinga 1st-order averaged perceptron.
DesrCombinedwas used to parse english_ptb_train.conll,the original training corpus for English.
By com-paring this parse with the original, one can detectwhere such parser makes mistakes.
The rationalefor using an inaccurate parser is to obtain parseswith many errors so that they form a suitably largetraining set for the next step: parser revision.We then used a parsing revision technique (At-tardi and Ciaramita, 2007) to learn how to correctthese errors, producing a parse reviser called DesrReviser.
The revision technique consists ofcomparing the parse trees produced by the parserwith the gold standard parse trees, from theannotated corpus.
Where a difference is noted, a1116revision rule is determined to correct the mistake.Such rules consist in movements of a single link toa different head.
Learning how to revise a parsetree consists in training a classifier on a set oftraining examples consisting of pairs ?
(wi, d, wj),ti?, i.e.
the link to be modified and thetransformation rule to apply.
Attardi and Ciaramita(2007) showed that 80% of the corrections can betypically dealt with just 20 tree revision rules.
Forthe adaptation track we limited the training toerrors recurring at least 20 times and to 30 rules.
DesrReviser was then applied to pchemtb_test_base.desr producing pchemtb_test_rev.desr, our final submission.Many conjunction errors were corrected, in par-ticular by moving the head of the sentence from acoordinate verb to the conjunction ?and?
linkingtwo coordinate phrases.The revision step produced an improvement of0.42% LAS over the score achieved by using justthe base DeSRwsj parser.Table 2 reports the official accuracy scores onthe closed adaptation track.
DeSR achieved a closesecond best UAS on the ptchemtb test set andthird best on CHILDES.
The results are quite en-couraging, particularly considering that the revi-sion step does not yet correct the dependencylabels and that our base English parser had a lowerrank in the multilingual track.LAS UASTask1st DeSR Avg 1st DeSR AvgCHILDES     61.37 58.67 57.89Pchemtb  81.06 80.40 73.03 83.42  83.08 76.42Table 2.
Closed adaptation track scores.Notice that the adaptation process could be iter-ated.
Since the combination DeSRwsj+DesrReviser is a more accurate parserthan DeSRwsj, we could use it again to parse pchemtb_unlab1.conll and so on.11 ConclusionsFor performing multilingual parsing in the CoNLL2007 shared task we employed DeSR, a classifier-based Shift/Reduce parser.
We used a second orderaveraged perceptron as classifier and achieved ac-curacy scores quite above the average in all lan-guages.
For proper comparison with otherapproaches, one should take into account that theparser is incremental and deterministic; hence it istypically faster than other non linear algorithms.For the adaptation track we used a novel ap-proach, based on the technique of tree revision,applied to a parser trained on a corpus combiningsentences from both the training and the adaptationdomain.
The technique achieved quite promisingresults and it also offers the interesting possibilityof being iterated, allowing the parser to incorporatelanguage knowledge from additional domains.Since the technique is applicable to any parser,we plan to test it also with more accurate Englishparsers.Acknowledgments.
The following treebankswere used for training the parser: (Aduriz et al,2003; B?hmov?
et al, 2003; Chen et al, 2003; Ha-ji?
et al, 2004; Marcus et al, 1993; Mart?
et al,2002; Montemagni et al 2003; Oflazer et al, 2003;Prokopidis et al, 2005; Csendes et al, 2005).Ryan McDonald and Jason Baldridge made avail-able mstparser and helped us using it.
We grate-fully acknowledge Hugo Zaragoza and RicardoBaeza-Yates for supporting the first author duringa sabbatical at Yahoo!
Research Barcelona.ReferencesA.
Abeill?, editor.
2003.
Treebanks: Building and UsingParsed Corpora.
Kluwer.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A.Diaz de Ilarraza, A. Garmendia and M. Oronoz.2003.
Construction of a Basque Dependency Tree-bank.
In Proc.
of the 2nd Workshop on Treebanksand Linguistic Theories (TLT), 201?204.G.
Attardi.
2006.
Experiments with a Multilanguagenon-projective dependency parser.
In Proc.
of theTenth CoNLL, 2006.G.
Attardi, M. Ciaramita.
2007.
Tree Revision Learningfor Dependency Parsing.
In Proc.
of NAACL/HLTC2007.A.
B?hmov?, J. Hajic, E. Hajicov?
and B. Hladk?.
2003.The PDT: a 3-level annotation scenario.
In Abeill?
(2003), chapter 7, 103?127.R.
Brown.
1973.
A First Language: The Early Stages.Harvard University Press.K.
Chen, C. Luo, M. Chang, F. Chen, C. Chen, C.Huang and Z. Gao.
2003.
Sinica Treebank: DesignCriteria, Representational Issues and Implementation.In Abeill?
(2003), chapter 13, 231?248.1117M.
Ciaramita, G. Attardi.
2007.
Dependency Parsingwith Second-Order Feature Maps and Annotated Se-mantic Information.
Proc.
of the 12th InternationalWorkshop on Parsing Technologies (IWPT), 2007.K.
Crammer, Y.
Singer.
2003.
Ultraconservative OnlineAlgorithms for Multiclass Problems.
Journ.
of Ma-chine Learning Research.D.
Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor.2005.
The Szeged Treebank.
Springer.DgAnnotator.
2006.http://medialab.di.unipi.it/Project/Parser/DgAnnotator/.J.
Hajic, O. Smrz, P. Zem?nek, J. Snaidauf and E.Beska.
2004.
Prague Arabic Dependency Treebank:Development in Data and Tools.
In Proc.
of theNEMLAR Intern.
Conf.
on Arabic Language Re-sources and Tools, 110?117.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProc.
of the 16th Nordic Conference onComputational Linguistics (NODALIDA).S.
Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-Donald, M. Palmer, A. Schein, and L. Ungar.
2004.Integrated annotation for biomedical information ex-traction.
In Proc.
of the Human LanguageTechnology Conference and the Annual Meeting ofthe North American Chapter of the Association forComputational Linguistics (HLT/NAACL).B.
MacWhinney.
2000.
The CHILDES Project: Toolsfor Analyzing Talk.
Lawrence Erlbaum.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: thePenn Treebank.
Computational Linguistics,19(2):313?330.M.
A.
Mart?, M.
Taul?, L. M?rquez and M. Bertran.2007.
CESS-ECE: A Multilingual and MultilevelAnnotated Corpus.
Available for download from:http://www.lsi.upc.edu/~mbertran/cess-ece/.R.
McDonald, et al 2005.
Non-projective DependencyParsing using Spanning Tree Algorithms.
In Proc.
ofHLT-EMNLP.B.
MacWhinney.
2000.
The CHILDES Project: Toolsfor Analyzing Talk.
Lawrence Erlbaum.S.
Montemagni, F. Barsotti, M. Battista, N. Calzolari,O.
Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M.Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D.Saracino, F. Zanzotto, N. Nana, F. Pianesi, and R.Delmonte.
2003.
Building the Italian Syntactic-Semantic Treebank.
In Abeill?
(2003), chapter 11,189?210.Mstparser 0.4.3.
2007.http://sourceforge.net/projects/mstparser/J.
Nivre, et al 2004.
Memory-based Dependency Pars-ing.
In Proc.s of the Eighth CoNLL, ed.
H. T. Ng andE.
Riloff, Boston, Massachusetts, 49?56.J.
Nivre and J. Nilsson.
2005.
Pseudo-Projective De-pendency Parsing.
In Proc.
of the 43rd Annual Meet-ing of the ACL, 99?106.J.
Nivre and M. Scholz.
2004.
Deterministic Depend-ency Parsing of English Text.
In Proc.
of COLING2004, Geneva, Switzerland, 64?70.J.
Nivre, J.
Hall, S. K?bler, R. McDonald, J. Nilsson, S.Riedel, and D. Yuret.
2007.
The CoNLL 2007 sharedtask on dependency parsing.
In Proc.
of the CoNLL2007 Shared Task.
Joint Conf.
on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL).K.
Oflazer, B.
Say, D. Zeynep Hakkani-T?r, and G. T?r.2003.
Building a Turkish treebank.
In Abeill?
(2003),chapter 15, 261?277.M.F.
Porter.
2001.
Snowball Stemmer.http://www.snowball.tartarus.org/P.
Prokopidis, E. Desypri, M. Koutsombogera, H.Papageorgiou, and S. Piperidis.
2005.
Theoreticaland practical issues in the construction of a Greekdepen- dency treebank.
In Proc.
of the 4th Workshopon Treebanks and Linguistic Theories (TLT), pages149?160.H.
Schmid.
1994.
Probabilistic Part-of-Speech TaggingUsing Decision Trees.
In Proc.
of International Con-ference on New Methods in Language Processing.V.
N. Vapnik.
1998.
The Statistical Learning Theory.Springer.H.
Yamada and Y. Matsumoto.
2003.
Statistical De-pendency Analysis with Support Vector Machines.
InProc.
of the 8th International Workshop on ParsingTechnologies (IWPT), 195?206.1118
