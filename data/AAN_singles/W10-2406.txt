Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 48?52,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsAbstractThe system presented in this paper uses acombination of two techniques to directlytransliterate from grapheme to grapheme.
Thetechnique makes no language specific as-sumptions, uses no dictionaries or explicitphonetic information; the process transformssequences of tokens in the source languagedirectly into to sequences of tokens in thetarget.
All the language pairs in our experi-ments were transliterated by applying thistechnique in a single unified manner.
Theapproach we take is that of hypothesis re-scoring to integrate the models of two state-of-the-art techniques: phrase-based statisticalmachine translation (SMT), and a joint multi-gram model.
The joint multigram model wasused to generate an n-best list of translitera-tion hypotheses that were re-scored using themodels of the phrase-based SMT system.
Theboth of the models?
scores for each hypothesiswere linearly interpolated to produce a finalhypothesis score that was used to re-rank thehypotheses.
In our experiments on develop-ment data,  the combined system was able tooutperform both of its component systemssubstantially.1 IntroductionIn statistical machine translation the re-scoringof hypotheses produced by a system with addi-tional models that  incorporate information notavailable to the original system has been shownto be an effective technique to improve systemperformance (Paul et al, 2006).
Our approachuses a re-scoring technique to integrate themodels of two transliteration systems that areeach capable in their own right: a phrase-basedstatistical machine translation system (Koehn etal., 2003), and a joint  multigram model (Deligneand Bimbot, 1995; Bisani and Ney, 2008).In this work we treat the process of translit-eration as a process of direct  transduction fromsequences of tokens in the source language tosequences of tokens in the target language withno modeling of the phonetics of either source ortarget  language (Knight and Graehl, 1997).
Tak-ing this approach allows for a very generaltransliteration system to be built  that does notrequire any language specific knowledge to beincorporated into the system (for some languagepairs this may not be the best strategy since lin-guistic information can be used to overcomeissues of data sparseness on smaller datasets).2 Component SystemsFor this shared task we chose to combine twosystems through a process of re-scoring.
Thesystems were selected because of their expectedstrong level of performance (SMT systems havebeen used successfully in the field, and jointmultigram models have performed well both ingrapheme to phoneme conversion and Arabic-English transliteration).
Secondly, the joint mul-tigram model relies on key features not presentin the SMT system, that is the history of bilin-gual phrase pairs used to derive the target.
Forthis reason we felt the systems would comple-ment each other well.
We now briefly describethe component systems.2.1 Joint Multigram ModelThe joint  multigram approach proposed by (De-ligne and Bimbot, 1995) has arisen as an exten-sion of the use of variable-length n-grams (mul-tigrams) in language modeling.
In a joint  multi-gram, the units in the model consist of multipleinput  and output symbols.
(Bisani and Ney,2008) refined the approach and applied to itgrapheme-to-phoneme conversion, where itsperformance was shown to be comparable tostate-of-the-art systems.
The approach was laterapplied to Arabic-English transliteration (Dese-laers et al, 2009) again with promising results.Joint multigram models have the followingcharacteristics:?The symbols in the source and target areco-segmentedTransliteration using a Phrase-based Statistical Machine TranslationSystem to Re-score the Output of a Joint Multigram ModelAndrew FinchNICT3-5 HikaridaiKeihanna Science City619-0289 JAPANandrew.finch@nict.go.jpEiichiro SumitaNICT3-5 HikaridaiKeihanna Science City619-0289 JAPANeiichiro.sumita@nict.go.jp48-Maximum likelihood training using anEM algorithm (Deligne and Bimbot,1995)?The probability of sequences of joint mul-tigrams is modeled using an n-grammodelIn these respects the model can be viewed asa close relative of the joint source channelmodel proposed by (Li et  al., 2004) for translit-eration.2.2 Phrase-based SMTIt  is possible to view the process of translitera-tion as a process of translation at the characterlevel, without  re-ordering.
From this perspectiveit is possible to directly employ a phrase-basedSMT  system in the task of transliteration (Finchand Sumita, 2008; Rama and Gali, 2009).
Aphrase-based SMT system has the followingcharacteristics:?The symbols in the source and target arealigned one to many in both directions.Joint sequences of source and target sym-bols are heuristically extracted giventhese alignments?Transliteration is performed using a log-linear model with weights tuned on de-velopment data?The models include: a translation model(with 5 sub-models), and a target lan-guage modelThe bilingual phrase-pairs are analogous tothe joint  multigrams, however the translationmodel of the SMT system doesn?t use the con-text of previously translated phrase-pairs, in-stead relying on a target language model.3 Experimental Conditions3.1 SMT DecoderIn our experiments we used an in-house phrase-based statistical machine translation decodercalled CleopATRa.
This decoder operates onexactly the same principles as the publiclyavailable MOSES decoder (Koehn et al, 2003).Our decoder was modified to be able to decodesource sequences with reference to a target se-quence; the decoding process being forced togenerate the target.
The decoder was also con-figured to combine scores of multiple deriva-tions yielding the same target  sequence.
In thisway the models in the decoder were used to de-rive scores used to re-score the n-best (we usedn=20 for our experiments) hypotheses generatedby the joint  multigram model.
The phrase-extraction process was symmetrized with re-spect  to token order using the technique pro-posed in (Finch and Sumita, 2010).
In order toadapt  the SMT system to the task of translitera-tion, the decoder was constrained decode in amonotone manner, and furthermore during train-ing, the phrase extraction process was con-strained such that  only phrases with monotoneorder were extracted in order to minimize theeffects of errors in the word alignment process.In a final step the scores from both systemswere linearly interpolated to produce a singleintegrated hypothesis score.
The hypotheseswere then re-ranked according to this integratedscore for the final submission.3.2 Joint Multigram modelFor the joint  multigram system we used the pub-licly available Sequitur G2P  grapheme-to-phoneme converter (Bisani and Ney, 2008).
Thesystem was used with its default settings, andpilot experiments were run on development  datato determine appropriate settings for the maxi-mum size of the multigrams.
The results for theEnglish-to-Japanese task are shown in Figure 1.As can be seen in the figure, the system rapidlyimproves to a near-optimal value with a maxi-mum multigram size of 4.
No improvement  atall was observed for sizes over 7.
We thereforechose a maximum multigram size of 8 for theexperiments presented in this paper, and for thesystems entered in the shared task.3.3 Pre-processingIn order to reduce data sparseness we took thedecision to work with data in only its lowercaseform.We  chose not  to perform any tokenization orphonetic mapping for any of the language pairsFigure 1: The effect on F-score by tuning withrespect to joint multigram size0.30.40.60.70.91 2 3 4 5 6 7 8 9 10F-ScoreJoint Multigram Size49in the shared task.
We adopted this approachbecause:?It  allowed us to have a single unifiedapproach for all language pairs?It  was in the spirit  of the shared task, asit did not  require extra knowledge out-side of the supplied corpora3.4 Handling Multi-Word SequencesThe data for some languages contained somemulti-word sequences.
To handle these we hadto consider the following strategies:?Introduce a <space> token into the se-quence, and treat  it  as one long charac-ter sequence to transliterate; or?Segment the word sequences into indi-vidual words and transliterate these in-dependently, combining the n-best hy-pothesis lists for all the individual wordsin the sequence into a single output se-quence.We adopted both approaches: for those multi-word sequences where the number of words inthe source and target matched, the latter ap-proach was taken; for those where the numbersof source and target words differed, the formerapproach was taken.
The decoding process formulti-word sequences is shown in Figure 2.During recombination, the score for the targetword sequence was calculated as the product ofthe scores of each hypothesis for each word.Therefore a search over all combinations of hy-potheses is required.
In almost all cases we wereable to perform a full search.
For the rare longword sequences in the data, a beam search strat-egy was adopted.3.5 Building the ModelsFor the final submissions, all systems weretrained on the union of the training data and de-velopment data.
It was felt that the training setwas sufficiently small that  the inclusion of thedevelopment  data into the training set  wouldyield a reasonable boost  in performance by in-creasing the coverage of the systems.
All tunableparameters were tuned on development data us-ing systems built  using only the training data.Under the assumption that  these parameterswould perform well in the systems trained onthe combined development/training corpora,these tuned parameters were transferred directlyto the systems trained on all available data.3.6 Parameter TuningThe SMT  systems were tuned using the mini-mum error rate training procedure introduced in(Och, 2003).
For convenience, we used BLEUas a proxy for the various metrics used in theshared task evaluation.
The BLEU score iscommonly used to evaluate the performance ofFigure 2: The transliteration process for multi-word sequencesWord 1 Word 2 Word mSegment into individual words and transliterate each word independentlyTransliterateTransliterateTransliteraten-besthypothesis 1hypothesis 2...hypothesis nn-besthypothesis 1hypothesis 2...hypothesis nn-besthypothesis 1hypothesis 2...hypothesis nSearch for the best pathFigure 3: The effect on the F-score of the integratedsystem by tuning with respect to the SMT system?sinterpolation weight0.830.840.850 0.2 0.4 0.6 0.8 1.0F-ScoreSMT System Interpolation Weight50machine translation systems and is a function ofthe geometric mean of n-gram precision.
Theuse of BLEU score as a proxy has been shownto be a reasonable strategy for the metrics usedin these experiments (Finch and Sumita, 2009).Nonetheless, it is reasonable to assume that  onewould be able to improve the performance in aparticular evaluation metric by doing minimumerror rate training specifically for that metric.The interpolation weight  was tuned by a gridsearch to find the value that gave the maximal f-score (according to the official f-score evalua-tion metric for the shared task) on the develop-ment data, the process for English-Japanese isshown in Figure 3.4 ResultsThe results of our experiments are shown in Ta-ble 1.
These results are the official shared taskevaluation results on the test  data, and the scoresfor all of the evaluation metrics are shown in thetable.
The reader is referred to the workshopwhite paper (Li et al, 2010) for details of theevaluation metrics.
The system achieved a highlevel of performance on most of the languagepairs.
Comparing the individual systems to eachother, and to the integrated system, the jointmultigram system outperformed the phrase-based SMT  system.
In experiments run on theEnglish-to-Japanese katakana task, the jointmultigram system in isolation achieved an F-score of 0.837 on development data, whereas theSMT  system in isolation achieved an F-score of0.824.
When integrated the models of the sys-tems complemented each other well, and on thesame English-Japanese task the integrated sys-tem achieved an F-score of 0.843.We feel that for some language pairs, mostnotably Arabic-English where a large differenceexisted between our system and the top-rankedsystem, there is much room for improvement.One of the strengths in terms of the utility of ourapproach is that it  is free from dependence onthe linguistic characteristics of the languagesbeing processed.
This property makes it  gener-ally applicable, but due to the limited amountsof data available for the shared task, we believethat in order to progress, a language-dependentapproach will be required.5 ConclusionWe applied a system that  integrated two state-of-the-art  systems through a process of re-scoring,to the NEWS 2010 Workshop shared task ontransliteration generation.
Our systems gave astrong performance on the shared task test  set,and our experiments show the integrated systemwas able to outperform both of its componentsystems.
In future work we would like to departfrom the direct grapheme-to-grapheme approachtaken here and address the problem of how bestto represent  the source and target  sequences byeither analyzing their symbols further, or ag-glomerating them.
We would also like to inves-tigate the use of co-segmentation schemes thatdo not rely on maximum likelihood training toovercome the issues inherent in this technique.AcknowledgementsThe results presented in this paper draw on thefollowing data sets.
For English-Japanese andArabic-English, the reader is referred to the CJKwebsite: http://www.cjk.org.
For English-Hindi,English-Tamil, and English-Kannada, andEnglish-Bangla the data sets originated from thework of Kumaran and Kellner, 2007.Language PairAccuracy intop-1MeanF-scoreMRR MAPrefEnglish ?
Thai 0.412 0.883 0.550 0.412Thai ?
English 0.397 0.873 0.525 0.397English ?
Hindi 0.445 0.884 0.574 0.445English ?
Tamil 0.390 0.887 0.522 0.390English ?
Kannada 0.371 0.871 0.506 0.371English ?
Japanese 0.378 0.783 0.510 0.378Arabic ?
English 0.403 0.891 0.512 0.327English ?
Bangla 0.412 0.883 0.550 0.412Table 1: The results of our system in the official evaluation on the test data on all performance metrics.51ReferencesPeter Brown, Stephen Della Pietra, Vincent DellaPietra, and Robert Mercer, 1991.
The mathematicsof statistical machine translation: parameter esti-mation.
Computational Linguistics,  19(2), 263-311.Sabine Deligne, and Fr?d?ric Bimbot, 1995.
Lan-guage modeling by variable length sequences:theoretical formulation and evaluation of multi-grams.
In: Proc.
IEEE Internat.
Conf.
on Acous-tics, Speech and Signal Processing, Vol.
1, Detroit,MI, USA, pp.
169?172.Maximilian Bisani and Hermann Ney, 2008.
Joint-Sequence Models for Grapheme-to-PhonemeConversion.
Speech Communication, Volume 50,Issue 5, Pages 434-451.Thomas Deselaers, Sasa Hasan, Oliver Bender, andHermann Ney, 2009.
A Deep Learning Approachto Machine Transliteration.
In Proceedings of theEACL 2009 Workshop on Statistical MachineTranslation (WMT09), Athens, Greece.Andrew Finch and Eiichiro Sumita, 2008.
Phrase-based machine transliteration.
In Proceedings ofWTCAST'08, pages 13-18.Andrew Finch and Eiichiro Sumita,  2009.
Translit-eration by Bidirectional Statistical Machine Trans-lation, Proceedings of the 2009 Named EntitiesWorkshop: Shared Task on Transliteration, Singa-pore.Andrew Finch and Eiichiro Sumita, 2010.
ExploitingDirectional Asymmetry in Phrase-table Generationfor Statistical Machine Translation,  In Proceed-ings of NLP2010, Tokyo, Japan.Kevin Knight and Jonathan Graehl, 1997.
MachineTransliteration.
Proceedings of the Thirty-FifthAnnual Meeting of the Association for Computa-tional Linguistics and Eighth Conference of theEuropean Chapter of the Association for Compu-tational Linguistics, pp.
128-135, Somerset,  NewJersey.Philipp Koehn, Franz Josef Och, and Daniel Marcu,2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of the Human Language TechnologyConference 2003 (HLT-NAACL 2003), Edmonton,Canada.Franz Josef Och, 2003.
Minimum error rate trainingfor statistical machine translation, Proceedings ofthe ACL.A Kumaran and Tobias Kellner, 2007.
A genericframework for machine transliteration, Proc.
ofthe 30th SIGIR.Haizhou Li, Min Zhang, Jian Su, 2004.
A joint sourcechannel model for machine transliteration, Proc.of the 42nd ACL.Haizhou Li, A Kumaran, Min Zhang and VladimirPervouchine, 2010.
Whitepaper of NEWS 2010Shared Task on Transliteration Generation.
InProc.
of ACL2010 Named Entities Workshop.Michael Paul, Eiichiro Sumita and Seiichi Yama-moto,  2006, Multiple Translation-Engine-basedHypotheses and Edit-Distance-based Rescoringfor a Greedy Decoder for Statistical MachineTranslation, Information and Media Technologies,Vol.
1, No.
1, pp.446-460 .Taraka Rama and Karthik Gali, 2009.
Modeling ma-chine transliteration as a phrase based statisticalmachine translation problem, Proceedings of the2009 Named Entities Workshop: Shared Task onTransliteration, Singapore.52
