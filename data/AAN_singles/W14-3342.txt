Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 335?341,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsLIG System for Word Level QE task at WMT14Ngoc-Quang Luong Laurent BesacierLIG, Campus de Grenoble41, Rue des Math?ematiques,UJF - BP53, F-38041 Grenoble Cedex 9, France{ngoc-quang.luong,laurent.besacier,benjamin.lecouteux}@imag.frBenjamin LecouteuxAbstractThis paper describes our Word-level QEsystem for WMT 2014 shared task onSpanish - English pair.
Compared toWMT 2013, this year?s task is differentdue to the lack of SMT setting informationand additional resources.
We reporthow we overcome this challenge to retainmost of the important features whichperformed well last year in our system.Novel features related to the availability ofmultiple systems output (new point of thisyear) are also proposed and experimentedalong with baseline set.
The systemis optimized by several ways: tuningthe classification threshold, combiningwith WMT 2013 data, and refiningusing Feature Selection strategy on ourdevelopment set, before dealing with thetest set for submission.1 Introduction1.1 Overview of task 2 in WMT14This year WMT calls for methods which predictthe MT output quality at run-time, on both levels:sentence (Task 1) and word (Task 2).
Towardsa SMT system-independent and widely-appliedestimation, MT outputs are collected frommultiple translation means (machine and human),therefore all SMT specific settings (and theassociated features that could have been extractedfrom it) become unavailable.
This initiative putsmore challenges on participants, yet motivatesnumber of SMT-unconventional approaches andinspires the endeavors aiming at an ?EvaluationFor All?.We focus our effort on Task 2 (Word-level QE),where, unlike in WMT2013, participants arerequested to generate prediction labels for wordsin three variants:?
Binary: words are judged as Good (notranslation error), or Bad (need for editing).?
Level 1: the Good class is kept intact,whereas Bad one is further divided intosubcategories: Accuracy issue (the word doesnot accurately reflect the source text) andFluency issue (the word does not relate to theform or content of the target text).?
Multi-class: more detailed judgement, wherethe translation errors are further decomposedinto 16 labels based on MQM1metric.1.2 Related workWMT 2013 witnessed several attempts dealingwith this evaluation type in its first launch.
Hanet al.
(2013); Luong et al.
(2013) employed theConditional Random Fields (CRF) (Lafferty et al.,2001) model as their Machine Learning methodto address the problem as a sequence labelingtask.
Meanwhile, Bicici (2013) extended theglobal learning model by dynamic training withadaptive weight updates in the perceptron trainingalgorithm.
As far as prediction indicators areconcerned, Bicici (2013) proposed seven wordfeature types and found among them the ?commoncover links?
(the links that point from the leafnode containing this word to other leaf nodesin the same subtree of the syntactic tree) themost outstanding.
Han et al.
(2013) focusedonly on various n-gram combinations of targetwords.
Inheriting most of previously-recognizedfeatures, Luong et al.
(2013) integrated a numberof new indicators relying on graph topology,pseudo reference, syntactic behavior (constituentlabel, distance to the semantic tree root) andpolysemy characteristic.
Optimization endeavorswere also made to enhance the baseline, includingclassification threshold tuning, feature selectionand boosting technique (Luong et al., 2013).1http://www.qt21.eu/launchpad/content/training3351.3 Paper outlineThe rest of our paper is structured as follows:in the next section, we describe 2014 provideddata for Task 2, and the additional data usedto train the system.
Section 3 lists the entirefeature set, involving WMT 2013 set as well asa new feature proposed for this year.
Baselinesystem experiments and methods for optimizing itare furthered discussed in Section 4 and Section5 respectively.
Section 6 selects the mostoutstanding system for submission.
The lastsection summarizes the approach and opens newoutlook.2 Data and Supporting ResourcesFor English - Spanish language pair in Task 2,the organizers released two bilingual data sets:the training and the test ones.
The trainingset contains 1.957 MT outputs, in which eachtoken is annotated with one appropriate label.In the binary variant, the words are classifiedinto ?OK?
(no translation error) or ?BAD?
(editoperators needed) label.
Meanwhile, in the level1 variant, they belong to ?OK?, ?Accuracy?or ?Fluency?
(two latter ones are divided from?BAD?
label of the first subtask).
In the lastvariant, multi-class, beside ?Accuracy?
and?Fluency?
we have further 15 labels based onMQM metric: Terminology, Mistranslation,Omission, Addition, Untranslated, Style/register,Capitalization, Spelling, Punctuation,Typography, Morphology (word form),Part of speech, Agreement, Word order,Function words, Tense/aspect/mood, Grammarand Unintelligible.
The test set consists of 382sentences where all the labels accompanyingwords are hidden.
For optimizing parameters ofthe classifier, we extract last 200 sentences fromthe training set to form a development (dev) set.Besides, the Spanish - English corpus provided inWMT 2013 (total of 1087 tuples) is also exploitedto enrich our WMT 2014 system.
Unfortunately,2013 data can only help us in the binary variant,due to the discrepancy in training labels.
Somestatistics about each set can be found in Table 1.In addition, additional (MT-independent)resources are used for the feature extraction,including:?
Spanish and English Word Language Models(LM)?
Spanish and English POS Language Models?
Spanish - English 2013 MT systemOn the contrary, no specific MT setting is provided(e.g.
the code to re-run Moses system likeWMT 2013), leading to the unavailability of somecrucial resources, such as the N-best list andalignment information.
Coping with this, wefirstly thought of using the Moses ?ConstrainedDecoding?
option as a method to tie our (alreadyavailable) decoder?s output to the given targettranslations (this feature is supported by thelatest version of Moses (Koehn et al., 2007) in2013).
Our hope was that, by doing so, bothN-best list and alignment information would begenerated during decoding.
But the decoderfailed to output all translations (only 1/4 wasobtained) when the number of allowed unknownwords (-max-unknowns) was set as 0.
Switchingto non zero value for this option did not helpeither since, even if more outputs were generated,alignment information was biased in that casedue to additional/missing words in the obtainedMT output.
Ultimately, we decided to employGIZA++ toolkit (Och and Ney, 2003) to obtainat least the alignment information (and associatedfeatures) between source text and target MToutput.
However, no N-best list were extractednor available as in last year system.
Nevertheless,we tried to extract some features equivalent tolast year N-best features (details can be found inSection 3.2).3 Feature ExtractionIn this section, we briefly list out all thefeatures used in WMT 2013 (Luong et al.,2013) that were kept for this year, followedby some proposed features taking advantage ofthe provided resources and multiple translationsystem outputs (for a same source sentence).3.1 WMT13 features?
Source word features: all the source wordsthat align to the target one, represented inBIO2format.?
Source alignment context features: thecombinations of the target word and oneword before (left source context) or after(right source context) the source wordaligned to it.2http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/336Statistics WMT2014 WMT2013train dev test train dev test#segments 1757 200 382 753 50 284#words 40975 6436 9613 18435 1306 7827%G (OK) : %B (BAD) 67 : 33 58 : 42 - 70 : 30 77 : 23 75 : 25Table 1: Statistics of corpora used in LIG?s system.
We use the notion name+year to indicate the dataset.For instance, train14 stands for the training set of WMT14?
Target alignment context features: thecombinations of the source word and eachword in the window ?2 (two before, twoafter) of the target word.?
Backoff Behaviour: a score assigned to theword according to how many times the targetLanguage Model has to back-off in order toassign a probability to the word sequence, asdescribed in (Raybaud et al., 2011).?
Part-Of-Speech (POS) features (usingTreeTagger3toolkit): The target word?s POS;the source POS (POS of all source wordsaligned to it); bigram and trigram sequencesbetween its POS and the POS of previousand following words.?
Binary lexical features that indicate whetherthe word is a: stop word (based on the stopword list for target language), punctuationsymbol, proper name or numerical.?
Language Model (LM) features: the ?longesttarget n-gram length?
and ?longest sourcen-gram length?
(length of the longestsequence created by the current target(source aligned) word and its previous onesin the target (source) LM).
For example,with the target word wi: if the sequencewi?2wi?1wiappears in the target LM butthe sequence wi?3wi?2wi?1widoes not, then-gram value for wiwill be 3.?
The word?s constituent label and its depth inthe tree (or the distance between it and thetree root) obtained from the constituent treeas an output of the Berkeley parser (Petrovand Klein, 2007) (trained over a Spanishtreebank: AnCora4).?
Occurrence in Google Translate hypothesis:we check whether this target word appears in3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/4http://clic.ub.edu/corpus/en/ancorathe sentence generated by Google Translateengine for the same source.?
Polysemy Count: the number of senses ofeach word given its POS can be a reliableindicator for judging if it is the translationof a particular source word.
Here, weinvestigate the polysemy characteristic inboth target word and its aligned source word.For source word (English), the numberof senses can be counted by applying aPerl extension named Lingua::WordNet5,which provides functions for manipulatingthe WordNet database.
For target word(Spanish), we employ BabelNet6- amultilingual semantic network that workssimilarly to WordNet but covers moreEuropean languages, including Spanish.3.2 WMT14 additional features?
POS?s LM based features: we exploitthe Spanish and English LMs of POStag (provided as additional resources forthis year?s QE tasks) for calculating themaximum length of the sequences createdby the current target token?s POS and thoseof previous ones.
The same score for POSof aligned source word(s) is also computed.Besides, the back-off score for word?s POStag is also taken into consideration.
Actually,these feature types are listed in Section3.1 for target word, and we proposed thesimilar ones for POS tags.
In summary, threePOS LM?s new features are built, including:?longest target n-gram length?, ?longestsource n-gram length?
and back-off score forPOS tag.?
Word Occurrence in multiple translations:one novel point in this year?s shared taskis that the targets come from multiple MT5http://search.cpan.org/dist/Lingua-Wordnet/Wordnet.pm6http://babelnet.org337outputs (from systems or from humans) forthe same source sentences.
Obviously, onewould have a ?natural?
intuition that: theoccurrence of a word in all (or almost)systems implies a higher likelihood of beinga correct translation.
Relying on thisobservation, we add a new binary-valuefeature, telling whether the current tokencan be found in more than N% (in ourexperiments, we choose N = 50) outof all translations generated for the samesource sentence.
Here, in order to makethe judgments more accurate, we proposeseveral additional references besides thoseprovided in the corpora, coming from: (1)Google Translate system, (2) The baselineSMT engine provided for WMT2013 English- Spanish QE task.
These two MT outputsare added to the already available MT outputsof a given source sentence, before calculatingthe (above described) binary feature.4 Baseline Experiments andOptimization Strategies4.1 Machine Learning MethodMotivated by the idea of addressing WordConfidence Estimation (WCE) problem asa sequence labeling process, we employ theConditional Random Fields (CRFs) for ourmodel training, with WAPITI toolkit (Lavergneet al., 2010).
Let X = (x1, x2, .
.
.
, xN) be therandom variable over data sequence to be labeled,Y = (y1, y2, .
.
.
, yN) be the output sequenceobtained after the labeling task.
Basically, CRFcomputes the probability of the output sequenceY given the input sequence X by:p?
(Y |X) =1Z?
(X)exp{K?k=1?kFk(X,Y )}(1)where Fk(X,Y ) =?Tt=1fk(yt?1, yt, xt);{fk} (k = 1,K) is a set of feature functions;{?k} (k = 1,K) are the associated parametervalues; and Z?
(x) is the normalization function.In the training phase, we set the maximumnumber of iterations, the stop window size,and stop epsilon value at 200; 6 and 0.00005respectively.System Label Pr(%) Rc(%) F(%)BL(bin) OK 66.67 81.92 73.51Bad 60.69 41.92 49.58BL(L1) OK 63.86 82.83 72.12Accuracy 22.14 14.89 17.80Fluency 50.40 27.98 35.98BL(mult) OK 63.32 87.56 73.49Fluency 14.44 10.10 11.88Mistranslation 9.95 5.69 7.24Terminology 3.62 3.89 3.75Unintelligible 52.97 16.56 25.23Agreement 5.93 11.76 7.88Untranslated 5.65 7.76 6.53Punctuation 56.97 25.82 35.53BL+WMT OK 68.62 82.69 75.0113(bin) Bad 64.38 45.73 53.47Table 2: Average Pr, Rc and F for labelsof all-feature binary and multi-class systems,obtained on our WMT 2014 dev set (200sentences).
In BL(multi), classes with zero valuefor Pr or Rc will not be reported4.2 Experimental ClassifiersWe experiment with the following classifiers:?
BL(bin): all features (WMT14+WMT13)trained on train14 only, using binary labels(?OK?
and ?BAD?)?
BL(L1): all features trained on train14 only,using level 1 labels (?OK?, ?Accuracy?, and?Fluency?)?
BL(mult): all features trained on train14only, using 16 labels?
BL+WMT13(bin): all features trained ontrain14 + {train+dev+test}13, using binarylabels.System quality in Precision (Pr), Recall (Rc) andF score (F) are shown in Table 2.
It can beobserved that promising results are found in binaryvariant where both BL(bin) and BL+WMT(bin)are able to reach at least 50% F score in detectingerrors (BAD class), meanwhile the performancesin ?OK?
class go far beyond (73.51% and 75.01%respectively).
Interestingly, the combinationwith WMT13 data boosts the baseline predictioncapability in both labels: BL+WMT13(bin)outperforms BL(bin) in 1.10% ( 3.89%) for OK(BAD) label.
Nevertheless, level 1 and multi-classsystems maintain only good score for ?OK?
class.In addition, BL(mult) seems suffer seriouslyfrom its class imbalance, as well as the lack oftraining data for each, resulting in the inabilityof prediction for several among them (not all arereported in Table 2 ).3384.3 Decision threshold tuning for binary taskIn binary systems BL(bin) andBL+WMT13(bin), we run the classificationtask multiple times, corresponding to a decisionthreshold increase from 0.300 to 0.975 (step= 0.025).
The values of Precision (Pr), Recall(Rc) and F-score (F) for OK and BAD label aretracked along this threshold variation, allowingus to select the optimal threshold that yields thehighest Favg=F (OK)+F (BAD)2.
Figure 1 showsthat BL(bin) reaches the best performance at thethreshold value of 0.95, meanwhile the one forBL+WMT13(bin) is 0.75.
The latter threshold(0.75) has been used for the primary systemsubmitted.Figure 1: Decision threshold tuning on BL(bin)and BL+WMT2013(bin)4.4 Feature SelectionIn order to improve the preliminary scoresof all-feature systems, we conduct a featureselection which is based on the hypothesisthat some features may convey ?noise?
ratherthan ?information?
and might be the obstaclesweakening the other ones.
In order to preventthis drawback, we propose a method to filter thebest features based on the ?Sequential BackwardSelection?
algorithm7.
We start from the full set ofN features, and in each step sequentially removethe most useless one.
To do that, all subsets of(N-1) features are considered and the subset thatleads to the best performance gives us the weakestfeature (not involved in the considered set).
Thisprocedure is also called ?leave one out?
in theliterature.
Obviously, the discarded feature is notconsidered in the following steps.
We iterate the7http://research.cs.tamu.edu/prism/lectures/pr/pr l11.pdfprocess until there is only one remaining feature inthe set, and use the following score for comparingsystems: Favg(all) =Favg(OK)+Favg(BAD)2,where Favg(OK) and Favg(BAD) are theaveraged F scores for OK and BAD label,respectively, when threshold varies from 0.300 to0.975.
This strategy enables us to sort the featuresin descending order of importance, as displayedin Table 3.
Figure 2 shows the evolution ofthe performance as more and more features areremoved.
The feature selection is done from theBL+WMT2013(bin) system.We observe in Table 3 four valuable featureswhich appear in top 10 in both WMT13and WMT14 systems: Source POS, Occur inGoogle Translate, Left source context and Righttarget context.
Among our proposed features,?Occurrence in multiple systems?
is the mostoutstanding one with rank 3, ?longest target POSgram length?
plays an average role with rank 12,whereas ?longest source POS gram length?
ismuch less beneficial with the last position in thelist.
Figure 2 reveals that the optimal subset offeatures is the top 18 in Table 3, after discarding 6weakest ones.
This set will be used to train againthe classifiers in all subtasks and compare to thebaseline ones.Figure 2: The evolution of the performanceas more and more features are removed (fromBL+WMT2013(bin) system)5 SubmissionsAfter finishing the optimization process andcomparing systems, we select two mostout-standing ones (of each subtask) for thesubmission of this year?s shared task.
They arethe following:?
Binary variant: BL+WMT13(bin) andFS(bin) (feature selection from the samecorresponding system)?
Level 1 variant: BL(L1) and FS(L1) (featureselection from the same correspondingsystem)339Rank WMT2014 WMT20131 Target POS Source POS2 Longest target gram length Occur in Google Translate3 Occurrence in multiple systems Nodes4 Target word Target POS5 Occur in Google Translate WPP any6 Source POS Left source context7 Numeric Right target context8 Polysemy count (target) Numeric9 Left source context Polysemy count(target)10 Right Target context Punctuation11 Constituent label Stop word12 Longest target POS gram length Right source context13 Punctuation Target word14 Stop word Distance to root15 Number of occurrences Backoff behaviour16 Left target context Constituent label17 Backoff behaviour Proper name18 Polysemy count (source) Number of occurrences19 Source Word Min20 Proper Name Max21 Distance to root Left target context22 Longest source gram length Polysemy count (source)23 Right source context Longest target gram length24 Longest source POS gram length Longest source gram length25 Source WordTable 3: The rank of each feature (in term of usefulness) in WMT2014 and WMT2013 systems.
Thebold ones perform well in both cases.
Note that feature sets are not exactly the same for 2013 and 2014(see explanations in section 3).?
Multi-class variant: BL(mult) andFS(mult) (feature selection from thesame corresponding system)The official results can be seen in Table 4.
Thisyear, in order to appreciate the translation errordetection capability of WCE systems, the officialevaluation metric used for systems ranking is theaverage F score for all but the ?OK?
class.
Forthe non-binary variant, this average is weightedby the frequency of the class in the test data.Nevertheless, we find the F scores for ?OK?
classare also informative, since they reflect how goodour systems are in identifying correct translations.Therefore, both scores are reported in Table 4.6 Conclusion and perspectivesWe presented our preparation for this year?s sharedtask on QE at word level, for the English - Spanishlanguage pair.
The lack of some informationon MT system internals was a challenge.
Wemade efforts to maintain most of well-performingSystem F(?OK?)
(%) Average F(%)FS(bin) (primary) 74.0961 0.444735FS(L1) 73.9856 0.317814FS(mult) 76.6645 0.204953BL+WMT2013(bin) 74.6503 0.441074BL(L1) 74.0045 0.317894BL(mult) 76.6645 0.204953Table 4: The F scores for ?OK?
class and theaverage F scores for the remaining classes (officialWMT14 metric) , obtained on test set.2013 features, especially the source side ones,and propose some novel features based on thisyear?s corpus specificities, as well as combinethem with those of last year.
Generally, ourresults are not able to beat those in WMT13 forthe same language pair, yet still promising underthese constraints.
As future work, we are thinkingof using more efficiently the existing references(coming from provided translations and otherreliable systems) to obtain stronger indicators, as340well as examine other ML methods besides CRF.ReferencesErgun Bicici.
Referential translation machinesfor quality estimation.
In Proceedings ofthe Eighth Workshop on Statistical MachineTranslation, pages 343?351, Sofia, Bulgaria,August 2013.
Association for ComputationalLinguistics.
URL http://www.aclweb.org/anthology/W13-2242.Aaron Li-Feng Han, Yi Lu, Derek F. Wong,Lidia S. Chao, Liangye He, and Junwen Xing.Quality estimation for machine translationusing the joint method of evaluation criteriaand statistical modeling.
In Proceedings ofthe Eighth Workshop on Statistical MachineTranslation, pages 365?372, Sofia, Bulgaria,August 2013.
Association for ComputationalLinguistics.
URL http://www.aclweb.org/anthology/W13-2245.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen,Christine Moran, Richard Zens, Chris Dyer,Ondrej Bojar, Alexandra Constantin, andEvan Herbst.
Moses: Open source toolkit forstatistical machine translation.
In Proceedingsof the 45th Annual Meeting of the Associationfor Computational Linguistics, pages 177?180,Prague, Czech Republic, June 2007.John Lafferty, Andrew McCallum, andFernando Pereira.
Conditional randomfields: Probabilistic models for segmentinget labeling sequence data.
In Proceedings ofICML-01, pages 282?289, 2001.Thomas Lavergne, Olivier Capp?e, and Franc?oisYvon.
Practical very large scale crfs.
InProceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics,pages 504?513, 2010.Ngoc Quang Luong, Laurent Besacier, andBenjamin Lecouteux.
Word confidenceestimation and its integration in sentencequality estimation for machine translation.In Proceedings of the fifth internationalconference on knowledge and systemsengineering (KSE), Hanoi, Vietnam, October2013.Franz Josef Och and Hermann Ney.
A systematiccomparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51, 2003.Slav Petrov and Dan Klein.
Improved inferencefor unlexicalized parsing.
In Proceedings ofNAACL HLT 2007, pages 404?411, Rochester,NY, April 2007.S.
Raybaud, D. Langlois, and K.
Sma??
li.
?thissentence is wrong.?
detecting errors in machine- translated sentences.
In Machine Translation,pages 1?34, 2011.Matthew Snover, Nitin Madnani, Bonnie Dorr,and Richard Schwartz.
Terp system description.In MetricsMATR workshop at AMTA, 2008.341
