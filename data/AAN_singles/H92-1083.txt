PERFORMANCE OF SRI'S DECIPHER TM SPEECH RECOGNIT ION SYSTEMON DARPA'S CSR TASKHy Murveit, John Butzberger, and Mitch WeintraubSRI InternationalSpeech Research and Technology ProgramMenlo Park, CA, 940251.
ABSTRACTSRI has ported its DECIPHER TM speech recognition system fromDARPA's ATIS domain to DARPA's CSR domain (read and spon-taneous Wall Street Journal speech).
This paper describes whatneeded to be done to port DECIPHER TM, and reports experimentsperformed with the CSR task.The system was evaluated on the speaker-independent (SI) portionof DARPA's February 1992 "Dry-Run" WSJ0 test and achieved17.1% word error without verbalized punctuation (NVP) and16.6% error with verbalized punctuation (VP).
In addition, weincreased the amount of training data and reduced the VP errorrate to 12.9%.
This SI error rate (with a larger amount of trainingdata) equalled the best 600-training-sentence speaker-dependenterror rate reported for the February CSR evaluation.
Finally, thesystem was evaluated on the VP data using microphones unknownto the system instead of the training-set's Sennheiser microphoneand the error ate only inere~ased to 26.0%.ways; it includes speaker-dependent vs. speaker indepen-dent sections and sentences where the users were asked toverbalize the punctuation (VP) vs. those where they wereasked not to verbalize the punctuation (NVP).
There arealso a small number of recordings of spontaneous speechthat can be used in development and evaluation.The corpus and associated evelopment and evaluationmaterials were designed so that speech recognition systemsmay be evaluated in an open-vocabulary mode (none of thewords used in evaluation are known in advance by thespeech recognition system) or in a closed vocabulary mode(all the words in the test sets are given in advance).
Thereare suggested 5,000-word and 20,000-word open- andclosed-vocabulary language models that may be used fordevelopment and evaluation.
This paper discusses a pre-liminary evaluation of SRI's DECIPHER TM system usingread speech from the 5000-word closed-vocabulary taskswith verbalized and nonverbalized punctuation.2.
DECIPHER TMThe SRI has developed the DECIPHERm system, anHMM-based speaker-independent, continuous-speech rec-ognition system.
Several of DECIPHERr~'s attributes arediscussed in the references (Butzberger et al, \[1\]; Murveitet al, \[2\]).
Until recently, DECIPHERm's application hasbeen limited to DARPA's resource management task (Pal-let, \[3\]; Price et al, \[4\]), DARPA's ATIS task (Price, \[5\]),the Texas Instruments continuous-digit recognition task(Leonard, \[6\]), and other small vocabulary recognitiontasks.
This paper describes the application of DECIPHERrUto the task of recognizing words from a large-vocabularycorpus composed of primarily read-speech.3.
THE CSR TASKDoddington \[7\] gives a detailed description of DARPA'sCSR task and corpus.
Briefly, the CSR corpus* is composedof recordings of speakers reading passages from the WallStreet Journal newspaper.
The corpus is divided in many4.
PORTING DECIPHER TMTO THE CSR TASKSeveral types of data are needed to port DECIPHER~ to anew domain:?
A target vocabulary list?
A target language model?
Task-specific training data (optional)?
Pronunciations for all the words in the target vocab-ulary (mandatory) and for all the words in the train-ing data (optional)?
A backend which converts recognition output oactions in the domain (not applicable to the CSRtask).
*The current CSR corpus, designated WSJ0 is a pilotfor a large corpus to be collected in the future.4104.1.
CSR Vocabulary Lists and LanguageModelsDoug Paul at Lincoln Laboratories provided us with base-line vocabularies and language models for use in the Febru-ary 1992 CSR evaluation.
This included vocabularies forthe closed vocabulary 5,000 and 20,000-word tasks as wellas backed-off bigram language models for these tasks.Since we used backed-off bigrarns for our ATIS system, itwas straightforward touse the Lincoln language models aspart of the DECIPHERa~-CSR system.4.2.
CSR PronunciationsSRI maintains alist of words and pronunciations that haveassociated probabilities automatically estimated (Cohen etal., \[8\]).
However, a significant number of words in thespeaker-independent CSR training, development, and(closed vocabulary) test data were outside this list.
Becauseof the tight schedule for the CSR evaluation, SRI looked toDragon Systems which generously provided SRI and otherDARPA contractors with limited use of a pronunciationtable for all the words in the CSR task.
SRI combined itsintemal lexicon with portions of the Dragon pronunciationlist to generate a pronunciation table for the DECIPHERa~-CSR system.4.3.
CSR Training DataThe National Institute of Standards and Technology pro-vided to SRI several CDROMS containing training, devel-opment, and evaluation data for the February 1992 DARPACSR evaluation.
The data were recorded at SRI, MIT, andTI.
The baseline training conditions for the speaker-inde-pendent CSR task include 7240 sentences from 84 speak-ers, 3,586 sentences from 42 men and 3,654 sentences from42 women.5.2.
Results for a Simplified SystemOur strategy was to implement a system as quickly as possi-ble.
Thus we initially implemented a system using four vec-tor-quantized speech features with no cross-word acousticmodeling.
Performance ofthe system on our developmentset is described in the tables below.Table 1: Simple RecognizerSpeakerVerbalizedPunctuation%word errNonVerbalizedPunctuation%word err050 10.0 11.8053 14.0 17.6420 14.7 18.1421 11.9 17.9051 21.1 18.8052 20.7 20.222g 15.4 19.622h 20.8 13.0422 57.9 40.4423 15.0 24.620.1 Average 20.25.
PRELIMINARY CSR PERFORMANCE5.1.
Development DataWe have partitioned the speaker-independent CSRdevelop-ment data into four portions for the purpose of this study.Each set contains 100 sentences.
The respective sets aremale and female speakers using verbalized and nonverbal-ized punctuation.
There are 6 male speakers and 4 femalespeakers in the SI WSJ0 development data.The next section shows word recognition performance onthis development set using 5,000-word, closed-vocabularylanguage models with verbalized and nonverbalized bigramgrammars.
The perplexity of the verbalized punctuationsentences inthe development set is 90.The female speakers are those above the bold line in Table1.
Recognition speed on a Sun Sparcstation-2 was approxi-mately 40 times slower than real time (over 4 minutes/sen-tence) using a beam search and no fast match (our standardsmaller-vocabulary algorithm), although it was dominatedby paging time.A brief analysis of Speaker 422 shows that he speaks muchfaster than the other speakers which may contribute to thehigh error rate for his speech.5.3.
Full DECIPHER~-CSR PerformanceWe then tested a larger DECIPHER~ system on our VPdevelopment set.
That is, the previous ystem was extendedto model some cross-word acoustics, increased from four to411six spectral  features (second der ivat ives of  cepstra andenergy were added) and a t ied-mixture h idden Markermodel (HMM) replaced the vector-quantized HMM above.This resulted in a modest improvement asshown in theTable 2.Table 2: Full RecognizerVerbalizedSpeaker Punctuation%word err050 11.1053 11.7420 13.7421 11.0051 20.0052 14.222g 15.722h 14.9422 48.3423 13.0Average 17.46.
DRY-RUN EVALUATIONSubsequent tothe system development, above, we evalu-ated the "full recognizer' system on the February 1991 Dry-Run evaluation materials for speaker-independent sys ems.We achieved word error rates of 17.1% without VP and16.6% error rates with VP as measured by NIST.
*Table 3: Dry-Run Evaluation ResultsSpeaker427NonVerbalizedPunctuation%word err9.4VerbalizedPunctuation%word err9.0425 20.1 15.1zOO 14.4 16.7063 24.5 17.8426 10.2 10.8060 17.0 22.9061 12.3 13.622k 25.3 17.6221 17.8 12.4424 20.0 18.4Average 17.1 15.47.
OTHER MICROPHONE RESULTSThe WSJ0 corpus was col lected using two microphonessimultaneously recording the talker.
One was a SennheiserHMD-410 and the other was chosen randomly for eachspeaker from among a large group of microphones.
Such*The NIST error ates differ slightly (insigrtificantly)from our own measures (17.1% and 16.6%), however, tobe consistent with the other error ates reported in thispaper, we are using our internally measured error ratesin the tables.412dual recordings are available for the training, development,and evaluation materials.We chose to evaluate our full system on the "other-micro-phone" data without using other-microphone training data.The error rate increased only 62.3% when evaluating withother-microphone recordings vs. the Sennheiser recordings.In these tests, we configured our system exactly as for thestandard microphone evaluation, except hat we used SRI'snoise-robust front end (Erell and Weintraub, \[9,10\]; Mur-veit, et al, \[11\]) as the signal processing component.Table 4 summarizes the "other-microphone" evaluationresults.
Speaker 424's performance, where the error rateincreases 208.2% (from 18.4% to 56.7%) when using aShure SM91 microphone isa problem for our system.
How-ever, the microphone is not the sole source of the problem,since the performance of Speaker 427, with the samemicrophone, is only degraded 18.9% (from 9.0 to 10.7%).We suspect that the problem is due to a loud buzz in therecordings that is absent from the recordings of other speak-errs.8.
EXTRA TRAINING DATAWe suspected that he set of training data specified as thebaseline for the February 1992 Dry Run Evaluation wasinsufficient to adequately estimate the parameters of theDECIPHER TM system.
The baseline SI training conditioncontains approximately 7,240 from 84 speakers (half42male, 42 female).We used the SI and SD training and development data totrain the system to see if performance ould be improvedwith extra data.
However, to save time, we used only speechfrom male speakers to train and test the system.
Thus, thetraining data for the male system was increased from 3586sentences (42 male speakers) to 9109 sentences (53 malespeakers).
* The extra training data reduced the error rate byapproximately 20% as shown in Table 5.
*The number of speakers did not increase substantiallysince the bulk of the extra training data was taken fromthe speaker-dependent portion of the corpus.Table 4: Verbalized Punctuation Evaluation Results Using "Other Microphones"%word error %word errorSpeaker Microphone "other mic" Sennheiser mic %degradation427 Shure SM91 desktop 10.7 9.0 18.9425 Radio Shack Highball 21.4 15.1 41.8zOO Crown PCC 160 desktop 24.9 1627 49.129.4 17.8 65.2 063426Crown PCC160 desktopATT720 telephoneover local phone lines 12.1 10.8 12.0060 Crown PZM desktop 30.5 22.9 33.2061 Sony ECM-50PS lavaliere 18.8 13.6 38.222k Sony ECM-55 lavaliere 25.3 17.6 i 43.8221 Crown PCC160 desktop 22.8 12.4 83.9424 Shure SM91 desktop 56.7 18.4 208.2Average 25.0 15.4 62.3413'Fable 5: Evaluation Male Speakerswith Extra Training DataSpeaker Baseline Larger-SetTraining Training060 22.6 15.5061 13.6 8.222k 17.6 16.8221 12.4 11.3424 18.4 15.7426 10.8 9.8Average 15.8 12.9Interestingly, this reduced error rate equalled that forspeaker-dependent systems trained with 600 sentences perspeaker and tested with the same language model used here.However, speaker-dependent systems trained on 2000+sentences per speaker did perform significantly better thanthis system.9.
SUMMARYThis is a preliminary report demonstrating that the DECI-PHER TM speech recognition system was ported from a1,000-word task (ATIS) to a large vocabulary (5,000-word)task (DARPA's CSR task).
We have achieved word errorrates between of 16.6% and 17.1% as measured by NIST onDARPA's February 1992 Dry-Run WSJ0 evaluation whereno test words were outside the prescribed vocabulary.
Weevaluated using alternate microphone data and found thatthe error rate increased only by 62%.
Finally, by increasingthe amount of training data, we were able to achieve anerror rate that matched the error rates reported for this taskfrom 600 sentence/speaker speaker-dependent systems.This could not have been done without substantial supportfrom the rest of the DARPA community in the form ofspeech data, pronunciation tables, and language models.ACKNOWLEDGEMENTSWe gratefully acknowledge support for this work fromDARPA through Office of Naval Research ContractN00014-90-C-0085.
The Government has certain rights inthis material.
Any opinions, findings, and conclusions orrecommendations expressed inthis material are those of theauthors and do not necessarily reflect he views of the gov-ernment funding agencies.We would like to that Doug Paul at Lincoln Laboratories forproviding us with the Bigram language models used in thisstudy, and Dragon Systems for providing us with theDragon pronunciations described above.
We would also liketo thank the many people at various DARPA sites involvedin specifying, collecting, and transcribing the speech corpusused to gain, develop, and evaluate the system described.REFERENCES1.
Butzberger, J. H. Murveit, E. Shriberg, and P. Price, "Mod-eling Spontaneous Speech Effects in Large VocabularySpeech Recognition," DARPA SLS Workshop Proceed-ings, Feb 1992.2.
Murveit, H., J. Butzberger, and M. Weintraub, "SpeechRecognition i  SRI's Resource Management and ATISSystems," DARPA SLS Workshop, February 1991, pp.
94-100.3.
Pallet, D., "Benchmark Tests for DARPA Resource Man-agement Database Performance Evaluations," IEEEICASSP 1989, pp.
536-539.4.
Price, P., W.M.
Fisher, J. Bernstein, and D.S.
Pallet, "TheDARPA 1000-Word Resource Management Database forContinuous Speech Recognition," IEEE ICASSP 1988, pp.651-654.5.
Price, P., "Evaluation of SLS: the ATIS Domain," DARPASLS Workshop, June 1990, pp.
91-95.6.
Leonard, R.G., "A Database for Speaker-Independent DigitRecognition," 1EEE 1CASSP 1984, p. 42.117.
Doddington, G., "CSR Corpus Development," DARPASLS Workshop, Feb 1992.8.
Cohen, M., H. Murveit, J. Bernstein, P. Price, and M.Weintraub, "The DECIPHER TM Speech Recognition Sys-tem," IEEE ICASSP-90.9.
Erell, A., and M. Weintraub, "Spectral Estimation forNoise Robust Speech Recognition," DARPA SLS Work-shop October 89, pp.
319-324.10.
Erell, A., and M. Weintraub, "Recognition of NoisySpeech: Using Minimum-Mean Log-Spectral DistanceEstimation," DARPA SLS Workshop, June 1990, pp.
341-345.11.
Murveit, H., J. Butzberger, and M. Weintraub, "ReducedChannel Dependence for Speech Recognition", DARPASLS Workshop Proceedings, February 1992.414
