Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 5?6,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsStructured Belief Propagation for NLPMatthew R. Gormley Jason EisnerDepartment of Computer ScienceJohns Hopkins University, Baltimore, MD{mrg,jason}@cs.jhu.edu1 Tutorial OverviewStatistical natural language processing relies onprobabilistic models of linguistic structure.
Morecomplex models can help capture our intuitionsabout language, by adding linguistically meaning-ful interactions and latent variables.
However, in-ference and learning in the models we want oftenposes a serious computational challenge.Belief propagation (BP) and its variants pro-vide an attractive approximate solution, especiallyusing recent training methods.
These approachescan handle joint models of interacting compo-nents, are computationally efficient, and have ex-tended the state-of-the-art on a number of com-mon NLP tasks, including dependency parsing,modeling of morphological paradigms, CCG pars-ing, phrase extraction, semantic role labeling, andinformation extraction (Smith and Eisner, 2008;Dreyer and Eisner, 2009; Auli and Lopez, 2011;Burkett and Klein, 2012; Naradowsky et al., 2012;Stoyanov and Eisner, 2012).This tutorial delves into BP with an emphasis onrecent advances that enable state-of-the-art perfor-mance in a variety of tasks.
Our goal is to eluci-date how these approaches can easily be appliedto new problems.
We also cover the theory under-lying them.
Our target audience is researchers inhuman language technologies; we do not assumefamiliarity with BP.In the first three sections, we discuss applica-tions of BP to NLP problems, the basics of mod-eling with factor graphs and message passing, andthe theoretical underpinnings of ?what BP is do-ing?
and how it relates to other inference tech-niques.
In the second three sections, we coverkey extensions to the standard BP algorithm to en-able modeling of linguistic structure, efficient in-ference, and approximation-aware training.
Wesurvey a variety of software tools and introducea new software framework that incorporates manyof the modern approaches covered in this tutorial.2 Outline1.
Probabilistic Modeling [15 min., Eisner]?
Intro: Modeling with factor graphs?
Constituency and dependency parsing?
Joint CCG Parsing and supertagging?
Transliteration; Morphology?
Alignment; Phrase extraction?
Joint models for NLP; Semantic role label-ing; Targeted sentiment?
Variable-centric view of the world2.
Belief Propagation Basics [40 min., Eisner]?
Messages and beliefs?
Sum-product algorithm?
Relation to the forward-backward andViterbi algorithms?
BP as dynamic programming?
Acyclic vs. loopy graphs3.
Theory [25 min., Gormley]?
From sum-product to max-product?
From arc consistency to BP?
From Gibbs sampling to particle BP to BP?
Convergence properties?
Bethe free energy4.
Incorporating Structure into Factors and Vari-ables [30 min., Gormley]?
Embedding dynamic programs (e.g.inside-outside) within factors?
String-valued variables and finite state ma-chines5.
Message approximation and scheduling [20min., Eisner]?
Computing fewer messages?
Pruning messages?
Expectation Propagation and Penalized EP6.
Approximation-aware Training [30 min., Gorm-ley]?
Empirical risk minimization under approx-imations (ERMA)?
BP as a computational expression graph?
Automatic differentiation (AD)7.
Software [10 min., Gormley]53 InstructorsMatt Gormley is a PhD student at Johns HopkinsUniversity working with Mark Dredze and JasonEisner.
His current research focuses on joint mod-eling of multiple linguistic strata in learning set-tings where supervised resources are scarce.
Hehas authored papers in a variety of areas includingtopic modeling, global optimization, semantic rolelabeling, relation extraction, and grammar induc-tion.Jason Eisner is a Professor in Computer Sci-ence and Cognitive Science at Johns Hopkins Uni-versity, where he has received two school-wideawards for excellence in teaching.
His 90+ pa-pers have presented many models and algorithmsspanning numerous areas of NLP.
His goal is todevelop the probabilistic modeling, inference, andlearning techniques needed for a unified model ofall kinds of linguistic structure.
In particular, heand his students introduced structured belief prop-agation (which incorporates classical NLP modelsand their associated dynamic programming algo-rithms), as well as loss-calibrated training for usewith belief propagation.ReferencesMichael Auli and Adam Lopez.
2011.
A compari-son of loopy belief propagation and dual decompo-sition for integrated CCG supertagging and parsing.In Proceedings of ACL.David Burkett and Dan Klein.
2012.
Fast inference inphrase extraction models with belief propagation.
InProceedings of NAACL.Markus Dreyer and Jason Eisner.
2009.
Graphicalmodels over multiple strings.
In Proceedings ofEMNLP.Jason Naradowsky, Sebastian Riedel, and David Smith.2012.
Improving NLP through marginalizationof hidden syntactic structure.
In Proceedings ofEMNLP 2012.David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In Proceedings ofEMNLP.Veselin Stoyanov and Jason Eisner.
2012.
Minimum-risk training of approximate CRF-Based NLP sys-tems.
In Proceedings of NAACL-HLT.6
