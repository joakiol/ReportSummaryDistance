Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 259?264,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsPart-of-Speech Tagging using Conditional Random Fields: ExploitingSub-Label Dependencies for Improved AccuracyMiikka SilfverbergaTeemu RuokolainenbKrister Lind?naMikko KurimobaDepartment of Modern Languages, University of Helsinki,firstname.lastname@helsinki.fibDepartment of Signal Processing and Acoustics, Aalto University,firstname.lastname@aalto.fiAbstractWe discuss part-of-speech (POS) taggingin presence of large, fine-grained la-bel sets using conditional random fields(CRFs).
We propose improving taggingaccuracy by utilizing dependencies withinsub-components of the fine-grained labels.These sub-label dependencies are incor-porated into the CRF model via a (rela-tively) straightforward feature extractionscheme.
Experiments on five languagesshow that the approach can yield signifi-cant improvement in tagging accuracy incase the labels have sufficiently rich innerstructure.1 IntroductionWe discuss part-of-speech (POS) tagging usingthe well-known conditional random field (CRF)model introduced originally by Lafferty et al(2001).
Our focus is on scenarios, in which thePOS labels have a rich inner structure.
For exam-ple, considerPRON+1SG V+NON3SG+PRES N+SGI like ham,where the compound labels PRON+1SG,V+NON3SG+PRES, and N+SG stand for pro-noun first person singular, verb non-third singularpresent tense, and noun singular, respectively.Fine-grained labels occur frequently in mor-phologically complex languages (Erjavec, 2010;Haverinen et al, 2013).We propose improving tagging accuracy by uti-lizing dependencies within the sub-labels (PRON,1SG, V, NON3SG, N, and SG in the above ex-ample) of the compound labels.
From a technicalperspective, we accomplish this by making use ofthe fundamental ability of the CRFs to incorporatearbitrarily defined feature functions.
The newly-defined features are expected to alleviate data spar-sity problems caused by the fine-grained labels.Despite the (relative) simplicity of the approach,we are unaware of previous work exploiting thesub-labels to the extent presented here.We present experiments on five languages (En-glish, Finnish, Czech, Estonian, and Romanian)with varying POS annotation granularity.
By uti-lizing the sub-labels, we gain significant improve-ment in model accuracy given a sufficiently fine-grained label set.
Moreover, our results indi-cate that exploiting the sub-labels can yield largerimprovements in tagging compared to increasingmodel order.The rest of the paper is organized as follows.Section 2 describes the methodology.
Experimen-tal setup and results are presented in Section 3.Section 4 discusses related work.
Lastly, we pro-vide conclusions on the work in Section 5.2 Methods2.1 Conditional Random FieldsThe (unnormalized) CRF model (Lafferty et al,2001) for a sentence x = (x1, .
.
.
, x|x|) and a POSsequence y = (y1, .
.
.
, y|x|) is defined asp (y |x;w) ?|x|?i=nexp(w??
(yi?n, .
.
.
, yi, x, i)),(1)where n denotes the model order,w the model pa-rameter vector, and ?
the feature extraction func-tion.
We denote the tag set as Y , that is, yi?
Yfor i ?
1 .
.
.
|x|.2.2 Baseline Feature SetWe first describe our baseline feature set{?j(yi?1, yi, x, i)}|?|j=1by defining emission andtransition features.
The emission feature set as-sociates properties of the sentence position i with259the corresponding label as{?j(x, i)1(yi= y?i) | j ?
1 .
.
.
|X | , ?y?i?
Y} ,(2)where the function 1(q) returns one if and only ifthe proposition q is true and zero otherwise, that is1(yi= y?i) ={1 if yi= y?i0 otherwise, (3)and X = {?j(x, i)}|X |j=1is the set of functionscharacterizing the word position i.
Following theclassic work of Ratnaparkhi (1996), our X com-prises simple binary functions:1.
Bias (always active irrespective of input).2.
Word forms xi?2, .
.
.
, xi+2.3.
Prefixes and suffixes of the word form xiupto length ?suf= 4.4.
If the word form xicontains (one or more)capital letter, hyphen, dash, or digit.Binary functions have a return value of either zero(inactive) or one (active).
Meanwhile, the transi-tion features{1(yi?k= y?i?k) .
.
.1(yi= y?i) |y?i?k, .
.
.
, y?i?
Y ,?k ?
1 .
.
.
n} (4)capture dependencies between adjacent labels ir-respective of the input x.2.2.1 Expanded Feature Set LeveragingSub-Label DependenciesThe baseline feature set described above can yielda high tagging accuracy given a conveniently sim-ple label set, exemplified by the tagging resultsof Collins (2002) on the Penn Treebank (Mar-cus et al, 1993).
(Note that conditional randomfields correspond to discriminatively trained hid-den Markov models and Collins (2002) employsthe latter terminology.)
However, it does to someextent overlook some beneficial dependency infor-mation in case the labels have a rich sub-structure.In what follows, we describe expanded feature setswhich explicitly model the sub-label dependen-cies.We begin by defining a function P(yi) whichpartitions any label yiinto its sub-label compo-nents and returns them in an unordered set.
Forexample, we could define P(PRON+1+SG) ={PRON, 1, SG}.
(Label partitions employed inthe experiments are described in Section 3.2.)
Wedenote the set of all sub-label components as S.Subsequently, instead of defining only (2), weadditionally associate the feature functionsX withall sub-labels s ?
S by defining{?j(x, i)1(s ?
P(yi)) | ?j ?
1 .
.
.
|X | ,?s ?
S} ,(5)where 1(s ?
P(yi)) returns one in case s is inP(yi) and zero otherwise.
Second, we exploit sub-label transitions using features{1(si?k?
P(yi?k)) .
.
.1(si?
P(yi)) |?si?k, .
.
.
, si?
S ,?k ?
1 .
.
.m} .
(6)Note that we define the sub-label transitions upto order m, 1 ?
m ?
n, that is, an nth-orderCRF model is not obliged to utilize sub-label tran-sitions all the way up to order n. This is be-cause employing high-order sub-label transitionsmay potentially cause overfitting to training datadue to substantially increased number of features(equivalent to the number of model parameters,|w| = |?|).
For example, in a second-order(n = 2) model, it might be beneficial to em-ploy the sub-label emission feature set (5) andfirst-order sub-label transitions while discardingsecond-order sub-label transitions.
(See the exper-imental results presented in Section 3.
)In the remainder of this paper, we use the fol-lowing notations.1.
A standard CRF model incorporating (2) and(4) is denoted as CRF(n,-).2.
A CRF model incorporating (2), (4), and (5)is denoted as CRF(n,0).3.
A CRF model incorporating (2), (4), (5), and(6) is denoted as CRF(n,m).2.3 On Linguistic IntuitionThis section aims to provide some intuition on thetypes of linguistic phenomena that can be capturedby the expanded feature set.
To this end, we con-sider an example on the plural number in Finnish.First, consider the plural nominative word formkissat (cats) where the plural number is denotedby the 1-suffix -t. Then, by employing the features(2), the suffix -t is associated solely with the com-pound label NOMINATIVE+PLURAL.
However,by incorporating the expanded feature set (5), -t260will also be associated to the sub-label PLURAL.This can be useful because, in Finnish, also adjec-tives and numerals are inflected according to num-ber and denote the plural number with the suffix-t (Hakulinen et al, 2004, ?79).
Therefore, onecan exploit -t to predict the plural number also inwords such as mustat (plural of black) with a com-pound analysis ADJECTIVE+PLURAL.Second, consider the number agreement (con-gruence).
For example, in the sentence fragmentmustat kissat juoksevat (black cats are running),the words mustat and kissat share the plural num-ber.
In other words, the analyses of both mustatand kissat are required to contain the sub-labelPLURAL.
This short-span dependency betweensub-labels will be captured by a first-order sub-label transition feature included in (6).Lastly, we note that the feature expansion sets(5) and (6) will, naturally, capture any short-spandependencies within the sub-labels irrespective ifthe dependencies have a clear linguistic interpre-tation or not.3 Experiments3.1 DataFor a quick overview of the data sets, see Table 1.Penn Treebank.
The English Penn Treebank(Marcus et al, 1993) is divided into 25 sectionsof newswire text extracted from the Wall StreetJournal.
We split the data into training, develop-ment, and test sets using the sections 0-18, 19-21,and 22-24, according to the standardly applied di-vision introduced by Collins (2002).Turku Depedency Treebank.
The FinnishTurku Depedendency Treebank (Haverinen et al,2013) contains text from 10 different domains.The treebank does not have default partition totraining and test sets.
Therefore, from each 10consecutive sentences, we assign the 9th and 10thto the development set and the test set, respec-tively.
The remaining sentences are assigned tothe training set.Multext-East.
The third data we consider is themultilingual Multext-East (Erjavec, 2010) corpus,from which we utilize the Czech, Estonian and Ro-manian sections.
The corpus corresponds to trans-lations of the novel 1984 by George Orwell.
Weapply the same data splits as for Turku Depen-dency Treebank.lang.
train.
dev.
test tags train.
tagsEng 38,219 5,527 5,462 45 45Rom 5,216 652 652 405 391Est 5,183 648 647 413 408Cze 5,402 675 675 955 908Fin 5,043 630 630 2,355 2,141Table 1: Overview on data.
The training (train.
),development (dev.)
and test set sizes are given insentences.
The columns titled tags and train.
tagscorrespond to total number of tags in the data setand number of tags in the training set, respectively.3.2 Label PartitionsThis section describes the employed compound la-bel splits.
The label splits for all data sets are sub-mitted as data file attachments.
All the splits areperformed a priori to model learning, that is, wedo not try to optimize them on the developmentsets.The POS labels in the Penn Treebank are splitin a way which captures relevant inflectional cat-egories, such as tense and number.
Consider, forexample, the split for the present tense third sin-gular verb label P(VBZ) = {VB, Z}.In the Turku Dependency Treebank, eachmorphological tag consists of sub-labels mark-ing word-class, relevant inflectional categories,and their respective values.
Each inflec-tional category, such as case or tense, com-bined with its value, such as nominative orpresent, constitutes one sub-label.
Consider,for example, the split for the singular, adessivenoun P(N+CASE_ADE+NUM_SG) = {POS_N,CASE_ADE, NUM_SG}.The labeling scheme employed in the Multext-East data set represents a considerably differentannotation approach compared to the Penn andTurku Treebanks.
Each morphological analysis isa sequence of feature markers, for example Pw3?r.
The first feature marker (P) denotes word classand the rest (w, 3, and r) encode values of inflec-tional categories relevant for that word class.
Afeature marker may correspond to several differ-ent values depending on word class and its posi-tion in the analysis.
Therefore it becomes ratherdifficult to split the labels into similar pairs of in-flectional category and value as we are able to dofor the Turku Dependency Treebank.
Since the in-terpretation of a feature marker depends on its po-sition in the analysis and the word class, the mark-ers have to be numbered and appended with the261word class marker.
For example, consider the splitP(Pw3?r) = {0 : P, 1 : Pw, 2 : P3, 5 : Pr}.3.3 CRF Model SpecificationWe perform experiments using first-order andsecond-order CRFs with zeroth-order and first-order sub-label features.
Using the notationintroduced in Section 2, the employed mod-els are CRF(1,-), CRF(1,1), CRF(2,-), CRF(2,0),and CRF(2,1).
We do not report results us-ing CRF(2,2) since, based on preliminary exper-iments, this model overfits on all languages.The CRF model parameters are estimated usingthe averaged perceptron algorithm (Collins, 2002).The model parameters are initialized with a zerovector.
We evaluate the latest averaged parameterson the held-out development set after each passover the training data and terminate training if noimprovement in accuracy is obtained during threelast passes.
The best-performing parameters arethen applied on the test instances.We accelerate the perceptron learning usingbeam search (Zhang and Clark, 2011).
The beamwidth, b, is optimized separately for each lan-guage on the development sets by considering b =1, 2, 4, 8, 16, 32, 64, 128 until the model accuracydoes not improve by at least 0.01 (absolute).Development and test instances are decoded us-ing Viterbi search in combination with the tag dic-tionary approach of Ratnaparkhi (1996).
In thisapproach, candidate tags for known word formsare limited to those observed in the training data.Meanwhile, word forms that were unseen duringtraining consider the full label set.3.4 Software and HardwareThe experiments are run on a standard desktopcomputer (Intel Xeon E5450 with 3.00 GHz and64 GB of memory).
The methods discussed inSection 2 are implemented in C++.3.5 ResultsThe obtained tagging accuracies and trainingtimes are presented in Table 2.
The times in-clude running the averaged perceptron algorithmand evaluation of the development sets.
The col-umn labeled it.
corresponds to the number ofpasses over the training data made by the percep-tron algorithm before termination.
We summarizethe results as follows.First, compared to standard feature extractionapproach, employing the sub-label transition fea-tures resulted in improved accuracy on all lan-guages apart from English.
The differences werestatistically significant on Czech, Estonian, andFinnish.
(We establish statistical significance(with confidence level 0.95) using the standard 1-sided Wilcoxon signed-rank test performed on 10randomly divided, non-overlapping subsets of thecomplete test sets.)
This results supports the in-tuition that the sub-label features should be mostuseful in presence of large, fine-grained label sets,in which case the learning is most affected by datasparsity.Second, on all languages apart from English,employing a first-order model with sub-label fea-tures yielded higher accuracy compared to asecond-order model with standard features.
Thedifferences were again statistically significant onCzech, Estonian, and Finnish.
This result suggeststhat, compared to increasing model order, exploit-ing the sub-label dependencies can be a preferableapproach to improve the tagging accuracy.Third, applying the expanded feature set in-evitably causes some increase in the computa-tional cost of model estimation.
However, asshown by the running times, this increase is notprohibitive.4 Related WorkIn this section, we compare the approach pre-sented in Section 2 to two prior systems which at-tempt to utilize sub-label dependencies in a similarmanner.Smith et al (2005) use a CRF-based systemfor tagging Czech, in which they utilize expandedemission features similar to our (5).
However, theydo not utilize the full expanded transition features(6).
More specifically, instead of utilizing a sin-gle chain as in our approach, Smith et al employfive parallel structured chains.
One of the chainsmodels the sequence of word-class labels such asnoun and adjective.
The other four chains modelgender, number, case, and lemma sequences, re-spectively.
Therefore, in contrast to our approach,their system does not capture cross-dependenciesbetween inflectional categories, such as the de-pendence between the word-class and case of ad-jacent words.
Unsurprisingly, Smith et al failto achieve improvement over a generative HMM-based POS tagger of Haji?c (2001).
Meanwhile,our system outperforms the generative trigram tag-ger HunPos (Hal?csy et al, 2007) which is an im-262model it.
time (min) acc.
OOV.EnglishCRF(1, -) 8 9 97.04 88.65CRF(1, 0) 6 17 97.02 88.44CRF(1, 1) 8 22 97.02 88.82CRF(2, -) 9 15 97.18 88.82CRF(2, 0) 11 36 97.17 89.23CRF(2, 1) 8 27 97.15 89.04RomanianCRF(1, -) 14 29 97.03 85.01CRF(1, 0) 13 68 96.96 84.59CRF(1, 1) 16 146 97.24 85.94CRF(2, -) 7 19 97.08 85.21CRF(2, 0) 18 99 97.02 85.42CRF(2, 1) 12 118 97.29 86.25EstonianCRF(1, -) 15 28 93.39 78.66CRF(1, 0) 17 66 93.81 80.44CRF(1, 1) 13 129 93.77 79.37CRF(2, -) 15 30 93.48 77.13CRF(2, 0) 13 53 93.78 79.60CRF(2, 1) 16 105 94.01 79.53CzechCRF(1, -) 6 28 89.28 70.90CRF(1, 0) 10 112 89.94 74.44CRF(1, 1) 10 365 90.78 76.83CRF(2, -) 19 91 89.81 72.44CRF(2, 0) 13 203 90.35 76.37CRF(2, 1) 24 936 91.00 77.75FinnishCRF(1, -) 10 80 87.37 59.29CRF(1, 0) 13 249 88.58 63.46CRF(1, 1) 12 474 88.41 62.63CRF(2, -) 11 106 86.74 56.96CRF(2, 0) 13 272 88.52 63.46CRF(2, 1) 12 331 88.68 63.62Table 2: Results.proved open-source implementation of the well-known TnT tagger of Brants (2000).
The obtainedHunPos results are presented in Table 3.Eng Rom Est Cze FinHunPos 96.58 96.96 92.76 89.57 85.77Table 3: Results using a generative HMM-basedHunPos tagger of Halacsy et al (2007).Ceaus?u (2006) uses a maximum entropyMarkov model (MEMM) based system for tag-ging Romanian which utilizes transitional behav-ior between sub-labels similarly to our feature set(6).
However, in addition to ignoring the most in-formative emission-type features (5), Ceaus?u em-beds the MEMMs into the tiered tagging frame-work of Tufis (1999).
In tiered tagging, the fullmorphological analyses are mapped into a coarsertag set and a tagger is trained for this reduced tagset.
Subsequent to decoding, the coarser tags aremapped into the original fine-grained morpholog-ical analyses.
There are several problems associ-ated with this tiered tagging approach.
First, thesuccess of the approach is highly dependent on awell designed coarse label set.
Consequently, itrequires intimate knowledge of the tag set and lan-guage.
Meanwhile, our model can be set up withrelatively little prior knowledge of the languageor the tagging scheme (see Section 3.2).
More-over, a conversion to a coarser label set is neces-sarily lossy (at least for OOV words) and poten-tially results in reduced accuracy since recoveringthe original fine-grained tags from the coarse tagsmay induce errors.
Indeed, the accuracy 96.56, re-ported by Ceaus?u on the Romanian section of theMultext-East data set, is substantially lower thanthe accuracy 97.29 we obtain.
These accuracieswere obtained using identical sized training andtest sets (although direct comparison is impossiblebecause Ceaus?u uses a non-documented randomsplit).5 ConclusionsWe studied improving the accuracy of CRF-basedPOS tagging by exploiting sub-label dependencystructure.
The dependencies were included in theCRF model using a relatively straightforward fea-ture expansion scheme.
Experiments on five lan-guages showed that the approach can yield signif-icant improvement in tagging accuracy given suf-ficiently fine-grained label sets.In future work, we aim to perform a morefine-grained error analysis to gain a better under-standing where the improvement in accuracy takesplace.
One could also attempt to optimize thecompound label splits to maximize prediction ac-curacy instead of applying a priori partitions.AcknowledgementsThis work was financially supported by Langnet(Finnish doctoral programme in language studies)and the Academy of Finland under the grant no251170 (Finnish Centre of Excellence Program(2012-2017)).
We would like to thank the anony-mous reviewers for their useful comments.263ReferencesThorsten Brants.
2000.
Tnt: A statistical part-of-speech tagger.
In Proceedings of the Sixth Con-ference on Applied Natural Language Processing,pages 224?231.A.
Ceausu.
2006.
Maximum entropy tiered tagging.In The 11th ESSLI Student session, pages 173?179.Michael Collins.
2002.
Discriminative training meth-ods for Hidden Markov Models: Theory and experi-ments with perceptron algorithms.
In Proceedingsof the 2002 Conference on Empirical Methods inNatural Language Processing (EMNLP 2002), vol-ume 10, pages 1?8.Toma?z Erjavec.
2010.
Multext-east version 4: Multi-lingual morphosyntactic specifications, lexicons andcorpora.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC?10).Jan Haji?c, Pavel Krbec, Pavel Kv?eto?n, Karel Oliva, andVladim?r Petkevi?c.
2001.
Serial combination ofrules and statistics: A case study in czech tagging.In Proceedings of the 39th Annual Meeting on Asso-ciation for Computational Linguistics, pages 268?275.Auli Hakulinen, Maria Vilkuna, Riitta Korhonen, VesaKoivisto, Tarja Riitta Heinonen, and Irja Alho.2004.
Iso suomen kielioppi.
Suomalaisen Kirjal-lisuuden Seura, Helsinki, Finland.P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz.2007.
Hunpos: An open source trigram tagger.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 209?212.Katri Haverinen, Jenna Nyblom, Timo Viljanen,Veronika Laippala, Samuel Kohonen, Anna Missil?,Stina Ojala, Tapio Salakoski, and Filip Ginter.
2013.Building the essential resources for Finnish: theTurku Dependency Treebank.
Language Resourcesand Evaluation.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth In-ternational Conference on Machine Learning, pages282?289.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of en-glish: The penn treebank.
Computational linguis-tics, 19(2):313?330.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedingsof the conference on empirical methods in natu-ral language processing, volume 1, pages 133?142.Philadelphia, PA.Noah A. Smith, David A. Smith, and Roy W. Tromble.2005.
Context-based morphological disambiguationwith random fields.
In Proceedings of the Confer-ence on Human Language Technology and Empiri-cal Methods in Natural Language Processing, pages475?482.Dan Tufis.
1999.
Tiered tagging and combined lan-guage models classifiers.
In Proceedings of the Sec-ond International Workshop on Text, Speech and Di-alogue, pages 28?33.Yue Zhang and Stephen Clark.
2011.
Syntactic pro-cessing using the generalized perceptron and beamsearch.
Computational Linguistics, 37(1):105?151.264
