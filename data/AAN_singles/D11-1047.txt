Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 508?518,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsEfficient retrieval of tree translation examples forSyntax-Based Machine TranslationFabien CromieresGraduate School of InformaticsKyoto UniversityKyoto, Japanfabien@nlp.kuee.kyoto-u.ac.jpSadao KurohashiGraduate School of InformaticsKyoto UniversityKyoto, Japankuro@i.kyoto-u.ac.jpAbstractWe propose an algorithm allowing to effi-ciently retrieve example treelets in a parsedtree database in order to allow on-the-fly ex-traction of syntactic translation rules.
We alsopropose improvements of this algorithm al-lowing several kinds of flexible matchings.1 IntroductionThe popular Example-Based (EBMT) and StatisticalMachine Translation (SMT) paradigms make use ofthe translation examples provided by a parallel bilin-gual corpus to produce new translations.
Most ofthese translation systems process the example datain a similar way: The parallel sentences are firstword-aligned.
Then, translation rules are extractedfrom these aligned sentences.
Finally, the transla-tion rules are used in a decoding step to translatesentences.
We use the term translation rule in a verybroad sense here, as it may refer to substring pairs asin (Koehn et al, 2003), synchronous grammar rulesas in (Chiang, 2007) or treelet pairs as in (Quirk etal., 2005; Nakazawa and Kurohashi, 2008).As the size of bilingual corpus grow larger, thenumber of translation rules to be stored can easilybecome unmanageable.
As a solution to this prob-lem in the context of phrase-based Machine Transla-tion, (Callison-Burch et al, 2005) proposed to pre-align the example corpora, but delay the rule extrac-tion to the decoding stage.
They showed that usingSuffix Arrays, it was possible to efficiently retrieveall sentences containing substrings of the sentenceto be translated, and thus extract the needed trans-lation rules on-the-fly.
(Lopez, 2007) proposed anextension of this method for retrieving discontinu-ous substrings, making it suitable for systems suchas (Chiang, 2007).In this paper, we propose a method to apply thesame idea to Syntax-Based SMT and EBMT (Quirket al, 2005; Mi et al, 2008; Nakazawa and Kuro-hashi, 2008).
Since Syntax-Based systems usuallywork with the parse trees of the source-side sen-tences, we will need to be able to retrieve effi-ciently examples trees from fragments (treelets) ofthe parse tree of the sentence we want to translate.We will also propose extensions of this method al-lowing more flexible matchings.2 Overview of the method2.1 Treelet retrievalWe first formalize the setting of this chapter by pro-viding some definitions.Definition 2.1 (Treelets).
A treelet is a connectedsubgraph of a tree.
A treelet T1 is a subtreelet of an-other treelet T2 if T1 is itself a connected subgraphof T2.
We note |T| the number of nodes in a treelet.If |T| = 1, T is called an elementary treelet.
A lin-ear treelet is a treelet whose nodes have at most 1child.
A subtree rooted at node n of a tree T is atreelet containing all nodes descendants of n.Definition 2.2 (Sub- and Supertreelets).
If T1 is asubtreelet of T2 and |T1| = |T2| ?
1, we call T1an immediate subtreelet of T2.
Reciprocally, T2 isan (immediate) supertreelet of T1.
Furthermore, ifT2 and T1 are rooted at the same node in the originaltree, we say that T2 is a descending supertreelet ofT1.
Otherwise it is an ascending supertreelet of T1.508In treelet retrieval, we are given a certain treelettype and want to find all of the tokens of this type inthe database DB.
Each token of a given treelet typewill be identified by a mapping from the node of thetreelet type to the nodes of the treelet token in thedatabase.Definition 2.3 (Matching).
Given a treelet T and atree database DB, a matching of T in DB is a func-tion M that associate the treelet T to a tree T inDB and every node of T to nodes of T in such away that: ?n ?
T, label(M(n)) = label(n) and?
(n1, n2) ?
T s.t n2 is a child of n1, M(n2) is achild of M(n1).In the common case where the siblings of a treeare ordered, a matching must satisfy the additionalrestriction: ?n1, n2 ?
T, n1 <s n1 ?
M(n1) <sM(n1), where <s is the partial order relation be-tween nodes meaning ?is a sibling and to the left of?We note occ(T) (for ?occurrences of T ?)
the setof all possible matchings from T to DB.
We willcall computing T the task of finding occ(T).
If|occ(T)| = 0, we call T an empty treelet.
Computinga query tree TQ means computing all of its treelets.Definition 2.4 (Notations).
Although treelets arethemselves trees, we will use the word treelet toemphasize they are a subpart of a bigger tree.
Wewill note T a treelet, and T a tree.
TQ is the querytree we want to compute.
DB will refer to the set oftrees in our database.
We will use a bracket notationto describe trees or treelets.
Thus ?a(b c d(e))?
isthe tree at the bottom of figure 2.2.2 General approachThere exists already a large body of research abouttree pattern matching (Dubiner et al, 1994; Brunoet al, 2002).
However, our problem is quite differ-ent from finding the tokens of a given treelet in adatabase.
We actually want to find all the tokens ofall of the treelets of a given query tree.
The querytree itself is unlikely to appear in full even once inthe database.
In this respect, our approach will havemany similarities with (Callison-Burch et al, 2005)and (Lopez, 2007), and can be seen as an extensionof these works.The basis of the method in (Lopez, 2007) is tolook for the occurrences of continuous substrings us-ing a Suffix Array, and then intersect them to find theoccurrences of discontinuous substrings.
We willhave a similar approach with two variants.
The firstvariant consists in using an adaptation of the con-cept of suffix arrays to trees, which we will call Path-To-Root Arrays (section 3.4), that allows us to findefficiently the set of occurrences of a linear treelet.Occurrences of non-linear treelets can then be com-puted by intersection.
The second variant is to use aninverted index (section 3.5).
Then the occurrences ofall treelets, even the linear treelets, are computed byintersection.The main additional difficulty in considering treesinstead of strings is that while a string has aquadratic number of continuous substrings, a treehas in general an exponential number of treelets (eg.several trillion for the dependency tree of a 70 wordssentence).
There is also an exponential number ofdiscontinuous substrings, but (Lopez, 2007) onlyconsider substrings of bounded size, limiting thisproblem.
We will not try to bound the size of treeletsretrieved.
It is therefore crucial to avoid computingthe occurrences of treelets that have no occurrencesin the database, and also to eliminate as much redun-dant calculation as is possible.Lopez proposes to use Prefix Trees for avoidingany redundant or useless computation.
We will usea similar idea but with an hypergraph that we willcall ?computation hypergraph?
(section 3.2).
Thishypergraph will not only fit the same role as the Pre-fix Tree of (Lopez, 2007), but also will allow us toeasily implement different search strategies for flex-ible search (section 6).2.3 Representing positionsWhether we use a Path-to-Root Array or an invertedindex, we will need a compact way to represent theposition of a node in a tree.
It is straightforward todefine such a position for strings, but slightly less fortrees.
Especially, if we consider ordered trees, wewill want to be able to compare the relative locationof the nodes by comparing their positions.The simplest possibility is to use an integer corre-sponding to the rank of the node in a in-order depth-first traversal of the tree.
It is then easy, for twonodes b and c, children of a parent node a, to checkif b is on the left of c, or on the left of a, for example.A more advanced possibility is to use a represen-tation inspired from (Zhang et al, 2001), in which509the position of a node is a tuple consisting of its rankin a preorder (ie.
children last) and a postorder (chil-dren first) depth-first traversal, and of its distance tothe root.
This allows to test easily whether a nodeis an ancestor of another, and their distance to eachother.
This allows in turn to compute by intersec-tion the occurrences of discontinuous treelets, muchlike what is done in (Lopez, 2007) for discontinuousstrings.
This is discussed in section 7.2.3 Computing treelets incrementallyWe describe here in more details how the treelets canbe efficiently computed incrementally.3.1 Dependence of treelet computationLet us first define how it is possible to compute atreelet from two of its subtreelets.
Let us considera treelet T and two treelets T1 and T2 such thatT = T1 ?
T2, where, in the equality and the union,the treelet are seen as the set of their nodes.
Thereare two possibilities.
If T1?T2 = ?, then the root ofT1 is a child of a node of T2 or vice-versa.
We thensay that T1 and T2 form a disjoint coverage (abbrevi-ated as D-coverage) of T. If T1?T2 6= ?, we will saythat T1 and T2 form an overlapping coverage (abbre-viated as O-coverage) of T.Given two treelets T1 and T2 forming a cover-age of T, we can compute occ(T) from occ(T1) andocc(T2) by combining their matchings.Definition 3.1 (compatibility for O-coverage).
LetT be a treelet of TQ.
Let T1 and T2 be 2 treeletsforming a O-coverage of T. Let M1 ?
occ(T1)and M2 ?
occ(T2).
M1 and M2 are compat-ible if and only if M1|T1?T2 = M2|T1?T2 andI(M1|T1\T2) ?
I(M2|T2\T1) = ?.In the definition above, |S is the restriction of a func-tion to a set S and I is the image set of a function.If the children of a tree are ordered, we must addthe additional restriction: ?
(n1, n2) ?
(T1 \ T2) ?
(T2 \ T1), n1 <s n2 ?M1(n1) <s M2(n2).Definition 3.2 (compatibility for D-coverage).
LetT1 and T2 be 2 treelets forming a D-coverageof T. Let?s suppose that the root n2 of T2 is achild of node n1 of T1.
Let M1 ?
occ(T1) andM2 ?
occ(T2).
M1 and M2 are compatible if andonly if M2(n2) is a child of M1(n1).Figure 1: A computing hypergraph for ?a(b c)?.Definition 3.3 (intersection (?)
operation).
If twomatchings are compatible, we can form their union,which is defined as (M1 ?
M2)(n) = M1(n)if n ?
T1 and M2(n) else.
We noteocc(T1) ?
occ(T2) = {M1 ?
M2 | M1 ?occ(T1),M2 ?
occ(T2) and M1 is compati-ble with M2 }.
Then,we have the property:occ(T ) = occ(T1)?
occ(T2)In practice, the intersection operation will be im-plemented using merge and binary merge algorithms(Baeza-Yates and Salinger, 2005), following (Lopez,2007).3.2 The computation hypergraphWe have seen that it is possible to compute occ(T)from two subtreelets forming a coverage of T. Thiscan be represented by a hypergraph in which nodesare all the treelets of a given query tree, and everypair of overlapping or adjacent treelet is linked byan hyperedge to their union treelet.
Whenever wehave computed two starting points of an hyper-edge,we can compute its destination treelet.
An exampleof a small computation hypergraph is described infigure 1.It is very convenient to represent the incremen-tal computation of the treelets as a traversal of thishypergraph.
First because it contributes to avoidredundant computations: each treelet is computedonly once, even if it is used to compute several othertreelets.
Also, if a query tree contains two distinctbut identical treelets, only one computation will bedone, provided the two treelets are represented bythe same node in the hypergraph.
The hypergraphalso allows us to avoid computing empty treelets, aswe describe in next section.
This hypergraph there-fore has the same role for us as the prefix tree used510Figure 2: Inclusion DAG for the tree a(bcd(e))in (Lopez, 2007).
Of course, the hypergraph is gen-erated on-the-fly during the traversal.Furthermore, different traversals will define dif-ferent computation strategies, and we will be able touse some more advanced graph exploration methodsin section 6.3.3 The Immediate Inclusion DAGIn many cases (but not always: see section 4.3),the most optimal computation strategy should beto always compute a treelet from two of its imme-diate subtreelets.
This is because the computationtime will be proportional to the size of the small-est occurrence set of the two treelets, and thus the?cheapest?
subtreelet is always one of the immedi-ate subtreelets.
With this computation strategy, wecan replace the general computation hypergraph bya DAG (Directed Acyclic Graph) in which everytreelet point to its immediate supertreelets.
An ex-ample is given on figure 2.
We will call this DAGthe (Immediate) Inclusion DAG.Traversals of the Inclusion DAG should be prunedwhen an empty treelet is found, since all of its su-pertreelets will also be empty.
The algorithm 1 pro-vide a general traversal of the DAG avoiding to com-pute as many empty treelets as possible.
It uses aqueue D of discovered treelets, and a data-structureC that associate a treelet to those of its subtreeletsthat have been already computed.
Once a treelet Thas been computed and is found to be non empty, wediscover its immediate supertreelets TS1, TS2, .
.
.
(ifthey have not been discovered already) and add T toC (TS1), C (TS2), .
.
.
.
The operation min(C (T)) re-Algorithm 1: Generic DAG traversalAdd the set of precomputed treelets to D;1while ?T ?
D s.t T ?
precomputed or |C(T )| > 22dopop T from D;3if T in precomputed then4occ(T )?
precomputed[T ];5else6T1,T2=min(C (T));7if |occ(T1)| = 0 then8occ(T )?
?
;9else10occ(T )?
occ(T1)?
occ(T2);11for TS ?
supertree(T ) do12if occ(TS) = undef then13Add T to C(TS);14if |occ(T )| > 0 and TS /?
D then15Add TS to D;16trieve the 2 subtreelets from C (T) that have the leastoccurrences.
If one of them is empty, we can di-rectly conclude that T is empty.
No treelet whose allimmediate subtreelets are empty is ever put in thediscovered queue, which allows us to prune most ofthe empty treelets of the Inclusion DAG.A treelet in the inclusion DAG can be computedas soon as two of its antecedents have been com-puted.
To start the computation (or rather, ?seed?it), it is necessary to know the occurrences of treeletof smaller size.
In the following sections 3.4 and3.5, we describe two methods for efficiently obtain-ing the set of occurrences of some initial treelets.3.4 Path-to-Root ArrayWe present here a method to compute very effi-ciently occ(T) when T is linear.
This method is sim-ilar to the use of Suffix Arrays (Manber and My-ers, 1990) to find the occurrences of continuous sub-strings in a text.Definition 3.4 (Paths-to-Root Array).
Given a la-beled tree T and a node n ?
T , the path-to-rootof n is the sequence of labels from n to the root.The Paths-to-Root Array of a set of trees DB is thelexicographically sorted list of the Path-to-Roots ofevery node in DB.Just as with suffixes, a path-to-root can be rep-resented compactly by a pointer to its starting nodein DB.
We then need to keep the database DB in511Pos PtR Pos PtR Pos PtR1 3:4 a 8 2:2 bf 15 1:3 fba2 1:7 a 9 1:6 ca 16 3:3 fga3 2:6 af 10 1:8 da 17 3:2 ga4 1:4 afba 11 2:7 daf 18 2:1 gbf5 3:7 ba 12 3:1 ega 19 1:5 gca6 1:2 ba 13 2:3 f 20 1:1 hba7 2:5 baf 14 3:8 fba 21 3:5 hebaFigure 3: Path To Root Array for a set of three trees.?Pos.?
is the position of the starting point of a given path-to-root (noted as indexOfTree:positionInTree), and PtR isthe sequence of labels on this path-to-root.
The path-to-root are sorted in lexicographic order.
We can find the setof occurrences of any linear treelet with a binary search.For example, the treelet a(b) corresponds to the label se-quence ?ba?.
With a binary search, we find that the path-to-root starting with ?ba?
are between indexes 5 and 7.The corresponding occurrences are then 3:7, 1:2 and 2:5.memory to retrieve efficiently the pointed-to path-to-root.
Once the Path-to-Root Array is built, for alinear treelet T, we can find its occurrences by a bi-nary search of the first and last path-to-root startingwith the labels of T. See figure 3 for an example.Memory cost is quite manageable, since we onlyneed 10 bytes per nodes in total.
5 bytes per pointerin the array (tree id: 4 bytes, start position: 1 byte),and 5 bytes per nodes to store the database in mem-ory (label id:4 bytes, parent position: 1 byte).All the optimization tricks proposed in (Lopez,2007) for Suffix Arrays can be used here, espe-cially the optimization proposed in (Zhang and Vo-gel, 2005).3.5 Inverted Index and PrecomputationInstead of a Path-to-Root array, one can simply usean inverted index.
The inverted index associateswith every label the set of its occurrences, each oc-currences being represented by a tuple containingthe index of the tree, the position of the label in thetree, and the position of the parent of the label inthe tree.
Knowing the position of the parent willallow to compute treelets of size 2 by intersection(D-coverage).
This is less effective than the Path-To-Root Array approach, but open the possibilitiesfor the flexible search discussed in section 6.Taking the idea further, we can actually con-sider the possibility of precomputing treelets of sizegreater than 1, especially if they appear frequentlyin the corpus.4 Practical implementation of the traversal4.1 Postorder traversalThe way we choose the treelet to be popped out online 3 of algorithm 1 will define different computa-tion strategies.
For concreteness, we describe now amore specific traversal.
We will process treelets inan order depending on their root node.
More pre-cisely, we consider the nodes of the query tree in theorder given by a depth-first postorder traversal of thequery tree.
This way, when a treelet rooted at n isprocessed, all of the treelets rooted at a descendantof n have already been processed.We can suppose that every processed treelet is as-signed an index that we note #T. This allows a con-venient recursive representation of treelets.Definition 4.1 (Recursive representation).
Let T bea treelet rooted at node n of TQ.
We note ni theith child of n in TQ.
For all i, ti is the subtree ofT rooted at ni.
We note ti = ?
and #ti = 0 if Tdoes not contain ni.
The recursive representation ofT is then: [n, (#t1,#t2, .
.
.
,#tm)].
We note T i thevalue #ti.For example, if TQ =?a(b c d(e))?
and the treelets?b?
and ?d(e)?
have been assigned the indexes 2and 4, the recursive representation of the treelet ?a(bd(e))?
would be [a,(2,0,4)].Algorithm 2 describes this ?postorder traversal?.DNode is a priority queue containing the treeletsrooted atNode discovered so far.
The priority queuepop out the smallest treelets first.
Line 14 maintain alist L of processed treelets and assign the index of Tin L to #T. Line 22 keeps track of the non-emptyimmediate supertreelets of every treelet through adictionary S. This is used in the procedure compute-supertreelets (algorithm 3) to generate the immedi-ate supertreelets of a treelet T given its recursive rep-resentation.
In this procedure, line 6 produces the512Algorithm 2: DAG traversal by query-tree pos-torderfor Node in postorder-traversal(query-tree) do1Telem = [Node, (0, 0, .., 0)];2DNode ?
Telem;3while |DNode| > 0 do4T=pop-first(DNode);5if T in precomputed then6occ(T )?
precomputed[Node.label];7else8T1, T2=min(C (t));9if |occ(T1)| = 0 then10occ(T )?
?
;11else12occ(T )?
occ(T1)?
occ(T2);13Append T to L;14#T ?
|L|;15for TS in compute-supertree(T,#T ) do16Add T to C(TS);17if |occ(T )| > 0 then18if TS /?
DNode and19root(TS)=Node thenAdd TS to D;20for #t in C (T ) do21Add #T to S(#t);22descending supertreelets, and line 8 produces the as-cending supertreelet.
Figure 4 describes the contentof all these data structures for a simple run of thealgorithm.This postorder traversal has several advantages.A treelet is only processed once all of its immedi-ate supertreelets have been computed, which is op-timal to reduce the cost of the ?
operation.
Theway the procedure compute-supertreelets discoversupertreelets from the info in S has also severalbenefit.
One is that, by not adding empty treelets(line 18) to S , we naturally prevent the discoveryof larger empty treelets.
Similarly, in the next sec-tion, we will be able to prevent the discovery of non-maximal treelets by modifying S .
Modifications ofcompute-supertreelets will also allow different kindof retrieval in section 6.4.2 Pruning non-maximal treeletsWe now try to address another aspect of the over-whelming number of potential treelets in a querytree.
As we said, in most practical cases, most of thelarger treelets in a query tree will be empty.
Still, it isAlgorithm 3: compute-supertreesInput: T ,#TOutput: lst: list of immediate supertreelets of Tm?
|root(T)|;1for i in 1 .
.
.m do2for #TS in S(#T i) do3if root(#TS) 6= root(T) then4Tnew ?
[root(T), T 0, ..#T ?, .
.
.
, Tm];5Append Tnew to lst;6Tnew ?
[parent(root(T)), (0, .
.
.
,#T, .
.
.
, 0)];7Append Tnew to lst;8possible that some tree exactly identical to the querytree (or some tree having a very large treelet in com-mon with the query tree) do exist in the database.This case is obviously a best case for translation,but unfortunately could be a worst-case for our al-gorithm, as it means that all of the (possibly trillionsof) treelets of the query tree will be computed.To solve this issue, we try to consider a conceptanalogous to that of maximal substring, or substringclass, found in Suffix Trees and Suffix Arrays (Ya-mamoto and Church, 2001).
The idea is that in mostcases where a query tree is ?full?
(that is all of itstreelets are not empty), most of the larger treeletswill share the same occurrences (in the databasetrees that are very similar to the query tree).
We for-malize this as follow:Definition 4.2 (domination and maximal treelets).Let T1 be a subtreelet of T2.
If for every matchingM1 of occ(T1), there exist a matching M2 ofocc(T2) such that M2|T1 = M1, we say that T1 isdominated by T2.
A treelet is maximal if it is notdominated by any other treelet.If T1 is dominated by T2, it means that all occur-rences of T1 are actually part of an occurrence ofT2.
We will therefore be, in general, more interestedby the larger treelet T2 and can prune as many non-maximal treelets as we want in the traversal.
The keypoint is that the algorithm has to avoid discoveringmost non maximal treelets.The algorithm 2 can eas-ily be modified to do this.
We will use the followingproperty.Property 4.1.
Given k treelets T1 .
.
.
Tk with k dis-tinct roots, all the roots being children of a samenode n. We note n(T1 .
.
.
Tk) the treelet whose rootis n, and for which the k subtrees rooted at the k513T d e b b(d) [Empty] b(e) b(d e) [Empty] c a a(b) a(b(e)) a(c) a(b c) a(b(e) c)# 1 2 3 4 5 6 7 8 9 10 11 12 13R d e b(..) b(1.)
b(.2) b(1 2) c a(..) a(3.)
a(5.)
a(.7) a(3 7) a(5 7)C - - - 1,3 2,3 4,5 - - 8,3 5,9 7,8 9,11 10,12S - - 5 - - - - 9,11 10,12 13 12 13 -Figure 4: A run of the algorithm 2, for the query tree a(b(d e) c).
The row ?T?
represents the treelets in the orderthey are discovered.
The row ?#?
is the index #T, and the row ?R?
is the recursive representation of the treelet.
Alsorepresented are the content of C and S at the end of the computation.
When a treelet is poped out of DNode, occ(T) iscomputed from the treelets listed in C (T ).
If occ(T) is not empty, the entries of the immediate subtreelets of T in Sare updated with #T. We suppose here that |occ( b(d))|=0.
Then, b(d e) is marked as empty and neither b(d) nor b(d e)are added to the entries of their subtreelets in S. This way, when considering treelets rooted at the upper node ?a?, thealgorithm will not discover any of the treelets containing b(d).children of n are T1 .
.
.
Tk.
Let us further supposethat for all i, Ti is dominated by a descending su-pertreelet T di (with the possibility that Ti = T di ).Then n(T1 .
.
.
Tk) is dominated by n(T d1 .
.
.
T dk ).For example, if b(c) is dominated by b(c d), thena(b(c) e) will be dominated by a(b(c d) e).In algorithm 2, after processing each node, weproceed to a cleaning of the S dictionary in the fol-lowing way: for every treelet T (considering thetreelets by increasing size) that is dominated byone of its supertreelets TS ?
S(T) and for everysubtreelet T ?
of T such that T ?
S(T ?
), we re-place T by TS in S(T ?).
The procedure compute-supertreelets, when called during the processing ofthe parent node, will thus skip all of the treelets thatare ?trivially?
dominated according to property 4.1.Let?s note that testing for the domination of atreelet T by one of its supertrelets TS is not a matterof just testing if |occ(T)| = |occ(TS)|, as would bethe case with substring: a treelet can have less oc-currences than one of its supertreelets (eg.
b(a) hasmore occurrences than b in b(a a) ).
An efficient wayis to first check that the two treelets occurs in thesame number of sentences, then confirm this with asystematic check of the definition.4.3 The case of constituent treesWe have focused our experiments on dependencytrees, but the method can be applied to any tree.However, the computations strategies we have usedmight not be optimal for all kind of trees.
In a de-pendency tree, nodes are labeled by words and mostnon-elementary treelets have a small number of oc-currences.
In a constituent tree, many treelets con-taining only internal nodes have a high frequencyand will be expensive to compute.If we have enough memory, we can solve this byprecomputing the most common (and therefore ex-pensive) treelets.However, it is usually not very interesting to re-trieve all the occurrences of treelets such as ?NP(DetNN)?
in the context of aMT system.
Such very com-mon pattern are best treated by some pre-computedrules.
What is interesting is the retrieval of lexical-ized rules.
More precisely, we want to retrieve ef-ficiently treelets containing at least one leaf of thequery tree.
Therefore, an alternative computationstrategy would only explore treelets containing atleast one terminal node.
We would thus computesuccessively ?dog?, ?NN(dog)?
?NP(NN(dog))?,?NP(Det NN(dog))?, etc.4.4 ComplexityProcessing time will be mainly dependent on twofactors: the number of treelets in a query tree thatneed to be computed, and the average time to com-pute a treelet.Let NC be the size of the corpus.
It can be shownquite easily that the time needed to compute a treeletwith our method is proportional to its number of oc-currences, which is itself growing as O(NC).Let m be the size of the query tree.
The numberof treelets needing to be computed is, in the worstcase, exponential in m. In practice, the only casewhere most of the treelets are non-empty is when thedatabase contains trees similar to the query tree inthe database, and this is handled by the modificationof the algorithm is section 4.2.
In other cases, mostof the treelets are empty, and empirically, we findthat the number of non-empty treelets in a query tree514Database size (#nodes) 6M 60MLargest non-empty treelet size 4.6 8.7Processing time (PtR Array) 0.02 s 0.7 sProcessing time (Inv.
Index) 0.02 s 0.9 sSize on disk 40 MB 500 MBFigure 5: Performances averaged on 100 sentences.grows approximately as O(m ?N0.5C ).
It is also pos-sible to bound the size of the retrieved treelets (onlyretrieving treelets with less than 10 nodes, for exam-ple), similarly to what is done in (Lopez, 2007).
Thenumber of treelets will then only grows as O(m).The total processing time of a given query treewill therefore be on the order of O(m ?
N1.5C ) (orO(m ?
NC) if we bound the treelet size).
The factthat this give a complexity worse than linear withrespect to the database size might seem a concern,but this is actually only because we are retrievingmore and more different types of treelets.
The costof retrieving one treelet remain linear with respect tothe size of the corpus.
We empirically find that evenfor very large values of NC , processing time remainvery reasonable (see next section).It should be also noted that the constant hid-den in the big-O notation can be (almost) arbitrar-ily reduced by precomputing more and more of themost common (and more expensive) treelets (a time-memory trade-off).5 ExperimentsWe conducted experiments on a large database of2.9 million automatically parsed dependency trees,with a total of nearly 60 million nodes1.
The largesttrees in the database have around 100 nodes.
In or-der to see how performance scale with the size of thedatabase, we also used a smaller subset of 230,000trees containing near 6 million nodes.We computed, using our algorithm, 100 randomlyselected query trees having from 10 to 70 nodes,with an average of 27 nodes per tree.
Table 5shows the average performances per sentence.
Con-sidering the huge size of the database, a process-1This database was an aggregate of several Japanese-Englishcorpora, notably the Yomiuri newspaper corpus (Utiyama andIsahara, 2003) and the JST paper abstract corpus created atNICT(www.nict.go.jp) through (Utiyama and Isahara, 2007).Method Treelet Ourdictionary methodDisk space used 23 GB 500 MBBLEU 11.6% 12.0%Figure 6: Comparison with a dictionary-based baseline(performances averaged over 100 sentences).ing time below 1 second seems reasonable.
Theincrease in processing time between the small andthe large database is in line with the explanationsof section 4.4.
Path-to-Root Arrays are slightly bet-ter than Inverted indexes (we suspect a better im-plementation could increase the difference further).Both methods use up about the same disk space:around 500MB.
We also find that the approach ofsection 4.2 brings virtually no overhead and givessimilar performances whether the query tree is in thedatabase or not (effectively reducing the worst-casecomputation time from days to seconds).We also conducted a small English-to-Japanesetranslation experiment with a simple translation sys-tem using Synchronous Tree Substitution Grammars(STSG) for translating dependency trees.
The sys-tem we used is still in an experimental state andprobably not quite at the state-of-the-art level yet.However, we considered it was good enough for ourpurpose, since we mainly want to test our algorithmis a practical way.
As a baseline, from our cor-pus of 2.9 millions dependency trees, we automat-ically extracted STSG rules of size smaller than 6and stored them in a database, considering that ex-tracting rules of larger sizes would lead to an un-manageable database size.
We compared MT resultsusing only the rules of size smaller than 6 to usingall the rules computed on-the-fly after treelet retriev-ing by our method.
These results are summarized onfigure 6.6 Flexible matchingWe now describe an extension of the algorithm forapproximate matching of treelets.
We consider thateach node of the query tree and database is labeledby 2 labels (or more) of different generality.
Forconcreteness, let?s consider dependency trees whosenodes are labeled by words and the Part-Of-Speech(POS) of these words.
We want to retrieve treelets515that match by word or POS with the query tree.6.1 Processing multi-Label treesTo do this, the inverted index will just need toinclude entries for both words and POS.
For ex-ample, the dependency tree ?likes,V:1 (Paul,N:0Salmon,N:2 (and,CC:3 (Tuna,N:4)))?
would pro-duce the following (node,parents) entries in the in-verted index: {N:[(0,1) (2,1) (4,3)], Paul:[(0,1)],Salmon:[(2,1)],.
.
.
}.
This allows to search for atreelet containing any combination of labels, like?likes(N Salmon(CC(N)))?.We actually want to compute all of the treelets ofa query tree TQ labeled by words and POS (meaningeach node can be matched by either word or POS).We can compute TQ without redundant computa-tions by slightly modifying the algorithm 2.
First,we modify the recursive representation of a treeletso that it also includes the chosen label of its rootnode.
Then, the only modifications needed in algo-rithm 2 are the following: 1- at initialization (line 3),the elementary treelets corresponding to every pos-sible labels are added to the discovered treelets setD; 2- in procedure compute-supertrees, at line 8, wegenerate one ascending supertreelet per label.6.2 Weighted searchWhile the previous method would allow us to com-pute as efficiently as possible all the treelets in-cluded in a multi-labeled query tree, there is stilla problem: even avoiding redundant computations,the number of treelets to compute can be huge, sincewe are computing all combinations of labels.
Foreach treelet of size m we would have had in a singlelabel query tree, we now virtually have 2m treelets.Therefore, it is not reasonable in general to try tocompute all these treelets.However, we are not really interested in comput-ing all possible treelets.
In our case, the POS la-bels allow us to retrieve larger examples when nonecontaining only words would be available.
But westill prefer to find examples matched by words ratherthan by POS.
We therefore need to tell the algorithmthat some treelets are more important that some oth-ers.
While we have used the Computation Hypertreerepresentation to compute treelets efficiently, we canalso use it to prioritize the treelets we want to com-pute.
This is easily implemented by giving a weightPOS matchings Without WithProcessing time 0.9 s 22 sLargest non-empty treelet size 8.7 11.4Treelets of size>8 0.4 102BLEU 12.0% 12.1%Figure 7: Effect of POS-matchingto every treelet.
We can then modify our traversalstrategy of the Inclusion DAG to compute treeletshaving the biggest weights first: we just need tospecify that the treelet popped out on line 3 is thetreelet with the highest score (more generally, wecould consider a A* search).6.3 ExperimentsUsing the above ideas, we have made some experi-ments for computing query dependency trees labeledwith both words and POS.
We score the treelets bygiving them a penalty of -1 for each POS they con-tain, and stop the search when all remaining treeletshave a score lower than -2 (in other words, treeletsare allowed at most 2 POS-matchings).
We also re-quire POS-matched nodes to be non-adjacent.We only have some small modifications to do toalgorithm 2.
In line 3 of algorithm 2, elementarytreelets are assigned a weight of 0 or -1 depend-ing on whether their label is a word or POS.
Line 5is replaced by ?pop the first treelet with minimalweight and break the loop if the minimal weight isinferior to -2?.
In compute-supertreelets, we give aweight to the generated supertreelets by combiningthe weights of the child treelets.Table 7 shows the increase in the size of thebiggest non-empty treelets when allowing 2 nodesto be matched by POS.
It also shows the impact onBLEU score of using these additional treelets for on-the-fly rule generation in our simpleMT system.
Im-provement on BLEU is limited, but it might be dueto a very experimental handling of approximatelymatched treelet examples in our MT system.The computation time, while manageable, wasmuch slower than in the one-label case.
This is dueto the increased number of treelets to be computed,and to the fact that POS-labeled elementary treeletshave a high number of occurrences.
It would bemore efficient to use more specific labeling (e.g V-516Figure 8: A packed forest.mvt for verbs of movement instead of V).7 Additional extensionsWe briefly discuss here some additional extensionsto our algorithm that we will not detail for lack ofroom and practical experiments.7.1 Packed forestDue to parsing ambiguities and automatic parserserrors, it is often useful to use multiple parses ofa given sentence.
These parses can be representedby a packed forest such as the one in figure 8.
Ourmethod allows the use of packed representation ofboth the query tree and the database.For the inverted index, the only difference isthat now, an occurrence of a label can have morethan one parent.
For example, the inverted in-dex of a database containing the packed forestof figure 8 would contain the following entries:{held: [(1,10a),(1,10b)], NP: [(6,9),(7,9),(9,10a)],VP:[(10,N)], PP:[(8,10b)], a:[(2,6)], talk:[(3,6)],with:[(4,7) (4,8)], Sharon:[(5,7) (5,8)]}.
Where 10aand 10b are some kind of virtual position that help tospecify that held and NP8 belong to the same chil-dren list.
We could also include a cost on edgesin the inverted index, which would allow to prunematchings to unlikely parses.The inverted index can now be used to search inthe trees contained in a packed forest database with-out any modification.
Modifications to the algorithmin order to handle a packed forest query are similarto the ones developed in section 6.7.2 Discontinuous treeletsAs we discussed in section 2.3, using a representa-tion for the position of every node similar to (Zhanget al, 2001), it is possible to determine the distanceand ancestor relationship of two nodes by just com-paring their positions.
This opens the possibility ofcomputing the occurrences of discontinuous treeletsin much the same way as is done in (Lopez, 2007)for discontinuous substrings.
We have not studiedthis aspect in depth yet, especially since we are notaware of any MT system making use of discontin-uous syntax tree examples.
This is nevertheless aninteresting future possibility.8 Related workAs we previously mentioned, (Lopez, 2007) and(Callison-Burch et al, 2005) propose a method sim-ilar to ours for the string case.We are not aware of previous proposals for ef-ficient on-the-fly retrieving of translation examplesin the case of Syntax-Based Machine Translation.Among the works involving rule precomputation,(Zhang et al, 2009) describes a method for effi-ciently matching precomputed treelets rules.
Theserules are organized in a kind of prefix tree that al-lows efficient matching of packed forests.
(Liu et al,2006) also propose a greedy algorithm for matchingTSC rules to a query tree.9 Conclusion and future workWe have presented a method for efficiently retriev-ing examples of treelets contained in a query tree,thus allowing on-the-fly computation of translationrules for Syntax-Based systems.
We did this bybuilding on approaches previously proposed for thecase of string examples, proposing an adaptation ofthe concept of Suffix Arrays to trees, and formaliz-ing computation as the traversal of an hypergraph.This hypergraph allows us to easily formalize dif-ferent computation strategy, and adapt the methodsto flexible matchings.
We still have a lot to do withrespect to improving our implementation, exploringthe different possibilities offered by this frameworkand proceeding to more experiments.AcknowledgmentsWe thank the anonymous reviewers for their usefulcomments.517ReferencesR.
Baeza-Yates and A. Salinger.
2005.
Experimentalanalysis of a fast intersection algorithm for sorted se-quences.
In String Processing and Information Re-trieval, page 1324.N.
Bruno, N. Koudas, and D. Srivastava.
2002.
Holis-tic twig joins: optimal XML pattern matching.
InProceedings of the 2002 ACM SIGMOD internationalconference on Management of data, page 310321.C.
Callison-Burch, C. Bannard, and J. Schroeder.
2005.Scaling phrase-based statistical machine translation tolarger corpora and longer phrases.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics, pages 255?262.
Association forComputational Linguistics Morristown, NJ, USA.David Chiang.
2007.
Hierarchical Phrase-Based trans-lation.
Computational Linguistics, 33(2):201?228,June.M.
Dubiner, Z. Galil, and E. Magen.
1994.
Fastertree pattern matching.
Journal of the ACM (JACM),41(2):205213.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisti-cal phrase-based translation.
In Proceedings of HLT-NAACL, pages 48?54.
Association for ComputationalLinguistics.Z.
Liu, H. Wang, and H. Wu.
2006.
Example-basedmachine translation based on tree?string correspon-dence and statistical generation.
Machine translation,20(1):25?41.A.
Lopez.
2007.
Hierarchical phrase-based translationwith suffix arrays.
In Proc.
of EMNLP-CoNLL, page976985.U.
Manber and G. Myers.
1990.
Suffix arrays: anew method for on-line string searches.
In Proceed-ings of the first annual ACM-SIAM symposium on Dis-crete algorithms, pages 319?327, San Francisco, CA,USA.
Society for Industrial and Applied MathematicsPhiladelphia, PA, USA.H.
Mi, L. Huang, and Q. Liu.
2008.
Forest based trans-lation.
Proceedings of ACL-08: HLT, page 192199.Toshiaki Nakazawa and Sadao Kurohashi.
2008.
Syn-tactical EBMT system for NTCIR-7 patent translationtask.
In Proceedings of NTCIR-7 Workshop Meeting,Tokyo, Japon.C.
Quirk, A. Menezes, and C. Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguis-tics, page 279.M.
Utiyama and H. Isahara.
2003.
Reliable measures foraligning japanese-english news articles and sentences.In Proceedings of ACL, pages 72?79, Sapporo,Japon.M.
Utiyama and H. Isahara.
2007.
A japanese-englishpatent parallel corpus.
In MT summit XI, pages 475?482.M.
Yamamoto and K. W. Church.
2001.
Using suffixarrays to compute term frequency and document fre-quency for all substrings in a corpus.
ComputationalLinguistics, 27(1):1?30.Y.
Zhang and S. Vogel.
2005.
An efficient phrase-to-phrase alignment model for arbitrarily long phrase andlarge corpora.
In Proceedings of EAMT, pages 294?301, Budapest, Hungary.C.
Zhang, J. Naughton, D. DeWitt, Q. Luo, andG.
Lohman.
2001.
On supporting containment queriesin relational database management systems.
In Pro-ceedings of the 2001 ACM SIGMOD internationalconference on Management of data, page 425436.H.
Zhang, M. Zhang, H. Li, and Chew Lim Tan.
2009.Fast translation rule matching for syntax-based statis-tical machine translation.
In Proc.
of EMNLP, pages1037?1045.518
