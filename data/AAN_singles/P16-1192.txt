Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2042?2051,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsEfficient techniques for parsing with tree automataJonas Groschwitz and Alexander KollerDepartment of LinguisticsUniversity of PotsdamGermanygroschwi|akoller@uni-potsdam.deMark JohnsonDepartment of ComputingMacquarie UniversityAustraliamark.johnson@mq.edu.auAbstractParsing for a wide variety of grammar for-malisms can be performed by intersectingfinite tree automata.
However, naive im-plementations of parsing by intersectionare very inefficient.
We present techniquesthat speed up tree-automata-based pars-ing, to the point that it becomes practicallyfeasible on realistic data when applied tocontext-free, TAG, and graph parsing.
Forgraph parsing, we obtain the best runtimesin the literature.1 IntroductionGrammar formalisms that go beyond context-freegrammars have recently enjoyed renewed atten-tion throughout computational linguistics.
Clas-sical grammar formalisms such as TAG (Joshi andSchabes, 1997) and CCG (Steedman, 2001) havebeen equipped with expressive statistical mod-els, and high-performance parsers have becomeavailable (Clark and Curran, 2007; Lewis andSteedman, 2014; Kallmeyer and Maier, 2013).Synchronous grammar formalisms such as syn-chronous context-free grammars (Chiang, 2007)and tree-to-string transducers (Galley et al, 2004;Graehl et al, 2008; Seemann et al, 2015) arebeing used as models that incorporate syntac-tic information in statistical machine translation.Synchronous string-to-tree (Wong and Mooney,2006) and string-to-graph grammars (Chiang etal., 2013) have been applied to semantic parsing;and so forth.Each of these grammar formalisms requires itsusers to develop new algorithms for parsing andtraining.
This comes with challenges that are bothpractical and theoretical.
From a theoretical per-spective, many of these algorithms are basicallythe same, in that they rest upon a CKY-style pars-ing algorithm which recursively explores substruc-tures of the input object and assigns them non-terminal symbols, but their exact relationship israrely made explicit.
On the practical side, thisparsing algorithm and its extensions (e.g.
to EMtraining) have to be implemented and optimizedfrom scratch for each new grammar formalism.Thus, development time is spent on reinventingwheels that are slightly different from previousones, and the resulting implementations still tendto underperform.Koller and Kuhlmann (2011) introduced Inter-preted Regular Tree Grammars (IRTGs) in orderto address this situation.
An IRTG representsa language by describing a regular language ofderivation trees, each of which is mapped to a termover some algebra and evaluated there.
Gram-mars from a wide range of monolingual and syn-chronous formalisms can be mapped into IRTGsby using different algebras: Context-free and tree-adjoining grammars use string algebras of differ-ent kinds, graph grammars can be captured by us-ing graph algebras, and so on.
In addition, IRTGscome with a universal parsing algorithm based onclosure results for tree automata.
Implementingand optimizing this parsing algorithm once, onecould apply it to all grammar formalisms that canbe mapped to IRTG.
However, while Koller andKuhlmann show that asymptotically optimal pars-ing is possible in theory, it is non-trivial to imple-ment their algorithm optimally.In this paper, we introduce practical algorithmsfor the two key operations underlying IRTG pars-ing: computing the intersection of two tree au-tomata and applying an inverse tree homomor-phism to a tree automaton.
After defining IRTGs(Section 2), we will first illustrate that a naivebottom-up implementation of the intersection al-gorithm yields asymptotic parsing complexitiesthat are too high (Section 3).
We will then2042show how the parsing complexity can be im-proved by combining algebra-specific index datastructures with a generic parsing algorithm (Sec-tion 4), and by replacing bottom-up with top-downqueries (Section 5).
In contrast to the naive al-gorithm, both of these methods achieve the ex-pected asymptotic complexities, e.g.
O(n3) forcontext-free parsing, O(n6) for TAG parsing, etc.Furthermore, an evaluation with realistic gram-mars shows that our algorithms improve practi-cal parsing times with IRTG grammars encodingcontext-free grammars, tree-adjoining grammars,and graph grammars by orders of magnitude (Sec-tion 6).
Thus our algorithms make IRTG pars-ing practically feasible for the first time; for graphparsing, we obtain the fastest reported runtimes.2 Interpreted Regular Tree GrammarsWe will first define IRTGs and explain how theuniversal parsing algorithm for IRTGs works.2.1 Formal foundationsFirst, we introduce some fundamental theoreticalconcepts and notation.A signature ?
is a finite set of symbols r, f, .
.
.,each of which has an arity ar(r) ?
0.
A tree t overthe signature ?
is a term of the form r(t1, .
.
.
, tn),where the tiare trees and r ?
?
has arity n. Weidentify the nodes of t by their Gorn addresses, i.e.paths pi ?
N?from the root to the node, and writet(pi) for the label of pi.
We write T?for the set ofall trees over ?, and T?
(Xk) for the trees in whicheach node either has a label from ?, or is a leaflabeled with one of the variables {x1, .
.
.
, xk}.A (linear, nondeleting) tree homomorphism hfrom a signature ?
to a signature ?
is a mappingh : T??
T?.
It is defined by specifying, for eachsymbol r ?
?
of arity k, a term h(r) ?
T?
(Xk)in which each variable occurs exactly once.
Thissymbol-wise mapping is lifted to entire trees byletting h(r(t1, .
.
.
, tk)) = h(r)[h(t1), .
.
.
, h(tk)],i.e.
by replacing the variable xiin h(r) by the re-cursively computed value h(ti).Let ?
be a signature.
A ?-algebra A con-sists of a nonempty set A, called the domain, andfor each symbol f ?
?
with arity k, a functionfA: Ak?
A, the operation associated withf .
We can evaluate any term t ?
T?to a valuetA?
A, by evaluating the operation symbolsbottom-up.
In this paper, we will be particularlyinterested in the string algebra E?over the finiteautomaton rule homomorphismS?
r1(NP,VP) ?
(x1, x2)NP?
r2JohnVP?
r3walksVP?
r4(VP,NP) ?
(x1, ?
(on, x2))NP?
r5MarsFigure 1: An example IRTG.alphabet E. Its domain is the set of all strings overE.
For each symbol a ?
E, it has a nullary oper-ation symbol a with aE?= a.
It also has a singlebinary operation symbol ?, such that ?E?
(w1, w2)is the concatenation of the stringsw1andw2.
Thusthe term ?
(John, ?
(walks, ?
(on,Mars))) in Fig.
2bevaluates to the string ?John walks on Mars?.A finite tree automaton M over the signature ?is a structure M = (?, Q,R,XF), where Q is afinite set of states and XF?
Q is a final state.R is a finite set of transition rules of the formX ?
r(X1, .
.
.
, Xk), where the terminal symbolr ?
?
is of arity k and X,X1, .
.
.
, Xk?
Q. Atree automaton can run non-deterministically ona tree t ?
T?by assigning states to the nodesof t bottom-up.
If we have t = r(t1, .
.
.
, tn)and M can assign the state Xito each ti, writtenXi?
?ti, then we also have X ??t.
We say thatM accepts t if XF?
?t, and define the languageL(M) ?
T?of M as the (possibly infinite) setof all trees that M accepts.
An example of a treeautomaton (with states S, NP, etc.)
is shown inthe ?automaton rule?
column of Fig.
1.
It accepts,among others, the tree ?1in Fig.
2a.Tree automata can be defined top-down orbottom-up, and are equivalent to regular treegrammars.
The languages that can be accepted byfinite tree automata are called the regular tree lan-guages.
See e.g.
Comon et al (2008) for details.2.2 Interpreted regular tree grammarsWe can combine tree automata, homomorphisms,and algebras into grammars that can describe lan-guages of arbitrary objects, as well as relations be-tween such objects ?
in a way that inherits manytechnical properties from context-free grammars,while extending the expressive capacity.An interpreted regular tree grammar(IRTG, Koller and Kuhlmann (2011))G = (M, (h1,A1), .
.
.
, (hn,An)) consists ofa tree automaton M over some signature ?,together with an arbitrary number n of inter-2043r1r2r4r3r5(a) Tree ?1.h??
?John ?walks ?on Mars(b) Term h (?1).evaluate?????
?John walks on Mars?
(c) h (?1) evaluated in E?.Figure 2: The tree ?1, evaluated by the homomorphism h and the algebra E?pretations (hi,Ai), where each Aiis an algebraover some signature ?iand each hiis a treehomomorphism from ?
to ?i.
The automatonM describes a language L(M) of derivationtrees which represent abstract syntactic structures.Each derivation tree ?
is then interpreted n ways:we map it to a term hi(?)
?
T?i, and then weevaluate hi(?)
to a value ai= hi(?)Ai?
Aiof thealgebra Ai.
Thus, the IRTG G defines a languageL(G) = {(h1(?
)A1, .
.
.
, hn(?
)An) | ?
?
L(M)},which is an n-place relation between the domainsof the algebras.Consider the IRTG G shown in Fig.
1.
The ?au-tomaton rule?
column indicates the five rules ofM ; the final state is S. We already saw the deriva-tion tree ?1?
L(M).
G has a single interpre-tation, into a string algebra E?, and with a ho-momorphism that is specified by the ?homomor-phism?
column; for instance, h(r1) = ?
(x1, x2)and h(r2) = John.
Applying this homomorphismto ?1, we obtain the term h(?1) in Fig.
2b.
As wesaw earlier, this term evaluates in the string alge-bra to the string ?John walks on Mars?
(Fig.
2c).Thus this string is an element of L(G).We assume that no two rules of M use the sameterminal symbol; this is generally not requiredin tree automata, but every IRTG can be broughtinto this convenient form.
Furthermore, we focus(but only for simplicity of presentation) on IRTGsthat use a single string-algebra interpretation, as inFig.
1.
Such grammars capture context-free gram-mars.
However, IRTGs can capture a wide vari-ety of grammar formalisms by using different al-gebras.
For instance, an interpretation that usesa TAG string algebra (or TAG derived-tree alge-bra) models a tree-adjoining grammar (Koller andKuhlmann, 2012), and an interpretation into ans-graph algebra models a hyperedge replacementgraph grammar (HRG, Groschwitz et al (2015)).By using multiple algebras, IRTGs can also repre-sent synchronous grammars and (bottom-up) tree-to-tree and tree-to-string transducers.
In general,any grammar formalism whose grammars describederivations in terms of a finite set of states can typ-ically be converted into IRTG.2.3 Parsing IRTGsKoller and Kuhlmann (2011) present a uniformparsing algorithm for IRTGs based on tree au-tomata.
The (monolingual) parsing problem ofIRTG consists in determining, for an IRTG G andan input object a ?
A, a representation of the setparses(a) = {?
?
L(M) | h(?
)A= a}, i.e.
of thederivation trees that are grammatically correct andare mapped to a by the interpretation.
In the ex-ample, we have parses(?John walks on Mars?)
={?1}, where ?1is as above.
In general, parses(a)may be infinite, and thus we aim to representit using a tree automaton Chawith L(Cha) =parses(a), the parse chart of a.We can compute Chaas follows.
First, ob-serve that parses(a) = L(M) ?
h?1(terms(a)),where h?1(L) = {?
?
T?| h(?)
?
L} (the in-verse homomorphic image, or invhom, of L) andterms(a) = {t ?
T?| tA= a}, i.e.
the setof all terms that evaluate to a.
Now assume thatthe algebra A is regularly decomposable, whichmeans that every a ?
A has a decomposition au-tomatonDa, i.e.
there is a tree automatonDasuchthat L(Da) = terms(a).
Because regular tree lan-guages are closed under invhom and intersection,we can then compute a tree automaton Chaby in-tersecting M with the invhom of Da.To illustrate the IRTG parsing algorithm, let uscompute a chart for the sentence s = ?John walkson Mars?
with the example grammar G of Fig.
1.The states of the decomposition automaton Dsarespans [i, k] of s; the final state is XF= [1, 5].
Theautomaton contains fourteen rules, including theones shown in Fig.
3a.2044[1, 5]?
?
([1, 2], [2, 5])[2, 5]?
?
([2, 3], [3, 5])[3, 5]?
?
([3, 4], [4, 5])[3, 4]?
on[4, 5]?
Mars(a) Some rules of Ds.
[1, 5]?
r1([1, 2], [2, 5])[2, 5]?
r4([2, 3], [4, 5])[2, 4]?
r1([2, 3], [3, 4])[1, 2]?
r2[2, 3]?
r3(b) Some rules of I = h?1(Ds).S[1, 5]?
r1(NP[1, 2],VP[2, 5])VP[2, 5]?
r4(VP[2, 3],NP[4, 5])NP[1, 2]?
r2VP[2, 3]?
r3NP[4, 5]?
r5(c) The parse chart Chs.Figure 3: Example rules for the sentence s = ?John walks on Mars?Algorithm 1 Naive bottom-up intersection1: initialize agenda with state pairs for constants2: initialize P as empty3: while agenda is not empty do4: T?X??
pop(agenda)5: add T?X?to P6: for T??X???
P do7: for {T1X1, T2X2} = {T?X?, T??X??}
do8: for T ?
r(T1, T2) in MLdo9: for X ?
r(X1, X2) in MRdo10: store TX ?
r(T1X1, T2X2)11: add TX to agenda if newWe can then compute the invhom automaton I ,such that L(I) = h?1(L(Ds)).
I uses the samestates as Ds, but uses terminal symbols from ?
in-stead of ?.
Some rules of the invhom automaton Iin the example are shown in Fig.
3b.
Notice that Ialso contains rules that are not consistent with M ,i.e.
that would not occur in a grammatical parseof the sentence, such as [2, 4] ?
r1([2, 3], [3, 4]).Finally, the chart Chsis computed by intersectingM with I (see Fig.
3c).
The states of Chsare pairsof states from M and states from I .
It accepts ?1,because ?1?
parses(s).
Observe the similarity toa traditional context-free parse chart.3 Bottom-up intersectionBoth the practical efficiency of this algorithmand its asymptotic complexity depend crucially onhow we compute intersection and invhom.
We il-lustrate this using an overly naive intersection al-gorithm as a strawman, and then analyze the prob-lem to lay the foundations for the improved algo-rithms in Sections 4 and 5.Let?s say that we want to compute a tree au-tomaton C for the intersection of a ?left?
automa-ton MLand a ?right?
automaton MRboth overthe same signature ?.
In the application to IRTGparsing, MLis typically the derivation tree au-tomaton (called M above) and MRis the inv-hom of a decomposition automaton.
As in theproduct construction for finite string automata, thestates of C will be pairs TX of states T of MLand states X of MR, and the rules of C willall have the form TX ?
r(T1X1, .
.
.
, TnXn),where T ?
r(T1, .
.
.
, Tn) is a rule in ML, andX ?
r(X1, .
.
.
, Xn) is a rule in MR.3.1 Naive intersectionA naive bottom-up algorithm is shown in Alg.
1.1This algorithm maintains an agenda of state pairsthat have been discovered, but not explored aschildren of bottom-up rule applications; and achart-like set P of all state pairs that have everbeen popped off the agenda.
The algorithm main-tains the invariant that if TX is on the agenda orin P , then T and X are partners (written T ?
X),i.e.
there is a tree t ?
T?such that T ?
?t in MLand X ?
?t in MR.The agenda is initialized with all state pairsTX , for which MLhas a rule T ?
r and MRhas a rule X ?
r for some nullary symbol r ?
?.Then, while there are state pairs left on the agenda,Alg.
1 pops a state pair T?X?off the agenda andadds it to P ; iterates over all state pairs T??X?
?inP ; and queriesMLandMRbottom-up for rules inwhich these states appear as children.2The itera-tion in line 7 allows T?and X?to be either left orright children in these rules.
For each pair of leftand right rules, the rules are combined into a ruleof C, and the pair of the parent states T and X isadded to the agenda.This naive intersection algorithm yields anasymptotic complexity for IRTG parsing that ishigher than expected.
Assume, for example,1We assume binary symbols for simplicity; all algorithmsgeneralize to arbitrary arities.2For the invhom automaton this can be done by substi-tuting the variables in the homomorphic image h(r) with thecorresponding states X?and X?
?, and running the decompo-sition automaton on the resulting tree.2045Algorithm 2 Bottom-up intersection with BU1: initialize agenda with state pairs for constants2: generate new Sr= S(MR, r) for every r ?
?3: while agenda is not empty do4: T?X??
pop(agenda)5: for T ?
r(T1, T2) in MLs.t.
Ti= T?do6: for X ?
r(X1, X2) ?
BU(Sr, i,X?)
do7: store rule TX ?
r(T1X1, T2X2)8: add TX to agenda if newthat we are parsing with an IRTG encoding of acontext-free grammar, i.e.
with a string algebra(as in Fig.
1).
Then the states of MRare spans[i, k], i.e.
MRhas O(n2) states.
Once line 4 haspicked a span X?= [i, j], line 6 iterates over allspans X?
?= [k, l] that have been discovered sofar ?
including ones in which j 6= k and i 6= l.Thus the bottom-up lookup in line 9 is executedO(n4) times, most of which will yield no rules.The overall runtime of Alg.
1 is therefore higherthan the asymptotic runtime ofO(n3) expected forcontext-free parsing.
Similar problems arise forother algebras; for instance, the runtime of Alg.
1for TAG parsing is O(n8) rather than O(n6).3.2 IndexingIn context-free parsing algorithms, such as CKYor Earley, this issue is addressed through appro-priate index datastructures, which organize P suchthat the lookup in line 5 only returns state pairswhere X?
?is of the form [j, k] or [k, i].
This re-duces the runtime to cubic.The idea of obtaining optimal asymptotic com-plexities in IRTG parsing through appropriate in-dexing was already mentioned from a theoreti-cal perspective by Koller and Kuhlmann (2011).However, they assumed an optimal indexing datastructure as given.
In practice, indexing requiresalgebra-specific knowledge about X??
: A CKY-style index only works if we assume that the statesof the decomposition automaton are spans (this isnot the case in other algebras), and that the onlybinary operation in the string algebra is ?, whichcomposes spans in a certain way.
Furthermore, inIRTG parsing the rules of the invhom automatondo not directly correspond to algebra operations,but to terms of operations, which further compli-cates indexing.In this paper, we incorporate indexing into theintersection algorithm through sibling-finders.
Asibling-finder S = S(M, r) for an automaton Mand a label r in M ?s signature is a data structurethat supports a single operation, BU(S, i,X?).
Werequire that a call to BU(S, i,X?)
returns the setof rules X ?
r(X1, .
.
.
, Xn) of M such that X?is the i-th child state, and for every j 6= i, Xjmust be a state for which we previously calledBU(S, j,Xj).
Thus a sibling-finder performs abottom-up rule lookup, changing its state aftereach call by caching the state and position.Assume that we have sibling-finders for MR.Then we can modify the naive Alg.
1 to the closelyrelated algorithm shown as Alg.
2.
This algorithmmaintains the same agenda as Alg.
1, but insteadof iterating over all explored partner states T??X?
?,it first iterates over all rules in MLthat have T?asa child (line 5).
In line 6, Alg.
2 then queries MRsibling-finders ?
we maintain one for each rule la-bel ?
for right rules with matching rule label andchild positions.
Note that because there is only onerule with label r inML, the sibling-finders implic-itly keep track of the partners of T2we have seenso far.
Thus they play the role of a more structuredvariant of P .There are a number of ways in which sibling-finders can be implemented.
First, they couldsimply maintain sets chi(Sr, i) where a call toBU(Sr, i,X?)
first adds X?to chi(Sr, i).
Thequery can then iterate over the set chi(Sr, 3?i), tocheck for each stateX?
?in that set whetherMRac-tually contains a rule with terminal symbol r andchildren X?and X??
(in the right order).
This es-sentially reimplements the behavior of Alg.
1, andcomes with the same complexity issues.Second, we could theoretically iterate over allrules of MRto implement the sibling finders viaa bottom-up index (e.g., a trie) that supports effi-cient BU queries.
However, in IRTG parsing MRis the invhom of a decomposition automaton.
Be-cause the decomposition automaton represents allthe ways in which the input object can be built re-cursively out of smaller structures, including oneswhich will later be rejected by the grammar, suchautomata can be very large in practice.
Thus wewould like to work with a lazy representation ofMRand avoid iterating over all rules.4 Efficient bottom-up lookupFinally, we can exploit the fact that in IRTGparsing, MRis the invhom of a decompositionautomaton.
Below, we first show how to de-fine algebra-specific sibling-finders for decompo-2046Algorithm 3 passUpwards(Y, pi, i, r)1: rules?
BU(Sr,pi, i, Y )2: if pi = pi?k 6=  then3: for X ?
f(X1, .
.
.
, Xn) ?
rules do4: passUpwards(X,pi?, k, r)sition automata.
Then we develop an algebra-independent way to generate invhom sibling-finders out of those for the decomposition au-tomata.
These can be plugged into Alg.
2 toachieve the expected parsing complexity.4.1 Sibling-finders for decompositionautomataFirst, consider the special case of sibling-findersfor a decomposition automaton D. The terminalsymbols f of D are the operation symbols of analgebra.
If we have information about the opera-tions of this algebra, and how they operate on thestates of D, a sibling-finder S = S(D, f) can useindexing specific to the operation f to look up po-tential siblings, and only for them query D to an-swer BU(S, i,X)For instance, a sibling-finder for the ???
op-eration of the string algebra may store all states[k, l] for i = 1 under the index l. Thus a lookupBU(S, 2, [l,m]) can directly retrieve siblings fromthe l-bin, just as a traditional parse chart would.Spans which do not end at l are never consid-ered.
Different algebras require different indexstructures.
For instance, sibling-finders for thestring-wrapping operation in the TAG string alge-bra might retrieve all pairs of substrings [k, l,m, o]that wrap around [l,m] instead.
Analogous datastructures can be defined for the s-graph algebra.4.2 Sibling-finders for invhomWe can build upon the D-sibling-finders to con-struct sibling-finders for the invhom I of D.The basic idea is as follows.
Consider the termh (r1) = ?
(x1, x2) from Fig.
1.
It contains a sin-gle operation symbol ?
(plus variables); the homo-morphism only replaces one symbol with another.Thus a sibling-finder S(D, ?)
from the decompo-sition automaton can directly serve as a sibling-finder S(I, r1).
We only need to replace the ?
labelon the returned rules with r1.In general, the situation is more complicated,because t = h(r) may be a complex term con-sisting of many algebra operations.
In such acase, we construct a separate sibling-finder Sr,pi=new S(D, t(pi)) for each node pi with at least twochildren.
For instance, consider the term t =h (r4) in Fig.
1.
It contains three nodes whichare labeled by algebra operations, two of whichare the concatenation.
We decorate these with thesibling-finders Sr4,and Sr4,1.
Each of these is asibling-finder for the algebra?s concatenation op-eration; but they may have different state becausethey received different queries.We can then construct an invhom sibling-finder Sr= S(I, r), which answers a queryBU(Sr, i,X?)
in two steps.
First, we substitute thevariable xiby the state X?and percolate it upwardthrough t using the D-sibling-finders on the pathfrom xito the root.
If pi = pi?k is the path to xi,we do this by calling passUpwards(X?, pi?, k, r),as defined in Alg.
3.
If the local sibling-finder re-turns rules and we are not at the root yet, we recur-sively call passUpwards at the parent node pi?witheach parent state of these rules.As we do this, we let each sibling-finder main-tain the set of rules it found, indexed by theirparent state.
This allows us to perform the sec-ond step: we traverse t top-down from the rootto extract the rules of the invhom automaton thatanswer the BU query.
Recall that BU(Sr, i,X?
)should return only rules X ?
r(X1, X2) whereBU(Sr, 3?
i,X3?i) was called before.
Here, thisis guaranteed by having distinct D-sibling-findersSpi,rfor every node pi at every tree h(r).
A finaldetail is that before the first query to r, we initial-ize the sibling-finders by calling passUpwards forall the leaves that are labeled by constants.This process is illustrated in Fig.
4, on thesibling-finder S = S(I, r4) and the input string?John walks on a hill on Mars?, parsed with asuitable extension of the IRTG in Fig.
1.
The de-composition automaton can accept the word ?on?from states [3, 4] and [6, 7], which during initial-ization are entered into position 1 of the lowerD-sibling-finder Sr4,2, indexed by their end po-sitions (a).
Alg.
2 may then generate the queryBU(S, 2, [4, 6]).
This enters [4, 6] into the lowerD-sibling-finder, and because there is a state withend position 4 on the left side of this sibling-finder, BU(Sr4,2, 2, [4, 6]) returns a rule with par-ent [3, 6].
The parent is subsequently entered intothe upper sibling-finder (b).
Finally, the queryBU(S, 1, [2, 3]) enters [2, 3] into the upper D-sibling-finder and discovers its sibling [3, 6] (c).This yields a state X = [2, 6] for the whole phrase2047?[?
?]x1?
[4 : [3,4] ?
]7 : [6,7]on x2(a) After initialization.?[?
3 : [3,6]]x1?
[4 : [3, 4] 4 : [4,6]]7 : [6, 7]on x2(b) After BU(S, 2, [4, 6]).?
[3 : [2,3] 3 : [3, 6]]x1?
[4 : [3, 4] 4 : [4, 6]]7 : [6, 7]on x2(c) After BU(S, 1, [2, 3]).Figure 4: Three stages of BU on S(I, r4) for the sentence ?John walks on a hill on Mars?.Algorithm 4 Top-down intersection1: function expand(X):2: if X /?
visited then3: add X to visited4: for X ?
r(X1, X2) in MRdo5: call expand(Xi) for i = 1, 26: for T?r(T1, T2) s.t.
Ti?
prt(Xi) do7: store rule TX?r(T1X1, T2X2)8: add T to prt(X)?walks on a hill?.
The top-down traversal of thesibling-finders reveals that this state is reached bycombining x1= [2, 3], for which this BU queryasked, with x2= [4, 6], and thus the BU queryyields the rule [2, 6] ?
r4([2, 3], [4, 6]).
A sub-sequent query for BU(S, 2, [4, 8]) would yield therule [2, 8]?
r4([2, 3], [4, 8]), and so on.The overall construction allows us to answerBU queries on invhom automata while making useof algebra-specific index structures.
Given suit-able index structures, the asymptotic complexitydrops down to the expected levels, e.g.
O(n3) forIRTGs using the string algebra,O(n6) for the TAGstring algebra, and so on.
This yields a practicalalgorithm that can be flexibly adapted to new al-gebras by implementing their sibling-finders.5 Top-down intersectionInstead of investing into efficient bottom-upqueries, we can also explore the use of top-downqueries instead.
These ask for all rules with parentstateX and terminal symbol r. Such queries com-pletely avoid the problem of finding siblings inMR.
An invhom automaton can answer top-downqueries for r efficiently by running the decomposi-tion automaton top-down on h(r), collecting childstates at the variable nodes.
For instance, if wequery I from Section 2 top-down for rules withthe parent [1, 5] and symbol r1, it will enumer-ate the rules [1, 5] ?
r1([1, 2], [2, 5]), [1, 5] ?r1([1, 3], [3, 5]), and [1, 5] ?
r1([1, 4], [4, 5]),without ever considering any other combination ofchild states.This is the idea underlying the intersection al-gorithm in Alg.
4.
It recursively visits states X ofMR, collecting for each X a set prt(X) of statesT ofMLsuch that T ?
X .
Line 5 ensures that theprt sets have been computed for both child statesof the rule X ?
r(X1, X2).
Line 6 then does abottom-up lookup of MLrules with the terminalsymbol r and with child states that are partnersof X1and X2.
Applied to our running example,Alg.
4 parses ?John walks on Mars?
by recursivecalls on expand([1, 5]) and expand([2, 5]), fol-lowing the rules of I top-down.
Recursive callsfor [2, 3] and [4, 5] establish VP ?
prt([2, 3]) andNP ?
prt([4, 5]), which enables the recursive callfor [2, 5] to apply r4in line 6 and consequently addVP to prt([2, 5]) in line 8.The algorithm mixes top-down queries to MRwith bottom-up queries toML.
Line 6 implementsthe core idea of the CKY parser, in that it performsbottom-up queries on sets of nonterminals that arepartners of adjacent spans ?
but generalized to ar-bitrary IRTGs instead of just the string algebra.The top-down query to MRin line 4 is boundedby the number of rules that actually exist in MR,which isO(n3) for the string algebra,O(n6) in theTAG string algebra, and O(ns?
3dsds)for graphsof degree d and treewidth s ?
1 in the graph al-gebra.
Thus Alg.
4 achieves the same asymptoticcomplexity as native parsing algorithms.Condensed top-down intersection.
One weak-ness of Alg.
4 is that it iterates over all rulesX ?
r(X1, X2) of MRindividually.
This can beextremely wasteful when MRis the invhom of adecomposition automaton, because it may contain20480 10 20 30 40 50sentenceLength101102103104105106107runtime[ms]bottom-uptop-downtop-down cond.sibling-finderFigure 5: Runtimes for context-free parsing.a great number of rules that have the same statesand only differ in the terminal symbol r. For in-stance, when we encode a context-free grammar asan IRTG, for every rule r of the form A?
B C wehave h(r) = ?
(x1, x2).
The rules of the invhomautomaton are the same for all terminal symbols rwith the same term h(r).
But Alg.
4 iterates overrules r and not over different terms h(r), repeatingthe exact same computation for every binary ruleof the context-free grammar.To solve this, we define condensed tree au-tomata, which have rules of the form X ??
(X1, .
.
.
, Xn), where ?
?
?
is a nonempty setof symbols with arity n. A condensed automatonrepresents the tree automaton which for all con-densed rules X ?
?
(X1, .
.
.
, Xn) and all r ?
?has the rule X ?
r(X1, .
.
.
, Xn).
It is straight-forward to represent an invhom automaton as acondensed condensed automaton, by determiningfor each distinct homomorphic image t the set?t= {r1, .
.
.
, rk} of symbols with h(ri) = t.We can modify Alg.
4 to iterate over condensedrules in line 4, and to iterate in line 6 over the rulesT ?
r(T1, T2) for which Ti?
prt(Xi) and r ?
?.This bottom-up query to MLcan be answered ef-ficiently from an appropriate index on the rules ofML.
Altogether, this condensed intersection algo-rithm can be dramatically faster than the originalversion, if the grammar contains many symbolswith the same homomorphic image.6 EvaluationWe compare the runtime performance of the pro-posed algorithms on practical grammars and in-puts, from three very different grammar for-malisms: context-free grammars, TAG, and HRGgraph grammars.
In each setting, we measure the0 10 20 30 40 50sentenceLength102103104105106107runtime[ms]bottom-uptop-downtop-down cond.sibling-finderFigure 6: Runtimes for TAG parsing.1 2 3 4 5 6 7 8 9 10nodeCount101102103104105106107runtime[ms]bottom-uptop-downtop-down cond.sibling-finderGKT 15Figure 7: Runtimes for graph parsing.runtime of four algorithms: the naive bottom-upbaseline of Section 3; the sibling-finder algorithmfrom Section 4; and the non-condensed and thecondensed version of the top-down algorithm fromSection 5.
The results are shown in Figures 5, 6and 7.
We measure the runtimes for computingthe complete chart, and plot the geometric meanof runtimes for each input size on a log scale.We measured all runtimes on an Intel Xeon E7-8857 CPU at 3 GHz using Java 8.
The JVM waswarmed up before the measurements.
The parserfiltered each grammar automatically, removing allrules whose homomorphic image contained a con-stant that could not be used for a given input (e.g.,a word that did not occur in the sentence).PCFG.
We extracted a binarized context-freegrammar with 6929 rules from Section 00 of thePenn Treebank, and parsed the sentences of Sec-tion 00 with it.
The homorphism in the corre-sponding IRTG assigns every terminal symbol aconstant or the term ?
(x1, x2), as in Fig.
1.
As aconsequence, the condensed automaton optimiza-tion from Section 5 outperforms all other algo-2049rithms, achieving a 100x speedup over the naivebottom-up algorithm when it was cancelled.TAG.
We also extracted a tree-adjoining gram-mar from Section 00 of the PTB as described byChen and Vijay-Shanker (2000), converted it toan IRTG as described by Koller and Kuhlmann(2012), and binarized it, yielding an IRTG with26652 rules.
Each term h(r) in this grammarrepresents an entire TAG elementary tree, whichmeans the terms are much more complex thanfor the PCFG and there are much fewer terminalsymbols with the same homomorphic term.
As aconsequence, condensing the invhom is much lesshelpful.
However, the sibling-finder algorithm ex-cels at maintaining state information within eachelementary tree, yielding a 1000x speedup over thenaive bottom-up algorithm when it was cancelled.Graphs.
Finally, we parsed a corpus of graphsinstead of strings, using the 13681-rule graphgrammar of Groschwitz et al (2015) to parse the1258 graphs with up to 10 nodes from the ?LittlePrince?
AMR-Bank (Banarescu et al, 2013).
Thetop-down algorithms are slow in this experiment,confirming Groschwitz et al?s findings.
Again,the sibling-finder algorithm outperforms all otheralgorithms.
Note that Groschwitz et al?s parser(?GKT 15?
in Fig.
7) shares much code with oursystem.
It uses the same decomposition automata,but a less mature version of the sibling-findermethod which fully computes the invhom automa-ton.
Our new system achieves a 9x speedup forparsing the whole corpus, compared to GKT 15.7 Related WorkDescribing parsing algorithms at a high level ofabstraction has a long tradition in computationallinguistics, e.g.
in deductive parsing with parsingschemata (Shieber et al, 1995).
A key challengeunder this view is to index chart entries so theycan be retrieved efficiently, which parallels thesituation in automata intersection discussed here.G?omez-Rodr?
?guez et al (2009) present an algo-rithm that automatically establishes index struc-tures that guarantee optimal asymptotic runtime,but also requires algebra-specific extensions forgrammar formalisms that go beyond context-freestring grammars.Efficient parsing has also been studied in othergeneralized grammar formalisms beyond IRTG.Kanazawa (to appear) shows how the parsingproblem of Abstract Categorial Grammars (deGroote, 2001) can be translated into Datalog,which enables the use of generic indexing strate-gies for Datalog to achieve optimal asymptoticcomplexity.
Ranta (2004) discusses parsing for hisGrammatical Framework formalism in terms ofpartial evaluation techniques from functional pro-gramming, which are related to the step-by-stepevaluation of sibling-finders in Figure 4.
Like theapproach of Gomez-Rodriguez et al, these meth-ods have not been evaluated for large-scale gram-mars and realistic evaluation data, which makes ithard to judge their relative practical merits.Most work in the tree automata community hasa theoretical slant, and there is less research on theefficient implementation of algorithms for tree au-tomata than one would expect; Cleophas (2009)and Lengal et al (2012) are notable exceptions.Even these tend to be motivated by applicationssuch as specification and verification, where thetree automata are much smaller and much less am-biguous than in computational linguistics.
Thismakes these systems hard to apply directly.8 ConclusionWe have presented novel algorithms for comput-ing the intersection and the inverse homomorphicimage of finite tree automata.
These can be usedto implement a generic algorithm for IRTG pars-ing, and apply directly to any grammar formalismthat can be represented as an IRTG.
An evaluationon practical data from three different grammar for-malisms shows consistent speed improvements ofseveral orders of magnitude, and our graph parserhas the fastest published runtimes.A Java implementation of our algorithms isavailable as part of the Alto parser, http://bitbucket.org/tclup/alto.We focused here purely on symbolic parsing,and on computing complete parse charts.
In thepresence of a probability model (e.g.
for IRTG en-codings of PCFGs), our algorithms could be madefaster through the use of appropriate pruning tech-niques.
It would also be interesting to combinethe strengths of the condensed and sibling-finderalgorithms for further efficiency gains.Acknowledgments.
We thank the anonymousreviewers for their comments.
We are gratefulto Johannes Gontrum for an early implementationof Alg.
4, and to Christoph Teichmann for manyfruitful discussions.
This work was supported bythe DFG grant KO 2916/2-1.2050ReferencesLaura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
2013.
Abstract meaning representationfor sembanking.
In Proceedings of the LinguisticAnnotation Workshop (LAW VII-ID).John Chen and K. Vijay-Shanker.
2000.
Automatedextraction of TAGs from the Penn Treebank.
In Pro-ceedings of IWPT.David Chiang, Jacob Andreas, Daniel Bauer,Karl Moritz Hermann, Bevan Jones, and KevinKnight.
2013.
Parsing graphs with hyperedgereplacement grammars.
In Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguistics (ACL).David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Stephen Clark and James Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Loek Cleophas.
2009.
Forest FIRE and FIRE Wood:Tools for tree automata and tree algorithms.
In Pro-ceedings of the Conference on Finite-State Methodsand Natural Language Processing (FSMNLP).Hubert Comon, Max Dauchet, R?emi Gilleron, Flo-rent Jacquemard, Denis Lugiez, Christof L?oding,Sophie Tison, and Marc Tommasi.
2008.
Treeautomata techniques and applications.
http://tata.gforge.inria.fr/.Philippe de Groote.
2001.
Towards abstract catego-rial grammars.
In Proceedings of the 39th ACL/10thEACL.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of HLT/NAACL.Carlos G?omez-Rodr?
?guez, Jes?us Vilares, and Miguel A.Alonso.
2009.
A compiler for parsing schemata.Software: Practice and Experience, 39(5):441?470.Jonathan Graehl, Kevin Knight, and Jonathan May.2008.
Training tree transducers.
ComputationalLinguistics, 34(3).Jonas Groschwitz, Alexander Koller, and Christoph Te-ichmann.
2015.
Graph parsing with s-graph gram-mars.
In Proceedings of the 53rd ACL and 7th IJC-NLP.Aravind K. Joshi and Yves Schabes.
1997.
Tree-Adjoining Grammars.
In G. Rozenberg and A. Salo-maa, editors, Handbook of Formal Languages, vol-ume 3, pages 69?123.
Springer-Verlag, Berlin.Laura Kallmeyer and Wolfgang Maier.
2013.
Data-driven parsing using probabilistic linear context-free rewriting systems.
Computational Linguistics,39(1):87?119.Makoto Kanazawa.
to appear.
Parsing and genera-tion as datalog query evaluation.
IfCoLog Journalof Logics and Their Applications.Alexander Koller and Marco Kuhlmann.
2011.
A gen-eralized view on parsing and translation.
In Pro-ceedings of the 12th International Conference onParsing Technologies (IWPT).Alexander Koller and Marco Kuhlmann.
2012.
De-composing TAG algorithms using simple alge-braizations.
In Proceedings of the 11th TAG+ Work-shop.Ondrej Lengal, Jiri Simacek, and Tomas Vojnar.
2012.Vata: A library for efficient manipulation of non-deterministic tree automata.
In C. Flanagan andB.
K?onig, editors, Tools and Algorithms for the Con-struction and Analysis of Systems: 18th Interna-tional Conference, TACAS 2012.
Springer.Mike Lewis and Mark Steedman.
2014.
A* CCG pars-ing with a supertag-factored model.
In Proceedingsof EMNLP.Aarne Ranta.
2004.
Grammatical framework: A type-theoretical grammar formalism.
Journal of Func-tional Programming, 14(2):145?189.Nina Seemann, Fabienne Braune, and Andreas Maletti.2015.
String-to-tree multi bottom-up tree transduc-ers.
In Proceedings of the 53rd ACL and 7th IJC-NLP.Stuart M Shieber, Yves Schabes, and Fernando CNPereira.
1995.
Principles and implementation of de-ductive parsing.
The Journal of logic programming,24(1):3?36.Mark Steedman.
2001.
The Syntactic Process.
MITPress, Cambridge, MA.Yuk Wah Wong and Raymond J. Mooney.
2006.Learning for semantic parsing with statistical ma-chine translation.
In Proceedings of the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics (HLT/NAACL-2006).2051
