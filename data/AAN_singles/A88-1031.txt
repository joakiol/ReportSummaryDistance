MORPHOLOGICAL  PROCESSINGIN THE NABU SYSTEMJonathan SlocumMicroelectronics and ComputerTechnology Corporation (MCC)3500 West Balcones Center DriveAustin, Texas 78759ABSTRACTThe Nabu morphological processor isdesigned to perform a number of different func-tions, of which five have so far been identified:analysis, guessing (about unknown words), syn-thesis, defaulting (proposing the most likely in-flectional paradigm for a new base form), andcoding (producing all possible inflectionalparadigm variants for a new base form).
Com-plete or very substantial analyzers have beenproduced for a number of diverse languages;other functions have been implemented as well.This paper discusses our design philosophy, aswell as our technique and its implementation.INTRODUCTIONNabu is a multilingual Natural LanguageProcessing system under development in theHuman Interface Laboratory at MCC, forshareholder companies.
Its morphological com-ponent is designed to perform a number of dif-ferent functions.
This has been used to producea complete analyzer for Arabic; very substantialanalyzers for English, French, German, andSpanish; and small collections of rules for Rus-sian and Japanese.
In addition, other functionshave been implemented for several of these lan-guages.In this paper we discuss our philosophy,which constrained our design decisions; elaboratesome specific functions a morphological com-ponent should support; survey some competingapproaches; describe our technique, whichprovides the necessary functionality while meet-ing the other design constraints; and support ourapproach by characterizing our success indeveloping/testing processors for various com-binations of language and function.DES IGN PHILOSOPHYBefore we set about designing our mor-phological processor, we elaborated ourphilosophical commitments regarding an NLPsystem.
These include: (1) multilingual applica-tion, (2) fault-tolerant, fail-soft behavior, (3)rule reversibility, (4) disparate functionality, (5)inherent parallelism, (6) grammatical clarity,and (7) rigorous testing.MULT IL INGUAL APPL ICAT IONThe algorithms and their software instantia-tion must admit application to any human lan-guage likely to confront our system; these in-clude the languages of major demographicand/or economic significance (and theirrelatives).
Our selected representatives areEnglish, French, German, Spanish, Russian,Arabic, Chinese, and Japanese.FAULT-TOLERANT,  FA IL -SOFTBEHAVIORReal-world NLP applications, whether fortext or interactive dialog, will be confrontedwith numerous errors of various types.
As far asusers are concerned, guaranteed failure in thepresence of any error is intolerable: a systemmust overcome simple mistakes without discern-able problems.
For example, insignificant typingand/or spelling mistakes should be ignored, asshould minor morphological blunders.
Users donot like to be asked for corrections of(seemingly) simple mistakes, and of courseprinted texts cannot be queried in any case.
Inthe presence of more serious problems, perfor-mance should degrade only gradually.
This isnothing more than a commitment tounderstanding the utterance, rather thanpunishing the user for errors in it.
We contendthat human-like fault-tolerant, fail-soft behaviormust be incorporated in the fundamental designof a system: it cannot be tacked-on after systemdevelopment.
Creating an applied system for a"perfect" natural language that is hypothesized,but never observed, is misguided, aside from be-ing wasteful.RULE  REVERSIB IL ITYTo the extent feasible and reasonable, thelinguistic rules in an NLP system should be re-versible -- useful for both analysis and synthesis.But it is not enough to have one, completely re-versible grammar performing two functions.Indeed, reversibility may not be alwaysdesirable: in keeping with the commitment ofault-tolerant, fail-soft behavior, an analyzershould over-generate (accepting some incorrect228forms as input), whi lea  synthesizer should neverproduce them as output.
Since these two func-tions are, therefore, rather different processes,one must search for a means to distinguish therules (linguistic descriptions) from the strategies(linguistic processes, called grammars) controll-ing their application: the former can be revers-ible, while the latter might not be.D ISPARATE FUNCTIONAL ITYAnalysis and synthesis constitute two obviouslinguistic processes (grammars imposed uponrule sets).
There are, however, even moreprocesses than these of interest in a practical set-ting: that is, there may be a number of gram-mars built from a single set of linguistic rules, aswe demonstrate below.
Thus, a processor mustadmit the simultaneous instantiation of a num-ber of grammars in order to be called general.INHERENT PARALLEL ISMAcceptable runtime performance in any sig-nificant real-world NLP setting is now under-stood to require implementation on parallelmachines.
Thus, grammars must be inherentlysuited to parallel execution, and such oppor-tunities must somehow be representable in theformalism in which the grammars are expressed.GRAMMATICAL  CLARITYIn any real-world application, the number oflinguistic rules and the complexities of grammarsimposed upon them is considerable.
Successfulimplementation and maintenance thus requiresthat the grammars be clearly and conciselystated, however powerful they may be.
Not onlymust the rule formalism be relatively clean andsimple, but also a grammar must be viewable atvarious levels of detail.
Variable granularity inthe presentation enhances the opportunity forcomprehensibility.R IGOROUS TEST INGIn order to become practical, a system mustadmi t - -  and have undergone -- rigorous testing.It is not enough to develop a micro-implementation, then claim that the system canbe scaled up to become a real application(presumably to be tested on end-users).
Morethan just a philosophical commitment o theidea of testing, this requires that the system ac-tually be fast enough during the developmentphase that thorough testing (of grammars, etc.
)can take place at that time.
If its speed is gla-cial during the development phase, a complexsystem cannot be completed, and its practicalitywill never be shown.MORPHOLOGICAL  PROCESSESWe have, so far, identified five interestingmorphological processes: analysis, guessing, syn-thesis, defaulting, and coding.
The first twoconcern comprehension; the other three, produc-tion.ANALYS ISMorphological analysis is a relatively well-understood notion -- which is not to say thatthere is agreement concerning what the resultshould be, or how it should be produced.
But,generally speaking, analysis is agreed to involvethe decomposition of a surface-form string(usually in English called a word) into its con-stituent base-form morphemes and their func-tional attachments; this may be finer-grained, aswhen each morpheme is associated with lexicalor other linguistic information, and indeed theprocess is usually understood to imply access toa stored lexicon of morphemes, in order to cor-rectly identify those contained in the string.Analysis is assumed to perform this decomposi-tion according to a set of language-specificstrategies (i.e., a grammar) limiting the possibledecompositions.GUESS INGIn keeping with a commitment to fault-tolerant, fail-soft behavior, a system must, e.g.,deal with unknown words in an uttterance bymaking plausible guesses regarding their mor-phological, lexical, syntactic, and even semanticproperties.
A morphological guess inggrammar ,  presumably operating after theanalysis grammar has failed, must embodyheuristic strategies, and these may well differfrom those of the analyzer, even though the rulestock upon which they draw might be identical.For example, while the analyzer must, sooner orlater, entertain all possible decompositionhypotheses, the guesser might best be con-strained to produce only the "mostl ikely/plausible" hypotheses.SYNTHESISSynthesis, like analysis, is a relatively well-understood notion, albeit characterized bydebate concerning the details.
Generally speak-ing, synthesis is the composition of a surface-form string encoding the information containedin one or more base-form morphemes havingknown functional attachments.
Synthesis, likeanalysis, is assumed to perform this compositionas dictated by a grammar.
Note again that, inpractice, this grammar cannot be the simple in-verse of the one controlling analysis: a syn-thesizer should be prohibited from making mis-takes that are tolerated (if, indeed, even noticed)by an analyzer.229DEFAULT INGGuessing is relevant to end-users, dealing asit does with unknown words in an input ut-terance.
Developers, on the other hand, facedwith teaching the system \[how to synthesizeonly\] the correct surface forms of words beingdefined, can make use of additional functions,such as a de fau l t ing  grammar .
Given a rootor stem form and part of speech, a lexicaldefaulter can propose the most likely inflectionalparadigm for a word.
This is better than requir-ing the lexicographer to type in the variants, ormanually encode the paradigm in some otherfashion: greater human reliability is experiencedwhen validating good guesses and correcting afew wrong ones than when entering everythingfrom scratch.CODINGWhen the lexical defaulter guesses incor-rectly, a cod ing grammar  could render allpotential inflectional paradigms (as permitted bythe language), from which the lexicographercould select the correct one(s) for the newly-defined word.
This is desirable, because greaterhuman reliability is experienced in selecting fromamong possible inflections than in producing alland only the correct variants.SURVEYOne of the more popular frameworks formorphological processing is the two-level modeldeveloped by Koskenniemi \[1983\], modified andextended by Karttunen \[1983\].
Two-level modelsare fully reversible, performing both analysisand synthesis, and correspond to finite-statemachines, hence they appear to enjoy some com-putational advantages.
However, there are boththeoretical and practical problems.
It appearsthat the model is not sufficiently powerful to ac-count for some human languages (e.g., Icelandic,which exhibits recursive vowel harmony).
Themodel can be decidedly inelegant in somerespects (e.g., in handling alternations uch asthe English nominate/nominee by positing a"lexical entry" nomin).
There is a very sub-stantial time penalty involved in compiling two-level grammars (it may be measured in hours),which impedes rapid debugging and testing.Finally, the "advantages" of reversibility are ar-guable, for reasons discussed above and below.Affix-stripping models are commonlyemployed, especially for English \[Slocum, 1981\].The IBM system \[Byrd, 1983\] also uses affixrules in a strictly word-based model of morphol-ogy, but the rules are unordered and thus onlymarginally may be said to constitute a gram-mar; certainly, morphosyntactic behavior is nothighlighted.
These two systems were developedfor English analysis only; they are not reversiblein any sense, and have hot been tested on otherlanguages.
A serendipitous ituation exists, inthat they have a certain degree of fault-tolerancebuilt in, though this was not a goal of thedesign/implementation process.In order to elucidate morphosyntax, some ap-proaches use a simple (e.g., two-level, or similar)model to account for orthographic behavior, anda "grammar of words" to analyze morpheme se-quences yntactically (e.g., \[Bear, 1986\], \[Russellet al, 1986\]).
It does not seem that these ap-proaches, as described, lend themselves to anysort of reversal, or other form of rule-sharing;furthermore, only analysis is mentioned as anapplication.Even simpler models have employed allomor-phic segmentation; morphosyntax may be ap-proximated by a longest-match heuristic\[Pounder and Kommenda, 1986\] or defined by afinite-state grammar \[Bennett and Slocum,1985\].
The required entry of every allomorphicvariant of every word is both a theoretical and apractical disadvantage, but runtime processing isspeeded up as a positive consequence due to totalneglect of spelling changes.
Fault-tolerant be-havior is not built-in, but can be almost triviallyadded (whereas, in many other models, it wouldbe difficult to incorporate).
The systems citedhere are used for analysis only.No previous models of morphology seem tohave been used for anything but analysis, andoccasionally synthesis; the other three functionsmentioned above, and others that might exist,are neglected.
Although the author has dis-cussed other functionality in earlier work\[Slocum and Bennett, 1982\], even there the mor-phological processors used for analysis, synthesis,and defaulting/coding were distinct, being im-plemented by entirely different softwaremodules, and shared only the dictionary entries.THE NABU TECHNIQUEIn Nabu, rules are separate from thestrategies imposed upon them.
Rules may bethought of as declarative in nature; they are or-ganized in an inheritance hierarchy.
The"grammars of words" imposed upon them,however, may be thought of as procedures -- ac-tually, dataflow networks.RULE H IERARCHYThe structure of the rule hierarchy is deter-mined by linguists, purely for their own con-venience, and implies no runtime behavior ofany kind.
That is, the rule hierarchy is purelystatic and declarative in nature.
The one con-straint is that, at the top level, collections ofrules are distinguished by the language theybelong to -- i.e., the first division is by language.Typically, though not necessarily, the second-230level division imposed by the lingusts is that ofcategory: the part-of-speech of the surface-formsfor which the rule subset is relevant.
For lan-guages that distinguish inflection from deriva-tion, our linguists have generally found it con-venient to divide rules at the third level in termsof these two classes.
Other than the top-levellanguage division, however, the structure of thehierarchy is entirely at the discretion of theresponsible linguists.
Consequently, we have ob-served that different linguists - each responsiblefor the morphological rules (and grammars) ofentire languages - prefer and employ differentorganizational principles.Rules may also be thought of as declarativein nature -- though, in fact, for the purposes ofmaintenance they embody certain procedural be-haviors such as a self-test facility.
A mor-phological rule is an equation between one letter-string+feature-set, which we call a g losseme,and another.
One side of the equation describeswhat might be called the "surface" side of aglosseme, as it represents an encoding of infor-mation nearer to (but not necessarily at!)
thesurface-string level: all this really means is thatrelatively more information is expressed in thestring part than in the feature part.
The otherside, in turn, describes what might be called the"base" glosseme, as it represents an encoding ofinformation closer to (but not necessarily at!
)the base-form level, with relatively less infor-mation expressed in the string part than in thefeature part.
It is important o note that the in-formation content of the two sides is the same- only the form of the information changes.This is why we classify a rule as an equat ion ,and this also admits rule reversibility in the im-plementation.As a trivial example of such an equation,consider the English inflectional rule\ ["+s" 0\] -~ \ [ "+"  (NOUN PL)\].The "surface" side, on the left, describes aglosseme string whose last character is the letters \[by means of the pattern "-t-s"\] and places noconstraints on the glosseme's feature set \[bymeans of the empty list 0\].
The "base" side, onthe right, describes an equivalent glosseme whosestring lacks the final letter 8 \[by means of thepattern "+" \ ]  and constrains the feature set \[bymeans of the two features (NOUN PL)\].
Ex-ecuted in one direction, this rule removes an 8and adds the two features (NOUN PL); reversed,the rule removes the two features (NOUN PL)and adds the morpheme s. Obviously, thisEnglish rule conveys the notion that a NOUNmay be inflected for PLural by means of the\[bound\] morpheme s at its right end.The plus sign (+) in a pattern is used to in-dicate whether prefixation or suffixation of theglosseme string is being described, depending onwhether the pattern precedes or follows it; or apattern may describe an entire glosseme stringvia omission of the sign.
In a pattern, al-phabetic case is important: a lower-case lettersignifies a constant, and must be matched by thesame letter (in any case) in the glosseme; anupper-case letter signifies a restricted variable,and must be matched by some letter from theset over which it is defined.
Thus, for example,in English rules we use the letter V to stand forVowel; C, for Consonant; and G, for Gemminat-ing consonant.
(Of course, the variable restric-tions are entirely arbitrary as far as Nabu isconcerned.
Each linguist defines sets of vari-ables and their match restrictions according totaste and the language at hand.
)If the same variable appears more than oncein a pattern, it is required to match the sameletter in the glosseme: the equation\ [ "+GGing"  01 = \ [ "+G"  (VERB PRPL)\]thus describes doubling of the last consonantin an English verb, before suffixation of thepresent participial morpheme.
Another facilityis required for convenience in describing alter-nation.
In German, for example, certain mor-phological operations involve the umlauting ofvowels; thus, an unmarked vowel on the "base"side may require replacement by an umlautedvowel on the "surface" side.
If only one vowelbehaved this way, this would be no problem: thecorresponding letters would simply appear intheir places in the patterns.
But there are threevowels that behave like this in German (a, o,and u) in the identical context.
In order toeliminate the need for tripling the size of therule base otherwise required to describe thisphenomenon, we provide the linguists with ameans for pairing-up variables, so that a charac-ter matching one may be replaced by the cor-responding character in the other's set.
Manyother languages exhibit this kind of alternation,making this a useful technique.A character in a pattern string matches oneand only one character in a glosseme string.Generally speaking, the characters appearing ina pattern string are those of the language beingdescribed, as one would expect.
Some languages,however, lack a case distinction -- Arabic, for ex-ample -- rendering the variable notation prob-lematic.
In this situation, upper-case lettersfrom the Roman alphabet are used to representvariables in rules.Given this framework, creating a bidirec-tional rule interpreter is relatively straightfor-231ward.
Rule execution is a matter of matchingaccording to one side of the equation and (if thatis successful) transforming according to the otherside.
So long as every rule is truly an equation(and only a human linguist can decide this, ex-cept in trivial cases) then every rule is reversible-- that is, can be used for comprehension as wellas production, because thc interpreter can trans-form a rule's output back into the original in-put.
In Nabu, as noted curler, there are cur-rently two comprehension processes (analysis andguessing) and three production processes(synthesis, defaulting, and coding).
But neithercollections of rules nor their hierarchical struc-ture describes uch functionality.CONTROL GRAPHSA morphological grammar is an executionstrategy imposed upon a bocly of morphologicalrules.
Bearing in mind that morphological rulescan apply to the outputs of other rules (considerthe derivational process in English, as for ex-ample when the word derivational is constructedfrom derive + ation + al), and that such com-pounding is not free, but linguistically con-strained, it is obvious that the compoundingconstraints -- as well as the individual rulesthemselves -- must be accounted for.
In Nabu,an execution strategy is represented as adataflow network, which we loosely term acontrol graph.A control graph is a collection of nodes con-nected by directed arcs.
Each node is composedof a bundle of morphological rules, which maybe applied when input reaches that node, andwhose output may be passed across arcs to othernodes.
There will be one or more designatedstart nodes,  where input appears and process-ing begins (conceptually, in parallel, if there aremultiple start nodes).
From each start node,there will be a path to one or more designatedtermina l  nodes,  where processing ends and thegraph's output is emitted.
The path may be ofarbitrary length; start nodes may themselves beterminal nodes.
In analysis, encountering a ter-minal node entails dictionary look-up; in syn-thesis, output of a surface-form.There are two types of arcs, Success andFai lure;  the former are arcs across whichsuccessfully-applied rules will pass their output,and the latter are arcs across which the originalinput to a node is passed, should no rule insideit be successful.
A successful rule is one whoseinput side ("surface" or "base," depending onwhether the graph is engaged in comprehensionor production) matches the input glosseme, andwhose output side represents the same infor-mation as was present in the input, only refor-mulated.Conceptually, the rule~ inside a node may beexecuted in series or in paral le l .
The linguistcontrols this by setting a flag in each node; thus,some nodes may fire rules in parallel, whileothers fire their rules serially.
(In serial nodes,rule execution terminates as soon as one rulesucceeds; there is no backup.)
In either case, allsuccess arcs are traversed in parallel, or else allfailure arcs are traversed in parallel, dependingon whether any rule(s) succeeded -- meaning allpossible morphological analyses are produced forlater consideration.To take a simple example, consider the wordderivations, and assume that neither it nor thesingular form derivation is in the dictionary.An English analyzer graph might have multiplestart nodes, one of which is intended (by thelinguist) for inflected nouns.
The input glosseme\["derivations" O\]is thus passed to the node PLURAL-NOUN,which contains, among others, the rule\ ["+s" 0l = \ [ "+"  (NOUN PL)\].The suffix pattern "+s"  matches the glos-seme string, and no features are required to bepresent in the glosseme, thus this rule succeedsand produces the output glosseme\["derivation" (NOUN PL)\[.If PLURAL-NOUN has been marked as a ter-minal node (in addition to being a start node),then dictionary look-up will take place.
If so, byour assumption above it fails.
Our hypotheticalgraph contains a Success arc from PLURAL-NOUN to DEVERBAL-NOUN, which contains,among others, the rule\["+ation" (NOUN)I = ("+e" (VERB(DERIVE NOUN +ION))).When (and if) this rule fires, it would matchthe suffix (ation) in the glosseme stringderivation, and the feature (NOUN), and there-fore transform that glosseme intopL)f'.
'derive" (VERB (DERIVE NOUN +ION)Note the e restoration: rules can in principleremove and add any letter sequence, hence affixalternation is handled in a straightforward man-232ner.
If DEVERBAL-NOUN has been marked asa terminal node, then dictionary look-up willtake place: the VERB entry derive is retrieved.The glosseme feature li~t, in addition to indicat-ing the main-entry ("derive") and lexical catego-ry (VERB) to look for in the dictionary, containssufficient information to allow transformation ofthe stored entry for derive into a representationof its surface-form realization (derivations), interms of syntactic category (NOUN), sub-categorization features (PL), and semantics (themeaning of derive, transformed by +ION).There remains only one problem to accountfor: that of compositional vs. non-compositionalreadings.
Words with strictly non-compositionalreadings (e.g., fruitful, which is not computablefrom fruit and ful) simply must be stored in thedictionary; this is not a contentious claim.Words with strictly compositional readings (e.g.,derivation, which is computable from derive andation) may or may not be stored in the diction-ary: this is an efficiency consideration, based onthe usual time vs. space trade-off, and is also notsubject to significant debate -- at least, not withrespect o theoretical implications.The problem arises for cases where both com-positional and non-compositional readings existfor the same word (e.g., largely).
In such situa-tions, the non-compositional reading must ofcourse be stored, but it would be nice if this didnot require storage of the compositional readingas well.
In Nabu, we solve this by means of aDECOMPOSABLE flag that must appear withinthe non-compositional definition of a word, incase that word also has a compositional readingwhich is to be computed.
During the course ofmorphological analysis, performing successfuldictionary look-up (at a "terminal" node) willaffect subsequent processing: if the DECOM-POSABLE flag is noted, then the glosseme justsubmitted to dictionary .look-up will also bepassed across any success arcs leaving that node,for further analysis.
In the absence of aDECOMPOSABLE flag, successful dictionarylook-up marks a truly terminal case, and theresults are returned (possibly along with read-ings from other paths in the graph) forthwith.The operations of other types of controlgraphs are analogous to those of the analyzer.The principles are the same, with small excep-tions (some noted above), and so are not ex-emplified here.PROCESSORS IMPLEMENTEDIn addition to the rule and graph interpretersper se, delivered to MCC shareholders inmid-1985, Nabu includes a variety of tools sup-porting the development, esting, maintenance,and multilingual documentation of morphologi-cal rule hierarchies and grammars.
These tools(not described in this paper, for reasons of space)have been used to create a great many rules andseveral morphological processors.
Non-terminalsincluded, the English morph-rule hierarchy num-bers 626 nodes; French, 434; German, 493;Spanish, 1395; and Arabic, 882.
In addition tothese mature rule hierarchies, some preliminarywork has been done on the Russian and Japaneserule hierarchies.ANALYZERSThe English analyzer is complete withrespect to inflection; it has been successfullytested on, among other things, the entire collec-tion of inflectional variants presented inWebster's Seventh New Collegiate Dictionary(ca.
42,500 nouns, 8,750 verbs, and 13,250adjectives).
It also accounts for the great bulkof English derivation, as determined by variousword frequency lists, and is undergoing gradual,evolutionary extension to the missing (low-frequency) affixes and their combinations.
Afirst version of this grammar  was delivered toMCC shareholders in mid-1985, followed byupgrades in 1986 and 1987.
The currentanalyzer numbers 20 nodes and 60 arcs.As mentioned earlier, a complete Arabic mor-phological analyzer exists; so far as we areaware, it accounts for all morphologicalphenomena in the language -- no mean feat, fora language in which a single root form could intheory be realized as over 200,000 surface forms,and in which morphemes are frequently discon-tinuous (i.e., cannot be described by simple af-fixation models) \[Aristar, 1987\].
This 371-node,l133-arc analyzer was delivered to MCCshareholders in mid-1986, and may represent thefirst complete analyzer ever produced for Arabic.The French and German analyzers are com-plete with respect to inflection (highly irregularforms, like sein in German, naturally excepted).The former numbers 71 nodes and 121 arcs; thelatter, 54 nodes and 79 arcs.
The 19-node, 17-arc Spanish analyzer is nearly complete withrespect to inflection; adjectives remain a tem-porary exception.
With respect o verbs, for ex-ample, it has been tested on an extensive list ofconjugated verbs \[Noble and Lacasa, 1980\], com-prising over 6,000 surface forms, and in the firstsuch test it was 970"/0 accurate.GUESSERSA 76-node, 116-arc German guessing graphhas been implemented and tested.
It is still ex-perimental, and incomplete, but it does gobeyond inflection to account for some deriva-tional processes.
Our Arabic guesser is actuallythe Arabic analyzer graph: such strategy sharingis not always appropriate, as discussed above,but it would seem to be so for languages that,233like Arabic, are morphologically very rich, ad-mitting as a consequence v ry strong predictions.DEFAULTERA 21-node, 23-arc English defaulting graphexists.
It seems to be complete (insofar as such aprocessor might be), in that it constitutes aseemingly adequate component of a dictionaryentry coding tool.CONCLUSIONSMorphological grammars in Nabu are able toaccount for all compositional readings ofarbitrarily-complex surface-forms in a widerange of languages.
Furthermot'e, the formalismand development environment are reasonablycomfortable.
These claims are supported by ourimplementation and large-scale testing of severaldiverse grammars.For philosophical reasons, we are opposed tothe idea that grammars (as opposed to in-dividual rules) must be reversible: even if it werenot for the need of five-fold rather than merelydual functionality, the need for fault-tolerance ina practical system, without consequent fault-exhibition, argues for separate analysis and syn-thesis grammars.
We also point out that, in ourimplementations, the \[non-reversible\] controlgraphs tend to be much smaller in size than thehierarchies of \[reversible\] rules, hence the storagepenalty for "redundancy" is inconsequential.REFERENCESAristar, A., "Unification and the Computa-tional Analysis of Arabic," Computers andTranslation 2, 2, (April-June) 1987, pp.
67-75.Bear, J., "A Morphological Recognizer withSyntactic and Phonological Rules," Proceedingsof COLING86, Bonn, 1986, pp.
272-276.Bennett, W.S., and J. Sloeum, "The LRCMachine Translation System," ComputationalLinguistics 11 (2-3), 1985, pp.
111-121.Byrd, R.J., "Word Formation in NaturalLanguage Processing Systems," Proceedings ofthe 8th IJCAI, Karlsruhe, 1983, pp.
704-706.Karttunen, L., "Kimmo - A General Mor-phological Processor," Texas Linguistic Forum22, 1983, pp.
165-186.Koskenniemi, K., "Two-level Model for Mor-phological Analysis," Proceedings of the 8th IJ-CAI, Karlsruhe, 1983, pp.
683-685.Noble, J., and J. Lacasa.
Handbook ofSpanish Verbs.
Iowa State University Press,Ames, Iowa, 1980.Pounder, A., and M. Kommenda,"Morphological Analysis for a German Text-to-Speech System," Proceedings of COLING86,Bonn, 1986, pp.
263-268.Russell, G.J., S.G. Pulman, G.D. Ritchie,and A.W.
Black, "A Dictionary and Morphologi-cal Analyser for English," Proceedings ofCOLING86, Bonn, 1986, pp.
277-279.Slocum, J., "An English Affix Analyzer withIntermediate Dictionary Look-up," WorkingPaper LRC-81-1, Linguistics Research Center,University of Texas, February 1981.Slocum, J., and W.S.
Bennett, "The LRCMachine Translation System: An Application ofState-of-the-Art Text and Natural LanguageProcessing Techniques to the Translation ofTechnical Manuals," Working Paper LRC-82-1,Linguistics Research Center, University of Texas,July 1982.234
