Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 283?287,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAutomatic Extraction of Lexico-Syntactic Patterns for Detection of Negationand Speculation ScopesEmilia ApostolovaDePaul UniversityChicago, IL USAemilia.aposto@gmail.comNoriko TomuroDePaul UniversityChicago, IL USAtomuro@cs.depaul.eduDina Demner-FushmanNational Library of MedicineBethesda, MD USAddemner@mail.nih.govAbstractDetecting the linguistic scope of negated andspeculated information in text is an impor-tant Information Extraction task.
This paperpresents ScopeFinder, a linguistically moti-vated rule-based system for the detection ofnegation and speculation scopes.
The systemrule set consists of lexico-syntactic patternsautomatically extracted from a corpus anno-tated with negation/speculation cues and theirscopes (the BioScope corpus).
The systemperforms on par with state-of-the-art machinelearning systems.
Additionally, the intuitiveand linguistically motivated rules will allowfor manual adaptation of the rule set to newdomains and corpora.1 MotivationInformation Extraction (IE) systems often facethe problem of distinguishing between affirmed,negated, and speculative information in text.
Forexample, sentiment analysis systems need to detectnegation for accurate polarity classification.
Simi-larly, medical IE systems need to differentiate be-tween affirmed, negated, and speculated (possible)medical conditions.The importance of the task of negation and spec-ulation (a.k.a.
hedge) detection is attested by a num-ber of research initiatives.
The creation of the Bio-Scope corpus (Vincze et al, 2008) assisted in the de-velopment and evaluation of several negation/hedgescope detection systems.
The corpus consists ofmedical and biological texts annotated for negation,speculation, and their linguistic scope.
The 2010i2b2 NLP Shared Task1 included a track for detec-tion of the assertion status of medical problems (e.g.affirmed, negated, hypothesized, etc.).
The CoNLL-2010 Shared Task (Farkas et al, 2010) focused ondetecting hedges and their scopes in Wikipedia arti-cles and biomedical texts.In this paper, we present a linguistically moti-vated rule-based system for the detection of nega-tion and speculation scopes that performs on parwith state-of-the-art machine learning systems.
Therules used by the ScopeFinder system are automat-ically extracted from the BioScope corpus and en-code lexico-syntactic patterns in a user-friendly for-mat.
While the system was developed and tested us-ing a biomedical corpus, the rule extraction mech-anism is not domain-specific.
In addition, the lin-guistically motivated rule encoding allows for man-ual adaptation to new domains and corpora.2 Task DefinitionNegation/Speculation detection is typically brokendown into two sub-tasks - discovering a nega-tion/speculation cue and establishing its scope.
Thefollowing example from the BioScope corpus showsthe annotated hedging cue (in bold) together with itsassociated scope (surrounded by curly brackets):Finally, we explored the {possible role of 5-hydroxyeicosatetraenoic acid as a regulator of arachi-donic acid liberation}.Typically, systems first identify nega-tion/speculation cues and subsequently try toidentify their associated cue scope.
However,the two tasks are interrelated and both require1https://www.i2b2.org/NLP/Relations/283syntactic understanding.
Consider the followingtwo sentences from the BioScope corpus:1) By contrast, {D-mib appears to be uniformly ex-pressed in imaginal discs }.2) Differentiation assays using water soluble phor-bol esters reveal that differentiation becomes irreversiblesoon after AP-1 appears.Both sentences contain the word form appears,however in the first sentence the word marks a hedg-ing cue, while in the second sentence the word doesnot suggest speculation.Unlike previous work, we do not attempt to iden-tify negation/speculation cues independently of theirscopes.
Instead, we concentrate on scope detection,simultaneously detecting corresponding cues.3 DatasetWe used the BioScope corpus (Vincze et al, 2008)to develop our system and evaluate its performance.To our knowledge, the BioScope corpus is theonly publicly available dataset annotated with nega-tion/speculation cues and their scopes.
It consistsof biomedical papers, abstracts, and clinical reports(corpus statistics are shown in Tables 1 and 2).Corpus Type Sentences Documents Mean Document SizeClinical 7520 1954 3.85Full Papers 3352 9 372.44Paper Abstracts 14565 1273 11.44Table 1: Statistics of the BioScope corpus.
Document sizesrepresent number of sentences.Corpus Type Negation Cues Speculation Cues Negation SpeculationClinical 872 1137 6.6% 13.4%Full Papers 378 682 13.76% 22.29%Paper Abstracts 1757 2694 13.45% 17.69%Table 2: Statistics of the BioScope corpus.
The 2nd and 3dcolumns show the total number of cues within the datasets; the4th and 5th columns show the percentage of negated and spec-ulative sentences.70% of the corpus documents (randomly selected)were used to develop the ScopeFinder system (i.e.extract lexico-syntactic rules) and the remaining30% were used to evaluate system performance.While the corpus focuses on the biomedical domain,our rule extraction method is not domain specificand in future work we are planning to apply ourmethod on different types of corpora.4 MethodIntuitively, rules for detecting both speculation andnegation scopes could be concisely expressed as aFigure 1: Parse tree of the sentence ?T cells {lack active NF-kappa B } but express Sp1 as expected?
generated by the Stan-ford parser.
Speculation scope words are shown in ellipsis.
Thecue word is shown in grey.
The nearest common ancestor of allcue and scope leaf nodes is shown in a box.combination of lexical and syntactic patterns.
Forexample, O?zgu?r and Radev (2009) examined sampleBioScope sentences and developed hedging scoperules such as:The scope of a modal verb cue (e.g.
may, might, could)is the verb phrase to which it is attached;The scope of a verb cue (e.g.
appears, seems) followedby an infinitival clause extends to the whole sentence.Similar lexico-syntactic rules have been also man-ually compiled and used in a number of hedge scopedetection systems, e.g.
(Kilicoglu and Bergler,2008), (Rei and Briscoe, 2010), (Velldal et al,2010), (Kilicoglu and Bergler, 2010), (Zhou et al,2010).However, manually creating a comprehensive setof such lexico-syntactic scope rules is a laboriousand time-consuming process.
In addition, such anapproach relies heavily on the availability of accu-rately parsed sentences, which could be problem-atic for domains such as biomedical texts (Clegg andShepherd, 2007; McClosky and Charniak, 2008).Instead, we attempted to automatically extractlexico-syntactic scope rules from the BioScope cor-pus, relying only on consistent (but not necessarilyaccurate) parse tree representations.We first parsed each sentence in the trainingdataset which contained a negation or speculationcue using the Stanford parser (Klein and Manning,2003; De Marneffe et al, 2006).
Figure 1 shows theparse tree of a sample sentence containing a nega-tion cue and its scope.Next, for each cue-scope instance within the sen-tence, we identified the nearest common ancestor284Figure 2: Lexico-syntactic pattern extracted from the sentencefrom Figure 1.
The rule is equivalent to the following stringrepresentation: (VP (VBP lack) (NP (JJ *scope*) (NN *scope*)(NN *scope*))).which encompassed the cue word(s) and all words inthe scope (shown in a box on Figure 1).
The subtreerooted by this ancestor is the basis for the resultinglexico-syntactic rule.
The leaf nodes of the resultingsubtree were converted to a generalized representa-tion: scope words were converted to *scope*; non-cue and non-scope words were converted to *; cuewords were converted to lower case.
Figure 2 showsthe resulting rule.This rule generation approach resulted in a largenumber of very specific rule patterns - 1,681 nega-tion scope rules and 3,043 speculation scope ruleswere extracted from the training dataset.To identify a more general set of rules (and in-crease recall) we next performed a simple transfor-mation of the derived rule set.
If all children of arule tree node are of type *scope* or * (i.e.
non-cue words), the node label is replaced by *scope*or * respectively, and the node?s children are prunedfrom the rule tree; neighboring identical siblings oftype *scope* or * are replaced by a single node ofthe corresponding type.
Figure 3 shows an exampleof this transformation.
(a) The children of nodes JJ/NN/NN arepruned and their labels are replaced by*scope*.
(b) The childrenof node NP arepruned and its la-bel is replaced by*scope*.Figure 3: Transformation of the tree shown in Figure 2.
Thefinal rule is equivalent to the following string representation:(VP (VBP lack) *scope* )The rule tree pruning described above reduced thenegation scope rule patterns to 439 and the specula-tion rule patterns to 1,000.In addition to generating a set of scope findingrules, we also implemented a module that parsesstring representations of the lexico-syntactic rulesand performs subtree matching.
The ScopeFindermodule2 identifies negation and speculation scopesin sentence parse trees using string-encoded lexico-syntactic patterns.
Candidate sentence parse sub-trees are first identified by matching the path of cueleaf nodes to the root of the rule subtree pattern.
If anidentical path exists in the sentence, the root of thecandidate subtree is thus also identified.
The candi-date subtree is evaluated for a match by recursivelycomparing all node children (starting from the rootof the subtree) to the rule pattern subtree.
Nodesof type *scope* and * match any number of nodes,similar to the semantics of Regex Kleene star (*).5 ResultsAs an informed baseline, we used a previously de-veloped rule-based system for negation and spec-ulation scope discovery (Apostolova and Tomuro,2010).
The system, inspired by the NegEx algorithm(Chapman et al, 2001), uses a list of phrases splitinto subsets (preceding vs. following their scope) toidentify cues using string matching.
The cue scopesextend from the cue to the beginning or end of thesentence, depending on the cue type.
Table 3 showsthe baseline results.Correctly Predicted Cues All Predicted CuesNegation P R F FClinical 94.12 97.61 95.18 85.66Full Papers 54.45 80.12 64.01 51.78Paper Abstracts 63.04 85.13 72.31 59.86SpeculationClinical 65.87 53.27 58.90 50.84Full Papers 58.27 52.83 55.41 29.06Paper Abstracts 73.12 64.50 68.54 38.21Table 3: Baseline system performance.
P (Precision), R (Re-call), and F (F1-score) are computed based on the sentence to-kens of correctly predicted cues.
The last column shows theF1-score for sentence tokens of all predicted cues (including er-roneous ones).We used only the scopes of predicted cues (cor-rectly predicted cues vs. all predicted cues) to mea-2The rule sets and source code are publicly available athttp://scopefinder.sourceforge.net/.285sure the baseline system performance.
The base-line system heuristics did not contain all phrase cuespresent in the dataset.
The scopes of cues that aremissing from the baseline system were not includedin the results.
As the baseline system was not penal-ized for missing cue phrases, the results representthe upper bound of the system.Table 4 shows the results from applying the fullextracted rule set (1,681 negation scope rules and3,043 speculation scope rules) on the test data.
Asexpected, this rule set consisting of very specificscope matching rules resulted in very high precisionand very low recall.Negation P R F AClinical 99.47 34.30 51.01 17.58Full Papers 95.23 25.89 40.72 28.00Paper Abstracts 87.33 05.78 10.84 07.85SpeculationClinical 96.50 20.12 33.30 22.90Full Papers 88.72 15.89 26.95 10.13Paper Abstracts 77.50 11.89 20.62 10.00Table 4: Results from applying the full extracted rule set on thetest data.
Precision (P), Recall (R), and F1-score (F) are com-puted based the number of correctly identified scope tokens ineach sentence.
Accuracy (A) is computed for correctly identi-fied full scopes (exact match).Table 5 shows the results from applying the ruleset consisting of pruned pattern trees (439 negationscope rules and 1,000 speculation scope rules) on thetest data.
As shown, overall results improved signif-icantly, both over the baseline and over the unprunedset of rules.
Comparable results are shown in boldin Tables 3, 4, and 5.Negation P R F AClinical 85.59 92.15 88.75 85.56Full Papers 49.17 94.82 64.76 71.26Paper Abstracts 61.48 92.64 73.91 80.63SpeculationClinical 67.25 86.24 75.57 71.35Full Papers 65.96 98.43 78.99 52.63Paper Abstracts 60.24 95.48 73.87 65.28Table 5: Results from applying the pruned rule set on the testdata.
Precision (P), Recall (R), and F1-score (F) are computedbased on the number of correctly identified scope tokens in eachsentence.
Accuracy (A) is computed for correctly identified fullscopes (exact match).6 Related WorkInterest in the task of identifying negation and spec-ulation scopes has developed in recent years.
Rele-vant research was facilitated by the appearance of apublicly available annotated corpus.
All systems de-scribed below were developed and evaluated againstthe BioScope corpus (Vincze et al, 2008).O?zgu?r and Radev (2009) have developed a super-vised classifier for identifying speculation cues anda manually compiled list of lexico-syntactic rules foridentifying their scopes.
For the performance of therule based system on identifying speculation scopes,they report 61.13 and 79.89 accuracy for BioScopefull papers and abstracts respectively.Similarly, Morante and Daelemans (2009b) de-veloped a machine learning system for identifyinghedging cues and their scopes.
They modeled thescope finding problem as a classification task thatdetermines if a sentence token is the first token ina scope sequence, the last one, or neither.
Resultsof the scope finding system with predicted hedgesignals were reported as F1-scores of 38.16, 59.66,78.54 and for clinical texts, full papers, and abstractsrespectively3.
Accuracy (computed for correctlyidentified scopes) was reported as 26.21, 35.92, and65.55 for clinical texts, papers, and abstracts respec-tively.Morante and Daelemans have also developed ametalearner for identifying the scope of negation(2009a).
Results of the negation scope finding sys-tem with predicted cues are reported as F1-scores(computed on scope tokens) of 84.20, 70.94, and82.60 for clinical texts, papers, and abstracts respec-tively.
Accuracy (the percent of correctly identifiedexact scopes) is reported as 70.75, 41.00, and 66.07for clinical texts, papers, and abstracts respectively.The top three best performers on the CoNLL-2010 shared task on hedge scope detection (Farkaset al, 2010) report an F1-score for correctly identi-fied hedge cues and their scopes ranging from 55.3to 57.3.
The shared task evaluation metrics usedstricter matching criteria based on exact match ofboth cues and their corresponding scopes4.CoNLL-2010 shared task participants applied avariety of rule-based and machine learning methods3F1-scores are computed based on scope tokens.
Unlike ourevaluation metric, scope token matches are computed for eachcue within a sentence, i.e.
a token is evaluated multiple times ifit belongs to more than one cue scope.4Our system does not focus on individual cue-scope pair de-tection (we instead optimized scope detection) and as a resultperformance metrics are not directly comparable.286on the task - Morante et al (2010) used a memory-based classifier based on the k-nearest neighbor ruleto determine if a token is the first token in a scope se-quence, the last, or neither; Rei and Briscoe (2010)used a combination of manually compiled rules, aCRF classifier, and a sequence of post-processingsteps on the same task; Velldal et al(2010) manu-ally compiled a set of heuristics based on syntacticinformation taken from dependency structures.7 DiscussionWe presented a method for automatic extractionof lexico-syntactic rules for negation/speculationscopes from an annotated corpus.
The devel-oped ScopeFinder system, based on the automati-cally extracted rule sets, was compared to a base-line rule-based system that does not use syntac-tic information.
The ScopeFinder system outper-formed the baseline system in all cases and exhib-ited results comparable to complex feature-based,machine-learning systems.In future work, we will explore the use of statisti-cally based methods for the creation of an optimumset of lexico-syntactic tree patterns and will evalu-ate the system performance on texts from differentdomains.ReferencesE.
Apostolova and N. Tomuro.
2010.
Exploring surface-level heuristics for negation and speculation discoveryin clinical texts.
In Proceedings of the 2010 Workshopon Biomedical Natural Language Processing, pages81?82.
Association for Computational Linguistics.W.W.
Chapman, W. Bridewell, P. Hanbury, G.F. Cooper,and B.G.
Buchanan.
2001.
A simple algorithmfor identifying negated findings and diseases in dis-charge summaries.
Journal of biomedical informatics,34(5):301?310.A.B.
Clegg and A.J.
Shepherd.
2007.
Benchmark-ing natural-language parsers for biological applica-tions using dependency graphs.
BMC bioinformatics,8(1):24.M.C.
De Marneffe, B. MacCartney, and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In LREC 2006.
Citeseer.R.
Farkas, V. Vincze, G. Mo?ra, J. Csirik, and G. Szarvas.2010.
The CoNLL-2010 Shared Task: Learning toDetect Hedges and their Scope in Natural LanguageText.
In Proceedings of the Fourteenth Conference onComputational Natural Language Learning (CoNLL-2010): Shared Task, pages 1?12.H.
Kilicoglu and S. Bergler.
2008.
Recognizing specu-lative language in biomedical research articles: a lin-guistically motivated perspective.
BMC bioinformat-ics, 9(Suppl 11):S10.H.
Kilicoglu and S. Bergler.
2010.
A High-PrecisionApproach to Detecting Hedges and Their Scopes.CoNLL-2010: Shared Task, page 70.D.
Klein and C.D.
Manning.
2003.
Fast exact infer-ence with a factored model for natural language pars-ing.
Advances in neural information processing sys-tems, pages 3?10.D.
McClosky and E. Charniak.
2008.
Self-training forbiomedical parsing.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Linguis-tics on Human Language Technologies: Short Papers,pages 101?104.
Association for Computational Lin-guistics.R.
Morante and W. Daelemans.
2009a.
A metalearningapproach to processing the scope of negation.
In Pro-ceedings of the Thirteenth Conference on Computa-tional Natural Language Learning, pages 21?29.
As-sociation for Computational Linguistics.R.
Morante and W. Daelemans.
2009b.
Learning thescope of hedge cues in biomedical texts.
In Proceed-ings of the Workshop on BioNLP, pages 28?36.
Asso-ciation for Computational Linguistics.R.
Morante, V. Van Asch, and W. Daelemans.
2010.Memory-based resolution of in-sentence scopes ofhedge cues.
CoNLL-2010: Shared Task, page 40.A.
O?zgu?r and D.R.
Radev.
2009.
Detecting speculationsand their scopes in scientific text.
In Proceedings ofthe 2009 Conference on Empirical Methods in Natu-ral Language Processing: Volume 3-Volume 3, pages1398?1407.
Association for Computational Linguis-tics.M.
Rei and T. Briscoe.
2010.
Combining manual rulesand supervised learning for hedge cue and scope detec-tion.
In Proceedings of the 14th Conference on Natu-ral Language Learning, pages 56?63.E.
Velldal, L. ?vrelid, and S. Oepen.
2010.
Re-solving Speculation: MaxEnt Cue Classification andDependency-Based Scope Rules.
CoNLL-2010:Shared Task, page 48.V.
Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and J. Csirik.2008.
The BioScope corpus: biomedical texts anno-tated for uncertainty, negation and their scopes.
BMCbioinformatics, 9(Suppl 11):S9.H.
Zhou, X. Li, D. Huang, Z. Li, and Y. Yang.
2010.Exploiting Multi-Features to Detect Hedges and TheirScope in Biomedical Texts.
CoNLL-2010: SharedTask, page 106.287
