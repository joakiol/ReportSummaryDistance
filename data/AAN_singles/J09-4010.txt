An Empirical Study of Corpus-BasedResponse Automation Methods for anE-mail-Based Help-Desk DomainYuval Marom?Monash UniversityIngrid Zukerman?
?Monash UniversityThis article presents an investigation of corpus-based methods for the automation of help-deske-mail responses.
Specifically, we investigate this problem along two operational dimensions:(1) information-gathering technique, and (2) granularity of the information.
We consider twoinformation-gathering techniques (retrieval and prediction) applied to information representedat two levels of granularity (document-level and sentence-level).
Document-level methods corre-spond to the reuse of an existing response e-mail to address new requests.
Sentence-level methodscorrespond to applying extractive multi-document summarization techniques to collate units ofinformation from more than one e-mail.
Evaluation of the performance of the different methodsshows that in combination they are able to successfully automate the generation of responsesfor a substantial portion of e-mail requests in our corpus.
We also investigate a meta-selectionprocess that learns to choose one method to address a new inquiry e-mail, thus providing a unifiedresponse automation solution.1.
IntroductionE-mail inquiries sent to help desks often ?revolve around a small set of common ques-tions and issues.
?1 This means that help-desk operators spendmost of their time dealingwith problems that have been previously addressed.
Further, a significant proportion ofhelp-desk responses contain a low level of technical content, addressing, for example,inquiries sent to the wrong group, or requests containing insufficient detail about thecustomer?s problem.
Organizations and clients would benefit if an automated processwas employed to deal with the easier problems, and the efforts of human operatorswere focused on difficult, atypical problems.However, even the automation of responses to the ?easy?
problems is a difficulttask.
Although such inquiries revolve around a relatively small set of issues, specific?
Faculty of Information Technology, Monash University, Wellington Road, Clayton, Victoria 3800,Australia.
Currently employed at Pacific Brands Services Group, Building 10, 658 Church St, Richmond,Victoria 3121, Australia.
E-mail: yuvalmarom@gmail.com.??
Faculty of Information Technology, Monash University, Wellington Road, Clayton, Victoria 3800,Australia.
E-mail: Ingrid.Zukerman@infotech.monash.edu.au.1 http ://customercare.telephonyonline.com/ar/telecom next generation customer.Submission received: 7 November 2007; revised submission received: 20 March 2009; accepted for publication:3 June 2009.?
2009 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 4circumstances can make each inquiry unique, and hence care must be taken to composea response that does not confuse, irritate, or mislead the customer.
It is therefore nosurprise that early attempts at response automation were knowledge-driven (Barr andTessler 1995; Watson 1997; Delic and Lahaix 1998).
These systems were carefully de-signed to produce relevant and correct responses, but required significant human inputand maintenance (Delic and Lahaix 1998).In recent times, such knowledge-intensive approaches to content delivery havebeen largely superseded by data-intensive, statistical approaches.
An outcome of therecent proliferation of statistical approaches, in particular in recommender systemsand search engines, is that people have become accustomed to responses that are notprecisely tailored to their queries.
This indicates that help-desk customers may havealso become more tolerant of inaccurate or incomplete automatically generated replies,provided these replies are still relevant to their problem, and so long as the customerscan follow up with a request for human-generated responses if necessary.
Despite this,to date, there has been little work on corpus-based approaches to help-desk responseautomation (notable exceptions are Carmel, Shtalhaim, and Soffer 2000; Lapalme andKosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007).
Amajor factor limiting this work is the dearth of corpora?help-desk e-mails tend to beproprietary and are subject to privacy issues.
Further, this application lacks the kind ofbenchmark data sets that are used in question-answering and text summarization.2In this article, we report on our experiments with corpus-based techniques for theautomation of help-desk responses.
Our study is based on a large corpus of request?response e-mail dialogues between customers and operators at Hewlett-Packard.
Ob-servations from this corpus have led us to consider several methods that implementdifferent types of corpus-based strategies.
Specifically, we have investigated two typesof methods (retrieval and prediction) applied at two levels of granularity (documentand sentence).
In this article, we present these methods and compare their performance.A key issue in the generation of help-desk responses is the ability to determine when anautomatically generated response for a particular query can be sent to a user, and whenthe query should be passed to an operator.
In Section 3, we employ method-specific,empirically determined, applicability thresholds to make this decision; in Section 6,we propose a meta-level process for selecting a response-generation method, whichobviates the need for these thresholds.The rest of the article is organized as follows.
In the next section, we discussproperties of the help-desk domain.
In Section 3, we describe our response-generationmethods.
An automatic evaluation of these methods is presented in Section 4, and asmall user-based evaluation in Section 5.
In Section 6, we describe themeta-level processwhich learns to select between the different methods, and evaluate its performance inSection 7.
Related work is discussed in Section 8, and concluding remarks are presentedin Section 9.2.
The Help-Desk DomainThe help-desk domain offers interesting challenges to response automation in that, onone hand, responses are generalized to fit standard solutions, and on the other hand,responses are tailored to the initiating request in order to meet specific customer needs.2 http://trec.nist.gov/pubs/trec15/t15 proceedings.html andhttp://www-nlpir.nist.gov/projects/duc/data.html.598Marom and Zukerman Empirical Study of Response Automation MethodsFigure 1Sample responses from our corpus: (a) mixed generic?specific, (b) specific, and (c) generic.For example, the first sentence of the response in Figure 1(a) is tailored to the user?srequest, whereas the rest of the response is generic, and may be used when replyingto other queries.3 In addition to responses that contain such a mixture of specific andgeneric information, there are inquiries that warrant very specific or completely genericresponses, as seen in Figures 1(b) and 1(c), respectively.A distinctive feature of the help-desk domain is that help-desk e-mail responsescontain a high level of repetition and redundancy.
This may be attributed to commonal-ities in customer issues combined with the provision of in-house manuals to help-deskoperators.
These manuals connect particular topics with standard response templates,prescribe a particular presentation style, and even suggest specific responses to certainqueries.
For example, Figure 2 shows two rather different response e-mails which sharea sentence (italicized).
Thus, having access to these manuals would enable us to easilyidentify prescribed sentences.
More importantly, it would enable us to determine thecontext in which these sentences are used, which in turn would allow us to postulateadditional response sentences.
An interesting avenue of investigation would involveadapting our approach to help-desk situations where such manuals are accessible.3 The examples shown in this article are reproduced verbatim from the corpus (except for URLs and phonenumbers which have been disguised by us), and some have user or operator errors.599Computational Linguistics Volume 35, Number 4If you are able to see the Internet then it sounds like it is working, you may want to get intouch with your IT department to see if you need to make any changes to your settings toget it to work.
Try performing a soft reset by pressing the stylus pen in the small hole on the bottomleft hand side of the Ipaq and then release.I would recommend doing a soft reset by pressing the stylus pen in the small hole on the left handside of the Ipaq and then release.
Then charge the unit overnight to make sure it has been longenough and then see what happens.
If the battery is not charging then the unit will need tobe sent in for repair.Figure 2Sample responses that share a sentence.Despite the high degree of repetition in help-desk responses, the specific issuesraised by different customers imply that the responses sent to these customers containvarying degrees of overlap (rather than being identical).
Hence, providing a responsefor a new request may involve reusing an existing response in its entirety, puttingtogether parts of responses that match individual components of the request, or com-posing a completely new response.
This suggests that different response-generationstrategies may be suitable, depending on the content of the initiating request and howwell it matches previous requests or responses.
In our work, we focus on the first twoof these situations, where either complete existing responses or parts of responses arereused to address a new request.The example in Figure 1(b) illustrates a situation where specific words in the request(docking station and install) are also mentioned in the response.
This situation suggestsa response-automation approach that follows the document retrieval paradigm (Saltonand McGill 1983), where a new request is matched with existing response documents(e-mails).
However, specific words in the request do not always match a response well,and sometimes do not match a response at all, as demonstrated by the examples inFigures 1(a) and 1(c), respectively.Sometimes requests match each other quite well, suggesting an approach where anew request is matched with an old one, and the corresponding response is reused.However, analysis of our corpus shows that this does not occur very often, becauseunlike response e-mails, request e-mails exhibit a high language variability: There aremany customers who write these e-mails, and they differ in their background, levelof expertise, and pattern of language usage.
Further, there are many requests that raisemultiple issues, hence matching a new request e-mail in its entirety is often not possible.In situations where requests do not match existing responses or other requests,it may be possible instead to find correlations between requests and responses.
Forexample, the generic portion of the response in Figure 1(a), and the entire responsein Figure 1(c), may be repeated for many different kinds of requests.
If repeated suffi-ciently, entire responses or parts of responses will be strongly correlated with particularcombinations of request words, such as send, battery and replace in Figure 1(a), andfirewall, ip, and network in Figure 1(c).3.
Response-Generation MethodsThe properties of the help-desk domain outlined in the previous section have motivatedus to study the response-automation task along two dimensions.
The first dimensionpertains to the strategy applied to determine the information in a response, and the600Marom and Zukerman Empirical Study of Response Automation Methodssecond dimension pertains to the granularity of the information.
We implemented twoalternative strategies.
The first is a retrieval strategy that attempts to match a newrequest with previous requests and responses, and the second is a prediction strategythat looks for correlations between requests and responses in order to predict a responsefor a new request.
For both types of strategies, we considered two levels of granularityfor a unit of information: document and sentence.We implemented four methods according to the two alternatives for each dimen-sion: Document Retrieval, Document Prediction, Sentence Retrieval, and SentencePrediction.
We also implemented a fifth method for addressing situations such as theexample shown in Figure 1(a), which warrant a mixed response that has both a genericcomponent and a component tailored to the user?s request.
This is a hybrid prediction?retrieval method implemented at the sentence level: Sentence Prediction?Retrieval Hy-brid.
These five methods are summarized in Table 2 (Section 3.3).
The implementationof these methods relies on the judicious selection of thresholds for different aspects ofthe processes in question.
In principle, machine learning techniques could be used todetermine optimal threshold values.
However, owing to practical considerations, weselected these values by trial and error.
Table 3 shows the range of values we tried forthese thresholds, and the values we selected (Section 3.3).3.1 Document-Level MethodsA document-level method attempts to reuse an existing response document (e-mail)in its entirety.
Unlike sentence-based methods that attempt to put together portions ofdifferent responses (Section 3.2), a document-level approach avoids issues pertaining tothe coherence and completeness of a response, as a response composed by a help-deskoperator is likely to be both coherent and complete.
Therefore, if a particular requestcan be addressed with a single existing response document, then a document reuseapproach would be preferred.
An important capability of a response-generation systemis to be able to determine when such an approach is appropriate, and when there isinsufficient evidence to reuse a complete response document.As stated herein, we studied two document-based methods: Document RetrievalandDocument Prediction.3.1.1 Document Retrieval (Doc-Ret).
This method follows a traditional Information Re-trieval paradigm (Salton and McGill 1983), where a query is represented by the contentterms it contains, and the system retrieves from the corpus a set of documents thatbest match this query.
In our case, the query is a new request e-mail to be addressedby the system, and we have considered three views of the documents in the corpus:(1) previous response e-mails, (2) previous request e-mails, or (3) previous request?response pairs.
The first alternative corresponds to the more traditional view of retrievalas applied in question-answering tasks, where the terms in the question are matchedto those in the answer documents.
We consider the second alternative in order toaddress situations such as the example in Figure 1(c), where a request might not matcha particular response, but it may match another request, yielding the response to thatrequest.
The third alternative addresses situations where a new request matches part ofanother request and part of its response.We use cosine similarity (between a request e-mail and each document in thecorpus) to determine a retrieval score, and pick the document with the highest score.The similarity is calculated using a bag-of-lemmas representation with TF.IDF (term-frequency-inverse-document frequency) weightings (Salton andMcGill 1983), but in the601Computational Linguistics Volume 35, Number 4request-to-response option we use TF = 1, as it yields the best results.
We posit that thishappens because a response to a request does not necessarily contain multiple instancesof request terms.
Hence, what is important when matching a request to a response is thenumber of (significant) terms in common, rather than their frequency.
In contrast, whenmatching a request to a request, or a request to a request?response pair, term frequencywould be more indicative of the goodness of the match, as the document also has arequest component.We consider retrieval to be successful only if the similarity score is higher thanan applicability threshold, which is currently set empirically (Table 3).
If retrieval issuccessful, then the response associated with the retrieved document is reused to replyto the user?s request.We carried out a preliminary experiment in order to compare the three variantsof the Doc-Ret method.
The evaluation is performed by considering each requeste-mail in turn, removing it and its response from the corpus, carrying out the retrievalprocess, and then comparing the retrieved response with the actual response (ifthere are several similar responses in the corpus, an appropriate response can stillbe retrieved).
The results of this experiment are shown in Table 1.
The first columnshows which document retrieval variant is being evaluated.
The second column showsthe proportion of requests for which one or more documents were retrieved (usingour applicability threshold).
We see that matching on requests yields more retrieveddocuments than matching on responses, and that matching on request?responsepairs yields even more retrieved documents.
For the cases where retrieval took place,we used F-score (van Rijsbergen 1979; Salton and McGill 1983) to determine thesimilarity between the response from the top-ranked document and the real response(the formulas for F-score and its contributing factors, recall and precision, appear inSection 4.2).
The third column in Table 1 shows the proportion of requests for whichthis similarity is non-zero.
Again, the third variant (matching on request?responsepairs) retrieves the highest proportion of responses that bear some similarity to the realresponses.
The fourth column shows the average similarity between the top retrievedresponse and the real response for the cases where retrieval took place.
Here too thethird variant yields the best similarity score (0.52).From this preliminary experiment it appears that the third document retrievalvariant is superior.
Hence, we use this variant as the Doc-Ret method in subsequentexperiments.3.1.2 Document Prediction (Doc-Pred).
The Doc-Ret method may fail in situations wherethe presence or absence of some terms in the requests triggers a generic templateresponse.
For instance, in the example in Figure 1(c), given the terms firewall and CP-2W,Table 1Comparison between the three document retrieval variants.Match type Percent Percent retrieved Average similarityretrievals docs with sim > 0 for retrieved docsrequest to response 11% 11% 0.40request to request 37% 26% 0.50request to request?response 43% 32% 0.52602Marom and Zukerman Empirical Study of Response Automation Methodswe would like to retrieve the generated response.
However, the first Doc-Ret alernativewould fail, as the response has no common terms with the request.
The other two Doc-Ret alernatives would fail if different requests in the corpus mention different issuesabout these two terms, and thus no single request or request?response document in thecorpus would yield a good match with a new request that mentions these terms.In order to handle such cases, we offer a predictive approach, which is guided bycorrelations between terms, rather thanmatches.
In principle we could look for direct cor-relations between request terms and response terms.
However, since we have observedstrong regularities in the responses at the document level, we decided to reduce thedimensionality of the problem by abstracting the response documents and then lookingfor correlations at this higher level.
This approach did not seem profitable for requeste-mails, which unlike responses, have a high language variability.
Hence, we keep theirrepresentation at a low level of abstraction (bag-of-lemmas).The idea behind the Doc-Pred method is similar to Bickel and Scheffer?s (2004):Response documents are grouped into clusters, one of these clusters is predicted fora new request on the basis of the request?s features, and the response that is mostrepresentative of the predicted cluster (closest to the centroid) is selected.
In our case,the clustering is performed by the program Snob, which implements mixture model-ing combined with model selection based on the Minimum Message Length (MML)criterion (Wallace and Boulton 1968; Wallace 2005).
We chose this program because thenumber of clusters does not have to be specified in advance, and it returns a probabilisticinterpretation for its clusters (this interpretation is used by the Sent-Pred method,Section 3.2.2).
The input to Snob is a set of binary vectors, one vector per responsedocument.
The values of a vector correspond to the presence or absence of each (lem-matized) corpus word in the document in question (after removing stop-words andwords with very low frequency).4 The predictive model is a Decision Graph (Oliver1993), which, like Snob, is based on the MML principle.
The Decision Graph is trainedon unigram and bigram lemmas in the request as input features,5 and the identifierof the response cluster that contains the actual response for the request as the targetfeature.
The model predicts which response cluster is most suitable for a given re-quest, and returns the probability that this prediction is correct.
This probability isour indicator of whether the Doc-Pred method can address a new request.
As for theDoc-Ret method, an applicability threshold for this parameter is currently determinedempirically (Table 3).3.2 Sentence-Level MethodsThe document-level methods presented in the previous section are designed to addresssituations where requests are sufficiently specific to strongly match a previous requestor response e-mail (Doc-Ret), or requests contain terms that are predictive of completetemplate response e-mails (Doc-Pred).4 We used a binary representation, rather than a representation based on TF.IDF scores, because importantdomain-related words, such as monitor and network, are actually quite frequent.
Thus, their low TF.IDFscore may have an adverse influence on clustering performance.
Nonetheless, in the future, it may beworth investigating a TF.IDF-based representation.5 Significant bigrams are obtained using the n-gram statistics package NSP (Banerjee and Pedersen 2003),which offers statistical tests to decide whether to accept or reject the null hypothesis regarding a bigram(that it is not a collocation).603Computational Linguistics Volume 35, Number 4As discussed in Section 2, there are situations that cannot be addressed by adocument-level approach, because requests only predict or match portions of responses.An alternative approach is to look for promising sentences from one or more previousresponses, and collate them into a new response.
This task can be cast as extractivemulti-document summarization.
Unlike a document reuse approach, sentence-levelapproaches need to consider issues of discourse coherence in order to ensure that theextracted combination of sentences is coherent or at least understandable.
In our work,we gather sets of sentences, and assume (but do not employ) existing approaches fortheir organization (Goldstein et al 2000; Barzilay, Elhadad, andMcKeown 2001; Barzilayand McKeown 2005).The appeal of a sentence-level approach is that it supports the generation of a ?com-bination response?
in situations where there is insufficient evidence for a single docu-ment containing a full response, but there is enough evidence for parts of responses.Although such a combined response is generally less satisfactory than a full response,the information included in it may address a user?s problem or point the user in theright direction.
As argued in the Introduction, when it comes to obtaining informationquickly on-line, this option may be preferable to having to wait for a human-generatedresponse.
In contrast, the document-level approach is an all-or-nothing approach: Ifthere is insufficient evidence for a complete response, then no automated response isgenerated.3.2.1 Sentence Retrieval (Sent-Ret).
As we saw in Section 2 (Figure 1(b)), there are situa-tions where the terms in response sentences match well the terms in request sentences.To address these situations, we consider the Sent-Ret method, which employs retrievaltechniques as for the Doc-Ret method, but at the sentence level.There are two main differences between Sent-Ret and Doc-Ret: (1) in Sent-Ret welook for response sentences that match individual request sentences, rather than entiredocuments; and (2) in Sent-Ret we perform recall-based retrieval, rather than retrievalbased on cosine-similarity, where the request sentence is the reference document forthe recalled terms.
The second difference is a result of the first, as a good candidate forsentence retrieval contains terms that appear in a request sentence, but is also likely tocontain additional terms that expand on the matching terms (Figure 1(b)).
Thus, recallis calculated for each response sentence with respect to each request sentence as follows(since TF=1 yielded the best result for the request?response option in Doc-Ret, we alsouse it for Sent-Ret).recall =sum of TF.IDF of lemmas in request sentence & response sentencesum of TF.IDF of lemmas in request sentence(1)We retain the response sentences whose recall exceeds an empirically determined ap-plicability threshold (Table 3), and produce a response if at least one sentence wasretained.3.2.2 Sentence Prediction (Sent-Pred).
As for the Doc-Pred method, the Sent-Pred methodstarts by abstracting the responses.
It clusters response sentences using the same604Marom and Zukerman Empirical Study of Response Automation MethodsFigure 3A fictitious example that demonstrates the Sent-Pred and Sent-Hybrid methods.clustering program (Snob) and bag-of-lemmas representation as Doc-Pred.6 Unlike theDoc-Predmethod, where only a single response cluster is predicted (resulting in a singleresponse document being selected), the Sent-Pred method may predict several promis-ing Sentence Clusters (SCs).
A response is then composed by extracting sentences fromthe predicted SCs.To illustrate these ideas, consider the fictitious example in Figure 3, which showsthree small SCs (in practice SCs can have tens and even hundreds of sentences).
Thethick arrows correspond to high-confidence predictions, while the thin arrows corre-spond to sentence selections.
The other components of the diagram demonstrate theworkings of the Sent-Hybridmethod (Section 3.2.3).
In this example, three of the requestterms, repair, faulty and monitor, result in a confident prediction of two SCs: SC1 and SC2.The sentences in SC1 are identical, so we can arbitrarily select a sentence for inclusionin the generated response.
The sentences in SC2 are similar but not identical, hence weare less confident in arbitrarily selecting a sentence from SC2, and may select more thanone sentence (see the subsequent discussion on removing redundant sentences).We use a Support Vector Machine (SVM) with a Radial Basis Function kernel topredict SCs from users?
requests.7 A separate SVM is trained for each SC, with unigramand bigram lemmas in a request as input features, and a binary target feature specifyingwhether the SC contains a sentence from the response to this request.
During the6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features, such asnumber of syntactic phrases, grammatical mood, and grammatical person (Marom and Zukerman 2006),but the simple binary bag-of-lemmas representation yielded similar results.7 We employed the LIBSVM package (Chang and Lin 2001).605Computational Linguistics Volume 35, Number 4prediction stage, the SVMs predict zero or more SCs for each request, as shown inFigure 3.
We then apply the following steps.1.
Calculate the scores of the sentences in the predicted SCs.2.
Remove redundant sentences from cohesive SCs; these are SCs whichcontain similar sentences.3.
Calculate the confidence of the generated response.Calculating the score of a sentence.
The score of each sentence sj is calculated using thefollowing formula.Score(sj) =m?i=1Pr(SCi)?
Pr(sj|SCi) (2)where m is the number of SCs, Pr(sj|SCi) is the probability that sj appears in SCi(obtained from Snob), and Pr(SCi) is approximated as follows.8Pr(SCi) ={Precision(SCi) if SCi is very cohesive and predicted with high probability0 otherwise(3)where Precision(SCi) is defined as follows.Precision(SCi) =# of times SCi was correctly predicted# of times SCi was predicted(4)This measure, which reflects the reliability of an SVM that was trainedto predict SCi, is obtained by performing 10-fold cross-validation on theperformance of this SVM for the training data. An SC is cohesive if the sentences in it are similar to each other.
Thismeans that it is possible to obtain a sentence that represents the clusteradequately (this is not the case for an uncohesive SC).
The cohesion ofan SC is calculated as follows.Cohesion(SC) = 1NN?k=1[ Pr(wk ?
SC) ?
?
?
Pr(wk ?
SC) ?
1?
? ]
(5)where N is the number of content lemmas under consideration;Pr(wk ?
SC) is the probability that (lemmatized) word wk appears in8 We tried several alternatives for representing Pr(SCi ).
The best results were obtained with Equation (3).606Marom and Zukerman Empirical Study of Response Automation Methodsthe SC (this probability is obtained from the centroid9); and ?
is anempirically determined threshold that establishes an upper and lowerbound for this probability (Table 3).
For values of ?
close to zero,Equation (5) behaves like entropy, in the sense that it favors extremeprobabilities.
It implements the idea that a cohesive group of sentencesshould agree strongly on both the words that appear in these sentencesand the words that are omitted.
For example, the italicized sentences inFigure 2 belong to an SC with cohesion 0.93, whereas the opening responsesentence in Figure 1(b) belongs to an SC that contains diverse sentences(about the Rompaq power management) and has cohesion 0.7.
We employan empirically determined SC-cohesion threshold (Table 3) to determinewhether an SC is sufficiently cohesive for redundant sentences to be safelyremoved from it. A high-probability prediction is one where for the request in question, theSVM has predicted the SC with a probability that exceeds an empiricallydetermined threshold (Table 3).To illustrate the distinction between prediction probability and SVM reliability(precision), let us return to the request in Figure 3.
SC1 is predicted with high prob-ability for this request, because SC1 includes sentences from responses to many otherrequests that contain the words repair and faulty.
The word monitor also contributesto this high prediction probability, but not as strongly as repair and faulty.
This isbecause SC1 includes sentences from responses whose requests mention different faultyproducts, for example, monitor, printer, or notebook.
However, if there are more cases offaulty monitors in the corpus than other faulty products, then requests about repairingmonitors will have a higher prediction probability than requests about repairing otherproducts.
In contrast to prediction probability, SVM reliability reflects its overall per-formance (on the training data), and is independent of particular requests.
Thus, theSVM for SC1 has a higher reliability than that for SC3, because it is easier for an SVMto learn when SC1 is appropriate (predominantly from the presence of the words faultyand repair).In order to ensure the relevance of the generated replies, we have placed tightrestrictions on prediction probability and cluster cohesion (Table 3), which cause theSent-Pred method to often return partial responses.Removing redundant sentences.
After calculating the raw score of each sentence,we use a modified version of the Adaptive Greedy Algorithm by Filatova andHatzivassiloglou (2004) to penalize redundant sentences in cohesive clusters.
This isdone by decrementing the score of a sentence that belongs to an SC for which there is ahigher or equal scoring sentence (if there are several highest-scoring sentences, we re-tain one sentence as a reference sentence?i.e., its score is not decremented).
Specifically,given a sentence sk in cluster SCl which contains a sentence with a higher or equal score,the contribution of SCl to Score(sk) (= Pr(SCl)?
Pr(sk|SCl)) is subtracted from Score(sk).After applying these penalties, we retain only the sentences whose adjusted score isgreater than zero (for a highly cohesive cluster, typically only one sentence remains).9 For each feature in the input (i.e., lemmatized words), the centroid of the cluster contains afrequency-based estimate of the probability that an item with this feature value appears in this cluster.607Computational Linguistics Volume 35, Number 4Calculating the confidence of an automated response.
The calculation of sentencescores described previously determines which sentences should be included in an auto-matically generated response.
In order to decide whether this response should be used,we need an overall measure of the confidence in it.
Our confidence measure aggregatesinto a single number the values of the attributes used to assign a score to the individualsentences in a response, as follows.Confidence = # of usable SCs# of possible SCs(6)where usable SCs are those that satisfy the cohesion and prediction probabilitythresholds mentioned for sentence scoring, and also satisfy a furtherthreshold for SC precision (Table 3). possible SCs are those that satisfy a minimum prediction probabilitythreshold (Table 3).This measure combines our confidence in the SCs selected to generate response sen-tences with the completeness of the resultant response.
Confidence is represented bythe thresholds employed to select suitable SCs; and completeness is represented by theratio of the number of SCs that were deemed suitable, and the number of SCs that couldpossibly be used to generate a response.
These are SCs whose prediction probability isgreater than 0 (i.e., there is some evidence in the corpus for their use in the generationof a response sentence for the current request).
We also use a minimal applicabilitythreshold of 0 for the confidence measure (Table 3).
This threshold reflects our notionthat a partial response, even a response with one sentence, may still be useful.3.2.3 Sentence Prediction?Retrieval Hybrid (Sent-Hybrid).
As seen in cluster SC2 in Fig-ure 3, it is possible for an SC to be strongly predicted without it being sufficientlycohesive for a confident selection of a representative sentence.
However, sometimesthe ambiguity can be resolved through cues in the request.
In this example, one of thesentences in SC2 matches the request terms better than the other sentences, as it containsthe word monitor.
In order to capture such situations, we combine prediction confidencewith retrieval score to guide sentence selection (as for Sent-Pred, we use a recall measurewith TF = 1; the values for the thresholds mentioned herein appear in Table 3). For highly cohesive SCs predicted with high confidence, we selectrepresentative sentences as described in Section 3.2.2. For SCs with medium cohesion that were predicted with high confidence,we attempt to match the candidate response sentences with the requestsentences.
We can use a liberal (low) recall threshold here, because thehigh prediction confidence guarantees that the sentences in the clusterare suitable for the request, so there is no need for a conservative (high)recall threshold.
The role of retrieval in this situation is to select thesentence whose content terms best match the request, regardless of howgood the match is.
For instance, in the example in Figure 3, the sentence inSC2 that best matches the request only matches on one word (monitor), butthis is sufficient to distinguish the winning sentence from the othersentences in SC2.608Marom and Zukerman Empirical Study of Response Automation Methods For uncohesive clusters or clusters predicted with low confidence, we canrely only on retrieval.
Now we must use a more conservative recallthreshold to ensure that only sentences that are a good match for therequest sentences are included in the response.
SC3 in Figure 3 is anexample of an SC for which there is insufficient evidence to form strongcorrelations between it and request terms.
However, one of its sentences isa very good match for the second sentence in the request.
In fact, all thecontent lemmas in that request sentence are matched, resulting in a perfectrecall score of 1.0 (the non-matching words are stop words), which meansthat this response sentence is likely to be informative.Once we have a set of candidate response sentences that satisfy the appropriaterecall thresholds, we remove redundant sentences as follows.
Redundant sentences areremoved from cohesive clusters as described in Section 3.2.2; for SCs with mediumcohesion, we retain the sentence with the highest recall;10 and for uncohesive SCs, weretain all the sentences.
The rationale for this policy is that the sentences in an SC withmedium cohesion are sufficiently similar to each other, so the selection of more than onesentence may introduce redundancy.
In contrast, sentences that belong to uncohesiveSCs are deemed sufficiently dissimilar to each other, so we can select all the sentencesthat satisfy the recall criterion.As for Sent-Pred, the Sent-Hybrid method produces a response if it can returnat least one response sentence.
This happens when (a) the confidence in the highlycohesive SCs exceeds an applicability threshold; or (b) the confidence in one of the SCswith medium or low cohesion exceeds an applicability threshold, and the number ofsentences retrieved for this SC exceeds an applicability threshold.
As for Sent-Pred,confidence is calculated using Equation (6).
Both applicability thresholds (confidenceand number of retrieved sentences) are set to 0 (Table 3).3.3 SummaryThe focus of our work is on the general applicability of the different response automa-tion methods, rather than on comparing the performance of particular implementa-tion techniques.
Hence, throughout the course of this project, the different methodshad minor implementational variations, which do not affect the overall insights ofthis research.
Specifically, we used Decision Graphs (Oliver 1993) for Doc-Pred, andSVMs (Vapnik 1998) for Sent-Pred.11 Additionally, we used unigrams for clusteringdocuments and sentences, and unigrams and bigrams for predicting document clustersand sentence clusters (Sections 3.1.2 and 3.2.2).
Because this variation was uniformlyimplemented for both approaches, it does not affect their relative performance.
Thesemethodological variations are summarized in Table 2.As indicated at the beginning of this section, the implementation of these meth-ods requires the selection of different thresholds, which are subjective and applicationdependent.
Table 3 summarizes the thresholds required for the different methods, the10 We also experimented with the retention of sentences with high F-scores and with different weights forrecall and precision.
Our results were insensitive to these variations.11 This change of technique had a practical motivation: As seen in Section 3.2.2, we generated manypredictive models for sentence clusters.
This process could be automated with SVMs, but would have tobe done manually if Decision Graphs had been used.609Computational Linguistics Volume 35, Number 4Table 2Summary of the response-generation methods.Method Implementation FeaturesDoc-Ret Cosine similarity bag of lemmas (TF.IDF)Doc-Pred Clustering: Snob bag of lemmas (binary)Classification: Decision Graphs lemma unigrams andbigrams (binary)Sent-Ret Recall bag of lemmas (TF.IDF)Sent-Pred Clustering: Snob bag of lemmas (binary)Classification: SVMs lemma unigrams andbigrams (binary)Sent-Hybrid Combine sentence predictionwith sentence retrievalrange of values we considered, and the values we selected.
The applicability thresholdsare boldfaced, and those learned by the meta learning process (Section 6) are indicatedin the rightmost column (Sent-Ret is not considered by this process owing to its poorperformance; see Section 4).Table 3Thresholds for the different response-generation methods.Method Threshold Range Selected Learnedtried valueDoc-Ret cosine similarity score 0.1?0.7 0.2 YESDoc-Pred prediction probability 0.6?0.9 0.7 YESSent-Ret recall score 0.4?0.9 0.6 N/ASent-Pred confidence 0 YESSC inclusion (Equation (3))SC prediction probability 0.1?0.9 0.5SC cohesion 0.7?1.0 0.9probability of a lemma in SC(?, Equation (5)) 0.01?0.05 0.01confidence (Equation (6))SC precision 0.7?0.9 0.7SC minimum prediction probability 0.1?0.2 0.1Sent-Hybrid confidence 0 YESnumber of sentences 0 YESSC inclusionSC prediction probability 0.1?0.9 0.5SC high cohesion 0.7?1.0 0.95SC medium cohesion 0.6?0.8 0.75sentence recall scoreliberal 0.2?0.4 0.4conservative 0.4?0.9 0.6confidence (Equation (6))SC precision 0.7?0.9 0.7SC minimum prediction probability 0.1?0.2 0.1610Marom and Zukerman Empirical Study of Response Automation Methods4.
Automatic Evaluation of Individual MethodsIn this section, we offer a comparative evaluation of the response automation methodspresented in Section 3, where wemeasure the ability of the different methods to addressthe requests in the corpus.
We first describe the data used in our experiments, followedby the experimental set-up and results.4.1 The CorpusOur initial corpus consisted of 30,000 e-mail dialogues between customers and help-desk operators at Hewlett-Packard.
The dialogues deal with a variety of user requests,which include requests for technical assistance, inquiries about products, and queriesabout how to return faulty products or parts.
To focus our work on simple dialogues,we extracted a sub-corpus that satisfies two conditions:1.
The dialogues contain exactly two turns: a request e-mail followed by aresponse e-mail.
These dialogues represent situations where a request canbe resolved with a single response.2.
The response e-mails are reasonably concise (15 lines at most).
Thisrestriction is based on the observation that longer responses are quitecomplex?they often address multiple issues or have a strong temporalstructure (i.e., a sequence of steps).The resultant sub-corpus consisted of 6,659 dialogues, which deal with a wide rangeof topics.
We were hoping to account for significant differences between groups ofdialogues on the basis of their topic.
In addition, there was a practical motivation tobreak up this large sub-corpus into smaller chunks for ease of handling.
We thereforeapplied Snob to automatically cluster the sub-corpus into separate topic-based data sets.The clustering, which was done using as input the lemmas in the subject line of theusers?
e-mails, produced 51 data sets, some of which were quite small (11 data sets hadless than 25 dialogues).
Because Snob returns the significant terms in each cluster, wemerged the smaller data sets manually according to these terms?a process that yielded15 data sets in total, each data set containing between 135 and 1,236 e-mail dialogues.Owing to the time limitations of the project, the procedures described in this articlewere applied to 8 of the 15 data sets, which contain a total of 4,904 dialogues (73.6% ofthe sub-corpus, and 16.3% of the original corpus).
These data sets, which were chosenon the basis of their coverage of the corpus, are described in Table 4,12 together with aqualitative overview of our results, which are discussed in Section 4.3.It is worth noting that theremay be factors other than topic that distinguish betweendialogues, and cause differences in the performance of response generation methods fordifferent types of dialogues.
However, these factors are not readily apparent upon initialanalysis.
Topic-based clustering, which is readily apparent, is a reasonable starting pointfor distinguishing between different data sets.
The analysis presented in Section 4.3considers other features that characterize the data sets and the behavior of the responsegeneration methods.12 The topics of the omitted data sets were: servers, laptops specializing in EVO notebooks, desktops, andmiscellaneous.611Computational Linguistics Volume 35, Number 4Table 4Details of the data sets included in our experiments, and overview of results per data set.Data Topic Sub-category # of Best method (coverage/set no.
dialogues performance)Retrieval Prediction1 hand helds general 268 Doc-Ret (lo/lo) NONE2 hand helds DG models 1,160 Doc-Ret (med/lo) SENTENCE (med/hi)3 product replacement 1,236 Doc-Ret (lo/hi) ALL (hi/hi)4 laptops Armada models 561 Doc-Ret (med/lo) NONE5 laptops general 305 Doc-Ret (med/lo) SENTENCE (lo/hi)6 laptops merged (7 clusters) 632 Doc-Ret (med/lo) NONE7 desktops merged (7 clusters) 389 Doc-Ret (med/lo) Sent-Pred (lo/hi)8 misc.
merged (10 clusters) 353 Doc-Ret (med/lo) Sent-Hybrid (med/lo)TOTAL 4,9044.2 Experimental Set-UpOur experimental set-up is designed to evaluate the ability of the different response-generation methods to address unseen request e-mails.
In particular, we want to deter-mine the applicability of our methods to different situations, namely, whether differentrequests are addressed only by some methods, or whether there is a significant overlapbetween the methods.Our evaluation is performed by measuring the quality of the generated responses.Quality is a subjective measure, which is best judged by the users of the system (i.e.,the help-desk customers or operators).
In Section 5, we discuss the difficulties asso-ciated with such user studies, and describe a human-based evaluation we conductedfor a small subset of the responses generated by our system (Marom and Zukerman2007b).
However, our more comprehensive evaluation is an automatic one that treatsthe responses generated by the help-desk operators as model responses, and performstext-based comparisons between the model responses and the automatically generatedones.We employ 10-fold cross-validation, where we split each data set in the corpusinto 10 test sets, each comprising 10% of the e-mail dialogues; the remaining 90% ofthe dialogues constitute the training set.
For each of the cross-validation folds, theresponses generated for the requests in the test split are compared against the actualresponses generated by help-desk operators for these requests.
Although thismethod ofassessment is less informative than human-based evaluations, it enables us to evaluatethe performance of our systemwith substantial amounts of data, and produce represen-tative results for a large corpus such as ours.We use two measures from Information Retrieval to determine the quality of anautomatically generated response: precision and F-score (van Rijsbergen 1979; Saltonand McGill 1983).
Precision measures how much of the information in an automati-cally generated response is correct (i.e., appears in the model response), and F-scoremeasures the overall similarity between the automatically generated response andthe model response.
F-score is the harmonic mean of precision and recall, whichmeasures how much of the information in the model response appears in the gener-ated response.
We consider precision separately because it does not penalize missing612Marom and Zukerman Empirical Study of Response Automation Methodsinformation, enabling us to better assess our sentence-based methods.
Precision, recall,and F-score are calculated as follows using a word-by-word comparison (stop-wordsare excluded).13Precision =# words in both model response and automated response# of words in automated response(7)Recall =# words in both model response and automated response# of words in model response(8)F-score =2?
Precision?
RecallPrecision+ Recall(9)These measures are applied to responses generated after the thresholds in Table 3have been used to determine the applicability or coverage of each response-generationmethod.
Recall that the sentence-based methods can generate partial responses, manyof which contain only one obvious and non-informative sentence, such as Thank youfor contacting HP and Thank You, Mike, HP eServices.
We have manually excluded suchresponses from the calculation of coverage, in order to prevent these responses fromartificially improving this metric for the sentence-based methods.
This was done byvisually inspecting the sentence clusters created by Snob for these methods, and remov-ing clusters composed of non-informative sentences such as the above (between fourand six clusters were removed from each data set in this manner).
Once a response isdeemed to cover a request, then the full response (including these sentences) is used tocalculate its quality.
This has a small impact on the results of our evaluation, as a typicalresponse includes two such sentences (opening and closing), and the average length ofa response is 8.11 sentences.4.3 ResultsFigure 4 shows the coverage, average precision, and average F-score of each response-generation method per data set, where the averages are computed only for requeststhat are covered by the method in question.
For example, the average precision of theDoc-Ret method for data set no.
2, 0.39, is calculated over the 59% of the data set thatwas covered by this method.
Table 4 shows data set descriptions and sizes, togetherwith an overview of the best retrieval-based and prediction-based method for eachdata set (SENTENCE refers to both Sent-Pred and Sent-Hybrid).
The best method wasselected manually on the basis of the results in Figure 4.
To this effect, we considered themethods whose coverage exceeds some minimum (e.g., 10%), and chose the method(s)which could adequately answer the largest number of queries in the data set (basedon coverage combined with F-score and precision).
Table 5 presents the coverage andunique/best coverage of each method (the percentage of queries covered only by thismethod or for which this method produces a better reply than other methods), and theaverage and standard deviation of the precision and F-score obtained by each method(calculated over the requests that are covered).13 We also employed sequence-based measures using the ROUGE tool set (Lin and Hovy 2003), with similarresults to those obtained with the word-by-word measures.613Computational Linguistics Volume 35, Number 4Figure 4Performance of the different methods for each data set: (a) coverage, (b) F-score, and(c) precision.614Marom and Zukerman Empirical Study of Response Automation MethodsTable 5Coverage, uniqueness, precision, and F-score for the response-generation methods.Method Coverage Unique Avg.
(St dev.
)or best Precision F-scoreDoc-Ret 43% 22% 0.37 (0.34) 0.35 (0.33)Doc-Pred 29% 3% 0.82 (0.21) 0.82 (0.24)Sent-Ret 9% 0% 0.19 (0.19) 0.12 (0.11)Sent-Pred 34% 5% 0.94 (0.13) 0.78 (0.18)Sent-Hybrid 43% 10% 0.81 (0.29) 0.66 (0.25)Combined 72% 0.80 (0.25) 0.50 (0.33)As seen in Figure 4 and Tables 4 and 5, there is great variability in coverageand performance of the different methods for the different data sets (this result wasconfirmed with an ANOVA statistical test).
Our results support the following specificobservations. All prediction methods perform well for data set no.
3 (productreplacement).
The vast majority of the requests in this data set, whichcomprises 18% of the corpus, ask for a return shipping label to be mailedto the customer, so that he or she can return a faulty product.
Althoughthese requests often contain detailed product descriptions, the responsesrarely refer to the actual products, and often contain the generic responseshown in Figure 5.
Thus, the prediction methods are well suited for thisdata set, as the mention of a shipping label is a strong predictor of ageneric response, either in its entirety (as done by Doc-Pred) or broken upinto individual sentences (as done by Sent-Pred or Sent-Hybrid).
Thegeneration of complete responses by Doc-Pred explains its slightly higherF-score but lower precision compared to Sent-Pred, as a complete responsemay include sentences that are not appropriate for the situation at hand.Also note that Doc-Ret has a much lower coverage than the predictionmethods for data set no.
3, because each request has precise informationabout the actual product, so a new request can neither match an oldrequest (about a different product) nor can it match the generic response. No method performs well for data sets no.
1 (hand-helds general),4 (laptops general), and 6 (laptops merged).
Although Doc-Ret has someapplicability to these data sets, it has low performance for all of them.
Thisindicates that these data sets do not contain sufficient recurring cases forDoc-Ret to perform well, nor do they contain sufficient request?responsepairs that support the generation of predictive patterns. Sent-Ret performs poorly on all data sets.
We postulate that this happensbecause the important terms in a request and a response are spread acrossYour request for a return airbill has been received and has been sent for processing.
Yourreplacement airbill will be sent to you via email within 24 hours.Figure 5A sample response from the product replacement data set (data set no.
3).615Computational Linguistics Volume 35, Number 4several sentences in the respective documents.
Hence, the match betweenindividual response and request sentences paints only a partial (andinaccurate) picture, and the aggregation of matching response sentencesdoes not add up to an appropriate response. Overall, there is a tension between coverage and performance, wherebyhigher coverage yields lower performance, and lower coverage results inhigher performance.
This tension can be observed for the sentence-basedprediction methods with respect to data sets no.
2 (hand-held DG models),5 (laptops general), 7 (desktops merged), and 8 (miscellaneous merged);for Doc-Pred with respect to data sets no.
2 and 5 (Doc-Pred is notapplicable to data sets no.
7 and 8); and for Doc-Ret for most data sets(nos.
1 and 4?8).
This suggests that our applicability thresholds may needfurther adjustments in order to strike a better balance between coverageand performance (Table 3).Let us now examine our results separately for each of the four response-generationmethods that perform well.Doc-Ret.As seen in Table 5, Doc-Ret uniquely addresses 22% of the requests.
However,the performance of this method is quite variable (high standard deviation), which maybe due to an overly liberal setting of the applicability threshold (which results in bothpoor and good responses being generated, hence the high variability).
Nevertheless,there are some cases where this method uniquely addresses requests quite well.
Thishappens in situations such as that in Figure 1(b), where the initiating request is suffi-ciently similar to other requests with the same response.
In contrast, Doc-Ret would notwork well for a request such as that in Figure 1(a), which is quite detailed and specific,and hence unlikely to match any other request?response document.Doc-Pred.
Only about a tenth of the requests covered by Doc-Pred are uniquely ad-dressed by this method, but the generated responses are of a fairly high quality, withan average precision and F-score of 0.82.
As indicated previously, the higher F-scoreand lower precision of Doc-Pred (compared to Sent-Pred) may be explained by thefact that the complete responses produced by the Doc-Pred method sometimes containspecific sentences that are inappropriate for the current situation.
The rather large stan-dard deviation for F-score and precision suggests that Doc-Pred exhibits a somewhatinconsistent behavior.
This may be explained by Figure 4, which shows that Doc-Predperforms very well on data set no.
3 (product replacement), but not so well on theothers (Doc-Pred also has good F-score and precision scores for data set no.
2, but poorcoverage).Sent-Pred.
In contrast to Doc-Pred, Sent-Pred can find regularities at the sub-documentlevel, and generate partial responses, which typically omit inappropriate sentences thatmay be included by Doc-Pred.
As a result, the responses generated by Sent-Pred have aconsistently high precision (average 0.94, standard deviation 0.13), but this can be at theexpense of recall, which explains the lower F-score (compared to Doc-Pred).
Overall,the Sent-Pred method outperforms the other methods in 5% of the cases, where it eitheruniquely addresses requests, or produces responses with a higher F-score than thosegenerated by other methods.
As an example of situations where Sent-Pred outperformsall other methods, consider Figure 1(a), where Sent-Pred outputs the response shown616Marom and Zukerman Empirical Study of Response Automation MethodsFigure 6Example showing an appropriate response generated by the Sent-Hybrid method (bottom) thatdiffers from the model response.in this example, but without the first sentence.
In other words, the generic portionof the response is confidently produced, and the specific portion is left out due toinsufficient evidence.
This example shows the benefit of a partial response: Althoughthe response does not actually answer the user?s specific question (which would bedifficult to automate due to the complex nature of the request), it can potentially savethe user valuable time by referring him or her to the appropriate repair service.Sent-Hybrid.
The Sent-Hybrid method extends the Sent-Pred method by performingsentence retrieval.
Sent-Hybrid?s higher coverage is achieved by the retrieval compo-nent, which disambiguates between groups of candidate sentences, thus enabling moresentences to be included in a generated response.
However, this is at the expense ofprecision.
Although retrieval selects sentences that match closely a given request, thesesentences can differ from the ?selections?
made by a human operator in the modelresponse.
Precision (and hence F-score) penalizes such sentences, even when they aremore appropriate than those in the model response.
For instance, consider the exampleat the top of Figure 6.
The response is quite generic, and is used almost identicallyfor several other requests.
The Sent-Hybrid method produces a very similar response,shown in the text at the bottom of Figure 6.
Only the first sentence differs from the firstsentence in the model response (the different parts have been italicized).
The sentenceselected by the Sent-Hybrid method, which matches more request words than the firstsentence in the model response, was chosen from a sentence cluster with mediumcohesion (Section 3.2.3), which contains sentences that describe different reasons forsetting up a repair (the matching word is screen).
The rather high standard deviationin the precision (and hence F-score) for Sent-Hybrid may be due to these kinds ofsituations.
Nonetheless, this method outperforms the other methods in about 10% ofthe cases, where it either uniquely addresses requests, or produces responses with ahigher F-score than those generated by other methods.All the methods combined.
The bottom row of Table 5 shows that all the methodstogether have a coverage of 72%, which means that at least one of the methods canproduce a non-empty and non-trivial response for 72% of the requests.
The combinedF-score and precision averages are calculated on the basis of the best-performingmethod for each request.
At first glance, using the best method may appear too lenient,617Computational Linguistics Volume 35, Number 4as in practice, we cannot always automatically select this method in advance.
However,these averages also suffer from the fact that in many cases only the Doc-Ret method isapplicable, but its performance is poor.
As mentioned previously, this tension betweencoverage and performance may be attributed to our empirically determined applicabil-ity thresholds (Section 3).4.4 SummaryWe have investigated the suitability of different response generation methods for thehelp-desk task.
These methods, which vary significantly in their approach to responseautomation, cover a wide range of situations that arise in a help-desk corpus.Ideally, we would like to explain our results in terms of features of the data sets, sothat users of our system can select a single best response-generationmethod on the basisof these features.
However, with the exception of data set no.
3, no clear set of featurespresents itself to support such a selection.
Further, as seen in Table 4, superficial featuresof the data sets, such as topic and size, are not sufficient to characterize the applicabilityand performance of the different methods.
These results indicate that (1) there aredeeper features of data sets that must be considered in order to select a single suitabletechnique; or (2) the selection of a technique does not depend on features of the dataset itself, but on the spread of situations in the data set.
That is, the applicability andperformance of a response-generation method depends on the specifics of the situation,and different data sets contain a different spread of situations.
This second conjecture issupported by the results in Figure 4 and Table 5, which indicate that different methodshave pockets of unique applicability in each data set.
Of course, this still begs thequestion of why different data sets have different spreads of situations.
Unfortunately,we do not have an answer to this question, and circumvent it by using themeta-learningtechnique described in Section 6, which has the added benefit of obviating the problemof selecting applicability thresholds.However, if one wishes to select a response-generation method without meta-learning, the following considerations can be applied. Predictive methods are suitable for situations where there are relativelyfew responses for many, varied requests.
That is, the responses generalizecommon answers to a variety of problems.
Specifically, Doc-Pred issuitable when this observation applies to complete answers, as is the casefor data set no.
3, while sentence prediction methods are appropriate whenparts of replies generalize common answers to a variety of problems.Because the sentence-based prediction methods outperform Doc-Pred forall data sets except no.
3 (where the three prediction methods performsimilarly), we recommend the sentence-based methods.
Among these, weprefer Sent-Pred due to its high and consistent precision and F-score,pending a further investigation of Sent-Hybrid. Doc-Ret applies to situations where there are specific problems whichwarrant a specific complete answer.5.
User-Based Evaluation of Individual MethodsThe size of our corpus necessitates an automatic evaluation in order to produce mean-ingful results, especially because we are comparing several methods under a number of618Marom and Zukerman Empirical Study of Response Automation Methodsexperimental settings.
Although our automatic evaluation has yielded useful insights,it has two main limitations. As we saw in Section 4.3 (Figure 6), appropriate responses are sometimespenalized when they do not match precisely the model response.However, it is often the case that there is not one single appropriateresponse to a query, and even a help-desk operator may respond to thesame question in different ways on different occasions. The relationship between the results obtained by the automatic evaluationof the responses generated by our system and people?s assessments ofthese responses is unclear, in particular for partial responses.These limitations reinforce the notion that automated responses should be assessed ontheir own merit, rather than with respect to some model response.In Marom and Zukerman (2007a) we identified several systems that resemble oursin that they provide answers to queries.
These systems addressed the evaluation issueas follows. Only qualitative observations of the responses were reported (no formalevaluation was performed) (Lapalme and Kosseim 2003; Roy andSubramaniam 2006). Only an automatic evaluation was performed, which relied on havingmodel responses (Berger and Mittal 2000; Berger et al 2000). A user study was performed, but it was either very small compared to thecorpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), orthe corpus itself was significantly smaller than ours (Feng et al 2006;Leuski et al 2006).
The representativeness of the sample size was notdiscussed in any of these studies.There are significant practical difficulties associated with conducting the user stud-ies needed to produce meaningful results for our system.
Firstly, the size of our corpusand the number of parameters and settings that we need to test mean that in order fora user study to be representative, a fairly large sample involving several hundreds ofrequest?response pairs would have to be used.
Further, user-based evaluations of theoutput produced by our system require the subjects to read relatively long request?response e-mails, which quickly becomes tedious.In order to address these limitations in a practical way, we conducted a small userstudy where we asked four judges (graduate students from the Faculty of Informa-tion Technology at Monash University) to assess the responses generated by our sys-tem (Marom and Zukerman 2007a).
Our judges were instructed to position themselvesas help-desk customers who know that they are receiving an automated response,and that such a response is likely to arrive quicker than a response composed by anoperator.
Our user study assessed the response-generation methods from the followingperspectives, which yield information that is beyond the F-score and precisionmeasuresobtained in the automatic evaluation. Informativeness: Is there anything useful in the response that wouldmake it a good automatic response, given that otherwise the customer has619Computational Linguistics Volume 35, Number 4to wait for a human-generated response?
We used a scale from 0 to 3,where 0 corresponds to ?not at all informative?
and 3 corresponds to?very informative.
? Missing information: Is any crucial information item missing?
Y/N. Misleading information: Is there any misleading information?
Y/N.
Weasked the judges to consider only information that might misguide thecustomer, and ignore information that is so irrelevant that it would beignored by a customer who knows that the response is automated (forexample, receiving an answer for a printer, when the request was for alaptop). Compare to model response: How does the generated response comparewith the model response?
Worse/Same/Better.
This is a summaryquestion that rates a ?customer?s?
overall impression of a response.5.1 Experimental Set-UpWe had two specific goals for this evaluation.
First, we wanted to compare document-level versus sentence-level methods.
Second, we wanted to evaluate cases whereonly the sentence-level methods can produce a response, and establish whether suchresponses, which are often partial, provide a good alternative to a non-response.
Wetherefore presented two evaluation sets to each judge.1.
The first set contained responses generated by Doc-Pred and Sent-Hybrid.These two methods obtained similar precision values in the automaticevaluation (Table 5), so we wanted to compare how they would fare withour judges.2.
The second set contained responses generated by Sent-Pred andSent-Hybrid for requests for which Doc-Pred could not produce aresponse.
The added benefit of this evaluation set is that it enables us toexamine the individual contribution of the sentence retrieval component.Each evaluation set contained 80 cases, randomly selected from the corpus in proportionto the size of each data set, where a case contained a request e-mail, the model response,and the responses generated by the twomethods being compared.
Each judgewas given20 of these cases, and was asked to assess the generated responses on the four criterialisted previously.14Wemaximized the coverage of this study by allocating different cases to each judge,thus avoiding a situation where a particularly good or bad set of cases is evaluatedby all judges.
In addition, we tried to ensure that the sets of cases shown to thejudges were of similar quality, so that the judges?
assessments would be comparable.Because the judges do not evaluate the same cases, we could not employ standardinter-annotator agreement measures (Carletta 1996).
However, it is still necessary to14 We asked the judges to leave a question unanswered if they felt they did not have the technicalknowledge to make a judgment, but this did not occur.620Marom and Zukerman Empirical Study of Response Automation Methodshave some measure of agreement, and control for bias from specific judges or specificcases.
This was done by performing pairwise significance testing, treating the data fromtwo judges as independent samples (we used the Wilcoxon Rank-Sum Test for equalmedians).
We conducted this significance test separately for each method and eachof the four criteria, and eliminated the data from a particular judge if he or she hada significant disagreement with other judges.
This happened with one of the judges,who was significantly more lenient than the others on the Sent-Pred method for thefirst, second, and fourth criteria, and with another judge, who was significantly morestringent on the Sent-Hybrid method for the third criterion.5.2 ResultsFigure 7 shows the results for the four criteria.
The left-hand side of the figure pertainsto the first evaluation set, and the right-hand side to the second set.The left-hand side of Figure 7(a) shows that when both Doc-Pred and Sent-Hybridare applicable, the former is generally preferred, rarely receiving a zero informativenessjudgment.
Because the two methods are evaluated for the same set of cases, we canperform a paired significance test for differences between them.
Using a Wilcoxonsigned rank test for a zeromedian difference, we obtain a p-value	 0.01, indicating thatthe differences in judgments between the two methods are statistically significant.
TheFigure 7Results of the human study for the evaluation of generated responses.621Computational Linguistics Volume 35, Number 4right-hand side of Figure 7(a), which compares the two sentence-based methods, showsthat there do not appear to be significant differences between our subjects?
assessment ofthese methods.
This result is confirmed by the paired significance test, which producesa p-value of 0.13.As shown in Figure 7(b), the results for the missing-information and misleading-information criteria also favor the Doc-Pred method.
The responses produced byDoc-Pred were judged to have significantly less missing information than those gen-erated by Sent-Hybrid (the paired significance test produces a p-value 	 0.01).
Theresponses produced by Doc-Pred were also judged to have less misleading informa-tion than those generated by Sent-Hybrid, but the paired differences between the twomethods are not statistically significant (the p-value is 0.125).
The second evaluationset was judged to have missing information in approximately 55% of the cases forboth sentence-level methods (the p-value is 0.11, indicating an insignificant differencebetween these methods).
This high proportion of missing information is in line with therelatively low F-scores obtained in the automatic evaluation (Table 5), as missing infor-mation results in low recall and hence a lower F-score.
The results for the misleading-information criterion also indicate no significant difference between the sentence-levelmethods (the p-value is 1).
The low proportion of misleading information is in linewith the high precision values obtained in the automatic evaluation (Table 5)?whereasresponses with a high precision may be incomplete, they generally contain correctinformation.The left-hand side of Figure 7(c) shows that Doc-Pred receives more ?same?than ?worse?
judgments, although the opposite is true for Sent-Hybrid, and that bothDoc-Pred and Sent-Hybrid receive a small proportion of ?better?
judgments.
The pairedsignificance test produces a p-value	 0.01, confirming that these differences are signif-icant.
The right-hand side of Figure 7(c) shows smaller differences between Sent-Predand Sent-Hybrid, and indeed the p-value for the paired differences is 0.27.
Notice thatSent-Pred does not receive any ?better?
judgments, whereas Sent-Hybrid does.5.3 SummaryThe results of this study show that responses provided by document-level methodswere preferred to responses provided by sentence-level methods, but when document-level methods cannot be used, the sentence-level methods provide a good alternative.Additionally, although our trial subjects showed a slight preference for the outputproduced by the Sent-Hybrid method compared to Sent-Pred, this preference was notstatistically significant.Although these results confirm those obtained by the automatic evaluation (Sec-tion 4), the result regarding the Sent-Hybrid method is somewhat disappointing.
Thisis because we hoped that our trial subjects would prefer Sent-Hybrid to Sent-Pred, asthe former is designed to better tailor a response to a request.
However, we cannotdetermine from this result whether indeed there is no difference between the sentence-basedmethods, or whether such a difference simply could not be observed from our testsample of at most 80 cases, which constitutes 1.8% of the corpus used in our automaticevaluation (as indicated previously, it would be quite difficult to conduct user studieswith a much larger data set).This outcome reinforces the previously mentioned problems associated with con-ducting meaningful user studies for a large corpus such as ours.
These problems areexacerbated by our proportional data-selection policy, which is necessary to make the622Marom and Zukerman Empirical Study of Response Automation Methodstest set representative of the corpus, but increases the difficulty of drawing specific con-clusions, for example, determining whether a particular method is favored for specificdata sets.6.
Meta-LearningIn Section 4, we employed empirically determined applicability thresholds to cir-cumscribe the coverage of the different response-generation methods.
However, asshown by our results, these thresholds were sometimes sub-optimal.
In this section,we describe a meta-level process which can automatically select a response-generationmethod to address a new request without using such thresholds.A common way to combine different models consists of selecting the model that ismost confident regarding its decision (Burke 2002).
However, in our case, the individualconfidence (applicability) measures employed by our response-generation methods arenot comparable (e.g., the retrieval score in Doc-Ret is different in nature from the pre-diction probability in Doc-Pred).
Hence, prior to selecting the most confident method,we need to find a way to compare the different measures of confidence.
Because theperformances of the different methods are comparable, we do this by establishing a linkbetween confidence and performance.
In other words, our meta-level process learnsto predict the performance of the different methods from their confidence levels onthe basis of previous experience.
These predictions enable our system to recommenda particular method for handling a new (unseen) request (Marom, Zukerman, andJapkowicz 2007).Following Lekakos and Giaglis (2007), one approach for achieving this objectiveconsists of applying supervised learning, where a winning method is selected for eachcase in the training set, all the training cases are labeled accordingly, and then thesystem is trained to predict a winner for unseen cases.
However, in our situation, thereis not always one single winner (two methods can perform similarly well for a givenrequest), and there are different ways to pick winners (for example, based on F-scoreor precision).
Therefore, such an approach would require the utilization of subjectiveheuristics for creating labels, which would significantly influence what is being learned.Instead, we adopt an unsupervised approach that finds patterns in the data?confidencevalues coupled with performance scores (Section 6.1)?and then attempts to fit unseendata to these patterns (Section 6.2).
Heuristics are still needed in order to decide whichresponse-generation method to apply to an unseen case, but they are applied only afterthe learning is complete (Section 6.3).
In other words, the subjective process of settingperformance criteria (which should be conducted by the organization running the help-desk) does not influence the machine learning process.6.1 TrainingWe train the system by clustering the ?experiences?
of the response-generation methodsin addressing requests, where each experience is characterized by the value of theconfidence measure employed by a method and its subsequent performance, reflectedby precision and recall (Equations (7) and (8), respectively).
We then use the programSnob (Wallace and Boulton 1968; Wallace 2005) to cluster these experiences.
Figure 8(a)is a projection of the centroids of the clusters produced by Snob into the three mostsignificant dimensions discovered by Principal Component Analysis (PCA)?these di-mensions account for 95% of the variation in the data.
The bottom part of Figure 8(b)623Computational Linguistics Volume 35, Number 4Figure 8Clusters of response-generation methods obtained from the training set: (a) dimensionsproduced by PCA and (b) sample clusters.shows the (unprojected) centroid values of three of the clusters (the top part of the figurewill be discussed subsequently).15 These clusters were chosen because they illustrateclearly three situations of interest. Single winner ?
Cluster 8 shows a case where a single strategy is clearlypreferred.
In this case the winner is Doc-Ret.
Its precision and recall valuesin this cluster are 0.91 and 0.76, respectively. No winner ?
Cluster 11 shows a case where none of the methods do well.They all yield precision and recall values of 0. Multiple winners ?
In Cluster 16, both Doc-Pred and Sent-Pred arecompetitive, yielding precision and recall values of (0.90, 0.89) and (0.97,0.78), respectively.
A decision between the two methods depends onwhether we favor precision or recall, as discussed subsequently.6.2 PredictionWe test the system with an unseen set of requests, which we feed to each response-generation method.
Each method then outputs a value for its confidence measure.
Wedo not know in advance how each method will perform?this information is missing,and we predict it on the basis of the clusters obtained from the training set.
Ourprediction of how well the different methods will perform on an unseen case is basedon (1) howwell the unseen case fits each of the clusters and (2) the average performancevalues in each cluster as indicated by its centroid.15 The Sent-Ret method was excluded from our experiments due to its poor performance in the comparativestudy described in Section 4.624Marom and Zukerman Empirical Study of Response Automation MethodsThe top part of Figure 8(b) shows an example of an unseen case whose confidencevalues are most similar to those in the centroid of Cluster 16.
In this case, the selectionof a method depends on whether we favor recall or precision, as Doc-Pred has ahigher recall than Sent-Pred, but Sent-Pred has a higher precision.
Now, Cluster 15(not labeled in Figure 8(a)) contains similar confidence values to those of Cluster 16,but its (precision, recall) values for Doc-Pred and Sent-Pred are (0.76, 0.66) and (0.84,0.67) respectively.
If Cluster 15 had the strongest match with the unseen case, thenSent-Pred would have been chosen, regardless of any preferences for precision or recall.However, it is not clear that the best policy consists of simply choosing the methodsuggested by the best-matching cluster.
This is particularly the case when an unseenexample has a reasonably good match with more than one cluster (e.g., Clusters 15and 16).The prediction step is implemented using Snob, which is able to accept data withmissing values.
For each unseen data point x (with missing performance values), Snobcalculates Pr(ci|x), the posterior probability of each cluster ci given this data point.These probabilities indicate how well an unseen case matches each of the clusters.
Forexample, for the unseen case in Figure 8(b), Snob may assign posterior probabilitiesof 0.5 and 0.3 to Clusters 16 and 15, respectively (and lower probabilities to weaker-matching clusters, such as Cluster 8).16We utilize these probabilities in two alternative ways for estimating the per-formance of each response-generation method: Max, which considers only the best-matching cluster (i.e., that with the highest posterior probability); and Weighted,which considers all clusters, weighted by their posterior probabilities.
These techniquesare used to calculate the estimated precision (?pk) and estimated recall (?rk) of eachresponse-generation method methodk ?
{Doc-Ret, Doc-Pred, Sent-Pred, Sent-Hybrid} asfollows. Max:?pk = p(i?
)kand ?rk = r(i?
)k, for i?
= argmaxi=1,...,NPr(ci|x) (10)where p(i)kand r(i)kare the precision and recall components, respectively, formethodk in the centroid of cluster i, and N is the number of clusters. Weighted:?pk =N?i=1Pr(ci|x)?
p(i)kand ?rk =N?i=1Pr(ci|x)?
r(i)k(11)6.3 Method SelectionIn order to select a method for a given request, we need to combine our estimatesof precision and recall into an overall estimate of performance, and then choose themethod with the best estimated performance.
The standard approach for combiningprecision and recall is to compute their harmonic mean, F-score, as we have done in our16 In principle, we could have used a classification method to predict clusters from the values of theconfidence measures for unseen cases.
We posit that this would not have a significant effect on theresults, in particular for MML-based classification techniques, such as Decision Graphs (Oliver 1993).625Computational Linguistics Volume 35, Number 4comparative evaluation in Section 4.
However, in order to accommodate different levelsof preference towards precision or recall, as discussed herein, we use the followingweighted F-score calculation (van Rijsbergen 1979).F-score ={wPrecision+1?
wRecall}?1(12)where w is a weight between 0 and 1 given to precision.
When w = 0.5 we have thestandard usage of F-score (Equation (9)), and for w > 0.5, we have a preference forhigh precision.
For example, for w = 0.5, the precision and recall values of Cluster 16(Figure 8(b)) translate to F-scores of 0.895 and 0.865 for Doc-Pred and Sent-Pred, respec-tively, leading to a choice of Doc-Pred.
In contrast, for w = 0.75, the respective F-scoresare 0.897 and 0.914, leading to a choice of Sent-Pred.7.
Evaluation of Meta-LearningWe evaluate the meta-learning system by looking at the quality of the response pro-duced by the method selected by this system, where, as done in Section 4, qualityis measured using F-score and precision.
However, here we employ 5-fold cross-validation (instead of 10-fold) to ensure that we get a good spread of selected methodsin each testing split.
This is particularly important when only a few methods dominatefor a data set.7.1 Experimental Set-UpIn our evaluation, we compare the alternative approaches for estimating performance(Equations (10) and (11)), and consider the effect of favoring precision when selecting amethod via the weighted F-score calculation (Equation (12)).
To perform these compar-isons we employ the following configurations. Max50: Use the argmax alternative for estimating performance(Equation (10)), and w = 0.5 in Equation 12. Max75: As Max50, but with w = 0.75. Weighted50: Use the weighted alternative for estimating performance(Equation (11)), and w = 0.5 in Equation (12). Weighted75: As Weighted50, but with w = 0.75.We also devised the following baselines to help ground our results. Random: Select between the methods randomly. Gold50: Select between the methods based on their actual performance (asopposed to their estimated performance), using w = 0.5 in Equation (12). Gold75: As Gold50, but with w = 0.75.As we saw from Cluster 11 in Figure 8(b), the estimated performance can be lowfor all the response-generation methods.
Therefore, we also test these configurations in626Marom and Zukerman Empirical Study of Response Automation MethodsTable 6Precision and F-score for the meta-learning methods averaged over the corpus.All cases Cases with precision ?
0.8Precision F-score Precision F-score CoverageAvg.
(St dev) Avg.
(St dev) Avg.
(St dev) Avg.
(St dev) Avg.
(actual precision ?
0.8)Random 0.558 (0.37) 0.376 (0.33) 0.955 (0.06) 0.696 (0.25) 37.6%Gold50 0.725 (0.26) 0.548 (0.30) 0.934 (0.06) 0.732 (0.26) 53.0%Gold75 0.781 (0.25) 0.537 (0.29) 0.952 (0.06) 0.689 (0.26) 60.6%(estimated precision ?
0.8)Max50 0.704 (0.28) 0.507 (0.31) 0.844 (0.22) 0.648 (0.30) 56.7%Max75 0.768 (0.27) 0.498 (0.28) 0.911 (0.17) 0.629 (0.27) 56.7%Weighted50 0.727 (0.27) 0.512 (0.30) 0.874 (0.18) 0.649 (0.29) 57.1%Weighted75 0.776 (0.26) 0.499 (0.28) 0.919 (0.16) 0.626 (0.27) 57.1%a practical setting where the system has the choice of not selecting any method if theestimated performance of all the methods is poor.
We envisage that a practical systemwould behave in this manner, in the sense that a request for which none of the existingmethods can produce an appropriate response would be passed to an operator.
Asmentioned in Section 4.2, we consider precision to be an important practical criterionbecause it does not penalize partial but correct responses.
Therefore, we ?implement?our practical system by selecting only responses whose estimated precision is above0.8.
For these tests we also report on coverage, that is, the percentage of cases where thiscondition is met.
Note that the baselines do not have an estimated precision becausethey do not use meta-learning.
However, for completeness, we implement the practicalsystem for them as well, with a threshold of 0.8 on actual precision.7.2 ResultsTable 6 shows the results of our tests averaged over all the cases in the corpus (withstandard deviations in parentheses).
The left-hand side corresponds to the setting wherethe system always selects a response-generation method, and the right-hand side corre-sponds to the setting where a method is selected only if its precision equals or exceeds0.8 (this is an estimated precision for the Max and Weighted configurations, and anactual precision for the Gold and Random baselines).Let us first consider the left-hand side of Table 6.
As expected, the Random base-line has the worst performance.
The Gold baselines outperform their correspondingmeta-learning counterparts (except for the precision of Weighted50), but the differ-ences in precision are not statistically significant between the Gold and the Weightedconfigurations (using a t-test with a 1% significance level).
Comparing the correspond-ing Weighted and Max configurations, the former is superior, but this is statisticallysignificant only for the difference in precision values between Weighted50 and Max50.Comparing a standard F-score calculationwith a precision-favoring calculation (w = 0.5versus w = 0.75 in Equation (12)), as expected, precision is significantly higher for thelatter in all testing configurations (p < 0.01).
This increase in precision is at the expenseof a reduced F-score, but the increase is larger than the reduction.627Computational Linguistics Volume 35, Number 4Figure 9Proportion of methods selected by meta-learning with the Weighted50 and Weighted75configurations for each data set.Now, in the right-hand side of Table 6, we see that the Random configuration has thebest precision and the second-best F-score,17 but its coverage is quite low (only 37.6%).In contrast, the meta-learning configurations cover a proportion of the requests thatis comparable to the coverage of the Gold baselines (approximately 57%), and all theresults are substantially improved; as expected, all the precision values are high, andalso more consistent than before (they have a lower standard deviation).
These resultsare quite impressive for the meta-learning configurations, as their selection betweenmethods is based on estimated precision, as opposed to the baselines, whose selectionsare based on actual precision, which is not available in practice.
Comparing the corre-sponding Weighted and Max configurations, there are no significant differences in F-score, but Weighted outperforms Max on precision (the difference between Weighted75and Max75 produces a p-value of 0.035).
Finally, comparing w = 0.5 with w = 0.75 forboth Weighted and Max, as for the All-cases results, the increase in precision is largerthan the reduction in F-score (p < 0.01).Now that we have evaluated the ability of the meta-level process to select betweenresponse-generation methods, let us inspect what happens for each data set.
We sawin Section 4 that the various methods differ in their applicability to the different datasets (Figure 4).
Hence, we would expect the meta-level process to select between themethods differently for each data set.
Figure 9 shows the method-selection proportionsfor each data set for the Weighted50 and Weighted75 configurations, using the practicalsetting where the system can choose not to select any method.
What is immediatelynotable is that no method is selected for two of the data sets (nos.
4 and 6).
This followsfrom the poor performance of all the methods for these data sets (Figures 4(b) and 4(c)).At first glance, it appears that the selection of the Sent-Pred method for data set no.
1contradicts the results in Figure 4, which shows a low coverage of Sent-Pred for thisdata set.
However, this selection is justified by the fact that the meta-learning procedure17 This seemingly good performance represents the average precision and resultant F-score of all theresponses with actual precision ?
0.8, and is not the result of the application of a selection strategy.628Marom and Zukerman Empirical Study of Response Automation Methodsselects methods based on their historical performance (precision and recall), withoutfiltering on applicability threshold (which is the basis for coverage).
Figure 9 alsohighlights the impact of favoring precision on the selection of the Sent-Pred methodinstead of Doc-Pred.
This effect is most dramatic for data set no.
3, but it can also beobserved for data sets no.
2 and 5.Overall, Sent-Pred dominates for most data sets (except data set no.
3 under theWeighted50 configuration, and data set no.
8).
Also notable is the fact that Sent-Hybridis selected only for data set no.
7.
We postulate that this happens because data setno.
7 merges several sub-topics, and has different versions of similar responses, whereeach version has a generic component combined with a request-specific component(this feature is not shared by the other merged data sets no.
6 and 8).
Sent-Pred is notconfident enough to select one of these specific components, whereas Sent-Hybrid is.This ability to select a specific response is demonstrated in Figure 10, which showsa request from data set no.
7 and its automated response.
This response belongs to amedium-cohesion cluster which contains responses that share the generic segment upto regarding, but the rest of the response refers specifically to terms in the request.7.3 SummaryThe meta-learning results may be summarized as follows. The meta-learning system significantly outperforms the random selectionbaseline, and is competitive with the gold baseline. The Weighted option for estimating performance (Equation (11)) ispreferable to the Max option (Equation (10)), as it is better able to handlethe uncertainty that arises when a particular method has a wide range ofprecision and recall values. A precision-favoring approach is recommended, as the resultant increasein precision (reflecting a higher proportion of correct information) is largerthan the reduction in F-score. Overall, Doc-Pred and Sent-Pred were the preferred methods, withSent-Pred clearly dominating for the Weighted75 configuration.Sent-Hybrid was selected often for data set no.
7, and Doc-Ret was theonly method selected, albeit seldom, for data set no.
8. Because the system can estimate performance prior to producing aresponse, it is able to opt for a non-response rather than risk producing abad one.
The decision of what is a bad response should be made by theorganization using the system.
With the stringent criterion we have chosen(precision?
0.8), the system yields a good performance for approximately57% of the requests.Figure 10Example showing an appropriate response generated by the Sent-Hybrid method.629Computational Linguistics Volume 35, Number 48.
Related ResearchThe automation of help-desk responses has been previously tackled using mainlyknowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) andcase-based reasoning (Watson 1997).
Such technologies require significant human in-put, and are difficult to create and maintain (Delic and Lahaix 1998).
In contrast, thetechniques examined in this article are corpus-based and data-driven.
The process ofcomposing a planned response for a new request is informed by probabilistic and lexicalproperties of the requests and responses in the corpus.There are very few reported attempts at corpus-based automation of help-deskresponses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel andScheffer 2004; Malik, Subramaniam, and Kaushik 2007).eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000), re-trieves a list of request?response pairs and presents a ranked list of responses to the user.If the user is unsatisfied with this list, an operator is asked to generate a new response.The operator is assisted in this task by the retrieval results: The system highlights therequest-relevant sentences in the ranked responses.
However, there is no attempt toautomatically generate a single response.Bickel and Scheffer (2004) compared the performance of document retrieval anddocument prediction for generating help-desk responses.
Their retrieval technique,which is similar to our request-to-request Doc-Ret method, matches user questions tothe questions in a database of question?answer pairs.
Their prediction method, which issimilar to Doc-Pred, is based on clustering the responses in the corpus into semanticallyequivalent answers, and then training a classifier to match a query with one of theseclasses.
The generated response is the answer that is closest to the centroid of the cluster.Bickel and Scheffer?s results are consistent with ours, in the sense that the performanceof the Doc-Ret method is significantly worse than that of Doc-Pred.
However, it is worthnoting that their corpus is significantly smaller than ours (805 question?answer pairs),their questions seem to be much simpler and shorter than those in our corpus, and thereplies shorter and more homogeneous.Malik, Subramaniam, and Kaushik (2007) developed a system that builds question?answer pairs from help-center e-mails, and then maps new questions to existing ques-tions in order to retrieve an answer.
This part of their approach resembles our Doc-Retmethod, but instead of retrieving entire response documents, they retrieve individualsentences.
In addition, rather than including actual response sentences in a reply, theirsystemmatches response sentences to pre-existing templates and returns the templates.Lapalme and Kosseim (2003) investigated three approaches to the automatic gener-ation of response e-mails: text classification, case-based reasoning, and question answer-ing.
Text classification was used to group request e-mails into broad categories, someof which, such as requests for financial reports, can be automatically addressed.
Thequestion-answering approach and the retrieval component of the case-based reasoningapproach were data driven, using word-level matches.
However, the personalizationcomponent of the case-based reasoning approach was rule-based (e.g., rules were ap-plied to substitute names of individuals and companies in texts).With respect to these systems, the contribution of our work lies in the considerationof different kinds of corpus-based approaches (namely, retrieval and prediction) appliedat different levels of granularity (namely, document and sentence).Two applications that, like help-desk, deal with question?answer pairs are: sum-marization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004),and answer extraction in FAQs (Frequently Asked Questions) (Berger and Mittal 2000;630Marom and Zukerman Empirical Study of Response Automation MethodsBerger et al 2000; Jijkoun and de Rijke 2005; Soricut and Brill 2006).
An importantdifference between these applications and help-desk is that help-desk request e-mailsare not simple queries.
In fact, some e-mails do not contain any queries at all, and evenif they do, it is not always straightforward to distinguish the queries from the text thatprovides background information.
Therefore, the generation of a help-desk responseneeds to consider a request e-mail in its entirety, and ensure that there is sufficientevidence to match the request with a response or parts of responses.In e-mail-thread summarization, Dalli, Xia, and Wilks (2004) applied a proceduralapproach where they recognized named entities and performed anaphora resolutionprior to applying ranking metrics to select sentences for inclusion in a thread summary.However, their approach does not specifically address the question?answer aspect of ane-mail thread, potentially omitting important information.
This problem was addressedby Shrestha and McKeown (2004), who performed supervised learning in order tomatch questions with answers in e-mail threads, as a first step in the summarizationof such threads.
A significant difference between our approach and theirs is in ouruse of unsupervised learning, which is necessitated by the size of our data set.
Also,Shrestha and McKeown used high-level features for machine learning, as well as word-based features.
As indicated in Section 3.2.2, our Sent-Pred experiments with high-levelfeatures (specifically syntactic features) did not improve our results.
Finally, Shresthaand McKeown used paragraphs as a unit of information?an approach we tried latein our project with encouraging results.
This suggests that there are situations whereone can generalize a response that is longer than a sentence but shorter than a wholedocument.
Unfortunately, we could not pursue this avenue of research owing to timelimitations.In FAQs, Berger and Mittal (2000) employed a sentence retrieval approach based ona language model where the entire response to an FAQ is considered a sentence, andthe questions and answers are embedded in an FAQ document.
They complementedthis approach with machine learning techniques that automatically learn the weightsof different retrieval models.
Berger et al (2000) compared two retrieval approaches(TF.IDF and query expansion) and two predictive approaches (statistical translationand latent variable models).
Jijkoun and de Rijke (2005) compared different variants ofretrieval techniques.
Soricut and Brill (2006) compared a predictive approach (statisticaltranslation), a retrieval approach based on a language-model, and a hybrid approachwhich combines statistical chunking and traditional retrieval.
Two significant differ-ences between help-desk and FAQs are the following. The responses in the help-desk corpus are personalized, which meansthat on one hand, we must abstract from them sufficiently to obtainmeaningful regularities, and on the other hand, we must be careful notto abstract away specific information that addresses particular issues. Help-desk responses have much more repetition than FAQs, becausethe corpus is made up of individual dialogues, rather than typicalrequest?response pairs.
This motivates the use of multi-documentsummarization techniques, rather than question-answering approaches,to extract individual answers.These issues also differentiate the help-desk application from other types of question-answering applications, specifically those found in the field of restricted domain ques-tion answering (Molla?
and Vicedo 2007).631Computational Linguistics Volume 35, Number 4In addition to the different response-generationmethods, we have proposed ameta-level strategy to combine them.
This kind of meta-learning is referred to as stacking bythe DataMining community (Witten and Frank 2000).
Lekakos andGiaglis (2007) imple-mented a supervised version of this approach for a recommender system, as opposedto our unsupervised version.
They also proposed two major categories of meta-learningapproaches for recommender systems, merging and ensemble, each subdivided intothemore specific subclasses suggested by Burke (2002) as follows.
Themerging categorycorresponds to techniques where the individual methods affect each other in differ-ent ways (this category encompasses Burke?s feature combination, cascade, featureaugmentation, and meta-level sub-categories).
The ensemble category corresponds totechniques where the predictions of the individual methods are combined to producea final prediction (this category encompasses Burke?s weighted, switching, and mixedsub-categories).Our system falls into the ensemble category, because it combines the results ofthe various methods into a single outcome.
More specifically, it belongs to Burke?sswitching sub-category, where a single method is selected on a case-by-case basis.
Asimilar approach is taken in Rotaru and Litman?s (2005) reading comprehension system,but their system does not perform any learning.
Instead it uses a voting mechanismto select the answer given by the majority of methods.
The question answering systemdeveloped by Chu-Carroll et al (2003) belongs to the merging category of approaches,where the output of an individual method can be used as input to a different method(this corresponds to Burke?s cascade sub-category).
Because the results of all themethods are comparable, no learning is required: At each stage of the ?cascade ofmethods,?
the method that performs best is selected.
In contrast to these two systems,our system employs methods that are not comparable, because they use differentmetrics.
Therefore, we need to learn from experience when to use each method.9.
ConclusionDespite its theoretical importance and commercial impact, the generation of e-mail-based help-desk responses has received scant attention to date.
In this article, wehave investigated complementary corpus-based, information-gathering methods forautomatically addressing help-desk requests.
Our results show that a large corpus ofrequest?response e-mail pairs supports the automation of a significant portion of thehelp-desk task with data-driven techniques that reuse responses or parts thereof.
Thesetechniques are particularly suitable for repetitive, non-technical issues, allowing help-desk operators to focus on more challenging, technical requests.Our results also show that different methods are applicable to different situationsthat arise in the help-desk domain, and that the performance of different methods variesfor different data sets.
This suggests that there must be an underlying relationshipbetween methods and features of data sets that needs to be accounted for (Section 4.3).Additionally, our results yield insights regarding the following issues. Retrieval versus prediction ?
Some situations warrant a retrievalapproach, whereas others require a predictive approach.
The formerapplies when the content of a request matches previous cases in thecorpus.
The latter applies when requests and responses do not match oncontent, but correlations exist between a few predictive words in a requestand a response, which is often generic in these cases.
A hybridprediction?retrieval approach was also investigated.
Our hybrid method632Marom and Zukerman Empirical Study of Response Automation Methodsextracts generalizations at the sentence level, and employs a retrievalcomponent that tailors the selection of sentences to the specific issuesraised in a request e-mail.
Although this appears to be a sensible approach,the results of our evaluation are not conclusive, and further investigationis required. Levels of granularity ?
Although simple document-level reuse methodsare sufficient in many cases, more complex sentence-level reuse methods,which involve extractive multi-document summarization, provide a viablealternative when a complete response document cannot be reused.
Thishappens when there is insufficient evidence for the reuse of such adocument, but there is enough evidence for a partial response.We have also performed a small user study, which highlighted issues regarding theevaluation of a large corpus such as ours.
Despite its modest size, our user study wasuseful, as it provided a subjective evaluation of the methods considered, and linked theresults of our automatic evaluation with these subjective assessments.Our work also provides a unified solution to help-desk response automation viathe meta-learning component.
Although our comparative investigation demonstratesthe applicability of the different methods, the meta-learning component provides away to automatically select a method.
We have offered an unsupervised approach thatlearns which is the most promising method based on previous experience.
It does so bylearning the relationship between the value of the confidence (applicability) measure ofeach method and its subsequent performance.
This eliminates the need to set subjectivethresholds on these confidence values, and instead transfers subjective decision-makingto a more intuitive part of the system, namely, the actual performance of the methods,measured by the quality of the generated responses.
In this way, help-desk managerscan decide how strict the system should be, for example, on the precision of responses.We are encouraged by the fact that we have achieved a reasonable level of per-formance using only a simple, low-level bag-of-words representation.
One avenue forfuture research is to investigate more sophisticated representations, such as incorpo-rating word-based similarity metrics into the bag-of-words representation, employingquery expansion during retrieval, and taking into account syntactic features duringretrieval and prediction (recall that we incorporated grammatical and sentence-basedsyntactic features into the Sent-Pred method without significantly affecting perfor-mance, Section 3).Another avenue for future research is the investigation of intermediate levels ofgranularity, such as paragraphs.
Ideally there should be a mechanism that determinesdynamically the most suitable level of granularity for capturing the regularities in acollection of e-mails.
An information-theoretic approach, such as the MML crite-rion (Wallace and Boulton 1968; Wallace 2005), may be a promising way to address thisproblem.AcknowledgmentsThis research was supported in part by grantLP0347470 from the Australian ResearchCouncil and by an endowment fromHewlett-Packard.
The authors also thankHewlett-Packard for the extensiveanonymized help-desk data, NathalieJapkowicz for her advice on themeta-learning portions of this work, and theanonymous reviewers for their insightfulcomments.ReferencesBanerjee, S. and T. Pedersen.
2003.
Thedesign, implementation, and use of633Computational Linguistics Volume 35, Number 4the Ngram Statistics Package.
InCICLing 2003 ?
Proceedings of the FourthInternational Conference on Intelligent TextProcessing and Computational Linguistics,pages 370?381, Mexico City.
Mexico.Barr, A. and S. Tessler.
1995.
Expert systems:A technology before its time.
AI Expert,available at www.stanford.edu/group/scip/avsgt/expertsystems/aiexpert.html.Barzilay, R., N. Elhadad, and K. R. McKeown.2001.
Sentence ordering in multidocumentsummarization.
In HLT01 ?
Proceedings ofthe First Human Language TechnologyConference, pages 1?7, San Diego, CA.Barzilay, R. and K. R. McKeown.
2005.Sentence fusion for multidocument newssummarization.
Computational Linguistics,31(3):297?328.Berger, A., R. Caruana, D. Cohn, D. Freitag,and V. Mittal.
2000.
Bridging the lexicalchasm: Statistical approaches toanswer-finding.
In SIGIR?00 ?
Proceedingsof the 23rd Annual International ACMInternational Conference on Research andDevelopment in Information Retrieval,pages 192?199, Athens.Berger, A. and V. Mittal.
2000.
Query-relevant summarization using FAQs.In ACL2000 ?
Proceedings of the 38thAnnual Meeting of the Association forComputational Linguistics, pages 294?301,Hong Kong.Bickel, S. and T. Scheffer.
2004.
Learningfrom message pairs for automatic emailanswering.
In ECML04 ?
Proceedings of theEuropean Conference on Machine Learning,pages 87?98, Pisa.Burke, R. 2002.
Hybrid recommendersystems.
User Modeling and User-AdaptedInteraction, 12(4):331?370.Carletta, J.
1996.
Assessing agreement onclassification tasks: The Kappa statistic.Computational Linguistics, 22(2):249?254.Carmel, D., M. Shtalhaim, and A. Soffer.2000.
eResponder: Electronic questionresponder.
In CoopIS?02 ?
Proceedingsof the 7th International Conference onCooperative Information Systems,pages 150?161, Eilat.Chang, C. C. and C. J. Lin, 2001.
LIBSVM: ALibrary for Support Vector Machines.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Chu-Carroll, J., K. Czuba, J. M. Prager, andA.
Ittycheriah.
2003.
In questionanswering, two heads are better thanone.
In HLT-NAACL 2003 ?
Proceedingsof the 2003 Language TechnologyConference, pages 24?31, Edmonton.Dalli, A., Y. Xia, and Y. Wilks.
2004.
Adaptiveinformation management: FASiL emailsummarization system.
In COLING?04 ?Proceedings of the 20th InternationalConference on Computational Linguistics,pages 23?27, Geneva.Delic, K. A. and D. Lahaix.
1998.Knowledge harvesting, articulation, anddelivery.
The Hewlett-Packard Journal,May:74?81.Feng, D., E. Shaw, J. Kim, and E. Hovy.2006.
An intelligent discussion-botfor answering student queries inthreaded discussions.
In IUI?06 ?Proceedings of the 11th InternationalConference on Intelligent User Interfaces,pages 171?177, Sydney.Filatova, E. and V. Hatzivassiloglou.
2004.Event-based extractive summarization.In Proceedings of the ACL?04 Workshopon Summarization, pages 104?111,Barcelona.Goldstein, J., V. Mittal, J. Carbonell, andM.
Kantrowitz.
2000.
Multi-documentsummarization by sentence extraction.In Proceedings of the ANLP/NAACL 2000Workshop on Automatic Summarization,pages 40?48, Seattle, WA.Jijkoun, V. and M. de Rijke.
2005.Retrieving answers from frequentlyasked questions pages on the Web.
InCIKM?05 ?
Proceedings of the ACM 14thConference on Information and KnowledgeManagement, pages 76?83, Bremen.Lapalme, G. and L. Kosseim.
2003.
Mercure:Towards an automatic e-mail follow-upsystem.
IEEE Computational IntelligenceBulletin, 2(1):14?18.Lekakos, G. and G. M. Giaglis.
2007.
Ahybrid approach for improvingpredictive accuracy of collaborativefiltering algorithms.
User Modelingand User-Adapted Interaction,17(1):5?40.Leuski, A., R. Patel, D. Traum, andB.
Kennedy.
2006.
Building effectivequestion answering characters.
InProceedings of the 7th SIGdial Workshop onDiscourse and Dialogue, pages 18?27,Sydney.Lin, C. Y. and E. H. Hovy.
2003.
Automaticevaluation of summaries using n-gramco-occurrence statistics.
In HLT-NAACL2003 ?
Proceedings of the 2003 LanguageTechnology Conference, pages 71?78,Edmonton.Malik, R., L. V. Subramaniam, andS.
Kaushik.
2007.
Automaticallyselecting answer templates to respond to634Marom and Zukerman Empirical Study of Response Automation Methodscustomer emails.
In IJCAI?07 ?
Proceedingsof the 20th International Joint Conference onArtificial Intelligence, pages 1659?1664,Hyderabad.Marom, Y. and I. Zukerman.
2006.Automating help-desk responses:A comparative study ofinformation-gathering approaches.In Proceedings of the COLING-ACLWorkshop on Task-Focused Summarizationand Question Answering, pages 40?47,Sydney.Marom, Y. and I. Zukerman.
2007a.Evaluation of a large-scale email responsesystem.
In Proceedings of the IJCAI?07Workshop on Knowledge and Reasoning inPractical Dialogue Systems, pages 28?33,Hyderabad.Marom, Y. and I. Zukerman.
2007b.
Apredictive approach to help-desk responsegeneration.
In IJCAI?07 ?
Proceedings of the20th International Joint Conference onArtificial Intelligence, pages 1665?1670,Hyderabad.Marom, Y., I. Zukerman, and N. Japkowicz.2007.
A meta-learning approach forselecting between response automationstrategies in a help-desk domain.
InAAAI-07 ?
Proceedings of the 22ndConference on Artificial Intelligence,pages 907?912, Vancouver.Molla?, D. and J. L. Vicedo.
2007.
Questionanswering in restricted domains: Anoverview.
Computational Linguistics,33(1):41?61.Oliver, J. J.
1993.
Decision graphs?anextension of decision trees.
In Proceedingsof the 4th International Workshop on ArtificialIntelligence and Statistics, pages 343?350,Fort Lauderdale, FL.Rotaru, M. and D. J. Litman.
2005.Improving question answering forreading comprehension tests bycombining multiple systems.
In Proceedingsof the AAAI 2005 Workshop on QuestionAnswering in Restricted Domains,pages 46?50, Pittsburgh, PA.Roy, S. and L. V. Subramaniam.
2006.Automatic generation of domainmodels for call-centers from noisytranscriptions.
In COLING-ACL?06 ?Proceedings of the 21st InternationalConference on Computational Linguistics and44th Annual Meeting of the Association forComputational Linguistics, pages 737?744,Sydney.Salton, G. and M. J. McGill.
1983.
AnIntroduction to Modern InformationRetrieval.
McGraw Hill, New York.Shrestha, L. and K. R. McKeown.
2004.Detection of question-answer pairs inemail conversations.
In COLING?04 ?Proceedings of the 20th InternationalConference on Computational Linguistics,pages 889?895, Geneva.Soricut, R. and E. Brill.
2006.
Automaticquestion answering using the Web:Beyond the factoid.
Information Retrieval,9(2):191?206.van Rijsbergen, C. J.
1979.
InformationRetrieval.
Buttersworth, London.Vapnik, V. N. 1998.
Statistical Learning Theory.Wiley-Interscience, New York.Wallace, C. S. 2005.
Statistical and InductiveInference by Minimum Message Length.Springer, Berlin.Wallace, C. S. and D. M. Boulton.
1968.
Aninformation measure for classification.
TheComputer Journal, 11(2):185?194.Watson, I.
1997.
Applying Case-BasedReasoning: Techniques for EnterpriseSystems.
Morgan Kaufmann Publishers,San Mateo, CA.Witten, I. H. and E. Frank.
2000.
Data Mining:Practical Machine Learning Tools andTechniques with Java Implementations.Morgan Kaufmann Publishers, SanFrancisco, CA.635
