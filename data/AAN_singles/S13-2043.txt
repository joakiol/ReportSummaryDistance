Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 250?254, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsDAEBAK!
: Peripheral Diversity for Multilingual Word SenseDisambiguationSteve L. ManionUniversity of CanterburyChristchurch, New Zealandsteve.manion@pg.canterbury.ac.nzRaazesh SainudiinUniversity of CanterburyChristchurch, New Zealandr.sainudiin@math.canterbury.ac.nzAbstractWe introduce Peripheral Diversity (PD) as aknowledge-based approach to achieve multi-lingual Word Sense Disambiguation (WSD).PD exploits the frequency and diverse useof word senses in semantic subgraphs de-rived from larger sense inventories such asBabelNet, Wikipedia, and WordNet in orderto achieve WSD.
PD?s f -measure scores forSemEval 2013 Task 12 outperform the MostFrequent Sense (MFS) baseline for two ofthe five languages: English, French, German,Italian, and Spanish.
Despite PD remain-ing under-developed and under-explored, itdemonstrates that it is robust, competitive, andencourages development.1 IntroductionBy reading out aloud ?A minute is a minute divi-sion of time?
(Nelson, 1976), we can easily makethe distinction between the two senses of the homo-graph minute.
For a machine this is a complex taskknown as Word Sense Disambiguation (WSD).
Task12 of SemEval 2013 (Navigli et al 2013) calls for alanguage-independent solution to WSD that utilisesa multilingual sense inventory.Supervised approaches to WSD have dominatedfor some time now (Ma`rquez et al 2007).
Homo-graphs such as minute are effortlessly disambiguatedand more polysemous words such as bar or linecan also be disambiguated with reasonable compe-tence (Agirre and Edmonds, 2007).
However our ap-proach is purely knowledge-based and employs se-mantic graphs.
This allows us to avoid the notoriouspredicament Gale et al(1992) name the informationbottleneck, in which supervised approaches fail to beportable across alternative languages and domainsif the annotated corpora do not exist.
Conversely,knowledge-based approaches for WSD are usuallyapplicable to all words in unrestricted text (Mihal-cea, 2007).
It is this innate scalability that moti-vates us to pursue knowledge-based approaches.
Re-gardless of whether sense inventories can maintainknowledge-richness as they grow, their continued re-finement by contributors is directly beneficial.Knowledge-based approaches that employ se-mantic graphs increasingly rival leading supervisedapproaches to WSD.
They can beat a Random orLESK (Lesk, 1986) baseline (see Mihalcea (2005),Navigli and Lapata (2007), Sinha and Mihalcea(2007), Navigli and Lapata (2010)) and can com-pete with or even beat the Most Frequent Sense(MFS) baseline in certain contexts which is by nomeans an easy task (see Navigli et al(2007), EnekoAgirre and Aitor Soroa (2009), Navigli and Ponzetto(2012a)).2 MethodologyPD is a framework for knowledge-based WSD ap-proaches that employ semantic graphs.
However be-fore we can elaborate we must first cover the funda-mental resources it is built upon.2.1 Fundamental Resource Definitions2.1.1 Lemma SequencesAt a glance across the text of any language, we ab-sorb meaning and new information through its lexi-cal composition.
Depending on the length of text250we are reading, we could interpret it as one of manystructural subsequences of writing such as a para-graph, excerpt, quote, verse, sentence, among manyothers.
LetW = (wa, ..., wb) be this subsequence ofwords, which we will utilise as a sliding window forPD.
Again let W = (w1, ..., wm) be the larger bodyof text of length m, such as a book, newspaper, orcorpus of text, that our sliding window of length b?amoves through.In SemEval Task 12 on Multilingual Word SenseDisambiguation all words are lemmatised, which isthe process of unifying the different inflected formsof a word so they can be analysed as a consolidatedlemma (or headword).
Therefore words (or lexemes)such as runs and ran are all mapped to their unifyinglemma run1.To express this, let `w : W ?
L be a many-to-one mapping from the sequence of words W tothe sequence of lemmas L, in which (wa, ..., wb) 7?
(`wa , ..., `wb) = (`a, ..., `b).
To give an examplefrom the test data set2, the word sequenceW = (And,it, ?s, nothing, that, runs, afoul, of, ethics, rules,.)
maps to the lemma sequence L = (and, it, be,nothing, that, run, afoul, of, ethic, rule, .).
In or-der to complete this SemEval task we disambiguatea large sequence of lemmas L = (`1, ..., `m), via ourlemma-based sliding window L = (`a, ..., `b).2.1.2 SynsetsEach lemma `i ?
L may refer up to k senses inS(`i) = {si,1, si,2, ..., si,k} = S .
Furthermore eachsense si,j ?
S maps to a set of unique concepts inthe human lexicon.
To clarify let us consider oneof the earliest examples of modern ambiguity takenfrom Bar-Hillel?s (1960) critique of Machine Trans-lation: W = (The, box, was, in, the, pen, .).
Thesense of pen could be either a) a certain writing uten-sil or b) an enclosure where small children can play,therefore {senclosure, sutensil} ?
S(`pen) = S. Humanscan easily resolve the ambiguity between the pos-sible senses of pen by accessing their own internallexicon and knowledge of the world they have builtup over time.In the same vein, when accessing sense invento-ries such as BabelNet, WordNet (Fellbaum, 1998),1While all words are lemmatised, this task strictly focuseson the WSD of noun phrases.2This is sentence d010.s014 in the English test data set.and Wikipedia which are discrete representations ofthe human lexicon, we refer to each sense si,j ?
Sas a synset.
Depending on the sense inventory thesynset belongs to, it may contain alternative or trans-lated lexicalisations, glosses, links to other semanticresources, among a collection of semantically de-fined relations to other synsets.2.1.3 SubgraphsPD makes use of subgraphs derived from a di-rected graph G = (V, E) that can be crafted froma sense inventory, such as BabelNet, WordNet, orWikipedia.
We construct subgraphs using the Babel-Net API which accesses BabelNet3 and Babel synsetpaths4 indexed into Apache Lucene5 to ensure speedof subgraph construction.
This process is describedin Navigli and Ponzetto (2012a) and demonstratedin Navigli and Ponzetto (2012b).
Our formalisationof subgraphs is adapted into our own notation fromthe original papers of Navigli and Lapata (2007) andNavigli and Lapata (2010).
We refer the reader tothese listed sources if they desire an extensive ex-planation of our subgraph construction as we havebuilt PD on top of the same code base therefore wedo not deviate from it.For a given lemma sequence L = (`i, ..., `n) anddirected graph G = (V, E) we construct our sub-graph GL = (VL, EL) in two steps:1.
Initialize VL :=?ni=1 S(`i) and EL := ?.2.
For each node v ?
VL, we perform a depth-first search (DFS) of G, such that, every timewe encounter a node v?
?
VL (v?
6= v) along apath v, v1, ..., vk, v?
of length ?
L in G, we addall intermediate nodes and edges on the pathfrom v to v?, i.e., VL := VL ?
{v1, ..., vk} andEL := EL ?
{{v, v1}, ..., {vk, v?
}}.2.2 Interpretation of ProblemFor the lemmatisation of any word wi 7?
`i :wi ?
W, `i ?
L, we must estimate the most ap-propriate synset si,?
?
S(`i) = {si,1, si,2, ..., si,k}.Our system associates a PD score ?
(si,j) for each3BabelNet 1.1.1 API & Sense Inventory - http://lcl.uniroma1.it/babelnet/download.jsp4BabelNet 1.0.1 Paths - http://lcl.uniroma1.it/babelnet/data/babelnet_paths.tar.bz25Apache Lucene - http://lucene.apache.org251si,j ?
S(`i) by taking GL as input.
We estimatesi,?, the most appropriate sense for `i, by s?i,?
=argmaxsi,j?S(`i) ?(si,j).
It?s worth noting here thatGL ensures the estimation of s?i,?
is not an indepen-dent scoring rule, since GL embodies the context sur-rounding `i via our sliding lemma-based window L.2.3 Peripheral Diversity FrameworkPD is built on the following two ideas that are ex-plained in the following subsections:1.
For a subgraph derived from one lone lemma`i, in which no other lemmas can provide con-text, the synset si,j ?
G`i that has the largestand most semantically diverse set of peripheralsynset nodes is assumed to be the MFS for `i.2.
For a larger subgraph derived from a slidinglemma window L, in which other lemmas canprovide context, the synset si,j ?
GL that ob-serves the largest increase in size and semanticdiversity of its peripheral synset nodes is esti-mated to be si,?, the most appropriate synset forlemma `i.Therefore PD is merely a framework that exploitsthese two assumptions.
Now we will go through theprocess of estimating si,?
for a given lemma `i.2.3.1 Pairwise Semantic DissimilarityFirst, for each synset si,j ?
S, we need to acquirea set of its peripheral synsets.
We do this by travel-ling a depth of up to d (stopping if the path ends),then adding the synset we reach to our set of periph-eral synsets P?d = {sj,1, sj,2, ..., sj,k?
}.Next for every pair of synsets v and v?
that arenot direct neighbours in P?d such that v 6= v?,we calculate their Pairwise Semantic Dissimilarity(PSD) ?
(v, v?)
which we require for a synset?sPD score.
To generate our results for this task wehave used the complement to Cosine Similarity,commonly known as the Cosine Distance as ourPSD measure:?
(v, v?)
=???1?(|O(v)?O(v?)|?|O(v)|?|O(v?
)|), if |O(v)||O(v?
)| 6= 01, otherwise,where O(v) is the outgoing (out-neighbouring)synsets for v ?
P?d, and |O(v)| denotes the numberof elements in O(v).2.3.2 Peripheral Diversity ScoreOnce we have PSD scores for every permittedpairing of v and v?, we have a number of ways togenerate our ?
(si,j) values.
To generate our resultsfor this task, we chose to score synsets on the sumof their minimum PSD values, which is expressedformally below:?
(si,j) =?v?P?d(si,j)minv?
6=vv??P?d(si,j)?
(v, v?
)The idea is that this summing over the peripheralsynsets in P?d(si,j) accounts for how frequentlysynset si,j is used, then each increment in size isweighted by a peripheral synset?s minimumal PSDacross all synsets in P?d(si,j).
Therefore periph-eral set size and semantic diversity are rewardedsimultaneously by ?.
To conclude, the final esti-mated synset sequence for a given lemma sequence(`1, ..., `m) based on ?
is (s?1,?, s?2,?, ..., s?m,?
).2.3.3 Strategies, Parameters, & FiltersWikipedia?s Did You Mean?
We account for de-viations and errors in spelling to ensure lemmashave the best chance of being mapped to a synset.Absent synsets in subgraph GL will naturally de-grade system output.
Therefore if `i 7?
?,we make an HTTP call to Wikipedia?s Did youmean?
and parse the response for any alternativespellings.
For example in the test data set6 themisspelt lemma: ?feu de la rampe?
is corrected to?feux de la rampe?.Custom Back-off Strategy As back-off strate-gies7 have proved useful in (Navigli and Ponzetto,2012a) and (Navigli et al 2007), we designed ourown back-off strategy.
In the event our system pro-vides a null result, the Babel synset si,j ?
S(`i) =S with the most senses associated with it will bechosen with preference to its region in BabelNetsuch that WIKIWN WN WIKI.6Found in sentence d001.s002.t005 in the French testdata set.7In the event the WSD technique fails to provide an answer,a back-off strategy provides one for the system to output.252Input Parameters We set our sliding windowlength (b?
a) to encompass 5 sentences at a time, inwhich the step size is also 5 sentences.
For subgraphconstruction the maximum lengthL = 3.
Finally weset our peripheral search depth d = 3.Filters For the purposes of reproducibility onlywe briefly mention two filters we apply to our sub-graphs that ship with the BabelNet API.
We re-move WordNet contributed domain relations withthe ILLEGAL POINTERS filter and apply theSENSE SHIFTS filter.
For more information onthese filters we suggest the reader consult the Ba-belNet API documentation.3 Results & Discussion3.1 Results of SemEval SubmissionLanguage DAEBAK!
MFSBaseline +/-DE German 59.10 68.60 -9.50EN English 60.40 65.60 -5.20ES Spanish 60.00 64.40 -4.40FR French 53.80 50.10 +3.70IT Italian 61.30 57.20 +4.10Mean 58.92 61.18 -2.26Table 1: DAEBAK!
vs MFS Baseline on BabelNetAs can be seen in Table 1, the results of our singlesubmission were varied and competitive.
The worstresult was for German in which our system fell be-hind the MFS baseline by a margin of 9.50.
Againfor French and Italian we exceeded the MFS base-line by a margin of 3.70 and 4.10 respectively.
OurDaebak back-off strategy contributed anywhere be-tween 1.12% (for French) to 2.70% (for Spanish) inour results, which means our system outputs a re-sult without the need for a back-off strategy at least97.30% of the time.
Overall our system was slightlyoutperformed by the MFS baseline by a margin of2.26.
Overall PD demonstrated to be robust acrossa range of European languages.
With these prelimi-nary results this surely warrants further investigationof what can be achieved with PD.3.2 Exploratory ResultsThe authors observed some inconsistencies in thetask answer keys across different languages as Ta-ble 2 illustrates.
For each Babel synset ID found inthe answer key, we record where its original sourcesynsets are from, be it Wikipedia (WIKI), WordNet(WN), or both (WIKIWN).Language WIKI WN WIKIWNDE German 43.42% 5.02% 51.55%EN English 10.36% 32.11% 57.53%ES Spanish 30.65% 5.40% 63.94%FR French 40.81% 6.55% 52.64%IT Italian 38.80% 7.33% 53.87%Table 2: BabelNet Answer Key BreakdownThis is not a critical observation but rather anempirical enlightenment on the varied mechanicsof different languages and the amount of devel-opment/translation effort that has gone into thecontributing subparts of BabelNet: Wikipedia andWordNet.
The heterogeneity of hybrid sense inven-tories such as BabelNet creates new obstacles forWSD, as seen in (Medelyan et al 2013) it is diffi-cult to create a disambiguation policy in this context.Future work we would like to undertake would be toinvestigate the heterogenous nature of BabelNet andhow this affects various WSD methods.4 Conclusion & Future DirectionsTo conclude PD has demonstrated in its early stagesthat it can perform well and even outperform theMFS baselines in certain experimental contexts.Furthermore it leaves a lot left to be explored interms of what this approach is capable of via adjust-ing subgraph filters, strategies, and input parametersacross both heterogenous and homogenous semanticgraphs.AcknowledgmentsThis research was completed with the help of theKorean Foundation Graduate Studies Fellowship8.5 ResourcesThe code base for this work can be found in the nearfuture at http://www.stevemanion.com/.8KF Graduate Studies Fellowship - http://www.kf.or.kr/eng/01_sks/sks_fel_sfb01.asp253ReferencesEneko Agirre and Philip Edmonds.
2007.
Introduction.Word Sense Disambiguation Algorithms and Applica-tions, Chapter 1:1-28.
Springer, New York.Eneko Agirre and Aitor Soroa.
2009.
Personaliz-ing PageRank for Word Sense Disambiguation.
InProceedings of the 12th Conference of the EuropeanChapter of the ACL, April:33?41.
Association forComputational Linguistics.Yehoshua Bar-Hillel.
1960.
The Present Status of Au-tomatic Translation of Languages.
Advances in Com-puters, 1:91?163.Christiane Fellbaum.
1998, ed.
WordNet: An ElectronicLexical Database., Cambridge, MA: MIT Press.William A Gale, Kenneth W Church, David Yarowsky.1992.
A Method for Disambiguating Word Senses in aLarge Corpus.
Computers and the Humanities, 26(5?6):415?439.Michael Lesk.
1986.
Automatic Sense DisambiguationUsing Machine Readable Dictionaries: How to Tella Pine Cone from an Ice Cream Cone.
Proceedingsof the 5th Annual International Conference on SystemDocumentation., 24?26.
ACM.Llus Ma`rquez, Gerard Escudero, David Mart?
?nez, Ger-man Rigau.
2007.
Supervised Corpus-Based Meth-ods for WSD.
Word Sense Disambiguation Algorithmsand Applications, Chapter 7:167-216.
Springer, NewYork.Rada Mihalcea.
2005.
Unsupervised Large-VocabularyWord Sense Disambiguation with Graph-based Algo-rithms for Sequence Data Labeling.
Proceedings ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Processing,411-418.
Association for Computational Linguistics.Rada Mihalcea.
2007.
Knowledge-Based Methodsfor WSD.
Word Sense Disambiguation Algorithmsand Applications, Chapter 5:107?131.
Springer, NewYork.Alyona Medelyan, Steve Manion, Jeen Broekstra, AnnaDivoli, Anna-lan Huang, and Ian H Witten.
2013.Constructing a Focused Taxonomy from a DocumentCollection Extended Semantic Web Conference, (Ac-cepted, in press)Roberto Navigli and Mirella Lapata.
2007.
Graph con-nectivity measures for unsupervised word sense dis-ambiguation.
IJCAI?07 Proceedings of the 20th In-ternational Joint Conference on Artifical Intelligence,1683?1688.Roberto Navigli, Kenneth C Litkowski, and Orin Har-graves.
2007.
SemEval-2007 Task 07: Coarse-Grained English All-Words Task.
In Proceedings ofthe 4th International Workshop on Semantic Evalua-tions, 30?35.Roberto Navigli and Mirella Lapata.
2010.
An Experi-mental Study of Graph Connectivity for UnsupervisedWord Sense Disambiguation.
IEEE transactions onpattern analysis and machine intelligence, 32(4):678?692.Roberto Navigli and Simone Paolo Ponzetto.
2012.
Ba-belNet: The automatic construction, evaluation andapplication of a wide-coverage multilingual semanticnetwork.
Artificial Intelligence, 193:217?250.Roberto Navigli and Simone Paolo Ponzetto.
2012.
Mul-tilingual WSD with Just a Few Lines of Code: the Ba-belNet API.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguistics,67?72.Roberto Navigli, David Jurgens, and Daniele Vannella.2013.
SemEval-2013 Task 12: Multilingual WordSense Disambiguation.
Proceedings of the 7th Inter-national Workshop on Semantic Evaluation (SemEval2013), in conjunction with the Second Joint Confer-ence on Lexical and Computational Semantcis (*SEM2013).Frederic Nelson.
1976.
Homographs American Speech,51(3):296?297.Ravi Sinha and Rada Mihalcea.
2007.
UnsupervisedGraph-based Word Sense Disambiguation Using Mea-sures of Word Semantic Similarity.
Proceedings ofIEEE International Conference on Semantic Comput-ing.254
