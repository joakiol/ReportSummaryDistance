Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 62?70,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsA Multimodal Vocabulary for Augmentative and Alternative Communi-cation from Sound/Image Label DatasetsXiaojuan Ma Christiane Fellbaum Perry R. CookPrinceton University35 Olden St. Princeton, NJ 08544, USA{xm,fellbaum,prc}@princeton.eduAbstractExisting Augmentative and Alternative Com-munication vocabularies assign multimodalstimuli to words with multiple meanings.
Theambiguity hampers the vocabulary effective-ness when used by people with language dis-abilities.
For example, the noun ?a missingletter?
may refer to a character or a writtenmessage, and each corresponds to a differentpicture.
A vocabulary with images and soundsunambiguously linked to words can bettereliminate misunderstanding and assist com-munication for people with language disorders.We explore a new approach of creating such avocabulary via automatically assigning se-mantically unambiguous groups of synonymsto sound and image labels.
We propose an un-supervised word sense disambiguation (WSD)voting algorithm, which combines differentsemantic relatedness measures.
Our voting al-gorithm achieved over 80% accuracy with asound label dataset, which significantly out-performs WSD with individual measures.
Wealso explore the use of human judgments ofevocation between members of concept pairs,in the label disambiguation task.
Results showthat evocation achieves similar performance tomost of the existing relatedness measures.1 IntroductionIn natural languages, a word form may refer to dif-ferent meanings.
For instance, the word ?fly?means ?travel through the air?
in context like ?flyto New York,?
while it refers to an insect in thephrase ?a fly on the trashcan.?
Speakers determinethe appropriate sense of a polysemous word basedon the context.
However, people with languagedisorders and access/retrieval problems, may havegreat difficulty in understanding words individual-ly or in a context.
To overcome such language bar-riers, visual and auditory representations are intro-duced to help illustrate concepts (Ma et al,2009a)(Ma et al, 2010).
For example, a personwith a language disability can tell the word ?fly?refers to ?travel through the air?
when he sees aplane in the image (rather than an insect); likewisehe can distinguish the meaning of ?fly?
given theplane engine sound vs. the insect buzzing sound.This approach has been employed in Augmentativeand Alternative Communication (AAC), in theform of multimodal vocabularies in assistive de-vices (Steele et al 1989)(Lingraphica, 2010).However, current AAC vocabularies assign vis-ual stimuli to words instead of specific meanings,and thus bring in ambiguity when a user with lan-guage disability tries to comprehend and commu-nicate a concept.
For example, for the word ?fly,?Lingraphica only has an icon showing a plane anda flock of birds flying.
Confusion arises when asentence like ?I want to kill the fly (the insect)?
isexplained using the airplane/bird icon.
Similarly, itwill lead to miscommunication if the sound of keysjingling is used to express ?a key is missing?
whenthe person intends to refer to a key on the keyboard.People with language impairment are relying onthe AAC vocabularies for language access, and anyambiguity may result in communication failure.To address this problem, we propose building asemantic multimodal AAC vocabulary with visualand auditory representations expressing conceptsrather than words (Figure 1), as the backbone ofthe language assistant system for people withaphasia (Ma et al 2009b).
Our work is exploratorywith the following innovations: 1) we target theinsufficiency of current assistive vocabularies byresolving ambiguity; 2) we enrich concept invento-ry and connect concepts through language, envi-ronmental sounds, and images (little research haslooked into conveying concepts through naturalnonspeech sounds); and 3) our vocabulary has adynamic scalable semantic network structure rather62than simply grouping words into categories asconventional assistive devices do.One intuitive way to build a disambiguated mul-timodal vocabulary is to manually assign meaningsto each word in the existing vocabulary.
However,the task is time consuming with poor scalability ?no new multimedia representations are generatedfor concepts that are missing in the vocabulary.ImageNet (Jia et al, 2009) was constructed bypeople verifying the assignment of web images togiven synonym sets (synsets).
ImageNet has overnine million images linked to about 15 thousandsnoun synsets in WordNet (Fellbaum, 1998).
De-spite the huge human effort, ImageNet, with thegoal of creating a computer vision database, doesnot yet include all the most commonly used wordsacross different parts of speech.
It is not yet suita-ble for a language support application.We explore a new approach for generating a vo-cabulary with concept to sound/image associations,that is, conducting word sense disambiguation(WSD) techniques used in Natural LanguageProcessing on sound/image label datasets.
For ex-ample, the labels ?car, drive, fast?
for the sound?car ?
passing.wav?
are assigned to synsets ?car: amotor vehicle,?
?drive: operate or control a ve-hicle,?
and ?fast: quickly or rapidly?
via WSD.
Itmeans the sound ?car ?
passing.wav?
can be usedto depict those concepts.
This approach is viablebecause the words in the sound/image labels wereshown to evoke one another based on the audito-ry/visual content, and their meanings can be identi-fied by considering all the tags generated for agiven sound or image as a context.
With the avail-ability of large sound/image label datasets, the vo-cabulary created from WSD can be easilyexpanded.A variety of WSD methods (e.g.
knowledge-based methods (Lesk, 1986), unsupervised me-thods (Lin, 1997), semi-supervised methods(Hearst, 1991) (Yarowsky, 1995), and supervisedmethods (Novischi et al, 2007)) were developedand evaluated with corpus data and other text doc-uments like webpages.
Compared to the text datathat WSD methods work with, labels for soundsand images have unique characteristics.
The labelsare a bag of words related to the visual/auditorycontent; there is no syntactic or part of speech in-formation, nor are the words necessarily contextualneighbors.
For example, contexts suggest land-scape senses for the word pair ?bank?
and ?water?,whereas in an image, a person may drink waterinside a bank building.
Furthermore, few annotatedimage or sound label datasets are available, makingit hard to apply supervised or semi-supervisedWSD methods.To efficiently and effectively create a disambi-guated multimodal vocabulary, we need to achievetwo goals.
First, optimize the accuracy of the WSDalgorithm to minimize the work required for ma-nual checking and correction afterwards.
Second,construct a semantic network across different partsof speech, and thus explore linking semantic rela-tedness measures that can capture aspects differentfrom existing ones.
In this paper, we target the firstgoal by proposing an unsupervised sense disam-Figure 1.
Disambiguated AAC multimedia vocabulary; dash arrows are semantic relations between concepts.63biguation algorithm combining a variety of seman-tic relatedness measures.
We chose an unsuper-vised method because of the lack of a largemanually annotated gold standard.
The measure-combined voting algorithm presented here drawsadvantages from different semantic relatednessmeasures and has them vote for the best-fittingsense to assign to a label.
Evaluation shows thatthe voting algorithm significantly exceeds WSDwith each individual measure.To approach the second goal, we proposed andtested a semantic relatedness measure called evo-cation (Boyd-Graber et al, 2006) in disambigua-tion of sound/image labels.
Evocation measureshuman judgements of relatedness between a di-rected concepts pair.
It provides cross parts ofspeech evocativeness information which supple-ments most of the knowledge-based semantic rela-tedness measures.
Evaluation results showed thatthe performance of WSD with evocation is noworse than most of the relatedness measures thatwe applied, despite the relatively small size of thecurrent evocation dataset.2 Dataset: Semantic Labels for Environ-mental Sounds and ImagesOur ultimate goal is to create an AAC vocabularyof associations between environmental sounds andimages and groups of synonymous words that arerelevant to the content.
We are working with twodatasets of human labels for multimedia data,SoundNet and the Peekaboom dataset.2.1 SoundNet Sound Label DatasetThe SoundNet Dataset (Ma, Fellbaum, and Cook,2009) consists of 327 environmental ?soundnails?
(5-second audio clips) each with semantic labelscollected from participants via a large scale Ama-zon Mechanical Turk (AMT) study.
The sound-nails cover a wide range of auditory scenes, fromvehicle (e.g.
car starting), mechanical tools (e.g.handsaw) and electrical devices (e.g.
TV), to natu-ral phenomena (e.g.
rain), animals (e.g.
a dog bark-ing), and human sounds (e.g.
a baby crying).
In theAMT study, participants were asked to generatetags for each soundnail labeling its source, possiblelocation, and actions involved in making the sound.Each soundnail was labeled by over 100 people.The tags were clustered into meaning units thatSoundNet refers to as ?sense sets.?
A sense set in-cludes a set of words with similar meanings.
Forinstance, for the soundnail pre-labeled ?bag, zipO-pen?
which is the sound of opening the zipper of abag, the following sense sets were generated:(a) ?zipper?
{zipper, zip up, zip, unzip};(b) ?bag?
{bag, duffle bag, nylon bag, suitcase,luggage, backpack, purse, pack, briefcase};(c) ?house?
{house, home, building}, and(d) ?clothes?
{clothes, jacket, coat, pants, jeans,dress, garment}.The word in bold is was judged by SoundNet tobe the best representative of the sense set, and oth-er words, possibly belonging to different parts ofspeech are included in the curly brackets enclosingthe sense sets.
SoundNet uses sense sets rather thansingle words because 1) people may use differentwords to describe the same underlying concept,(e.g.
?baby?
and ?infant;?
?rain?
as a noun and as averb); 2) people cannot draw fine distinctions be-tween objects and events that generate similarsounds, and thus may come up with different butrelated categories (e.g.
?plate,?
?cup,?
and ?bowl?for the dish clinking sound); and 3) people mayperceive objects and events that are not explicitlypresented in the sound very differently (e.g.
?bag?vs.
?clothes?
for the sound made by a zipper).
Inthis experiment, only sense sets (labels) that weregenerated by at least 25% of the labelers wereused.In our disambiguation experiment, two kinds ofcontexts were explored.
In the Context 1 scheme,each label is treated separately: all its membersplus the representatives of the other sense sets areconsidered.
Take the soundnail ?bag, zipOpen?
asan example.
The context for disambiguating label(a) ?zipper?
{zipper, zip up, zip, unzip} is:zipper, zip up, zip, unzip, bag, house, clothes.The context for label (d) ?clothes?
{clothes, jacket,coat, pants, jeans, dress, garment} is:clothes, jacket, coat, pants, jeans, dress, garment,zipper, bag, house.In the Context 1 scheme, all representativewords will be disambiguated multiple times.
Thefinal result will be the synset that gets the mostvotes.
In the Context 2 scheme, as for the imagedataset described below, all members from eachsense set are put together to create the context, andeach word is disambiguated only once.2.2 Peekaboom Image Label Dataset64The ESP Game Dataset (Von Ahn and Dabbish,2004) contains a large number of web images andhuman labels produced via an online game.
Forexample, an image of a glass of hard liquor is la-beled ?full, shot, alcohol, clear, drink, glass, beve-rage.?
The Peekaboom Game (Von Ahn et al,2006) is the successor of the ESP Game.
In ourexperiment, part of the Peekaboom Dataset (3,086images) was used.
For each image, all the labelstogether form the context for sense disambigua-tion.The Peekaboom labels are noisier than theSoundNet labels for several reasons.
First, randomobjects may appear in a picture and thus be in-cluded in the labels.
For example, an image is la-beled ?computer, shark?
because there is a sharkpicture on the computer screen.
Second, texts inthe images are often included in the labels.
Forexample, the word ?green?
is one of the labels foran image with a street sign ?Green St.?
Third, thePeekaboom labels are not stemmed, which addsanother layer of ambiguity.
For example, the labels?bridge, building?
could refer to a building eventor to a built entity.
In the experiment, all labels foran image are used in their unstemmed form to con-struct the context for WSD.3 Evocation and Other Semantic Related-ness MeasuresA set of measures were selected to assess the rela-tedness between possible senses of words in thesound/image labels.
Apart from existing methods,an additional measure, evocation, is introduced.3.1 EvocationEvocation (Boyd-Graber et al, 2006) measuresconcept similarity based on human judgment.
It isa directed measure, with evocation(synset A, syn-set B) defined as how much synset A brings tomind synset B.
The evocation dataset has been ex-tended to scores for 100,000 directed synset pairs(Nikolova et al, 2009).The evocation data were collected independentlyof WordNet or corpus data.
We propose the use ofevocation in WSD for image and sound labels forthe following reasons.
First, the sound and imagelabels are generated based on human perception ofthe content and common knowledge.
In SoundNetin particular, many of the evoked labels reflectedthe most obvious objects or events in a soundscene.
For example, ?bag?
and ?coat?
were evokedfrom the zipper soundnail.
In this case, the evoca-tion score may be a good evaluation of the related-ness between the labels.
Second, evocationassesses relatedness of concepts across differentparts of speech, which is suitable for identifyingimage and sound labels containing nouns, verbs,adjectives, adverbs, etc.This paper is a first attempt to compare the ef-fectiveness of the use of evocation measure insense disambiguation to the conventional, relative-ly better tested similarity measures, in the contextof assigning synsets to sound/image labels.
Consi-dering that the evocation dataset is small in sizeand susceptible to noise given the method bywhich it was collected, we have not yet incorpo-rated evocation into the measure-combined votingalgorithm described in the Section 4.3.2 Semantic Relatedness MeasuresNine measures of semantic relatedness1  betweensynsets are used in the experiment, both as contri-butors to the voting algorithm and as baselines forcomparison, including:1) WordNet path based measures.?
?path?
?
shortest path length between syn-sets,  inversely proportional to the numberof nodes on the path.?
?wup?
(Wu and Palmer, 1994) ?
ratio of thedepth of the Least Common Subsumer(LCS) to the depths of two synsets in theWordnet taxonomy.?
?lch?
(Leacock and Chodorow, 1998) ?considering the length of the shortest pathbetween two synsets to the depth of theWordNet taxonomy.2) Information and content based measures.?
?res?
(Resnik, 1995) ?
the informationalcontent (IC) of a given corpus of the LCSbetween two synsets.?
?lin?
(Lin, 1997) ?
the ratio of the IC of theLCS to the IC of the two synsets.?
?jcn?
(Jiang and Conrath, 1997) ?
inverselyproportional to the difference between theIC of the two synsets and the IC of the LCS.1 ?hso?
(Hirst and St-Onge, 1998) extensively slows down theWSD process with over five context words, and thus, is notincluded in the experiment.653) WordNet definition based measures.?
?lesk?
(Banerjee and Pedersen, 2002) ?overlaps in the definitions of two synsets.?
?vector?
(Patwardhan and Pedersen, 2006)?
cosine of the angle between the co-occurrence vector computed from the defi-nitions around the two synsets.?
?vector_pairs?
?
co-occurrence vectors arecomputed from definition pairs separately.The computation of the relatedness scores usingmeasures listed above were carried out by codesfrom the WordNet::Similarity (Pedersen et al,2004) and WordNet::SenseRelate projects (Peder-sen and Kolhatkar, 2009).
In contrast to Word-Net::SenseRelated, which employs only onesimilarity measure in the WSD process, this paperproposes a strategy of having several semantic re-latedness measures vote for the best synset for eachword.
The voting algorithm intends to improveWSD performance by combining conclusions fromvarious measures to eliminate a false result.
Sincethere is no syntax among the words generated for asound/image, they should all be considered forWSD.
Thus, the width of the context window is thetotal number of words in the context.4 Label Sense Disambiguation AlgorithmFigure 2 shows the overall process of the measure-combined voting algorithm for disambiguatingsound/image labels.
After the context for WSD isgenerated, the process is divided into two steps.
InStep I, the relatedness scores of each sense of aword based on the context is computed by eachmeasure separately.
Step II combines results fromall measures and generates the disambiguated syn-sets for all words in the sound/image labels.
Evo-cation did not participate in Step II.4.1 Step I: Generate Candidate Synsets Basedon Individual MeasuresGiven the context of M words (w1, ?, wM), and Krelatedness measures (k = 1, ?, K), the task is toassign each word wj (j = 1, ?, M) to the synsetsx,wj that is the most appropriate within the context.Here, the word wj has Nj synsets, denoted as sn,wj (n= 1, ?, Nj).
Step I is to calculate the relatednessscore for each synset of each word in the context., , ,1,...,1,...,( ) max ( ( , ))j j mmm jk i w k i w n wn Nm Mscore s measure s s?=== ?The evocation score between two sysnets sa, sb isthe maximum of the directed evocation ratings.
( , ) max( ( , ), ( , ))a b a b b aevocationscore s s evocation s s evocation s s=, , ,1,...,1,...,( ) max ( ( , ))j j mmm ji w i w n wevocation n N evocationm Mscore s score s s?=== ?The synset that evocation assigns to word j is theone with the highest score., ,j jw x ws s if==, ,1,...,( ) max ( ( ))j jjx w i wevocation i N evocationscore s score s==4.2 Step II: Vote for the Best CandidateThree voting schemes were tested, including un-weighted simple votes, weighted votes among topcandidates, and weighted votes among all synsets.1) Unweighted Simple VotesSynset sn,wj of word wj gets a vote from related-ness measure k if its scorek is the maximum amongall the synsets for wj, and it becomes the candidatesynset for wj elected by measure k (Ck,wj):, ,1,...,,1, ( ) max ( ( ))( )0,j jjjk x w k i wi Nk x wif score s score svote selse==?
?= ??
?, ,( ) , ( ) 1j j jk w x w k x wcandidate s s if vote s= =The candidate list for word wj (candidates(Swj))is the union of all candidate synsets elected by in-dividual relatedness measures.1,...,( ) ( ( ))j jw k wk Kcandidates s union candidate s==For each candidate in the list, the votes from allmeasures are calculated.
The one receiving themost votes becomes the proposed synset for wj., ,1( ) ( )j jKi w k i wkvoteCount s vote s==?Figure 2.
Measure-Combined Voting Algorithm.66,,, ,( ),( ) max ( ( ))j jj ji w wj jw x wx w i ws candidates ss s ifvoteCount s voteCount s?==2) Weighted Votes among Top CandidatesThe weighted voting scheme avoids a situationwhere the false results win by a very small margin.The weight under relatedness measure k for si,wj iscalculated as the relative score to the maximumscorek among all synsets for word wj.
It suggestshow big of a difference in relatedness score of anygiven synset is to the highest score among all thepossible synsets for the target word., , ,1,...,( ) ( ) / max ( ( ))j j jjk x w k x w k i wi Nweight s score s score s==The weighted votes synset si,wj receives over allmeasures is the sum of its weight under individualmeasure.
In voting scheme 2, the synset from thecandidate list which gets the highest weightedvotes becomes the winner., ,1( ) ( )j jKi w k i wkweightedVote s weight s==?,,, ,( ),( ) max ( ( ))j jj ji w wj jw x wx w i ws candidates ss s ifweightedVote s weightedVote s?==3) Weighted Votes among All SynsetsVoting scheme 3 differs from 2 in that the synsetfrom all synsets for word wj which gets the highestweighted votes is the proposed synset for wj.,, ,1,...,,( ) max ( ( ))j jj jjw x wx w i wi Ns s ifweightedVote s weightedVote s===5 EvaluationThe evaluation of WSD with evocation and themeasure-combined voting algorithm was carriedout primarily on the SoundNet label dataset be-cause of the availability of ground truth data.SoundNet provides manual annotation for 1,553different words for 327 soundnails (e.g.
the word?road?
appears in 41 sounds).The accuracy rate (precision) was computed foreach WSD method.
The sound level accuracy of aWSDk is the average percentage of correct senseassignments over the 327 sounds.
The word levelaccuracy is the mean over 1553 distinctive words.Accuracy rates of different measures at both levelaccepted the null hypothesis in homogeneity test.327115531( ) ( (% ) ) / 327( ) ( (% ) ) /1553k isound level ik wword level waccuracy WSD correctnessaccuracy WSD correctness?
=?
===?
?Due to the lack of ground truth in the Peekaboomdataset, we only computed the overlap between theWSD result of 3,086 images from the voting algo-rithm, evocation and each relatedness measures.5.1 Overall Comparison across WSD me-thods with Various Relatedness MeasuresFigures 3 show the overall comparison among dif-ferent methods at both sound level and word level.It suggests that the performance of the evocationmeasure in sense disambiguation is as good as thepath-based and context-based measures.
The defi-nition-based measures (?lesk?
and ?vector?)
aresignificantly better than other measures if used in-dividually (similar to (Patwardhan et al2003)).Figure 3.
Accuracy rate at word and sound level in comparison among evocation, voting, and nine individualsense similarity measures.67However, the voting algorithms proposed in thiswork significantly outperformed each individualmeasure based on ANOVA results.
At sound level,Context 1: (F(12, 20176) = 102.92, p < 0.001);Context 2: (F(12, 4238) = 89.42, p < 0.001).
Atword level, Context 1: (F(12, 20176) = 68.78, p <0.001); Context 2: (F(12, 4238) = 60.72, p < 0.001).The scheme of composing context (Section 2.1)has significant impact on the accuracy, with Con-text 1 (taking all members in the related sense setand representatives from the others) outperformingContext 2 (taking all words in all sense sets) at theword level (F(1, 40352) = 20.19, p < 0.001).
Theinfluence of context scheme is not significant at thesound level (F(1, 8476) = 0.35, p = 0.5546).
Theinteraction between measures and context schemesis not significant, indicating that accuracy differ-ences are similar regardless of context construction.5.2 Performance of the Voting AlgorithmFigure 4 shows the histogram (distribution) for theaccuracy rate at sound and word levels.
We seethat for the voting algorithm, the accuracy rates aregreater than 0.7 for most of the sounds, and greaterthan 0.9 for majority of the words to disambiguate.Figure 5 show the percentage of sense disam-biguation results overlapping between voting algo-rithm and individual relatedness measures.
Notethat any two methods may come up with differentcorrect results (e.g.
?lesk?
assigned ?chirp?
as ?asharp sound?
while the voting algorithm assigned?chirp?
as ?making a sharp sound?).
This indicatesthe change of the contribution of each relatednessmeasures in different voting schemes.
In the simplevoting scheme, more disambiguation results camefrom the ?path,?
?wup,?
and ?lch?
(the WordNetpath based measures), while the weighted votingFigure 4.
Histogram of accuracy rate at sound (327, left) and word level (1553, right) among different measures,contexts, and voting schemes.
EVC1 = Evocation (Context 1); SR11 = Voting (Context 1, voting scheme 1).Figure 5.
Percentage of sense disambiguation results overlap between voting algorithm, evocation, and individ-ual sense relatedness measures at image (3,086 images) and sound (327 sounds) level.68scheme took more of the recommendations from?lesk,?
?lin,?
and ?jcn?
(context and definitionbased measures) into consideration.
At the soundlevel, there is no significant accuracy differenceamong the three voting schemes, and the influenceof the context composition is similar.
However, atthe word level (Figure 3), the weighted votingschemes significantly outperformed the simple vot-ing scheme (F(2, 9312) = 5.20, p = 0.0055), and allof them have significantly better accuracy whenthe context contains mainly members from thesame sense set (F(1, 9312) = 4.79, p = 0.0287).5.3 Performance of WSD with EvocationAs shown in Figures 3, the performance of theevocation measure is not significantly differentfrom path-based and some context-based measuresat sound level, including ?path,?
?wup,?
?lch,??res,?
?lin,?
and ?jcn?
(for Context 1, F(6, 2282) =2.0582, p = 0.0551; for Context 2, F(6, 2282) =1.6679, p = 0.1249); and is significantly better thanthe vector_pairs measure (for Context 1, F(1, 652)= 61.37, p < 0.001; for Context 2, F(1, 652) =36.47, p < 0.001).
At the word level, the perfor-mance of the evocation measure is not significantlydifferent from that of measures including ?path,??wup,?
?lch,?
?res?
(F(4, 7760) = 0.39, p = 0.8135),and ?lin,?
?jcn,?
and ?vector_pairs?
(F(3, 6208) =1.52, p = 0.2077).
Figure 8 (SoundNet) and Figure9 (Peekaboom) show the percentage of synset as-signment overlap between evocation and the othernine relatedness measures.
The overlap with ?lesk?and ?vector?
are significantly higher than that withthe other measures (F(8, 5877) = 34.67, p < 0.001).It suggests that evocation as a semantic relatednessmeasure may be closer to the definition-basedmeasures than path and content based measures.For the SoundNet dataset, 34% to 44% of evoca-tion WSD results overlap with that of other meas-ures; for the Peekaboom dataset, the overlap is25% to 35% (Figure 6).
Given that evocation per-formed similarly in accuracy to most of othermeasures with relatively low overlap in WSD re-sults, evocation may capture different aspects ofsemantic relatedness from existing measures.6 Conclusion and Future WorkWe explored the construction of a sense disambi-guated semantic AAC multimodal vocabulary fromsound/image label datasets.
Two WSD approachesare introduced to assign specific meanings to envi-ronmental sound and image labels, and furthercreate concept-sound/image associations.
Themeasure-combined voting algorithm targets theaccuracy of WSD and achieves significantly betterperformance than each relatedness measure indivi-dually.
Our second approach applies a new rela-tedness measure, evocation.
Evocation achievessimilar performance to most of the existing rela-tedness measures with sound labels.
Results sug-gest that evocation provides different semanticinformation from current measures.Future work includes: 1) expanding the evoca-tion dataset and investigating the potential im-provement in its WSD accuracy; 2) incorporatingthe extended evocation dataset into the voting al-gorithm; 3) exploring additional information suchas image and sound similarity to help with WSD.AcknowledgmentsFigure 6.
Percentage of WSD results overlap between evocation and various relatedness measures.69We thank the Kimberley and Frank H. Moss ?71Princeton SEAS Research Fund for supporting ourproject.ReferencesSatanjeev Banerjee and Ted Pedersen.
2002.
AnAdapted Lesk Algorithm for Word Sense Disambig-uation Using WordNet.
Proceedings of the 3rd Inter-national Conference on Intelligent Text Processingand Computational Linguistics.Jordan Boyd-Graber, Christaine Fellbaum, DanielOsherson, and Robert Schapire.
2006.
Adding Dense,Weighted Connections to WordNet.
Proceedings ofthe Thirds International WordNet Conference.Jia Deng, Wei Dong, Richard Socher, Li -J. Li, Kai Liand Li Fei-Fei.
2009.
ImageNet: A Large-Scale Hie-rarchical Image Database.
Proceedings of the IEEEComputer Vision and Pattern Recognition (CVPR).Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Marti Hearst.
1991.
Noun Homograph DisambiguationUsing Local Context in Large Text Corpora.
Proc.
ofthe 7th Annual Conference of the University of Water-loo Center for the New OED and Text Research.Graeme Hirst and David St. Onge.
1998.
Lexical Chainsas Representations of Context for the Detection andCorrection of Malapropisms.
In Christiane Fellbaum,editor, WordNet: An Electronic Lexical Database.Jay Jiang and David Conrath.
1997.
Semantic SimilarityBased on Corpus Statistics and Lexical Taxonomy.Proceedings on International Conference on Re-search in Computational Linguistics.Claudia Leacock and Martin Chodorow.
1998.
Combin-ing Local Context and WordNet Similarity for WordSense Identification.
In Christiane Fellbaum, editor,WordNet: An Electronic Lexical Database.Michael Lesk.
1986.
Automatic Sense DisambiguationUsing Machine Readable Dictionaries: How to Tell aPine Cone from an Ice Cream Cone.
Proceedings ofSIGDOC?86.Dekang Lin.
1997.
Using Syntactic Dependency as aLocal Context to Resolve Word Sense Ambiguity.Proceedings of the 35th Annual Meeting of the Asso-ciation for Computational Linguistics, pp.
64-71.Lingraphica.
http://www.aphasia.com/.
2010.Xiaojuan Ma, Christiane Fellbaum.
and Perry Cook.2010.
SoundNet: Investigating a Language Com-posed of Environmental Sounds.
In Proc.
CHI 2010.Xiaojuan Ma, Jordan Boy-Graber, Sonya Nikolova, andPerry Cook.
2009a.
Speaking Through Pictures: Im-ages vs. Icons.
Proceedings of ASSETS09.Xiaojuan Ma, Sonya Nikolova and Perry Cook.
2009b.W2ANE: When Words Are Not Enough - OnlineMultimedia Language Assistant for People withAphasia.
Proceedings of ACM Multimedia 2009.Sonya Nikolova, Jordan Boyd-Graber, and ChristianeFellbaum.
2009.
Collecting Semantic Similarity Rat-ings to Connect Concepts in Assistive Communica-tion Tools (in press).
Modelling, Learning andProcessing of Text-Technological Data Structures,Springer Studies in Computational Intelligence.Adrian Novischi, Muirathnam Srikanth, and AndrewBennett.
2007.
Lcc-wsd: System Description forEnglish Coarse Grained All Words Task at SemEval2007.
Proceedings of the 4th International Workshopon Semantic Evaluations(SemEval-2007), pp 223-226.Siddharth Patwardhan, Satanjeev Benerjee and Ted Pe-dersen.
Using Measures of Semantic Relatedness forWord Sense Disambiguation.
2003.
Proceeding ofCICLing2003, pp.
241-257.Siddharth Patwardhan and Ted Pedersen Using Word-Net Based Context Vectors to Estimate the SemanticRelatedness of Concepts.
2006.
Proceedings of theEACL 2006 Workshop Making Sense of Sense -Bringing Computational Linguistics and Psycholin-guistics Together, pp.
1-8Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
WorNet::Similarity ?
Measuring the Re-latedness of Concepts.
Proceedings of HumanLanguage Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics Demonstrations, pp.
38-41.Ted Pedersen and Varada Kolhatkar.
2009.
Word-Net::SenseRelate::AllWords - A Broad CoverageWord Sense Tagger that Maximimizes Semantic Re-latedness.
Proceedings of Human Language Tech-nology Conference of the North American Chapter ofthe Association for Computational Linguistics Dem-onstrations, pp.
17-20.Philip Resnik.
1995.
Using Information Content to Eva-luate Semantic Similarity in a Taxonomy.
Proceed-ings of the 14th International Joint Conference onArtificial Intelligence.Richard Steele, Michael Weinrich, Robert Wertz, GloriaCarlson, and Maria Kleczewska.
Computer-basedvisual communication in aphasia.
Neuropsychologia.27(4): pp 409-26.
1989.Luis von Ahn, Laura Dabbish.
2004.
Labeling imageswith a computer game.
Proceedings of the SIGCHIconference on Human factors in computing systems,p.319-326.Luis von Ahn, Ruoran Liu, Manuel Blum.
2006 Peeka-boom: a game for locating objects in images.
Pro-ceedings of the SIGCHI conference on HumanFactors in computing systems.Zhibiao Wu and Martha Palmer.
1994.
Verb Semanticsand Lexical Selection.
Proc.
of ACL, pp 133-138.David Yarowsky.
1995.
Unsupervised Word Sense Dis-ambiguation Rivaling Supervised Methods.
Proceed-ings of the 33rd Annual Meeting on Association ForComputational Linguistics.70
