Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50?61,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsUnsupervised Structure Predictionwith Non-Parallel Multilingual GuidanceShay B. Cohen Dipanjan Das Noah A. SmithLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{scohen,dipanjan,nasmith}@cs.cmu.eduAbstractWe describe a method for prediction of lin-guistic structure in a language for which onlyunlabeled data is available, using annotateddata from a set of one or more helper lan-guages.
Our approach is based on a modelthat locally mixes between supervised mod-els from the helper languages.
Parallel datais not used, allowing the technique to be ap-plied even in domains where human-translatedtexts are unavailable.
We obtain state-of-the-art performance for two tasks of structure pre-diction: unsupervised part-of-speech taggingand unsupervised dependency parsing.1 IntroductionA major focus of recent NLP research has involvedunsupervised learning of structure such as POStag sequences and parse trees (Klein and Manning,2004; Johnson et al, 2007; Berg-Kirkpatrick et al,2010; Cohen and Smith, 2010, inter alia).
In itspurest form, such research has improved our un-derstanding of unsupervised learning practically andformally, and has led to a wide range of new algo-rithmic ideas.
Another strain of research has soughtto exploit resources and tools in some languages (es-pecially English) to construct similar resources andtools for other languages, through heuristic ?projec-tion?
(Yarowsky and Ngai, 2001; Xi and Hwa, 2005)or constraints in learning (Burkett and Klein, 2008;Smith and Eisner, 2009; Das and Petrov, 2011; Mc-Donald et al, 2011) or inference (Smith and Smith,2004).
Joint unsupervised learning (Snyder andBarzilay, 2008; Naseem et al, 2009; Snyder et al,2009) is yet another research direction that seeks tolearn models for many languages at once, exploitinglinguistic universals and language similarity.
Thedriving force behind all of this work has been thehope of building NLP tools for languages that lackannotated resources.1In this paper, we present an approach to usingannotated data from one or more languages (helperlanguages) to learn models for another language thatlacks annotated data (the target language).
Unlikethe previous work mentioned above, our frameworkdoes not rely on parallel data in any form.
This isadvantageous because parallel text exists only in afew text domains (e.g., religious texts, parliamentaryproceedings, and news).We focus on generative probabilistic models pa-rameterized by multinomial distributions.
We be-gin with supervised maximum likelihood estimatesfor models of the helper languages.
In the secondstage, we learn a model for the target language usingunannotated data, maximizing likelihood over inter-polations of the helper language models?
distribu-tions.
The tying is performed at the parameter level,through coarse, nearly-universal syntactic categories(POS tags).
The resulting model is then used to ini-tialize learning of the target language?s model usingstandard unsupervised parameter estimation.Some previous multilingual research, such asBayesian parameter tying across languages (Co-hen and Smith, 2009) or models of parameter1Although the stated objective is often to build systems forresource-poor languages and domains, for evaluation purposes,annotated treebank test data figure prominently in this research(including in this paper).50drift down phylogenetic trees (Berg-Kirkpatrick andKlein, 2010) is comparable, but the practical as-sumption of supervised helper languages is new tothis work.
Naseem et al (2010) used universalsyntactic categories and rules to improve grammarinduction, but their model required expert hand-written rules as constraints.Herein, we specifically focus on two problemsin linguistic structure prediction: unsupervised POStagging and unsupervised dependency grammar in-duction.
Our experiments demonstrate that the pre-sented method outperforms strong state-of-the-artunsupervised baselines for both tasks.
Our approachcan be applied to other problems in which a sub-set of the model parameters can be linked acrosslanguages.
We also experiment with unsupervisedlearning of dependency structures from words, bycombining our tagger and parser.
Our results showthat combining our tagger and parser with jointinference outperforms pipeline inference, and, inseveral cases, even outperforms models built usinggold-standard part-of-speech tags.2 OverviewFor each language `, we assume the presence of aset of fine-grained POS tags F`, used to annotate thelanguage?s treebank.
Furthermore, we assume thatthere is a set of universal, coarse-grained POS tagsC such that, for every language `, there is a determin-istic mapping from fine-grained to coarse-grainedtags, ?` : F` ?
C. Our approach can be summa-rized using the following steps for a given task:1.
Select a set of L helper languages for which thereexists annotated data ?D1, .
.
.
,DL?.
Here, we usetreebanks in these languages.2.
For all ` ?
{1, .
.
.
, L}, convert the examples inD` by applying ?` to every POS tag in the data,resulting in D?`.
Estimate the parameters of aprobabilistic model using D?`.
In this work, suchmodels are generative probabilistic models basedon multinomial distributions,2 including an HMMand the dependency model with valence (DMV)of Klein and Manning (2004).
Denote the subsetof parameters that are unlexicalized by ?(`).
(Lex-icalized parameters will be denoted ?(`).
)2In ?4 we also consider a feature-based parametrization.3.
For the target language, define the set of valid un-lexicalized parameters?
={??????
?k =L?`=1?`,k?
(`)k ,L?`=1?`,k = 1,?
?
0},(1)for each group of parameters k, and maximizelikelihood over that set, using the target-languageunannotated data U .
Because the syntactic cate-gories referenced by each ?
(`) and all models in ?are in C, the models will be in the same parametricfamily.
(Figure 1 gives a graphical interpretationof ?.)
Let the resulting model be ?.4.
Transform ?
by expanding the coarse-grainedsyntactic categories into the target language?sfine-grained categories.
Use the resulting modelto initialize parameter estimation, this time overfine-grained tags, again using the unannotatedtarget-language data U .
Initialize lexicalized pa-rameters ?
for the target language using standardmethods (e.g., uniform initialization with randomsymmetry breaking).The main idea in the approach is to estimate acertain model family for one language, while usingsupervised models from other languages.
The linkbetween the languages is achieved through coarse-grained categories, which are now now common-place (and arguably central to any theory of naturallanguage syntax).
A key novel contribution is theuse of helper languages for initialization, and of un-supervised learning to learn the contribution of eachhelper language to that initialization (step 3).
Addi-tional treatment is required in expanding the coarse-grained model to the fine-grained one (step 4).3 Interpolated Multilingual ProbabilisticContext-Free GrammarsOur focus in this paper is on models that consistof multinomial distributions that have relationshipsbetween them through a generative process such asa probabilistic context-free grammar (PCFG).
Morespecifically, we assume that we have a model defin-ing a probability distribution over observed surfaceforms x and derivations y parametrized by ?
:51(0,1,0)(0,0,1) (1,0,0)EnglishCzechGermanItalianFigure 1: A simple case of interpolation within the 3-event probability simplex.
The shaded area correspondsto a convex hull inside the probability simplex, indicatinga mixture of the parameters of the four languages shownin the figure.p(x,y | ?)
=K?k=1Nk?i=1?fk,i(x,y)k,i (2)= expK?k=1Nk?i=1fk,i(x,y) log ?k,i (3)where fk,i is a function that ?counts?
the numberof times the kth distribution?s ith event occurs inthe derivation.
The parameters ?
are a collectionof K multinomials ?
?1, .
.
.
,?K?, the kth of whichincludes Nk events.
Letting ?k = ?
?k,1, .
.
.
, ?k,Nk?,each ?k,i is a probability, such that ?k,?i, ?k,i ?
0and ?k,?Nki=1 ?k,i = 1.3.1 Multilingual InterpolationOur framework places additional, temporary con-straints on the parameters ?.
More specifically, weassume that we have L existing, parameter estimatesfor the multinomial families from Eq.
3.
Each suchestimate ?
(`), for 1 ?
` ?
L, corresponds to a themaximum likelihood estimate based on annotateddata for the `th helper language.
Then, to create amodel for new language, we define a new set of pa-rameters ?
as:?k,i =L?`=1?`,k?
(`)k,i , (4)where ?
is the set of coefficients that we will nowbe interested in estimating (instead of directly esti-mating ?).
Note that for each k,?L`=1 ?`,k = 1 and?`,k ?
0.3.2 Grammatical InterpretationWe now give an interpretation of our approach relat-ing it to PCFGs.
We assume familiarity with PCFGs.For a PCFG ?G,??
we denote the set of nontermi-nal symbols by N , the set of terminal symbols by?, and the set of rewrite rules for each nonterminalA ?
N by R(A).
Each r ?
R(A) has the formA ?
?
where ?
?
(N ?
?)?.
In addition, there isa probability attached to each rule ?A??
such that?A ?
N ,??:(A??
)?R(A) ?A??
= 1.
A PCFG canbe framed as a model using Eq.
3, where ?
corre-spond to K = |N | multinomial distributions, whereeach distribution attaches probabilities to rules witha specific left hand symbol.We assume that the model we are trying toestimate (over coarse part-of-speech tags) can beframed as a PCFG ?G,??.
This is indeed the casefor part-of-speech tagging and dependency grammarinduction we experiment with in ?6.
In that case,our approach can be framed for PCFGs as follow-ing.
We assume that there exists L set of parametersfor this PCFG ?
(1), .
.
.
,?
(L), each corresponding toa helper language.
We then create a new PCFG G?with parameters ??
and ?
as follows:1.
G?
contains all nonterminal and terminal symbolsinG, and none of the rules inG.2.
For each nonterminal A in G, we create a newnonterminal aA,` for ` ?
{1, .
.
.
, L}.3.
For each nonterminal A in G, we create rulesA ?
aA,` for ` ?
{1, .
.
.
, L} which have proba-bilities ?A?aA,` .4.
For each rule A?
?
inG, we add toG?
the ruleaA,` ?
?
with??aA,`??
= ?(`)A??.
(5)where ?(`)A??
is the probability associated withrule A?
?
in the `th helper language.At each point, the derivational process of thisPCFG uses the nonterminal?s specific ?
coefficients52to choose one of the helper languages.
It then se-lects a rule according to the multinomial from thatlanguage.
This step is repeated until a whole deriva-tion is generated.This PCFG representation of the approach in ?3points to a possible generalization.
Instead of usingan identical CFG backbone for each language, wecan use a set of PCFGs, ?G(`),?(`)?
with an iden-tical nonterminal set and alphabet, and repeat thesame construction as above, replacing step 4 withthe addition of rules of the form aA,` ?
?
for eachrule A ?
?
in G(`).
Such a construction allowsmore syntactic variability in the language we are try-ing to estimate, originating in the syntax of the var-ious helper languages.
In this paper, we do not usethis generalization, and always use the same PCFGbackbone for all languages.Note that the interpolated model can still be un-derstood in terms of the exponential model of Eq.
3.For a given collection of multinomials and basemodels of the form of Eq.
3, we can analogouslydefine a new log-linear model over a set of ex-tended derivations.
These derivations will now in-clude L ?
K features of the form g`,k(x,y), cor-responding to a count of the event of choosing the`th mixture component for multinomial k. In addi-tion, the feature set fk,i(x,y) will be extended toa feature set of the form f`,k,i(x,y), analogous tostep 4 in constructed PCFG above.
The model pa-rameterized according to Eq.
4 can be recovered bymarginalizing out the ?g?
features.
We will refer tothe model with these new set of features as ?the ex-tended model.
?4 Inference and Parameter EstimationThe main building block commonly required for un-supervised learning in NLP is that of computing fea-ture expectations for a given model.
These featureexpectations can be used with an algorithm such asexpectation-maximization (where the expectationsare normalized to obtain a new set of multinomialweights) or with other gradient based log-likelihoodoptimization algorithms such as L-BFGS (Liu andNocedal, 1989) for feature-rich models.Estimating Multinomial Distributions Given asurface form x, a multinomial k and an event i in themultinomial, ?feature expectation?
refers to the cal-culation of the following quantities (in the extendedmodel):E[f`,k,i(x,y)] = ?y p(x,y | ?
)f`,k,i(x,y) (6)E[g`,k(x,y)] = ?y p(x,y | ?
)g`,k(x,y) (7)These feature expectations can usually be computedusing algorithms such as the forward-backward al-gorithm for hidden Markov models, or more gener-ally, the inside-outside algorithm for PCFGs.
In thispaper, however, the task of estimation is differentthan the traditional task.
As mentioned in ?2, we areinterested in estimating ?
from Eq.
4, while fixing?(`).
Therefore, we are only interested in computingexpectations of the form of Eq.
7.As explained in ?3.2, any model interpolatingwith the ?
parameters can be reduced to a new log-linear model with additional features representingthe mixture coefficients of ?.
We can then use theinside-outside algorithm to obtain the necessary fea-ture expectations for features of the form g`,k(x,y),expectations which assist in the estimation of the ?parameters.These feature expectations can readily be usedin estimation algorithms such as expectation-maximization (EM).
With EM, the update at itera-tion t would be:?
(t)`,k =E[g`,k(x,y)]?` E[g`,k(x,y)], (8)where the expectations are taken with respect to?
(t?1) and the fixed ?
(l) for ` = 1, .
.
.
, L.Estimating Feature-Rich Directed Models Re-cently Berg-Kirkpatrick et al (2010) found thatreplacing traditional multinomial parameterizationswith locally normalized, feature-based log-linearmodels was advantageous.
This can be understoodas parameterizing ?
:?k,i =exp?>h(k, i)?i?exp?>h(k, i?
)(9)where h(k, i) are a set of features looking at event iin context k. For such a feature-rich model, our mul-tilingual modeling framework still substitutes ?
witha mixture of supervised multinomials for L helperlanguages as in Eq.
4.
However, for computational53convenience, we also reparametrize the mixture co-efficients ?
:?`,k =exp ?`,k?L`?=1 exp ?`?,k(10)Here, each ?`,k is an unconstrained parameter, andthe above ?softmax?
transformation ensures that ?lies within the probability simplex for context k.This is done so that a gradient-based optimizationmethod like L-BFGS (Liu and Nocedal, 1989) canbe used to estimate ?
without having to worry aboutadditional simplex constraints.
For optimization,derivatives of the data log-likelihood with respect to?
need to be computed.
We calculate the derivativesfollowing Berg-Kirkpatrick et al (2010, ?3.1), mak-ing use of feature expectations, calculated exactly asbefore.In addition to these estimation techniques, whichare based on the optimization of the log-likelihood,we also consider a trivially simple technique for es-timating ?
: setting ?l,k to the uniform weight L?1,where L is the number of helper languages.5 Coarse-to-Fine Multinomial ExpansionTo expand these multinomials involving coarse-grained categories into multinomials over fine-grained categories specific to the target language t,we do the following:?
Whenever a multinomial conditions on a coarsecategory c ?
C, we make copies of it for each fine-grained category in ?
?1t (c) ?
Ft.3 If the multino-mial does not condition on coarse categories, it issimply copied.?
Whenever a probability ?i within a multinomialdistribution involves a coarse-grained category cas an event (i.e., it is on the left side of the condi-tional bar), we expand the event into |?
?1t (c)| newevents, one per corresponding fine-grained cate-gory, each assigned the value ?i|?
?1t (c)| .43We note that in the models we experiment with, we alwayscondition on at most one fine-grained category.4During this expansion process for a coarse event, we triedadding random noise to ?i|?
?1t (c)| and renormalizing, to breaksymmetry between the fine events, but that was found to beharmful in preliminary experiments.The result of this expansion is a model in thedesired family; we use it to initialize conventionalunsupervised parameter estimation.
Lexical param-eters, if any, do not undergo this expansion pro-cess, and they are estimated anew in the fine grainedmodel during unsupervised learning, and are initial-ized using standard methods.6 Experiments and ResultsIn this section, we describe the experiments under-taken and the results achieved.
We first note thecharacteristics of the datasets and the universal POStags used in multilingual modeling.6.1 DataFor our experiments, we fixed a set of four helperlanguages with relatively large amounts of data,displaying nontrivial linguistic diversity: Czech(Slavic), English (West-Germanic), German (West-Germanic), and Italian (Romance).
The datasets arethe CoNLL-X shared task data for Czech and Ger-man (Buchholz and Marsi, 2006),5 the Penn Tree-bank for English (Marcus et al, 1993), and theCoNLL 2007 shared task data for Italian (Monte-magni et al, 2003).
This was the only set of helperlanguages we tested; improvements are likely pos-sible.
We leave an exploration of helper languagechoice (a subset selection problem) to future re-search, instead demonstrating that the concept hasmerit.We considered ten target languages: Bulgarian(Bg), Danish (Da), Dutch (Nl), Greek (El), Japanese(Jp), Portuguese (Pt), Slovene (Sl), Spanish (Es),Swedish (Sv), and Turkish (Tr).
The data comefrom the CoNLL-X and CoNLL 2007 shared tasks(Buchholz and Marsi, 2006; Nivre et al, 2007).
Forall the experiments conducted, we trained modelson the training section of a language?s treebank andtested on the test set.
Table 1 shows the number ofsentences in the treebanks and the size of fine POStagsets for each language.Following standard practice, in unsupervisedgrammar induction experiments we remove punctu-ation and then eliminate sentences from the data oflength greater than 10.5These are based on the Prague Dependency Treebank(Hajic?, 1998) and the Tiger treebank (Brants et al, 2002) re-spectively.54Pt Tr Bg Jp El Sv Es Sl Nl DaTraining sentences 9,071 4,997 12,823 17,044 2,705 11,042 3,306 1,534 13,349 5,190Test sentences 288 623 398 709 197 389 206 402 386 322Size of POS tagset 22 31 54 80 38 41 47 29 12 25Table 1: The first two rows show the sizes of the training and test datasets for each language.
The third row shows thenumber of fine POS tags in each language including punctuations.6.2 Universal POS TagsOur coarse-grained, universal POS tag set consistsof the following 12 tags: NOUN, VERB, ADJ(adjective), ADV (adverb), PRON (pronoun), DET(determiner), ADP (preposition or postposition),NUM (numeral), CONJ (conjunction), PRT (parti-cle), PUNC (punctuation mark) and X (a catch-allfor other categories such as abbreviations or foreignwords).
These follow recent work by Das and Petrov(2011) on unsupervised POS tagging in a multilin-gual setting with parallel data, and have been de-scribed in detail by Petrov et al (2011).While there might be some controversy aboutwhat an appropriate universal tag set should include,these 12 categories (or a subset) cover the most fre-quent parts of speech and exist in one form or an-other in all of the languages that we studied.
Foreach language in our data, a mapping from thefine-grained treebank POS tags to these universalPOS tags was constructed manually by Petrov et al(2011).6.3 Part-of-Speech TaggingOur first experimental task is POS tagging, and herewe describe the specific details of the model, train-ing and inference and the results attained.6.3.1 ModelThe model is a hidden Markov model (HMM),which has been popular for unsupervised taggingtasks (Merialdo, 1994; Elworthy, 1994; Smith andEisner, 2005; Berg-Kirkpatrick et al, 2010).6 Weuse a bigram model and a locally normalized log-linear parameterization, like Berg-Kirkpatrick et al(2010).
These locally normalized log-linear mod-els can look at various aspects of the observation xgiven a tag y, or the pair of tags in a transition, in-corporating overlapping features.
In basic monolin-6HMMs can be understood as a special case of PCFGs.gual experiments, we used the same set of featuresas Berg-Kirkpatrick et al (2010).
For the transi-tion log-linear model, Berg-Kirkpatrick et al (2010)used only a single indicator feature of a tag pair, es-sentially equating to a traditional multinomial dis-tribution.
For the emission log-linear model, sev-eral features were used: an indicator feature con-joining the state y and the word x, a feature checkingwhether x contains a digit conjoined with the state y,another feature indicating whether x contains a hy-phen conjoined with y, whether the first letter of x isupper case along with the state y, and finally indica-tor features corresponding to suffixes up to length 3present in x conjoined with the state y.Since only the unlexicalized transition distribu-tions are common across multiple languages, assum-ing that they all use a set of universal POS tags, akinto Eq.
4, we can have a multilingual version of thetransition distributions, by incorporating supervisedhelper transition probabilities.
Thus, we can write:?y?y?
=L?`=1?`,y?(`)y?y?
(11)We use the above expression to replace the transi-tion distributions, obtaining a multilingual mixtureversion of the model.
Here, the transition probabili-ties ?(`)y?y?
for the `th helper language are fixed afterbeing estimated using maximum likelihood estima-tion on the helper language?s treebank.6.3.2 Training and InferenceWe trained both the basic feature-based HMMmodel as well as the multilingual mixture model byoptimizing the following objective function:7L(?)
=N?i=1log?yp(x(i),y | ?)?
C??
?227Note that in the objective function, for brevity, we abusenotation by using ?
for both models ?
monolingual and multi-lingual; the latter model is also parameterized by ?.55Method Pt Tr Bg Jp El Sv Es Sl Nl Da AvgUniform+DG 45.7 43.6 38.0 60.4 36.7 37.7 31.8 35.9 43.7 36.2 41.0Mixture+DG 51.5 38.6 35.8 61.7 38.9 39.9 40.5 36.0 50.2 39.9 43.3DG (B-K et al, 2010) 53.5 27.9 34.7 52.3 35.3 34.4 40.0 33.4 45.4 48.8 40.6(a)Method Pt Tr Bg Jp El Sv Es Sl Nl Da AvgUniform+DG 83.8 50.4 81.3 77.9 80.3 69.0 82.3 82.8 79.3 82.0 76.9Mixture+DG 84.7 50.0 82.6 79.9 80.3 67.0 83.3 82.8 80.0 82.0 77.3DG (B-K et al, 2010) 75.4 50.4 80.7 83.4 88.0 61.5 82.3 75.6 79.2 82.3 75.9(b)Table 2: Results for unsupervised POS induction (a) without a tagging dictionary and (b) with a tag dictionary con-structed from the training section of the corresponding treebank.
DG (at the bottom) stands for the direct gradientmethod of Berg-Kirkpatrick et al (2010) using a monolingual feature-based HMM.
?Mixture+DG?
is the model wheremultilingual mixture coefficients ?
of helper languages are estimated using coarse tags (?4), followed by expansion(?5), and then initializing DG with the expanded transition parameters.
?Uniform+DG?
is the case where ?
are setto 1/4, transitions of helper languages are mixed, expanded, and then DG is initialized with the result.
For (a), eval-uation is performed using one-to-one mapping accuracy.
In case of (b), the tag dictionary solves the problem of tagidentification and performance is measured using per word POS accuracy.
?Avg?
denotes macro-average across theten languages.Note that this involves marginalizing out all possiblestate configurations y for a sentence x, resulting ina non-convex objective.
As described in ?4, we opti-mized this function using L-BFGS.
For the mono-lingual model, derivatives of the feature weightstook the exact same form as Berg-Kirkpatrick et al(2010), while for the mixture case, we computedgradients with respect to ?, the unconstrained pa-rameters used to express the mixture coefficients ?
(see Eq.
10).
The regularization constant C was setto 1.0 for all experiments, and L-BFGS was run tillconvergence.During training, for the basic monolingualfeature-based HMM model, we initialized all param-eters using small random real values, sampled fromN (0, 0.01).
For estimation of the mixture parame-ters ?
for our multilingual model (step 3 in ?2), wesimilarly sampled real values fromN (0, 0.01) as aninitialization point.
Moreover, during this stage, theemission parameters also go through parameter es-timation, but they are monolingual, and are initial-ized with real values sampled from N (0, 0.01); asexplained in ?2, coarse universal tags are used bothin the transitions and emissions during multilingualestimation.After the mixture parameters ?
are estimated, wecompute the mixture probabilities ?
using Eq.
10.Next, for each tag pair y, y?, we compute ?y?y?
,which are the coarse transition probabilities inter-polated using ?, given the helper languages.
Wethen expand these transition probabilities (see ?5) toresult in transition probabilities based on fine tags.Finally, we train a feature-HMM by initializing itstransition parameters with natural logarithms of theexpanded ?
parameters, and the emission parame-ters using small random real values sampled fromN (0, 0.01).
This implies that the lexicalized emis-sion parameters ?
that were previously estimated inthe coarse multilingual model are thrown away andnot used for initialization; instead standard initial-ization is used.For inference at the testing stage, we use min-imum Bayes-risk decoding (or ?posterior decod-ing?
), by choosing the most probable tag for eachword position, given the entire observation x. Wechose this strategy because it usually performsslightly better than Viterbi decoding (Cohen andSmith, 2009; Ganchev et al, 2010).6.3.3 Experimental SetupFor experiments, we considered three configura-tions, and for each, we implemented two variants ofPOS induction, one without any kind of supervision,and the other with a tag dictionary.
Our baseline is56the direct gradient approach of Berg-Kirkpatrick etal.
(2010), which is the current state of the art for thistask, outperforming classical HMMs.
Because thismodel achieves strong performance using straight-forward MLE, it also serves as the core model withinour approach.
This model has also been applied ina multilingual setting with parallel data (Das andPetrov, 2011).
In this baseline, we set the numberof HMM states to the number of fine-grained tree-bank tags for the given language.We test two versions of our model.
The first ini-tializes training of the target language?s POS modelusing a uniform mixture of the helper language mod-els (i.e., each ?`,y = 1L = 14 ), and expansion fromcoarse-grained to fine-grained POS tags as describedin ?5.
We call this model ?Uniform+DG.
?The second version estimates the mixture coeffi-cients to maximize likelihood, then expands the POStags (?5), using the result to initialize training of thefinal model.
We call this model ?Mixture+DG.
?No Tag Dictionary For each of the above configura-tions, we ran purely unsupervised training without atag dictionary, and evaluated using one-to-one map-ping accuracy constraining at most one HMM stateto map to a unique treebank tag in the test data, us-ing maximum bipartite matching.
This is a variant ofthe greedy one-to-one mapping scheme of Haghighiand Klein (2006).8With a Tag Dictionary We also ran a second ver-sion of each experimental configuration, where weused a tag dictionary to restrict the possible path se-quences of the HMM during both learning and infer-ence.
This tag dictionary was constructed only fromthe training section of a given language?s treebank.It is widely known that such knowledge improvesthe quality of the model, though it is an open debatewhether such knowledge is realistic to assume.
Forthis experiment we removed punctuation from thetraining and test data, enabling direct use within thedependency grammar induction experiments.8We also evaluated our approach using the greedy version ofthis evaluation metric, and results followed the same trends withonly minor differences.
We did not choose the other variant,many-to-one mapping accuracy, because quite often the metricmapped several HMM states to one treebank tag, leaving manytreebank tags unaccounted for.6.3.4 ResultsAll results for POS induction are shown in Ta-ble 2.
Without a tag dictionary, in eight out of tencases, either Uniform+DG or Mixture+DG outper-forms the monolingual baseline (Table 2a).
For sixof these eight languages, the latter model where themixture coefficients are learned automatically faresbetter than uniform weighting.
With a tag dictionary,the multilingual variants outperform the baseline inseven out of ten cases, and the learned mixture out-performs or matches the uniform mixture in five ofthose seven (Table 2b).6.4 Dependency Grammar InductionWe next describe experiments for dependency gram-mar induction.
As the basic grammatical model,we adopt the dependency model with valence (Kleinand Manning, 2004), which forms the basis for state-of-the-art results for dependency grammar induc-tion in various settings (Cohen and Smith, 2009;Spitkovsky et al, 2010; Gillenwater et al, 2010;Berg-Kirkpatrick and Klein, 2010).
As shown in Ta-ble 3, DMV obtains much higher accuracy in the su-pervised setting than the unsupervised setting, sug-gesting that more can be achieved with this modelfamily.9 For this reason, and because DMV is eas-ily interpreted as a PCFG, it is our starting point andbaseline.We consider four conditions.
The independentvariables are (1) whether we use uniform?
(all set to14 ) or estimate them using EM (as described in ?4),and (2) whether we simply use the mixture model todecode the test data, or to initialize EM for the DMV.The four settings are denoted ?Uniform,?
?Mixture,??Uniform+EM,?
and ?Mixture+EM.
?The results are given in Table 3.
In general, theuse of data from other languages improves perfor-mance considerably; all of our methods outperformthe Klein and Manning (2004) initializer, and weachieve state-of-the-art performance for eight out often languages.
Uniform and Mixture behave simi-larly, with a slight advantage to the trained mixturesetting.
Using EM to train the mixture coefficientsmore often hurts than helps (six languages out often).
It is well known that likelihood does not cor-9Its supervised performance is still far from the supervisedstate of the art in dependency parsing.57Method Pt Tr Bg Jp El Sv Es Sl Nl Da AvgUniform 78.6 45.0 75.6 56.3 57.0 74.0 73.2 46.1 50.7 59.2 61.6Mixture 76.8 45.3 75.5 58.3 59.5 73.2 75.9 46.0 51.1 59.9 62.2Uniform+EM 78.7 43.9 74.7 59.8 73.0 70.5 75.5 41.3 45.9 51.3 61.5Mixture+EM 79.8 44.1 72.8 63.9 72.3 68.7 76.7 41.0 46.0 55.2 62.1EM (K & M, 2004) 42.5 36.3 54.3 43.0 41.0 42.3 38.1 37.0 38.6 41.4 41.4PR (G et al, ?10) 47.8 53.4 54.0 60.2 - 42.2 62.4 50.3 37.9 44.0 -Phylo.
(B-K & K, ?10) 63.1 - - - - 58.3 63.8 49.6 45.1 41.6 -Supervised (MLE) 81.7 75.7 83.0 89.2 81.8 83.2 79.0 74.5 64.8 80.8 79.3Table 3: Results for dependency grammar induction given gold-standard POS tags, reported as attachment accuracy(fraction of parents which are correct).
The three existing methods are: our replication of EM with the initializer fromKlein and Manning (2004), denoted ?EM?
; reported results from Gillenwater et al (2010) for posterior regularization(?PR?
); and reported results from Berg-Kirkpatrick and Klein (2010), denoted ?Phylo.?
?Supervised (MLE)?
are oracleresults of estimating parameters from gold-standard annotated data using maximum likelihood estimation.
?Avg?denotes macro-average across the ten languages.Figure 2: Projection of the learned mixture coefficientsthrough PCA.
In green, Japanese.
In red, Dutch, Danishand Swedish.
In blue, Bulgarian and Slovene.
In ma-genta, Portuguese and Spanish.
In black, Greek.
In cyan,Turkish.relate with the true accuracy measurement, and soit is unsurprising that this holds in the constrainedmixture family as well.
In future work, a differentparametrization of the mixture coefficients, throughfeatures, or perhaps a Bayesian prior on the weights,might lead to an objective that better simulates ac-curacy.Table 3 shows that even uniform mixture coef-ficients are sufficient to obtain accuracy which su-percedes most unsupervised baselines.
We were in-terested in testing whether the coefficients which arelearned actually reflect similarities between the lan-guages.
To do that, we projected the learned vectors?
for each tested language using principal compo-nent analysis and plotted the result in Figure 2.
Itis interesting to note that languages which are closerphylogenetically tend to appear closer to each otherin the plot.Our experiments also show that multilinguallearning performs better for dependency grammarinduction than part-of-speech tagging.
We believethat this happens because of the nature of the mod-els and data we use.
The transition matrix in part-of-speech tagging largely depends on word order inthe various helper languages, which differs greatly.This means that a mixture of transition matrices willnot necessarily yield a meaningful transition matrix.However, for dependency grammar, there are certainuniversal dependencies which appear in all helperlanguages, and therefore, a mixture between multi-nomials for these dependencies still yields a usefulmultinomial.6.5 Inducing Dependencies from WordsFinally, we combine the models for POS tagging andgrammar induction to perform grammar inductiondirectly from words, instead of gold-standard POStags.
Our approach is as follows:1.
With a tag dictionary, learn a fine-grained POStagging model unsupervised, using either DG orMixture+DG as described in ?6.3 and shown inTable 2b.58Method Tags Pt Tr Bg Jp El Sv Es Sl Nl Da AvgJoint DG 68.4 52.4 62.4 61.4 63.5 58.2 67.7 47.2 48.3 50.4 57.9Joint Mixture+DG 62.2 47.4 67.0 69.5 52.2 49.1 69.3 36.8 52.2 50.1 55.6Pipeline DG 60.0 50.8 57.7 64.2 68.2 57.9 65.8 45.8 49.9 48.9 56.9Pipeline Mixture+DG 59.8 47.1 62.9 68.6 50.0 47.6 68.1 36.4 51.2 48.3 54.0Gold-standard tags 79.8 45.3 75.6 63.9 73.0 74.0 76.7 46.1 50.7 59.9 64.5Table 4: Results for dependency grammar induction over words.
?Joint?/?Pipeline?
refers to joint/pipeline decodingof tags and dependencies as described in the text.
See ?6.3 for a description of DG and Mixture+DG.
For the inductionof dependencies we use the Mixture+EM setting as described in ?6.4.
All tag induction uses a dictionary as specifiedin ?6.3.
The last row in this table indicates the best results using multilingual guidance taken from our methods inTable 3.
?Avg?
denotes macro-average across the ten languages.2.
Apply the fine-grained tagger to the words in thetraining data for the dependency parser.
We con-sider two variants: the most probable assignmentof tags to words (denoted ?Pipeline?
), and the pos-terior distribution over tags for each word, repre-sented as a weighted ?sausage?
lattice (denoted?Joint?).
This idea was explored for joint infer-ence by Cohen and Smith (2007).3.
We apply the Mixture+EM unsupervised parserlearning method from ?6.4 to the automaticallytagged sentences, or the lattices.4.
Given the two models, we infer POS tags on thetest data using DG or Mixture+DG to get a lattice(Joint) or a sequence (Pipeline) and then parse us-ing the model from the previous step.10 The re-sulting dependency trees are evaluated against thegold standard.Results are reported in Table 4.
In almost all cases,joint decoding of tags and trees performs better thanthe pipeline.
Even though our part-of-speech taggerwith multilingual guidance outperforms the com-pletely unsupervised baseline, there is not always anadvantage of using this multilingually guided part-of-speech tagger for dependency grammar induc-tion.
For Turkish, Japanese, Slovene and Dutch, ourunsupervised learner from words outperforms unsu-pervised parsing using gold-standard part-of-speechtags.We note that some recent work gives a treatmentto unsupervised parsing (but not of dependencies)10The decoding method on test data (Joint or Pipeline) wasmatched to the training method, though they are orthogonal inprinciple.directly from words (Seginer, 2007).
Earlier workthat induced part-of-speech tags and then performedunsupervised parsing in a pipeline includes Kleinand Manning (2004) and Smith (2006).
Headdenet al (2009) described the use of a lexicalized vari-ant of the DMV model, with the use of gold part-of-speech tags.7 ConclusionWe presented an approach to exploiting annotateddata in helper languages to infer part-of-speech tag-ging and dependency parsing models in a different,target language, without parallel data.
Our approachperforms well in many cases.
We also described away to do joint decoding of part-of-speech tags anddependencies which performs better than a pipeline.Future work might consider exploiting a larger num-ber of treebanks, and more powerful techniques forcombining models than simple local mixtures.AcknowledgmentsWe thank Ryan McDonald and Slav Petrov for help-ful comments on an early draft of the paper.
This re-search has been funded by NSF grants IIS-0844507 andIIS-0915187 and by U.S. Army Research Office grantW911NF-10-1-0533.ReferencesT.
Berg-Kirkpatrick and D. Klein.
2010.
Phylogeneticgrammar induction.
In Proceedings of ACL.T.
Berg-Kirkpatrick, A.
B.
Co?te?, J. DeNero, and D. Klein.2010.
Painless unsupervised learning with features.
InProceedings of NAACL-HLT.59S.
Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.2002.
The TIGER treebank.
In Proceedings of theWorkshop on Treebanks and Linguistic Theories.S.
Buchholz and E. Marsi.
2006.
CoNLL-X shared taskon multilingual dependency parsing.
In Proceedingsof CoNLL.D.
Burkett and D. Klein.
2008.
Two languages are bet-ter than one (for syntactic parsing).
In Proceedings ofEMNLP.S.
B. Cohen and N. A. Smith.
2007.
Joint morpholog-ical and syntactic disambiguation.
In Proceedings ofEMNLP-CoNLL.S.
B. Cohen and N. A. Smith.
2009.
Shared logisticnormal distributions for soft parameter tying in unsu-pervised grammar induction.
In Proceedings of HLT-NAACL.S.
B. Cohen and N. A. Smith.
2010.
Covariance in unsu-pervised learning of probabilistic grammars.
Journalof Machine Learning Research, 11:3017?3051.D.
Das and S. Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projections.In Proceedings of ACL-HLT.D.
Elworthy.
1994.
Does Baum-Welch re-estimationhelp taggers?
In Proceedings of ACL.K.
Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.2010.
Posterior regularization for structured latentvariable models.
Journal of Machine Learning Re-search, 11:2001?2049.J.
Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, andB.
Taskar.
2010.
Sparsity in dependency grammarinduction.
In Proceedings of ACL.A.
Haghighi and D. Klein.
2006.
Prototype driven learn-ing for sequence models.
In Proceedings of HLT-NAACL.J.
Hajic?.
1998.
Building a syntactically annotatedcorpus: The Prague Dependency Treebank.
In Is-sues of Valency and Meaning.
Studies in Honor ofJarmila Panevova?.
Prague Karolinum, Charles Univer-sity Press.W.
P. Headden, M. Johnson, and D. McClosky.
2009.Improving unsupervised dependency parsing withricher contexts and smoothing.
In Proceedings ofNAACL-HLT.M.
Johnson, T. L. Griffiths, and S. Goldwater.
2007.Bayesian inference for PCFGs via Markov chainMonte Carlo.
In Proceedings of NAACL.D.
Klein and C. D. Manning.
2004.
Corpus-based induc-tion of syntactic structure: Models of dependency andconstituency.
In Proceedings of ACL.D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory BFGS method for large scale optimization.
Math.Programming, 45:503?528.M.
P. Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of English:the Penn treebank.
Computational Linguistics, 19.R.
McDonald, S. Petrov, and K. Hall.
2011.
Multi-sourcetransfer of delexicalized dependency parsers.
In Pro-ceedings of EMNLP.B.
Merialdo.
1994.
Tagging English text with a proba-bilistic model.
Compulational Lingustics, 20(2):155?72.S.
Montemagni, F. Barsotti, M. Battista, N. Calzolari,O.
Corazzari, A. Zampolli, F. Fanciulli, M. Massetani,R.
Raffaelli, R. Basili, M. T. Pazienza, D. Saracino,F.
Zanzotto, N. Mana, F. Pianesi, and R. Delmonte.2003.
Building the Italian Syntactic-Semantic Tree-bank.
In Building and using Parsed Corpora, Lan-guage and Speech Series.
Kluwer, Dordrecht.T.
Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.2009.
Multilingual part-of-speech tagging: Two un-supervised approaches.
JAIR, 36.T.
Naseem, H. Chen, R. Barzilay, and M. Johnson.
2010.Using universal linguistic knowledge to guide gram-mar induction.
In Proceedings of EMNLP.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007shared task on dependency parsing.
In Proceedingsof CoNLL.S.
Petrov, D. Das, and R. McDonald.
2011.
A universalpart-of-speech tagset.
ArXiv:1104.2086.Y.
Seginer.
2007.
Fast unsupervised incremental parsing.In Proceedings of ACL.N.
A. Smith and J. Eisner.
2005.
Contrastive estimation:Training log-linear models on unlabeled data.
In Pro-ceedings of ACL.D.
A. Smith and J. Eisner.
2009.
Parser adaptation andprojection with quasi-synchronous grammar features.In Proceedings of EMNLP.D.
A. Smith and N. A. Smith.
2004.
Bilingual parsingwith factored estimation: Using English to parse Ko-rean.
In Proceedings of EMNLP.N.
A. Smith.
2006.
Novel Estimation Methods for Unsu-pervised Discovery of Latent Structure in Natural Lan-guage Text.
Ph.D. thesis, Johns Hopkins University.B.
Snyder and R. Barzilay.
2008.
Unsupervised multi-lingual learning for morphological segmentation.
InProceedings of ACL.B.
Snyder, T. Naseem, and R. Barzilay.
2009.
Unsuper-vised multilingual grammar induction.
In Proceedingsof ACL-IJCNLP.V.
Spitkovsky, H. Alshawi, and D. Jurafsky.
2010.
Frombaby steps to leapfrog: How ?less is more?
in unsuper-vised dependency parsing.
In Proceedings of NAACL.C.
Xi and R. Hwa.
2005.
A backoff model for bootstrap-ping resources for non-English languages.
In Proceed-ings of HLT-EMNLP.60D.
Yarowsky and G. Ngai.
2001.
Inducing multilingualPOS taggers and NP bracketers via robust projectionacross aligned corpora.
In Proceedings of NAACL.61
