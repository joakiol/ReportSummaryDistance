Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 130?140,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsModeling and Learning Semantic Co-Compositionalitythrough Prototype Projections and Neural NetworksMasashi Tsubaki, Kevin Duh, Masashi Shimbo, Yuji MatsumotoGraduate School of Information ScienceNara Institute of Science and Technology8916-5, Takayama, Ikoma, Nara 630-0192, Japan{masashi-t,kevinduh,shimbo,matsu}@is.naist.jpAbstractWe present a novel vector space model for se-mantic co-compositionality.
Inspired by Gen-erative Lexicon Theory (Pustejovsky, 1995),our goal is a compositional model whereboth predicate and argument are allowed tomodify each others?
meaning representationswhile generating the overall semantics.
Thisreadily addresses some major challenges withcurrent vector space models, notably the pol-ysemy issue and the use of one represen-tation per word type.
We implement co-compositionality using prototype projectionson predicates/arguments and show that thisis effective in adapting their word represen-tations.
We further cast the model as aneural network and propose an unsupervisedalgorithm to jointly train word representationswith co-compositionality.
The model achievesthe best result to date (?
= 0.47) on thesemantic similarity task of transitive verbs(Grefenstette and Sadrzadeh, 2011).1 IntroductionVector space models of words have been verysuccessful in capturing the semantic and syntacticcharacteristics of individual lexical items (Turneyand Pantel, 2010).
Much research has addressedthe question of how to construct individual wordrepresentations, for example distributional models(Mitchell and Lapata, 2010) and neural models(Collobert and Weston, 2008).
These word repre-sentations are used in various natural language pro-cessing (NLP) tasks such as part-of-speech tagging,chunking, named entity recognition, and semanticVkPcompany=VkTVkCo-Compositionality with Prototype ProjectionsCo-compositional vectorof verb and objectstartbuildbuyoperate =companyrunVERBruncompanyPrototypeProjectioncompanyrunOBJ??????Okfirmbankhotel??????Prun=OkTOk?
?m?
?Figure 1: Here, we capture the semantics of run in runcompany by projecting the original word representationof run to the prototype space of company (and vice versa).role labeling (Turian et al 2010; Collobert et al2011).Recently, modeling of semantic compositionality(Frege, 1892) in vector space has emerged as anotherimportant line of research (Mitchell and Lapata,2008; Mitchell and Lapata, 2010; Baroni and Zam-parelli, 2010; Socher et al 2012; Grefenstette andSadrzadeh, 2011; Van de Cruys et al 2013).
Thegoal is to formulate how individual word represen-tations ought to be combined to achieve phrasal orsentential semantics.The main questions for semantic compositionalitythat we are concerned with are: (1) how can poly-semy be handled by a single vector representationper word type, learned by either a distributional orneural model, and (2) how does composition resolve130these ambiguities.
To this end, we are inspired bythe idea of type coercion and co-compositionalityin Generative Lexicon Theory (Pustejovsky, 1995).Co-compositionality advocates that instead of apredicate-argument view of composition, both pred-icate and argument influence/coerce each other togenerate the overall meaning.
For example, considera polysemous word like run:?
(a) He runs the company.?
(b) He runs the marathon.Run may have several senses, but the prototypicalverbs that select for company differ from thosethat select for marathon, and thus the ambiguityat the word level is resolved at the sentence level.The same is true for the other direction, where thepredicate also coerces meaning to the argument tofit expectation.We believe that models for semantic com-position ought to incorporate elements of co-compositionality.
We propose such a model here,using what we call prototype projections.
For eachpredicate, we transform its vector representation byprojecting it into a latent space that is prototypicalof its argument.
This projection is performed anal-ogously for each argument as well, and the finalmeaning is computed by composition of these trans-formed vectors (Figure 1).
In addition, the model iscast as a neural network where word representationscould be re-trained or fine-tuned.1Our contributions are two-fold:1.
We propose a novel model for semantic co-compositionality.
This model, based onprototype projections, is easy to implementand achieves state-of-the-art performance inthe sentence similarity dataset developed byGrefenstette and Sadrzadeh (2011).2.
Our results empirically confirm that existingword representations (eg., SDS and NLM inSection 2) are sufficiently effective at capturing1While we are inspired by co-compositionality, it is impor-tant to note that our model does not implement qualia structureand other important components of Generative Lexicon Theory.We operate within the vector space model of distributionalsemantics, so these ideas are implemented with matrix algebra,which is a natural fit with neural networks.polysemy, as long as we have the proper mech-anism to tease out the proper sense during com-position.
We further propose an unsupervisedneural network training algorithm that jointlyfine-tunes the word representations within theco-composition model, resulting in even betterperformance on the sentence similarity task.We would like to emphasize the second contribu-tion especially.
Semantics research is divided in twostrands, one focusing on learning word represen-tations without consideration for compositionality,and the other focusing on compositional semanticsusing the representations only as an input.
But issuesare actually related from the linguistics perspective,and even more so if we adopt a Generative Lexiconperspective.
Our neural network model bridgesthese two strands of research by modeling co-compositionality and learning word representationssimultaneously.
We note that methods using contexteffects have been explored by Erk and Pado?
(2008;2009) and Thater et al(2010; 2011), but to thebest of our knowledge, ours is the first model toperform co-compositionality and learning of wordrepresentations jointly.In the following, we first provide background tothe word representations employed here (Section 2).We describe the model for co-compositionality inSection 3 and the corresponding neural network inSection 4.
Evaluation and experiments are presentedin Sections 5 and 6.
Finally, we end with relatedwork (Section 7) and conclusions (Section 8).2 Word Vector Representations2.1 Simple Distributional Semantic space(SDS) word vectorsWord meaning is often represented in a high di-mensional space, where each element corresponds tosome contextual element in which the word is found.Mitchell and Lapata (2010) present a co-occurrence-based semantic space called Simple DistributionalSemantic space (SDS).
Their SDS model uses a con-text window of five words on either side of the targetword and 2,000 vector components, representing themost frequent context words (excluding a list of stopwords).
These components vi(t) were set to theratio of the probability of the context word given the131target word to the probability of the context wordoverall:vi(t) =p(ci|t)p(ci)= freqci,t ?
freqtotalfreqt ?
freqci(1)where freqci,t, freqtotal, freqt and freqci are thefrequencies of the context word ci with the targetword t, the total count of all word tokens, thefrequency of the target word t, and the frequencyof the context word ci, respectively.2.2 Neural Language Model (NLM) wordembeddingsAnother popular way to learn word representationsis based on the Neural Language Model (NLM)(Bengio et al 2003).
In comparison with SDS,NLM tend to be low-dimensional (e.g.
50 dimen-sions) but employ dense features.
These densefeature vectors are usually called word embeddings,and it has been shown that such vectors can cap-ture interesting linear relationships, such as king ?man + woman ?
queen (Mikolov et al 2013).In this work, we adopt the model by Collobertand Weston (2008).
The idea is to construct aneural network based on word sequences, whereone outputs high scores for n-grams that occur in alarge unlabeled corpus and low scores for nonsensen-grams where one word is replaced by a randomword.
This word representation with NLM has beenused to good effect, for example in (Turian et al2010; Collobert et al 2011; Huang et al 2012)where induced word representations are used withsophisticated features to improve performance invarious NLP tasks.Specifically, we first represent the word sequenceas a vector x = [d(w1);d(w2); .
.
.
;d(wm)], wherewi is ith word in the sequence, m is the win-dow size, d(w) is the vector representation ofword w (an n-dimensional column vector) and[d(w1);d(w2); .
.
.
;d(wm)] is the concatenation ofword vectors as an input of neural network.
Second,we compute the score of the sequence,score(x) = sT(tanh(Wx+ b)) (2)where W ?
Rh?
(mn) and s ?
Rh are the firstand second layer weights of the neural network,and b ?
Rh is the bias unit of hidden layer.
Thesuperscript T represents transposition, and tanh isapplied element-wise.
We also create a corruptedsequence xc = [d(w1);d(w2); .
.
.
;d(wm?)]
wherewm?
is chosen randomly from the vocabulary.
Wecompute the score of this implicit negative sequencexc with the same neural network, score(xc) =sT(tanh(Wxc + b)).
Finally, we get the costfunction of this training algorithm as follow.J = max(0, 1 ?
score(x) + score(xc)) (3)In order to minimize this cost function, we optimizethe parameters ?
= (s,W,b,x) via backpropagationwith stochastic gradient descent (SGD).3 The Model3.1 Prototype ProjectionGenerative Lexicon Theory (Pustejovsky, 1995)makes a distinction between accidental polysemy(homonyms, e.g.
bank as financial institution vs.as river side) and logical polysemy (e.g.
figure andground meanings of door).
Our model handles bothcases using the concept of projection to latent proto-type space.
The fundamental idea is that for eachword w and a syntactic/semantic (binary) relationR (such as verb-object relation), w has a set ofprototype words with which it frequently occurs inrelation R. For example, if w is a word company,and R is the object-verb relation, prototype wordsshould include start, build, and buy (Figure 1).For each word-relation pair, we pre-compute thelatent semantic subspace spanned by these prototypewords.Later, when we encounter a phrase expressing arelationR between two wordsw1 andw2, each wordis first projected onto a latent subspace determinedby the other word and relation R. The projectionoperation shifts the meaning of individual words inaccordance with context, and through this operationwe realize coercion/co-composition.
And finally, themeaning of the phrase is computed from the twoprojected points in the semantic space.Let us describe how to compute the latent sub-space associated with a word w0 and a relation R.First, we collect from a corpus a set of prototypewords that occur frequently in relation R with targetword w0.
So for example in Figure 1, if w0 =132verb object landmark similarity(verb, landmark) similarity(projected verb, landmark)run company operate 0.40 0.70meet criterion satisfy 0.49 0.71spell name write 0.04 0.50Table 1: Examples of verb-object pairs.
Original verb and landmark verb similarity, prototype projected verb andlandmark verb similarity, as measure by cosine using Collobert and Weston?s word embeddings.
Meet has a abstractmeaning itself, but after prototype projection with matrix constructed by word vectors of W (VerbOf, criterion), meetis more close to meaning of satisfy.company, and R = VerbOf is the object-verbrelation,W (VerbOf, company) = {start, build, .
.
.
, buy}.Now let W (R,w0) = {w1, w2, ?
?
?
, wm} bethe m prototype words we collected, and let d(w)denote the n-dimensional (column) vector represen-tation of word w (either by SDS or NLM representa-tion).
We make anm?nmatrixC(R,w0) by stackingthe prototype word vectors, i.e.,C(R,w0) = [d(w1),d(w2), ?
?
?
,d(wm)]T (4)and then apply Singular Value Decomposition(SVD) to extract the latent space from this matrix:C(R,w0) ?
Uk?kVTk .
(5)word1word vector dimension nword2wordmm?
?
??????kkn?
?
??
?
???????
?
??
?
?kk?k VkTUkC ?Figure 2: Graphical representation of SVD in our model.Figure 2 shows the graphical representation ofthis matrix factorization.
In NLP tasks, SVD isoften applied to a term-document matrix, but in ourmodel, we apply SVD to the matrix consisting ofword vectors.Intuitively, ?kVTk represents the latent sub-space formed by prototypical words W (R,w0) ={w1, w2, ?
?
?
, wm}.
We call this matrix the proto-type space of word w0 with respect to relation R.Note that the matrix of orthogonal projectiononto this prototype space is given by P(R,w0) =(?kVTk)T(?kVTk).
Hence, when we observe a rela-tion R(w0, w), the projected representation of wordw in this context is computed by prpj(R,w0)(w)defined as follows:prpj(R,w0)(w) = P(R,w0)d(w).
(6)Table 1 shows several examples of how meaningschange after prototype projection using word em-beddings of Collobert and Weston (2008).23.2 Co-CompositionalityIn order to model co-compositionality, we applyprototype projection to both the verb and the object.In particular, suppose verb is wv and object is wo,C(VerbOf,wo) is used to project wv and C(ObjOf,wv)is used to project wo.
The vector that representsthe overall meaning of verb-object with prototypeprojection is computed by:cocomp(wv, wo) =f(prpj(VerbOf,wo)(wv),prpj(ObjOf,wv)(wo)) (7)Function f can be a compositional computation likesimple addition or element-wise multiplication oftwo vectors.
This is graphically shown in Figure 1.4 Unsupervised Learning ofCo-CompositionalityIn this section, we propose a new neural languagemodel that learns word representations while jointlyaccounting for compositional semantics.
One cen-tral assumption of our work (and many other worksin compositional semantics) is that a single vector2ronan.collobert.com/senna/133voz = f(v, o)sscore = sTzCompositionalNeural Language Model (C-NLM)verbobjFigure 3: Compositional Neural Language Model (C-NLM).per word type sufficiently represents the multiplemeanings and usage patterns of a word.3 Thatmeans that for a polysemous word, its word vectoractually represents an aggregation of the distinctlydifferent contexts it occurs in.
We will show thatsuch an assumption is quite reasonable under ourmodel, since the prototype projections successfullytease out the proper semantics from these aggregaterepresentations.However, it is natural to wonder whether onecan do better if one incorporates the compositionalmodel into the training of the word representationsin the first place.
To do so, we formulate a nov-el model called Compositional Neural LanguageModel (Section 4.1).
This model is a combinationof an unsupervised training algorithm with basiccompositionality (addition/multiplications).
Then,we extend this model with the projection idea insection 3.2 to formulate a Co-Compositional NeuralLanguage Model (Section 4.2).4.1 Compositional Neural Language Model(C-NLM)Compositional Neural Language Model (C-NLM)is a combination of a word representation learningmethod and compositional rule.
In contrast to othercompositional models based on machine learning,our model has no complex parameters for model-ing composition.
Composition is modeled usingstraightforward vector addition/multiplications; in-stead, what is learned is the word representation.Figure 3 shows the C-NLM.
The learning al-gorithm is unsupervised, and works by artificially3There are works on multiple representations, e.g.,(Reisinger and Mooney, 2010); we focus on single represen-tation here.z = f(v, o)sscore = sTzPobjPverbCo-CompositionalNeural Language Model (CoC-NLM)voyxverbobjFigure 4: Co-Compositional Neural Language Model(CoC-NLM) is C-NLM with prototype projection.generating negative examples in a fashion analogousto the NLM learning algorithm of (Collobert andWeston, 2008) and contrastive estimation (Smithand Eisner, 2005).
First, given some initial wordrepresentations and raw sentences, we compute thecompositional vector with function f (in this sec-tion, we will assume that we will be using theaddition operator).
Second, in order to obtain thescore of compositional vector, we compute the dotproduct with vector s ?
Rn (n is the dimension ofthe word vector space): verb vector v = d(wv) andobject vector o = d(wo).score(v,o) = sTf(v,o) = sT(v + o) (8)We also create a corrupted pair by substituting a ran-dom verb wverb?.
The cost function J = max(0, 1?score(v,o) + score(vc,o)), where vc is the wordvector of wverb?, encourages that the score of correctpair is higher than the score of the corrupt pair.
Letz = v + o, our model parameters are ?
= (s, z,v).The optimization is divided into two steps:1.
Optimize s and z via SGD.2.
Let znew be the updated z via step 1.
The newverb vector vnew trained within additive composi-tionality is just vnew = znew ?
o.
Note that if wealso want to optimize o, we may want to also corruptthe object and run SGD in step 2 as well.4.2 Co-Compositional Neural Language Model(CoC-NLM)We now add prototype projection into C-NLM,making our final model: Co-Compositional Neural134Language Model (CoC-NLM).
We define the scorefunction as dot product of s and additional vector ofprototype projected vectors (Figure 4).
Let Pobj =P(VerbOf,wo) and Pverb = P(ObjOf,wv),score(v,o) = sT(Pobjv +Pverbo).
(9)Let x = Pobjv, y = Pverbo and z = x + y.Our model parameters are ?
= (s, z,v).
Theoptimization algorithm of CoC-NLM is divided intothree steps like C-NLM.
First, we optimize s andz.
Second, the projected verb vector is updatedas xnew = znew ?
y.
Finally we optimize v tominimize the Euclidean distance between xnew andPobjv, where ?
is a regularization hyper-parameter:J(v) = 12 ||xnew ?Pobjv||2 + ?2vTv (10)5 Evaluation5.1 DatasetIn order to evaluate the performance of our newco-compositional model with prototype projectionand word representation learning algorithm, wemake use of the disambiguation task of transitivesentences developed by Grefenstette and Sadrzadeh(2011).
This is an extension of the two wordsphrase similarity task defined in Mitchell and Lapata(2008), and constructed according to similar guide-lines.
The dataset consists of similarity judgmentsbetween a landmark verb and a triple consisting ofa transitive target verb, subject and object extractedfrom the BNC corpus.
Human judges give scoresbetween 1 to 7, with higher scores implying highersemantic similarity.
For example, Table 2 showssome examples from the data: we see that the verbmeet with subject system and object criterion isjudged similar to the landmark verb satisfy but notvisit.
The dataset contains a total of 2500 similarityjudgements, provided by 25 participants.4 Thetask is to have the model produce a score for eachpair of landmark verb and verb-subject-object triple.Models are evaluated by computing the Spearman?s?
correlation between its similarity scores and thatof the human judgments.4http://www.cs.ox.ac.uk/people/edward.grefenstette/verb subj obj landmark simmeet system criterion satisfy 6meet system criterion visit 1write student name spell 7write student paper spell 2Table 2: Examples from the disambiguation task de-veloped by Grefenstette and Sadrzadeh (2011).
Humanjudges give scores between 1 to 7, with higher scoresimplying higher semantic similarity.
Verb meet withsubject system and object criterion is judged similar tothe landmark verb satisfy but not visit.5.2 BaselinesWe compare our model against multiple baselinesfor semantic compositionality:1.
Mitchell and Lapata?s (2008) additive andelement-wise multiplicative model as simplestbaselines.2.
Grefenstette and Sadrzadeh?s (2011) modelbased on the abstract categorical framework(Coecke et al 2010).
This model computesthe outer product of the subject and objectvector, the outer product of the verb vectorwith itself, and then the element-wise productof both results.3.
Erk and Pado?
?s (2008) model, which adapts theword vectors based on context and is the mostsimilar in terms of motivation to ours.4.
Van de Cruy et al(2013) multi-way interactionmodel based on matrix factorization.
Thisachieves the best result for this task to date.A detailed explanation of these models will beprovided in Section 7.
For the underlying word rep-resentations, we experiment with sparse 2000-dimSDS and dense 50-dim NLM.
These are providedby Blacoe and Lapata (2012)5 and trained on theBritish National Corpus (BNC).
We are interestedin knowing how sensitive each model is to theunderlying word representation.
In general, this isa challenging task: the upper-bound of ?
= 0.62 isthe inter-annotator agreement.5http://homepages.inf.ed.ac.uk/s1066731/index.php?page=resources1355.3 Implementation detailsIn terms of implementation detail, our model and ourre-implementation of Erk and Pado?s model makeuse of the ukWaC corpus (Baroni et al 2009).6 Thiscorpus is a two billion word corpus automaticallyharvested from the web and parsed by the Malt-Parser (Nivre et al 2006).
We use ukWaC corpusto collect W (VerbOf, wo) and W (ObjOf, wv) forprototype projections.
We also extract about 5000verb-object pairs that relevant for testdata from thiscorpus to train our neural network learning algorith-m.
In our co-compositional model, the contributionratio of SVD is set to 80% (i.e.
automaticallyfixing k in SVD to include 80% of the top singularvalues).
We set the number of prototype vectorsto be m = 20, where W (VerbOf, wo) is filteredwith high frequency words and W (ObjOf, wv) isfiltered with both high frequency and high similaritywords.
In our model, we output the scores for SVOtriple sentence dataset as (subject=ws, verb=wv,object=wo, f = Addition/Multiplication):cocomp(ws, wv, wo) =f(d(ws), cocomp(wv, wo)) (11)6 Results and Discussion6.1 Main Results: The CorrelationTable 3 shows the correlation scores of variousmodels.
Our observations are as follows:1.
The best reported result for this task (Van deCruys et al 2013) is ?
= 0.37.
Ourmodel (with NLM as word representation andf=Addition as operator) achieves ?
= 0.44,outperforming it by a large margin.
To the bestof our knowledge, this is now state-of-the-artresult for this task.2.
Our model is not very sensitive to the underly-ing word representation.
With f=Addition, wehave ?
= 0.41 for SDS vs ?
= 0.44 for NLM.With f=Multiply, we have ?
= 0.37 for SDSvs.
?
= 0.35 for NLM.
This implies that theprototype projection is robust to the underlyingword representation, which is a desired charac-teristic of compositional models.6http://wacky.sslmit.unibo.it/doku.php?id=corporaModel ?Grefenstette and Sadrzadeh (2011) ?
0.21Add (SDS) ?
0.31Add (NLM) ?
0.31Multiply (SDS) ?
0.35Multiply (NLM) ?
0.30Van de Cruys et al(2013) 0.37Erk and Pado?
(SDS) 0.39Erk and Pado?
(NLM) 0.03Co-Comp with f=Add (SDS) 0.41Co-Comp with f=Add (NLM) ?
0.44Co-Comp with f=Multiply (SDS) 0.37Co-Comp with f=Multiply (NLM) 0.35Upper bound ?
0.62Table 3: Results of the different compositionality modelson the similarity task.
The number of prototype wordsm = 20 in all our models.
Our model (f=Addition andNLM) achieves the new state-of-the-art performance forthis task (?
= 0.44).3.
The contextual model of Erk and Pado?
(SDS)also performed relatively well (?
= 0.39),in fact outperforming the Van de Cruy et al(2013) result as well.
This means that thegeneral idea of adapting word representationsbased on context is a very powerful one.
How-ever, Erk and Pado?
?s model using the NLM rep-resentation is extremely poor (?
= 0.03).
Thereason is that it uses a product operation under-the-hood to adapt the vectors, which inherentlyassumes a sparse representation.
In this sense,our projection approach is more robust.The state-of-the-art result for our model in Table3 does not yet make use of the training algorithmdescribed in Section 4.
It is simply implementingthe co-compositionality idea using prototype projec-tions (Section 3.2).
Next in Section 6.2 we will showadditional gains using unsupervised learning.6.2 Improvements from unsupervised learningIn this experiment, we examine how much gain ispossible by re-training the word representation ofverbs using the unsupervised algorithm describedin Section 4.
We focus on the additive modelof Compositional NLM, both basic and prototypeprojection.
The initial word representation is from136model original representation re-trainedC-NLM 0.31 0.38CoC-NLM 0.44 ?
0.47Table 4: Results of re-training the word representationfor C-NLM and CoC-NLM.
Learning rate ?
= 0.01,regularization ?
= 10?4 and iteration = 20.
One iterationis one run through the dataset of 5000 verb-object pairswhich we made from the ukWaC corpus.NLM.
Table 4 shows the gains in correlation score.This result shows that our learning model suc-cessfully captures good representation within co-compositionality of additive model.
In contrast toother previous compositional models, our modeldoes not require estimating a large number of pa-rameters for computation of compositional vectorsand word representation itself is more suitable forit.
Furthermore, learning is very fast, taking about10 minutes for C-NLM on a standard machine withIntel Core i7 2.93Ghz CPU and 8GB of RAM.6.3 The number of prototype wordsThe number of prototype words (m in Figure 1) weuse to generate the prototype space is one hyper-parameter that our model has.
Here, we analyze theeffect of the choice of m. Figure 5 shows the rela-tion of m and the performance of co-compositionalmodel with prototype projections using either SDSor NLM representations.
In general, both NLMand SDS show relatively smooth and flat curvesacross m, indicating the relative robustness of theapproach.
Nevertheless, results do degrade for largem, due to increase in noise from non-prototypewords.
Further, it does appear that NLM has a slow-er drop in correlation with increasing m comparedwith SDS.
This suggests that NLM is more robust,which is possibly attributable to the dense and low-dimensional distributed features.6.4 Variations in model configurationWe have presented a compositional model of theform d(ws) + cocomp(wv, wo), where prototypeprojections are performed on both wv and wo andws is composed as is without projection.
In general,we have the freedom to choose what to projectand what not to project under this co-compositionalframework.
Here in Table 5 we show the results ofFigure 5: The relation between the number of prototypewords and correlation of SDS or NLM.
In general, NLMhas higher correlation than SDS and is more robust acrossthe m.Subj Verb Obj NLM ?
SDS ?prpj prpj prpj 0.39 0.37+ prpj prpj 0.44 0.41prpj prpj 0.45 0.41+ prpj + 0.43 0.38prpj + 0.43 0.38+ + + 0.31 0.31Table 5: Variants of the full co-compositional model,based on how subject, verb, and object vector repre-sentations are included.
prpj indicates that prototypeprojection is used.
+ indicates that the vector is addedwithout projection first.
Blank indicates that the vector isnot used in the final compositional score.these variants, using f =Addition and SDS/NLMrepresentations without re-training.
We note thatour positive results mainly come from the verbprojections.
Subject information actually does nothelp.
We believe this best configuration is task-dependent; in this test collection, the subjects appearto have little contribution to the landmark verb.7 Related workIn recent years, several sophisticated vector spacemodels have been proposed for computing compo-sitional semantics.
Mitchell and Lapata (2010), Erk(2012) and Baroni et al(2013) are recommendedsurvey papers.137One of the first approaches is the vector ad-dition/multiplication idea of Mitchell and Lapata(2008).
The appeal of this kind of simple approachis its intuitive geometric interpretation and its ro-bustness to various datasets.
However, it may notbe sufficiently expressive to represent the variousfactors involved in compositional semantics, suchas syntax and context.
To this end, Baroni andZamparelli (2010) present a compositional modelfor adjectives and nouns.
In their model, an adjectiveis a matrix operator that modifies the noun vectorinto an adjective-noun vector.
Zanzotto et al(2010)and Guevara (2010) also proposed linear transfor-mation models for composition and address the issueof estimating large matrices with least squares orregression techniques.
Socher et al(2012) extendthis linear transformation approach with the morepowerful model of Matrix-Vector Recursive NeuralNetworks (MV-RNN).
Each node in a parse tree isassigned both a vector and a matrix.
The vectorcaptures the actual meaning of the word itself, whilethe matrix is modeled as a operator that modify themeaning of neighboring words and phrases.
Thismodel captures semantic change phenomenon likenot bad is similar to good due to a compositionof the bad vector with a meaning-flipping not ma-trix.
But this MV-RNN also need to optimize allmatrices of words from initial value (identity plusa small amount of Gaussian noise) with superviseddataset like movie reviews.
Our prototype projectionmodel is similar to these models as a matrix-vectoroperation, except that the matrix is not learned andcomputed from prototype words.
In future work,we can imagine integrating the two models, usingthese prototype projection matrices as initial valuesfor MV-RNN training (Socher et al 2012).Another approach is exemplified by Coecke etal.
(2010).
In their mathematical framework u-nifying categorical logic and vector space models,the sentence vector is modeled as a function of theKronecker product of its word vectors.
Grefenstetteand Sadrzadeh (2011) implement this based on un-supervised learning of matrices for relational wordsand apply them to the vectors of their arguments.Their idea is that words with relational types, such asverbs, adjectives, and adverbs are matrices that actas a filter on their arguments.
They also developeda new semantic similarity task based on transitiveComposition Operator ParameterAdd: w1u + w2v w1, w2 ?
RMultiply: uw1 ?
vw2 w1, w2 ?
RFullAdd: W1u + W2v W1,W2 ?
Rn?nLexFunc: Auv Au ?
Rn?nFullLex: ?
([W1Auv,W2Avu])?Au, Av ?
Rn?nW1,W2 ?
Rn?nOurs (Add): P(R,v)u + P(R,u)v SVD?s (m, k)Ours (Mult): P(R,v)u?
P(R,u)v SVD?s (m, k)Table 6: Comparison of composition operators that com-bine two word vector representations, u, v ?
Rn andtheir learning parameters.
Our model only needs twohyper-parameters: the number of prototype words m anddimensional reduction k in SVDverbs, which is the dataset we used here.
The pre-vious state-of-the-art result for this task comes fromthe model of Van de Cruys et al(2013).
They modelcompositionality as a multi-way interaction betweenlatent factors, which are automatically constructedfrom corpus data via matrix factorization.Comprehensive evaluation of various existingmodels are reported in (Blacoe and Lapata, 2012; D-inu et al 2013).
Blacoe and Lapata (2012) highlightthe importance of jointly examining word represen-tations and compositionality operators.
However,two out of three composition methods they evaluateare parameter-free, so that they can side-step theissue of parameter estimation.
Dinu et al(2013) de-scribe the relation between word vector and compo-sitionality in more detail with free parameters.
Table6 summarizes some ways to compose the meaningof two word vectors (u, v), following (Dinu et al2013).
These range from simple operators (e.g.
Addand Multiply) to expressive models with many freeparameters (e.g.
LexFunc, FullLex).
Many of thesemodels need to optimize n ?
n parameters, whichmay be large.
On the other hand, our model onlyneeds two hyper-parameters: the number of proto-type words m and dimensional reduction k in SVD(Table 6).
Furthermore, our model performance withneural language model word embeddings is robust tovariations in m.Most closely related to our work is the work byErk and Pado?
(2008; 2009) and Thater et al(2010;2011), which falls under the research theme ofcomputing word meaning in context.
Both methodsare characterized by the use of selectional prefer-138ence information for subjects, verbs, and objects incontext; our prototype word vectors are essentiallyequivalent to this idea.
The main difference is inhow we modify the target word representation vusing this information: whereas we project v ontoa latent subspace formed by collection of prototypevectors, Erk and Pado?
(2008; 2009) and Thateret al(2010; 2011) use the prototype vectors todirectly modify the elements of v, i.e.
by element-wise product with the centroid prototype vector.Intuitively, both our method and theirs essentiallydelete part of a word vector representation to adaptthe meaning in context.
We believe the projectionis more robust to the underlying word representation(and this is shown in the results for SDS vs. NLMrepresentations), but we note that we may be ableto borrow some of more sophisticated ways to findprototype vectors from Erk and Pado?
(2008; 2009)and Thater et al(2010; 2011).8 Conclusion and Future WorkWe began this work by asking how it is possible tohandle polysemy issues in compositional semantics,especially when adopting distributional semanticsmethods that construct only one representation perword type.
After all, the different senses of thesame word are all conflated into a single vectorrepresentation.
We found our inspiration in Gen-erative Lexicon Theory (Pustejovsky, 1995), whereambiguity is resolved due to co-compositionality ofthe words in the sentence, i.e., the meaning of anambiguous verb is generated by the properties theobject it takes, and vice versa.
We implement thisidea in a novel neural network model using proto-type projections.
The advantages of this model isthat it is robust to the underlying word representationused and that it enables an effective joint learningof word representations.
The model achieves thecurrent state-of-the-art performance (?
= 0.47)on the semantic similarity task of transitive verbs(Grefenstette and Sadrzadeh, 2011).Directions for future research include:?
Experiments on other semantics tasks, suchas paraphrase detection, word sense induction,and word meaning in context.?
Extension to more holistic sentence-level com-position using a matrix-vector recursive frame-work like (Socher et al 2012).?
Explore further the potential synergy betweenDistributional Semantics and the GenerativeLexicon.AcknowledgmentsThis work was partially supported by JSPS KAK-ENHI Grant Number 24800041, JSPS KAKENHI2430057 and Microsoft Research CORE Project.We would like to thank Hiroyuki Shindo and anony-mous reviewers for their helpful comments.ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP).Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: Acollection of very large linguistically processed web-crawled corpora.
Language resources and evaluation,43(3):209?226.Marco Baroni, Raffaella Bernardi, and Roberto Zampar-elli.
2013.
Frege in space: A program for compo-sitional distributional semantics.
Linguistic Issues inLanguage Technologies.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research,3:1137?1155.William Blacoe andMirella Lapata.
2012.
A comparisonof vector-based representations for semantic composi-tion.
In Proceedings of the Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL).Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.2010.
Mathematical foundations for a composi-tional distributional model of meaning.
CoRR, ab-s/1003.4394.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the International Conference on MachineLearning (ICML).Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.139Journal of Machine Learning Research, 12:2493?2537.Georgiana Dinu, Nghia The Pham, and Marco Barori.2013.
General estimation and evaluation of composi-tional distributional semantic models.
In Proceedingsof the Workshop on Continuous Vector Space Modelsand their Compositionality.Katrin Erk and Sebastian Pado?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP).Katrin Erk and Sebastian Pado?.
2009.
Paraphrase assess-ment in structured vector space: Exploring parametersand datasets.
In Proceedings of the Workshop onGeometrical Models of Natural Language Semantics.Katrin Erk.
2012.
Vector space models of word meaningand phrase meaning: A survey.
Language and Lin-guistics Compass, 6(10):635?653.G Frege.
1892.
U?ber sinn und bedeutung.
In Zeitschfriftfu?r Philosophie und philosophische Kritik, 100.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical compositionaldistributional model of meaning.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of the Workshop on GEomet-rical Models of Natural Language Semantics.Eric Huang, Richard Socher, Christopher Manning, andAndrew Ng.
2012.
Improving word representationsvia global context and multiple word prototypes.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics (ACL).Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous space wordrepresentations.
In Human Language Technologies:The Conference of the North American Chapter of theAssociation for Computational Linguistics (NAACL-HLT).Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofthe Annual Meeting of the Association for Computa-tional Linguistics (ACL).Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1439.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.Maltparser: A data-driven parser-generator for depen-dency parsing.
In Proceedings of the InternationalConference on Language Resources and Evaluation(LREC).James Pustejovsky.
1995.
The Generative Lexicon.
MITPress, Cambridge, MA.Joseph Reisinger and Raymond J Mooney.
2010.
Multi-prototype vector-space models of word meaning.
InHuman Language Technologies: The Conference ofthe North American Chapter of the Association forComputational Linguistics (NAACL-HLT).Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of the Annual Meeting of theAssociation for Computational Linguistics (ACL).Richard Socher, Brody Huval, Christopher D. Manning,and Andrew Y. Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of the Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL).Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations usingsyntactically enriched vector models.
In Proceedingsof the Annual Meeting of the Association for Compu-tational Linguistics (ACL).Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2011.
Word meaning in context: A simple andeffective vector model.
In Asian Federation of NaturalLanguage Processing (IJCNLP).Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceedingsof the Annual Meeting of the Association for Compu-tational Linguistics (ACL).Peter D Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.Journal of artificial intelligence research, 37(1):141?188.Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen.2013.
A tensor-based factorization model of semanticcompositionality.
In Human Language Technologies:The Conference of the North American Chapter of theAssociation for Computational Linguistics (NAACL-HLT).Fabio Massimo Zanzotto, Ioannis Korkontzelos,Francesca Fallucchi, and Suresh Manandhar.2010.
Estimating linear models for compositionaldistributional semantics.
In Proceedings ofthe International Conference on ComputationalLinguistics (COLING).140
