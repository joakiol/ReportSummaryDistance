Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 243?248,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsAn Entity-Focused Approach to Generating Company DescriptionsGavin Saldanha*Columbia UniversityOr Biran**Columbia UniversityKathleen McKeown**Columbia UniversityAlfio Gliozzo?IBM Watson* gvs2106@columbia.edu ** {orb, kathy}@cs.columbia.edu?gliozzo@us.ibm.comAbstractFinding quality descriptions on the web,such as those found in Wikipedia arti-cles, of newer companies can be difficult:search engines show many pages withvarying relevance, while multi-documentsummarization algorithms find it difficultto distinguish between core facts and otherinformation such as news stories.
In thispaper, we propose an entity-focused, hy-brid generation approach to automaticallyproduce descriptions of previously unseencompanies, and show that it outperforms astrong summarization baseline.1 IntroductionAs new companies form and grow, it is impor-tant for potential investors, procurement depart-ments, and business partners to have access to a360-degree view describing them.
The numberof companies worldwide is very large and, forthe vast majority, not much information is avail-able in sources like Wikipedia.
Often, only fir-mographics data (e.g.
industry classification, lo-cation, size, and so on) is available.
This cre-ates a need for cognitive systems able to aggre-gate and filter the information available on the weband in news, databases, and other sources.
Provid-ing good quality natural language descriptions ofcompanies allows for easier access to the data, forexample in the context of virtual agents or withtext-to-speech applications.In this paper, we propose an entity-focused sys-tem using a combination of targeted (knowledgebase driven) and data-driven generation to createcompany descriptions in the style of Wikipedia de-scriptions.
The system generates sentences fromRDF triples, such as those found in DBPedia andFreebase, about a given company and combinesthese with sentences on the web that match learnedexpressions of relationships.
We evaluate our hy-brid approach and compare it with a targeted-onlyapproach and a data-driven-only approach, as wellas a strong multi-document summarization base-line.
Our results show that the hybrid approachperforms significantly better than either approachalone as well as the baseline.The targeted (TD) approach to company de-scription uses Wikipedia descriptions as a modelfor generation.
It learns how to realize RDF re-lations that have the company as their subject:each relation contains a company/entity pair andit is these pairs that drive both content and expres-sion of the company description.
For each com-pany/entity pair, the system finds all the ways inwhich similar company/entity pairs are expressedin other Wikipedia company descriptions, clus-tering together sentences that express the samecompany/entity relation pairs.
It generates tem-plates for the sentences in each cluster, replacingthe mentions of companies and entities with typedslots and generates a new description by insert-ing expressions for the given company and entityin the slots.
All possible sentences are generatedfrom the templates in the cluster, the resulting sen-tences are ranked and the best sentence for eachrelation selected to produce the final description.Thus, the TD approach is a top-down approach,driven to generate sentences expressing the rela-tions found in the company?s RDF data using real-izations that are typically used on Wikipedia.In contrast, the data-driven (DD) approach usesa semi-supervised method to select sentences fromdescriptions about the given company on the web.Like the TD approach, it also begins with aseed set of relations present in a few companies?DBPedia entries, represented as company/entitypairs, but instead of looking at the correspondingWikipedia articles, it learns patterns that are typ-243ically used to express the relations on the web.In the process, it uses bootstrapping (Agichteinand Gravano, 2000) to learn new ways of ex-pressing the relations corresponding to each com-pany/entity pair, alternating with learning newpairs that match the learned expression patterns.Since the bootstrapping process is driven only bycompany/entity pairs and lexical patterns, it hasthe potential to learn a wider variety of expressionsfor each pair and to learn new relations that mayexist for each pair.
Thus, this approach lets datafor company descriptions on the web determinethe possible relations and patterns for expressingthose relations in a bottom-up fashion.
It then usesthe learned patterns to select matching sentencesfrom the web about a target company.2 Related WorkThe TD approach falls into the generation pipelineparadigm (Reiter and Dale, 1997), with contentselection determined by the relation in the com-pany?s DBpedia entry while microplanning andrealization are carried out through template gen-eration.
While some generation systems, partic-ularly in early years, used sophisticated gram-mars for realization (Matthiessen and Bateman,1991; Elhadad, 1991; White, 2014), in recentyears, template-based generation has shown aresurgence.
In some cases, authors focus on doc-ument planning and sentences in the domain arestylized enough that templates suffice (Elhadadand Mckeown, 2001; Bouayad-Agha et al, 2011;Gkatzia et al, 2014; Biran and McKeown, 2015).In other cases, learned models that align databaserecords with text snippets and then abstract outspecific fields to form templates have proven suc-cessful for the generation of various domains (An-geli et al, 2010; Kondadadi et al, 2013).
Others,like us, target atomic events (e.g., date of birth,occupation) for inclusion in biographies (Filatovaand Prager, 2005) but the templates used in otherwork are manually encoded.Sentence selection has also been used for ques-tion answering and query-focused summarization.Some approaches focus on selection of relevantsentences using probabilistic approaches (Daum?eIII and Marcu, 2005; Conroy et al, 2006), semi-supervised learning (Wang et al, 2011) and graph-based methods (Erkan and Radev, 2004; Otter-bacher et al, 2005).
Yet others use a mixture oftargeted and data-driven methods for a pure sen-tence selection system (Blair-Goldensohn et al,2003; Weischedel et al, 2004; Schiffman et al,2001).
In our approach, we target both relevanceand variety of expression, driving content by se-lecting sentences that match company/entity pairsand inducing multiple patterns of expression.
Sen-tence selection has also been used in prior work ongenerating Wikipedia overall articles (Sauper andBarzilay, 2009).
Their focus is more on learningdomain-specific templates that control the topicstructure of an overview, a much longer text thanwe generate.3 Targeted GenerationThe TD system uses a development set of 100S&P500 companies along with their Wikipediaarticles and DBPedia entries to form templates.For each RDF relation with the company as thesubject, it identifies all sentences in the corre-sponding article containing the entities in the re-lation.
The specific entities are then replaced withtheir relation to create a template.
For example,?Microsoft was founded by Bill Gates and PaulAllen?
is converted to ??company?
was foundedby ?founder?,?
with conjoined entities collapsedinto one slot.
Many possible templates are created,some of which contain multiple relations (e.g.,?
?company?, located in ?location?, was foundedby ?founder??).
In this way the system learns howWikipedia articles express relations between thecompany and its key entities (founders, headquar-ters, products, etc).At generation time, we fill the template slotswith the corresponding information from the RDFentries of the target company.
Conjunctions areinserted when slots are filled by multiple entities.Continuing with our example, we might now pro-duce the sentence ?Palantir was founded by PeterThiel, Alex Karp, Joe Lonsdale, Stephen Cohen,and Nathan Gettings?
for target company Palan-tir.
Preliminary results showed that this methodwas not adequate - the data for the target com-pany often lacked some of the entities needed tofill the templates.
Without those entities the sen-tence could not be generated.
As Wikipedia sen-tences tend to have multiple relations each (highinformation density), many sentences containingimportant, relevant facts were discarded due tophrases that mentioned lesser facts we did not havethe data to replace.
We therefore added a post-processing step to remove, if possible, any phrases244from the sentence that could not be filled; other-wise, the sentence is discarded.This process yields many potential sentencesfor each relation, of which we only want to choosethe best.
We cluster the newly generated sen-tences by relation and score each cluster.
Sen-tences are scored according to how much informa-tion about the target company they contain (num-ber of replaced relations).
Shorter sentences arealso weighted more as they are less likely to con-tain extraneous information, and sentences withmore post-processing are scored lower.
The high-est scored sentence for each relation type is addedto the description as those sentences are the mostinformative, relevant, and most likely to be gram-matically correct.4 Data-Driven GenerationThe DD method produces descriptions using sen-tences taken from the web.
Like the TD approach,it aims to produce sentences realizing relations be-tween the input company and other entities.
It usesa bootstrapping approach (Agichtein and Gravano,2000) to learn patterns for expressing the relations.It starts with a seed set of company/entity pairs,representing a small subset of the desired rela-tions, but unlike previous approaches, can gener-ate additional relations as it goes.Patterns are generated by reading text from theweb and extracting those sentences which con-tain pairs in the seed set.
The pair?s entitiesare replaced with placeholder tags denoting thetype of the entity, while the words around themform the pattern (the words between the tags areselected as well as words to the left and rightof the tags).
Each pattern thus has the form??L??T1??M??T2??R?,?
where L, M, and R are re-spectively the words to the left of, between, andto the right of the entities.
T1 is the type of thefirst entity, and T2 the type of the second.
Likethe TD algorithm, this is essentially a templatebased approach, but the templates in this case arenot aligned to a relation between the entity and thecompany; only the type of entity (person, location,organization, etc) is captured by the tag.New entity pairs are generated by matching thelearned patterns against web text.
A sentence isconsidered to match a pattern if it has the sameentity types in the same order and its L, M, and Rwords fuzzy match the corresponding words in thepattern.1The entities are therefore assumed to berelated since they are expressed in the same wayas the seed pair.
Unlike the TD approach, the ac-tual relationship between the entities is unknown(since the only data we use is the web text, not thestructured RDF data); all we need to know here isthat a relationship exists.We alternate learning the patterns and generat-ing entity pairs over our development set of 100companies.
We then take all the learned patternsand find matching sentences in the Bing search re-sults for each company in the set of target compa-nies.2Sentences that match any of the patterns areselected and ranked by number of matches (morematches means greater probability of strong rela-tion) before being added to the description.4.1 Pruning and OrderingAfter selecting the sentences for the description,we perform a post-processing step that removesnoise and redundancy.
To address redundancy, weremove those sentences which were conveyed pre-viously in the description using exactly the samewording.
Thus, sentences which are equal to orsubsets of other sentences are removed.
We alsoremove sentences that come from news stories;analysis of our results on the development set indi-cated that news stories rarely contain informationthat is relevant to a typical Wikipedia description.To do this we use regular expressions to capturecommon newswire patterns (e.g., [CITY, STATE:sentence]).
Finally, we remove incomplete sen-tences ending in ?.
.
.
?, which sometimes appearon websites which themselves contain summaries.We order the selected sentences using a scoringmethod that rewards sentences based on how theyrefer to the company.
Sentences that begin withthe full company name get a starting score of 25,sentences that begin with a partial company namestart with a score of 15, and sentences that do notcontain the company name at all start at -15 (ifthey contain the company name in the middle ofthe sentece, they start at 0).
Then, 10 points areadded to the score for each keyword in the sen-tence (keywords were selected from the most pop-ulous DBPedia predicates where the subject is acompany).
This scoring algorithm was tuned onthe development set.
The final output is ordered in1We use a threshold on the cosine similarity of the textsto determine whether they match.2We excluded Wikipedia results to better simulate thecase of companies which do not have a Wikipedia page245descending order of scores.5 Hybrid systemIn addition to the two approaches separately, wealso generated hybrid output from a combinationof the two.
In this approach, we start with the DDoutput; if (after pruning) it has fewer than threesentences, we add the TD output and re-order.The hybrid approach essentially supplementsthe large, more noisy web content of the DD out-put with the small, high-quality but less diverseTD output.
For companies that are not consumer-facing or are relatively young, and thus have a rel-atively low web presence - our target population -this can significantly impact the description.6 ExperimentsTo evaluate our approach, we compare the threeversions of our output - generated by the TD, DD,and hybrid approach - against multi-documentsummaries generated (from the same search re-sults used by our DD approach) by TextRank (Mi-halcea and Tarau, 2004).
For each one of the ap-proaches as well as the baseline, we generated de-scriptions for all companies that were part of theS&P500 as of January 2016.
We used our devel-opment set of 100 companies for tuning, and theevaluation results are based on the remaining 400.We conducted two types of experiments.
Thefirst is an automated evaluation, where we usethe METEOR score (Lavie and Agarwal, 2007)between the description generated by one of ourapproaches or by the baseline and the first sec-tion of the Wikipedia article for the company.In Wikipedia articles, the first section typicallyserves as an introduction or overview of the mostimportant information about the company.
ME-TEOR scores capture the content overlap betweenthe generated description and the Wikipedia text.To avoid bias from different text sizes, we set thesame size limit for all descriptions when compar-ing them.
We experimented with three settings:150 words, 500 words, and no size limit.In addition, we conducted a crowd-sourcedevaluation on the CrowdFlower platform.
In thisevaluation, we presented human annotators withtwo descriptions for the same company, one de-scribed by our approach and one by the baseline,in random order.
The annotators were then askedto choose which of the two descriptions is a betteroverview of the company in question (they were150 words 500 words no limitTextRank 13.7 12.8 6.3DD 15.0 14.5 14.0TD 11.3 11.3 11.3Hybrid 15.5 14.6 14.2Table 1: First experiment results: average ME-TEOR scores for various size limits% best Avg.
scoreTextRank 25.79 2.82Hybrid 74.21 3.81Table 2: Second experiment results: % of compa-nies for which the approach was chosen as best bythe human annotators, and average scores givenprovided a link to the company?s Wikipedia pagefor reference) and give a score on a 1-5 scale toeach description.
For quality assurance, each pairof descriptions was processed by three annota-tors, and we only included in the results instanceswhere all three agreed.
Those constituted 44% ofthe instances.
In this evaluation we only used thehybrid version, and we limited the length of boththe baseline and our output to 150 words to reducebias from a difference in lengths and keep the de-scriptions reasonably short for the annotators.7 ResultsThe results of the automated evaluation are shownin Table 1.
Our DD system achieves higher ME-TEOR scores than the TextRank baseline under allsize variations, while TD by itself is worse in mostcases.
In all cases the combined approach achievesa better result than the DD system by itself.The results of the human evaluation are shownin Table 2.
Here the advantage of our approachbecomes much more visible: we clearly beat thebaseline both in terms of how often the annotatorschose our output to be better (almost 75% of thetimes) and in terms of the average score given toour descriptions (3.81 on a 1?
5 point scale).All results are statistically significant, but thedifference in magnitude between the results ofthe two experiments are striking: we believe thatwhile the TextRank summarizer extracts sentenceswhich are topically relevant and thus achieve re-sults close to ours in terms of METEOR, the morestructured entity-focused approach we presenthere is able to extract content that seems muchmore reasonable to humans as a general descrip-tion.
One example is shown in Figure 1.Right from the start, we see that our system out-246performs TextRank.
Our first sentence introducesthe company and provides a critical piece of his-tory about it, while TextRank does not even im-mediately name it.
The hybrid generation outputhas a more structured output, going from the ori-gins of the company via merger, to its board, andfinally its products.
TextRank?s output, in compar-ison, focuses on the employee experience and onlymentions products at the very end.
Our system ismuch more suitable for a short description of thecompany for someone unfamilar with it.TextRank: The company also emphasizes stretch assign-ments and on-the-job learning for development, while its for-mal training programs include a Masters in the Business ofActivision(or ?MBActivision?)
program that gives employ-ees a deep look at company operations and how its games aremade, from idea to development to store shelves.
How easyis it to talk with managers and get the information I need?Will managers listen to my input?
At Activision Blizzard, 78percent of employees say they often or almost always experi-ence a free and transparent exchange of ideas and informationwithin the organization.
Gaming is a part of day-to-day life atActivision Blizzard, and the company often organizes internaltournaments for Call of Duty, Hearthstone: Heroes of War-craft, Destiny, Skylanders and other titles.
What inspires em-ployees?
company spirit here Do people stand by their teams?work What impact do people have outside the organization.Hybrid: Activision Blizzard was formed in 2007 from amerger between Activision and Vivendi Games (as well asBlizzard Entertainment, which had already been a division ofVivendi Games.)
Upon merger, Activision Blizzard?s boardof directors initially formed of eleven members: six directorsdesignated by Vivendi, two Activision management directorsand three independent directors who currently serve on Ac-tivision?s board of directors.
It?s comprised of Blizzard En-tertainment, best known for blockbuster hits including Worldof Warcraft, Hearthstone: Heroes of Warcraft, and the War-craft, StarCraft, and Diablo franchises, and Activision Pub-lishing, whose development studios (including Infinity Ward,Toys for Bob, Sledgehammer Games, and Treyarch, to namejust a few) create blockbusters like Call of Duty, Skylanders,Guitar Hero, and Destiny.Figure 1: Descriptions for Activision Blizzard8 ConclusionWe described two approaches to generating com-pany descriptions as well as a hybrid approach.We showed that our output is overwhelmingly pre-ferred by human readers, and is more similar toWikipedia introductions, than the output of a state-of-the-art summarization algorithm.These complementary methods each have theiradvantages and disadvantages: the TD approachensures that typical expressions in Wikipedia com-pany descriptions - known to be about the fun-damental relations of a company - will occur inthe generated output.
However, since it modifiesthem, it risks generating ungrammatical sentencesor sentences which contain information about an-other company.
The latter can occur because thesentence is uniquely tied to the original.
For in-stance, the following Wikipedia sentence fragment?
?Microsoft is the world?s largest software makerby revenue?
- is a useful insight about the com-pany, but our system would not be able to correctlymodify that to fit any other company.In contrast, by selecting sentences from the webabout the given company, the DD approach en-sures that the resulting description will be bothgrammatical and relevant.
It also results in a widervariety of expressions and a greater number of sen-tences.
However, it can include nonessential factsthat appear in a variety of different web venues.It is not surprising, therefore, that the hybrid ap-proach performs better than either by itself.While in this paper we focus on company de-scriptions, the system can be adapted to generatedescriptions for other entities (e.g.
Persons, Prod-ucts) by updating the seed datasets for both ap-proaches (to reflect the important facts for the de-sired descriptions) and retuning for best accuracy.AcknowledgmentThis research was supported in part by an IBMShared University Research grant provided to theComputer Science Department of Columbia Uni-versity.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snow-ball: Extracting relations from large plain-text col-lections.
In Proceedings of the fifth ACM conferenceon Digital libraries, pages 85?94.
ACM.Gabor Angeli, Percy Liang, and Dan Klein.
2010.
Asimple domain-independent probabilistic approachto generation.
In Proceedings of the 2010 Confer-ence on Empirical Methods in Natural LanguageProcessing, EMNLP ?10, pages 502?512, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Or Biran and Kathleen McKeown.
2015.
Discourseplanning with an n-gram model of relations.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 1973?1977, Lisbon, Portugal, September.
Association forComputational Linguistics.Sasha Blair-Goldensohn, Kathleen R. McKeown, andAnd rew Hazen Schlaikjer.
2003.
Defscriber: a hy-247brid system for definitional qa.
In SIGIR ?03: Pro-ceedings of the 26th annual international ACM SI-GIR conference on Research and development in in-formaion retrieval, pages 462?462.Nadjet Bouayad-Agha, Gerard Casamayor, and LeoWanner.
2011.
Content selection from an ontology-based knowledge base for the generation of footballsummaries.
In Proceedings of the 13th EuropeanWorkshop on Natural Language Generation, ENLG?11, pages 72?81, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.John Conroy, Judith Schlesinger, and Dianne O?Leary.2006.
Topic-focused multi-document summariza-tion using an approximate or acle score.
In Proceed-ings of ACL.Hal Daum?e III and Daniel Marcu.
2005.
Bayesianmulti-document summarization at mse.
In Proceed-ings of the Workshop on Multilingual Summariza-tion Eva luation (MSE), Ann Arbor, MI, June 29.Noemie Elhadad and Kathleen R. Mckeown.
2001.Towards generating patient specific summaries ofmedical articles.
In In Proceedings of NAACL-2001Workshop Automatic.Michael Elhadad.
1991.
FUF: The Universal UnifierUser Manual ; Version 5.0.
Department of Com-puter Science, Columbia University.G?unes?
Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based centrality as salience in text summa-rization.
Journal of Artificial Intelligence Research(JAIR).Elena Filatova and John Prager.
2005.
Tell mewhat you do and i?ll tell you what you are: Learn-ing occupation-related activities for biographies.
InProceedings of the Conference on Human Lan-guage Technology and Empirical Methods in Natu-ral Language Processing, HLT ?05, pages 113?120,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Dimitra Gkatzia, Helen F. Hastie, and Oliver Lemon.2014.
Finding middle ground?
multi-objective nat-ural language generation from time-series data.
InGosse Bouma and Yannick Parmentier, editors, Pro-ceedings of the 14th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, EACL 2014, April 26-30, 2014, Gothen-burg, Sweden, pages 210?214.
The Association forComputer Linguistics.Ravi Kondadadi, Blake Howald, and Frank Schilder.2013.
A statistical nlg framework for aggregatedplanning and realization.
In ACL (1), pages 1406?1415.
The Association for Computer Linguistics.Alon Lavie and Abhaya Agarwal.
2007.
Meteor: Anautomatic metric for mt evaluation with high levelsof correlation with human judgments.
In Proceed-ings of the Second Workshop on Statistical MachineTranslation, StatMT ?07, pages 228?231.
Associa-tion for Computational Linguistics.Christian M.I.M.
Matthiessen and John A. Bateman.1991.
Text generation and systemic-functional lin-guistics: experiences from english and japanese.Rada Mihalcea and Paul Tarau.
2004.
Textrank:Bringing order into texts.
In Conference on Em-pirical Methods in Natural Language Processing,Barcelona, Spain.Jahna Otterbacher, Gunes Erkan, and Dragomir R.Radev.
2005.
Using random walks for question-focused sentence retrieval.
In Proceedings of HLT-EMNLP.Ehud Reiter and Robert Dale.
1997.
Building appliednatural language generation systems.
Nat.
Lang.Eng., 3(1):57?87, March.Christina Sauper and Regina Barzilay.
2009.
Auto-matically generating wikipedia articles: A structure-aware approach.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 1 - Volume1, ACL ?09, pages 208?216, Stroudsburg, PA, USA.Association for Computational Linguistics.B.
Schiffman, Inderjeet.
Mani, and K. Concepcion.2001.
Producing biographical summaries: Combin-ing linguistic knowledge with corpus statistics.
InProceedings of the 39th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL-EACL2001), Toulouse, France, July.William Yang Wang, Kapil Thadani, and KathleenMcKeown.
2011.
Identifying event descriptionsusing co-training with online news su mmaries.In Proceedings of IJNLP, Chiang-Mai, Thailand,November.Ralph M. Weischedel, Jinxi Xu, and Ana Licuanan.2004.
A hybrid approach to answering biographicalquestions.
In Mark T. Maybury, editor, New Direc-tions in Question Answering, pages 59?70.
AAAIPress.Michael White.
2014.
Towards surface realiza-tion with ccgs induced from dependencies.
InProceedings of the 8th International Natural Lan-guage Generation Conference (INLG), pages 147?151, Philadelphia, Pennsylvania, U.S.A., June.
As-sociation for Computational Linguistics.248
