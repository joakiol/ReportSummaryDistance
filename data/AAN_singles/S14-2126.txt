Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 704?710,Dublin, Ireland, August 23-24, 2014.UKPDIPF: A Lexical Semantic Approach to Sentiment PolarityPrediction in Twitter DataLucie Flekova?
?, Oliver Ferschke?
?and Iryna Gurevych??
?Ubiquitous Knowledge Processing Lab (UKP-TUDA)Computer Science Department, Technische Universit?at Darmstadt?Ubiquitous Knowledge Processing Lab (UKP-DIPF)German Institute for Educational Researchhttp://www.ukp.tu-darmstadt.deAbstractWe present a sentiment classification sys-tem that participated in the SemEval 2014shared task on sentiment analysis in Twit-ter.
Our system expands tokens in a tweetwith semantically similar expressions us-ing a large novel distributional thesaurusand calculates the semantic relatedness ofthe expanded tweets to word lists repre-senting positive and negative sentiment.This approach helps to assess the polarityof tweets that do not directly contain po-larity cues.
Moreover, we incorporate syn-tactic, lexical and surface sentiment fea-tures.
On the message level, our systemachieved the 8th place in terms of macro-averaged F-score among 50 systems, withparticularly good performance on the Life-Journal corpus (F1=71.92) and the Twittersarcasm (F1=54.59) dataset.
On the ex-pression level, our system ranked 14 outof 27 systems, based on macro-averagedF-score.1 IntroductionMicroblogging sites, such as Twitter, have becomean important source of information about currentevents.
The fact that users write about their ex-periences, often directly during or shortly afteran event, contributes to the high level of emo-tions in many such messages.
Being able to auto-matically and reliably evaluate these emotions incontext of a specific event or a product would behighly beneficial not only in marketing (Jansen etal., 2009) or public relations, but also in politicalsciences (O?Connor et al., 2010), disaster manage-This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/ment, stock market analysis (Bollen et al., 2011)or the health sector (Culotta, 2010).Due to its large number of applications, senti-ment analysis on Twitter is a very popular task.Challenges arise both from the character of thetask and from the language specifics of Twit-ter messages.
Messages are normally very shortand informal, frequently using slang, alternativespelling, neologism and links, and mostly ignor-ing the punctuation.Our experiments have been carried out as partof the SemEval 2014 Task 9 - Sentiment Anal-ysis on Twitter (Rosenthal et al., 2014), a rerunof a SemEval-2013 Task 2 (Nakov et al., 2013).The datasets are thus described in detail in theoverview papers.
The rerun uses the same train-ing and development data, but new test data fromTwitter and a ?surprise domain?.
The task con-sists of two subtasks: an expression-level subtask(Subtask A) and a message-level subtask (SubtaskB).
In subtask A, each tweet in a corpus containeda marked instance of a word or phrase.
The goalis to determine whether that instance is positive,negative or neutral in that context.
In subtask B,the goal is to classify whether the entire messageis of positive, negative, or neutral sentiment.
Formessages conveying both a positive and negativesentiment, the stronger one should be chosen.The key components of our system are the sen-timent polarity lexicons.
In contrast to previousapproaches, we do not only count exact lexiconhits, but also calculate explicit semantic related-ness (Gabrilovich and Markovitch, 2007) betweenthe tweet and the sentiment list, benefiting fromresources such as Wiktionary and WordNet.
Ontop of that, we expand content words (adjectives,adverbs, nouns and verbs) in the tweet with sim-ilar words, which we derive from a novel corpusof more than 80 million English Tweets gatheredby the Language Technology group1at TU Darm-1http://www.lt.informatik.tu-darmstadt.de704stadt.2 Experimental setupOur experimental setup is based on an open-sourcetext classification framework DKPro TC2(Daxen-berger et al., 2014), which allows to combine NLPpipelines into a configurable and modular systemfor preprocessing, feature extraction and classifi-cation.
We use the unit classification mode ofDKPro TC for Subtask A and the document clas-sification mode for Subtask B.2.1 PreprocessingWe customized the message reader for Subtask Bto ignore the first part of the tweet when the wordbut is found.
This approach helps to reduce themisleading positive hits when a negative messageis introduced positively (It?d be good, but).For preprocessing the data, we use componentsfrom DKPro Core3.
Preprocessing is the samefor subtasks A and B, with the only differencethat in the subtask A the target expression is addi-tionally annotated as text classification unit, whilethe rest of the tweet is considered to be a doc-ument context.
We first segment the data withthe Stanford Segmenter4, apply the Stanford POSTagger with a Twitter-trained model (Derczynskiet al., 2013), and subsequently apply the Stan-ford Lemmatizer4, TreeTagger Chunker (Schmid,1994), Stanford Named Entity Recognizer (Finkelet al., 2005) and Stanford Parser (Klein and Man-ning, 2003) to each tweet.
After this linguistic pre-processing, the token segmentation of the Stanfordtools is removed and overwritten by the ArkTweetTagger (Gimpel et al., 2011), which is more suit-able for recognizing hashtags and smileys as oneparticular token.
Finally, we expand the tweet andproceed to feature extraction as described in detailin Section 3.2.2 ClassificationWe trained our system on the provided trainingdata only, excluding the dev data.
We use clas-sifiers from the WEKA (Hall et al., 2009) toolkit,which are integrated in the DKPro TC framework.Our final configuration consists of a SVM-SMOclassifier with a gaussian kernel.
The optimal hy-perparameters have been experimentally derived2http://code.google.com/p/dkpro-tc3http://code.google.com/p/dkpro-core-asl4http://nlp.stanford.edu/software/corenlp.shtmland finally set to C=1 and G=0.01.
The resultingmodel was wrapped in a cost sensitive meta classi-fier from the WEKA toolkit with the error costs setto reflect the class imbalance in the training set.3 Features usedWe now describe the features used in our exper-iments.
For Subtask A (contextual polarity), weextracted each feature twice - once on the tweetlevel and once on the focus expression level.
Onlyn-gram features were extracted solely from the ex-pressions.
For Subtask B (tweet polarity), we ex-tracted features on tweet level only.
In both cases,we use the Information Gain feature selection ap-proach in WEKA to rank the features and prunethe feature space with a threshold of T=0.005.3.1 Lexical featuresAs a basis for our similarity and expansion ex-periments (sections 3.4 and 3.5), we use the bi-nary sentiment polarity lexicon by Liu (2012) aug-mented with the smiley polarity lexicon by Beckeret al.
(2013) and an additional swear word list5[further as Liuaugmented].
We selected this aug-mented lexicon for two reasons: firstly, it was thehighest ranked lexical feature on the development-test and crossvalidation experiments, secondly itconsists of two plain word lists and therefore doesnot introduce another complexity dimension foradvanced feature calculations.We further measure lexicon hits normalized pernumber of tweet tokens for the following lexicons:Pennebaker?s Linguistic Inquiry and Word Count(LIWC) (Pennebaker et al., 2001), the NRC Emo-tion Lexicon (Mohammad and Turney, 2013), theNRC Hashtag Emotion Lexicon (Mohammad etal., 2013) and the Sentiment140 lexicon (Moham-mad et al., 2013).
We use an additional lexiconof positive, negative, very positive and very nega-tive words, diminishers, intensifiers and negationscomposed by Steinberger et al.
(2012), where wecalculate the polarity score as described in theirpaper.In a complementary set of features we combineeach of the lexicons above with a list of weightedintensifying expressions as published by Brooke(2009).
The intensity of any polar word found inany of the emotion lexicons used is intensified ordiminished by a given weight if an intensifier (a5based on http://www.youswear.com705bit, very, slightly...) is found within the precedingthree tokens.Additionally, we record the overall counts oflexicon hits for positive words, negative words andthe difference of the two.
In one set of featureswe consider only lexicons clearly meant for binarypolarity, while a second set of features also in-cludes other emotions, such as fear or anger, fromthe NRC and the LIWC corpora.3.2 NegationWe handle negation in two ways.
On the expres-sion level (Subtask A) we rely on the negationdependency tag provided by the Stanford Depen-dency Parser.
This one captures verb negationsrather precisely and thus helps to handle emotionalverb expressions such as like vs don?t like.
On thetweet level (all features of Subtask B and entire-tweet-level features of Subtask A) we adopt theapproach of Pang et al.
(2002), considering as anegation context any sequence of tokens betweena negation expression and the end of a sentencesegment as annotated by the Stanford Segmenter.The negation expressions (don?t, can?t...) are rep-resented by the list of invertors from Steinberger?slexicon (Steinberger et al., 2012).
We first assignpolarity score to each word in the tweet based onthe lexicon hits and then revert it for the words ly-ing in the negation context.
This approach is morerobust than the one of the dependency governorbut is error-prone in the area of overlapping (cas-caded) negation contexts.3.3 N-gram featuresWe extract the 5,000 most frequent word uni-grams, bigrams and trigrams cleaned with theSnowball stopword list6as well as the sameamount of skip-n-grams and character trigrams.These are extracted separately on the target ex-pression level for subtask A and on documentlevel for subtask B.
On the syntactic level, wemonitor the most frequent 5,000 part-of-speechngrams with the size up to part-of-speech quadru-ples.
Additionally, as an approximation for ex-ploiting the key message of the sentence, we ex-tract from the tweets a verb chunk and its left andright neighboring noun chunks, obtaining combi-nations such as we-go-cinema.
The 1,000 mostfrequent chunk triples are then used as featuressimilarly to ngrams.6http://snowball.tartarus.org/algorithms/english/stop.txtWord Score Word (continued) Scoreawesome 1,000 fun 60amazing 194 sexy 59great 148 cold 59cool 104 crazy 57good 96 fantastic 56best 93 bored 55beautiful 93 excited 54nice 87 true 53funny 84 stupid 53cute 81 gr8 52perfect 70 entertaining 52wonderful 67 favorite 52lovely 66 talented 49tired 65 other 49annoying 63 depressing 48Great 63 flawless 48new 62 inspiring 47hilarious 62 incredible 46bad 61 complicated 46hot 61 gorgeous 45Table 1: Unsupervised expansion of ?awesome?3.4 Tweet expansionWe expanded the content words in a tweet, i.e.nouns, verbs, adjectives and adverbs, with sim-ilar words from a word similarity thesaurus thatwas computed on 80 million English tweets from2012 using the JoBim contextual semantics frame-work (Biemann and Riedl, 2013).
Table 1 showsan example for a lexical expansion of the wordawesome.
The score was computed using left andright neighbor bigram features for the holing oper-ation.
The value hence shows how often the wordappeared in the same left and right context as theoriginal word.
The upper limit of the score is setto 1,000.We then match the expanded tweet against theLiuaugmentedpositive and negative lexicons.
Weassign to the lexicon hits of the expanded wordstheir (contextual similarity) expansion score, us-ing a score of 1,000 as an anchor-value for theoriginal tweet, setting an expansion cut at 100.The overall tweet score is then normalized by thesum of word expansion scores.3.5 Semantic similarityTweet messages are short and each emotionalword is very valuable for the task, even when itmay not be present in a specific lexicon.
There-fore, we calculate a semantic relatedness scorebetween the tweet and the positive or negativeword list.
We use the ESA similarity measure(Gabrilovich and Markovitch, 2007) as imple-mented in the DKPro similarity software pack-706age (B?ar et al., 2013), calculated on English Wik-tionary and WordNet as two separate conceptspaces.
The ESA vectors are freely available7.This way we obtain in total six features: sim(orig-inal tweet word list, positive word list), sim(orig-inal tweet word list, negative word list), differ-ence between the two, sim(expanded tweet wordlist, positive word list), sim(expanded tweet wordlist, negative word list) and difference between thetwo.
Our SemEval run was submitted using Word-Net vectors mainly for the shorter computationtime and lower memory requirements.
However,in our later experiments Wiktionary performedbetter.
We presume this can be due to a bettercoverage for the Twitter corpus, although detailedanalysis of this aspect is yet to be performed.3.6 Other featuresPak and Paroubek (2010) pointed out a relationbetween the presence of different part-of-speechtypes and sentiment polarity.
We measure theratio of each part-of-speech type to each chunk.We furthermore count the occurrences of thedependency tag for negation.
We use the StanfordNamed Entity Recognizer to count occurrenceof persons, organizations and locations in thetweet.
Additionaly, beside basic surface metrics,such as the number of tokens, characters andsentences, we measure the number of elon-gated words (such as coool) in a tweet, ratioof sentences ending with exclamation, ratio ofquestions and number of positive and negativesmileys and their proportion.
We capture thesmileys with the following two regular expres-sions for positive, respectively negative ones:[<>]?[:;=8][-o*?]?[)]dDpPxXoO0*}],[<>]?[:;=8][-o*?]?[([/:{|].
We alsoseparately measure the sentiment of smileys atthe end of the tweet body, i.e.
followed only by ahashtag, hyperlink or nothing.4 ResultsIn Subtask A, our system achieved an averagedF-score of 81.42 on the LiveJournal corpus and79.67 on the Twitter 2014 corpus.
The highestscores achieved in related work were 85.61 and86.63 respectively.
For subtask B, we scored 71.92on LifeJournal and 63.77 on Twitter 2014, whilethe highest F-scores reported by related work were74.84 and 70.96.7https://code.google.com/p/dkpro-similarity-asl/downloads/listFeatures with the highest Information Gainwere the ones based on Liuaugmented.
Adding theweighted intensifiers of Brooke to the sentimentlexicons did not outperform the simple lexiconlookup.
They were followed by features derivedfrom the lexicons of Steinberger, which includesinvertors, intensifiers and four polarity levels ofwords.
On the other hand, adding the weightedintensifiers of Brooke to lexicons did not outper-form the simple lexicon lookup.
Overall, lexicon-based features contributed to the highest perfor-mance gain, as shown in Table 3.
The negationapproach based on the Stanford dependency parserwas the most helpful, although it tripled the run-time.
Using the simpler negation context as sug-gested in Pang et al.
(2002) performed still on av-erage better than using none.When using WordNet, semantic similarity tolexicons did not outperform direct lexicon hits.Usage of Wiktionary instead lead to major im-provement (Table 3), unfortunately after the Se-mEval challenge.Tweet expansion appears to improve the clas-sification performance, however the threshold of100 that we used in our setup was chosed tooconservatively, expanding mainly stopwords withother stopwords or words with their spelling al-ternatives, resulting in a noisy, little valuable fea-ture (expansion full in Table 3).
Settingup the threshold to 50 and cleaning up both thetweet and the expansion with Snowball stopwordlist (expansion clean in Table 3), the perfor-mance increased remarkably.Amongst other prominent features were parts oflexicons such as LIWC Positive emotions, LIWCAffect, LIWC Negative emotions, NRC Joy, NRCAnger and NRC Disgust.
Informative were alsothe proportions of nouns, verbs and adverbs, theexclamation ratio or number of positive and nega-tive smileys at the end of the tweet.Feature(s) ?F1Twitter2014 ?F1LifeJournalSimilarity Wikt.
0.56 3.65Similarity WN 0.0 2.61Expansion full 0.0 0.0Expansion clean 0.59 3.82Lexical negation 0.24 0.13N-gram features 0.30 0.32Lexicon-based f. 7.85 4.74Table 3: Performance increase where featureadded to the full setup707# Gold label Prediction Message1 negative positive Your plans of attending the Great Yorkshire Show may have been washed out becauseof the weather, so how about...2 neutral positive sitting here with my belt in jean shorts watching Cena win his first title.I think we tie for 1st my friend xD3 neutral positive saw your LJ post ... yay for Aussies ;)4 positive negative haha , that sucks , because the drumline will be just fine5 positive negative ...woah, Deezer.
Babel only came out on Monday, can you leave it up for longer than a dayto give slow people like me a chance?6 positive negative Yeah so much has changed for the 6th.
Lots of combat fighting.
And inventory is different.7 positive negative just finish doing it and tomorrow I?m going to the celtics game and don?t fucking say?thanks for the invite?
it?s annoying8 positive negative Haha... Yup hopefully we will lose a few kg by mon.
after hip hop can go orchard and weigh9 positive negative U r just like my friends?
I made them feel warm, happy, then make them angry and they cry?Finally they left me?
Will u leave 2?
I hope not.
Really hope so.Table 2: Examples of misclassified messages5 Error analysisTable 2 lists a sample of misclassified messages.The majority of errors resulted from misclassify-ing neutral tweets as emotionally charged.
Thiswas partly caused by the usage of emoticons andexpressions such as haha in a neutral context, suchas in examples 2 and 3.
Other errors were cause bylexicon hits of proper nouns (example 1), or by us-ing negative words and swearwords in overall pos-itive tweet (examples 4, 7, 9).
Some tweets con-tained domain specific vocabulary that would hitthe negative lexicon, e.g., discussing fighting andviolence in computer games would, in contrast toother topic domains, usually have positive polar-ity (example 6).
Similar domain-specific polaritydistinction could be applied to certain verbs, e.g.,lose weight vs. lose a game (example 8).Another challenge for the system was the non-standard language in twitter with a large number ofspelling variants, which was only partly capturedby the emotion lexicons tailored for this domain.A twitter-specific lemmatizer, which would groupall variations of a misspelled word into one, couldhelp to improve the performance.The length of the negation context window doesnot suit all purposes.
Also double negations suchas I don?t think he couldn?t... can easily misdirectthe polarity score.6 ConclusionWe presented a sentiment classification systemthat can be used on both message level and ex-pression level with only small changes in theframework configuration.
We employed a con-textual similarity thesaurus for the lexical expan-sion of the messages.
The expansion was notefficient without an extensive stopword cleaning,overweighting more common words and introduc-ing noise.
Utilizing the semantic similarity oftweets to lexicons instead of a direct match im-proves the score only with certain lexicons, possi-bly dependent on the coverage.
Negation by de-pendency parsing was more beneficial to the clas-sifier than the negation by keyword span anno-tation.
Naive combination of sentiment lexiconswas not more helpful than using individual onesseparately.
Among the common source of errorswere laughing signs used in neutral messages andswearing used in positive messages.
Even withinTwitter, same words can have different polarity indifferent domains (lose weight, lose game, gamewith nice violent fights...).
Deeper semantic in-sights are necessary to distinguish between polarwords in context.7 AcknowledgementThis work has been supported by the VolkswagenFoundation as part of the Lichtenberg-Professorship Program under grant No.
I/82806.We warmly thank Chris Biemann, Martin Riedland Eugen Ruppert of the Language Technologygroup at TU Darmstadt for providing us with theTwitter-based distributional thesaurus.ReferencesDaniel B?ar, Torsten Zesch, and Iryna Gurevych.
2013.Dkpro similarity: An open source framework fortext similarity.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics: System Demonstrations, pages 121?126,Sofia, Bulgaria.Lee Becker, George Erhart, David Skiba, and Valentine708Matula.
2013.
Avaya: Sentiment analysis on twit-ter with self-training and polarity lexicon expansion.Atlanta, Georgia, USA, page 333.Chris Biemann and Martin Riedl.
2013.
Text: Nowin 2d!
a framework for lexical expansion with con-textual similarity.
Journal of Language Modelling,1(1):55?95.Johan Bollen, Huina Mao, and Xiaojun Zeng.
2011.Twitter mood predicts the stock market.
Journal ofComputational Science, 2(1):1 ?
8.Julian Brooke.
2009.
A semantic approach to auto-mated text sentiment analysis.Aron Culotta.
2010.
Towards detecting influenza epi-demics by analyzing twitter messages.
In Proceed-ings of the First Workshop on Social Media Analyt-ics, pages 115?122, New York, NY, USA.Johannes Daxenberger, Oliver Ferschke, IrynaGurevych, and Torsten Zesch.
2014.
Dkpro tc:A java-based framework for supervised learningexperiments on textual data.
In Proceedings ofthe 52nd Annual Meeting of the Association forComputational Linguistics.
System Demonstrations,page (to appear), Baltimore, MD, USA.Leon Derczynski, Alan Ritter, Sam Clark, and KalinaBontcheva.
2013.
Twitter part-of-speech taggingfor all: Overcoming sparse and noisy data.
In Pro-ceedings of the International Conference on RecentAdvances in Natural Language Processing, Hissar,Bulgaria.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43nd Annual Meet-ing of the Association for Computational Linguistics(ACL 2005), pages 363?370.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In Proceedingsof the 20th International Joint Conference on Arti-ficial Intelligence, volume 7, pages 1606?1611, Hy-derabad, India.Kevin Gimpel, Nathan Schneider, Brendan O?Con-nor, Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flani-gan, and Noah A Smith.
2011.
Part-of-speech tag-ging for twitter: Annotation, features, and experi-ments.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies: short papers-Volume 2, pages 42?47.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The weka data mining software: an up-date.
ACM SIGKDD Explorations Newsletter,11(1):10?18.Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Ab-dur Chowdury.
2009.
Twitter power: Tweets aselectronic word of mouth.
Journal of the Ameri-can Society for Information Science and Technology,60(11):2169?2188.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.Saif M Mohammad and Peter D Turney.
2013.
Crowd-sourcing a word?emotion association lexicon.
Com-putational Intelligence, 29(3):436?465.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets.
In Proceedingsof the seventh international workshop on Seman-tic Evaluation Exercises (SemEval-2013), Atlanta,Georgia, USA.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wil-son.
2013.
Semeval-2013 task 2: Sentiment anal-ysis in twitter.
In Second Joint Conference on Lex-ical and Computational Semantics (*SEM), Volume2: Proceedings of the Seventh International Work-shop on Semantic Evaluation (SemEval 2013), pages312?320, Atlanta, Georgia, USA.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R Routledge, and Noah A Smith.
2010.From tweets to polls: Linking text sentiment topublic opinion time series.
In Fourth InternationalAAAI Conference on Weblogs and Social Media,pages 122?129.Alexander Pak and Patrick Paroubek.
2010.
Twit-ter as a corpus for sentiment analysis and opin-ion mining.
In Nicoletta Calzolari (ConferenceChair), Khalid Choukri, Bente Maegaard, JosephMariani, Jan Odijk, Stelios Piperidis, Mike Ros-ner, and Daniel Tapias, editors, Proceedings of theSeventh International Conference on Language Re-sources and Evaluation (LREC?10), Valletta, Malta.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in naturallanguage processing-Volume 10, pages 79?86.James W Pennebaker, Martha E Francis, and Roger JBooth.
2001.
Linguistic inquiry and word count:Liwc 2001.
Mahway: Lawrence Erlbaum Asso-ciates, 71:2001.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
SemEval-2014 Task 9:Sentiment Analysis in Twitter.
In Preslav Nakov and709Torsten Zesch, editors, Proceedings of the 8th Inter-national Workshop on Semantic Evaluation, Dublin,Ireland.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of in-ternational conference on new methods in languageprocessing, volume 12, pages 44?49.Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann,Ali Hurriyetoglu, Mijail Kabadjov, Polina Lenkova,Ralf Steinberger, Hristo Tanev, Silvia V?azquez, andVanni Zavarella.
2012.
Creating sentiment dictio-naries via triangulation.
Decision Support Systems,53(4):689?694.710
