Transactions of the Association for Computational Linguistics, 1 (2013) 219?230.
Action Editor: Brian Roark.Submitted 1/2013; Revised 3/2013; Published 5/2013.
c?2013 Association for Computational Linguistics.Joint Arc-factored Parsing of Syntactic and Semantic DependenciesXavier Llu?
?s and Xavier Carreras and Llu?
?s Ma`rquezTALP Research CenterUniversitat Polite`cnica de CatalunyaJordi Girona 1?3, 08034 Barcelona{xlluis,carreras,lluism}@lsi.upc.eduAbstractIn this paper we introduce a joint arc-factoredmodel for syntactic and semantic dependencyparsing.
The semantic role labeler predictsthe full syntactic paths that connect predicateswith their arguments.
This process is framedas a linear assignment task, which allowsto control some well-formedness constraints.For the syntactic part, we define a standardarc-factored dependency model that predictsthe full syntactic tree.
Finally, we employ dualdecomposition techniques to produce consis-tent syntactic and predicate-argument struc-tures while searching over a large space ofsyntactic configurations.
In experiments onthe CoNLL-2009 English benchmark we ob-serve very competitive results.1 IntroductionSemantic role labeling (SRL) is the task of identi-fying the arguments of lexical predicates in a sen-tence and labeling them with semantic roles (Gildeaand Jurafsky, 2002; Ma`rquez et al 2008).
SRL isan important shallow semantic task in NLP sincepredicate-argument relations directly represent se-mantic properties of the type ?who?
did ?what?
to?whom?, ?how?, and ?why?
for events expressed bypredicates (typically verbs and nouns).Predicate-argument relations are strongly relatedto the syntactic structure of the sentence: the ma-jority of predicate arguments correspond to somesyntactic constituent, and the syntactic structure thatconnects an argument with the predicate is a strongindicator of its semantic role.
Actually, semanticroles represent an abstraction of the syntactic formof a predicative event.
While syntactic functions ofarguments change with the form of the event (e.g.,active vs. passive forms), the semantic roles of argu-ments remain invariant to their syntactic realization.Consequently, since the first works, SRL systemshave assumed access to the syntactic structure of thesentence (Gildea and Jurafsky, 2002; Carreras andMa`rquez, 2005).
A simple approach is to obtainthe parse trees as a pre-process to the SRL system,which allows the use of unrestricted features of thesyntax.
However, as in other pipeline approaches inNLP, it has been shown that the errors of the syn-tactic parser severely degrade the predictions of theSRL model (Gildea and Palmer, 2002).
A commonapproach to alleviate this problem is to work withmultiple alternative syntactic trees and let the SRLsystem optimize over any input tree or part of it(Toutanova et al 2008; Punyakanok et al 2008).As a step further, more recent work has proposedparsing models that predict syntactic structure aug-mented with semantic predicate-argument relations(Surdeanu et al 2008; Hajic?
et al 2009; Johansson,2009; Titov et al 2009; Llu?
?s et al 2009), which isthe focus of this paper.
These joint models shouldfavor the syntactic structure that is most consistentwith the semantic predicate-argument structures ofa sentence.
In principle, these models can exploitsyntactic and semantic features simultaneously, andcould potentially improve the accuracy for both syn-tactic and semantic relations.One difficulty in the design of joint syntactic-semantic parsing models is that there exist impor-tant structural divergences between the two layers.219Mary loves to play guitar .
?SBJ OPRD IM OBJPARG0 ARG1ARG0ARG1Figure 1: An example .
.
.Das et al(2012) .
.
.Riedel and McCallum (2011) .
.
.3 A Syntactic-Semantic DependencyModelWe will describe structures of syntactic and seman-tic dependencies with vectors of binary variables.We will denote by yh,m,l a syntactic dependencyfrom head token h to dependant token m labeledwith syntactic function l. Similarly we will denoteby zp,a,r a semantic dependency between predicatetoken p and argument token a labeled with seman-tic role r. We will use y and z to denote vectors ofbinary variables indexed by syntactic and semanticdependencies, respectively.A joint model for syntactic and semantic depen-dency parsing could be defined as:argmaxy,zs syn(x,y) + s srl(x, z,y) .In the equation, s syn(x,y) gives a score for thesyntactic tree y.
In the literature, it is standard touse arc-factored models defined ass syn(x,y) =?yh,m,l=1s syn(x, h,m, l) ,where we overload s syn to be a function thatcomputes scores for individual labeled syntacticdependencies.
In discriminative models one hass syn(x, h,m, l) = wsyn ?
fsyn(x, h,m, l), wherefsyn is a feature vector for the syntactic dependencyand wsyn is a vector of parameters (McDonald et al2005).The other term, s srl(x, z,y), gives a score fora semantic dependency structure using the syntacticstructure y as features.
Previous work has empiri-cally proved the importance of exploiting syntacticfeatures in the semantic component (Gildea and Ju-rafsky, 2002; Xue and Palmer, 2004; Punyakanok etal., 2008).
However, without further assumptions,this property makes the optimization problem com-putationally hard.
One simple approximation is touse a pipeline model: first compute the optimal syn-tactic tree, and then optimize for the best semanticstructure given the syntactic tree.
In the rest of thepaper we describe a method that searches over syn-tactic and semantic dependency structures jointly.We first impose the assumption that syntactic fea-tures of the semantic component are restricted to thesyntactic path between a predicate and an argument,following previous work (Johansson, 2009).
For-mally, for a predicate p, argument a and role r wewill define a vector of dependency indicators ?p,a,rsimilar to the ones above: ?p,a,rh,m,l indicates if a de-pendency ?h,m, l?
is part of the syntactic path thatlinks predicate p with token a.
Figure 1 gives an ex-ample of one such paths.
Given full syntactic andsemantic structures y and z it is trivial to construct avector ?
that is a concatenation of vectors ?p,a,r forall ?p, a, r?
in z.
We can now define a linear seman-tic model ass srl(x, z,?)
=?zp,a,r=1s srl(x, p, a, r,?p,a,r) ,(1)where s srl computes a score for a semantic de-pendency ?p, a, r?
together with its syntactic path?p,a,r.
As in the syntactic component, this functionis typically defined as a linear function over a set offeatures of the semantic dependency and its path.With this joint model, the inference problem canbe formulated as:argmaxy,z,?s syn(x,y) + s srl(x, z,?)
(2)subject tocTree : y is a valid dependency treecRole : ?p, r :?azp,a,r ?
1cArg : ?p, a :?rzp,a,r ?
1cPath : ?p, a, r : if zp,a,r = 1 then?p,a,r is a path from p to aotherwise ?p,a,r = 0cSubtree : ?p, r, a : ?p,r,a is a subtree of yFigure 1: A sentenc with synt ctic dependencies (top)and semantic dependencies for the predicates ?loves?
and?play?
(bottom).
The thick arcs illustrate a structural di-vergence where the argument ?Mary?
is linked to ?play?with a path involving three syntactic dependencies.This is cl arly seen in dependency-based representa-tions of syntax and semantic roles (Surdeanu et al2008), such as in the example in Figure 1: the con-struct ?loves to?
causes the argument ?Mary?
to besyntactically distant fro the predicate ?play?.
Lin-guistic phenomena such as auxiliary verbs, controland raising, typically result in syntactic structureswhere semantic arguments are not among the directdepend ts of their predicate ?e.g., abou 25% ofarguments are distant in the English development setof the CoNLL-2009 shared task.
Besides, standardmodels for dependency parsing crucially depend onarc factorizations of the dependency structure (Mc-Donald et al 2005; Nivre and Nilsson, 2005), other-wise their computational properties break.
Hence, itis challenging to define efficient methods for syntac-tic and semantic dependency parsing that can exploitfeatures of both layers simultaneously.
In this paperwe propose a method for this joint task.In our method we define predicate-centric seman-tic models that, rather than predicting just the ar-gument that realizes each semantic role, they pre-dict the full syntactic path that connects the predi-cate with the argument.
We show how efficient pre-dictions with these models can be made using as-signment algorithms in bipartite graphs.
Simulta-neously, we use a standard arc-factored dependencymodel that predicts the full syntactic tree of the sen-tence.
Finally, we employ dual decomposition tech-niques (Koo et al 2010; Rush et al 2010; Sontaget al 2010) to find agreement between the full de-pendency tree and the partial syntactic trees linkingeach predicate with its arguments.
In summary, themain contributions of this paper are:?
We frame SRL as a weighted assignment prob-lem in a bipartite graph.
Under this frameworkwe can control assignment constraints betweenroles and arguments.
Key to our method, wec n efficiently search over a large space of syn-actic realizations of se antic argument .?
We solve joint inference of syntactic and se-mantic dependencies with a dual decomposi-tion method, similar to that of Koo et al(2010).Our system produces consistent syntactic andpredicate-argument structures while searchingover a large space of syntactic configurations.In the experimental section we compare jointand pipeline models.
The final results of our jointsyntactic-semantic system are competitive with thestate-of-the-art and improve over the best resultspublished by a joint method on the CoNLL-2009English dataset.2 A Syntactic-S mantic DependencyModelWe first describe how we represent structures of syn-tactic and semantic dependencies like the one in Fig-ure 1.
Throughout the paper, we will assume a fixedinput sentence x with n tokens where lexical predi-cates are marked.
We will also assume fixed sets ofsyntactic functions Rsyn and semantic roles Rsem.We will represent depencency structures using vec-tors of binary variables.
A variable yh,m,l will in-dicate the presence of a syntactic dependency fromhead token h to dependant tokenm labeled with syn-tactic function l. Then, a syntactic tree will be de-noted as a vector y of variables indexed by syntacticdependencies.
Similarly, a variable zp,a,r will indi-cate the presence of a semantic dependency betweenpredicate token p and argument token a labeled withsemantic role r. We will represent a semantic rolestructure as a vector z indexed by semantic depen-dencies.
Whenever we enumerate syntactic depen-dencies ?h,m, l?
we will assume that they are in thevalid range for x, i.e.
0 ?
h ?
n, 1 ?
m ?
n,h 6= m and l ?
Rsyn where h = 0 stands for aspecial root token.
Similarly, for semantic depen-dencies ?p, a, r?
we will assume that p points to apredicate of x, 1 ?
a ?
n and r ?
Rsem.220A joint model for syntactic and semantic depen-dency parsing could be defined as:argmaxy,zs syn(x,y) + s srl(x, z,y) .
(1)In the equation, s syn(x,y) gives a score for thesyntactic tree y.
In the literature, it is standard touse arc-factored models defined ass syn(x,y) =?yh,m,l=1s syn(x, h,m, l) , (2)where we overload s syn to be a function thatcomputes scores for individual syntactic depen-dencies.
In linear discriminative models one hass syn(x, h,m, l) = wsyn ?
fsyn(x, h,m, l), wherefsyn is a feature vector for a syntactic dependencyand wsyn is a vector of parameters (McDonald etal., 2005).
In Section 6 we describe how we trainedscore functions with discriminative methods.The other term in Eq.
1, s srl(x, z,y), gives ascore for a semantic dependency structure z usingfeatures of the syntactic structure y.
Previous workhas empirically proved the importance of exploit-ing syntactic features in the semantic component(Gildea and Jurafsky, 2002; Xue and Palmer, 2004;Punyakanok et al 2008).
However, without furtherassumptions, this property makes the optimizationproblem computationally hard.
One simple approx-imation is to use a pipeline model: first compute theoptimal syntactic tree y, and then optimize for thebest semantic structure z given y.
In the rest of thepaper we describe a method that searches over syn-tactic and semantic dependency structures jointly.We first note that for a fixed semantic dependency,the semantic component will typically restrict thesyntactic features representing the dependency to aspecific subtree of y.
For example, previous workhas restricted such features to the syntactic path thatlinks a predicate with an argument (Moschitti, 2004;Johansson, 2009), and in this paper we employ thisrestriction.
Figure 1 gives an example of a sub-tree, where we highlight the syntactic path that con-nects the semantic dependency between ?play?
and?Mary?
with role ARG0.Formally, for a predicate p, argument a and roler we define a local syntactic subtree pip,a,r repre-sented as a vector: pip,a,rh,m,l indicates if a dependency?h,m, l?
is part of the syntactic path that links pred-icate p with token a and role r.1 Given full syntacticand semantic structures y and z it is trivial to con-struct a vector pi that concatenates vectors pip,a,r forall ?p, a, r?
in z.
The semantic model becomess srl(x, z,pi) =?zp,a,r=1s srl(x, p, a, r,pip,a,r) ,(3)where s srl computes a score for a semantic de-pendency ?p, a, r?
together with its syntactic pathpip,a,r.
As in the syntactic component, this functionis typically defined as a linear function over a set offeatures of the semantic dependency and its path.The inference problem of our joint model is:argmaxy,z,pis syn(x,y) + s srl(x, z,pi) (4)subject tocTree : y is a valid dependency treecRole : ?p, r : ?azp,a,r ?
1cArg : ?p, a : ?rzp,a,r ?
1cPath : ?p, a, r : if zp,a,r = 1 thenpip,a,r is a path from p to a,otherwise pip,a,r = 0cSubtree : ?p, a, r : pip,a,r is a subtree of yConstraint cTree dictates that y is a valid depen-dency tree; see (Martins et al 2009) for a detailedspecification.
The next two sets of constraints con-cern the semantic structure only.
cRole imposes thateach semantic role is realized at most once.2 Con-versely, cArg dictates that an argument can realizeat most one semantic role in a predicate.
The finaltwo sets of constraints model the syntactic-semanticinterdependencies.
cPath imposes that each pip,a,rrepresents a syntactic path between p and a when-ever there exists a semantic relation.
Finally, cSub-tree imposes that the paths in pi are consistent withthe full syntactic structure, i.e.
they are subtrees.1In this paper we say that structures pip,a,r are paths frompredicates to arguments, but they could be more general sub-trees.
The condition to build a joint system is that these subtreesmust be parseable in the way we describe in Section 3.1.2In general a semantic role can be realized with more thanone argument, though it is rare.
It is not hard to modify ourframework to allow for a maximum number of occurrences of asemantic role.221In Section 3 we define a process that optimizesthe semantic structure ignoring constraint cSubtree.Then in Section 4 we describe a dual decompositionmethod that uses the first process repeatedly to solvethe joint problem.3 SRL as AssignmentIn this section we frame the problem of finding se-mantic dependencies as a linear assignment task.The problem we optimize is:argmaxz,pis srl(x, z,pi) (5)subject to cRole, cArg, cPathIn this case we dropped the full syntactic structurey from the optimization in Eq.
4, as well as thecorresponding constraints cTree and cSubtree.
Asa consequence, we note that the syntactic paths piare not tied to any consistency constraint other thaneach of the paths being a well-formed sequence ofdependencies linking the predicate to the argument.In other words, the optimal solution in this case doesnot guarantee that the set of paths from a predicate toall of its arguments satisfies tree constraints.
We firstdescribe how these paths can be optimized locally.Then we show how to find a solution z satisfyingcRole and cArg using an assignment algorithm.3.1 Local Optimization of Syntactic PathsLet z?
and p?i be the optimal values of Eq.
5.
For any?p, a, r?, letp?ip,a,r = argmaxpip,a,rs srl(x, p, a, r,pip,a,r) .
(6)For any ?p, a, r?
such that z?p,a,r = 1 it has to be thatp?ip,a,r = p?ip,a,r.
If this was not true, replacing p?ip,a,rwith p?ip,a,r would improve the objective of Eq.
5without violating the constraints, thus contradictingthe hypothesis about optimality of p?i.
Therefore, foreach ?p, a, r?
we can optimize its best syntactic pathlocally as defined in Eq.
6.In this paper, we will assume access to a list oflikely syntactic paths for each predicate p and argu-ment candidate a, such that the optimization in Eq.
6can be solved explicitly by looping over each path inthe list.
The main advantage of this method is that,since paths are precomputed, our model can makeunrestricted use of syntactic path features.
(1)Mary(2)plays(3)guitar (4)NULL (5)NULL (6)NULL(1)ARG0(2)ARG1(3)ARG2 (4)NULL (5)NULL (6)NULLW1,1W4,2W2,3W3,4W5,5 W6,6Figure 2: Illustration of the assignment graph for the sen-tence ?Mary plays guitar?, where the predicate ?plays?can have up to three roles: ARG0 (agent), ARG1 (theme)and ARG2 (benefactor).
Nodes labeled NULL representa null role or token.
Highlighted edges are the correctassignment.It is simple to employ a probabilistic syntactic de-pendency model to create the list of likely paths foreach predicate-argument pair.
In the experiments weexplore this approach and show that with an averageof 44 paths per predicate we can recover 86.2% ofthe correct paths.We leave for future work the development of ef-ficient methods to recover the most likely syntacticstructure linking an argument with its predicate.3.2 The Assignment AlgorithmComing back to solving Eq.
5, it is easy to seethat an optimal solution satisfying constraints cRoleand cArg can be found with a linear assignmentalgorithm.
The process we describe determinesthe predicate-argument relations separately for eachpredicate.
Assume a bipartite graph of size N withrole nodes r1 .
.
.
rN on one side and argument nodesa1 .
.
.
aN on the other side.
Assume also a matrix ofnon-negative scoresWi,j corresponding to assigningargument aj to role ri.
A linear assignment algo-rithm finds a bijection f : i?
j from roles to argu-ments that maximizes?Ni=1Wi,f(i).
The Hungarianalgorithm finds the exact solution to this problem inO(N3) time (Kuhn, 1955; Burkard et al 2009).All that is left is to construct a bipartite graph rep-resenting predicate roles and sentence tokens, suchthat some roles and tokens can be left unassigned,which is a common setting for assignment tasks.
Al-gorithm 1 describes a procedure for constructing aweighted bipartite graph for SRL, and Figure 2 il-lustrates an example of a bipartite graph.
We then222Algorithm 1 Construction of an Assignment Graphfor Semantic Role LabelingLet p be a predicate with k possible roles.
Let n be thenumber of argument candidates in the sentence.
This al-gorithm creates a bipartite graph withN = n+k verticeson each side.1.
Create role vertices ri for i = 1 .
.
.
N , where?
for 1 ?
i ?
k, ri is the i-th role,?
for 1 ?
i ?
n, rk+i is a special NULL role.2.
Create argument vertices aj for j = 1 .
.
.
N , where?
for 1 ?
j ?
n, aj is the j-th argument candidate,?
for 1 ?
j ?
k, an+j is a special NULL argument.3.
Define a matrix of model scores S ?
R(k+1)?n:(a) Optimization of syntactic paths:For 1 ?
i ?
k, 1 ?
j ?
nSi,j = maxpip,aj,ris srl(x, p, aj , ri,pip,aj ,ri)(b) Scores of NULL assignments3:For 1 ?
j ?
nSk+1,j = 04.
Let S0 = mini,j Si,j , the minimum of any scorein S. Define a matrix of non-negative scores W ?RN?N as follows:(a) for 1 ?
i ?
k, 1 ?
j ?
nWi,j = Si,j ?
S0(b) for k < i ?
N, 1 ?
j ?
nWi,j = Sk+1,j ?
S0(c) for 1 < i ?
N, n < j ?
NWi,j = 0run the Hungarian algorithm on the weighted graphand obtain a bijection f : ri ?
aj , from which itis trivial to recover the optimal solution of Eq.
5.Finally, we note that it is simple to allow for multi-ple instances of a semantic role by adding more rolenodes in step 1; it would be straightforward to addpenalties in step 3 for multiple instances of roles.4 A Dual Decomposition AlgorithmWe now present a dual decomposition method to op-timize Eq.
4, that uses the assignment algorithm pre-sented above as a subroutine.
Our method is sim-ilar to that of Koo et al(2010), in the sense that3In our model we fix the score of null assignments to 0.
It isstraightforward to compute a discriminative score instead.our joint optimization can be decomposed into twosub-problems that need to agree on the syntacticdependencies they predict.
For a detailed descrip-tion of dual decomposition methods applied to NLPsee (Sontag et al 2010; Rush et al 2010).We note that in Eq.
4 the constraint cSubtree tiesthe syntactic and semantic structures, imposing thatany path pip,a,r that links a predicate p with an argu-ment a must be a subtree of the full syntactic struc-ture y.
Formally the set of constraints is:yh,m,l ?
pip,a,rh,m,l ?
p, a, r, h,m, l .These constraints can be compactly written asc ?
yh,m,l ?
?p,a,rpip,a,rh,m,l ?
h,m, l ,where c is a constant equal to the number of dis-tinct semantic dependencies ?p, a, r?.
In addition,we can introduce a vector non-negative slack vari-ables ?
with a component for each syntactic depen-dency ?h,m,l, turning the constraints into:c ?
yh,m,l ?
?p,a,rpip,a,rh,m,l ?
?h,m,l = 0 ?
h,m, lWe can now rewrite Eq.
4 as:argmaxy,z,pi,?
?0s syn(x,y) + s srl(x, z,pi) (7)subject tocTree, cRole, cArg, cPath?h,m, l : c ?
yh,m,l ?
?p,a,rpip,a,rh,m,l ?
?h,m,l = 0As in Koo et al(2010), we will relax subtree cons-traints by introducing a vector of Lagrange multipli-ers ?
indexed by syntactic dependencies, i.e.
eachcoordinate ?h,m,l is a Lagrange multiplier for theconstraint associated with ?h,m, l?.
The Lagrangianof the problem is:L(y, z,pi, ?,?
)= s syn(x,y) + s srl(x, z,pi)+ ?
?
(c ?
y ?
?p,a,rpip,a,r ?
?
)(8)We can now formulate Eq.
7 as:maxy,z,pi,??0s.t.
cTree,cRole,cArg,cPathc?y?
?p,a,r pip,a,r?
?=0L(y, z,pi, ?,?)
(9)223This optimization problem has the property that itsoptimum value is the same as the optimum of Eq.
7for any value of ?.
This is because whenever theconstraints are satisfied, the terms in the Lagrangianinvolving ?
are zero.
If we remove the subtree con-straints from Eq.
9 we obtain the dual objective:D(?)
= maxy,z,pi,??0s.t.
cTree,cRole,cArg,cPathL(y, z,pi, ?,?)
(10)= maxy s.t.
cTree(s syn(x,y) + c ?
y ?
?
)+ maxz,pis.t.
cRole,cArg,cPath(s srl(x, z,pi)?
?
?
?p,a,rpip,a,r)+ max??0(??
?
?)
(11)The dual objective is an upper bound to the opti-mal value of primal objective of Eq.
7.
Thus, weare interested in finding the minimum of the dual inorder to tighten the upper-bound.
We will solvemin?D(?)
(12)using a subgradient method.
Algorithm 2 presentspseudo-code.
The algorithm takes advantage of thedecomposed form of the dual in Eq.
11, wherewe have rewritten the Lagrangian such that syntac-tic and semantic structures appear in separate terms.This will allow to compute subgradients efficiently.In particular, the subgradient of D at a point ?
is:?(?)
= c ?
y?
?
?p,a,rp?ip,a,r ?
??
(13)wherey?
= argmaxy s.t.
cTree(s syn(x,y) + c ?
y ?
?)
(14)z?, p?i = argmaxz,pi s.t.cRole,cArg,cPaths srl(x, z,pi)?
??
?p,a,rpip,a,r (15)??
= argmax??0??
?
?
(16)Whenever p?i is consistent with y?
the subgradientwill be zero and the method will converge.
Whenpaths p?i contain a dependency ?h,m, l?
that is in-consistent with y?, the associated dual ?h,m,l will in-crease, hence lowering the score of all paths that use?h,m, l?
at the next iteration; at same time, the to-tal score for that dependency will increase, favoringsyntactic dependency structures alternative to y?.
AsAlgorithm 2 A dual-decomposition algorithm forsyntactic-semantic dependency parsingInput: x, a sentence; T , number of iterations;Output: syntactic and semantic structures y?
and z?Notation: we use cSem= cRole ?
cArg ?
cPath1: ?1 = 0 # initialize dual variables2: c =number of distinct ?h,m, l?
in x3: for t = 1 .
.
.
T do4: y?
= argmaxy s.t.
cTree(s syn(x,y) + c ?
?t ?
y)5: z?, p?i = argmax z,pis.t.
cSem(s srl(x, z,pi)?
?t ?
?p,a,r pip,a,r)6: ?t+1 = ?t # dual variables for the next iteration7: Set ?t, the step size of the current iteration8: for each ?h,m, l?
do9: q = ?p,a,r ?p,a,rh,m,l # num.
paths using ?h,m, l?10: if q > 0 and y?h,m,l = 0 then11: ?t+1h,m,l = ?t+1h,m,l + ?tq12: break if ?t+1 = ?t # convergence13: return y?, z?in previous work, in the algorithm a parameter ?tcontrols the size of subgradient steps at iteration t.The key point of the method is that solutions toEq.
14 and 15 can be computed efficiently using sep-arate processes.
In particular, Eq.
14 correspondsto a standard dependency parsing problem, wherefor each dependency ?h,m, l?
we have an additionalscore term c ?
?h,m,l ?in our experiments we use theprojected dependency parsing algorithm by (Eisner,2000).
To calculate Eq.
15 we use the assignmentmethod described in Section 3, where it is straight-forward to introduce additional score terms ?
?h,m,lto every factor pip,a,rh,m,l.
It can be shown that wheneverthe subgradient method converges, the solutions y?and z?
are the optimal solutions to our original prob-lem in Eq.
4 (see (Koo et al 2010) for a justifi-cation).
In practice we run the subgradient methodfor a maximum number of iterations, and return thesolutions of the last iteration if it does not converge.5 Related WorkRecently, there have been a number of approachesto joint parsing of syntactic and semantic dependen-cies, partly because of the availability of treebanks inthis format popularized by the CoNLL shared tasks(Surdeanu et al 2008; Hajic?
et al 2009).Like in our method, Johansson (2009) defined amodel that exploits features of a semantic depen-224dency together with the syntactic path connectingthe predicate and the argument.
That method uses anapproximate parsing algorithm that employs k-bestinference and beam search.
Similarly, Llu?
?s et al(2009) defined a joint model that forces the predi-cate structure to be represented in the syntactic de-pendency tree, by enriching arcs with semantic in-formation.
The semantic component uses features ofpre-computed syntactic structures that may divergefrom the joint structure.
In contrast, our joint pars-ing method is exact whenever the dual decomposi-tion algorithm converges.Titov et al(2009) augmented a transition-baseddependency parser with operations that producesynchronous derivations of syntactic and semanticstructures.
Instead of explicitly representing seman-tic dependencies together with a syntactic path, theyinduce latent representations of the interactions be-tween syntactic and semantic layers.In all works mentioned the model has no con-trol of assignment constraints that disallow label-ing multiple arguments with the same semantic role.Punyakanok et al(2008) first introduced a systemthat explicitly controls these constraints, as well asother constraints that look at pairwise assignmentswhich we can not model.
They solve SRL usinggeneral-purpose Integer Linear Programming (ILP)methods.
In similar spirit, Riedel and McCallum(2011) presented a model for extracting structuredevents that controls interactions between predicate-argument assignments.
They take into account pair-wise assignments and solve the optimization prob-lem with dual decomposition.
More recently, Daset al(2012) proposed a dual decomposition methodthat deals with several assignment constraints forpredicate-argument relations.
Their method is analternative to general ILP methods.
To our knowl-edge, our work is the first that frames SRL as a linearassignment task, for which simple and exact algo-rithms exist.
We should note that these works modelpredicate-argument relations with assignment con-straints, but none of them predicts the underlyingsyntactic structure.Our dual decomposition method follows from thatof Koo et al(2010).
In both cases two separate pro-cesses predict syntactic dependency structures, andthe dual decomposition algorithm seeks agreementat the level of individual dependencies.
One dif-ference is that our semantic process predicts partialsyntax (restricted to syntactic paths connecting pred-icates and arguments), while in their case each of thetwo processes predicts the full set of dependencies.6 ExperimentsWe present experiments using our syntactic-semantic parser on the CoNLL-2009 Shared TaskEnglish benchmark (Hajic?
et al 2009).
It consistsof the usual WSJ training/development/test sectionsmapped to dependency trees, augmented with se-mantic predicate-argument relations from PropBank(Palmer et al 2005) and NomBank (Meyers et al2004) also represented as dependencies.
It also con-tains a PropBanked portion of the Brown corpus asan out-of-domain test set.Our goal was to evaluate the contributions of pars-ing algorithms in the following configurations:Base Pipeline Runs a syntactic parser and then runsan SRL parser constrained to paths of the bestsyntactic tree.
In the SRL it only enforces con-straint cArg, by simply classifying the candi-date argument in each path into one of the pos-sible semantic roles or as NULL.Pipeline with Assignment Runs the assignment al-gorithm for SRL, enforcing constraints cRoleand cArg, but constrained to paths of the bestsyntactic tree.Forest Runs the assignment algorithm for SRL ona large set of precomputed syntactic paths, de-scribed below.
This configuration correspondsto running Dual Decomposition for a single it-eration, and is not guaranteed to predict consis-tent syntactic and semantic structures.Dual Decomposition (DD) Runs dual decomposi-tion using the assignment algorithm on the setof precomputed paths.
Syntactic and semanticstructures are consistent when it reaches con-vergence.All four systems used the same type of discrimina-tive scorers and features.
Next we provide detailsabout these systems.
Then we present the results.6.1 ImplementationSyntactic model We used two discriminative arc-factored models for labeled dependency parsing: a225first-order model, and a second-order model withgrandchildren interactions, both reimplementationsof the parsers by McDonald et al(2005) and Car-reras (2007) respectively.
In both cases we usedprojective dependency parsing algorithms based on(Eisner, 2000).4 To learn the models, we used alog-linear loss function following Koo et al(2007),which trains probabilistic discriminative parsers.At test time, we used the probabilistic parsers tocompute marginal probabilities p(h,m, l | x), us-ing inside-outside algorithms for first/second-ordermodels.
Hence, for either of the parsing models, wealways obtain a table of first-order marginal scores,with one score per labeled dependency.
Then werun first-order inference with these marginals to ob-tain the best tree.
We found that the higher-orderparser performed equally well on development us-ing this method as using second-order inference topredict trees: since we run the parser multiple timeswithin Dual Decomposition, our strategy results infaster parsing times.Precomputed Paths Both Forest and Dual De-composition run assignment on a set of precomputedpaths, and here we explain how we build it.
We firstobserved that 98.4% of the correct arguments in de-velopment data are either direct descendants of thepredicate, direct descendants of an ancestor of thepredicate, or an ancestor of the predicate.5 All meth-ods we test are restricted to this syntactic scope.
Togenerate a list of paths, we did as follows:?
Calculate marginals of unlabeled dependenciesusing the first-order parser: p(h,m | x) =?l p(h,m, l | x).
Note that for each m, theprobabilities p(h,m|x) for all h form a distri-bution (i.e.
they sum to one).
Then, for eachm,keep the most-likely dependencies that cover atleast 90% of the mass, and prune the rest.?
Starting from a predicate p, generate a pathby taking any number of dependencies that as-cend, and optionally adding one dependencythat descends.
We constrained paths to be pro-jective, and to have a maximum number of 64Our method allows to use non-projective dependency pars-ing methods seamlessly.5This is specific to CoNLL-2009 data for English.
In gen-eral, for other languages the coverage of these rules may belower.
We leave this question to future work.ascendant dependencies.?
Label each unlabeled edge ?h,m?
in the pathswith l = argmaxl p(h,m, l | x).On development data, this procedure generated anaverage of 43.8 paths per predicate that cover 86.2%of the correct paths.
In contrast, enumerating pathsof the single-best tree covers 79.4% of correct pathsfor the first-order parser, and 82.2% for the second-order parser.6SRL model We used a discriminative model withsimilar features to those in the system of Johansson(2009).
In addition, we included the following:?
Unigram/bigram/trigram path features.
Forall n-grams in the syntactic path, patternsof words and POS tags (e.g., mary+loves+to,mary+VB+to).?
Voice features.
The predicate voice togetherwith the word/POS of the argument (e.g., pas-sive+mary).?
Path continuity.
Count of non-consecutive to-kens in a predicate-argument path.To train SRL models we used the averaged per-ceptron (Collins, 2002).
For the base pipeline wetrained standard SRL classifiers.
For the rest ofmodels we used the structured Perceptron runningthe assignment algorithm as inference routine.
Inthis case, we generated a large set of syntactic pathsfor training using the procedure described above,and we set the loss function to penalize mistakes inpredicting the semantic role of arguments and theirsyntactic path.Dual Decomposition We added a parameter ?weighting the syntactic and semantic components ofthe model as follows:(1?
?)
s syn(x,y) + ?
s srl(x, z,pi) .As syntactic scores we used normalized marginalprobabilities of dependencies, either from the firstor the higher-order parser.
The scores of all factorsof the SRL model were normalized at every sen-tence to be between -1 and 1.
The rest of details6One can evaluate the maximum recall on correct argumentsthat can be obtained, irrespective of whether the syntactic pathis correct: for the set of paths it is 98.3%, while for single-besttrees it is 91.9% and 92.7% for first and second-order models.226o LAS UAS semp semr semF1 semppPipeline 1 85.32 88.86 86.23 67.67 75.83 45.64w.
Assig.
1 85.32 88.86 84.08 71.82 77.47 51.17Forest - - - 80.67 73.60 76.97 51.33Pipeline 2 87.77 90.96 87.07 68.65 76.77 47.07w.
Assig.
2 87.77 90.96 85.21 73.41 78.87 53.80Table 1: Results on development for the baseline and as-signment pipelines, running first and second-order syn-tactic parsers, and the Forest method.
o indicates the or-der of syntactic inference.of the method were implemented following Koo etal.
(2010), including the strategy for decreasing thestep size ?t.
We ran the algorithm for up to 500 it-erations, with initial step size of 0.001.6.2 ResultsTo evaluate syntactic dependencies we use unla-beled attachment score (UAS), i.e., the percentageof words with the correct head, and labeled attach-ment scores (LAS), i.e., the percentage of wordswith the correct head and syntactic label.
Semanticpredicate-argument relations are evaluated with pre-cision (semp), recall (semr) and F1 measure (semF1)at the level of labeled semantic dependencies.
In ad-dition, we measure the percentage of perfectly pre-dicted predicate structures (sempp).7Table 1 shows the results on the development setfor our three first methods.
We can see that thepipeline methods running assignment improve overthe baseline pipelines in semantic F1 by about 2points, due to the application of the cRole constraint.The Forest method also shows an improvement inrecall of semantic roles with respect to the pipelinemethods.
Presumably, the set of paths available inthe Forest model allows to recognize a higher num-ber of arguments at an expense of a lower preci-sion.
Regarding the percentage of perfect predicate-argument structures there is a remarkable improve-ment in the systems that apply the full set of con-7Our evaluation metrics differ slightly from the official met-ric at CoNLL-2009.
That metric considers predicate sensesas special semantic dependencies and, thus, it includes themin the calculation of the evaluation metrics.
In this paper, weare not addressing predicate sense disambiguation and, conse-quently, we ignore predicate senses when presenting evaluationresults.
When we report the performance of CoNLL systems,their scores will be noticeably lower than the scores reported atthe shared task.
This is because predicate disambiguation is areasonably simple task with a very high baseline around 90%.o ?
LAS UAS semp semr semF1 sempp %conv1 0.1 85.32 88.86 84.09 71.84 77.48 51.77 1001 0.4 85.36 88.91 84.07 71.94 77.53 51.85 1001 0.5 85.38 88.93 84.08 72.03 77.59 51.96 1001 0.6 85.41 88.95 84.05 72.19 77.67 52.03 99.81 0.7 85.44 89.00 84.10 72.42 77.82 52.24 99.71 0.8 85.48 89.02 83.99 72.69 77.94 52.57 99.51 0.9 85.39 88.93 83.68 72.82 77.88 52.49 99.82 0.1 87.78 90.96 85.20 73.11 78.69 53.74 1002 0.4 87.78 90.96 85.21 73.12 78.70 53.74 1002 0.5 87.78 90.96 85.19 73.12 78.70 53.72 1002 0.6 87.78 90.96 85.20 73.13 78.70 53.72 99.92 0.7 87.78 90.96 85.19 73.13 78.70 53.72 99.82 0.8 87.80 90.98 85.20 73.18 78.74 53.77 99.82 0.9 87.84 91.02 85.20 73.23 78.76 53.82 100Table 2: Results of the dual decomposition method ondevelopment data, for different values of the ?
parame-ter.
o is the order of the syntactic parser.
%conv is thepercentage of examples that converged.straints using the assignment algorithm.
We believethat the cRole constraint that ensures no repeatedroles for a given predicate is a key factor to predictthe full set of arguments of a predicate.The Forest configuration is the starting point torun the dual decomposition algorithm.
We ran ex-periments for various values of the ?
parameter.
Ta-ble 2 shows the results.
We see that as we increase?, the SRL component has more relative weight, andthe syntactic structure changes.
The DD methods arealways able to improve over the Forest methods, andfind convergence in more than 99.5% of sentences.Compared to the pipeline running assignment, DDimproves semantic F1 for first-order inference, butnot for higher-order inference, suggesting that 2ndorder predictions of paths are quite accurate.
Wealso observe slight benefits in syntactic accuracy.Table 3 presents results of our system on thetest sets, where we run Pipeline with Assignmentand Dual Decomposition with our best configura-tion (?
= 0.8/0.9 for 1st/2nd order syntax).
Forcomparison, the table also reports the results ofthe best CoNLL?2009 joint system, Merlo09 (Ges-mundo et al 2009), which proved to be very com-petitive ranking third in the closed challenge.
Wealso include Llu?
?s09 (Llu?
?s et al 2009), which is an-other joint syntactic-semantic system from CoNLL?2009.8 In the WSJ test DD obtains the best syntacticaccuracies, while the Pipeline obtains the best se-8Another system to compare to is the joint system by Jo-hansson (2009).
Unfortunately, a direct comparison is not possi-ble because it is evaluated on the CoNLL-2008 datasets, which227WSJ LAS UAS semp semr semF1 semppLlu?
?s09 87.48 89.91 73.87 67.40 70.49 39.68Merlo09 88.79 91.26 81.00 76.45 78.66 54.80Pipe-Assig 1st 86.85 89.68 85.12 73.78 79.05 54.12DD 1st 87.04 89.89 85.03 74.56 79.45 54.92Pipe-Assig 2nd 89.19 91.62 86.11 75.16 80.26 55.96DD 2nd 89.21 91.64 86.01 74.84 80.04 55.73Brown LAS UAS semp semr semF1 semppLlu?
?s09 80.92 85.96 62.29 59.22 60.71 29.79Merlo09 80.84 86.32 68.97 63.06 65.89 38.92Pipe-Assig 1st 80.96 86.58 72.91 60.16 65.93 38.44DD 1st 81.18 86.86 72.53 60.76 66.12 38.13Pipe-Assig 2nd 82.56 87.98 73.94 61.63 67.23 38.99DD 2nd 82.61 88.04 74.12 61.59 67.28 38.92Table 3: Comparative results on the CoNLL?2009 En-glish test sets, namely the WSJ test (top table) and theout of domain test from the Brown corpus (bottom table).mantic F1.
The bottom part of Table 3 presents re-sults on the out-of-domain Brown test corpus.
In thiscase, DD obtains slightly better results than the rest,both in terms of syntactic accuracy and semantic F1.Table 4 shows statistical significance tests for thesyntactic LAS and semantic F1 scores of Table 3.We have applied the sign test (Wackerly et al 2007)and approximate randomization tests (Yeh, 2000)to all pairs of systems outputs.
The differences be-tween systems in the WSJ test can be consideredsignificant in almost all cases with p = 0.05.
Inthe Brown test set, results are more unstable and dif-ferences are not significant in general, probably be-cause of the relatively small size of that test.Regarding running times, our implementation ofthe baseline pipeline with 2nd order inference parsesthe development set (1,334 sentences) in less than7 minutes.
Running assignment in the pipeline in-creases parsing time by ?8% due to the overheadfrom the assignment algorithm.
The Forest method,with an average of 61.3 paths per predicate, is?13%slower than the pipeline due to the exploration of thespace of precomputed paths.
Finally, Dual Decom-position with 2nd order inference converges in 36.6iterations per sentence on average.
The first itera-tion of DD has to perform roughly the same workas Forest, while subsequent iterations only need tore-parse the sentence with respect to the dual up-are slightly different.
However, note that Merlo09 is an applica-tion of the system by Titov et al(2009).
In that paper authorsreport results on the CoNLL-2008 datasets, and they are com-parable to Johansson?s.WSJ BrownME PA1 DD1 PA2 DD2 ME PA1 DD1 PA2 DD2LL ?
? ?
? ? ?
? ?
?    ?
? ?
?ME ??
?
? ?
? ?
? ?
?PA1 ?
? ?
? ?
? ?
?
? ?
?DD1 ?
? ?
? ?
? ?
?PA2 ?Table 4: Statistical tests of significance for LAS andsemF1 differences between pairs of systems from Table 3.?/?
= LAS difference is significant by the sign/ approxi-mate randomization tests at 0.05 level.
/ = same mean-ing for semF1 .
The legend for systems is: LL: Llu?
?s09,ME: Merlo09, PA1/2: Pipeline with Assignment, 1st/2ndorder, DD1/2: Dual Decomposition, 1st/2nd order.dates, which are extremely sparse.
Our current im-plementation did not take advantage of the sparsityof updates, and overall, DD was on average 13 timesslower than the pipeline running assignment and 15times slower than the baseline pipeline.7 ConclusionWe have introduced efficient methods to parsesyntactic dependency structures augmented withpredicate-argument relations, with two key ideas.One is to predict the local syntactic structure thatlinks a predicate with its arguments, and seek agree-ment with the full syntactic structure using dualdecomposition techniques.
The second is to con-trol linear assignment constraints in the predicate-argument structure.In experiments we observe large improvementsresulting from the assignment constraints.
As forthe dual decomposition technique for joint parsing,it does improve over the pipelines when we use afirst order parser.
This means that in this configu-ration the explicit semantic features help to find asolution that is better in both layers.
To some ex-tent, this empirically validates the research objec-tive of joint models.
However, when we move tosecond-order parsers the differences with respect tothe pipeline are insignificant.
It is to be expectedthat as syntactic parsers improve, the need of jointmethods is less critical.
It remains an open questionto validate if large improvements can be achievedby integrating syntactic-semantic features.
To studythis question, it is necessary to have efficient pars-ing algorithms for joint dependency structures.
Thispaper contributes with a method that has optimality228guarantees whenever it converges.Our method can incorporate richer families of fea-tures.
It is straightforward to incorporate better se-mantic representations of predicates and argumentsthan just plain words, e.g.
by exploiting WordNet ordistributional representations as in (Zapirain et al2013).
Potentially, this could result in larger im-provements in the performance of syntactic and se-mantic parsing.It is also necessary to experiment with differ-ent languages, where the performance of syntacticparsers is lower than in English, and hence there ispotential for improvement.
Our treatment of localsyntactic structure that links predicates with argu-ments, based on explicit enumeration of likely paths,was simplistic.
Future work should explore meth-ods that model the syntactic structure linking predi-cates with arguments: whenever this structure can beparsed efficiently, our dual decomposition algorithmcan be employed to define an efficient joint system.AcknowledgmentsWe thank the editor and the anonymous reviewers fortheir valuable feedback.
This work was financed bythe European Commission for the XLike project (FP7-288342); and by the Spanish Government for projectOpenMT-2 (TIN2009-14675-C03-01), project Skater(TIN2012-38584-C06-01), and a Ramo?n y Cajal contractfor Xavier Carreras (RYC-2008-02223).ReferencesRainer Burkard, Mario Dell?Amico, and SilvanoMartello.
2009.
Assignment Problems.
Society forIndustrial and Applied Mathematics.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduc-tion to the CoNLL-2005 shared task: Semantic rolelabeling.
In Proceedings of the Ninth Conference onComputational Natural Language Learning (CoNLL-2005), pages 152?164, Ann Arbor, Michigan, June.Xavier Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL 2007,pages 957?961, Prague, Czech Republic, June.Michael Collins.
2002.
Discriminative training meth-ods for Hidden Markov Models: Theory and experi-ments with Perceptron algorithms.
In Proceedings ofthe 2002 Conference on Empirical Methods in NaturalLanguage Processing, pages 1?8, July.Dipanjan Das, Andre?
F. T. Martins, and Noah A. Smith.2012.
An exact dual decomposition algorithm forshallow semantic parsing with constraints.
In Pro-ceedings of the First Joint Conference on Lexical andComputational Semantics - Volume 1: Proceedings ofthe main conference and the shared task, and Volume2: Proceedings of the Sixth International Workshop onSemantic Evaluation, SemEval ?12, pages 209?217,Stroudsburg, PA, USA.Jason Eisner.
2000.
Bilexical grammars and their cubic-time parsing algorithms.
In Harry Bunt and AntonNijholt, editors, Advances in Probabilistic and OtherParsing Technologies, pages 29?62.
Kluwer AcademicPublishers, October.Andrea Gesmundo, James Henderson, Paola Merlo, andIvan Titov.
2009.
A latent variable model of syn-chronous syntactic-semantic parsing for multiple lan-guages.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning(CoNLL 2009): Shared Task, pages 37?42, Boulder,Colorado, June.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288, September.Daniel Gildea and Martha Palmer.
2002.
The necessityof parsing for predicate argument recognition.
In Pro-ceedings of 40th Annual Meeting of the Association forComputational Linguistics, pages 239?246, Philadel-phia, Pennsylvania, USA, July.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic dependen-cies in multiple languages.
In Proceedings of the13th Conference on Computational Natural LanguageLearning (CoNLL-2009): Shared Task, pages 1?18,Boulder, Colorado, USA, June.Richard Johansson.
2009.
Statistical bistratal depen-dency parsing.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Process-ing, pages 561?569, Singapore, August.Terry Koo, Amir Globerson, Xavier Carreras, andMichael Collins.
2007.
Structured prediction mod-els via the matrix-tree theorem.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 141?150, Prague, Czech Republic, June.Terry Koo, Alexander M. Rush, Michael Collins, TommiJaakkola, and David Sontag.
2010.
Dual decompo-sition for parsing with non-projective head automata.In Proceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages1288?1298, Cambridge, MA, October.229Harold W. Kuhn.
1955.
The Hungarian method for theassignment problem.
Naval Research Logistics Quar-terly, 2(1-2):83?97.Xavier Llu?
?s, Stefan Bott, and Llu?
?s Ma`rquez.
2009.A second-order joint eisner model for syntactic andsemantic dependency parsing.
In Proceedings ofthe Thirteenth Conference on Computational Natu-ral Language Learning (CoNLL 2009): Shared Task,pages 79?84, Boulder, Colorado, June.Llu?
?s Ma`rquez, Xavier Carreras, Kenneth C. Litkowski,and Suzanne Stevenson.
2008.
Semantic Role Label-ing: An Introduction to the Special Issue.
Computa-tional Linguistics, 34(2):145?159, June.Andre?
Martins, Noah Smith, and Eric Xing.
2009.
Con-cise integer linear programming formulations for de-pendency parsing.
In Proceedings of the Joint Con-ference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 342?350, Sun-tec, Singapore, August.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL?05), pages 91?98, Ann Arbor, Michigan, June.Adam Meyers, Ruth Reeves, Catherine Macleod, RachelSzekely, Veronika Zielinska, Brian Young, and RalphGrishman.
2004.
The NomBank Project: An interimreport.
In A. Meyers, editor, HLT-NAACL 2004 Work-shop: Frontiers in Corpus Annotation, pages 24?31,Boston, Massachusetts, USA, May.Alessandro Moschitti.
2004.
A study on convolutionkernels for shallow statistic parsing.
In Proceedingsof the 42nd Meeting of the Association for Computa-tional Linguistics (ACL?04), Main Volume, pages 335?342, Barcelona, Spain, July.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projectivedependency parsing.
In Proceedings of the 43rdAnnual Meeting of the Association for Computa-tional Linguistics (ACL?05), pages 99?106, Ann Ar-bor, Michigan, June.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106, March.Vasin Punyakanok, Dan Roth, and Wen tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(3):257?287, June.Sebastian Riedel and Andrew McCallum.
2011.
Fast androbust joint models for biomedical event extraction.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages 1?12,Edinburgh, Scotland, UK., July.Alexander M Rush, David Sontag, Michael Collins, andTommi Jaakkola.
2010.
On dual decomposition andlinear programming relaxations for natural languageprocessing.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 1?11, Cambridge, MA, October.David Sontag, Amir Globerson, and Tommi Jaakkola.2010.
Introduction to dual decomposition for infer-ence.
In S. Sra, S. Nowozin, and S. J. Wright, editors,Optimization for Machine Learning.
MIT Press.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The conll2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In CoNLL 2008: Proceedingsof the Twelfth Conference on Computational Natu-ral Language Learning, pages 159?177, Manchester,England, August.Ivan Titov, James Henderson, Paola Merlo, and GabrieleMusillo.
2009.
Online graph planarisation for syn-chronous parsing of semantic and syntactic dependen-cies.
In Proceedings of the 21st international jontconference on Artifical intelligence, IJCAI?09, pages1562?1567.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A global joint model for semanticrole labeling.
Computational Linguistics, 34(2):161?191, June.Dennis D. Wackerly, William Mendenhall, andRichard L. Scheaffer, 2007.
Mathematical Statis-tics with Applications, chapter 15: Nonparametricstatistics.
Duxbury Press.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Dekang Linand Dekai Wu, editors, Proceedings of EMNLP 2004,pages 88?94, Barcelona, Spain, July.Alexander S. Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Proceed-ings of the 18th conference on Computational linguis-tics, pages 947?953.Ben?at Zapirain, Eneko Agirre, Llu?
?s Ma`rquez, and MihaiSurdeanu.
2013.
Selectional preferences for semanticrole classification.
Computational Linguistics, 39(3).230
