Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2094?2103,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsEvaluating Sentiment Analysis in the Context of Securities TradingSiavash Kazemian and Shunan Zhao and Gerald PennDepartment of Computer ScienceUniversity of Toronto{kazemian,szhao,gpenn}@cs.toronto.eduAbstractThere are numerous studies suggestingthat published news stories have an im-portant effect on the direction of the stockmarket, its volatility, the volume of trades,and the value of individual stocks men-tioned in the news.
There is even somepublished research suggesting that auto-mated sentiment analysis of news docu-ments, quarterly reports, blogs and/or twit-ter data can be productively used as partof a trading strategy.
This paper presentsjust such a family of trading strategies, andthen uses this application to re-examinesome of the tacit assumptions behind howsentiment analyzers are generally evalu-ated, in spite of the contexts of their appli-cation.
This discrepancy comes at a cost.1 IntroductionThe proliferation of opinion-rich text on the WorldWide Web, which includes anything from productreviews to political blog posts, led to the growth ofsentiment analysis as a research field more than adecade ago.
The market need to quantify opinionsexpressed in social media and the blogosphere hasprovided a great opportunity for sentiment analy-sis technology to make an impact in many sectors,including the financial industry, in which interestin automatically detecting news sentiment in or-der to inform trading strategies extends back atleast 10 years.
In this case, sentiment takes ona slightly different meaning; positive sentiment isnot the emotional and subjective use of laudatorylanguage.
Rather, a news article that contains pos-itive sentiment is optimistic about the future finan-cial prospects of a company.Zhang and Skiena (2010) experimented withnews sentiment to inform simple market neutraltrading algorithms, and produced an impressivemaximum yearly return of around 30% ?
evenmore when using sentiment from blogs and twitterdata.
They did so, however, without an appropri-ate baseline, making it very difficult to appreciatethe significance of this number.
Using a very stan-dard, and in fact somewhat dated sentiment ana-lyzer, we are regularly able to garner annualizedreturns over twice that percentage, and in a man-ner that highlights two of the better design deci-sions that Zhang and Skiena (2010) made, viz., (1)their decision to trade based upon numerical SVMscores rather than upon discrete positive or nega-tive sentiment classes, and (2) their decision to golong (resp., short) in the n best- (worst-) rankingsecurities rather than to treat all positive (negative)securities equally.On the other hand, we trade based upon theraw SVM score itself, rather than its relative rankwithin a basket of other securities as Zhang andSkiena (2010) did, and we experimentally tune athreshold for that score that determines whether togo long, neutral or short.
We sampled our stocksfor both training and evaluation in two runs, onewithout survivor bias, the tendency for long po-sitions in stocks that are publicly traded as of thedate of the experiment to pay better using histor-ical trading data than long positions in randomstocks sampled on the trading days themselves.Most of the evaluations of sentiment-based tradingeither unwittingly adopt this bias, or do not need toaddress it because their returns are computed oververy brief historical periods.
We also provide ap-propriate trading baselines as well as Sharpe ratios(Sharpe, 1966) to attempt to quantify the relativerisk inherent to our experimental strategies.
Astacitly assumed by most of the work on this sub-ject, our trading strategy is not portfolio-limited,and our returns are calculated on a percentage ba-sis with theoretical, commission-free trades.2094It is important to understand at the outset, how-ever, that the purpose of this research was not tobeat Zhang and Skiena?s (2010) returns (althoughwe have), nor merely to conduct the first prop-erly controlled, sufficiently explicit, scientific testof the descriptive hypothesis that sentiment analy-sis is of benefit to securities trading (although, toour knowledge, we did).
The main purpose of thisstudy was in fact to reappraise the evaluation stan-dards used by the sentiment analysis community.It is not at all uncommon within this communityto evaluate a sentiment analyzer with a variety ofclassification accuracy or hypothesis testing scoressuch as F-measures, SARs, kappas or Krippendorfalphas derived from human-subject annotations ?even when more extensional measures are avail-able, such as actual market returns from historicaldata in the case of securities trading.
With Holly-wood films, another popular domain for automaticsentiment analysis, one might refer to box-officereturns or the number of award nominations thata film receives rather than to its star-rankings onreview websites where pile-on and confirmationbiases are widely known to be rampant.
Are theopinions of human judges, paid or unpaid, a suf-ficient proxy for the business cases that actuallydrive the demand for sentiment analyzers?We regret to report that they do not seem to be.As a case study to demonstrate this point (Sec-tion 4.3), we exhibit one particular modification toour experimental financial sentiment analyzer that,when evaluated against an evaluation test set sam-pled from the same pool of human-subject annota-tions as the analyzer?s training data, returns poorerperformance, but when evaluated against actualmarket returns, yields better performance.
Thisshould worry any researcher who relies on classifi-cation accuracies, because the improvements thatthey report, whether due to better feature selectionor different pattern recognition algorithms, may infact not be improvements at all.
Differences in theamount or degree of improvement might arguablybe rescalable, but Section 4.3 shows that such in-trinsic measures are not even accurate up to a de-termination of the delta?s sign.On the other hand, the results reported hereshould not be construed as an indictment of sen-timent analysis as a technology or its potential ap-plication.
In fact, one of our baselines alterna-tively attempts to train the same classifier directlyon market returns, and the experimental approachhandily beats that, too.
It is important to train onhuman-annotated sentiments, but then it is equallyimportant to tune, and eventually evaluate, on anempirically grounded task-specific measure, suchas market returns.
This paper thus presents, to ourknowledge, the first real proof that sentiment isworth analyzing in this or any other domain.A likely machine-learning explanation for thisexperimental result is that whenever two unbiasedestimators are pitted against each other, they oftenresult in an improved combined performance be-cause each acts as a regularizer against the other.If true, this merely attests to the relative indepen-dence of task-based and human-annotated knowl-edge sources.
A more HCI-oriented view, how-ever, would argue that direct human-subject anno-tations are highly problematic unless the annota-tions have been elicited in manner that is ecologi-cally valid.
When human subjects are paid to an-notate quarterly reports or business news, they arepaid regardless of the quality of their annotations,the quality of their training, or even their degreeof comprehension of what they are supposed to bedoing.
When human subjects post film reviews onweb-sites, they are participating in a cultural activ-ity in which the quality of the film under consider-ation is only one factor.
These sources of annota-tion have not been properly controlled in previousexperiments on sentiment analysis.Regardless of the explanation, this is a lessonthat applies to many more areas of NLP thanjust sentiment analysis, and to far more recentinstances of sentiment analysis than the one thatwe based our experiments on here.
Indeed, wechose sentiment analysis because this is an areathat can set a higher standard; it has the rightsize for an NLP component to be embedded inreal applications and to be evaluated properly.This is noteworthy because it is challenging to ex-plain why recent publications in sentiment anal-ysis research would so dramatically increase thevalue that they assign to sentence-level sentimentscoring algorithms based on syntactically compo-sitional derivations of ?good-for/ bad-for?
anno-tation (Anand and Reschke, 2010; Deng et al,2013), when statistical parsing itself has spent thelast twenty-five years staggering through a linguis-tically induced delirium as it attempts to documentany of its putative advances without recourse toclear empirical evidence that PTB-style syntacticderivations are a reliable approximation of seman-2095tic content or structure.We submit, in light of our experience with thepresent study, that the most crucial obstacle fac-ing the state of the art in sentiment analysis is nota granularity problem, nor a pattern recognitionproblem, but an evaluation problem.
Those evalu-ations must be task-specific to be reliable, and sen-timent analysis, in spite of our careless use of theterm in the NLP community, is not a task.
Stocktrading is a task ?
one of many in which a sen-timent analyzer is a potentially useful component.This paper provides an example of how to test thatutility.2 Related Work in Financial SentimentAnalysisStudies confirming the relationship between me-dia and market performance date back to atleast Niederhoffer (1971), who looked at NYTimes headlines and determined that large marketchanges were more likely following world eventsthan on random days.
Conversely, Tetlock (2007)looked at media pessimism and concluded thathigh media pessimism predicts downward prices.Tetlock (2007) also developed a trading strategy,achieving modest annualized returns of 7.3%.
En-gle and Ng (1993) looked at the effects of news onvolatility, showing that bad news introduces morevolatility than good news.
Chan (2003) claimedthat prices are slow to reflect bad news and stockswith news exhibit momentum.
Antweiler andFrank (2004) showed that there is a significant, butnegative correlation between the number of mes-sages on financial discussion boards about a stockand its returns, but that this trend is economicallyinsignificant.
Aside from Tetlock (2007), none ofthis work evaluated the effectiveness of an actualsentiment-based trading strategy.There is, of course, a great deal of work on au-tomated sentiment analysis itself; see Pang andLee (2008) for a survey.
More recent develop-ments germane to our work include the use of in-formation retrieval weighting schemes (Paltoglouand Thelwall, 2010), with which accuracies of upto 96.6% have models based upon Latent DirichletAllocation (LDA) (Lin and He, 2009).There has also been some work that analyzesthe sentiment of financial documents without actu-ally using those results in trading strategies (Kop-pel and Shtrimberg, 2004; Ahmad et al, 2006; Fuet al, 2008; O?Hare et al, 2009; Devitt and Ah-mad, 2007; Drury and Almeida, 2011).
As to therelationship between sentiment and stock price,Das and Chen (2007) performed sentiment anal-ysis on discussion board posts.
Using this, theybuilt a ?sentiment index?
that computed the time-varying sentiment of the 24 stocks in the MorganStanley High-Tech Index (MSH), and tracked howwell their index followed the aggregate price of theMSH itself.
Their sentiment analyzer was basedupon a voting algorithm, although they also dis-cussed a vector distance algorithm that performedbetter.
Their baseline, the Rainbow algorithm, alsocame within 1 percentage point of their reportedaccuracy.
This is one of the very few studies thathas evaluated sentiment analysis itself (as opposedto a sentiment-based trading strategy) against mar-ket returns (versus gold-standard sentiment anno-tations).
Das and Chen (2007) focused exclusivelyon discussion board messages and their evaluationwas limited to the stocks on the MSH, whereaswe focus on Reuters newswire and evaluate overa wide range of NYSE-listed stocks and marketcapitalization levels.Butler and Keselj (2009) try to determine sen-timent from corporate annual reports using bothcharacter n-gram profiles and readability scores.They also developed a sentiment-based tradingstrategy with high returns, but do not report howthe strategy works or how they computed the re-turns, making the results difficult to compare toours.
Basing a trading strategy upon annual re-ports also calls into question the frequency withwhich the trading strategy could be exercised.The work most similar to ours is Zhang andSkiena?s (2010).
They look at both financial blogposts and financial news, forming a market-neutraltrading strategy whereby each day, companies areranked by their reported sentiment.
The strat-egy then goes long and short on equal numbersof positive- and negative-sentiment stocks, respec-tively.
They conduct their trading evaluation overthe period from 2005 to 2009, and report a yearlyreturn of roughly 30% when using news data, andyearly returns of up to 80% when they use Twit-ter and blog data.
Crucially, they trade based uponthe ranked relative order of documents by senti-ment rather than upon the documents?
raw senti-ment scores.Zhang and Skiena (2010) compare their strategyto two baselines.
The ?Worst-sentiment?
Strat-egy trades the opposite of their strategy: short2096on positive-sentiment stocks and long on negativesentiment stocks.
The ?Random-selection?
Strat-egy randomly picks stocks to go long and shorton.
As trading strategies, these baselines set a verylow standard.
Our evaluation uses standard tradingbenchmarks such as momentum trading and hold-ing the S&P, as well as oracle trading strategiesover the same holding periods.3 Method and Materials3.1 News DataOur dataset combines two collections of Reutersnews documents.
The first was obtained for aroughly evenly weighted collection of 22 small-, mid- and large-cap companies, randomly sam-pled from the list of all companies traded on theNYSE as of 10thMarch, 1997.
The second wasobtained for a collection of 20 companies ran-domly sampled from those companies that werepublicly traded in March, 1997 and still listed on10thMarch, 2013.
For both collections of com-panies, we collected every chronologically thirdReuters news document about them from the pe-riod March, 1997 to March, 2013.
The news arti-cles prior to 10thMarch, 2005 were used as train-ing data, and the news articles on or after 10thMarch, 2005 were reserved as testing data.1Wesplit the dataset at a fixed date rather than ran-domly in order not to incorporate future news intothe classifier through lexical choice.In total, there were 1256 financial news docu-ments.
Each was labelled by two human annota-tors as being negative, positive, or neutral in sen-timent.
The annotators were instructed to gaugethe author?s belief about the company, rather thanto make a personal assessment of the company?sprospects.
Only the 991 documents that were la-belled twice as negative or positive were used fortraining and evaluation.3.2 Sentiment Analysis AlgorithmFor each selected document, we first filter outall punctuation characters and the most common429 stop words.
Because this is a document-level sentiment scoring task, not sentence-level,1An anonymous reviewer expressed concern aboutchronological bias in the training data relative to the test databecause of this decision.
While this may indeed influence ourresults, ecological validity requires us to situate all trainingdata before some date, and all testing data after that date, be-cause traders only have access to historical data before mak-ing a future trade.Representation Accuracybm25 freq 81.143%term presence 80.164%bm25 freq sw 79.827%freq with sw 75.564%freq 79.276%Table 1: Average 10-fold cross validation ac-curacy of the sentiment classifier using differentterm-frequency weighting schemes.
The samefolds were used in all feature sets.our sentiment analyzer is a support-vector ma-chine with a linear kernel function implementedusing SVMlight(Joachims, 1999), using all of itsdefault parameters.2We have experimented withraw term frequencies, binary term-presence fea-tures, and term frequencies weighted by the BM25scheme, which had the most resilience in thestudy of information-retrieval weighting schemesfor sentiment analysis by Paltoglou and Thelwall(2010).
We performed 10 fold cross-validation onthe training data, constructing our folds so thateach contains an approximately equal number ofnegative and positive examples.
This ensures thatwe do not accidentally bias a fold.Pang et al (2002) use word presence featureswith no stop list, instead excluding all words withfrequencies of 3 or less.
Pang et al (2002) nor-malize their word presence feature vectors, ratherthan term weighting with an IR-based scheme likeBM25, which also involves a normalization step.Pang et al (2002) also use an SVM with a linearkernel on their features, but they train and com-pute sentiment values on film reviews rather thanfinancial texts, and their human judges also clas-sified the training films on a scale from 1 to 5,whereas ours used a scale that can be viewed asbeing from -1 to 1, with specific qualitative inter-pretations assigned to each number.
Antweiler andFrank (2004) use SVMs with a polynomial kernel(of unstated degree) to train on word frequenciesrelative to a three-valued classification, but theyonly count frequencies for the 1000 words withthe highest mutual information scores relative tothe classification labels.
Butler and Keselj (2009)also use an SVM trained upon a very different setof features, and with a polynomial kernel of degree2There has been one important piece of work (Tang et al,2015) on neural computing architectures for document-levelsentiment scoring (most neural computing architectures forsentiment scoring are sentence-level), but the performanceof this type of architecture is not mature enough to replaceSVMs just yet.20973.As a sanity check, we measured our sentimentanalyzer?s accuracy on film reviews by trainingand evaluating on Pang and Lee?s (2004) filmreview dataset, which contains 1000 positivelyand 1000 negatively labelled reviews.
Pang andLee conveniently labelled the folds that they usedwhen they ran their experiments.
Using thesesame folds, we obtain an average accuracy of86.85%, which is comparable to Pang and Lee?s86.4% score for subjectivity extraction.
The pur-pose of this comparison is simply to demonstratethat our implementation is a faithful rendering ofPang and Lee?s (2004) algorithm.Table 1 shows the performance of SVM withBM25 weighting on our Reuters evaluation setversus several baselines.
All baselines are iden-tical except for the term weighting schemes used,and whether stop words were removed.
As can beobserved, SVM-BM25 has the highest sentimentclassification accuracy: 80.164% on average overthe 10 folds.
This compares favourably with pre-vious reports of 70.3% average accuracy over 10folds on financial news documents (Koppel andShtrimberg, 2004).
We will nevertheless adhere tonormalized term presence for now, in order to stayclose to Pang and Lee?s (2004) implementation.3.3 Trading AlgorithmOverall, our trading strategy is simple: go longwhen the classifier reports positive sentiment in anews article about a company, and short when theclassifier reports negative sentiment.We will embed the aforementioned sentimentanalyzer into three different trading algorithms.In Section 4.1, we use the discrete polarity re-turned by the classifier to decide whether golong/abstain/short a stock.
In Section 4.2.1 weinstead use the distance of the current documentfrom the classifier?s decision boundary reportedby the SVM.
These distances do have meaning-ful interpretations apart from their internal use inassigning class labels.
Platt (Platt, 1999) showedthat they can be converted into posterior proba-bilities, for example, by fitting a sigmoid func-tion onto them, but we will simply use the rawdistances.
In Section 4.2.2, we impose a safetyzone onto the interpretation of these raw distancescores.4 ExperimentsIn the experiments of this section, we will evaluatean entire trading strategy, which includes the senti-ment analyzer and the particulars of the trading al-gorithm itself.
The purpose of these experimentsis to refine the trading strategy itself and so thesentiment analyzer will be held constant.
In Sec-tion 4.3, we will hold the trading strategy constant,and instead vary the document representation fea-tures in the underlying sentiment analyzer.In all three experiments, we compare the per-position returns of the following four standardstrategies, where the number of days for which aposition is held remains constant:1.
The momentum strategy computes the priceof the stock h days ago, where h is the hold-ing period.
Then, it goes long for h days ifthe previous price is lower than the currentprice.
It goes short otherwise.2.
The S&P strategy simply goes long on theS&P 500 for the holding period.
This strat-egy completely ignores the stock in questionand the news about it.3.
The oracle S&P strategy computes the valueof the S&P 500 index h days into the future.If the future value is greater than the currentday?s value, then it goes long on the S&P 500index.
Otherwise, it goes short.4.
The oracle strategy computes the value of thestock h days into the future.
If the futurevalue is greater than the current day?s value,then it goes long on the stock.
Otherwise, itgoes short.The oracle and oracle S&P strategies are includedas toplines to determine how close the experimen-tal strategies come to ones with perfect knowledgeof the future.
?Market-trained?
is the same as ?ex-perimental?
at test time, but trains the sentimentanalyzer on the market return of the stock in ques-tion for h days following a training article?s publi-cation, rather than the article?s annotation.4.1 Experiment One: Utilizing SentimentLabelsGiven a news document for a publicly traded com-pany, the trading agent first computes the senti-ment class of the document.
If the sentiment ispositive, the agent goes long on the stock on thedate the news is released; if negative, it goes short.2098Strategy Period Return S. RatioExperimental30 days -0.037% -0.0025 days 0.763% 0.0943 days 0.742% 0.1001 day 0.716% 0.108Momentum30 days 1.176% 0.0665 days 0.366% 0.0453 days 0.713% 0.0961 day 0.017% -0.002S&P30 days 0.318% 0.0595 days -0.038% -0.0163 days -0.035% -0.0171 day 0.046% 0.036Oracle S&P30 days 3.765% 0.9595 days 1.617% 0.9743 days 1.390% 0.9491 day 0.860% 0.909Oracle30 days 11.680% 0.8745 days 5.143% 0.8093 days 4.524% 0.7611 day 3.542% 0.630Market-trained30 days 0.286% 0.0165 days 0.447% 0.0543 days 0.358% 0.0481 day 0.533% 0.080Table 2: Returns and Sharpe ratios for the Experi-mental, baseline and topline trading strategies over30, 5, 3, and 1 day(s) holding periods.All trades are made based on the adjusted closingprice on this date.
We evaluate the performance ofthis strategy using four different holding periods:30, 5, 3, and 1 day(s).The returns and Sharpe ratios are presented inTable 2 for the four different holding periods andthe five different trading strategies.
The Sharperatio is a return-to-risk ratio, with a high value in-dicating good return for relatively low risk.
TheSharpe ratio is calculated as: S =E[Ra?Rb]?var(Ra?Rb),where Rais the return of a single asset and Rbis the risk-free return of a 10-year U.S. Treasurynote.The returns from this experimental trading sys-tem are fairly low, although they do beat the base-lines.
A one-way ANOVA test among the exper-imental, momentum and S&P strategies using thepercent returns from the individual trades yields pvalues of 0.06493, 0.08162, 0.1792, and 0.4164,respectively, thus failing to reject the null hypoth-esis that the returns are not significantly higher.33An anonymous reviewer observed that Tetlock (2007)showed a statistically significant improvement from the useof sentiment, apparently contradicting this result.
Tetlock?s(2007) sentiment-based trading strategy used a safety zone(see Section 4.2.2), and was never compared to a realisticbaseline or control strategy.
Instead, Tetlock?s (2007) sig-nificance test was conducted to demonstrate that his returns(positive in 12 of 15 calendar years of historical market data)Figure 1: Percent returns for 1 day holding periodversus market capitalization of the traded stocks.Furthermore, the means and medians of all threetrading strategies are approximately the same andcentred around 0.
The standard deviations of theexperimental strategy and the momentum strategyare nearly identical, differing only in the thou-sandths digit.
The standard deviations for the S&Pstrategy differ from the other two strategies due tothe fact that the strategy buys and sells the entireS&P 500 index and not the individual stocks de-scribed in the news articles.
There is, in fact, noconvincing evidence that discrete sentiment classleads to an improved trading strategy from this orany other study with which we are familiar, basedon their published details.
One may note, how-ever, that the returns from the experimental strat-egy have slightly higher Sharpe ratios than eitherof the baselines.One may also note that using a sentiment ana-lyzer mostly beats training directly on market data.This vindicates using sentiment annotation as aninformation source.Figure 1 shows the market capitalizations ofeach individual trade?s companies plotted againsttheir percent return with a 1 day holding period.The correlation between the two variables is notsignificant.
Returns for the other holding periodsare similarly dispersed.The importance of having good baselines isdemonstrated by the fact that when we annualizeour returns for the 3-day holding period, we get70.086%.
This number appears very high, but theannualized return from the momentum strategy iswere unlikely to have been generated by chance from a nor-mal distribution centred at zero.209970.066%4, which is not significantly lower.Figure 2 shows the percent change in sharevalue plotted against the raw SVM score for thedifferent holding periods.
We can see a weak cor-relation between the two.
For the 30 days, 5 days,3 days, and 1 day holding periods, the correlationsare 0.017, 0.16, 0.16, and 0.16, respectively.
Theline of best fit is shown.
This prompts our nextexperiment.4.2 Utilizing SVM scores4.2.1 Experiment Two: Variable SingleThresholdBefore, we labelled documents as positive (nega-tive) when the score was above (below) 0, because0 was the decision boundary.
But 0 might not bethe best threshold, ?, for high returns.
To deter-mine ?, we divided the evaluation dataset, i.e.
thedataset with news articles dated on or after March10, 2005, into two folds having an equal number ofdocuments with positive and negative sentiment.We used the first fold to determine ?
and tradedusing the data from the second fold and ?.
For ev-ery news article, if the SVM score for that article isabove (below) ?, then we go long (short) on the ap-propriate stock on the day the article was released.A separate theta was determined for each holdingperiod.
We varied ?
from ?1 to 1 in increments of0.1.Using this method, we were able to obtain sig-nificantly higher returns.
In order of 30, 5, 3, and1 day holding periods, the returns were 0.057%,1.107%, 1.238%, and 0.745% (p < 0.001 in ev-ery case).
This is a large improvement over theprevious returns, as they are average per-positionfigures.54.2.2 Experiment Three: Safety ZonesFor every news item classified, SVM outputs ascore.
For a binary SVM with a linear kernel func-tion f , given some feature vector x, f(x) can beviewed as the signed distance of x from the de-cision boundary (Boser et al, 1992).
It is thenpossibly justified to interpret raw SVM scores asdegrees to which an article is positive or negative.As in the previous section, we separate the eval-uation set into the same two folds, only now we4The momentum strategy has a different number of possi-ble trades in any actual calendar year because it is a functionof the holding period.5Training directly on market data, by comparison, yields-0.258%, -0.282%, -0.036% and -0.388%, respectively.Representation Accuracy 30 days 5 days 3 days 1 dayterm presence 80.164% 3.843% 1.851% 1.691% 2.251%bm25 freq 81.143% 1.110% 1.770% 1.781% 0.814%bm25 freq dnc 62.094% 3.458% 2.834% 2.813% 2.586%bm25 freq sw 79.827% 0.390% 1.685% 1.581% 1.250%freq 79.276% 1.596% 1.221% 1.344% 1.330%freq with sw 75.564% 1.752% 0.638% 1.056% 2.205%Table 3: Sentiment classification accuracy (aver-age 10-fold cross-validation) and trade returns ofdifferent feature sets and term frequency weight-ing schemes in Exp.
3.
The same folds wereused for the different representations.
The non-annualized returns are presented in columns 3-6.use two thresholds, ?
?
?.
We will go long whenthe SVM score is above ?, abstain when the SVMscore is between ?
and ?, and go short when theSVM score is below ?.
This is a strict generaliza-tion of the above experiment, in which ?
= ?.For convenience, we will assume in this sectionthat ?
= ?
?, leaving us again with one parameterto estimate.
We again vary ?
from 0 to 1 in in-crements of 0.1.
Figure 3 shows the returns as afunction of ?
for each holding period on the devel-opment dataset.
If we increased the upper boundon ?
to be greater than 1, then there would be toofew trading examples (less than 10) to reliably cal-culate the Sharpe ratio.
Using this method with?
= 1, we were able to obtain even higher returns:3.843%, 1.851%, 1.691, and 2.251% for the 30,5, 3, and 1 day holding periods, versus 0.057%,1.107%, 1.238%, and 0.745% in the second task-based experiment.4.3 Experiment Four: Feature SelectionIn our final experiment, let us now hold the trad-ing strategy fixed (at the third one, with safetyzones) and turn to the underlying sentiment ana-lyzer.
With a good trading strategy in place, it isclearly possible to vary some aspect of the senti-ment analyzer in order to determine its best settingin this context.
We will measure both market re-turn and classifier accuracy to determine whetherthey agree.
Is the latter a suitable proxy for the for-mer?
Indeed, we may hope that classifier accuracywill be more portable to other possible tasks, butthen it must at least correlate well with task-basedperformance.In addition to evaluating those feature sets at-tempted in Section 3.2, we now hypothesize thatthe passive voice may be useful to emphasize inour representations, as the existential passive canbe used to evade responsibility.
So we add to the2100Figure 2: Percent change of trade returns plotted against SVM values for the 1, 3, 5, and 30 day holdingperiods in Exp.
1.
Graphs are cropped to zoom in.Figure 3: Returns for different thresholds on the development data for 30, 5, 3, and 1 day holding periodsin Exp.
2 with safety zone.2101BM25 weighted vector the counts of word tokensending in ?n?
or ?d?
as well as the total count ofevery conjugated form of the copular verb: ?be?,?is?, ?am?, ?are?, ?were?, ?was?, and ?been?.These three features are superficial indicators ofthe passive voice.
Clearly, we could have used apart-of-speech tagger to detect the passive voicemore reliably, but we are more interested herein how well our task-based evaluation will cor-respond to a more customary classifier-accuracyevaluation, rather than finding the world?s best in-dicators of the passive voice.Table 3 presents returns obtained from these 6feature sets.
The feature set with BM25-weightedterm frequencies plus the number of copulars andtokens ending in ?n?, ?d?
(bm25 freq dnc) yieldshigher returns than any other representation at-tempted on the 5, 3, and 1 day holding periods, andthe second-highest on the 30 days holding period.But it has the worst classification accuracy by far:a full 18 percentage points below term presence.This is a very compelling illustration of how mis-leading an intrinsic evaluation can be.5 ConclusionIn this paper, we examined sentiment analysis ap-plied to stock trading strategies.
We built a bi-nary sentiment classifier that achieves high accu-racy when tested on movie data and financial newsdata from Reuters.
In four task-based experiments,we evaluated the usefulness of sentiment analysisto simple trading strategies.
Although high an-nual returns are achieved simply by utilizing sen-timent labels while trading, they can be increasedby incorporating the output of the SVM?s decisionfunction.
But classification accuracy alone is notan accurate predictor of task-based performance.This calls into question the suitability of intrinsicsentiment classification accuracy, particularly (ashere) when the relative cost of a task-based eval-uation may be comparably low.
We have also de-termined that training on human-annotated senti-ment does in fact perform better than training onmarket returns themselves.
So sentiment analysisis an important component, but it must be tunedagainst task data.Our price data only included adjusted openingand closing prices and most of our news data con-tain only the date of the article, with no specifictime.
This limits our ability to test much shorter-term trading strategies.Deriving sentiment labels for supervised train-ing is an important topic for future study, as isinferring the sentiment of published news fromstock price fluctuations instead of the reverse.
Weshould also study how ?sentiment?
is defined inthe financial world.
This study has used a rathergeneral definition of news sentiment, and a moreprecise definition may improve trading perfor-mance.AcknowledgmentsThis research was supported by the Canadian Net-work Centre of Excellence in Graphics, Animationand New Media (GRAND).ReferencesKhurshid Ahmad, David Cheng, and Yousif Almas.2006.
Multi-lingual sentiment analysis of financialnews streams.
In Proceedings of the 1st Interna-tional Conference on Grid in Finance.Pranav Anand and Kevin Reschke.
2010.
Verb classesas evaluativity functor classes.
In InterdisciplinaryWorkshop on Verbs: The Identification and Repre-sentation of Verb Features (Verb 2010).Werner Antweiler and Murray Z Frank.
2004.
Is allthat talk just noise?
the information content of inter-net stock message boards.
The Journal of Finance,59(3):1259?1294.Bernhard E. Boser, Isabelle M. Guyon, andVladimir N. Vapnik.
1992.
A training algo-rithm for optimal margin classifiers.
In Proceedingsof the fifth annual workshop on Computationallearning theory, COLT ?92, pages 144?152, NewYork, NY, USA.
ACM.Matthew Butler and Vlado Keselj.
2009.
Finan-cial forecasting using character n-gram analysis andreadability scores of annual reports.
In Proceedingsof Canadian AI?2009, Kelowna, BC, Canada, May.Wesley S. Chan.
2003.
Stock price reaction to newsand no-news: Drift and reversal after headlines.Journal of Financial Economics, 70(2):223?260.Sanjiv R. Das and Mike Y. Chen.
2007.
Yahoo!
foramazon: Sentiment extraction from small talk on theweb.
Management Science, 53(9):1375?1388.Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.2013.
Benefactive/malefactive event and writer at-titude annotation.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 120?125.Association for Computational Linguistics.Ann Devitt and Khurshid Ahmad.
2007.
Sentimentpolarity identification in financial news: A cohesion-based approach.
In Proceedings of the ACL.2102Brett Drury and J. J. Almeida.
2011.
Identificationof fine grained feature based event and sentimentphrases from business news stories.
In Proceedingsof the International Conference on Web Intelligence,Mining and Semantics, WIMS ?11, pages 27:1?27:7,New York, NY, USA.
ACM.Robert F. Engle and Victor K. Ng.
1993.
Measuringand testing the impact of news on volatility.
TheJournal of Finance, 48(5):1749?1778.Tak-Chung Fu, Ka ki Lee, Donahue C. M. Sze, Fu-LaiChung, Chak man Ng, and Chak man Ng.
2008.Discovering the correlation between stock time se-ries and financial news.
In Web Intelligence, pages880?883.Thorsten Joachims.
1999.
Making large-scale svmlearning practical.
advances in kernel methods-support vector learning, b. sch?olkopf and c. burgesand a. smola.Moshe Koppel and Itai Shtrimberg.
2004.
Good newsor bad news?
let the market decide.
In AAAI SpringSymposium on Exploring Attitude and Affect in Text,pages 86?88.
Press.Chenghua Lin and Yulan He.
2009.
Joint senti-ment/topic model for sentiment analysis.
In Pro-ceedings of the 18th ACM conference on Informa-tion and knowledge management, CIKM ?09, pages375?384, New York, NY, USA.
ACM.Victor Niederhoffer.
1971.
The analysis of worldevents and stock prices.
Journal of Business, pages193?219.Neil O?Hare, Michael Davy, Adam Bermingham,Paul Ferguson, P?araic Sheridan, Cathal Gurrin, andAlan F. Smeaton.
2009.
Topic-dependent senti-ment analysis of financial blogs.
In Proceedingsof the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion measurement.Georgios Paltoglou and Mike Thelwall.
2010.
A studyof information retrieval weighting schemes for sen-timent analysis.
In Proceedings of the ACL, pages1386?1395.
Association for Computational Linguis-tics.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe ACL, pages 271?278.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in natu-ral language processing - Volume 10, EMNLP ?02,pages 79?86, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.John C. Platt.
1999.
Probabilistic outputs for sup-port vector machines and comparisons to regularizedlikelihood methods.
In Advances in Large MarginClassifiers, pages 61?74.
MIT Press.William F Sharpe.
1966.
Mutual fund performance.The Journal of business, 39(1):119?138.Duyu Tang, Bing Qin, and Ting Liu.
2015.
Documentmodeling with gated recurrent neural network forsentiment classification.
In Proceedings of EMNLP,pages 1422?1432.Paul C. Tetlock.
2007.
Giving content to investor sen-timent: The role of media in the stock market.
TheJournal of Finance, 62(3):1139?1168.Wenbin Zhang and Steven Skiena.
2010.
Tradingstrategies to exploit blog and news sentiment.
In The4th International AAAI Conference on Weblogs andSocial Media.2103
