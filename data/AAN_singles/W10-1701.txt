Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 1?10,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsA Semi-supervised Word Alignment Algorithmwith Partial Manual AlignmentsQin Gao, Nguyen Bach and Stephan VogelLanguage Technologies InstituteCarnegie Mellon University5000 Forbes Avenue, Pittsburgh PA, 15213{qing, nbach, stephan.vogel}@cs.cmu.eduAbstractWe present a word alignment frameworkthat can incorporate partial manual align-ments.
The core of the approach is anovel semi-supervised algorithm extend-ing the widely used IBM Models witha constrained EM algorithm.
The par-tial manual alignments can be obtainedby human labelling or automatically byhigh-precision-low-recall heuristics.
Wedemonstrate the usages of both methodsby selecting alignment links from manu-ally aligned corpus and apply links gen-erated from bilingual dictionary on unla-belled data.
For the first method, we con-duct controlled experiments on Chinese-English and Arabic-English translationtasks to compare the quality of word align-ment, and to measure effects of two differ-ent methods in selecting alignment linksfrom manually aligned corpus.
For thesecond method, we experimented withmoderate-scale Chinese-English transla-tion task.
The experiment results show anaverage improvement of 0.33 BLEU pointacross 8 test sets.1 IntroductionWord alignment is used in various natural lan-guage processing applications, and most statisticalmachine translation systems rely on word align-ment as a preprocessing step.
Traditionally theword alignment model is trained in an unsuper-vised manner, e.g.
the most widely used toolGIZA++ (Och and Ney, 2003), which implementsthe IBM Models (Brown et.
al., 1993) and theHMM model (Vogel et al, 1996).
However, forlanguage pairs such as Chinese-English, the wordalignment quality is often unsatisfactory (Guzmanet al, 2009).
There has been increasing interest onusing manual alignments in word alignment tasks.Ittycheriah and Roukos (2005) proposed to useonly manual alignment links in a maximum en-tropy model.
A number of semi-supervised wordaligners are proposed (Blunsom and Cohn, 2006;Niehues and Vogel, 2008; Taskar et al, 2005; Liuet al, 2005; Moore, 2005).
These approaches useheld-out manual alignments to tune the weightsfor discriminative models, with the model param-eters, model scores or alignment links from un-supervised word aligners as features.
Also, sev-eral models are proposed to address the prob-lem of improving generative models with smallamount of manual data, including Model 6 (Ochand Ney, 2003) and the model proposed by Fraserand Marcu (2006) and its extension called LEAFaligner (Fraser and Marcu, 2007).
The approachesuse labelled data to tune parameters to combinedifferent components of the IBM Models.2005?
?
?
?2005nian     de      xiatianThe   summer   of    2005Figure 1: Partial and full alignmentsAn interesting question is, if we only have par-tial alignments of sentences, can we make use ofthem?
Figure 1 shows the comparison of par-tial alignments (the bold link) and full alignments(both of the dashed and the bold links).
A partialalignment of a sentence only provides a portion oflinks of the full alignment.
Although it seems to betrivial, they actually convey different information.In the example, if the full alignment is given, wecan assert 2005 is only aligned to 2005nian, not tode or xiatian, but if only the partial alignment isgiven we cannot make such assertion.Partial alignments can be obtained from vari-ous sources, for example, we can fetch them bymanually correcting unsupervised alignments, bysimple heuristics such as dictionaries of technical1terms, by rule-based alignment systems that havehigh accuracy but low recall rate.
The function-ality is considered useful in many scenarios.
Forexample, the researchers can analyse the align-ments generated by GIZA++ and fix commonerror patterns, and perform training again.
Onanother way, an application can combine activelearning (Arora et al, 2009) and crowdsourcing,asking non-expertise such as workers of AmazonMechanical Turk to label crucial alignment linksthat can improve the system with low cost, whichis now a promising methodology in NLP areas(Callison-Burch, 2009).In this paper, we propose a semi-supervised ex-tension of the IBM Models that can utilize partialalignment links.
More specifically, we are seekinganswers for the following questions:?
Given the partial alignment of a sentence,how to find the most probable alignment thatis consistent with the partial alignment.?
Given a set of partially aligned sentences,how to get the parameters that maximize thelikelihood of the sentence pairs with align-ments consistent with the partial alignments?
Given a set of partially aligned sentences,with conflicting partial alignments, how toanswer the two questions above.In the proposed approach, the manual partialalignment links are treated as ground truth, there-fore, they will be fixed.
However, for all otherlinks we make no additional assumption.
Whenusing manual alignments, there can be links con-flicting with each other.
These conflicting evi-dences are treated as options and the generativemodel will choose the most probable alignmentfrom them.
An efficient training algorithm forfertility-based models is proposed.
The algorithmmanipulates the Moving and Swapping matricesused in the hill-climbing algorithm (Och and Ney,2003) to rule out inconsistent alignments in bothE-step and M-step of the training.A similar attempt has been made by Callison-Burch et al (2004), where the authors interpo-late the parameters estimated by sentence-alignedand word-aligned corpus.
Our approach is differ-ent from their method that we do not require fullyaligned data and we do not need to interpolate twoparameter sets.
All the training is done within aunified framework.
Our approach is also differentfrom LEAF (Fraser and Marcu, 2007) and Model6 (Och and Ney, 2003) that we do not use theseadditional links to tune additional parameters tocombine model components, as a result, it is notlimited to fully aligned corpus.A question may raise why the proposed methodis superior over using the partial alignment linksas features in discriminative aligners?
There arethree possible explanations.
First, the method pre-serves the power of the generative model in whichthe algorithm utilizes large amount of unlabeleddata.
More importantly, the additional informationcan propagate over the whole corpus through bet-ter estimation of model parameters.
In contrast, ifwe use the alignment links in discriminative align-ers as a feature, one link can only affect the par-ticular word, or at most the sentence.
Second, al-though the discriminative word alignment meth-ods provide flexibility to utilize labeled data, mostof them still rely on generative aligners.
Somerely on the model parameters of the IBM Mod-els (Liu et al, 2005; Blunsom and Cohn, 2006),others rely on the alignment links from GIZA++as features or as training data (Taskar et al, 2005),or use both the model parameters and the align-ment links (Niehues and Vogel, 2008).
Therefore,improving the generative aligner is still importanteven when using discriminative aligners.
Third,these methods require full alignment of sentencesto provide positive (aligned) and negative (non-aligned) information, which limits the availabilityof data (Niehues and Vogel, 2008).The proposed method has been successfully ap-plied on various tasks, such as utilizing manualalignments harvested from Amazon MechanicalTurk (Gao and Vogel, 2010), and active learningmethods for improving word alignment (Ambatiet al, 2010).
This paper provides the detailed al-gorithm of the method and controlled experimentsto demonstrate its behavior.The paper is organized as follows, in section2 we describe the proposed model as well as themodified training algorithm.
Section 3 presentstwo approaches of obtaining manual alignmentlinks, The experimental results will be shown insection 4.
We conclude the paper in section 5.2 Semi-supervised word alignment2.1 Problem SetupThe IBM Models (Brown et.
al., 1993) are aseries of generative models for word alignment.GIZA++ (Och and Ney, 2003) is the most widelyused implementation of the IBM Models and the2HMM model (Vogel et al, 1996).
Given twostrings from target and source languages fJ1 =f1, ?
?
?
, fj , ?
?
?
fJ and eI1 = e1, ?
?
?
, ei, ?
?
?
eI , analignment of the sentence pair is defined as aJ1 =[a1, a2, ?
?
?
, aJ ], aj ?
[0, I].
The IBM Modelsassume all the target words must be covered ex-actly once (Brown et.
al., 1993).
We try to modelP (fJ1 |eI1), which is the probability of observingsource sentence given target sentence eI1.
In sta-tistical models a hidden alignment variable is in-troduced, so that we can write the probability asP (fJ1 |eI1) =?aJ1Pr(fJ1 , aJ1 |eJ1 , ?
), where Pr(?
)is the estimated probability given the parameter set?.
The IBM Models define several different set ofparameters, from Model 1 to Model 5.
Startingfrom Model 3, the fertility model is introduced.EM algorithm is employed to estimate themodel parameters of the IBM Models.
In E-step,it is possible to obtain sufficient statistics fromall possible alignments with simplified formulasfor simple models such as Model 1 and Model 2.Meanwhile for fertility-based models, enumerat-ing all possibilities is NP-complete and hence itcannot be carried out for long sentences.
A solu-tion is to explore only the ?neighbors?
of Viterbialignments.
However, obtaining Viterbi align-ments itself is NP-complete for these models.
Inpractice, a greedy algorithm is employed to finda local optimal alignments based on Viterbi align-ments generated by simpler models.First, we define the neighbor alignments of a asthe set of alignments that differ by one of the twooperators from the original ?center alignment?.?
Move operator m[i,j], that changes aj := i,i.e.
arbitrarily set word fj in source sentenceto align to word fi in target sentence.?
Swap operator s[j1,j2] that exchanges aj1 andaj2 .We denote the neighbor alignments set ofcurrent center alignment a as nb(a).
Ineach step of hill-climbing algorithm, we findthe alignment b(a) in nb(a), s.t.
b(a) =argmaxa?
?nb(a) p(a?|e, f), and update the currentcenter alignment.
The algorithm iterates untilthere is no update could be made.
The statistics ofthe neighbor alignments of the final center align-ment will be collected for normalization step (M-step).
The algorithm is greedy, so a reasonablestart point is important.
In practice GIZA++ usesModel 2 or HMM to generate the seed alignment.To improve the speed of hill climbing, GIZA++caches the cost of all possible move and swap op-erations in two matrices.
In the so called MovingMatrix M , the element Mij stores the likelihooddifference of a move operator aj = i:Mij =Pr(m[i,j](a)|e, f)Pr(a|e, f)?
(1?
?
(aj , i)) (1)and in the Swapping Matrix S, the element Sjj?stores the likelihood difference of a swap operatorbetween aj and aj?
:Sjj?
={Pr(S[j,j?
](a)|e,f)Pr(a|e,f) ?
(1?
?
(aj , aj?))
if j < j?0 otherwise(2)The matrices will be updated whenever an oper-ator is made, but the update is limited to the rowsand columns involved in the operator.We define a partial alignment of a sentencepair (fJ1 , eI1) as ?JI = {(i, j), 0 ?
i < I, 0 ?j < J}, note that the partial alignment does notassume 1-to-N restriction on either side, and theword from neither source nor target side need to becovered with links.
If an index is missing, it doesnot mean the word is aligned to the empty word.Instead it just means no information is provided.We use a link (0, j) or (i, 0) to explicitly representthe information that word fj or ei is aligned to theempty word.In order to find the most probable align-ment that is consistent the partial alignments,we treat the partial alignment as constraints, i.e.for an alignment aJ1 = [a1, a2, ?
?
?
, aj ] on thesentence pair fJ1 , eI1, the translation probabilityPr(fJ1 , aJ1 |eI1, ?JI ) will be zero if the alignment isinconsistent with the partial alignments.Pr(fJ1 |eI1, aJ1 , ?JI ) ={0, aJ1 is inconsistent with?JIPr(fJ1 |eI1, aJ1 , ?
), otherwise(3)Under the constraints of the IBM Models, thereare two situations that aJ1 is inconsistent with ?JI :1.
Target word misalignment: The IBM Modelsassume one target word can only be alignedto one source word.
Therefore, if the targetword fj aligns to a source word ei, while theconstraint ?JI suggests fj should be alignedto ei?
, the alignment violates the constraintand thus is considered inconsistent.32005???
?thesummerof2005Manual Alignment Link(a)2005???
?thesummerOf2005Seed Alignment Consistent Alignment Center Alignment(b)                    (c)2005???
?thesummerof2005Figure 2: Illustration of Algorithm 12.
Source word to empty word misalignment:Since one source word can be aligned to mul-tiple target words, it is hard to constrain thealignments of source words.
However, if asource word is aligned to the empty word,it cannot be aligned to any concrete targetword.However, we are facing the problem of con-flicting evidences.
The problem is not necessar-ily caused by errors in manual alignments, butthe assumption of the IBM Models that one tar-get word can only be aligned to one source word.This assumption causes multiple alignment linksfrom one target word conflict with each other.
Inthis case, we relax the constraints of situation 1that if the alignment link aj?
is consistent with anytarget-to-source links (i, j) that j = j?, it will beconsidered consistent.
Also, we arbitrarily assignthe source word to empty word constraints higherpriorities than other constraints.In EM algorithm, to ensure the final model bemarginalized on the fixed alignment links, andthe final Viterbi alignment is consistent with thefixed alignment links, we need to guarantee thatno statistics from inconsistent alignments be col-lected into the sufficient statistics.
On fertility-based models, we have to make sure:1.
The hill-climbing algorithm outputs align-ment links consistent with the fixed align-ment links.2.
The count collection algorithm rules out allthe inconsistent statistics.With the constrained hill-climbing algorithmand count collection algorithm which will be de-scribed below, the above two criteria are satisfied.2.2 Constrained hill-climbing algorithmAlgorithm 1 shows the algorithm outline of con-strained hill-climbing.
First, similar to the originalhill-climbing algorithm described above, HMM(or Model 2) is used to obtain a seed alignment.To ensure the resulting center alignment be con-sistent with manual alignment, we need to split theAlgorithm 1 Constrained Hill-Climbing1: Calculate the seed alignment a0 using HMM model2: while ic(a0) > 0 do3: if {a : ic(a) < ic(a0)} = ?
then4: break5: end if6: a0 := argmaxa?nb(a0),ic(a)<ic(a0) Pr(f |e, a)7: end while8: Mij := ?1 if (i, j) 6?
?JI or (i, 0) ?
?JI9: loop10: Sjj?
:= ?1 if (j, aj?)
6?
?JI or (j?, aj) 6?
?JI11: Mi1j1 = argmaxMij ; Sj1j?1 = argmaxSij12: if Mi1j1 ?
1 and Sj1j?1 ?
1 then13: Break14: end if15: if Mi1j1 > Sj1j?1 then16: Update Mi1?,Mj1?,M?i1 ,M?j1and Si1?, Sj1?, S?i1 , S?j1 , set a0 := Mi1j1(a0)17: else18: Update Mj1?,Mj?1?,M?j1 ,M?j?1and Sj?1?, Sj1?, S?j?1 , S?j1 , set a0 := Sj1j?1(a0)19: end if20: end loop21: Return a0hill-climbing algorithm into two stages, i.e.
opti-mize towards the constraints and towards the opti-mal alignment under the constraints.From a seed alignment, we first try to move thealignment towards the constraints by choosing amove or swap operator that:1. has highest likelihood among alignmentsgenerated by other operators, excluding theoriginal alignment,2.
eliminates at least one inconsistent link.The first step reflects in line 2 through 7 in thealgorithm, where we use ic(?)
to denote the totalnumber of inconsistent links in the alignment, andnb(?)
to denote the neighbor alignments.We iteratively update the alignment until no ad-ditional inconsistent link can be removed.
The al-gorithm implies that we force the seed alignmentto become closer to the constraints while tryingto find the best consistent alignment.
Figure 2demonstrates the idea, given the manual alignmentlink shown in (a), and the seed alignment shown assolid links in (b), we move the inconsistent link tothe dashed link by a move operation.After we find the consistent alignment, we pro-ceed to optimize towards the optimal alignmentwithin the constraints.
The algorithm sets the cellsto negative if the corresponding operations are notallowed.
The Moving matrix only need to be up-dated once, as in line 8 of the algorithm.
Whereasthe swapping matrix need to be updated every it-4eration, Since once the alignment is updated, thepossible violations will also change.
This is donein line 10.If source words ik are aligned to the emptyword, we set Mik,j = ?1,?j, as shown in line 8.The swapping matrix does not need to be modifiedin this case because the swapping operator will notintroduce new links.
Again, Figure 2 demonstratesthe optimization step in (c), two move operatorsor one swap operator can move the link markedwith cross to the dashed line, which can be a bet-ter alignment.Because the cells that can lead to violations areset to negative, the operators will never be pickedin line 11, therefore we effectively ensure the con-sistency of the final center alignment.The algorithm will end when no better updatecan be made (line 12 through 14), otherwise, wepick the new update with highest likelihood as newcenter alignment and update the cells in the Mov-ing and Swapping matrices that will be affectedby the update.
Line 15 through line 19 performthe operation.2.3 Count CollectionAfter finding the center alignment, we collectcounts from the neighbor alignments so that theM-step can normalize the counts to produce themodel parameters for the next step.
All statis-tics from inconsistent alignments are ruled out toensure the final sufficient statistics marginalizedon the fixed alignment links.
Similar to the con-strained hill climbing algorithm, we can manipu-late the Moving/Swapping matrices to effectivelyexclude inconsistent alignments.
We just need tobypass all the cells whose values are negative, i.e.represent inconsistent alignments.By combining the constrained EM algorithmand the count collection, the Viterbi alignment isguaranteed to be consistent with the fixed align-ment links, and the sufficient statistics is guar-anteed to contain no statistics from inconsistentalignments.2.4 Training schemeWe extend the multi-thread GIZA++ (Gao andVogel, 2008) to load the alignments from a mod-ified corpus file.
The links are appended to theend of each sentence in the corpus file in the formof indices pairs, which will be read by the alignerduring training.
In practice, we first training un-constrained models up to Model 4, and then switchto constrained Model 4 and continue training forseveral iterations, the actual number of trainingorder is: 5 iterations of Model 1, 5 iterations ofHMM, 3 iterations of Model 3, 3 iterations ofunconstrained Model 4 and 3 iterations of con-strained Model 4.
Because here we actually havemore Model 4 iterations, to make the comparisonfair, in all the experiments below we perform 6 it-erations of Model 4 in the baseline systems.3 Obtaining alignment linksGiven the algorithm described in the Section 2,we still face the problem of obtaining alignmentlinks to constrain the system.
In this section, wedescribe two approaches to obtain the links, thefirst is to resort to human labels, while the secondapplies high-precision-low-recall heuristic-basedaligner on large unsupervised corpus.3.1 Using manual alignment linksUsing manual alignment links is simple andstraight-forward, however the problem is how toselect links for human to label given that labellingthe whole corpus is impossible.
We propose twolink selectors, the first is the random selector inwhich every links in the manual alignment hasequal probability of being selected.
Obviously,the random selecting method is far from optimalbecause it pays no attention on the quality of ex-isting links.
In order to demonstrate that by select-ing links carefully we can achieve better alignmentquality with less manual alignment links, we pro-pose the second selector based on disagreementsof alignments from two directions.
We first clas-sify the source and target words fj and ei intothree categories.
Use fj as an example, the cat-egories are:?
C1: fj aligns to ei, i > 0 in e ?
f ,1 but inreversed direction ei does not align to fj butto another word.?
C2: fj aligns to ei, i > 0, in f ?
e, but inreversed direction (e ?
f ), fj aligns to theempty word.?
C3: no word aligns to fj , in f ?
e, but inreversed direction fj aligns to ei, i > 0.2The criteria of ei are the same as fj after swap-ping the definitions of ?source?
and ?target?.We prioritize the links ?JI = (i, j) by looking atthe classes of the source/target words.
The order of1Recall that fj can align to only one word.2This class is different from C1 that whether ei aligns toconcrete words or the empty word.5Order Criterion Order Criterion1 fj ?
C1 5 ei ?
C22 fj ?
C2 4 ei ?
C13 fj ?
C3 6 ei ?
C3Table 1: The priorities of alignment linkspriorities is shown in Table 1.
All the links not inthe six classes will have the lowest priorities.
Thelinks with higher priorities will be selected first,but the order of two links in a same priority classis not defined and they will be selected randomly.3.2 Using heuristics on unlabelled dataAnother possible way of getting alignment linksis to make use of heuristics to generate high-precision-low-recall links and feed them into thealigner.
The heuristics can be number map-ping, person name translator or more sophisticatedmethods such as alignment confidence measure(Huang, 2009).
In this paper we propose to usemanual dictionaries to generate alignment links.First we filter out from the dictionary the en-tries with high frequency in the source side, andthen build an aligner based on it.
The aligner out-put links between words if them match an entryin the dictionary.
The method can be applied onlarge unlabelled corpus and generate large num-ber of links, after that we use the links as manualalignment links in proposed method.The readers may notice that GIZA++ supportsutilizing manual dictionary as well, however it isdifferent from our method.
The dictionary is usedin GIZA++ only in the initialization step of Model1, where only the statistics of the word pairs ap-peared in the dictionary will be collected and nor-malized.
Given the fact that Model 1 converges toglobal optimal, the effect will fade out after sev-eral iterations.
In contrast, our method imposea hard constraint on the alignments.
Also, ourmethod can be used side-by-side with the methodin GIZA++.4 Experiments4.1 Experiments on manual link selectorsWe designed a set of controlled experiments toshow that the algorithm acts as desired.
Particu-larly, with a number of manual alignment links fedinto the aligner, we should be able to correct moremisaligned alignment links than the manual align-ment links through better alignment models.
Also,carefully selected alignment links should outper-form randomly selected alignment links.We used Chinese-English and Arabic-Englishmanually aligned corpus in the experiments.
Ta-ble 2 shows the statistics of the corpora:Number of Num.
of Words AlignmentSentences Source Target LinksCh-En 21,863 424,683 524,882 687,247Ar-En 29,876 630,101 821,938 830,349Table 2: Corpus statistics of the corporaFirst the corpora is trained as unlabelled datato serve as baselines, and then we feed a portionof alignment links into the proposed aligner.
Weexperimented with different methods of choosingalignment links and adjust the number of links vis-ible to the aligner.
Because of the limitations ofthe IBM Models, such as no N-to-1 alignments,the manual alignment is not reachable from ei-ther direction.
We then define the best align-ment that the IBM Models can express ?oraclealignment?, which can be obtained by droppingall N-to-1 links from manual alignment.
Also, toshow the upper-bound performance, we feed allthe manual alignment links to our aligner, and callthe alignment ?force alignment?.
Table 3 showsthe alignment qualities of oracle alignments andforce alignments of both systems.
For force align-ments, we show the scores with and without im-plicit empty links derived from the manual align-ment.3 The oracle alignments are the performanceupper-bounds of all aligners under IBM Model?s1-to-N assumption.
The result from Table 3 showsthat, if we include the derived empty links, theforce alignments are close to the oracle results.Then the question is how fast we can approach theupper-bound.To answer the question, we gradually increasethe number of links being fed into the aligner.
Inthese experiments the seeds for random numbergenerator are fixed so that the links selected inlater experiments are always superset of that ofearlier experiments.
The comparison of the align-ment quality is shown in Figure 3 and 4.
To showthe actual improvement brought in by the algo-rithm instead of the manual alignment links them-selves, we compare the alignment results of theproposed method with directly fixing the align-ments from original GIZA++ training.
By fix-ing alignments we mean that first the conventional3We can derive empty links if one word has no alignmentlink from the full alignment we have access to.60 1 2 3 4 5 6x 105707580859095100Number of LinksPrecisionPrecision?Number of links (Chinese?English)NNWNDFFRFD0 1 2 3 4 5 6x 1054850525456586062Number of LinksRecallRecall?Number of links (Chinese?English)NNWNDFFRFD0 1 2 3 4 5 6x 105510152025303540Number of LinksAERAER?Number of links (Chinese?English)NNWNDFFRFD0 1 2 3 4 5 6x 105707580859095100Number of LinksPrecisionPrecision?Number of links (English?Chinese)NNWNDFFRFD0 1 2 3 4 5 6x 1056065707580Number of LinksRecallRecall?Number of links (English?Chinese)NNWNDFFRFD0 1 2 3 4 5 6x 105510152025303540Number of LinksAERAER?Number of links (English?Chinese)NNWNDFFRFD0 1 2 3 4 5 6x 105707580859095Number of LinksPrecisionPrecision?Number of links (Combined Ch/En)NNWNDFFRFD0 1 2 3 4 5 6x 10560657075808590Number of LinksRecallRecall?Number of links (Combined Ch/En)NNWNDFFRFD0 1 2 3 4 5 6x 1055101520253035Number of LinksAERAER?Number of links (Combined Ch/En)NNWNDFFRFDFigure 3: Alignment qualities of Chinese-English word alignment, NN: Random selector without emptylinks, WN: Random seletor with empty links, DF: Disagreement selector, FR: Directly fixing the align-ments with random selector, FD: Directly fixing the alignments with disagreement selector.
Each rowshows the precision, recall and AER when applying different number of manual alignment links.
Thethree rows are for Chinese-English, English-Chinese and heuristically symmetrized alignments (grow-diag-final-and) accordingly.GIZA++ training is performed and then we add themanual alignment links to the resulting alignment.In case that the 1-to-N restriction of the IBM Mod-els is violated, we keep the manual alignment linksand remove the links from GIZA++.We show the results as FR (dashed curves withdiamond markers) and FD (dashed curves withsquare markers) in the plots, corresponding toalignments selected from the random link selectorand the disagreement-based link selector.
Thesetwo curves serve as baseline, and the gaps betweenthe FR curves and the WN curves (dotted curveswith cross markers) and the gaps between the FDcurves and the DF curves (solid curves) show theamount of improvement we achieved using themethod in addition to the manual alignment links.Therefore, they represent the effectiveness of theproposed alignment approach.
Also the gaps be-tween DF and WN curves indicate the differencesin the performance of two link selectors.The plots illustrate that when the number oflinks is small, the WN and DF curves are al-ways higher than the FR/FD curves.
It provesthat our system does not just fix the links pro-vided by manual alignments, instead the informa-tion propagates to other links.
The largest gapbetween FD and DF is 8% absolute in com-bined alignment of Chinese-English system with200,000 manual alignment links.
Also, we cansee that the disagreement-based link selector (DF)always outperform the random selector (WN).
Itsuggest that, if we want to harvest manual align-ment links, it is possible to apply active learningmethod to minimize the user labelling effort whilemaximizing the improvement on word alignmentqualities.
Especially, notice that in the lower parts70 1 2 3 4 5 6 7 8x 105606570758085Number of LinksPrecisionPrecision?Number of links (Arabic?English)NNWNDFFRFD0 1 2 3 4 5 6 7 8x 10555606570Number of LinksRecallRecall?Number of links (Arabic?English)NNWNDFFRFD0 1 2 3 4 5 6 7 8x 10551015202530354045Number of LinksAERAER?Number of links (Arabic?English)NNWNDFFRFD0 1 2 3 4 5 6 7 8x 105606570758085Number of LinksPrecisionPrecision?Number of links (English?Arabic)NNWNDFFRFD0 1 2 3 4 5 6 7 8x 1057075808590Number of LinksRecallRecall?Number of links (English?Arabic)NNWNDFFRFD0 1 2 3 4 5 6 7 8x 1055101520253035Number of LinksAERAER?Number of links (English?Arabic)NNWNDFFRFD0 1 2 3 4 5 6 7 8x 1056570758085Number of LinksPrecisionPrecision?Number of links (Combined En/Ar)NNWNDFFRFD0 1 2 3 4 5 6 7 8x 105707580859095100Number of LinksRecallRecall?Number of links (Combined En/Ar)NNWNDFFRFD0 1 2 3 4 5 6 7 8x 1055101520253035Number of LinksAERAER?Number of links (Combined En/Ar)NNWNDFFRFDFigure 4: Alignment qualities of Arabic-English word alignment, NN: Random selector without emptylinks, WN: Random selector with empty links, DF: Disagreement selector, FR: Directly fixing the align-ments with random selector, FD: Directly fixing the alignments with disagreement selector.
Each rowshows the precision, recall and AER when applying different number of manual alignment links.
Thethree rows are for Arabic-English, English-Arabic and heuristically symmetrized alignments (grow-diag-final-and) accordingly.of the curves, with a small number of manualalignment links, we can already improve the align-ment quality by a large gap.
This observation canbenefit low-resource word alignment tasks.4.2 Experiment on using heuristicsThe previous experiment shows the potential ofusing the method on manual aligned corpus, herewe demonstrate another possible usage of the pro-posed method that uses heuristics to generate high-precision-low-recall links.
We use LDC Chinese-English dictionary as an example.
The entries withsingle Chinese character and more than six En-glish words are filtered out.
The heuristic-basedaligner yields alignment that has 79.48% preci-sion and 17.36% recall rate on the test set we usedin 4.1.
By applying the links as manual links,we run proposed method on the same Chinese-English test data presented in 4.1, and the resultsof alignment qualities are shown in 5.
As we cansee, the AER reduced by 1.64 from 37.23 to 35.61on symmetrized alignment.We also experimented with translation taskswith moderate-size corpus.
We used the corpusLDC2006G05 with 25 million words.
The train-ing scheme is the same as previous experiments,where the filtered LDC dictionary is used.
Afterword alignment, standard Moses phrase extractiontool (Och and Ney, 2004) is used to build the trans-lation models and finally Moses (Koehn et.
al.,2007) is used to tune and decode.We tune the system on the NIST MT06 testset (1664 sentences), and test on the MT08 (1357sentences) and the DEV075 (1211 sentences) testsets, which are further divided into two sources(newswire and web data).
A trigram language5It is a test set used by GALE Rosseta Team8MT02 MT03 MT04 MT05 MT08-NW MT08-WB Dev07NW Dev07WBBaseline 28.87 27.82 30.08 26.77 25.09 17.72 24.88 21.76Dict-Link 29.59 27.67 31.01 27.13 25.14 17.96 25.51 21.88Table 4: Comparison of the performance of baseline and the alignment generated by new aligner withdictionary links in BLEU scoresPrecision Recall AERORL 100.00 62.61 23.00Ch-En F/NE 89.25 62.47 26.50F/WE 99.59 62.47 23.22ORL 100.00 80.98 10.51En-Ch F/NE 93.49 80.79 13.32F/WE 99.82 80.79 10.70F/NE 90.79 87.49 10.89Comb F/WE 99.78 87.23 6.92ORL 100.00 72.07 16.23Ar-En F/NE 82.46 72.00 23.13F/WE 94.25 72.00 18.36ORL 100.00 90.14 5.18En-Ar F/NE 79.81 90.06 15.37F/WE 93.27 90.10 8.34F/NE 78.91 93.07 14.59Comb F/WE 94.64 93.21 6.08Table 3: Alignment quality of oracle alignmentand force alignment, the rows with ?ORL?
in thesecond column are oracle alignments, ?F/NE?
and?F/WE?
represent force alignments with emptylinks and without empty links correspondingly.For ?F/NE?
and ?F/WE?
we also listed thescores of heuristically symmetrized alignment4.(?Comb?
)model trained from GigaWord V1 and V2 cor-pora is used.
Table 4 shows the comparison ofthe performances on BLEU metric (Papineni etal., 2002).
As we can observe from the results,the proposed method outperforms the baseline onall test sets except MT03, and has significant6improvement on MT02 (+0.72), MT04 (+0.93),and Dev07NW(+0.63).
The average improvementacross all test sets is 0.35 BLEU points.As a summary, the purpose of the this experi-ment is to demonstrate an important characteris-tic of the proposed method.
Even with imperfectmanual alignment links, we can get better align-ment by applying our method.
This characteristicopens a possibility to integrate other more sophis-ticated aligners.5 ConclusionIn this study, our major contribution is a novelgenerative model extended from IBM Model 4 to6We used the confidence measurement described in(Zhang and Vogel, 2004)Chinese-EnglishPrecision Recall AERBaseline 68.22 46.88 44.43Dict-Link 69.93 48.28 42.88English-ChinesePrecision Recall AERBaseline 65.35 55.05 40.24Dict-Link 66.70 56.45 38.85grow-diag-final-andPrecision Recall AERBaseline 69.15 57.47 37.23Dict-Link 70.11 59.54 35.61Table 5: Comparison on alignment error rate byusing alignment links generated by dictionariesutilize partial manual alignments.
The proposedmethod enables us to efficiently enforce subtlealignment constraints into the EM training.
Weperformed experiments on manually aligned cor-pora to prove the validity.
We also demonstratedusing the method with simple heuristics to boostthe translation quality on moderate size unlabelledcorpus.
The results show that our method is ef-fective in promoting the word alignment quali-ties with small amounts of partial alignments andwith high-precision-low-recall heuristics.
Also themethod of using dictionary to generate manualalignment links showed an average improvementof 0.35 BLEU points across 8 test sets.The algorithm has small impact on the speed ofGIZA++, and can easily be added to current multi-thread implementation of GIZA++.
Therefore it issuitable for large scale training.Future work includes applying the proposed ap-proach on low resource language pairs and in-tegrating the algorithm with other rule-based ordiscriminative aligners that can generate high-precision-low-recall partial alignments.AcknowledgementThis work is supported by DARPA GALEproject and NSF CluE project.9ReferencesV.
Ambati, S. Vogel, and J. Carbonell.
2010.
Ac-tive semi-supervised learning for improving wordalignment.
In Proceedings of the NAACL HLT 2010Workshop on Active Learning for Natural LanguageProcessing.S.
Arora, E. Nyberg, and C. P. Rose?.
2009.
Estimat-ing annotation cost for active learning in a multi-annotator environment.
In HLT ?09: Proceedings ofthe NAACL HLT 2009 Workshop on Active Learningfor Natural Language Processing, pages 18?26.P.
Blunsom and T. Cohn.
2006.
Discriminativeword alignment with conditional random fields.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pages 65?72.P.
F. Brown et.
al.
1993.
The mathematics of sta-tistical machine translation: Parameter estimation.In Computational Linguistics, volume 19(2), pages263?331.C.
Callison-Burch, D. Talbot, and M. Osborne.2004.
Statistical machine translation with word-andsentence-aligned parallel corpora.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, pages 175?183.C.
Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using amazons me-chanical turk.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 286?295.A.
Fraser and D. Marcu.
2006.
Semi-supervisedtraining for statistical word alignment.
In ACL-44:Proceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pages 769?776.A.
Fraser and D. Marcu.
2007.
Getting the struc-ture right for word alignment: LEAF.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 51?60.Q.
Gao and S. Vogel.
2008.
Parallel implementationsof word alignment tool.
In Proceedings of the ACL2008 Software Engineering, Testing, and Quality As-surance Workshop, pages 49?57.Q.
Gao and S. Vogel.
2010.
Consensus versus exper-tise : A case study of word alignment with mechan-ical turk.
In NAACL 2010 Workshop on CreatingSpeech and Language Data With Mechanical Turk,pages 30?34.F.
Guzman, Q. Gao, and S. Vogel.
2009.
Reassessmentof the role of phrase extraction in pbsmt.
In Thetwelfth Machine Translation Summit.F.
Huang.
2009.
Confidence measure for word align-ment.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Pro-cessing of the AFNLP, pages 932?940.
Associationfor Computational Linguistics.A.
Ittycheriah and S. Roukos.
2005.
A maximum en-tropy word aligner for arabic-english machine trans-lation.
In Proceedings of Human Language Technol-ogy Conference and Conference on Empirical Meth-ods in Natural Language Processing, pages 89?96.P.
Koehn et.
al.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedings ofthe 45th Annual Meeting of the Association for Com-putational Linguistics Companion Volume Proceed-ings of the Demo and Poster Sessions, pages 177?180.Y.
Liu, Q. Liu, and S. Lin.
2005.
Log-linear modelsfor word alignment.
In ACL ?05: Proceedings of the43rd Annual Meeting on Association for Computa-tional Linguistics, pages 459?466.R.
C Moore.
2005.
A discriminative framework forbilingual word alignment.
In Proceedings of theconference on Human Language Technology andEmpirical Methods in Natural Language Process-ing, pages 81?88.J.
Niehues and S. Vogel.
2008.
Discriminative wordalignment via alignment matrix modeling.
In Pro-ceedings of the Third Workshop on Statistical Ma-chine Translation, pages 18?25.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
In Compu-tational Linguistics, volume 1:29, pages 19?51.F.
J. Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
In Com-putational Linguistics, volume 30, pages 417?449.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: A method for automatic evaluation of ma-chine translation.
In Proceedings of ACL?02, pages311?318, Philadelphia, PA, July.B.
Taskar, S. Lacoste-Julien, and Klein D. 2005.
A dis-criminative matching approach to word alignment.In Proceedings of the conference on Human Lan-guage Technology and Empirical Methods in Nat-ural Language Processing, pages 73?80.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMMbased word alignment in statistical machine trans-lation.
In Proceedings of 16th International Confer-ence on Computational Linguistics), pages 836?841.Y.
Zhang and S. Vogel.
2004.
Measuring confidenceintervals for the machine translation evaluation met-rics.
In Proceedings of The 10th International Con-ference on Theoretical and Methodological Issues inMachine Translation, October.10
