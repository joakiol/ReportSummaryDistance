Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 82?90,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPReinforcement Learning for Mapping Instructions to ActionsS.R.K.
Branavan, Harr Chen, Luke S. Zettlemoyer, Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{branavan, harr, lsz, regina}@csail.mit.eduAbstractIn this paper, we present a reinforce-ment learning approach for mapping nat-ural language instructions to sequences ofexecutable actions.
We assume access toa reward function that defines the qual-ity of the executed actions.
During train-ing, the learner repeatedly constructs ac-tion sequences for a set of documents, ex-ecutes those actions, and observes the re-sulting reward.
We use a policy gradientalgorithm to estimate the parameters of alog-linear model for action selection.
Weapply our method to interpret instructionsin two domains ?
Windows troubleshoot-ing guides and game tutorials.
Our resultsdemonstrate that this method can rival su-pervised learning techniques while requir-ing few or no annotated training exam-ples.11 IntroductionThe problem of interpreting instructions writtenin natural language has been widely studied sincethe early days of artificial intelligence (Winograd,1972; Di Eugenio, 1992).
Mapping instructions toa sequence of executable actions would enable theautomation of tasks that currently require humanparticipation.
Examples include configuring soft-ware based on how-to guides and operating simu-lators using instruction manuals.
In this paper, wepresent a reinforcement learning framework for in-ducing mappings from text to actions without theneed for annotated training examples.For concreteness, consider instructions from aWindows troubleshooting guide on deleting tem-porary folders, shown in Figure 1.
We aim to map1Code, data, and annotations used in this work are avail-able at http://groups.csail.mit.edu/rbg/code/rl/Figure 1: A Windows troubleshooting article de-scribing how to remove the ?msdownld.tmp?
tem-porary folder.this text to the corresponding low-level commandsand parameters.
For example, properly interpret-ing the third instruction requires clicking on a tab,finding the appropriate option in a tree control, andclearing its associated checkbox.In this and many other applications, the valid-ity of a mapping can be verified by executing theinduced actions in the corresponding environmentand observing their effects.
For instance, in theexample above we can assess whether the goaldescribed in the instructions is achieved, i.e., thefolder is deleted.
The key idea of our approachis to leverage the validation process as the mainsource of supervision to guide learning.
This formof supervision allows us to learn interpretationsof natural language instructions when standard su-pervised techniques are not applicable, due to thelack of human-created annotations.Reinforcement learning is a natural frameworkfor building models using validation from an envi-ronment (Sutton and Barto, 1998).
We assume thatsupervision is provided in the form of a rewardfunction that defines the quality of executed ac-tions.
During training, the learner repeatedly con-structs action sequences for a set of given docu-ments, executes those actions, and observes the re-sulting reward.
The learner?s goal is to estimate a82policy ?
a distribution over actions given instruc-tion text and environment state ?
that maximizesfuture expected reward.
Our policy is modeled in alog-linear fashion, allowing us to incorporate fea-tures of both the instruction text and the environ-ment.
We employ a policy gradient algorithm toestimate the parameters of this model.We evaluate our method on two distinct applica-tions: Windows troubleshooting guides and puz-zle game tutorials.
The key findings of our ex-periments are twofold.
First, models trained onlywith simple reward signals achieve surprisinglyhigh results, coming within 11% of a fully su-pervised method in the Windows domain.
Sec-ond, augmenting unlabeled documents with evena small fraction of annotated examples greatly re-duces this performance gap, to within 4% in thatdomain.
These results indicate the power of learn-ing from this new form of automated supervision.2 Related WorkGrounded Language Acquisition Our workfits into a broader class of approaches that aim tolearn language from a situated context (Mooney,2008a; Mooney, 2008b; Fleischman and Roy,2005; Yu and Ballard, 2004; Siskind, 2001; Oates,2001).
Instances of such approaches includework on inferring the meaning of words fromvideo data (Roy and Pentland, 2002; Barnard andForsyth, 2001), and interpreting the commentaryof a simulated soccer game (Chen and Mooney,2008).
Most of these approaches assume someform of parallel data, and learn perceptual co-occurrence patterns.
In contrast, our emphasisis on learning language by proactively interactingwith an external environment.Reinforcement Learning for Language Pro-cessing Reinforcement learning has been previ-ously applied to the problem of dialogue manage-ment (Scheffler and Young, 2002; Roy et al, 2000;Litman et al, 2000; Singh et al, 1999).
Thesesystems converse with a human user by taking ac-tions that emit natural language utterances.
Thereinforcement learning state space encodes infor-mation about the goals of the user and what theysay at each time step.
The learning problem is tofind an optimal policy that maps states to actions,through a trial-and-error process of repeated inter-action with the user.Reinforcement learning is applied very differ-ently in dialogue systems compared to our setup.In some respects, our task is more easily amenableto reinforcement learning.
For instance, we are notinteracting with a human user, so the cost of inter-action is lower.
However, while the state space canbe designed to be relatively small in the dialoguemanagement task, our state space is determined bythe underlying environment and is typically quitelarge.
We address this complexity by developinga policy gradient algorithm that learns efficientlywhile exploring a small subset of the states.3 Problem FormulationOur task is to learn a mapping between documentsand the sequence of actions they express.
Figure 2shows how one example sentence is mapped tothree actions.Mapping Text to Actions As input, we aregiven a document d, comprising a sequence of sen-tences (u1, .
.
.
, u`), where each ui is a sequenceof words.
Our goal is to map d to a sequence ofactions ~a = (a0, .
.
.
, an?1).
Actions are predictedand executed sequentially.2An action a = (c,R,W ?)
encompasses a com-mand c, the command?s parameters R, and thewords W ?
specifying c and R. Elements of R re-fer to objects available in the environment state, asdescribed below.
Some parameters can also referto words in document d. Additionally, to accountfor words that do not describe any actions, c canbe a null command.The Environment The environment state Especifies the set of objects available for interac-tion, and their properties.
In Figure 2, E is shownon the right.
The environment state E changesin response to the execution of command c withparameters R according to a transition distribu-tion p(E ?|E , c, R).
This distribution is a priori un-known to the learner.
As we will see in Section 5,our approach avoids having to directly estimatethis distribution.State To predict actions sequentially, we need totrack the state of the document-to-actions map-ping over time.
A mapping state s is a tuple(E , d, j,W ), where E refers to the current environ-ment state; j is the index of the sentence currentlybeing interpreted in document d; and W containswords that were mapped by previous actions for2That is, action ai is executed before ai+1 is predicted.83Figure 2: A three-step mapping from an instruction sentence to a sequence of actions in Windows 2000.For each step, the figure shows the words selected by the action, along with the corresponding systemcommand and its parameters.
The words of W ?
are underlined, and the words of W are highlighted ingrey.the same sentence.
The mapping state s is ob-served after each action.The initial mapping state s0 for document d is(Ed, d, 0, ?
); Ed is the unique starting environmentstate for d. Performing action a in state s =(E , d, j,W ) leads to a new state s?
according todistribution p(s?|s, a), defined as follows: E tran-sitions according to p(E ?|E , c, R), W is updatedwith a?s selected words, and j is incremented ifall words of the sentence have been mapped.
Forthe applications we consider in this work, environ-ment state transitions, and consequently mappingstate transitions, are deterministic.Training During training, we are provided witha set D of documents, the ability to sample fromthe transition distribution, and a reward functionr(h).
Here, h = (s0, a0, .
.
.
, sn?1, an?1, sn) isa history of states and actions visited while in-terpreting one document.
r(h) outputs a real-valued score that correlates with correct actionselection.3 We consider both immediate reward,which is available after each action, and delayedreward, which does not provide feedback until thelast action.
For example, task completion is a de-layed reward that produces a positive value afterthe final action only if the task was completed suc-cessfully.
We will also demonstrate how manu-ally annotated action sequences can be incorpo-rated into the reward.3In most reinforcement learning problems, the rewardfunction is defined over state-action pairs, as r(s, a) ?
in thiscase, r(h) =Pt r(st, at), and our formulation becomes astandard finite-horizon Markov decision process.
Policy gra-dient approaches allow us to learn using the more generalcase of history-based reward.The goal of training is to estimate parameters ?of the action selection distribution p(a|s, ?
), calledthe policy.
Since the reward correlates with ac-tion sequence correctness, the ?
that maximizesexpected reward will yield the best actions.4 A Log-Linear Model for ActionsOur goal is to predict a sequence of actions.
Weconstruct this sequence by repeatedly choosing anaction given the current mapping state, and apply-ing that action to advance to a new state.Given a state s = (E , d, j,W ), the space of pos-sible next actions is defined by enumerating sub-spans of unused words in the current sentence (i.e.,subspans of the jth sentence of d not in W ), andthe possible commands and parameters in envi-ronment state E .4 We model the policy distribu-tion p(a|s; ?)
over this action space in a log-linearfashion (Della Pietra et al, 1997; Lafferty et al,2001), giving us the flexibility to incorporate a di-verse range of features.
Under this representation,the policy distribution is:p(a|s; ?)
=e???(s,a)?a?e???(s,a?
), (1)where ?
(s, a) ?
Rn is an n-dimensional featurerepresentation.
During test, actions are selectedaccording to the mode of this distribution.4For parameters that refer to words, the space of possiblevalues is defined by the unused words in the current sentence.845 Reinforcement LearningDuring training, our goal is to find the optimal pol-icy p(a|s; ?).
Since reward correlates with correctaction selection, a natural objective is to maximizeexpected future reward ?
that is, the reward weexpect while acting according to that policy fromstate s. Formally, we maximize the value function:V?
(s) = Ep(h|?)
[r(h)] , (2)where the history h is the sequence of states andactions encountered while interpreting a singledocument d ?
D. This expectation is averagedover all documents in D. The distribution p(h|?
)returns the probability of seeing history h whenstarting from state s and acting according to a pol-icy with parameters ?.
This distribution can be de-composed into a product over time steps:p(h|?)
=n?1?t=0p(at|st; ?
)p(st+1|st, at).
(3)5.1 A Policy Gradient AlgorithmOur reinforcement learning problem is to find theparameters ?
that maximize V?
from equation 2.Although there is no closed form solution, policygradient algorithms (Sutton et al, 2000) estimatethe parameters ?
by performing stochastic gradi-ent ascent.
The gradient of V?
is approximated byinteracting with the environment, and the resultingreward is used to update the estimate of ?.
Policygradient algorithms optimize a non-convex objec-tive and are only guaranteed to find a local opti-mum.
However, as we will see, they scale to largestate spaces and can perform well in practice.To find the parameters ?
that maximize the ob-jective, we first compute the derivative of V?.
Ex-panding according to the product rule, we have:???V?
(s) = Ep(h|?)[r(h)?t??
?log p(at|st; ?
)],(4)where the inner sum is over all time steps t inthe current history h. Expanding the inner partialderivative we observe that:??
?log p(a|s; ?)
= ?
(s, a)??a??
(s, a?
)p(a?|s; ?
),(5)which is the derivative of a log-linear distribution.Equation 5 is easy to compute directly.
How-ever, the complete derivative of V?
in equation 4Input: A document set D,Feature representation ?,Reward function r(h),Number of iterations TInitialization: Set ?
to small random values.for i = 1 .
.
.
T do1foreach d ?
D do2Sample history h ?
p(h|?)
where3h = (s0, a0, .
.
.
, an?1, sn) as follows:3a for t = 0 .
.
.
n?
1 do3b Sample action at ?
p(a|st; ?
)3c Execute at on state st: st+1 ?
p(s|st, at)end??Pt`?
(st, at)?Pa?
?
(st, a?
)p(a?|st; ?)?4?
?
?
+ r(h)?5endendOutput: Estimate of parameters ?Algorithm 1: A policy gradient algorithm.is intractable, because computing the expectationwould require summing over all possible histo-ries.
Instead, policy gradient algorithms employstochastic gradient ascent by computing a noisyestimate of the expectation using just a subset ofthe histories.
Specifically, we draw samples fromp(h|?)
by acting in the target environment, anduse these samples to approximate the expectationin equation 4.
In practice, it is often sufficient tosample a single history h for this approximation.Algorithm 1 details the complete policy gradi-ent algorithm.
It performs T iterations over theset of documents D. Step 3 samples a history thatmaps each document to actions.
This is done byrepeatedly selecting actions according to the cur-rent policy, and updating the state by executing theselected actions.
Steps 4 and 5 compute the empir-ical gradient and update the parameters ?.In many domains, interacting with the environ-ment is expensive.
Therefore, we use two tech-niques that allow us to take maximum advantageof each environment interaction.
First, a his-tory h = (s0, a0, .
.
.
, sn) contains subsequences(si, ai, .
.
.
sn) for i = 1 to n ?
1, each with itsown reward value given by the environment as aside effect of executing h. We apply the updatefrom equation 5 for each subsequence.
Second,for a sampled history h, we can propose alterna-tive histories h?
that result in the same commandsand parameters with different word spans.
We canagain apply equation 5 for each h?, weighted by itsprobability under the current policy, p(h?|?)p(h|?)
.85The algorithm we have presented belongs toa family of policy gradient algorithms that havebeen successfully used for complex tasks such asrobot control (Ng et al, 2003).
Our formulation isunique in how it represents natural language in thereinforcement learning framework.5.2 Reward Functions and ML EstimationWe can design a range of reward functions to guidelearning, depending on the availability of anno-tated data and environment feedback.
Consider thecase when every training document d ?
D is an-notated with its correct sequence of actions, andstate transitions are deterministic.
Given these ex-amples, it is straightforward to construct a rewardfunction that connects policy gradient to maxi-mum likelihood.
Specifically, define a rewardfunction r(h) that returns one when h matches theannotation for the document being analyzed, andzero otherwise.
Policy gradient performs stochas-tic gradient ascent on the objective from equa-tion 2, performing one update per document.
Fordocument d, this objective becomes:Ep(h|?
)[r(h)] =?hr(h)p(h|?)
= p(hd|?
),where hd is the history corresponding to the an-notated action sequence.
Thus, with this rewardpolicy gradient is equivalent to stochastic gradientascent with a maximum likelihood objective.At the other extreme, when annotations arecompletely unavailable, learning is still possi-ble given informative feedback from the environ-ment.
Crucially, this feedback only needs to cor-relate with action sequence quality.
We detailenvironment-based reward functions in the nextsection.
As our results will show, reward func-tions built using this kind of feedback can providestrong guidance for learning.
We will also con-sider reward functions that combine annotated su-pervision with environment feedback.6 Applying the ModelWe study two applications of our model: follow-ing instructions to perform software tasks, andsolving a puzzle game using tutorial guides.6.1 Microsoft Windows Help and SupportOn its Help and Support website,5 Microsoft pub-lishes a number of articles describing how to per-5support.microsoft.comNotationo Parameter referring to an environment objectL Set of object class names (e.g.
?button?
)V VocabularyFeatures onW and object oTest if o is visible in sTest if o has input focusTest if o is in the foregroundTest if o was previously interacted withTest if o came into existence since last actionMin.
edit distance between w ?W and object labels in sFeatures on words inW , command c, and object o?c?
?
C, w ?
V : test if c?
= c and w ?W?c?
?
C, l ?
L: test if c?
= c and l is the class of oTable 1: Example features in the Windows do-main.
All features are binary, except for the nor-malized edit distance which is real-valued.form tasks and troubleshoot problems in the Win-dows operating systems.
Examples of such tasksinclude installing patches and changing securitysettings.
Figure 1 shows one such article.Our goal is to automatically execute these sup-port articles in the Windows 2000 environment.Here, the environment state is the set of visi-ble user interface (UI) objects, and object prop-erties such as label, location, and parent window.Possible commands include left-click, right-click,double-click, and type-into, all of which take a UIobject as a parameter; type-into additionally re-quires a parameter for the input text.Table 1 lists some of the features we use for thisdomain.
These features capture various aspects ofthe action under consideration, the current Win-dows UI state, and the input instructions.
For ex-ample, one lexical feature measures the similar-ity of a word in the sentence to the UI labels ofobjects in the environment.
Environment-specificfeatures, such as whether an object is currently infocus, are useful when selecting the object to ma-nipulate.
In total, there are 4,438 features.Reward Function Environment feedback canbe used as a reward function in this domain.
Anobvious reward would be task completion (e.g.,whether the stated computer problem was fixed).Unfortunately, verifying task completion is a chal-lenging system issue in its own right.Instead, we rely on a noisy method of check-ing whether execution can proceed from one sen-tence to the next: at least one word in each sen-tence has to correspond to an object in the envi-86Figure 3: Crossblock puzzle with tutorial.
For thislevel, four squares in a row or column must be re-moved at once.
The first move specified by thetutorial is greyed in the puzzle.ronment.6 For instance, in the sentence from Fig-ure 2 the word ?Run?
matches the Run... menuitem.
If no words in a sentence match a currentenvironment object, then one of the previous sen-tences was analyzed incorrectly.
In this case, weassign the history a reward of -1.
This reward isnot guaranteed to penalize all incorrect histories,because there may be false positive matches be-tween the sentence and the environment.
Whenat least one word matches, we assign a positivereward that linearly increases with the percentageof words assigned to non-null commands, and lin-early decreases with the number of output actions.This reward signal encourages analyses that inter-pret al of the words without producing spuriousactions.6.2 Crossblock: A Puzzle GameOur second application is to a puzzle game calledCrossblock, available online as a Flash game.7Each of 50 puzzles is played on a grid, where somegrid positions are filled with squares.
The objectof the game is to clear the grid by drawing verticalor horizontal line segments that remove groups ofsquares.
Each segment must exactly cross a spe-cific number of squares, ranging from two to sevendepending on the puzzle.
Humans players havefound this game challenging and engaging enoughto warrant posting textual tutorials.8 A samplepuzzle and tutorial are shown in Figure 3.The environment is defined by the state of thegrid.
The only command is clear, which takes aparameter specifying the orientation (row or col-umn) and grid location of the line segment to be6We assume that a word maps to an environment object ifthe edit distance between the word and the object?s name isbelow a threshold value.7hexaditidom.deviantart.com/art/Crossblock-1086691498www.jayisgames.com/archives/2009/01/crossblock.phpremoved.
The challenge in this domain is to seg-ment the text into the phrases describing each ac-tion, and then correctly identify the line segmentsfrom references such as ?the bottom four from thesecond column from the left.
?For this domain, we use two sets of binary fea-tures on state-action pairs (s, a).
First, for eachvocabulary word w, we define a feature that is oneif w is the last word of a?s consumed words W ?.These features help identify the proper text seg-mentation points between actions.
Second, we in-troduce features for pairs of vocabulary word wand attributes of action a, e.g., the line orientationand grid locations of the squares that a would re-move.
This set of features enables us to matchwords (e.g., ?row?)
with objects in the environ-ment (e.g., a move that removes a horizontal seriesof squares).
In total, there are 8,094 features.Reward Function For Crossblock it is easy todirectly verify task completion, which we use asthe basis of our reward function.
The reward r(h)is -1 if h ends in a state where the puzzle cannotbe completed.
For solved puzzles, the reward isa positive value proportional to the percentage ofwords assigned to non-null commands.7 Experimental SetupDatasets For the Windows domain, our datasetconsists of 128 documents, divided into 70 fortraining, 18 for development, and 40 for test.
Inthe puzzle game domain, we use 50 tutorials,divided into 40 for training and 10 for test.9Statistics for the datasets are shown below.Windows PuzzleTotal # of documents 128 50Total # of words 5562 994Vocabulary size 610 46Avg.
words per sentence 9.93 19.88Avg.
sentences per document 4.38 1.00Avg.
actions per document 10.37 5.86The data exhibits certain qualities that makefor a challenging learning problem.
For instance,there are a surprising variety of linguistic con-structs ?
as Figure 4 shows, in the Windows do-main even a simple command is expressed in atleast six different ways.9For Crossblock, because the number of puzzles is lim-ited, we did not hold out a separate development set, and re-port averaged results over five training/test splits.87Figure 4: Variations of ?click internet options onthe tools menu?
present in the Windows corpus.Experimental Framework To apply our algo-rithm to the Windows domain, we use the Win32application programming interface to simulate hu-man interactions with the user interface, and togather environment state information.
The operat-ing system environment is hosted within a virtualmachine,10 allowing us to rapidly save and resetsystem state snapshots.
For the puzzle game do-main, we replicated the game with an implemen-tation that facilitates automatic play.As is commonly done in reinforcement learn-ing, we use a softmax temperature parameter tosmooth the policy distribution (Sutton and Barto,1998), set to 0.1 in our experiments.
For Windows,the development set is used to select the best pa-rameters.
For Crossblock, we choose the parame-ters that produce the highest reward during train-ing.
During evaluation, we use these parametersto predict mappings for the test documents.Evaluation Metrics For evaluation, we com-pare the results to manually constructed sequencesof actions.
We measure the number of correct ac-tions, sentences, and documents.
An action is cor-rect if it matches the annotations in terms of com-mand and parameters.
A sentence is correct if allof its actions are correctly identified, and analo-gously for documents.11 Statistical significance ismeasured with the sign test.Additionally, we compute a word alignmentscore to investigate the extent to which the inputtext is used to construct correct analyses.
Thisscore measures the percentage of words that arealigned to the corresponding annotated actions incorrectly analyzed documents.Baselines We consider the following baselinesto characterize the performance of our approach.10VMware Workstation, available at www.vmware.com11In these tasks, each action depends on the correct execu-tion of all previous actions, so a single error can render theremainder of that document?s mapping incorrect.
In addition,due to variability in document lengths, overall action accu-racy is not guaranteed to be higher than document accuracy.?
Full Supervision Sequence prediction prob-lems like ours are typically addressed us-ing supervised techniques.
We measure howa standard supervised approach would per-form on this task by using a reward signalbased on manual annotations of output ac-tion sequences, as defined in Section 5.2.
Asshown there, policy gradient with this re-ward is equivalent to stochastic gradient as-cent with a maximum likelihood objective.?
Partial Supervision We consider the casewhen only a subset of training documents isannotated, and environment reward is usedfor the remainder.
Our method seamlesslycombines these two kinds of rewards.?
Random and Majority (Windows) We con-sider two na?
?ve baselines.
Both scan througheach sentence from left to right.
A com-mand c is executed on the object whose nameis encountered first in the sentence.
Thiscommand c is either selected randomly, orset to the majority command, which is left-click.
This procedure is repeated until nomore words match environment objects.?
Random (Puzzle) We consider a baselinethat randomly selects among the actions thatare valid in the current game state.128 ResultsTable 2 presents evaluation results on the test sets.There are several indicators of the difficulty of thistask.
The random and majority baselines?
poorperformance in both domains indicates that na?
?veapproaches are inadequate for these tasks.
Theperformance of the fully supervised approach pro-vides further evidence that the task is challenging.This difficulty can be attributed in part to the largebranching factor of possible actions at each step ?on average, there are 27.14 choices per action inthe Windows domain, and 9.78 in the Crossblockdomain.In both domains, the learners relying onlyon environment reward perform well.
Althoughthe fully supervised approach performs the best,adding just a few annotated training examplesto the environment-based learner significantly re-duces the performance gap.12Since action selection is among objects, there is no natu-ral majority baseline for the puzzle.88Windows PuzzleAction Sent.
Doc.
Word Action Doc.
WordRandom baseline 0.128 0.101 0.000 ??
0.081 0.111 ?
?Majority baseline 0.287 0.197 0.100 ??
??
??
?
?Environment reward ?
0.647 ?
0.590 ?
0.375 0.819 ?
0.428 ?
0.453 0.686Partial supervision  0.723 ?
0.702 0.475 0.989 0.575 ?
0.523 0.850Full supervision  0.756 0.714 0.525 0.991 0.632 0.630 0.869Table 2: Performance on the test set with different reward signals and baselines.
Our evaluation measuresthe proportion of correct actions, sentences, and documents.
We also report the percentage of correctword alignments for the successfully completed documents.
Note the puzzle domain has only single-sentence documents, so its sentence and document scores are identical.
The partial supervision linerefers to 20 out of 70 annotated training documents for Windows, and 10 out of 40 for the puzzle.
Eachresult marked with ?
or  is a statistically significant improvement over the result immediately above it;?
indicates p < 0.01 and  indicates p < 0.05.Figure 5: Comparison of two training scenarios where training is done using a subset of annotateddocuments, with and without environment reward for the remaining unannotated documents.Figure 5 shows the overall tradeoff between an-notation effort and system performance for the twodomains.
The ability to make this tradeoff is oneof the advantages of our approach.
The figure alsoshows that augmenting annotated documents withadditional environment-reward documents invari-ably improves performance.The word alignment results from Table 2 in-dicate that the learners are mapping the correctwords to actions for documents that are success-fully completed.
For example, the models that per-form best in the Windows domain achieve nearlyperfect word alignment scores.To further assess the contribution of the instruc-tion text, we train a variant of our model withoutaccess to text features.
This is possible in the gamedomain, where all of the puzzles share a singlegoal state that is independent of the instructions.This variant solves 34% of the puzzles, suggest-ing that access to the instructions significantly im-proves performance.9 ConclusionsIn this paper, we presented a reinforcement learn-ing approach for inducing a mapping between in-structions and actions.
This approach is able to useenvironment-based rewards, such as task comple-tion, to learn to analyze text.
We showed that hav-ing access to a suitable reward function can signif-icantly reduce the need for annotations.AcknowledgmentsThe authors acknowledge the support of the NSF(CAREER grant IIS-0448168, grant IIS-0835445,grant IIS-0835652, and a Graduate Research Fel-lowship) and the ONR.
Thanks to Michael Collins,Amir Globerson, Tommi Jaakkola, Leslie PackKaelbling, Dina Katabi, Martin Rinard, and mem-bers of the MIT NLP group for their suggestionsand comments.
Any opinions, findings, conclu-sions, or recommendations expressed in this paperare those of the authors, and do not necessarily re-flect the views of the funding organizations.89ReferencesKobus Barnard and David A. Forsyth.
2001.
Learningthe semantics of words and pictures.
In Proceedingsof ICCV.David L. Chen and Raymond J. Mooney.
2008.
Learn-ing to sportscast: a test of grounded language acqui-sition.
In Proceedings of ICML.Stephen Della Pietra, Vincent J. Della Pietra, andJohn D. Lafferty.
1997.
Inducing features of ran-dom fields.
IEEE Trans.
Pattern Anal.
Mach.
Intell.,19(4):380?393.Barbara Di Eugenio.
1992.
Understanding natural lan-guage instructions: the case of purpose clauses.
InProceedings of ACL.Michael Fleischman and Deb Roy.
2005.
Intentionalcontext in situated language learning.
In Proceed-ings of CoNLL.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of ICML.Diane J. Litman, Michael S. Kearns, Satinder Singh,and Marilyn A. Walker.
2000.
Automatic optimiza-tion of dialogue management.
In Proceedings ofCOLING.Raymond J. Mooney.
2008a.
Learning languagefrom its perceptual context.
In Proceedings ofECML/PKDD.Raymond J. Mooney.
2008b.
Learning to connect lan-guage and perception.
In Proceedings of AAAI.Andrew Y. Ng, H. Jin Kim, Michael I. Jordan, andShankar Sastry.
2003.
Autonomous helicopter flightvia reinforcement learning.
In Advances in NIPS.James Timothy Oates.
2001.
Grounding knowledgein sensors: Unsupervised learning for language andplanning.
Ph.D. thesis, University of MassachusettsAmherst.Deb K. Roy and Alex P. Pentland.
2002.
Learn-ing words from sights and sounds: a computationalmodel.
Cognitive Science 26, pages 113?146.Nicholas Roy, Joelle Pineau, and Sebastian Thrun.2000.
Spoken dialogue management using proba-bilistic reasoning.
In Proceedings of ACL.Konrad Scheffler and Steve Young.
2002.
Automaticlearning of dialogue strategy using dialogue simula-tion and reinforcement learning.
In Proceedings ofHLT.Satinder P. Singh, Michael J. Kearns, Diane J. Litman,and Marilyn A. Walker.
1999.
Reinforcement learn-ing for spoken dialogue systems.
In Advances inNIPS.Jeffrey Mark Siskind.
2001.
Grounding the lexical se-mantics of verbs in visual perception using force dy-namics and event logic.
J. Artif.
Intell.
Res.
(JAIR),15:31?90.Richard S. Sutton and Andrew G. Barto.
1998.
Re-inforcement Learning: An Introduction.
The MITPress.Richard S. Sutton, David McAllester, Satinder Singh,and Yishay Mansour.
2000.
Policy gradient meth-ods for reinforcement learning with function approx-imation.
In Advances in NIPS.Terry Winograd.
1972.
Understanding Natural Lan-guage.
Academic Press.Chen Yu and Dana H. Ballard.
2004.
On the integra-tion of grounding language and learning objects.
InProceedings of AAAI.90
