Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 449?456Manchester, August 2008The Choice of Features for Classification of Verbs in Biomedical TextsAnna KorhonenUniversity of CambridgeComputer Laboratory15 JJ Thomson AvenueCambridge CB3 0FD, UKalk23@cl.cam.ac.ukYuval KrymolowskiDept.
of Computer ScienceHaifa UniversityIsraelyuvalkry@gmail.comNigel CollierNational Institute of InformaticsHitotsubashi 2-1-2Chiyoda-ku, Tokyo 101-8430Japancollier@nii.ac.jpAbstractWe conduct large-scale experiments to in-vestigate optimal features for classificationof verbs in biomedical texts.
We intro-duce a range of feature sets and associatedextraction techniques, and evaluate themthoroughly using a robust method new tothe task: cost-based framework for pair-wise clustering.
Our best results comparefavourably with earlier ones.
Interestingly,they are obtained with sophisticated fea-ture sets which include lexical and seman-tic information about selectional prefer-ences of verbs.
The latter are acquired au-tomatically from corpus data using a fullyunsupervised method.1 IntroductionRecent years have seen a massive growth in thescientific literature in the domain of biomedicine.Because future research in the biomedical sciencesdepends on making use of all this existing knowl-edge, there is a strong need for the development ofnatural language processing (NLP) tools which canbe used to automatically locate, organize and man-age facts related to published experimental results.Major progress has been made on informationretrieval and on the extraction of specific rela-tions (e.g.
between proteins and cell types) frombiomedical texts (Ananiadou et al, 2006).
Othertasks, such as the extraction of factual information,remain a bigger challenge.Researchers have recently begun to use deeperNLP techniques (e.g.
statistical parsing) for im-c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.PROTEINS: p53p53Tp53Dmp53...ACTIVATEsuggestsdemonstratesindicatesimplies...GENES: WAF1WAF1CIP1p21...ItINDICATEthatactivatesup-regulatesinducesstimulates......Figure 1: Sample lexical classesproved processing of the challenging linguisticstructures (e.g.
complex nominals, modal subordi-nation, anaphoric links) in biomedical texts.
Foroptimal performance, many of these techniquesrequire richer syntactic and semantic informa-tion than is provided by existing domain lexicons(e.g.
UMLS metathesaurus and lexicon1).
This par-ticularly applies to verbs, which are central to thestructure and meaning of sentences.Where the information is absent, lexical classi-fication can compensate for it, or aid in obtainingit.
Lexical classes which capture the close rela-tion between the syntax and semantics of verbsprovide generalizations about a range of linguis-tic properties (Levin, 1993).
For example, con-sider the INDICATE and ACTIVATE verb classes inFigure 1.
Their members have similar subcatego-rization frames SCFs (e.g.
activate / up-regulate /induce / stimulate NP) and selectional preferences(e.g.
activate / up-regulate / induce / stimulateGENES:WAF1), and they can be used to make sim-ilar statements describing similar events (e.g.
PRO-TEINS:P53 ACTIVATE GENES:WAF1).Lexical classes can be used to abstract awayfrom individual words, or to build a lexical or-ganization which predicts much of the behaviourof a new word by associating it with an appro-priate class.
They have proved useful for variousNLP application tasks, e.g.
parsing, word sense dis-1http://www.nlm.nih.gov/research/umls449ambiguation, semantic role labeling, informationextraction, question-answering, machine transla-tion (Dorr, 1997; Prescher et al, 2000; Swierand Stevenson, 2004; Dang, 2004; Shi and Mi-halcea, 2005).
A large-scale classification spe-cific to the biomedical data could support key BIO-NLP tasks such as anaphora resolution, predicate-argument identification, event extraction and theidentification of biomedical (e.g.
interaction) rela-tions.
However, no such classification is available.Recent research shows that it is possible to auto-matically induce lexical classes from corpora withpromising accuracy (Schulte im Walde, 2006; Joa-nis et al, 2007; Sun et al, 2008).
A number ofmachine learning (ML) methods have been appliedto classify mainly syntactic features (e.g.
subcat-egorization frames (SCFs)) extracted from cross-domain corpora using e.g.
part-of-speech taggingor robust statistical parsing techniques.
Korho-nen et al (2006) have recently applied such anapproach to biomedical texts.
Their preliminaryexperiment shows encouraging results but furtherwork is required before such an approach can beused to benefit practical BIO-NLP.We conduct a large-scale investigation to findoptimal features for biomedical verb classification.We introduce a range of theoretically-motivatedfeature sets and evaluate them thoroughly usinga robust method new to the task: a cost-basedframework for pairwise clustering.
Our best re-sults compare favourably with earlier ones.
Inter-estingly, they are obtained using feature sets whichhave proved challenging in general language verbclassification: ones which incorporate informationabout selectional preferences of verbs.
Unlike inearlier work, we acquire the latter from corpus datausing a fully unsupervised method.We present our lexical classification approach insection 2 and data in section 3.
Experimental eval-uation is reported in section 4.
Section 5 providesdiscussion and section 6 concludes.2 ApproachOur lexical classification approach involves (i) ex-tracting features from corpus data and (ii) cluster-ing them.
These steps are described in the follow-ing two sections, respectively.2.1 FeaturesLexical classifications are based on diathesis alter-nations which manifest in alternating sets of syn-tactic frames (Levin, 1993).
Most verb classifi-cation approaches have therefore employed shal-low syntactic slots or SCFs as basic features.
Somehave supplemented them with further informationabout verb tense, voice, and/or semantic selec-tional preferences on argument heads.2The preliminary experiment on biomedical verbclassification (Korhonen et al, 2006) employedbasic syntactic features only: SCFs extractedfrom corpus data using the system of Briscoeand Carroll (1997) which operates on the outputof a domain-independent robust statistical parser(RASP) (Briscoe and Carroll, 2002).
Because suchdeep syntactic features seem ideally suited forchallenging biomedical data, we adopted the samebasic approach, but we designed and extracted arange of novel feature sets which include addi-tional syntactic and semantic information.The SCF extraction system assigns each occur-rence of a verb in the parsed data as a member ofone of the 163 verbal SCFs, builds a lexical entryfor each verb (type) and SCF combination, and fil-ters noisy entries out of the lexicon.
We do notemploy the filter in our work because its primaryaim is to filter out SCFs containing adjuncts (as op-posed to arguments).
Adjuncts have been shownto be beneficial for general language verb classifi-cation (Sun et al, 2008; Joanis et al, 2007) andparticularly meaningful in biomedical texts (Co-hen and Hunter, 2006).The lexical entries provide various informationuseful for verb classification, including e.g.
the fre-quency of the entry in the data, the part-of-speech(POS) tags of verb tokens, the argument heads inargument positions, the prepositions in PP frames,and the number of verbal occurrences in active andpassive.
Making use of this information we de-signed ten feature sets for experimentation.The first three feature sets F1-F3 include basicSCF frequency information for each verb:F1: SCFs and their relative frequencies.
The SCFsabstract over lexically governed particles andprepositions.F2: F1 with two high frequency PP frames pa-rameterized for prepositions: the simple PPand NP-PP frames refined according to theprepositions provided in the lexical entries(e.g.
PP at, PP on, PP in).2See section 5 for discussion on previous work.450F3: F2 with 13 additional high frequency PPframes parameterized for prepositions.Although prepositions are an important part ofthe syntactic description of lexical classes andtherefore F3 should be the most informative fea-ture set, we controlled the number of PP framesparameterized for prepositions to examine the ef-fect of sparse data in automatic classification.F4-F7 build on the most refined SCF-based fea-ture set F3, supplementing it with informationabout verb tense (F4-F5) and voice (F6-F7):F4: The frequencies of POS tags (e.g.
VVD foractivated) calculated over all the SCFs of theverb.F5: The frequencies of POS tags calculated spe-cific to each SCF of the verb.F6: The frequency of the active and passive oc-currences of the verb (calculated over all theSCFs of the verb).F7: The frequency of the active and passive occur-rences of the verb (calculated specific to eachSCF of the verb).Also F8-F10 build on feature set F3.
They sup-plement it with information about lexical or se-mantic selectional preferences (SPs) of the verbsin the following slots: subject, direct object, sec-ond object, and the NP within the PP complement.The SPs are acquired using argument head data inthe ten most frequent SCFs.
We use two baselinemethods (F8 and F9) which employ raw data andone method based on clustering (F10):F8: The raw argument head types are consideredas SP classes.F9: Only those raw argument head types whichoccur with four or more verbs with frequencyof ?
3 are considered as SP classes.F10: SPs are acquired by clustering those argu-ment heads which occur with ten or moreverbs with frequency of ?
3.
We used the PCclustering method described below in section2.
The number of clusters Knpwas set to 10,20, and 50 to produce SP classes.
We call thefeature sets corresponding to these differentvalues of KnpF10A, F10B and F10C, respec-tively.
Since the clustering algorithms havean element of randomness, clustering was ran100 times.
The output is a result of votingamong the outputs of the runs.F3-F10 are entirely novel feature sets in biomed-ical verb classification.
Variations of some of themhave been used in earlier work on general languageclassification (see section 5 for details).2.2 ClassificationThe clustering method which proved the best in thepreliminary experiment on biomedical verb classi-fication was Information Bottleneck (IB) (Tishbyet al, 1999).
We compare this method against aprobabilistic method: a cost-based framework forpairwise clustering (PC) (Puzicha et al, 2000).2.2.1 Information BottleneckIB is an information-theoretic method whichcontrols the balance between: (i) the loss ofinformation by representing verbs as clusters(I(Clusters;V erbs)), which has to be min-imal, and (ii) the relevance of the outputclusters for representing the SCF distribution(I(Clusters; SCFs)) which has to be maximal.The balance between these two quantities ensuresoptimal compression of data through clusters.
Thetrade-off between the two constraints is realizedthrough minimising the cost function:LIB= I(Clusters;V erbs)?
?I(Clusters; SCFs) ,where ?
is a parameter that balances the con-straints.
IB takes three inputs: (i) SCF-verb -baseddistributions, (ii) the desired number of clusters K,and (iii) the initial value of ?.
It then looks for theminimal ?
that decreases LIBcompared to its valuewith the initial ?, using the given K. IB delivers asoutput the probabilities p(K|V ).2.2.2 Pairwise ClusteringPC is a method where a cost criterion guidesthe search for a suitable clustering configuration.This criterion is realized through a cost functionH(S,M) where(i) S = {sim(a, b)}, a, b ?
A : a collection of pairwisesimilarity values, each of which pertains to a pair ofdata elements a, b ?
A.
(ii) M = (A1, .
.
.
, Ak) : a candidate clustering configu-ration, specifying assignments of all elements into thedisjoint clusters (that is ?Aj= A and Aj?
Aj?= ?for every 1 ?
j < j??
k).4511 Have an effect on activity (BIO/29) 9 Report (GEN/30)1.1 Activate / Inactivate 9.1 Investigate1.1.1 Change activity: activate, inhibit 9.1.1 Examine: evaluate, analyze1.1.2 Suppress: suppress, repres s 9.1.2 Establish: test, investigate1.1.3 Stimulate: stimulate 9.1.3 Confirm: verify, determine1.1.4 Inactivate: delay, diminish 9.2 Suggest1.2 Affect 9.2.1 Presentational:1.2.1 Modulate: stabilize, modulate hypothesize, conclude1.2.2 Regulate: control, support 9.2.2 Cognitive:1.3 Increase / decrease: increase, decrease consider, believe1.4 Modify: modify, catalyze 9.3 Indicate: demonstrate, implyTable 1: Sample classes from the gold standardJournal Years WordsGenes & Development 2003-5 4.7MJournal of Biological Chemistry 2004 5.2M(Vol.1-9)The Journal of Cell Biology 2003-5 5.6MCancer Research 2005 6.5MCarcinogenesis 2003-5 3.4MNature Immunology 2003-5 2.3MDrug Metabolism and Disposition 2003-5 2.3MToxicological Sciences 2003-5 3.1MTotal: 33.1MTable 2: Data from MEDLINEThe cost function is defined as follows:H = ?Pnj?Avgsimj,Avgsimj=1nj?
(nj?1)P{a,b?Aj}sim(a, b)where njis the size of the jthcluster and Avgsimjis the average similarity between cluster members.We used the Jensen-Shannon divergence (JS) as thesimilarity measure.3 Data3.1 Test Verbs and Gold StandardWe employed in our experiments the same goldstandard as earlier employed by Korhonen et al(2006).
This three level gold standard was createdby a team of human experts: 4 domain experts and2 linguists.
It includes 192 test verbs (typically fre-quent verbs in biomedical journal articles) classi-fied into 16, 34 and 50 classes, respectively.
Theclasses created by domain experts are labeled asBIO and those created by linguists as GEN. BIOclasses include 116 verbs whose analysis requireddomain knowledge (e.g.
activate, solubilize, har-vest).
GEN classes include 76 general or scientifictext verbs (e.g.
demonstrate, hypothesize, appear).Each class is associated with 1-30 member verbs.Table 1 illustrates two of the gold standard classeswith 1-2 example verbs per (sub-)class.3.2 Test DataWe downloaded the data from the MEDLINEdatabase, from eight journals covering various ar-SCF F1 98 39F2 247 64F3 486 75F3 + tense F4 490 79F5 920 176F3 + voice F6 488 77F7 682 153F3 + SP F8 150407 2112F9 13352 344F10A 110280 2091F10B 115208 2091F10C 114793 2091Table 3: (i) The total number of features and (ii)the average per verb for all the feature setseas of biomedicine.
The first column in table 2lists each journal, the second shows the years fromwhich the articles were downloaded, and the thirdindicates the size of the data.
We experimentedwith two test sets: 1) The 15.5M word sub-setshown in the first three rows of the table (this wasused for creating the gold standard).
2) All thedata: this new larger data was necessary for exper-iments with new feature sets as the most refinedones do not appear in 1) with sufficient frequency.4 Experimental Evaluation4.1 Processing the DataThe data was first processed using the feature ex-traction module.
Table 3 shows (i) the total num-ber of features in each feature set and (ii) the av-erage per verb in the resulting lexicon.
The clas-sification module was then applied.
We requestedK = 2 to 60 clusters from both clustering meth-ods.
We did not want to enforce the actual num-ber of classes but preferred to let the class hierar-chy emerge from the clustering results.
In orderto find the values of K where the clustering outputmight correspond to a level in the class hierarchywe used the relevance criterion.
For each method(clustering method and feature set combination)we choose as informative K?s the values for whichthe relevance information I(Clusters; SCFs)) in-creases more sharply between K?1 and K clustersthan between K and K+1.
We then chose for eval-uation the outputs corresponding only to informa-tive values of K. The clustering was run 50 timesfor each method.
The output is a result of votingamong the outputs of the runs.4.2 MeasuresThe clusters were evaluated against the gold stan-dard using four methods.
The first measure, the452adjusted pairwise precision, evaluates clusters interms of verb pairs:APP =1KKPi=1num.
of correct pairs in kinum.
of pairs in ki?|ki|?1|ki|+1APP is the average proportion of all within-cluster pairs that are correctly co-assigned.
Mul-tiplied by a factor that increases with cluster size itcompensates for a bias towards small clusters.The second measure is modified purity, a globalmeasure which evaluates the mean precision ofclusters.
Each cluster is associated with its preva-lent class.
The number of verbs in a cluster K thattake this class is denoted by nprevalent(K).
Verbsthat do not take it are considered as errors.
Clusterswhere nprevalent(K) = 1 are disregarded as not tointroduce a bias towards singletons:mPUR =Pnprevalent(ki)?2nprevalent(ki)number of verbsThe third measure is the weighted class accu-racy, the proportion of members of dominant clus-ters DOM-CLUSTiwithin all classes ci.ACC =CPi=1verbs in DOM-CLUSTinumber of verbsmPUR can be seen to measure the precision ofclusters and ACC the recall.
We define an F mea-sure as the harmonic mean of mPUR and ACC:F =2 ?mPUR ?
ACCmPUR + ACCThe experiments were run 50 times on each in-put to get the distribution of performance due tothe randomness in the initial clustering.
We calcu-lated the average performance and standard devia-tion from the results of these runs.4.3 Results for Test Set 1We first compared IB and PC on the smaller test set1 using feature set F2.
We chose for evaluation theoutputs corresponding to the most informative val-ues of K: 20, 33, 53 for IB, and 19, 26, 51 for PC.In the results included in table 4 IB shows slightlybetter performance than PC, but the difference isnot significant for K=34 and 50.
We decided to usePC for larger experiments because it has two ad-vantages over IB: 1) It can cluster the large test set2 with K = 10 ?
60 in minutes, while IB requiresa day for this.
2) It can deal with (and combine)different feature sets, while IB runs into numeri-cal problems.
Due to its speed and flexibility PCis thus more suitable for larger-scale experimentsinvolving comparison of complex feature sets.4.4 Results for Test Set 2Tables 5 and 6 include the PC results on the largertest set 2.
Table 5 shows the results for each in-dividual feature set (indicated in the second col-umn).
It shows also the standard deviations (?avg)of the four performance measures averaged acrossall the runs.
These are very similar for 16, 34, and50 classes and hence only included in one of thecolumns.
In addition, ?diffis indicated.
This is?2 ?
?avgand used for calculating the significanceof the performance differences.
In the followingdiscussion we consider a difference of more than2?diff(p > 97.7%) as significant.The first feature sets F1-F3 include basic SCF(frequency) information for each verb, F2-F3 re-fined with prepositions.
F2 shows clearly betterresults than F1 (over 10 F-measure) at all the levelsof gold standard.
This demonstrates the usefulnessof prepositions for the task.
When moving to F3the performance decreases for 34 and 50 classes,while improving for 16 classes, but these differ-ences are not statistically significant.Feature sets F4-F10 build on F3.
F4-F5 includeinformation about verb tense.
This informationproves quite useful for verb classification, partic-ularly when specific to individual SCFs.
Whencompared against the baseline featureset F3, F5is clearly better - particularly at 50 classes wherethe difference is 3.9 in F-measure (2?diff).
Verbvoice information is not equally helpful: F6-F7 arenot better than F3.
In some comparisons they areworse, e.g.
F7 vs. F3 at 16 classes.F8-F10 supplement F3 with information aboutSPs.
Surprisingly, these lexical and semantic fea-tures prove the most useful for our task.
At thelevel of 34 and 50 classes, the best SP features areeven better than the best tense features (the dif-ference is statistically significant), and they yieldnotable improvement over the baseline features(e.g.
6.8 difference in F-measure between F9 andF3).
The performance is not equally good at 16classes.
This makes perfect sense because classmembers are unlikely to have similar SPs at such acoarse level of semantic classification.When comparing the five sets of SPs featuresagainst each other, F9 and F10C produce the bestresults at 34 and 50 classes.
F9 uses raw (filtered)argument head data for SP acquisition while F10Cuses clustering.
It is interesting that the differ-ence between these two very different methods isnot statistically significant.
Whether one employs45316 Classes 34 Classes 50 ClassesAPP mPUR ACC F APP mPUR ACC F APP mPUR ACC FIB 74 77 66 71 69 75 81 77 54 72 79 75PC 71 78 58 67 64 71 81 75 63 71 73 72?
1.1 1.0 1.0 0.8 1.8 1.6 1.3 1.4 2.1 1.5 1.6 1.1Table 4: Performance on test set 116 Classes 34 Classes 50 ClassesAPP mPUR ACC F APP mPUR ACC F APP mPUR ACC FSCF F1 62.7 68.2 54.6 60.6 50.4 58.4 53.4 55.8 41.5 50.3 55.7 52.9F2 68.7 76.4 66.4 71.1 61.9 65.5 65.8 65.6 53.9 61.2 65.4 63.2F3 69.3 77.7 67.6 72.3 61.6 66.0 64.0 65.0 53.7 60.2 65.9 62.9F3 + tense F4 70.1 77.5 65.5 71.0 62.0 70.3 69.4 69.8 53.3 60.6 68.0 64.1F5 68.5 75.4 71.7 73.5 61.9 67.8 68.2 68.0 58.2 62.7 71.7 66.8F3 + voice F6 70.6 78.1 64.0 70.4 61.2 66.0 65.8 65.9 54.3 59.6 70.1 64.4F7 74.0 79.5 59.7 68.2 62.6 65.4 65.1 65.2 55.1 60.9 69.2 64.7F3 + SP F8 77.1 78.2 61.6 68.9 69.6 69.3 71.2 70.2 61.3 62.7 71.1 66.6F9 72.4 77.1 64.0 69.9 72.2 72.0 71.6 71.8 62.3 65.6 72.4 68.8F10A 75.6 80.0 63.2 70.6 66.1 69.2 70.6 69.9 59.4 63.5 69.0 66.2F10B 68.8 77.1 69.2 72.9 65.3 67.2 69.8 68.5 59.9 61.9 70.5 65.9F10C 74.1 78.9 65.7 71.7 68.8 71.7 69.7 70.7 59.8 63.4 71.1 67.0?avg2.2 1.5 1.8 1.4?diff3.1 2.1 2.5 2.0Table 5: Performance on test set 2: PC clustering results for individual feature sets at the three levels ofgold standard.
?avgand ?diffwere calculated across all the three classification levels.16 CL.
F5+F9 F4+ F10C F5 F5+ F8APP 72.3 68.2 68.5 72.2mPUR 76.4 77.0 75.4 76.5ACC 73.6 70.9 71.7 69.9F 75.0 73.8 73.5 73.034 CL.
F5+ F9 F5+ F8 F9 F4+ F10AAPP 68.7 71.0 72.2 62.9mPUR 70.1 71.0 72.0 68.4ACC 74.8 73.4 71.6 75.0F 72.4 72.2 71.8 71.550 CL.
F9 F5+ F9 F5+ F8 F4+ F9APP 62.3 59.8 62.8 59.7mPUR 65.6 63.8 64.1 63.1ACC 72.4 72.7 71.0 71.8F 68.8 68.0 67.4 67.1Table 6: Results for the top four feature set combi-nations.
All the feature sets build on F3.fine grained clusters (F10C) or coarse-grained ones(F10A) as SPs does not make much difference.We next combined various feature sets.
Table 6shows the performance for the top four combina-tions.
Comparing these results against the ones inTable 5, (see the ?diffvalues in Table 5) we can seethat combining feature sets does not result in betterperformance3.
The only exception is the differencein APP and mPUR between F9 and F4 + F10A atN=34.
However, these results show similar ten-dencies as the earlier ones: at 16 classes the most3Recall that all F4-F10 are actually already ?combined?with F3 - we do not refer to this combination here.useful features are based on verb tense, while at 34and 50 classes they are based on SPs.5 DiscussionThe results presented in the previous section arein interesting contrast with those reported in ear-lier work.
In previous work on general lan-guage verb classification, syntactic features (slotsor SCFs) have proved generally the most help-ful features, e.g.
(Schulte im Walde, 2006; Joa-nis et al, 2007).
The preliminary experiment onbiomedical verb classification (Korhonen et al,2006) experimented only with them.
In our ex-periments, SCFs proved useful baseline features.When we refined them further, we faced sparsedata problems: considerable improvement was ob-tained when moving from F1 to F2, but not whenmoving to F3.
Although many verb classes aresensitive to preposition types, many of the typesare low in frequency.
Future work could addressthis problem by employing smoothing techniques,or backing off to preposition classes.Joanis et al (2007) experimented with tenseand voice -based features in general English verbclassification.
They offered no significant im-provement over basic syntactic features.
Also inour experiments, we obtained little improvementwith voice features.
This could be due to the454un-distinctiveness of passive in biomedical textswhere it is used typically with high frequency.However, tense-based features clearly improvedthe baseline performance in our experiments.
Thiscould be partly because we ?parameterize?
POS in-formation for SCFs, and partly because semanti-cally similar verbs in biomedical language tend tobehave similarly also in terms of tense (Friedmanet al, 2002).Joanis (2002) and Schulte im Walde (2006) usedSP-based features in general English and Germanverb classifications, respectively.
The former ac-quired them from WordNet (Miller, 1990) andthe latter from GermaNet (Kunze, 2000).
Joa-nis (2002) obtained no improvement over syntacticfeatures while Schulte im Walde (2006) obtained,but the improvement was not significant.
In ourexperiments, SP features gave the best results andthe clearest improvement over the baseline featuresat the finer-grained levels of classification whereclass members are indeed likely to be the most uni-form in terms of their SPs.We obtained this improvement despite usinga fully unsupervised approach to SP acquisition.We did not exploit lexical resources like Joa-nis (2002) and Schulte im Walde (2006) becauseit would have required combining general re-sources (e.g.
WordNet) with domain specific ones(e.g.
UMLS).
We opted for a simpler approach inthis initial work ?
using raw argument heads andclustering ?
and obtained surprisingly good results.In our experiments filtering of raw argument headsand clustering with N=50 produced equivalent re-sults, suggesting that relatively fine-grained clus-ters are optimal.
Future work will require quali-tative analysis of noun clusters and comparison ofthese against classes in lexical resources to deter-mine an optimal method for SP acquisition.Does the fact that we obtain good results withfeatures which have not proved helpful in generallanguage classification indicate a need for domain-specific feature engineering?
We do not believeso.
The feature sets we experimented with are the-oretically well-motivated and should, in principle,also aid general language verb classification.
Webelieve they proved helpful in our experiments be-cause being domain-specific, biomedical languageis conventionalised and therefore less varied interms of verb sense and usage than general lan-guage.
For example, verbs have stronger SPs fortheir argument heads when many of their corpusoccurrences are of similar sense.
This renders SP-based features more useful for classification.Due to differences in the data, methods, and ex-perimental setup, direct comparison of our perfor-mance figures with previously published ones isdifficult.
The closest comparison point with gen-eral language is (Korhonen et al, 2003) which re-ported 59% mPUR using IB to assign 110 polyse-mous English verbs into 34 classes.
Our best re-sults are substantially better (72-80% mPUR).
Itis encouraging that we obtained such good resultsdespite focusing on a linguistically challenging do-main.In addition to the points mentioned earlier, ourfuture plans include seeding automatic classifica-tion with more sophisticated information acquiredautomatically from domain-specific texts (e.g.
us-ing named entity recognition and anaphoric link-ing (Vlachos et al, 2006)).
We will also exploresemi-automatic ML technology and active learn-ing in aiding the classification.
Finally, we plan toconduct a bigger experiment with a larger numberof verbs, make the resulting classification publiclyavailable, and demonstrate its usefulness for prac-tical BIO-NLP application tasks.6 ConclusionWe reported large-scale experiments to investigatethe optimal characteristics of features required forbiomedical verb classification.
A range of featuresets and associated extraction methods were intro-duced for this work, along with a robust cluster-ing method capable of dealing with large data andcomplex feature sets.
A number of experimentswere reported.
The best performing feature setsproved to be the ones which include informationabout SCFs supplemented with information aboutverb tense and SPs in particular.
The latter wereacquired automatically from corpus data using anunsupervised method.
Similar feature sets havenot proved equally useful in earlier work in gen-eral language verb classification.
We discussedreasons for this and highlighted several areas forfuture work.AcknowledgementWork on this paper was funded by the Royal So-ciety, EPSRC (?ACLEX?
project, GR/T19919/01)and MRC (?CRAB?
project, G0601766), UK.455ReferencesAnaniadou, S., B. D. Kell, and J. Tsujii.
2006.
Textmining and its potential applications in systems biol-ogy.
Trends in Biotechnology, 24(12):571?579.Briscoe, E. J. and J. Carroll.
1997.
Automatic extrac-tion of subcategorization from corpora.
In 5thACLConference on Applied Natural Language Process-ing, pages 356?363, Washington DC.Briscoe, E. J. and J. Carroll.
2002.
Robust accuratestatistical annotation of general text.
In 3rdInterna-tional Conference on Language Resources and Eval-uation, pages 1499?1504, Las Palmas, Gran Canaria.Cohen, K. B. and L. Hunter.
2006.
A critical review ofPASBio?s argument structures for biomedical verbs.BMC Bioinformatics, 7(3).Dang, H. T. 2004.
Investigations into the Role of Lexi-cal Semantics in Word Sense Disambiguation.
Ph.D.thesis, CIS, University of Pennsylvania.Dorr, B. J.
1997.
Large-scale dictionary constructionfor foreign language tutoring and interlingual ma-chine translation.
Machine Translation, 12(4):271?322.Friedman, C., P. Kra, and A. Rzhetsky.
2002.
Twobiomedical sublanguages: a description based on thetheories of zellig harris.
Journal of Biomedical In-formatics, 35(4):222?235.Joanis, E., S. Stevenson, and D. James.
2007.
A gen-eral feature space for automatic verb classification.Natural Language Engineering.Joanis, E. 2002.
Automatic verb classification using ageneral feature space.
Master?s thesis, University ofToronto.Korhonen, A., Y. Krymolowski, and N. Collier.
2006.Automatic classification of verbs in biomedical texts.In ACL-COLING, Sydney, Australia.Kunze, C. 2000.
Extension and use of germanet,a lexical-semantic database.
In 2nd InternationalConference on Language Resources and Evaluation,Athens, Greece.Levin, B.
1993.
English Verb Classes and Alterna-tions.
Chicago University Press, Chicago.Miller, G. A.
1990.
WordNet: An on-line lexicaldatabase.
International Journal of Lexicography,3(4):235?312.Prescher, D., S. Riezler, and M. Rooth.
2000.
Usinga probabilistic class-based lexicon for lexical am-biguity resolution.
In 18th International Confer-ence on Computational Linguistics, pages 649?655,Saarbr?ucken, Germany.Puzicha, J., T. Hofmann, and J. M. Buhmann.
2000.A theory of proximity-based clustering: structuredetection by optimization.
Pattern Recognition,33(4):617?634.Schulte im Walde, S. 2006.
Experiments on the au-tomatic induction of german semantic verb classes.Computational Linguistics, 32(2):159?194.Shi, L. and R. Mihalcea.
2005.
Putting pieces to-gether: Combining FrameNet, VerbNet and Word-Net for robust semantic parsing.
In Proceedings ofthe Sixth International Conference on Intelligent TextProcessing and Computational Linguistics, MexicoCity, Mexico.Sun, L., A. Korhonen, and Y. Krymolowski.
2008.Verb class discovery from rich syntactic data.
In9th International Conference on Intelligent Text Pro-cessing and Computational Linguistics, Haifa, Is-rael.Swier, R. and S. Stevenson.
2004.
Unsupervised se-mantic role labelling.
In Proceedings of the 2004Conference on Empirical Methods in Natural Lan-guage Processing, pages 95?102, Barcelona, Spain,August.Tishby, N., F. C. Pereira, and W. Bialek.
1999.
Theinformation bottleneck method.
In Proc.
of the37thAnnual Allerton Conference on Communica-tion, Control and Computing, pages 368?377.Vlachos, A., C. Gasperin, I. Lewin, and E. J. Briscoe.2006.
Bootstrapping the recognition and anaphoriclinking of named entitites in drosophila articles.
InPacific Symposium in Biocomputing, Maui, Hawaii.456
