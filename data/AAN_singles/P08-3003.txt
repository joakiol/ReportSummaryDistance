Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 13?18,Columbus, June 2008. c?2008 Association for Computational LinguisticsInferring Activity Time in News through Event ModelingVladimir EidelmanDepartment of Computer ScienceColumbia UniversityNew York, NY 10027vae2101@columbia.eduAbstractMany applications in NLP, such as question-answering and summarization, either requireor would greatly benefit from the knowledgeof when an event occurred.
Creating an ef-fective algorithm for identifying the activ-ity time of an event in news is difficult inpart because of the sparsity of explicit tem-poral expressions.
This paper describes adomain-independent machine-learning basedapproach to assign activity times to eventsin news.
We demonstrate that by applyingtopic models to text, we are able to clustersentences that describe the same event, andutilize the temporal information within theseevent clusters to infer activity times for all sen-tences.
Experimental evidence suggests thatthis is a promising approach, given evaluationsperformed on three distinct news article setsagainst the baseline of assigning the publica-tion date.
Our approach achieves 90%, 88.7%,and 68.7% accuracy, respectively, outperform-ing the baseline twice.1 IntroductionMany practical applications in NLP either requireor would greatly benefit from the use of temporalinformation.
For instance, question-answering andsummarization systems demand accurate process-ing of temporal information in order to be usefulfor answering ?when?
questions and creating coher-ent summaries by temporally ordering information.Proper processing is especially relevant in news,where multiple disparate events may be describedwithin one news article, and it is necessary to iden-tify the separate timepoints of each event.Event descriptions may be confined to one sen-tence, which we establish as our text unit, or bespread over many, thus forcing us to assign all sen-tences an activity time.
However, only 20%-30%of sentences contain an explicit temporal expres-sion, thus leaving the vast majority of sentenceswithout temporal information.
A similar proportionis reported in Mani et al (2003), with only 25%of clauses containing explicit temporal expressions.The sparsity of these expressions poses a real chal-lenge.
Therefore, a method for efficiently and accu-rately utilizing temporal expressions to infer activitytimes for the remaining 70%-80% of sentences withno temporal information is necessary.This paper proposes a domain-independentmachine-learning based approach to assign activitytimes to events in news without deferring to the pub-lication date.
Posing the problem in an informa-tion retrieval framework, we model events by ap-plying topic models to news, providing a way toautomatically distribute temporal information to allsentences.
The result is prototype system whichachieves promising results.In the following section, we discuss related workin temporal information processing.
Next we moti-vate the use of topic models for our task, and presentour methods for distributing temporal information.We conclude by presenting and discussing our re-sults.2 Related WorkMani and Wilson (2000) worked on news and in-troduced an annotation scheme for temporal ex-pressions, and a method for using explicit tempo-13Sentence Order Event Temporal Expression1 Event X None2 Event Y January 10, 20073 Event X None4 Event X November 16, 19675 Event Y None6 Event Y January 10, 20077 Event X NoneTable 1: Problematic Exampleral expressions to assign activity times to the en-tirety of an article.
Their preliminary work on in-ferring activity times suggested a baseline methodwhich spread time values of temporal expressionsto neighboring events based on proximity.
Fila-tova and Hovy (2001) also process explicit tempo-ral expressions within a text and apply this informa-tion throughout the whole article, assigning activitytimes to all clauses.More recent work has tried to temporally anchorand order events in news by looking at clauses (Maniet al, 2003).
Due to the sparsity of temporal ex-pressions, they computed a reference time for eachclause.
The reference time is inferred using a num-ber of linguistic features if no explicit reference ispresent, but the algorithm defaults to assigning themost recent time when all else fails.A severe limitation of previous work is the depen-dence on article structure.
Mani and Wilson (2000)attribute over half the errors of their baseline methodto propagation of an incorrect event time to neigh-boring events.
Filatova and Hovy (2001) infer timevalues based on the most recently assigned date orthe date of the article.
The previous approaches willall perform unfavorably in the example presented inTable 1, where a second historical event is referredto between references to a current event.
This kindof example is quite common.3 Modeling NewsTo address the aforementioned issues of sparsitywhile relieving dependence on article structure, wetreat event discovery as a clustering problem.
Clus-tering methods have previously been used for eventidentification (Hatzivassiloglou et al, 2000; Sid-dharthan et al, 2004).
After a topic model of newstext is created, sentences are clustered into topics -where each topic represents a specific event.
Thisallows us to utilize all available temporal informa-tion in each cluster to distribute to all the sentenceswithin that cluster, thus allowing for assigning of ac-tivity times to sentences without explicit temporalexpressions.
Our key assumption is that similar sen-tences describe the same event.Our approach is based on information retrievaltechniques, so we subsequently use the standard lan-guage of text collections.
We may refer to sentences,or clusters of sentences created from a topic modelas ?documents?, and a collection of sentences, or col-lection of clusters of sentences from one or morenews articles as a ?corpus?.
We use Latent Dirich-let Allocation (LDA) (Blei et al, 2003), a genera-tive model for describing collections of text corpora,which represents each document as a mixture over aset of topics, where each topic has associated with ita distribution over words.
Topics are shared by alldocuments in the corpus, but the topic distribution isassumed to come from a Dirichlet distribution.
LDAallows documents to be composed of multiple topicswith varying proportions, thus capturing multiple la-tent patterns.Depending on the words present in each docu-ment, we associate it with one ofN topics, whereNis the number of latent topics in the model.
We as-sign each document to the topic which has the high-est probability of having generated that document.We expect document similarity in a cluster to befairly high, as evidenced by document modeling per-formance in Blei et al (2003).
Since each cluster isa collection of similar documents, with our assump-tion that similar documents describe the same event,we conclude that each cluster represents a specificevent.
Thus, if at least one sentence in an event clus-ter contains an explicit temporal expression, we candistribute that activity time to other sentences in thecluster using an inference algorithm we explain inthe next section.
More than one event cluster mayrepresent the same event, as in Table 3, where bothtopics describe a different perspective on the sameevent: the administrative reaction to the incident atDuke.Creating a cluster of similar documents whichrepresent an event can be powerful.
First, we are nolonger restricted by article structure.
To refer back to14Table 1, our approach will assign the correct activ-ity time for all event X sentences, even though theyare separated in the article and only one contains anexplicit temporal expression, by utilizing an eventcluster which contains the four sentences describingevent X to distribute the temporal information1.Second, we are not restricted to using only onearticle to assign activity times to sentences.
In fact,one of the major strengths of this approach is theability to take a collection of articles and treat themall as one corpus, allowing the model to use allexplicit temporal expressions on event X presentthroughout all of the articles to distribute activitytimes.
This is especially helpful in multidocumentsummarization, where we have multiple articles onthe same event.Additionally, using LDA as a method for eventidentification may be advantageous over other clus-tering methods.
For one, Siddharthan et al (2004)reported that removing relative clauses and appos-itives, which provide background or discourse re-lated information, improves clustering.
LDA allowsus to discover the presence of multiple events withina sentence, and future work will focus on exploitingthis to improve clustering.3.1 CorpusWe obtained 22 news articles, which can be dividedinto three distinct sets: Duke Rape Case (DR), Ter-rorist Bombings in Mumbai (MB), Israeli-Lebaneseconflict (IC) (Table 2).
All articles come from En-glish Newswire text, and each sentence was manu-ally annotated with an activity time by people out-side of the project.
The Mumbai Bombing articlesall occur within a several day span, as do the Israeli-Conflict articles.
The Duke Rape case articles arean exception, since they are comprised of multi-ple events which happened over the course of sev-eral months: Thus these articles contain many casessuch as ?The report said...on March 14...?, wherethe report is actually in May, yet speaks of eventsin March.
For the purposes of this experiment wetook the union of the possible dates mentioned in asentence as acceptable activity times, thus both thereport statement date and the date mentioned in the1Analogously, our approach will assign correct activity timeto all event Y sentencesArticle Set # of Articles # of SentencesDuke Rape Case 5 151Mumbai Bombing 8 284Israeli Conflict 9 300Table 2: Article and Sentence distributionreport are correct activity times for the sentence.
Fu-ture work will investigate whether we can discrimi-nate between these two dates.Our approach relies on prior automatic linguisticprocessing of the articles by the Proteus system (Gr-ishman et al, 2005).
The articles are annotated withtime expression tags, which assign values to bothabsolute ?July 16, 2006?
and relative ?now?
tem-poral expressions.
Although present, our approachdoes not currently use activity time ranges, such as?past 2 weeks?
or ?recent days?.
The articles arealso given entity identification tags, which assigns aunique intra-article id to entities of the types speci-fied in the ACE 2005 evaluation.
For example, both?they?
- an anaphoric reference - and ?police offi-cers?
are recognized as referring to the same real-world entity.3.2 Feature ExtractionFrom this point on unless otherwise noted, refer-ence to news articles indicates one of the three setsof news articles, not the complete set.
We beginby breaking news articles into their constituent sen-tences, which are our ?documents?, the collectionof them being our ?corpus?, and indexing the doc-uments.We use the bag-of-words assumption to representeach document as an unordered collection of words.This allows the representation of each document asa word vector.
Additionally, we add any entity iden-tification information and explicit temporal expres-sions present in the document to the feature vectorrepresentation of each document.3.3 Intra-Article Event RepresentationTo represent events within one news article, we con-struct a topic model for each article separately.
TheIntra-Article (IAA) model constructed for an articleallows us to group sentences within that article to-gether according to event.
This allows the forma-tion of new ?documents?, which consist not of single15The administrators did not know of the racial dimension until March 24, the report said.The report did say that Brodhead was hampered by the administration?s lack of diversity.He said administrators would be reviewed on their performance on the normal scheduleand he had no immediate plans to make personnel changes.Administrators allowed the team to keep practicing; Athletics Director Joe Alleva calledthe players ?wonderful young men.
?Yet even Duke faculty members, many of them from the ?60s and ?70s generations thatpushed college administrators to ease their controlling ways, now are urging the universityto require greater social as well as scholastic discipline from students.Duke professors, in fact, are offering to help draft new behavior codes for the school.With years of experience and academic success to their credit, faculty members ought tobe listened to.For the moment, five study committees appointed by Brodhead seem to mean business,which is encouraging.Table 3: Two topics representing a different perspectiveon the same eventsentences, but a cluster of sentences representing anevent.
Accordingly, we combine the feature vectorrepresentations of the single sentences in an eventcluster into one feature vector, forming an aggregateof all their features.
Although at this stage we haveeverything we need to infer activity times, our ap-proach allows incorporating information from mul-tiple articles.3.4 Inter-Article Event RepresentationTo represent events over multiple articles, we sug-gest two methods for Inter-Article (IRA) topic mod-eling.
The first, IRA.1, is to combine the articlesand treat them as one large article.
This allows pro-cessing as described in IAA, with the exception thatevent clusters may contain sentences from multiplearticles.
The second, IRA.2, builds on IAA mod-els of single articles and uses them to construct anIRA model.
The IRA.2 model is constructed overa corpus of documents containing event clusters, al-lowing a grouping of event clusters from multiplearticles.
Event clusters may now be composed ofsentences describing the same event from multiplearticles, thus increasing our pool of explicit tempo-ral expressions available for inference.3.5 Activity Time AssignmentTo accurately infer activity times of all sentences, itis crucial to properly utilize the available temporalexpressions in the event clusters formed in the IRAor IAA models.
Our proposed inference algorithmis a starting point for further work.
We use the mostfrequent activity time present in an event cluster asthe value to assign all the sentences in that eventcluster.
In phase one of the algorithm we processeach event cluster separately.
If the majority of sen-tences with temporal expressions have the same ac-tivity time, then this activity time is distributed to theother sentences.
If there is a tie between the num-ber of occurrences of two activity times, both thesetimes are distributed as the activity time to the othersentences.
If there is no majority time and no tiein the event cluster, then each of the sentences witha temporal expression retains its activity time, butno information is distributed to the other sentences.Phase two of the inference algorithm reassemblesthe sentences back into their original articles, withmost sentences now having activity times tags as-signed from phase one.
Sentences that remain un-marked, indicating that they were in event clusterswith no majority and no tie, are assigned the ma-jority activity time appearing in their reassembledarticle.4 Empirical EvaluationIn evaluating our approach, we wanted to comparedifferent methods of modeling events prior to per-forming inference.?
Method (1) IAA then IRA.2 - Creating IAAmodels with 20 topics for each news article,and IRA.2 models for each of the three sets ofIAA models with 20, 50, and 100 topic.?
Method (2) IAA only - Creating an IAA modelwith 20 topics for each article?
Method (3) IRA.1 only - Creating IRA.1 modelwith 20 and 50 topics for each of the three setsof articles.4.1 ResultsTable 4 presents results for the three sets of articleson the six different experiments performed.
Sinceour approach assigns activity times to all sentences,overall accuracy is measured as the total number ofcorrect activity time assignments made out of thetotal number of sentences.
The baseline accuracyis computed by assigning each sentence the articlepublication date, and because news generally de-scribes current events, this achieves remarkably highperformance.16The overall accuracy measures performance ofthe complete inference algorithm, while the rest ofthe metrics measure the performance of phase oneonly, where we process each event cluster separately.Assessing the performance of phase one allows us toindirectly evaluate the event clusters which we cre-ate using LDA.
M1 accuracy represents the numberof sentences that were assigned the correct activitytime in phase one out of the total number of activ-ity time inferences made in phase one.
Thus, thisdoes not take into account any assignments made byphase two, and allows us to examine our assump-tions about event representation expressed earlier.
Alarge denominator in M1 indicates that many sen-tences were assigned in phase one, while a low oneindicates the presence of event clusters which wereunable to distribute temporal information.M2 looks at how well the algorithm performs onthe difficult cases where the activity time is not thesame as the publication date.
M3 looks at how wellthe algorithm performs on the majority of sentenceswhich have no temporal expressions.For the IC and DR sets, results show that Method(1), where IAA is performed prior to IRA.2 achievesthe best performance, with accuracy of 88.7% and90%, respectively, giving credence to the claim thatrepresenting events within an article before combin-ing multiple articles improves inference.The MB set somewhat counteracts this claim, asthe best performance was achieved by Method (3),where IRA.1 is performed.
This may be due to thefact that MB differs from DR and IC sets in that itcontains several regurgitated news articles.
Regurgi-tated news articles are comprised almost entirely ofstatements made at a previous time in other news ar-ticles.
Method (3) combines similar sentences fromall the articles right away, placing sentences from re-gurgitated articles in an event cluster with the orig-inal sentences.
This allows our approach to outper-form the baseline system by 4.3%, with and accu-racy of 68.7%.5 DiscussionThere are limitations to our approach which needto be addressed.
Foremost, evidence suggests thatevent clusters are not perfect, as error analysis hasshown event clusters which represent two or moreSet Setup Accur.
M1 M2 M3DR Base 135/15189.4%DR (1) 20 121/15180.1%55/8366.2%5/1241.6%27/4362.7%DR (1) 50 136/15190.0%91/10586.6%4/1330.7%60/6690.9%DR (1)100 128/15184.7%87/10979.8%4/1330.7%58/7082.8%DR (2) 20 106/15170.2%45/6866.2%4/1136.4%20/3360.6%DR (3) 20 111/15173.5%82/11074.7%8/1457.1%49/7169.0%DR (3) 50 99/15165.5%92/13568.1%6/1442.9%63/9566.3%Set Setup Accur.
M1 M2 M3MB Base 183/28464.4%MB (1) 20 166/28458.5%116/18762.0%41/6860.2%60/10457.7%MB (1) 50 152/28453.5%121/20658.7%41/7256.9%66/12055.0%MB (1)100 139/28448.9%112/20454.9%41/8150.6%60/12448.4%MB (2) 20 143/28450.3%103/16163.9%40/6363.5%49/8557.3%MB (3) 20 146/28451.4%99/16061.9%45/6470.3%47/8158.0%MB (3) 50 195/28468.7%123/18466.8%32/6747.8%74/10371.8%Set Setup Accur.
M1 M2 M3IC Base 272/30090.7%IC (1) 20 250/30083.3%158/20577.1%12/2254.5%118/15178.1%IC (1) 50 263/30087.7%168/19287.5%12/1963.2%127/13991.4%IC (1)100 266/30088.7%173/20285.6%11/2055.0%130/14987.2%IC (2) 20 250/30083.3%156/18186.2%11/1861.1%117/13090.0%IC (3) 20 225/30075.0%112/14577.2%14/2166.7%75/9578.9%IC (3) 50 134/30044.7%115/26243.9%14/2556.0%76/20636.9%Table 4: Results : Sentence Breakdown17events.
Event clusters which contain sentences de-scribing several events pose a real challenge, asthey are primarily responsible for inhibiting perfor-mance.
This limitation is not endemic to our ap-proach for event discovery, as Xu et al (2006) statedthat event extraction is still considered as one of themost challenging tasks, because an event mentioncan be expressed by several sentences and differentlinguistic expressions.One of the major strengths of our approach is theability to combine all temporal information on anevent from multiple articles.
However, due the im-perfect event clusters, combining temporal informa-tion from different articles within an event clusterhas not yet yielded satisfactory results.Although sentences from the same article in IRAevent clusters usually represent the same event, othersentences from different articles may not.
We mod-ified the inference algorithm to reflect this, and onlyconsider sentences from the same news article whendistributing temporal information, even though sen-tences from other articles may be present in the eventcluster.
Therefore, further work to construct eventclusters which more closely represent events is ex-pected to yield improvements in performance.
Fu-ture work will explore a richer feature set, includingsuch features as cross-document entity identificationinformation, linguistic features, and outside seman-tic knowledge to increase robustness of the featurevectors.
Finally, the optimal model parameters arecurrently selected by an oracle, however, we hope tofurther evaluate our approach on a larger dataset inorder to determine how to automatically select theoptimal parameters.6 ConclusionThis paper presented a novel approach for inferringactivity times for all sentences in a text.
We demon-strate we can produce reasonable event representa-tions in an unsupervised fashion using LDA, pos-ing event discovery as a clustering problem, and thatevent clusters can further be used to distribute tem-poral information to the sentences which lack ex-plicit temporal expressions.
Our approach achieves90%, 88.7%, and 68.7% accuracy, outperformingthe baseline set forth in two cases.
Although differ-ences prevent a direct comparison, Mani and Wil-son (2000) achieved an accuracy of 59.4% on 694verb occurrences using their baseline method, Fi-latova and Hovy (2001) achieved 82% accuracy ontime-stamping clauses for a single type of event on172 clauses, and Mani et al (2003) achieved 59%accuracy in their algorithm for computing a refer-ence time for 2069 clauses.
Future work will im-prove upon the majority criteria used in the inferencealgorithm, on creating more accurate event represen-tations, and on determining optimal model parame-ters automatically.AcknowledgementsWe wish to thank Kathleen McKeown and BarrySchiffman for invaluable discussions and comments.ReferencesDavid M. Blei, Andrew Y. Ng and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, vol.
3, pp.993?1022Elena Filatova and Eduard Hovy.
2001.
Assigning Time-Stamps to Event-Clauses.
Workshop on Temporal andSpatial Information Processing, ACL?2001 88-95.Ralph Grishman, David Westbrook, and Adam Meyers.2005.
NYU?s English ACE 2005 system description.In ACE 05 Evaluation Workshop.Vasileios Hatzivassiloglou, Luis Gravano, and AnkineeduMaganti.
2000.
An Investigation of Linguistic Fea-tures and Clustering Algorithms for Topical DocumentClustering.
In Proceedings of the 23rd ACM SIGIR,pages 224-231.Inderjeet Mani, Barry Schiffman and Jianping Zhang.2003.
Inferring Temporal Ordering of Events in News.Proceedings of the Human Language Technology Con-ference.Inderjeet Mani and George Wilson.
2000.
Robust Tem-poral Processing of News.
Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics, 69-76.
Hong Kong.Advaith Siddharthan, Ani Nenkova, and Kathleen McK-eown.
2004.
Syntactic simplification for improvingcontent selection in multi-document summarization.In 20th International Conference on ComputationalLinguistics .Feiyu Xu, Hans Uszkoreit, and Hong Li.
2006.
Auto-matic event and relation detection with seeds of vary-ing complexity.
In Proceedings of the AAAI WorkshopEvent Extraction and Synthesis, pages 1217, Boston.18
